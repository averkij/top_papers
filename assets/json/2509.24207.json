{
    "paper_title": "Humanline: Online Alignment as Perceptual Loss",
    "authors": [
        "Sijia Liu",
        "Niklas Muennighoff",
        "Kawin Ethayarajh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Online alignment (e.g., GRPO) is generally more performant than offline alignment (e.g., DPO) -- but why? Drawing on prospect theory from behavioral economics, we propose a human-centric explanation. We prove that online on-policy sampling better approximates the human-perceived distribution of what the model can produce, and PPO/GRPO-style clipping -- originally introduced to just stabilize training -- recovers a perceptual bias in how humans perceive probability. In this sense, PPO/GRPO act as perceptual losses already. Our theory further suggests that the online/offline dichotomy is itself incidental to maximizing human utility, since we can achieve the same effect by selectively training on any data in a manner that mimics human perception, rather than restricting ourselves to online on-policy data. Doing so would allow us to post-train more quickly, cheaply, and flexibly without sacrificing performance. To this end, we propose a design pattern that explicitly incorporates perceptual distortions of probability into objectives like DPO/KTO/GRPO, creating humanline variants of them. Surprisingly, we find that these humanline variants, even when trained with offline off-policy data, can match the performance of their online counterparts on both verifiable and unverifiable tasks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 7 0 2 4 2 . 9 0 5 2 : r preprint HUMANLINE: ONLINE ALIGNMENT AS PERCEPTUAL LOSS Sijia Liu Princeton University sijia.liu@princeton.edu Niklas Muennighoff Stanford University muennighoff@stanford.edu Kawin Ethayarajh University of Chicago kawin@uchicago.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Online alignment (e.g., GRPO) is generally more performant than offline alignment (e.g., DPO)but why? Drawing on prospect theory from behavioral economics, we propose human-centric explanation. We prove that online on-policy sampling better approximates the human-perceived distribution of what the model can produce, and PPO/GRPO-style clippingoriginally introduced to just stabilize trainingrecovers perceptual bias in how humans perceive probability. In this sense, PPO/GRPO act as perceptual losses already. Our theory further suggests that the online/offline dichotomy is itself incidental to maximizing human utility, since we can achieve the same effect by selectively training on any data in manner that mimics human perception, rather than restricting ourselves to online on-policy data. Doing so would allow us to post-train more quickly, cheaply, and flexibly without sacrificing performance. To this end, we propose design pattern that explicitly incorporates perceptual distortions of probability into objectives like DPO/KTO/GRPO, creating humanline variants of them. Surprisingly, we find that these humanline variants, even when trained with offline off-policy data, can match the performance of their online counterparts on both verifiable and unverifiable tasks."
        },
        {
            "title": "INTRODUCTION",
            "content": "Aligning generative models with feedbackfrom human, learned reward model, or groundtruth verifieris an increasingly important part of post-training, with methods categorized as offline off-policy (e.g., DPO, KTO) or online on-policy (e.g., GRPO). Despite flurry of initial optimism around the former, recent work concurs that the latter have higher performance ceiling, though they come at the cost of more compute, training time, and instability (Xu et al., 2024b; Ivison et al., 2024). But why are they better? Explanations range from online methods having better data coverage (Song et al., 2024), emphasizing generation over discrimination (Tang et al., 2024a), and navigating simpler search space over policies (Swamy et al., 2025). Although all these explanations have merit, we argue that if the goal is to maximize models utility to humans, then the dichotomy itself is incidental. We start with prospect theory, framework in behavioral economics that explains why humans make decisions about random variables that do not necessarily maximize their expected value (Tversky & Kahneman, 1992). Classically, the random variable would describe monetary outcome, measured in dollars; when extended to generative modeling, it describes the goodness of outputs, measured in nats (Ethayarajh et al., 2024). Prospect theory offers well-defined and empirically validated model of the subjective probability distribution that humans implicitly assign to outcomes. As we will show, compared to random offline data, online on-policy sampling better approximates the putative human-perceived distribution of what the model can produce, offering human-centric explanation for why online alignment should be more performant (3). Equal contribution. Work started while the last author was at Princeton Language & Intelligence. 1 preprint Figure 1: On instruction-following, Llama3-8B-Instruct aligned with online on-policy data (blue) is 1.3x to 1.6x better than one aligned with offline off-policy data (red). However, when the same offline data is fed to the humanline variant of the objective (orange), the gap vanishes. However, this also suggests that online on-policy data is suboptimal on its own, as it reflects what the policy is literally capable of producing, as opposed to what humans perceive it is capable of: for example, people systematically overestimate the chance of extreme outcomes and underestimate the chance of typical ones. We then prove that PPO/GRPO-style clippingoriginally introduced to just stabilize training (Schulman et al., 2017)implicitly recovers special case of this perceptual bias, as formalized in prospect theory. In other words, state-of-the-art alignment methods are, to some extent, perceptual losses already (4). If the success of PPO/GRPO can be ascribed to them being perceptual losses, then we do not necessarily need online on-policy data: we can source data from anywhereonline, offline, on-policy, off-policyand selectively use it in manner that reflects human perception. If we can source data from anywhere while not sacrificing performance, then state-of-the-art post-training becomes much faster and cheaper. To this end, we propose design pattern for creating variant of most alignment objectives (including DPO, KTO, and GRPO) that explicitly incorporates these perceptual distortions of probability while keeping the rest of the pipeline intact. This amounts to: (1) syncing the reference model with the previous version of the policy at the end of each step; (2) asymmetrically clipping the log-probability ratio of each token upstream of the loss. These simple changes, when applied correctly, create what we call the humanline variant of the original objective. We consider two testbeds (5), where an LLM is aligned to be better at: (1) instruction-following, with unverifiable rewards; (2) mathematical reasoning, with verifiable feedback. On instructionfollowing, an LLM trained with the online variant of DPO/KTO/GRPO has 1.3x to 1.6x higher winrates against frontier model than one trained with the offline variant of the objective; when the same offline data is fed to the humanline variant however, the gap with online alignment vanishes (Figure 1). Even on mathematical reasoning, where human utility is seemingly irrelevant, humanline GRPO allows training data to be sampled up to 64x less frequently without performance degradation. We do not claim that you can match the performance of online alignment by simply using the humanline variant on any offline data; data quality always matters. Rather, what we find is that when humanline variant is employed, there is consistently some offline data that renders the final model as good as its online-aligned counterpart. This is an empirical regularity, not guarantee. Still, its importance should not be understated: humanline variants offer us the flexibility to source good-quality data from anywhere, which has the potential to not only make post-training many times faster and cheaper, but also make models much more adaptable to new tasks and user populations."
        },
        {
            "title": "2 BACKGROUND",
            "content": "We provide high-level overview of alignment methods and leave more detailed survey to Appendix A. For the sake of brevity, we will at times refer to the online on-policy(offline off-policy) variant of method as the online(offline) variant, in line with the literature. In most alignment al2 preprint gorithms, including all those discussed in this paper, two copies are made of our initial model: reference model πref that serves as an anchor, whose weights are not backpropagated through; and policy πθ whose parameters θ are updated to minimize the loss. Online On-policy Alignment Samples are drawn from the policy, labeled with feedbackfrom learned reward model, ground-truth verifier, etc.and fed into loss function, which is minimized by updating θ. This is done iteratively until the desired level of progress has been made, with the reference model periodically synced with the policy. The choice of loss function depends on many factors. Proximal Policy Optimization (PPO) (Schulman et al., 2017) has long been the default, since its clipped objective helps reduce training instability. Given that Grouped Relative Policy Optimization (GRPO) simplifies PPO while often improving performance (Shao et al., 2024), we use it instead. Its objective is to maximize: LGRPO(θ) = xD,{yi}G i=1πθold (x)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 yi yi (cid:88) t=1 {min[rθ(i, t) ˆAi,t, clip(rθ(i, t), 1 ε, 1 + ε) ˆAi,t] β KL[πθ π0]} (1) where yi is an output sequence, θold is the last version of the policy, ˆAi,t = (Ri mean(R))/std(R) is the sequence-level advantage of output yi compared to other outputs (applied per token), {ϵ, G, β} are constants, and rθ(i, t) = πθ(yi,tx, yi,<t)/πθold (yi,tx, yi,<t) is token-wise probability ratio. KL denotes the token-wise forward KL divergence between the policy and fixed baseline π0 (e.g., the initial model).1 Offline Off-policy Alignment Online alignment is often unstable and slow, since new data needs to be continually sampled and labeled. For this reason, offline off-policy alignment has emerged as popular alternative. Here, outputs are not drawn from the policy but from another source (e.g., human demonstrations), then fed into closed-form loss that is minimized by updating θ. The choice of loss again depends on many factors, but the most popular options are DPO (Rafailov et al., 2023), which operates on preference pairs (x, yw, yl) where yw yl, and KTO (Ethayarajh et al., 2024), which operates on unpaired feedback (x, yw) and (x, yl) (see Appendix for precise definitions). Online vs. Offline Recent work concurs that online alignment has higher performance ceiling, although this comes at the expense of more compute, training time, and instability (Xu et al., 2024b; Ivison et al., 2024). Recognizing their complementary strengths, some have proposed online versions of offline methods and vice-versa. For example, online DPO trains on samples generated from the latest version of the policy, closing muchbut not allof the gap with standard PPO (Guo et al., 2024; Xu et al., 2024b). Conversely, offline PPOwhere the reference model is never synced and training data is staticperforms similarly to offline DPO (Ethayarajh et al., 2024). Explanations of why online alignment works better have traditionally been rooted in RL theory (Song et al., 2024; Tang et al., 2024a; Swamy et al., 2025) and are thus complementary to this work. Verifiability The literature increasingly focuses on verifiable tasks whose correctness can be checked programmatically, such as mathematical reasoning (Lambert et al., 2025). When correctness is determined by preferences or open-ended judgments, the task is considered unverifiable."
        },
        {
            "title": "3 ALIGNMENT AS PROSPECT THEORETIC OPTIMIZATION",
            "content": "Given gamble that returns +$100 with 80% probability and $100 with 20% probability, how much would player have to be offered to forgo playing? Classical decision theory tells us that an agent meeting certain axioms of rationality (Von Neumann & Morgenstern, 1947) would have to be offered the expected value of the gamble: 0.8(+$100) + 0.2($100) = +$60. Most humans in this situation accept far less than $60 however, even though in expectation they could make more money gambling. Prospect theory offers general framework of why, when presented with an uncertain 1Note that under our terminology πθold would be called the reference model, since it determines the ratio rθ(i, t), and π0, which is called the reference in Shao et al. (2024), would be called the baseline. This is to ensure terminological consistency with offline methods. 3 preprint Figure 2: We prove that human utility is maximized when outputs are sampled from the typical human-perceived distribution of what the policy can produce, whose inverted S-shape comes from prospect theory. Online on-policy sampling (dashed black) is superior to offline off-policyboth from worse (red) and better (blue) modelsbecause the latter deviate more from human perception (solid black). Rejection-sampling with perceptual bias gives us humanline sampling (green) that can mimic this, and special case of it simplifies to the humanline clipping used in our design pattern. event, humans may choose not to maximize their expected value (Kahneman & Tversky, 1979; Tversky & Kahneman, 1992). Its model of human utility is as follows: Definition 3.1. value function : maps an outcome z, relative to reference point z0, to its subjective value as perceived by the human. When is real-valued, the typical form of is: v(z; λ, α, z0) = (cid:26)(z z0)α λ(z0 z)α if z0 if < z0 (2) where λ, α R+ are constants. The salient qualities of value function are: the existence of reference point z0 used to determine the relative gain or loss; concavity in relative gains (α < 1), known as risk aversion; and greater sensitivity to relative losses than gains (λ > 1), known as loss aversion. Under these settings, it is easy to see how the expected subjective valueas induced by vcould be less than $60. Definition 3.2. The weighting function ω, when applied to an outcome zi, supplants its objective probability. Let pi denote the objective probability of outcome zi and Ω+ capacity function that maps cumulative probabilities to perceived cumulative probabilities. typical functional form for the capacity function is Ω+(a; γ) = aγ (aγ + (1 a)γ)1/γ (3) where γ R+ is constant. γ = 1 recovers the objective probability but lies in (0, 1) for most humans. Letting zi denote positive outcome relative to z0 and z1, ..., zn the ordered outcomes from least to most positive, the weights are then: ω(zi) = (cid:40) Ω+((cid:80)n Ω+(pn) j=i pj) Ω+((cid:80)n j=i+1 pj) if < if = (4) If zi were instead negative outcome relative to z0, then it would be compared to outcomes even more negative than it, with separate function Ω(a; γ) following the same form as (3). For example, suppose that in our gamble, there were now two positive outcomes instead of one: winning +$50 with 60% probability and winning +$100 with 20% probability. The probability of an outcome as good or better than $50 is 0.8 (= 0.6 + 0.2) and one as good or better than $100 is 0.2 (itself); these are our cumulative probabilities. Since Ω+ captures the human tendency to overweight extreme outcomes at the expense of moderate ones, let us say the perceived cumulative probabilities are 0.8 and 0.3 respectively. By applying (4), we then get weights of 0.5 (= 0.8 0.3) for the $50 outcome and 0.3 for the $100 outcome. That is, the extreme outcome of winning $100 has been up-weighted from its objective probability of 0.2 to subjective probability of 0.3 while the moderate outcome of winning $50 has been down-weighted from 0.6 to 0.5. 4 preprint Definition 3.3. The expected subjective utility of random variable is weighted combination of the subjective values of its outcomes: u(Z; ω) (cid:80) zZ ω(z)v(z; λ, α, z0). Although every human has unique value and capacity function, the functional forms in (2) and (3) describe those belonging to the majority of people in human studies (Tversky & Kahneman, 1992).2 In the original literature, random variables were only studied in monetary context, where outcomes can be measured in dollars. Ethayarajh et al. (2024) were the first to extend prospect theory to the alignment of generative models. Taking any output (token or sequence) given context x, they treat the surprisal term zx,y = log[πθ(yx)/πref(yx)] as the outcome, whose units are nats of information. They propose that the goal of alignment is to modify θ such that desirable outputs have zx,y > z0 and undesirable outputs have zx,y < z0, formally proving that all the commonly used alignment objectivesDPO, KTO, PPO, and GRPOencode prospect theoretic model of utility3, differing only in the shape of their value function and the distribution over which the expected surprisal is taken to construct z0. However, they ignore the weighting function, assuming that the human perception of probability is effectively objective when it comes to generative model outputs. But what if it were not? It is intractable to infer the human-perceived probability distribution over large output spaces (e.g., token vocabularies), which is why the original prospect theory experiments were limited to handful of possible monetary outcomes (Tversky & Kahneman, 1992). Because of this, we will assume that the perceptual distortion of probability in the generative model setting has the same shape as in the monetary setting (Figure 2), allowing us to use the well-established parameterization in Definitions 3.1 through 3.3. Proposition 3.4. For any input and bounded value function v, let the outcome of an output be its surprisal log[πθ(yx)/πref(yx)] and be candidate distribution over outcomes. Then to guarantee u(Z; ω) u(Z; Q) δ for some δ 0, it suffices that (cid:112)KL(ωQ) δ/ (cid:0) (cid:1). 2v The proof is deferred to Appendix B. This means that even if we had oracle access to value function and thus knew exactly which alignment objective to use, we could not necessarily maximize human utility. As the proposition suggests, the simplest way to do so would be to sample generations according to the subjective distribution that was implicitly assigned to the outputs. This offers human-centric explanation for why online on-policy sampling is superior to offline offpolicy sampling for alignment, one that is complementary to the RL-theoretic explanations in prior work. As illustrated in Figure 2, if the median human capacity function (solid black) is function of the probabilities from the current version of the policy, then the subjective probabilities will loosely track online on-policy sampling (dashed black). In contrast, offline off-policy sampling can deviate sharply from both. Consider the desirable outputs for some context x: 1. When sampling from model worse than the policy, the outputs surprisalscomputed under the current policywill on average be lower (i.e., less positive), since they are more likely under the worse model and less likely under the current one. Recall that in (3), positive outcomes are ordered from least good to most good. This means that the implied capacity function (red) will saturate much more quickly than the human capacity function. 2. Conversely, when sampling from model better than the current policy, surprisals will be larger. Given that more-positive outputs are more plentiful than they would be under the policy, the implied capacity function (blue) will saturate more slowly. Returning to Proposition 3.4, if we cannot directly sample the perceived distribution, then natural solution is to rejection-sample our outputs to simulate the drawing of tokens according to their subjective probabilities.4 Moreover, this allows us to use data sourced from anywhere instead of limiting ourselves to online on-policy data. In 4, we modify the standard rejection sampling algorithm to capture the perceptual bias in (4), which we call humanline sampling. By tweaking its hyperparameters, we can mimic wide range of distributions (Figure 2, green). 2Other parameterizations have been proposed, however: Prelec (1998); Gonzalez & Wu (1999), inter alia. 3Even losses without an explicit surprisal term, such as PPO and GRPO, do have tokenwise likelihood ratio [πθ(yx)/πref(yx)] that can be framed as the exponentiated surprisal. 4Although importance sampling is another option, it comes with its own problems in the context of generative models, such as degenerate importance weights. 5 preprint"
        },
        {
            "title": "4 CLIPPING RECOVERS PERCEPTUAL BIAS",
            "content": "In 3, we established that human utility is maximized when outputs are drawn according to the human-perceived distribution. Given that we do not have access to anyones perceived distribution, we will instead modify the standard rejection sampling algorithm to simulate drawing from the typical humans, as formalized by prospect theory (4). We call this humanline sampling. However, instead of sampling data anew from some proposal distribution, we will use already-sampled sequences by leveraging the following assumptions: Assumption 4.1. The proposal distribution is the current iteration of the reference model: i.e., any output sequence was produced by autoregressively sampling tokens from πref(x, yi,<t). Moreover, πref(x, yi,<t) and πθ(x, yi,<t) have the same support and finite likelihood ratio bound. Assumption 4.2. The capacity functions Ω+, Ω have the standard functional form (3). Assumption 4.3. For context with output sequence yi, let surprisal zi = log[πθ(yx)/πref(yx)] denote the outcome. The cumulative probability of outcomes with higher absolute surprisal than zi is negligible (i.e., assume we are in the tail regime). The first assumption allows us to work with any data, whether it is online on-policy or offline offpolicyor mixture of bothby assuming common origin, even though it is difficult to fully satisfy in practice. The last assumption means that if an output in our dataset has positive outcome under the current policy, then the proportion of outputs that are as good or better for the same context is vanishingly small. This assumption is based on the fact that for any context, the vast majority of all possible output sequences is nonsensical or irrelevant. Proposition 4.4. For any context x, simulating output sequences from ω is equivalent to performing token-wise rejection sampling with the rejection criterion πθ(ytx; y<t)/πref(ytx; y<t) < , where Beta(γ, 1), is finite upper bound on the token-level likelihood ratio under the vocabulary (i.e., yt, πθ(ytx;y<t) πref(ytx;y<t) < ), and γ (0, 1] is the capacity function constant. We defer the proof to Appendix B. Still, applying rejection sampling during training comes with several practical concerns. For one, in an online setting, both the reference and policy models change, and the objective probabilities that are fed into human observers capacity function could reflect exposure to either the current policy or the previous one. Second, resampling only those tokens that have been rejected while leaving the others untouched will not guarantee that the final output is coherent or relevant. Third, zeroing out the rejected tokens could destabilize sequencewise losses like KTO whose training dynamics are affected by the saturation induced by all tokens. Taking this into account, we propose humanline sampling: Definition 4.5. Given output sequence y, humanline sampling rejects tokens yt that meet the following rejection criteria by detaching them from the computational graph: πθ(ytx; y<t) πref(ytx; y<t) < MP BP or πref(ytx; y<t) πθ(ytx; y<t) < MRBR (5) where MP , MR are constants such that πθ(ytx; y<t) < MP πref(ytx; y<t) and πref(ytx; y<t) < MRπθ(ytx; y<t) for all yt, BP Beta(γP , βP ) and BR Beta(γR, βR) are independent Beta random variables, and γP , γR, βP , βR are Beta distribution-specific constants. The two-sided criteria address the first concern about the origins of the objective probabilities that are fed to human observers capacity functions in an online setting. Keeping the rejected tokens in the sequence addresses the second and third concerns, while detaching the tokens from the computational graph (i.e., stopping gradient flow for those tokens) ensures that they do not contribute to the updates of θ that minimize the loss. Even though we are not resampling tokens, γR, γP effectively control an exploration-exploitation trade-off. If γP < γR, there is more emphasis on drawing from the policy (i.e., more exploitation); if γP > γR, there is more emphasis on exploration. Theorem 4.6. The clipped component in PPO/GRPO is special case of humanline sampling that arises under limit conditions. We defer the proof to Appendix B. The intuition is that there exists construction such that sampling from the Beta distributions is equivalent to deterministically sampling their means. The two criteria can then be combined into range that the likelihood ratio must fall in, analogous to the clipping 6 preprint Figure 3: In offline objectives (left), the reference model does not change during training. In online objectives (middle), the reference is synced with the policy at the current step; at scale, some asynchrony is permitted (a lag of one step is depicted here). In humanline syncing (right), every steps, the reference is synced with the policy from the previous step (k = 1 is depicted here). Figure 4: In humanline clipping, the token-wise likelihood ratios rθ(i, t) are asymmetrically clipped to [ϵP , ϵR] upstream of the loss. In the humanline variant of GRPO, instead of there being an unclipped rθ and [1 ϵ, 1 + ϵ]-clipped rθ as in (1), we have once-clipped and twice-clipped rθ. Though humanline clipping should in theory be most impactful for losses without any clipping to begin with (e.g., DPO, KTO), it still benefits GRPO (see Figure 5, left). range. In both cases, the gradient is zero outside this range: the clipping function due to its derivative, and humanline sampling because it explicitly stops the gradients for those tokens from flowing through the graph. However, the unclipped component in PPO/GRPO does allow ratios outside this range to affect the overall gradient of the loss; to more fully integrate this perceptual bias, we would need to clip the ratios upstream of the objective, not just within it (5)."
        },
        {
            "title": "5 HUMANLINE VARIANTS",
            "content": "5.1 METHOD If the success of PPO/GRPO can be ascribed to them being perceptual losses (3, 4), then we need not limit ourselves to using online on-policy data; we can source data from anywhere and selectively train on it in manner that reflects the prospect theoretic model of perceived probability. To this end, we propose creating humanline variant of any alignment objective that is function of both policy πθ and reference model πref.5 This is done by applying two-part design pattern: 1. Humanline Syncing: Every steps, after the loss is calculated but before the optimizer step is taken, sync the weights of πref with πθ (Figure 3). In general, lower leads to better performance but also more instability (Figure 7, Appendix D). 2. Humanline Clipping: Clip all token-wise likelihood ratios πθ(ytx, y<t)/πref(ytx, y<t) to the range [ϵP , ϵR] even before they are fed into the loss, where ϵP , ϵR R+ and the range can be asymmetric. Losses that already do some clipping, such as GRPO, will do clipping twice over (Figure 4). We clip in log-space for greater numerical precision. The motivation behind humanline syncing is that as the policy changes over the course of training, the standard against which the policy is judged also changes. Since the outcome is defined as the surprisal log[πθ(yx)/πref(yx)], this means that the reference model must change as well, at rate 5As defined in 2, the reference model is the one against which the surprisal is calculated (explicitly, in the log-ratios of DPO and KTO; implicitly in the ratios of PPO and GRPO). 7 preprint Figure 5: The majority of the improvement comes from humanline syncing (left). While humanline clipping does add surplus benefit, without syncing, the performance would be no better than that of regular offline variants. Although humanline clipping is special case of the more general humanline sampling (4), it performs as well while being stabler and simpler to implement (right). controlled by k. We choose to implement humanline clipping instead of the humanline sampling proposed in 3 for few different reasons. For one, humanline clipping is special case of humanline sampling, one that arises under limit conditions (Theorem 4.6). However, it is much faster (since no new tensors have to be allocated), requires fewer hyperparameters, and is more stable than humanline sampling while being as or more performant (Figure 5, right). Clipping multiple times (Team et al., 2025) and asymmetric clipping (Yu et al., 2025) have been explored in past work, but to our knowledge, the specific formulation in humanline clipping has not been used. Note that the humanline variant of each method can be used with both online on-policy data and offline off-policy data, which we denote as online+humanline and offline+humanline respectively. In contrast, the online variant of method is only used with online on-policy data; the offline variant, only with offline off-policy data. Alignment objectives without reference model, such as SimPO (Meng et al., 2024), cannot have humanline variant because neither change is applicable. 5.2 EXPERIMENTS We create humanline variants of DPO/KTO/GRPO and compare them to both their offline and online counterparts, ensuring that the number of examples seen by the different variants is the same. Details on how we created the online version of DPO/KTO and the offline version of GRPO can be found in Appendix C. We test these variants in an unverifiable reward setting where the goal is to follow openended instructions and verifiable reward setting where the goal is to do mathematical reasoning. 5.2.1 UNVERIFIABLE REWARDS Using offline DPO/KTO/GRPO, we first align Llama3-8B-Instruct (AI@Meta, 2024) on an instruction-following dataset called UltraFeedback ArmoRM (Meng et al., 2024). For online DPO/KTO/GRPO, we use the same contexts but sample completions from the policy, score them with the ArmoRM reward model (Wang et al., 2024), and then construct preference pairs. This is how the offline data was constructed as wellbut sampled from different modelsallowing for an apples-to-apples comparison between the online and offline variants of the same objective. Using the humanline variants with either the online or offline data does not require further changes. The models are evaluated with AlpacaEval2 (Dubois et al., 2024).6 For all objectives, the offline+humanline variant performs significantly better than the offline variant (p < 0.05)7 and is on par with the online variant, as seen in Figure 1. The magnitude of improvement is large, with offline+humanline GRPO performing 1.6x better than its offline counterpart. Improvements persist at the 27B scale and with different model families (Appendix D). However, the online+humanline variants are only slightly better than their online counterparts. This is not surprising: under our theory (3), online on-policy sampling is superior to offline off-policy 6We use GPT 4.1 as the judge instead of the default GPT-4-Turbo, as it is cheaper and more performant. 7We apply the Holm-Bonferroni correction to adjust for multiple comparisons (Holm, 1979; Dunn, 1961). 8 preprint Figure 6: For mathematical reasoning (MATH500), sampling data 64x less frequently (orange) than in online GRPO (blue) leads to significantly worse performance, even though the total volume of data seen remains the same. In contrast, using the humanline variant of GRPO while being 64x more offline does not incur performance degradation (green). Less frequent humanline syncing (k = 20, violet) leads to slower but more stable learning; at = 1, the instability would cause collapse. sampling because it deviates far less from human perception; the marginal benefit of humanline objectives will naturally be smaller in this case. Humanline objectives do not obviate the need for good-quality offline data. We stress that although offline+humanline variants can match the performance of their online counterparts, this is not given for any offline data. For example, when aligning Gemma2-27B-Instruct using offline+humanline DPO on two different versions of the UltraFeedback ArmoRM datasetboth versions having the same prompts but completions from different sub-10B parameter LLMsonly one model matches the performance of its online-aligned counterpart (Appendix D, Table 5). This is in part because the more the data distribution diverges from πref, the less Assumption 4.1 will hold. Humanline syncing is responsible for most of the improvement; the effect of humanline clipping is neutral-to-positive. In Figure 5, we plot the drop in performance as one or both changes are ablated. Humanline syncing, done here every step (k = 1) is the more crucial ingredient; without it, the performance would be as bad as with the offline variant. However, it can be done as infrequently as = 4 without loss in performance (Appendix D, Figure 7). Note that not all kinds of syncing are equal: trust region-style syncing (Gorbatovski et al., 2024), which happens after the policy is updatedthus rendering the policy and reference equalleads to worse results (Appendix D, Figure 9). Humanline clipping is still needed for the offline+humanline variants to match the performance of their online counterparts (Figure 5, left). For instruction-following, clipping range of log ϵP = 1.5, log ϵR = 1.5 ϵP = 0.22, ϵR = 4.48 works best for the humanline variants of all methods, and performance is robust to small changes (Appendix D,Table 6). Humanline variants do not require changing method-specific hyperparameters, but the learning rate or maximum gradient norm need to be adjusted. The use of humanline variant introduces two counteracting forces. On one hand, the likelihood ratios rθ can get smaller compared to offline learningexplicitly, due to clipping, and implicitly, due to the syncing of the reference model, since πθ(yx) cannot drift too far from πref(yx)causing gradients to get smaller. The learning rate or maximum gradient norm needs to increase to make up for this. Conversely, updating the reference model introduces more training instability, which demands lower learning rate or maximum gradient norm. Therefore, depending on the circumstances, this shift could require increasing or decreasing the learning rate/gradient norm by 0.1x4x (Appendix D, Table 3). Offline+humanline GRPO is over 6x faster to train with than the online variant, while attaining equal performance. Compared to offline GRPO, the offline+humanline variant takes roughly twice as long to run when syncing every step (Appendix D, Figure 10). However, this is comparatively small price to pay to match the performance of online GRPO, which takes over 12x the wall-clock time of offline GRPO. 5.2.2 VERIFIABLE REWARDS When doing alignment for mathematical reasoning, it is standard to be fully online on-policy and use the correctness of the final output as the only reward (DeepSeek-AI et al., 2025). Our goal 9 preprint with the humanline variants will be to push the extent to which the data can be offline off-policy. For example, sampling completions every 10 steps instead of every step would make the process much more efficient: in the fully online on-policy setup, training waits on the next batch of samples from the current policy and inference requires the policy to finish training on the current batch; by sampling less frequently, training, inference, and labeling can all be asynchronously overlapped. Humanline GRPO allows data to be sampled up to 64x less often with no performance degradation on mathematical reasoning. We first align Qwen2.5-1.5B-Instruct8 (Yang et al., 2025) with online GRPO on the MATH500 training set (Lightman et al., 2023), largely following the setup in Open-R1 (Hugging Face, 2025) and assigning rewards based on formatting and correctness. Instead of sampling every step, we then sample 64 times as much data every 64 steps to get model that is significantly worse (p < 0.05) (Figure 6). Running the same off-policy setup with the humanline variant of GRPO closes the gap in rewards within 1000 steps. After 1600 steps, the Pass@1 accuracy on the MATH500 test set is 0.593 0.019 for both the online and humanline runs. The degree of humanline clipping remains the same as in instruction-following (log ϵR = 1.5, log ϵP = 1.5), suggesting that it works as strong default for wide variety of tasks. However, we find that syncing too frequently (k = 1) leads to reward collapse. Increasing leads to slower but stable warning, with any [12, 24] closing the gap with online alignment in 1000 steps while avoiding collapse. Although human utility seems irrelevant to mathematical correctness, the fact that reasoning is still expressed in language, human abstraction, may help explain why incorporating perceptual biases via humanline objective is still useful for this task."
        },
        {
            "title": "6 LIMITATIONS & FUTURE WORK",
            "content": "We stress that although humanline variants trained with offline off-policy data are able to match the performance of their online counterparts, this is an empirical regularity, not formal guarantee. Just because there is consistently some offline data that enables the gap to be closed does not mean that this will hold for any arbitrary offline data (5.2.1). What then makes offline data good-quality, and can these characteristics be formalized? Conversely, are there settings under which alignment data must necessarily be online and on-policy? We leave these as directions for future work. The model of human utility discussed in this paper comes directly from prospect theory, which was originally developed in the context of monetary random variables. Although it has since been empirically validated in other contexts, there is no guarantee that it naturally extends to the generative modeling setting. Assuming that it does is another limitation of our work, one we accept because our primary goal is to improve the post-training of generative models. Developing new theories of human probability perception as it relates to generative models is another future direction. Lastly, humanline variants raise practical questions: How large are the systems gains from fully overlapping training/inference/labeling? Can we reduce the cost of syncing (e.g., by only syncing some of the model weights)? Should γ be personalized instead of using one setting for all?"
        },
        {
            "title": "7 CONCLUSION",
            "content": "Based on prospect theoretic framework, we proposed that the online-offline dichotomy central to post-training is incidental to actually maximizing utility: what matters is not the source of data per se, but whether it reflects the human-perceived distribution over model outcomes. This perspective interprets PPO/GRPOs clipping as recovering form of probability distortion, suggesting that these state-of-the-art objectives are successful because they are perceptual losses. We then proposed generic design pattern for explicitly incorporating perceptual biases into commonly used alignment objectives, giving us humanline variants of DPO/KTO/GRPO. When trained with offline off-policy data, the humanline variants were able to match the performance of their online counterparts, closing 1.31.6x gaps in winrate for instruction-following and enabling up to 64 less frequent sampling in mathematical reasoning without performance degradation. This opens the door to cheaper, faster, and more parallelizable alignment that is not constrained by the need for online on-policy data. 8Since mathematical reasoning on MATH500 requires the generation of many intermediate reasoning tokens, we were forced to use smaller model than in 5.2.1 due to memory constraints. 10 preprint"
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "We thank Dilip Arumugam for his helpful feedback and Peter Henderson for early discussions about this project. This work was funded by Princeton Language & Intelligence and UChicago Booth. 11 preprint"
        },
        {
            "title": "REFERENCES",
            "content": "AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/ llama3/blob/main/MODEL_CARD.md. Zachary Ankner, Mansheej Paul, Brandon Cui, Jonathan Chang, and Prithviraj Ammanabrolu. Critique-out-loud reward models. arXiv preprint arXiv:2408.11791, 2024. Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics, pp. 44474455. PMLR, 2024. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022b. Lawrence Chan, Andrew Critch, and Anca Dragan. Human irrationality: both bad and good for reward inference. arXiv preprint arXiv:2111.06956, 2021. Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335, 2024. Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan Ni, Guotong Xie, Ruobing Xie, Yankai Lin, et al. Ultrafeedback: Boosting language models with scaled ai feedback. In International Conference on Machine Learning, pp. 97229744. PMLR, 2024. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda 12 preprint Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Shihan Dou, Yan Liu, Haoxiang Jia, Limao Xiong, Enyu Zhou, Wei Shen, Junjie Shan, Caishuang Huang, Xiao Wang, Xiaoran Fan, et al. Stepcoder: Improve code generation with reinforcement learning from compiler feedback. arXiv preprint arXiv:2402.01391, 2024. Yann Dubois, Balazs Galambosi, Percy Liang, and Tatsunori Hashimoto. Length-controlled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024. Olive Jean Dunn. Multiple comparisons among means. Journal of the American Statistical Association, 56(293):5264, 1961. Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Model alignment as prospect theoretic optimization. In International Conference on Machine Learning, pp. 1263412651. PMLR, 2024. Jonas Gehring, Kunhao Zheng, Jade Copet, Vegard Mella, Quentin Carbonneaux, Taco Cohen, and Gabriel Synnaeve. Rlef: Grounding code llms in execution feedback with reinforcement learning. arXiv preprint arXiv:2410.02089, 2024. Richard Gonzalez and George Wu. On the shape of the probability weighting function. Cognitive psychology, 38(1):129166, 1999. Alexey Gorbatovski, Boris Shaposhnikov, Alexey Malakhov, Nikita Surnachev, Yaroslav Aksenov, Ian Maksimov, Nikita Balagansky, and Daniil Gavrilov. Learn your reference model for real good alignment. arXiv preprint arXiv:2404.09656, 2024. Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, et al. Direct language model alignment from online ai feedback. arXiv preprint arXiv:2402.04792, 2024. Joey Hejna, Rafael Rafailov, Harshit Sikchi, Chelsea Finn, Scott Niekum, Bradley Knox, and Dorsa Sadigh. Contrastive preference learning: learning from human feedback without rl. arXiv preprint arXiv:2310.13639, 2023. Sture Holm. simple sequentially rejective multiple test procedure. Scandinavian Journal of Statistics, 6(2):6570, 1979. Jiwoo Hong, Noah Lee, and James Thorne. Orpo: Monolithic preference optimization without reference model. arXiv preprint arXiv:2403.07691, 2024. Hugging Face. Open r1: fully open reproduction of deepseek-r1, January 2025. URL https: //github.com/huggingface/open-r1. Hamish Ivison, Yizhong Wang, Jiacheng Liu, Zeqiu Wu, Valentina Pyatkin, Nathan Lambert, Noah Smith, Yejin Choi, and Hanna Hajishirzi. Unpacking dpo and ppo: Disentangling best practices for learning from preference feedback. Advances in neural information processing systems, 37:3660236633, 2024. Seungjae Jung, Gunsoo Han, Daniel Wontae Nam, and Kyoung-Woon On. Binary classifier optimization for large language model alignment. arXiv preprint arXiv:2404.04656, 2024. Daniel Kahneman and Amos Tversky. Prospect theory: An analysis of decision under risk. Econometrica, 47(2):263292, 1979. Minae Kwon, Erdem Biyik, Aditi Talati, Karan Bhasin, Dylan Losey, and Dorsa Sadigh. When humans arent optimal: Robots that collaborate with risk-aware humans. In Proceedings of the 2020 ACM/IEEE international conference on human-robot interaction, pp. 4352, 2020. Xin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xiangru Peng, and Jiaya Jia. Step-dpo: Stepwise preference optimization for long-chain reasoning of llms. arXiv preprint arXiv:2406.18629, 2024. preprint Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. Tulu 3: Pushing frontiers in open language model post-training, 2025. URL https: //arxiv.org/abs/2411.15124. Jack Lanchantin, Angelica Chen, Janice Lan, Xian Li, Swarnadeep Saha, Tianlu Wang, Jing Xu, Ping Yu, Weizhe Yuan, Jason Weston, Sainbayar Sukhbaatar, and Ilia Kulikov. Bridging offline and online reinforcement learning for llms, 2025a. URL https://arxiv.org/abs/2506. 21495. Jack Lanchantin, Angelica Chen, Janice Lan, Xian Li, Swarnadeep Saha, Tianlu Wang, Jing Xu, Ping Yu, Weizhe Yuan, Jason Weston, et al. Bridging offline and online reinforcement learning for llms. arXiv preprint arXiv:2506.21495, 2025b. Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Advances in Neural Information Processing Systems, 35:2131421328, 2022. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Yen-Ting Lin, Di Jin, Tengyu Xu, Tianhao Wu, Sainbayar Sukhbaatar, Chen Zhu, Yun He, YunNung Chen, Jason Weston, Yuandong Tian, et al. Step-kto: Optimizing mathematical reasoning through stepwise binary feedback. arXiv preprint arXiv:2501.10799, 2025. Jiate Liu, Yiqin Zhu, Kaiwen Xiao, Qiang Fu, Xiao Han, Wei Yang, and Deheng Ye. Rltf: Reinforcement learning from unit test feedback. arXiv preprint arXiv:2307.04349, 2023. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, arXiv preprint and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv:2503.20783, 2025. Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with reference-free reward, 2024. URL https://arxiv.org/abs/2405.14734. Remi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Andrea Michi, et al. Nash learning from human feedback. arXiv preprint arXiv:2312.00886, 18, 2023. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021. Michael Noukhovitch, Shengyi Huang, Sophie Xhonneux, Arian Hosseini, Rishabh Agarwal, and Aaron Courville. Asynchronous rlhf: Faster and more efficient off-policy rl for language models. arXiv preprint arXiv:2410.18252, 2024. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, March 2022. URL http://arxiv.org/abs/2203.02155. arXiv:2203.02155 [cs]. Richard Yuanzhe Pang, Weizhe Yuan, He He, Kyunghyun Cho, Sainbayar Sukhbaatar, and Jason Weston. Iterative reasoning preference optimization. Advances in Neural Information Processing Systems, 37:116617116637, 2024. Drazen Prelec. The probability weighting function. Econometrica, pp. 497527, 1998. 14 preprint Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model, 2023. URL https://arxiv.org/abs/2305.18290. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 116. IEEE, 2020. Corby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce, Ahmed Awadallah, and Tengyang Xie. Direct nash optimization: Teaching language models to self-improve with general preferences. arXiv preprint arXiv:2404.03715, 2024. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347. Rulin Shao, Shuyue Stella Li, Rui Xin, Scott Geng, Yiping Wang, Sewoong Oh, Simon Shaolei Du, Nathan Lambert, Sewon Min, Ranjay Krishna, et al. Spurious rewards: Rethinking training signals in rlvr. arXiv preprint arXiv:2506.10947, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402. 03300. Yuda Song, Gokul Swamy, Aarti Singh, Bagnell, and Wen Sun. The importance of online data: Understanding preference fine-tuning via coverage. Advances in Neural Information Processing Systems, 37:1224312270, 2024. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize with human feedback. Advances in neural information processing systems, 33:30083021, 2020. Liting Sun, Wei Zhan, Yeping Hu, and Masayoshi Tomizuka. Interpretable modelling of driving In 2019 IEEE behaviors in interactive driving scenarios based on cumulative prospect theory. Intelligent Transportation Systems Conference (ITSC), pp. 43294335. IEEE, 2019. Gokul Swamy, Sanjiban Choudhury, Wen Sun, Zhiwei Steven Wu, and Andrew Bagnell. All roads lead to likelihood: The value of reinforcement learning in fine-tuning. arXiv preprint arXiv:2503.01067, 2025. Yunhao Tang, Daniel Zhaohan Guo, Zeyu Zheng, Daniele Calandriello, Yuan Cao, Eugene Tarassov, Remi Munos, Bernardo Avila Pires, Michal Valko, Yong Cheng, et al. Understanding the performance gap between online and offline alignment algorithms. arXiv preprint arXiv:2405.08448, 2024a. Yunhao Tang, Zhaohan Daniel Guo, Zeyu Zheng, Daniele Calandriello, Remi Munos, Mark Rowland, Pierre Harvey Richemond, Michal Valko, Bernardo Avila Pires, and Bilal Piot. GenarXiv preprint eralized preference optimization: unified approach to offline alignment. arXiv:2402.05749, 2024b. Yunhao Tang, Sid Wang, Lovish Madaan, and Remi Munos. Beyond verifiable rewards: Scaling reinforcement learning for language models to unverifiable data. arXiv preprint arXiv:2503.19618, 2025. Prime Intellect Team, Sami Jaghouar, Justus Mattern, Jack Min Ong, Jannik Straube, Manveer Basra, Aaron Pazdera, Kushal Thaman, Matthew Di Ferrante, Felix Gabriel, et al. Intellect-2: reasoning model trained through globally decentralized reinforcement learning. arXiv preprint arXiv:2505.07291, 2025. Amos Tversky and Daniel Kahneman. Advances in prospect theory: Cumulative representation of uncertainty. Journal of Risk and uncertainty, 5:297323, 1992. 15 preprint Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. John Von Neumann and Oskar Morgenstern. Theory of games and economic behavior, 2nd rev. 1947. Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. Interpretable preferences via multi-objective reward modeling and mixture-of-experts, 2024. URL https://arxiv. org/abs/2406.12845. Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. arXiv preprint arXiv:2312.08935, 2023. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939, 2025a. Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Liyuan Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, et al. Reinforcement learning for reasoning in large language models with one training example. arXiv preprint arXiv:2504.20571, 2025b. Junkang Wu, Yuexiang Xie, Zhengyi Yang, Jiancan Wu, Jinyang Gao, Bolin Ding, Xiang Wang, and Xiangnan He. β-dpo: Direct preference optimization with dynamic β. Advances in Neural Information Processing Systems, 37:129944129966, 2024a. Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason Weston, and Sainbayar Sukhbaatar. Meta-rewarding language models: Self-improving alignment with llm-as-a-meta-judge. arXiv preprint arXiv:2407.19594, 2024b. Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better rewards for language model training. Advances in Neural Information Processing Systems, 36:5900859033, 2023. Tengyang Xie, Dylan Foster, Akshay Krishnamurthy, Corby Rosset, Ahmed Awadallah, and Alexander Rakhlin. Exploratory preference optimization: Harnessing implicit q*-approximation for sample-efficient rlhf. arXiv preprint arXiv:2405.21046, 2024. Huajian Xin, Daya Guo, Zhihong Shao, Zhizhou Ren, Qihao Zhu, Bo Liu, Chong Ruan, Wenda Li, and Xiaodan Liang. Deepseek-prover: Advancing theorem proving in llms through large-scale synthetic data. arXiv preprint arXiv:2405.14333, 2024. Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong, Heng Ji, Nan Jiang, and Tong Zhang. Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint. arXiv preprint arXiv:2312.11456, 2023. Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, and Young Jin Kim. Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation. arXiv preprint arXiv:2401.08417, 2024a. Jing Xu, Andrew Lee, Sainbayar Sukhbaatar, and Jason Weston. Some things are more cringe Iterative preference optimization with the pairwise cringe loss. arXiv preprint than others: arXiv:2312.16682, 2023. Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei, Guangju Wang, Chao Yu, and Yi Wu. Is dpo superior to ppo for llm alignment? comprehensive study. In International Conference on Machine Learning, pp. 5498354998. PMLR, 2024b. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025. 16 preprint Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale, 2025. URL https://arxiv.org/abs/2503.14476. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. Xuandong Zhao, Zhewei Kang, Aosong Feng, Sergey Levine, and Dawn Song. Learning to reason without external rewards. arXiv preprint arXiv:2505.19590, 2025. Wenxuan Zhou, Ravi Agrawal, Shujian Zhang, Sathish Reddy Indurthi, Sanqiang Zhao, Kaiqiang Song, Silei Xu, and Chenguang Zhu. Wpo: Enhancing rlhf with weighted preference optimization. arXiv preprint arXiv:2406.11827, 2024. Xiangxin Zhou, Zichen Liu, Anya Sims, Haonan Wang, Tianyu Pang, Chongxuan Li, Liang Wang, Min Lin, and Chao Du. Reinforcing general reasoning without verifiers. arXiv preprint arXiv:2505.21493, 2025. Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. Yuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen Zhang, Xinwei Long, Ermo Hua, et al. Ttrl: Test-time reinforcement learning. arXiv preprint arXiv:2504.16084, 2025. 17 preprint"
        },
        {
            "title": "A RELATED WORK",
            "content": "Alignment Methods Reinforcement Learning from Human Feedback (RLHF) involves training reward model on human preference data and then using it to fine-tune policy, commonly via online reinforcement learning (Christiano et al., 2017; Schulman et al., 2017; Nakano et al., 2021; Ouyang et al., 2022). The complexity of online RL has motivated line of research on simpler, offline methods that optimize policy on static dataset (Ziegler et al., 2019; Rafailov et al., 2023; Ethayarajh et al., 2024; Hejna et al., 2023; Azar et al., 2024; Hong et al., 2024; Munos et al., 2023; Xu et al., 2024a; Jung et al., 2024; Wu et al., 2024a; Xie et al., 2024; Pang et al., 2024; Tang et al., 2024b). Other work aims to bridge the gap between offline and online methods via iteratively collecting new data from the policy (Stiennon et al., 2020; Xu et al., 2023; Xiong et al., 2023; Wu et al., 2024b; Chen et al., 2024; Rosset et al., 2024; Pang et al., 2024; Lanchantin et al., 2025b), reweighting offline loss terms (Zhou et al., 2024), or recasting offline methods as online (Guo et al., 2024). Recently, there has been an increase in interest in online reinforcement learning with verifiable rewards, including training hyperparameters (Yu et al., 2025; Liu et al., 2025; Wang et al., 2025a) and other aspects (Wang et al., 2025b; Zuo et al., 2025; Shao et al., 2025; Yue et al., 2025; Zhao et al., 2025). The humanline design pattern can be applied to most alignment algorithms in both offline and online settings. Sources of Feedback The performance of alignment algorithms is directly linked to the type and quality of the feedback signal. This signal often comes from direct human judgment (Bai et al., 2022a; Wu et al., 2023). To improve scalability, researchers have also explored the use of AIgenerated feedback (RLAIF) (Bai et al., 2022b). For more objective domains such as coding, verifiable feedback can be derived from execution results and unit tests (Le et al., 2022; Liu et al., 2023; Gehring et al., 2024; Dou et al., 2024). Due to the difficulty of obtaining feedback in some domains, researchers are exploring learning without external feedback (Tang et al., 2025; Zhou et al., 2025). Another axis of differentiation is whether feedback is based on the final output of the model (outcome-based) (Xin et al., 2024; Ankner et al., 2024; DeepSeek-AI et al., 2025) or its intermediate steps (process-based) (Uesato et al., 2022; Lightman et al., 2023; Wang et al., 2023; Lai et al., 2024; Lin et al., 2025). The humanline paradigm works with different forms of feedback and is independent of whether that feedback is outcomeor process-based. Prospect Theory Having revolutionized behavioral economics, prospect theory (Kahneman & Tversky, 1979; Tversky & Kahneman, 1992) has recently been incorporated into LLM alignment via Kahneman-Tversky Optimization (KTO) (Ethayarajh et al., 2024). Previously, it has had only limited impact in machine learning, mostly in human-robot interaction research (Kwon et al., 2020; Sun et al., 2019; Chan et al., 2021). While KTO focuses on human biases in the value function (2), the humanline design pattern does so for the weighting function (4). 18 preprint"
        },
        {
            "title": "B PROOFS",
            "content": "Proposition 3.4 (restated) For any input and bounded value function v, let the outcome of an output be its surprisal log[πθ(yx)/πref(yx)] and be candidate distribution over outcomes. Then to guarantee u(Z; ω) u(Z; Q) δ for some δ 0, it suffices that (cid:112)KL(ωQ) δ/ (cid:0) (cid:1). 2v Proof. Let zx,y denote the outcome of an input-output pair (x, y), where as in Ethayarajh et al. (2024), it is measured as the surprisal term log[πθ(yx)/πref(yx)]. Assume that the human value function is bounded (as is the case in prospect theory), that the human-perceived distribution has support subsuming that of (i.e., supp(q) supp(ω)), and that zx,y is measurable with respect to the support of both distributions. Note that ω(zx,y) denotes the subjective probability (weight) assigned to output based on its outcome zx,y, not probability distribution over itself. That is, ω is distortion of the cumulative distribution over outcomes (surprisals), as defined in Eq. (4). Then using Definition (3.3) of subjective utility: u(Z; ω) u(Z; Q) = = (cid:88) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:88) (cid:88) ω(zx,y)v(zx,y) (cid:88) Q(zx,y)v(zx,y) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (ω(zx,y) Q(zx,y))v(zx,y) (cid:12) (cid:12) v(zx,y) ω(zx,y) Q(zx,y) (triangle inequality) vω Q1 (cid:112)2 KL(ωQ) (Pinskers inequality) Then if (cid:112)KL(ωQ) δ/ (cid:0) 2v (cid:1), we get u(Z; ω) u(Z; Q) δ. Proposition 4.4 (restated) For any context x, simulating output sequences from ω is equivalent to performing token-wise rejection sampling with the rejection criterion πθ(ytx; y<t)/πref(ytx; y<t) < where Beta(γ, 1), is finite upper bound on the LHS for all tokens in the vocabulary, and γ (0, 1] is the capacity function constant. Proof. Given Assumptions 4.2 and 4.3, we are operating in the tail regime: given outcome zi, (cid:80) j<i pj 0 for Ω. Following from 4, the weight (i.e., subjective j>i pj 0 for Ω+ and (cid:80) probability) of sequence is therefore ω(z) pγ (pγ + (1 p)γ)1/γ For sufficiently long sequence, the denominator will approach 1, meaning ω(z) pγ. The numerator (and thus the weight) can be factorized over tokens as pγ = (cid:81) , meaning that instead of rejection-sampling entire sequences, we can just rejection sample one token at time according to the criterion: pγ πγ θ (ytx; y<t) πγ ref (ytx; y<t) < πθ(ytx; y<t) πref (ytx; y<t) < 1 γ 1 γ where Uniform(0, 1), is finite upper bound on the LHS for all tokens yt, and the proposal distribution is the reference model (Assumption 4.1) filtered through the lens of human perception. 1 Let γ , where γ (0, 1] is the capacity function constant in (3). To get the density of this new random variable, we apply the transformation rule, noting that because is uniform on [0, 1], fU () = 1: fB(b) = fU (bγ) = 1 γbγ1 (cid:12) (cid:12) (cid:12) (cid:12) db bγ (cid:12) (cid:12) (cid:12) (cid:12) 19 preprint This is the density of Beta(γ, 1). Therefore, Beta(γ, 1), = 1 γ . Theorem 4.6 (restated) The clipped component in PPO/GRPO is special case of humanline sampling that arises under limit conditions. Proof. Let BP denote the Beta random variable in Definition 4.5 and MP its corresponding constant that bounds the likelihood ratio. By definition, its mean and variance are E[BP ] = γP γP + βP , Var[BP ] = γP βP (γP + βP )2(γP + βP + 1) . Let k, ϵP R+ be constants such that ϵP < MP . Setting γP = kϵP MP , βP = k(1 ϵP MP ), we get E[BP ] = εP MP + k(1 εP MP = ) εP MP , εP MP Var[BP ] = (k εP MP εP MP + k(1 εP MP k(1 εP MP + k(1 εP MP ))2(k εP MP ) (cid:16) εP MP 1 εP MP + (cid:17) . = ) + 1) As , δ > 0, Pr(BP ϵP /MP δ) 0 (i.e., we deterministically sample the mean). Similarly, for BR, MR in Definition 4.5, let ϵR R+ be such that ϵR > 1/MR and set γR = k/(ϵRMR), βR = k(1 1/(ϵRMR)). Then as , δ > 0, Pr(BR 1/ϵRMR δ) 0. Thus as , the rejection criteria in token-level humanline sampling simplify to: πθ(ytx; y<t) πref (ytx; y<t) < MP εP MP = εP , πref (ytx; y<t) πθ(ytx; y<t) < MR 1 εRMR = 1 εR which means that the tokens that are accepted satisfy: εP πθ(ytx; y<t) πref (ytx; y<t) εR. Recall that MP , MR are upper bounds on the likelihood ratios, and given that πθ, πref are distributions over the same support and are generally not identical, there will exist tokens for which these ratios are both greater than 1. Thus MP 1 and MR 1. For any fixed ε (0, 1), it is therefore given that (1 ϵ) < MP and (1 + ϵ) > 1/MR, meeting the constraints imposed earlier on ϵP , ϵR. Letting εR = 1 + ε and εP = 1 ε, we get the following inequality that is satisfied by accepted tokens: 1 ε πθ(ytx; y<t) πref (ytx; y<t) 1 + ε. This recovers the clipped term in PPO and GRPO, where the ratio πθ(ytx; y<t)/πref(ytx; y<t) for each token is clipped to the range [1 ϵ, 1 + ϵ]. Note that the equivalence is not only due to the likelihood ratios being bound to the same range, but also due to ratios outside the range contributing nothing to the gradient, either due to the shape of the clipping function (in PPO/GRPO) or due to being explicitly detached from the computation graph (in humanline sampling). 20 preprint"
        },
        {
            "title": "C ALGORITHMS",
            "content": "C.1 DEFINITIONS C.1.1 OFFLINE DPO/KTO DPO (Rafailov et al., 2023) and KTO (Ethayarajh et al., 2024) were originally proposed as offline algorithms, and we use the original definitions without any change for offline DPO/KTO. Where (x, yw, yl) is tuple from an offline dataset representing preference for output yw over yl given context x, the DPO loss is: LDPO(πθ, πref ) = Ex,yw,ylD log σ β log (cid:20) (cid:18) πθ(ywx) πref(ywx) β log πθ(ylx) πref(ylx) (cid:19)(cid:21) (6) where β R+ is hyperparameter and σ is the sigmoid function. Instead of paired preferences, KTO frames outputs as undesirable or desirable. Where λy R+ denotes λD(λU ) when is desirable(undesirable) respectively, the default KTO loss is: LKTO(πθ, πref) = Ex,yD[λy v(x, y)] (7) where rθ(x, y) = log πθ(yx) πref(yx) z0 = KL(πθ(yx)πref(yx)) v(x, y) = (cid:26)λDσ(β(rθ(x, y) z0)) if ydesirablex λU σ(β(z0 rθ(x, y))) if yundesirablex There is no backpropagation through z0; it exists purely to control the loss saturation. In practice, for the sake of efficiency, shared KL estimate is used for all examples in the same batch by taking the average rθ over mismatched input-output pairs (x, y). In our experiments, for an apples-toapples comparison across methods, we break up DPO preference pairs to get unpaired data for KTO, although we use twice the batch size so that the same number of steps are taken. C.1.2 ONLINE DPO/KTO Our implementation of online DPO combines features of the online DPO implementation in Guo et al. (2024), iterative DPO in Xu et al. (2024b), and semi-online DPO in Lanchantin et al. (2025a). Like Guo et al. (2024) and Lanchantin et al. (2025a), we sample completions from the policy being actively aligned. However, like Lanchantin et al. (2025a), we do not sample every step, because it is slower, more computationally expensive, and leads to worse results. Asynchronous training is typical in RLHF, especially in large-scale distributed settings (Noukhovitch et al., 2024). We find that sampling once every 1024 contexts (i.e., 32 steps) leads to best performance in the instruction-following setting, which we call one round. For each of the 1024 contexts in round, we sample 8 completions {yi}8 i=1 πθ(x), score them with reward model, compare the highestand lowest-scoring for each x, and construct paired preference (x, yw, yl) if the difference in score exceeds τ = 0.01. We use this methodology because it is nearly identical to how the offline instruction-following data was constructed (Meng et al., 2024), even using the exact same reward model (Wang et al., 2024) and contexts (Cui et al., 2024) to enable an apples-to-apples comparison. The differences are: (1) our use of threshold τ , which is needed to construct feedback that is sufficiently discriminative; (2) using 22% more contexts than in the offline data to adjust for the fact that using τ leads to roughly 18% of the preferences (albeit low-signal preferences) being discarded. Therefore the volume of data seen by both offline DPO and online DPO is approximately the same, although the latter sees more diversity in contexts, which may provide an additional advantage. At the end of round, the policy is checkpointed, and at the start of the next round, the new policy and reference model are loaded from this checkpoint. Online KTO is implemented the exact same way, albeit the final loss is calculated with (7) instead of (6). Notably, we construct DPO-style paired preferences before breaking them up to create unpaired data for KTO, instead of directly creating unpaired data using positive and negative thresholds. Not only does this allow for better comparison with DPO, but it also works better in practice. See Algorithm 1 for the pseudo-code of Online DPO/KTO. 21 preprint Set reference model πref πθ; Dtrain Sample batch of contexts Db from D, where Db = 1024 for prompt Db do Algorithm 1 Online DPO / KTO Input: initial policy model πθinit; reward model rφ; reward threshold τ ; prompts D; hyperparameters β, λdesirable, λundesirable Output: policy model πθ 1: Initiate policy model πθ πθinit 2: for round = 1 to do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: end for Compute token-level surprisal ˆri,t for every token in (yw, yl) via πθ and πref Update πθ by maximizing the DPO / KTO objective (Eq. 6; 7) Sample outputs {yi}G Compute rewards {ri}G if maxj rj minj rj τ then (yw, yl) (cid:0)arg maxj rj, arg minj rj Dtrain Dtrain {(x, yw, yl)} i=1 πθ( x), where = 8 i=1 for each yi via rφ end for for batch Dtrain do Train with newly generated preference data 18% samples will be filtered out Online sampling and relabeling end for end if (cid:1) Algorithm 2 Offline GRPO Input: initial policy model πθinit; prompts and completions D; hyperparameters β, ϵ Output: policy model πθ 1: Initiate policy model πθ πθinit; reference model πref πθ 2: for step = 1, ..., do 3: 4: 5: 6: 7: 8: 9: 10: end for Set {yw, yl} from off-policy (x, yw, yl) tuples Compute token-level surprisal ˆri,t for every token in {yw, yl} via πθ and πref Compute ˆAi,t for every token in {yw, yl} through group relative advantage estimation Update πθ by maximizing the GRPO objective (Eq. 1) Sample batch Db from for prompt Db do Train with off-policy preference data Default group size = 2 end for C.1.3 OFFLINE GRPO Given that GRPO is inherently an online method (Shao et al., 2024), we make few different changes to create an offline variant, which largely follow those made by Ethayarajh et al. (2024) to make an offline variant of PPO. For one, instead of sampling new completions, we take tuples (x, yw, yl) in an offline preference dataset (e.g., UltraFeedback (Cui et al., 2024)) and treat them as group of two: = {yi}2 i=1 = {yw, yl}. The reference model is never updated: its weights remain those of the policy at initialization. See Algorithm 2 for the pseudo-code of offline GRPO. C.1.4 ONLINE GRPO Instead of sampling every step, we sample data as in Online DPO/KTO, the only difference being that we retain the raw scores from the scoring step so that they can later be fed into the loss calculation (1). We use this approach to allow for an apples-to-apples comparison with Online DPO/KTO, as well as because some asynchronicity is usually permitted in practice and we find that sampling once per round (i.e., roughly every 32 steps) does not degrade performance. consequence of this choice is that the group size is exactly 2 for all contexts, making the relative advantages either -1 or 1. Naturally, this does not unlock the full potential of GRPO, since one of its strengths is its ability to leverage scalar rewards. However, we consider it more important that the volume of training data to be roughly the same across different variants and methods. We also use DAPO-style normalization 22 preprint (i.e., taking the average loss over the number of tokens in the batch instead of within sequence), as we find this leads to better performance on instruction-following (Yu et al., 2025). Lastly, we reuse the reference model as the base model for calculating the KL penalty, both because it saves us the space of storing third model and because prior work has identified the KL penalty to not be of much import, allowing its estimate to be less precise. C.2 HUMANLINE IMPLEMENTATION The instruction-following experiments were done in fork of the HALOs repository. Below, we provide relatively straightforward implementation of the humanline design pattern for DPO, KTO and GRPO, which is triggered by setting self.config.humanline = True in our codebase. The mathematical reasoning experiments were implemented in fork of the Open-R1 repository (Hugging Face, 2025), which itself is based on Huggingfaces TRL library. Because of this, humanline syncing is implemented differently, by over-writing callback methods: before the optimizer step happens, the current state of the policy is stored locally; at the end of the step, the stored policy is loaded into the reference model. This avoids any syncing before the policy has been updated, which we found improves stability when doing distributed training with ZeRO2 (Rajbhandari et al., 2020). C.2.1 HUMANLINE SYNCING In our codebase, we first modify the training loop to implement humanline syncing. For the sake of brevity, we highlight only the relevant changes in train() and omit code used for logging. The accelerator object is used to manage distributed training with FSDP in our own codebase: def train(): ... self.optimizer.zero_grad() loss, metrics = self.get_batch_metrics(batch) self.accelerator.backward(loss) grad_norm = self.accelerator.clip_grad_norm_(self.policy.parameters(), self.config.model.max_grad_norm) if self.config.loss.sync_reference or self.config.humanline: self.sync_reference_with_policy() self.optimizer.step() self.scheduler.step() ... def sync_reference_with_policy(self): \"\"\" Update the reference model to have the policy weights. \"\"\" if self.batch_counter % self.config.sync_freq == 0: state_dict = self.accelerator.unwrap_model(self.policy).state_dict() self.accelerator.unwrap_model(self.reference_model).load_state_dict(state_dict) self.accelerator.wait_for_everyone() Humanline clipping is even easier to implement, although it has to be implemented in two different places in our codebase because of the different abstractions used for DPO/KTO and GRPO: def get_sequence_rewards(self, policy_logps: torch.FloatTensor, reference_logps: torch.FloatTensor, length_normalized=False, ): \"\"\" If regular alignment, return the surprisal for the sequence (log [policy(yx)/reference(yx)]). This is called the \"sequence reward\", following DPO terminology. Apply humanline if specified. Args: 1 2 3 4 5 7 8 9 10 11 13 14 15 16 17 19 20 21 22 23 2 3 4 5 6 8 9 10 11 12 preprint 13 14 15 16 17 19 20 21 22 23 25 26 27 28 29 31 32 33 34 35 37 38 39 40 41 43 44 45 46 47 49 50 51 52 53 55 56 57 58 59 61 62 63 64 65 67 68 69 70 71 73 74 75 76 77 policy_logps: token-level probabilities according to policy (microbatch_size, maximum sequence length) reference_logps: token-level probabilities according to reference model (microbatch_size, maximum sequence length) length_normalized: divide the sequence reward by the number of non-rejected tokens Returns: The sequence-level rewards (microbatch_size, 1). \"\"\" if self.config.humanline: token_rewards = (policy_logps - reference_logps).clamp( self.config.log_epsilon_P, self.config.log_epsilon_R) else: token_rewards = policy_logps - reference_logps normalization_factor = (token_rewards.abs() != 0).float().sum(-1) if length_normalized else 1 sequence_rewards = token_rewards.sum(-1) / normalization_factor return sequence_rewards class DPOTrainer(PairedPreferenceTrainer): def loss(self, batch: Dict, policy_chosen_logps: torch.FloatTensor, policy_rejected_logps: torch.FloatTensor, reference_chosen_logps: torch.FloatTensor, reference_rejected_logps: torch.FloatTensor, *args, ): \"\"\"Compute the DPO loss for batch of policy and reference model token-level log probabilities.\"\"\" # apply humanline clipping via get_sequence_rewards on token-level # log probabilities before they are fed into loss computation chosen_rewards = self.get_sequence_rewards(policy_chosen_logps, reference_chosen_logps) rejected_rewards = self.get_sequence_rewards(policy_rejected_logps, reference_rejected_logps) chosen_rewards *= self.config.loss.beta rejected_rewards *= self.config.loss.beta losses = -F.logsigmoid(chosen_rewards - rejected_rewards) return losses, chosen_rewards.detach(), rejected_rewards.detach() class KTOTrainer(UnpairedPreferenceTrainer): def loss(self, batch: Dict, policy_chosen_logps: torch.FloatTensor, policy_rejected_logps: torch.FloatTensor, policy_KL_logps: torch.FloatTensor, reference_chosen_logps: torch.FloatTensor, reference_rejected_logps: torch.FloatTensor, reference_KL_logps: torch.FloatTensor, *args, ): \"\"\"Compute the KTO loss for batch of policy and reference model log probabilities. If generation p_desirable, we have the 'desirable' loss: L(x, y) := 1 - sigmoid(beta * ([log p_policy(yx) 24 preprint - log p_reference(yx)] - KL(p_policy p_reference))) If generation p_undesirable, we have the 'undesirable' loss: L(x, y) := 1 - sigmoid(beta * (KL(p_policy p_reference) - [log p_policy(yx) - log p_reference(yx)])) The desirable losses are weighed by config.loss.desirable_weight. The undesirable losses are weighed by config.loss.undesirable_weight. This should be used to address imbalances in the ratio of desirable:undesirable examples respectively. The KL term is estimated by matching with unrelated outputs y', then calculating the average log ratio log p_policy(y'x) - log p_reference(y'x). \"\"\" if policy_chosen_logps.shape[0] != 0: chosen_rewards = self.get_sequence_rewards( policy_chosen_logps, reference_chosen_logps) else: chosen_rewards = torch.Tensor([]).to(self.policy_dtype).to( self.accelerator.device) if policy_rejected_logps.shape[0] != 0: rejected_rewards = self.get_sequence_rewards( policy_rejected_logps, reference_rejected_logps) else: rejected_rewards = torch.Tensor([]).to(self.policy_dtype).to( self.accelerator.device) # For KTO, humanline also applies to the KL term KL_rewards, _ = self.get_sequence_rewards(policy_KL_logps.detach(), reference_KL_logps.detach()) KL = (KL_rewards.sum() / (KL_rewards.abs() != 0).float().sum().item() .clamp(min=1)).clamp(min=0) if policy_chosen_logps.shape[0] != 0: chosen_losses = self.config.loss.desirable_weight * (1 - F.sigmoid(self.config.loss.beta * (chosen_rewards - KL))) else: chosen_losses = torch.Tensor([]).to(self.policy_dtype).to( self.accelerator.device) if policy_rejected_logps.shape[0] != 0: rejected_losses = self.config.loss.undesirable_weight * (1 - F.sigmoid(self.config.losbeta * (KL - rejected_rewards))) else: rejected_losses = torch.Tensor([]).to(self.policy_dtype).to( self.accelerator.device) losses = torch.cat((chosen_losses, rejected_losses), 0) return losses, chosen_rewards.detach(), rejected_rewards.detach(), KL.detach() For both DPO and KTO, we apply the same token-wise likelihood clipping with the function get sequence rewards as shown above. For GRPO, we do the same but with the function get ratios to return the probability ratio under the policy and the reference models instead of the log probability ratio, as defined in the clipped surrogate objective. Note that we clamp in logspace for greater numerical precision. def get_ratios(self, policy_logps, reference_logps, ): \"\"\" If regular alignment, return the token-level probability ratio under the policy vs the reference [policy(yx)/reference(yx)]. Apply humanline if specified. 79 80 81 82 83 85 86 87 88 89 91 92 93 94 95 97 98 99 100 101 103 104 105 106 107 109 110 111 112 113 115 116 117 118 119 121 122 123 124 125 127 128 129 1 2 4 5 6 7 8 preprint Args: policy_logps: token-level probabilities according to policy (microbatch_size, maximum sequence length) reference_logps: token-level probabilities according to reference model (microbatch_size, maximum sequence length) Returns: The probability ratios (microbatch_size, sequence length) if sequence_level; otherwise, (microbatch_size, 1) \"\"\" if self.config.humanline: logratio = (policy_logps - reference_logps).clamp( self.config.log_epsilon_P, self.config.log_epsilon_R ) else: logratio = policy_logps - reference_logps ratio = logratio.exp() return ratio class GRPOTrainer(BasicTrainer): def loss(self, batch: Dict, policy_logps: torch.FloatTensor, reference_logps: torch.FloatTensor, advantages: torch.FloatTensor, group_size: torch.FloatTensor, *args, ): \"\"\" Compute the GRPO loss. Args: policy_logps: log probability of the output under the policy (microbatch_size, sequence_length) reference_logps: log probability of the output under the reference model (microbatch_size, sequence_length) advantages: sequence level advantages (microbatch_size,) group_size: number of outputs (in entire batch) belonging to prompt associated with sequence (microbatch_size,) Returns: average loss, average KL, average weighted advantage, average unweighted advantage \"\"\" # apply humanline clipping via get_ratios on token-level # log probabilities which returns probability ratios ratio = self.get_ratios(policy_logps, reference_logps) masks = (batch['target_labels'][:, 1:] != -100).clone().to( self.policy_dtype) advantages = advantages.unsqueeze(-1) group_size = group_size.unsqueeze(-1) weighted_adv = advantages * ratio # probability ratios get clipped again in the GRPO surrogate # objective controlled by separate hyperparameter epsilon weighted_adv_clipped = advantages * ratio.clamp( 1 - self.config.loss.epsilon, 1 + self.config.loss.epsilon) # humanline clipping does not apply to KL term in GRPO per_token_KL = torch.exp(reference_logps - policy_logps) 26 9 11 12 13 14 15 17 18 19 20 21 23 24 25 26 27 29 30 31 32 33 35 36 37 38 39 41 42 43 44 45 47 48 49 50 51 53 54 55 56 57 59 60 61 62 63 65 66 67 68 69 71 72 73 74 preprint 76 77 78 79 80 82 83 84 - (reference_logps - policy_logps) - 1 per_token_loss = -torch.min(weighted_adv, weighted_adv_clipped) + self.config.loss.beta * per_token_KL # do DAPO-style normalization return masked_mean(per_token_loss, masks), masked_mean(per_token_KL.detach(), masks), masked_mean(weighted_adv.abs().detach(), masks), advantages.abs().mean() 27 preprint"
        },
        {
            "title": "D ADDITIONAL EXPERIMENTS",
            "content": "Table 1: Hyperparameters that are common to all of our instruction-following experiments, across different alignment objectives and different variants for each objective. Note that we generate much more data than we ultimately use during online training (see Appendix C.1 for details). Note the data volume for KTO/GRPO is twice that of DPO because the latter operates on paired preferences containing two sequences each. Hyperparameter Optimizer AdamW β1 AdamW β2 AdamW ϵ Weight Decay Warmup Offline Training Examples Offline Batch Size Online Training Contexts Online Batch Size Round (Number of Contexts) Generations per Context (sampled) Generations per Context (after filtering) Maximum Generation Length Top-p (Nucleus Sampling) Sampling Temperature Reward Threshold τ Humanline log ϵP Humanline log ϵR Humanline Value AdamW 0.9 0.999 1e-5 1e-2 10% 10K (DPO) / 20K (KTO, GRPO) 32 (DPO) / 64 (KTO, GRPO) 12288 32 (DPO) / 64 (KTO, GRPO) 1024 8 2 or 0 2048 0.95 0.7 0.01 1.5 1.5 1 Table 2: Hyperparameters that are common to all of our mathematical reasoning experiments with GRPO. We use the setup in Huggingfaces Open-R1 (Hugging Face, 2025), except instead of placing equal weight on the tag count, format, and accuracy rewards, we place weights of 1, 1, and 8 respectively (i.e., emphasizing accuracy over the rest). Doing humanline syncing every step (k = 1) will lead to collapse in this setup because of the smaller models involved; [12, 20] closes the gap with the online reward curves within 1000 steps. Hyperparameter Optimizer AdamW β1 AdamW β2 AdamW ϵ Weight Decay Warmup Learning Rate Max Gradient Norm Training Contexts Batch Size (incl. duplicate prompts due to groups) Group Size Maximum Generation Length Top-p (Nucleus Sampling) Sampling Temperature Humanline log ϵP Humanline log ϵR Value AdamW 0.9 0.999 1e-8 0 10% 1e-6 1.0 12K 256 8 2048 1.0 0.7 1.5 1. 28 preprint Table 3: The performance of Llama3-8B-Instruct trained with all variants of all objectives on AlpacaEval2, along with the objective-specific hyperparameters. Note that while humanline alignment usually reduces the average length (Length) of completions, this is not universal characteristic of humanline variants, but of the data and hyperparameters used; results from training on different version of UltraFeedback, where completions are sampled from different model, lead to offline+humanline variants having roughly the same length as their offline counterparts  (Table 4)  . The hyperparameters that require the most adjusting are the learning rate (LR) and the maximum gradient clipping norm (Max Norm); going from offline to online requires the LR and Max Norm to be scaled by 0.5x-1x, but adding humanline variant on top can increase or decrease the Max Norm (see 5.2.1 for discussion of why). Objective-specific hyperparameters remain fixed across variants to allow for fair comparison, with the exception of Offline GRPO ϵ, which needs to be much larger when the reference model is fixed. Objective Offline KTO Offline+Humanline KTO Online KTO Online+Humanline KTO Objective Offline DPO Offline+Humanline DPO Online DPO Online+Humanline DPO Objective Offline GRPO Offline+Humanline GRPO Online GRPO Online+Humanline GRPO LR 5.0e-6 5.0e-6 2.5e-6 2.5e-6 β 0.25 0.25 0.25 0.25 λD 1.1 1.1 1.1 1.1 LR 5.0e-6 5.0e-6 2.5e-6 2.5e-6 β 0.01 0.01 0.01 0.01 LR 5.0e-6 5.0e-6 5.0e-6 5.0eλU Max Norm LC-WR WR Length 17.40 1 22.19 1 22.45 1 22.79 1 14.13 14.70 19.47 18.78 1658 1407 1744 1663 1.0 4.0 0.5 0.1 β Max Norm LC-WR WR Length 18.07 24.82 24.96 26.84 16.07 20.18 22.99 23. 1767 1637 1828 1774 1.0 1.0 0.5 1.0 0.10 0.10 0.10 0.10 ϵ 0.50 0.15 0.15 0.15 Max Norm LC-WR WR Length 1.0 1.0 1.0 0. 15.52 25.24 25.05 26.10 12.61 18.11 18.82 21.57 1648 1488 1529 1647 Table 4: AlpacaEval2 results when Llama3-8B-Instruct is trained on two different versions of offline UltraFeedback ArmoRM (Meng et al., 2024), one where completions are generated by Llama3-8B-Instruct (a separate unaligned version producing offline data, not to be confused with the policy) and another where completions are generated by Gemma2-9B-Instruct. The contexts are the same in both cases. Significant differences (p < 0.05) are highlighted in red. Although the performance is not significantly different in most cases, when trained on the Llama3 completions, the offline+humanline-aligned policies tend to produce shorter completions than their offline counterparts; when trained on the Gemma2 completions, this is not necessarily the case. Using humanline variants does not permit one to ignore the data, as it will always make difference in the quality of the aligned model. Objective Offline KTO Offline+Humanline KTO Offline DPO Offline+Humanline DPO Offline GRPO Offline+Humanline GRPO Llama3-8B-Instruct Data Gemma2-9B-Instruct Data LC-WR WR Length LC-WR WR Length 17.40 22.19 18.07 24.82 15.52 25.24 14.13 14.70 16.07 20.18 12.61 18.11 1658 1407 1767 1637 1648 1488 18.10 22.18 18.63 26.26 12.64 24. 15.28 20.30 15.44 21.91 10.84 18.99 1698 1836 1690 1642 1696 1587 29 preprint Table 5: AlpacaEval2 results when Gemma2-27B-Instruct is aligned with DPO on two different versions of offline UltraFeedback ArmoRM (Meng et al., 2024), one where completions are generated by Llama3-8B-Instruct (L3-8B) and another where completions are generated by Gemma2-9B-Instruct (G2-9B). The contexts are the same in both cases. Note that using the offline+humanline variant is only able to match the performance of the online variant when the offline off-policy data comes from the latter of the two sources, ostensibly because the data distribution in the former case does not satisfy Assumption 4.1 to sufficient degree. Objective (DPO Variants) Baseline Offline (L3-8B Completions) Offline+Humanline (L3-8B Completions) Offline (G2-9B Completions) Offline+Humanline (G2-9B Completions) Online LC-WR 45.90 48.59 56.27 56.58 67.45 66.49 Std. Err WR 1.54 1.58 1.67 1.68 1.64 1.48 35.45 32.36 44.49 45.17 61.37 74.22 LR β 2.5e-6 2.5e-6 2.5e-6 2.5e-6 2.5e0.3 0.3 0.1 0.1 0.1 Table 6: AlpacaEval2 results when Llama3-8B-Instruct is aligned with humanline DPO with different choices of humanline clipping hyperparameters ϵP , ϵR. Humanline syncing is done every step (k = 1) and other hyperparameters are fixed. The performance of the aligned model is fairly robust to the choice of clipping values in both directions, with most length-controlled winrates (LCWR) falling within the standard error of the highest one. However, the length of the outputs does grow monotonically as the clipping range gets looser. log ϵP -2.0 -1.5 -1.0 -1.0 -1.0 -1.0 log ϵR LC-WR Std. Err WR 1.5 1.5 1.5 1.5 2.0 3. 23.33 24.82 24.37 24.37 22.55 23.98 1.35 1.36 1.33 1.33 1.31 1.33 1663 1637 1588 1588 1619 1636 Figure 7: When aligning Llama3-8B-Instruct with humanline GRPO, the performance on instruction-followingmeasured here as the length-controlled winrate against GPT-4-Turbo baselineis robust to the frequency of humanline syncing up to = 4. Past that point, syncing less frequently leads to log-linear decline in performance. In other setups not shown here, such as our mathematical reasoning experiments with Qwen2.5-1.5B-Instruct, syncing less frequently is not only beneficial but necessary, since anything less than = 12 introduces too much instability and leads to reward collapse. 30 preprint Figure 8: The performance benefits of the humanline variants of KTO/DPO/GRPO persist at larger scale with different model families, with Gemma2-27B-Instruct seeing 1.151.30x improvement in performance. This is slightly smaller than the relative improvement seen by Llama3-8B-Instruct, and can be ascribed to the former being better base model. Figure 9: Trust region-style syncing (Gorbatovski et al., 2024) performs much worse than humanline syncing. In offline+trust region, we sync the reference model with the policy after the update every 1024 steps, the best performing setup in Gorbatovski et al. (2024). This suggests that it is not enough to merely sync the reference model; the way in which it is done matters as well. 31 preprint Figure 10: Average wall-clock time for aligning Llama3-8B-Instruct on UltraFeedback, reported with standard error and 95% confidence intervals across 5 random seeds. Note that offline+humanline GRPO takes almost twice as long as offline GRPO due to the syncing of the reference model weights. However, this is still less than 1/6 of the time needed to run online GRPO (without any overlapping of training/inference) while reaching the same performance (Figure 1). preprint Table 7: Generations from the different DPO-aligned versions of Gemma2-27B-Instruct given simple math question. Note that only the offline+humanline output explicitly states the weighting idea, explains why simple sum/divide is only valid with equal weights, and then shows the perquarter contributionsbut without the verbosity of offline DPO. Prompt Baseline Offline In this math problem, we are asked to figure out the percentage of total profits that are made in each quarter. They are 90% in the first quarter, 92% in the second quarter, 81% in the third quarter and 88% in the fourth quarter. What is the total profit percent over period of time, based upon these quarters? Heres how to calculate the average profit percentage over the year: 1. Add up the profit percentages: 90% + 92% + 81% + 88% = 351% 2. Divide the total percentage by the number of quarters: 351% / 4 = 87.75% Therefore, the average profit percentage over the year is 87.75%. Heres how to calculate the total profit percentage over the year: 1. You cant directly add percentages like this. Percentages represent parts of whole, and simply adding them could result in number over 100%, which doesnt make sense in this context. 2. Think of it as weighted averages: Each quarters profit percentage reflects its contribution to the overall yearly profit. To find the total profit percentage, we need to calculate weighted average. 3. Heres the formula: * Weighted Average = (Q1% * Q1 Weight) + (Q2% * Q2 Weight) + (Q3% * Q3 Weight) + (Q4% * Q4 Weight) * Where Q1%, Q2%, Q3%, and Q4% are the profit percentages for each quarter. * The weight for each quarter is typically 1/4 (since there are four quarters in year). 4. Calculation: * Total Profit Percentage = (0.90 * 0.25) + (0.92 * 0.25) + (0.81 * 0.25) + (0.88 * 0.25) * Total Profit Percentage = 0.225 + 0.23 + 0.2025 + 0.22 * Total Profit Percentage = 0.8775 or 87.75%. Therefore, the total profit percentage over the year is 87.75%. Offline+Humanline Heres how to calculate the total profit percentage over the year: 1. Understand its not simple sum: You cant just add 90% + 92% + 81% + 88% = 351% and divide by 4. Percentages represent portions of the whole, and simply adding them like this would imply profits exceeding 100%, which isnt possible. 2. Weighted Average Approach: * Think of each quarters profit as weighted contribution: The weight is the proportion of the year that quarter represents (1/4 or 25%). * Calculate the weighted profit for each quarter: * Q1: 90% * 0.25 = 22.5% * Q2: 92% * 0.25 = 23% * Q3: 81% * 0.25 = 20.25% * Q4: 88% * 0.25 = 22% Online * Add the weighted profits: 22.5% + 23% + 20.25% + 22% = 87.75% Therefore, the total profit percentage over the year is 87.75%. Heres how to calculate the average profit percentage over the year: 1. Add up the percentages: 90% + 92% + 81% + 88% = 351% 2. Divide the total by the number of quarters: 351% / 4 = 87.75% Therefore, the average profit percentage over the year is 87.75%. Important Note: This is an average. It doesnt mean the company made profit of 87.75% overall. It shows the average across the quarters, which were individually quite variable."
        }
    ],
    "affiliations": [
        "Princeton University",
        "Stanford University",
        "University of Chicago"
    ]
}