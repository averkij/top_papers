{
    "paper_title": "Back to Basics: Let Denoising Generative Models Denoise",
    "authors": [
        "Tianhong Li",
        "Kaiming He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Today's denoising diffusion models do not \"denoise\" in the classical sense, i.e., they do not directly predict clean images. Rather, the neural networks predict noise or a noised quantity. In this paper, we suggest that predicting clean data and predicting noised quantities are fundamentally different. According to the manifold assumption, natural data should lie on a low-dimensional manifold, whereas noised quantities do not. With this assumption, we advocate for models that directly predict clean data, which allows apparently under-capacity networks to operate effectively in very high-dimensional spaces. We show that simple, large-patch Transformers on pixels can be strong generative models: using no tokenizer, no pre-training, and no extra loss. Our approach is conceptually nothing more than \"$\\textbf{Just image Transformers}$\", or $\\textbf{JiT}$, as we call it. We report competitive results using JiT with large patch sizes of 16 and 32 on ImageNet at resolutions of 256 and 512, where predicting high-dimensional noised quantities can fail catastrophically. With our networks mapping back to the basics of the manifold, our research goes back to basics and pursues a self-contained paradigm for Transformer-based diffusion on raw natural data."
        },
        {
            "title": "Start",
            "content": "Back to Basics: Let Denoising Generative Models Denoise"
        },
        {
            "title": "MIT",
            "content": "5 2 0 2 7 1 ] . [ 1 0 2 7 3 1 . 1 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Todays denoising diffusion models do not denoise in the classical sense, i.e., they do not directly predict clean images. Rather, the neural networks predict noise or noised In this paper, we suggest that predicting clean quantity. data and predicting noised quantities are fundamentally different. According to the manifold assumption, natural data should lie on low-dimensional manifold, whereas noised quantities do not. With this assumption, we advocate for models that directly predict clean data, which allows apparently under-capacity networks to operate effectively in very high-dimensional spaces. We show that simple, large-patch Transformers on pixels can be strong generative models: using no tokenizer, no pre-training, and no extra loss. Our approach is conceptually nothing more than Just image Transformers, or JiT, as we call it. We report competitive results using JiT with large patch sizes of 16 and 32 on ImageNet at resolutions of 256 and 512, where predicting high-dimensional noised quantities can fail catastrophically. With our networks mapping back to the basics of the manifold, our research goes back to basics and pursues self-contained paradigm for Transformerbased diffusion on raw natural data. 1. Introduction When diffusion generative models were first developed [57, 59, 23], the core idea was supposed to be denoising, i.e., predicting clean image from its corrupted version. However, two significant milestones in the evolution of diffusion models turned out to deviate from the goal of directly predicting clean images. First, predicting the noise itself (known as ϵ-prediction) [23] made pivotal difference in generation quality and largely popularized these models. Later, diffusion models were connected to flow-based methods [37, 38, 1] through predicting the flow velocity (v-prediction [52]), quantity that combines clean data and noise. Today, diffusion models in practice commonly predict noise or noised quantity (e.g., velocity). Extensive studies [52, 29, 25, 15] have shown that preFigure 1. The Manifold Assumption [4] hypothesizes that natural images lie on low-dimensional manifold within the highdimensional pixel space. While clean image can be modeled as on-manifold, the noise ϵ or flow velocity (e.g., = ϵ) is inherently off-manifold. Training neural network to predict clean image (i.e., x-prediction) is fundamentally different from training it to predict noise or noised quantity (i.e., ϵ/v-prediction). dicting the clean image (x-prediction [52]) is closely related to ϵand v-prediction, provided that the weighting of the prediction loss is reformulated accordingly (detailed in Sec. 3). Owing to this relation, less attention has been paid to what the network should directly predict, implicitly assuming that the network is capable of performing the assigned task. However, the roles of clean images and noised quantity (including the noise itself) are not equal. In machine learning, it has long been hypothesized [4, 3] that (highdimensional) data lie (roughly) on low-dimensional manifold ([4, p.6]). Under this manifold assumption, while clean data can be modeled as lying on low-dimensional manifold, noised quantity is inherently distributed across the full high-dimensional space [69] (see Fig. 1). Predicting clean data is fundamentally different from predicting noise or noised quantity. Consider scenario where low-dimensional manifold is embedded in high-dimensional observation space. Predicting noise in this high-dimensional space requires high capacity: the network needs to preserve all information about the noise. In contrast, limited-capacity network can 1 still predict the clean data, as it only needs to retain the lowdimensional information while filtering out the noise. When low-dimensional space (e.g., image latent [49]) is used, the difficulty of predicting noise is alleviated, yet at the same time is hidden rather than solved. When it comes to pixel or other high-dimensional spaces, existing diffusion models can still struggle to address the curse of dimensionality [4]. The heavy reliance on pre-trained latent space prevents diffusion models from being self-contained. In pursuit of self-contained principle, there has been strong focus on advancing diffusion modeling in the pixel space [7, 25, 26, 6, 70]. In general, these methods explicitly or implicitly avoid the information bottleneck in the networks, e.g., by using dense convolutions or smaller patches, increasing channels, or adding long skip connections. We suggest that these designs may stem from the demand to predict high-dimensional noised quantities. In this paper, we return to first principles and let the neural network directly predict the clean image. By doing so, we show that plain Vision Transformer (ViT) [13], operating on large image patches consisting of raw pixels, can be effective for diffusion modeling. Our approach is selfcontained and does not rely on any pre-training or auxiliary loss no latent tokenizer [49], no adversarial loss [16, 49], no perceptual loss [77, 49] (thus no pre-trained classifier [56]), and no representation alignment [74] (thus no self-supervised pre-training [45]). Conceptually, our model is nothing more than Just image Transformers, or JiT, as we call it, applied to diffusion. We conduct experiments on the ImageNet dataset [11] at resolutions of 256 and 512, using JiT models with patch sizes 16 and 32 respectively. Even though the patches are very high-dimensional (hundreds or thousands), our models using x-prediction can easily produce strong results, where ϵand v-prediction fail catastrophically. Further analysis shows that it is unnecessary for the network width to match or exceed the patch dimension; in fact, and surprisingly, bottleneck design can even be beneficial, echoing observations from classical manifold learning. Our effort marks step toward self-contained Diffusion + Transformer philosophy [46] on native data. Beyond computer vision, this philosophy is highly desirable in other domains involving natural data (e.g., proteins, molecules, or weather), where tokenizer can be difficult to design. By minimizing domain-specific designs, we hope that the general Diffusion + Transformer paradigm originated from computer vision will find broader applicability. 2. Related Work Diffusion Models and Their Predictions. The pioneering work on diffusion models [57] proposed to learn reversed stochastic process, in which the network predicts the parameters of normal distribution (e.g., mean and standard deviation). Five years after its introduction, this method was revitalized and popularized by Denoising Diffusion Probabilistic Models (DDPM) [23]: pivotal discovery was to make noise the prediction target (i.e., ϵ-prediction). The relationships among different prediction targets were then investigated in [52] (originally in the context of model distillation), where the notion of v-prediction was also introduced. Their work focused on the weighting effects introduced by reparameterization. Meanwhile, EDM [29] reformulated the diffusion problem around denoiser function, which constitutes major milestone in the evolution of diffusion models. However, EDM adopted pre-conditioned formulation, where the direct output of the network is not the denoised image. While this formulation is preferable in low-dimensional scenarios, it still inherently requires the network to output quantity that mixes data and noise (more comparisons in appendix). Flow Matching models [37, 38, 1] can be interpreted as form of v-prediction [52] within the diffusion modeling framework. Unlike pure noise, is combination of data and noise. The connections between flow-based models and previous diffusion models have been established [15]. Today, diffusion models and their flow-based counterparts are often studied under unified framework. Denoising Models. Over the past decades, the concept of denoising has been closely related to representation learning. Classical methods, exemplified by BM3D [9] and others [47, 14, 79], leveraged the assumptions of sparsity and low dimensionality to perform image denoising. Denoising Autoencoders (DAEs) [68, 69] were developed as an unsupervised representation learning method, using denoising as their training objective. They leveraged the manifold assumption [4]  (Fig. 1)  to learn meaningful representations that approximate the low-dimensional data manifold. DAEs can be viewed as form of Denoising Score Matching [67], which in turn is closely related to modern score-based diffusion models [59, 60]. Nevertheless, while it is natural for DAEs to predict clean data for manifold learning, in score matching, predicting the score function effectively amounts to predicting the noise (up to scaling factor), i.e., ϵ-prediction. Manifold Learning. Manifold learning has been classical field [51, 63] focused on learning low-dimensional, nonlinear representations from observed data. In general, manifold learning methods leverage bottleneck structures [64, 48, 41, 2] that encourage only useful information to pass through. Several studies have explored the connections between manifold learning and generative models [39, 27, 8]. Latent Diffusion Models (LDMs) [49] can be viewed as manifold learning in the first stage via an autoencoder, followed by diffusion in the second stage. Pixel-space Diffusion. While latent diffusion [49] has become the default choice in the field today, the development of diffusion models originally began with pixel-space formulations [59, 23, 44, 12]. Early pixel-space diffusion models were typically based on dense convolutional networks, most often U-Net [50]. These models often use over-complete channel representations (e.g., transforming HW 3 into HW 128 in the first layer), accompanied by long-range skip connections. While these models work well for ϵand v-prediction, their dense convolutional structures are typically computationally expensive. Applying these convolutional models to high-resolution images does not lead to catastrophic degradation, and as such, research in this direction has commonly focused on noise schedules and/or weighting schemes [7, 25, 26, 30] to further improve generation quality. In contrast, applying Vision Transformer (ViT) [13] directly to pixels presents more challenging task. Standard ViT architectures adopt an aggressive patch size (e.g., 1616 pixels), resulting in high-dimensional token space that can be comparable to, or larger than, the Transformers hidden dimension. SiD2 [26] and PixelFlow [6] adopt hierarchical designs that start from smaller patches; however, these models are FLOP-heavy [26] and lose the inherent generality and simplicity of standard Transformers. PixNerd [70] adopts NeRF head [43] that integrates information from the Transformer output, noisy input, and spatial coordinates, with training further assisted by representation alignment [74]. Even with these special-purpose designs, the architectures in these works typically start from the (Large) or XL size. In fact, latest work [73] suggests that large hidden size appears necessary for high dimensionality. High-dimensional Diffusion. When using ViT-style architectures, modern diffusion models are still challenged by high-dimensional input spaces, whether in pixels or latents. In the literature [8, 73, 55], it has been repeatedly reported that ViT-style diffusion models degrade rapidly and catastrophically when the per-token dimensionality increases, regardless of the use of pixels or latents. Concurrently with our work, line of research [78, 34, 55] resorts to self-supervised pre-training to address highdimensional diffusion. In contrast to these efforts, we show that high-dimensional diffusion is achievable without any pre-training, and using just Transformers. x-prediction. The formulation of x-prediction is natural and not new; it can be traced back at least to the original DDPM [23] (see their code [24]). However, DDPM observed that ϵ-prediction was substantially better, which later became the go-to solution. In later works (e.g., [26]), although the analysis was sometimes preferably conducted in the x-space, the actual prediction was often made in other spaces, likely for legacy reasons. When it comes to the image restoration application addressed by diffusion [10, 72, 42], it is natural for the network to predict the clean data, as this is the ultimate goal of image restoration. Concurrent with our work, [18] also advocates the use of x-prediction, for generative world models that are conditional on previous frames. Our work does not aim to reinvent this fundamental concept; rather, we aim to draw attention to largely overlooked yet critical issue in the context of high-dimensional data with underlying low-dimensional manifolds. 3. On Prediction Outputs of Diffusion Models Diffusion models can be formulated in the space of x, ϵ, or v. The choice of the space determines not only where the loss is defined, but also what the network predicts. Importantly, the loss space and the network output space need not be the same. This choice can make critical differences. 3.1. Background: Diffusion and Flows Diffusion models can be formulated from the perspective of ODEs [5, 60, 37, 58]. We begin our formulation with the flow-based paradigm [37, 38, 1], i.e., in the v-space, as simpler starting point, and then discuss other spaces. Consider data distribution pdata(x) and noise distribution ϵ pnoise(ϵ) (e.g., ϵ (0, I)). During training, noisy sample zt is an interpolation: zt = at + bt ϵ, where at and bt are pre-defined noise schedules at time [0, 1]. In this paper, we use linear schedule [37, 38, 1]1: at = and bt = 1 t. This gives: zt = + (1 t) ϵ, (1) which leads to zt pdata when t=1. We use the logitnormal distribution over [15], i.e., logit(t) (µ, σ2). flow velocity is defined as the time-derivative of z, that is, vt = t = tx + tϵ. Given Eq. (1), we have: = ϵ. (2) The flow-based methods [37, 38, 1] minimize loss function defined as: = Et,x,ϵvθ(zt, t) v2, (3) where vθ is function parameterized by θ. While vθ is often the direct output of network vθ = netθ(zt, t) [37, 38, 1], it can also be transform of it, as we will elaborate. Given the function vθ, sampling is done by solving an ordinary differential equation (ODE) for [37, 38, 1]: dzt/dt = vθ(zt, t), (4) starting from z0 pnoise and ending at = 1. In practice, this ODE can be approximately solved using numerical solvers. By default, we use 50-step Heun. 1Our analysis in this paper is applicable to other schedules. 3 (a) x-pred xθ := netθ(zt, t) xθ (1) x-loss: Exθ x2 (2) ϵ-loss: ϵθ ϵ 2 ϵθ = (ztt xθ )/(1 t) (3) v-loss: Evθ v2 vθ = ( xθ zt)/(1 t) (b) ϵ-pred ϵθ := netθ(zt, t) xθ = (zt(1t) ϵθ )/t ϵθ vθ = (zt ϵθ )/t (c) v-pred vθ := netθ(zt, t) xθ = (1t) vθ +zt ϵθ = ztt vθ vθ Table 1. All possible combinations of defining the loss and network prediction in x, v, or ϵ spaces. The direct network outputs are highlighted in colors. For any off-diagonal entry where the network output space differs from the loss space, transformation on the network output is applied. ground-truth x-pred ϵ-pred v-pred 3.2. Prediction Space and Loss Space Prediction Space. The networks direct output can be defined in any space: v, x, or ϵ. Next, we discuss the resulting transformation. Note that in the context of this paper, we refer to it as x, ϵ, v-prediction, only when the network netθs direct output is strictly x, ϵ, v, respectively. Given three unknowns (x, ϵ, v) and one network output, we require two additional constraints to determine all three unknowns. The two constraints are given by Eq. (1) and (2). For example, when we let the direct network output netθ be x, we solve the following set of equations: D=2 D= D=16 xθ = netθ zt = xθ + (1 t) ϵθ vθ = xθ ϵθ D=512 (5) Here, the notations xθ, ϵθ, and vθ suggest that they are all predictions dependent on θ. Solving this equation set gives: ϵθ = (zt txθ)/(1t) and vθ = (xθ zt)/(1t), that is, both ϵθ and vθ can be computed from zt and the network xθ. These are summarized in Tab. 1 in column (a). Similarly, when we let the direct network output netθ be ϵ or v, we obtain the other sets of equations (by replacing the first one in Eq. (5)). The transformations are summarized in Tab. 1 in the columns of (b), (c) for ϵ-, v-prediction. This shows that when one quantity in {x, ϵ, v} is predicted, the other two can be inferred. The derivations in many prior works (e.g., [52, 15]) are special cases covered in Tab. 1. Loss Space. While the loss is often defined in one reference space (e.g., v-loss in Eq. (3)), conceptually, one can define it in any space. It has been shown [52, 15] that with given reparameterization from one prediction space to another, the loss is effectively reweighted. For example, consider the combination of x-prediction and v-loss in Tab. 1(3)(a). We have vθ = (xθ zt)/(1 t) as the prediction and = (x zt)/(1 t) as the target. The v-loss in Eq. (3) becomes: = Evθ(zt, t) v2 = 1 (1t)2 xθ(zt, t) x2, which is reweighted form of the x-loss. transformation like this one can be done for any prediction space and any loss space listed in Tab. 1. Put together, consider the three unweighted losses defined in {x, ϵ, v}, and the three forms of the network direct output, there are nine possible combinations (Tab. 1). Each combination constitutes valid formulation, and no Figure 2. Toy Experiment: d-dimensional (d = 2) underlying data is buried in D-dimensional space, by fixed, random, In the D-dim space, we column-orthogonal projection matrix. train simple generative model (5-layer ReLU MLP with 256-dim hidden units). The projection matrix is unknown to the model, and we only use it for visualizing the output. In this toy experiment, with the observed dimension increasing, only x-prediction can produce reasonable results. two among the nine cases are mathematically equivalent. Generator Space. Regardless of the combination used, to perform generation at inference-time, we can always transform the network output to the v-space (Tab. 1, row (3)) and solve the ODE in Eq. (4) for sampling. As such, all nine combinations are legitimate generators. 3.3. Toy Experiment According to the manifold assumption [4], the data tends to lie in low-dimensional manifold  (Fig. 1)  , while noise ϵ and velocity are off-manifold. Letting network directly predict the clean data should be more tractable. We verify this assumption in toy experiment in this section. We consider the toy case of d-dimensional underlying data buried in an observed D-dimensional space (d < D). We synthesize this scenario using projection matrix RDd that is column-orthogonal, i.e., = Idd. This matrix is randomly created and fixed. The observed data is = ˆx RD, where the underlying data is ˆx Rd. The matrix is unknown to the model, and as such, it is D-dimensional generation problem for the model. We train 5-layer ReLU MLP with 256-dim hidden units as the generator and visualize the results in Fig. 2. We obtain these visualizations by projecting the D-dim generated samples back to d-dim using . We investigate the cases of {2, 8, 16, 512} for = 2. We study x, ϵ, or vprediction, all using the v-loss, i.e., Tab. 1(3)(a-c). Fig. 2 shows that only x-prediction can produce reasonable results when increases. For ϵ-/v-prediction, the models struggle at D=16, and fail catastrophically when D=512, where the 256-dim MLP is under-complete. Notably, x-prediction can work well even when the model is under-complete. Here, the 256-dim MLP inevitably discards information in the D=512-dim space. However, since the true data is in low-dimensional d-dim space, x-prediction can still perform well, as the ideal output is implicitly d-dim. We draw similar observations in the case of real data on ImageNet, as we show next. 4. Just Image Transformers for Diffusion Driven by the above analysis, we show that plain Vision Transformers (ViT) [13] operating on pixels can work surprisingly well, simply using x-prediction. 4.1. Just Image Transformers The core idea of ViT [13] is Transformer on Patches (ToP)2. Our architecture design follows this philosophy. Formally, consider HW C-dim image data (C=3). All x, ϵ, and zt share this same dimensionality. Given an image, we divide it into non-overlapping patches of size pp, resulting in sequence of length . Each patch is pp3-dim vector. This sequence is processed by linear embedding projection, added with positional embedding [66], and mapped by stack of Transformer blocks [66]. The output layer is linear predictor that projects each token back to pp3-dim patch. See Fig. 3. As standard practice, the architecture is conditioned on time and given class label. We use adaLN-Zero [46] for conditioning and will discuss other options later. Conceptually, this architecture amounts to the Diffusion Transformer (DiT) [46] directly applied to patches of pixels. The overall architecture is nothing more than Just image Transformers, which we refer to as JiT. For example, we investigate JiT/16 (i.e., patch size p=16, [13]) on 256256 images, and JiT/32 (p=32) on 512512 images. These settings respectively result in dimensionality of 768 (16163) and 3072 (32323) per patch. Such highdimensional patches can be handled by x-prediction. 4.2. What to Predict by the Network? We have summarized the nine possible combinations of loss space and prediction space in Tab. 1. For each of these com2Quoting Lucas Beyer. Figure 3. The Just image Transformer (JiT) architecture: simply plain ViT [13] on patches of pixels for x-prediction. binations, we train Base [13] model (JiT-B), which has hidden size of 768-dim per token. We study JiT-B/16 at 256256 resolution in Tab. 2(a). As reference, we examine JiT-B/4 (i.e., p=4) at 6464 in Tab. 2(b). In both settings, the sequence length is the same (1616). We draw the following observations: x-prediction is critical. In Tab. 2(a) with JiT-B/16, only x-prediction performs well, and it works across all three losses. Here, patch is 768-d (16163), which coincides with the hidden size of 768 in JiT-B. While this may seem about enough, in practice the models may require additional capacity, e.g., to handle positional embeddings. For ϵ-/v-prediction, the model does not have enough capacity to separate and retain the noised quantities. These observations are similar to those in the toy case  (Fig. 2)  . As comparison, we examine JiT-B/4 at 6464 resolution (Tab. 2(b)). Here, all cases perform reasonably well: the accuracy gaps among the nine combinations are marginal, not decisive. The dimensionality is 48 (443) per patch, well below the hidden size of 768 in JiT-B, which explains why all combinations work reasonably well. We note that many previous latent diffusion models have similarly small input dimensionality and therefore were not exposed to the issue we discuss here. Loss weighting is not sufficient. Our work is not the first to enumerate the combinations of relevant factors. In [52], it has explored the combinations of loss weighting and network predictions. Their experiments were done on the lowdimensional CIFAR-10 dataset, using U-net. Their observations were closer to ours on ImageNet 6464.3 However, Tab. 2(a) on ImageNet 256256 suggests that loss weighting is not the whole story. On one hand, both 3In the CIFAR-10 experiments in [52], they enumerated three types of In 8 out of these 9 loss weighting and three types of network outputs. combinations, their models work reasonably well (see their Tab. 1). x-loss ϵ-loss v-loss x-loss ϵ-loss v-loss x-pred 10.14 10.45 8.62 ϵ-pred 379.21 394.58 372. v-pred 107.55 126.88 96.53 (lower noise) t-shift (µ) x-pred 0.0 0.4 0.8 1. 14.44 9.79 8.62 8.99 ϵ-pred 464.25 372.91 372.36 355.25 v-pred 120.03 109.93 96.53 106.85 (a) ImageNet 256256, JiT-B/ (higher noise) x-pred ϵ-pred v-pred 5.76 3.56 3.55 6.20 4.02 3. 6.12 3.76 3.46 (b) ImageNet 6464, JiT-B/4 Table 3. Noise-level shift (JiT-B/16, ImageNet 256256, FID50K). We shift the noise level by adjusting µ in the logit-normal t-sampler [15]. An appropriate noise level is useful, but is not sufficient for addressing the catastrophic failure in ϵ-/v-prediction. Settings (the same as Tab. 2): 200 epochs, with CFG. Table 2. Results of all combinations of loss space and network space (see Tab. 1), evaluated by FID-50K on ImageNet: (a) JiTB/16 at 256 resolution, 768-d per patch; (b) JiT-B/4 at 64 resolution, 48-d per patch. We annotate catastrophic failures in red and reasonable results by green. Settings: 200 epochs, with CFG [22]. ϵand v-prediction fail catastrophically in Tab. 2(a), regardless of the loss space, which corresponds to different effective weightings in different loss spaces (as discussed). On the other hand, x-prediction works across all three loss spaces: the loss weighting induced by the v-loss is preferable, but not critical.4 Noise-level shift is not sufficient. Prior works [7, 25, 26] have suggested that increasing the noise level is useful for high-resolution pixel-based diffusion. We examine this in Tab. 3 with JiT-B/16. As we use the logit-normal distribution [15] for sampling (see appendix), the noise level can be shifted by changing the parameter µ of this distribution: intuitively, shifting µ towards the negative side results in smaller and thus increases the noise level (Eq. (1)). Tab. 3 shows that when the model already performs decently (here, x-pred), appropriately high noise is beneficial, which is consistent with prior observations [7, 25, 26]. However, adjusting the noise level alone cannot remedy ϵor v-prediction: their failure stems inherently from the inability to propagate high-dimensional information. As side note, according to Tab. 3, we set µ = 0.8 in other experiments on ImageNet 256256. Increasing hidden units is not necessary. Since the capacity can be limited by the network width (i.e., numbers of hidden units), natural idea is to increase it. However, this remedy is neither principled nor feasible when the observed dimension is very high. We show that this is not necessary in the case of x-prediction. In Tab. 5 and Tab. 6 in the next section, we show results of JiT/32 at resolution 512 and JiT/64 at resolution 1024, using proportionally large patch size of p=32 or p=64. This amounts to 3072-dim (i.e., 32323) or 12288-dim per patch, substantially larger than the hidden size of B, L, and models (defined in [13]). Nevertheless, x-prediction 4From Tab. 1(a), we see that with x-prediction, the coefficients of xθ are 1, t/(1t), and 1/(1t) in the three rows. When converting to xloss, the weights of x-loss are 1, t2/(1t)2, and 1/(1t)2, respectively. Figure 4. Bottleneck linear embedding. Results are for JiT-B/16 on ImageNet 256256. raw patch is 768-dim (16163) and is embedded by two sequential linear layers with an intermediate bottleneck dimension (d < 768). Here, bottleneck embedding is generally beneficial, and our x-prediction model can work decently even with aggressive bottlenecks as small as 32 or 16. Settings (the same as Tab. 3): 200 epochs, with CFG. works well; in fact, it works without any modification other than scaling the noise proportionally (e.g., by 2 and 4 at resolution 512 and 1024; see appendix). This evidence suggests that the network design can be largely decoupled from the observed dimensionality, as is the case in many other neural network applications. Increasing the number of hidden units can be beneficial (as widely observed in deep learning), but it is not decisive. Bottleneck can be beneficial. Even more surprisingly, we find that, conversely, introducing bottleneck that reduces dimensionality in the network can be beneficial. Specifically, we turn the linear patch embedding layer into low-rank linear layer, by replacing it with pair of bottleneck (yet still linear) layers. The first layer reduces the dimension to d, and the second layer expands it to the hidden size of the Transformer. Both layers are linear and serve as low-rank reparameterization. Fig. 4 plots the FID vs. the bottleneck dimension d, using JiT-B/16 (768-d per raw patch). Reducing the bottleneck dimension, even to as small as 16-d, does not cause catastrophic failure. In fact, bottleneck dimension across wide range (32 to 512) can improve the quality, by decent margin of up to 1.3 FID. From broader perspective of representation learning, this observation is not entirely unexpected. Bottleneck designs are often introduced to encourage the learning of inherently low-dimensional representations [64, 48, 41, 2]. 6 Algorithm 1 Training step # net(z, t): JiT network # x: training batch = sample t() = randn like(x) = * + (1 - t) * = (x - z) / (1 - t) x_pred = net(z, t) v_pred = (x_pred - z) / (1 - t) loss = l2 loss(v - v_pred) Algorithm 2 Sampling step (Euler) # z: current samples at x_pred = net(z, t) v_pred = (x_pred - z) / (1 - t) z_next = + (t_next - t) * v_pred 4.3. Our Algorithm Our final algorithm adopts x-prediction and v-loss, which corresponds to Tab. 1(3)(a). Formally, we optimize: = Et,x,ϵ (cid:13) (cid:13) (cid:13)vθ(zt, t) (cid:13) 2 (cid:13) (cid:13) , (6) where: vθ(zt, t) = (netθ(zt, t) zt)/(1 t). Alg. 1 shows the pseudo-code of training step, and Alg. 2 is that of sampling step (Euler solver; can be extended to Heun or other solvers). For brevity, class conditioning and CFG [22] are omitted, but both follow standard practice. To prevent zero division in 1/(1t) , we clip its denominator (by default, 0.05) whenever computing this division. 4.4. Just Advanced Transformers The strength of general-purpose Transformer [66] is partly in that, when its design is decoupled from the specific task, it can benefit from architectural advances developed in other applications. This property underpins the advantage of formulating diffusion with task-agnostic Transformer. Following [73], we incorporate popular general-purpose improvements5: SwiGLU [54], RMSNorm [75], RoPE [62], qk-norm [19], all of which were originally developed for language models. We also explore in-context class conditioning: but unlike original ViT [13] that appends one class token to the sequence, we appends multiple such tokens (by default, 32; see appendix), following [35]. Tab. 4 reports the effects of these components. 5Our baselines in previous sections all use SwiGLU + RMSNorm. Removing them has slight degradation: FID goes from 7.48 to 7.89. 7 Baseline (SwiGLU, RMSNorm) + RoPE, qk-norm + in-context class tokens JiT-B/16 7.48 (6.32) 6.69 (5.44) 5.49 (4.37) JiT-L/16 - - 3.39 (2.79) Table 4. Just Advanced Transformers with general-purpose designs. All are JiT/16 for ImageNet 256256, with bottleneck patch embedding (128-d, Fig. 4), evaluated by FID-50K. Settings: 200 epochs, with CFG (and with CFG interval [33] in brackets). resolution model 256256 JiT-B/16 256 512512 JiT-B/32 256 10241024 JiT-B/64 256 len patch dim hiddens params Gflops FID 4.37 4.64 4.82 768 3072 12288 768 768 768 131 133 141 25 26 Table 5. ImageNet 10241024 with JiT-B/64. All entries have roughly the same number of parameters and compute. Settings: if not specified here, the same as Tab. 4 (all are with CFG interval). 256256 JiT-B/16 JiT-L/16 JiT-H/16 JiT-G/16 200-ep 4.37 2.79 2.29 2.15 600-ep 3.66 2.36 1.86 1.82 512512 JiT-B/32 JiT-L/32 JiT-H/32 JiT-G/32 200-ep 4.64 3.06 2.51 2. 600-ep 4.02 2.53 1.94 1.78 Table 6. Scalability on ImageNet 256256 and 512512, evaluated by FID-50K. All models have the same sequence length of 1616, and thus the models at 512 resolution have nearly the same compute as their 256 counterparts. Settings: the same as Tab. 5. 5. Comparisons High-resolution generation on pixels. In Tab. 5, we further report our base-size model (JiT-B) on ImageNet at resolutions 512 and even 1024. We use patch sizes proportional to image sizes, and therefore the sequence length at different resolutions remains the same. The per-patch dimension can be as high as 3072 or 12288, and none of the common models would have sufficient hidden units. Tab. 5 shows that our models perform decently across resolutions. All models have similar numbers of parameters and computational cost, which only differ in the input/output patch embeddings. Our method does not suffer from the curse of observed dimensionalities. Scalability. core goal of decoupling the Transformer design with the task is to leverage the potential for scalability. Tab. 6 provides results on ImageNet 256 and 512 with four model sizes (note that at resolution 512, none of these models have more hidden units than the patch dimension). The model sizes and flops are shown in Tab. 7 and 8: our model at resolution 256 has similar cost as its counterpart at 512. Our approach benefits from scaling. Interestingly, the FID difference between resolution 256 and 512 becomes smaller with larger models. For JiT-G, the FID at 512 resolution is even lower. For very large models on ImageNet, FID performance largely depends on overfitting, and denoising at 512 resolution poses more challenging task, making it less susceptible to overfitting. pre-training token perc. self-sup. params Gflops FID IS - - - - - - ImgNet 256256 Latent-space Diffusion 675+49M 119 DiT-XL/2 [46] SD-VAE VGG 675+49M 119 SiT-XL/2 [40] SD-VAE VGG REPA [74], SiT-XL/2 SD-VAE VGG DINOv2 675+49M 119 LightningDiT-XL/2 [73] VA-VAE VGG DINOv2 675+49M 119 DDT-XL/2 [71] SD-VAE VGG DINOv2 675+49M 119 RAE [78], DiTDH-XL/2 RAE VGG DINOv2 839+415M 146 Pixel-space (non-diffusion) JetFormer [65] FractalMAR-H [36] Pixel-space Diffusion ADM-G [12] RIN [28] SiD [25] , UViT/2 VDM++, UViT/2 SiD2 [26], UViT/2 SiD2 [26], UViT/1 PixelFlow [6], XL/4 PixNerd [70], XL/16 JiT-B/16 JiT-L/16 JiT-H/16 JiT-G/16 - - - - - - - DINOv2 - - - - 559M 320M 2B 2B N/A N/A 677M 700M 131M 459M 953M 2B 1120 334 555 555 137 653 2909 134 25 88 182 383 2.8B 848M - - - - - - - - - - - - - - - - - - - - - - - - - - - - 2.27 278.2 2.06 277.5 1.42 305.7 1.35 295.3 1.26 310.6 1.13 262.6 - 6.64 6.15 348. - - 7.72 172.7 3.95 216.0 2.44 256.3 2.12 267.7 1.73 1.38 1.98 282.1 2.15 297 3.66 275.1 2.36 298.5 1.86 303.4 1.82 292.6 Figure 5. Qualitative Results. Selected examples on ImageNet 512512 using JiT-H/32. More uncurated results are in appendix. Reference results from previous works. As reference, we compare with previous results in Tab. 7 and 8. We mark the pre-training components involved for each method. Compared with other pixel-based methods, ours is purely driven by plain, general-purpose Transformers. Our models are compute-friendly and avoid the quadratic scaling of expense with doubled resolution (see flops in Tab. 8). Our approach does not use extra losses or pre-training, which may lead to further gains (an example is in appendix). These directions are left for future work. 6. Discussion and Conclusion Noise is inherently different from natural data. Over the years, the development of diffusion models has focused primarily on probabilistic formulations, while paying less attention to the capabilities (and limitations) of the neural networks used. However, neural networks are not infinitely capable, and they can better use their capacity to model data rather than noise. Under these perspectives, our findings on x-prediction are, in hindsight, natural outcome. Our work adopts minimalist and self-contained design. By reducing domain-specific inductive biases, we hope our approach can generalize to other domains where tokenizers are difficult to obtain. This property is particularly desirable for scientific applications that involve raw, highTable 7. Reference results on ImageNet 256256. FID [21] and IS [53] of 50K samples are evaluated. The pre-training columns list the external models required to obtain the results (note that the perceptual loss [77] uses pre-trained VGG classifier [56]). The parameters include the generator and tokenizer decoder (used at inference-time), but exclude other pre-trained components. The Giga-flops are measured for single forward pass (not counting the tokenizer) and are roughly proportional to the computational cost of an iteration during both training and inference (for the multi-scale method [6], we measure the finest level). pre-training token perc. self-sup. params Gflops FID IS - - ImgNet 512512 Latent-space Diffusion DiT-XL/2 [46] 675+49M 525 SD-VAE VGG SiT-XL/2 [40] 675+49M 525 SD-VAE VGG REPA [74], SiT-XL/2 SD-VAE VGG DINOv2 675+49M 525 SD-VAE VGG DINOv2 675+49M 525 DDT-XL/2 [71] RAE [78], DiTDH-XL/2 RAE VGG DINOv2 839+415M 642 Pixel-space Diffusion ADM-G [12] RIN [28] SiD [25] , UViT/4 VDM++, UViT/4 SiD2 [26], UViT/4 SiD2 [26], UViT/2 PixNerd [70], XL/16 JiT-B/32 JiT-L/32 JiT-H/32 JiT-G/32 - - - - - - DINOv2 - - - - 559M 320M 2B 2B N/A N/A 700 133M 462M 956M 2B 1983 415 555 555 137 653 583 26 89 183 384 - - - - - - - - - - - - - - - - - - - - - - 3.04 240.8 2.62 252.2 2.08 274.6 1.28 305.1 1.13 259.6 - - 7.72 172.7 3.95 216.0 3.02 248.7 2.65 278.1 2.19 1.48 2.84 245.6 4.02 271.0 2.53 299.9 1.94 309.1 1.78 306.8 Table 8. Reference results on ImageNet 512512. JiT has an aggressive patch size and can use small compute to achieve strong results. Notations are similar to Tab. 7. dimensional natural data. We envision that the generalpurpose Diffusion + Transformer paradigm will be potential foundation in other areas. A. Implementation Details Our implementation closely follows the public codebases of DiT [46] and SiT [40]. Our configurations are summarized in Tab. 9. We describe the details as follows. Time distribution. Following [15], during training, we adopt the logit-normal distribution over [15]: logit(t) (µ, σ2). Specifically, we sample (µ, σ2) and let t=sigmoid(s). The hyper-parameter µ determines the noise level (see Tab. 3), and by default we set µ = 0.8 on ImageNet at resolution 256 (or 512, 1024), and fix σ as 0.8. ImageNet 512512 and 10241024. We adopt JiT/32 (i.e., patch size of 32) on ImageNet 512512. The model leads to sequence of 256 = 1616 patches, the same as JiT/16 on ImageNet 256256. As such, JiT/32 only differs from JiT/16 in the input/output patch dimension, increasing from 768-d to 3072-d per patch; all other computations and costs are exactly the same. To reuse the exact same recipe from ImageNet 256256, for 512512 images we rescale the magnitude of the noise ϵ by 2: that is, ϵ (0, 22I). This simple modification approximately maintains the signal-to-noise ratio (SNR) between the 256256 and 512512 resolutions [25, 7, 26]. No other changes to the ImageNet 256256 configuration are required or applied. For ImageNet 10241024, we use the model JiT/64 and scale the noise ϵ by 4. No other change is needed. In-context class conditioning. Standard DiT [46] performs class conditioning through adaLN-Zero. In Tab. 4, we further explore in-context class-conditioning. Specifically, following ViT [13], one can prepend class token to the sequence of patches. This is referred to as incontext conditioning in DiT [46]. Note that we use incontext conditioning jointly with the default adaLN-Zero conditioning, unlike DiT. In addition, following MAR [35], we further prepend multiple such tokens to the sequence. These tokens are repeated instances of the same class token, with different positional embeddings added. We prepend 32 such tokens. Moreover, rather than prepending these tokens to the Transformers input, we find that prepending them at later blocks can be beneficial (see in-context start block in Tab. 9). Tab. 4 shows that our implementation of in-context conditioning improves FID by 1.2. Dropout and early stop. We apply dropout [61] in JiT-H and to mitigate the risk of overfitting. Specifically, we apply dropout to the middle half of the Transformer blocks. For Transformer blocks with dropout, we apply it to both the attention block and the MLP block. As the G-size models still tend to overfit under our current dropout setting, we apply early stopping when the monitored FID begins to degrade. This occurs at around 320 epochs for both JiT-G/16 and JiT-G/32. architecture depth hidden dim heads image size patch size bottleneck dropout in-context class tokens in-context start block training epochs warmup epochs [17] optimizer batch size learning rate learning rate schedule weight decay ema decay time sampler noise scale clip of (1 t) in division class token drop (for CFG) sampling ODE solver ODE steps time steps CFG scale sweep range [22] CFG interval [33] JiT-B JiT-L JiT-H JiT-G 24 1024 16 32 1280 16 40 12 1664 768 12 16 256 (other settings: 512, or 1024) image size / 16 128 (B/L), 256 (H/G) 0 (B/L), 0.2 (H/G) 32 (if used) 10 10 4 200 (ablation), 600 5 Adam [31], β1, β2 = 0.9, 0.95 1024 2e-4 constant 0 {0.9996, 0.9998, 0.9999} logit(t)N (µ, σ2), µ = 0.8, σ = 0.8 1.0 image size / 256 0.05 0.1 Heun [20] 50 linear in [0.0, 1.0] [1.0, 4.0] [0.1, 1] (if used) Table 9. Configurations of experiments. Figure 6. The influence of CFG, without and with CFG interval (for JiT-L/16 on ImageNet 256256, 600 epochs). EMA and CFG. Our study covers wide range of configurations, including variations in loss and prediction spaces, model sizes, and architectural components. The optimal values of the CFG scale [22] and EMA (exponential moving average) decay vary from case to case, and fixing them may lead to incomplete or misleading observations. Since maintaining these hyperparameter configurations is relatively inexpensive, we strive to adopt the optimal values. Specifically, for the CFG scale ω [22], we determine the optimal value by searching over range of candidate scales at inference time, as is common practice in existing work. For EMA decays, we maintain multiple copies of the moving-averaged parameters during training, which introduces negligible computational overhead. To avoid memory overhead, different EMA copies can be stored on separate devices (e.g., GPUs). As such, both the CFG scale 9 Fig. 7 (top) shows that v-prediction yields substantially higher loss values (about 25%) than x-prediction, even though v-prediction appears to be the native parameterization for the v-loss. This comparison indicates that the task of x-prediction is inherently easier, as the data lie on low-dimensional manifold. We also observe that ϵprediction (not shown here) has about 3 higher loss and is unstable, which may explain its failure in Tab. 2(a). Fig. 7 (bottom) compares the denoised images corresponding to the two training curves. For x-prediction, the denoised image is simply xθ = netθ; for v-prediction, the denoised image is xθ=(1t)netθ+zt with vθ=netθ (see Tab. 1(c)(1)). The higher loss of v-prediction in Fig. 7 (top) can be reflected by its noticeable artifacts in Fig. 7 (bottom). Note that the artifact in Fig. 7 (bottom) is that of single denoising step. In the generation process, this error can accumulate in the multi-step ODE solver, which leads to the catastrophic failure in Tab. 2(a). B.2. Pre-conditioner In EDM [29], an extra pre-conditioner is applied to wrap the direct output of the network. Using the notation of our paper, the pre-conditioner formulation can be written as: xθ(zt, t) = cskip zt + cout netθ(zt, t), where cskip and cout are pre-defined coefficients. This equation suggests that unless cskip 0, the network output in pre-conditioner must not perform x-prediction. And according to the manifold assumption, this formulation should not remedy the issue we consider, as we examine next. Formulation of pre-conditioners. Given the definition of pre-conditioner, we can rewrite the pre-conditioned xprediction as: xθ = cskip zt + cout netθ ϵθ = (zt txθ)/(1 t) vθ = (xθ zt)/(1 t) (7) Accordingly, the objective in Eq. (6) (v-loss) is written as: = Et,x,ϵ (cid:13) (cid:13) (cid:13)vθ(zt, t) (cid:13) 2 (cid:13) (cid:13) , (8) where: vθ(zt, t) = (xθ(zt, t) zt)/(1 t) and xθ(zt, t) = cskip zt + cout netθ(zt, t). Comparing with Eq. (6), the only difference is in how we compute xθ from the network. As EDM [29] uses variance-exploding schedule (that is, zt=x+σtϵ) and we use (roughly) variance-preserving version (that is, zt=tx+(1t)ϵ), fully equivalent conversion is impossible. To have pre-conditioner in our case, zt = + 1t we rewrite our version as 1 ϵ. As such, we set σt = 1t , which is the noise added to unscaled imt ages, similar to EDMs. With this, we can rewrite the coefficients defined by EDM [29] as: cskip = 1 and σ2 data data+σ2 σ2 = 0.1 = 0. = 0.3 = 0.4 = 0.5 noisy image denoised image from x-pred denoised image from v-pred Figure 7. (Top): Training loss of xand v-prediction, using the same loss space of v-loss (Tab. 2(a), third row). We plot the loss averaged per pixel per channel. (Bottom): Denoised images from xand v-prediction, where v-predictions denoised output is visualized according to Tab. 1(c)(1). The denoised image from vprediction has noticeable artifacts, as reflected by the higher loss. and EMA decay are essentially inference-time decisions. Our CFG scale candidates range from 1.0 to 4.0 with step size of 0.1. The influence of CFG is shown in Fig. 6 for JiT-L/16 model. Our EMA decay candidates are 0.9996, 0.9998, and 0.9999, evaluated with batch size of 1024. For each model (including any one in the ablation), we search for the optimal setting using 8K samples and then apply it to evaluate 50K samples. Evaluation. Following common pratice, we evaluate the FID [21] against the ImageNet training set. We evaluate FID on 50K generated images, with 50 samples for each of the 1000 ImageNet classes. We evaluate the Inception Score (IS) [53] on the same 50K images. B. Additional Experiments B.1. Training Loss and Denoised Images In Tab. 2(a), the failure of ϵ-/v-prediction is caused by the inherent incapability of predicting high-dimensional output from limited-capacity network. This failure can be seen from the training loss curves. In Fig. 7 (top), we compare the training loss curves under the same v-loss, defined as = Evθ(zt, t)v2, using vprediction (i.e., vθ = netθ) versus x-prediction (i.e., vθ = (netθ zt)/(1t)). Since the loss is computed in the same space and only the parameterization differs, comparing the loss values is legitimate. 10 x-pred cskip = 0 cout = 10.14 10.45 8.62 x-loss ϵ-loss v-loss pre-conditioned predictions EDM-style σ2 data data+σ2 σ2 cskip = linear cskip= 1 cout= σdataσt (cid:113) data+σ2 σ2 cout = 1 39.50 67.56 46.25 28.94 72.05 35.49 Table 10. Comparisons with pre-conditioners (FID-50K, ImageNet 256, JiT-B/16). The settings are the same as Tab. 2 (a). FID-50K v-loss only w/ cls loss JiT-B/16 4.37 4.14 JiT-L/16 2.79 2.50 This minor modification leads to decent improvement, as shown in Tab. 11. This exploration suggests further potential for combining our simple method with additional loss terms, which we leave for future work. Despite the improvement, we do not use this or any additional loss in the other experiments presented in this paper. B.4. Cross-resolution Generation model trained at one resolution can be applied to another by simply downsampling or upsampling the generated images. We refer to this as cross-resolution generation. In our setup, JiT/16 at 256 and JiT/32 at 512 have comparable parameters and compute, making their cross-resolution comparison meaningful. The results are in Tab. Table 11. Exploration: additional classification loss. We do not use this loss in any other experiments. Settings: ImageNet 256256, 200-ep, with CFG interval. JiT-G/16@256 JiT-G/32@512, 256 FID@256 1.82 1.84 JiT-G/16@256, 512 JiT-G/32@512 FID@512 2.45 1.78 data+σ2 σ2 where σdata is the data standard deviation cout = σdataσt set as 0.5 [29]. We choose cin = (= 1 σt+1 ), so that the direct input to the network (i.e., cin 1 zt) is still zt. It can be shown that only when 0, we have: σt + , cskip0, cout1, which approaches x-prediction. We also consider simpler linear pre-conditioner: cskip = and cout = 1 t, which also performs x-prediction only when = 0. Results of pre-conditioners. Tab. 10 shows that the preconditioned versions all fail catastrophically, suggesting that deviating from x-prediction is not desired in highdimensional spaces. Interestingly, the pre-conditioned versions are much better than ϵ-/v-prediction in Tab. 2(a). We hypothesize that this is because they are more similar to xprediction when t0, which alleviates this issue. Table 12. Cross-resolution Generation (noted in gray), using JiT-G/16 trained at resolution 256 and JiT-G/32 trained at 512, followed by upsampling () or downsampling (). All entries have similar parameters and flops (see Tab. 7 and 8). Downsampling the images generated by the 512 model to 256 resolution yields decent FID@256 of 1.84. This result remains competitive when compared with the 256resolution expert (FID@256 of 1.82), while maintaining similar computational cost and gaining the additional ability to generate at 512 resolution. On the other hand, upsampling the images generated by the 256 model to 512 resolution results in noticeably worse FID@512 of 2.45, compared with the 512-resolution experts FID@512 of 1.78. This degradation is caused by the loss of higher-frequency details due to upsampling. B.3. Exploration: Classification Loss B.5. Additional Metrics Our paper focuses on minimalist design, and we intentionally do not use any extra loss. However, we note that latentbased methods [49] typically rely on tokenizers trained with adversarial and perceptual losses, and thus their generation process is not purely driven by diffusion. Next, we discuss simple extension on our pixel-based models with an additional classification loss. Formally, we attach classifier head after specific Transformer block (the 4th in JiT-B and the 8th in JiT-L). The classifier consists of global average pooling followed by linear layer, and cross-entropy loss is applied for the 1000-class ImageNet classification task. This classification loss Lcls is scaled by weight λ (e.g., 100) and added to the ℓ2-based (i.e., element-wise sum of squared errors) regression loss. To prevent label leakage, we disable class conditioning for all layers before the classifier head. We note that this modification remains minimal and does not rely on any pre-trained classifier, unlike the perceptual loss [76]. For completeness, we report precision and recall [32] on ImageNet 256256 in Tab. 13, compared with the commonly used baselines of DiT and SiT, and the latest RAE: DiT-XL/2 [46] SiT-XL/2 [40] RAE [78], DiTDH-XL/2 JiT-B/16 JiT-L/16 JiT-H/16 JiT-G/ FID 2.27 2.06 1.13 3.66 2.36 1.86 1.82 IS 278.2 277.5 262.6 275.1 298.5 303.4 292.6 Prec 0.83 0.82 0.78 0.82 0.80 0.78 0.79 Rec 0.57 0.59 0.67 0.50 0.59 0.62 0.62 Table 13. Precision and recall on ImageNet 256256. C. Qualitative Results In Fig. 8 to 11, we provide additional uncurated examples on ImageNet 256256. 11 Acknowledgements. We thank Google TPU Research Cloud (TRC) for granting us access to TPUs, and the MIT ORCD Seed Fund Grants for supporting GPU resources."
        },
        {
            "title": "References",
            "content": "[1] Michael Samuel Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In ICLR, 2023. [2] Alexander Alemi, Ian Fischer, Joshua Dillon, and Kevin Murphy. Deep variational information bottleneck. In ICLR, 2017. [3] Gunnar Carlsson. Topology and data. Bulletin of the American Mathematical Society, 46(2):255308, 2009. [4] Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien, editors. Semi-Supervised Learning. MIT Press, Cambridge, MA, USA, 2006. [5] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential equations. In NeurIPS, 2018. [6] Shoufa Chen, Chongjian Ge, Shilong Zhang, Peize Sun, and Ping Luo. PixelFlow: Pixel-space generative models with flow. arXiv:2504.07963, 2025. [16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and In NeurIPS, Yoshua Bengio. Generative adversarial nets. 2014. [17] Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training ImageNet in 1 hour. arXiv:1706.02677, 2017. [18] Danijar Hafner, Wilson Yan, and Timothy Lillicrap. Training agents inside of scalable world models. arXiv:2509.24527, 2025. [19] Alex Henry, Prudhvi Raj Dachapally, Shubham Shantaram Pawar, and Yuxuan Chen. Query-key normalization for Transformers. In Findings of EMNLP, 2020. [20] Karl Heun. Neue methoden zur approximativen integration der differentialgleichungen einer unabhangigen veranderlichen. Z. Math. Phys, 45:2338, 1900. [21] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by two time-scale update rule converge to local Nash equilibrium. NeurIPS, 2017. [22] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS Workshops, 2021. [7] Ting Chen. On the importance of noise scheduling for diffu- [23] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion models. arXiv:2301.10972, 2023. sion probabilistic models. In NeurIPS, 2020. [8] Xinlei Chen, Zhuang Liu, Saining Xie, and Kaiming He. Deconstructing denoising diffusion models for self-supervised learning. In ICLR, 2025. [9] Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and Karen Egiazarian. Image denoising by sparse 3-D transformdomain collaborative filtering. IEEE Transactions on image processing, 16(8):20802095, 2007. [10] Mauricio Delbracio and Peyman Milanfar. Inversion by direct iteration: An alternative to denoising diffusion for image restoration. Transactions on Machine Learning Research, 2023. [11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: large-scale hierarchical image database. In CVPR, 2009. [12] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat GANs on image synthesis. In NeurIPS, 2021. [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. [14] Michael Elad and Michal Aharon. Image denoising via sparse and redundant representations over learned dictionaries. IEEE Transactions on Image processing, 15(12):3736 3745, 2006. [15] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, and Robin Rombach. Scaling rectified flow Transformers for high-resolution image synthesis. In ICML, 2024. [24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. DDPM github repo. L155, diffusion utils 2.py, 2020. [25] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution images. ICML, 2023. [26] Emiel Hoogeboom, Thomas Mensink, Jonathan Heek, Kay Lamerigts, Ruiqi Gao, and Tim Salimans. Simpler Diffusion (SiD2): 1.5 FID on ImageNet512 with pixel-space diffusion. In CVPR, 2025. [27] Ahmed Imtiaz Humayun, Ibtihel Amara, Cristina Vasconcelos, Deepak Ramachandran, Candice Schumann, Junfeng He, Katherine Heller, Golnoosh Farnadi, Negar Rostamzadeh, and Mohammad Havaei. What secrets do your manifolds hold? understanding the local geometry of generative models. In ICLR, 2025. [28] Allan Jabri, David Fleet, and Ting Chen. Scalable adaptive computation for iterative generation. In ICML, 2023. [29] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In NeurIPS, 2022. [30] Diederik Kingma and Ruiqi Gao. Understanding diffusion objectives as the ELBO with simple data augmentation. In NeurIPS, 2023. [31] Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. In ICLR, 2015. [32] Tuomas Kynkaanniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. NeurIPS, 2019. [33] Tuomas Kynkaanniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, and Jaakko Lehtinen. Applying guidance in limited interval improves sample and distribution quality in diffusion models. In NeurIPS, 2024. 12 [34] Jiachen Lei, Keli Liu, Julius Berner, Haiming Yu, Hongkai Zheng, Jiahong Wu, and Xiangxiang Chu. Advancing endto-end pixel space generative modeling via self-supervised pre-training. arXiv:2510.12586, 2025. [35] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. In NeurIPS, 2024. [36] Tianhong Li, Qinyi Sun, Lijie Fan, and Kaiming He. Fractal generative models. arXiv:2502.17437, 2025. [37] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. In ICLR, 2023. [38] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In ICLR, 2023. [39] Gabriel Loaiza-Ganem, Brendan Leigh Ross, Rasa Hosseinzadeh, Anthony Caterini, and Jesse Cresswell. Deep generative models through the lens of the manifold hypothesis: survey and new connections. Transactions on Machine Learning Research, 2024. [40] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. SiT: Exploring flow and diffusion-based generative models with scalable interpolant Transformers. In ECCV, 2024. [41] Alireza Makhzani and Brendan Frey. K-sparse autoencoders. arXiv:1312.5663, 2013. [42] Peyman Milanfar and Mauricio Delbracio. Denoising: powerful building block for imaging, inverse problems and machine learning. Philosophical Transactions A, 383(2299): 20240326, 2025. [43] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. [44] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In ICML, 2021. [45] Maxime Oquab et al. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2023. [46] William Peebles and Saining Xie. Scalable diffusion models with Transformers. In ICCV, 2023. [47] Javier Portilla, Vasily Strela, Martin Wainwright, and Eero Simoncelli. Image denoising using scale mixtures of Gaussians in the wavelet domain. IEEE Transactions on Image processing, 12(11):13381351, 2003. [48] Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. Contractive auto-encoders: Explicit invariance during feature extraction. In ICML, 2011. [49] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. [50] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. UNet: Convolutional networks for biomedical image segmentation. In MICCAI, 2015. [51] Sam Roweis and Lawrence Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290 (5500):23232326, 2000. [52] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In ICLR, 2022. [53] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training GANs. NeurIPS, 29, 2016. [54] Noam Shazeer. GLU variants improve Transformer. arXiv:2002.05202, 2020. [55] Minglei Shi, Haolin Wang, Wenzhao Zheng, Ziyang Yuan, Xiaoshi Wu, Xintao Wang, Pengfei Wan, Jie Zhou, and Jiwen Lu. Latent diffusion model without variational autoencoder. arXiv:2510.15301, 2025. [56] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv:1409.1556, 2014. [57] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML, 2015. [58] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. [59] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In NeurIPS, 2019. [60] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2021. [61] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):19291958, 2014. [62] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. RoFormer: Enhanced Transformer with rotary position embedding. Neurocomputing, 568: 127063, 2024. [63] Joshua Tenenbaum, Vin de Silva, and John Langford. global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):23192323, 2000. [64] Naftali Tishby, Fernando Pereira, and William Bialek. arXiv preprint The information bottleneck method. physics/0004057, 2000. [65] Michael Tschannen, Andre Susano Pinto, and Alexander Kolesnikov. JetFormer: an autoregressive generative model of raw images and text. In ICLR, 2025. [66] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017. [67] Pascal Vincent. connection between score matching and denoising autoencoders. Neural computation, 23(7):1661 1674, 2011. [68] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In ICML, 2008. [69] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua and Leon Bottou. Bengio, Pierre-Antoine Manzagol, Stacked denoising autoencoders: Learning useful representations in deep network with local denoising criterion. Journal of Machine Learning Research, 11(12), 2010. 13 [70] Shuai Wang, Ziteng Gao, Chenhui Zhu, Weilin Huang, and Limin Wang. PixNerd: Pixel neural field diffusion. arXiv:2507.23268, 2025. [71] Shuai Wang, Zhi Tian, Weilin Huang, and Limin Wang. DDT: Decoupled diffusion Transformer. arXiv:2504.05741, 2025. [72] Yutong Xie, Minne Yuan, Bin Dong, and Quanzheng Li. Diffusion model for generative image denoising. In ICCV, 2023. [73] Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. In CVPR, 2025. [74] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion Transformers is easier than you think. In ICLR, 2025. [75] Biao Zhang and Rico Sennrich. Root mean square layer normalization. In NeurIPS, 2019. [76] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. [77] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. [78] Boyang Zheng, Nanye Ma, Shengbang Tong, and Saining Xie. Diffusion Transformers with representation autoencoders. arXiv:2510.11690, 2025. [79] Daniel Zoran and Yair Weiss. From learning models of natIn ICCV, ural image patches to whole image restoration. 2011. class 012: house finch, linnet, Carpodacus mexicanus class 014: indigo bunting, indigo finch, indigo bird, Passerina cyanea class 042: agama class 081: ptarmigan class 107: jellyfish class 108: sea anemone, anemone class 110: flatworm, platyhelminth class 117: chambered nautilus, pearly nautilus, nautilus class 130: flamingo class 279: Arctic fox, white fox, Alopex lagopus Figure 8. Uncurated samples on ImageNet 256256 using JiT-G/16 conditioned on the specified classes. Unlike the common practice of visualizing with higher CFG, here we show images using the CFG value (2.2) that achieves the reported FID of 1.82. class 288: leopard, Panthera pardus class 309: bee class 349: bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn class 397: puffer, pufferfish, blowfish, globefish class 425: barn class 448: birdhouse class 453: bookcase class 458: brass, memorial tablet, plaque class 495: china cabinet, china closet class 500: cliff dwelling Figure 9. Uncurated samples on ImageNet 256256 using JiT-G/16 conditioned on the specified classes. Unlike the common practice of visualizing with higher CFG, here we show images using the CFG value (2.2) that achieves the reported FID of 1.82. class 658: mitten class 661: Model class 718: pier class 724: pirate, pirate ship class 725: pitcher, ewer class 757: recreational vehicle, RV, R.V. class 779: school bus class 780: schooner class 829: streetcar, tram, tramcar, trolley, trolley car class 853: thatch, thatched roof Figure 10. Uncurated samples on ImageNet 256256 using JiT-G/16 conditioned on the specified classes. Unlike the common practice of visualizing with higher CFG, here we show images using the CFG value (2.2) that achieves the reported FID of 1.82. class 873: triumphal arch class 900: water tower class 911: wool, woolen, woollen class 913: wreck class 927: trifle class 930: French loaf class 946: cardoon class 947: mushroom class 975: lakeside, lakeshore class 989: hip, rose hip, rosehip Figure 11. Uncurated samples on ImageNet 256256 using JiT-G/16 conditioned on the specified classes. Unlike the common practice of visualizing with higher CFG, here we show images using the CFG value (2.2) that achieves the reported FID of 1.82."
        }
    ],
    "affiliations": []
}