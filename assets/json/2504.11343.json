{
    "paper_title": "A Minimalist Approach to LLM Reasoning: from Rejection Sampling to Reinforce",
    "authors": [
        "Wei Xiong",
        "Jiarui Yao",
        "Yuhui Xu",
        "Bo Pang",
        "Lei Wang",
        "Doyen Sahoo",
        "Junnan Li",
        "Nan Jiang",
        "Tong Zhang",
        "Caiming Xiong",
        "Hanze Dong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) has become a prevailing approach for fine-tuning large language models (LLMs) on complex reasoning tasks. Among recent methods, GRPO stands out for its empirical success in training models such as DeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In this work, we revisit GRPO from a reinforce-like algorithm perspective and analyze its core components. Surprisingly, we find that a simple rejection sampling baseline, RAFT, which trains only on positively rewarded samples, yields competitive performance than GRPO and PPO. Our ablation studies reveal that GRPO's main advantage arises from discarding prompts with entirely incorrect responses, rather than from its reward normalization. Motivated by this insight, we propose Reinforce-Rej, a minimal extension of policy gradient that filters both entirely incorrect and entirely correct samples. Reinforce-Rej improves KL efficiency and stability, serving as a lightweight yet effective alternative to more complex RL algorithms. We advocate RAFT as a robust and interpretable baseline, and suggest that future advances should focus on more principled designs for incorporating negative samples, rather than relying on them indiscriminately. Our findings provide guidance for future work in reward-based LLM post-training."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 3 4 3 1 1 . 4 0 5 2 : r Minimalist Approach to LLM Reasoning: from Rejection Sampling to Reinforce Wei Xiong Jiarui Yao Yuhui Xu Bo Pang Lei Wang Doyen Sahoo Junnan Li Nan Jiang Tong Zhang Caiming Xiong Hanze Dong Salesforce AI Research University of Illinois Urbana-Champaign Abstract Reinforcement learning (RL) has become prevailing approach for fine-tuning large language models (LLMs) on complex reasoning tasks. Among recent methods, GRPO stands out for its empirical success in training models such as DeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In this work, we revisit GRPO from reinforce-like algorithm perspective and analyze its core components. Surprisingly, we find that simple rejection sampling baseline, RAFT, which trains only on positively rewarded samples, yields competitive performance than GRPO and PPO. Our ablation studies reveal that GRPOs main advantage arises from discarding prompts with entirely incorrect responses, rather than from its reward normalization. Motivated by this insight, we propose Reinforce-Rej, minimal extension of policy gradient that filters both entirely incorrect and entirely correct samples. Reinforce-Rej improves KL efficiency and stability, serving as lightweight yet effective alternative to more complex RL algorithms. We advocate RAFT as robust and interpretable baseline, and suggest that future advances should focus on more principled designs for incorporating negative samples, rather than relying on them indiscriminately. Our findings provide guidance for future work in reward-based LLM post-training."
        },
        {
            "title": "1 Introduction",
            "content": "We investigate reinforcement learning (RL) algorithms in the context of fine-tuning large language models (LLMs) with verifiable rewards. Our focus is on mathematical reasoning tasks, which have recently received significant attention following the release of models such as OpenAIs O1 Model (Jaech et al., 2024) and DeepSeek-R1 (DeepSeek-AI et al., 2025). The dominant approach in LLM post-training has been Proximal Policy Optimization (PPO) (Schulman et al., 2017; Bai et al., 2022; Ouyang et al., 2022). However, PPO requires an additional critic network beyond the vanilla Reinforce algorithm (Williams and Peng, 1991), introducing both computational overhead and algorithmic complexity. Meanwhile, the deterministic transition nature of LLM also simplifies the problem with relatively lower variance, many of PPOs sophisticated components may be unnecessary in this setting. This observation has inspired growing interest in designing simpler yet effective RL algorithms for post-training LLMs. Several recent works revisit Reinforce-style approaches, including ReMax (Li et al., 2023), RLOO (Ahmadian et al., 2024; Kool et al., 2019), GRPO (Shao et al., 2024), and Reinforce++ (Hu, 2025). In parallel, other methods explore different directions beyond policy gradients. Reward-ranked fine-tuning (RAFT) (Anthony et al., 2017; Dong et al., 2023) iteratively generates responses per prompt, filter out those with incorrect answers, and fine-tune the LLM on the remaining accepted samples. Direct preference-based methods, such *HD and WX contributed equally to this work. Corresponding to hanze.dong@salesforce.com and wx13@illinois.edu. 1 as SFT-based contrastive learning (Slic-HF) (Zhao et al., 2023) and DPO (Rafailov et al., 2023), optimize contrastive objectives based on pairwise comparison dataset. Among these, GRPO stands out as one of the most widely used algorithms for enhancing LLMs on math reasoning tasks due to its success in training DeepSeek-R1 (DeepSeek-AI et al., 2025). However, its algorithmic details remain largely undocumented, and it is unclear whether its adoption stems from inherent advantages or, rather, from continuity with methods used in their previous studies. In contrast, RAFT has established itself as one of the simplest and most interpretable baselines, consistently showing good empirical performance in prior literature despite its minimalistic design. In this project, we revisit (1) RAFT, also know as rejection sampling in LLM literature, which is arguably the most basic RL algorithm for LLM post-training; (2) Vanilla Reinforce, classical policy gradient algorithm, serves as simplified version of PPO by eliminating the critic model, and (3) GRPO, Reinforce algorithm variant, samples responses per prompt and computes relative advantages by normalizing the sample reward using mean and standard deviation within each prompt. key difference between GRPO (Reinforce) and RAFT lies in how they handle negative samples: GRPO mixes both accepted and rejected examples during training, whereas RAFT relies only on positive samples. While it is commonly believed that RL methods leveraging negative signals significantly outperform SFT-like algorithms that only use positive samples, in our preliminary experiments, we observe that the performance gap is surprisingly small, and RAFT-like approach even exhibits faster convergence in the early training stage (e.g., the first 100-200 iterations). Our further analysis reveals that certain types of negative signals, such as prompts with entirely incorrect responses, can even can significantly hurt model performance. Meanwhile, other techniques like reward normalization appear to have minimal impact. To better understand these dynamics, we conducted ablation studies isolating individual design choices using both Qwen (Yang et al., 2024) and LLaMA (Grattafiori et al., 2024) models across several Reinforce variants. Our results highlight the following key findings: 1. We revisit RAFT, simple rejection sampling baseline that uses only positive samples, and find that its performance is competitive with the state-of-the-art RL method GRPO with surprisingly small gap and faster convergence rate in the early training stages. deeper analysis reveals that RAFT, which trains solely on positive samples, leads to rapid reduction in policy entropy, limiting exploration and eventually being surpassed by GRPO. 2. Through set of controlled experiments across different Reinforce variants, we find that for on-policy methods, training on prompts where all sampled responses are incorrect can significantly harm performance. We further identify that the performance gain of GRPO over standard Reinforce largely stems from its implicit filtering of these harmful prompts. In contrast, reward normalization techniques by mean and standard deviation within prompt have minimal impact. 3. Motivated by our studies with both RAFT and Reinforce, we study new Reinforce variant, Reinforcerej, which selectively filters out prompts with either all correct or all incorrect responses. This method enjoys comparable final performance to GRPO, and demonstrates superior KL efficiency. These insights highlight the importance of sample selection over algorithmic design in reward-based LLM post-training. The codes of this project are available at https://github.com/RLHFlow/Minimal-RL."
        },
        {
            "title": "2 Method",
            "content": "Notation. Given prompt, an LLM is denoted as policy that can map the prompt to distribution over response a: π(ax). We also denote r(x, a) {1, 1} as binary reward function that assigns scalar feedback to prompt-response pair, which can be implemented by the verifier1. We denote the dataset of collected prompt-response pairs as D. For each prompt x, we can generate candidate responses a1, , an, and their corresponding rewards are r1, , rn. 1https://github.com/huggingface/Math-Verify 2 Let at be the t-th token in response = (a1, , aa), and let st(θ) = πθ(atx,a1:t1) πθold (atx,a1:t1) denote the importance sampling ratio for token t. We also define the baseline of rewards as mean(r1, , rn) and its standard deviation as std(r1, , rn). We now review several representative algorithms used for the LLM post training. RAFT. The RAFT algorithm is also referred to as the rejection sampling fine-tuning (Touvron et al., 2023; Yuan et al., 2023) in the literature. We follow the formalization in Dong et al. (2023), which consists of the following three steps: Data Collection. For batch of prompts {x1, , xM }, we sample responses per prompt from reference model (e.g., the current model) to obtain candidate responses {ai,1, , ai,n} for each xi. Data ranking (Rejection Sampling). For each prompt xi, we compute the reward values of each response {ri,1, , ri,n} using the binary reward function r(x, a), and retain only the responses with the highest reward (typically those with = 1). The resulting set of positive samples is aggregated into dataset D. Model Fine-Tuning. The current policy π is then fine-tuned to maximize the log-likelihood over the selected dataset: LRAFT(θ) = (cid:88) log πθ(ax). (x,a)D (1) closely related algorithm is STaR (Zelikman et al., 2022), which also trains on self-generated CoT responses. In comparison, STaR retrains from the original pre-trained model in each iteration instead of the current model. Meanwhile, STaR uses greedy decoding and generate only one response, as compared to the rejection sampling used in RAFT. Lastly, STaR also proposes to provide the answer in the prompt to generate CoT responses for difficult problems. Policy Gradient and Reinforce. We illustrate the idea by taking the action as whole for simplicity and extend to the autoregressive model later. The policy gradient algorithm is designed to solve the following learning objective: J(θ) = J(πθ) = Exd (cid:2)Eaπθ(x)r(x, a)(cid:3), (2) where θ is the parameter of the neural network. We can use policy ascent to update the policy network: θ θ + β θJ(θ), where θJ(θ) is referred to as the policy gradient in the literature. The policy gradient is given by: J(θ) θ = Exd0 (cid:104) Eaπθ(x) (cid:2) log πθ(ax) θ r(x, a)(cid:3)(cid:105) . In practice, similar to the pipeline of RAFT, we usually use πθold to collect the trajectories into the replay buffer and use these samples to compute stochastic policy gradient to update πθold. However, for strict on-policy training, we have to collect new data after single step of gradient ascent. To accelerate training, we usually perform multiple steps in mini-batch manner, and adopt the importance sampling technique to correct the distribution. Specifically, we can re-write the objective function as: J(θ) = J(πθ) = Exd (cid:104) Eaπθold (x) (cid:2) πθ(ax) πθold (ax) r(x, a)(cid:3)(cid:105) . (3) Then, with batch of trajectories {x, a, r} collected by πθold , we can update multiple steps using the above importance sampling trick. However, the importance sampling can lead to high variance if the distribution of πθ and πθold are too far away. To stabilize the training, we can also leverage the clipping techniques from the PPO. Finally, the loss function is: LReinforce(θ) = 1 (cid:88) (cid:104) min x,aD (cid:16) πθ(ax) πθold (ax) r(x, a), clip( πθ(ax) πθold (ax) , 1 ϵ, 1 + ϵ) r(x, a) (cid:17)(cid:105) . (4) Since LLM is autoregressive, we typically regard each token as an action. Therefore, we can extend the loss to the token-level counterpart: LReinforce(θ) = 1 (cid:88) x,aD 1 a (cid:88) (cid:104) (cid:16) min t=1 st(θ), clip(st(θ), 1 ϵ, 1 + ϵ) r(x, a) (cid:17)(cid:105) , (5) where st(θ) = πθ(atx,a1:t1) πθold (atx,a1:t1) and at is the t-th token of a. GRPO. GRPO adopts loss function similar to Equation (5), but replaces r(x, a) with an advantage function At(x, a) for the t-th token of response a. Specifically, for each prompt x, GRPO will sample > 1 responses and compute the following advantage for the t-th token of the i-th response: At(x, ai) = ri mean(r1, rn) std(r1, , rn) . mean(r1, rn) is often referred to as the baseline in the RL literature, which serves to reduce the variance of the stochastic gradient. (Iterative) DPO. The DPO algorithm relies on pairwise comparison dataset {(x, a+, a)}, where a+ are two responses to the prompt x. Then, DPO optimizes the following contrastive loss: LDPO(θ) = log σ (cid:16) β log πθ(a+x) πref (a+x) β log πθ(ax) πref (ax) (cid:17) , where β > 0 and πref is usually set as the initial checkpoint. The original DPO algorithm trains on offline and off-policy data. In the subsequent studies (Liu et al., 2023; Xiong et al., 2023; Xu et al., 2023; Hoang Tran, 2024; Dong et al., 2024), it is shown that we can iteratively use the intermediate checkpoints to generate new responses, label the preference signals, and train on the self-generated on-policy data to largely improve the model performance. RAFT++. We notice that RAFT can also be viewed as hybrid algorithm that can be off-policy when performing multiple steps on the replay buffer at each iteration. As natural extension, we also apply the importance sampling and clipping techniques to the original RAFT, arriving at similar loss function: LRAFT++(θ) = 1 (cid:88) x,aD 1 a (cid:88) (cid:104) (cid:16) min t=1 st(θ), clip(st(θ), 1 ϵ, 1 + ϵ) I(cid:0)r(x, a) = argmax r(x, ai)(cid:1)(cid:17)(cid:105) , (6) where the indicator ensures that we only train on the response with the highest reward (positive samples)."
        },
        {
            "title": "3 Experiment Setup",
            "content": "We focus on the mathematical reasoning task in this project. The implementations are mainly based on the verl (Sheng et al., 2024) framework. 4 Model Algorithm Math500 Minerva Math Olympiad Bench Average Qwen2.5-Math-7B-base LLaMA-3.2-3B-instruct Base RAFT RAFT++ Iterative DPO Reinforce GRPO PPO Base RAFT RAFT++ Reinforce GRPO PPO 41.3 77.4 80.5 75.7 80.6 81.6 79.6 27.3 46.6 48.8 42.9 51.0 47.7 11.0 34.4 35.8 30.5 36.1 36.7 34.8 8.3 16.8 16.9 14.8 18.9 17. 18.6 37.8 41.2 38.3 42.1 43.3 41.1 6.5 15.5 16.8 12.5 18.4 16.1 23.6 49.9 52.5 48.2 52.9 53.9 51.8 14.0 26.3 27.5 23.4 29.4 27.0 Table 1: Performance of different algorithms across three benchmarks including Math500 (Hendrycks et al., 2021), Minerva Math (Lewkowycz et al., 2022), and Olympiad Bench (He et al., 2024). The reported accuracy is average@16 with temperature 1.0 and maximal generation length of 4096 tokens. Dataset and Models. We train the models using the prompt set Numina-Math (Beeching et al., 2024), which consists of approximately 860k math problems and labeled ground-truth answers. The sources of Numina-Math ranges from Chinese high school math exercises to US and international mathematics olympiad competition problems. We conduct experiments with both Qwen2.5-Math-7B-base, and LLaMA-3.2-3Binstruct for generality. We use the default chat template of these models and use CoT prompting: Lets think step by step and output the final answer within boxed{}. Hyper-parameters. We follow most of the hyper-parameter setups recommended in the verl framework for the Reinforce, GRPO, and PPO training. The hyper-parameters for RAFT and RAFT++ are also the same with the GRPO script. Specifically, we use the AdamW optimizer with learning rate of 1 106. We sample 1024 prompts per iteration, and generate = 4 responses per prompt for RAFT and GRPO. The training mini-batch size is set to be 512. The models are allowed to generate 4096 tokens at most during training. More detailed scripts are available in the GitHub repository. For the baseline of iterative DPO, we use the codebase developed in Zhang et al. (2025). Evaluation. We evaluate the models reasoning ability by Math500 (Hendrycks et al., 2021), Minerva Math (Lewkowycz et al., 2022), Olympiad Bench (He et al., 2024). We do not include the popular AIME2024 benchmark since it only consists of 30 problems. In our preliminary experiments, we observe that the trend on this benchmark is very noisy for all the considered algorithms. We mainly use average@16 to evaluate our models, where we generate 16 responses per prompt with temperature 1.0, and use the average accuracy as the metric. The models are allowed to generate 4096 token at most. The codes are available at https://github.com/RLHFlow/Minimal-RL."
        },
        {
            "title": "4 Main Results",
            "content": "RAFT and RAFT++ approach deep RL methods with surprisingly small performance gap. We summarize the test accuracy of models trained using various algorithms in Table 1. Our first observation is that RAFT (and its variant RAFT++), which is arguably the simplest algorithm, achieves competitive performance compared to more complex methods such as iterative DPO and deep RL-based approaches. Specifically, with Qwen2.5-Math-7B-base, vanilla RAFT reaches an average accuracy of 49.9%, outperforming 5 iterative DPO (48.2%) and approaching PPO (51.8%). With the additional importance sampling and clipping techniques, RAFT++ further improves over vanilla RAFT, achieving 52.5% average accuracy. This result is remarkably close to the state-of-the-art deep RL method GRPO, which reaches an average accuracy of 53.9% in its best model. similar trend is observed on the LLaMA-3.2-3B-instruct model, demonstrating the robustness of RAFT and RAFT++ across different models. These results are somewhat counter-intuitive, as RL methods are often believed to be more powerful due to their ability to utilize negative feedback. Interestingly, in the LLaMA-based setting, Reinforce performs substantially worse than RAFT++, with an average accuracy of 23.4% compared to the 27.5% of RAFT++. One possible explanation is that defining negative samples solely based on final answer correctness may be too coarse, potentially limiting the benefits of using negative feedback in RL training. We will also include more ablation studies to investigate the role of the negative samples in current practice of RL training. Figure 1: The learning dynamics of RAFT and RAFT++, initialized from Qwen2.5-Math-7B-base (left) and LLaMA-3.2-3B-instruct (right). The y-axis is the average@16 accuracy, that is further averaged on MATH500, Minerva Math, and Olympiad Bench. We also plot the best model of GRPO, PPO, and Iterative DPO for reference. Distribution correction and clipping improve vanilla RAFT. Table 1 also shows that applying importance sampling to correct for distribution shift in the replay buffer improves the final test accuracy of RAFT, leading to stronger variant we refer to as RAFT++. We further illustrate the learning dynamics of RAFT and RAFT++ in Figure 1. Both methods are capable of steadily enhancing the models reasoning ability through online updates, with RAFT++ achieving faster convergence and higher final accuracy than vanilla RAFT. As part of our ablation study, we also evaluate an intermediate variant that applies importance sampling without clipping. As shown in Figure 2, this variant underperforms vanilla RAFT. This observation contradicts the findings of Ahmadian et al. (2024), which suggest that clipping rarely occurs and is therefore unnecessary. We hypothesize that although clipping may be infrequent, it happens when πθ deviates far πθold from 1. In such cases, unbounded updates can severely violate the on-policy assumption underlying policy gradient methods, leading to instability and degraded performance. 6 Figure 2: Left: the training reward curves of RAFT, RAFT++, RAFT++ without clipping (i.e., RAFT and importance sampling), and GRPO, initialized from Qwen2.5-Math-7B-base. Right: the training reward curves of RAFT++ and RAFT++ enhanced by clip higher trick, initialized from LLaMA-3.2-3B-instruct. We transform the original reward using (1 + r)/2 so that the resulting value corresponds to the accuracy on the training data. We also apply moving average with window size of 20 to smooth the curves. RAFT++ achieves faster early-stage convergence but is surpassed by GRPO in later training. From Figure 2, we also observe that RAFT++ exhibits faster early-stage learning compared to GRPO. Moreover, we also observe clear turning point in the training dynamics around iteration 100, where its growth rate slows noticeably after this point. Eventually, RAFT++ is surpassed by GRPO in the later stage of training in terms of final model test accuracy. We will also conduct ablation experiments to investigate the cause of this slowdown in RAFT++ and the role of the missing negative samples in this process."
        },
        {
            "title": "4.1 Ablation Study",
            "content": "In this subsection, we aim to understand the underlying reasons behind the key findings presented earlier. To this end, we conduct series of ablation studies designed to answer the following questions: 1. From RAFT++ to Reinforce (including GRPO): Why is RAFT++ faster in the early stage but ultimately outperformed later in training? What role do negative samples play? 2. From Vanilla Reinforce to GRPO: What is the key factor behind GRPOs superior performance? Learning from only positive samples leads to faster convergence and entropy collapse. We begin by examining the policy entropy and KL divergence from the initial policy for RAFT++ and GRPO, as shown in Figure 3. key observation is that RAFT++, which trains exclusively on positive samples, exhibits much more rapid decline in policy entropy compared to GRPO. This trend is consistent across both Qwen and LLaMA models. Once the entropy stabilizes at low level, the performance improvement of RAFT++ slows noticeably. We attribute this to reduced exploration with the low-entropy policies, since they are less likely to generate diverse reasoning paths. In parallel, the KL divergence from the initial policy increases more rapidly in RAFT++ during early training, reflecting its initial advantage in test accuracy. However, due to the lack of continued exploration, RAFT++ quickly plateaus, while GRPO continues to improve and ultimately surpasses it. These findings suggest that negative samples play crucial role in maintaining exploration and preventing distributional collapse. This exploration benefit is likely contributing factor to the performance gap between RAFT++ and RL-based methods such as Reinforce and GRPO. To further investigate the relationship between policy entropy and reward learning, we incorporate the clip higher technique from Yu et al. (2025), which uses an asymmetric clipping range with ϵ1 = 0.1 for the lower bound and larger ϵ2 = 0.2 for the upper bound. We apply this variant to the LLaMA-3.2-3B-instruct model and visualize both the 7 training reward curves and policy entropy curves in the right figure of Figure 2. Consistent with the findings in Yu et al. (2025), using larger ϵ2 helps stabilize the policy entropy over the online training. As result, this enhanced RAFT++ variant outperforms the original RAFT++ during the later stages of training. Figure 3: The learning dynamics of RAFT++ and GRPO, initialized from Qwen2.5-Math-7B-base (first row) and LLaMA-3.2-3B-instruct (second row). We also plot the KL loss in the left column and the policy entropy loss in the right column. From Reinforce to GRPO: what is the key role to the success of GRPO? The primary differences between GRPO and RAFT lie in two aspects: the use of negative samples and the application of reward normalization. To isolate the contributions of each component and better understand their respective effects, we designed set of controlled experiments to systematically evaluate their impacts. Specifically, we consider the following algorithms: 1. Reinforce: the vanilla one introduced in Equation (5); 2. Reinforce + Mean Zero: we subtract the mean reward within each prompt; 3. Reinforce + Remove all correct: we filter out prompts whose responses are entirely correct; 4. Reinforce + Remove all wrong: we filter out prompts whose responses are entirely wrong; 5. Reinforce + Remove both: remove both fully correct and fully incorrect prompts; 6. Reinforce + Remove both + Normalized Std: in addition to removing both fully correct and fully incorrect prompts, we further divide the reward by its standard deviation within each prompt for normalization. As shown in Figure 4, the variant Reinforce + Remove all wrong achieves the significant performance improvement than vanilla Reinforce in terms of reward, clearly indicating that incorrect samples are particularly harmful in the Reinforce training process. This is likely due to their high variance and misleading gradients, which can dominate updates and misguide learning. In contrast, removing only correct samples (Reinforce + Remove all correct) does not help much. Meanwhile, removing both all correct and wrong samples result in more well-behaved entropy loss and slightly better reward, suggesting that it can help maintain exploration. We also observe that normalization alone, such as in the Reinforce + Mean Zero variant, leads to increased KL divergence and does not improve reward, indicating potential instability. Moreover, applying standard deviation normalization (Reinforce + Remove both + Normalize Std) yields little additional gain over simply removing bad samples, suggesting that variance normalization is not key contributor to performance. Taken together, these results highlight that the core strength of GRPO lies in rejecting low-quality (especially incorrect) samples, rather than normalization per se. We refer to the variant that removes both correct and incorrect samplesReinforce + Remove bothas Reinforce-Rej, which serves as simplified yet competitive baseline for reward-based policy optimization in LLMs. Figure 4: Ablation study on the components of GRPO and Reinforce-type algorithms with LLaMA-3.2-3Binstruct. We compare GRPO with other Reinforce-based variants to isolate the effects of removing incorrect samples, correct samples, and applying normalization. Removing incorrect samples (Remove all wrong) provides the largest gain in reward, highlighting their harmful impact. In contrast, the reward of removing correct samples is still not satisfactory. Mean-zero normalization increases KL loss and destabilizes training. Normalizing by standard deviation shows minimal additional benefit. The variant Reinforce + Remove both achieves good balance between reward, KL stability, and entropy regularization. We transform the original reward using (1 + r)/2 so that the resulting value corresponds to the accuracy on the training data. We also apply moving average with window size of 20 to smooth the curves."
        },
        {
            "title": "5 Conclusion",
            "content": "We revisited the design space of reinforcement learning algorithms for LLM post-training through the lens of rejection sampling. Our study shows that RAFTa simple rejection-based method relying solely on positively rewarded samplesserves as surprisingly strong baseline, outperforming or matching more sophisticated approaches such as PPO and iterative DPO. We further improved RAFT by incorporating 9 importance sampling and clipping, resulting in RAFT++, which achieves near state-of-the-art performance while maintaining simple and stable training pipeline. Through extensive ablations, we identified that GRPOs primary benefit comes not from its reward normalization, but from discarding prompts with entirely correct and incorrect responses. Building on this insight, we proposed Reinforce-Rej, minimal policy gradient variant that filters both entirely incorrect and entirely correct samples. Reinforce-Rej improves KL efficiency and entropy stability, highlighting the role of exploration in reward-based fine-tuning. Our findings suggest that the utility of negative samples in RL-based LLM training is more nuanced than previously assumed. Rather than relying on raw negative feedback, future methods should consider more selective and principled mechanisms for incorporating sample quality. We advocate RAFT and Reinforce-Rej as lightweight, interpretable, and effective baselines for future work on reward-driven LLM post-training."
        },
        {
            "title": "References",
            "content": "Ahmadian, A., Cremer, C., Galle, M., Fadaee, M., Kreutzer, J., Pietquin, O., Ustun, A., and Hooker, S. (2024). Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740. Anthony, T., Tian, Z., and Barber, D. (2017). Thinking fast and slow with deep learning and tree search. Advances in neural information processing systems, 30. Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et al. (2022). Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862. Beeching, E., Huang, S. C., Jiang, A., Li, J., Lipkin, B., Qina, Z., Rasul, K., Shen, Z., Soletskyi, R., and Tunstall, L. (2024). Numinamath 7b cot. https://huggingface.co/AI-MO/NuminaMath-7B-CoT. DeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H., Ding, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J. L., Ni, J., Liang, J., Chen, J., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L., Zhang, M., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R., Zhang, R., Pan, R., Wang, R., Chen, R. J., Jin, R. L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S., Li, S. S., Zhou, S., Wu, S., Ye, S., Yun, T., Pei, T., Sun, T., Wang, T., Zeng, W., Zhao, W., Liu, W., Liang, W., Gao, W., Yu, W., Zhang, W., Xiao, W. L., An, W., Liu, X., Wang, X., Chen, X., Nie, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yang, X., Li, X., Su, X., Lin, X., Li, X. Q., Jin, X., Shen, X., Chen, X., Sun, X., Wang, X., Song, X., Zhou, X., Wang, X., Shan, X., Li, Y. K., Wang, Y. Q., Wei, Y. X., Zhang, Y., Xu, Y., Li, Y., Zhao, Y., Sun, Y., Wang, Y., Yu, Y., Zhang, Y., Shi, Y., Xiong, Y., He, Y., Piao, Y., Wang, Y., Tan, Y., Ma, Y., Liu, Y., Guo, Y., Ou, Y., Wang, Y., Gong, Y., Zou, Y., He, Y., Xiong, Y., Luo, Y., You, Y., Liu, Y., Zhou, Y., Zhu, Y. X., Xu, Y., Huang, Y., Li, Y., Zheng, Y., Zhu, Y., Ma, Y., Tang, Y., Zha, Y., Yan, Y., Ren, Z. Z., Ren, Z., Sha, Z., Fu, Z., Xu, Z., Xie, Z., Zhang, Z., Hao, Z., Ma, Z., Yan, Z., Wu, Z., Gu, Z., Zhu, Z., Liu, Z., Li, Z., Xie, Z., Song, Z., Pan, Z., Huang, Z., Xu, Z., Zhang, Z., and Zhang, Z. (2025). Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Dong, H., Xiong, W., Goyal, D., Zhang, Y., Chow, W., Pan, R., Diao, S., Zhang, J., SHUM, K., and Zhang, T. (2023). RAFT: Reward ranked finetuning for generative foundation model alignment. Transactions on Machine Learning Research. Dong, H., Xiong, W., Pang, B., Wang, H., Zhao, H., Zhou, Y., Jiang, N., Sahoo, D., Xiong, C., and Zhang, T. (2024). Rlhf workflow: From reward modeling to online rlhf. arXiv preprint arXiv:2405.07863. 10 Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., et al. (2024). The llama 3 herd of models. arXiv preprint arXiv:2407.21783. He, C., Luo, R., Bai, Y., Hu, S., Thai, Z. L., Shen, J., Hu, J., Han, X., Huang, Y., Zhang, Y., et al. (2024). Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. (2021). Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874. Hoang Tran, Chris Glaze, B. H. (2024). Snorkel-mistral-pairrm-dpo. https://huggingface.co/snorkelai/ Snorkel-Mistral-PairRM-DPO. Hu, J. (2025). Reinforce++: simple and efficient approach for aligning large language models. arXiv preprint arXiv:2501.03262. Jaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. (2024). Openai o1 system card. arXiv preprint arXiv:2412.16720. Kool, W., van Hoof, H., and Welling, M. (2019). Buy 4 reinforce samples, get baseline for free! Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., et al. (2022). Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:38433857. Li, Z., Xu, T., Zhang, Y., Yu, Y., Sun, R., and Luo, Z.-Q. (2023). Remax: simple, effective, and efficient reinforcement learning method for aligning large language models. arXiv e-prints, pages arXiv2310. Liu, T., Zhao, Y., Joshi, R., Khalman, M., Saleh, M., Liu, P. J., and Liu, J. (2023). Statistical rejection sampling improves preference optimization. arXiv preprint arXiv:2309.06657. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. (2022). Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:2773027744. Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., and Finn, C. (2023). Direct preference optimization: Your language model is secretly reward model. arXiv preprint arXiv:2305.18290. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Zhang, M., Li, Y., Wu, Y., and Guo, D. (2024). Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Sheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. (2024). Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. (2023). Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Williams, R. J. and Peng, J. (1991). Function optimization using connectionist reinforcement learning algorithms. Connection Science, 3(3):241268. Xiong, W., Dong, H., Ye, C., Wang, Z., Zhong, H., Ji, H., Jiang, N., and Zhang, T. (2023). Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint. 11 Xu, J., Lee, A., Sukhbaatar, S., and Weston, J. (2023). Some things are more cringe than others: Preference optimization with the pairwise cringe loss. arXiv preprint arXiv:2312.16682. Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., et al. (2024). Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Fan, T., Liu, G., Liu, L., Liu, X., et al. (2025). Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476. Yuan, Z., Yuan, H., Li, C., Dong, G., Tan, C., and Zhou, C. (2023). Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825. Zelikman, E., Wu, Y., Mu, J., and Goodman, N. (2022). Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488. Zhang, H., Yao, J., Ye, C., Xiong, W., and Zhang, T. (2025). Online-dpo-r1: Unlocking effective reasoning without the ppo overhead. Zhao, Y., Joshi, R., Liu, T., Khalman, M., Saleh, M., and Liu, P. J. (2023). Slic-hf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425."
        }
    ],
    "affiliations": [
        "Salesforce AI Research",
        "University of Illinois Urbana-Champaign"
    ]
}