{
    "paper_title": "MagicID: Hybrid Preference Optimization for ID-Consistent and Dynamic-Preserved Video Customization",
    "authors": [
        "Hengjia Li",
        "Lifan Jiang",
        "Xi Xiao",
        "Tianyang Wang",
        "Hongwei Yi",
        "Boxi Wu",
        "Deng Cai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video identity customization seeks to produce high-fidelity videos that maintain consistent identity and exhibit significant dynamics based on users' reference images. However, existing approaches face two key challenges: identity degradation over extended video length and reduced dynamics during training, primarily due to their reliance on traditional self-reconstruction training with static images. To address these issues, we introduce $\\textbf{MagicID}$, a novel framework designed to directly promote the generation of identity-consistent and dynamically rich videos tailored to user preferences. Specifically, we propose constructing pairwise preference video data with explicit identity and dynamic rewards for preference learning, instead of sticking to the traditional self-reconstruction. To address the constraints of customized preference data, we introduce a hybrid sampling strategy. This approach first prioritizes identity preservation by leveraging static videos derived from reference images, then enhances dynamic motion quality in the generated videos using a Frontier-based sampling method. By utilizing these hybrid preference pairs, we optimize the model to align with the reward differences between pairs of customized preferences. Extensive experiments show that MagicID successfully achieves consistent identity and natural dynamics, surpassing existing methods across various metrics."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 9 8 6 2 1 . 3 0 5 2 : r MagicID: Hybrid Preference Optimization for ID-Consistent and Dynamic-Preserved Video Customization Hengjia Li1,, Lifan Jiang1,, Xi Xiao2,, Tianyang Wang2, Hongwei Yi3, Boxi Wu1, Deng Cai1 1Zhejiang University 2University of Alabama at Birmingham 3Hedra AI lihengjia98@gmail.com Project page: https://echopluto.github.io/MagicID-project/ Figure 1. Results of MagicID. Given few reference images, our method is capable of generating highly realistic and personalized videos that maintain consistent identity features while exhibiting natural and visually appealing motion dynamics."
        },
        {
            "title": "Abstract",
            "content": "Video identity customization seeks to produce highfidelity videos that maintain consistent identity and exhibit significant dynamics based on users reference images. However, existing approaches face two key challenges: identity degradation over extended video length and reduced dynamics during training, primarily due to their reliance on traditional self-reconstruction training with static * Equal Contribution images. To address these issues, we introduce MagicID, novel framework designed to directly promote the generation of identity-consistent and dynamically rich videos tailored to user preferences. Specifically, we propose constructing pairwise preference video data with explicit identity and dynamic rewards for preference learning, instead of sticking to the traditional self-reconstruction. To address the constraints of customized preference data, we introduce hybrid sampling strategy. This approach first prioritizes identity preservation by leveraging static videos derived from reference images, then enhances dynamic motion quality in the generated videos using Frontier-based sampling method. By utilizing these hybrid preference pairs, we optimize the model to align with the reward differences between pairs of customized preferences. Extensive experiments show that MagicID successfully achieves consistent identity and natural dynamics, surpassing existing methods across various metrics. 1. Introduction Identity-specific content generation [7, 10, 11, 23, 24, 27, 30, 3638, 41, 42] has long been central focus in the field of computer vision. Recent breakthroughs with the advent of diffusion models, have elevated personalized content creation to prominent position especially in identityconsistency image generation. However, replicating this level of accuracy in video generation [3, 4, 9, 18, 35] continues to pose significant challenges. It seeks to tailor diverse array of captivating videos utilizing limited set of users images, enabling the production of personalized video that showcases individuals in various actions and settings, all while ensuring exceptional identity (ID) fidelity. This innovation holds significant promise for applications within the film and television sectors. Compared to image customization, the challenge of video customization lies in utilizing limited number of static images rather than videos as references. Following the conventional approach of image customization, existing video customization methods [27, 37, 38] naively employ self-reconstruction of reference images to maintain the identity. However, we observe that this process introduces two significant issues, i.e., identity degradation for more frames and dynamic reduction during the training. (1) The longer the video length, the worse the identity consistency becomes. Since the reference images are essentially single-frame representations, the inherent disparity in temporal resolution with video sequences (comprising multiple frames) creates fundamental domain shift. Conventional self-reconstruction training paradigms fail to bridge this gap, resulting in inconsistent performance between training and inference phases. As illustrated in Fig. 2a, models trained with conventional methods exhibit poorer ID consistency when sampling videos with higher number of frames in the inference phase. (2) The longer the training duration, the less the video dynamics become. Since traditional methods merely reconstruct static images during training, it typically leads to severe distribution shift, which drives the model to generate videos with less dynamic degree. As shown in Fig. 2b, traditional methods demonstrate increasingly inferior dynamics over the course of the customization. To solve these problems, we propose MagicID, novel framework to maintain consistent identity with the reference images and preserve the natural motion dynamics. Inspired by the Direct Preference Optimization for language [17, 40] and image generation [15, 25], we introduce hybrid customized preference optimization to directly encourage the model to generate ID-consistent and dynamicpreserved videos. Instead of previous self-reconstruction approach on reference images, we propose to construct pairwise hybrid-preference video data with explicit customized rewards for the training. However, it is challenging to obtain preference video data that simultaneously preserves consistent identity and exhibits good dynamics for the preference learning. To overcome this limitation, we introduce hybrid sampling strategy to respectively construct identity-prefered and dynamic-prefered training pairs for two stages. (1) Intuitively, the simplistic utilization of generated sample pairs will constrain the models ability to assimilate the identity in the invisible references. Therefore, in the first identityprefered stage, we incorporate the static videos inflated from the reference images with selection of videos generated by the preliminary model into the base dataset repository. Since the objective of this stage is to maximally incentivize the model to preserve the identity consistently with the reference, thereby we prioritize the disparity in identity reward while permitting degree of tolerance for dynamic reward. (2) In the subsequent dynamic-prefered stage, we aim to construct Pareto-optimal pairs of dynamics and identity to enhance dynamics while maintaining identity learning. Thus, we incorporate the samples generated by the finetuned model into the base dataset repository and construct the preference pairs from the total base dataset repository. Then we introduce Frontier-based sampling method to select training pairs from the upper and lower Pareto frontiers according to the dynamic and identity rewards, ensuring the learning of both customized preferences. We evaluate our methodology using variety of general metrics and compare it with prior competitive methods for identity-preserved video generation. Comprehensive experimental and visual analyses reveal that our technique effectively produces high-quality videos featuring dynamic content and robust facial consistency, as depicted in Fig. 1. Our contributions are summarized as follows: We introduce MagicID, novel preference optimization to effectively solve the degradation of identity and dynamics. To the best of our knowledge, this is the first work to effectively achieve customized video generation with direct preference optimization. We propose hybrid sampling strategy to construct highquality preference video pairs, which addresses the issue of lacking the preference data with consistent identity and considerable dynamics. We demonstrate the effectiveness of MagicID with favorable video customization quality against state-of-the-art (a) ID Consistency for longer video. (b) Dynamic Degree for more steps. Figure 2. Analysis of identity degratation and dynamic reduction. (a) We compute the mean identity similarity with the reference images for generated videos of different lengths. As shown, traditional approaches suffer from diminished identity consistency as video length increases. In contrast, our method maintains strong identity robustness throughout prolonged video generations. (b) We calculate the dynamic degree for different training steps. As the customization progresses, traditional methods experience gradual loss of motion dynamic during customization, whereas our method preserves original video dynamics across the entire training. timization (DPO) [29] has emerged as promising alternative to traditional Reinforcement Learning from Human Feedback (RLHF) [44], as it eliminates the need for training separate reward model. Initially, DPO was widely adopted in LLMs [17, 31, 40] to better align model outputs with human preferences. More recently, its application has extended to stable diffusion and related generative tasks. In particular, some studies [20, 34] successfully applied DPO to text-to-image generation, significantly improving the aesthetic quality of 2D-generated images. Furthermore, DreamDPO [43] extended DPO to text-to-3D tasks, enhancing both the quality and controllability of generated 3D objects. Similarly, some studies [15, 25] have introduced DPO to text-to-video generation, leading to notable improvements in video generation performance. Despite DPOs success, its application to personalized video generation remains challenging due to the tasks unique nature. There is an urgent need to apply DPO to address identity inconsistency and dynamic loss. models within variety of general metrics. 2. Related Works 3. Method 3.1. Preliminary Text-to-Image Customization. In the realm of Text-toImage (T2I) generation, numerous methodologies [7, 8, 21, 22, 24, 26, 28, 33, 39] have surfaced for identity (ID) customization. As foundational work, Textual Inversion [7] encodes the user-provided identity into specific token embedding within frozen T2I model. To enhance ID fidelity, DreamBooth [30] refines the original model, with efficient fine-tuning techniques like LoRA [12] also being applicable. Conversely, encoder-based methods strive to directly infuse ID into the generation process. PhotoMaker [24] proposes to enhance the ID embedding based on large-scale datasets comprising diverse images of each ID. PuLID [10] proposes optimizing an ID loss between the generated and reference images in more precise configuration. Text-to-Video Customization. Text-to-Video (T2V) customization introduces additional complexities compared to Text-to-Image (T2I) customization, primarily due to the temporal motion dynamics inherent in videos. Presently, only limited number of studies [11, 23, 27, 37, 38, 42] have conducted preliminary explorations in this domain. MagicMe [27] employs an identity (ID) module based on an extended Textual Inversion approach. Nonetheless, training under self-reconstruction on the reference images diverges the videos in the inference, resulting in suboptimal ID fidelity and model degradation. ID-Animator [11] and ConsisID [42] suggests encoding ID-relevant information with face adapter, which demands thousands of high-quality human videos for fine-tuning, thereby imposing substantial costs related to dataset construction and model training. Direct Preference Optimization. Direct Preference OpDirect Preference Optimization implicitly optimizes policy based on human preference data while constraining deviation from given reference policy πref via KullbackLeibler divergence [19]. Although DPO does not explicitly maximize predefined reward function r(x, y), its intrinsic optimization objective can still be represented as: max π ExX, yπ(x) [r(x, y)] β DKL [π(yx) πref(yx)] , (1) where r(x, y) is the implicit reward derived from preference data and β controls the divergence from πref. In practical applications, DPO explicitly learns the relative preference between preferred output yw and less-preferred output yl by employing cross-entropy loss function derived from the Bradley-Terry model: LDP = E(x,yw,yl)D ln σ β log (cid:34) (cid:32) β log π(ywx) πref(ywx) (cid:33)(cid:35) . π(ylx) πref(ylx) (2) 3.2. Preference Data Generation In this work, we aim to directly optimize the model to generate identity-consistent and dynamic-preserved videos with the preference video data. However, the key difference between our customization task and other generation tasks lies in the fact that we can only obtain prior information from few reference images provided by the user. As result, we Figure 3. Overview of pairwise preference video data construction. In Step 1, we construct preference video repository using videos generated by fine-tuned and Initial T2V models, along with static videos derived from reference images. In Step 2, we evaluate each video sequentially based on ID consistency using ID Encoder [6], dynamic degree using optical flow [13], and prompt following using VLM[2]. In Step 3, we perform Hybrid Pair Selection, first selecting pairs based on ID consistency differences with pre-defined dynamic threshold to address identity inconsistency, then selecting pairs based on both dynamic and identity to mitigate the dynamic reduction. lack high-quality training data with good identity consistency, dynamic degree, and prompt following, which do not meet the traditional data requirements for preference optimization, posing significant challenge to the fine-tuning process. To overcome this issue, we propose to construct the base dataset repository through three different approaches. First, we sample several prompts from Large Language Model and feed them into the video generator πref with the initial LoRA fine-tuning to generate video data Vt. On the other hand, we extend the preference data with videos Vs generated by the original model without LoRA finetuning. These videos represent the original distribution of the T2V model, which remains unaffected by the customization process. However, the simplistic utilization of generated data will constrain the models ability to assimilate the identity in the invisible references. Therefore, we combine the generated videos with the static videos Vid inflated from reference images provided by the user, forming an base video dataset repository = Vt Vs Vid. 3.3. Customized Video Reward According to the goal of our customization, we propose to evaluate the videos in the base dataset repository with three customized rewards: identity consistency, dynamic degree, and prompt following, with all scores ranging from 1 to 10. First, we use the pretrained ID Encoder [6] to assess the mean identity consistency Rid of the videos with the reference images, which indicates the ability of the video to maintain the identity. Secondly, we use the RAFT model [32] to analyze optical flow and calculate the motion intensity Rdy between consecutive frames, which scores the dynamic degree of the videos, reflecting the level of dynamism in the video. Finally, we use VLM [2] to evaluate prompt following reward Rsem, which further assess the original dynamics of the video in terms of the semantic alignment with the given prompts. 3.4. Hybrid Pair Selection To address the issues of identity degradation and dynamic reduction, we aim to construct the video pairs for preference optimization, which is essential to consider both identity and dynamics. To this end, we introduce hybrid sampling strategy to construct identity-preferred and dynamicpreferred training pairs for two distinct stages, which is illustrated in Fig. 3. In the first stage, we exclusively utilize Vid and Vs from the video libraries, selecting video pairs Pid with significant differences in identity consistency scores while permitting certain degree of tolerance for dynamic reward. Clearly, the selected video pairs in this stage will guide the model toward maintaining high level of identity consistency. In the second stage, we select dynamic-preferred video pairs Pdy from Vs and Vt based on Frontier-based sampling method. Specifically, we first find the upper and lower Pareto frontier set using non-dominated sorting algorithm [5]. For all videos from Vs and Vt, we define dominates if and only if R(a) > R(b) for {Rid, Rdy, Rsem}. Then we can find the non-dominated set whose elements are not be dominated by any other videos. Conversely, we can also identify the dominated set and then construct the preference pairs from them. After that, we rank all video pairs from Pdy based on the differences in identity consistency scores and retain only the top 100 pairs as our preference data. By combining these video pairs with Pid, we construct = Pdy Pid, forming training pairs that prioritize both identity consistency and dynamic effects, effectively addressing the two primary limitations of personalized customization tasks. 3.5. Hybrid Preference Optimization Based on the construction pipeline above, our pairwise pref0 , vl erence video data can be represented as = {(c, vw 0)} 0 , vl where each sample contains and pair of videos (vw 0) filtered by Sec. 3.4 and their text prompts c, with vw 0 vl 0 indicating that vw 0 better aligns with human preferences. Now, our goal is to train new model pθ. To measure the quality of the complete generation path, we define reward function R(c, v0:T ) and further derive the expected reward r(c, v0) given and v0: r(c, v0) = Epθ(v1:T v0,c)[R(c, v0:T )]. (3) Following previous work [15, 25], we substitute the upper bound of the KL divergence [19] and the reward function r(c, v0) into Eq. (1), yielding: max pθ EcDc,v0:T pθ(v0:T c)[r(c, v0)] βDKL (cid:104) pθ(v0:T c) pref(v0:T c) (cid:105) . (4) To optimize this objective, the conditional distribution pθ(x0:T ) is directly used. Similar to Eq. (2), the final loss function can be defined. By applying Jensens inequality [14], the final upper-bound loss function LHPO(θ) is expressed as: LHPO(θ) (vw 0 ,vl 0)D 1:T pθ(v1:T vw vw log σ (cid:0)β (cid:0)(vw 0 ), vl 0:T ) (vl 1:T pθ(v1:T vl 0) 0:T )(cid:1)(cid:1) , (5) where (v0:T ) is defined as: (v0:T ) = log pθ(v0:T ) pref(v0:T ) . (6) Due to the complexity of calculating high-dimensional video sequence probabilities in video generation tasks, we introduce an approximate posterior distribution q(v1:T v0) to estimate pθ(v0:T ). more detailed derivation can be found in the supplementary material. Combining this with the evidence lower bound (ELBO) method [16], we can transform the probability distribution of video generation into KL divergence representation: (v0:T ) = Dθ KL + Dref KL + C. (7) Furthermore, by relating KL divergence to noise prediction, we can rewrite it as follows: Dθ KL ϵ ϵθ(vt, t)2 Dref KL ϵ ϵref(vt, t)2. (8) Finally, the complete HPO loss function can be mathematically represented as follows: LHPO(θ) = (cid:34) (cid:32) β log σ (vw 0)D, t{1..T } 0 ,vl (cid:0)ϵw ϵθ(vw , t)2 ϵw ϵref(vw , t)2(cid:1) (cid:0)ϵl ϵθ(vl t, t)2 ϵl ϵref(vl t, t)2(cid:1) (cid:33)(cid:35) . (9) 4. Experiments 4.1. Experimental Setup Implementation details. We utilize the recently developed Text-to-Video DiT model, HunyuanVideo [18], as our foundational model. For the training process, we employ the AdamW optimizer configured with learning rate of 2e-5 and weight decay parameter of 1e-4. We first fine-tune the model for 1000 steps in the initial training stage. Then we use our training method to optimize for 5000 steps, which maintains parity with the total number of training steps used in the baseline method, ensuring fair comparison. Please refer to Appendix for more details. Baselines. Our comparative analysis benchmarks the proposed method against MagicMe [27], recent identityspecific T2V customization method, as well as DreamBooth with LoRA [12]. For fair comparison, we apply both DreamBooth and MagicMe on the HunyuanVideo model. Additionally, we compare the results with encoder-based methods like IDAnimator [11] and ConsisID [42], both trained on meticulously collected large-scale video datasets. Evaluation. Our evaluation dataset consists of 40 characters ensuring demographic representation, with 40 actionspecific prompts for comprehensive motion evaluation. The evaluation framework employs the standardized VBench benchmark [13] for quantitative assessment of dynamic degree and text alignment. To measure identity preservation, we implement facial recognition embedding similarity metrics [1] complemented by specialized facial motion analysis Figure 4. Qualitative comparison with tuning-based methods. As observed, both Dreambooth and MagicMe suffer from inferior ID fidelity, while our method maintains consistent identity and natural dynamics. protocols. Additionally, we calculate other well-established metrics for comprehensive video evaluation including Temporal Consistency, Image CLIP Score, and FVD. Unless otherwise specified, we generate 61-frame videos during the inference phase. 4.2. Main Results Qualitative results. We present qualitative assessment comparing MagicID with baseline methods. As illustrated in Fig. 4, both DreamBooth and MagicMe exhibit suboptimal identity fidelity, primarily due to their self-reconstruction training approach on reference images, which creates significant discrepancy between the training 1-frame image and the multi-frame videos generated during inference. Conversely, our MagicID achieves superior ID fidelity while preserving the integrity of motion dynamics. To further validate the robustness of our methodology, we also train using single reference image for fair comparison with encoder-based methods. As shown in Fig. 5, IDAnimator struggles with maintaining identity consistency and delivering high-quality video outputs. Although ConsisID enhances identity fidelity to certain degree, it introduces noticeable copy-paste effects, resulting in unnatural motion dynamics and suboptimal text alignment, as illustrated in the final example featuring the helmet. In contrast, our results further highlight the advantages of MagicID, showcasing its robustness in achieving high ID fidelity while maintaining promising motion dynamics. Quantitative results. We present the quantitative results in Tab. 1. As observed, DreamBooth exhibits inferior face similarity due to its self-reconstruction training approach. MagicMe achieves better face similarity but at the cost of degraded motion dynamics and prompt adherence. Leveraging extensive video datasets, ID-Animator and ConsisID attain superior facial similarity and motion dynamics, albeit with diminished prompt adherence and temporal conFigure 5. Qualitative comparison with tuning-based methods. As shown, ID-Animator suffers from poor identity consistency and video quality. While ConsisID improves identity fidelity to some extent, it exhibits severe copy-paste artifacts, demonstrating unnatural motion dynamics and text alignment, as seen in the last example with the helmet. In contrast, our method achieves strong performance in identity consistency, motion dynamics, and text alignment, significantly outperforming the baseline approaches. sistency. In comparison, our MagicID method outperforms prior approaches, particularly in facial similarity and dynamic quality. It achieves exceptional identity fidelity while effectively preserving the original text-to-video models capabilities, aligning with the qualitative observations. User study. To further evaluate the effectiveness of our methodology, we conduct human-centric assessment, comparing our approach with existing text-to-video identity customization techniques. We recruit 25 evaluators to assess 40 sets of generated video results. For each set, we present reference images alongside videos produced using identical seeds and textual prompts across various methods. The quality of the generated videos is evaluated based on four criteria: ID Consistency (the degree of resemblance between the generated subject and the reference image), Dynamic Degree (the extent of motion dynamics within the video), Text Alignment (the fidelity of the video to the textual prompt), and Overall Quality (the general user satisfaction with the video quality). As depicted in Fig. 8, our MagicID achieves higher user preference across all evaluative dimensions, underscoring its superior effectiveness. 4.3. Ablation Study Effects of hybrid pair selection. To validate the impact of the proposed strategy for preference pair selection, we conduct detailed ablation study in Fig. 6 and Tab. 2. In contrast to the self-reconstruction method, utilizing identitypreferred pairs during preference optimization significantly enhances identity consistency in the results. Additionally, integrating dynamic-preferred pairs notably improves the motion dynamics of the generated outputs, demonstrating the effectiveness of our hybrid pair selection. Method Face Sim. () Dyna. Deg. () T. Cons. () CLIP-T () CLIP-I () FVD () DreamBooth [30] MagicMe [27] IDAnimator [11] ConsisID [42] MagicID 0.276 0.322 0.433 0.482 0.600 5.690 5.332 10.33 9.26 14.42 0.9922 0.9924 0.9938 0.9811 0.9933 25.83 25.42 25.21 26.12 26. 46.55 62.45 49.33 63.88 78.83 1423.55 1438.66 1558.33 1633.21 1228.33 Table 1. Quantitative comparison. We conduct comprehensive comparison including the ability to achieve high ID fidelity (i.e., Face Similarity and CLIP-I), dynamic degree, temporal consistency, text alignment (i.e., CLIP-T), and distribution distance (i.e., FVD). Figure 6. Ablation study for the hybrid pair selection. Compared to the self-reconstruction approach, training with identitypreferred pairs significantly enhances the identity consistency. On the other hand, the incorporation of dynamic-preferred pairs markedly improves the dynamics of the generated outcomes. Figure 7. Ablation study for the customized video rewards. While the ID reward encourages the model to learn consistent identity features, the addition of the dynamic reward leads to generated results with significantly improved motion dynamics. Furthermore, incorporating semantic reward can effectively enhance video dynamics to some extent and improve prompt-following capabilities, such as the prompts involving turning motions. Effects of customized video reward. To verify the effects of our customized video reward, we conduct experiments to compare the results of different rewards. As shown in Fig. 7 and Tab. 3, while the ID reward encourages the model to acquire more consistent identity characteristics, it somewhat neglects the dynamic quality of the video. In comparison, the inclusion of the dynamic reward results in generated outcomes with enhanced motion dynamics. Additionally, integrating semantic reward can augment video dynamics Figure 8. User Study. ID pairs Dynamic pairs Face () Dynamic () CLIP-T () 0.276 0.605 0.600 5.690 7.382 14. 25.83 25.94 26.28 Table 2. Quantitative ablation study of hybrid pair selection. ID Dynamic Semantic Face () Dynamic () CLIP-T () 0.598 0.607 0. 6.332 12.33 14.42 24.92 25.73 26.28 Table 3. Quantitative ablation study of customized video reward. and also improves prompt-following abilities. 5. Conclusion & Limitation In this paper, we present MagicID, novel framework designed to address the significant challenges in identityspecific video generation. By introducing hybrid customized preference optimization, our method effectively maintains consistent identity fidelity and preserves natural motion dynamics, overcoming the limitations of traditional self-reconstruction techniques. Through two-stage hybrid sampling strategy, we construct identity-preferred and dynamic-preferred training pairs, ensuring robust identity learning and enhanced video dynamics. Our framework demonstrates superior performance in generating high-quality personalized videos, which holds substantial promise for applications in the film and television industries, paving the way for more realistic and engaging content creation. Future work will explore further refinements and broader applications of our approach in diverse multimedia domains. However, our method also has some limitations. For example, it focuses on single-person consistent identity video generation, but fails to generate customized videos that contain multiple identities. One possible solution is to introduce reward mechanism tailored for multi-person video generation, which will be explored in our future work."
        },
        {
            "title": "References",
            "content": "[1] Xiang An, Xuhan Zhu, Yuan Gao, Yang Xiao, Yongle Zhao, Ziyong Feng, Lan Wu, Bin Qin, Ming Zhang, Debing Zhang, et al. Partial fc: Training 10 million identities on single machine. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14451449, 2021. 5 [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond, 2023. 4 [3] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. 2 [4] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In CVPR, pages 73107320, 2024. 2 [5] Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMT Meyarivan. fast and elitist multiobjective genetic algorithm: Nsga-ii. IEEE transactions on evolutionary computation, 6(2):182197, 2002. 5 [6] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep In Proceedings of the IEEE/CVF conface recognition. ference on computer vision and pattern recognition, pages 46904699, 2019. 4 [7] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 2, 3 [8] Rinon Gal, Or Lichter, Elad Richardson, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. Lcmlookahead for encoder-based text-to-image personalization. arXiv preprint arXiv:2404.03620, 2024. [9] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 2 [10] Zinan Guo, Yanze Wu, Zhuowei Chen, Lang Chen, and Qian He. Pulid: Pure and lightning id customization via contrastive alignment. arXiv preprint arXiv:2404.16022, 2024. 2, 3 [11] Xuanhua He, Quande Liu, Shengju Qian, Xin Wang, Tao Hu, Ke Cao, Keyu Yan, Man Zhou, and Jie Zhang. Id-animator: Zero-shot identity-preserving human video generation. arXiv preprint arXiv:2404.15275, 2024. 2, 3, 5, 8 [12] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 3, 5 [13] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. 4, [14] Johan Ludwig William Valdemar Jensen. Sur les fonctions convexes et les inegalites entre les valeurs moyennes. Acta mathematica, 30(1):175193, 1906. 5 [15] Lifan Jiang, Boxi Wu, Jiahui Zhang, Xiaotong Guan, and Shuang Chen. Huvidpo: Enhancing video generation through direct preference optimization for human-centric alignment. arXiv preprint arXiv:2502.01690, 2025. 2, 3, 5 [16] Michael Jordan, Zoubin Ghahramani, Tommi Jaakkola, and Lawrence Saul. An introduction to variational methods for graphical models. Machine learning, 37:183233, 1999. 5 [17] Dahyun Kim, Yungi Kim, Wonho Song, Hyeonwoo Kim, Yunsu Kim, Sanghoon Kim, and Chanjun Park. sdpo: Dont use your data all at once. arXiv preprint arXiv:2403.19270, 2024. 2, 3 [18] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 2, 5 [19] Solomon Kullback and Richard Leibler. On information and sufficiency. The annals of mathematical statistics, 22(1): 7986, 1951. 3, [20] Kyungmin Lee, Xiaohang Li, Qifei Wang, Junfeng He, Junjie Ke, Ming-Hsuan Yang, Irfan Essa, Jinwoo Shin, Feng Yang, and Yinxiao Li. Calibrated multi-preference oparXiv preprint timization for aligning diffusion models. arXiv:2502.02588, 2025. 3 [21] Hengjia Li, Yang Liu, Linxuan Xia, Yuqi Lin, Wenxiao Wang, Tu Zheng, Zheng Yang, Xiaohui Zhong, Xiaobo Ren, and Xiaofei He. Few-shot hybrid domain adaptation of image generator. In The Twelfth International Conference on Learning Representations, 2023. 3 [22] Hengjia Li, Yang Liu, Yuqi Lin, Zhanwei Zhang, Yibo Zhao, Tu Zheng, Zheng Yang, Yuchun Jiang, Boxi Wu, Deng Cai, et al. Unihda: Towards universal hybrid domain adaptation of image generators. arXiv preprint arXiv:2401.12596, 2024. 3 [23] Hengjia Li, Haonan Qiu, Shiwei Zhang, Xiang Wang, Yujie Wei, Zekun Li, Yingya Zhang, Boxi Wu, and Deng Cai. Personalvideo: High id-fidelity video customization without dynamic and semantic degradation. arXiv preprint arXiv:2411.17048, 2024. 2, 3 [24] Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, MingMing Cheng, and Ying Shan. Photomaker: Customizing realistic human photos via stacked id embedding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 86408650, 2024. 2, 3 [25] Runtao Liu, Haoyu Wu, Zheng Ziqiang, Chen Wei, Yingqing He, Renjie Pi, and Qifeng Chen. Videodpo: Omnipreference alignment for video diffusion generation. arXiv preprint arXiv:2412.14167, 2024. 2, 3, [26] Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu. Subjectdiffusion: Open domain personalized text-to-image genera- [39] Guangxuan Xiao, Tianwei Yin, William Freeman, Fredo Durand, and Song Han. Fastcomposer: Tuning-free multisubject image generation with localized attention. International Journal of Computer Vision, pages 120, 2024. 3 [40] Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Is dpo Zhiyu Mei, Guangju Wang, Chao Yu, and Yi Wu. superior to ppo for llm alignment? comprehensive study. arXiv preprint arXiv:2404.10719, 2024. 2, 3 [41] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 2 [42] Shenghai Yuan, Jinfa Huang, Xianyi He, Yunyuan Ge, Yujun Shi, Liuhan Chen, Jiebo Luo, and Li Yuan. Identitypreserving text-to-video generation by frequency decomposition. arXiv preprint arXiv:2411.17440, 2024. 2, 3, 5, 8 [43] Zhenglin Zhou, Xiaobo Xia, Fan Ma, Hehe Fan, Yi Yang, and Tat-Seng Chua. Dreamdpo: Aligning text-to-3d generation with human preferences via direct preference optimization. arXiv preprint arXiv:2502.04370, 2025. 3 [44] Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences, 2020. URL https://arxiv. org/abs, page 14, 1909. 3 tion without test-time fine-tuning. In ACM SIGGRAPH 2024 Conference Papers, pages 112, 2024. [27] Ze Ma, Daquan Zhou, Chun-Hsiao Yeh, Xue-She Wang, Xiuyu Li, Huanrui Yang, Zhen Dong, Kurt Keutzer, and Jiashi Feng. Magic-me: Identity-specific video customized diffusion. arXiv preprint arXiv:2402.09368, 2024. 2, 3, 5, 8 [28] Xu Peng, Junwei Zhu, Boyuan Jiang, Ying Tai, Donghao Luo, Jiangning Zhang, Wei Lin, Taisong Jin, Chengjie Wang, and Rongrong Ji. Portraitbooth: versatile portrait model for fast identity-preserved personalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2708027090, 2024. 3 [29] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. 3 [30] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR, pages 2250022510, 2023. 2, 3, 8 [31] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize with human feedback. Advances in neural information processing systems, 33:30083021, 2020. 3 [32] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field In Computer VisionECCV transforms for optical flow. 2020: 16th European Conference, Glasgow, UK, August 23 28, 2020, Proceedings, Part II 16, pages 402419. Springer, 2020. 4 [33] Dani Valevski, Danny Lumen, Yossi Matias, and Yaniv Leviathan. Face0: Instantaneously conditioning text-toimage model on face. In SIGGRAPH Asia 2023 Conference Papers, pages 110, 2023. 3 [34] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82288238, 2024. [35] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. 2 [36] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, and Anthony Chen. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024. 2 [37] Yujie Wei, Shiwei Zhang, Hangjie Yuan, Xiang Wang, Haonan Qiu, Rui Zhao, Yutong Feng, Feng Liu, Zhizhong Huang, Jiaxin Ye, et al. Dreamvideo-2: Zero-shot subjectdriven video customization with precise motion control. arXiv preprint arXiv:2410.13830, 2024. 2, 3 [38] Tao Wu, Yong Zhang, Xintao Wang, Xianpan Zhou, Guangcong Zheng, Zhongang Qi, Ying Shan, and Xi Li. Customcrafter: Customized video generation with preserving motion and concept composition abilities. arXiv preprint arXiv:2408.13239, 2024. 2, 3 MagicID: Hybrid Preference Optimization for ID-Consistent and Dynamic-Preserved Video Customization A. Implement Details"
        },
        {
            "title": "Supplementary Material",
            "content": "To construct the preference video repository, we generate 20 prompts using LLM model. Then we sample 100 and 20 videos with fine-tuned T2V model and the initial T2V model respectively, which are incorporated with the static videos inflated from the reference images. After the pair selection, we sample the top 100 pairs as our preference data. In the training stage, we employ the AdamW optimizer configured with learning rate of 2e-5 and weight decay parameter of 1e-4. We first fine-tune the model for 1000 steps in the initial training stage. Then we use our training method to optimize for 5000 steps. During inference, we use 50 steps of DDIM sampler and classifier-free guidance with scale of 7.5. We generate 61-frame videos with 720 1280 spatial resolution. All experiments are conducted on single NVIDIA H00 GPU. B. Baseline Details We compare our method with both optimization methods, such as Magic-Me and Dreambooth with LoRA, and encoderbased methods such as IDAnimator and ConsisID. Specifically, Magic-Me is recent T2V customization method that trains extended keywords and injects it into HunyuanVideo. Besides, we compare with Dreambooth-LoRA, which uses traditional reconstructive loss during training. For fair comparison, we train them for the same total steps with our method. C. DPO Objective Function This section provides systematic derivation of the Direct Preference Optimization (DPO) formula, detailing the derivation of the DPO objective function and constructing preference-driven optimization framework based on the reward function and KL divergence. The goal of DPO is to maximize rewards while aligning the policy with baseline model. The objective function is defined as: max π ExX,yπ [r(x, y)] β DKL [π(yx)πref(yx)] , (10) where DKL denotes the Kullback-Leibler divergence between the learned policy π and reference policy πref, enforcing consistency with the baseline model. To simplify, this objective is reformulated as minimization problem: where Z(x) is defined as: ExX,yπ min π (cid:20) log π(yx) π(yx) (cid:21) log Z(x) , Z(x) = (cid:88) πref(yx) exp (cid:19) r(x, y) . (cid:18) 1 β This reformulation leads to the final optimization objective: ExD [DKL(π(yx)π(yx))] . min π Under the minimization of KL divergence, the policy π(yx) adheres to the following form: π(yx) = π(yx) = 1 Z(x) πref(yx) exp (cid:19) r(x, y) . (cid:18) 1 β Reversing this equation yields the reward function: r(x, y) = β log π(y x) πref(y x) . (11) (12) (13) (14) (15) Incorporating the Bradley-Terry model, the cross-entropy loss function is defined, which quantifies the difference between the preferred and non-preferred responses. This loss function is essential for deriving the gradient necessary to optimize the DPO objective: = E(x,yw,yl)D (cid:20) (cid:18) ln σ β log π(ywx) πref(ywx) β log π(ylx) πref(ylx) (cid:19)(cid:21) , (16) where σ denotes the sigmoid function, which maps the difference in log-probabilities to range of [0, 1]. Differentiating provides the gradient needed to optimize the DPO objective with respect to the preference data. D. Applying DPO Strategy into our MagicID 0 , vl In adapting Dynamic Preference Optimization (DPO) to our MagicID task, we consider pairwise preference video data = {(c, vw 0)}. In this dataset, each example contains their text prompts and pair of videos (vw 0) generated by reference model pref, where vw 0. The goal of DPO is to train new model pθ so that its generated videos align with human preferences rather than merely imitating the reference model. However, directly computing the distribution pθ(v0c) is highly complex, as it requires marginalizing over all possible generation paths (v1, . . . , vT ) to produce v0, which is practically infeasible. 0 indicates that humans prefer vw 0 over vl 0 vl 0 , vl To address this challenge, researchers leverage Evidence Lower Bound (ELBO) by introducing latent variables v1:T . The reward function R(c, v0:T ) is defined to measure the quality of the entire generation path, allowing the expected reward r(c, v0) for given and v0 to be formulated as: In DPO, KL regularization term is also included to constrain the generated distribution relative to the reference distribution. Here, an upper bound on the KL divergence is used, converting it to joint KL divergence: r(c, v0) = Epθ(v1:T v0,c)[R(c, v0:T )] (17) DKL[pθ(v0:T c) pref(v0:T c)] (18) This upper bound ensures that the distribution of the generated model pθ(v0:T c) remains consistent with the reference model pref(v0:T c), preserving the models generation capabilities while optimizing human preference alignment. Plugging in this KL divergence upper bound and the reward function r(c, v0) into the objective function, we obtain: max pθ EcDc,v0:T pθ(v0:T c)[r(c, v0)] βDKL[pθ(v0:T c) pref(v0:T c)] (19) The definition of this objective function is optimized over the path v0:T . Its primary goal is to maximize the reward for the reverse process pθ(v0:T ) while maintaining distributional consistency with the original reference reverse process. To optimize this objective, the conditional distribution pθ(v0:T ) is directly used. The final DPO-MagicID loss function LHPO(θ) is expressed as follows: LHPO(θ) = (vw 0 ,vl 0)P log σ (cid:18) βE 1:T pθ(v1:T vw vw 0 ), vl 1:T pθ(v1:T vl 0) (cid:20) log pθ(vw pref(vw 0:T ) 0:T ) log (cid:21)(cid:19) pθ(vl pref(vl 0:T ) 0:T ) (20) By applying Jensens inequality, the expectation can be moved outside of the log σ function, resulting in an upper bound. This simplifies the formula and facilitates optimization. After applying Jensens inequality, the upper bound of the loss function is given by: LHPO(θ) (vw 0 ,vl 0)P vw 1:T pθ(v1:T vw 0 ), vl 1:T pθ(v1:T vl 0) log σ (cid:18) (cid:20) β log pθ(vw pref(vw 0:T ) 0:T ) log (cid:21)(cid:19) pθ(vl pref(vl 0:T ) 0:T ) (21) To handle the complexity of calculating high-dimensional video sequence probabilities with total of = 1000 time steps, we employ an approximation approach. We introduce an approximate posterior q(v1:T v0) for the subsequent time steps and utilize the Evidence Lower Bound (ELBO) to approximate log pθ(v0:T ). Then, by expressing pθ(v0:T ) and q(v1:T v0) as products of conditional probabilities at each time step, we achieve stepwise sampling approach. The final approximate expression is: log pθ(v0:T ) Eq(vtvt1),t{1..T } log (cid:34) pθ(v0) q(v0) (cid:35) + log pθ(vtvt1) q(vtvt1) . (22) Since q(vtvt1) is conditional probability distribution that generally sums to 1, the KL divergence can be expressed as: DKL (q(vtvt1) pθ(vtvt1)) = log q(vtvt1) pθ(vtvt1) . Based on Eqs. (22) and (23), we rewrite log pθ(v0:T ) as: log pθ(v0:T ) Eq(v1:T v0),t{1..T } (cid:20) log (cid:21) pθ(v0) q(v0) DKL (q(vtvt1) pθ(vtvt1)) . (23) (24) Moreover, the derivation of log pref(v0:T ) is consistent with that of log pθ(v0:T ). Based Eq. (24), we can rewrite (v0:T ) as: log pθ(v0:T ) pref(v0:T ) = Dθ KL + Dref KL + C. By rewriting the KL divergence in terms of noise prediction, we can express it as follows: Dθ KL ϵ ϵθ(vt, t)2 Dref KL ϵ ϵref(vt, t)2. Finally, based on Eqs. (8), (21) and (25), the complete form of the DPO loss function for our MagicID is: LHPO(θ) = (cid:34) (cid:32) β log σ (vw 0)D, t{1..T } 0 ,vl (cid:0)ϵw ϵθ(vw , t)2 ϵw ϵref(vw , t)2(cid:1) (cid:0)ϵl ϵθ(vl t, t)2 ϵl ϵref(vl t, t)2(cid:1) (cid:33)(cid:35) . E. More Results As shown in Fig. 9, Fig. 10, Fig. 11, Fig. 12, and Fig. 13, we present more customization results of MagicID. They showcase it achieves consistent identity and preserves natural motion dynamics, which provides further evidence of its promising performance. F. Reproducibility Statement We make the following efforts to ensure the reproducibility of MagicID: (1) Our training and inference codes together with the trained model weights will be publicly available. (2) We provide training details in the appendix, which is easy to follow. (3) We provide the details of the human evaluation setups. G. Impact Statement Our main objective in this work is to empower novice users to generate visual content creatively and flexibly. However, we acknowledge the potential for misuse in creating fake or harmful content with our method. Thus, we believe its essential to develop and implement tools to detect biases and malicious use cases to promote safe and equitable usage. (25) (26) (27) Figure 9. More results of MagicID. Figure 10. More results of MagicID. Figure 11. More results of MagicID. Figure 12. More results of MagicID. Figure 13. More results of MagicID."
        }
    ],
    "affiliations": [
        "Hedra AI",
        "University of Alabama at Birmingham",
        "Zhejiang University"
    ]
}