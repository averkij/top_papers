{
    "paper_title": "Randomized Autoregressive Visual Generation",
    "authors": [
        "Qihang Yu",
        "Ju He",
        "Xueqing Deng",
        "Xiaohui Shen",
        "Liang-Chieh Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper presents Randomized AutoRegressive modeling (RAR) for visual generation, which sets a new state-of-the-art performance on the image generation task while maintaining full compatibility with language modeling frameworks. The proposed RAR is simple: during a standard autoregressive training process with a next-token prediction objective, the input sequence-typically ordered in raster form-is randomly permuted into different factorization orders with a probability r, where r starts at 1 and linearly decays to 0 over the course of training. This annealing training strategy enables the model to learn to maximize the expected likelihood over all factorization orders and thus effectively improve the model's capability of modeling bidirectional contexts. Importantly, RAR preserves the integrity of the autoregressive modeling framework, ensuring full compatibility with language modeling while significantly improving performance in image generation. On the ImageNet-256 benchmark, RAR achieves an FID score of 1.48, not only surpassing prior state-of-the-art autoregressive image generators but also outperforming leading diffusion-based and masked transformer-based methods. Code and models will be made available at https://github.com/bytedance/1d-tokenizer"
        },
        {
            "title": "Start",
            "content": "Liang-Chieh Chen ByteDance https://yucornetto.github.io/projects/rar.html 4 2 0 2 1 ] . [ 1 6 7 7 0 0 . 1 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "This paper presents Randomized AutoRegressive modeling (RAR) for visual generation, which sets new state-of-theart performance on the image generation task while maintaining full compatibility with language modeling frameworks. The proposed RAR is simple: during standard autoregressive training process with next-token prediction objective, the input sequencetypically ordered in raster formis randomly permuted into different factorization orders with probability r, where starts at 1 and linearly decays to 0 over the course of training. This annealing training strategy enables the model to learn to maximize the expected likelihood over all factorization orders and thus effectively improve the models capability of modeling bidirectional contexts. Importantly, RAR preserves the integrity of the autoregressive modeling framework, ensuring full compatibility with language modeling while significantly improving performance in image generation. On the ImageNet-256 benchmark, RAR achieves an FID score of 1.48, not only surpassing prior state-of-the-art autoregressive image generators but also outperforming leading diffusion-based and masked transformer-based methods. Code and models will be made available at https: //github.com/bytedance/1d-tokenizer. 1. Introduction AutoRegressive (AR) models have driven remarkable advancements across both natural language processing and computer vision tasks in recent years. In language modeling, they serve as the fundamental framework for Large Language Models (LLMs) such as GPT [43], Llama [59, 60], and Gemini [57], along with other state-of-the-art models [1, 67]. In the realm of computer vision, autoregressive models1 have also shown substantial potential, delivering 1While MaskGIT-style models [10] could be classified as generalized autoregressive models as defined in [36], in this paper, we primarily use the term autoregressive to refer to GPT-style models [22, 52, 69], which are characterized by causal attention, next-token prediction, and operate without the need for mask tokens as placeholders. Figure 1. Comparison among different language modeling compatible autoregressive (AR) image generators. The proposed RAR demonstrates significant improvements over previous AR methods. RAR-B, with only 261M parameters, achieves an FID score of 1.95, outperforming both LlamaGen-XXL (1.4B parameters) and Open-MAGVIT2-XL (1.5B parameters). competitive performance in image generation tasks [22, 35, 39, 52, 69, 70] to diffusion models [6, 18, 36, 45] or nonautoregressive transformers [10, 65, 7173]. More importantly, autoregressive modeling is emerging as promising pathway toward unified models across multiple modalities and tasks [5, 9, 14, 55, 56, 66]. Despite the dominance of autoregressive models in language modeling, they often yield suboptimal performance in comparison to diffusion models or non-autoregressive transformers in visual generation tasks [39, 52]. This discrepancy can be attributed to the inherent differences between text and visual signals. Text is highly compact and semantically meaningful, while visual data tends to be more low-level and redundant [29, 73], making bidirectional context modeling more critical. For instance, several studies [7, 21, 36] have demonstrated that causal attention applied to image tokens leads to inferior performance compared to bidirectional attention in vision tasks. To address this, recent works [36, 58] have attempted to reintroduce bidirectional attention by redesigning the autoregressive formulation, achieving state-of-the-art re1 Figure 2. Overview of the proposed Randomized AutoRegressive (RAR) model, which is fully compatible with language modeling frameworks. Left: RAR introduces randomness annealing training strategy to enhance the models ability to learn bidirectional contexts. During training, the input sequence is randomly permuted with probability r, which starts at 1 (fully random permutations) and linearly decreases to 0, transitioning the model to fixed scan order, such as raster scan, by the end of training. Right: Randomly selected images generated by RAR, trained on ImageNet. sults in image generation. However, these approaches often deviate from the traditional autoregressive paradigm. For example, VAR [58] shifts from next-token prediction to next-scale prediction, enabling bidirectional attention within each scale, and MAR [36] generalizes MaskGITstyle framework [10] to the autoregressive definition, which naturally introduces back the bidirectional attention. While effective, these modifications complicate their integration into universal transformer architectures that aim to unify different modalities, which proves to work well with conventional autoregressive modeling [55, 56]. In this paper, we aim to enhance the generation quality of autoregressive image models while preserving the core autoregressive structure, maintaining compatibility with language modeling frameworks. Specifically, we enable bidirectional context learning within an autoregressive transformer by maximizing the expected likelihood over all possible factorization order. In this way, all tokens will be trained and predicted under all possible contexts, facilitating learning bidirectional representation. Moreover, we introduce permutation probability r, which controls the ratio of training data between random factorization order and the standard raster order. Initially, is set to 1 (fully random factorization) and it linearly decays to 0 over the course of training, gradually reverting the model to the raster order commonly used by other autoregressive image generators. To this end, we present simple, effective, and scalable autoregressive model training paradiam named Randomized AutoRegressive modeling (RAR). RAR retains the original autoregressive model architecture and formulation, ensuring full compatibility with language modeling. At the same time, it significantly improves the generation quality of autoregressive models at no additional cost. On the ImageNet-256 benchmark [16], RAR achieves an FID score of 1.48, substantially outperforming previous state-of-the-art autoregressive image generators, as illustrated in Fig. 1. By addressing the limitations of unidirectional context modeling, RAR represents critical step towards autoregressive visual generation and opens up new possibilities for further advancements in the field. 2. Related Work Autoregressive Language Modeling. The advent of autoregressive language models [14, 9, 13, 20, 43, 46, 47, 57, 59, 60, 67] has paved promising path toward generalpurpose AI systems. At the core of these models is simple yet powerful next-token prediction paradigm, where the objective is to predict the next word or token in sequence based on preceding inputs. This approach has demonstrated both scalability, as evidenced by scaling laws, and versatility through zero-shot generalization, enabling explorations beyond traditional language tasks to diverse modalities. Autoregressive Visual Modeling. Pioneering research [12, 27, 44, 62, 63] in autoregressive visual modeling has focused on representing images as sequences of pixels. Nevertheless, inspired by advancements in autoregressive language modeling, subsequent wave of studies has transitioned to modeling images as sequences of discrete-valued tokens [22, 48, 49, 64, 69], resulting in notable improvements in performance. This direction has been further explored through efforts [39, 52] aimed at enhancing tokeniza2 tion quality and leveraging modern autoregressive architectures initially developed for language tasks. However, all of these works strictly adhere to raster-scan order for processing pixels or tokens, resulting in unidirectional information flow that is sub-optimal for visual modeling. In this work, we instead explore learning across all possible factorization orders to enhance bidirectional context learning while retaining the core autoregressive framework. Other Visual Generation Models. In addition to autoregressive visual modeling, there have been numerous efforts in exploring other formats of visual generation models, including generative adversarial networks (GANs) [8, 26, 32], diffusion models [18, 23, 31, 37, 45, 50], masked transformers [10, 11, 65, 71, 73], scale-wise autoregressive modeling (VAR) [41, 54, 58, 75], and masked autoregressive modeling with diffusion loss (MAR) [24, 36]. It is worth noting that MAR [36] also experimented random order based AR framework similar to the proposed RAR. However, as indicated in our experiments (see Sec. 4.2), simply replacing the raster order with random order only brings marginal improvement, coinciding the observation in [36]. This further demonstrates the importance on the randomness annealing strategy in RAR, leading to substantial improvement for the AR image generators. 3. Method In this section, we first provide an overview of autoregressive modeling in Sec. 3.1, followed by our proposed Randomized AutoRegressive modeling (RAR) in Sec. 3.2. 3.1. Background We provide brief overview of autoregressive modeling with next-token prediction objective. Given discrete token sequence = [x1, x2, , xT ], the goal of autoregressive modeling is to maximize the likelihood of the sequence under forward autoregressive factorization. Specifically, the objective is to maximize the joint probability of predicting the current token xt based on all preceding tokens [x1, x2, , xt1], = 1, , : max θ pθ(x) = (cid:89) t=1 pθ(xtx1, x2, , xt1), (1) where pθ denotes token distribution predictor with model parameterized by θ. As shown in the equation, each token xt at position is conditioned solely on the preceding tokens, which limits context modeling to unidirectional manner. This contrasts with methods such as masked transformer [10, 65, 71, 72] and diffusion models [31, 37, 45, 50], which can leverage bidirectional context at the training time. Additionally, while natural language has an inherent sequential order (left-to-right in most languages), image data lacks clear, predefined order for processing tokens. Among the possible orders for image generation, the row-major order (i.e., raster scan) is the most widely adopted and has demonstrated superior performance compared to other alternatives [22]. 3.2. RAR: Randomized AutoRegressive Modeling Visual signals inherently exhibit bidirectional correlations, making effective global context modeling essential. However, conventional autoregressive models rely on causal attention masking, which enforces unidirectional dependency on the token sequence, contradicting the nature of visual data, as noted in prior works [7, 21, 36], where bidirectional attention works significantly better than causal attention for visual modality. Furthermore, there is no universally correct way to arrange image tokens into causal sequence. While the widely adopted raster order has achieved some success, it introduces biases in the autoregressive training process. For instance, each token is conditioned solely on the preceding tokens in the scanning order, restricting the models ability to learn dependencies from other directions. To address these challenges, we propose randomized autoregressive modeling approach that incorporates optimization objective with bidirectional context: max θ pθ(x) = t= (cid:89) pθ(xtx1, , xt1, xt+1, , xT ). (2) Unlike BERT-style [17] or MaskGIT-style [10] methods, our method follows the permuted objective approach [61, 68], where the model is trained in an autoregressive manner across all possible factorization orders. This enables the model to gather bidirectional context while preserving the autoregressive framework in expectation. Formally, we have: max θ pθ(x) = Eτ ST (cid:35) pθ(xτtxτ<t) , (3) (cid:34) (cid:89) t=1 where ST denotes the set of all possible permutations of the index sequence [1, 2, , ], and τ represents randomly sampled permutation from ST . The notation τt refers to the t-th element in the permuted sequence, and τ<t represents all preceding positions to τt. Since the model parameters θ are shared across all sampled factorization orders, each token xt is exposed to every possible context and learns relationships with every other token xi = t, during training. This allows the model to effectively capture bidirectional context while preserving the integrity of the autoregressive formulation. Although simple, this modification significantly improves image generation performance, highlighting the power of bidirectional context in improving autoregressive 3 Figure 3. Illustration of the target-aware positional embedding. Subfigure (a) shows the training process of the proposed Randomized AutoRegressive (RAR) model, along with the target-aware position embedding. Following Vision Transformer [19], images are tokenized into patches with original position embeddings (blue tokens). The token sequence is then randomly permuted, with the target-aware positional embeddings (green tokens) added to guide the model. Subfigures (b) and (c) highlight the importance of the target-aware positional embedding: (b) demonstrates failure case where both permuted sequences yield identical prediction logits, while (c) shows that the target-aware positional embedding correctly guides the model to predict the next token accurately. image generator capability. Our findings align with those observed in autoregressive training for language modeling in NLP [9, 17, 61, 68] as well. Discussion. While the permutation objective allows for bidirectional context learning within the autoregressive framework in expectation, it remains challenging to fully capture global context during the generation process. This is because there are always some tokens generated before others, without having access to the full global context. This limitation is not unique to autoregressive methods [22, 52] but also present in non-autoregressive models [10]. Techniques such as resampling or refinement [28, 42] may help address this issue by ensuring that every token is generated with sufficient context. However, such designs may complicate the system; thus, exploring such solutions lies beyond the scope of this paper and is left for future work. Target-aware Positional Embedding. One limitation of the permuted training objective is that standard positional embeddings may fail in certain scenarios. For instance, consider two different permutations: τa = [1, 2, , 2, 1, ] and τb = [1, 2, , 2, T, 1] (i.e., only the last two tokens positions are swapped). When predicting the second to last token, both permutations will yield identical features and thus identical prediction logits, even though they correspond to different ground-truth labels (i.e., pθ(xτT 1 xτ1 , xτ2 , , xτT 2) is the same for both permutations τa and τb). This problem, in general randomized autoregressive training process and beyond this specific example, can happen for all token locations except the last one (since the last token does not need to predict next token). To address this issue, we introduce an additional set of positional embeddings, which we refer to as target-aware positional embeddings. These embeddings encode information about which token is being predicted next. Formally, we define set of target-aware positional embeddings pta = [p1, p2, , pT ]. The positional embedding corresponding to the next token is added to the current token embedding, resulting in target-aware token embedding ˆxτ : ˆxτ = xτ +pτ = [xτ1+pτ2, xτ2+pτ3, , xτT 1+pτT , xτT ], (4) where xτ and pτ are permuted tokens for and pta w.r.t. to the permutation τ , respectively. By associating the target tokens positional embedding with the next-token prediction, each token prediction is aware of the target tokens index, alleviating the potential confusion in permuted objective. Notably, we omit the target-aware positional embedding for the final token xτT , as it does not participate in the loss computation and has no prediction target. visual illustration of this concept is provided in Fig. 3. It is also noteworthy that the target-aware positional embedding can be merged with original positional embedding after the training is finished, because our method anneals to fixed raster scan in the end, and thus leads to no increase on the parameters or computation during inference. Randomness Annealing. While the proposed randomized autoregressive training with permutation enables the model to capture bidirectional context within unidirectional framework, it may introduce sub-optimal behavior for visual generation due to two main factors: (1) The sheer number of possible permutations is vast, potentially causing the model to focus on learning how to handle the different permutation orders rather than improving generation quality. For example, for token sequence of length 256, the number of possible permutations is 256! > 10506, which can overwhelm the model and reduce training efficiency. (2) Although images can be processed in arbitrary orders, certain scan orders tend to outperform others. For instance, [22] evaluated six different scan orders (row-major, spiral in, spiral out, z-curve, subsample, and alternate) and found that row-major (i.e., raster order) consistently performed the best, result that has made it the most widely used order for visual generation. To address these issues, we propose Randomness Annealing, strategy designed to balance the randomness of permutations with the known effectiveness of the raster order. This method introduces single parameter, r, which controls the probability of using random permutation versus the raster order. At the start of training, = 1, meaning that the model exclusively uses random permutations. Over the course of training, linearly decays to 0, transitioning the model to the raster order by the end of training. Specifically, we define training schedule for r, controlled by two hyper-parameters start and end indicating the training epoch when starts to anneal and when the annealing ends. Formally, we have: = 1.0, 0.0, 1.0 epochstart endstart , otherwise, if epoch < start, if epoch > end, (5) where epoch is the current training epoch. We will ablate the hyper-parameters start and end in the experiments. The schedule allows the model to initially explore the diverse random permutations for better bidirectional representation learning, and ultimately converge to the more effective row-major scan order for better visual generation quality, as is used by other typical autoregressive methods [22]. It is worth noting that this strategy not only improves generation performance but also maintains compatibility with the standard scan order used in previous works. 5 model RAR-B RAR-L RAR-XL RAR-XXL depth width mlp 3072 768 4096 1024 5120 1280 6144 1408 24 24 32 40 heads 16 16 16 16 #params 261M 461M 955M 1499M Table 1. Architecture configurations of RAR. We follow prior works scaling up ViT [19, 74] for different configurations. 4. Experimental Results In this section, we outline the implementation details of our method in Sec. 4.1. Next, we present ablation studies on key design choices in Sec. 4.2. The main results are discussed in Sec. 4.3, followed by scaling study and visualizations. 4.1. Implementation Details We implement the RAR on top of language modeling autoregressive framework with minimal changes. VQ Tokenizer. Following prior works [10, 22] which use VQ tokenizer to tokenize the input images into discrete token sequences, we use the MaskGIT-VQGAN [10] with the official weight trained on ImageNet. This tokenizer is purely CNN-based tokenizer which tokenizes 256 256 image into 256 discrete tokens (i.e., downsampling factor 16) with codebook size (i.e., vocabulary size) 1024. Autoregressive Transformer. We use vision transformers [19] of different model configurations [74] including RAR-S (133M), RAR-B (261M), RAR-L (461M), RARXL (955M), and RAR-XXL (1499M). For all of these model variants, we apply causal attention masking in the self-attention module and QK LayerNorm [15] to stabilize the large-scale model training. We use plain ViT for all ablation studies to speed up the experiments, and we enhance the model with adaLN [45] for final models. The detailed architecture configuration and model size are available at Tab. 1. Positional Embedding. We use learnable embeddings for both original positional embedding in ViT and target-aware positional embedding. Notably, as our model anneals to raster order-based autoregressive image generation after the training is finished, the two positional embeddings can be combined into one, making it identical to conventional autoregressive image generator. Dataset. We train our model on ImageNet-1K [16] training set, which contains 1, 281, 167 training images across 1000 object classes. We pre-tokenize the whole training set with MaskGIT-VQGAN tokenizer [10] to speed up the training. For ablation studies, we pre-tokenize the dataset with only center crop and horizontal flipping augmentation, while we further enhance the diversity in pretokenized datasets with ten-crop transformation [52, 53] for final models. Training Protocols. We use the same training hyperstart epoch 0 0 0 0 0 100 100 100 100 200 200 200 300 300 end epoch 0 100 200 300 400 100 200 300 400 200 300 400 300 400 400 FID 3.08 2.68 2.41 2.40 2.43 2.48 2.28 2.33 2.39 2.39 2.18 2.55 2.41 2.74 3.01 IS 245.3 237.3 251.5 258.4 265.3 247.5 253.1 258.4 266.5 259.7 269.7 241.6 269.1 236.4 305.6 Pre. Rec. 0.52 0.85 0.54 0.84 0.54 0.84 0.54 0.84 0.53 0.84 0.54 0.84 0.55 0.83 0.54 0.83 0.54 0.84 0.54 0.84 0.55 0.83 0.54 0.84 0.53 0.84 0.54 0.83 0.52 0.84 Table 2. Different start and end epochs for randomness annealing, with total of 400 training epochs and model size RAR-L. The final setting is labeled in gray. : When start epoch and end epoch are both 0 (1st row), the training reverts to standard raster order training. : When start epoch and end epoch are both 400 (last row), the training becomes purely random order training. After training is finished, all results are obtained with raster order sampling, except for the purely random order training (i.e., last row), where we also randomly sample the scan order following [36], which otherwise could not produce reasonable result. parameters for all model variants. The model is trained with batch size 2048 for 400 epochs (250k steps). The learning rate will be linearly increased from 0 to 4 104 at the first 100 epochs (warm-up), then it will be gradually decayed to 1 105 following cosine decay schedule. We use AdamW [33, 38] optimizer with beta1 0.9, beta2 0.96, and weight decay 0.03. We perform gradient clipping with maximum gradient norm 1.0. During training, the class condition will be dropped at probability 0.1. The training setting remain the same for both ablation studies and main results across all RAR model variants. Sampling Protocols. We sample 50000 images for FID computation using the evaluation code from [18]. We do not use any top-k or top-p based filtering techniques. We also follow prior arts [11, 25, 73] to use classifier-free guidance [30]. In ablation study, we use simpler linear guidance schedule [11] and for final models we use the improved power-cosine guidance schedule [25]. The final detailed hyper-parameters for each model variant can be found in appendix. 4.2. Ablation Studies We study different configurations for RAR, including the randomness annealing strategy and scan orders that RAR converges to. Randomness Annealing Strategy. In Tab. 2 we compare different randomness annealing strategies. We adopt linear decaying schedule and focus on when should the scan order row-major spiral in spiral out z-curve subsample alternate FID 2.18 2.50 2.46 2.29 2.39 2.48 IS 269.7 256.1 256.6 262.7 258.0 270.9 Precision 0.83 0.84 0.84 0.83 0.84 0. Recall 0.55 0.54 0.54 0.55 0.54 0.53 Table 3. Effect of different scan orders RAR-L converges to. We mainly consider 6 different scan orders (row major, spiral in, spiral out, z-curve, subsample, alternate) as studied in [22]. Our default setting is marked in gray. visual illustration of different scan orders are available in the appendix. randomization annealing starts and ends by changing two hyper-parameters start and end, as defined in Eq. (5). For training lasting for 400 epochs, we enumerate all possible combinations for every 100 epochs. For example, when start = 200 and end = 300, the model is trained with random permutations from 0 to 200 epochs and raster order from 300 to 400 epochs. During 200 to 300 epoch, the model is trained via random permutation with probability and raster order with probability 1 r, where is computed as in Eq. (5). It is noteworthy that when start = end = 0, the model is trained with purely raster order, i.e., the standard autoregressive training. When start = end = 400, the model is always trained with randomly permuted input sequence. Both cases are important baselines of the proposed randomness annealing, and they achieve FID scores of 3.08 and 3.01, respectively. Interestingly, we observe all other variants achieve substantial improvement over these two baselines. For example, even simply replacing the first 100 epochs of raster order with random permutation, it (i.e., start = 100 and end = 100) improves the FID to 2.48 by 0.6. Besides, we also note that the model prefers to keep some beginning epochs for pure random permutation training and some last epochs for better adapting to raster scan order, which usually leads to better performance compared to other variants. All the results demonstrate that adding randomized autoregressive training with permuted objective is beneficial to the autoregressive visual generator and leads to boosted FID score, thanks to the improved bidirectional representation learning process. Additionally, among all variants, we found that the case, where start = 200 and end = 300, works the best, which improves the baseline (purely raster order) FID from 3.08 to 2.18. This strategy allocates slightly more computes on the training with random permutation order, and focuses on the purely raster order for the last 100 epochs. Therefore, we default to adopt this annealing strategy for all RAR models. Different Scan Orders Besides Raster. Although rowmajor order (i.e., raster scan) has been the de facto scan order in the visual generation, there lacks systematic study on how good it is compared to other scan orders. We note that the work [22] conducted similar study 4 years ago. However, it is worth re-examining the conclusion considering the significant progress generative models have achieved in recent years. Specifically, we consider 6 different scan orders (row-major, spiral in, spiral out, z-curve, subsample, and alternative) following [22] that RAR may converge to. Instead of reporting the training loss and validation loss as the comparison metric [22], we directly evaluate their generation performance. The results are summarized in Tab. 3. Interestingly, we observe that all variants achieve reasonably good score, which indicates that RAR is capable of handling different scan orders. Considering that the row-major (raster scan) still demonstrates advantages over the other scan orders, we thus use the raster scan order for all final RAR models. 4.3. Main Results We report RAR results against state-of-the-art image generators on ImageNet-1K 256 256 benchmark [16]. As shown in Tab. 4, RAR achieves significantly better performance compared to previous AR image generators. Specifically, the most compact RAR-B with 261M parameters only, achieves an FID score 1.95, already significantly outperforming current state-of-the-art AR image generators LlamaGen-3B-384 (3.1B, FID 2.18, crop size 384) [52] and Open-MAGVIT2-XL (1.5B, FID 2.33) [39], while using 91% and 81% fewer model parameters respectively. It also surpasses the widely used diffusion models such as DiTXL/2 (FID 1.95 vs. 2.27) and SiT-XL (FID 1.95 vs. 2.06) while only using 39% model parameters compared to them. In Tab. 4, we further explore RAR at different model sizes (from 261M to 1.5B), where we observe strong scalability behavior with consistent performance improvement as model size scales up. Notably, the largest variant RARXXL sets new state-of-the-art result on ImageNet benchmark, with an FID score 1.48. When compared to the other two recent methods VAR [58] and MAR [36], both of which attempt to amend AR formulation for better visual generation quality, RAR not only demonstrates superior performance (FID 1.48 from RAR vs. 1.73 from VAR and 1.55 from MAR), but also keeps the whole framework compatible with language modeling and thus is more friendly for adapting the mature optimization and speed-up techniques for large language models to visual generation [52]. Moreover, RAR demonstrates superior performance to state-of-the-art visual generators in different frameworks. It performs better against the leading autoregressive models, diffusion models and masked transformer models, surpassing LlamaGen-3B-384 [52], MDTv2-XL/2 [25] and MaskBit [65] respectively (FID 1.48 from RAR vs. 2.18 from LlamaGen, 1.58 from MDTv2, and 1.52 from MaskBit). To the best of our knowledge, this is the first time that the language modeling style autoregressive visual type tokenizer VQ [50] Diff. VAE [50] Diff. VAE [51] Diff. VQ [10] Mask. VQ [73] Mask. VQ [72] Mask. VQ [65] Mask. VAE [36] MAR VQ [58] VAR VQ [22] AR VQ [69] AR VQ [39] AR VQ [52] AR VQ [10] AR - - - - - - - - #params FID IS Pre. Rec. generator 258M 7.76 209.5 0.84 0.35 LDM-8 [50] 400M 3.60 247.7 0.87 0.48 LDM-4 [50] 287M 3.40 219.9 0.83 0.52 UViT-L/2 [6] 501M 2.29 263.9 0.82 0.57 UViT-H/2 [6] 458M 5.02 167.2 0.75 0.57 DiT-L/2 [45] 675M 2.27 278.2 0.83 0.57 DiT-XL/2 [45] 675M 2.06 270.3 0.82 0.59 SiT-XL [40] 505M 1.70 289.0 0.79 0.63 DiMR-XL/2R [37] 676M 1.58 314.7 0.79 0.65 MDTv2-XL/2 [25] 177M 6.18 182.1 MaskGIT [10] 287M 1.97 281.8 TiTok-S-128 [73] 307M 1.78 319.4 MAGVIT-v2 [72] 305M 1.52 328.6 MaskBit [65] 208M 2.31 281.7 0.82 0.57 MAR-B [36] 479M 1.78 296.0 0.81 0.60 MAR-L [36] 943M 1.55 303.7 0.81 0.62 MAR-H [36] 2.0B 1.92 323.1 0.82 0.59 VAR-d30 [58] 2.0B 1.73 350.2 0.82 0.60 VAR-d30-re [58] 1.4B 15.78 74.3 GPT2 [22] 1.4B 5.20 280.3 GPT2-re [22] 1.7B 4.17 175.1 VIM-L [69] 1.7B 3.04 227.4 VIM-L-re [69] 343M 3.08 258.3 0.85 0.51 Open-MAGVIT2-B [39] 804M 2.51 271.7 0.84 0.54 Open-MAGVIT2-L [39] Open-MAGVIT2-XL [39] 1.5B 2.33 271.8 0.84 0.54 343M 3.80 248.3 0.83 0.51 775M 3.39 227.1 0.81 0.54 1.4B 3.09 253.6 0.83 0.53 3.1B 3.05 222.3 0.80 0.58 343M 3.07 256.1 0.83 0.52 775M 2.62 244.1 0.80 0.57 1.4B 2.34 253.9 0.80 0.59 3.1B 2.18 263.3 0.81 0.58 261M 1.95 290.5 0.82 0.58 461M 1.70 299.5 0.81 0.60 955M 1.50 306.9 0.80 0.62 1.5B 1.48 326.0 0.80 0. LlamaGen-L [52] LlamaGen-XL [52] LlamaGen-XXL [52] LlamaGen-3B [52] LlamaGen-L-384 [52] LlamaGen-XL-384 [52] LlamaGen-XXL-384 [52] LlamaGen-3B-384 [52] RAR-B (ours) RAR-L (ours) RAR-XL (ours) RAR-XXL (ours) - - - - - - - - Table 4. ImageNet-1K 256 256 generation results evaluated with ADM [18]. type refers to the type of the generative model, where Diff. and Mask. stand for diffusion models and masked transformer models, respectively. VQ denotes discrete tokenizers and VAE stands for continuous tokenizers. -re stands for rejection sampling. -384 denotes for generating images at resolution 384 and resize back to 256 for evaluation, as is used in [52]. generators outperform state-of-the-art diffusion models and masked transformer models. Sampling Speed. One key advantage of AR methods is their ability to leverage established optimization techniques from LLMs, such as KV-caching. In Tab. 5, we compare the sampling speed (measured as images/sec) of RAR against other types of generative models, such diffusion models [45], masked transformers [65, 73], VAR [58], and MAR [36]. Among them, AR models (RAR) and VAR models (VAR-d30) are compatible with the KV-cache optimization, providing significant advantage in generation speed over other methods. As shown in Tab. 5, RAR achieves state-of-the-art FID score while also significantly (a) training losses (b) FID scores w/o classifier-free guidance (c) FID scores w/ classifier-free guidance Figure 4. Scaling behavior of RAR models. The scaled-up RAR models demonstrate (a) reduced training losses, and improved FID scores both (b) without and (c) with classifier-free guidance. method DiT-XL/2 [45] type Diff. TiTok-S-128 [73] Mask. VAR MAR AR MAR AR VAR-d30 [58] MAR-B [36] RAR-B (ours) MAR-L [36] RAR-L (ours) MaskBit [65] Mask. MAR MAR-H [36] RAR-XL (ours) AR RAR-XXL (ours) AR #params FID steps images/sec 675M 2.27 287M 1.97 2.0B 1.92 208M 2.31 261M 1.95 479M 1.78 461M 1.70 305M 1.52 943M 1.55 955M 1.50 1.48 1.5B 0.6 7.8 17.3 0.8 17.0 0.5 15.0 0.7 0.3 8.3 6.4 250 64 10 256 256 256 256 256 256 256 256 Sampling throughput comparison (including deTable 5. tokenization process) categorized by methods with similar FID scores. Throughputs are measured as samples generated per second on single A100 using float32 precision and batch size of 128, based on their official codebases. For VAR [58] and our RAR, KV-cache is applied. Diff. and Mask. refer to diffusion models and masked transformer models, respectively. surpassing other methods in generation speed. For instance, at an FID score around 1.5, MaskBit [65] and MAR-H [36] generate image samples at 0.7 and 0.3 images per second, respectively. In comparison, RAR-XL not only achieves better FID score but can generate 8.3 high-quality visual samples per second11.9 faster than MaskBit and 27.7 faster than MAR-H. The largest RAR variant, RAR-XXL, further improves the FID score while maintaining notable speed advantage, being 9.1 faster than MaskBit and 21.3 faster than MAR-H. Additionally, RAR may benefit further from LLM optimization techniques such as vLLM [34], as seen with other AR methods [52]. Scaling Behavior. We study the scaling behavior of RAR. Specifically, we plot the training loss curves and FID score curves (with and without classifier-free guidance [30]) in Fig. 4. As shown in the figure, we observe that RAR scales well at different model sizes, where larger model size leads to consistently lower training loss and better FID score, regardless of using the enhancement of classifier-free guidance or not. We note that as RAR keeps the AR formulation and framework intact, it also inherits the scalability 8 Figure 5. Visualization of samples generated by RAR across various model sizes. RAR generates high-quality visual samples across all model sizes. As model size increases, fidelity and diversity improve, especially in challenging classes (e.g., dogsled). from AR methods. Visualization. We visualize generated samples by different RAR variants in Fig. 5, which shows that RAR is capable of generating high-quality samples with great fidelity and diversity. More visualizations are provided in the appendix. 5. Conclusion In this paper, we introduced simple yet effective strategy to enhance the visual generation quality of language modeling-compatible autoregressive image generators. By employing randomized permutation objective, our aplearning proach enables improved bidirectional context while preserving the autoregressive structure. Consequently, the proposed RAR model not only surpasses previous state-of-the-art autoregressive image generation models but also outperforms leading non-autoregressive transformer and diffusion models. We hope this research contributes to advancing autoregressive transformers toward unified framework for visual understanding and generation. Acknowledgment. We sincerely thank Tianhong Li for his insightful discussion and feedback on this project."
        },
        {
            "title": "Supplementary Material",
            "content": "Algorithm 1 PyTorch Pseudo-Code for Randomized AutoRegressive (RAR) Modeling class RAR(nn.Module): def sample orders(self, tokens, global step): # sample permutation order at training step global step. orders = [] # compute the randomized probability as in Eq. (5). prob = 1.0 - min(1.0, max(0.0, (global step - self.anneal start) / (self.anneal end - self.anneal start))) for in range(tokens.shape[0]): if random.random() <prob: # random permutation. orders.append(torch.randperm(tokens.shape[1])) else: # raster order (no permutation). orders.append(torch.arange(tokens.shape[1])) return torch.stack(orders) def permute(self, inputs, orders): # permute inputs based on orders. B, = inputs.shape[:2] indices = torch.arange(B).unsqueeze(1).expand(-1, L) return x[indices, orders] def forward(self, tokens, condition, global step): # get permutation orders. orders = self.sample orders(global step, tokens) # permute labels for next-token prediction. labels = self.permute(tokens.clone(), orders) # token embeddings with positional embedding. = self.tok emb(tokens) + self.pos emb # permute the token orders. = self.permute(x, orders) # add target-aware postional embedding as in Eq. (4). target pos emb = self.target pos emb.repeat(x.shape[0], 1, 1) target pos emb = self.permute(target pos emb, orders) # shifting so each token will see next-tokens embedding. target pos emb = target pos emb[:, 1:] = torch.cat([x[:, :-1] + target pos emb, x[:, -1:]], dim=1) # transformer forwarding. pred = self.transformers(x, condition) # next token prediction loss. loss = nn.CrossEntropy(pred[:, :-1], labels[:, 1:]) return loss D. Visualization on Generated Samples We provide visualization results in Fig. 7, Fig. 8, and Fig. 9."
        },
        {
            "title": "Appendix",
            "content": "The supplementary material includes the following additional information: Sec. provides the detailed hyper-parameters for the final RAR models. Sec. provides the pseudo-code for randomized autoregressive modeling. Sec. visualizes the scan orders used in the ablation study. Sec. provides more visualization samples of RAR models. A. Hyper-parameters for Final RAR Models We list the detailed training hyper-parameters and sampling hyper-parameters for all RAR models in Tab. 6. config training hyper-params value optimizer learning rate weight decay optimizer momentum batch size learning rate schedule ending learning rate total epochs warmup epochs annealing start epoch annealing end epoch precision max grad norm dropout rate attn dropout rate class label dropout rate AdamW [33, 38] 4e-4 0.03 (0.9, 0.96) 2048 cosine decay 1e-5 400 100 200 300 bfloat16 1.0 0.1 0.1 0.1 sampling hyper-params guidance schedule temperature scale power guidance scale pow-cosine [25] 1.0 (B) / 1.02 (L, XL, XXL) 2.75 (B) / 2.5 (L) / 1.5 (XL) / 1.2 (XXL) 16.0 (B) / 15.5 (L) / 6.9 (XL) / 8.0 (XXL) Table 6. Detailed hyper-parameters for final RAR models. B. Pseudo-Code for RAR We provide simple pseudo-code of RAR in PyTorch style in Algorithm 1. C. Visualization of Scan Orders We visualize the 6 scan orders studied in the main paper ( Tab. 3) in Fig. 6. (a) row-major (b) spiral in (c) spiral out (d) z-curve (e) subsample (f) alternate Figure 6. Different scan orders for 16 16 grid (256 tokens). The number indicates the tokens indices in the scanning order. Figure 7. Visualization samples from RAR. RAR is capable of generating high-fidelity image samples with great diversity. 2 Figure 8. Visualization samples from RAR. RAR is capable of generating high-fidelity image samples with great diversity. Figure 9. Visualization samples from RAR. RAR is capable of generating high-fidelity image samples with great diversity."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. 1, 2 [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [3] Rohan Anil, Andrew Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023. [4] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei arXiv preprint Huang, et al. Qwen technical report. arXiv:2309.16609, 2023. 2 [5] Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar, Alan Yuille, Trevor Darrell, Jitendra Malik, and Alexei Efros. Sequential modeling enables scalable learning for large vision models. In CVPR, 2024. 1 [6] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In CVPR, 2023. 1, 7 [7] Lucas Beyer, Andreas Steiner, Andre Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. Paligemma: versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726, 2024. 1, 3 [8] Andrew Brock. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. [9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. NeurIPS, 2020. 1, 2, 4 [10] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In CVPR, 2022. 1, 2, 3, 4, 5, 7 [11] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William Freeman, Michael Rubinstein, et al. Muse: Textto-image generation via masked generative transformers. In ICML, 2023. 3, 6 [12] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In ICML, 2020. 2 [13] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240): 1113, 2023. 2 [14] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instructionfinetuned language models. JMLR, 25(70):153, 2024. 1 [15] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In ICML, pages 74807512. PMLR, 2023. 5 [16] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In CVPR, 2009. 2, 5, [17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transIn NAACL, 2018. 3, formers for language understanding. 4 [18] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. NeurIPS, 2021. 1, 3, 6, 7 [19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 4, 5 [20] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 2 [21] Alaaeldin El-Nouby, Michal Klein, Shuangfei Zhai, Miguel Angel Bautista, Alexander Toshev, Vaishaal Shankar, Joshua Susskind, and Armand Joulin. Scalable pretraining of large autoregressive image models. ICML, 2024. 1, 3 [22] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, 2021. 1, 2, 3, 4, 5, 6, 7 [23] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. [24] Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen Sun, Michael Rubinstein, Deqing Sun, Kaiming He, and Yonglong Tian. Fluid: Scaling autoregressive text-to-image generative models with continuous tokens. arXiv preprint arXiv:2410.13863, 2024. 3 [25] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is strong image synthesizer. In ICCV, 2023. 6, 7, 1 [26] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. NeurIPS, 2014. 3 [27] Karol Gregor, Ivo Danihelka, Andriy Mnih, Charles Blundell, and Daan Wierstra. Deep autoregressive networks. In International Conference on Machine Learning, pages 12421250. PMLR, 2014. 2 [28] Jiatao Gu, Changhan Wang, and Junbo Zhao. Levenshtein [46] Alec Radford. Improving language understanding by genertransformer. NeurIPS, 32, 2019. 4 ative pre-training. OpenAI, 2018. 2 [29] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2022. 1 [30] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 6, 8 [31] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 2020. 3 [32] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In CVPR, 2019. 3 [33] Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. In ICLR, 2015. 6, 1 [34] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611626, 2023. 8 [35] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In CVPR, 2022. [36] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. NeurIPS, 2024. 1, 2, 3, 6, 7, 8 [37] Qihao Liu, Zhanpeng Zeng, Ju He, Qihang Yu, Xiaohui Shen, and Liang-Chieh Chen. Alleviating distortion in image generation via multi-resolution diffusion models. NeurIPS, 2024. 3, 7 [38] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. ICLR, 2019. 6, 1 [39] Zhuoyan Luo, Fengyuan Shi, Yixiao Ge, Yujiu Yang, Limin Wang, and Ying Shan. Open-magvit2: An open-source project toward democratizing auto-regressive visual generation. arXiv preprint arXiv:2409.04410, 2024. 1, 2, 7 [40] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. ECCV, 2024. 7 [41] Xiaoxiao Ma, Mohan Zhou, Tao Liang, Yalong Bai, Tiejun Zhao, Huaian Chen, and Yi Jin. Star: Scale-wise text-toimage generation via auto-regressive representations. arXiv preprint arXiv:2406.10797, 2024. [42] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. NeurIPS, 36, 2023. 4 [43] OpenAI. Gpt-4 technical arXiv:2303.08774, 2023. 1, 2 report. arXiv preprint [44] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. ImIn International conference on machine age transformer. learning, pages 40554064. PMLR, 2018. [45] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. 1, 3, 5, 7, 8 [47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. 2 [48] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, 2021. 2 [49] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. NeurIPS, 2019. 2 [50] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 3, 7 [51] stabilityai, 2023. 7 [52] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 1, 2, 4, 5, 7, [53] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR, 2015. 5 [54] Haotian Tang, Yecheng Wu, Shang Yang, Enze Xie, Junsong Chen, Junyu Chen, Zhuoyang Zhang, Han Cai, Yao Lu, and Song Han. Hart: Efficient visual generation with hybrid autoregressive transformer. arXiv preprint arXiv:2410.10812, 2024. 3 [55] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 1, 2 [56] Emu3 Team. Emu3: Next-token prediction is all you need. Tech Report, 2024. 1, 2 [57] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 1, [58] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. NeurIPS, 2024. 1, 2, 3, 7, 8 [59] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 1, 2 [60] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 1, 2 [61] Benigno Uria, Marc-Alexandre Cˆote, Karol Gregor, Iain Murray, and Hugo Larochelle. Neural autoregressive distribution estimation. JMLR, 17(205):137, 2016. 3, 4 5 [62] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. NeurIPS, 2016. [63] Aaron Van Den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In International conference on machine learning, pages 17471756. PMLR, 2016. 2 [64] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. NeurIPS, 2017. 2 [65] Mark Weber, Lijun Yu, Qihang Yu, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. Maskbit: arXiv Embedding-free image generation via bit tokens. preprint arXiv:2409.16211, 2024. 1, 3, 7, 8 [66] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. Finetuned language models are zero-shot learners. In ICLR, 2022. 1 [67] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. 1, [68] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. Xlnet: Generalized autoregressive pretraining for language understanding. NeurIPS, 2019. 3, 4 [69] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. In ICLR, 2022. 1, 2, 7 [70] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. TMLR, 2022. 1 [71] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose Lezama, Han Zhang, Huiwen Chang, Alexander Hauptmann, MingHsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video transformer. In CVPR, 2023. 1, 3 [72] Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, et al. Language model beats diffusiontokenizer is key to visual generation. In ICLR, 2024. 3, 7 [73] Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is worth 32 tokens for reconstruction and generation. NeurIPS, 2024. 1, 3, 6, 7, [74] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and LuIn CVPR, pages cas Beyer. Scaling vision transformers. 1210412113, 2022. 5 [75] Qian Zhang, Xiangzi Dai, Ninghua Yang, Xiang An, Ziyong Feng, and Xingyu Ren. Var-clip: Text-to-image generator with visual auto-regressive modeling. arXiv preprint arXiv:2408.01181, 2024."
        }
    ],
    "affiliations": [
        "ByteDance"
    ]
}