{
    "paper_title": "OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding",
    "authors": [
        "Keda Tao",
        "Wenjie Du",
        "Bohan Yu",
        "Weiqiang Wang",
        "Jian Liu",
        "Huan Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Omnimodal large language models have made significant strides in unifying audio and visual modalities; however, they often lack the fine-grained cross-modal understanding and have difficulty with multimodal alignment. To address these limitations, we introduce OmniAgent, a fully audio-guided active perception agent that dynamically orchestrates specialized tools to achieve more fine-grained audio-visual reasoning. Unlike previous works that rely on rigid, static workflows and dense frame-captioning, this paper demonstrates a paradigm shift from passive response generation to active multimodal inquiry. OmniAgent employs dynamic planning to autonomously orchestrate tool invocation on demand, strategically concentrating perceptual attention on task-relevant cues. Central to our approach is a novel coarse-to-fine audio-guided perception paradigm, which leverages audio cues to localize temporal events and guide subsequent reasoning. Extensive empirical evaluations on three audio-video understanding benchmarks demonstrate that OmniAgent achieves state-of-the-art performance, surpassing leading open-source and proprietary models by substantial margins of 10% - 20% accuracy."
        },
        {
            "title": "Start",
            "content": "OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding Keda Tao 1 2 3 Wenjie Du 2 Bohan Yu 3 Weiqiang Wang 3 Jian liu 3 Huan Wang 2 https://kd-tao.github.io/OmniAgent 5 2 0 2 9 2 ] . [ 1 6 4 6 3 2 . 2 1 5 2 : r Figure 1. (a): Illustration of OmniAgent, an audio-guided active perception agent designed for omnimodal understanding. Given complex user query, our agent employs recursive Think-Act-Observe-Reflect loop and actively orchestrates multimodal tools (video, audio, and event tools) for fine-grained audio-video understanding. The presented video clip is from real-life vlog; the question is about two Chinese characters on hanging signboard in the video. Initially, the agent utilizes acoustic cues to locate the temporal segment containing the key information (the kitten), then invokes the video clip tool within that time window. Within the salient segment, given the same GPU memory budget, we can afford model inference at an increased spatial and temporal resolution. With sufficient relevant visual evidence and the audio as input, the agent derives the correct answer. In contrast, the end-to-end video understanding model Qwen3-Omni (Xu et al., 2025b) (b) cannot achieve such fine-grained understanding and gives the wrong answer. (c): Performance comparison on three audio-video understanding benchmarks. OmniAgent demonstrates superior performance, consistently outperforming strong end-to-end OmniLLMs such as Qwen3-Omni (Xu et al., 2025b) and Gemini 2.5-Flash (Comanici et al., 2025)."
        },
        {
            "title": "Abstract",
            "content": "Omnimodal large language models have made significant strides in unifying audio and visual modalities; however, they often lack the fine-grained cross-modal understanding and have difficulty with multimodal alignment. To address these limitations, we introduce OmniAgent, fully audioguided active perception agent that dynamically orchestrates specialized tools to achieve more fine-grained audio-visual reasoning. Unlike previous works that rely on rigid, static workflows 1Zhejiang University 2Westlake University 3Ant Group. Correspondence to: Huan Wang <wanghuan@westlake.edu.cn>, Jian Liu <rex.lj@antgroup.com>. Preprint. December 30, 2025. 1 and dense frame-captioning, this paper demonstrates paradigm shift from passive response generation to active multimodal inquiry. OmniAgent employs dynamic planning to autonomously orchestrate tool invocation on demand, strategically concentrating perceptual attention on taskrelevant cues. Central to our approach is novel coarse-to-fine audio-guided perception paradigm, which leverages audio cues to localize temporal events and guide subsequent reasoning. Extensive empirical evaluations on three audio-video understanding benchmarks demonstrate that OmniAgent achieves state-of-the-art performance, surpassing leading open-source and proprietary models by substantial margins of 10% - 20% accuracy. OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding 1. Introduction Recently, end-to-end omnimodal large language models (OmniLLMs) have achieved encouraging results by integrating visual and audio encoders into unified architecture (Tang et al., 2025; Zhang et al., 2024c; Xu et al., 2025a;b; Yang et al., 2025b; Ge et al., 2025; Shu et al., 2025; Yang et al., 2025c; Shu et al., 2025; AI et al., 2025). Despite this progress, as shown in Figure 2(a), OmniLLMs still struggle with in fine-grained cross-modal understanding, and the joint alignment training of audio and video representations poses significant challenges (Galougah et al., 2025; Chowdhury et al., 2024; Sun et al., 2023). Consequently, models often cannot respond accurately. critical empirical observation is that while MLLMs have demonstrated exceptional proficiency in unimodal tasks (Wang et al., 2025a; Team et al., 2025; Bai et al., 2025b; Wang et al., 2024a; Chu et al., 2023; Ding et al., 2025; Bai et al., 2025a), cross-modal understanding remains constrained by the challenges of temporal and feature alignment (Chowdhury et al., 2024; Jiang et al., 2025; Fan et al., 2025). Consequently, developing agent-based frameworks to synergize the capabilities of distinct modalities is now promising direction. Although preliminary investigations into agentic audio-visual understanding exist (Cao et al., 2025; Zhou et al., 2025), current approaches rely predominantly on static workflows, as illustrated in Figure 2(b). These methods not only incur substantial computational overhead but also face challenges in fully leverage the inherent thinking capabilities of models for dynamic planning, thereby precluding genuine fine-grained understanding. Recent works have yielded significant advancements in agent-based video understanding (Zhang et al., 2025; 2024b; Fan et al., 2024; Wang et al., 2024b; Yang et al., 2025d; Wang et al., 2025c; Pang & Wang, 2025; Wang et al., 2025d; Chowdhury et al., 2025; Yin et al., 2025). Specifically, temporal event localization is paramount for fine-grained analysis. Prevailing approaches predominantly rely on frame-captioning, where captions are generated for sampled frames, stored, and subsequently retrieved and analyzed iteratively by agents (Wang et al., 2024b; Zhang et al., 2025; Wang et al., 2025c). While these methods refine their hypotheses through multi-step inference, they incur substantial computational overhead. Sometimes, even worse, the generated caption of the frames may be irrelevant to the query. However, in the context of audio-visual understanding, the audio modality presents distinct challenges yet offers unique opportunity: unlike redundant visual signals, audio naturally provides accurate and concise temporal cues regarding the salient events (Guo et al., 2025; Tao et al., 2025). These cues can efficiently guide agent-based event localization, thereby facilitating more accurate fine-grained understanding at low cost. Figure 2. (a) End-to-end OmniLLMs implicitly fuse modalities but suffer from high training costs, difficult alignment, and limited fine-grained reasoning. (b) Fixed workflow agents rely on rigid pipelines, lacking the flexibility to allocate attention for finegrained analysis adaptively. (c) Caption-based agents incur high precomputation costs and noise sensitivity, often failing to capture comprehensive multimodal context. (d) Our OmniAgent employs active perception reasoning and inquiry. Within an iterative reflective loop, the agent strategically calls on the ability of video and audio understanding. This explicitly solves the cross-modal alignment difficulty and achieves fine-grained understanding. Building on these insights, we present OmniAgent, an agent specifically designed for omnimodal (audio-visual) understanding in an autonomous and active reasoning fashion, which treats strong single-modal models as callable tools. In contrast to previous methods constrained by fixed workflows, as shown in Figure 2(d), our approach initiates fundamental paradigm shift from passive response generation to active information inquiry. The agent employs an LLM as the central component to orchestrate tool invocation, determining the optimal modality to use it explicitly decides whether to attend to audio or video, and to what extent and how to understand. Note, this process is completely autonomous, decided by the LLM itself. The tool calling and decision making is transparent to us, thus explainable and optimizable. By strategically selecting spatial 2 OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding and temporal focus range (i.e., deciding precisely where to look and listen), OmniAgent achieves genuine, fine-grained cross-modal understanding. Specifically, we construct comprehensive library of tools categorized into three distinct sets: (1) Video tools, (2) Audio tools, and (3) Event tools. The video toolset enables global captioning and general visual QA, while also allowing for the analysis of specific temporal windows at higher sampling rates to support more fine-grained understanding. The audio toolset incorporates audio captioning and detailed QA capabilities, complemented by timestamped automatic speech recognition (ASR) for precise speech grounding. Within the event toolset, we propose novel audio-based event localization strategy. This mechanism empowers the agent to autonomously query and temporally localize events across the entire audio stream, establishing temporal anchors for subsequent fine-grained analysis. By synthesizing the capabilities of distinct MLLMs, our agent adaptively leverages their complementary strengths to facilitate joint, fine-grained analysis, utilizing cross-modal corroboration to maximize audio-visual comprehension performance. Extensive experiments show that OmniAgent achieves the best accuracy on all audio-video understanding benchmarks, surpassing the state-of-the-art open-source and closedsource models, such as Qwen3-Omni-30B (Xu et al., 2025b) and Gemini2.5-Flash (Comanici et al., 2025), by significant margin of 10-20% accuracy. The main contributions of this work are: We introduce OmniAgent, novel agent-based framework tailored for comprehensive audio-video understanding. Employing an active perception strategy, it dynamically modulates attention between auditory and visual modalities, and, via self-reflective mechanism, solves the cross-modal alignment problem. We construct comprehensive modality-specific toolkit and introduce an audio-guided event localization algorithm designed to facilitate fine-grained cross-modal reasoning. Experimental results on several audio-video understanding benchmarks show that OmniAgent achieves the new SoTA, with significant accuracy improvement compared with open-source and closed-source models. 2. Related Work deeper understanding of inter-modal relationships (Xu et al., 2025a; Tong et al., 2025; Xu et al., 2025b; Xie & Wu, 2024; Tang et al., 2025; Zhang et al., 2024c; Yang et al., 2025b; Ge et al., 2025; Fu et al., 2024; Shu et al., 2025; Sun et al., 2024; Li et al., 2024). Recent works, such as Qwen3-Omni (Xu et al., 2025a) and the Video-SALMONN series (Sun et al., 2024; Tang et al., 2025), have introduced state-of-the-art end-to-end models capable of unified multimodal perception. Among closed-source models, Gemini stands as powerful baseline, distinguished by its strong multimodal understanding capabilities. However, end-to-end models face significant hurdles: they require complex alignment training across multiple modalities and often struggle to achieve fine-grained cross-modal understanding. Specifically, models like Qwen-Omni (Xu et al., 2025a;b) interleave clip-level vision and audio tokens as part of input embeddings. However, while they can ingest nearly complete audio information, the visual representation is forced into lossy compression to fit the vision encoders capacity (Tao et al., 2025). This imbalance prevents the model from synchronizing fine-grained acoustic cues with their corresponding visual anchors, causing the model to struggle with tasks requiring precise multimodal alignment. 2.2. Video Understanding Agent Leveraging the advanced capabilities of MLLMs, recent studies have investigated agentic approaches to address the intricacies of video understanding with video clip captioning (Wang et al., 2024b; Zhang et al., 2024a; Jeoung et al., 2024; Wang et al., 2025b; Park et al., 2024; Ma et al., 2025; Kahatapitiya et al., 2025; Jeoung et al., 2024; Kugo et al., 2025). Concurrently, other methodologies have focused on decomposing complex queries into multi-step processes utilizing specialized tool modules (Fan et al., 2024; Liu et al., 2025a; Min et al., 2024; Zhu et al., 2025; Zhang et al., 2024b). More recently, research has shifted away from static workflows to explore active agentic perception (Yuan et al., 2025; Zhang et al., 2025; Wang et al., 2025d; Gao et al., 2025; Yang et al., 2025a), thereby enhancing long-form video comprehension. However, comprehensive audio-video understanding remains challenging due to the complexities of end-to-end cross-modal alignment and fine-grained reasoning. Addressing this gap, we introduce OmniAgent to apply agentic methodologies specifically to this holistic multimodal context. Departing from the conventional frame-captioning strategy, we propose novel mechanism that utilizes an audio-guided reasoning process. 2.1. Omnimodal Large Language Models 3. OmniAgent End-to-end OmniLLMs aim to achieve common understanding across all modalities, including image, audio, video, and text. By leveraging multimodal data, these architectures acquire richer contextual representations and gain We introduce the OmniAgent, specifically designed to achieve precise cross-modal audio-video understanding. In contrast to conventional paradigms that rely on passive OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding Figure 3. Overview of the OmniAgent framework. The system processes audio and video inputs through an iterative thinking-actionobserve-reflection cycle. The agent utilizes comprehensive suite of perception tools (video, audio, and event) to gather fine-grained evidence, while the reflection module synthesizes observations to update the memory and decide whether to rethink or conclude the task. frame processing or rigid execution protocols, OmniAgent functions as an active perception. It dynamically orchestrates suite of modality-specific perception tools (video, audio, and event), effectively reformulating audio-video understanding from passive retrieval task into an active, sequential decision-making process. This approach circumvents the alignment bottlenecks inherent in end-to-end models and addresses their limitations in achieving fine-grained multimodal understanding. To the best of our knowledge, OmniAgent constitutes the first active perception agentic framework for omnimodal understanding. 3.1. Overview and Problem Formulation Motivation. Existing end-to-end OmniLLMs typically process video and audio streams by projecting them into shared latent space. However, this paradigm exhibits fundamental limitation: the inability to allocate attentional resources between modalities adaptively. Crucially, queryrelevant information is often modality-specific; valid responses may hinge exclusively on auditory cues or demand scrutiny of high-resolution visual details. Constrained by fixed token budgets and joint optimization objectives, OmniLLMs lack the architectural flexibility to dynamically prioritize specific modalities or adjust processing granularity. This deficiency frequently results in the degradation of fine-grained understanding. Formulation. To address this, we formulate omnimodal understanding not as static task, but as sequential, active decision-making process. Let and denote the visual and audio streams, and be the user query. We define an agent π and store memory = {a0, o0, . . . , aT , oT }. At each step t, the agent assesses its state st and actively selects an action at and gets an observation ot (e.g., Listen to segment or Watch specific region), to maximize the information gain regarding q. By explicitly decoupling the modalities into callable tools, OmniAgent empowers the model to autonomously determine the optimal modality and granularitydeciding when to rely on low-cost auditory cues and when to demand high-cost visual inspectionthereby solving the cross-modality alignment difficulty. 3.2. Modality-Aware Expert Toolset To facilitate precise interaction with the environment, we devise comprehensive toolset, denoted as , stratified by both modality and granularity. Functioning as the agents perceptual interfaces, these tools offer varying degrees of information density and computational overhead. (cid:229) Video Perception Tools (TV). While visual processing yields rich semantic information, it incurs high computational costs. Furthermore, relying solely on global representations often leads to the loss of fine-grained granular details. To address these trade-offs, we design two distinct visual tools: Global QA: TVGA and Clip QA: TVCA. For TVGA, we employ sparse frame sampling to mitigate the overhead of long sequences, enabling high-level inquiries. Additionally, this tool allows the agent to identify initial visual cues for coarse temporal localization. Conversely, TVCA serves as the fine-grained analysis engine. It extracts video slices within target temporal window, dynamically increasing the sampling rate and input resolution. This enables deep visual reasoningfacilitating the detailed analysis of object properties, actions, and spatial relationshipswhile maintaining balanced computational budget. (cid:229) Audio Perception Tools (TA). Audio signals provide dense, complementary information essential for holistic video reasoning. We introduce suite of three audio-centric tools designed to facilitate both global perception and finegrained analysis. First, the ASR: TASR transcribes spoken dialogue into text with precise timestamp alignment. This capability is indispensable for queries dependent on specific verbal cues or semantic narratives conveyed through speech. Second, the Global Caption: TAGC synthesizes summary of the acoustic environment, establishing global auOmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding at, argst πplan(q, Mt) if at is ANSWER then Algorithm 1 OmniAgent Inference Process Input: User Query q, Audio A, Video V, Toolset Output: Answer 1: Initialize Memory M0 2: while not Answered do 3: 4: 5: 6: 7: 8: 9: Mt+1 Mt {(at, ot)} 10: 11: end while end if ot ExecuteTool(at, argst, A, V) Reasoning = Reflect(q, Mt, ot) return argst + Reflect cycle, empowering the agent to autonomously orchestrate reasoning, planning, and execution across both modalities (audio and video). As shown in Figure 3. Active Thinking. Upon receiving an input query, our agent formulates strategic inference plan designed to maximize accuracy while utilizing information from both modes. Crucially, the system assesses the cross-modal dependency of the query to prioritize the optimal modality dynamicallydetermining whether to employ listen or watch strategyand selects the appropriate retrieval tools accordingly. In the step t, we have: at, argst = πplan(q, Mt), (1) where the agent maintains comprehensive contextual memory Mt, aggregating the initial query, sequential observations, and accumulated background information to facilitate robust multi-step reasoning. argst is the corresponding action parameter of at, such as the sub-question for TAQ or TVGA and the video clip start time & end time for TVCA. Action & Observation The selected tool by the agent is executed on the corresponding modality streams: ot = Execute(at, argst, V, A), (2) where ot is the output of tools with text response or timestamp. The model will update the perception of the entire audio and video based on the initial thought derived from the observation of both modalities. And the memory is updated: Mt+1 = Mt {(at, ot)}. Reflection & Rethinking. Before the next iteration, the agent critically assesses all the acquired evidence. It determines the efficacy of the executed tool by synthesizing current outputs with historical context, using this data to refine the execution plan dynamically. Crucially, the module executes cross-modal consistency check to identify potential discrepancies between visual and auditory signals. Consequently, if the agent determines that the accumulated Figure 4. Visualization of the responses and underlying reasoning processes generated by our OmniAgent and Gemini2.5-Flash to an audio-video understanding question. ditory context. Finally, the Audio QA: TAQ tool empowers the agent to formulate targeted inquiries, extracting specific acoustic details required for comprehensive understanding. (cid:229) Event Perception Tools (TE). Video understanding faces significant hurdles due to the computational prohibitiveness of high-frame-rate sampling over long sequences, rendering precise event localization persistent challenge. We identify the audio modality as pivotal opportunity to address this bottleneck; unlike video, audio captures global context and event semantics with significantly fewer computational tokens. Leveraging this efficiency, we propose an audioguided event localization algorithm. This approach exploits the audio signals to rapidly identify, bypassing the need for computationally expensive exhaustive visual scanning. Specifically, the Event List: TEL tool processes the entire audio stream to extract discrete list of detectable sound events, enabling the agent to discern the global semantic context. Complementarily, Event Location: TELO accepts specific queries to return precise timestamps. This serves as an effective temporal proposal mechanism, allowing the agent to pinpoint occurrence times efficiently. 3.3. Agentic Design To fully exploit the intrinsic reasoning and planning capabilities of the large language models (LLMs), we eschew rigid, handcrafted workflows or prescriptive tool usage patterns. Instead, we formulate an iterative Think-Act-Observe5 OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding Method Modality AV Event Alignment Comparative Context Understanding Event Sequence Inference Reasoning 30s Subset 60s Subset Avg GPT-4o Qwen3-VL-Plus Gemini 2.0-Flash Gemini 2.5-Flash Unified-IO-2 XXL-8B VideoLLaMA2-7B Qwen2.5-Omni-7B Ola-7B Qwen3-Omni-30B DVD Daily-Omni XGC-AVis OmniAgent (Ours) A+V A+V A+V A+V A+V A+V A+V A+V A+V A+V 47.90 51.68 62.18 - 25.63 35.71 44.12 40.34 61.90 49.32 51.68 63.50 80. Closed-source Models 52.33 61.66 63.73 - Open-source Models 26.42 35.75 38.86 40.41 69.47 Agent-based Methods 57.12 60.10 68.40 80. 52.61 66.99 63.72 - 25.82 31.70 40.52 43.46 65.32 58.45 53.92 64.40 81.05 62.60 77.10 73.28 - 31.30 35.88 51.15 61.07 79.25 57.47 68.70 77.10 83. 66.23 71.43 76.62 - 35.06 40.91 57.79 63.64 82.67 70.37 78.57 85.10 83.36 66.29 68.57 75.43 - 29.71 34.29 61.71 69.71 85.92 63.24 71.43 82.30 86. 55.64 63.68 67.23 - 26.74 38.02 46.68 51.47 71.28 56.31 63.99 71.60 80.37 57.45 66.55 68.55 - 30.00 31.82 48.36 49.82 74.29 62.41 59.27 71.50 85. 56.47 65.00 67.84 72.70 28.24 35.17 47.45 50.71 72.08 59.22 61.82 71.50 82.71 Table 1. Comparison of different models on the Daily-Omni benchmark. The best result among token pruning methods for each metric is in bold, and the second-best is underlined. denotes the incorporation of audio modalities, whereas indicates reliance on visual modalities extracted from video inputs. The symbol signifies that the model employs Chain-of-Thought (CoT) reasoning or extended inference before generating an answer. multimodal evidence is insufficient to resolve the query, or if cross-modal discrepancy arises necessitating further exploration, it reinitiates the thinking cycle. This iterative loop persists until the ANSWER operation is explicitly invoked, at which point the system synthesizes final response and summary addressing the original user query. Notably, while OmniAgent predominantly leverage audio cues for event localization, our framework retains the capability to execute precise visual event localization when necessary. Figure 4 illustrates the our reasoning process compared to Gemini2.5-Flash. We equip the agent with an evidence-based, reflective, and flexible action execution mechanism. Mirroring human cognitive processes, the system selectively extracts multimodal information and reasons about modal perception subqueries. Consequently, this approach circumvents the computational complexity associated with rigid, dense cross-modal alignment. Through this iterative reasoning and decision process, OmniAgent achieves fine-grained cross-modal understanding by synthesizing perceptions from both modalities, ultimately delivering superior accuracy in response to the given question. 4. Experimental Results 4.1. Experimental Settings Benchmarks. We evaluate our method on three widely-used audio-video understanding benchmarks: Daily-Omni (Zhou et al., 2025), OmniVideoBench (Li et al., 2025a), and WorldSense (Hong et al., 2025). Daily-Omni primarily evaluates performance on short-form video segments with durations of 30s and 60s, whereas OmniVideoBench comprehensively Method Modality (0,1] min (1,5] min (5,10] min (10,30] min Avg. Qwen3-VL-Plus Gemini-2.0-Flash Gemini-2.5-Flash Qwen2.5-VL-72B VideoLLaMA2-7B Qwen2.5-Omni-7B Baichuan-Omni-1.5 Qwen3-Omni-30B A+V A+V A+V A+V A+V A+V Closed-source Models 36.92 49.40 55.42 45.27 43.15 55.10 Open-source Models 33.13 32.00 41.57 28.92 45. 30.03 28.20 27.41 31.78 37.03 Agent-based Methods 37.87 41.05 47.37 31.88 29.60 25.33 28.38 38.86 30.65 34.87 52.11 24.43 28.29 26.72 32.44 35. 38.93 41.50 52.40 29.50 29.20 29.30 30.70 38.40 OmniAgent (Ours) A+V 66.08 58. 59.03 55.64 59.10 Table 2. Comparison of different models on the OmniVideoBench. The best result among token pruning methods for each metric is in bold, and the second-best is underlined. denotes the incorporation of audio modalities, whereas indicates reliance on visual modalities extracted from video inputs. The symbol signifies that the model employs reasoning or extended inference before generating an answer. assesses audio-visual understanding capabilities in longform videos. Complementarily, WorldSense gauges multimodal comprehension across eight distinct domains, focusing specifically on medium-length videos. Compared Methods. We compare our agent with opensource MLLMs: VideoLLaMA2 (Cheng et al., 2024), Ola (Liu et al., 2025b), Unified-IO-2 (Lu et al., 2024), Qwen2.5-Omni (Xu et al., 2025a), Qwen2.5-VL (Bai et al., 2025b), Baichuan-Omni-1.5 (Li et al., 2025b), videoSALMONN 2 (Sun et al., 2024; Tang et al., 2025), Qwen3VL (Bai et al., 2025a) and the state-of-the-art model Qwen3Omni (Xu et al., 2025b). And various closed-source MLLMs: Gemini2.5-Flash (Comanici et al., 2025), GPT4o (OpenAI, 2023), Gemini2.0-Flash, and OpenAI o3. In addition, we also compare with the SOTA agent-based audiovideo understanding framework: Daily-Omni (Zhou et al., 6 OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding Method #Params Modality Tech & Science Culture & Politics Daily Life Film & TV Performance Games Sports Music Avg. (cid:25) GPT-4o (cid:25) Gemini 1.5 Pro (cid:25) Gemini 2.5 Flash Qwen2.5-Omni video-SALMONN 2+ Qwen3-Omni OmniAgent - - - 7B 72B 30B - A+V A+V A+V A+V A+V A+V 48.0 53.7 55. 47.8 59.0 - 64.3 44.0 47.2 48.2 49.8 63.1 - 66.3 38.3 50.3 53. 43.6 54.0 - 59.4 43.5 50.4 48.8 43.8 59.9 - 63.1 41.9 52.4 56. 48.3 58.1 - 62.2 41.2 46.8 47.2 39.1 54.1 - 59.2 42.6 40.2 46. 43.5 51.9 - 55.8 42.7 42.0 50.0 47.3 54.4 - 60.3 42.6 48.0 50. 45.4 56.5 54.0 61.2 Table 3. Comparison of different models on the WorldSense. We compare our agent-based ( ) method against various baselines, including closed-source ((cid:25) ) and open-source ( ) models. The best result among token pruning methods for each metric is in bold. Figure 5. Analysis of the behavior of OmniAgent with different core LLM models. We quantified tool utilization patterns by calculating both the proportion of invocations (call ratio) and the average number of reasoning steps per call. In the resulting visualization, the sector angle represents the tool call ratio, and the magnitude of the radius denotes the specific execution steps at which the tool was invoked. 2025) and Xgc-avis (Cao et al., 2025). These approaches are predicated on static or semi-rigid agent workflows to varying degrees, standing in distinct contrast to the dynamic adaptability of our proposed framework. We also compare with the video understanding agent DVD (Zhang et al., 2025). Implementation Details. For the core of the agent, we will use OpenAI o3 as the brain because of its excellent reasoning capabilities. We restrict the maximum number of iteration steps to 30. Regarding the component modules, we employ Qwen3-VL as the backbone for video perception and utilize Qwen3-Omni for audio global caption and ASR. Additionally, we select Gemini-2.5-Flash for the event perception tool and TAQ, capitalizing on its superior temporal grounding capabilities. To streamline execution, we explicitly disable the internal Chain-of-Thought (CoT) reasoning mechanism for all tools. For the DVD (Zhang et al., 2025), we used the official code for evaluation and set the FPS to 5 for fair comparison. For our tools TVGA and TVCA, we set the video sampling FPS to 2 and 5, respectively. 4.2. Comparison with SoTA Models Table 1 presents the results on the Daily-Omni Benchmark. Specifically, OmniAgent substantially outperforms both proprietary baselines, such as Gemini-2.5-Flash-Thinking (72.7%), and state-of-the-art open-source models like Qwen3-Omni (72.08%), achieving remarkable overall accuracy of 82.71%. This result validates that our agentic framework, by effectively synergizing specialized unimodal capabilities, circumvents the inherent challenges of rigid cross-modal alignment. Furthermore, it demonstrates the efficacy of audio guidance in enhancing fine-grained audio-visual understanding. Relative to competing agentbased architectures, OmniAgent yields performance gains of 10%-20%, underscoring the critical value of its selfplanning, self-reflection, and inquiry mechanisms. For long video evaluation and for more difficult questions, Table 2 presents the results on the OmniVideoBench. The enhancement of OmniAgent compared to Qwen3-Omni30B is notably significant, achieving an overall accuracy rate of 59.1%. This performance substantially surpasses that of other open-source and closed-source end-to-end models, thereby further validating the efficacy of our agent algorithm. Figure 4 illustrates the comparative inference capabilities of OmniAgent against Gemini2.5-Flash, demonstrating that OmniAgent effectively resolves complex queries by leveraging active cross-modal reasoning. For medium-length video in the WorldSense benchmark, as shown in Table 3, it can also reflect the leading position of our OmniAgent. 7 OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding Method Input Token Cost Visual Only Average Latency On Daily-Omni Benchmark DVD OmniAgent (Ours) 18.6k 8.3k 104s 71s Table 4. Comparison of token consumption and latency with DVD in the Daily-Omni benchmark. For fair comparison, in this experiment, we use Qwen3-VL for the caption generation in DVD. 4.3. Analyses on Reasoning Behaviors The core LLM serves as the central reasoning engine within our OmniAgent. It autonomously synthesizes multimodal information to dynamically orchestrate tool execution. To elucidate these mechanisms, we conducted quantitative analysis of tool invocation patterns across various LLMs on the Daily-Omni Benchmark (Zhou et al., 2025). Specifically, we measured both the distribution of tool calls and the associated average reasoning steps, as illustrated in Figure 5. Finding 1. Across all LLM backbones, we observe consistent strategic pattern: agents prioritize TAGC (AGC) in the initial phase to establish global contextual background. Subsequently, the reasoning process culminates with TVCA (VCA), employed to extract the precise, fine-grained evidence necessary for the final response. This distinct sequential progressionfrom global audio context to localized visual verificationempirically validates the efficacy of our algorithmic design and the utility of the provided toolset. Finding 2. The behavior of using OpenAI o3 as the LLM in our agent design aligns precisely with the core objective of cross-modal fine-grained understanding. We observe that the model preferentially utilizes granular toolsspecifically TVCA and TAQduring the final resolution phase, while strategically deploying TELO for event localization during the intermediate reasoning stages. Conversely, computationally intensive tools (TELO and TVGA) provide macro-level insights, as they are unable to provide granular, fine-grained details. This progression effectively exemplifies the coarseto-fine cognitive flow orchestrated by our agentic algorithm. Finding 3. OpenAI o4-mini and GPT-4o exhibit propensity for rapid convergence, frequently bypassing deeper reflection and iterative inspection phases. This tendency is particularly pronounced in GPT-4o, which demonstrates an excessive reliance on coarse-grained TVGA outputs. Consequently, it fails to interrogate fine-grained visual details, resulting in low accuracy. Similarly, o4-mini displays significant modality bias, disproportionately prioritizing visual information while neglecting the exploration of audio. Insight. GPT-4o yields suboptimal performance, largely attributable to premature convergence on coarse-grained evidence. In contrast, o3 demonstrates more deliberative process, effectively leveraging both modalities to unearth fine-grained details. This disparity underscores the critical necessity of the from thinking to reflection cycle. Fur8 thermore, our findings suggest that agentic systems must actively mitigate modal biases and strive for cross-modal consensus. Design protocols should prevent dominant visual signals from overshadowing critical auditory cues, thereby ensuring holistic and accurate understanding of video. 4.4. Efficiency Analyses As detailed in Table 4, comparison with the DVD (Zhang et al., 2025) baseline demonstrates that our method substantially reduces visual token redundancy. Moreover, despite the computational overhead incurred by processing audio signals, OmniAgent achieves significant reduction in inference latency. Notably, our design prioritizes fine-grained understanding over inference speed, and this work aims to propose new paradigm for agent-based algorithms to solve audio-video understanding. We will work on more efficient improvements later. 5. Conclusion We introduce OmniAgent, an audio-driven active perception agent tailored for audio-visual understanding. Operating via recursive Think-Act-Observe-Reflect loop, the system actively orchestrates tools to progressively accumulate multimodal evidence, facilitating fine-grained comprehension. Departing from conventional static workflows, we integrate novel audio-driven event localization mechanism. This enables the model to autonomously select query-relevant information across modalities, thereby addressing the challenges of cross-modal alignment and fine-grained understanding. Experimental evaluations across diverse benchmarks demonstrate that OmniAgent significantly outperforms existing open-source and closed-source OmniLLMs. Discussion. To the best of our knowledge, this work represents the first exploration of active perception agent technology for omnimodal understanding. However, OmniAgents extensibility reaches beyond this scope; it can integrate additional modality-specific tools, such as image OCR or sensor interfaces, to achieve holistic multimodal perception. While the current reliance on external models and extended contexts improves performance, it constrains reasoning efficiency. To address this, in the future, we envision training an omnimodal agentic model. This architecture will ingest diverse modal inputs and feature tool self-calling, enabling the system to actively decide how to attend to specific audio or visual. By explicitly retaining memory within the KV cache, this approach addresses the bottleneck of inference latency. Thus, this work constitutes pivotal bridge facilitating the advancement of omnimodal agentic algorithms. OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding"
        },
        {
            "title": "Release Notes",
            "content": "v1: Technical Report. This version presents the idea, method, major experimental results, and discussions."
        },
        {
            "title": "References",
            "content": "AI, I., Gong, B., Zou, C., Zheng, C., Zhou, C., Yan, C., Jin, C., Shen, C., Zheng, D., Wang, F., et al. Ming-omni: unified multimodal model for perception and generation. arXiv preprint arXiv:2506.09344, 2025. Bai, S., Cai, Y., Chen, R., Chen, K., Chen, X., Cheng, Z., Deng, L., Ding, W., Gao, C., Ge, C., et al. Qwen3vl technical report. arXiv preprint arXiv:2511.21631, 2025a. Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025b. Cao, Y., Min, X., Gao, Y., Sun, W., Zhang, Z., Han, J., and Zhai, G. Xgc-avis: Towards audio-visual content understanding with multi-agent collaborative system. arXiv preprint arXiv:2509.23251, 2025. Cheng, Z., Leng, S., Zhang, H., Xin, Y., Li, X., Chen, G., Zhu, Y., Zhang, W., Luo, Z., Zhao, D., and Bing, L. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. URL https:// arxiv.org/abs/2406.07476. Chowdhury, S., Nag, S., Dasgupta, S., Chen, J., Elhoseiny, M., Gao, R., and Manocha, D. Meerkat: Audio-visual large language model for grounding in space and time. In ECCV, 2024. Chowdhury, S., Elmoghany, M., Abeysinghe, Y., Fei, J., Nag, S., Khan, S., Elhoseiny, M., and Manocha, D. Magnet: multi-agent framework for finding audio-visual needles by reasoning over multi-video haystacks. arXiv preprint arXiv:2506.07016, 2025. Chu, Y., Xu, J., Zhou, X., Yang, Q., Zhang, S., Yan, Z., Zhou, C., and Zhou, J. Qwen-audio: Advancing universal audio understanding via unified large-scale audioarXiv preprint arXiv:2311.07919, language models. 2023. Comanici, G., Bieber, E., Schaekermann, M., Pasupat, I., Sachdeva, N., Dhillon, I., Blistein, M., Ram, O., Zhang, D., Rosen, E., et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Ding, D., Ju, Z., Leng, Y., Liu, S., Liu, T., Shang, Z., Shen, K., Song, W., Tan, X., Tang, H., et al. Kimi-audio technical report. arXiv preprint arXiv:2504.18425, 2025. Fan, B., Liu, L., Li, X., Zhang, R., Jin, L., and Zhang, J. Fine-grained audiovisual event localization. IEEE Transactions on Neural Networks and Learning Systems, 2025. Fan, Y., Ma, X., Wu, R., Du, Y., Li, J., Gao, Z., and Li, Q. Videoagent: memory-augmented multimodal agent for video understanding. In ECCV, 2024. Fu, C., Lin, H., Long, Z., Shen, Y., Dai, Y., Zhao, M., Zhang, Y.-F., Dong, S., Li, Y., Wang, X., et al. Vita: Towards open-source interactive omni multimodal llm. arXiv preprint arXiv:2408.05211, 2024. Galougah, S. S., Raj, R., Chowdhury, S., Nag, S., and Duraiswami, R. Aura: fine-grained benchmark and decomposed metric for audio-visual reasoning. arXiv preprint arXiv:2508.07470, 2025. Gao, H., Bao, Y., Tu, X., Xu, Y., Jin, Y., Mu, Y., Zhong, B., Yue, L., and Zhang, M.-L. Agentic video intelligence: flexible framework for advanced video exploration and understanding. arXiv preprint arXiv:2511.14446, 2025. Ge, Y., Ge, Y., Li, C., Wang, T., Pu, J., Li, Y., Qiu, L., Ma, J., Duan, L., Zuo, X., et al. Arc-hunyuan-video7b: Structured video comprehension of real-world shorts. arXiv preprint arXiv:2507.20939, 2025. Guo, Y., Ma, S., Ma, S., Bao, X., Xie, C.-W., Zheng, K., Weng, T., Sun, S., Zheng, Y., and Zou, W. Aligned better, listen better for audio-visual large language models. In ICLR, 2025. Hong, J., Yan, S., Cai, J., Jiang, X., Hu, Y., and Xie, W. Worldsense: Evaluating real-world omnimodal arXiv preprint understanding for multimodal llms. arXiv:2502.04326, 2025. Jeoung, S., Huybrechts, G., Ganesh, B., Galstyan, A., and Bodapati, S. Adaptive video understanding agent: Enhancing efficiency with dynamic frame sampling and feedback-driven reasoning. arXiv preprint arXiv:2410.20252, 2024. Jiang, S., Liang, J., Wang, J., Dong, X., Chang, H., Yu, W., Du, J., Liu, M., and Qin, B. From specific-mllms to omnimllms: survey on mllms aligned with multi-modalities. In Findings of ACL, 2025. Kahatapitiya, K., Ranasinghe, K., Park, J., and Ryoo, M. S. Language repository for long video understanding. In ACL, 2025. 9 OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding Kugo, N., Li, X., Li, Z., Gupta, A., Khatua, A., Jain, N., Patel, C., Kyuragi, Y., Ishii, Y., Tanabiki, M., et al. Videomultiagents: multi-agent framework for video question answering. arXiv preprint arXiv:2504.20091, 2025. Li, C., Chen, Y., Ji, Y., Xu, J., Cui, Z., Li, S., Zhang, Y., Tang, J., Song, Z., Zhang, D., et al. Omnivideobench: Towards audio-visual understanding evaluation for omni mllms. arXiv preprint arXiv:2510.10689, 2025a. Li, Y., Sun, H., Lin, M., Li, T., Dong, G., Zhang, T., Ding, B., Song, W., Cheng, Z., Huo, Y., Chen, S., Li, X., Pan, D., Zhang, S., Wu, X., Liang, Z., Liu, J., Zhang, T., Lu, K., Zhao, Y., Shen, Y., Yang, F., Yu, K., Lin, T., Xu, J., Zhou, Z., and Chen, W. Baichuan-omni technical report. arXiv preprint arXiv:2410.08565, 2024. Li, Y., Liu, J., Zhang, T., Chen, S., Li, T., Li, Z., Liu, L., Ming, L., Dong, G., Pan, D., et al. Baichuan-omni1.5 technical report. arXiv preprint arXiv:2501.15368, 2025b. Liu, Y., Lin, K. Q., Chen, C. W., and Shou, M. Z. Videomind: chain-of-lora agent for long video reasoning. arXiv preprint arXiv:2503.13444, 2025a. Liu, Z., Dong, Y., Wang, J., Liu, Z., Hu, W., Lu, J., and Rao, Y. Ola: Pushing the frontiers of omni-modal language model with progressive modality alignment. arXiv preprint arXiv:2502.04328, 2025b. Lu, J., Clark, C., Lee, S., Zhang, Z., Khosla, S., Marten, R., Hoiem, D., and Kembhavi, A. Unified-io 2: Scaling autoregressive multimodal models with vision language audio and action. In CVPR, 2024. Ma, Z., Gou, C., Shi, H., Sun, B., Li, S., Rezatofighi, H., and Cai, J. Drvideo: Document retrieval based long video understanding. In CVPR, 2025. Min, J., Buch, S., Nagrani, A., Cho, M., and Schmid, C. Morevqa: Exploring modular reasoning models for video question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1323513245, 2024. OpenAI. Gpt-4 model, 2023. URL https://platform. openai.com. Accessed: 2023-11-08. Pang, Z. and Wang, Y.-X. Mr. video: mapreduce is the principle for long video understanding. arXiv preprint arXiv:2504.16082, 2025. Park, J., Ranasinghe, K., Kahatapitiya, K., Ryu, W., Kim, D., and Ryoo, M. S. Too many frames, not all useful: Efficient strategies for long-form video qa. arXiv preprint arXiv:2406.09396, 2024. Shu, F., Zhang, L., Jiang, H., and Xie, C. Audio-visual llm for video understanding. In CVPR, 2025. Sun, G., Yu, W., Tang, C., Chen, X., Tan, T., Li, W., Lu, L., Ma, Z., and Zhang, C. Fine-grained audio-visual joint representations for multimodal large language models. arXiv preprint arXiv:2310.05863, 2023. Sun, G., Yu, W., Tang, C., Chen, X., Tan, T., Li, W., Lu, L., Ma, Z., Wang, Y., and Zhang, C. video-salmonn: Speech-enhanced audio-visual large language models. arXiv preprint arXiv:2406.15704, 2024. Tang, C., Li, Y., Yang, Y., Zhuang, J., Sun, G., Li, W., Ma, Z., and Zhang, C. video-salmonn 2: Captioningenhanced audio-visual large language models. arXiv preprint arXiv:2506.15220, 2025. Tao, K., Shao, K., Yu, B., Wang, W., Wang, H., et al. Omnizip: Audio-guided dynamic token compression for fast omnimodal large language models. arXiv preprint arXiv:2511.14582, 2025. Team, K., Du, A., Yin, B., Xing, B., Qu, B., Wang, B., Chen, C., Zhang, C., Du, C., Wei, C., et al. Kimi-vl technical report. arXiv preprint arXiv:2504.07491, 2025. Tong, W., Guo, H., Ran, D., Chen, J., Lu, J., Wang, K., Li, K., Zhu, X., Li, J., Li, K., et al. Interactiveomni: unified omni-modal model for audio-visual multi-turn dialogue. arXiv preprint arXiv:2510.13747, 2025. Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, X., Wang, J., Ge, W., et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024a. Wang, W., Gao, Z., Gu, L., Pu, H., Cui, L., Wei, X., Liu, Z., Jing, L., Ye, S., Shao, J., et al. Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025a. Wang, X., Zhang, Y., Zohar, O., and Yeung-Levy, S. Videoagent: Long-form video understanding with large language model as agent. In ECCV, 2024b. Wang, Z., Chen, B., Yue, Z., Wang, Y., Qiao, Y., Wang, L., and Wang, Y. Videochat-a1: Thinking with long arXiv preprint videos by chain-of-shot reasoning. arXiv:2506.06097, 2025b. Wang, Z., Yu, S., Stengel-Eskin, E., Yoon, J., Cheng, F., Bertasius, G., and Bansal, M. Videotree: Adaptive treebased video representation for llm reasoning on long videos. In CVPR, 2025c. Wang, Z., Zhou, H., Wang, S., Li, J., Xiong, C., Savarese, S., Bansal, M., Ryoo, M. S., and Niebles, J. C. Active video 10 OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding perception: Iterative evidence seeking for agentic long video understanding. arXiv preprint arXiv:2512.05774, 2025d. Zhang, Y., Wu, J., Li, W., Li, B., Ma, Z., Liu, Z., and Li, C. Video instruction tuning with synthetic data, 2024c. URL https://arxiv.org/abs/2410.02713. Xie, Z. and Wu, C. Mini-omni2: Towards open-source gpt-4o with vision, speech and duplex capabilities. arXiv preprint arXiv:2410.11190, 2024. Zhou, Z., Wang, R., and Wu, Z. Daily-omni: Towards audio-visual reasoning with temporal alignment across modalities. arXiv preprint arXiv:2505.17862, 2025. Zhu, M., Zhong, H., Zhao, C., Du, Z., Huang, Z., Liu, M., Chen, H., Zou, C., Chen, J., Yang, M., et al. Active-o3: Empowering multimodal large language modarXiv preprint els with active perception via grpo. arXiv:2505.21457, 2025. Xu, J., Guo, Z., He, J., Hu, H., He, T., Bai, S., Chen, K., Wang, J., Fan, Y., Dang, K., et al. Qwen2. 5-omni technical report. arXiv preprint arXiv:2503.20215, 2025a. Xu, J., Guo, Z., Hu, H., Chu, Y., Wang, X., He, J., Wang, Y., Shi, X., He, T., Zhu, X., et al. Qwen3-omni technical report. arXiv preprint arXiv:2509.17765, 2025b. Yang, H., Tang, F., Zhao, L., An, X., Hu, M., Li, H., Zhuang, X., Lu, Y., Zhang, X., Swikir, A., et al. Streamagent: Towards anticipatory agents for streaming video understanding. arXiv preprint arXiv:2508.01875, 2025a. Yang, Q., Yao, S., Chen, W., Fu, S., Bai, D., Zhao, J., Sun, B., Yin, B., Wei, X., and Zhou, J. Humanomniv2: From understanding to omni-modal reasoning with context. arXiv preprint arXiv:2506.21277, 2025b. Yang, Y., Zhuang, J., Sun, G., Tang, C., Li, Y., Li, P., Jiang, Y., Li, W., Ma, Z., and Zhang, C. Audio-centric video understanding benchmark without text shortcut. In EMNLP, 2025c. Yang, Z., Chen, D., Yu, X., Shen, M., and Gan, C. Vca: Video curious agent for long video understanding. In CVPR, 2025d. Yin, Y., Meng, Q., Chen, M., Ding, J., Shao, Z., and Yu, Z. Videoarm: Agentic reasoning over hierarchical memory for long-form video understanding. arXiv preprint arXiv:2512.12360, 2025. Yuan, H., Liu, Z., Zhou, J., Wen, J.-R., and Dou, Z. Videodeepresearch: Long video understanding with agentic tool using. arXiv preprint arXiv:2506.10821, 2025. Zhang, C., Lu, T., Islam, M. M., Wang, Z., Yu, S., Bansal, M., and Bertasius, G. simple llm framework for longrange video question-answering. In EMNLP, 2024a. Zhang, L., Zhao, T., Ying, H., Ma, Y., and Lee, K. Omagent: multi-modal agent framework for complex video understanding with task divide-and-conquer. arXiv preprint arXiv:2406.16620, 2024b. Zhang, X., Jia, Z., Guo, Z., Li, J., Li, B., Li, H., and Lu, Y. Deep video discovery: Agentic search with tool use for long-form video understanding. arXiv preprint arXiv:2505.18079, 2025."
        }
    ],
    "affiliations": [
        "Ant Group",
        "Westlake University",
        "Zhejiang University"
    ]
}