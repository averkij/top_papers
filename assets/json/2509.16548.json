{
    "paper_title": "SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward Learning",
    "authors": [
        "Yuyang Ding",
        "Xinyu Shi",
        "Juntao Li",
        "Xiaobo Liang",
        "Zhaopeng Tu",
        "Min Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Process reward models (PRMs) offer fine-grained, step-level evaluations that facilitate deeper reasoning processes in large language models (LLMs), proving effective in complex tasks like mathematical reasoning. However, developing PRMs is challenging due to the high cost and limited scalability of human-annotated data. Synthetic data from Monte Carlo (MC) estimation is a promising alternative but suffers from a high noise ratio, which can cause overfitting and hinder large-scale training. In this work, we conduct a preliminary study on the noise distribution in synthetic data from MC estimation, identifying that annotation models tend to both underestimate and overestimate step correctness due to limitations in their annotation capabilities. Building on these insights, we propose Self-Denoising Monte Carlo Annotation (SCAN), an efficient data synthesis and noise-tolerant learning framework. Our key findings indicate that: (1) Even lightweight models (e.g., 1.5B parameters) can produce high-quality annotations through a self-denoising strategy, enabling PRMs to achieve superior performance with only 6% the inference cost required by vanilla MC estimation. (2) With our robust learning strategy, PRMs can effectively learn from this weak supervision, achieving a 39.2 F1 score improvement (from 19.9 to 59.1) in ProcessBench. Despite using only a compact synthetic dataset, our models surpass strong baselines, including those trained on large-scale human-annotated datasets such as PRM800K. Furthermore, performance continues to improve as we scale up the synthetic data, highlighting the potential of SCAN for scalable, cost-efficient, and robust PRM training."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 8 4 5 6 1 . 9 0 5 2 : r SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward Learning Yuyang Ding1, Xinyu Shi1, Juntao Li1, Xiaobo Liang1, Zhaopeng Tu2, Min Zhang1 1Soochow University 2Tencent {yyding23,xyshi02}@stu.suda.edu.cn {ljt,xbliang,minzhang}@suda.edu.cn zptu@tencent.com Project Page: https://scan-prm.github.io"
        },
        {
            "title": "Abstract",
            "content": "Process reward models (PRMs) offer fine-grained, step-level evaluations that facilitate deeper reasoning processes in large language models (LLMs), proving effective in complex tasks like mathematical reasoning. However, developing PRMs is challenging due to the high cost and limited scalability of human-annotated data. Synthetic data from Monte Carlo (MC) estimation is promising alternative but suffers from high noise ratio, which can cause overfitting and hinder large-scale training. In this work, we conduct preliminary study on the noise distribution in synthetic data from MC estimation, identifying that annotation models tend to both underestimate and overestimate step correctness due to limitations in their annotation capabilities. Building on these insights, we propose Self-Denoising Monte Carlo Annotation (SCAN), an efficient data synthesis and noise-tolerant learning framework. Our key findings indicate that: (1) Even lightweight models (e.g., 1.5B parameters) can produce high-quality annotations through self-denoising strategy, enabling PRMs to achieve superior performance with only 6% the inference cost required by vanilla MC estimation. (2) With our robust learning strategy, PRMs can effectively learn from this weak supervision, achieving 39.2 F1 score improvement (from 19.9 to 59.1) in ProcessBench. Despite using only compact synthetic dataset, our models surpass strong baselines, including those trained on large-scale human-annotated datasets such as PRM800K. Furthermore, performance continues to improve as we scale up the synthetic data, highlighting the potential of SCAN for scalable, cost-efficient, and robust PRM training."
        },
        {
            "title": "Introduction",
            "content": "The recent advent of large language models (LLMs) such as OpenAI o1 [1, 13] and DeepSeek R1 [9] has sparked significant interest in scaling test-time compute to encourage slower and deeper reasoning processes. In this context, process reward models [17, 35] have emerged as promising approach, offering fine-grained, step-level evaluations that facilitate iterative self-refinement [29] and exploration of solution spaces [18]. This proves to be particularly effective in tackling complex problems like mathematical reasoning tasks. However, critical challenge for developing process reward models (PRMs) lies in data annotation. While human-annotated methods [17] can produce high-quality data that effectively guides PRM training, they come at high cost. To address this, many works [35, 18, 26] have explored synthetic data generation via Monte Carlo estimation. However, synthetic data still falls short of matching the quality of human-annotated data, as explored by Zheng et al. [44]. The primary challenge stems Corresponding author 39th Conference on Neural Information Processing Systems (NeurIPS 2025). from the high noise ratio inherent in Monte Carlo-generated data, as models tend to quickly overfit noisy samples, hindering effective training at larger data scales. Recent studies [30, 28, 43] have demonstrated that introducing stronger supervision from large-scale critic models (e.g., Qwen-72B) is an effective strategy for mitigating noise by retaining only consensus samples agreed upon by both the critic model and Monte Carlo estimation [5, 14]. However, how noise is distributed and how In this paper, we to train PRMs robustly in the presence of such noise are still underexplored. investigate the full potential of denoising in MC estimation itself and robust learning in PRMs, without relying on any external stronger supervision. To the best of our knowledge, we are the first to systematically explore Process Reward Learning from the perspective of noise distribution and robust learning. We begin with preliminary study to investigate the noise distribution present in synthetic data generated through Monte Carlo estimation. Our findings indicate that this noise arises mainly from two factors: the annotation model tends to under-estimate and over-estimate step correctness, primarily due to inherent limitations in its annotation capabilities. To address this issue, we introduce self-confidence metric designed to assess the reliability of model-generated annotations. Guided by these insights, we propose strategies to mitigate two distinct types of noise: noisy positive samples and inaccurate negative samples. We develop selective sampling approach to reduce overall sample noise and design model-wise self-denoising loss for robust learning. By leveraging the self-confidence metric, we systematically reduce the annotation models bias and enhance the overall data quality. Furthermore, to improve data synthesis efficiency, we selectively apply Monte Carlo annotation to the most informative samples, optimizing both accuracy and computational resources. Using weak supervision from lightweight annotation model, Qwen2.5-Math-1.5B-Instruct [39], we construct synthetic dataset consisting of 101K samples. We evaluate the trained PRM from two perspectives: test-time scaling (best-of-8 evaluation) and step-wise error detection (ProcessBench [44]). The results demonstrate that our model consistently outperforms strong existing baselines, achieving performance comparable to that of the human-annotated dataset PRM800K. Further scaling with an additional 97K synthetic samples generated by Llama3.2-3B-Instruct and Qwen2.5-Math-7B-Instruct leads to our PRM surpassing the performance of the human-annotated PRM800K on both evaluation benchmarks. As the dataset size continues to grow, we observe further scaling potential."
        },
        {
            "title": "2 Preliminary: Unveiling the Noise Distribution in Monte Carlo Annotaion",
            "content": "2.1 Problem Definition Monte Carlo Estimation In the context of process annotation, Monte Carlo estimation [35] is proposed as an automated approach for evaluating the correctness of each step. Formally, given question and corresponding response containing steps, i.e., = [x1, x2, . . . , xn], the correctness score of the tth step, denoted as ct, can be estimated by completer model through multiple rollouts or simulations. Specifically, the completer model is prompted to sample completions based on the question and prefix steps xt = [x1, . . . , xt] until terminate state is reached. ct = ErPθ(q,xt)[J (r, a)], ˆct = 1 (cid:88) i=1 (r(i), a), (1) where (r, a) is an indicator function that equals 1 if the sampled response matches the ground truth answer a, and 0 otherwise. PRM Training After obtaining the correctness score ct for each step, we can train the PRM using binary classification loss: LBCE(θ) = E(xt,yt)Dtrain [yt log(Pθ(ytq, xt)) + (1 yt) log(1 Pθ(ytq, xt))], where yt represents the correctness label of for the tth step. We use hard labels to annotate yt, defined as yt = 1[ct > 0], assigning yt = 1 for any positive correctness score and yt = 0 otherwise. We consider step correct if at least one rollout leads to the correct final answer. (2) Noise issue in MC estimation significant effectiveness gap exists between synthetic data and human-annotated data [44] for training PRMs. This discrepancy can be attributed to the noise label 2 Figure 1: Noise distribution analysis of Llama3.1-8B-Instruct and Qwen2.5-Math-7B-Instruct. Left: Overall distribution of noise samples across varying self-confidence levels. Middle: Noise distribution in predicted positive samples where tpred = inf. Right: Distance distribution between tpred and ttrue for inaccurate negative samples. Additional results of more models can be found in Figure 4. issue inherent in Monte Carlo estimation [18, 42], where the correctness of each reasoning step is often misestimated, primarily due to the limitations of the completer model. More specifically, the correctness of the current step is annotated by the completer model based on whether the completions lead to the correct final answer. As result, the PRM trained on this data only estimates the potential of the completer model to reach the correct final answer from the current step. This differs significantly from the correctness of the current step, as the completer model is not perfect and can make mistakes. This is why the noise phenomenon exists in Monte Carlo estimation. 2.2 Noise Distribution in MC estimation Given question and corresponding response = [x1, x2, . . . , xn], we focus only on the first error step during the training [17] and evaluation [44] of the PRM. We denote the first error location predicted by the completer through Monte Carlo estimation as tpred, while the ground truth error location is ttrue. For cases where the response is entirely correct, we use the label inf (i.e., +) to indicate the absence of any errors. Then we can categorize the mistakes made by the completer model into the following two types (fully correct samples are also taken into account): Under-Estimation (tpred < ttrue): This likely occurs because the completer struggles with complex or nuanced reasoning. Even when provided with correct prefix steps, it may fail to generate correct rollouts, leading to early error detection. Over-Estimation (tpred > ttrue): This happens when the completer initially corrects an error, producing correct rollout. However, as subsequent errors accumulate, the model cannot fully address them, causing delayed detection of the true error location. To quantify this relationship between the noise distribution and the completers problem-solving capability, we propose metric called self-confidence, which measures the confidence level of the completer model on the annotated question: SCθ(q) = ErPθ(q)J (r, a), (3) where (r, a) is an indicator function that evaluates whether the response matches the ground truth answer a. In practice, SCθ(q) is estimated by computing the empirical mean over multiple randomly sampled responses given question q. Experimental Settings We select four representative open-source models, i.e., Llama3.18B-Instruct, Llama3.2-3B-Instruct [6], Qwen2.5-Math-1.5B-Instruct, and Qwen2.5-Math-7BInstruct [39], as the completer models. For the dataset, we use ProcessBench [44], which contains 3 3,400 human-annotated process data points spanning multiple difficulty levels. To compute selfconfidence, we sample 16 completions for each question. For evaluating step correctness, we perform 8 rollouts per step to determine its correctness. Key Observations The left of figure 1 illustrates the overall noise distribution of different models across different self-confidence values.2 We mainly have the following observations: Observation 1: Noisy cases where tpred < ttrue are predominantly concentrated in low selfconfidence regions, supporting the under-estimation hypothesis discussed before. Observation 2: The noise distribution of tpred > ttrue varies across models. For the more capable Qwen model, its stronger error-correction ability results in noise being concentrated in high self-confidence regions. In contrast, noise is more evenly distributed for the Llama model. Observation 3: Clean samples (tpred = ttrue) are primarily located in high self-confidence regions. Overall, high-confidence regions exhibit lower proportion of noise. 2.3 Detailed Analysis of Noise Distribution At finer granularity, we classify noise into two distinct categories: Noisy Positive Samples and Inaccurate Negative Samples. Noisy Positive Samples Samples observed as fully correct positives with tpred = inf, but actually contain errors, i.e., ttrue = inf. These cases indicate that the model fails to detect existing errors, leading to noisy positives in its predictions. For clarity, we categorize self-confidence into three levels: low confidence as [0, 0.25], medium confidence as (0.25, 0.75), and high confidence as [0.75, 1]. These thresholds are empirically determined through the overall noise distribution. The middle of figure 1 illustrates the distribution of noise positive samples, from which we can conclude that: Observation 4: For predicted positive samples (tpred = inf), the noise positive ratio is significantly lower in high self-confidence samples, making them more suitable for training. Inaccurate Negative Samples Samples observed as containing errors (tpred = inf), but the predicted error location is incorrect, i.e., tpred = ttrue. This reflects the models inability to precisely identify the true error location, even when it detects the presence of errors. From Observations 1, 3, 4, we can conclude that the completer model makes fewer mistakes in high self-confidence samples. Therefore, we focus on the high-confidence subset to investigate how noise is distributed in inaccurate negative samples. We visualize the relationship between tpred and ttrue in the right of figure 1, from which we can conclude: Observation 5: In most cases, the model can roughly predict the error location but often lacks precision. Furthermore, the model tends to overestimate the error location, i.e., tpred > ttrue, and the number of noisy samples decreases as the deviation increases. This also supports the over-estimation hypothesis discussed before."
        },
        {
            "title": "3 Methodology",
            "content": "Building on the observations of noise distribution discussed in section 2, we propose targeted approaches to address key challenges in Monte Carlo annotation. Specifically, our method contains two modules: (1) an efficient data synthesis framework to reduce substantial inference costs, and (2) robust training methods to mitigate the high noise ratio in synthetic data and enable robust learning with noisy labels. Figure 2 illustrates the overall workflow of our proposed method. 3.1 Efficient Data Synthesis Framework Generate Responses The synthesis process begins with dataset of questions and their corresponding golden answers, denoted as = {(qi, ai)}M i=1. For each question qi, generator model 2We include fully correct samples, assigning them an error location label of + for convenience. 4 Figure 2: Overview of our data synthesis and robust training framework. parameterized by π1 produces responses, denoted as {ri,1, ri,2, . . . , ri,N }. The confidence score of the generator in qi is then computed as: SCπ1(qi) = 1 (cid:88) j=1 (r(j) , ai), where ri Pθ( qi) (4) where (r(j) , ai) evaluates the correctness of the generated response. Next, we collect the negative samples with tpred = inf from these responses for further step-wise annotation. In this process, we do not select the annotated correct (positive) samples for subsequent annotation. Although these positive samples may contain false positives that could potentially be filtered out through detailed step-wise correctness checks, we find that the annotation cost for this process is prohibitively high. For instance, performing 8 rollouts for each step in 10-step response requires 8 10 = 80 rollouts per sample. Moreover, the positive samples, particularly those in high self-confidence regions, contain minimal noise (from Observation 4). Therefore, we directly use these high-confidence samples as positive examples for training without further annotation. By applying Monte Carlo estimation exclusively to negative samples, we ensure 100% sample utilizationevery sample annotated through Monte Carlo estimation is included in the final training dataset. Step-wise correctness annotation Negative samples with high self-confidence scores, SCπ1 (qi) > ϵ, are selected for step-wise correctness annotation using completer model parameterized by π2. We employ vLLM [15] and implement distributed inference via Ray [22] to accelerate the annotation process. To support the subsequent robust learning process, we also need to collect the self-confidence scores SCπ2 (qi) during annotation. For convenience, we use the same model as the generator and completer models, i.e., π1 = π2 = π, allowing us to reuse the self-confidence scores and further enhance data generation efficiency. 3.2 Robust Learning with Noisy Labels The final annotated dataset, denoted as Df inal, consists of tuples (q, x, c, SCπ(q)), where is the question, = [x1, x2, . . . , xn] represents the n-step responses, and = [c1, c2, . . . , cn] contains the corresponding correctness scores annotated via Monte Carlo (MC) estimation. The term SCπ(q) denotes the self-confidence score of the completer model π for question q. We then train PRMs with the reweighted step label: LSCAN(θ) = E(xt,yt)Dfinal[yt log(Pθ(ytq, xt)) + (1 yt) log(1 Pθ(ytq, xt))] ˆyt = (cid:26)min(ct/SCπ(q), 1), I(ct > 0), pred if te Otherwise , where ct = Pπ(yt = correctq, xt), (5) where te pred denotes the first error location with ct = 0. Compared to the traditional BCE loss, our modifications focus on two main aspects: noise-tolerant labeling and confidence-wise reweighting. 5 Noise-tolerant Labeling The completer model tends to overestimate the correctness of the current step due to its strong self-correction capability. As errors continue to accumulate, the model eventually makes mistakes, leading to tpred > ttrue, with high probability of similar errors occurring at nearby positions (Observation 5). To enable more robust learning with these noisy labels, we propose noise-tolerant labeling strategy that applies soft labels to steps preceding the error, within tolerance distance d. We discuss the choice of in section 4.4. Through experiments, we demonstrate that the strategy allows the PRM to learn more effectively from noisy labels without overfitting. Confidence-wise Reweighting After applying the denoising process, critical issue remains: the annotated labels are still heavily influenced by the capability of the completer model, i.e., = (yi = correctπgold, q, xt), while ci = (yi = correctπθ, q, xt). (6) Here, represents the true correctness probability assigned by an assumed golden annotator πgold, while ci is the estimated probability derived from the completer model πθ. Since we can only estimate ci through rollouts performed by the imperfect completer πθ, the true label remains unobservable. To mitigate the model-dependent bias, we introduce correction factor δi = /ci. We leverage the self-confidence score SCπ(q) to approximate and reduce this model-induced bias. The rationale behind this approach is as follows: consider two models annotating the same samplea strong model with SCstrong(q) and weak model with SCweak(q). Naturally, the stronger model will yield higher correctness score ci. However, we aim for the final corrected scores to be consistent across models, regardless of their inherent strengths. Thus, we adjust the estimated correctness score using the self-confidence score as follows: ˆc = min(ci/SCπ(q), 1). (7) This adjustment helps to normalize the influence of model capability on the annotated labels, leading to more reliable and unbiased training data. This reweighting procedure is particularly effective when integrating annotations from multiple completer models, as demonstrated by the experimental results in section 4.3."
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Training Dataset Construction SCAN-Base (101K) We use 7,500 questions with golden answers from the MATH training set. For each question, we generate responses and perform eight rollouts per step to annotate process correctness via MC estimation. We set tolerance distance to 2, with discussion in section 4.4. SCAN-Base is generated by Qwen2.5-Math-1.5B-Instruct [39] as both the generator and completer. We experiment with varying numbers of responses from {64, 128} and found that larger number of responses provides more accurate estimation of the models self-confidence, which is crucial for denoising and robust learning processes, as elaborated in Appendix B.4. Additionally, we explore other data sources, including GSM8K [2] and Numina-Math [16], and discover that the MATH dataset offers more suitable level of difficulty and higher data quality for training (Appendix B.4). SCAN-Pro (197K) We further incorporate Qwen2.5-Math-7B-Instruct [39] and Llama3.2-3BInstruct [6] for Monte Carlo estimation. With these two models, we generate an additional 97K data points. Integrated with SCAN-Base, we construct mixed dataset containing 197K samples. More dataset details are shown in Appendix A.1. We observe that (1) combining these two datasets increases the diversity of the data, thereby boosting model performance, and (2) the model-wise reweighted loss design mitigates any data inconsistency arising from the capability gap between the annotation models, with experiment results in section 4.4. 4.2 Experimental Setup We evaluate the effectiveness of the Process Reward Model (PRM) from two key perspectives: Best-of-N (BoN) Evaluation In this evaluation, the PRM functions as verifier to select the best response from multiple candidate answers generated by policy model. Specifically, the PRM assigns scores to each step within response and then aggregates these step-wise scores into an overall 6 Table 1: Best-of-8 evaluation results of the policy model Qwen2.5-Math-7B-Instruct. Results of policy model Llama3.1-8B-Instruct can be found in Appendix B.3. To reduce potential errors, we re-evaluated all these models based on the same set of responses. Model Greedy Majority Vote@8 Pass@8 (Upper Bound) UniversalPRM-7B Qwen2.5-Math-PRM-7B RLHFlow-PRM-Mistral-8B RLHFlow-PRM-DeepSeek-8B EurusPRM-Stage1 EurusPRM-Stage2 Math-PSA-7B Qwen2.5-Math-7B-Math-Shep Skywork-PRM-Qwen2.5-7B Qwen2.5-Math-7B-PRM800K Qwen2.5-Math-7B-SCAN-Base Qwen2.5-Math-7B-SCAN-Pro Training Samples Annotation Method GSM8K MATH College Math Olympiad Bench 95.6 96.9 98. 96.8 96.8 97.1 96.8 96.9 97.0 96.4 96.9 97.0 97.0 97.1 97.1 83.6 87.3 92.0 86.9 88.1 87.3 87.3 86.0 86.7 86.0 86.8 87.9 87.6 86.9 87.3 46.9 47.4 52. 47.6 47.7 47.3 47.9 47.4 47.9 47.7 47.6 47.8 47.7 47.8 48.1 40.6 43.0 60.4 48.0 47.6 43.0 43.9 42.8 45.3 45.9 42.3 44.6 45.0 44.4 47.7 690K 1500K 273K 253K 463K 693K 1395K 445K - 264K 101K 197K MC, KD MC, KD MC-Only MC-Only Implicit Implicit MC-Only MC-Only - Human MC-Only MC-Only Avg. 66.7 68.7 75.7 69.8 70. 68.7 69.0 68.3 69.2 69.0 68.3 69.3 69.3 69.1 70.1 Table 2: Evaluation Results on ProcessBench. MC denotes Monte Carlo estimation, while KD represents knowledge distillation from more capable critic models (with 32B or larger parameters). Model GSM8K MATH Olympiad Bench Omni Math Avg. error correct F1 error correct F1 error correct F1 error correct F1 Criric Models (LLM-as-a-judge) GPT-4o-0806 Qwen2.5-Math-7B-Instruct Llama-3.3-70B-Instruct Qwen2.5-72B-Instruct QwQ-32B-Preview 70.0 15.5 72.5 62.8 81.6 91.2 100.0 96.9 96.9 95.3 79.2 26.8 82.9 76.2 88. 54.4 14.8 43.3 46.3 78.1 76.6 96.8 83.2 93.1 79.3 63.6 25.7 59.4 61.8 78.7 45.8 7.7 31.0 38.7 61.4 58.4 91.7 94.1 92.6 54.6 51.4 14.2 46.7 54.6 57. 45.2 6.9 28.2 36.6 55.7 65.6 88.0 90.5 90.9 68.0 53.5 61.9 12.7 19.9 43.0 58.0 52.2 61.2 61.3 71.5 Process Reward Models (MC + KD) UniversalPRM-7B Qwen2.5-Math-PRM-7B - 72. - 96.4 85.8 82.4 - 68.0 - 90.4 77.7 77.6 - 55. - 85.5 67.6 67.5 - 55.2 - 83.0 66.4 74.3 66.3 73.5 7-8B Process Reward Models (w/o KD) 33.8 RLHFlow-PRM-Mistral-8B 24.2 RLHFlow-PRM-Deepseek-8B 46.9 EurusPRM-Stage1 51.2 EurusPRM-Stage2 46.4 Qwen2.5-Math-7B-Math-Shep Skywork-PRM-Qwen2.5-7B 61.8 Qwen2.5-Math-7B-PRM800K 53.1 67.1 Qwen2.5-Math-7B-SCAN-Base 72.9 Qwen2.5-Math-7B-SCAN-Pro 99.0 98.4 42.0 44.0 95.9 82.9 95.3 81.9 90.7 50.4 38.8 44.3 47.3 62.5 70.8 68.2 73.8 80.9 21.7 21.4 33.3 36.4 18.9 43.8 48.0 55.6 58.6 72.2 80.0 38.2 35.0 96.6 62.2 90.1 69.5 73.6 33.4 33.8 35.6 35.7 31.6 53.6 62.6 61.7 65. 8.2 10.1 23.9 25.7 7.4 17.9 35.7 44.9 44.2 43.1 51.0 19.8 18.0 93.8 31.9 87.3 45.4 47.8 13.8 16.9 21.7 21.2 13.7 22.9 50.7 45.2 45.9 9.6 10.9 21.9 23.1 4.0 14.0 29.8 41.6 37.8 45.2 51.9 24.5 19.1 95.0 41.9 86.3 52.7 53.1 15.8 28.4 16.9 26.6 23.1 31.2 20.9 31.3 7.7 28.9 21.0 42.1 44.3 56.5 46.5 56.8 44.2 59. reward score for the entire response. The response with the highest reward score is selected as the final answer. For the aggregation process, we take the lowest score among all steps as the overall reward score. Further experiments and discussions regarding different aggregation methods are provided in Appendix 4. The evaluation datasets cover various difficulty levels, including GSM8K [2] (elementary), MATH [11] (competition), College Math [31] (college), and Olympiad Bench [10] (Olympiad). We employ Qwen2.5-Math-7B-Instruct and Llama3.1-8B-Instruct as the policy model and set to 8 (a reasonably practical setting in real-world applications). We report the majority voting [36] as the baseline and pass@8 as the upper bound. Step-wise Error Detection We further investigate the PRMs ability to detect error locations within responses accurately. We use ProcessBench [44] as the evaluation benchmark, which measures the PRMs capability to identify the first error location in given response. This evaluation focuses on the models performance in recognizing fully correct samples and accurately identifying the error in incorrect responses. Formally, the evaluation metrics can be defined as: The final F1 score is the harmonic mean of the accuracies on correct and erroneous samples. 7 Figure 3: Ablation results in BoN evaluation and ProcessBench. For ProcessBench, we directly calculate the overall F1 score of full samples. Left: Scaling curve of the PRM training of different datasets. Middle: Scaling curve of selection of tolerance distance. Right: Effectiveness of each component. Baseline here represents the vanilla MC estimation method. Compared Baselines Our primary comparisons are against 7B-scale process reward models, including Math-Shepherd [35], RLHFlow-PRM [38], Skywork-PRM [23], Math-PSA [34], and EurusPRM [40, 3]. We also include models trained with strong supervision, such as Qwen2.5Math-PRM-7B[42] and UniversalPRM[30], as points of reference. However, we do not include direct comparisons with these strongly supervised models, as they rely on large-scale external critic models for guidance. As shown in Table 2, these critic models are already highly capable, and their supervision, typically via knowledge distillation, plays key role in the final PRM performance. However, we address different issue from these approaches, i.e., denoising MC estimation itself. For the Best-of-N evaluation, we directly test the publicly available checkpoints of the compared models. For ProcessBench, the results are directly sourced from Zheng et al. [44]. 4.3 Main Results Best-of-8 Evaluation Table 1 presents the Best-of-8 evaluation results based on the policy model Qwen2.5-Math-7B-Instruct. With only 101K synthetic samples from the SCAN-Base dataset, generated by the 1.5B model, the Qwen2.5-Math-7B-SCAN-Base model outperforms PRMs trained on larger synthetic datasets and approaches the performance of PRMs trained on human-annotated data (PRM800K). Notably, when trained with additional synthetic data generated by the 7B model, the resulting Qwen2.5-Math-7B-SCAN-Pro model surpasses PRM800K. This demonstrates that even with 1.5B model, it is possible to synthesize data of comparable quality to human annotations. ProcessBench Table 2 presents the evaluation results on ProcessBench. Both Qwen2.5-Math-7BSCAN-Base and Qwen2.5-Math-7B-SCAN-Pro outperform all other process reward models, including those trained on PRM800K. Remarkably, their error detection capabilities even surpass those of the 70B-scale critic model Llama-3.3-70B-Instruct. Through the SCAN approach, Qwen2.5-Math-7B-Ins is able to generate process data for self-training, leading to substantial improvement in its own error detection capability (from 19.9 to 59.1), demonstrating strong self-improvement potential. 4.4 Ablation Study We conducted an ablation study to further demonstrate the effectiveness of each component. Additional results can be seen in Appendix B. Scaling Curve of SCAN-Base and SCAN-Pro The left of figure 3 illustrates the performance variation of the model during training. We observe that without any denoising strategy, the model quickly overfits to noisy samples (see Baseline results). Our denoising strategy enables the model to grow steadily without overfitting to noisy samples and further improves the models performance. By comparing the results of SCAN-Base and SCAN-Pro, we conclude that incorporating additional data sources enhances the diversity of the data, which in turn optimizes the models upper bound. Choices of Tolerance Distance Choosing an appropriate tolerance distance is critical, as both very small and very large values can introduce noise. When = 0, it results in hard labeling, leading to severe noise, as shown in the scaling curve. Conversely, when = n, it becomes soft labeling, 8 which also adds significant noise and hinders scaling [42]. The middle of figure 3 shows the results for different values of d. We find that = 2 offers good balance, reducing overfitting during training. Effectiveness of each component The right of figure 3 shows the effectiveness of each component. We begin with baseline where no denoising strategy is applied to the SCAN-Pro dataset. As we progressively incorporate denoising techniques, we observe consistent improvements in model performance across both best-of-n and ProcessBench evaluations. Both tolerance distance labeling and confidence reweighting contribute to performance gains. Tolerance distance labeling enhances the models robustness when handling noisy samples, while confidence reweighting helps de-bias the probability estimation from different models on annotated samples."
        },
        {
            "title": "5 Related Work",
            "content": "Reward Models in Reasoning Tasks Reward models play crucial role in enhancing the capabilities of large language models (LLMs), particularly in complex reasoning tasks such as mathematical problem-solving [6, 39] and competitive programming [12]. In this context, reward models act as verifiers to assess the correctness of generated responses [4] or directly enhance LLM capabilities through preference alignment [19, 27]. Unlike outcome reward models (ORMs), which evaluate only the correctness of the final answer, process reward models [32, 20] (PRMs) provide more fine-grained evaluations by assessing each step of the reasoning process. Recent advancements [35, 18, 33] have demonstrated the significant potential of PRMs in scaling test-time compute [26, 42] and preference learning [35, 8], further highlighting their importance in the development of more capable and reliable LLMs. Process Reward Learning Human-annotated data [20] is the primary solution, but due to the complexity of the task, manual annotation is highly costly. Recent works show the potential of Monte Carlo estimation as promising alternative; however, it also introduces considerable noise [35, 18, 44]. This noise can be mitigated by incorporating critic models [21, 41, 7, 37], where LLMs are prompted to assess the correctness of each step directly. However, recent research [44] indicates that only large-scale models (e.g., Qwen2.5-Math-72B-Instruct) possess strong critic capabilities. Thus, this approach essentially distills the error-detection ability of large models into smaller process reward models through data distillation. Numerous works extend the strong supervision to more complex methods, like code verification [43], reverse verification [30], and data synthesis for preference learning [28]. Another line of work [40, 3] focuses on different training paradigm by simply training an ORM on response-level labels, as optimized language models inherently function as reward models as well [24]. Our work takes novel noisy-learning perspective and aims to address two key challenges in Monte Carlo annotation: high computational overhead and high noise ratio problem."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduce an effective approach to improving process reward models from the perspective of data annotation. We first conduct preliminary study to reveal the noise distribution in the Monte Carlo annotation process by introducing confidence metric. We then propose an efficient data synthesis and robust learning framework to address the key challenges in Monte Carlo estimation. Through extensive experiments, we demonstrate the effectiveness of our proposed approach and the potential of model self-improvement from robust learning perspective."
        },
        {
            "title": "Acknowledgement",
            "content": "We want to thank all the anonymous reviewers for their valuable comments. This work was supported by the National Science Foundation of China (NSFC No. 62206194), the Natural Science Foundation of Jiangsu Province, China (Grant No. BK20220488), and the Young Elite Scientists Sponsorship Program by CAST (2023QNRC001). We also acknowledge MetaStone Tech. Co. for providing us with the software, optimisation on high performance computing and computational resources required by this work."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [3] Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. [4] Yuyang Ding, Xinyu Shi, Xiaobo Liang, Juntao Li, Qiaoming Zhu, and Min Zhang. Unleashing reasoning capability of llms via scalable question synthesis from scratch. arXiv preprint arXiv:2410.18693, 2024. [5] Keyu Duan, Zichen Liu, Xin Mao, Tianyu Pang, Changyu Chen, Qiguang Chen, Michael Qizhe Shieh, and Longxu Dou. Efficient process reward model training via active learning. arXiv preprint arXiv:2504.10559, 2025. [6] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [7] Bofei Gao, Zefan Cai, Runxin Xu, Peiyi Wang, Ce Zheng, Runji Lin, Keming Lu, Junyang Lin, Chang Zhou, Wen Xiao, et al. Llm critics help catch bugs in mathematics: Towards better mathematical verifier with natural language feedback. CoRR, 2024. [8] Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. rstar-math: Small llms can master math reasoning with self-evolved deep thinking. arXiv preprint arXiv:2501.04519, 2025. [9] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [10] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. [11] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [12] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. [13] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [14] Muhammad Khalifa, Rishabh Agarwal, Lajanugen Logeswaran, Jaekyeom Kim, Hao Peng, Moontae Lee, Honglak Lee, and Lu Wang. Process reward models that think. arXiv preprint arXiv:2504.16828, 2025. [15] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. 10 [16] Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13:9, 2024. [17] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. [18] Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, et al. Improve mathematical reasoning in language models by automated process supervision. arXiv preprint arXiv:2406.06592, 2024. [19] Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. Reft: Reasoning with reinforced fine-tuning. arXiv preprint arXiv:2401.08967, 2024. [20] Qianli Ma, Haotian Zhou, Tingkai Liu, Jianbo Yuan, Pengfei Liu, Yang You, and Hongxia Yang. Lets reward step by step: Step-level reward model as the navigators for reasoning. arXiv preprint arXiv:2310.10080, 2023. [21] Nat McAleese, Rai Michael Pokorny, Juan Felipe Ceron Uribe, Evgenia Nitishinskaya, Maja Trebacz, and Jan Leike. Llm critics help catch llm bugs. arXiv preprint arXiv:2407.00215, 2024. [22] Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael Jordan, et al. Ray: distributed framework for emerging {AI} applications. In 13th USENIX symposium on operating systems design and implementation (OSDI 18), pages 561577, 2018. [23] Skywork o1 Team. Skywork-o1 open series. https://huggingface.co/Skywork, November 2024. [24] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. [25] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. [26] Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated process verifiers for llm reasoning. arXiv preprint arXiv:2410.08146, 2024. [27] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [28] Shuaijie She, Junxiao Liu, Yifeng Liu, Jiajun Chen, Xin Huang, and Shujian Huang. R-prm: Reasoning-driven process reward modeling. arXiv preprint arXiv:2503.21295, 2025. [29] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. [30] Xiaoyu Tan, Tianchu Yao, Chao Qu, Bin Li, Minghao Yang, Dakuan Lu, Haozhe Wang, Xihe Qiu, Wei Chu, Yinghui Xu, et al. Aurora: Automated training framework of universal process reward models via ensemble prompting and reverse verification. arXiv preprint arXiv:2502.11520, 2025. [31] Zhengyang Tang, Xingxing Zhang, Benyou Wan, and Furu Wei. Mathscale: Scaling instruction tuning for mathematical reasoning. arXiv preprint arXiv:2403.02884, 2024. 11 [32] Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. [33] Chaojie Wang, Yanchen Deng, Zhiyi Lyu, Liang Zeng, Jujie He, Shuicheng Yan, and Bo An. Q*: Improving multi-step reasoning for llms with deliberative planning. arXiv preprint arXiv:2406.14283, 2024. [34] Jun Wang, Meng Fang, Ziyu Wan, Muning Wen, Jiachen Zhu, Anjie Liu, Ziqin Gong, Yan Song, Lei Chen, Lionel Ni, et al. Openr: An open source framework for advanced reasoning with large language models. arXiv preprint arXiv:2410.09671, 2024. [35] Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Wu, and Zhifang Sui. Math-shepherd: label-free step-by-step verifier for llms in mathematical reasoning. arXiv preprint arXiv:2312.08935, 2023. [36] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. [37] Shijie Xia, Xuefeng Li, Yixin Liu, Tongshuang Wu, and Pengfei Liu. Evaluating mathematical reasoning beyond accuracy. arXiv preprint arXiv:2404.05692, 2024. [38] Wei Xiong, Hanning Zhang, Nan Jiang, and Tong Zhang. An implementation of generative prm. https://github.com/RLHFlow/RLHF-Reward-Modeling, 2024. [39] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. [40] Lifan Yuan, Wendi Li, Huayu Chen, Ganqu Cui, Ning Ding, Kaiyan Zhang, Bowen Zhou, Zhiyuan Liu, and Hao Peng. Free process rewards without process labels. arXiv preprint arXiv:2412.01981, 2024. [41] Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240, 2024. [42] Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. The lessons of developing process reward models in mathematical reasoning. arXiv preprint arXiv:2501.07301, 2025. [43] Jian Zhao, Runze Liu, Kaiyan Zhang, Zhimu Zhou, Junqi Gao, Dong Li, Jiafei Lyu, Zhouyi Qian, Biqing Qi, Xiu Li, et al. Genprm: Scaling test-time compute of process reward models via generative reasoning. arXiv preprint arXiv:2504.00891, 2025. [44] Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. Processbench: Identifying process errors in mathematical reasoning. arXiv preprint arXiv:2412.06559, 2024. [45] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023."
        },
        {
            "title": "A Data Synthesis and Training Details",
            "content": "A.1 Data Synthesis Details Table 3: Data component of SCAN-Base and SCAN-Pro datasets. Dataset Samples Generator & Completer Generate Response temp. top_p Perform Rollouts top_p temp. GPU hours 15K 16K 31K 40K 13K 13K 71K Qwen2.5-Math-1.5B-Instruct Qwen2.5-Math-1.5B-Instruct Qwen2.5-Math-1.5B-Instruct Qwen2.5-Math-1.5B-Instruct Qwen2.5-Math-7B-Instruct Qwen2.5-Math-7B-Instruct Llama3.2-3B-Instruct 64 64 128 128 64 64 128 0.7 1.0 0.7 1.0 0.7 1.0 1. 0.8 1.0 0.8 1.0 0.8 1.0 1.0 8 8 8 8 8 8 8 0.7 0.7 0.7 0.7 0.7 0.7 0. 0.8 0.8 0.8 0.8 0.8 0.8 0.8 SCAN-Base SCAN-Pro (Increment) SCAN-Pro (Full) 174. 200.1 374.5 (Total) Data Components and Inference Cost Table 3 presents the composition of the SCAN-Base and SCAN-Pro datasets, along with specific hyperparameter settings, including the total inference cost. Through our efficient framework design, we generated 197K samples using 374.5 GPU hours. All experiments were conducted on single machine equipped with 8 Nvidia A800-SXM4-80G GPUs, requiring only 47 hours in real time. Monte Carlo Tree Search Regarding the choice of inference strategy, we found that the commonly used Monte Carlo Tree Search (MCTS) method is inefficient for large-scale data synthesis. Annotating single sample with MCTS involves multiple sequential steps, where each action depends on the state of the tree, making parallelization impractical. Additionally, maintaining extensive tree node information for each sample results in substantial memory overhead. As an alternative, we adopted the vanilla Monte Carlo method for annotation. This approach not only delivers strong annotation performance but also achieves results comparable to those of MCTS [18], while offering significantly higher efficiency. Implementation Details For inference deployment, we explored two approaches: (1) launching multiple vLLM servers and making API calls, using distributed routing strategies powered by FastChat [45], and (2) batching multiple queries and utilizing Ray to schedule resources for distributed inference. We found the second approach to be significantly more efficient, achieving over twice the speed of the first method. When estimating step-wise correctness via Monte Carlo (MC) estimation, we begin from the first step of the response and proceed sequentially until encountering the first position where ct = 0, at which point the evaluation stops. While this introduces some dependency between steps, the depth of dependency is limited to the number of steps, which is considerably shallower and less complex compared to the recursive depth of Monte Carlo Tree Search (MCTS). A.2 PRM Training We train our process reward models based on Qwen2.5-Math-7B-Instruct with constant learning rate of 7 106 and batch size of 128. The model is trained for one epoch, as we observed that multiple epochs lead to rapid overfitting, particularly on synthetic data. Although overfitting occurs more slowly on human-annotated data, it remains concern with extended training."
        },
        {
            "title": "B Additional results",
            "content": "B.1 Noise Distribution Across More Models We present the noise distribution of additional models in Figure 4, including Llama-3.2-3B-instruct, Llama3.1-8B-Instruct, Qwen2.5-Math-1.5B-Instruct and Qwen2.5-Math-7B-Instruct. Overall, the observations discussed in section 2 remain consistent across these models as well. 13 Figure 4: Noise distribution of additional models. Similar observations can be concluded across these models, further validating the consistency of our findings. Table 4: Best-of-8 evaluation results of Qwen2.5-Math-7B-SCAN-Base and Qwen2.5-Math-7BSCAN-Pro using different aggregation methods. Models Aggregation Method GSM8K MATH College Math Olympiad Bench Average Qwen2.5-Math-7B-SCAN-Base Qwen2.5-Math-7B-SCAN-Pro Min-Max Min-Vote Last-Max Last-Vote Min-Max Min-Vote Last-Max Last-Vote 97.1 96.9 96.8 96.8 97.1 97.2 96.9 96.7 86.9 87.5 86.8 87. 87.3 87.8 86.4 87.3 47.8 47.7 48.1 47.6 48.1 47.9 47.6 47.7 44.4 44.9 45.3 45.3 47.7 46.7 44.2 45.5 69.1 69.3 69.3 69. 70.1 70.2 68.9 69.3 B.2 Aggregation Methods We experiment with the following four aggregation methods: Min-Max: This is the method used in our main experiments. The final selected response is given by = argmaxr min(p), where = [p1, p2, . . . , pt] represents the predicted step scores of the trained process reward model. Last-Max: The final selected response is = argmaxrpt, where pt is the reward score of the last step. 14 Min-Vote: Inspired by the majority vote method, we designed weighted voting strategy. The final answer is computed as = argmaxa function that equals 1 if the i-th response matches answer and 0 otherwise. I(ai = a) min(p), where I(ai = a) is an indicator (cid:80) i=1 Last-Vote: The final answer is computed as = argmaxa (cid:80) i=1 I(ai = a) pt. Table 4 presents the results. We observe that the voting-based final answer yields better performance compared to the Min-based strategy. Therefore, if the primary objective is to obtain single accurate answer, the Min-Vote strategy may be the more effective choice. B.3 BoN results on Llama3.1-8B-Instruct Table 5: Best-of-8 evaluation results of the policy model Llama3.1-8B-Instruct. Model Greedy Majority Vote@8 Pass@8 (Upper Bound) Qwen2.5-Math-PRM-7B UniversalPRM-7B RLHFlow-PRM-Mistral-8B RLHFlow-PRM-DeepSeek-8B EurusPRM-Stage1 EurusPRM-Stage2 Math-PSA-7B Skywork-PRM-Qwen2.5-7B Qwen2.5-Math-7B-PRM800K Qwen2.5-Math-7B-SCAN-Base Qwen2.5-Math-7B-SCAN-Pro Training Samples Annotation Method GSM8K MATH College Math Olympiad Bench 86.1 90.5 95.7 92.5 93.3 90.8 90.6 93.0 93.4 92.5 93.3 92.0 93.1 93.0 51.5 60.3 75. 63.3 65.6 60.4 60.6 64.7 66.4 63.8 67.1 64.0 64.8 65.8 34.0 37.2 48.3 40.2 40.4 37.7 37.1 40.7 41.3 39.8 41.3 40.9 40.9 41.5 16.9 24.7 40. 25.6 26.5 24.3 24.7 28.3 28.6 27.7 28.4 28.2 27.6 28.4 1500K 690K 273K 253K 463K 693K 1395K - 264K 101K 197K MC, KD MC, KD MC-Only MC-Only Implicit Implicit MC-Only - Human MC-Only MC-Only Avg. 47.1 53.2 65.0 55.4 56.5 53.3 53.2 56.7 57.4 56.0 57.5 56.3 56.6 57.2 In addition to the Qwen model, we also evaluated the performance of our trained PRMs on Llama3.18B-Instruct. The responses were generated with temperature setting of 0.5. Table 5 presents the results, showing that with only 197K data samples, our PRM achieves performance comparable to other PRMs and outperforms the human-annotated PRM800K dataset. B.4 Ablation in Data Components Figure 5: Left: Ablation results of different responses per question estimating self-confidence value. Right: Ablation results on external data sources. Accurate Estimation of Self-Confidence We estimate the models self-confidence for given question by sampling multiple responses. Our findings indicate that more accurate estimation of self-confidence scores significantly improves data quality, primarily in: (1) more precise selection 15 of high-confidence samples, as high-confidence samples tend to have lower noise, and (2) more accurate reweighting of step-wise correctness scores, reducing bias in the learning process. We experiment with different numbers of responses per question, setting {64, 128}. Notably, larger leads to more precise self-confidence estimation. The left of figure 5 presents the results. To ensure fair comparison, we maintain fixed training dataset size of 30K samples. Our results demonstrate that more accurate self-confidence estimation brings significant improvements in both Best-of-8 and ProcessBench evaluations. Incorporation More Data Sources We explore the potential of incorporating additional data sources, including GSM8K [2] and Numina-Math [16]. Numina-Math consists of various data sources, such as Olympiad problems and synthetic math datasets. We selected specific subsets from it for our experiments. Figure 5 presents the results. Incorporating GSM8K provides certain degree of improvement; however, we find that generating responses for GSM8K incurs higher computational cost compared to MATH. This is because the model makes fewer errors when generating responses for GSM8K, given its relatively simpler problem set. Moreover, we observe that incorporating subsets of synthetic math data significantly degrades performance. Upon further investigation, we suspect this is due to high proportion of unsolvable problems or incorrect reference answers in the dataset. Overall, we find that problem difficulty and the quality of question-answer pairs are two key factors that significantly impact performance. B.5 Reduction in Noise Ratio We further investigate whether our method effectively reduces the noisy data. To quantify this, we measure the noise ratio levels. While direct noise measurement is infeasible for our generated data due to the absence of ground truth, we employ ProcessBench as proxy benchmark, given its similar data sources (including MATH) and response generation process (using Qwen and Llama series models). As shown in Table 6, our denoising process achieves significant reduction in noisy data content. Table 6: Noise ratio of synthetic data with and without SCAN denoising. Completer Vanilla MC + SCAN Denoising Llama-3.1-8B-Ins Qwen2.5-Math-7B-Ins 56.2% 51.8% 19.1% (37.1%) 29.4% (22.4%) B.6 SCAN Inference Speed Table 7 shows the comparison of performance and inference speed of SCAN Models and other large critic models. The inference speed is tested on 4 A100-40G GPUs. Experimental results demonstrate that discriminative models exhibit significantly superior inference speed compared to critic models. Particularly, long-chain-of-thought (long CoT) critic models such as DeepSeek-R1-Distill-Qwen-7B show even more pronounced efficiency bottlenecks. Table 7: Performance and inference speed comparison between SCAN models and critic models. Best-of-N (Avg. Acc.) ProcessBench (Avg. F1) Infer Speed Model Qwen2.5-Math-7B-Instruct Qwen2.5-7B-Instruct DeepSeek-R1-Distill-Qwen-7B Qwen2.5-Math-7B-SCAN-Base Qwen2.5-Math-7B-SCAN-Pro - - - 69.1 70. 16 17.3 36.8 53.4 56.8 59.1 1.5 samples / 10.8 samples / 0.5 samples / 44 samples / 44 samples / B.7 Incorporate SCAN with Knowledge Distillation Method Zheng et al. [42] demonstrated the effectiveness of consensus filtering, where Monte Carlo estimation can be combined with knowledge distillation to further improve data quality. Since any improvement in MC estimation directly benefits downstream pipelines, we integrated our method (SCAN) into consensus filtering framework to evaluate its effectiveness. Table 8: Results of SCAN models with knowledge distillation (KD) from QwQ-32B. Model Method # Samples Best-of-N (Avg. Acc) ProcessBench (Avg. F1) Qwen2.5-Math-PRM-7B MC & KD 1500K 70.1 73. Baseline-7B (w/o denoising) + KD MC & KD SCAN-Base-7B (w/ denoising) + KD MC & KD 100K 100K 69.0 70.3 (+1.3) 52.5 60.8 (+8.3) As shown in Table 8, using SCANs denoised MC data with KD (SCAN + KD) significantly outperforms using standard MC data with KD (Baseline + KD), especially on the fine-grained ProcessBench. Therefore, the improved version of MC can be readily used as plug-in or substitute in any framework that involves MC estimation, offering further performance gains. B.8 SCAN Effectiveness in Broader Domains We further examine the generalization ability of SCAN beyond mathematical reasoning. Specifically, we extend our Math PRM to general-domain task, GPQA-Diamond [25], with results reported in Table 9. Our PRM consistently outperforms the majority-vote baseline, indicating that the reasoning capability it acquires is transferable beyond mathematics. This suggests that developing robust, domain-specialized PRMs represents an important direction for future research. Table 9: Best-of-N results of SCAN-Pro in GPQA-Diamond. Method = 1 = 2 = 4 = 8 Maj @ PRM @ 33.8 33. 33.8 36.4 38.9 40.4 37.3 39.4 B.9 Analysis of Process Error Types To further understand the noise in MC-annotated data, we conducted qualitative analysis by categorizing errors in 60 samples from ProcessBench, with results illustrated in Table 10. Table 10: Error Type Analysis Results and PRM Accuracy. Error Type Description PRM Accuracy Calculation Error Mistakes in arithmetic or computation. Logical Error Conception Error Misunderstanding of concepts or formulas. Inconsistencies or unjustified steps. 15 / 20 9 / 20 10 / SCAN-PRM is highly effective at detecting calculation errors but less sensitive to abstract logical or conceptual mistakes, indicating that SCAN primarily captures procedural correctness. Achieving deeper semantic accuracy, however, remains more challenging and will require further investigation in future work."
        },
        {
            "title": "C Discussion",
            "content": "C.1 Performance discrepancy of models in test-time scaling and ProcessBench We observe notable discrepancy between model performance in Best-of-N evaluation and ProcessBench. For example, models such as Qwen2.5-Math-PRM perform comparably to SCAN models 17 under Best-of-N , yet show larger performance gains on ProcessBench. This contrast reflects the fundamental difference between Monte Carlo (MC) and knowledge distillation (KD) annotation. MC annotation is coarse-grained: it judges correctness solely based on the final outcome of solution. This provides strong global signal of solution quality, which explains why MC-trained models excel in Best-of-N evaluation, where the objective is to identify the best overall response. KD annotation is fine-grained: powerful critic model can analyze reasoning step by step, pinpointing the exact location of errors. Such supervision is crucial for ProcessBench, which explicitly evaluates step-level error detection. These differences in supervision quality directly influence model performance. In Best-of-N evaluation  (Table 1)  , MC-trained models perform well, as their coarse error modeling still captures useful global signals. However, in ProcessBench  (Table 2)  , which requires precise step-level correctness, models trained only with MC supervision underperform due to their weaker error localization ability. Therefore, while our cost-efficient SCAN method achieves strong results in selecting the best overall solution, the more expensive fine-grained supervision used by Qwen-PRM naturally yields better step-level error detection. Notably, our experiments in Table 8 show that integrating SCAN with KD significantly reduces this performance gap. C.2 Effectiveness of SCAN Annotator Model The effectiveness of SCAN is closely tied to the capability of the base annotator, and central insight of our work is that this capability must be well-matched to the difficulty of the problem. To quantify this alignment, we propose self-confidence metric. In general, the quality of process annotation depends on two factors: (1) the capability of the base model and (2) the difficulty of the problem. Effective supervision arises when these two are properly aligned For instance, if highly capable model is applied to an extremely simple problem, it may always recover the correct final answer, even when given erroneous prefix steps, as it can effortlessly detect and correct mistakes. Conversely, if the model is too weak for challenging problem (e.g., Olympiad-level), it is unlikely to reach the correct answer under any prefix, producing noisy or uninformative annotations. The key insight, therefore, is that annotation quality improves when the models capability matches the task difficulty, and this match can be estimated using SCANs self-confidence metric."
        },
        {
            "title": "D Limitations and Future Work",
            "content": "Despite the promising results, our work still faces several limitations. Limitations of Monte Carlo Estimation During our experiments, we observed type of noise that cannot be effectively handled by Monte Carlo (MC) estimationfalse positives, where response is predicted to be entirely correct but actually contains errors. This issue arises from strong assumption underlying MC estimation: if the final answer is correct, all intermediate steps are assumed to be correct as well. Consequently, step-level errors embedded in seemingly correct responses remain undetected. This limitation makes purely MC-based annotation strategies insufficient for ensuring process-level fidelity. Limitations of Process Reward Models (PRMs) in AI alignment Although PRMs show encouraging improvements in test-time scaling and error localization, their effectiveness in AI alignment and safety remains limited. In particular, PRMs are still vulnerable to reward hacking, where model exploits imperfections in the reward signal to achieve high scores without genuine reasoning. This issue becomes especially pronounced in out-of-distribution evaluations, where models may inadvertently optimize for misaligned objectives. Promising Direction: Generative Process Reward Models (GenPRMs). To address both the annotation and alignment challenges, we view generative process-level reward models (GenPRMs) as promising path forward. Unlike discriminative PRMs, GenPRMs are tasked with producing explicit rationales that identify both the location and nature of reasoning errors. This grounds the 18 reward signal in the actual reasoning process, thereby reducing the likelihood of reward hacking and improving interpretability. In data synthesis, large critic models can further enhance GenPRMs by pinpointing step-level mistakes, which can be integrated with MC sampling to yield higherquality supervision. We believe GenPRMs represent key direction for future research, offering stronger process-consistent reward signals and safer, more interpretable framework for supervising high-capability reasoning models."
        },
        {
            "title": "E Human Verification",
            "content": "We further analyze the capability of our trained PRMs through human evaluation. Specifically, we manually inspect whether the PRM accurately identifies correct and incorrect samples, as well as its ability to locate errors within responses precisely. Our evaluation focuses on two challenging settings in the best-of-n scenario: High-confidence samples, where almost every response is correctcan the PRM identify the few erroneous responses hidden among predominantly correct ones? Low-confidence samples, where only few responses are correctcan the PRM successfully identify those correct responses? We list some representative cases below, with human verification and analysis. 19 Best-Of-N Evaluation Example 1 Best-of-8 Evaluation Whether PRM can select incorrect responses in high-conf Task: Purpose: questions. Dataset Name: GSM8K Response Generator: Process Reward Model: Problem: Carla is downloading 200 GB file. Normally she can download 2 GB/minute, but 40% of the way through the download, Windows forces restart to install updates, which takes 20 minutes. to restart the download from the beginning. download the file? Qwen2.5-Math-7B-Scan-Pro Qwen2.5-Math-7B-Instruct Then Carla has How load does it take to Response (incorrect, 7 others correct): To determine the total time it takes Carla to download the 200 GB file, we need to break down the process into segments and calculate the time for each segment. Ground Truth: Correct, Predicted Score: 96.1, 1. **Calculate the time to download 40% of the file:** - The file size is 200 GB. - 40% of 200 GB is 0.40 200 = 80 GB. - Carla downloads at rate of 2 GB per minute. - Time to download 80 GB is 80 2 = 40 minutes. Ground Truth: Correct, Predicted Score: 91.8, 2. **Add the time taken for the restart:** - The restart takes 20 minutes. - Total time so far is 40 + 20 = 60 minutes. Ground Truth: Correct, Predicted Score: 94.1, 3. **Calculate the time to download the remaining 160 GB:** - The remaining file size is 200 80 = 120 GB. - Carla downloads at rate of 2 GB per minute. - Time to download 120 GB is 120 2 = 60 minutes. Ground Truth: Incorrect, Predicted Score: Explanation: Since the reboot requires downloading from scratch, she needs to re-download the entire 200 GB instead of the remaining 120 GB. 3.4, 4. **Add the time taken for the second restart:** - The restart takes 20 minutes. - Total time so far is 60 + 60 + 20 = 140 minutes. Therefore, the total time it takes Carla to download the file is 140 minutes. Analysis: PRM can select incorrect responses with accurate error location from numerous correct ones, demonstrating strong discriminative capability. 20 Best-Of-N Evaluation Example 2 (part 1) Best-of-8 Evaluation Whether PRM can select correct responses in low-conf Task: Purpose: questions. Dataset Name: Olympiad Bench Response Generator: Process Reward Model: Problem: Find all triples (a, b, c) of positive integers such that a3 + b3 + c3 = (abc)2. Qwen2.5-Math-7B-Scan-Base Qwen2.5-Math-7B-Instruct Response (correct, 7 others incorrect): To find all triples (a, b, c) of positive integers such that a3 + b3 + c3 = (abc)2, we start by analyzing the equation and considering possible values for a, b, and c. Ground Truth: Correct, Predicted Score: 95.3, First, lets check if there are any small solutions. = = c: We start with a3 + a3 + a3 = (a3) = (a3)2 = 3a3 = a6 = a6 3a3 = 0 = a3(a3 3) = 0. Since is positive integer, a3 = 0, so we have: a3 = 3 = = 3 3. Since 3 Ground Truth: Correct, Predicted Score: 85.2, 3 is not an integer, = = does not provide solution. Next, lets try some small distinct values for a, b, and c. = 1: 13 + b3 + c3 = (1 c)2 = 1 + b3 + c3 = (bc)2. Suppose Rearranging terms, we get: b3 + c3 = (bc)2 1. We need to find positive integers and that satisfy this equation. Lets try = 2: 23 + c3 = (2c)2 1 = 8 + c3 = 4c2 1 = c3 4c2 + 9 = 0. We test possible integer values for c: - If = 1, then 13 4 12 + 9 = 1 4 + 9 = 6 = 0. - If = 2, then 23 4 22 + 9 = 8 16 + 9 = 1 = 0. - If = 3, then 33 4 32 + 9 = 27 36 + 9 = 0. So, = 3 is solution. By symmetry, (1, 3, 2), (2, 1, 3), (2, 3, 1), (3, 1, 2), and (3, 2, 1) are also solutions. Ground Truth: Correct, Predicted Score: 67.2, Therefore, (a, b, c) = (1, 2, 3) is solution. (continue on the next page) 21 Best-Of-N Evaluation Example 2 (part 2) Now, lets check if there are any other solutions. Suppose = 2: 23 + b3 + c3 = (2bc)2 = 8 + b3 + c3 = 4b2c2. Rearranging terms, we get: b3 + c3 = 4b2c2 8. We need to find positive integers and that satisfy this equation. Lets try = 1: 13 + c3 = 4 12 c2 8 = 1 + c3 = 4c2 8 = c3 4c2 + 9 = 0. We already know that c3 4c2 + 9 = 0 has no positive integer solutions. Lets try = 2: 23 + c3 = 4 22 c2 8 = 8 + c3 = 16c2 8 = c3 16c2 + 16 = 0. We test possible integer values for c: - If = 1, then 13 16 12 + 16 = 1 16 + 16 = 1 = 0. - If = 2, then 23 16 22 + 16 = 8 64 + 16 = 40 = 0. - If = 4, then 43 16 42 + 16 = 64 256 + 16 = 176 = 0. Since c3 16c2 + 16 = 0 has no positive integer solutions for 2, there are no other solutions when = 2. Ground Truth: Correct, Predicted Score: 61.7, By symmetry, there are no other solutions when Therefore, the only solutions are: = 3 or higher. (1, 2, 3), (1, 3, 2), (2, 1, 3), (2, 3, 1), (3, 1, 2), (3, 2, 1) . Ground Truth: Correct, Predicted Score: 81.3, Analysis: PRM can select incorrect responses from numerous incorrect ones, but it is less confident in some of the correct steps, as demonstrated by the relatively low step scores of intermediate steps."
        }
    ],
    "affiliations": [
        "Soochow University",
        "Tencent"
    ]
}