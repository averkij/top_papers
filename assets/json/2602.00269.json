{
    "paper_title": "VoxServe: Streaming-Centric Serving System for Speech Language Models",
    "authors": [
        "Keisuke Kamahori",
        "Wei-Tzu Lee",
        "Atindra Jha",
        "Rohan Kadekodi",
        "Stephanie Wang",
        "Arvind Krishnamurthy",
        "Baris Kasikci"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Deploying modern Speech Language Models (SpeechLMs) in streaming settings requires systems that provide low latency, high throughput, and strong guarantees of streamability. Existing systems fall short of supporting diverse models flexibly and efficiently. We present VoxServe, a unified serving system for SpeechLMs that optimizes streaming performance. VoxServe introduces a model-execution abstraction that decouples model architecture from system-level optimizations, thereby enabling support for diverse SpeechLM architectures within a single framework. Building on this abstraction, VoxServe implements streaming-aware scheduling and an asynchronous inference pipeline to improve end-to-end efficiency. Evaluations across multiple modern SpeechLMs show that VoxServe achieves 10-20x higher throughput than existing implementations at comparable latency while maintaining high streaming viability. The code of VoxServe is available at https://github.com/vox-serve/vox-serve."
        },
        {
            "title": "Start",
            "content": "VOXSERVE: Streaming-Centric Serving System for Speech Language Models 6 2 0 2 0 3 ] . [ 1 9 6 2 0 0 . 2 0 6 2 : r Keisuke Kamahori 1 Wei-Tzu Lee 1 Atindra Jha 2 Rohan Kadekodi 1 Stephanie Wang 1 Arvind Krishnamurthy 1 Baris Kasikci 1 Abstract Deploying modern Speech Language Models (SpeechLMs) in streaming settings requires systems that provide low latency, high throughput, and strong guarantees of streamability. Existing systems fall short of supporting diverse models flexibly and efficiently. We present VOXSERVE, unified serving system for SpeechLMs that optimizes streaming performance. VOXSERVE introduces model-execution abstraction that decouples model architecture from system-level optimizations, thereby enabling support for diverse SpeechLM architectures within single framework. Building on this abstraction, VOXSERVE implements streaming-aware scheduling and an asynchronous inference pipeline to improve endto-end efficiency. Evaluations across multiple modern SpeechLMs show that VOXSERVE achieves 1020 higher throughput than existing implementations at comparable latency while maintaining high streaming viability. The code of VOXSERVE is available at https://github .com/vox-serve/vox-serve. 1. Introduction In recent years, speech models built upon large language model (LLM) foundations have made substantial progress in tasks such as Text-to-Speech (TTS) and Speech-to-Speech (STS) (Arora et al., 2025). These Speech Language Models (SpeechLMs) leverage LLM backbones and neural audio codec models (Mousavi et al., 2025) to generate and understand speech representations. SpeechLMs are increasingly deployed at scale in real-world applications, including virtual assistants, content generation, and language access services (OpenAI, 2025; ElevenLabs, 2025; Yao, 2025). This widespread adoption demands serving systems that are both low-latency and cost-efficient. The 1University of Washington 2Stanford University. Correspondence to: Keisuke Kamahori <kamahori@cs.washington.edu>, Baris Kasikci <baris@cs.washington.edu>. Preprint. February 3, 2026. 1 proliferation of powerful open-source models has further accelerated the development of speech applications that leverage SpeechLMs (Siperco, 2025; Staniszewski, 2025; Canopy Labs, 2025; Du et al., 2024; Wu et al., 2025), increasing demand for efficient SpeechLM serving (Peng et al., 2024). However, deploying SpeechLMs poses challenges that are largely unaddressed by existing LLM serving systems. Unlike text-only models, SpeechLMs combine an LLM backbone with audio-specific modules, such as audio detokenizers to generate audio from LLM outputs, resulting in multi-stage inference pipelines with heterogeneous compute, memory, and I/O characteristics. Efficient deployment must therefore coordinate scheduling, caching, and streaming across components. These challenges are exacerbated by the architectural diversity of modern SpeechLMs, which vary in architecture, codebook representations, and modelspecific sampling or post-processing (see 2.4). Consequently, existing serving implementations rely on fragmented, bespoke inference stacks (2.4.1) that do not holistically manage the pipeline in single framework, resulting in suboptimal serving performance and high engineering cost to switch between different model families. Moreover, streaming applications demand unique performance requirements: As discussed in 2.3, the system must begin audio playback with minimal delay and subsequently generate audio chunks at rate sufficient to ensure uninterrupted, natural-sounding output (Schweiger, 2025). Therefore, we need carefully designed system that considers all model components and optimizes end-to-end performance. VOXSERVE addresses these challenges by providing unified interface that supports diverse SpeechLMs within single system, with high performance for streaming applications as the core design goal. This is achieved by designing an abstraction that decouples model-architecture details from system-level optimizations. As discussed in 3.1, we design model execution interface that can support wide range of SpeechLMs. Using this abstraction, we implement number of model-agnostic optimizations for serving performance, including batching, chunk-wise detokenization for streaming, cache management, and CUDA graph. While these are established primitives for performance opVOXSERVE : Streaming-Centric Serving System for Speech Language Models 2. Background and Motivation 2.1. SpeechLM Background Modern SpeechLMs typically consist of an LLM backbone and an audio detokenizer model: the LLM autoregressively generates discrete audio tokens, and the detokenizer then converts those tokens into continuous audio data (Peng et al., 2024; Arora et al., 2025; Mousavi et al., 2025) as shown in Figure 1. Some models also include encoder modules to process audio inputs and compute feature representations (Wu et al., 2025). 2.2. Speech Encoding & Detokenization Audio tokens are discrete representations derived from continuous speech using neural audio codec models, such as vector-quantized autoencoders trained with audio reconstruction objectives (Kumar et al., 2023; Siuzdak et al., 2024; Defossez et al., 2024; Du et al., 2024). In these systems, an encoder transforms raw waveforms into latent features and quantizes into tokens, while detokenizer reconstructs audio from those representations. Many modern tokenizers adopt multi-codebook formulation, in which single audio segment is mapped to multiple tokens that capture complementary information (e.g., semantic versus acoustic content) or represent the signal at different granularities. Diverse model architectures. Modern audio tokenizers vary widely in architecture, token rate, and codebook design. For example, DAC (Kumar et al., 2023), which comprises convolutional layers and residual vector quantization (RVQ) modules with 75M parameters, operates at 86 tokens/s with 9 codebooks. SNAC (Siuzdak et al., 2024) (used in Orpheus 3B (Canopy Labs, 2025)) has similar architecture, but each codebook captures information at different temporal granularity. The detokenizer used in CosyVoice 2 (Du et al., 2024) is more complex, employing flow-matching module built on Transformer layers and HiFi-GAN (Kong et al., 2020) vocoder, producing 25 tokens/s with single codebook and more than 320M parameters in total. 2.3. Metrics for Streaming Speech Serving In the context of SpeechLM, streaming refers to the incremental generation of audio chunks rather than waiting for the entire sequence to be generated. In streaming setup, detokenizer is invoked at regular intervals, usually every 10-50 tokens, to reconstruct audio and reduce perceived response latency. Streaming services require specialized performance metrics that reflect users perceived quality of service. We focus on the following two metrics, which are standard for evaluating speech services (Shao & Lee, 2025; Howard, 2025; Zeghidour et al., 2025; Ethiraj et al., 2025). Importantly, these metrics differ significantly from those Figure 1. Typical workflow of SpeechLM inference. timization, their application to SpeechLMs has been limited due to architectural heterogeneity; to our knowledge, VOXSERVE is the first to unify these optimizations across multiple SpeechLM families under single abstraction. As result, VOXSERVE provides platform for developing efficient speech systems: system designers can explore optimizations that generalize across model architectures, while model developers can benefit from efficient serving without the need to reinvent serving-related optimizations. To validate this, we implement support for seven modern SpeechLMs with diverse architectures (listed in 3.3). Additionally, we propose new scheduling algorithm optimized for streaming-specific performance metrics (3.2.1) and asynchronous pipeline design to reduce overhead (3.2.2). Our evaluation of the streaming serving setting demonstrates that VOXSERVE achieves substantially better performance. Across three models with existing serving baselines, VOXSERVE can serve 1020 higher request rate than the existing implementations with similar response latency, while ensuring streaming viability (4). Additionally, VOXSERVE can flexibly adapt to distributed inference and other application scenarios (e.g., throughput-oriented inference). To summarize, we make the following contributions: 1. We design VOXSERVE, SpeechLM serving system that provides an abstraction for diverse SpeechLM architectures, decoupling model design from systemlevel optimizations. 2. We propose an optimized scheduling algorithm and an asynchronous pipeline design to improve the serving performance for streaming applications. 3. VOXSERVE achieves significantly higher performance for streaming applications than existing systems. 2 VOXSERVE : Streaming-Centric Serving System for Speech Language Models used in text LLMs, such as Time-To-First-Token (TTFT) and Time-Per-Output-Token (TPOT) (Zhong et al., 2024). Time-To-First-Audio (TTFA). For streaming speech generation, the latency perceived by the client is critical performance metric, which is defined as TTFA t1 t0, (1) where t0 denotes the time at which the client submits the request, and t1 denotes the time at which the first playable waveform chunk is generated and delivered to the client. In contrast to TTFT in text-based LLMs, TTFA encompasses not only the LLM prefill latency but also the generation of certain number of tokens, followed by inference through the audio detokenizer. When applicable, speech encoder inference is additionally required. Streaming Viability. Once the first audio chunk has been delivered, uninterrupted streaming requires that each subsequent chunk arrive before playback of the previous chunk completes. Let ti denote the wall-clock time at which the ith playable audio chunk becomes available at the client, and let Ci be the playback duration of that chunk. Disruptionfree streaming requires ti+1 t1 (cid:88) k= Ck, 1, (2) i.e., the (i + 1)-th chunk must be delivered no later than the end of playback of the i-th chunk. Unlike TTFA, which is continuous metric where smaller values directly improve perceived responsiveness, streaming viability is binary metric for each chunk: as long as chunks are delivered in time to sustain continuous playback, further reductions in latency provide no perceived benefit. Hence, the objective after the first chunk is not to minimize latency, but just to satisfy Equation 2s constraint throughout the generation. Goals. To summarize, the objective of SpeechLM serving system is to minimize the TTFA (Equation 1), or keep it below prescribed target, while strictly satisfying the streaming viability constraints of Equation 2. Subject to these constraints, the system seeks to serve stream of incoming requests at the lowest possible operational cost (i.e., by maximizing the number of requests served per device). 2.4. Challenges in SpeechLM Deployment The rapid advancement of LLMs has catalyzed the development of highly optimized serving systems for text generation models (Kwon et al., 2023; Zheng et al., 2024; Zhu et al., 2025). In contrast, system support for SpeechLMs has lagged for two primary reasons, as detailed below. Figure 2. SpeechLMs have diversity in how to represent both text and audio data, including number of codebooks, usage of continuous feature values from audio inputs, and the existence of depth-wise LLM. Challenge 1: Supporting diverse and multi-stage speech pipelines. SpeechLMs combine an LLM backbone with audio-specific modules, resulting in multi-stage inference pipeline with heterogeneous components and I/O characteristics. This is compounded by the architectural diversity of modern speech models. As discussed in 2.2, detokenizers vary widely in their architectures and tokenization rates. LLM backbones also vary in data representation. Some models simply have both text/audio tokens in single token space (Canopy Labs, 2025), while others process multiple codebooks in parallel (Zyphra Team, 2025), or use continuous feature values from audio input (Du et al., 2024; Wu et al., 2025). Others employ smaller depth-wise LLM to generate multiple tokens per backbone LLM iteration (Defossez et al., 2024; Iribe et al., 2025) (Figure 2). Due to the lack of standardized SpeechLM serving frameworks, inference engines are typically coupled with specific architectures, but this necessitates that developers manually reimplement core optimizations, such as request batching, chunk-wise detokenizer inference, and CUDA graph optimization, for every new model variant. Challenge 2: Optimizing for unique streaming performance metrics. Moreover, optimal inference scheduling is highly use-case dependent, even for fixed model. The interval at which the detokenizer is invoked relative to the LLM backbone, as well as cache management policies for each component, must be carefully designed. Moreover, streaming speech applications introduce unique performance metrics (2.3) that are not captured by existing LLM serving systems. Therefore, achieving high performance requires holistic system design that jointly accounts for all the components. 3 VOXSERVE : Streaming-Centric Serving System for Speech Language Models VOXSERVE overcomes the challenges detailed in 2.4 using the following design principles: P1: Building single inference framework for SpeechLMs. VOXSERVE builds single serving the components in integrates all framework that SpeechLMs to enable system-wide optimizations. P2: Decoupling system-level optimization from model details. VOXSERVE provides layer of abstraction that enables performance optimization to work in model-agnostic manner, while making it possible to serve new SpeechLM architectures without reinventing common serving techniques. P3: Optimizing performance for streaming scenarios. VOXSERVE proposes custom scheduling policy and an asynchronous execution pipeline to optimize the performance for streaming services, beyond just combination of existing techniques. Figure 4 shows the overall system architecture of VOXSERVE. VOXSERVE consists of two high-level processes: the interface process and the execution process. The interface process exposes an HTTP endpoint for the users to submit requests. The execution process serves user requests using three components: Scheduler, Worker, and Model. The Scheduler is responsible for orchestrating the request lifecycle. It tracks the status of all requests and runs an infinite while loop to determine which requests to run the LLM or detokenizer on at each iteration. The Worker manages GPU resources and executes the actual inference operations (prefill/decode/detokenize) requested by the Scheduler. The Model implements the neural network architecture and model-specific logic. Each model subclass encapsulates all model-specific behavior. 3.1. Unified Model Interface (P1 & P2) VOXSERVE supports diverse SpeechLMs through common interface, decoupling system-level optimization from model architecture. Each interface function in the Model module constitutes step in the inference workflow for request. The Scheduler can schedule each of these steps and batch requests to achieve high performance. We now discuss the model interface in more detail. Preprocess. Preprocess performs all operations required before the LLM backbone forward pass, including prompt formatting, text tokenization, allocating buffers for requestspecific metadata, and, optionally, running the audio encoder inference for models that accept audio input. The metadata includes input data for the LLM (token IDs, masks, features, as discussed below), and optional cache buffers for complex sampling methods (e.g., repetition-penalty with specified window size) or for stateful detokenizers. Figure 3. (Left) SpeechLM deployment is currently fragmented by bespoke, architecture-specific inference stacks, leading to suboptimal scheduling and resource management, and requires significant engineering cost to adopt new architecture. (Right) We design unified serving system that supports diverse SpeechLMs, which enables holistic system optimization. 2.4.1. CURRENT LANDSCAPE. In practice, SpeechLM deployment remains fragmented and inefficient. New model releases often ship with bespoke inference libraries that are rarely optimized for serving multiple concurrent requests in streaming setting and support only specific model architecture, making architecture changes significant effort. common workaround is to combine multiple frameworks (e.g., combining an existing LLM serving system with custom engine for audio-specific parts), but this overlooks system-wide optimization opportunities, such as coordinating LLM and detokenizer inference. Moreover, this approach is incompatible when the backbone LLM is not supported by the LLM serving system out of the box (e.g., when multi-codebook prediction is required). As illustrated in Figure 3, the absence of standardized serving framework results in multiple independent components competing for shared hardware resources, with no single entity coordinating system-wide resource management. Moreover, even when performance optimizations are developed for specific models, the introduction of new model architectures necessitates reimplementing the entire set of serving-related optimizations. Motivated by these gaps, our goal is to design SpeechLM serving system that (1) uniformly works for diverse landscape of modern SpeechLMs, and (2) provides high efficiency for multi-tenant and streaming inference. 3. Design VOXSERVE is serving system for SpeechLMs that abstracts architectural diversity behind unified execution model, while optimizing goodput for streaming inference. VOXSERVE : Streaming-Centric Serving System for Speech Language Models Figure 4. Overview of VOXSERVE architecture. The execution process has three modules: Scheduler for request orchestration, Worker for GPU management, and Model for providing common abstraction across various SpeechLMs. Together, this design enables holistic and model-agnostic optimization of SpeechLM serving. LLM Forward. The forward stage runs the backbone LLM to generate the next tokens. While the computation is similar to that of typical LLM serving systems, it exposes modified interface that supports diverse data representations. As shown in Figure 2, the way SpeechLMs handle both text and audio data is not standardized. ference: we run the detokenizer with specified number of tokens per request. The generated audio is delivered to the client in streaming manner. The interval at which to run the detokenizer (i.e., the chunk size) is determined by the serving system operator based on model configuration and application requirements. To accommodate them, VOXSERVEs LM forward interface accepts the input token IDs, masks, and features. The IDs are 2D tensor of integers representing input token IDs across the temporal and codebook dimensions, and the features are float tensor, optionally used to store continuous input. The mask is boolean tensor with the same shape as the IDs, optionally used to branch the operation (e.g., when text and audio tokens use separate embedding layers, or to mask embedding values corresponding to input features). The specific usage of features and masks is model-defined, implemented independently in each model subclass, while ensuring consistent interface. This allows straightforward implementation and optimization at the worker layer. Sampling. Sampling converts the LLM backbones output logits into next-token decisions and updates per-request state for subsequent iterations. This stage implements standard sampling algorithms (e.g., temperature, top-k, top-p) and optionally with repetition penalty. This method also prepares the inputs (IDs, masks, and features) for the next LM forward pass. Postprocess. The Postprocess method runs the audio detokenizer model to convert generated audio tokens (or intermediate audio representations) into waveform chunks. Since the architecture of the audio detokenizer shows significant diversity in modern SpeechLMs, we implement all the tokenizers in way that (1) supports batch inference and (2) does not use dynamic tensor shapes to be compatible with optimizations like CUDA graph. To support streaming generation, we use chunk-based inAdditionally, we maintain cache state for some detokenizers (e.g., Mimi and CosyVoices detokenizer) that require information from previous chunks, such as KV caches for self-attention layers or activation values in causal convolution layers. This cache is initialized in the preprocess method and stored per request. Other components. Some SpeechLMs generate audio using depth-wise model that autoregressively samples multiple codebooks. VOXSERVE treats this as an optional depth-forward/sampling method, since it operates at different interval from the detokenizer. 3.1.1. MODEL OPTIMIZATIONS The unified model interface allows VOXSERVE to optimize different steps in the inference workflow. For NVIDIA GPUs, VOXSERVE places the LLM Forward and Postprocess stages on CUDA-graph-captured fast paths to reduce kernel-launch overhead and improve predictability, utilizing FlashInfer (Ye et al., 2025) for the attention backend. To increase capture hit-rate of fast paths despite dynamic batching, VOXSERVE standardizes tensor contracts at the model interface boundary (input tokens, input features, input masks) and uses stable execution shapes per policy (e.g., fixed chunk sizes for streaming). Control-flow-heavy components (preprocess and sampling) remain outside CUDA graphs, preserving model flexibility for diverse sampling strategies while keeping the dominant compute on optimized paths. 5 VOXSERVE : Streaming-Centric Serving System for Speech Language Models 3.2.2. ASYNCHRONOUS PIPELINE Another key performance challenge in SpeechLM serving is that each audio chunk requires inference of both the LLM backbone and detokenizer, with CPU-side sampling and request-dependent control flow interleaved between these stages. purely synchronous execution model introduces pipeline bubbles and additional bookkeeping overhead for managing per-request state, including detokenizer caches or request-specific metadata. To mitigate these inefficiencies, VOXSERVE adopts an asynchronous pipeline that overlaps independent work across device streams, as illustrated in Figure 5. Specifically, the LLM backbone forward pass and the detokenizer forward pass are scheduled as distinct GPU tasks, with explicit dependencies on per-request state, thereby enabling fine-grained control over execution order. This decoupling allows GPU inference to overlap with CPU-side processing, improving overall device utilization and reducing end-to-end latency. 3.3. Implementation VOXSERVE is implemented in Python using PyTorch with approximately 20,000 lines of code. It currently supports wide range of open-source TTS and STS models, including Chatterbox TTS (Resemble AI, 2025), CosyVoice 2.0 (Du et al., 2024), CSM 1B (Iribe et al., 2025), GLM-4-Voice (Zeng et al., 2024), Orpheus 3B (Canopy Labs, 2025), StepAudio 2 (Wu et al., 2025), and Zonos-v0.1 (Zyphra Team, 2025). 4. Evaluation 4.1. Setups In our evaluation, we focus on three models: CosyVoice 2.0, Orpheus 3B, and Step-Audio 2. We select these models because their developers provide official serving implementations in their GitHub repositories, whereas other models lack open-source serving support. Since no existing system uniformly supports all three models, we compare each model against its official serving implementation as the baseline. Each of the baselines combines an LLM serving system with custom detokenizer engine. Nevertheless, these models collectively cover broad range of approaches in the SpeechLM literature, spanning TTS and STS models and different detokenizer architectures. Evaluations of remaining models are reported in the Appendix B. We measure TTFA and streaming viability across varying request rates on single NVIDIA H100 GPU. Requests are sampled from LibriTTS (Zen et al., 2019) for TTS and VoiceBench (Chen et al., 2024) (AlpacaEval subset (Li et al., 2023)) for STS models. Requests are issued over 60Figure 5. Asynchronous pipeline design. VOXSERVE overlaps GPU computation with independent CPU-side tasks to reduce scheduling overhead. 3.2. Scheduling and Pipelining Requests (P3) VOXSERVE exposes scheduling policies at the Scheduler module to optimize the performance for streaming serving, i.e., TTFA and streaming viability. The Scheduler is responsible for orchestrating the request lifecycle. It tracks the status of all requests and runs an infinite while loop to decide which requests to run the LLM or detokenizer on at each iteration. 3.2.1. OPTIMIZED SCHEDULING FOR STREAMING Each request naturally decomposes into two phases: (1) startup phase, during which the first audio chunk has not yet been generated, and the system must execute LLM backbone steps followed by detokenization to generate the initial chunk (TTFA-critical), and (2) steady-state phase, in which subsequent audio chunks are produced continuously (streaming-viability-critical). To optimize performance for streaming applications, the VOXSERVE scheduler continuously monitors the latency requirements of all active requests and dynamically adjusts their priorities. Scheduling decisions distinguish between two execution phases. The key insight is that streaming viability is binary property: for some requests, temporarily delaying inference does not degrade quality of service. This slack can therefore be exploited to allocate resources to more time-critical requests without affecting overall system performance. During the startup phase, newly admitted requests are prioritized until their first audio chunk is produced. This prioritization is subject to bounded concurrency limit to prevent pathological starvation of steady-state streams. Once request enters the steady-state phase, it is assigned soft deadline based on its chunk duration and the accumulated timestamp lag. The scheduler prioritizes requests based on their risk of violating streaming viability, defined as being within 1 second of the deadline. 6 VOXSERVE : Streaming-Centric Serving System for Speech Language Models Figure 7. TTFA comparison across scheduling strategies, highlighting the benefit of optimizations for streaming modes and asynchronous pipelining. ing in queue buildup and high TTFA even at low request rates. This is exacerbated for Step-Audio, where detokenizer batching is infeasible due to cache-management constraints in the baselines; in contrast, VOXSERVE can maintain cache state under batched inference. 4.3. Ablation Study 4.3.1. SCHEDULING ALGORITHM Figure 7 presents an ablation study of the scheduling methodologies, demonstrating the benefits of an optimized scheduling algorithm for streaming-specific metrics (3.2.1) and the asynchronous pipeline (3.2.2). Results are reported for the CosyVoice model using p90 TTFA. The scheduling algorithm has substantial impact on TTFA. Under fixed TTFA target, optimization significantly increases serving throughput (e.g., 3.5 req/s with optimized scheduling achieves comparable TTFA to only 1.5 req/s without optimization). Conversely, under fixed request rate, optimized scheduling markedly reduces TTFA; at 2.0 req/s, TTFA is reduced by approximately 2.5. Asynchronous pipelining provides additional improvements beyond optimized scheduling, particularly at higher request rates. For instance, at 4.0 req/s, asynchronous pipelining further reduces TTFA by approximately 15%. 4.3.2. MULTI-GPU SCALING While the main evaluation focuses on single-GPU setting, VOXSERVE scales flexibly to multi-device deployments. To demonstrate this capability, we evaluate two distributed inference scenarios, shown in Figure 8. Data Parallelism. The top panel reports performance under data parallelism (DP) with up to four H100 GPUs, evaluated using the CosyVoice model and p90 TTFA. This setup is implemented by instantiating one scheduler process per GPU and randomly routing each incoming request to scheduler. The results show near-linear scaling in serving capacity. For example, under 500ms TTFA constraint, DP=4 sustains approximately four times the request rate of the single-GPU configuration (16 req/s versus 4 req/s). Figure 6. Serving performance of VOXSERVE compared against existing systems. The x-axis shows the request rate, and the y-axis shows the TTFA latency. For each system, we show the TTFA of p90 and p99. The percentage at each point shows the fraction of audio chunks that satisfied the streaming viability requirement. second run, with intervals drawn from Poisson distribution for each request rate, following prior work (Kwon et al., 2023). We report streaming viability as the fraction of output chunks that arrive in time to enable real-time playback. Further details are provided in Appendix A. 4.2. Goodput Performance Figure 6 compares VOXSERVE against baselines on three models, showing p90/p99 TTFA (y-axis) and streaming viability (annotations). In all cases, VOXSERVE sustains 10 20 higher request rates while keeping TTFA comparable and maintaining high streaming viability. For CosyVoice, the baseline reaches 500 ms p90 TTFA at 0.4 req/s, whereas VOXSERVE maintains the same TTFA up to 4.0 req/s with 100% streaming viability. For Orpheus, p90 TTFA stays below 500 ms up to 10 req/s, but streaming viability drops past 8.0 req/s due to its high token rate (86 tokens/s); VOXSERVE nevertheless delivers more than 10 higher throughput for given TTFA than the baseline. CosyVoice and Step-Audio incur higher detokenization costs, which increase TTFA under high concurrency. Step-Audio achieves the lowest request rate due to its large size (9B), yet VOXSERVE again outperforms the baseline. Although baselines support streaming, they lack systemwide scheduling and efficient detokenizer batching, resultVOXSERVE : Streaming-Centric Serving System for Speech Language Models We implement custom scheduler subclass that simply maximizes the batch sizes of both the LLM backbone and the detokenizer at each iteration. Figure 9 reports throughput measured as the inverse Real-Time Factor, defined as the total duration of generated audio divided by execution latency. The experiment uses the CosyVoice model and issues 1,000 concurrent requests from LibriTTS. The baseline system achieves approximately 10 real-time throughput. In contrast, VOXSERVE without scheduling optimization achieves 53, whereas the optimized scheduler further improves throughput to approximately 134 realtime. These results highlight the flexibility of VOXSERVE across diverse application scenarios, extending well beyond online streaming workloads. 5. Related Work Modern LLM serving systems have introduced techniques to improve throughput and latency for text generation, such as via KV cache management (Kwon et al., 2023; LMSYS Org, 2025), disaggregation (Zhong et al., 2024), or operation-level optimizations (Zhu et al., 2025). Recent work has extended LLM serving systems to multimodal models. EPD disaggregation (Singh et al., 2024) separates different stages onto dedicated resources for large multimodal models. CornServe (Ma et al., 2025) supports anyto-any multimodal models by splitting models into independently scalable components and automatically sharing components across applications. vLLM-Omni (vLLM Team, 2025) and SGLang-Diffusion (LMSYS Org, 2025) extend their respective frameworks to support omni-modality generation, including diffusion-based image and audio synthesis. Some other works improve the efficiency of speech model inference via context compression (Liu et al., 2025), speculative decoding (Li et al., 2025), or low-rank approximation (Kamahori et al., 2025). However, none of these systems address the challenge of serving SpeechLMs for high-throughput, real-time streaming generation. VOXSERVE addresses this gap by designing system to optimize TTFA and streaming viability, along with abstractions that account for the architectural diversity of SpeechLMs (stateful detokenizers, depth-wise models, and varying codebook representations). 6. Conclusion We presented VOXSERVE, streaming-centric serving system designed to efficiently deploy modern SpeechLMs. VOXSERVE introduces unified model execution interface that decouples system-level optimizations from modelspecific architectural details, enabling single serving framework to support wide range of SpeechLM designs. Building on this abstraction, VOXSERVE incorporates Figure 8. Multi-GPU serving performance. Top: p90 TTFA with data parallelism across up to four H100 GPUs for CosyVoice. Bottom: p90 TTFA for disaggregated inference across two GPUs for Step-Audio. Figure 9. Performance for throughput-oriented scenario, measured by total generated audio duration divided by execution latency, for CosyVoice model. Disaggregated Inference. The bottom panel shows p90 TTFA for disaggregated inference scenario with the StepAudio model (the largest), in which the LLM backbone and the detokenizer run on separate GPUs, using two H100 GPUs in total. We implement distributed scheduler that runs asynchronous execution loops on each device and coordinates inter-device communication. We compare against baseline system modified to operate under the same disaggregated setup. While the baseline exhibits high TTFA even at low request rates, VOXSERVE maintains low TTFA at substantially higher request rates, despite the additional inter-device latency, compared to the single-GPU case (Figure 6). 4.3.3. THROUGHPUT-ORIENTED INFERENCE While VOXSERVE is primarily designed for streaming applications, it can be readily adapted to other deployment scenarios by modifying the scheduler. To demonstrate this flexibility, we evaluate VOXSERVE in throughput-oriented settings, such as audiobook or podcast generation and synthetic data generation for model training (Zhang et al., 2023; Ju et al., 2025; Roy et al., 2026). In these scenarios, only end-to-end batch-generation throughput matters, and streaming-specific metrics (e.g., TTFA and streaming viability) are irrelevant. 8 VOXSERVE : Streaming-Centric Serving System for Speech Language Models streaming-aware scheduling policy and an asynchronous execution pipeline that jointly optimizes TTFA and sustained streaming viability. Across multiple state-of-the-art SpeechLMs and deployment scenarios, VOXSERVE substantially outperforms existing, model-specific serving implementations, achieving 1020 higher serving throughput at comparable latency while maintaining uninterrupted audio streaming."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Arora, S., Chang, K.-W., Chien, C.-M., Peng, Y., Wu, H., Adi, Y., Dupoux, E., Lee, H.-Y., Livescu, K., and Watanabe, S. On the landscape of spoken language models: comprehensive survey. arXiv preprint arXiv:2504.08528, 2025. Bakhturina, E., Lavrukhin, V., Ginsburg, B., and Zhang, Y. Hi-fi multi-speaker english tts dataset. arXiv preprint arXiv:2104.01497, 2021. Canopy Labs. Towards Human-Sounding TTS. http s://canopylabs.ai/model-releases, 2025. Accessed: 2026-01-29. Chen, Y., Yue, X., Zhang, C., Gao, X., Tan, R. T., and Li, H. Voicebench: Benchmarking llm-based voice assistants. arXiv preprint arXiv:2410.17196, 2024. Defossez, A., Mazare, L., Orsini, M., Royer, A., Perez, P., Jegou, H., Grave, E., and Zeghidour, N. Moshi: speech-text foundation model for real-time dialogue. arXiv preprint arXiv:2410.00037, 2024. Du, Z., Wang, Y., Chen, Q., Shi, X., Lv, X., Zhao, T., Gao, Z., Yang, Y., Gao, C., Wang, H., et al. Cosyvoice 2: Scalable streaming speech synthesis with large language models. arXiv preprint arXiv:2412.10117, 2024. ElevenLabs. Elevenlabs documentation. https://el evenlabs.io/docs/overview/intro, 2025. Accessed: 2026-01-29. Ethiraj, V., David, A., Menon, S., and Vijay, D. Toward lowlatency end-to-end voice agents for telecommunications using streaming asr, quantized llms, and real-time tts. arXiv preprint arXiv:2508.04721, 2025. Howard, A. How ai-powered testing enabled sub-second latency for agentforce voice. https://engineerin g.salesforce.com/how-ai-driven-testi ng-enabled-sub-second-latency-for-a gentforce-voice/, 2025. Accessed: 2026-01-29. Iribe, B., Kumar, A., and the Sesame team. Crossing the uncanny valley of conversational voice. https://www. sesame.com/research/crossing_the_unc anny_valley_of_voice, 2025. Accessed: 202601-29. Ito, K. and Johnson, L. The lj speech dataset. https: //keithito.com/LJ-Speech-Dataset/, 2017. Accessed: 2026-01-29. Ju, Z., Yang, D., Yu, J., Shen, K., Leng, Y., Wang, Z., Tan, X., Zhou, X., Qin, T., and Li, X. Mooncast: Highquality zero-shot podcast generation. arXiv preprint arXiv:2503.14345, 2025. Kamahori, K., Kasai, J., Kojima, N., and Kasikci, B. LiteASR: Efficient automatic speech recognition with In Proceedings of the 2025 low-rank approximation. Conference on Empirical Methods in Natural Language Processing, 2025. Kong, J., Kim, J., and Bae, J. Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis. Advances in neural information processing systems, 33:1702217033, 2020. Kumar, R., Seetharaman, P., Luebs, A., Kumar, I., and Kumar, K. High-fidelity audio compression with improved rvqgan. Advances in Neural Information Processing Systems, 2023. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th symposium on operating systems principles, 2023. Li, B., Wang, H., Zhang, S., Guo, Y., and Yu, K. Fast and high-quality auto-regressive speech synthesis via speculative decoding. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2025. Li, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani, I., Guestrin, C., Liang, P., and Hashimoto, T. B. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_ eval, 2023. Accessed: 2026-01-29. Liu, W., Chen, Q., Wang, W., Yang, G., Li, W., Fang, M., Zuo, J., Yang, X., Jin, T., Xu, J., et al. Speech token prediction via compressed-to-fine language modeling for In Proceedings of the 33rd ACM speech generation. International Conference on Multimedia, 2025. 9 VOXSERVE : Streaming-Centric Serving System for Speech Language Models LMSYS Org. Sglang diffusion: Accelerating video and image generation. https://lmsys.org/blog/202 5-11-07-sglang-diffusion/, 2025. Accessed: 2026-01-29. Ma, J. J., Chung, J.-W., Ahn, J., Liang, Y., Jajoo, A., Lee, M., and Chowdhury, M. Cornserve: Efficiently serving any-to-any multimodal models. arXiv preprint arXiv:2512.14098, 2025. Mousavi, P., Maimon, G., Moumen, A., Petermann, D., Shi, J., Wu, H., Yang, H., Kuznetsova, A., Ploujnikov, A., Marxer, R., et al. Discrete audio tokens: More than survey! arXiv preprint arXiv:2506.10274, 2025. NVIDIA. Triton inference server. https://github .com/triton-inference-server/server. Accessed: 2026-01-29. OpenAI. Introducing gpt-realtime and realtime api updates for production voice agents. https://openai.com /index/introducing-gpt-realtime/, 2025. Accessed: 2026-01-29. Peng, J., Wang, Y., Fang, Y., Xi, Y., Li, X., Zhang, X., and Yu, K. survey on speech large language models. arXiv preprint arXiv:2410.18908, 2024. Resemble AI. Chatterbox TTS, 2025. URL https:// huggingface.co/ResembleAI/chatterbox. Accessed: 2026-01-29. Roy, R., Raiman, J., Lee, S.-g., Ene, T.-D., Kirby, R., Kim, S., Kim, J., and Catanzaro, B. Personaplex: Voice and role control for full duplex conversational speech models. 2026. Schweiger, M. The 300ms rule: Why latency makes or breaks voice ai applications, 2025. URL https://ww w.assemblyai.com/blog/low-latency-voi ce-ai. Accessed: 2026-01-29. Shao, W. and Lee, D. Engineering low-latency voice agents. https://sierra.ai/blog/voice-latency, 2025. Accessed: 2026-01-29. Singh, G., Wang, X., Hu, Y., Yu, T., Xing, L., Jiang, W., Wang, Z., Bai, X., Li, Y., Xiong, Y., et al. Efficiently serving large multimodal models using epd disaggregation. arXiv preprint arXiv:2501.05460, 2024. Siperco, A. Alexa+ launches in canada, the first country to get the next generation of alexa outside the us. https: //www.aboutamazon.com/news/devices/a lexa-plus-canada-launch, 2025. Accessed: 2026-01-29. Siuzdak, H., Grotschla, F., and Lanzendorfer, L. A. Snac: Multi-scale neural audio codec. arXiv preprint arXiv:2410.14411, 2024. Staniszewski, M. Elevenlabs raises $180m series to be the voice of the digital world. https://elevenlabs.i o/blog/series-c, 2025. Accessed: 2026-01-29. Vaidya, N., Oh, F., and Comly, N. Optimizing inference on large language models with nvidia tensorrt-llm, now publicly available. NVIDIA Developer Blog, 2023. URL https://developer.nvidia.com/blog/op timizing-inference-on-llms-with-ten sorrt-llm-now-publicly-available/. Accessed: 2026-01-29. vLLM Team. Announcing vllm-omni: Easy, fast, and cheap omni-modality model serving. https://blog.v llm.ai/2025/11/30/vllm-omni.html, 2025. Accessed: 2026-01-29. Wu, B., Yan, C., Hu, C., Yi, C., Feng, C., Tian, F., Shen, F., Yu, G., Zhang, H., Li, J., et al. Step-audio 2 technical report. arXiv preprint arXiv:2507.16632, 2025. Yao, R. Bringing state-of-the-art gemini translation capabilities to google translate. https://blog.google/ products-and-platforms/products/sear ch/gemini-capabilities-translation-u pgrades/, 2025. Accessed: 2026-01-29. Ye, Z., Chen, L., Lai, R., Lin, W., Zhang, Y., Wang, S., Chen, T., Kasikci, B., Grover, V., Krishnamurthy, A., et al. Flashinfer: Efficient and customizable attention engine for llm inference serving. In Eighth Conference on Machine Learning and Systems, 2025. Zeghidour, N., Kharitonov, E., Orsini, M., Volhejn, V., de Marmiesse, G., Grave, E., Perez, P., Mazare, L., and Defossez, A. Streaming sequence-to-sequence learnarXiv preprint ing with delayed streams modeling. arXiv:2509.08753, 2025. Zen, H., Dang, V., Clark, R., Zhang, Y., Weiss, R. J., Jia, Y., Chen, Z., and Wu, Y. Libritts: corpus derived from librispeech for text-to-speech. arXiv preprint arXiv:1904.02882, 2019. Zeng, A., Du, Z., Liu, M., Wang, K., Jiang, S., Zhao, L., Dong, Y., and Tang, J. Glm-4-voice: Towards intelligent and human-like end-to-end spoken chatbot. arXiv preprint arXiv:2412.02612, 2024. Zhang, W., Yeh, C.-C., Beckman, W., Raitio, T., Rasipuram, R., Golipour, L., and Winarsky, D. Audiobook synthesis In 12th Speech with long-form neural text-to-speech. Synthesis Workshop (SSW) 2023, 2023. 10 VOXSERVE : Streaming-Centric Serving System for Speech Language Models Zheng, L., Yin, L., Xie, Z., Sun, C. L., Huang, J., Yu, C. H., Cao, S., Kozyrakis, C., Stoica, I., Gonzalez, J. E., et al. Sglang: Efficient execution of structured language model programs. Advances in neural information processing systems, 2024. Zhong, Y., Liu, S., Chen, J., Hu, J., Zhu, Y., Liu, X., Jin, X., and Zhang, H. Distserve: Disaggregating prefill and decoding for goodput-optimized large language model serving. In 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), 2024. Zhu, K., Gao, Y., Zhao, Y., Zhao, L., Zuo, G., Gu, Y., Xie, D., Ye, Z., Kamahori, K., Lin, C.-Y., et al. Nanoflow: Towards optimal large language model serving throughput. In 19th USENIX Symposium on Operating Systems Design and Implementation (OSDI 25), 2025. Zyphra Team. Beta Release of Zonos-v0.1. https://ww w.zyphra.com/post/beta-release-of-zon os-v0-1, 2025. Accessed: 2026-01-29. Figure 10. Serving performance for additional models. A. Evaluation Setup Details Experiments in 4 adopt the chunking strategies, sampling configurations, and reference-audio conditioning schemes specified in the model providers official inference implementations. CosyVoice employs chunk size of 15 with sampling parameters temperature = 0.8, top = 0.95, top = 50, and repetition penalty of 1.1. fixed reference audio clip from the LJSpeech dataset (Ito & Johnson, 2017) is used for voice conditioning. Following the baseline implementation, the detokenizer at each iteration receives both the reference audio tokens and the newly generated audio tokens from the LLM backbone as input. Orpheus uses chunk size of 28 with an overlap of 21, returning only the middle portion of each chunk to the client at every iteration. Its sampling configuration consists of temperature = 0.6, top = 0.8, and repetition penalty of 1.3. Voice conditioning is provided via preset voice specified directly in the prompt text. Step-Audio operates with chunk size of 25 and lookahead of 3 tokens, using temperature = 0.7, top = 0.9, and repetition penalty of 1.05. Voice conditioning is achieved through fixed reference waveform provided in the official GitHub repository. During detokenizer inference, the model reuses the KV cache and activation cache from the previous iteration in addition to the newly generated tokens. For CosyVoice and Orpheus, we use maximum batch size of 128. For Step-Audio, the maximum batch size is set to 32 due to the KV caches higher memory consumption. An overview of the baseline systems is provided below: CosyVoice: TensorRT-LLM (Vaidya et al., 2023) implementation for the LLM backbone, and Triton In11 VOXSERVE : Streaming-Centric Serving System for Speech Language Models Figure 11 reports p90 TTFA across these datasets for varying request rates for the CosyVoice model. The results show that VOXSERVE consistently achieves significantly lower TTFA than the baseline system across all input sources. Performance trends remain stable despite changes in input statistics, indicating robustness of VOXSERVE to datasetspecific properties. Figure 11. Serving performance with different input data sources. ference Server (NVIDIA) for the detokenizer, which consists of flow matching and vocoder. Orpheus: vLLM (Kwon et al., 2023) implementation for LLM backbone, and custom PyTorch implementation for SNAC-based detokenizer (Siuzdak et al., 2024). Step-Audio: customized vLLM implementation for LLM backbone and custom PyTorch implementation for CosyVoice-based detokenizer, with caching enabled. B. Additional Evaluation Results Here, we present additional evaluation results that complement the main experiments in 4. These results demonstrate that VOXSERVE generalizes across broader set of SpeechLM architectures and remains robust under varying input data distributions. B.1. Other Models In addition to the three primary models evaluated in the main paper, we assess VOXSERVE on several other modern SpeechLMs with diverse architectures and generation characteristics, including Chatterbox TTS, CSM, GLM-4Voice, and Zonos-v0.1. Figure 10 reports the serving performance of VOXSERVE on these models, measured by p90 and p99 TTFA under increasing request rates. Across all models, VOXSERVE maintains low TTFA while preserving high streaming viability over wide operating range. Although the absolute throughput differs across models due to architectural and computational differences, the results consistently demonstrate that VOXSERVE can efficiently serve heterogeneous SpeechLMs within unified system. B.2. Varying Input Statistics To evaluate robustness to input distribution shifts, we measure serving performance across different input datasets, including LibriTTS, the Hi-Fi Multi-Speaker English TTS dataset (Bakhturina et al., 2021), and the LJ Speech dataset (Ito & Johnson, 2017)."
        }
    ],
    "affiliations": [
        "Stanford University",
        "University of Washington"
    ]
}