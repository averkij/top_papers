{
    "paper_title": "Hyperspherical Latents Improve Continuous-Token Autoregressive Generation",
    "authors": [
        "Guolin Ke",
        "Hui Xue"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Autoregressive (AR) models are promising for image generation, yet continuous-token AR variants often trail latent diffusion and masked-generation models. The core issue is heterogeneous variance in VAE latents, which is amplified during AR decoding, especially under classifier-free guidance (CFG), and can cause variance collapse. We propose SphereAR to address this issue. Its core design is to constrain all AR inputs and outputs -- including after CFG -- to lie on a fixed-radius hypersphere (constant $\\ell_2$ norm), leveraging hyperspherical VAEs. Our theoretical analysis shows that hyperspherical constraint removes the scale component (the primary cause of variance collapse), thereby stabilizing AR decoding. Empirically, on ImageNet generation, SphereAR-H (943M) sets a new state of the art for AR models, achieving FID 1.34. Even at smaller scales, SphereAR-L (479M) reaches FID 1.54 and SphereAR-B (208M) reaches 1.92, matching or surpassing much larger baselines such as MAR-H (943M, 1.55) and VAR-d30 (2B, 1.92). To our knowledge, this is the first time a pure next-token AR image generator with raster order surpasses diffusion and masked-generation models at comparable parameter scales."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 5 3 3 4 2 . 9 0 5 2 : r Hyperspherical Latents Improve Continuous-Token Autoregressive Generation Guolin Ke1, Hui Xue2 1DP Technology, 2Peking University Code: https://github.com/guolinke/SphereAR"
        },
        {
            "title": "Abstract",
            "content": "Autoregressive (AR) models are promising for image generation, yet continuoustoken AR variants often trail latent diffusion and masked-generation models. The core issue is heterogeneous variance in VAE latents, which is amplified during AR decoding, especially under classifier-free guidance (CFG), and can cause variance collapse. We propose SphereAR to address this issue. Its core design is to constrain all AR inputs and outputsincluding after CFGto lie on fixed-radius hypersphere (constant ℓ2 norm), leveraging hyperspherical VAEs. Our theoretical analysis shows that hyperspherical constraint removes the scale component (the primary cause of variance collapse), thereby stabilizing AR decoding. Empirically, on ImageNet generation, SphereAR-H (943M) sets new state of the art for AR models, achieving FID 1.34. Even at smaller scales, SphereAR-L (479M) reaches FID 1.54 and SphereAR-B (208M) reaches 1.92, matching or surpassing much larger baselines such as MAR-H (943M, 1.55) and VAR-d30 (2B, 1.92). To our knowledge, this is the first time pure next-token AR image generator with raster order surpasses diffusion and masked-generation models at comparable parameter scales. Figure 1: Left: FID vs. parameters on ImageNet 256256 class-conditional generation, SphereAR attains lower FID with fewer parameters. Right: 256256 samples generated by SphereAR-L (479M)."
        },
        {
            "title": "Introduction",
            "content": "Autoregressive (AR) models have achieved remarkable success in text [1, 2] and have been extended to images [3, 4], speech [5], video [6], and other modalities [7]. Early multimodal AR systems discretized latents with vector quantization (VQ) [8, 9]; more recently, continuous-token AR dispenses Corresponding author: kegl@dp.tech Preprint. Figure 2: Overview of SphereAR. Left: hyperspherical VAE (S-VAE) encodes raw data into sequence of latent tokens constrained to fixed-radius hypersphere Sd1. The encoder outputs unit mean direction µ and concentration κ that parameterize von MisesFisher (vMF) or Power Spherical posterior. Right: causal Transformer with token-level diffusion head models the next-token distribution over the hyperspherical token sequence. At inference, the AR models predictions, including CFG-rescaled ones, are projected back onto the fixed-radius hypersphere. The VAE decoder then reconstructs the image from the predicted hyperspherical latents. with codebooks: VAE [10] emits token-level latents and the AR model predicts the next latent in continuous space (e.g., Gaussian mixtures [11] or diffusion objectives [12, 13]). Yet, when built on the same VAE latents, continuous-token AR models often trail latent diffusion and masked-generation models.2 Prior analyses attribute this gap to variance pathologies during AR decoding [13, 17]: latent variances are heterogeneous across dimensions/tokens and are amplified due to exposure bias and classifier-free guidance (CFG) [18], causing stepwise variance drift and collapse. Strengthening the KL term [11] or fixing large variance [13] improves stability but leaves the root cause intact: scale heterogeneity remains and can still drift during AR decoding with CFG. We address this with more principled solution: make all AR inputs and outputs scale-invariant. As illustrated in Fig. 2, the proposed SphereAR couples hyperspherical VAE (S-VAE) [19, 20] with an autoregressive Transformer [21] and token-level diffusion head [12]. The S-VAE constrains each latent token to fixed-radius hypersphere (constant ℓ2 norm), parameterizing only direction via unit mean direction vector µ and scalar concentration κ. During training, the AR model consumes these hyperspherical latents under teacher forcing. During inference, AR models predictionsincluding those after CFG rescalingare projected back onto the fixed-radius hypersphere to remove the radial (scale) component. Thus, every signal provided to or produced by the AR model is ℓ2-normalized to the same radius. concise theoretical justification supports these design choices, showing why scale-invariant inputs/outputs stabilize AR decoding and why hyperspherical posterior is preferable to Gaussian alternatives. Empirically, SphereAR-H (943M) sets new state of the art for AR models on ImageNet 256256 class-conditional generation, achieving FID 1.34. Even at smaller scales, SphereAR-L (479M) attains FID 1.54, outperforming comparably sized diffusion (DiT-XL/2, FID 2.27) and bidirectional maskedgeneration (MAR-L, FID 1.78) baselines, while matching MAR-H (943M, FID 1.55) with roughly half the parameters. At the base scale, SphereAR-B (208M) achieves FID 1.92, surpassing VAR-d20 (600M, FID 2.57) and the prior continuous-token AR model LatentLM-L (479M, FID 2.24), while matching VAR-d30 (2B, FID 1.92) with 10 fewer parameters. Ablations show that AR models with hyperspherical VAEs consistently outperform diagonal-Gaussian and fixed-variance σ-VAE [13] baselines; moreover, applying post-hoc normalization to diagonal-Gaussian latents helps but still underperforms S-VAE. To our knowledge, this is the first time pure next-token AR image generator with raster order surpasses diffusion and masked-generation models at comparable parameter scales. 2In this paper, autoregressive denotes token-by-token generation with unidirectional (causal) self-attention, excluding bidirectional masked/next-scale methods such as MaskGIT [14], MAR [12], and VAR [15]. With the same VAE latents, [12] reports an AR model at FID 4.69 vs. 1.98 for MAR and 2.27 for DiT [16]."
        },
        {
            "title": "2 Related Work",
            "content": "Image Tokenizers large body of work improves the performance of image tokenizers by enhancing reconstruction fidelity and semantic alignment. Typical ingredients include (i) refined training objectives [3, 22, 23], (ii) CLIP-aligned distillation for better text guidance [24, 25], and (iii) various decoder improvements [26, 23]. These techniques are orthogonal to our approach and can be combined with it. In parallel, complementary line of work targets the quantization mechanism itselfimproving codebook utilization, training stability, and the rate-distortion tradeoff. Building on VQ-VAE [9], extensions include hierarchical VQ-VAE-2 [27], residual/hierarchical quantization [28], and multi-codebook schemes [29]. Some methods also adopt spherical or normalized feature geometry in the quantizer: for instance, ViT-VQGAN [4] normalizes latent features before computing codebook distances, and BSQ [30] constructs binarized spherical latents for bit-efficient quantization. By contrast, comparatively less work targets continuous image tokenizers tailored to autoregressive modeling. Most prior approaches follow latent diffusion practice [16] and employ diagonal-Gaussian VAEs. GIVT [11] and LatentLM [13] mitigate instability by inflating or fixing latent variance (e.g., β-VAE, σ-VAE), which helps but does not remove scale degrees of freedom. NextStep-1 [17] instead normalizes Gaussian-posterior latents to constant norm, achieving scale invariance. However, both our theoretical analysis and empirical results indicate that hyperspherical posteriors are preferable to post-hoc normalization of diagonal-Gaussian latents. Autoregressive Image Generation Autoregressive image generation can be grouped into three families: next-scale, next-set, and next-token prediction. In next-scale prediction (e.g., VAR [15]), images are generated coarse-to-fine across resolutions; within each scale, context is modeled bidirectionally. In next-set prediction (also called masked generation; e.g., MaskGIT [14], MAR [12]), single scale is used and set of tokens is updated in parallel under bidirectional attention. In next-token prediction (e.g., VQGAN [3], LlamaGen [31]), the model follows language-style sequence modeling: one token is predicted at time with strictly unidirectional (causal) attention. We focus on next-token models because they align naturally with autoregressive language modeling and offer headroom for unified multimodal models. wide range of next-token variants has been explored: discrete tokens [3, 4, 31] vs. continuous tokens [11, 13]; raster order [31, 11] vs. randomized order [32, 33]; and more [34]. However, at comparable parameter scales, these models have often trailed next-set and next-scale approaches. key reason is the variance collapse that emerges during autoregressive decoding [13, 17]. We address this by enforcing scale-invariant latents via hyperspherical VAEs, thereby removing scale degrees of freedom. Empirically, this yields substantial gains for sequential AR decoding, with performance that matches or surpasses state-of-the-art next-set and next-scale methods at comparable model budgets."
        },
        {
            "title": "3 Method",
            "content": "We observe that with discrete tokens, next-token autoregressive (AR) models can outperform bidirectional masked-generation (MG) approaches. For example, LlamaGen-L (343M, FID 3.07; [31]) vs. MaskGIT (207M, FID 4.02; [14]). system-level study [35] further reports that, with discrete tokens, AR consistently achieves better FID than MG across model sizes from 166M to 3.1B. By contrast, with continuous tokens, MG is consistently stronger than AR. This divergencediscrete tokens thriving under AR while continuous tokens do notmotivates us to probe what truly differentiates the two. As illustrated in Fig.3, discrete tokens (Fig.3 a) are normalized on the probability simplex (components sum to 1), yielding scale-invariant inputs and outputs that stabilize AR decoding. In contrast, diagonal-Gaussian latents (Fig.3 b) are unconstrained and can destabilize multi-step AR decoding due to scale drift that compounds across steps. We hypothesize this scale sensitivity is the key issue, and therefore constrain continuous latents to fixed-radius hypersphere to enforce constant norm (Fig.3 c). This idea underpins SphereAR: hyperspherical VAE paired with causal Transformer equipped with token-level diffusion head; we detail these components below. 3.1 From VAE to Hyperspherical VAE variational autoencoder (VAE) [10] is widely used to compress raw data into lower-dimensional latent vector. It consists of an encoder qϕ(z x) that parameterizes an approximate posterior over 3 (a) Discrete (probability simplex; (cid:80) = 1) (b) Diagonal-Gaussian (unconstrained) (c) Hyperspherical (constant ℓ2 norm) Figure 3: Visualization of token distributions. Each panel shows one token type, with three tokens in different colors. (a) Discrete tokens lie on the probability simplex and are intrinsically scale-invariant. (b) Diagonal-Gaussian latents are unconstrained in scale; despite KL prior, per-dimension/token variances remain heterogeneous. (c) Hyperspherical latents constrain each token to fixed norm (e.g., z2 = R), yielding scale-invariant representations. In practice, (a) and (c) are robust under AR decoding, whereas (b) is prone to scale drift and occasional variance collapse (e.g., with CFG). and decoder pψ(x z) that reconstructs from z. We train the model by maximizing the evidence lower bound (ELBO): L(ϕ, ψ; x) = Eqϕ(zx) (cid:2) log pψ(x z)(cid:3) DKL (cid:0)qϕ(z x) p(z)(cid:1). (1) By default, both the prior p(z) and the approximate posterior qϕ(z x) are parameterized as Gaussians with diagonal covariance; the prior is the isotropic standard Normal (0, I). Using the reparameterization trick, = µϕ(x) + σϕ(x) ϵ with ϵ (0, I), makes the sampling step differentiable so that gradients backpropagate from the decoder to the encoder. With this diagonal-Gaussian posterior, the encoders scale σϕ(x) is data-dependent and per-dimension, yielding heterogeneous variances across dimensions and tokens. This imbalance amplifies exposure bias and can trigger variance collapse in AR decoding, particularly under CFG [13, 17]. Hyperspherical VAE (S-VAE) To fully address this issue, we remove the scale degree of freedom in the latent representation, rendering the AR models inputs and outputs scale-invariant. Specifically, leveraging hyperspherical VAEs (S-VAEs) [19, 20], we constrain each latent token to lie on fixed-radius hypersphere. For each token, the S-VAE encoder parameterizes directional posterior on the unit sphere by outputting unit mean direction µϕ(x) Sd1 (via ℓ2 normalization; is the latent dimension) and nonnegative concentration κϕ(x) R0. For notational convenience, let µ = µϕ(x) and κ = κϕ(x). S-VAE adopts von MisesFisher (vMF) distribution [19] for the directional approximate posterior: (cid:0)κ(cid:1) exp(cid:0)κ µu(cid:1), Sd1, qϕ(u x) = Cd (2) 2 1 κ 2 (2π) 2 1(κ) where Cd(κ) = is the normalizing constant and Iν() is the modified Bessel function of the first kind. Intuitively, µ sets the preferred direction and κ controls concentration: κ = 0 gives the uniform distribution on Sd1, and larger κ yields tighter mass around µ. Because µu is the cosine similarity on the sphere, the density in equation 2 increases as aligns with µ. We take the prior over directions to be uniform on the sphere, p(u) = Unif(Sd1), and use fixed radius > 0 (hyperparameter) so that = is fed to the decoder. The ELBO becomes LS-VAE(ϕ, ψ; x) = Eqϕ(ux) (cid:2) log pψ(x = Ru)(cid:3) DKL (cid:0)qϕ(u x) p(u)(cid:1). (3) While vMF is principled for spherical latents, it can be less efficient due to the need for rejection sampling. For efficiency, we adopt the Power Spherical posterior [20] on Sd1, qϕ(u x) (cid:0)1 + µu(cid:1)κ Sd1, (4) , which preserves spherical support and rotational symmetry yet admits fully reparameterizable sampler without rejection sampling. For convenience, define the axial projection (cosine similarity) = µu [1, 1] with the affine transform = (c + 1)/2 [0, 1]. Under equation 4, the marginal of is Beta distribution with parameters determined by and κ: (cid:16) Beta α = d1 2 + κ, β = d1 2 (cid:17) , so that = 2C 1. (5) Sampling proceeds by drawing from the Beta and setting = 2C 1, then sampling unit vector uniformly in the tangent space orthogonal to µ and composing = µ + (cid:112) 1 c2 v, (6) optionally implemented via Householder transform to align reference basis with µ. This inverseCDF construction yields low-variance, fully reparameterizable gradients and improved numerical stability; the spherical ELBO in equation 3 remains unchanged with qϕ taken as Power Spherical. In downstream autoregressive models, we keep the radius fixed and renormalize latent inputs/outputs back to z2 = (also after CFG rescaling) to remove scale degrees of freedom. Why Scale-Invariant Inputs and Outputs Matter in AR We normalize each provisional nexttoken prediction by the radius-R projection NR(z) = z/z2 onto the hypersphere. At reference token on the sphere, the differential of NR is exactly the orthogonal projector onto the tangent space; thus, to first order, normalization removes radial (scale) perturbations and preserves only tangential (directional) ones. Consequently, composing normalization with the next-token predictor removes the radial (scale) component of the linearized one-step error prior to refeeding, so scale errors cannot accumulate across autoregressive steps. See Appendix for the formal statement and proof. Limitations of Gaussian Posterior with Post-hoc Normalization tempting alternative to achieve scale invariance is to retain diagonal-Gaussian posterior qϕ(z x) and normalize the sampled latents (via NR), before feeding them to the decoder (henceforth Gaussian+norm). However, this choice is theoretically suboptimal: it optimizes strictly looser variational bound than spherical posterior (see Appendix B). Intuitively, the decoder discards radius by normalization, yet the ELBO still incurs an extra nonnegative radial KL term that does not arise with hyperspherical posterior. By contrast, hyperspherical posterior aligns the training objective with the constant-norm constraint and avoids this mismatch. Moreover, hyperspherical posteriors are axially symmetric about µ and governed by single concentration parameter κ, whereas Gaussian+norm induces projected-normal (Angular Central Gaussian) directional law whose level sets are elliptical and generally not axially symmetric; this geometric mismatch makes it poorer fit to purely directional structure (details in Appendix B). Empirically (Sec. 4.3), S-VAE outperforms Gaussian+norm, corroborating this analysis. 3.2 Continuous-Token Autoregressive Transformer Given an image RHW 3, S-VAE encodes it into latent tensor Rhwd with fixed per-token norm: for every spatial location (i, j), Zi,j2 = (each Zi,j Rd). For sequential autoregressive modeling, we flatten in raster-scan (row-major) order to obtain sequence {z1, . . . , zl} of length = w, where zk is simply the latent at the k-th position in row-major order. We employ causal (unidirectional) Transformer to model the conditional distribution of the next token in the flat sequence. At position 1, the model takes the prefix {z1, . . . , zk1} as input and produces hidden state hk1 = (z<k; θ), where θ denotes the Transformer parameters. Optionally, discrete class labels or text prompts are prepended as conditioning tokens to the prefix and included in the causal context. To predict the next continuous token zk, we follow MAR [12] and attach token-level diffusion head. Conditioned on hk1, the head progressively transforms simple prior (e.g., (0, I)) into the data distribution of the next token zk. We train the diffusion head with Rectified Flow [36, 37]. Given prior z0 and continuous time (0, 1), we form the linear interpolation (0, I), target z1 = zk, The diffusion head, parameterized by ω, takes the noisy interpolation zt condition hk1 as inputs, and predicts velocity, vω (cid:0)zt k, t, hk1 (7) k, the scalar time t, and the (cid:1) Rd. The training target is the = (1 t) z0 zt + z1 k. 5 flow velocity along the straight path, dzt LRF = k, z1 z0 k, z vω (cid:0)zt k, t, hk1 (cid:1)(cid:13) 2 (cid:13) 2 . (8) , and the objective is mean-squared error: (cid:105) z0 dt = z1 (cid:104)(cid:13) (cid:13)z1 At inference, we initialize z0 vω(zt k, t, hk1) up to = 1 using uniform steps = 1/N (e.g., explicit Euler): (0, I) at = 0 and integrate the learned velocity field t+ + vω (cid:0)z k, t, hk1 (cid:1). (9) After steps, we enforce the constant-norm constraint with single projection onto the radius-R hypersphere: zk 1 2. The resulting token zk is then fed to the next AR step and ultimately to the VAE decoder. When using classifier-free guidance (CFG), the velocity at each step is obtained from guided (rescaled) combination of conditional and unconditional predictions; we perform no intermediate normalization and apply single constant-norm projection only after steps. (cid:14)z 1 3.3 Model Architectures S-VAE Although VQGAN-style CNN backbones [3] are effective for latent VAEs, their throughput is limited by large convolutional activation maps. To improve efficiency without sacrificing quality, we adopt hybrid backbone: the encoder uses lightweight CNN stem with downsampling for patchification, followed by stack of Transformer blocks; the decoder mirrors this with Transformer stack that refines latent tokens and lightweight CNN with upsampling for unpatchification and pixel reconstruction. This preserves the CNNs strong local inductive bias while leveraging the Transformers efficient global modeling at token resolution, yielding favorable speedquality trade-off. As shown in Appendix D, the hybrid matches CNN baselines in quality while being about 2.6 faster. Autoregressive Transformer Following prior work [31, 13], we adopt modern causal Transformer. Concretely, we use pre-norm Transformer blocks with RMSNorm [38, 39], FlashAttention for efficient attention computation [40], and SwiGLU feed-forward layers [41]. For image positional encoding, we employ 2D rotary embeddings (RoPE) [42] applied in raster-scan order. All self-attention is strictly unidirectional (causal mask). For the diffusion head, we follow MAR [12] and use an MLP architecture."
        },
        {
            "title": "4 Experiments",
            "content": "We evaluate SphereAR on ImageNet-1K [43] class-conditional generation of resolution of 256256, comparing against previous strong baselines. Beyond end-to-end comparisons, we include targeted studies to substantiate our design choices, focusing on the following questions: (1) S-VAE vs. diagonal-Gaussian: Does S-VAE outperform diagonal-Gaussian VAEs for continuous-token AR? (2) Post-hoc normalization: If we apply ℓ2 normalization to latents from diagonal-Gaussian VAE, how does it compare with S-VAE? (3) Component contributions: Which parts of S-VAE drive the gains(i) the hyperspherical posterior, (ii) normalization applied to the VAE decoder input, or (iii) normalization applied to AR inputs/outputs? 4.1 Implementation Details S-VAE We adopt Power Spherical [20] directional posterior with latent dimensionality = 16 and fix the radius to = d. Complete setting for the S-VAEs backbone is provided in Appendix D. We train S-VAE from scratch on ImageNet-1K [43] with random-crop augmentation, optimizing weighted sum of ELBO (reconstruction + KL), perceptual [44, 45], and adversarial [46] losses. Optimization uses AdamW [47, 48] for 100 epochs (batch size 256, learning rate 1 104, β = (0.9, 0.95), weight decay 0.05). Autoregressive Transformer Following MAR [12], we instantiate three model sizes for SphereAR. SphereAR-B uses 24 Transformer blocks (hidden size 768) and diffusion head with 6 feed-forward blocks (hidden size 768). SphereAR-L uses 32 Transformer blocks (hidden size 1024) and diffusion head with 8 feed-forward blocks (hidden size 1024). SphereAR-H uses 40 Transformer blocks (hidden size 1280) and diffusion head with 12 feed-forward blocks (hidden size 1280). As in 6 Table 1: Overall comparison on ImageNet 256256 class-conditional generation. Abbreviations: AR = next-token (causal) autoregression; Mask. = masked generation (next-set); N.S. = next-scale; Diff. = diffusion. An asterisk (*) indicates models trained at 384384 and evaluated at 256256 by resizing. Model Type Order #Params #Epochs FID IS Pre. Rec. Discrete Tokens AR VQGAN [3] AR ViT-VQGAN [4] LlamaGen-L [31] AR LlamaGen-XL* [31] AR LlamaGen-XXL* [31] AR AR RandAR-L [32] AR RandAR-XL [32] AR RandAR-XXL [32] AR RAR-B [33] AR RAR-L [33] Mask. MaskGIT [14] Mask. MAGVIT-v2 [49] N.S. VAR-d20 [15] N.S. VAR-d30 [15] Continuous Tokens LDM-4 [50] DiT-XL/2 [16] SiT-XL/2 [51] GIVT [11] LatentLM-L [13] MAR-B [12] MAR-L [12] MAR-H [12] SphereAR-B (Our) SphereAR-L (Our) SphereAR-H (Our) Diff. Diff. Diff. AR AR Mask. Mask. Mask. AR AR AR raster raster raster raster raster random random random hybrid hybrid random random - - - - - raster raster random random random raster raster raster 1.4B 1.7B 343M 775M 1.4B 343M 775M 1.4B 261M 461M 227M 307M 600M 2B 400M 675M 675M 1.67B 479M 208M 479M 943M 208M 479M 943M 240 240 300 300 300 300 300 300 400 400 300 270 350 350 - 400 400 500 400 800 800 800 400 400 5.20 280.3 3.04 227.4 3.07 256.1 2.62 244.1 2.34 253.9 2.55 288.8 2.22 314.2 2.15 322.0 1.95 290.5 1.70 299.5 4.02 355.6 1.78 319.4 2.57 302.6 1.92 323.1 3.60 247.7 2.27 278.2 2.06 277.5 2.59 - 2.24 253.8 2.31 281.7 1.78 296.0 1.55 303.7 1.92 277.8 1.54 295.9 1.34 300.0 - - 0.83 0.80 0.80 0.81 0.80 0.79 0.82 0.81 0.78 - 0.83 0.82 0.87 0.83 0.83 0.81 - 0.82 0.81 0.81 0.81 0.80 0. - - 0.52 0.57 0.59 0.58 0.60 0.62 0.58 0.60 0.50 - 0.56 0.59 0.48 0.57 0.59 0.57 - 0.57 0.60 0.62 0.61 0.63 0.64 MAR, we employ multiple class-conditioning tokens (16 in our models vs. 64 in MAR) and apply class-token dropout with probability 0.1 during training to enable classifier-free guidance (CFG) at inference. Models are trained on ImageNet-1K for 400 epochs using S-VAE latents with AdamW (batch size 512, β = (0.9, 0.95), weight decay 0.05), cosine learning-rate schedule with 20k linear warmup steps and peak learning rate 3 104, and an exponential moving average (EMA) of weights with decay 0.9999. Under these settings, SphereAR-B, SphereAR-L and SphereAR-H contain 208M, 479M and 943M parameters, respectively. Inference Settings For next-token prediction we integrate the learned velocity field with fixedstep Euler scheme (100 steps). We use the linear CFG schedule from MAR. We enable KV cache to improve autoregressive decoding efficiency. 4. Image Generation Result We report Fréchet Inception Distance (FID) [52] as the primary metric, computed on 50k samples drawn with fixed random seed using the ADM evaluation code [53]. The optimal CFG scale is determined through sweep with step size of 0.1. Following MAR [12], we additionally report Inception Score (IS) [54] and Precision/Recall (Pre./Rec.) [55]. From the results summarized in Table 1, we observe: (1) SphereAR-H (943M) achieves state-ofthe-art FID 1.34, outperforming VAR-d30 (2B, 1.92) and MAR-H (943M, 1.55). (2) SphereAR is parameter-efficient. At large scale, SphereAR-L (479M) matches MAR-H (943M, 1.55) with roughly half the parameters. Even at the base scale, SphereAR-B (208M) reaches FID 1.92, outperforming 2B-parameter VAR, diffusion baselines (DiT and SiT), prior continuous-token AR models (GIVT 7 Figure 4: Impact of VAE variants on generation performance (FID vs. CFG). All variants share the same backbone and training/evaluation setup; only the VAE objective/posterior differs. Left: diagonalGaussian with enlarged KL weight (G-01/04/08/16), σ-VAE with fixed scale (F-01/02/05), and S-VAE with Power Spherical posterior (S-01/04/08). Right: additionally includes diagonalGaussian with post-hoc normalization (N-01/04/08/16). and LatentLM-L), and larger discrete AR models (LlamaGen and RandAR). (3) Hyperspherical latents are critical. The key difference from LatentLM is the latent parameterizationfixed-variance diagonal-Gaussian vs. hyperspherical. The large gap (SphereAR-L: 1.54 vs. LatentLM-L: 2.24) indicates that constant-norm, directional latents are crucial for high-quality AR decoding. Overall, SphereAR delivers scale-invariant AR model that sets the best reported FID with far fewer parameters and outperforms diffusion, masked-generation, and next-scale baselines. Appendix shows qualitative results. 4.3 Ablation Study All variants in this ablation use the same model backbone and training/evaluation configuration; only the VAE objective/posterior differs. To reduce compute, each VAE is trained on ImageNet for 50 epochs. For the AR stage we use the SphereAR-L backbone, trained for 50 epochs with constant learning rate 1 104 and batch size 256; all other settings follow Sec. 4.2. S-VAE vs. Diagonal-Gaussian The core design of SphereAR is to constrain latents on hypersphere via S-VAE. We compare it with prior VAEs. In particular, we evaluate three settings: (1) Diagonal-Gaussian (β-VAE). standard diagonal-Gaussian posterior trained with an up-weighted KL term.3 We sweep four KL weights {0.01, 0.04, 0.08, 0.16}, denoted G-01, G-04, G-08, and G-16. (2) σ-VAE (fixed variance). Following LatentLM [13], we fix the posterior scale with fixed, non-learned scalar σ (0, Cσ), and sample = µϕ(x) + σ ϵ with ϵ (0, I). We sweep Cσ {0.1, 0.2, 0.5} (denoted F-01, F-02, F-05). (3) S-VAE (hyperspherical). Power Spherical posterior on Sd1 with KL-weight sweep {0.001, 0.004, 0.008}, denoted S-01, S-04, S-08. Fig. 4 (left) plots FID versus CFG for these posteriors. We observe: (1) S-VAE is consistently best and most stable across CFG. In particular, S-04 attains the lowest FID and S-08 is close second. (2) Stronger regularization helps diagonal-Gaussian but saturates. Increasing β (or Cσ) improves the curves, yet they become unstable at larger CFG and remain below S-VAE. (3) Fixed variance offers no advantage. σ-VAE variants achieve performance on par with standard diagonal-Gaussian VAEs (e.g., F-02 vs. G-04; F-05 vs. G-08), indicating that fixing the posterior scale does not help. 3Most prior VAEs compute the KL by taking sum over spatial-channel dimensions (h d) and then batch mean. We instead take mean over all elements (batch and spatial-channel), which lowers the numerical KL value; to match the effective regularization strength, we therefore use larger KL weights (our 102 roughly matches prior 2 106). 8 Overall, the above ablation isolates the tokenizers role: hyperspherical VAEs yield the most robust and best final AR performance. Post-hoc Normalization on Diagonal-Gaussian As discussed in Sec. 3.1, simple alternative to achieve scale invariance is to apply post-hoc normalization to latents from diagonal-Gaussian posterior. We therefore run an empirical ablation. Starting from G-x models, we project each latent onto the radius-R hypersphere via z/z2, yielding N-01, N-04, N-08, and N-16. Fig. 4 (right) plots FID versus CFG for these variants. From these results, we observe: (1) Post-hoc normalization helps. Each N-x improves over its G-x counterpart and is more stable at higher CFG scales, supporting our motivation that scaleinvariant inputs/outputs stabilize AR decoding. (2) S-VAE remains the best. S-04 outperforms the best post-hocnormalized Gaussian (N-08). This aligns with our analysis: Gaussian with posthoc normalization optimizes strictly looser variational bound than the hyperspherical ELBO (Appendix B) and induces non-axially symmetric directional law on Sd1. Table 2: Ablation of normalization (applied to the VAE decoder and AR) and posterior family. S-VAEs Component Contributions The above ablations indicate that both the normalization on latent tokens and the hyperspherical posterior are important. Because the normalization can affect two interfacesthe VAE decoders input and the AR models inputs/outputswe further isolate their effects by conducting variant that normalizes only the VAE decoders input. As summarized in Table 2, normalization applied to the AR inputs and outputs is more critical: normalizing only the VAE decoders input yields modest gain (FID 2.97 2.89), whereas additionally normalizing AR inputs/outputs produces larger improvement (FID 2.89 2.68). This matches our analysis: the AR pathway re-feeds tokens step by step, so scale errors would otherwise accumulate, while the VAE decoder consumes its input once and does not induce cascading scale drift. Finally, replacing the diagonal-Gaussian posterior with hyperspherical one gives further boost (FID 2.68 2.52), confirming that aligning the posterior with constant-norm geometry is beneficial. 2.97 240.2 Gaussian 2.89 254.3 Gaussian 2.68 257.3 Gaussian Spherical 2.52 258.4 Norm. on VAE Decoder Norm. on AR Posterior FID IS"
        },
        {
            "title": "5 Conclusion",
            "content": "To address variance collapse in continuous-token AR models, we propose SphereAR, whose core idea is to make all AR inputs and outputs scale-invariant. Concretely, it consists of (1) hyperspherical VAE (S-VAE) that produces latent tokens constrained to fixed-radius hypersphere; and (2) an autoregressive Transformer with token-level diffusion head modeling the next-token distribution over hyperspherical latents. During AR training and inference, all inputs and outputsincluding those after CFG rescalingare normalized onto this hypersphere. Our theoretical analysis demonstrates that scale-invariant inputs and outputs are critical to AR modeling. On ImageNet class-conditional generation, SphereAR-H (943M) achieves FID 1.34 and SphereAR-L (479M) achieves FID 1.54, surpassing prior diffusion and masked-generation baselines. Ablations point to two key factors: constant-norm AR refeeding and hyperspherical posterior. With both, S-VAE is best, exceeding diagonal-Gaussian, σ-VAE, and even diagonal-Gaussian with post-hoc normalization. Future work While our results substantiate the motivation and design choices of SphereAR, several extensions would further strengthen this work: (i) exploring Riemannian Flow Matching (RFM) [56], which may better align with hyperspherical latent geometry since trajectories of RFM remain on the hypersphere; and (ii) extending SphereAR to multimodal applications. We leave these to future work."
        },
        {
            "title": "Acknowledgments",
            "content": "We are grateful to Prof. Di He for his careful reading and helpful comments on earlier drafts of this manuscript."
        },
        {
            "title": "References",
            "content": "[1] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. [2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [3] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. [4] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627, 2021. [5] Lingwei Meng, Long Zhou, Shujie Liu, Sanyuan Chen, Bing Han, Shujie Hu, Yanqing Liu, Jinyu Li, Sheng Zhao, Xixin Wu, et al. Autoregressive speech synthesis without vector quantization. arXiv preprint arXiv:2407.08551, 2024. [6] Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tianning Zhang, WQ Zhang, Weifeng Luo, et al. Magi-1: Autoregressive video generation at scale. arXiv preprint arXiv:2505.13211, 2025. [7] Shuqi Lu, Haowei Lin, Lin Yao, Zhifeng Gao, Xiaohong Ji, Linfeng Zhang, Guolin Ke, et al. Uni-3dar: Unified 3d generation and understanding via autoregression on compressed spatial tokens. arXiv preprint arXiv:2503.16278, 2025. [8] Robert Gray. Vector quantization. IEEE Assp Magazine, 1(2):429, 1984. [9] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. [10] Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. [11] Michael Tschannen, Cian Eastwood, and Fabian Mentzer. Givt: Generative infinite-vocabulary transformers. In European Conference on Computer Vision, pages 292309. Springer, 2024. [12] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems, 37:5642456445, 2024. [13] Yutao Sun, Hangbo Bao, Wenhui Wang, Zhiliang Peng, Li Dong, Shaohan Huang, Jianyong Wang, and Furu Wei. Multimodal latent language modeling with next-token diffusion. arXiv preprint arXiv:2412.08635, 2024. [14] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1131511325, 2022. [15] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2024. [16] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [17] NextStep Team, Chunrui Han, Guopeng Li, Jingwei Wu, Quan Sun, Yan Cai, Yuang Peng, Zheng Ge, Deyu Zhou, Haomiao Tang, et al. Nextstep-1: Toward autoregressive image generation with continuous tokens at scale. arXiv preprint arXiv:2508.10711, 2025. [18] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 10 [19] Tim Davidson, Luca Falorsi, Nicola De Cao, Thomas Kipf, and Jakub Tomczak. Hyperspherical variational auto-encoders. arXiv preprint arXiv:1804.00891, 2018. [20] Nicola De Cao and Wilker Aziz. The power spherical distribution. arXiv preprint arXiv:2006.04437, 2020. [21] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [22] Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1570315712, 2025. [23] Jiawei Yang, Tianhong Li, Lijie Fan, Yonglong Tian, and Yue Wang. Latent denoising makes good visual tokenizers. arXiv preprint arXiv:2507.15856, 2025. [24] Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and Furu Wei. Beit v2: Masked image modeling with vector-quantized visual tokenizers. arXiv preprint arXiv:2208.06366, 2022. [25] Liao Qu, Huichao Zhang, Yiheng Liu, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Daniel Du, Zehuan Yuan, and Xinglong Wu. Tokenflow: Unified image tokenizer for multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 25452555, 2025. [26] Yinbo Chen, Rohit Girdhar, Xiaolong Wang, Sai Saketh Rambhatla, and Ishan Misra. Diffusion autoencoders are scalable image tokenizers. arXiv preprint arXiv:2501.18593, 2025. [27] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019. [28] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1152311532, 2022. [29] Xiang Li, Kai Qiu, Hao Chen, Jason Kuen, Jiuxiang Gu, Bhiksha Raj, and Zhe Lin. Imagefolder: Autoregressive image generation with folded tokens. arXiv preprint arXiv:2410.01756, 2024. [30] Yue Zhao, Yuanjun Xiong, and Philipp Krähenbühl. Image and video tokenization with binary spherical quantization. arXiv preprint arXiv:2406.07548, 2024. [31] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. [32] Ziqi Pang, Tianyuan Zhang, Fujun Luan, Yunze Man, Hao Tan, Kai Zhang, William Freeman, and Yu-Xiong Wang. Randar: Decoder-only autoregressive visual generation in random orders. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 4555, 2025. [33] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-Chieh Chen. Randomized autoregressive visual generation. arXiv preprint arXiv:2411.00776, 2024. [34] Tianhong Li, Qinyi Sun, Lijie Fan, and Kaiming He. Fractal generative models. arXiv preprint arXiv:2502.17437, 2025. [35] Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen Sun, Michael Rubinstein, Deqing Sun, Kaiming He, and Yonglong Tian. Fluid: Scaling autoregressive text-to-image generative models with continuous tokens. arXiv preprint arXiv:2410.13863, 2024. [36] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [37] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 11 [38] Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in neural information processing systems, 32, 2019. [39] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In International conference on machine learning, pages 1052410533. PMLR, 2020. [40] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in neural information processing systems, 35:1634416359, 2022. [41] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. [42] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [43] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. [44] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In European conference on computer vision, pages 694711. Springer, 2016. [45] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. [46] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei Efros. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 11251134, 2017. [47] Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [48] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [49] Lijun Yu, José Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. [50] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [51] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In European Conference on Computer Vision, pages 2340. Springer, 2024. [52] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [53] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. [54] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. [55] Tuomas Kynkäänniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. Advances in neural information processing systems, 32, 2019. [56] Ricky TQ Chen and Yaron Lipman. Flow matching on general geometries. arXiv preprint arXiv:2302.03660, 2023. First-Order Stability of Radius Projection in AR be the radial projection NR(z) = z/z2, where Sd1 Let NR : Rd {0} Sd1 z2 = R}. All linearizations are taken at reference tokens zk = uk with uk2 = 1. Let denote the pre-normalization next-token map implemented by our model: given the prefix z<k, the causal Transformer produces condition hk1, and the diffusion head returns provisional latent zk = g(z<k) Rd, which is then projected by NR. We assume is continuously differentiable in neighborhood of z<k (i.e., 1: its Jacobian g/z<k exists and varies continuously there), which holds for our architecture with smooth activations and fixed-step explicit ODE solvers. 4 = {z Rd : Lemma 1 (Jacobian is the tangent-space projector). For zk2 = R, DNR(zk) = Pk := zkz R2 . (10) Moreover, in particular, Pk2 = 1. Proof. Differentiate NR(z) = z1 2 = Pk, Pk zk = 0, and Pkv = for all Tzk = Pk, P2 Sd1 = {v : and evaluate at zk. = 0}; Lemma 2 (First-order scale invariance). For any small z, NR(zk + z) = zk + Pk + o(z), (11) so radial derivatives vanish and tangential derivatives are preserved (eigenvalues 0 and 1). Proof. First-order Taylor expansion with Lemma 1. Proposition (One-step AR refeeding error, linearized). Let be the (unnormalized) next-token predictor and define zk = g(z<k), zk = NR(zk), and Fk = NR g. Linearizing at z<k (with zk = Fk(z<k)) yields ek := zk zk Pk (cid:16) Jk e<k + ηk (cid:17) , Jk = (cid:17) (cid:16) z<k , z<k (12) where e<k stacks the prefix errors and ηk collects local modeling/integration error. Proof. Chain rule with Lemma 2. Corollary (No scale-channel cascade; norm bound). Writing Jke<k + ηk = αk zk + tk with tk Tzk Sd1 , ek Pk(αk zk + tk) = tk, and ek2 PkJk2 e<k2 + Pkηk2. (13) Thus radial (scale) errors are annihilated before refeeding and cannot cascade along the AR chain; only directional (tangential) errors propagate. Scope. Statements are local (first-order) at points on Sd1 and assume is 1 near z<k. Gaussian Posterior with Post-hoc Normalization: Looser Bound We compare the Gaussian+norm objective LG(ϕ, ψ; x) = Eqϕ(zx) (cid:2) log pψ(x NR(z))(cid:3) DKL (cid:0)qϕ(z x) p(z)(cid:1), NR(z) = z2 , with the spherical ELBO LS-VAE(ϕ, ψ; x) = Eqϕ(ux) (cid:2) log pψ(x Ru)(cid:3) DKL (cid:0)qϕ(u x) Unif(Sd1)(cid:1), Sd1. Write the polar decomposition = (r, u) with = z2 R0 and = z/z2 Sd1. Let qϕ(u x) be the pushforward of qϕ(z x) under (cid:55) u. Since NR(z) = Ru depends only on direction, the reconstruction terms coincide: (cid:2) log pψ(x NR(z))(cid:3) = Eqϕ(ux) 4For numerical robustness we implement NR(z) = z/ max(z2, ε) with ε = 107. (cid:2) log pψ(x Ru)(cid:3). Eqϕ(zx) 13 (a) Power Spherical density (axially symmetric about µ). (b) Gaussian with post-hoc normalization (projected normal / ACG): elliptical level sets, not axially symmetric. Figure 5: Directional distributions on the sphere. Left: Power Spherical respects purely directional geometrythe density depends only on µu and is axially symmetric, with single concentration parameter κ. Right: Gaussian+norm induces projected-normal (ACG) law whose level sets follow uΣ1u; symmetry axes are determined by Σ (and the Gaussian mean), so the density is typically elliptical rather than axially symmetric. For the isotropic Gaussian prior p(z) = (0, I), one has p(z) = p(r) Unif(u) (with p(r) the χ-law in Rd). The KL chain rule (disintegration over u) gives DKL (cid:0)qϕ(z x) p(z)(cid:1) = DKL (cid:104) (cid:0)qϕ(u x) Unif(Sd1)(cid:1) + Eqϕ(ux) (cid:0)qϕ(r u, x) p(r)(cid:1)(cid:105) DKL . Combining the two displays yields (cid:104) LG(ϕ, ψ; x) = LS-VAE(ϕ, ψ; x) Eqϕ(ux) (cid:0)qϕ(r u, x) p(r)(cid:1)(cid:105) DKL LS-VAE(ϕ, ψ; x), with equality only if qϕ(r u, x) = p(r) almost surely (i.e., the posteriors radial law exactly matches the prior and is independent of x, u). Thus, Gaussian posterior with post-hoc normalization pays an extra nonnegative radial KL penalty while the decoder discards radius; spherical posterior avoids this mismatch and aligns the bound with the constant-norm constraint. Remark (axial symmetry vs. projected normal on Sd1). The Power Spherical (PS) density on the unit sphere has the form fPS(u) = Zd(κ) (cid:0)1 + µu(cid:1)κ with µ2 = 1 and κ 0. It is axially rotationally symmetric about µ: for any rotation with Qµ = µ, one has fPS(Qu) = fPS(u). The single scalar κ monotonically controls geodesic concentration (with κ = 0 yielding the uniform law). By contrast, Gaussian+normtake (µg, Σ) in Rd and map to the sphere via = z/z2induces projected-normal (Angular Central Gaussian, ACG) directional law. For the zeromean case (µg = 0), its density is fACG(u) (uΣ1u)d/2: level sets follow the quadratic form uΣ1u and are generally elliptical, not axially symmetric; axial symmetry holds only if Σ (then the law is uniform). With nonzero mean µg = 0, the density also depends on µ Σ1u, producing skewed, non-axially symmetric level sets (and, for anisotropic Σ, possible antipodal bimodality when µg = 0). Therefore the Power Spherical family matches the intended purely directional geometry on Sd1, whereas Gaussian+norm inherits Euclidean anisotropy from (µg, Σ) and does not. Comparison with MARs VAE We perform an ablation that swaps our S-VAE for MARs VAE [12] while keeping the AR backbone, training schedule, and evaluation protocol identical (see Sec. 4.3). As summarized in Table 3, S-VAE yields large FID gain (4.54 2.52) and higher IS (241.6 258.4). These results indicate that constant-norm, directional latents from S-VAE materially strengthen continuous-token AR generation. 14 Table 3: Ablation: swapping our S-VAE for MARs VAE without retraining (same AR backbone and training/evaluation settings; only the VAE changes). FID IS Pre. Rec. Type AR + MARs VAE AR + S-VAE (ours) 4.54 2.52 241.6 258.4 0.84 0.82 0.45 0. Table 4: Comparison of VAE backbones. Training speed measured in iterations per second on 8 A100 GPUs, with batch size 256. Type #Params Training Speed LPIPS FID IS Pre. Rec. CNN ViT Hybrid (ours) 70M 170M 75M 2.25 7.19 5.81 0.167 0.178 0.166 2.63 262.6 2.81 251.3 2.52 258.4 0.82 0.82 0.82 0.55 0.55 0.56 VAE Architecture CNN vs. ViT vs. Hybrid Most latent VAEs for image generation adopt VQGAN-style encoder-decoder [3], i.e., convolutional (CNN) backbone with downsampling/upsampling blocks. This design is parameter-efficient (70M) but throughput is often limited by activation memory and bandwidth on large feature maps, leading to slow training and inference. ViT-VQGAN [4] replaces the CNN backbone with Vision Transformer (ViT) to improve efficiency; however, as shown in Table 4, pure ViT backbone yields weaker generative metrics than CNN. To balance efficiency and quality, we adopt hybrid VAE architecture. The encoder first uses lightweight CNN stem (with downsampling blocks) for patchification and early spatial mixing, imparting CNN inductive bias while reducing spatial resolution. The resulting patch tokens are then processed by bidirectional Transformer (ViT blocks) to model long-range dependencies. The decoder mirrors this design: ViT stack refines the latent tokens, followed by lightweight CNN (with upsampling blocks) for unpatchification and pixel-level reconstruction. This hybrid preserves the CNNs strong local bias while leveraging the ViTs global receptive field at token resolution, yielding favorable speed-quality tradeoff. In our implementation, to match the parameter scale of VQGAN-style CNN encoder-decoder [3], our S-VAE uses 6 ViT blocks in the encoder and 12 in the decoder, each with hidden size 512. The encoders CNN stem performs patchification via 4 downsampling stages (overall 16 reduction) with channel widths [64, 64, 128, 256, 512]; the decoder mirrors this with 4 upsampling stages and an extra residual block per stage. In total, the S-VAE has 75M parameters. We compare three encoder-decoder backbones under the same training setup and losses: (i) VQGAN-style CNN [3]; (ii) pure ViT [4] (12 Transformer blocks in both encoder and decoder, hidden size 768; 170M params); and (iii) our Hybrid design. To reduce compute, the training follows the setting in Sec. 4.3. We report training throughput (iterations/s) of VAE, perceptual distortion (LPIPS with VGG-16), and downstream ImageNet generative metrics. Results in Table 4 show: ViT is fastest (7.19 it/s) but slightly worse on LPIPS/FID/IS; the CNN is slowest (2.25 it/s) yet competitive in IS; our Hybrid retains the best reconstruction (LPIPS 0.166) and the best FID (2.52) while running at 5.81 it/sabout 2.6 faster than the CNN and at 81% of ViT throughput (5.81 vs 7.19 it/s). Overall, the hybrid backbone offers the most favorable speed-quality trade-off."
        },
        {
            "title": "E Model Generated Examples",
            "content": "We show the uncurated 256256 samples generated by our 479M SphereAR-L, from Fig. 6 to Fig. 17. 15 Figure 6: Uncurated 256256 SphereAR-L samples. Class label: \"frilled lizard\" (43). Figure 7: Uncurated 256256 SphereAR-L samples. Class label: \"sulphur-crested cockatoo\" (89). Figure 8: Uncurated 256256 SphereAR-L samples. Class label: \"golden retriever\" (207). Figure 9: Uncurated 256256 SphereAR-L samples. Class label: \"Border collie\" (232). Figure 10: Uncurated 256256 SphereAR-L samples. Class label: \"tabby cat\" (281). Figure 11: Uncurated 256256 SphereAR-L samples. Class label: \"ladybug\" (301). 17 Figure 12: Uncurated 256256 SphereAR-L samples. Class label: \"chimpanzee\" (367). Figure 13: Uncurated 256256 SphereAR-L samples. Class label: \"beacon\" (437). Figure 14: Uncurated 256256 SphereAR-L samples. Class label: \"castle\" (483). 18 Figure 15: Uncurated 256256 SphereAR-L samples. Class label: \"icecream\" (928). Figure 16: Uncurated 256256 SphereAR-L samples. Class label: \"cliff\" (972). Figure 17: Uncurated 256256 SphereAR-L samples. Class label: \"coral reef\" (973)."
        }
    ],
    "affiliations": [
        "DP Technology",
        "Peking University"
    ]
}