{
    "paper_title": "Skin Tokens: A Learned Compact Representation for Unified Autoregressive Rigging",
    "authors": [
        "Jia-peng Zhang",
        "Cheng-Feng Pu",
        "Meng-Hao Guo",
        "Yan-Pei Cao",
        "Shi-Min Hu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid proliferation of generative 3D models has created a critical bottleneck in animation pipelines: rigging. Existing automated methods are fundamentally limited by their approach to skinning, treating it as an ill-posed, high-dimensional regression task that is inefficient to optimize and is typically decoupled from skeleton generation. We posit this is a representation problem and introduce SkinTokens: a learned, compact, and discrete representation for skinning weights. By leveraging an FSQ-CVAE to capture the intrinsic sparsity of skinning, we reframe the task from continuous regression to a more tractable token sequence prediction problem. This representation enables TokenRig, a unified autoregressive framework that models the entire rig as a single sequence of skeletal parameters and SkinTokens, learning the complicated dependencies between skeletons and skin deformations. The unified model is then amenable to a reinforcement learning stage, where tailored geometric and semantic rewards improve generalization to complex, out-of-distribution assets. Quantitatively, the SkinTokens representation leads to a 98%-133% percents improvement in skinning accuracy over state-of-the-art methods, while the full TokenRig framework, refined with RL, enhances bone prediction by 17%-22%. Our work presents a unified, generative approach to rigging that yields higher fidelity and robustness, offering a scalable solution to a long-standing challenge in 3D content creation."
        },
        {
            "title": "Start",
            "content": "SkinTokens: Learned Compact Representation for Unified Autoregressive Rigging JIA-PENG ZHANG, BNRist, Department of Computer Science and Technology, Tsinghua University, China CHENG-FENG PU, Zhili College, Tsinghua University, China MENG-HAO GUO, BNRist, Department of Computer Science and Technology, Tsinghua University, China YAN-PEI CAO, VAST, China SHI-MIN HU, BNRist, Department of Computer Science and Technology, Tsinghua University, China 6 2 0 2 4 ] . [ 1 5 0 8 4 0 . 2 0 6 2 : r Fig. 1. Automated rigging with TokenRig. We present TokenRig, unified generative framework that produces high-quality rigs for diverse 3D assets. By leveraging SkinTokens, i.e., our novel, discrete representation for skinning weights, our method robustly generates high-fidelity skeletons and precise skinning maps (visualized as heatmaps) for complex, real-world geometries, ranging from stylized anime characters to quadrupeds and fantasy creatures. Project Page: https://zjp-shadow.github.io/works/SkinTokens/ The rapid proliferation of generative 3D models has created critical bottleneck in animation pipelines: rigging. Existing automated methods are fundamentally limited by their approach to skinning, treating it as an illposed, high-dimensional regression task that is inefficient to optimize and Authors Contact Information: Jia-Peng Zhang, zjp24@mails.tsinghua.edu.cn, BNRist, Department of Computer Science and Technology, Tsinghua University, Beijing, China; Cheng-Feng Pu, pcf22@mails.tsinghua.edu.cn, Zhili College, Tsinghua University, Beijing, China; Meng-Hao Guo, gmh20@mails.tsinghua.edu.cn, BNRist, Department of Computer Science and Technology, Tsinghua University, Beijing, China; Yan-Pei Cao, caoyanpei@gmail.com, VAST, Beijing, China; Shi-Min Hu, shimin@tsinghua.edu.cn, BNRist, Department of Computer Science and Technology, Tsinghua University, Beijing, China. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. 2026 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM XXXX-XXXX/2026/2-ART https://doi.org/10.1145/nnnnnnn.nnnnnnn is typically decoupled from skeleton generation. We posit this is representation problem and introduce SkinTokens: learned, compact, and discrete representation for skinning weights. By leveraging an FSQ-CVAE to capture the intrinsic sparsity of skinning, we reframe the task from continuous regression to more tractable token sequence prediction problem. This representation enables TokenRig, unified autoregressive framework that models the entire rig as single sequence of skeletal parameters and SkinTokens, learning the complicated dependencies between skeletons and skin deformations. The unified model is then amenable to reinforcement learning stage, where tailored geometric and semantic rewards improve generalization to complex, out-of-distribution assets. Quantitatively, the SkinTokens representation leads to 98%133% improvement in skinning accuracy over state-of-the-art methods, while the full TokenRig framework, refined with RL, enhances bone prediction by 17%22%. Our work presents unified, generative approach to rigging that yields higher fidelity and robustness, offering scalable solution to long-standing challenge in 3D content creation. Additional Key Words and Phrases: Auto-rigging Method, Auto-regressive Models , Vol. 1, No. 1, Article . Publication date: February 2026. 2 Jia-Peng Zhang, Cheng-Feng Pu, Meng-Hao Guo, Yan-Pei Cao, and Shi-Min Hu ACM Reference Format: Jia-Peng Zhang, Cheng-Feng Pu, Meng-Hao Guo, Yan-Pei Cao, and ShiMin Hu. 2026. SkinTokens: Learned Compact Representation for Unified Autoregressive Rigging. 1, 1 (February 2026), 14 pages. https://doi.org/10. 1145/nnnnnnn.nnnnnnn 1 introduction The rapid advancement of generative models has enabled the creation of 3D assets at an unprecedented scale [Jiang 2024; Li et al. 2025; Xiang et al. 2024; Zhang et al. 2024]. This progress, however, has exposed critical and long-standing bottleneck in the digital content pipeline: rigging. And skeleton-driven animation[Abu Rumman and Fratarcangeli 2015; Kim et al. 2017] is still an important part of the computer graphics industry, with rigging being an indispensable component. The manual process of creating skeleton and assigning skinning weightsessential for animationremains highly specialized, labor-intensive task. This fundamental mismatch in scalability between asset generation and rigging readiness must be addressed to unlock the full potential of modern 3D AI. Research in automatic rigging has produced variety of solutions. Many established approaches rely on template-based skeleton prediction [Blackman 2014; Li et al. 2021], where joint coordinates are fitted to fixed topology. While effective for common categories like humanoids, these methods exhibit limited generalizability. In contrast, template-free methods[Ma and Zhang 2023; Xu et al. 2022] such as RigNet [Xu et al. 2020] infer joint likelihoods via heatmaps before constructing skeletal edges, offering greater flexibility. More recently, inspired by the success of large language models, new class of autoregressive approaches has emerged [Deng et al. 2025; Guo et al. 2025a,b; Liu et al. 2025b; Song et al. 2025a,b; Zhang et al. 2025b]. By serializing the skeletal hierarchy into sequence of tokens, these models leverage the power of Transformer [Vaswani 2017] architectures to achieve remarkable generalization in skeleton generation. Yet, despite this rapid progress in skeletal generation, the equally critical task of skinning weight prediction remains significant and largely unsolved challenge. The vast majority of prior work treats skinning as separate, downstream problem, focusing on network architectures that regress the entire skinning matrix from geometric features [Liu et al. 2019; Xu et al. 2020]. This approach is fraught with fundamental issues. First, the direct regression of massive, high-dimensional ğ‘ ğ½ matrix is an ill-posed and inefficient learning problem. Skinning matrices are intrinsically sparse, but dense regression with standard losses like Mean Squared Error (MSE) struggles to enforce this prior, leading to noisy weights that manifest as visually jarring artifacts during motion. Second, many methods exhibit an excessive reliance on auxiliary geometric descriptors, such as geodesic distances [Baran and PopoviÄ‡ 2007; Dionne and de Lasa 2013]. This renders them fragile in realistic scenarios where meshes may be non-watertight or composed of disconnected components, leading to coarse or unreliable feature estimation. Most critically, the pervasive decoupling of skeleton and skinning prediction into independent models [Zhang et al. 2025b] creates , Vol. 1, No. 1, Article . Publication date: February 2026. conceptual barrier. This separation prevents any mutual reinforcement between the two tasks; the skeleton is generated without knowledge of the surface deformation it will induce, and the skinning is predicted for fixed, potentially suboptimal, skeletal structure. This architectural choice inherently constrains the performance ceiling of the entire system and is further compounded by the relative scarcity of datasets with comprehensive skeleton and skinning annotations [Deitke et al. 2024]. We argue that the path forward requires unified model, and that such unification is only possible with fundamental shift in the representation of skinning weights. To this end, we introduce SkinTokens, learned, compact, and discrete representation that reframes skinning from continuous regression problem to discrete token prediction task. We employ Finite Scalar Quantized Variational Autoencoder (FSQ-CVAE) [Mentzer et al. 2023; Sohn et al. 2015], conditioned on local mesh geometry, to compress the sparse weight assignments for each bone into short sequence of discrete SkinTokens. This representation enables TokenRig, unified, end-to-end autoregressive framework. TokenRig generates single, coherent sequence that interleaves skeletal parameters with their corresponding TokenRig. This holistic formulation allows the model to learn the complex, cross-modal dependencies between skeletal placement and surface skins, critical relationship ignored by prior decoupled approaches. Furthermore, the generative nature of TokenRig makes it uniquely suited for refinement using policy gradient methods [Rafailov et al. 2023; Schulman et al. 2017; Shao et al. 2024]. We introduce reinforcement learning stage with set of carefully designed reward functions that encode high-level principles of rig quality, such as bone-mesh alignment and deformation smoothness. This allows TokenRig to generalize far beyond its training data, successfully rigging complex, in-the-wild assets where purely supervised methods often fail. Our main contributions can be summarized as follows: learned discrete representation for skinning weights, SkinTokens, that transforms skinning from high-dimensional regression task into compact sequence prediction problem. unified autoregressive framework, TokenRig, that jointly models skeleton generation and skinning, capturing their mutual dependencies for higher-fidelity results. reinforcement learning framework for rig refinement, with novel reward functions designed to improve the generalization and robustness of the generated rigs on outof-distribution 3D models."
        },
        {
            "title": "2 Related Works\n2.1 Automatic Rigging Methods",
            "content": "2.1.1 Traditional Approaches. Early research [Baran and PopoviÄ‡ 2007; Tagliasacchi et al. 2009] largely relied on geometric heuristics to infer skeletal structures without data-driven priors. Seminal works like Pinocchio [Baran and PopoviÄ‡ 2007] utilize signed distance fields to approximate the medial surface, refining the embedding via multiplicative optimization. Other topological methods leverage medial representations, such as Voxel Cores [Yan et al. 2018] and Erosion Thickness [Yan et al. 2016], to extract skeletons. While these methods function reliably on watertight, geometrically coherent models, they lack semantic understanding and often necessitate significant manual post-processing. Tools like LazyBones [Nile 2025] for Blender [Blender 2018] automate parts of this process but still fundamentally rely on artists to edit and finalize the rig. Learning-Based Approaches. The advent of deep learning and 2.1.2 large-scale datasets, such as ArticulationXL 2.0 [Song et al. 2025b] and Rig-XL [Zhang et al. 2025b], has shifted the paradigm toward data-driven methods. Template-based approaches [Chu et al. 2024; Li et al. 2021; Ma and Zhang 2023] achieve high fidelity by fitting fixed templates (e.g., humanoids) but fail to generalize to arbitrary characters. Template-free methods like RigNet [Xu et al. 2020] and MoRig [Xu et al. 2022] overcome this by employing Graph Neural Networks (GNNs) to predict joint heatmaps, subsequently connecting them via Minimum Spanning Tree (MST) algorithms. However, MST-based connectivity is sensitive to noisy predictions, often yielding topologically inconsistent results. DRiVE [Sun et al. 2024] attempts to mitigate this using diffusion-based framework for robust joint placement. Most recently, inspired by the sequential editing workflows commonly observed in professional 3D software and the success of Large Language Models (LLMs) [Yang et al. 2025; Zhang et al. 2025a], researchers have reformulated skeleton generation as an autoregressive sequence modeling task. These methods [Deng et al. 2025; Guo et al. 2025a,b; Liu et al. 2025b; Song et al. 2025a,b; Zhang et al. 2025b] discretize the skeletal hierarchy into tokens, leveraging Transformers to capture global structural dependencies. Reinforcement learning has also been explored to refine topology, as seen in AutoConnect [Guo et al. 2025a]. Despite this progress, these pipelines predominantly treat skeleton prediction and skinning as decoupled stages. This separation precludes mutual information exchange and necessitates complex feature engineering. In contrast, our method unifies these modalities into single end-to-end framework, reducing inter-stage error propagation and improving generalization."
        },
        {
            "title": "2.2 Automatic Skinning Weight Prediction",
            "content": "2.2.1 Traditional Methods. Classic skinning methods derive weights from geometric properties. Graph-based techniques [Katz and Tal 2003] typically employ min-cut algorithms to segment vertices. Pinocchio [Baran and PopoviÄ‡ 2007] popularized heat diffusion, solving the steady-state heat equation to propagate influence from bone junctions. While prevalent in industrial tools due to their robustness, these methods produce weights purely based on geometry, lacking the semantic nuance required for expressive animation. 2.2.2 Learning-Based Methods. Modern methods aim to learn statistical skinning priors from data. RigNet [Xu et al. 2020] combines GNNs with geodesic distance features to regress weights directly. Similarly, NeuroSkinning [Liu et al. 2019] utilizes graph convolutions with multi-head attention, while Neural Blend Shapes [Li et al. 2021] employs MeshCNN [Hanocka et al. 2019] for local feature extraction. Alternative representations have also been proposed; SkinCells [Larionov et al. 2025] introduces Voronoi-based sparse weight fields, and MagicArticulate [Song et al. 2025b] adopts diffusion formulation to predict residuals relative to geodesic distances. SkinTokens : Learned Compact Representation for Unified Autoregressive Rigging 3 However, fundamental limitation persists across these methods: they typically frame skinning as high-dimensional regression problem. Regressing dense matrices for meshes with disconnected components or non-watertight geometry is notoriously unstable and computationally expensive. Furthermore, reliance on auxiliary descriptors like geodesic distance [Sun et al. 2024; Xu et al. 2020; Zhang et al. 2025b] limits robustness on complex topologies. Our approach diverges by introducing SkinTokens, discrete compression scheme that circumvents direct high-dimensional regression, enabling scalable and robust joint learning of skinning and structure."
        },
        {
            "title": "3 Method\n3.1 Overview",
            "content": "Our method reframes automatic rigging as unified, generative sequence modeling task (see Figure 2). This is achieved through three key stages. First, we introduce SkinTokens, novel discrete representation for skinning weights learned via FSQ-CVAE [Mentzer et al. 2023; Sohn et al. 2015]. This representation transforms the intractable problem of regressing high-dimensional, sparse matrix into tractable token prediction task (Section 3.2). Second, this representation enables TokenRig, unified autoregressive Transformer that learns to generate single, interleaved sequence of skeletal parameters and their corresponding SkinTokens, thereby jointly modeling the entire rig (Section 3.3). Finally, we employ reinforcement learning fine-tuning stage using set of tailored reward functions, which significantly enhances the models generalization capabilities to complex, out-of-distribution 3D assets (Section 3.4)."
        },
        {
            "title": "Skinning",
            "content": "The core of our framework is novel representation for skinning weights that circumvents the fundamental limitations of direct regression. We motivate and detail this representation below. 3.2.1 The Sparsity and Challenge of Skinning. Formally, given mesh = {V R3ğ‘ , } with ğ‘ vertices and skeleton with ğ½ joints, the skinning task is to predict the ğ‘ ğ½ weight matrix W. In production models, ğ‘ can exceed 105 and ğ½ can exceed 102, leading to matrices with over 107 elements. Directly regressing such large matrix is computationally demanding and statistically challenging. Crucially, the skinning matrix is intrinsically sparse. Practically, each vertex is typically influenced by no more than four joints, meaning the number of non-zero elements is at most 4ğ‘ . As shown in Table 1, the average sparsity ratio across several public datasets is extremely low (2 10%). This severe class imbalance makes training with standard dense losses like Mean Squared Error (MSE) highly inefficient, as the optimization is dominated by the trivial task of predicting zero-valued weights. Furthermore, the arbitrary ordering of vertices in mesh makes it impossible to apply traditional sparse matrix compression techniques that rely on structured sparsity. These challenges motivate learned approach to compression. FSQ-CVAE for Skinning. To address these challenges, we pro3.2.2 pose compressing the skinning weights for each individual bone ğ‘— [1, ğ½ ], denoted as = {ğ‘¤ (),ğ‘— } , into compact, discrete , Vol. 1, No. 1, Article . Publication date: February 2026. 4 Jia-Peng Zhang, Cheng-Feng Pu, Meng-Hao Guo, Yan-Pei Cao, and Shi-Min Hu Fig. 2. Overview of the TokenRig Framework. Our method consists of three key stages: (1) Learning SkinTokens (Section 3.2.2): We first train FSQ-CVAE [Kingma and Welling 2013; Mentzer et al. 2023; Sohn et al. 2015] to compress sparse skinning weights into compact, discrete representation. Mesh geometry and skinning weights are processed by VecSet [Zhang et al. 2023] encoders, and the resulting features are discretized into SkinTokens via Finite Scalar Quantization (FSQ) [Mentzer et al. 2023]. We employ nested dropout [Bachmann et al. 2025; Rippel et al. 2014] and importance sampling to ensure robust reconstruction of active deformation regions. (2) Unified Autoregressive Modeling (Section 3.3): We formulate rigging as sequence generation task. Transformer generates single, unified sequence comprising the complete skeleton followed by the learned SkinTokens (from Stage 1), conditioned on global shape embeddings to capture structural dependencies. (3) RL Refinement via GRPO (Section 3.4): To improve generalization to in-the-wild assets, we fine-tune the model using Group Relative Policy Optimization (GRPO) [Liu et al. 2024]. We introduce four specific rewards: Volumetric Joint Coverage (ensuring bone distribution), Bone-Mesh Containment (preventing protrusion), Skinning Coverage and Sparsity (ensuring valid weighting), and Deformation Smoothness (preventing artifacts during animation). avg. ğ‘ avg. ğ½ avg. (cid:205) I[ğ‘¤ > 0] avg. sparsity ModelsResource VRoid Hub Articulation 2.0 16929.78 95.56 28392.74 2.43% 6247.05 34.46 12655.45 9.38% 1297.69 19.87 1700.18 7.40% Table 1. Sparsity Analysis of Skinning Weights. Statistics across three major datasets [Isozaki et al. 2021; Models-Resource 2019; Song et al. 2025b] reveal that skinning matrices are extremely sparse (< 10% active weights), motivating our design of the compressed SkinTokens representation. representation. Our approach is based on the Conditional Variational Autoencoder (CVAE) framework [Kingma and Welling 2013; Sohn et al. 2015], which learns latent representation of data ğ‘¥ conditioned on an observed variable ğ‘¦. Here, the skinning weights are the data to be reconstructed, conditioned on the full mesh geometry M. , Vol. 1, No. 1, Article . Publication date: February 2026. As illustrated in Figure 2, our architecture consists of two distinct encoders, ğ¸ğ‘€ and ğ¸ğ‘Š , which follow the VecSet design [Zhang et al. 2023] to process the mesh and weights as unordered point sets. The mesh encoder ğ¸ğ‘€ (M) produces shape features, while the skin encoder ğ¸ğ‘Š (W) produces latent weight features ğ¿ğ‘Š . The objective is to convert this continuous latent representation into discrete format suitable for sequence modeling. We refer to this learned, quantized representation as SkinTokens. This crucial discretization step is performed by applying Finite Scalar Quantization (FSQ) [Mentzer et al. 2023] to the latent features ğ¿ğ‘Š . Unlike traditional vector quantization [Van Den Oord et al. 2017], FSQ avoids learnable codebook by quantizing each latent dimension to the nearest level on fixed grid. This simplifies training, requires no auxiliary losses, and provides excellent codebook utilization. Gradients are passed through the non-differentiable quantization step using the Straight-Through Estimator (STE) [Bengio et al. 2013]. SkinTokens : Learned Compact Representation for Unified Autoregressive Rigging 5 loss for the CVAE is weighted sum of these objectives: LVAE = ğœ†BCELBCE (W pred , W) + ğœ†MSELMSE (W pred ğœ†DiceLDice (W , W)+ , W), pred where we also retain small MSE term in our implementation for stability. This composite loss ensures both accurate reconstruction of values and precise localization of sparse, non-zero weight regions. Importance Sampling for Efficient Training. To accelerate train3.2.4 ing and focus the models capacity on critical deformation regions, we employ hybrid sampling strategy for the mesh points fed to the skin decoder. During each training step, we provide the decoder with combination of points sampled uniformly from the mesh surface, Puniform, and points sampled densely from regions with non-zero ground-truth skinning weights, Pdense. While the shape encoder ğ¸ğ‘€ only sees Puniform (to match inference conditions), this importance sampling for the decoder ensures that sparse, active deformation zones are well-represented in every training batch, leading to faster convergence and higher fidelity."
        },
        {
            "title": "3.3 TokenRig: Unified Autoregressive Modeling",
            "content": "The discrete and compact nature of SkinTokens enables us to move beyond the limitations of decoupled, multi-stage pipelines [Deng et al. 2025; Guo et al. 2025a,b; Liu et al. 2025b; Song et al. 2025a,b; Zhang et al. 2025b]. We can now represent the entire rig, i.e., both skeleton and skinning, as single, coherent sequence of discrete tokens. This allows us to formulate rigging as unified sequence generation task, which we solve with TokenRig, an autoregressive Transformer model that generates the complete skeleton first, followed by the corresponding skinning weights. 3.3.1 Unified Sequence Representation. The power of our approach stems from novel, coherent sequence representation that jointly captures skeletal structure and surface skins. Skeletal Tokenization. Following the methodology of recent work in skeleton generation [Zhang et al. 2025b], we first serialize the skeletal hierarchy. Joint coordinates are uniformly quantized and represented as sequence of discrete integer tokens ğ‘‘ğ‘– = (ğ‘‘ğ‘¥ğ‘–, ğ‘‘ğ‘¦ğ‘–, ğ‘‘ğ‘§ğ‘– ). The bone order is established using predefined templates (e.g., for bipeds) or chain partitioning strategies for more general structures: <bos> <type1> ğ‘‘ğ‘¥1 ğ‘‘ğ‘¦1 ğ‘‘ğ‘§1 ğ‘‘ğ‘¥2 ğ‘‘ğ‘¦2 ğ‘‘ğ‘§2 <type2> . . . <typeğ‘˜ > ğ‘‘ğ‘¥ğ‘¡ ğ‘‘ğ‘¦ğ‘¡ ğ‘‘ğ‘§ğ‘¡ . . . ğ‘‘ğ‘¥ğ‘‡ ğ‘‘ğ‘¦ğ‘‡ ğ‘‘ğ‘§ğ‘‡ <eos>. Here, <bos> and <eos> denote sequence boundaries, and each bone chain is prefixed with special <type> token that serves as categorical identifier (e.g., mixamo). Sequential Composition of SkinTokens. Following the complete skeletal sequence, we introduce the skinning information as subsequent sequence of SkinTokens. For each bone ğ‘– in the skeleton, its corresponding skinning influence is represented by sequence of Tğ· discrete SkinTokens from our pre-trained FSQ-CVAE (see Sec. 3.2.2). These individual SkinTokens sequences are then concatenated in canonical order to form single, continuous block representing the skinning for the entire model. The complete autoregressive , Vol. 1, No. 1, Article . Publication date: February 2026. Fig. 3. Gradient Analysis of Loss Functions. comparison of Binary Cross Entropy (BCE) and Dice loss [Sudre et al. 2017] landscapes for target weight ğ‘¤ = 0.2. While both minimize at the correct value, Dice loss provides significantly larger gradients for non-zero targets (ğ‘¤pred [0, 1]), effectively counteracting the extreme sparsity of skinning matrices where BCE gradients tend to vanish. The resulting discrete tokens ğ¿ğ· = FSQ(ğ¿ğ‘Š ) are then concatenated with the shape features from ğ¸ğ‘€ and passed to decoder. To improve robustness and encourage more compositional representation, we adopt nested dropout scheme analogous to FlexTok [Bachmann et al. 2025; Rippel et al. 2014], randomly selecting prefix of the token sequence during training. The decoder reconstructs the per-vertex skinning weights pred, with final sigmoid activation to ensure outputs are bounded in [0, 1]. 3.2.3 Loss Function for Sparse Weight Reconstruction. Standard VAEs often assume Gaussian likelihood [Kingma and Welling 2013], optimized with Mean Squared Error (MSE). However, as our outputs represent probability-like skinning weights in the range [0, 1], Bernoulli likelihood with Binary Cross-Entropy (BCE) loss is more suitable. More importantly, the extreme sparsity of presents significant class imbalance. To counteract this, we incorporate the Dice loss [Milletari et al. 2016], metric widely used in image segmentation that is equivalent to the F1-score and focuses supervision on positive (non-zero) samples: LDice = 1 pred 2W pred + = ğ‘— ğ½ 1 2 (cid:205)ğ‘– ğ‘¤predğ‘–,ğ‘—ğ‘¤ğ‘–,ğ‘— + ğœ€ (cid:205)ğ‘– ğ‘¤pred ğ‘–,ğ‘— + (cid:205)ğ‘– ğ‘¤ 2 2 ğ‘–,ğ‘— + ğœ€ , where small constant ğœ€ = 104 ensures numerical stability. As shown in Figure 3, the Dice loss provides stronger gradient signal for non-zero weights compared to BCE, effectively amplifying supervision where it matters most. More specifically, when ğ‘¤ = 0, the is small for ğ‘¤pred (ğœ€, 1], whereas for gradient magnitude ğ‘¤ > 0, it is considerably larger across ğ‘¤pred [0, 1]. This selectively amplifies gradients for positive entries, aligning well with the imbalanced sparsity in W. The minimum stationary condition ğ‘¤min = ğ‘¤ holds for LDice, LBCE, and LMSE, with LDice (ğ‘¤min) = 0, ensuring smooth blending across joint seams. Overall, the final reconstruction ğ‘¤pred 6 Jia-Peng Zhang, Cheng-Feng Pu, Meng-Hao Guo, Yan-Pei Cao, and Shi-Min Hu sequence is thus structured composition of these two modalities: bones and check their containment within the voxelized mesh: <bos> <type1> ğ‘‘ğ‘¥1 ğ‘‘ğ‘¦1 ğ‘‘ğ‘§1 <typeğ‘˜ > ğ‘‘ğ‘¥ğ‘‡ ğ‘‘ğ‘¦ğ‘‡ ğ‘‘ğ‘§ğ‘‡ D1,0 . . . D1,Tğ· . . . Dğ‘‡ ,0 . . . Dğ‘‡ ,Tğ· <eos>, where Dğ‘–,ğ‘— denotes the ğ‘—-th token of the ğ‘–-th joints skin latent. This sequential, two-part structure allows the generation of SkinTokens to be globally conditioned on the fully generated skeleton. The Transformers self-attention mechanism can access all joint positions and bone types when predicting the skinning for any given bone, enabling it to model complex, long-range dependencies. This holistic conditioning is significant advantage over methods that predict skinning based only on local features."
        },
        {
            "title": "3.4 Generalization via Reinforcement Learning Refinement",
            "content": "While the supervised TokenRig model captures the statistical distribution of rigs in the training data, it faces inherent limitations when applied to out-of-distribution (OOD) assets. Due to the nature of next-token prediction, the model may default to average solutions or fail to capture global geometric constraints on complex topologies (e.g., missing auxiliary limbs like wings or tails, or placing bones outside the mesh). To address this, we introduce post-training refinement stage using Reinforcement Learning (RL). Unlike prior work that relies on costly annotated preference data (DPO) [Guo et al. 2025a; Rafailov et al. 2023], we leverage Group Relative Policy Optimization (GRPO) [Shao et al. 2024]. This allows us to optimize the model directly against set of explicit, non-differentiable geometric and semantic rewards that encode professional rigging criteria. 3.4.1 Task-Specific Reward Design. We design suite of four rewards to guide the model toward topologically valid and functionally robust rigs. These rewards punish common failure modes such as bone protrusion, unconnected vertices, and varying skinning density. (1) Volumetric Joint Coverage (ğ‘…ğ‘£ ğ‘— ). To ensure the generated skeleton adequately spans the geometry of the character, we verify that joints are distributed throughout the meshs interior volume. We voxelize the mesh into grid of resolution ğ‘Ÿ 3 (where ğ‘Ÿ = 196). For each voxel center ğ‘£ğ‘– , we compute its Euclidean distance to every joint ğ½ğ‘— , and sum up the results using an exponential kernel. This gives the voxel-joint distance reward: ğ‘…ğ‘£ ğ‘— = 1 ğ‘‰ ğ‘‰ ğ‘–=1 exp (ğ›¼ ğ½ min ğ‘—=1 ğ‘£ğ‘– ğ½ğ‘— 2) where ğ‘‰ is the number of occupied voxels and ğ›¼ = 0.05 is scaling factor that controls the falloff. Intuitively, this reward encourages the placement of joints in all significant mesh parts, preventing missing bones in extremities. (2) Bone-Mesh Containment (ğ‘…ğ‘£ğ‘˜ ). fundamental rule of rigging is that bones should reside within the characters body. We penalize structural hallucinations where bones protrude outside the mesh surface. We sample ğ‘  points uniformly along each of the ğ½ generated , Vol. 1, No. 1, Article . Publication date: February 2026. ğ‘…ğ‘£ğ‘˜ = 1 ğ½ (ğ‘  + 1) ğ½ ğ‘ +1 ğ‘—=1 ğ‘–=1 I[ğ½ğ‘—,ğ‘– V] where ğ½ğ‘—,ğ‘– the ğ‘–-th uniformly sampled point along the ğ‘—-th bone, and I[] is the indicator function that returns 1 if point ğ½ğ‘—,ğ‘– lies inside an occupied voxel and 0 otherwise. This reward directly penalizes bones that protrude outside the mesh, ensuring that the skeleton remains geometrically consistent with the mesh. (3) Skinning Coverage and Sparsity (ğ‘…ğ‘ ğ‘ ). Naively optimizing for joint placement can lead to degenerate solutions (e.g., filling the volume with excessive joints). We therefore introduce skinning reward to enforce two constraints: every vertex must be influenced by at least one bone (avoiding unbound geometry), and no vertex should be influenced by too many bones (enforcing sparsity). ğ‘…ğ‘ ğ‘ = 1 ğ‘…ğ‘§ ğ‘…ğ‘š, 1 2 1 2 ğ½ (cid:214) (cid:32) (cid:32) ğ‘…ğ‘§ = ğ‘…ğ‘š = 1 ğ‘– 1 ğ‘– (cid:33)ğ›¼ğ‘§ I[Wğ‘–,ğ‘— < ğ›½] , ğ‘—=1 (cid:34)(cid:32) ğ½ ğ‘—=1 (cid:2)Wğ‘–,ğ‘— > ğ›½(cid:3) (cid:33) (cid:35) (cid:33)ğ›¼ğ‘š > 4 . Here, ğ‘…ğ‘§ measures the fraction of vertices with zero skinning weights (where Wğ‘–,ğ‘— < ğ›½), and ğ‘…ğ‘š measures the fraction of vertices influenced by more than 4 bones (where (cid:205)ğ½ I[Wğ‘–,ğ‘— > ğ›½]) > 4), with threshold ğ›½ = 0.1. ğ›¼ğ‘§, ğ›¼ğ‘š are hyperparameters controlling the penalty strength. This term effectively regularizes the interaction between skeleton and skinning. ğ‘—=1 (4) Deformation Smoothness (ğ‘…ğ‘šğ‘œ ). Since the ultimate goal of rigging is animation, we assess the quality of deformations under animation. We define motion reward that penalizes spiky or distorted artifacts. We apply the Linear Blend Skinning (LBS) algorithm to the mesh using randomly sampled poses and measure the distortion of mesh edges: (cid:18) ğ‘…ğ‘šğ‘œ = 1 + ğ‘  Eğ‘P (cid:20) max ğ‘’ (cid:18) 1, ğ‘™ (LBS(ğ‘’)) ğ‘™ (ğ‘’) + ğœ€ (cid:19) (cid:21) (cid:19) 1 where ğ‘™ (ğ‘’) is the L2 length of edge ğ‘’ in the rest pose, LBS is the linear blend skinning function that deforms the edge under pose ğ‘, represents the possible pose space of the skeleton, ğ‘  the hyperparameter to scale values and ğœ€ = 106 ensures numerical stability. This encourages the model to predict skinning weights that preserve local surface geometry during articulation. In practice, we approximate the expectation with 5 randomly sampled poses. 3.4.2 Policy Optimization with GRPO. The final reward ğ‘… is weighted sum of the above components: ğ‘… = ğ‘¤ğ‘£ ğ‘— ğ‘…ğ‘£ ğ‘— + ğ‘¤ğ‘£ğ‘˜ ğ‘…ğ‘£ğ‘˜ + ğ‘¤ğ‘ ğ‘ ğ‘…ğ‘ ğ‘ + ğ‘¤ğ‘šğ‘œ ğ‘…ğ‘šğ‘œ . Crucially, if the generated token sequence is invalid (cannot be decoded into usable rig), we assign ğ‘… = 0. We refine the policy ğœ‹ğœƒ using GRPO [Shao et al. 2024]. For each input mesh, we sample group of ğº outputs (i.e., token sequences) ğ‘œğ‘–, ğ‘– {0, 1, , ğµ 1} from the current policy ğœ‹ğ‘œğ‘™ğ‘‘ . We verify the structural consistency of each output, decode it to compute SkinTokens : Learned Compact Representation for Unified Autoregressive Rigging 7 Fig. 5. Learned Semantics of SkinTokens. t-SNE visualization of the continuous latent vectors ğ¿ğ‘Š prior to quantization, sampled from 300 instances in the VRoid dataset [Isozaki et al. 2021]. Points are colored by bone category (e.g., Head, Hips). The clear emergence of anatomical clusters indicates that the encoder captures semantic structural prior, learning to represent body part concepts invariant to specific mesh geometries. professional rigs with varied community-created assets. All geometry is normalized to the canonical unit cube [1, 1]3. 4.1.2 Robustness-Oriented Data Augmentation. We implement careful augmentation pipeline designed to simulate the topological imperfections and irregular structures found in in-the-wild assets. (1) Structural Variation: To prevent overfitting to specific skeletal hierarchies, we apply extensive structural perturbations. With probability ğ‘ = 0.5, we randomly delete up to 50% of joints (and their associated mesh vertices) or remove entire subtrees. During VAE training, we also reconnect up to 30% of joints to new parents (ğ‘ = 0.5), merging their skinning weights to simulate topology edits. (2) Geometric Perturbation: We apply non-uniform scaling (ğ‘ = 0.5), global rotations around principal axes (ğ‘ = 0.2, max 15), and random pose perturbations (ğ‘ = 0.5, max 30 rotation). Additionally, we inject Gaussian noise into joint coordinates (ğœ = 102) and vertex positions (ğœ = 103) to enforce resilience against noisy inputs. 4.1.3 Model Architecture and Training. SkinTokens (FSQ-CVAE). We utilize the 3DShape2Vecset backbone [Zhang et al. 2023] (110M parameters) with an asymmetric design (2 encoder layers, 10 decoder layers) to accelerate subsequent training, and PMPE encoding [Bhat et al. 2025]. The FSQ codebook is configured with level sizes [8, 8, 8, 5, 5, 5], yielding total vocabulary of 64, 000. During training, we used at most ğ‘‡ğ· = 32 skin tokens and ğ‘‡ğ‘Š = 384 shape tokens as auxiliary conditions. TokenRig (Autoregressive Model). We adopt the Qwen3-0.6B architecture [Yang et al. 2025], utilizing Grouped Query Attention (GQA) [Ainslie et al. 2023] and Rotary Position Embeddings (RoPE) [Su et al. 2024] for efficient sequence modeling. , Vol. 1, No. 1, Article . Publication date: February 2026. Fig. 4. SkinTokens Reconstruction Fidelity. We evaluate the reconstruction quality (IoU and L1 Error) of the FSQ-CVAE across varying codebook sizes ğ¶ and token sequence lengths ğ‘‡ğ· . The results demonstrate that SkinTokens achieve high fidelity with as few as 4 tokens, validating the compressibility of skinning data. The configuration ğ¶ = [8, 8, 8, 6, 5] = 15, 360 (lines with circles) is selected for our final model for its superior balance of compression and accuracy. The figure reports the IoU scores at ğœ€ = 102 and corresponding ğ¿1 reconstruction errors on the Articulation 2.0 [Song et al. 2025b] test dataset Codebook Size Total Entries Utilization Compression Ratio [8, 8, 8, 5, 6] [8, 8, 8, 8, 8] [8, 8, 8, 5, 5, 5] 208.23 195.22 183.74 99.6% 92.7% 86.2% 15360 32768 64000 Table 2. Codebook Utilization and Compression Analysis. We compare different FSQ codebook configurations (ğ¶). The compression ratio is computed against raw FP16 baseline using the Articulation 2.0 [Song et al. 2025b] dataset. For example, storing skinning weights for an average mesh (6, 247 vertices) typically requires 12, 494 bytes; our representation (ğ‘‡ğ· = 32 tokens) reduces this to just 64 (32 2) bytes, achieving substantial reduction in storage requirements. the rewards, and calculate the advantage ğ‘…ğ‘– for each sample by normalizing the rewards within the group. The training objective is: = 1 ğº ğº 1 ğ‘œğ‘– ğ‘œğ‘– min ğ‘–= ğ‘¡ =1 ğ›½DKL [ğœ‹ğœƒ ğœ‹ğ‘Ÿğ‘’ ğ‘“ ] (cid:20) ğœ‹ğœƒ (ğ‘œğ‘–,ğ‘¡ ) ğœ‹ğ‘œğ‘™ğ‘‘ (ğ‘œğ‘–,ğ‘¡ ) , clip (cid:18) ğœ‹ğœƒ (ğ‘œğ‘–,ğ‘¡ ) ğœ‹ğ‘œğ‘™ğ‘‘ (ğ‘œğ‘–,ğ‘¡ ) , 1 ğœ–, 1 + ğœ– (cid:19) (cid:21) ğ‘…ğ‘– where DKL is the KL divergence [Schulman 2020]. This approach stabilizes training by using group-relative baselines rather than separate critic network, allowing TokenRig to effectively selfcorrect its generation logic based on geometric validity."
        },
        {
            "title": "Implementation and Experimental Setup",
            "content": "4.1.1 Dataset Configuration. To ensure our model generalizes across diverse topologies and articulation styles, we construct composite dataset sourcing from Articulation2.0 [Song et al. 2025b] (70%), VRoid Hub [Isozaki et al. 2021] (20%), and ModelsResource [ModelsResource 2019; Xu et al. 2020] (10%). This mix balances high-quality, 8 Jia-Peng Zhang, Cheng-Feng Pu, Meng-Hao Guo, Yan-Pei Cao, and Shi-Min Hu Fig. 6. Qualitative Comparison of Skeleton Generation. We compare TokenRig (Ours) against state-of-the-art baselines. While baseline methods exhibit partial structures, missing details, or redundant joints, our method synthesizes structurally coherent and semantically faithful skeletons across diverse character types. Training. Both stages employ hybrid optimizer strategy for efficiency: Muon [Liu et al. 2025a] is used for all attention-related layers, while AdamW [Loshchilov 2017] handles remaining parameters. The FSQ-CVAE is trained for 400k iterations (batch size 320), and TokenRig for 300k iterations (batch size 160). 4.1.4 RL Refinement (GRPO). For the post-training stage, we curate smaller, high-complexity dataset of AI-generated meshes. We train for 800 steps (learning rate 106 ) with group size of ğº = 24, clip ratio ğœ– = 0.2, and KL penalty ğ›½ = 0.1. The reward weights are set to ğ‘¤ğ‘£ ğ‘— = 5 and ğ‘¤ğ‘£ğ‘˜ = ğ‘¤ğ‘ ğ‘ = ğ‘¤ğ‘šğ‘œ = 1 (see Section 3.4), emphasizing volumetric joint coverage while maintaining geometric validity. , Vol. 1, No. 1, Article . Publication date: February 2026."
        },
        {
            "title": "4.2 Analysis of the SkinTokens Representation",
            "content": "4.2.1 Metrics and Reconstruction Fidelity. To substantiate our hypothesis that skinning weights exhibit intrinsic compressibility, we evaluate the reconstruction quality of the FSQ-CVAE across various codebook configurations. While Mean Absolute Error (MAE) is commonly used [Liu et al. 2025b; Song et al. 2025b; Xu et al. 2020], we argue it is insufficient for assessing skinning quality; due to the extreme sparsity of the matrix, model can achieve low MAE by simply predicting near-zero values everywhere, failing to capture the critical active deformation zones. Therefore, we additionally employ Intersection over Union (IoU), metric standard in semantic segmentation, to measure the structural overlap of the predicted SkinTokens : Learned Compact Representation for Unified Autoregressive Rigging 9 Fig. 7. Qualitative Comparison of Skinning Prediction. We visualize predicted skinning weights and the corresponding average L1 error maps. Baseline methods often suffer from bleeding artifacts, where weights spill onto unconnected mesh parts (see UniRig/Puppeteer columns). TokenRig (Ours) produces clean, locally coherent influence maps that closely match the Ground Truth, particularly in fine-grained regions like fingers. influence regions. We categorize weights as active if they exceed threshold ğœ€ = 102. For ground truth weight ğ‘¤ğ‘–,ğ‘— and prediction Ë†ğ‘¤ğ‘–,ğ‘— , the IoU is defined as: IoU = (cid:205)ğ‘–,ğ‘— [ ğ‘¤ğ‘–,ğ‘— > ğœ€ Ë†ğ‘¤ğ‘–,ğ‘— > ğœ€ ] (cid:205)ğ‘–,ğ‘— [ ğ‘¤ğ‘–,ğ‘— > ğœ€ Ë†ğ‘¤ğ‘–,ğ‘— > ğœ€ ] . As illustrated in Figure 4, our model maintains high reconstruction fidelity even with highly compact representation. The IoU scores remain robust as the number of tokens (ğ‘‡ğ· ) decreases, demonstrating that the relevant information is effectively concentrated in the top few tokens. 4.2.2 Compression Efficiency. Table 2 quantifies the compression performance. We compare different FSQ level configurations, specifically targeting codebook sizes of 15, 360 and 64, 000. Our selected configuration (ğ¶ = [8, 8, 8, 5, 5, 5], size 64, 000) achieves 183.74 compression ratio compared to the raw FP16 representation, while maintaining codebook utilization of 86.2%. This confirms that the high-dimensional skinning matrix can be effectively reduced to sequence of discrete SkinTokens (totaling just 64 bytes per bone for ğ‘‡ğ· = 32) without significant information loss. 4.2.3 Latent Space Semantics. To understand what the encoder learns, we visualize the continuous latent space (ğ¿ğ‘Š ) prior to quantization. Figure 5 presents t-SNE [Maaten and Hinton 2008] projection of skinning latents from 300 instances in the VRoid dataset, colored by their corresponding bone identifiers (mapped to the Mixamo template). We observe distinct, well-separated clusters corresponding to specific anatomical parts (e.g., Head, Hips, LeftLeg), despite the significant geometric variation across the source meshes. This indicates that SkinTokens capture semantic structural prior, e.g., learning an abstract representation of what legs skinning looks like, rather than merely memorizing vertex indices. This learned invariance is key to the models ability to cross-compress and generalize to diverse characters."
        },
        {
            "title": "4.3 Comparison",
            "content": "We evaluate our framework against four leading learning-based rigging approaches: RigNet [Xu et al. 2020], MagicArticulate [Song et al. 2025b], UniRig [Zhang et al. 2025b], and Puppeteer [Song et al. 2025a]. Evaluations are conducted on the ModelsResource [ModelsResource 2019; Xu et al. 2020] and Articulation 2.0 [Song et al. 2025b] test sets. All skeletons are normalized to the [1, 1]3 cube. , Vol. 1, No. 1, Article . Publication date: February 2026. 10 Jia-Peng Zhang, Cheng-Feng Pu, Meng-Hao Guo, Yan-Pei Cao, and Shi-Min Hu RigNet [Xu et al. 2020] MagicArticulate [Song et al. 2025b] Puppeteer [Song et al. 2025a] UniRig [Zhang et al. 2025b] TokenRig (4 skin tokens) TokenRig (6 skin tokens) TokenRig (4 skin tokens, w/ GRPO) TokenRig (6 skin tokens, w/ GRPO) ModelsResource J2B 2.412 2.260 2.881 2.592 2.025 2.149 2.012 2.063 Table 3. Quantitative Comparison of Skeletal Generation. We evaluate skeletal structure accuracy using Chamfer Distance metrics: Joint-to-Joint (J2J), Joint-to-Bone (J2B), and Bone-to-Bone (B2B) on the ModelsResource [Models-Resource 2019] and Articulation 2.0 [Song et al. 2025b] datasets. Lower values () indicate better performance. TokenRig consistently outperforms state-of-the-art baselines across all metrics, demonstrating superior fidelity in joint placement and bone connectivity. Articulation 2.0 J2B 5.841 3.026 2.300 2.211 1.694 1.864 1.599 1.638 B2B 2.213 1.915 2.475 1.890 1.568 1.656 1.547 1.566 J2J 7.376 4.003 3.033 3.115 2.515 2.541 2.485 2.521 J2J 3.901 3.024 3.841 3.390 2.857 2.838 2.893 2.894 B2B 4.802 2.586 1.923 1.926 1.469 1.582 1.463 1. ModelsResource [Xu et al. 2020] Articulation 2.0 [Song et al. 2025b] Skin L1 L1 Var. Precision Recall Motion Skin L1 L1 Var. Precision Recall Motion 0.0464 0.0915 0.0573 RigNet [Xu et al. 2020] 0.0173 0.0314 0.0321 Puppeteer [Song et al. 2025a] 0.0212 0.0419 0.0381 UniRig [Zhang et al. 2025b] 0.0072 0.0236 0.0168 TokenRig (4 skin tokens) 0.0069 0.0228 0.0166 TokenRig (6 skin tokens) 0.0214 TokenRig (4 skin tokens, w/ GRPO) 0.0169 0.0071 0.0209 TokenRig (6 skin tokens, w/ GRPO) 0.0163 0.0068 0.0395 0.0431 0.0144 0.0278 0.0165 0.0297 0.0077 0.0178 0.0061 0.0153 0.0174 0.0069 0.0150 0.0058 0.0789 0.0279 0.0312 0.0184 0.0176 0.0166 0.0158 59.9 87.2 86.7 88.1 89.1 88.3 89. 54.6 75.1 73.5 87.7 88.8 87.7 89.2 67.8 76.7 72.6 78.1 78.8 78.1 79.0 62.4 64.4 65.8 78.9 79.1 78.9 79.2 Table 4. Quantitative Evaluation of Skinning Prediction. We compare skinning fidelity using L1 Error, Precision/Recall, and Motion Loss. threshold of ğœ€ = 102 is applied to filter negligible weights. TokenRig achieves superior performance across all categories, demonstrating significantly lower reconstruction error and higher precision/recall than baseline regression approaches. Skeletal Generation Quality. We assess skeletal structure us4.3.1 ing Chamfer Distance metrics: Joint-to-Joint (J2J), Joint-to-Bone (J2B), and Bone-to-Bone (B2B), as introduced in RigNet [Xu et al. 2020]. As summarized in Table 3, TokenRig consistently outperforms all baselines across both datasets. Notably, we achieve both the lowest J2J and B2B errors, indicating that our generated skeletons are not only closer to ground truth joints but also maintain superior topological alignment. These quantitative gains are reflected in the qualitative comparisons shown in Figure 6. Baseline methods exhibit distinct structural weaknesses: RigNet [Xu et al. 2020] frequently generates incomplete skeletons, often missing terminal chains due to the limitations of its MST-based connectivity inference; conversely, UniRig [Zhang et al. 2025b] tends to over-segment the mesh, producing an excessive number of joints with irregular topology that is difficult to animate. While newer autoregressive models like Puppeteer [Song et al. 2025a] and MagicArticulate [Song et al. 2025b] improve upon this, they struggle to preserve fine-grained semantic details, often failing to capture anatomical features such as ears or horns on non-humanoid characters. In contrast, TokenRig yields structurally coherent and semantically faithful skeletons, effectively balancing geometric coverage with topological simplicity. Skinning Prediction Fidelity. The most significant performance 4.3.2 gains are observed in skinning prediction, validating the efficacy of the SkinTokens representation. We employ five complementary , Vol. 1, No. 1, Article . Publication date: February 2026. metrics: Precision and Recall (quantifying the accuracy of joint influence regions), Motion Loss (measuring deformation fidelity under LBS), ğ¿1 Error, and ğ¿1 Variance (reflecting the consistency of errors). Measurements are computed with weight threshold ğœ€ = 102. As reported in Table 4, our method achieves state-of-the-art performance across all skinning metrics. We observe substantial reduction in ğ¿1 Error compared to RigNet [Xu et al. 2020] (0.0163 vs. 0.0573 on ModelsResource), corresponding to the 98%133% improvement highlighted in our findings, confirming that our discrete token prediction avoids the noise and mean-seeking behavior typical of continuous regression tasks. Also, the low ğ¿1 Variance indicates that TokenRig produces consistently high-quality weights across different vertices, avoiding the localized failures common in baseline methods. Furthermore, the superior Motion Loss scores demonstrate that our predicted weights result in lower distortion when applied to actual deformations. Qualitatively, these improvements translate to cleaner, more distinct segmentation. Figure 7 illustrates that while baselines like UniRig [Zhang et al. 2025b] and MagicArticulate [Song et al. 2025b] often produce bleeding artifacts, i.e., where skinning weights spill onto disconnected mesh components. Our FSQ-CVAE decoder enforces strict locality, resulting in artifact-free weight maps. This precision is particularly evident in complex articulations; for example, in the third row of Figure 7, TokenRig is the only method that accurately preserves the fine-grained spatial differentiation of finger SkinTokens : Learned Compact Representation for Unified Autoregressive Rigging 11 statically accurate but produce lower distortion when applied to actual deformations."
        },
        {
            "title": "4.4 Ablation Study",
            "content": "We conduct series of ablation studies to validate the critical components of our framework, specifically analyzing the impact of the loss function design, the reinforcement learning stage, and our data augmentation strategy. 4.4.1 Necessity of Dice Loss. As discussed in Section 3.2.3, the skinning weight matrix is characterized by extreme sparsity, leading to severe class imbalance between active and inactive weights. We hypothesized that the Dice Loss [Sudre et al. 2017] is essential to mitigate optimization difficulties arising from this imbalance. To evaluate this, we trained variant of our model using only Binary Cross Entropy (BCE) and Mean Squared Error (MSE), setting ğœ†Dice = 0. The quantitative results, presented in Table 5, confirm our hypothesis. Removing the Dice Loss leads to significant degradation in reconstruction accuracy, with IoU scores dropping from 87.1% to 82.2% on the VRoid dataset. Empirically, we observed that without Dice supervision, the model tends to under-predict active regions, struggling to distinguish subtle weight gradients from zerobackground noise. The inclusion of Dice Loss is thus critical for achieving stable convergence and high-fidelity sparse reconstruction. 4.4.2 Effectiveness of GRPO Training. key motivation for our unified framework is the ability to leverage reinforcement learning to generalize beyond the supervised training distribution. We assessed the impact of our GRPO-based post-training on both standard metrics and out-of-distribution (OOD) cases. As indicated in Table 3 and Table 4, the model fine-tuned with GRPO maintains or improves performance on standard benchmarks. The true value of GRPO, however, lies in its extrapolation capability. Figure 8 and Figure 9 visualize the qualitative gains on complex, in-the-wild meshes that differ significantly from the training data. In the base model, we occasionally observed failure cases where auxiliary structures such as demonic wings, capes, or tails were ignored or received ambiguous skinning. The GRPO-trained model, guided by the volumetric coverage and bone-mesh containment rewards, successfully synthesizes coherent bone hierarchies for these challenging structures. For instance, it accurately places bones within the thin geometry of wings and differentiates the skinning of closely interacting regions, such as horse tails and hind legs. These results suggest that our reward-based refinement effectively injects geometric reasoning into the generation process, enhancing robustness where supervised signals are scarce. 4.4.3 Effectiveness of Data Augmentation. Finally, we verify the importance of our robustness-oriented data augmentation pipeline. We performed an ablation by selectively disabling specific augmentation modules, such as non-uniform scaling, sub-tree dropping, and random joint deletion, and retraining the autoregressive model. As reported in Table 6, removing any single module results in consistent degradation of skeletal prediction accuracy across all datasets. For example, omitting non-uniform scaling increases the J2J error on ModelsResource from 2.857 to 3.030. The most significant drop , Vol. 1, No. 1, Article . Publication date: February 2026. Fig. 8. Impact of GRPO on Skeletal Topology. While supervised training alone often misses non-standard anatomy, the GRPO-refined model effectively synthesizes auxiliary bones for secondary structures, such as tails, horns, wings, and clothing accessories. Fig. 9. Impact of GRPO on Skinning Precision. In addition to topology, the reinforcement learning stage tightens skinning predictions. The model produces precise, locally distinct influence maps that minimize artifacts, yielding valid rigs suitable for high-quality animation. joints, whereas baseline methods tend to predict over-smoothed or bleeding weights that degrade animation fidelity. Furthermore, our superior Motion Loss scores indicate that these weights are not just 12 Jia-Peng Zhang, Cheng-Feng Pu, Meng-Hao Guo, Yan-Pei Cao, and Shi-Min Hu Fig. 10. Diverse Generation Results. We demonstrate the generalization capacity of TokenRig on wide range of inputs, including unseen test-set samples and complex in-the-wild assets. The model robustly synthesizes fully articulated skeletons and accurate skinning weights. occurs when removing joint deletion strategies, confirming that simulating topological imperfections during training is vital for handling the diverse and often noisy geometry found in production environments. , Vol. 1, No. 1, Article . Publication date: February 2026. SkinTokens : Learned Compact Representation for Unified Autoregressive Rigging 13 VRoid Hub ModelResource Articulation 2.0 TokenRig - w/o Dice loss L1 Error 5.41 103 5.98 10 L1 Error 1.38 102 1.35 102 Table 5. Ablation Study on Loss Function Design. We evaluate the impact of the Dice Loss term on reconstruction quality, using baseline configuration with codebook size ğ¶ = [8, 8, 8, 5, 6] = 15, 360 and ğ‘‡ğ· = 32 skin tokens. The results show that removing Dice supervision significantly degrades IoU performance across all datasets. Mask Accuracy quantifies the proportion of samples where the predicted non-zero support fully covers the ground truth active region (tolerance ğœ€ = 102), highlighting the losss role in preventing under-segmentation. IoU Mask 82.2 % 84.0 % 80.8 % 80.9 % IoU Mask 83.9 % 91.1 % 82.7 % 88.2 % IoU Mask 92.5 % 87.1 % 91.6 % 82.2 % L1 Error 1.40 102 1.36 102 TokenRig - w/o non-uniform scaling - w/o sub-tree dropping - w/o joints deleting J2J 2.857 3.030 3.019 3.077 ModelsResource J2B 2.025 2.171 2.179 2.220 B2B 1.568 1.689 1.704 1.742 J2J 2.515 2.679 2.648 2.818 Articulation 2.0 J2B 1.694 1.844 1.806 1.977 B2B 1.469 1.618 1.582 1. Table 6. Ablation Study on Data Augmentation Strategies. We analyze the contribution of each robustness-oriented augmentation module to skeletal prediction accuracy (J2J, J2B, B2B). Experiments are conducted using consistent lightweight configuration (ğ‘‡ğ· = 4). The results indicate that removing any augmentation component (particularly random joint deletion) leads to higher error rates, confirming that simulating imperfections is essential for achieving robust generalization."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we introduced TokenRig, an automated framework for skeletal rigging and skinning weight prediction that approaches the fidelity of professional artist workflows. Our core insight is that the longstanding bottleneck in automatic skinning is representation problem. By proposing SkinTokens, i.e., compact, discrete representation learned via an FSQ-CVAE, we successfully transformed the ill-posed regression of sparse skinning matrices into robust token prediction task. This design enabled us to train unified autoregressive model that jointly synthesizes skeletal structures and skinning weights, capturing the intrinsic dependencies between articulation and deformation that prior decoupled methods ignore. Our experiments demonstrate that this unified approach, combined with novel GRPO-based reinforcement learning stage, significantly outperforms state-of-the-art baselines. The integration of geometric and semantic reward functions proves particularly effective for generalization, allowing the model to generate coherent rigs for complex, out-of-distribution assets that defy standard supervised learning. Limitations and Discussions. Despite these advancements, several avenues for improvement remain. First, while our FSQ-CVAE offers high compression efficiency, comparative analysis suggests residual performance gap remains compared to continuous-latent VAEs in extremely challenging skinning scenarios. Recent developments in continuous token representations [Li et al. 2024; Sikder et al. 2025] may offer path to bridge this gap, potentially enhancing predictive precision without sacrificing the benefits of sequence modeling. Second, our current framework generates rigs autonomously based on learned priors. However, professional production often requires adherence to specific topological standards. Extending our autoregressive model to accept user-specified topological templates or interactive guidance would be valuable direction, effectively transforming TokenRig from an automatic generator into flexible, artist-directed co-pilot. Finally, while our RL stage improves geometric validity, future work could explore physics-based rewards to further ensure the dynamic plausibility of the generated deformations during animation."
        },
        {
            "title": "References",
            "content": "Nadine Abu Rumman and Marco Fratarcangeli. 2015. Position-based skinning for soft articulated characters. In Computer Graphics Forum, Vol. 34. Wiley Online Library, 240250. Joshua Ainslie, James Lee-Thorp, Michiel De Jong, Yury Zemlyanskiy, Federico LebrÃ³n, and Sumit Sanghai. 2023. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245 (2023). Roman Bachmann, Jesse Allardice, David Mizrahi, Enrico Fini, OÄŸuzhan Fatih Kar, Elmira Amirloo, Alaaeldin El-Nouby, Amir Zamir, and Afshin Dehghan. 2025. FlexTok: Resampling Images into 1D Token Sequences of Flexible Length. In Forty-second International Conference on Machine Learning. Ilya Baran and Jovan PopoviÄ‡. 2007. Automatic rigging and animation of 3d characters. ACM Transactions on graphics (TOG) 26, 3 (2007), 72es. Yoshua Bengio, Nicholas LÃ©onard, and Aaron Courville. 2013. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432 (2013). Kiran Bhat, Nishchaie Khanna, Karun Channa, Tinghui Zhou, Yiheng Zhu, Xiaoxia Sun, Charles Shang, Anirudh Sudarshan, Maurice Chu, Daiqing Li, et al. 2025. Cube: roblox view of 3D intelligence. arXiv preprint arXiv:2503.15475 (2025). Sue Blackman. 2014. Rigging with mixamo. Unity for Absolute Beginners (2014), 565 573. Blender. 2018. Blender - 3D modelling and rendering package. Blender Foundation, Stichting Blender Foundation, Amsterdam. http://www.blender.org Zedong Chu, Feng Xiong, Meiduo Liu, Jinzhi Zhang, Mingqi Shao, Zhaoxu Sun, Di Wang, and Mu Xu. 2024. HumanRig: Learning Automatic Rigging for Humanoid Character in Large Scale Dataset. arXiv preprint arXiv:2412.02317 (2024). Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. 2024. Objaverse-xl: universe of 10m+ 3d objects. Advances in Neural Information Processing Systems 36 (2024). Yufan Deng, Yuhao Zhang, Chen Geng, Shangzhe Wu, and Jiajun Wu. 2025. Anymate: dataset and baselines for learning 3d object rigging. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers. 110. Olivier Dionne and Martin de Lasa. 2013. Geodesic voxel binding for production character meshes. In Proceedings of the 12th ACM SIGGRAPH/Eurographics Symposium on Computer Animation. 173180. , Vol. 1, No. 1, Article . Publication date: February 2026. 14 Jia-Peng Zhang, Cheng-Feng Pu, Meng-Hao Guo, Yan-Pei Cao, and Shi-Min Hu Jingfeng Guo, Jian Liu, Jinnan Chen, Shiwei Mao, Changrong Hu, Puhua Jiang, Junlin Yu, Jing Xu, Qi Liu, Lixin Xu, et al. 2025a. Auto-Connect: Connectivity-Preserving RigFormer with Direct Preference Optimization. arXiv preprint arXiv:2506.11430 (2025). Zhiyang Guo, Jinxu Xiang, Kai Ma, Wengang Zhou, Houqiang Li, and Ran Zhang. 2025b. Make-it-animatable: An efficient framework for authoring animation-ready 3d characters. In Proceedings of the Computer Vision and Pattern Recognition Conference. 1078310792. Rana Hanocka, Amir Hertz, Noa Fish, Raja Giryes, Shachar Fleishman, and Daniel Cohen-Or. 2019. Meshcnn: network with an edge. ACM Transactions on Graphics (ToG) 38, 4 (2019), 112. Nozomi Isozaki, Shigeyoshi Ishima, Yusuke Yamada, Yutaka Obuchi, Rika Sato, and Norio Shimizu. 2021. VRoid studio: tool for making anime-like 3D characters using your imagination. In SIGGRAPH Asia 2021 Real-Time Live! 11. Chenhan Jiang. 2024. survey on text-to-3d contents generation in the wild. arXiv preprint arXiv:2405.09431 (2024). Sagi Katz and Ayellet Tal. 2003. Hierarchical mesh decomposition using fuzzy clustering and cuts. ACM transactions on graphics (TOG) 22, 3 (2003), 954961. Meekyoung Kim, Gerard Pons-Moll, Sergi Pujades, Seungbae Bang, Jinwook Kim, Michael Black, and Sung-Hee Lee. 2017. Data-driven physics for human soft tissue animation. ACM Transactions on Graphics (TOG) 36, 4 (2017), 112. Diederik Kingma and Max Welling. 2013. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 (2013). Egor Larionov, Igor Santesteban, Hsiao-yu Chen, Gene Lin, Philipp Herholz, Ryan Goldade, Ladislav Kavan, Doug Roble, and Tuur Stuyck. 2025. SkinCells: Sparse Skinning using Voronoi Cells. arXiv preprint arXiv:2506.14714 (2025). Peizhuo Li, Kfir Aberman, Rana Hanocka, Libin Liu, Olga Sorkine-Hornung, and Baoquan Chen. 2021. Learning skeletal articulations with neural blend shapes. ACM Transactions on Graphics (TOG) 40, 4 (2021), 115. Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. 2024. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems 37 (2024), 5642456445. Yangguang Li, Zi-Xin Zou, Zexiang Liu, Dehu Wang, Yuan Liang, Zhipeng Yu, Xingchao Liu, Yuan-Chen Guo, Ding Liang, Wanli Ouyang, et al. 2025. Triposg: Highfidelity 3d shape synthesis using large-scale rectified flow models. arXiv preprint arXiv:2502.06608 (2025). Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. 2024. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437 (2024). Isabella Liu, Zhan Xu, Wang Yifan, Hao Tan, Zexiang Xu, Xiaolong Wang, Hao Su, and Zifan Shi. 2025b. Riganything: Template-free autoregressive rigging for diverse 3d assets. ACM Transactions on Graphics (TOG) 44, 4 (2025), 112. Jingyuan Liu, Jianlin Su, Xingcheng Yao, Zhejun Jiang, Guokun Lai, Yulun Du, Yidao Qin, Weixin Xu, Enzhe Lu, Junjie Yan, et al. 2025a. Muon is scalable for LLM training. arXiv preprint arXiv:2502.16982 (2025). Lijuan Liu, Youyi Zheng, Di Tang, Yi Yuan, Changjie Fan, and Kun Zhou. 2019. Neuroskinning: Automatic skin binding for production characters with deep graph networks. ACM Transactions on Graphics (ToG) 38, 4 (2019), 112. Loshchilov. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 (2017). Jing Ma and Dongliang Zhang. 2023. TARig: Adaptive template-aware neural rigging for humanoid characters. Computers & Graphics 114 (2023), 158167. Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. Journal of machine learning research 9, Nov (2008), 25792605. Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen. 2023. Finite scalar quantization: Vq-vae made simple. arXiv preprint arXiv:2309.15505 (2023). Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. 2016. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In 2016 fourth international conference on 3D vision (3DV). Ieee, 565571. Models-Resource. 2019. The Models-Resource. Blue Nile. 2025. Lazy Bones. https://blendermarket.com/products/lazy-bones. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems 36 (2023), 5372853741. Oren Rippel, Michael Gelbart, and Ryan Adams. 2014. Learning ordered representations with nested dropout. In International Conference on Machine Learning. PMLR, 1746 1754. John Schulman. 2020. Approximating kl divergence. http://joschu.net/blog/kl-approx. html John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 (2017). Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 (2024). , Vol. 1, No. 1, Article . Publication date: February 2026. Md Fahim Sikder, Resmi Ramachandranpillai, and Fredrik Heintz. 2025. Transfusion: generating long, high fidelity time series using diffusion models with transformers. Machine Learning with Applications 20 (2025), 100652. Kihyuk Sohn, Honglak Lee, and Xinchen Yan. 2015. Learning structured output representation using deep conditional generative models. Advances in neural information processing systems 28 (2015). Chaoyue Song, Xiu Li, Fan Yang, Zhongcong Xu, Jiacheng Wei, Fayao Liu, Jiashi Feng, Guosheng Lin, and Jianfeng Zhang. 2025a. Puppeteer: Rig and Animate Your 3D Models. arXiv preprint arXiv:2508.10898 (2025). Chaoyue Song, Jianfeng Zhang, Xiu Li, Fan Yang, Yiwen Chen, Zhongcong Xu, Jun Hao Liew, Xiaoyang Guo, Fayao Liu, Jiashi Feng, et al. 2025b. Magicarticulate: Make your 3d models articulation-ready. In Proceedings of the Computer Vision and Pattern Recognition Conference. 1599816007. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing 568 (2024), 127063. Carole Sudre, Wenqi Li, Tom Vercauteren, Sebastien Ourselin, and Jorge Cardoso. 2017. Generalised dice overlap as deep learning loss function for highly unbalanced segmentations. In International Workshop on Deep Learning in Medical Image Analysis. Springer, 240248. Mingze Sun, Junhao Chen, Junting Dong, Yurun Chen, Xinyu Jiang, Shiwei Mao, Puhua Jiang, Jingbo Wang, Bo Dai, and Ruqi Huang. 2024. DRiVE: Diffusion-based Rigging Empowers Generation of Versatile and Expressive Characters. arXiv preprint arXiv:2411.17423 (2024). Andrea Tagliasacchi, Hao Zhang, and Daniel Cohen-Or. 2009. Curve skeleton extraction from incomplete point cloud. In ACM SIGGRAPH 2009 papers. 19. Aaron Van Den Oord, Oriol Vinyals, et al. 2017. Neural discrete representation learning. Advances in neural information processing systems 30 (2017). Vaswani. 2017. Attention is all you need. Advances in Neural Information Processing Systems (2017). Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. 2024. Structured 3D Latents for Scalable and Versatile 3D Generation. arXiv preprint arXiv:2412.01506 (2024). Zhan Xu, Yang Zhou, Evangelos Kalogerakis, Chris Landreth, and Karan Singh. 2020. Rignet: Neural rigging for articulated characters. arXiv preprint arXiv:2005.00559 (2020). Zhan Xu, Yang Zhou, Li Yi, and Evangelos Kalogerakis. 2022. Morig: Motion-aware rigging of character meshes from point clouds. In SIGGRAPH Asia 2022 conference papers. 19. Yajie Yan, David Letscher, and Tao Ju. 2018. Voxel cores: Efficient, robust, and provably good approximation of 3d medial axes. ACM Transactions on Graphics (TOG) 37, 4 (2018), 113. Yajie Yan, Kyle Sykes, Erin Chambers, David Letscher, and Tao Ju. 2016. Erosion thickness on medial axes of 3D shapes. ACM Transactions on Graphics (TOG) 35, 4 (2016), 112. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. 2025. Qwen3 Technical Report. arXiv preprint arXiv:2505.09388 (2025). Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 2023. 3dshape2vecset: 3d shape representation for neural fields and generative diffusion models. ACM Transactions on Graphics (TOG) 42, 4 (2023), 116. Jia-Peng Zhang, Cheng-Feng Pu, Meng-Hao Guo, Yan-Pei Cao, and Shi-Min Hu. 2025b. One model to rig them all: Diverse skeleton rigging with unirig. ACM Transactions on Graphics (TOG) 44, 4 (2025), 118. Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. 2024. CLAY: Controllable Large-scale Generative Model for Creating High-quality 3D Assets. ACM Transactions on Graphics (TOG) 43, 4 (2024), 120. Yi Zhang, Bolin Ni, Xin-Sheng Chen, Heng-Rui Zhang, Yongming Rao, Houwen Peng, Qinglin Lu, Han Hu, Meng-Hao Guo, and Shi-Min Hu. 2025a. Bee: High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs. arXiv preprint arXiv:2510.13795 (2025)."
        }
    ],
    "affiliations": [
        "BNRist, Department of Computer Science and Technology, Tsinghua University, China",
        "VAST, China",
        "Zhili College, Tsinghua University, China"
    ]
}