{
    "paper_title": "Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual Editing",
    "authors": [
        "Xiangyu Zhao",
        "Peiyuan Zhang",
        "Kexian Tang",
        "Hao Li",
        "Zicheng Zhang",
        "Guangtao Zhai",
        "Junchi Yan",
        "Hua Yang",
        "Xue Yang",
        "Haodong Duan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Multi-modality Models (LMMs) have made significant progress in visual understanding and generation, but they still face challenges in General Visual Editing, particularly in following complex instructions, preserving appearance consistency, and supporting flexible input formats. To address this gap, we introduce RISEBench, the first benchmark for evaluating Reasoning-Informed viSual Editing (RISE). RISEBench focuses on four key reasoning types: Temporal, Causal, Spatial, and Logical Reasoning. We curate high-quality test cases for each category and propose an evaluation framework that assesses Instruction Reasoning, Appearance Consistency, and Visual Plausibility with both human judges and an LMM-as-a-judge approach. Our experiments reveal that while GPT-4o-Native significantly outperforms other open-source and proprietary models, even this state-of-the-art system struggles with logical reasoning tasks, highlighting an area that remains underexplored. As an initial effort, RISEBench aims to provide foundational insights into reasoning-aware visual editing and to catalyze future research. Though still in its early stages, we are committed to continuously expanding and refining the benchmark to support more comprehensive, reliable, and scalable evaluations of next-generation multimodal systems. Our code and data will be released at https://github.com/PhoenixZ810/RISEBench."
        },
        {
            "title": "Start",
            "content": "ENVISIONING BEYOND THE PIXELS: BENCHMARKING REASONING-INFORMED VISUAL EDITING Xiangyu Zhao1,2, Peiyuan Zhang3, Kexian Tang2,4, Hao Li2, Zicheng Zhang1,2 Guangtao Zhai1,2, Junchi Yan1, Hua Yang1, Xue Yang1(cid:0), Haodong Duan2(cid:0) 1 Shanghai Jiao Tong University 3 Wuhan University Equal contribution (cid:0) Corresponding author {zhaoxiangyu, duanhaodong}@pjlab.org.cn"
        },
        {
            "title": "4 Tongji University",
            "content": "5 2 0 2 3 ] . [ 1 6 2 8 2 0 . 4 0 5 2 : r Figure 1: Comparison of leading models on our Reasoning-Informed viSual Editing tasks."
        },
        {
            "title": "ABSTRACT",
            "content": "Large Multi-modality Models (LMMs) have made significant progress in visual understanding and generation, but they still face challenges in General Visual Editing, particularly in following complex instructions, preserving appearance consistency, and supporting flexible input formats. To address this gap, we introduce RISEBench, the first benchmark for evaluating Reasoning-Informed viSual Editing (RISE). RISEBench focuses on four key reasoning types: Temporal, Causal, Spatial, and Logical Reasoning. We curate high-quality test cases for each category and propose an evaluation framework that assesses Instruction Reasoning, Appearance Consistency, and Visual Plausibility with both human judges and an LMM-as-a-judge approach. Our experiments reveal that while GPT-4o-Native significantly outperforms other open-source and proprietary models, even this state-of-the-art system struggles with logical reasoning tasks, highlighting an area that remains underexplored. As an initial effort, RISEBench aims to provide foundational insights into reasoning-aware visual editing and to catalyze future research. Though still in its early stages, we are committed to continuously expanding and refining the benchmark to support more comprehensive, reliable, and scalable evaluations of next-generation multimodal systems. Our code and data will be released at https://github.com/PhoenixZ810/RISEBench."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Multi-Modality Models (LMMs) have achieved remarkable progress in both visual understanding (Liu et al., 2023; Bai et al., 2023; Chen et al., 2024; Team et al., 2023; Luo et al., 2024) and visual generation (Podell et al., 2023; Rombach et al., 2022; Betker et al., 2023). Meanwhile, significant efforts (Team, 2024; Xie et al., 2024; Wang et al., 2024; Wu et al., 2024; Li et al., 2025) have been dedicated to unifying these two tasks, with the goal of enhancing overall performance"
        },
        {
            "title": "Technical Report",
            "content": "through joint learning. Some open-source models have demonstrated decent capability in either visual understanding or image generation; however, they still exhibit substantial limitations in General Visual Editing (i.e., transforming an input image based on textual instructions). Specifically, current open-source methods struggle with: (1) accurately following complex editing instructions (Sun et al., 2023); (2) preserving the original images appearance during visual editing (Koh et al., 2023); and (3) accommodating flexible input formats (Wang et al., 2024; Xie et al., 2024) (e.g., supporting both single and multiple images with natural language instructions). These limitations severely hinder their practical utility, making them hardly worth rigorous evaluation in this task. Recently, we observed that proprietary models such as GPT-4o (Hurst et al., 2024) and Gemini2.0-Flash (Team et al., 2023) have made significant advancements over open-source counterparts  (Fig. 1)  . Notably, these models exhibit remarkable capability in Reasoning-Informed viSual Editing (RISE) sophisticated ability that enables models to make intelligent visual modifications based on contextual understanding and logical reasoning. This advanced ability has exciting implications for various real-world applications, such as context-aware image modification (e.g., adjusting lighting to match scenes time of day), intelligent object insertion or removal with semantic consistency, and content adaptation based on inferred user intent. However, traditional image editing models (Kawar et al., 2023; Hertz et al., 2022; Brooks et al., 2023) that do not incorporate multi-modal reasoning lack these capabilities entirely. While such phenomenon is promising, we found that there is currently no well-established benchmark for systematically evaluating RISE, making it difficult to quantitatively assess and further study this ability in existing models. To this end, we introduce RISEBench, focused, small-scale benchmark specifically designed to evaluate reasoning-informed visual editing (RISE) capabilities. In this benchmark, we identify and categorize key image editing challenges that require four fundamental types of reasoning: temporal reasoning, causal reasoning, spatial reasoning, and logical reasoning. We manually curated diverse set of high-quality test cases for each category to ensure comprehensive evaluation  (Fig. 2)  . For evaluation, we decompose the quality of the edited output images into three key dimensions: instruction reasoning, appearance consistency, and generation plausibility. Evaluations are conducted using both human judges and an LMM-as-a-judge framework. Additionally, we perform extensive experiments to verify the reliability and effectiveness of our evaluation methodology. Using RISEBench, we conduct systematic evaluation of state-of-the-art LMMs with visual editing capabilities. Our results reveal that open-source visual editing models such as FLUX (Labs, 2024) and EMU2 (Sun et al., 2024) show limited reasoning capabilities, resulting in notably low performance across most test cases. GPT-4o* (Hurst et al., 2024) demonstrates relatively strong instructionfollowing behavior, yet its image-generation approach primarily relies on semantically reconstructing the original image content. In contrast, proprietary native models, such as Gemini-2.0-Flash (Team et al., 2023) and GPT-4o-Native, achieve significantly better overall performance. Notably, GPT-4oNative displays strong capabilities across temporal, causal, and spatial reasoning tasks. However, it still struggles with logical reasoning, highlighting an area for future research. In summary, our main contributions are as follows: 1. We propose the first dedicated benchmark for assessing Reasoning-Informed viSual Editing (RISE), establishing foundation for systematic assessment in this emerging area. 2. We define core categories of RISE challenges, design meaningful evaluation dimensions, and present an effective LMM-as-a-judge framework for scalable and automated assessment. 3. We conduct comprehensive evaluation and analysis of existing LMMs, offering novel insights into their reasoning-driven visual editing capabilities and highlighting areas for future improvement. The present version of this benchmark serves primarily as an exploratory study, offering valuable insights and facilitating an initial assessment of the current models performance. Moving forward, we remain committed to continually refining and expanding the benchmark to deliver more rigorous and robust analyses that further advance the field. *While image generation was not officially listed as capability of GPT-4o in prior releases, the official online platform offers this functionality. Accordingly, we use the online interface to generate images in our experiments. Throughout this paper, we refer to this online version as GPT-4o. With online demo released on 2025-03-26."
        },
        {
            "title": "Technical Report",
            "content": "Figure 2: Overview of RISEBench. We present illustrative example questions from each of the four problem categories, each demanding profound image understanding and reasoning capabilities."
        },
        {
            "title": "2 RISEBENCH",
            "content": "Humans possess deep, conceptual understanding of objects and scenes in the real world that goes far beyond superficial attributes such as color and shape. For example, people can effortlessly reason about: 1) Temporal evolution of objects (temporal reasoning), such as fruits rotting over time, iron tools rusting, or children growing into adults; 2) Transformative changes due to external factors (causal reasoning): like ice cream melting under sunlight or vehicles becoming damaged after collisions; 3) Spatial configurations (spatial reasoning): including how shapes appear from different viewing angles and how various components assemble into complete structures. 4) Additionally, people can easily solve visual puzzle problems (logical reasoning) such as tic-tac-toe or mazes, and concretely imagine their solutions. However, these capabilities present significant challenges for most generative models, which struggle to incorporate such reasoning into their visual outputs. To objectively assess current models performance on these tasks and clearly identify their limitations, we propose RISEBench, the first benchmark specifically designed to evaluate reasoning-informed visual editing capabilities of image generative models across these dimensions of human-like visual understanding."
        },
        {
            "title": "2.1 BENCHMARK CONSTRUCTION",
            "content": "Among the broad spectrum of visual editing tasks, we focus on four major problem categories that require deep image understanding coupled with accurate reasoning processes to be successfully completed. We term these categories as Temporal Reasoning, Causal Reasoning, Spatial Reasoning, and Logical Reasoning. For each category, we curate well-designed set of high-quality, diverse test cases. Each test sample consists of an input image paired with an instruction prompt, illustrating reasoning-driven image transformations (see illustrative examples in Fig. 2). Temporal Reasoning. Temporal reasoning tasks are designed to assess models ability to understand and anticipate the evolution of objects or scenes over time. Beyond recognizing static visual attributes such as color, shape, or size, reasoning-capable generative model should also capture how these attributes might change under natural temporal progression. This category spans wide range of objects or scenarios involving plants, animals, people, weather conditions, everyday objects, and"
        },
        {
            "title": "Technical Report",
            "content": "temporal events. For instance, given an image of fresh banana placed on table and the prompt, Draw what it will look like one year later in daily life setting, the model must infer that bananas naturally decay over time when left at room temperature. correct response would involve generating an image of heavily decomposed banana - blackened, shriveled, and possibly moldy - while preserving the broader scene context (e.g., the table and background) in consistent manner. Such tasks challenge models to go beyond superficial transformations and demonstrate deep, temporally grounded understanding of the world. Causal Reasoning. Causal reasoning constitutes critical dimension for evaluating whether generative model can capture the dynamics of real-world interactions. Unlike temporal reasoning, which focuses on natural progression over time, causal reasoning requires the model to understand how external forces or events directly alter an objects state. It involves reasoning about cause-effect relationships rather than just temporal sequences. For example, when prompted with an image of an intact apple and the instruction show the apple after someone has taken bite, the model must infer the visual consequences of the specific action: part of the apple should be missing, exposing the inner flesh. Similarly, it should understand how balloon changes when inflated, or how an egg appears once cracked open. These transformations require the model to possess implicit knowledge of material properties, physical laws, and expected outcomes of common interactions. Spatial Reasoning. Spatial reasoning tasks evaluate models ability to comprehend and manipulate spatial relationships among objects within scene. Unlike simple image classification or detection, these tasks require deeper understanding of geometric structure, relative positioning, physical feasibility, and 3D reasoning. They assess whether generative model can internalize the principles that govern how objects occupy and relate to space. We categorize spatial reasoning challenges into four representative subtypes: 1. Object Arrangement tasks test the models understanding of size, alignment, symmetry, and balance by requiring it to organize objects based on spatial attributes such as height, width, or shape. 2. Component Assembly tasks evaluate the models ability to infer how parts combine into coherent whole for example, assembling furniture or mechanical devices requiring knowledge of connectivity, orientation, and structural constraints. 3. Geometric Construction tasks involve forming complex shapes or silhouettes from basic geometric units or simulating transforms such as folding, stacking, or cutting. 4. Viewpoint Understanding tasks challenge the model to reason about objects from various perspectives, identify occluded or hidden parts, and simulate visual changes due to rotation or translationskills closely related to mental rotation and object permanence. Collectively, these tasks form comprehensive testbed for diagnosing whether model possesses the spatial intelligence required for visually grounded reasoning and structure-aware generation. Logical Reasoning. In contrast to other categories that primarily evaluate models understanding of physical or commonsense knowledge over natural images, logical reasoning tasks assess models ability to perform structured, rule-based inference grounded in visual input. These tasks require not only interpreting visual elements, but also applying formal rules systematically to reach correct conclusions level of reasoning that remains challenging for current generative models. To probe this capability, we curate diverse suite of carefully designed puzzles and logical challenges. This set includes classic visual problems such as Sudoku, Mazes, and Tic-Tac-Toe, as well as more complex tasks involving mathematical reasoning, such as computing shortest paths, solving formula-based questions, and executing multi-step deductive processes. Each task demands that models internalize the rules governing the problem and apply them consistently to generate plausible, image-based solutions grounded in the visual context of the input. By incorporating broad range of logical tasks with varying levels of abstraction and difficulty, this category rigorously tests models capacity for visual-symbolic reasoning and highlights current limitations in bridging perception with inference."
        },
        {
            "title": "2.2 EVALUATION PIPELINE",
            "content": "Effectively evaluating the quality of generated images in reasoning-informed visual editing remains an open challenge. To address this, we first develop detailed scoring guidelines and conduct comprehensive human evaluations across multiple dimensions, including Instruction Reasoning, Appearance Consistency, and Visual Plausibility. However, relying solely on human judgment is costly, time-"
        },
        {
            "title": "Technical Report",
            "content": "Figure 3: Evaluation metrics of RISEBench. RISEBench assesses the quality of generated images along three key dimensions: Instruction Following, Appearance Consistency, and Visual Plausibility. For each dimension, carefully crafted prompts are provided to the evaluator model (GPT-4o in this study), which analyzes various inputs and returns scores for each corresponding sub-dimension. consuming, and difficult to reproduce at scale. To mitigate these limitations, we further explore an LMM-as-a-Judge strategy. With the rapid advancement of LMMs, cutting-edge models have demonstrated strong capabilities in image comprehension and reasoning, enabling them to provide consistent and accurate evaluations. In this work, we design robust LMM-as-a-Judge evaluation pipeline and leverage state-of-the-art LMMs to generate automated assessments. Our approach offers scalable and reproducible alternative to human evaluation, while maintaining high degree of alignment with human judgment. Evaluation Dimensions. To comprehensively assess model performance across diverse task types, we define three key evaluation dimensions: Instruction Reasoning, Appearance Consistency, and Visual Plausibility. These dimensions are designed to capture not only whether the model understands and follows the given editing instruction (Instruction Reasoning), but also whether it preserves relevant visual attributes from the original image (Appearance Consistency), and whether the generated output is coherent, realistic, and physically or logically plausible within the given context (Visual Plausibility). Together, these criteria provide holistic evaluation framework. Dimension 1: Instruction Reasoning. This dimension assesses the models ability to accurately understand and execute the given instruction, with particular attention to both explicit directives and implicit requirements embedded within the prompt. high-quality response not only performs the literal task specified but also captures the underlying reasoning or intended visual effect implied by the instruction. For example, given the prompt: Place the apple under magnifying glass, the ideal output should depict red apple that appears visually magnified through the glassreflecting both the object placement and the associated optical transformation. If model generates an image where the apple is correctly positioned beneath magnifying glass but lacks any indication of magnification, it demonstrates understanding of the explicit instruction but overlooks the implicit visual effect. In such case, the response would receive an intermediate score (e.g., 3 out of 5). full score (e.g., 5) is reserved for outputs that satisfy both the literal placement and the expected magnification, indicating robust instruction comprehension and reasoning. Dimension 2: Appearance Consistency. Appearance consistency measures how well the visual elements unrelated to the instruction are preserved between the input and output images. This dimension is particularly important in visual editing tasks, as it distinguishes between models that perform grounded edits based on the original image (e.g., cascade-based models) and those that regenerate scenes from scratch (e.g., native generation models). faithful editing model should"
        },
        {
            "title": "Technical Report",
            "content": "retain aspects such as image style, background details, object textures, lighting conditions, and textual or graphical elements (e.g., fonts, lines) that are not explicitly targeted by the instruction. For tasks involving temporal, causal, or spatial reasoning where the input is typically natural image rich in visual complexity appearance consistency is scored on continuous scale from 1 to 5, allowing for nuanced evaluation of how well the core scene is preserved post-editing. In contrast, logical reasoning tasks often involve stylized or synthetic inputs with simple layouts, such as solid backgrounds, geometric shapes, or text. Given their minimalistic structure, consistency in these cases is evaluated using binary scheme: score of 5 indicates full preservation of visual properties, while 1 reflects major deviations. This dimension ensures that models not only generate correct content but also do so in way that respects the visual fidelity of the original input, which is essential for coherent and context-preserving visual editing. Dimension 3: Visual Plausibility. The visual quality and realism of the generated image are critical factors in evaluating the performance of generative models. This dimension assesses whether the output is free from common generation artifacts such as blurriness, unnatural distortions, structural incoherence, or violations of physical laws. plausible image should not only align with the instruction but also maintain visual integrity and realism consistent with how similar scenes would appear in the real world. The dimension only applies to tasks involving temporal, causal, or spatial reasoning where outputs are expected to resemble natural images visual plausibility is evaluated on graded scale from 1 to 5, allowing for nuanced differentiation between high-quality and flawed generations. This dimension ensures that, beyond correctness and consistency, the generated images meet basic threshold of visual fidelity and realism, which is essential for practical deployment of generative models in real-world applications. Other evaluation details, such as the specific instructions provided to judges (human evaluators and LMM-based assessors), carefully selected in-context examples, and the detailed configuration of the LMM judgement, are provided in Appx. C. Overall Evaluation Metrics. To provide comprehensive evaluation of model performance, we report two complementary metrics: Score and Accuracy. For each test sample, weighted average score is computed based on the three evaluation dimensions introduced previously: Score = α Consistency + β InsReasoning + γ Plausibility, (1) where the weights α, β, and γ are configured to reflect the relative importance of each dimension depending on the task category. Specifically, α is uniformly set to 0.4 across all task types, reflecting the foundational importance of appearance consistency in visual editing. For logical reasoning tasks, β is assigned higher weight of 0.6 to emphasize the central role of reasoning over visual fidelity, whereas for other task types, β is set to 0.4. For logical reasoning tasks, γ is set to 0 (doesnt apply). In contrast, for temporal, causal, and spatial tasks where image realism is important γ is set to 0.2. During evaluation, all dimension scores are normalized to the range [1, 5]. sample is considered successfully solved only if it achieves maximum score of 5, indicating full satisfaction of all applicable evaluation dimensions. Accuracy is then defined as the percentage of samples that are successfully solved out of the total number of test cases. The two complementary metrics offer both fine-grained performance measurement and an interpretable success rate across tasks."
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "To evaluate the performance of representative visual editing approaches, we selected diverse set of models spanning multiple model architectures and generation paradigms. Specifically, Flux1.0Canny (Labs, 2024) serves as representative diffusion-based editing model, while EMU2 (Sun et al., 2024) exemplifies the auto-regressive generation paradigm. We also include three proprietary models: GPT-4o (Hurst et al., 2024), Gemini 2.0-Flash (Team et al., 2023), and GPT-4o-Native (2025-0326). Since all of the proprietary models do not offer public APIs for programmatic evaluation, we obtained their outputs directly via their respective official online interfaces."
        },
        {
            "title": "Technical Report",
            "content": "Table 1: Overall performance on RISEBench. GPT-4o-Native achieves the highest performance with an accuracy of only 35.9%, followed by Gemini-2-Flash with the second-highest accuracy (10.9%). The remaining models perform close to zero, which highlights the significant challenges that remain in achieving robust reasoning-informed visual editing. Models Temporal Causal Spatial Logical Total Score Accuracy Score Accuracy Score Accuracy Score Accuracy Score Accuracy GPT-4o-Native Gemini-2.0-Flash (Team et al., 2023) GPT-4o (Hurst et al., 2024) EMU2 (Sun et al., 2024) FLUX.1-Canny (Labs, 2024) 85.3 57.2 64.0 57. 29.7 18.7% 0.0% 0.0% 6.3% 0.0% 91.8 65.3 70.3 49.3 24.7 43.7% 6.3% 6.3% 0.0% 0.0% 87.5 71. 61.9 33.1 31.2 43.7% 25.0% 6.3% 0.0% 0.0% 62.5 32.5 37.5 18. 11.2 37.5% 12.5% 6.3% 0.0% 0.0% 81.8 56.6 58.4 39.7 24.2 35.9% 10.9% 4.7% 1.5% 0.0% Figure 4: Comparison across models on three evaluation sub-dimensions. The native generation models, GPT-4o-Native and Gemini-2-Flash, demonstrate strong performance across all three evaluation dimensions. GPT-4o performs well in instruction reasoning but struggles with appearance consistency. The remaining models fail to follow instructions, highlighting the gap in reasoninginformed visual editing."
        },
        {
            "title": "3.1 EVALUATION RESULTS (LMM-AS-A-JUDGE)",
            "content": "For easy comparison, we report evaluation results on 100-point scale in Tab. 1, with representative output examples shown in Fig. 5. All scores are assigned by the GPT-4o model, serving as the judge in our LMM-as-a-Judge evaluation pipeline. Among all models, the recently released GPT-4o-Native (2025-03-26) achieves the highest performance. While it obtains the highest average score (81.8) among all models, its low accuracy (35.9%) highlights persistent limitations in its ability to perform complex reasoning required for visual editing tasks. Another native model, Gemini-2.0-Flash, ranks second, scoring 56.6 on average and reaching an accuracy of 10.9%. In contrast, GPT-4o, often repaints the input rather than performing reasoning-grounded edits. This results in significantly lower accuracy of 4.7%. Open-source models show notable deficiencies. Both EMU2 and Flux1.0-Canny exhibit limited understanding of the input image and fail to generate semantically meaningful edits. Instead, they tend to produce visually disconnected or randomly altered outputs, leading to near-zero accuracy on RISEBench. These results underscore the existing gap between open-source and proprietary models, while also highlighting the remaining challenges in achieving robust reasoning-informed visual editing. To gain deeper insights into the strengths and limitations of each model, we analyze their average performance across three evaluation dimensions, as illustrated in Fig. 4. The results show that GPT-4o-Native, Gemini-2.0-Flash, and GPT-4o perform competitively in Instruction Reasoning, demonstrating the ability to correctly interpret and follow task prompts. In contrast, both EMU2 and Flux1.0-Canny exhibit limited instruction comprehension, resulting in significantly lower scores. Although GPT-4o shows competence in understanding instructions, it often generates outputs by loosely aligning with the semantic content of the original image rather than performing precise, grounded edits. This tendency leads to relatively low score of 46.1 in Appearance Consistency."
        },
        {
            "title": "Technical Report",
            "content": "Figure 5: Examples of models outputs on RISEBench. The complete outputs of the five evaluated models are provided in Appx. for comprehensive comparison."
        },
        {
            "title": "Technical Report",
            "content": "Figure 6: Correlation between human and model-based judgments. (a) illustrates the score distributions from human annotators and the model-based evaluator, while (b) presents the MAE between the two, with scores ranging from 0 to 4. The close alignment in score distributions and the low MAE values together indicate strong correlation between human and model judgments. Meanwhile, Gemini-2.0-Flash demonstrates the ability to understand and apply edits for simpler instructions but struggles with more complex reasoning and maintaining consistent output quality. In comparison, GPT-4o-Native delivers the most well-rounded performance across all three dimensions. It not only reliably interprets wide range of instructions but also maintains high consistency with the input imagepreserving the scene layout, style, and object integritywhile generating high-quality, realistic outputs free from common visual artifacts such as blur or distortion. Despite these strengths, even the most advanced model, GPT-4o-Native, continues to face substantial challenges in logical reasoning tasks. This highlights logical reasoning as key bottleneck and an important direction for future research in reasoning-informed visual generation."
        },
        {
            "title": "3.2 VALIDITY OF LMM-AS-A-JUDGE",
            "content": "To assess the validity of using LMMs as evaluators, we analyze the correlation between LMM-based assessments and human expert judgments. We conduct the user study involving six human experts, who independently score the outputs of two models (Gemini-2.0-Flash and GPT-4o) based on criteria aligned with those used in LMM-based evaluations, as the distribution and the Mean Absolute Error (MAE) between the scores given by human experts and those assigned by GPT-4o shown in Fig. 6(a) and Fig. 6(b). The distribution indicates that the scores assigned by human experts are closely aligned with those predicted by the model, demonstrating strong overall consistency. Notably, the alignment is more pronounced at the level of the total score compared to the individual sub-dimensions. This may be attributed to the fact that the total score is weighted aggregation of the three sub-dimensions, which makes it more robust to human subjectivity. Furthermore, the mean absolute error (MAE) is low across all dimensions, with the total score MAE reaching only 0.53, and the MAEs for all three sub-dimensions remaining below 1 on 4-point scale. These results further confirm the strong agreement between the model-generated scores and human judgments. It can be observed that MAE in the Visual Plausibility dimension is slightly higher than in the other two dimensions. This discrepancy is primarily due to the limitations of the judge model, which struggles to detect violations of physical laws or unrealistic scenarios. In contrast, human evaluators are more sensitive to such inconsistencies with real-world knowledge, leading to larger deviations between human and model assessments. Nevertheless, because Visual Plausibility contributes only"
        },
        {
            "title": "Technical Report",
            "content": "small proportion to the overall score, its higher MAE exerts only minor impact on the final results. This issue highlights an important area for future improvement and further investigation."
        },
        {
            "title": "3.3 DISCUSSION: SEMANTIC RECONSTRUCTION VS. NATIVE GENERATION",
            "content": "By comparing the performance of GPT-4o and other native generation models, we observe an interesting phenomenon: GPT-4o tends to adopt Translation Reconstruction cascade-like pipeline. Specifically, it first interprets the input image to extract semantic meaning, and then generates new image that closely aligns with this translated informationpossibly using models such as DALLE, or even generates code to directly insert line and text on input image, as respectively illustrated in Fig. 5 and Appx. A. This strategy fully leverages the strengths of the underlying reasoning model, resulting in strong instruction-following and reasoning capabilities. However, reconstructions based solely on semantic content often fail to capture fine-grained environmental details, especially those that are difficult to express explicitly in language. In contrast, native generation models prioritize pixel-level fidelity, preserving the visual appearance of the original image during editing. This approach proves advantageous for tasks that require high degree of visual consistencysuch as temporal and spatial reasoningwhere maintaining the original context and fine details is critical."
        },
        {
            "title": "4 CONCLUSION",
            "content": "In this technical report, we introduced RISEBench the first dedicated benchmark for evaluating the Reasoning-Informed Visual Editing (RISE) capabilities of multimodal models. RISEBench targets four core types of reasoning: temporal, causal, spatial, and logical, and provides structured evaluation framework that takes into account instruction reasoning, appearance consistency, and generation plausibility. Through extensive experiments, we observed that GPT-4o-Native significantly outperform its open-source and proprietary counterparts. However, even the most advanced models continue to exhibit notable shortcomings in logical reasoning tasks, highlighting key area for future research and model development. The current version of RISEBench serves as an exploratory effort aimed at providing preliminary insights into the performance of contemporary multimodal models. While still in its early stages, we are committed to continuously scaling and refining the benchmark to enable more comprehensive and robust evaluations in the future."
        },
        {
            "title": "5 ACKNOWLEDGEMENT",
            "content": "We would like to sincerely thank Wenhao Chai for his valuable contributions to this work, particularly for his insightful suggestions and thorough review of the manuscript."
        },
        {
            "title": "REFERENCES",
            "content": "Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 1 James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. 1 Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1839218402, 2023. 2 Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites, 2024."
        },
        {
            "title": "Technical Report",
            "content": "Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Promptto-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. 2 Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 2, 6, 7 Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 60076017, 2023. 2 Jing Yu Koh, Daniel Fried, and Russ Salakhutdinov. Generating images with multimodal language models. Advances in Neural Information Processing Systems, 36:2148721506, 2023. 2 Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. 2, 6, 7 Hao Li, Changyao Tian, Jie Shao, Xizhou Zhu, Zhaokai Wang, Jinguo Zhu, Wenhan Dou, Xiaogang Wang, Hongsheng Li, Lewei Lu, and Jifeng Dai. Synergen-vl: Towards synergistic image understanding and generation with vision experts and token folding. In 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. 1 Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. 1 Gen Luo, Xue Yang, Wenhan Dou, Zhaokai Wang, Jifeng Dai, Yu Qiao, and Xizhou Zhu. Monointernvl: Pushing the boundaries of monolithic multimodal large language models with endogenous visual pre-training. arXiv preprint arXiv:2410.08202, 2024. 1 Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. 1 Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, et al. Generative multimodal models are in-context learners. arXiv preprint arXiv:2312.13286, 2023. 2 Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1439814409, 2024. 2, 6, 7 Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 1 Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 1, 2, 6, Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 1, 2 Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848, 2024. 1 Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 1,"
        },
        {
            "title": "Technical Report",
            "content": "A EXAMPLE OF GENERATION FROM GPT-4O We observe that the previous version of GPT-4o typically attempts to analyze the input instruction and image, generate textual reasoning process, and subsequently produce code to overlay elements such as colors, lines, or text onto the input image, as illustrated in Fig. 7. While this approach demonstrates strong instruction-following and reasoning capabilities, it often results in low visual consistency in the final output. Figure 7: Example of cascade generation pipeline in GPT-4o."
        },
        {
            "title": "B DETAILED OUTPUTS OF ALL EVALUATED MODELS",
            "content": "The outputs of all evaluated models on our RISEBench benchmark are presented below for comprehensive comparison."
        },
        {
            "title": "C PROMPT FOR JUDGEMENT",
            "content": "We exhibit all our prompts for GPT-4o judger across different metrics and dimensions here."
        },
        {
            "title": "Technical Report",
            "content": "Figure 8: Logical Reasoning Outputs Part 1."
        },
        {
            "title": "Technical Report",
            "content": "Figure 9: Logical Reasoning Outputs Part 2."
        },
        {
            "title": "Technical Report",
            "content": "Figure 10: Spatial Reasoning Outputs Part 1."
        },
        {
            "title": "Technical Report",
            "content": "Figure 11: Spatial Reasoning Outputs Part 2."
        },
        {
            "title": "Technical Report",
            "content": "Figure 12: Temporal Reasoning Outputs Part 1."
        },
        {
            "title": "Technical Report",
            "content": "Figure 13: Temporal Reasoning Outputs Part 2."
        },
        {
            "title": "Technical Report",
            "content": "Figure 14: Causal Reasoning Outputs Part 1."
        },
        {
            "title": "Technical Report",
            "content": "Figure 15: Causal Reasoning Outputs Part 2."
        },
        {
            "title": "Prompt for Appearance Consistency on Temporal and Causal Reasoning",
            "content": "You are highly skilled image evaluator. You will receive two images (an original image and modified image) along with specific modification instruction. The second image is known to have been altered based on this instruction, starting from the first image. Your task is to evaluate whether the two images maintain consistency in aspects not related to the given instruction. Task Evaluate the consistency between the images according to the following scale (1 to 5): - 5 (Perfect Consistency): Apart from changes explicitly required by the instruction, all other details (e.g., personal features, clothing, background, layout, colors, positions of objects) are completely identical between the two images. - 4 (Minor Differences): Apart from changes explicitly required by the instruction, the second image is mostly consistent with the original image but contains minor discrepancy (such as missing minor personal feature, accessory, or tattoo). - 3 (Noticeable Differences): Apart from changes explicitly required by the instruction, the second image has one significant difference from the original (such as noticeable alteration in persons appearance like hair or skin color, or significant change in background environment). - 2 (Significant Differences): Apart from changes explicitly required by the instruction, the second image has two or more significant differences or multiple noticeable inconsistencies (such as simultaneous changes in both personal appearance and background environment). - 1 (Severe Differences): Apart from changes explicitly required by the instruction, nearly all key details (e.g., gender, major appearance features, background environment, or scene layout) significantly differ from the original image, clearly deviating from the original. Example: Original image: blond, white-skinned man with tattoo on his right shoulder, furniture in the background. Instruction: Show him after gaining fifty pounds. - Score 5: heavier blond, white-skinned man, tattoo on right shoulder intact, identical furniture and layout. - Score 4: heavier blond, white-skinned man, missing the tattoo on his right shoulder, identical furniture and layout. - Score 3: heavier man with black hair instead of blond (change in hair color), or original blond man but with grassy background instead of furniture. - Score 2: heavier man with black hair (hair color changed), and the background changed to grass. - Score 1: heavier black-haired woman, and background changed to grass. Note: When assigning scores, only consider details unrelated to the instruction. Changes explicitly requested by the instruction should NOT be regarded as inconsistencies. Input Instruction: {instruct} Output Format Provide detailed, step-by-step explanation of your scoring process. Conclude clearly with the final score, formatted as: Final Score: 1-5 Figure 16: Prompt for evaluating Appearance Consistency in Temporal Reasoning and Causal Reasoning."
        },
        {
            "title": "Technical Report",
            "content": "Prompt for Instruction Reasoning on Temporal and Causal Reasoning tasks. You are an expert image evaluator. For each task, you will be provided with: 1. An instruction describing how an image should be modified. 2. ground-truth textual description that represents the intended result of the modification. 3. An output image generated by an assistant. Your task is to assess the output image based on the following evaluation dimension: Evaluation Dimension: Alignment Between Image and Reference Description Assess how accurately the output image aligns with the visual content described in the reference description, considering the context of the instruction. Scoring Criteria: - 5: The image completely matches the description, accurately reflecting every detail and degree. - 4: The image mostly matches the description, with minor discrepancies. - 3: The image partially matches the description but contains differences or lacks some details. - 2: The image contains noticeable difference. Important details are missed or clearly inaccurate. - 1: The image fails to follow the instruction and does not correspond to the description at all. Example Instruction: Draw what it will look like after it is broken. Description: An egg is completely broken, with eggshell scattered around and egg white and yolk clearly spilling out. - 5: Completely broken egg, clearly scattered eggshells, visible egg white and yolk spilling out. - 4: Broken egg, eggshell present but not fully scattered, clearly visible egg white and yolk spilling out. - 3: Broken egg with scattered eggshell, but egg white and yolk not spilled or still within eggshell. - 2: Only scattered eggshell visible, without clear egg white or yolk. - 1: Egg is intact, not broken. Input Instruction instruct GroundTruth Description: reference Output Format Provide detailed, step-by-step explanation of your scoring process. Conclude clearly with the final score, formatted as: Final Score: Figure 17: Prompt for evaluating instruction reasoning on Temporal and Causal Reasoning tasks."
        },
        {
            "title": "Technical Report",
            "content": "Prompt for visual plausibility on Temporal and Causal Reasoning tasks. You are an expert image evaluator. For each task, you will be provided with an output image generated by an assistant. Your task is to independently assess the image along the following dimension and assign an integer score from 1 to 5: Evaluation Dimension: Realism and Generation Quality Assess the overall visual realism and generation fidelity of the image. Consider the images clarity, natural appearance, and compliance with physical plausibility and real-world constraints. Scoring Guidelines: - 5 The image is sharp, visually coherent, and all elements appear highly realistic and physically plausible. - 4 The image is clear, with most elements appearing realistic; minor details may show slight unreality. - 3 The image is mostly clear, but some significant elements appear unrealistic or physically implausible. - 2 The image is noticeably blurry or contains major unrealistic components or visual distortions. - 1 The image is extremely blurry, incoherent, or severely unrealistic; realism is nearly absent. Output Format After the evaluation, conclude clearly with the final score, formatted as: Final Score: Figure 18: Prompt for evaluating visual plausibility on Temporal and Causal Reasoning tasks."
        },
        {
            "title": "Technical Report",
            "content": "Prompt for evaluating Appearance Consistency on Spatial Reasoning task. You are precise and analytical image consistency evaluator. You will be given: - Image A: the original image. - Image B: modified version of Image A. - Instruction: directive describing the intended modification to Image to produce Image B. Your task is to evaluate how consistent Image remains with Image in all aspects *except* those explicitly changed by the instruction. You must ignore the instructed changes and only assess unintended differences. Evaluation Scale (1 to 5): - 5 Perfect Consistency All elements not related to the instruction are visually identical between Image and Image (e.g., style, background, object positions, colors, shapes). No unintended change is present. - 4 Minor Difference One small unintended change is present (e.g., slight color variation or minor object shape shift), but overall the image remains highly consistent. - 3 Noticeable Difference One major or few minor unintended changes are present (e.g., an objects shape, color, or background differs noticeably, or style has shifted slightly). - 2 Significant Inconsistency Two or more significant differences unrelated to the instruction (e.g., changes in both object details and background or style), reducing overall fidelity. - 1 Severe Inconsistency Major unintended changes dominate the image (e.g., altered visual style, scene layout, or appearance), clearly breaking consistency with Image A. Note: - To receive score of 5, the modified image must be visually identical to the original in every unaffected aspectsymbols, patterns, background, texture, color, category, layout, and style must all match exactly. - If the background in the original is vague (e.g., plain white or composed of parts), and the background in Image is also similar vague, you may disregard background consistency. - If blue diamond shape appears in the bottom-left corner of Image 2, ignore it; it is watermark. Example Original image: silver-framed clock with white face. Three hands (hour, minute, second) are disassembled and lie beside it. Instruction: Assemble the clock to show 9:45. Scoring Criteria: - Score 5: Frame, face, and hand shapes exactly as original. - Score 4: One hand differs slightly in shape or thickness. - Score 3: All hands identical, differing from original specs, or some other things(like text, furniture in the background) is added. - Score 2: Frame color or face differs, and hand shapes are wrong. - Score 1: Frame, face, and hand appearance all significantly altered, background is totally different. Input Instruction: instruct Output Format After evaluation, conclude with: Final Score: 1-5 Figure 19: Prompt for evaluating Appearance Consistency on Spatial Reasoning task."
        },
        {
            "title": "Technical Report",
            "content": "Prompt for evaluating Instruction Reasoning on Spatial Reasoning task. You are an expert image evaluator. For each task, you will be provided with: 1. An instruction describing how an image should be modified. 2. ground-truth textual description that represents the intended result of the modification. 3. An output image generated by an assistant. Your task is to assess the output image based on the following evaluation dimension: Evaluation Dimension: Alignment Between Image and Reference Description Assess how accurately the output image aligns with the visual content described in the reference description, considering the context of the instruction. Scoring Criteria: - 5: The image completely matches the description, accurately reflecting every detail and degree. - 4: The image mostly matches the description, with minor discrepancies. - 3: The image partially matches the description but contains differences or lacks some details. - 2: The image contains noticeable difference. Important details are missed or clearly inaccurate. - 1: The image fails to follow the instruction and is entirely unrelated to the description. Input Instruction instruct GroundTruth Description: reference Output Format Conclude clearly with the final score, formatted as: Final Score: Figure 20: Prompt for evaluating Instruction Reasoning on Spatial Reasoning task. Prompt for evaluating Visual Plausibility on Spatial Reasoning task. You are highly skilled image evaluator. Given an image, your task is to assess and determine its clarity and distortion, and then provide score (an integer between 1 and 5) based on the following criteria: Task Requirements: Determine whether the image has blurriness, distortion, visual defects, or physical inaccuracies. Assign an appropriate score to the image based on the above criteria, considering its overall quality and detail integrity. Scoring Criteria: - 5 points: The image is very clear, with complete details, and no noticeable distortion or blurriness. All elements conform to physical laws. - 4 points: The image is clear, with only minor blurriness, and no noticeable distortion. - 3 points: The image has areas with clarity issues, such as slight blurriness or distortion. Some elements are physically incorrect. - 2 points: The image has noticeable blurriness or distortion, with significant detail loss, or lacks physical accuracy. - 1 point: The image is severely blurry or distorted, making it difficult to recognize its content, with serious degradation in visual quality, almost unusable. Output Format Provide clear conclusion with the final score, formatted as follows: Final Score: 1-5 where represents the score. Figure 21: Prompt for evaluating Visual Plausibility on Spatial Reasoning task."
        },
        {
            "title": "Technical Report",
            "content": "Prompt for evaluating Logical Reasoning Tasks with reference text answer. You are highly skilled image evaluator. Given an image with logical problem, you will receive: 1. Image 1: The original image. 2. Image 2: generated image from an assistant model. 3. Problem Description 4. Reference Answer Your task is to determine whether Image 2 correctly match the reference answer. Evaluate Image 2 based on the following metrics, each scored as either 0 or 1: 1. Logical Correctness (0/1) - Assess whether the content of Image 2 logically matches the reference answer. - For example, given Image 1 is teacher with 1+1=? on the blackboard, and the problem is Replace the question mark with the correct answer, if Image 2 replaces the question mark with 2, then the score is 1; other is 0. 2. Appearance Consistency (0/1) Determine whether the style, environment, arrangement of Image 2 are consistent with Image 1. - Consider factors such as color scheme, line/font style, background setting, etc. If Image 2s appearance fully aligns with Image 1, score 1; otherwise, score 0. - If the only difference is the actual problem solution (not the style or setting) or slightly lighter/darker color, still assign score of 1. - If Image 2 is created by directly adding pattern to Image 1, still assign score of 1. - If in Image 1, the nodes and edges form an irregular quadrilateral with varying edge lengths and angles but form square-like arrangement with equal edge lengths and right angles in Image 2, the score is 0. Inputs Problem Description: instruct Reference Answer: reference Output You should provide step-by-step explanation of how you arrived at each score and conclude with the total scores for all three requirements in the format: Final Score: X,Y where and are the scores for the two metrics (Logical Correctness and Appearance Consistency), respectively. Figure 22: Prompt for evaluating."
        },
        {
            "title": "Technical Report",
            "content": "Prompt for evaluating Logical Reasoning Tasks with reference image answer. You are highly skilled image evaluator. Given logical problem, you will receive: 1. Image 1: reference ground-truth image that correctly solves the problem. 2. Image 2: generated image from an assistant model. Your task is to determine whether Image 2 correctly solves the problem, using Image 1 as the reference answer. Evaluate Image 2 based on the following metrics, each scored as either 0 or 1: 1. Logical Correctness (0/1) Assess whether the content of Image 2 logically equal to Image 1. Examples - In tic-tac-toe problem, if the positions of the marks in Image 2 are exactly the same as in Image 1, score 1; otherwise, score 0. - If the problem is to , only if Image 2 is completely identical to Image 1(reference answer) in terms of shape, color, arrangement pattern, and pattern orientation, score 1; otherwise, score 0. - If Image 1 only contains 1 gt answer but Image 2 contains several answers, score 0. 2. Appearance Consistency (0/1) Determine whether the style and environment of Image 2 are consistent with Image 1. - Consider factors such as color scheme, line style, background setting, etc. If Image 2s appearance fully aligns with Image 1, score 1; otherwise, score 0. - If the only difference is the actual problem solution(such as Image 1 with red line as solution and Image 2 with blue line as solution) or slightly lighter/darker color, still assign score of 1. - If Image 2 is created by directly adding pattern to Image 1, still assign score of 1. If blue diamond shape appears in the bottom-left corner of Image 2, ignore it; it is watermark. Problem Description instruct Output You should provide step-by-step explanation of how you arrived at each score and conclude with the total scores for all three requirements in the format: Final Score: X,Y where and are the scores for the two metrics (Logical Correctness and Appearance Consistency), respectively. Figure 23: Prompt for evaluating."
        }
    ],
    "affiliations": [
        "Shanghai Jiao Tong University",
        "Wuhan University"
    ]
}