{
    "paper_title": "TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos via Diffusion Models",
    "authors": [
        "Mark YU",
        "Wenbo Hu",
        "Jinbo Xing",
        "Ying Shan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present TrajectoryCrafter, a novel approach to redirect camera trajectories for monocular videos. By disentangling deterministic view transformations from stochastic content generation, our method achieves precise control over user-specified camera trajectories. We propose a novel dual-stream conditional video diffusion model that concurrently integrates point cloud renders and source videos as conditions, ensuring accurate view transformations and coherent 4D content generation. Instead of leveraging scarce multi-view videos, we curate a hybrid training dataset combining web-scale monocular videos with static multi-view datasets, by our innovative double-reprojection strategy, significantly fostering robust generalization across diverse scenes. Extensive evaluations on multi-view and large-scale monocular videos demonstrate the superior performance of our method."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 8 3 6 5 0 . 3 0 5 2 : r TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos via Diffusion Models Mark Yu Wenbo Hu"
        },
        {
            "title": "Jinbo Xing Ying Shan",
            "content": "ARC Lab, Tencent PCG"
        },
        {
            "title": "The Chinese University of Hong Kong",
            "content": "https://TrajectoryCrafter.github.io Figure 1. We present TrajectoryCrafter, novel approach to redirect camera trajectories for monocular videos, achieving precise control over the view transformations and coherent 4D content generation. Please refer to the supplementary project page for video results."
        },
        {
            "title": "Abstract",
            "content": "ular videos demonstrate the superior performance of our method. We present TrajectoryCrafter, novel approach to redirect camera trajectories for monocular videos. By disentangling deterministic view transformations from stochastic content generation, our method achieves precise control over userspecified camera trajectories. We propose novel dualstream conditional video diffusion model that concurrently integrates point cloud renders and source videos as conditions, ensuring accurate view transformations and coherent 4D content generation. Instead of leveraging scarce multiview videos, we curate hybrid training dataset combining web-scale monocular videos with static multi-view datasets, by our innovative double-reprojection strategy, significantly fostering robust generalization across diverse scenes. Extensive evaluations on multi-view and large-scale monocCorresponding author. 1 1. Introduction Videos, whether user-captured or AI-generated, have emerged as pervasive medium on social media. Yet, conventional videos merely provide glimpse into the dynamic world, whereas empowering users to freely redirect the camera trajectory within casual videos promises more immersive experience. To this vision, reconstruction-based approaches often yield implausible results in occluded regions [39, 54, 72, 78] or require synchronized multi-view videos [11, 21, 46, 65], which remain impractical for most users. Recent advancements exploit the generative prowess of image and video diffusion models [8, 9, 13, 49, 67, 88] to traditional synthesize novel views for static scenes [25, 58, 68, 83, 95], achieving remarkable fidelity from sparse inputs. However, these methods, tailor-made for static scenes, struggle to deliver plausible videos with novel camera trajectories from dynamic videos. The most related work is GCD [75], which adapts video diffusion model for novel view synthesis from monocular videos with fixed pose, using synthetic multi-view video training dataset. Nonetheless, its performance is hindered by the domain gap between synthetic and real videos, and its reliance on implicit pose embeddings [53] limits precise camera trajectory control. In this paper, we propose TrajectoryCrafter, novel framework for generating high-fidelity videos with userdefined camera trajectories from monoocular inputs. Our approach targets three fundamental challenges: 1). achieving precise trajectory control; 2). ensuring rigorous 4D consistency with the source; and 3). exploiting large-scale, diverse training data for robust generalization. For trajectory control, we explicitly decouple deterministic view transformation from stochastic content generation, such that we can leverage cutting-edge monocular reconstruction advancements [33, 55, 99] to accurately model 3D transformations via point cloud renders. To secure 4D consistency in content generation, we propose dual-stream conditional video diffusion model that integrates both point cloud renders and the source video as conditions. The point cloud branch utilizes pretrained diffusion layers for precise view transformations, while the source video branch incorporates newly introduced layers to counter occlusions and geometric distortions, thereby elevating the coherence and quality of the generated videos. To train our model, synchronized multiview videos are ideal yet prohibitively scarce. Fortunately, our decoupled view transformation and content generation allow us to prepare training data for content generation only. Motivated by this, We leverage hybrid training source combining web-scale monocular videos with static multiview datasets. For monocular data, we introduce novel double-reprojection strategy that simulates point cloud renders by reprojecting the source video to novel view and back, aided by video depth estimation. For static multiview data, we employ cutting-edge point cloud reconstruction methods [41, 80] to derive source and target videos alongside corresponding point cloud renders. This innovative data curation approach substantially enriches our training repository, fostering high-fidelity video generation with specfied camera trajectories across diverse unseen scenes. We extensively evaluate TrajectoryCrafter on both synchronized multi-view datasets and large-scale monocular video datasets, using reference-based and no-reference video quality assessment metrics, respectively. Both quantitative and qualitative results demonstrate the superior performance of our method in generating high-fidelity videos with novel camera trajectories, and generalizing robustly across diverse scenes. Ablation studies also affirm the efficacy of our dual-stream conditioning and dataset curation strategies. Our contributions are summarized as follows: We present TrajectoryCrafter, novel approach that redirects camera trajectories for monocular videos with exceptional fidelity and broad generalization. We propose dual-stream conditioning mechanism that fuses point cloud renders with source videos, ensuring precise trajectory control and coherent 4D generation. We curate novel data strategy combining dynamic webscale monocular video datasets with static multi-view resources, bolstering the models generalization and robustness across diverse scenes. 2. Related Work Reconstruction-based novel view synthesis. The advent of neural representations like NeRF [57] and 3DGS [36] has revolutionized static scene novel view synthesis [57, 12, 15, 19, 20, 26, 29, 32, 43, 48, 50, 52, 59, 76, 93, 94, 97, 101, 106], catalyzing significant progress in dynamic novel view synthesis, which is more challenging due to temporal variations. Current approaches [1, 11, 21, 46, 65, 71, 89] predominantly concentrate on reconstructing 4D representations from synchronized multi-view videos, which remain out of reach for typical users. Early attempts at 4D scene reconstruction from monocular videos employed depth-based warping [90], later refined by networks to handle occlusions. Subsequent research [22, 39, 44, 45, 61, 74] utilized neural representations for dynamic scene modeling. Recent advancements [24, 40, 72, 78] have harnessed the efficiency of 3DGS to generate novel views from monocular videos, often incorporating additional regularizations like optical flow or depth information to enhance performance. However, these approaches are limited to reconstructing visible regions, resulting in artifacts when viewed from significantly different perspectives. Generative novel view synthesis. Diffusion models [31, 67, 70] have demonstrated remarkable promise in generating novel views from monocular images or videos [35, 53, 68, 81, 96, 104]. For static scenarios, Zero-1-to3 [53] presents camera-pose-conditioned diffusion model later extended to for objects with clean backgrounds, generic scenes by ZeroNVS [68]. ReconFusion [83] refines pose estimation with PixelNeRF [92] features, whereas CAT3D [25] employs ray maps and 3D attention for enhanced consistency. Recent studies advance novel view synthesis through video diffusion models [8, 13, 84], incorporating camera embeddings [81], depth-warped images [56, 58], Plucker coordinates [2, 3, 30, 47, 85], and point cloud renderings [95]. For dynamic scenes, NVSSolver [91] leverages pretrained SVD model [8] for training-free depth-warped video inpainting but faces challenges with geometric distortions. CAT4D [82] constructs 2 Figure 2. Overview of TrajectoryCrafter. Starting with source video, whether casually captured or AI-generated, we first lift it into dynamic point cloud via depth estimation. Users can then interactively render the point cloud with desired camera trajectories. Finally, the point cloud renders and the source video are jointly processed by our dual-stream conditional video diffusion model, yielding high-fidelity video that precisely aligns with the specified trajectory and remains 4D consistent with the source video. 4D scenes from monocular videos by fine-tuning CAT3D on data produced by diverse video generation methods, risking limited applicability to real-world environments. CameraDolly [75] adapts the SVD model using synthetic multiview datasets from the Kubric simulator [28], yet domain disparities arise with real videos. DimensionX [73] integrates specialized motion LoRAs for dynamic novel view synthesis but lacks free-view generation. ReCapture [98] re-generates videos with customized camera paths via point cloud rendering and masked fine-tuning, necessitating pervideo LoRA adaptation without broader generalization. Additionally, stereo video generation [102] and driving scene novel view synthesis [18, 79, 86] are also receiving significant attention. In contrast, our approach delivers highfidelity videos with user-defined camera trajectories from generic inputs, circumventing the need for per-video optimization. 3. Method 3.1. Preliminary: Video Diffusion Models Video diffusion models [8, 9, 13, 49, 67, 88] invovle forward process to progressively inject noise ϵ into clean video data x0 Rn3hw, creating noisy states xt = αtx0 + σtϵ over time t, and reverse process pθ to remove noise via noise estimator ϵθ, trained by minimizing: min θ EtU (0,1),ϵN (0,I)[ϵθ(xt, t) ϵ2 2]. (1) Following Sora [10], recent diffusion approaches [49, 88] employ the Diffusion Transformer (DiT) [62] for the noise estimator. During training, pre-trained 3D VAE encoder compresses videos into latent space = E(x). Then, is patchified, concatenated with text tokens, and fed into the DiT. At inference time, the noise is iteratively denoised into clean tokens, which are mapped back by the VAE decoder to yield the final video ˆx = D(z). }n 3.2. View Transformation via Dynamic Point Cloud i=1 Rn3hw, we aim to Given source video = {I explore its underlying 4D scene with desired camera trajectory. An overview of our pipeline is illustrated in Fig. 2. To achieve precise camera trajectory control, we disentangle the deterministic view transformation from the stochastic content generation by lifting the source video into dynamic point cloud and rendering novel views from it. We first estimate sequence of depth maps Ds = {Ds i=1 Rnhw from the source video via monocular depth estimation [33, 87, 99]. Next, we lift the source video into dynamic point cloud = {Pi}n Pi = Φ1([I i=1: , Ds ], K), }n (2) where Φ1 is the inverse perspective projection, and R33 is the camera intrinsic matrix. With this dynamic point cloud, we render novel views = {I }n i=1 accordi=1 Rn44: ing to target camera trajectory = {T (3) Pi, K), = Φ(T i }n where Φ denotes the perspective projection. Because the point cloud is reconstructed in the camera coordinate system, is defined as the relative transformation from the source camera. The point cloud renders have noticeable holes from occlusions and out-of-frame regions, marked by the rendered mask video = {M i=1. Yet, they accurately capture geometric relations and view transformations. Therefore, we leverage and as conditions to guide the generation of high-fidelity videos under the target camera trajectory. }n 3 Figure 3. Ref-DiT Block. The text and view tokens are first processed through 3D attention, followed by cross-attention that injects the detailed, yet mis-aligned, reference information into the view tokens, yielding refined view tokens for subsequent layers. 3.3. Dual-stream Conditional Video Diffusion Having the dynamic point cloud renders and masks r, as well as the source video s, we learn conditional distribution p(x s, r, r) using conditional video diffusion model. We build upon CogVideoX [16, 88], originally designed for text-guided image-to-video (I2V) generation. We leverage its 3D VAE for video compression/decompression, BLIP [42] and T5 [66] encoder for captioning the video, and its DiT blocks for vision-text token processing. To adapt the I2V model for our task, re-generating videos with desired camera trajectories, we propose dual-stream conditioning strategy to leverage both the point cloud renders and the source video. First, we replace the original image conditions with point cloud renders and masks rfor precise camera control  (Fig. 2)  . We encode and using the VAE encoder, concatenate them channelwise with sampled noise, and project them into view tokens. We then concatenate these view tokens with text tokens along the sequence dimension and feed them into the DiT blocks for denoising. Since the point cloud renders spatially align with the required view transformations, they provide strong geometric cues to ensure desired camera trajectories. However, point cloud renders include holes, distortions, and degraded textures due to occlusions and depth errors. Meanwhile, the source video offers rich appearance details for high-fidelity generation. Directly concatenating it is suboptimal, as to be demonstrated in Fig. 7, due to the spatial misalignment with the target view tokens. To this end, we encode into reference tokens via the VAE encoder, and propose Reference-conditioned Diffusion Transformer (Ref-DiT) block, inserted between the inherited DiT blocks  (Fig. 2)  . In each Ref-DiT block, as shown 4 Figure 4. Double-reprojection. Given target video, we lift it into dynamic point cloud to render novel view via random view transformation. Then is reprojected to the original camera pose, yielding through the inverse view transformation. contains occlusions and aligns with the target video, simulating the point cloud renders. in Fig. 3, text and view tokens are processed through 3D attention, followed by cross-attention layer, where the view tokens act as queries and the reference tokens as keys and values, to inject information from reference tokens into view tokens. Compared to direct concatenation, our RefDiT block enables transferring detailed, yet mis-aligned, information from the source video to the view tokens by the cross-attention mechanism, enhancing the fidelity of the generated novel views. These two conditioning streams, the point cloud renders and the source video, work synergistically to guide the denoising, enabling generation of highfidelity videos with accurate camera trajectories and consistent content. 3.4. Dataset Curation and Training Scheme To train the model, we ideally require synchronized multiview videos of diverse 4D scenes. However, existing multiview datasets [17, 27, 28, 69, 103] tend to be small in scale, non-photo-realistic in style, or lacking in diversity. Training solely on such datasets would limit performance and generalization in real-world scenarios. Data curation. Fortunately, our TrajectoryCrafter explicitly decouples view transformation from content generation, allowing us to only prepare training data for the dual-stream condition video diffusion model. Motivated by this, we propose two strategies to process web-scale monocular video datasets [14, 60, 77] and static multi-view datasets [51, 105], respectively. For monocular datasets, we propose doublereprojection strategy to produce large-scale training pairs of point cloud renders and target videos, serving as condition and supervision. Given target video t, we lift it into dynamic point cloud using Eq. 2, then render novel views and corresponding depths with ranFigure 5. Qualitative comparison of novel trajectory video synthesis. We compare our method with both reconstruction-based method, Shape-of-motion [78], and generative methods, GCD [75] and ViewCrafter [95] on the multi-view dataset, iphone [23]. domly sampled relative view transformation . Next, we back-project into point cloud with D, and re-render the view with the inverse transformation 1. As shown in Fig. 4, has view transformation from target video t, while realigns with yet contains holes from disocclusions and out-of-frame regions, like the point cloud renders. In this way, we can generate large-scale, diverse, and high-quality training pairs from web-scale monocular video datasets. We adopt the OpenVid-1M [60] monocular video dataset and employ DepthCrafter [33] to create 60K paired point cloud renders and target videos for training. Another valuable data source is static multi-view datasets [51, 105], which feature diverse and large camera movements. To incorporate these datasets, we design paradigm to generate large-scale training triplets, consisting of source video, target video, and point cloud render. For each video in the dataset, we sample segment and simuteously reconstruct its global point cloud and estimate camera pose for each frame, using advanced reconstruction method, e.g. MASt3R [41]. We then sample two video clips from the segment, with overlapping content, designating one as the source video and the other as the target video. Finally, the point cloud of the source video is rendered to match the camera pose of the target video, creating the required triplet for training. We adopt the DL3DV [51] and RealEstate10K [105] datasets, generating 120K static multi-view training data. Training scheme. Having the datasets, we adopt twostage training scheme to leverage both dynamic monocular and static multi-view data. In the first stage, we train the DiT blocks and the 3D attention layers in the Ref-DiT blocks using all data, but freeze the cross-attention layers and skip reference-token injection. This step focuses on synthesizing missing regions and correcting geometric distortions, while preserving the targeted view transformations in the point cloud renders. However, without reference tokens, the generated content may be inconsistent with the source video in 4D space. In the second stage, we enable the model to transfer detailed information from the reference tokens to the view tokens, by training on the triplet training data. Since only the multi-view static data provides the required triplets, we use it exclusively in the second stage. To prevent overfitting to static data, we only train the crossattention and patch embedding layers in the Ref-DiT blocks, while freezing all other modules. After these two stages, the model is capable of generating high-fidelity videos with accurate view transformations and consistent content. 4. Experiments 4.1. Implementation We implement our dual-stream conditional video diffusion model based on the pretrained CogVideoX-Fun-5B [16, 88] architecture. During training, the frame resolution is fixed at 384672, and the video length is set to 49 frames. The first training stage is conducted for 10,000 iterations with learning rate of 1105, while the second stage is conducted for 5,000 iterations with learning rate of 2106. Both training stages use mini-batch size of 8 and are conducted on eight GPUs. For producing dynamic point cloud, we use DepthCrafter [33] to estimate temporally consistent depth sequences from the source video and empirically set camera intrinsic parameters. 4.2. Evaluation on Multi-view Video Benchmark Dataset and evaluation metrics. To pixel-wisely evaluate the quality of the synthesized videos under novel trajectories, we employ the iPhone dataset [23], which contains 7 scenes captured with casually moving camera and two static cameras. We take the casually-captured video as the 5 Table 1. Quantitative comparison of novel trajectory video synthesis. We report the PSNR, SSIM, and LPIPS metrics for each scene and the mean values across all scenes on the multi-view dataset, iphone [23]. The best results are highlighted in bold. PSNR SSIM Apple Block Paper Spin Teddy Mean Apple Block Paper Spin Teddy Mean Apple Block Paper Spin Teddy Mean Method GCD 9.823 12.30 9.75 10.37 11.61 10.77 0.215 0.458 0.398 0.324 0.385 0.356 0.738 0.590 0.535 0.576 0.629 0.614 10.19 10.28 10.63 11.15 11.50 10.75 0.245 0.427 0.344 0.308 0.372 0.339 0.750 0.615 0.521 0.533 0.606 0.605 ViewCrafter Shape-of-motion 11.06 11.72 11.93 11.28 10.42 11.28 0.197 0.446 0.425 0.319 0.357 0.349 0.879 0.601 0.486 0.560 0.650 0.635 13.88 14.21 14.89 14.51 13.73 14.24 0.285 0.528 0.482 0.380 0.411 0.417 0.612 0.479 0.471 0.518 0.513 0.519 Ours LPIPS Table 2. VBench results on in-the-wild monocular videos. We compiled large-scale in-the-wild video benchmark with 100 real-world and 60 high-quality T2V-generated videos, and report the VBench scores of novel trajectory videos from GCD [75], ViewCrafter [95], and our method. The best results are highlighted in bold. Method GCD ViewCrafter Ours VBench Subject Consis. Background Consis. Temporal Flicker. Motion Smooth. Overall Consis. Aesthetic Quality Imaging Quality 0.7677 0.7305 0.9236 0.8533 0.8782 0.9512 0.8215 0.8954 0.9437 0.8950 0.9031 0. 0.2321 0.2432 0.2847 0.4154 0.4950 0.5920 0.5203 0.5548 0.6479 source and the first fixed-camera video as the target for evaluation, since the second target video is incomplete. Following [78], we discard the Space-out and Wheel scenes due to camera and LiDAR errors, and use the remaining 5 refined scenes with COLMAP poses and aligned depth for evaluation. We adopt PSNR, SSIM, and LPIPS [100] as evaluation metrics. Comparison baselines. We compare our method with three baselines: GCD [75], ViewCrafter [95], and Shapeof-motion [78], where the first two methods are diffusionbased generative novel view synthesis models, and the last one is the SOTA reconstruction-based 4D novel view synthesis method. GCD is 4D novel view synthesis method that incorporates implicit camera pose embeddings into video diffusion model, and can generate 14-frame novel view videos from source video with given relative camera pose changes. ViewCrafter is originally designed for static novel view synthesis from single or sparse images through the point cloud renders, but we found that it can be also used for 4D novel view synthesis by conditioning it on 4D point cloud renders. Shape-of-motion leverages dense point tracks, monocular depths, and motion masks as regularization to optimize dynamic 3DGS from monocular video. Qualitative results. We first compare the visual quality of results by our method and the baselines in Fig. 5, where the ground truth novel trajectory videos are shown in the last column. We can see that the videos synthesized by Shapeof-motion exhibit significant holes due to its reconstructionbased nature. And the results of GCD suffer from overly smoothed details and view misalignment. This is because GCD is trained on synthetic data with poor texture quality that significantly differs from real-world videos, and its view control mechanism is through an implicit pose embedding, which lacks precise control. Results from ViewCrafter show better pose accuracy but struggle to generate highfidelity frames since it is designed for static novel view synthesis and lacks the ability to maintain 4D coherence. In contrast, our method generates novel trajectory videos with high fidelity, 4D consistency, and precise pose control, thanks to the dual-stream conditional video diffusion model and the diverse web-scale training data. Quantitative comparison. We further quantitatively evaluate the results of our method and the baselines in Tab. 1. The results show that our method consistently outperforms the baselines across all metrics by significant margin, indicating that our approach maintains better quality of the synthesized novel trajectory videos and closer resemblance to the ground truth. 4.3. Evaluation on in-the-wild Video Benchmark Dataset and evaluation metrics. The previous multi-view video benchmark contains limited number of scenes, making it hard to evaluate the generalization ability. Moreover, using pixel-wise metrics like PSNR and SSIM to access similarity between generated and ground truth novel views is not ideal, as occluded regions can be filled with many semantically plausible contents, yet not resemble the ground truth. This is also reflected by the values in Tab. 1, where the PSNR values for all methods are relatively low (lower than 15 dB). For better evaluation, we collect relatively largescale in-the-wild video benchmark, comprising 100 realworld monocular videos [64] and 60 high-quality videos generated by T2V models [4, 10, 37, 38, 63]. For each video, we generate 6 different novel trajectory videos by our method and the generative baselines, GCD and ViewCrafter. For evaluation metrics, we use the VBench [34] protocol, which includes Subject Consistency, Background Consistency, Temporal Flickering, Motion Smoothness, Aesthetic Quality, Imaging Quality, and Overall Consistency scores. Quantitative results. As shown in Tab. 2, our method significantly outperforms the baselines across all video quality metrics, demonstrating state-of-the-art performance in gen6 Figure 6. Qualitative comparison on in-the-wild monocular videos. We show results of redirecting the camera trajectory as zoom-in and orbit to the right from the input videos, produced by our method and the generative baselines, GCD [75] and ViewCrafter [95]. Table 3. Ablation study. We report the PSNR, SSIM, and LPIPS metrics for the full model and its ablated versions on the multiview dataset, iphone [23]. The best results are highlighted in bold. Method Ours w/o Ref-DiT Ours w/ concat. condition Ours w/o dynamic data Ours w/o mv data Ours full PSNR 11.63 11.05 10.93 12.35 14. SSIM 0.341 0.352 0.348 0.361 0.417 LPIPS 0.612 0.627 0.621 0.588 0.519 ViewCrafter [81] demonstrates improved pose accuracy, it struggles to preserve consistency with the source video, leading to content drift and color discrepancies. By contrast, our method achieves higher visual fidelity, superior alignment with the target trajectory, and consistent content with the source video. 4.4. Ablation Study Effectiveness of the Ref-DiT blocks. In one of our key contributions, dual-stream conditional video diffusion model, we propose the Ref-DiT blocks to incorporate the source video information into the denoising process. To evaluate the effectiveness of the Ref-DiT blocks, we compare the novel view synthesis results of three variants: w/o Ref-DiT: the base model without Ref-DiT blocks, w/ Concat Condition: directly concatenating the source video with Figure 7. Effectiveness of Ref-DiT blocks. We compare our full model (w/ Ref-DiT) to two alternatives: baseline without RefDiT (w/o Ref-DiT), and variant that directly concatenates the source video with the point cloud renders (w/ Concat Condition). The yellow box highlights the most prominent differences. erating high-quality novel trajectory videos, in terms of subject and background consistency, temporal flickering, motion smoothness, aesthetic quality, and imaging quality. Qualitative results. Fig. 6 presents an example of novel trajectory video generation, illustrating the target camera trajectory as zoom-in and orbit to the right. Results from GCD exhibit noticeable artifacts, oversmoothed textures, and pronounced inaccuracies in camera motion, resulting in misalignment with the intended trajectory. While 7 Figure 8. Ablation on the training data. We compare our model trained with mixed data to two alternatives: training without multiview data and training without dynamic data. The yellow box highlights the most prominent differences of occulusions, geometric distortions, and motion consistency. the point cloud renders, and w/ Ref-DiT: our full model. From the qualitative results in Fig. 7, we can observe that the model without Ref-DiT blocks struggles to maintain content consistency, e.g. the eyebrows marked in the yellow box. This is because the point cloud renders may suffer from geometric distortions and texture artifacts, and the diffusion model cannot effectively generate content consistent with the source video. And the simple concatenation strategy also fails to generate plausible results, since the source video information is spatially misaligned with the target view and direct concatenation cannot effectively integrate the source video information. In contrast, our full model generates videos with high fidelity and content consistency, demonstrating the effectiveness of the Ref-DiT blocks. Furthermore, we quantitatively ablate the effectiveness of the Ref-DiT blocks on the iPhone dataset [23] in Tab. 3. Our full model significantly outperforms the other two variants across all metrics, which confirms that the Ref-DiT blocks significantly enhance the quality of novel trajectory video generation. Ablation on the training data. The dataset curation strategy is another key contribution of our work, where we train our model on mixed dataset of both dynamic monocular and static multi-view data. To validate its effectiveness, we conduct an ablation study to compare the novel trajectory video generation results of models trained without dynamic data, without multi-view data, and with the mixed data. The qualitative results are presented in Fig. 8, where we can observe that the model trained without multi-view data struggles to address occlusions and geometric distortions effectively, e.g. the yellow box marked regions. Conversely, the model trained without dynamic data fails to accurately folFigure 9. Failure case. Due to inaccuracies in depth estimation, the generated novel trajectory videos may exhibit physically implausible behavior, e.g., the dogs nose appears to pass through the glass of the door. low the motion of the source video. In contrast, training with the mixed data allows the model to generate semantically meaningful content in the occlusion regions and maintain consistent motion with the source video. The quantitative results in Tab. 3 further confirm the effectiveness of our dataset curation and training strategy. 4.5. Limitations While our method can generate high-fidelity novel trajectory videos, it still has several limitations. Firstly, it struggles to synthesize very large-range trajectories, such as 360degree viewpoints, due to insufficient 3D cues from monocular inputs and the constrained generation length of the video diffusion model. Secondly, because our approach relies on depth estimation to create dynamic point clouds from monocular videos, inaccuracies in this process may propagate, producing suboptimal novel trajectory videos (see Fig. 9). Finally, as video diffusion model, our method involves multi-step denoising during inference, leading to relatively high computational overhead. 5. Conclusion We introduce TrajectoryCrafter, an innovative approach to redirect camera trajectories for monocular videos. It allows users to re-generate high-fidelity videos with desired camera trajectories from casual captures or AI-generated footage, while maintaining 4D consistency with the source video. Our dual-stream conditional video diffusion model integrates point cloud renders and source videos as condition, ensuring accurate view transformations and coherent 4D content generation. Instead of relying on scarce synchronized multi-view video datasets, we assemble hybrid training corpus from large-scale dynamic monocular videos and static multi-view datasets, significantly enhancing model generalization across diverse scenarios. Extensive evaluations on multi-view and large-scale monocular videos confirm our methods performance in producing high-fidelity outputs with novel camera trajectories."
        },
        {
            "title": "References",
            "content": "[1] Benjamin Attal, Jia-Bin Huang, Christian Richardt, Michael Zollhoefer, Johannes Kopf, Matthew OToole, and Changil Kim. HyperReel: High-fidelity 6-DoF video with ray-conditioned sampling. In CVPR, 2023. 2 [2] Sherwin Bahmani, Ivan Skorokhodov, Aliaksandr Siarohin, Willi Menapace, Guocheng Qian, Michael Vasilkovsky, Hsin-Ying Lee, Chaoyang Wang, Jiaxu Zou, Andrea Tagliasacchi, et al. Vd3d: Taming large video diffusion transformers for 3d camera control. In ICLR, 2025. 2 [3] Jianhong Bai, Menghan Xia, Xintao Wang, Ziyang Yuan, Xiao Fu, Zuozhu Liu, Haoji Hu, Pengfei Wan, and Di Zhang. Syncammaster: Synchronizing multi-camera video generation from diverse viewpoints. In ICLR, 2025. 2 [4] Fan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min Zhao, Shilong Liu, Yaole Wang, and Jun Zhu. Vidu: highly consistent, dynamic and skilled text-to-video generator with diffusion models. arXiv preprint arXiv:2405.04233, 2024. 6 [5] Jonathan Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul Srinivasan. Mip-nerf: multiscale representation for anti-aliasing neural radiance fields. In ICCV, 2021. 2 [6] Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In CVPR, 2022. [7] Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Zip-nerf: Anti-aliased gridbased neural radiance fields. In ICCV, 2023. [8] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 1, 2, 3 [9] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In CVPR, 2023. 1, 3 [10] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. 3, 6 [11] Ang Cao and Justin Johnson. Hexplane: fast representation for dynamic scenes. In CVPR, 2023. 1, 2 [12] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo. In ICCV, 2021. [13] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. 1, 2, 3 [14] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, 9 Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. In CVPR, 2024. 4 [15] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images. In ECCV, 2024. 2 [16] CogVideoX-Fun. Cogvideox-fun, 2024. 4, 5 [17] Kellie Corona, Katie Osterdahl, Roderic Collins, and Anthony Hoogs. Meva: large-scale multiview, multimodal video dataset for activity detection. In WACV, 2021. 4 [18] Lue Fan, Hao Zhang, Qitai Wang, Hongsheng Li, and Zhaoxiang Zhang. Freesim: Toward free-viewpoint camera simulation in driving scenes. In CVPR, 2025. 3 [19] Zhiwen Fan, Wenyan Cong, Kairun Wen, Kevin Wang, Jian Zhang, Xinghao Ding, Danfei Xu, Boris Ivanovic, Marco Pavone, Georgios Pavlakos, et al. Instantsplat: Unbounded sparse-view pose-free gaussian splatting in 40 seconds. arXiv:2403.20309, 2024. [20] Chaoran Feng, Wangbo Yu, Xinhua Cheng, Zhenyu Tang, Junwu Zhang, Li Yuan, and Yonghong Tian. Ae-nerf: Augmenting event-based neural radiance fields for non-ideal conditions and larger scene, 2025. 2 [21] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk Warburg, Benjamin Recht, and Angjoo Kanazawa. Kplanes: Explicit radiance fields in space, time, and appearance. In CVPR, 2023. 1, 2 [22] Chen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang. Dynamic view synthesis from dynamic monocular video. In ICCV, 2021. 2 [23] Hang Gao, Ruilong Li, Shubham Tulsiani, Bryan Russell, and Angjoo Kanazawa. Monocular dynamic view synthesis: reality check. In NeurIPS, 2022. 5, 6, 7, 8 [24] Quankai Gao, Qiangeng Xu, Zhe Cao, Ben Mildenhall, Wenchao Ma, Le Chen, Danhang Tang, and Ulrich Neumann. Gaussianflow: Splatting gaussian dynamics for 4d content creation. arXiv preprint arXiv:2403.12365, 2024. 2 [25] Ruiqi Gao, Aleksander Holynski, Philipp Henzler, Arthur Brussee, Ricardo Martin-Brualla, Pratul Srinivasan, Jonathan Barron, and Ben Poole. CAT3D: Create Anything in 3D with Multi-View Diffusion Models. NeurIPS, 2024. [26] Stephan Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, and Julien Valentin. Fastnerf: High-fidelity neural rendering at 200fps. In ICCV, 2021. 2 [27] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In CVPR, 2022. 4 [28] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David Fleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, et al. Kubric: scalable dataset generator. In CVPR, 2022. 3, 4 [29] Guangcong, Zhaoxi Chen, Chen Change Loy, and Ziwei Liu. Sparsenerf: Distilling depth ranking for few-shot novel view synthesis. In ICCV, 2023. 2 [30] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. In ICLR, 2025. 2 [31] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 2 [32] Wenbo Hu, Yuling Wang, Lin Ma, Bangbang Yang, Lin Gao, Xiao Liu, and Yuewen Ma. Tri-miprf: Tri-mip representation for efficient anti-aliasing neural radiance fields. In ICCV, 2023. 2 [33] Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, and Ying Shan. Depthcrafter: Generating consistent long depth sequences for open-world videos. arXiv preprint arXiv:2409.02095, 2024. 2, 3, 5 [34] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In CVPR, 2024. 6 [35] Zhipeng Huang, Wangbo Yu, Xinhua Cheng, ChengShu Zhao, Yunyang Ge, Mingyi Guo, Li Yuan, and Yonghong Tian. Roompainter: View-integrated diffusion for consistent indoor scene texturing, 2025. 2 [36] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM TOG, 2023. [37] kling. kling, 2024. 6 [38] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 6 [39] Yao-Chih Lee, Zhoutong Zhang, Kevin Blackburn-Matzen, Simon Niklaus, Jianming Zhang, Jia-Bin Huang, and Feng Liu. Fast view synthesis of casual videos with soup-ofplanes. In ECCV, 2025. 1, 2 [40] Jiahui Lei, Yijia Weng, Adam Harley, Leonidas Guibas, and Kostas Daniilidis. Mosca: Dynamic gaussian fusion from casual videos via 4d motion scaffolds. arXiv preprint arXiv:2405.17421, 2024. 2 [41] Vincent Leroy, Yohann Cabon, and Jerome Revaud. Grounding image matching in 3d with mast3r. arXiv:2406.09756, 2024. 2, 5 [42] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, 2022. 4 [43] Ruilong Li, Matthew Tancik, and Angjoo Kanazawa. Nerfacc: general nerf acceleration toolbox. arXiv preprint arXiv:2210.04847, 2022. [44] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural scene flow fields for space-time view synthesis of dynamic scenes. In CVPR, 2021. 2 [45] Zhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker, and Noah Snavely. Dynibar: Neural dynamic image-based rendering. In CVPR, 2023. 2 [46] Zhan Li, Zhang Chen, Zhong Li, and Yi Xu. Spacetime gaussian feature splatting for real-time dynamic view synthesis. In CVPR, 2024. 1, 2 [47] Hanwen Liang, Junli Cao, Vidit Goel, Guocheng Qian, Sergei Korolev, Demetri Terzopoulos, Konstantinos Plataniotis, Sergey Tulyakov, and Jian Ren. Wonderland: NavarXiv preprint igating 3d scenes from single image. arXiv:2412.12091, 2024. 2 [48] Zhihao Liang, Qi Zhang, Wenbo Hu, Lei Zhu, Ying Feng, and Kui Jia. Analytic-splatting: Anti-aliased 3d gaussian splatting via analytic integration. In ECCV, 2024. 2 [49] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. 1, 3 [50] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Simon Lucey. Barf: Bundle-adjusting neural radiance fields. In ICCV, 2021. [51] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. Dl3dv-10k: large-scale scene dataset for deep learning-based 3d vision. In CVPR, 2024. 4, 5 [52] Junchen Liu, Wenbo Hu, Zhuo Yang, Jianteng Chen, Guoliang Wang, Xiaoxue Chen, Yantong Cai, Huan-ang Gao, and Hao Zhao. Rip-nerf: Anti-aliasing radiance fields with ripmap-encoded platonic solids. In ACM SIGGRAPH, 2024. 2 [53] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In ICCV, 2023. 2 [54] Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu Tseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Johannes Kopf, and Jia-Bin Huang. Robust dynamic radiance fields. In CVPR, 2023. 1 [55] Jiahao Lu, Tianyu Huang, Peng Li, Zhiyang Dou, Cheng Lin, Zhiming Cui, Zhen Dong, Sai-Kit Yeung, Wenping Wang, and Yuan Liu. Align3r: Aligned monocular depth estimation for dynamic videos. arXiv preprint arXiv:2412.03079, 2024. 2 [56] Baorui Ma, Huachen Gao, Haoge Deng, Zhengxiong Luo, Tiejun Huang, Lulu Tang, and Xinlong Wang. You see it, you got it: Learning 3d creation on pose-free videos at scale. In CVPR, 2025. 2 [57] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020. [58] Norman Muller, Katja Schwarz, Barbara Rossle, Lorenzo Porzi, Samuel Rota Bul`o, Matthias Nießner, and Peter Kontschieder. Multidiff: Consistent novel view synthesis from single image. In CVPR, 2024. 2 [59] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. ACM Transactions on Graphics (ToG), 41(4):115, 2022. 2 [60] Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai. 10 Openvid-1m: large-scale high-quality dataset for text-tovideo generation. arXiv preprint arXiv:2407.02371, 2024. 4, 5 [61] Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan Goldman, Steven M. Seitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In ICCV, 2021. [62] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. 3 [63] Pika. pika, 2024. 6 [64] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbelaez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017. 6 [65] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields for dynamic scenes. In CVPR, 2021. 1, 2 [66] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of Machine Learning Research, 2020. 4 [67] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 1, 2, [68] Kyle Sargent, Zizhang Li, Tanmay Shah, Charles Herrmann, Hong-Xing Yu, Yunzhi Zhang, Eric Ryan Chan, Dmitry Lagun, Li Fei-Fei, Deqing Sun, and Jiajun Wu. ZeroNVS: Zero-shot 360-degree view synthesis from single real image. In CVPR, 2024. 2 [69] Fadime Sener, Dibyadip Chatterjee, Daniel Shelepov, Kun He, Dipika Singhania, Robert Wang, and Angela Yao. Assembly101: large-scale multi-view video dataset for understanding procedural activities. In CVPR, 2022. 4 [70] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. 2 [71] Liangchen Song, Anpei Chen, Zhong Li, Zhang Chen, Lele Chen, Junsong Yuan, Yi Xu, and Andreas Geiger. Nerfplayer: streamable dynamic scene representation with decomposed neural radiance fields. IEEE TVCG, 2023. 2 [72] Colton Stearns, Adam Harley, Mikaela Uy, Florian Dubost, Federico Tombari, Gordon Wetzstein, and Leonidas Guibas. Dynamic gaussian marbles for novel view synthesis of casual monocular videos. In SIGGRAPH Asia 2024 Conference Papers, 2024. 1, 2 [73] Wenqiang Sun, Shuo Chen, Fangfu Liu, Zilong Chen, Yueqi Duan, Jun Zhang, and Yikai Wang. Dimensionx: Create any 3d and 4d scenes from single image with controllable video diffusion. arXiv preprint arXiv:2411.04928, 2024. [74] Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollhofer, Christoph Lassner, and Christian Theobalt. Non-rigid neural radiance fields: Reconstruction and novel view synthesis of dynamic scene from In Proceedings of the IEEE/CVF Inmonocular video. ternational Conference on Computer Vision (ICCV), pages 1295912970, 2021. 2 [75] Basile Van Hoorick, Rundi Wu, Ege Ozguroglu, Kyle Sargent, Ruoshi Liu, Pavel Tokmakov, Achal Dave, Changxi Zheng, and Carl Vondrick. Generative camera dolly: Extreme monocular dynamic novel view synthesis. In ECCV, 2024. 2, 3, 5, 6, 7 [76] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan Barron, and Pratul Srinivasan. Ref-nerf: Structured view-dependent appearance for neural radiance fields. In CVPR, 2022. 2 [77] Qiuheng Wang, Yukai Shi, Jiarong Ou, Rui Chen, Ke Lin, Jiahao Wang, Boyuan Jiang, Haotian Yang, Mingwu Zheng, Xin Tao, et al. Koala-36m: large-scale video dataset improving consistency between fine-grained conditions and video content. arXiv preprint arXiv:2410.08260, 2024. 4 [78] Qianqian Wang, Vickie Ye, Hang Gao, Jake Austin, Zhengqi Li, and Angjoo Kanazawa. Shape of motion: arXiv preprint 4d reconstruction from single video. arXiv:2407.13764, 2024. 1, 2, 5, 6 [79] Qitai Wang, Lue Fan, Yuqi Wang, Yuntao Chen, and Zhaoxiang Zhang. Freevs: Generative view synthesis on free driving trajectory. In ICLR, 2025. [80] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In CVPR, 2024. 2 [81] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH, 2024. 2, 7 [82] Rundi Wu, Ruiqi Gao, Ben Poole, Alex Trevithick, Changxi Zheng, Jonathan Barron, and Aleksander Holynski. Cat4d: Create anything in 4d with multi-view video diffusion models. arXiv preprint arXiv:2411.18613, 2024. 2 [83] Rundi Wu, Ben Mildenhall, Philipp Henzler, Keunhong Park, Ruiqi Gao, Daniel Watson, Pratul Srinivasan, Dor Verbin, Jonathan Barron, Ben Poole, et al. Reconfusion: 3d reconstruction with diffusion priors. In CVPR, 2024. 2 [84] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xintao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicrafter: Animating open-domain images with video diffusion priors. arXiv preprint arXiv:2310.12190, 2023. 2 [85] Dejia Xu, Weili Nie, Chao Liu, Sifei Liu, Jan Kautz, Zhangyang Wang, and Arash Vahdat. Camco: Camera-controllable 3d-consistent image-to-video generation. arXiv preprint arXiv:2406.02509, 2024. 2 [86] Yunzhi Yan, Zhen Xu, Haotong Lin, Haian Jin, Haoyu Guo, Yida Wang, Kun Zhan, Xianpeng Lang, Hujun Bao, Xiaowei Zhou, et al. Streetcrafter: Street view synthesis with controllable video diffusion models. In CVPR, 2025. 3 [87] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. arXiv:2406.09414, 2024. 3 [88] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-tovideo diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 1, 3, 4, 11 [103] Yang Zheng, Adam Harley, Bokui Shen, Gordon Wetzstein, and Leonidas Guibas. Pointodyssey: large-scale In ICCV, synthetic dataset for long-term point tracking. 2023. 4 [104] Haiyang Zhou, Xinhua Cheng, Wangbo Yu, Yonghong Tian, and Li Yuan. Holodreamer: Holistic 3d panoramic world generation from text descriptions. arXiv preprint arXiv:2407.15187, 2024. 2 [105] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. ACM TOG, 2018. 4, 5 [106] Zehao Zhu, Zhiwen Fan, Yifan Jiang, and Zhangyang Wang. Fsgs: Real-time few-shot view synthesis using gaussian splatting. In ECCV, 2024. 2 [89] Zeyu Yang, Hongye Yang, Zijie Pan, and Li Zhang. Realtime photorealistic dynamic scene representation and rendering with 4d gaussian splatting. In ICLR, 2024. 2 [90] Jae Shin Yoon, Kihwan Kim, Orazio Gallo, Hyun Soo Park, and Jan Kautz. Novel view synthesis of dynamic scenes with globally coherent depths from monocular camera. In CVPR, 2020. 2 [91] Meng You, Zhiyu Zhu, Hui Liu, and Junhui Hou. Nvssolver: Video diffusion model as zero-shot novel view synthesizer. In ICLR, 2025. [92] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4578 4587, 2021. 2 [93] Wangbo Yu, Yanbo Fan, Yong Zhang, Xuan Wang, Fei Yin, Yunpeng Bai, Yan-Pei Cao, Ying Shan, Yang Wu, Zhongqian Sun, et al. Nofa: Nerf-based one-shot facial avatar reconstruction. In ACM SIGGRAPH, 2023. 2 [94] Wangbo Yu, Chaoran Feng, Jiye Tang, Jiashu Yang, Zhenyu Tang, Xu Jia, Yuchao Yang, Li Yuan, and Yonghong Tian. Evagaussians: Event stream assisted gaussian splatting arXiv preprint arXiv:2405.20224, from blurry images. 2024. 2 [95] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024. 2, 5, 6, 7 [96] Wangbo Yu, Li Yuan, Yan-Pei Cao, Xiangjun Gao, Xiaoyu Li, Wenbo Hu, Long Quan, Ying Shan, and Yonghong Tian. Hifi-123: Towards high-fidelity one image to 3d content generation. In ECCV, 2024. 2 [97] Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, and Andreas Geiger. Mip-splatting: Alias-free 3d gaussian splatting. In CVPR, 2024. 2 [98] David Junhao Zhang, Roni Paiss, Shiran Zada, Nikhil Karnad, David Jacobs, Yael Pritch, Inbar Mosseri, Mike Zheng Shou, Neal Wadhwa, and Nataniel Ruiz. Recapture: Generative video camera controls for userprovided videos using masked video fine-tuning. arXiv preprint arXiv:2411.05003, 2024. [99] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and Ming-Hsuan Yang. Monst3r: simple approach for estimating geometry in the presence of motion. arXiv preprint arXiv:2410.03825, 2024. 2, 3 [100] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. 6 [101] Zheng Zhang, Wenbo Hu, Yixing Lao, Tong He, and Hengshuang Zhao. Pixel-gs: Density control with pixel-aware gradient for 3d gaussian splatting. In ECCV, 2024. 2 [102] Sijie Zhao, Wenbo Hu, Xiaodong Cun, Yong Zhang, Xiaoyu Li, Zhe Kong, Xiangjun Gao, Muyao Niu, and Ying Shan. Stereocrafter: Diffusion-based generation of long and high-fidelity stereoscopic 3d from monocular videos. arXiv preprint arXiv:2409.07447, 2024."
        }
    ],
    "affiliations": [
        "ARC Lab, Tencent PCG"
    ]
}