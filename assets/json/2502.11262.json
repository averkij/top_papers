{
    "paper_title": "Generating Skyline Datasets for Data Science Models",
    "authors": [
        "Mengying Wang",
        "Hanchao Ma",
        "Yiyang Bian",
        "Yangxin Fan",
        "Yinghui Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Preparing high-quality datasets required by various data-driven AI and machine learning models has become a cornerstone task in data-driven analysis. Conventional data discovery methods typically integrate datasets towards a single pre-defined quality measure that may lead to bias for downstream tasks. This paper introduces MODis, a framework that discovers datasets by optimizing multiple user-defined, model-performance measures. Given a set of data sources and a model, MODis selects and integrates data sources into a skyline dataset, over which the model is expected to have the desired performance in all the performance measures. We formulate MODis as a multi-goal finite state transducer, and derive three feasible algorithms to generate skyline datasets. Our first algorithm adopts a \"reduce-from-universal\" strategy, that starts with a universal schema and iteratively prunes unpromising data. Our second algorithm further reduces the cost with a bi-directional strategy that interleaves data augmentation and reduction. We also introduce a diversification algorithm to mitigate the bias in skyline datasets. We experimentally verify the efficiency and effectiveness of our skyline data discovery algorithms, and showcase their applications in optimizing data science pipelines."
        },
        {
            "title": "Start",
            "content": "Mengying Wang Case Western Reserve University Cleveland, Ohio, USA mxw767@case.edu Hanchao Ma Case Western Reserve University Cleveland, Ohio, USA hxm382@case.edu Yiyang Bian Case Western Reserve University Cleveland, Ohio, USA yxb227@case.edu Yangxin Fan Case Western Reserve University Cleveland, Ohio, USA yxf451@case.edu Yinghui Wu Case Western Reserve University Cleveland, Ohio, USA yxw1650@case.edu 5 2 0 F 6 1 ] . [ 1 2 6 2 1 1 . 2 0 5 2 : r ABSTRACT Preparing high-quality datasets required by various data-driven AI and machine learning models has become cornerstone task in data-driven analysis. Conventional data discovery methods typically integrate datasets towards single pre-defined quality measure that may lead to bias for downstream tasks. This paper introduces MODis, framework that discovers datasets by optimizing multiple user-defined, model-performance measures. Given set of data sources and model, MODis selects and integrates data sources into skyline dataset, over which the model is expected to have the desired performance in all the performance measures. We formulate MODis as multi-goal finite state transducer, and derive three feasible algorithms to generate skyline datasets. Our first algorithm adopts reducefrom-universal strategy, that starts with universal schema and iteratively prunes unpromising data. Our second algorithm further reduces the cost with bi-directional strategy that interleaves data augmentation and reduction. We also introduce diversification algorithm to mitigate the bias in skyline datasets. We experimentally verify the efficiency and effectiveness of our skyline data discovery algorithms, and showcase their applications in optimizing data science pipelines."
        },
        {
            "title": "1 INTRODUCTION\nHigh-quality machine learning (ML) models have become criti-\ncale assets for various domain sciences research. A routine task\nin data-driven domain sciences is to prepare datasets that can\nbe used to improve such data science models. Data augmenta-\ntion [36] and feature selection [27] have been studied to sug-\ngest data for ML models [7]. Nevertheless, they typically gen-\nerate data by favoring a pre-defined, single performance goal,\nsuch as data completeness or feature importance. Such data may\nbe biased and not very useful to actually improve the model\nperformance, and moreover, fall short at addressing multiple\nuser-defined ML performance measures (e.g., expected accuracy,\ntraining cost). Such need is evident in multi-variable experiment\noptimization [23, 30, 33], feature selection [27], and AI bench-\nmarking [9], among others.",
            "content": "Discovering datasets that can improve model over multiple user-defined performance measures remains to be desirable yet less studied issue. Consider the following real-world example. Example 1: To assess the impact and causes of harmful algal blooms (HABs) in lake, research team aims to forecast the 2025 Copyright held by the owner/author(s). Published in Proceedings of the 28th International Conference on Extending Database Technology (EDBT), 25th March-28th March, 2025, ISBN 978-3-89318-099-8 on OpenProceedings.org. Distribution of this paper is permitted under the terms of the Creative Commons license CC-by-nc-nd 4.0. Figure 1: Data generation for CI index prediction addressing multiple user-defined ML performance criteria, in order to improve an input ML model. chlorophyll-a index (CI-index), key measure of algal blooms. The team has gathered over 50 factors (e.g., fertilizer, water quality, weather) of upstream rivers and watershed systems, and trained random forest (RF) with small, regional dataset. The team wishes to find new data with important spatiotemporal and chemical attributes, to generalize the RF model. In particular, the model is expected to perform well over such dataset in terms of three performance measures: root mean square error (ùëÖùëÄùëÜùê∏), ùëÖ2 test, for Level 2 bloom CI-index, and training time cost. Desirably, the data generation process can inform what are crucial features to inspect, track where the feature values are from, and how they are integrated from the data sources. The research team may issue skyline query [4] that requests: Generate dataset for which our random forest model for predicting Level 2 bloom CI-index is expected to have RMSE below 0.3, ùëÖ2 score at least 0.7, and incur training cost in 5 minutes? Here, the thresholds 0.3, 0.7, and 5 minutes are set based on historical performance of the RF model over data sample. One may apply data integration, or perform feature engineering to refine existing datasets with important features. Nevertheless, these methods often fall short at consistently generate data towards optimizing user-defined ML performance, leaving alone the needs for addressing multiple measures e.g., accuracy and training cost. Another approach is to introduce utility function as linear weighted sum of multiple measures. This turns the need into single objective. However, achieving both high accuracy and low training cost can be conflicting; moreover, best dataset that optimizes such utility function may not necessarily satisfy the expected bounds as posed for each measure in the query. Ideally, data generation process should provide dataset that ensures the model achieves best expected performance on at least one measure, with compromisingly good performance on the rest, and all satisfying the user-defined bounds if any. The above example calls for data generation approaches that can respond to the question by providing skyline datasets that can address multiple ML performance measures. More formally, given query that specifies an input data science model ùëÄ, set of source tables = {ùê∑1, . . . ùê∑ùëõ }, and set of user-defined performance measures (e.g., accuracy, training time), our task is to generate new table from D, over which the expected performances of ùëÄ simultaneously reaches desirable goals for all measures in P. As remarked earlier, traditional data integration and feature engineering with pre-defined, single optimization objective falls short of generating data for such needs. Moreover, desirable data generation process should (1) declaratively produce such data by simple, primitive operators that are well supported by established query engines and data systems, (2) perform data discovery without expensive model inference and validation; and (3) ensure quality guarantees on the resulting skyline dataset, for multiple performance measures. In addition, the generation should be efficient. This remains challenging issue, considering large-scale data sources and the space of new datasets that can be generated from them. Contribution. We introduce MODis, multi-objective data discovery framework. MODis interacts data integration and ML model performance estimation to pursue multi-objective data discovery paradigm. We summarize our contributions as follows. (1) We provide formal computation model for the skyline data generation process in terms of finite state transducer (FST). An FST extends finite automata by associating an output artifact that undergoes modifications via sequences of state transitions. The formal model is equipped with (1) simple and primitive operators, and (2) model performance oracle (Section 3). We use FST as an abstract tool to describe data generation algorithms and perform formal analysis to verify costs and provable quality guarantees. (2) Based on the formal model, we introduce the skyline data generation problem, in terms of Pareto optimality (Section 4) . The goal is to generate skyline set of datasets, ensuring each has at least one performance measure where the models expected performance is no worse than any other dataset. While the problem is intractable, we present fixed-parameter tractable result, for polynomially bounded dataset exploration space from the running graph of an FST process, and fixed measures set P. Based on the above formulation, we provide three feasible algorithms to generate skyline datasets. (3) Our first algorithm provides an approximation on Pareto optimal datasets by exploring and verifying bounded number of datasets that can be generated from data sources (Section 5.1). The algorithm adopts reduce-from-universal strategy to dynamically drop values from universal dataset towards Pareto optimal set of tables. We show that this algorithm approximates Skyline set within factor of (1 + ùúñ) for all performance metric, and ensures exact dominance for at least one measure. In addition, we present special case with fully polynomial time approximation. (4) Our second algorithm further reduces unnecessary computation. It follows bi-directional scheme to prune unpromising data, and leverages correlation analysis of the performance metrics to early terminate the search (Section 5.3). (5) Moreover, we introduce diversification algorithm to mitigate the impact of data bias (Section 5.4). We show that the algorithm achieves 1 4 -approximation to an optimal diversified skyline dataset among all verified (1 + ùúñ) counterparts. Using real benchmark datasets and tasks, we experimentally verify the effectiveness of our data discovery scheme. We found that MODis is practical in use. For example, our algorithms take 30 seconds to generate new data, that can improve input models by 1.5-2 times in accuracy and simultaneously reduces their training cost by 1.7 times. It outperforms baseline approaches that separately performs data integration or feature selection; and remains feasible for larger datasets. Our case study also verified its practical application in domain science tasks. Related works. We categorize related works as follows. Feature Selection. Feature selection removes irrelevant and redundant attributes and identifies important ones for model training [27]. Filtering methods rank features in terms of correlation or mutual information [32, 35] and choose the top ones. They typically assume linear correlation among features, omitting collective effects from feature sets and hence are often limited to support directly optimizing model performance. Our method differs from feature selection in the following. (1) It generates skyline dataset with primitive data augmentation and reduction operators, beyond simply dropping the entire columns. (2) We generate data that improves the model over multiple ML performance measures, beyond retaining critical features; and (3) our method does not require internal knowledge of the models or incur learning overhead. Data Augmentation. Data augmentation aims to create data from multiple data sources towards unified view [7, 11, 36, 48]. It is often specified to improve data completeness and richness [36] and may be sensitive to the quality of schema. Our method aimes to generate data to improve the expected performance of datadriven models. This is different from the conventional data integration which mostly focuses on improving the data completeness. Generative data augmentation [6] synthesize new rows for multi-objective optimization with predefined schema. In contrast, MODis generates data with both rows and columns manipulation. Also, HydraGAN requires target column for each metric, while MODis supports user-defined metrics with configurable generation. Data Discovery. Data discovery aims to prepare datasets for ML models [11, 14, 18, 25, 36]. For example, Kitana [18] computes data profiles (e.g., MinHash) and factorized sketches for each dataset to build join plan, and then evaluates the plan using proxy model. METAM [14] involves the downstream task with utility score for joinable tables. Comparing with prior work, we formalize data generation with cell-level operators, beyond joins. We target multi-objective datasets and provide formalization in terms of Pareto optimality. We also provide algorithms with quality guarantees and optimization techniques. Model Estimation. Model estimation aims to provide accurate estimation of models performance without incurring expensive re-training and inference cost. For example, AutoML [20, 31, 45] train -surrogate models to estimate model performance [20, 31, 45], or predict the model performance by learning from past attempts [13] or Reinforcement Learning [10]. Model selection [42] leverages metadata and historical observations to build graph neural network-based estimator for estimating model performance. Our work leverage Multi-output Gradient Boosting as the surrogate model for fast and reliable estimation, and benefits from established ML performance estimation approaches or other surrogate models."
        },
        {
            "title": "EVALUATION",
            "content": "We start with several notations used in MODis framework. Datasets. dataset ùê∑ (ùê¥1, . . . ùê¥ùëö) is structured table instance that conforms to local schema ùëÖùê∑ (ùê¥1, . . . ùê¥ùëö). Each tuple ùë° ùê∑ is m-ary vector, where ùë° .ùê¥ùëñ = ùëé (ùëñ [1, ùëö]) means the ùëñth attribute ùê¥ùëñ of ùë° is assigned value ùëé. dataset may have missing values at some attribute ùê¥ (i.e., ùë° .ùê¥ = ). Given set of datasets = {ùê∑1, . . . ùê∑ùëõ }, each dataset ùê∑ùëñ confirms to local schema ùëÖùëñ . The universal schema ùëÖùëà is the union of the local schemas of datasets in D, i.e., set of all the attributes involved in D. The active domain of an attribute ùê¥ from ùê∑ùëà , denoted as adom(ùê¥), refers to the finite set of its distinct values occurring in D. The size of adom(ùê¥), denoted as adom(ùê¥), is the number of distinct values of ùê¥ in D. Models. data science model (or simply model) is function in the form of ùëÄ : ùê∑ Rùëë , which takes as input dataset ùê∑, and outputs result embedding in Rùëë for some ùëë N. Here and are real and integer sets. In practice, ùëÄ can be pretrained machine learning model, statistical model, or simulator. The input ùê∑ may represent feature matrix (a set of numerical feature vectors), or tensor (from real-world physical systems), to be used for data science model ùëÄ as training or testing data. The output embedding can be conveniently converted to taskdependent output (e.g., labels for classification, discrete cluster numbers for clustering, or Boolean values for outlier detection) with post-processing. Fixed Deterministic models. We say model ùëÄ is fixed, if its computation process does not change for fixed input. For example, regression model ùëÄ is fixed if any factors that determines its inference (e.g., number of layers, learned model weights) remain fixed. The model ùëÄ is deterministic if it always outputs the same result for the same input. We consider fixed, deterministic models for the needs of consistent performance, which is desired property in ML-driven data analytical tasks. Model Evaluation. performance measure ùëù (or simply measure) is performance indicator of model ùëÄ, such as accuracy e.g., precision, recall, F1 score (for classification); or mean average error (for regression analysis). It may also be cost measure such as training time, inference time, or memory consumption. We use the following settings. (1) We unify as set of normalized measures to minimize, with range (0, 1]. Measures to be maximized (e.g., accuracy) can be easily converted to an inversed counterpart (e.g., relative error). (2) Each measure ùëù has an optional range [ùëùùëô , ùëùùë¢ ] (0, 1]. It specifies desired lower bound ùëùùëô or an upper bound ùëùùë¢ for model performance, such as maximum training or inference time, memory capacity, or error ranges. Remarks. As we unify as set of measures to be minimized, it is intuitive that an upper bound ùëùùë¢ specifies tollerence for the estimated performance. We necessarily introduce lower bound ùëùùëô > 0 for the convinience of (1) ensuring well-defined theoretical quality guarantees (as will be discussed in Section 5.1), and (2) leaving the option open to users for the configuration needs of downstream tasks such as testing, comparison or benchmarking. Estimators. performance measure ùëù can often be efficiently estimated by an estimation model (or simply estimator), in PTIME in terms of ùê∑ (the number of tuples in ùê∑). An estimator makes use of set of historically observed performance of ùëÄ Symbol D, ùê∑, ùê∑ùëà ùëÖùê∑ , ùëÖùëà A, ùê¥, adom(ùê¥) ùëÄ P, ùëù, (ùëùùëô , ùëùùë¢ ) ùëá , ùë° = (ùëÄ, ùê∑, ), ùë° . = (ùë†ùëÄ , S, O, Sùêπ , ùõø ) ùê∂ = (ùë†ùëÄ , O, ùëÄ,ùëá , ) ùê∫T = ( V, ùõø ) ùë† ùë†, ùê∑ ùê∑ Notation set of datasets, single dataset, universal table local schema of ùê∑, and universal schema attribute set, attribute, and active domain data science model ùê∑ Rùëë perform. measures, measure, its range test set; single test, its performance vector data discovery system performance estimation model configuration of data discovery system running graph state dominance, dataset dominance Table 1: Table of notations (denoted as ùëá ) to infer its performance over new dataset. It can be regression model that learns from historical tuning records ùëá to predict the performance of ùëÄ given new dataset ùê∑. By default, we use multi-output Gradient Boosting Model [34] that allows us to obtain the performance vector by single call with high accuracy (see Section 6). Tests. Given model ùëÄ and dataset ùê∑, test ùë° is triple (ùëÄ, ùê∑, P), which specifies test dataset ùê∑, an input model ùëÄ, and set of user-defined measures = {ùëù1, . . . ùëùùëô }. test tuple ùë° = (ùëÄ, ùê∑, P) is valuated by an estimator if each of its measure ùëù is assigned (estimated) value by E. Example 2: Consider Example 1. pre-trained random forest (RF) model ùëÄ that predicts CI-index is evaluated by three measures = {RMSE, R2, Ttrain}, which specifies the root mean square error, the ùëÖ2 score, and the training cost. user specifies desired normalized range of RMSE to be within (0, 0.6], R2 in [0, 0.35] w.r.t. inversed lower bound 1 0.65, and Ttrain in (0, 0.5] w.r.t. an upper bound of 3600 seconds (i.e., no more than 1800 seconds). We summarize the main notations in Table 1."
        },
        {
            "title": "FORMALIZATION",
            "content": "Given datasets D, an input model ùëÄ and set of measures P, we formalize the generation process of skyline dataset with multi-goals finite state transducer (FST). An FST extends extends finite automata by associating outputs with transitions. We use FST to abstract and characterize the generation of Skyline datasets as data transformation process. We introduce this formalization, with counterpart for data integration [7, 26], to help us characterize the computation of skyline dataset generation. Data Generator. skyline dataset generator is finite-state transducer, denoted as = (ùë†ùëÄ, S, O, Sùêπ , ùõø), where (1) is set of states, (2) ùë†ùëÄ is designated start state, (3) is set of operators of types {, }; (4) Sùêπ is set of output states; and (5) ùõø refers to set of transitions. We next specify its components. States. state ùë† specifies table ùê∑ùë† that conforms to schema ùëÖùë† and active domains adomùë† . For each attribute ùê¥ ùëÖùë† , adomùë† (ùê¥) adom(ùê¥) refers to fraction of values ùê¥ can take at state ùë†. adomùë† (ùê¥) can be set as empty set , which indicates that the attribute ùê¥ is not involved for training or testing ùëÄ; or wildcard _ (dont care), which indicates that ùê¥ can take any value in adom(ùê¥). Operators. skyline data generator adopts two primitive polynomial-time computable operators, Augment and Reduct. ùê∑ùë† , for simplicity, we shall use single general term output, denoted as Dùêπ , to refer to output states or datasets. Running graph. running of can be naturally represented as the dynamic generation of running graph ùê∫ = (V, ùõø), which is directed acyclic graph (DAG) with set of state nodes V, and set of transition edges ùëü = (ùë†, ùëúùëù, ùë†). path of length ùëò is sequence of ùëò transitions ùúå = {ùëü1, . . . ùëüùëò } such that for any ùëüùëñ = (ùë†ùëñ, ùëúùëù, ùë†ùëñ+1), ùëüùëñ+1 = (ùë†ùëñ+1, ùëúùëù, ùë†ùëñ+2); i.e., it depicts sequence of transitions that converts an initial state ùë†1 with dataset ùê∑1 to result ùë†ùëò with ùê∑ùëò . Example 3: Following Example 1, Fig 2 shows fraction of running graph with input set D= {ùê∑ùë§, ùê∑ùëè, ùê∑ùëÅ , ùê∑ùëÉ } (water, basin, nitrogen, and phosphorus tables, respectively). The augmentation uses spatial joins [38], common query that join tables with tuple-level spatial similarity. With configuration ùê∂ = (ùë†ùëÄ, RF, {ùëÖùëÄùëÜùê∏, ùëÖ2,ùëáùë°ùëüùëéùëñùëõ }, E}) (where is an MO-GBM estimator), running starts by joining ùê∑ùë§ and ùê∑ùëè to get ùê∑2. ùê∑2 is then augmented with the attribute Phosphorus under literal year = 2013, resulting in ùê∑3 via path {1, . . . , 3}. In each step, test ùë° is initialized; and the estimator is consulted to valuate the performance vector of ùë°, and enrich ùëá . Consider another path from ùê∑ùëÄ that results dataset ùê∑5 in Fig. 1. It first augmentes ùê∑ùëÄ to ùê∑4 with data in Spring. reduction with condition year<2003 selects and removes all the tuples in ùê∑4 with historical data before 2003, which leads to dataset ùê∑5 that retains only the data since 2003."
        },
        {
            "title": "4 SKYLINE DATA GENERATION PROBLEM\nGiven T and a configuration ùê∂, MODis aims find a running of\nT that ideally leads to a ‚Äúglobal‚Äù optimal dataset, where ùëÄ is\nexpected to deliver the highest performance over all metrics.\nNevertheless, a single optimal solution may not always exist.\nFirst, two measures in P may in nature conflict due to trade-\noffs (e.g., training cost versus accuracy, precision versus recall).\nMoreover, the ‚Äúno free lunch‚Äù theorem [39] indicates that there\nmay not exist a single test that demonstrate best performance\nover all measures. We thus pursue Pareto optimality for Dùêπ . We\nstart with a dominance relation below.\nDominance. Given a data discovery system T and performance\nmeasures P, a state ùë† = (ùê∑ùë†, ùëÖùë†, adomùë† ) is dominated by ùë†‚Ä≤ =\n(ùê∑ùë† ‚Ä≤, ùëÖùë† ‚Ä≤, adomùë† ‚Ä≤ ), denoted as ùë† ‚â∫ ùë†‚Ä≤, if there are valuated tests ùë°\n= (ùëÄ, ùê∑ùë† ) and ùë° ‚Ä≤ = (ùëÄ, ùê∑ùë† ‚Ä≤ ) in ùëá , such that",
            "content": "for each ùëù P, ùë° .ùëù ùë° .ùëù; and there exists measure ùëù P, such that ùë° .ùëù < ùë° .ùëù. dataset ùê∑ùë† is dominated by ùê∑ùë† , denoted as ùê∑ùë† ùê∑ùë† , if ùë† ùë†. Skyline set. Given and configuration ùê∂, let Dùêπ be the set of all the possible output datasets from running of , set of datasets ùêπ Dùêπ is skyline set w.r.t. and ùê∂, if for any dataset ùê∑ ùêπ , and any performance measure ùëù P, there exists test ùë° ùëá , such that ùë° .ùëù [ùëùùëô , ùëùùë¢ ]; there is no pair {ùê∑1, ùê∑2} ùêπ such that ùê∑1 ùê∑2 or ùê∑2 ùê∑1; and for any other ùê∑ Dùêπ ùêπ , and any ùê∑ ùêπ , ùê∑ ùê∑. We next formulate the skyline data generation problem. Skyline Data Generation. Given skyline data generator and its configuration ùê∂ = (ùë†ùëÄ, O, ùëÄ,ùëá , E), the skyline data generation problem is to compute skyline set Dùêπ in terms of and ùê∂. Figure 2: skyline data generation process, with part of running graphs, and result datasets. These operators can be expressed by SPJ (select, project, join) queries, or implemented as user-defined functions (UDFs). (1) Augment has general form of ùëê (ùê∑ùëÄ, ùê∑), which augments dataset ùê∑ùëÄ with another ùê∑ subject to literal ùëê. Here ùëê is literal in form of ùê¥ = ùëé (an equality condition). An augmentation ùëê (ùê∑ùëÄ, ùê∑) executes the following queries: (a) augment schema ùëÖùëÄ of ùê∑ùëÄ with attribute ùê¥ from schema ùëÖùê∑ of ùê∑, if ùê¥ ùëÖùëÄ ; (b) augment ùê∑ùëÄ with tuples from ùê∑ satisfying constraint ùëê; and (c) fill the rest cells with null for unknown values. (2) Reduct ùëê (ùê∑ùëÄ ): this operator(a) selects from ùê∑ùëÄ the tuples that satisfy the selection condition posed by the literal ùê∂ posed on attribute ùëÖùëÄ .ùê¥; and (b) removes all such tuples from ùê∑ùëÄ . Here ùëê is single literal defined on ùëÖùëÄ .ùê¥ as in (1). Transitions. transition ùëü = (ùë†, ùëúùëù, ùë†) is triple that specifies state ùë† = (ùê∑, ùëÖ, adomùë† ), an operator op over ùë†, and result state ùë† = (ùê∑, ùëÖ, adom ùë† ), where ùê∑ and ùëÖ are obtained by applying op over ùê∑ and ùëÖ conforming to domain constraint adom ùë† , respectively. In practice, the operators can be enriched by task-specific UDFs that perform additional data imputation, or pruning operations, to further improve the quality of datasets. Running. configuration of , denoted as ùê∂ = (ùë†ùëÄ, O, ùëÄ,ùëá , E), initializes start state ùë†ùëÄ with dataset ùê∑ùëÄ , finite set of operators O, fixed deterministic model ùëÄ, an estimator E, and test set ùëá , where each test ùë° ùëá has valuated performance vector (ùë°). Both ùê∑ùëÄ and ùëá can be empty set . running of w.r.t. configuration ùê∂ = (ùë†ùëÄ, ùëÄ,ùëá , E) follows general, deterministic process below. (1) Starting from ùë†ùëÄ , and at each state ùë†, iteratively applies operators from to update table with new attributes and tuples or mask its tuple values. This spawns set of child states. (2) For each transition ùëü = (ùë†, op, ùë†) spawned from ùë† with result ùë† by applying op, (a) initializes test tuple ùë° (ùëÄ, ùê∑ùë† , P) if ùë° and invokes estimator at runtime to valuate the performance vector of ùë°; or (b) if ùë° is already in ùëá , it directly loads ùë° .P. Consistently, we say state node ùë† is valuated, if corresponding test ùë° (ùëÄ, ùê∑ùë†, P) is valuated by E. We denote its evaluated performance vector as ùë†.P. The above process terminates at set of output states Sùêπ , under an external termination condition, or no transition can be spawned (no new datasets can be generated with O). The result of running refers to the set of corresponding datasets Dùêπ induced from the output states Sùêπ . As each output state ùë† uniquely determines corresponding output dataset Example 4: Revisiting prior example and consider the temporal results Dùêπ = {ùê∑1, . . . , ùê∑5} with the following performance vectors valuated by the estimator so far: RMSE T: (D, M, P, E) ùë°1 : (ùê∑1, RF, P, MO GBM) 0.48 0.41 ùë°2 : (ùê∑2, RF, P, MO GBM) 0.26 ùë°3 : (ùê∑3, RF, P, MO GBM) 0.37 ùë°4 : (ùê∑4, RF, P, MO GBM) 0.25 ùë°5 : (ùê∑5, RF, P, MO GBM) ùëáùë°ùëüùëéùëñùëõ 0.37 0.37 0.37 0.39 0.35 ÀÜùëÖ2 0.33 0.24 0.15 0.22 0.18 Here ÀÜùëÖ2 is inversed as 1-ùëÖ2: the smaller, the better. All the measures are normalized in (0, 1] w.r.t. user-specified upper and lower bounds, and the optimal values are underlined. One can verify the following dominance relation among the datasets: (1) ùê∑1 ùê∑2 ùê∑3, and ùê∑4 ùê∑5; (2) ùê∑3 ùê∑5 and vice versa. Hence Skyline set Dùêπ currently contains {ùê∑3, ùê∑5}. We present the following hardness result. Theorem 1. Skyline data generation is (1) NP-hard; and (2) fixed-parameter tractable, if (a) is fixed, and (b) Dùêπ is polynomially bounded by the input size D. Proof sketch: The NP-hardness can be verified by reduction from the Multiobjective Shortest Path problem (MOSP). Given an edge-weighted graph ùê∫ùë§, where each edge ùëíùë§ has ùëë-dimensional cost vector ùëíùë§ .ùëê, the cost of path ùúåùë§ in ùê∫ùë§ is defined as ùúåùë§ .ùëê = (cid:205)ùëíùë§ ùúåùë§ ùëíùë§ .ùëê. The dominance relation between paths is determined by comparing their costs. MOSP is to compute Skyline set of paths from start node ùë† to target node ùë°. Given an instance of MOSP, we construct an instance of our problem as follows. (1) We assign an arbitrally ordered index to the edges of ùê∫ùë§, say ùëí1, . . . ùëíùëõ. (2) We initialize configuration as follows. (a) ùë†ùëÄ has single dataset ùê∑0, where for each edge ùëíùëñ = (ùë£, ùë£ ), there is distinct tuple ùë°ùëñ ùê∑0. (b) contains set of reduction operators, where each operator ùëúùëñ removes tuple ùë°ùëñ from ùê∑0, and incurs pre-defined performance measure ùëíùëñ .ùëê. (c) ùëÄ maps each tuple ùë°ùëñ in ùê∑0 to fixed embedding in Rùëë . (d) The test set ùëá is . We enforce the running graph of to be the input ùê∫ùë§, by setting the initial state as ùë† with associated dataset ùê∑0, unique termination state as the node ùë°, and the applicable transitions as the edges in ùê∫ùë§. One can verify that solution of MOSP is Pareto set of paths from ùë† to ùë°, each results in dataset by sequentially applying the reduction operators following the edges of the path. This yields set of datasets that constitutes corresponding skyline set Dùêπ as solution for our problem. As MOSP is shown to be NP-hard [16, 37], the hardness of skyline data generation follows. To see the fixed-parameter tractability, we outline an exact algorithm. (1) The algorithm exhausts the runnings of skyline generator , and invokes PTIME inference process of the model ùëÄ and valuate at most ùëÅ Dùêπ possible states (datasets). (2) It invokes multi-objective optimizer such as Kungs algorithm [24]. This incurs ùëÇ (ùëÅ log ùëÅ ) 2 valuations when 4, or ùëÇ (ùëÅ (log ùëÅ )) if < 4. As ùëÅ Dùêπ , and Dùêπ is in ùëÇ (ùê∑ ), and ùëÉ is fixed constant, the overall cost is in PTIME (see [43] for details). While the above exact algorithm is able to compute skyline dataset, it remains infeasible even when enlisting ùëÅ states as once-for-all cost is affordable. Moreover, solution may contain an excessive number of datasets to be inspected. We next present three feasible algorithms, that generate datasets that approximate skyline sets with bounded size and quality guarantees."
        },
        {
            "title": "5.1 Approximating Skyline Sets\nWe next present our first algorithm that generates a size-bounded\nset, which approximates a Skyline set in Dùêπ . To characterize the\napproximation quality, we introduce a notion of ùúñ-skyline set.\nùúñ-Skyline set. Given a data discovery system T with a configu-\nration ùê∂, Let DS be a set of ùëÅ valuated datasets in the running\nof T . Given a pair of datasets (ùê∑, ùê∑‚Ä≤) from DS, and a constant\nùúñ > 0, we say ùê∑‚Ä≤ ùúñ-dominates ùê∑, denoted as ùê∑‚Ä≤ ‚™∞ùúñ ùê∑, if for the\ncorresponding tests ùë° = (ùëÄ, ùê∑) and ùë° ‚Ä≤ = (ùëÄ, ùê∑‚Ä≤),\n‚Ä¢ ùë° ‚Ä≤.ùëù ‚â§ (1 + ùúñ)ùë° .ùëù for each ùëù ‚àà P, and\n‚Ä¢ there exists a measure ùëù‚àó ‚àà P, such that ùë° ‚Ä≤.ùëù‚àó ‚â§ ùë° .ùëù‚àó.\nIn particular, we call ùëù‚àó a decisive measure. Note that ùëù‚àó can",
            "content": "be any ùëù and may not be fixed. set of datasets Dùúñ DS is an ùúñ-Skyline set of DS, if for any dataset ùê∑ Dùúñ , and any performance measure ùëù P, there exists corresponding test ùë° ùëá , such that ùë° .ùëù [ùëùùëô , ùëùùë¢ ]; and for every dataset ùê∑ DS, there exists dataset ùê∑ Dùúñ such that ùê∑ ùúñ ùê∑. (ùëÅ , ùúñ)-approximation. We say an algorithm is an (ùëÅ , ùúñ)- approximation for MODis, if it satisfies the following: it explores and valuates at most ùëÅ states; for any constant ùúñ > 0, the system correctly outputs an ùúñ-Skyline set, as an approximation of Skyline set defined over ùëÅ valuated states; and the time cost is polynomial determined by , ùëÅ , and 1 ùúñ . Below we present our main result. (cid:18) min(ùëÅ ùëÖùë¢ Theorem 1. Given datasets D, configuration ùê∂, and number ùëÅ , there exists an (ùëÅ , ùúñ)-approximation for MODis in (cid:17) 1 time, where ùëÖùë¢ is the ùëÇ size of the universal schema, ùëÅùë¢ = ùëÖùë¢ + adomùëö (adomùëö the largest active domain), ùëùùëö = max ùëùùë¢ ùëùùëô as the measure ùëù ranges over P; and ùêº is the unit valuation cost per test. (cid:18)(cid:16) log(ùëùùëö ) ùúñ , ùëÅ ) + ùêº (cid:19) (cid:19) ùë¢ Remarks. The above result captures relative guarantee w.r.t. ùúñ and ùëÅ . When ùëÅ = Dùêπ , an (ùëÅ , ùúñ)-approximation ensures to output ùúñ-Skyline set. The paradigm is feasible as one can explicitly trade the closeness of the output to Skyline set with affordable time cost, by explicitly tuning ùúñ and ùëÅ . Moreover, the worst-case factor adomùëö can also be tightened by bound determined by the value constraints posed by the literals. For example, an attribute ùê¥ may contribute up to two necessary values in the search if the literals involving ùê¥ only enforce two equality conditions A=2 and A=5, regardless of how large adom(ùê¥) is (see Sections 3 and 6)."
        },
        {
            "title": "5.2 Approximation Algorithm\nAs a constructive proof of Theorem 1, we next present an (ùëÅ , ùúñ)-\napproximation algorithm, denoted as ApxMODis.\n‚ÄúReduce-from-Universal‚Äù. Algorithm ApxMODis simulates the\nrunning of T from a start state ùë†ùëà . The start state is initialized\nwith a ‚Äúuniversal‚Äù dataset ùê∑ùëà , which carries the universal schema\nùëÖùëà , and is populated by joining all the tables (with outer join to\npreserve all the values besides common attributes, by default).\nThis is to enforce the search from a set of rows that preserve all\nthe attribute values as much as possible to maximize the chance of",
            "content": "Algorithm 1 :ApxMODis 1: Input: Configuration ùê∂ = (ùë†ùëà , O, ùëÄ,ùëá , E), number ùëÅ , 2: constant ùúñ > 0, decisive measure ùëùùëë ; user-specified upper bound ùëùùë¢ for ùëù P; 3: Output: ùúñ-Skyline set Dùêπ . 4: Queue ùëÑ := , integer ùëñ := 0, ùëÑ.enqueue((ùë†ùëà , 0)); 5: while ùëÑ and number of valuated states < ùëÅ do 6: ùëñ+1 := Dùêπ (ùë†, i) := ùëÑ.dequeue(); Dùêπ for each (ùë†, ùê∑ùë† ) OpGen (ùë†) do ùëñ ; 7: 8: 9: ùëñ+1 := UPareto (ùë†, Dùêπ ùëÑ.enqueue((ùë†, + 1)); Dùêπ 10: return Dùêπ 11: procedure OpGen(s) set ùëÑ := ; 12: for each entry ùëô ùë†.ùêø do 13: ùëñ+1, ùúñ); if ùëô = 1 then ùëô := 0; create new state ùë†; ùë†.ùêø := ùë†.ùêø; generate dataset ùê∑ùë† accordingly; ùëÑ .append((ùë†, ùê∑ùë† )); return ùëÑ 20: procedure UPareto(ùë†, Dùêπ 21: ùëñ+1, ùúñ) update ùë†.P with estimator E; for each ùëù do if ùë†.P (ùëù) > ùëùùë¢ then return Dùêπ ùëñ+1; update pos(ùë†) with Equation (1); retrieve state ùë† where pos(ùë†) = pos(ùë†); if no such ùë† exists then ùëñ+1 {ùê∑ ùëñ+1 := Dùêπ Dùêπ ùë† }; else if ùë†.P (ùëùùëë ) < ùë†.P (ùëùùëë ) then ùëñ+1 {ùê∑ùë† } {ùê∑ùë† }; Dùêπ return Dùêπ ùëñ+1 := Dùêπ ùëñ+1 14: 15: 16: 17: 18: 19: 22: 23: 24: 25: 26: 27: 28: 29: 30: Figure 3: ApxMODis: Approximating Skyline sets sequential applications of reduction only. It transforms state dominance into path dominance counterpart. For transition edge (ùë†, , ùë†), weight is assigned to quantify the gap between the estimated model performance over datasets ùê∑ùë† and its reduced counterpart ùê∑ ùë† . The length of path from ùë†ùëà to ùë† aggregates the edge weights towards the estimated performance of its result. Advantage. We justify the reduce-from-universal strategy in the following context. (1) As the measures are to be minimized, we can extend shortest paths by prioritizing the valuation of datasets towards user-defined upper bounds with early pruning, to avoid unnecessary reduction. (2) Starting from universal dataset allows early exploration of dense datasets, over which the model always tends to have higher accuracy in practice. We next present the details of our algorithm. Auxiliary structure. ApxMODis follows dynamic levelwise state generation and valuation process, which yields running graph ùê∫ with up to ùëÅ nodes. It maintains the following. (1) ùëÑ is queue that maintains the dynamically generated and valuated state nodes. Each entry of ùëÑ is pair (ùë†, ùëñ) that records state and the level it resides. (2) Dùêπ is list of datasets. Dùëñ ùêπ specifies the datasets processed at level ùëñ. Each state node ùë† is associate with bitmap ùêø to encode if Figure 4: Reduct-from-Universal: an illustration of twolevel computation. It performs multiple level-wise spawns and updates the ùúñ-Skyline set. its schema ùë†.ùëÖùë† contains an attribute ùê¥ in ùê∑ùëà , and if ùê∑ùë† contains value from its active domain adom(ùê¥). The map is consulted to assert the applicability of reduct operators at runtime. (3) Each state ùë† is associated with position ùëùùëúùë† (ùë†) in discretized 1-ary space, which is defined as pos(ùë† ) = (cid:34) (cid:22) log1+ùúñ (cid:23) ùë†. (ùëù1 ) ùëùùëô1 (cid:36) , . . . , log1+ùúñ (cid:37) (cid:35) ùë†. (ùëùP 1 ) ùëùùëôP 1 (1) By default, we set the last measure in as decisive measure. We remark that one can choose any measure as decisive measure, and our results carry over. Algorithm. The algorithm ApxMODis is illustrated in Fig. 3. It initializes queue ùëÑ with start state ùë†ùëà , and set position to ùë†ùëà (line 4). In lines 5 to 9, it update the Skyline set Dùêπ for each level iteratively. At level ùëë, for each state ùë† ùëÑ, procedure OpGen (line 12 to 19) explores all one-flip transitions in ùë†.ùêø and generates set with applicable reduct operators. ApxMODis enqueue new ùëë+1 at next level accordingly states and update the Skyline set Dùêπ by invoking Procedure UPareto. This process terminations until ùëÅ states are valuated, or no new state can be generated. Procedure UPareto. Given new state ùë†, procedure UPareto determines if ùë† should be included in the current Skyline set. (1) It updates ùë†.P by consulting the estimator (line 1), and decide an early skipping if its performance fails to satisfy the upperbound ùëùùë¢ for some measure ùëù P. (2) Otherwise, UPareto updates the position of state ùë†, and decides if ùë† replaces valuated state ùë† at the same position due to (1 + ùúñ)-dominance (lines 6-9). The Skyline set at current level is updated accordingly with dataset ùê∑ùë† . Example 5: Fig. 4 illustrates data discovery of ApxMODis with ùëÅ =5 and ùúñ=0.3, over table set = {ùê∑1, . . . , ùê∑3} and measures = < ùëù1, ùëù2 >. The operator set ùëÇ contains four reduct operators {1, . . . 4}. (1) It first constructs universal dataset ùê∑ùëà with universal schema ùëÖùëà . ùê∑ùëà can be obtained by optimized multi-way join [46], augmentation [28], or UDFs [8]. The bitmap ùê∑ùëà .ùêø is initialized accordingly. Procedure OpGen then generates applicable reductions by flipping the entries in ùê∑ùëà .ùêø. This spawns states ùë†1 and ùë†2 obtained by applying reduct 1 and 2, respectively It then consults the estimator to valuate model performances, and identified that ùê∑1 0.3 ùê∑2 and vice versa. Thus ApxMODis sets the current 0.3-Skyline set as {ùê∑1, ùê∑2}. ApxMODis next spawns states with applicable reductions 3 and 4, extending ùúå1 that leads to ùë†1, and ùúå2 that leads to ùë†2, This generates new extended paths ùúå3 and ùúå4 with results ùê∑3 and ùê∑4, respectively. It verfies that D3 0.3 ùê∑1, but ùê∑2 0.3 ùê∑4; and ùê∑2 0.3 ùê∑3 and vice versa. This yields an updated 0.3-Skyline set {ùê∑2, ùê∑3}, after valuating 5 states. Correctness & Approximability. ApxMODis terminates as it spawns ùëÅ nodes with at most ùëÖùëà adomùëö distinct reduction, where adomùëö refers to the size of the largest active domain. For approximability, we present the result below. Lemma 2. For any constant ùúñ, ApxMODis correctly computes an ùúñ-Skyline set Dùêπ as an approximated Skyline set defined on the ùëÅ states it valuated. Proof sketch: We verify the ùúñ-approximability, with reduction to the multi-objective shortest path problem (MOSP) [40]. Given an edge-weighted graph ùê∫ùë§, where each edge carries ùëë-dimensional attribute vector ùëíùë§ .ùëê, it computes Skyline set of paths from start node ùë¢. The cost of path ùúåùë§ in ùê∫ùë§ is defined as ùúåùë§ .ùëê = (cid:205)ùëíùë§ ùúåùë§ ùëíùë§ .ùëê. The dominance relation between two paths is determined by the dominance relation of their cost. Our reduction (1) constructs ùê∫ùë§ as the running graph ùê∫ with ùëÅ valuated state nodes and spawned transition edges; and (2) for each edge (ùë†, ùë†), sets an edge weight as ùëíùë§ = ùë†.P ùë†.P. Given solution Œ†ùë§ of the above instance of MOSP, for each path ùúåùë§ Œ†, we set corresponding path ùúå in ùê∫ with result dataset ùê∑, and adds it into Dùêπ . We can verify that Œ†ùë§ is an ùúñ-Skyline set of paths Œ†ùë§, if and only if Dùêπ is an ùúñ-Skyline set of DùëÜ that contains the datasets from the set of ùëÅ valuated states in ùê∫ . We then show that ApxMODis is an optimized process of the algorithm in [40], which correctly computes Œ†ùë§ for ùê∫ùë§. Time cost. Let ùëÖùë¢ be the total number of attributes in the universal schema ùëÖùë¢ of ùê∑ùë¢ , and adomùëö be the size of the largest active domain. ApxMODis performs ùëÖùë¢ levels of spawning, and at each node, it spawns at most ùëÖùë¢ + adomùëö children, given that it flips one attribute for each reduction, and for each attribute, at most one domain value to mask. Let ùëÅùë¢ be ùëÖùë¢ +adomùëö . Thus, ApxMODis valuates at most min(ùëÅ ùëÖùë¢ , ùëÅ ) nodes (datasets), taking ùêº min(ùëÅ ùëÖùë¢ , ùëÅ ) time, where ùêº refers to polynomial time valuation cost of per test. For each node, (cid:16) (cid:106)log1+ùúñ it then takes at most (cid:206) 1 ) time to update the ùúñ-Skyline set. Given ùúñ is small, log(1 + ùúñ) ùúñ, and the total + 1(cid:17) ùëùùë¢ùëñ ùëùùëôùëñ ùëñ=1 ùë¢ ùë¢ (cid:107) cost is in ùëÇ min(ùëÅ ùëÖùë¢ ùë¢ , ùëÅ ) (cid:18) (cid:18)(cid:16) log(ùëùùëö ) ùúñ (cid:17) + ùêº (cid:19)(cid:19) time. Given ùúñ . Theorem 1 thus follows. ùëÖùë¢ and are small constants, the cost is polynomial in the input size ùê∑ùë¢ , ùëÅ and 1 An FPTAS case. We next present case when ApxMODis ensures stronger optimality guarantee. We say an (ùëÅ , ùúñ)- approximation is fully polynomial time approximation (FPTAS) for MODis, if (1) it computes an ùúñ-Skyline set for DùëÜ , where DùëÜ refers to all possible datasets that can be generated from ùê∑ùëà , and (2) it runs in time polynomial in the size of ùê∑ùëà and 1 ùúñ . Lemma 3. Given skyline dataset generator with configuration ùê∂, if DùëÜ has size that is polynomially bounded in ùëÇ (ùëì (ùê∑ùëà )), then ApxMODis is an FPTAS for MODis. Proof sketch: We show this by reduction from MODis to MOSP, similarly as in the approximability analysis. MOSP is known to have fully polynomial time approximable (FPTAS) in Algorithm 2 :BiMODis 1: Input: Configuration ùê∂ = (ùë†ùëà , O, ùëÄ,ùëá , E), constant ùúñ > 0; 2: Output: ùúñ-Skyline set Dùêπ . 3: set ùë†ùëè = BackSt(ùë†ùëà ); queue ùëÑ ùëì := {(ùë†ùëà , 0)}, ùëÑùëè := {(ùë†ùëè, 0)}; integer ùëñ := 0; 4: while ùëÑ ùëì , ùëÑùëè and ùëÑ ùëì ùëÑùëè = do 5: (ùë†, ùëñ) = ùëÑ ùëì .dequeue(); (ùë†, ùëñ) = ùëÑùëè .dequeue(); Dùêπ for all ùë† ùëì OpGen (ùë†) and ùë†ùëè OpGen (ùë†) do ùëñ+1 = Dùêπ Forward Serach Backward Serach ùëñ ; ùëñ+1 = UPareto (ùë† ùëì , Dùêπ Dùêπ ùëñ+1 = UPareto (ùë†ùëè , Dùêπ Dùêπ if canPrune(ùë† ùëì , ùë†ùëè ) then prune (C, ùë† ùëì , ùë†ùëè ); ùëñ+1, Dùêπ ùëñ+1, Dùêπ ùëñ , ùúñ); ùëñ , ùúñ); ùëÑ ùëì .enqueue((ùë† ùëì , ùëñ + 1)), ùëÑùëè .enqueue((ùë†ùëè , ùëñ + 1)); 6: 7: 8: 9: 10: 11: 12: 13: 14: return Dùêπ Figure 5: BiMODis: Bi-directional Search terms of ùúñ-dominance. We set ApxMODis to run as (DùëÜ , ùúñ)- approximation, which is simplified implementation of an FPTAS in [40] with multiple rounds of replacement strategy following path dominance. As DùëÜ is bounded by polynomial of the input size ùê∑ùëà , it approximates Skyline set for all in PTME. The size bound of DùëÜ is pragmatic and practical due to that the attributes often bear active domains that are much smaller than dataset size. Indeed, data science applications typically consider data with values under task-specific constraints. These suggest practical application of ApxMODis with affordable setting of ùëÅ and ùúñ. We present the detailed analysis in [43]."
        },
        {
            "title": "5.3 Bi-Directional Skyline Set Generation\nGiven our cost analysis, for skyline data generation with larger\n(more ‚Äútolerate‚Äù) ranges (ùëùùëô , ùëùùë¢ ) and larger |D |, ApxMODis may\nstill need to valuate a large number of datasets. To further reduce\nvaluation cost, we introduce BiMODis, its bi-directional variant.\nOur idea is to interact both augment and reduct operators, with a\n‚Äúforward‚Äù search from universal dataset, and a ‚Äúbackward‚Äù coun-\nterpart from a single dataset in D. We also introduce a pruning\nstrategy based on an early detection of dominance relation.\nAlgorithm. Algorithm BiMODis, as shown in Fig. 5, has the fol-\nlowing steps. (1) Initialization (lines 3). It first invokes a pro-\ncedure BackSt to initialize a back-end start state node ùë†ùëè . Two\nqueues ùëÑ ùëì and ùëÑùëè are initialized, seeded with start state ùë†ùëà for\nforward search, and a back state ùë†ùëè for backward search, respec-\ntively. They serve as the forward and backward frontiers, respec-\ntively. (2) Bi-directional Search (lines 4-13). BiMODis conducts an\nexploration from both directions, controlled by ùëÑ ùëì for forward\nsearch, and ùëÑùëè for backward search. Similar to ApxMODis, a Sky-\nline set Dùêπ is maintained in a levelwise manner. The difference\nis that it invokes a revised procedure OpGen (with original coun-\nterpart in ApxMODis in Fig. 3), which generates reduct operators\nfor the forward search, and augment operators for the backward\nsearch. The search process terminates when both ùëÑ ùëì and ùëÑùëè are\nempty, or when a path is formed, the result Dùêπ is returned.\nProcedure BackSt. This procedure initializes a backend dataset ùê∑ùëè\nfor augmentation. This procedure can be tailored to the specific\ntask. For example, for a classifier ùëÄ with input features and a",
            "content": "target attribute ùê¥ to be classified, we sample small (minimal) set of tuples in ùê∑ùëà to ùê∑ùëè that covers all values of the active domain adom of ùê¥, to ensure that no classes will be missed in dataset ùê∑ùëè . Other task-specific strategies can also be applied here. To reduce the valuation cost, BiMODis leverages correlation analysis over historical performance ùëá , to assert non-ùúñdominance early, without full valuation of their measures P. Correlation-Based Pruning. At runtime, BiMODis dynamically maintains correlation graph ùê∫ C, where each node represents measure in P, and there is an edge (ùëùùëñ, ùëù ùëó ) in ùê∫ if ùëùùëñ and ùëù ùëó are strongly correlated, with an associated weight corr(ùëùùëñ, ùëù ùëó ) [47]. Here we say two measures are strongly correlated, if their Spearman correlation coefficient corr(ùëùùëñ, ùëù ùëó ) ùúÉ , given their value distribution in the current set of tests ùëá , for user-defined threshold ùúÉ . ùê∫ is dynamically updated, as more valuated tests are added to ùëá . Parameterized Dominance. BiMODis also parameterize any unvaluated measures in the performance vector ùë†.P of state ùë† with potential range [ ÀÜùëùùëô , ÀÜùëùùë¢ ] [ùëùùëô , ùëùùë¢ ]. This range is derived from the valuated measures that are most strongly correlated, by consulting ùê∫ and test sets ùëá . The entire vector ùë†.P is incrementally updated, for each ùëù P, by setting (1) ùë†.P (ùëù) as ùë° .ùëù (valuated), if there is corresponding test ùë° = (ùëÄ, ùê∑ùë† ) ùëá with ùë° .ùëù valuated; or (2) ùë†.P (ùëù) as variable with an estimated range [ùë†. ÀÜùëùùëô , ùë†. ÀÜùëùùë¢ ], if no test over ùëù of ùê∑ùë† is valuated. state ùë† is parameterized ùúñ-dominated by another state ùë†, denoted as ùë† ùúñ ùë†, if for each ùëù P, ùë†.P (ùëù) (1 + ùúñ)ùë†.P (ùëù), if both are valuated; ùë†. ÀÜùëùùë¢ (1 + ùúñ)ùë†. ÀÜùëùùëô , if neither is valuated; or ùë†.P (ùëù) (1 + ùúñ)ùë†. ÀÜùëùùëô (resp. ùë†. ÀÜùëùùë¢ (1 + ùúñ)ùë†.P (ùëù)), if ùë†.P (ùëù) (resp. ùë†.P (ùëù)) is valuated but ùë†.P (ùëù) (resp. ùë†.P (ùëù)) is not. Based on the above construction, BiMODis monitors monotonicity condition as follows. Monotonicity Condition. Given the current test set ùëá , we say state ùë† (resp. ùë†) with performance measure ùëù at path ùúå has monotonicity property, if for any state ùë† reachable from ùë† (resp. can reach ùë†) via ùúå, ùë†. ÀÜùëùùë¢ < ùë† . ÀÜùëùùëô 1+ùúñ (resp. ùë†. ÀÜùëùùë¢ < ùë† . ÀÜùëùùëô 1+ùúñ ). Given two states ùë† and ùë†, where ùë† ùúñ ùë†, state ùë† on path ùúå from ùë† or to ùë† can be pruned by Correlation-based Pruning, if for every ùëù P, ùë† has ùëù at ùúå with monotonicity property w.r.t. ùë† (resp. ùë†). We present the following pruning rule. Lemma 4. Let ùë† ùëÑ ùëì and ùë† ùëÑùëè . If ùë† ùúñ ùë†, then for any state node ùë† on path from ùë† or to ùë† that can be pruned by CorrelationBased Pruning, ùê∑ùë† is not in any ùúñ-Skyline set of the datasets that can be generated from valuated states. Proof sketch: We verify the pruning rule with case study of ùë† and ùë†, subject to the monotonicity property. Case 1: Both ùë†.P (ùëù) and ùë†.P (ùëù) are valuated. If ùë† ùúñ ùë†, then by definition, ùë†.P (ùëù) (1 + ùúñ)ùë†.P (ùëù) for all ùëù P. This readily leads to ùúñdominance, i.e., ùë† ùúñ ùë†. As ùë† has every performance measures ùëù with monotonicity property w.r.t. ùë†, ùë† ùúñ ùë†. Hence ùë† can be safely pruned without valuation. Case 2: Neither ùë†.P (ùëù) nor ùë†.P (ùëù) is valuated. By definition, as ùë† ùúñ ùë†, then for every ùëù P, ùë†. ÀÜùëùùë¢ (1 + ùúñ)ùë†. ÀÜùëùùëô . Given that ùë† has every performance measures ùëù with monotonicity property w.r.t. ùë†, then by definition, for each ùëù P, we have ùë†.ùëù ùë†. ÀÜùëùùë¢ ùë† . ÀÜùëùùëô (1 + ùúñ)ùë†. ÀÜùëùùëô (1 + ùúñ)ùë†. ÀÜùëùùë¢ < (1 + ùúñ) 1+ùúñ ùë†.ùëù, for every ùëù P. By definition of state dominance, ùë† ùë†, for unevaluated ùë†. Following similar proof, one can infer that ùë† ùë† for state ùë† in the forward front of BiMODis. Hence ùë† can be safely pruned. Case 3: One of ùë†.P (ùëù) or ùë†.P (ùëù) is valuated. Given that ùë† ùúñ ùë†, we have (a) ùë†.P (ùëù) (1 + ùúñ)ùë†. ÀÜùëùùëô , if only ùë†.P (ùëù) is valuated; or (b) ùë†. ÀÜùëùùë¢ (1 + ùúñ)ùë†.P (ùëù), if only ùë†.P (ùëù) is valuated. Consider case 3(a). As ùë† can reach ùë† via path ùúå, and ùë† satisfiies the pruning condition, we can infer that ùë†.P (ùëù) (1 + ùúñ)ùë†. ÀÜùëùùëô ùë† . ÀÜùëùùëô (1 + ùúñ)ùë†. ÀÜùëùùë¢ < (1 + ùúñ) 1+ùúñ ùë†.ùëù, hence ùë† ùë†. Similarly for case 3(b), we can infer that ùë†. ÀÜùëùùë¢ (1 + ùúñ)ùë†.ùëù (1 + ùúñ)ùë†. ÀÜùëùùë¢ < ùë† . ÀÜùëùùëô 1+ùúñ ùë†.ùëù. hence ùë† ùë†. For both cases, ùë† can be (1 + ùúñ) pruned without evaluation. Lemma 4 hence follows. Procedures canPrune and prune (lines 11-12; omitted) asserts the Correlation-Based Pruning condition, and perform the maintenance of ùê∫ C, ùëá and other auxiliary structures, respectively. Note that the above rule is checkable in PTIME w.r.t. input size DùëÜ . When DùëÜ is large, one can generate path with its states unevaluated, and check at runtime if the condition holds between evaluated states in the forward and backward frontier. We present the detailed analysis in [43]. Example 6: We illustrate Correlation-Based Pruning in the figure below. From left to right, it depicts set of test records ùëÖ, the correlation graph ùê∫ C, and part of the running graph ùê∫ . ùê∫ is constructed from ùëá with measures as nodes and Spearman correlations as edge weights. For each ùë†ùëõ ùê∫ , the associated ùëù Pùë†ùëõ is obtained by test ùë°ùë†ùëõ = (ùëÄ, ùê∑ùë†ùëõ ). Label (1, 1, 1, 1) (1, 1, 1, 0) (1, 0, 0, 1) (0, 1, 0, 0) (0, 0, 0, 0) ùëù1 0.42 0.4 0.5 0.45 0. ùëù2 0.18 0.17 0.22 / 0.4 ùëù3 0.9 0.1 / / 0.3 ùë†ùëà ùë†1 ùë†2 ùë†3 ùë†ùëè At ùúÉ = 0.8, ùëù1 and ùëù2 are positively correlated with each other and negatively correlated with ùê∑ , so Pùëò = {ùëù1, ùëù2}. From ùë†ùëà and ùë†ùëè , the forward and backward frontiers derive states ùë†1 and ùë†3, respectively. To estimate ùë†3.P (ùëù2), note that ùë†3.P (ùëù1) = 0.45, which lies between ùë†ùëà .P (ùëù1) = 0.42 and ùë†2.P (ùëù1) = 0.5. Given the strong correlation between ùëù1 and ùëù2, we infer ùë†3.P (ùëù2) to be within the interval [0.18, 0.22], with ÀÜùëùùëô2 = ùë†ùëà .P (ùëù2) and ÀÜùëùùë¢2 = ùë†2.P (ùëù2). With ùúñ = 0.3, we find ùë†3 ùúñ ùë†1 because 0.45 (1 + 0.3) 0.4 and 0.22 (1 + 0.3) 0.17. For intermediate states ùë†4 (bitmap entry (1, 1, 0, 0)) and ùë†5 (bitmap entry (0, 1, 1, 0)), which are not recorded in ùëá and have ùê∑ùë†4 = ùê∑ùë†5 = 2, similar inference process shows they fall within the bounds set by [ùë†1.P, ùë†3.P]. As result, ùë†4 and ùë†5 can be pruned. Time Cost. BiMODis takes the same time complexity as ApxMODis. The (ùëÅ , ùúñ)-approximation holds for BiMODis given that it correctly updates the ùúñ-Skyline set by definition. Our experimental study verifies that it is much faster in practice and particularly suitable for larger ùúñ or search spaces (represented by maximum path length). It also scales more efficiently for large datasets (see Exp-3 in Section 6)."
        },
        {
            "title": "5.4 Diversified Skyline Dataset Generation\nA Skyline dataset may still contain data that largely overlap or\nare similar, hence leading to bias and reducing the generality of\nthe model if adopted. This may occur due to skewed value distri-\nbution in the active domains, common attributes, over specific\nperformance metrics in the skyline data generation process. It is",
            "content": "Algorithm 3 :Diversification step at level ùëñ 1: Input: ùúñ-Skyline set Dùêπ 2: Output: diversified ùëò-subset of Dùêπ ùëñ (from UPareto), integer ùëò; ùëñ (to be passed to level ùëñ + 1). ùëñ ùëò then return Dùêπ ùëñ ùêπ with ùëò random dataset in Dùêπ ùëñ ; 3: if Dùêπ 4: initialize DùëÉ 5: score := ùëëùëñùë£ (DùëÉ 6: for all ùê∑ DùëÉ 7: ùëñ do ùêπ ); ùêπ do for all ùê∑ Dùêπ if ùê∑ DùëÉ DùëÉ ùêπ := (DùëÉ score := ùëëùëñùë£ (DùëÉ ùêπ ) if score > score then ùêπ := DùëÉ DùëÉ ùêπ then Continue; ùêπ {ùê∑ }) {ùê∑}; ùêπ , score := score; 8: 9: 10: 11: 12: 13: return DùëÉ ùêπ Figure 6: Level-wise diversification of DivMODis often desirable to explore diversified variant of skyline set generation to create varied datasets that mitigate such bias [23, 30]. Given and configuration ùê∂, set of datasets D, constants ùëÅ , ùúñ and ùëò, the diversified skyline data generation is to compute set ùêπ is an ùúñ-Skyline set of ùëÅ valuated states by an (ùëÅ , ùúñ)-approximation of MODis, and (2) among all ùúñ-Skyline sets over ùëÅ states valuated in (1), it maximizes diversification score defined as: ùêπ of at most ùëò tables, such that (1) div(Dùêπ ) = ùëò 1 ùëò ùëñ=1 ùëó=ùëñ+1 dis(ùê∑ùëñ, ùê∑ ùëó ) (2) where distance function dis quantifies the difference of datasets in terms of both value distributions and estimated performance, and is defined as: dis(ùê∑ùëñ, ùê∑ ùëó ) = ùõº 1 cos(ùë†ùëñ .ùêø, ùë† ùëó .ùêø) 2 + (1 ùõº) euc(ùë°ùëñ .P, ùë° ùëó .P) eucm We adopt Cosine similarity cos and Euclid Distance (euc). The latter is normalized by the maximum Euclid Distance ùëíùë¢ùëêùëö among the historical performances in ùëá . We next outline an algorithm, denoted as DivMODis, that extends an (ùëÅ , ùúñ)-approximation to computes an diversified ùúñ-Skyline set Dùêπ of at most ùëò datasets. Algorithm. DivMODis revises MODis by incrementally diversify ùëñ at level ùëñ (partially shown in Fig.6). It an input ùúñ-Skyline set Dùêπ derives DùëÉ ùêπ by greedy selection and replace strategy as follows. (1) It initializes DùëÉ ùëñ , and updates ùêπ as random ùëò-set from Dùêπ DùëÉ ùêπ by incrementally replacing tables with the highest marginal gain in diversification, hence an improved ùëëùëñùë£ (DùëÉ ùêπ is passed to be processed at level ùëñ + 1, upon the arrival of new states. DivMODis returns the diversified set Dùêπ , following the same termination condition as in ApxMODis. ùêπ ). (2) DùëÉ We show that the diversified MODis can be approximated, for submodular diversification function div. Our result holds for the specification of div in Equation 2. Lemma 5. Given ùëÅ and ùúñ, DivMODis achieves 1 4 approximation for diversified MODis, i.e., (1) it correctly computes ùúñ-Skyline set ùê∑ùëÉ ùêπ over ùëÅ valuated datasets, and (2) div(ùê∑ùëÉ ùêπ ) 4 div(D ). Dataset Sets Kaggle OpenData HF # tables 1943 2457 255 # Columns 33573 71416 # Rows 7317K 33296K 10207K Table 2: Characteristics of Datasets preserving reduction to the stream submodular maximization problem [3]. Given stream ùê∏ = {ùëí0, . . . ùëíùëö }, an integer ùëò, and submodular diversification function ùëì , it computes ùëò-set of elements ùëÜ that can maximize ùëì (ùëÜ). Our reduction constructs stream of datasets DùëÜ following the level-wise generation. We show that the function div is submodular function. (2) By integrating greedy selection and replacement policy, DivMODis keeps ùëò-set with the most diverse and representative datasets to mitigate the biases in the Skyline set. DivMODis achieves 1 4 -approximation of an ùúñ-Skyline set with maximized diversity at each level ùëñ. Please see the detailed proof in [43]. ùë¢ ùë¢ Analysis. DivMODis incurs an overhead to update the diversified ùëò-set. As MODis valuates up to min(ùëÅ ùëÖùë¢ , ùëÅ ) nodes (datasets), the total additional overhead is in ùëÇ (min(ùëÅ ùëÖùë¢ , ùëÅ ) ùëò ùëáS), where ùëáS refers to the unit valuation cost for single table, which is in PTIME. As both ùëò and ùëáS are relatively small, the practical overhead for DivMODis remains small (see Sec. 6). Remarks. Alternatives that solve multi-objective optimization may be applied, such as evolutionary algorithms such as NSGAII [5], or reinforcement-learning based methods [29]. The former rely on costly stochastic processes (e.g., mutation and crossover) and may require extensive parameter tuning. The latter are effective for general state exploration but require high-quality training samples and may not converge over conflicting measures. In contrast, MODis is training and tuning free. Our experiments verified that ApxMODis provides early generation of high-quality datasets from few large input datasets, due to reduce-from-Universal strategy, BiMODis enhances efficiency through bidirectional exploration and pruning, hence benefits for larger number of small-scale datasets, and DivMODis benefits most for datasets with skewed distribution."
        },
        {
            "title": "6 EXPERIMENT STUDY\nWe next experimentally verify the efficiency and effectiveness of\nour algorithms. We aim to answer three questions: RQ1: How\nwell can our algorithms improve the performance of models\nin multiple measures? RQ2: What is the impact of generation\nsettings, such as data size? RQ3: How fast can they generate\nskyline sets, and how scalable are they? We also illustrate the\napplications of our approaches with case studies1.\nDatasets. We use three sets of tabular datasets: kaggle [21],\nOpenData [1], and HF [19] (summarized in Table 2).\nTasks and Models. A set of tasks are assigned for evaluation.\nWe trained: (1) a Gradient Boosting model (GBmovie) to predict\nmovie grosses using Kaggle for Task ùëá1; (2) a Random Forest\nmodel (RFhouse) to classify house prices using OpenData with\nthe same settings in [14] for Task ùëá2; and (3) a Logistic Regression\nmodel (LRavocado) to predict Avocado prices using HF for Task\nùëá3. (4) a LightGBM model (LGCmental) [22] to classify mental\nhealth status using Kaggle for Task ùëá4. We also introduced task\nùëá 5, a link regression task for recommendation. This task takes as\ninput a bipartite graph between users and products, and links in-\ndicate their interaction. A LightGCN [17] (LGRmodel), a variant",
            "content": "Proof sketch: We show an induction on the levels. (1) We verify the guarantee at single level, by constructing an approximation 1Our codes and datasets are available at github.com/wang-mengying/modis Notation ùëùùê¥ùëêùëê ùëùùëá ùëü ùëùùêπ 1 ùëùùê¥ùëà ùê∂ ùëùùëÅ ùëê (ùëõ) ùëùùëÄùê¥ùê∏ , ùëùùëÄùëÜùê∏ ùëùùëÉùëê (ùëõ) , ùëùùëÖùëê (ùëõ) ùëùùêπ ùë†ùëê ùëùùëÄùêº Measures Model Accuracy Training Time Cost ùêπ1 score Area under the curve NDCG(@n) Mean Absolute / Squared Error Precision(@n), Recall(@n) Fisher Score [27] Mutual Information [14, 27] Used In P1, P2, P4 P1-P4 P2, P4 P4 P5 P3 P5 P1, P2 P1, Table 3: Performance Measures of graph neural networks (GNN) optimized for fast graph learning, is trained to predict top-ùëò missing edges in an input bipartite graph to suggest products to users. set of 1873 bipartite graphs is constructed from Kaggle for ùëá5. The augment (resp. reduct) operators are defined as edge insertions (resp. edge deletions) to transform bipartite graph to another. We use the same training scripts for each task and all methods for fair comparison. We assigned measures P1 through P5 for tasks ùëá1 to ùëá5, respectively , as summarized in Table 3. We also report the size of the data (ùëùùê∑ùëÜùëñùëßùëí ) in terms of (total # of rows total # of columns), excluding attributes with all cells masked. Estimator E. We adopt MO-GBM [34] as desired model performance estimator. It outperforms other candidate models even with simple training set For example, for ùëá1, MO-GBM performs inference for all objectives on one state in at most 0.2 seconds, with small MSE of 0.0003 when predicting Accuracy. Algorithms. We implemented the following methods. (1) MODis: Our multi-objective data discovery algorithms, including ApxMODis, BiMODis, and DivMODis. We also implemented NOBiMODis, counterpart of BiMODis without correlation-based pruning. (2) METAM [14]: goal-oriented data discovery algorithm that optimizes single utility score with consecutive joins of tables. We also implemented an extension METAM-MO, by incorporating multiple measures into single linear weighted utility function. (3) Starmie [12]: data discovery method that focuses on table-union search and uses contrastive learning to identify joinable tables. For METAM and Starmie, we used the code from original papers. (4) SkSFM [34]: An automated feature selection method in scikit-learns SelectFromModel, which recommends important features with built-in estimator. (5) H2O [15]: an AutoML platform; we used its feature selection module, which fits features and predictors into linear model. Construction of ùê∑ùëà and Operators. To prepare universal datasets ùê∑ùëà for MODis, we preprocess Kaggle, OpenData and HF into joinable tables and construct ùê∑ùëà with multi-way joins. This results in ùê∑ùëà datasets with size (in terms of # of columns and # of rows): (12, 3732), (27, 1178), (13, 18249) and (20, 140700), for tasks ùëá1 to ùëá4, respectively. Specifically, we applied ùëò-means clustering over the active domain of each attribute (with maximum ùëò set as 30), and derived equality literals, one for each cluster. We then compressed the input tables by replacing rows into tuple clusters, reducing the number of rows. This pragmatically help us avoid starting from large ùê∑ùëà by only retaining the values of interests, and still yield desired skyline datasets. For ùëá5, large bipartite graph is constructed with size of (7925, 34) (# of edges, # of nodes features). The generation of graphs consistently aligns with its table data counterpart, by conveniently replacing augment and reduction to their graph counterpart that performs link insertions and deletions. Figure 7: Effectiveness: Multiple Measures Evaluation metrics. We adopt the following metrics to quantify the effectiveness of data discovery approaches. Denote ùê∑ùëÄ as an initial dataset, and Dùëú set of output datasets from data discovery algorithm. (1) We define the relative improvement rImp(ùëù) for given measure ùëù achieved by method as ùëÄ (ùê∑ùëÄ ).ùëù . As ùëÄ (ùê∑ùëú ).ùëù all metrics are normalized to be minimized, the larger rImp(ùëù) is, the better ùê∑ùëù is in improving ùëÄ w.r.t. ùëù. Here ùëÄ (ùê∑ùëÄ ).ùëù and ùëÄ (ùê∑ùëù ).ùëù are obtained by actual model inference test. This allows us to fairly compare all methods in terms of the quality of data suggestion. For efficiency, we compare the time cost of data discovery upon receiving given model or task as query. Exp-1: Effectiveness. We first evaluate MODis methods over five tasks. Results for ùëá1 and ùëá3 are shown in Fig. 7 (the outer, the better). While results for ùëá2 and ùëá4 are presented in Table 4. Results for ùëá5 are in Table 5. We also report the model performance over the input tables as yardstick (Original) for all methods. As all baselines output single table, to compare MODis algorithms, we select the table in the Skyline set with the best estimated ùëùùê¥ùëêùëê , ùëÉùêπ 1, ùëÉùëÄùëÜùê∏ , ùëùùê¥ùëêùëê and ùëùùëÉùëê5 for ùëá1 to ùëá5, respectively. As METAM optimizes single utility score, we choose the same measure for each task as the utility. We apply model inference to all the output tables to report actual performance values. We have the following observations. (1) MODis algorithms outperform all the baselines in all tasks. As shown in Table 4, for example, for ùëá4, the datasets that bear best ùëùùê¥ùëêùëê and the second best are returned by ApxMODis (0.9535) and BiMODis (0.9525), respectively, and all MODis methods generated datasets that achieve 0.87 on ùëùùêπ 1 in ùëá2. (2) Over the same dataset and for other measures, MODis algorithms outperform the baselines in most cases. For example, in ùëá1, the result datasets that most optimize ùëùùêπùë†ùëê and ùëùùëÄùêº are obtained by BiMODis and ApxMODis, respectively; also in ùëá2 and ùëá3, NOBiMODis and BiMODis show absolute dominance in most measures. Table 5 also verifies that MODis easily generalizes to suggest graph data for GNN-based analytics, beyond tabular data. (3) Methods with data augmentation (e.g.,METAM and Starmie) enriches data to improve model accuracy, at cost of training time, while feature selection methods (e.g.,SkSFM and H2O) reduce data at the cost of accuracy with improved training efficiency. MODis methods are able to balance these trade-offs better by explicitly performing multi-objective optimization. For example, ùëùùê¥ùëêùëê and ùëùùëá ùëüùëéùëñùëõ in ùëá4, The best result for training cost (0.2359s) is contributed from SkSFM, yet at cost of lowest model accuracy (0.8839). We also compared ùëùùê¥ùëêùëê on ùëá4 with HydraGAN, generative data augmentation method, which achieves 0.9355 with 330 rows ùëá2: House ùëùùêπ 1 ùëùùê¥ùëêùëê ùëùùëá ùëüùëéùëñùëõ ùëùùêπùë†ùëê ùëùùëÄùêº Output Size ùëá4: Mental ùëùùê¥ùëêùëê ùëùùëÉùëê ùëùùëÖùëê ùëùùêπ 1 ùëùùê¥ùëà ùê∂ ùëùùëá ùëüùëéùëñùëõ Output Size ùëá5: Model ùëùùëÉùëê5 ùëùùëÉùëê10 ùëùùëÖùëê5 ùëùùëÖùëê10 ùëùùëÅ ùëê5 ùëùùëÅ ùëê10 Output Size Original METAM METAM-MO 0.8288 0.8305 0.2000 0.0928 0.126 (1178, 27) 0.8310 0.8333 0.19 0.0894 0.1207 (1178, 28) 0.8510 0.8322 0.21 0.0889 0.1109 (1178, 28) Original METAM METAM-MO 0.9222 0.7940 0.7722 0.7829 0.9618 0.4098 (105, 14) 0.9468 0.7991 0.7846 0.7918 0.9757 0.3198 (105, 15) 0.9462 0.8070 0.7959 0.8014 0.9774 0.4027 (105, 15) Starmie 0.8351 0.8331 0.2100 0.0149 0.0243 (1178, 32) Starmie 0.9505 0.8106 0.8030 0.8068 0.9784 0.3333 (105, 16) SkSFM 0.7825 0.7826 0.2000 0. 0.2970 (1178, 4) SkSFM 0.8839 0.6577 0.7523 0.7018 0.9326 0.2359 (105 8) H2O 0.8333 0.8305 0.2000 0.0691 0.1054 (1178, 15) H2O 0.9236 0.7892 0.7879 0.7885 0.9615 0.2530 (105, 8) ApxMODis NOBiMODis 0.9044 0.9050 0.1533 0.2268 0.2039 (835, 17) 0.9125 0.9121 0.1519 0.2610 0.2018 (797, 17) ApxMODis NOBiMODis 0.9532 0.8577 0.8097 0.8330 0.9792 0.3327 (128332, 16) 0.9471 0.8454 0.8092 0.8269 0.9755 0.2818 (116048, 16) BiMODis 0.9125 0.9121 0.1519 0.2610 0.2018 (797, 17) BiMODis 0.9525 0.8549 0.8075 0.8305 0.9789 0.3201 (128332, 17) DivMODis 0.8732 0.8729 0.2128 0.2223 0.3164 (1129, 5) DivMODis 0.9471 0.8454 0.8092 0.8269 0.9755 0.2818 (116048, 16) Table 4: Comparison of Data Discovery Algorithms in Multi-Objective Setting (ùëá2, ùëá4) 0.8200 0. Original ApxMODis NOMODis BiMODis DivMODis 0.8000 0.7200 0.8000 0.6600 0.2022 0.1863 0.3816 0.3217 0.7875 0.6923 0.7891 0.6646 (1966, 6) (7925, 0) 0.8000 0.8000 0.2022 0.3816 0.7875 0.7891 (1966, 6) 0.8200 0.8200 0.2072 0.3977 0.7924 0.7935 0.7976 (5826, 30) 0.8033 (2869, 4) 0.2072 0. Table 5: Comparison of MODis Methods on ùëá5 Figure 8: Effectiveness: Impact of Factors but fell short of data discovery methods. Increasing the number of rows further reduced performance, reflecting the limitations of generative approaches in this context, which cannot utilize verified external data sources, and synthetic data often lacks inherent reliability and contextual relevance of discovered data. Exp-2: Impact factors. We next investigate the MODis methods under the impact of two factors: ùúñ and the maximum path length (maxl), as well as the impact of ùõº on DivMODis. Varying ùúñ. Fixing maxl = 6, we varied ùúñ from 0.5 to 0.1 for ùëá1. As shown in Fig. 8(a), MODis algorithms are able to improve the model in ùëùùëéùëêùëê better with smaller ùúñ, as they all ensure to output ùúñ-Skyline set that better approximate Skyline set when ùúñ is set to be smaller. In all cases, they achieve relative improvement rImp(ùëùùê¥ùëêùëê ) at least 1.07. BiMODis and NOBiMODis perform better in recognizing better solutions from both ends in reduction and augmentation as smaller ùúñ is enforced. ApxMODis, with reduction only, is less sensitive to the change of ùúñ due to that larger ùúñ may trap it to local optimal sets from one end. Adding diversification (DivMODis) is able to strike balance between ApxMODis and BiMODis by enforcing to choose difference datasets out of local optimal sets, thus improving ApxMODis for smaller ùúñ. We choose smaller range of ùúñ for ùëá2 in Fig. 8(c), as the variance of ùëùùêπ 1 is small. As ùúñ varies from 0.1 to 0.02, NOBiMODis improves F1 score from 0.84 to 0.91. Varying maxl. Fixing ùúñ = 0.1, we varied maxl from 2 to 6. Fig. 8(b, d) tells us that all MODis algorithms improve the task performance for more rounds of processing. Specifically, BiMODis and NOBiMODis benefit most as bi-directional search allows both to find better solution from wider search space as maxl becomes larger. ApxMODis is less sensitive, as the reduction strategy from dense tables incurs smaller loss in accuracy. DivMODis finds datasets that ensure best model accuracy when maxl = 5, yet may lose chance to maintain the accuracy, due to that the diversification step may update the Skyline set with less optimal but more different counterparts in future levels (e.g., when maxl = 6). Varying ùõº in DivMODis. We demonstrate the effectiveness of DivMODis by adjusting ùõº. smaller ùõº prioritizes performance, while larger ùõº emphasizes content diversity, measured by hamming distance. Fig. 9(a) illustrates Performance Diversity, where smaller ùõº results in wider accuracy range with balanced and stable distribution. Both the mean and median remain centered. As ùõº increases, the accuracy distribution narrows and shifts toward higher values, reflecting the dominance of high-accuracy datasets in the Skyline set. Fig. 9(b) verifies the impact of Content Diversity, visualized as the percentage contribution of each adom. Larger ùõº leads to more evenly distributed contributions. The standard deviation values above the heatmap quantify this trend, showing consistent decrease as ùõº increases, indicating improved balance. Exp-3: Efficiency and Scalibility. We next report the the efficiency of MODis algorithms for task ùëá1 and ùëá3 over Kaggle and HF, respectively, and the impact of factors ùúñ and maxl. We also evaluate their scalability for ùëá1 and ùëá5 in terms of input size. Efficiency: Varying ùúñ. Fixing maxl = 6 and varying ùúñ from 0.1 to 0.5, Fig. 10 (a) verifies the following. (1) BiMODis, NOBiMODis and DivMODis take less time as ùúñ increases, as larger ùúñ provides more chance to prune unnecessary valuations. DivMODis has comparable performance with NOBiMODis, as it mainly Figure 9: Impact of ùõº for DivMODis Figure 10: Efficiency and Scalabilitiy benefits from the bi-directional strategy, which exploits early pruning and stream-style placement strategy. (2) As shown in Fig. 10(a), for ùëá1, BiMODis, NOBiMODis, and DivMODis are 2.5, 2, and 2 times faster than ApxMODis on average, respectively. ApxMODis takes longer time to explore larger universal table with reduct operators. It is insensitive to ùúñ. We observe that its search from the data rich end may converge faster at high-quality ùúñ-Skyline sets. Efficiency: Varying maxl. Fixing ùúñ = 0.2 for task ùëá1 and ùúñ = 0.1 for task ùëá3, we varied maxl from 2 to 6, all MODis algorithms take longer as maxl increases, as shown in Fig. 10 (b). Indeed, larger maxl results in more states to be valuated, and more nonùúñ-dominance relation to be resolved. ApxMODis is sensitive to maxl due to the rapid growth of the search space. In contrast, BiMODis mitigates the impact with bi-directional strategy and effective pruning. Scalability. We varied the number of total attributes ùê¥ and size of the largest active domain adom. We perform ùëò-means clustering over the tuples of the universal table with ùëò = adom, and extended operators with range queries to control adom. Fig. 10 (c) and (d) show that all MODis algorithms take more time for larger ùê¥ and adom. BiMODis scales best due to bi-directional strategy. DivMODis remains more efficient than ApxMODis, indicating affordable overhead from diversification. While our algorithms scale well with ùê¥ and adom, highdimensional datasets may present challenges due to the search space growth. Dimensionality reduction such as PCA or feature selection, or correlation-based pruning (to identify and eliminate highly correlated or redundant features), can be tailored to specific tasks to mitigate these challenges. Figure 11: Case 1 (left): Discover Datasets for Materials Peak Classification Analysis. Case 2 (right): Test Data Generation for Model Performance Benchmarking Exp-4: Case study. We next report two real-world case studies to illustrate the application scenarios of MODis. (1) Find data with models. material science team trained random forest-based classifier to identify peaks in 2D X-ray diffraction data. They seek more datasets to improve the models accuracy, training cost, and F1 score for downstream fine-tuning. Original X-ray datasets and models are uploaded to crowdsourced X-ray data platform we deployed [44] with best performance of < 0.6435, 3.2, 0.77 >. Within available X-ray datasets, BiMODis created three datasets {ùê∑1, ùê∑2, ùê∑3} and achieved the best performance of 0.987, 2.88, and 0.91, respectively. We set METAM to optimize F1-score, and achieved performance score of < 0.972, 3.51, 0.89 > over its output dataset. Fig. 11 illustrates such case that is manually validated with ground-truth from third-party institution. (2) Generating test data for model evaluation. We configure MODis algorithms to generate test datasets for model benchmarking, where specific performance criteria can be posed [41]. Utilizing trained scientific image classifier from Kaggle, and pool of image feature datasets from HF with 75 tables, 768 columns, and over 1000 rows. We request BiMODis to generate datasets over which the classifier demonstrates: accuracy > 0.85 and training cost < 30s. BiMODis successfully generated 3 datasets to be chosen from within 15 seconds, with performance < 0.95, 0.27 >, < 0.94, 0.26 > and < 0.90, 0.25 >, as in Fig. 11."
        },
        {
            "title": "7 CONCLUSION\nWe have introduced MODis, a framework that generate skyline\ndatasets to improve data science models on multiple performance\nmeasures. We have formalized skyline data generation with trans-\nducers equipped with augment and reduction operators. We show\nthe hardness and fixed-parameter tractability of the problem. We\nhave introduced three algorithms that compute approximate Sky-\nline sets in terms of ùúñ-Skyline set, with reduce-from-universal,\nbi-directional, and diversification paradigms. Our experiments\nhave verified their effectiveness and efficiency. A future topic is\nto enhance MODis with query optimization techniques to scale\nit for larger input with high-dimensional data. Another topic is\nto extend MODis for distributed Skyline data generation.",
            "content": "ACKNOWLEDGMENTS This work is supported by NSF under OAC-2104007. Saif Khan, et al. 2023. Evolution-guided Bayesian optimization for constrained multi-objective optimization in self-driving labs. (2023). [31] Tien-Dung Nguyen, Tomasz Maszczyk, Katarzyna Musial, Marc-Andr√© Z√∂ller, and Bogdan Gabrys. 2020. Avatar-machine learning pipeline evaluation using surrogate model. In Advances in Intelligent Data Analysis XVIII: 18th International Symposium on Intelligent Data Analysis, IDA 2020, Konstanz, Germany, April 2729, 2020, Proceedings 18. Springer, 352365. [32] Xuan Vinh Nguyen, Jeffrey Chan, Simone Romano, and James Bailey. 2014. Effective global approaches for mutual information based feature selection. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. 512521. [33] Andrei Paleyes, Raoul-Gabriel Urma, and Neil Lawrence. 2022. Challenges in deploying machine learning: survey of case studies. Comput. Surveys 55, 6 (2022), 129. [34] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research 12 (2011), 28252830. [35] Hanchuan Peng, Fuhui Long, and Chris Ding. 2005. Feature selection based on mutual information criteria of max-dependency, max-relevance, and minredundancy. IEEE Transactions on pattern analysis and machine intelligence 27, 8 (2005), 12261238. [36] Yuji Roh, Geon Heo, and Steven Euijong Whang. 2019. survey on data collection for machine learning: big data-ai integration perspective. IEEE Transactions on Knowledge and Data Engineering 33, 4 (2019), 13281347. [37] Paolo Serafini. 1987. Some considerations about computational complexity for multi objective combinatorial problems. In Recent Advances and Historical Development of Vector Optimization: Proceedings of an International Conference on Vector Optimization Held at the Technical University of Darmstadt, FRG, August 47, 1986. Springer, 222232. [38] Darius ≈†idlauskas and Christian Jensen. 2014. Spatial joins in main memory: Implementation matters! Proceedings of the VLDB Endowment 8, 1 (2014), 97100. [39] Tom Sterkenburg and Peter Gr√ºnwald. 2021. The no-free-lunch theorems of supervised learning. Synthese 199, 3-4 (2021), 997910015. [40] George Tsaggouris and Christos Zaroliagis. 2009. Multiobjective optimization: Improved FPTAS for shortest paths and non-linear objectives with applications. Theory of Computing Systems 45, 1 (2009), 162186. [41] Francesco Ventura, Zoi Kaoudi, Jorge Arnulfo Quian√©-Ruiz, and Volker Markl. 2021. Expand your training limits! generating training data for ml-based data management. In SIGMOD. [42] Mengying Wang, Sheng Guan, Hanchao Ma, Yiyang Bian, Haolai Che, Abhishek Daundkar, Alp Sehirlioglu, and Yinghui Wu. 2023. Selecting Top-k Data Science Models by Example Dataset. In CIKM. [43] Mengying Wang, Hanchao Ma, Yiyang Bian, Yangxin Fan, and Yinghui Wu. 2024. Full Version. https://wangmengying.me/papers/modis.pdf [44] Mengying Wang, Hanchao Ma, Abhishek Daundkar, Sheng Guan, Yiyang Bian, Alpi Sehirlioglu, and Yinghui Wu. 2022. CRUX: crowdsourced materials science resource and workflow exploration. In CIKM. [45] Chengrun Yang, Jicong Fan, Ziyang Wu, and Madeleine Udell. 2020. Automl pipeline selection: Efficiently navigating the combinatorial space. In proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining. 14461456. [46] Zhuoyue Zhao, Feifei Li, and Yuxi Liu. 2020. Efficient join synopsis maintenance for data warehouse. In Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data. 20272042. [47] JH Zheng, YN Kou, ZX Jing, and QH Wu. 2019. Towards many-objective optimization: Objective analysis, multi-objective optimization and decisionmaking. IEEE Access 7 (2019), 9374293751. [48] Patrick Ziegler and Klaus Dittrich. 2007. Data integrationproblems, approaches, and perspectives. In Conceptual modelling in information systems engineering. Springer, 3958. tion. REFERENCES [1] U.S. General Services Administration. 2023. Data.gov. https://www.data.gov/ [2] Ashwinkumar Badanidiyuru, Baharan Mirzasoleiman, Amin Karbasi, and Andreas Krause. 2014. Streaming submodular maximization: Massive data summarization on the fly. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. [3] Amit Chakrabarti and Sagar Kale. 2015. Submodular maximization meets streaming: matchings, matroids, and more. Mathematical Programming (2015). [4] Jan Chomicki, Paolo Ciaccia, and Niccolo Meneghetti. 2013. Skyline queries, front and back. ACM SIGMOD Record 42, 3 (2013), 618. [5] Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMT Meyarivan. 2002. fast and elitist multiobjective genetic algorithm: NSGA-II. IEEE transactions on evolutionary computation 6, 2 (2002), 182197. [6] Chance DeSmet and Diane Cook. 2024. HydraGAN: Cooperative Agent Model for Multi-Objective Data Generation. ACM Transactions on Intelligent Systems and Technology 15, 3 (2024), 121. [7] AnHai Doan, Alon Halevy, and Zachary Ives. 2012. Principles of data integra- [8] Bin Dong, Kesheng Wu, Surendra Byna, Jialin Liu, Weijie Zhao, and Florin Rusu. 2017. ArrayUDF: User-defined scientific data analysis on arrays. In Proceedings of the 26th International Symposium on High-Performance Parallel and Distributed Computing. 5364. [9] Zahra Donyavi and Shahrokh Asadi. 2020. Diverse training dataset generation based on multi-objective optimization for semi-supervised classification. Pattern Recognition 108 (2020), 107543. [10] Iddo Drori, Yamuna Krishnamurthy, Raoni Lourenco, Remi Rampin, Kyunghyun Cho, Claudio Silva, and Juliana Freire. 2019. Automatic machine learning by pipeline synthesis using model-based reinforcement learning and grammar. arXiv preprint arXiv:1905.10345 (2019). [11] Mahdi Esmailoghli, Christoph Schnell, Ren√©e Miller, and Ziawasch Abedjan. 2023. Blend: unified data discovery system. arXiv preprint arXiv:2310.02656 (2023). [12] Grace Fan, Jin Wang, Yuliang Li, Dan Zhang, and Ren√©e Miller. 2023. Semantics-Aware Dataset Discovery from Data Lakes with Contextualized Column-Based Representation Learning. Proceedings of the VLDB Endowment 16, 7 (2023). [13] Matthias Feurer, Katharina Eggensperger, Stefan Falkner, Marius Lindauer, and Frank Hutter. 2018. Practical automated machine learning for the automl challenge 2018. In International Workshop on Automatic Machine Learning at ICML. 11891232. [14] Sainyam Galhotra, Yue Gong, and Raul Castro Fernandez. 2023. Metam: Goaloriented data discovery. In 2023 IEEE 39th International Conference on Data Engineering (ICDE). IEEE, 27802793. [15] H2O.ai. 2022. H2O: Scalable Machine Learning Platform. https://github.com/ h2oai/h2o-3 version 3.42.0.2. [16] Pierre Hansen. 1980. Bicriterion path problems. In Multiple Criteria Decision Making Theory and Application: Proceedings of the Third Conference Hagen/K√∂nigswinter, West Germany, August 2024, 1979. 109127. [17] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for recommendation. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval. 639648. [18] Zezhou Huang, Pranav Subramaniam, Raul Castro Fernandez, and Eugene Wu. 2023. Kitana: Efficient Data Augmentation Search for AutoML. arXiv preprint arXiv:2305.10419 (2023). [19] Hugging Face AI 2023. Hugging Face The AI Community Building the Future. https://huggingface.co/ [20] John Hwang and Joaquim RRA Martins. 2018. fast-prediction surrogate model for large datasets. Aerospace Science and Technology 75 (2018), 7487. [21] Kaggle. 2023. Kaggle: Your Home for Data Science. https://www.kaggle.com/ [22] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. 2017. Lightgbm: highly efficient gradient boosting decision tree. Advances in neural information processing systems 30 (2017). [23] Mina Konakovic Lukovic, Yunsheng Tian, and Wojciech Matusik. 2020. Diversity-guided multi-objective bayesian optimization with batch evaluations. Advances in Neural Information Processing Systems 33 (2020), 1770817720. [24] Hsiang-Tsung Kung, Fabrizio Luccio, and Franco Preparata. 1975. On finding the maxima of set of vectors. J. ACM 22, 4 (1975), 469476. [25] Berti-Equille Laure, Bonifati Angela, and Milo Tova. 2018. Machine learning to data management: round trip. In ICDE. 17351738. [26] Maurizio Lenzerini. 2002. Data integration: theoretical perspective. In PODS. [27] Jundong Li, Kewei Cheng, Suhang Wang, Fred Morstatter, Robert Trevino, Jiliang Tang, and Huan Liu. 2017. Feature selection: data perspective. ACM computing surveys (CSUR) 50, 6 (2017), 145. [28] Yuliang Li, Xiaolan Wang, Zhengjie Miao, and Wang-Chiew Tan. 2021. Data augmentation for ml-driven data preparation and integration. Proceedings of the VLDB Endowment 14, 12 (2021), 31823185. [29] Chunming Liu, Xin Xu, and Dewen Hu. 2014. Multiobjective reinforcement learning: comprehensive overview. IEEE Transactions on Systems, Man, and Cybernetics: Systems 45, 3 (2014), 385398. [30] Andre KY Low, Flore Mekki-Berrada, Aleksandr Ostudin, Jiaxun Xie, Eleonore Vissol-Gaudin, Yee-Fun Lim, Abhishek Gupta, Qianxiao Li, Yew Soon Ong, Appendix A: Algorithms and Proof A.1 ApxMODis Proof of Lemma 2 For any constant ùúñ, ApxMODis correctly computes an ùúñ-Skyline set Œ† that approximates Skyline set defined on the ùëÅ states it valuated. weights in ùê∫ùë§. Therefore, Œ†ùë§ is an ùúñ-Skyline set of paths Œ†ùë§ in ùê∫ùë§ if Dùêπ is an ùúñ-Skyline set of DùëÜ . Only If condition. Conversely, we assume the following: (1) Œ†ùë§ is an ùúñ-Skyline set of paths Œ†ùë§ in ùê∫ùë§, but (2) the induced Dùêπ is not an ùúñ-Skyline set of DùëÜ . Assumption (2) implies one of the following two cases: Proof. We establish the ùúñ-approximability of ApxMODis by constructing reduction from MODis to the multi-objective shortest path problem (MOSP) [40]. Reduction. An instance of MOSP consists of an edge-weighted graph ùê∫ùë§, where each edge ùëíùë§ is assigned ùëë-dimensional attribute vector ùëíùë§ .ùëê. The cost of path ùúåùë§ in ùê∫ùë§ is defined as ùúåùë§ .ùëê = (cid:205)ùëíùë§ ùúåùë§ ùëíùë§ .ùëê. The dominance relation between two paths ùúåùë§ and ùúå ùë§ is determined by comparing their costs. Specifically, ùúåùë§ dominates ùúå ùë§ if ùúåùë§ has equal or lower costs than ùúå ùë§ in all dimensions and is strictly better in at least one dimension. The objective is to compute Skyline set of paths from start node ùë¢ to all other nodes in the graph. We construct the reduction from our problem to MOSP. (1) We define ùê∫ùë§ as an edge weighted counterpart of running graph ùê∫ . (a) Each vertex in ùê∫ represents unique state ùë† during the execution of ApxMODis, with each state corresponding to specific dataset configuration in the data discovery process. The graph ùê∫ contains ùëÅ vertices, corresponding to the ùëÅ states that ApxMODis has spawned and valuated. (b) Each edge (ùë†, ùë†) in ùê∫ represents transition from state ùë† to state ùë†, resulting from applying an operation (e.g., reduction or augmentation) that modifies the dataset. The edge is weighted by the difference in performance measures in between the two states: ùëíùë§ = ùë†.P ùë†.P. Here, ùë†.P and ùë†.P are the performance vectors of the states ùë† and ùë†, respectively. The edge weight ùëíùë§ is ùëë-dimensional vector that quantifies how the performance metrics change as result of the transition. path ùúå ùê∫ corresponds to sequence of transitions between states, starting from the initial state ùë†ùëà . Similar to ùúåùë§ ùê∫ùë§, the cumulative cost of this path ùúå.ùëê is defined as the sum of the edge weights along the path, which represents the cumulative change in the performance measures as the dataset evolves through different states. Given solution Œ†ùë§ of an instance of MOSP, which is an ùúñSkyline set of paths, we construct solution for corresponding instance of MODis. For each path ùúåùë§ Œ†ùë§, we establish corresponding path ùúå in ùê∫ and identify the final state ùë† that the path reaches. The final state ùë† corresponds to specific dataset ùê∑, which is the result of applying the sequence of operations from ùúå. We then include ùê∑ in the set Dùêπ . This forms set of datasets as the solution to MODis. We next prove that Œ†ùë§ is an ùúñ-Skyline set of paths Œ†ùë§ in ùê∫ùë§. if and only if Dùêπ is an ùúñ-Skyline set of DùëÜ . If condition. Let Dùêπ be an ùúñ-Skyline set of DùëÜ . By the definition given in sec 4, this means that for every dataset ùê∑ (DùëÜ Dùêπ ), there exists at least one dataest ùê∑ Dùêπ such that ùê∑ ùúñ-dominates D. Specifically, this means that ùê∑ has costs that are at most (1 + ùúñ) times the costs of ùê∑ in all performance measures in P, and ùê∑ has strictly lower cost in at least one measure. From the reduction, each path ùúåùë§ Œ†ùë§ corresponds to sequence of transitions in ùê∫ leading to final state ùë†, which represents dataset ùê∑ Dùêπ . Similarly, ùúå ùë§ Œ†ùë§ corresponds to dataset ùê∑ (DùëÜ Dùêπ ). Since ùê∑ ùúñ-dominates ùê∑, the corresponding path ùúåùë§ ùúñ-dominates ùúå ùë§. This dominance is preserved because the performance measures P, directly corresponding to the edge Case 1: There exists dataset ùê∑ (DùëÜ Dùêπ ) that is not ùúñ-dominated by any dataset in Dùêπ . This means there is corresponding path ùúå ùë§ in ùê∫ùë§ that is not ùúñ-dominated by any path in Œ†ùë§. This contradicts assumption (1) because ùúå ùë§ should be ùúñ-dominated by at least one path in Œ†ùë§. Case 2: There exists dataset ùê∑ Dùêπ that is ùúñ-dominated by dataset ùê∑ (DùëÜ Dùêπ ). This means there exists path ùúå ùë§ corresponding to ùê∑ in ùê∫ùë§ that ùúñ-dominates the path ùúåùë§ corresponding to ùê∑ in Œ†ùë§. However, this would imply that Œ†ùë§ does not fully capture the ùúñ-Skyline set because ùúåùë§ should not be in Œ†ùë§ if it is ùúñ-dominated by ùúå ùë§. Thus, it contradicts assumption (1). Both cases lead to contradiction with the assumption (1) that Œ†ùë§ is an ùúñ-Skyline set of paths Œ†ùë§ in ùê∫ùë§. Therefore, the initial assumption (2) must be false, meaning Œ†ùë§ is an ùúñ-Skyline set of paths Œ†ùë§ in ùê∫ùë§, only if Dùêπ is an ùúñ-Skyline set of DùëÜ . By proving both directions, we establish the equivalence that Œ†ùë§ is an ùúñ-Skyline set of paths Œ†ùë§ in ùê∫ùë§, if and only if Dùêπ is an ùúñ-Skyline set of DùëÜ . Correctness. We then show that algorithm ApxMODis is an optimized process of the algorithm in [40], which correctly computes Œ†ùë§ for ùê∫ùë§. Specifically, this means that in ùê∫ùë§, for any path ùúå ùë§ Œ†ùë§ with corresponding state ùë†, there exists path ùúåùë§ Œ†ùë§ with corresponding state ùë†, such that for every performance measure ùëùùëñ (where 1 ùëñ ùëë, and ùëë = ), the condition ùë†.P (ùëùùëñ ) (1 + ùúñ)ùë†.P (ùëùùëñ ) holds. We prove the correctness of this result by induction. (cid:107) (cid:106)log1+ùúñ ùë†. (ùëùùëñ ) ùëùùëôùëñ ùë† . (ùëùùëñ ) ùëùùëôùëñ ùë†. (ùëùùëñ ) = ùëùùëôùëñ 1 log1+ùúñ (1) Base case. After the first iteration in the main procedure of ApxMODis, and due to the merge steps in the UPareto procedure, the position ùëùùëúùë† (ùë†) in Œ†1 ùë§ will be occupied by path ùúåùë§, for which: (i) ùëùùëúùë† (ùë†) = ùëùùëúùë† (ùë†); and (ii) ùë†.P (ùëùùëë ) ùë†.P (ùëùùëë ). From (i) and based on the Equation (1), for 1 ùëñ ùëë 1, we have (cid:106)log1+ùúñ (cid:107). This implies log1+ùúñ , so that ùë†.P (ùëùùëñ ) (1 + ùúñ)ùë†.P (ùëùùëñ ) for 1 ùëñ < ùëë. Combined with (ii), we conclude that ùë†.P (ùëùùëñ ) (1 + ùúñ)ùë†.P (ùëùùëñ ) holds for 1 ùëñ ùëë. (2) Induction. Assume that after ùëñ 1 iterations, ApxMODis correctly computes the ùúñ-Skyline set Œ†ùëñ 1 ùë§ for all paths from the source node ùë†ùëà that contain up to ùëñ 1 edges. This means that ùë§ Œ†ùëñ 1 for every path ùúå ùë§ with at most ùëñ 1 edges, there exists path ùúåùë§ Œ†ùëñ 1 ùë§ such that the corresponding states ùë† and ùë† satisfy: ùë† . (ùëùùëñ ) ùëùùëôùëñ ùë†.P (ùëùùëñ ) (1 + ùúñ)ùë†.P (ùëùùëñ ), 1 ùëñ ùëë We next prove that after ùëñ iterations, ApxMODis correctly ùë§ for all paths from the source node computes the ùúñ-Skyline set Œ†ùëñ ùë†ùëà that contain up to ùëñ edges. By induction, every path in Œ†ùëñ 1 ùë§ ùúñ-dominates any other paths of up to ùëñ 1 edges not included in Œ†ùëñ 1 ùë§ , so we only need to ensure the correctness of the ùëñth iteration. In this iteration, paths are expanded to include ùëñ edges. As seen in the base case, after the merge step in procedure UPareto, ApxMODis ensures that for any state ùë† corresponding to path not included in Œ†ùëñ ùë§, there exists at least one state ùë† with corresponding path in Œ†ùëñ ùë§, such that: ùë†.P (ùëùùëñ ) (1 + ùúñ)ùë†.P (ùëùùëñ ), 1 ùëñ ùëë Thus, after ùëñ iterations, Œ†ùëñ that should be included in the ùúñ-Skyline set. ùë§ covers all paths with up to ùëñ edges Putting these together, we show that ApxMODis correctly ùë§ for all ùëñ > 0. This verifies the computes the ùúñ-Skyline set Œ†ùëñ correctness of ApxMODis. Proof of Lemma 3. Given with configuration ùê∂, if DùëÜ has size in ùëÇ (ùëì (ùê∑ùëà )), where ùëì is polynomial, then ApxMODis is an FPTAS for MODis. Proof. We consider the reduction of an instance of MODis to its counterpart of MOSP as detailed in the proof of Lemma 2. MOSP is known to be solvable by an FPTAS. That is, there is an algorithm that can compute an ùúñ-Skyline set in polynomial time relative to the size of the input graph and 1 ùúñ [40]. We configure ApxMODis to run in (DùëÜ , ùúñ)-approximation, which is simplified implementation of an FPTAS in [40] with multiple rounds of replacement strategy following path dominance. In the proof of Lemma 2, we have already shown that ApxMODis correctly computes the ùúñ-Skyline set for ùê∫ùë§, which is equivalent to the ùúñ-Skyline set for DùëÜ in MODis. Meanwhile, as DùëÜ is bounded by polynomial of the input size ùê∑ùëà , the time (cid:19) (cid:19) (cid:18) complexity of ApxMODis is ùëÇ where ùëì is polynomial. This ensures that ApxMODis approximates the Skyline set for all datasets within PTIME. (cid:18)(cid:16) log(ùëùùëö ) ùúñ ùëì (ùê∑ùëà ) (cid:17) 1 + ùêº , Space cost. We also report the space cost. (1) It takes vector of length in ùëÇ (ùëÉ 1) to encode the position ùëùùëúùë† (ùúå). The replacement strategy in ApxMODis keeps one copy of position per path at runtime and recycles the space once it is verified to be dominated. According to Equation 1, there are at most (cid:206) 1 ùëñ=1 paths to be remained in (P 1)- log1+ùúñ + 1 (cid:18) (cid:22) (cid:23) (cid:19) ùëùùëöùëéùë• ùëñ ùëùùëöùëñùëõ ùëñ dimensional array at runtime, hence the total space cost is in (cid:19)(cid:19) (cid:18) (cid:22) (cid:23) (cid:18) ùëÇ (cid:206) 1 ùëñ=1 log1+ùúñ ùëùùëöùëéùë• ùëñ ùëùùëöùëñùëõ ùëñ + . A.2 BiMODis the details of Correlation based Pruning. We present Correlation-based Pruning. We first introduce monotonicity property as the condition for the applicability of the pruning. Monotonicity property. Given the current historical performances over valuated states, we say state ùë† (resp. ùë†) with performance measure ùëù at path ùúå has monotonicity property, if for any state ùë† reachable from ùë† (resp. can reach ùë†) via ùúå, ùë†. ÀÜùëùùë¢ < ùë† . ÀÜùëùùëô 1+ùúñ (resp. ùë†. ÀÜùëùùë¢ < ùë† . ÀÜùëùùëô 1+ùúñ ). Pruning rule. We next specify Correlation-based pruning with pruning rule as follows. First, recall that BiMODis dynamically maintains, for each performance ùëù and each state ùë†, an estimated range [ ÀÜùëùùëô , ÀÜùëùùë¢ ] [ùëùùëô , ùëùùë¢ ]. The bounds ÀÜùëùùëô (resp. ÀÜùëùùë¢ are updated with runtime performance estimation of ùë† upon the changes of correlated performance measures. Specifically, for any state ùë† on path ùúå obtained by augmented features of its ancestor state ùë† on ùúå, where ùë† has performance ùëù learning cost ùë†.ùëù with lower bound ùë†. ÀÜùëùùëô = 0.4, and an accuracy with estimated upperbound ùë†. ÀÜùëù ùë¢ = 0.8, then (1) ùë† has an estimated running cost initialized as ùë†. ÀÜùëùùëô = 0.4, indicating learning cost no smaller than the counterpart ùë† with smaller dataset; and (2) ùë† has an estimated accuracy with an upperbound ùë†. ÀÜùëù ùë¢ = 0.8, as ùëù and ùëù are statistically negatively correlated with rule specified as: for every current valuated ùë†, ùëù as learning cost, and ùëù as accuracy, if ùëù is larger, then ùëù is smaller. The algorithm BiMODis dynamically maintains bounds list for all created states ùë† in the bidirectional search. Given two states ùë† and ùë†, where ùë† ùúñ ùë†, state ùë† on path ùúå from ùë† or to ùë† can be pruned according to Correlation-Based Pruning if for every ùëù P, ùë† has ùëù at ùúå with monotonicity property w.r.t. ùë† (resp. ùë†). Note that the above rule is checkable in PTIME in terms of input size DùëÜ . When DùëÜ is large, one can generate path with all states unevaluated, and check at runtime if the condition holds between two evaluated states and any unevaluated state in betwen in PTIME, to prune the unevaluated states. We are now ready to show Lemma 4. Proof of Lemma 4 Let ùë† ùëÑ ùëì and ùë† ùëÑùëè . If ùë† ùúñ ùë†, then any state node ùë† on path from ùë† or to ùë†, that can be pruned according to Correlation-Based Pruning, ùê∑ùë† is not in ùúñ-Skyline sets of the datasets from valuated states. We next perform case study of ùë† and ùë† as follows, subject to the monotonicity property. Case 1: Both ùë†.P (ùëù) and ùë†.P (ùëù) are valuated. If ùë† ùúñ ùë†, then by definition, ùë†.P (ùëù) (1 + ùúñ)ùë†.P (ùëù) for all ùëù P. This readily leads to ùúñ-dominance, i.e., ùë† ùúñ ùë†. As ùë† has every performance measures ùëù with monotonicity property w.r.t. ùë†, ùë† ùúñ ùë†. Hence ùë† can be safely pruned without valuation. Case 2: Neither ùë†.P (ùëù) nor ùë†.P (ùëù) is valuated. By definition, as ùë† ùúñ ùë†, then for every ùëù P, ùë†. ÀÜùëùùë¢ (1 + ùúñ)ùë†. ÀÜùëùùëô . Given that ùë† has every performance measures ùëù with monotonicity property w.r.t. ùë†, then by definition, for each ùëù P, we have ùë† . ÀÜùëùùëô ùë†.ùëù ùë†. ÀÜùëùùë¢ (1 + ùúñ)ùë†. ÀÜùëùùëô (1 + ùúñ)ùë†. ÀÜùëùùë¢ < (1 + ùúñ) 1+ùúñ ùë†.ùëù, for every ùëù P. By definition of state dominance, ùë† ùë†, for unevaluated ùë†. Following similar proof, one can infer that ùë† ùë† for state ùë† in the forward front of BiMODis. Hence ùë† can be safely pruned. Case 3: One of ùë†.P (ùëù) or ùë†.P (ùëù) is valuated. Given that ùë† ùúñ ùë†, we have (a) ùë†.P (ùëù) (1 + ùúñ)ùë†. ÀÜùëùùëô , if only ùë†.P (ùëù) is valuated; or (b) ùë†. ÀÜùëùùë¢ (1 + ùúñ)ùë†.P (ùëù), if only ùë†.P (ùëù) is valuated. Consider case 3(a). As ùë† can reach ùë† via path ùúå, and ùë† satisfiies the pruning condition, we can infer that ùë†.P (ùëù) (1 + ùúñ)ùë†. ÀÜùëùùëô (1 + ùúñ)ùë†. ÀÜùëùùë¢ < (1 + ùúñ) ùë† . ÀÜùëùùëô 1+ùúñ ùë†.ùëù, hence ùë† ùë†. Similarly for case 3(b), we can infer that ùë†. ÀÜùëùùë¢ (1 + ùúñ)ùë†.ùëù ùë† . ÀÜùëùùëô 1+ùúñ ùë†.ùëù. hence ùë† ùë†. For both cases, (1 + ùúñ)ùë†. ÀÜùëùùë¢ < (1 + ùúñ) ùë† can be pruned without evaluation. Lemma 4 hence follows. We present the details of the algorithm BiMODis in Fig. 12. A.3 DivMODis 1 Proof of Lemma 5 Given ùëÅ and ùúñ, DivMODis achieves 4 approximation for diversified MODis, i.e., (1) it correctly computes ùúñ-Skyline set ùê∑ùëÉ ùêπ ) 1 4 div(D ). ùêπ over ùëÅ valuated datasets, and (2) div(ùê∑ùëÉ We here present detailed analysis for the lemma 5. Monotone submodularity. We first show that the diversification function div() is monotone submodular function. Given set of datasets Dùêπ , we show that for any set of datasets ùëå ùëã Dùêπ , Algorithm 4 BiMODis 1: Input: Configuration ùê∂ = (ùë†ùëà , O, ùëÄ,ùëá , E), Records ùëÖùëíùëê, constant ùúñ > 0; 2: Output: ùúñ-Skyline set Dùêπ . 3: Set Dùêπ := , BIB := , PrunS := ; ùë†ùëè = BackSt(ùë†ùê∏, ùë†ùëà ); 4: queue ùëÑ ùëì := {(ùë†ùëà , 0)}, queue ùëÑùëè := {(ùë†ùëè, 0)}; 5: D0 6: D0 7: while ùëÑ ùëì , ùëÑùëè and ùëÑ ùëì ùëÑùëè = do 8: ùêπ [ùëùùëúùë† (ùë†ùëà )] = CorrFP(ùë†ùëà , ùëÖùëíùëê, E); ùêπ [ùëùùëúùë† (ùë†ùëè )] = CorrFP(ùë†ùëè, ùëÖùëíùëê, E); (ùë†, ùëë) = ùëÑ ùëì .dequeue(), Dùêπ ùëë+1 = Dùêπ ùëë ; Forward Serach for all ùë† OpGen (ùë†, F) do Pùë† = CorrFP(ùë†, ùëÖùëíùëê, E); set ùúåùë† with Pùë† ; if ùëùùëúùë† (ùë†) PrunS then continue; pruned = False for bound in SandwBs do pruned = SandwPrun(ùúåùë† , bound, SandwBs) if pruned then break; pruned = UPareto (Dùëë+1 , PrunS, pos(s), ùúñ) if not pruned then ùëÑ ùëì .enqueue((s, d+1)) ùêπ (ùë†, ùëë) = ùëÑùëè .dequeue(), Dùêπ Serach ùëë+1 = Dùêπ ùëë ; Backward for all ùë† OpGen (ùë†, B) do same with line 10 to 16 in Forward Search if not pruned then ùëÑùëè .enqueue((s, d+1)) 22: procedure CorrFP(ùë†, ùëÖùëíùëê, E) 23: Build ùê∫ùëê for measures recorded in ùëÖùëíùëê; StrongRs = GetSR(ùê∫ùëê ) if in ùëÖùëíùëê.keys() then Pùë† = ùëÖùëíùëê [ùë†]; if ùë£ùëéùëôùëñùëë (Pùë† ) 0.8Pùë† then return Pùë† ; for all missing ùëùùë† ùëñ in Pùë† do if (ùëùùëñ, ùëù ùëó ) StrongRs and ùëùùë† Case 1: By ùëÖùëíùëê Case 2: By ùê∫ùëê ùëó ùëÖùëíùëê [ùë†] then ùëó in ùëÖùëíùëê; ùëó with ùëùùë† find closed ùëùùëô ùëñ + ùëùùë¢ ùëñ = (ùëùùëô ùëùùë† ùëó and ùëùùë¢ ùëñ )/2 if ùë£ùëéùëôùëñùëë (Pùë† ) < 0.8Pùë† then Case 3: By fill missing ùëùùë† Pùë† by invoking E; update ùëÖùëíùëê [ùë†] = Pùë† return Pùë† ; 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: Figure 12: Complete Version of BiMODis div(ùëå ) div(ùëã ); and ùë• Dùêπ ùëã , div(ùëã {ùë• })div(ùëã ) div(ùëå {ùë• })div(ùëå ). (1) To see div(ùëå ) div(ùëã ), we have div(ùëã ) div(ùëå ) = ùëò 1 ùëò ùëñ=1 ùëó =ùëñ+1 Given ùëå ùëã , we have dis(ùê∑ùëã ùëñ , ùê∑ùëã ùëó ) ùëò 1 ùëò ùëñ= ùëó =ùëñ+1 dis(ùê∑ùëå ùëñ , ùê∑ùëå ùëó ) div(ùëã ) div(ùëå ) = dis(ùê∑, ùê∑ ) 0; ùê∑ ùëã ùëå ,ùê∑ ùëå (2) We next show the submodularity of the function div(). To simplify the presentation, we introduce notation marginal gain. For any ùë• Dùêπ ùëã and ùëå ùëã , the marginal gain of mg(ùëã , ùë• ) = ùëò 1 ùëò ùëó =ùëñ+1 ùëñ=1 + ùëò 1 ùëò mg(ùëå , ùë• ) = ùëò 1 ùëò ùëó =ùëñ+1 ùëñ=1 + ùëò 1 ùëò diversification score for ùëã {ùë• } and ùëå {ùë• }, denoted as mg(ùëã, ùë•) and mg(ùëå, ùë•), are defined as: ùõº 1 cos(ùë†ùëñ .ùêø, ùë† ùëó .ùêø) 2 ùëò 1 ùëò ùëñ= ùëó =ùëñ+1 ùõº 1 cos(ùë†ùëô .ùêø, ùë†ùëö .ùêø) 2 (1 ùõº ) euc(ùë°ùëñ . P, ùë° ùëó . ) eucm ùëò 1 ùëò (1 ùõº ) euc(ùë°ùëô . P, ùë°ùëö . ) eucm ùëó =ùëñ+1 ùëñ=1 ùëó =ùëñ+1 where ùê∑ùëñ, ùê∑ ùëó ùëã {ùë• } and ùê∑ùëô , ùê∑ùëö ùëã . ùëñ=1 ùõº 1 cos(ùë†ùëñ .ùêø, ùë† ùëó .ùêø) 2 ùëò 1 ùëò ùëñ=1 ùëó =ùëñ+1 ùõº 1 cos(ùë†ùëô .ùêø, ùë†ùëö .ùêø) 2 (1 ùõº ) euc(ùë°ùëñ . P, ùë° ùëó . ) eucm ùëò 1 ùëò (1 ùõº ) euc(ùë°ùëô . P, ùë°ùëö . ) eucm ùëó =ùëñ+ ùëñ=1 where ùê∑ùëñ, ùê∑ ùëó ùëå ùë• and ùê∑ùëô , ùê∑ùëö ùëå . ùëñ=1 ùëó =ùëñ+1 In our problem,we only consider ùúñ-Skyline sets with size at most ùëò. With this condition, we observe that given the dataset ùë•, mg(ùëå, ùë•) and mg(ùëã, ùë•) measure the marginal gain of diversification scores by replacing dataset ùë• ùëå with ùë• and ùë• ùëã with ùë•. mg(ùëå, ùë•) and mg(ùëã, ùë•) measure the margin gain by replacing ùë• with same dataset ùë• over same ùúñ-Skyline set with size ùëò. In this case, we can have ùêπ at level ùëñ, and new batch of datasets ùê∑ùëñ mg(ùëã , ùë• ) = mg(ùëå , ùë• ) due to DivMODis replace the same dataset ùë• that are in ùëã and ùëå . Thus, we can see that the marginal gain of ùëã is no larger than marginal gain of ùëå by including ùë•. This analysis completes the proof of diversification function div() is monotone submodular function. Approximability. We next prove that DivMODis ensures 1 4 - approximation of diversified size-ùëò Skyline set. We verify this by proving an invariant that the approximation holds for any size-ùëò ùúñ-Skyline set ùê∑ùëÉ ùêπ generated at every level ùëñ. By integrating greedy selection and replacement policy, DivMODis keeps -set with the most diverse and representative datasets to mitigate the biases in the Skyline set at each level. Consider set of datasets ùê∑ùëñ 1 ùêπ arrives. DivMODis aims to maintain the set of datasets ùê∑ùëñ ùêπ such that, at level ùëñ, ùê∑ùëñ ùêπ ) is maximized. At any level ùëñ, DivMODis approximates the global optimal solution upon the newly generated datasets. Consider the global optimal solution at level ùëñ, over ùê∑ùëñ ùêπ , we can show that DivMODis maintains ùê∑ùëÉ ùêπ at any level ùëñ by solving streaming submodular maximization problem [3]. Reduction. We show there exists an approximation at any level by reduction from the diversification phase of MODis problem to stream submodular maximization problem [2, 3]. Given streaming of elements ùê∏ = {ùëí0, . . . ùëíùëö }, an integer ùëò, and submodular score function ùëì , it computes set of elements ùëÜ with size ùëò with maximized ùëì (ùëÜ). Given the ùúñ-Skyline set DùëÉ ùêπ , and integer ùëò, the diversification of MODis problem aims to compute an ùúñ-Skyline ùëÉ ) is maximized. set Dùêπ Given an instance of diversification of MODis problem at any level ùëñ, we construct an instance of stream submodular maximization problem by setting (1) ùëì = div; (2) ùê∏ = ùê∑ùëñ ùêπ ; (3) integer ùêπ ùëò, and div(ùê∑ùëñ ùëÉ ùëò and div(Dùêπ ùëÉ such that (1) Dùêπ ùêπ as ùê∑ùëÉ compared to other methods. The results reinforce the scalability and practicality of BiMODis across diverse datasets and tasks, including both graph-based and tabular data. Scalability. Fig. 14 presents the scalability test results for ùëá5. With universal graph size of (7925, 34), we performed ùëò-means clustering on edges, setting 5 as the minimum number of clusters, 30 as the maximum, and identifying 13 as the optimal number of clusters based on performance. For node features, we leveraged the graphs structure to reduce the input feature space from 34 to 10 by aggregating attributes from similar types of relations, such as combining multiple training records of an ML model, while preserving all augmented information. Across all settings, methods applied bi-directional search (BiMODis, NOBiMODis, and DivMODis) consistently achieve superior efficiency, handling both increasing attributes and active domain sizes effectively. In contrast, ApxMODis exhibits slower performance as ùê¥ and adom grows, highlighting the scalability of the bi-directional search strategy in managing large and complex graph datasets. Effectiveness. The effectiveness results for ùëá1 and ùëá3 are reported in Table 6, where we select the best results from the Skyline set based on the first metric for each task. These results align with those observed for other tasks, consistently showing that MODis methods outperform baseline approaches in most cases. Notably, NOBiMODis and BiMODis secure the first and second positions across the majority of metrics. Sensitivety analysis. Fig. 15 reports the impact of critical factors including the maximum length of paths, and ùúñ (as in ùúñ-Skyline set) to the accuracy measures. The larger the Percentage Change is, the better the generated Skyline set can improve the prformance of the input model. We found that all the MODis algorithms benefit from larger maximum length and smaller ùúñ in terms of percentage of accuracy improvement. This is consistent with our observation over the tests that report absolute accuracy measures. Moreover, MODis algorithms are relatively more sensitive to the maximum length, compared with the changes to ùúñ. Figure 13: Efficiency Analysis on ùëá5 and ùëá3 Figure 14: Scalability on ùëá5 ùêπ with ratio 1 ùëò is equal to the value of in the instance of diversification of MODis. Correctness.DivMODis approximates ùê∑ùëÉ 4 follows greedy selection and replacement policy that integrates the ùëñ at level ùëñ. DivMODis always \"replace\" strategy given the Dùêπ terminates when no datasets are generated at level ùëñ by procedure UPareto (See lines 1-3 in Fig. 6). Approximation. DùëÉ 4 when terminates at level ùëñ. DivMODis exploits the greedy selection as in [3] but specifies diversification function div(DùëÉ ùêπ ) to maintain the ùúñ-Skyline set of datasets and replaces newly arrived datasets whenever possible. It returns the ùúñ-Skyline set of datasets those corresponding elements in ùê∏ are selected by the instance stream submodular maximization problem. This ensures 1 4 approximability by this consistent construction from the solution for stream submodular maximization. ùêπ approximates DùëÉ ùêπ with ratio The above analysis completes the proof of Lemma 5. Appendix B: Additional Experiments We have performed more complementary experimental studies. Efficiency. Fig. 13 (a, b) evaluates the efficiency of MODis algorithms for taskùëá 5 on generating graph data for the link regression task. The observation is consistent with our findings for their counterparts over tabular data. In particular, BiMODis is quite feasible for generating graph data for GNN-based link regression task, with around 20 seconds in all settings, and consistently outperforms other MODis algorithms. Fig. 13 (c, d) presents the efficiency results for task ùëá3, which involves avocado price prediction. Similar to other tasks, BiMODis demonstrates superior efficiency, maintaining significantly lower search times ùëá1: Movie ùëùùê¥ùëêùëê ùëùùëá ùëüùëéùëñùëõ ùëùùêπ ùë†ùëê ùëùùëÄùêº Output Size ùëá3: Avocado MSE MAE Training Time Output Size Original METAM METAM-MO 0.8560 1.4775 0.0824 0.0538 (3264, 10) 0.8743 1.6276 0.0497 0.0344 (3264, 11) 0.8676 1.1785 0.0801 0.0522 (3264, 11) Original METAM METAM-MO 0.0428 0.1561 0.0280 (9999, 11) 0.0392 0.1497 0.0178 (9999, 12) 0.0312 0.1452 0.0350 (9999, 12) Starmie 0.8606 1.2643 0.1286 0.1072 (3264, 23) Starmie 0.036152 0.145259 0.043600 (9999, 12) SkSFM 0.8285 0.6028 0.7392 0.3921 (3264, 3) SkSFM 0.050903 0.173676 0.008618 (9999, 3) H2O 0.8545 0.9692 0.3110 0.1759 (3264, 8) H2O 0.0442 0.1592 0.0156 (9999, 5) ApxMODis NOBiMODis BiMODis DivMODis 0.9291 0.9947 0.6011 0.4178 (2958, 9) 0.9874 0.8766 0.7202 0.3377 (1980, 12) ApxMODis NOBiMODis 0.029769 0.127916 0.006516 (1589, 10) 0.022821 0.115326 0.003293 (817, 5) 0.9755 0.8027 0.9240 0.3839 (1835, 11) 0.9427 0.8803 0.8010 0.4165 (2176, 10) BiMODis DivMODis 0.027511 0.027511 0.123200 0.123200 0.004366 0.004366 (1310, 9) (1310, 9) Table 6: Comparison of Data Discovery Algorithms in Multi-Objective Setting (ùëá1, ùëá3) Figure 15: Sensitivity Analysis for Parameters on ùëá"
        }
    ],
    "affiliations": [
        "Case Western Reserve University Cleveland, Ohio, USA"
    ]
}