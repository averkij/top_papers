{
    "paper_title": "Generating Skyline Datasets for Data Science Models",
    "authors": [
        "Mengying Wang",
        "Hanchao Ma",
        "Yiyang Bian",
        "Yangxin Fan",
        "Yinghui Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Preparing high-quality datasets required by various data-driven AI and machine learning models has become a cornerstone task in data-driven analysis. Conventional data discovery methods typically integrate datasets towards a single pre-defined quality measure that may lead to bias for downstream tasks. This paper introduces MODis, a framework that discovers datasets by optimizing multiple user-defined, model-performance measures. Given a set of data sources and a model, MODis selects and integrates data sources into a skyline dataset, over which the model is expected to have the desired performance in all the performance measures. We formulate MODis as a multi-goal finite state transducer, and derive three feasible algorithms to generate skyline datasets. Our first algorithm adopts a \"reduce-from-universal\" strategy, that starts with a universal schema and iteratively prunes unpromising data. Our second algorithm further reduces the cost with a bi-directional strategy that interleaves data augmentation and reduction. We also introduce a diversification algorithm to mitigate the bias in skyline datasets. We experimentally verify the efficiency and effectiveness of our skyline data discovery algorithms, and showcase their applications in optimizing data science pipelines."
        },
        {
            "title": "Start",
            "content": "Mengying Wang Case Western Reserve University Cleveland, Ohio, USA mxw767@case.edu Hanchao Ma Case Western Reserve University Cleveland, Ohio, USA hxm382@case.edu Yiyang Bian Case Western Reserve University Cleveland, Ohio, USA yxb227@case.edu Yangxin Fan Case Western Reserve University Cleveland, Ohio, USA yxf451@case.edu Yinghui Wu Case Western Reserve University Cleveland, Ohio, USA yxw1650@case.edu 5 2 0 F 6 1 ] . [ 1 2 6 2 1 1 . 2 0 5 2 : r ABSTRACT Preparing high-quality datasets required by various data-driven AI and machine learning models has become cornerstone task in data-driven analysis. Conventional data discovery methods typically integrate datasets towards single pre-defined quality measure that may lead to bias for downstream tasks. This paper introduces MODis, framework that discovers datasets by optimizing multiple user-defined, model-performance measures. Given set of data sources and model, MODis selects and integrates data sources into skyline dataset, over which the model is expected to have the desired performance in all the performance measures. We formulate MODis as multi-goal finite state transducer, and derive three feasible algorithms to generate skyline datasets. Our first algorithm adopts reducefrom-universal strategy, that starts with universal schema and iteratively prunes unpromising data. Our second algorithm further reduces the cost with bi-directional strategy that interleaves data augmentation and reduction. We also introduce diversification algorithm to mitigate the bias in skyline datasets. We experimentally verify the efficiency and effectiveness of our skyline data discovery algorithms, and showcase their applications in optimizing data science pipelines."
        },
        {
            "title": "1 INTRODUCTION\nHigh-quality machine learning (ML) models have become criti-\ncale assets for various domain sciences research. A routine task\nin data-driven domain sciences is to prepare datasets that can\nbe used to improve such data science models. Data augmenta-\ntion [36] and feature selection [27] have been studied to sug-\ngest data for ML models [7]. Nevertheless, they typically gen-\nerate data by favoring a pre-defined, single performance goal,\nsuch as data completeness or feature importance. Such data may\nbe biased and not very useful to actually improve the model\nperformance, and moreover, fall short at addressing multiple\nuser-defined ML performance measures (e.g., expected accuracy,\ntraining cost). Such need is evident in multi-variable experiment\noptimization [23, 30, 33], feature selection [27], and AI bench-\nmarking [9], among others.",
            "content": "Discovering datasets that can improve model over multiple user-defined performance measures remains to be desirable yet less studied issue. Consider the following real-world example. Example 1: To assess the impact and causes of harmful algal blooms (HABs) in lake, research team aims to forecast the 2025 Copyright held by the owner/author(s). Published in Proceedings of the 28th International Conference on Extending Database Technology (EDBT), 25th March-28th March, 2025, ISBN 978-3-89318-099-8 on OpenProceedings.org. Distribution of this paper is permitted under the terms of the Creative Commons license CC-by-nc-nd 4.0. Figure 1: Data generation for CI index prediction addressing multiple user-defined ML performance criteria, in order to improve an input ML model. chlorophyll-a index (CI-index), key measure of algal blooms. The team has gathered over 50 factors (e.g., fertilizer, water quality, weather) of upstream rivers and watershed systems, and trained random forest (RF) with small, regional dataset. The team wishes to find new data with important spatiotemporal and chemical attributes, to generalize the RF model. In particular, the model is expected to perform well over such dataset in terms of three performance measures: root mean square error (ğ‘…ğ‘€ğ‘†ğ¸), ğ‘…2 test, for Level 2 bloom CI-index, and training time cost. Desirably, the data generation process can inform what are crucial features to inspect, track where the feature values are from, and how they are integrated from the data sources. The research team may issue skyline query [4] that requests: Generate dataset for which our random forest model for predicting Level 2 bloom CI-index is expected to have RMSE below 0.3, ğ‘…2 score at least 0.7, and incur training cost in 5 minutes? Here, the thresholds 0.3, 0.7, and 5 minutes are set based on historical performance of the RF model over data sample. One may apply data integration, or perform feature engineering to refine existing datasets with important features. Nevertheless, these methods often fall short at consistently generate data towards optimizing user-defined ML performance, leaving alone the needs for addressing multiple measures e.g., accuracy and training cost. Another approach is to introduce utility function as linear weighted sum of multiple measures. This turns the need into single objective. However, achieving both high accuracy and low training cost can be conflicting; moreover, best dataset that optimizes such utility function may not necessarily satisfy the expected bounds as posed for each measure in the query. Ideally, data generation process should provide dataset that ensures the model achieves best expected performance on at least one measure, with compromisingly good performance on the rest, and all satisfying the user-defined bounds if any. The above example calls for data generation approaches that can respond to the question by providing skyline datasets that can address multiple ML performance measures. More formally, given query that specifies an input data science model ğ‘€, set of source tables = {ğ·1, . . . ğ·ğ‘› }, and set of user-defined performance measures (e.g., accuracy, training time), our task is to generate new table from D, over which the expected performances of ğ‘€ simultaneously reaches desirable goals for all measures in P. As remarked earlier, traditional data integration and feature engineering with pre-defined, single optimization objective falls short of generating data for such needs. Moreover, desirable data generation process should (1) declaratively produce such data by simple, primitive operators that are well supported by established query engines and data systems, (2) perform data discovery without expensive model inference and validation; and (3) ensure quality guarantees on the resulting skyline dataset, for multiple performance measures. In addition, the generation should be efficient. This remains challenging issue, considering large-scale data sources and the space of new datasets that can be generated from them. Contribution. We introduce MODis, multi-objective data discovery framework. MODis interacts data integration and ML model performance estimation to pursue multi-objective data discovery paradigm. We summarize our contributions as follows. (1) We provide formal computation model for the skyline data generation process in terms of finite state transducer (FST). An FST extends finite automata by associating an output artifact that undergoes modifications via sequences of state transitions. The formal model is equipped with (1) simple and primitive operators, and (2) model performance oracle (Section 3). We use FST as an abstract tool to describe data generation algorithms and perform formal analysis to verify costs and provable quality guarantees. (2) Based on the formal model, we introduce the skyline data generation problem, in terms of Pareto optimality (Section 4) . The goal is to generate skyline set of datasets, ensuring each has at least one performance measure where the models expected performance is no worse than any other dataset. While the problem is intractable, we present fixed-parameter tractable result, for polynomially bounded dataset exploration space from the running graph of an FST process, and fixed measures set P. Based on the above formulation, we provide three feasible algorithms to generate skyline datasets. (3) Our first algorithm provides an approximation on Pareto optimal datasets by exploring and verifying bounded number of datasets that can be generated from data sources (Section 5.1). The algorithm adopts reduce-from-universal strategy to dynamically drop values from universal dataset towards Pareto optimal set of tables. We show that this algorithm approximates Skyline set within factor of (1 + ğœ–) for all performance metric, and ensures exact dominance for at least one measure. In addition, we present special case with fully polynomial time approximation. (4) Our second algorithm further reduces unnecessary computation. It follows bi-directional scheme to prune unpromising data, and leverages correlation analysis of the performance metrics to early terminate the search (Section 5.3). (5) Moreover, we introduce diversification algorithm to mitigate the impact of data bias (Section 5.4). We show that the algorithm achieves 1 4 -approximation to an optimal diversified skyline dataset among all verified (1 + ğœ–) counterparts. Using real benchmark datasets and tasks, we experimentally verify the effectiveness of our data discovery scheme. We found that MODis is practical in use. For example, our algorithms take 30 seconds to generate new data, that can improve input models by 1.5-2 times in accuracy and simultaneously reduces their training cost by 1.7 times. It outperforms baseline approaches that separately performs data integration or feature selection; and remains feasible for larger datasets. Our case study also verified its practical application in domain science tasks. Related works. We categorize related works as follows. Feature Selection. Feature selection removes irrelevant and redundant attributes and identifies important ones for model training [27]. Filtering methods rank features in terms of correlation or mutual information [32, 35] and choose the top ones. They typically assume linear correlation among features, omitting collective effects from feature sets and hence are often limited to support directly optimizing model performance. Our method differs from feature selection in the following. (1) It generates skyline dataset with primitive data augmentation and reduction operators, beyond simply dropping the entire columns. (2) We generate data that improves the model over multiple ML performance measures, beyond retaining critical features; and (3) our method does not require internal knowledge of the models or incur learning overhead. Data Augmentation. Data augmentation aims to create data from multiple data sources towards unified view [7, 11, 36, 48]. It is often specified to improve data completeness and richness [36] and may be sensitive to the quality of schema. Our method aimes to generate data to improve the expected performance of datadriven models. This is different from the conventional data integration which mostly focuses on improving the data completeness. Generative data augmentation [6] synthesize new rows for multi-objective optimization with predefined schema. In contrast, MODis generates data with both rows and columns manipulation. Also, HydraGAN requires target column for each metric, while MODis supports user-defined metrics with configurable generation. Data Discovery. Data discovery aims to prepare datasets for ML models [11, 14, 18, 25, 36]. For example, Kitana [18] computes data profiles (e.g., MinHash) and factorized sketches for each dataset to build join plan, and then evaluates the plan using proxy model. METAM [14] involves the downstream task with utility score for joinable tables. Comparing with prior work, we formalize data generation with cell-level operators, beyond joins. We target multi-objective datasets and provide formalization in terms of Pareto optimality. We also provide algorithms with quality guarantees and optimization techniques. Model Estimation. Model estimation aims to provide accurate estimation of models performance without incurring expensive re-training and inference cost. For example, AutoML [20, 31, 45] train -surrogate models to estimate model performance [20, 31, 45], or predict the model performance by learning from past attempts [13] or Reinforcement Learning [10]. Model selection [42] leverages metadata and historical observations to build graph neural network-based estimator for estimating model performance. Our work leverage Multi-output Gradient Boosting as the surrogate model for fast and reliable estimation, and benefits from established ML performance estimation approaches or other surrogate models."
        },
        {
            "title": "EVALUATION",
            "content": "We start with several notations used in MODis framework. Datasets. dataset ğ· (ğ´1, . . . ğ´ğ‘š) is structured table instance that conforms to local schema ğ‘…ğ· (ğ´1, . . . ğ´ğ‘š). Each tuple ğ‘¡ ğ· is m-ary vector, where ğ‘¡ .ğ´ğ‘– = ğ‘ (ğ‘– [1, ğ‘š]) means the ğ‘–th attribute ğ´ğ‘– of ğ‘¡ is assigned value ğ‘. dataset may have missing values at some attribute ğ´ (i.e., ğ‘¡ .ğ´ = ). Given set of datasets = {ğ·1, . . . ğ·ğ‘› }, each dataset ğ·ğ‘– confirms to local schema ğ‘…ğ‘– . The universal schema ğ‘…ğ‘ˆ is the union of the local schemas of datasets in D, i.e., set of all the attributes involved in D. The active domain of an attribute ğ´ from ğ·ğ‘ˆ , denoted as adom(ğ´), refers to the finite set of its distinct values occurring in D. The size of adom(ğ´), denoted as adom(ğ´), is the number of distinct values of ğ´ in D. Models. data science model (or simply model) is function in the form of ğ‘€ : ğ· Rğ‘‘ , which takes as input dataset ğ·, and outputs result embedding in Rğ‘‘ for some ğ‘‘ N. Here and are real and integer sets. In practice, ğ‘€ can be pretrained machine learning model, statistical model, or simulator. The input ğ· may represent feature matrix (a set of numerical feature vectors), or tensor (from real-world physical systems), to be used for data science model ğ‘€ as training or testing data. The output embedding can be conveniently converted to taskdependent output (e.g., labels for classification, discrete cluster numbers for clustering, or Boolean values for outlier detection) with post-processing. Fixed Deterministic models. We say model ğ‘€ is fixed, if its computation process does not change for fixed input. For example, regression model ğ‘€ is fixed if any factors that determines its inference (e.g., number of layers, learned model weights) remain fixed. The model ğ‘€ is deterministic if it always outputs the same result for the same input. We consider fixed, deterministic models for the needs of consistent performance, which is desired property in ML-driven data analytical tasks. Model Evaluation. performance measure ğ‘ (or simply measure) is performance indicator of model ğ‘€, such as accuracy e.g., precision, recall, F1 score (for classification); or mean average error (for regression analysis). It may also be cost measure such as training time, inference time, or memory consumption. We use the following settings. (1) We unify as set of normalized measures to minimize, with range (0, 1]. Measures to be maximized (e.g., accuracy) can be easily converted to an inversed counterpart (e.g., relative error). (2) Each measure ğ‘ has an optional range [ğ‘ğ‘™ , ğ‘ğ‘¢ ] (0, 1]. It specifies desired lower bound ğ‘ğ‘™ or an upper bound ğ‘ğ‘¢ for model performance, such as maximum training or inference time, memory capacity, or error ranges. Remarks. As we unify as set of measures to be minimized, it is intuitive that an upper bound ğ‘ğ‘¢ specifies tollerence for the estimated performance. We necessarily introduce lower bound ğ‘ğ‘™ > 0 for the convinience of (1) ensuring well-defined theoretical quality guarantees (as will be discussed in Section 5.1), and (2) leaving the option open to users for the configuration needs of downstream tasks such as testing, comparison or benchmarking. Estimators. performance measure ğ‘ can often be efficiently estimated by an estimation model (or simply estimator), in PTIME in terms of ğ· (the number of tuples in ğ·). An estimator makes use of set of historically observed performance of ğ‘€ Symbol D, ğ·, ğ·ğ‘ˆ ğ‘…ğ· , ğ‘…ğ‘ˆ A, ğ´, adom(ğ´) ğ‘€ P, ğ‘, (ğ‘ğ‘™ , ğ‘ğ‘¢ ) ğ‘‡ , ğ‘¡ = (ğ‘€, ğ·, ), ğ‘¡ . = (ğ‘ ğ‘€ , S, O, Sğ¹ , ğ›¿ ) ğ¶ = (ğ‘ ğ‘€ , O, ğ‘€,ğ‘‡ , ) ğºT = ( V, ğ›¿ ) ğ‘  ğ‘ , ğ· ğ· Notation set of datasets, single dataset, universal table local schema of ğ·, and universal schema attribute set, attribute, and active domain data science model ğ· Rğ‘‘ perform. measures, measure, its range test set; single test, its performance vector data discovery system performance estimation model configuration of data discovery system running graph state dominance, dataset dominance Table 1: Table of notations (denoted as ğ‘‡ ) to infer its performance over new dataset. It can be regression model that learns from historical tuning records ğ‘‡ to predict the performance of ğ‘€ given new dataset ğ·. By default, we use multi-output Gradient Boosting Model [34] that allows us to obtain the performance vector by single call with high accuracy (see Section 6). Tests. Given model ğ‘€ and dataset ğ·, test ğ‘¡ is triple (ğ‘€, ğ·, P), which specifies test dataset ğ·, an input model ğ‘€, and set of user-defined measures = {ğ‘1, . . . ğ‘ğ‘™ }. test tuple ğ‘¡ = (ğ‘€, ğ·, P) is valuated by an estimator if each of its measure ğ‘ is assigned (estimated) value by E. Example 2: Consider Example 1. pre-trained random forest (RF) model ğ‘€ that predicts CI-index is evaluated by three measures = {RMSE, R2, Ttrain}, which specifies the root mean square error, the ğ‘…2 score, and the training cost. user specifies desired normalized range of RMSE to be within (0, 0.6], R2 in [0, 0.35] w.r.t. inversed lower bound 1 0.65, and Ttrain in (0, 0.5] w.r.t. an upper bound of 3600 seconds (i.e., no more than 1800 seconds). We summarize the main notations in Table 1."
        },
        {
            "title": "FORMALIZATION",
            "content": "Given datasets D, an input model ğ‘€ and set of measures P, we formalize the generation process of skyline dataset with multi-goals finite state transducer (FST). An FST extends extends finite automata by associating outputs with transitions. We use FST to abstract and characterize the generation of Skyline datasets as data transformation process. We introduce this formalization, with counterpart for data integration [7, 26], to help us characterize the computation of skyline dataset generation. Data Generator. skyline dataset generator is finite-state transducer, denoted as = (ğ‘ ğ‘€, S, O, Sğ¹ , ğ›¿), where (1) is set of states, (2) ğ‘ ğ‘€ is designated start state, (3) is set of operators of types {, }; (4) Sğ¹ is set of output states; and (5) ğ›¿ refers to set of transitions. We next specify its components. States. state ğ‘  specifies table ğ·ğ‘  that conforms to schema ğ‘…ğ‘  and active domains adomğ‘  . For each attribute ğ´ ğ‘…ğ‘  , adomğ‘  (ğ´) adom(ğ´) refers to fraction of values ğ´ can take at state ğ‘ . adomğ‘  (ğ´) can be set as empty set , which indicates that the attribute ğ´ is not involved for training or testing ğ‘€; or wildcard _ (dont care), which indicates that ğ´ can take any value in adom(ğ´). Operators. skyline data generator adopts two primitive polynomial-time computable operators, Augment and Reduct. ğ·ğ‘  , for simplicity, we shall use single general term output, denoted as Dğ¹ , to refer to output states or datasets. Running graph. running of can be naturally represented as the dynamic generation of running graph ğº = (V, ğ›¿), which is directed acyclic graph (DAG) with set of state nodes V, and set of transition edges ğ‘Ÿ = (ğ‘ , ğ‘œğ‘, ğ‘ ). path of length ğ‘˜ is sequence of ğ‘˜ transitions ğœŒ = {ğ‘Ÿ1, . . . ğ‘Ÿğ‘˜ } such that for any ğ‘Ÿğ‘– = (ğ‘ ğ‘–, ğ‘œğ‘, ğ‘ ğ‘–+1), ğ‘Ÿğ‘–+1 = (ğ‘ ğ‘–+1, ğ‘œğ‘, ğ‘ ğ‘–+2); i.e., it depicts sequence of transitions that converts an initial state ğ‘ 1 with dataset ğ·1 to result ğ‘ ğ‘˜ with ğ·ğ‘˜ . Example 3: Following Example 1, Fig 2 shows fraction of running graph with input set D= {ğ·ğ‘¤, ğ·ğ‘, ğ·ğ‘ , ğ·ğ‘ƒ } (water, basin, nitrogen, and phosphorus tables, respectively). The augmentation uses spatial joins [38], common query that join tables with tuple-level spatial similarity. With configuration ğ¶ = (ğ‘ ğ‘€, RF, {ğ‘…ğ‘€ğ‘†ğ¸, ğ‘…2,ğ‘‡ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› }, E}) (where is an MO-GBM estimator), running starts by joining ğ·ğ‘¤ and ğ·ğ‘ to get ğ·2. ğ·2 is then augmented with the attribute Phosphorus under literal year = 2013, resulting in ğ·3 via path {1, . . . , 3}. In each step, test ğ‘¡ is initialized; and the estimator is consulted to valuate the performance vector of ğ‘¡, and enrich ğ‘‡ . Consider another path from ğ·ğ‘€ that results dataset ğ·5 in Fig. 1. It first augmentes ğ·ğ‘€ to ğ·4 with data in Spring. reduction with condition year<2003 selects and removes all the tuples in ğ·4 with historical data before 2003, which leads to dataset ğ·5 that retains only the data since 2003."
        },
        {
            "title": "4 SKYLINE DATA GENERATION PROBLEM\nGiven T and a configuration ğ¶, MODis aims find a running of\nT that ideally leads to a â€œglobalâ€ optimal dataset, where ğ‘€ is\nexpected to deliver the highest performance over all metrics.\nNevertheless, a single optimal solution may not always exist.\nFirst, two measures in P may in nature conflict due to trade-\noffs (e.g., training cost versus accuracy, precision versus recall).\nMoreover, the â€œno free lunchâ€ theorem [39] indicates that there\nmay not exist a single test that demonstrate best performance\nover all measures. We thus pursue Pareto optimality for Dğ¹ . We\nstart with a dominance relation below.\nDominance. Given a data discovery system T and performance\nmeasures P, a state ğ‘  = (ğ·ğ‘ , ğ‘…ğ‘ , adomğ‘  ) is dominated by ğ‘ â€² =\n(ğ·ğ‘  â€², ğ‘…ğ‘  â€², adomğ‘  â€² ), denoted as ğ‘  â‰º ğ‘ â€², if there are valuated tests ğ‘¡\n= (ğ‘€, ğ·ğ‘  ) and ğ‘¡ â€² = (ğ‘€, ğ·ğ‘  â€² ) in ğ‘‡ , such that",
            "content": "for each ğ‘ P, ğ‘¡ .ğ‘ ğ‘¡ .ğ‘; and there exists measure ğ‘ P, such that ğ‘¡ .ğ‘ < ğ‘¡ .ğ‘. dataset ğ·ğ‘  is dominated by ğ·ğ‘  , denoted as ğ·ğ‘  ğ·ğ‘  , if ğ‘  ğ‘ . Skyline set. Given and configuration ğ¶, let Dğ¹ be the set of all the possible output datasets from running of , set of datasets ğ¹ Dğ¹ is skyline set w.r.t. and ğ¶, if for any dataset ğ· ğ¹ , and any performance measure ğ‘ P, there exists test ğ‘¡ ğ‘‡ , such that ğ‘¡ .ğ‘ [ğ‘ğ‘™ , ğ‘ğ‘¢ ]; there is no pair {ğ·1, ğ·2} ğ¹ such that ğ·1 ğ·2 or ğ·2 ğ·1; and for any other ğ· Dğ¹ ğ¹ , and any ğ· ğ¹ , ğ· ğ·. We next formulate the skyline data generation problem. Skyline Data Generation. Given skyline data generator and its configuration ğ¶ = (ğ‘ ğ‘€, O, ğ‘€,ğ‘‡ , E), the skyline data generation problem is to compute skyline set Dğ¹ in terms of and ğ¶. Figure 2: skyline data generation process, with part of running graphs, and result datasets. These operators can be expressed by SPJ (select, project, join) queries, or implemented as user-defined functions (UDFs). (1) Augment has general form of ğ‘ (ğ·ğ‘€, ğ·), which augments dataset ğ·ğ‘€ with another ğ· subject to literal ğ‘. Here ğ‘ is literal in form of ğ´ = ğ‘ (an equality condition). An augmentation ğ‘ (ğ·ğ‘€, ğ·) executes the following queries: (a) augment schema ğ‘…ğ‘€ of ğ·ğ‘€ with attribute ğ´ from schema ğ‘…ğ· of ğ·, if ğ´ ğ‘…ğ‘€ ; (b) augment ğ·ğ‘€ with tuples from ğ· satisfying constraint ğ‘; and (c) fill the rest cells with null for unknown values. (2) Reduct ğ‘ (ğ·ğ‘€ ): this operator(a) selects from ğ·ğ‘€ the tuples that satisfy the selection condition posed by the literal ğ¶ posed on attribute ğ‘…ğ‘€ .ğ´; and (b) removes all such tuples from ğ·ğ‘€ . Here ğ‘ is single literal defined on ğ‘…ğ‘€ .ğ´ as in (1). Transitions. transition ğ‘Ÿ = (ğ‘ , ğ‘œğ‘, ğ‘ ) is triple that specifies state ğ‘  = (ğ·, ğ‘…, adomğ‘  ), an operator op over ğ‘ , and result state ğ‘  = (ğ·, ğ‘…, adom ğ‘  ), where ğ· and ğ‘… are obtained by applying op over ğ· and ğ‘… conforming to domain constraint adom ğ‘  , respectively. In practice, the operators can be enriched by task-specific UDFs that perform additional data imputation, or pruning operations, to further improve the quality of datasets. Running. configuration of , denoted as ğ¶ = (ğ‘ ğ‘€, O, ğ‘€,ğ‘‡ , E), initializes start state ğ‘ ğ‘€ with dataset ğ·ğ‘€ , finite set of operators O, fixed deterministic model ğ‘€, an estimator E, and test set ğ‘‡ , where each test ğ‘¡ ğ‘‡ has valuated performance vector (ğ‘¡). Both ğ·ğ‘€ and ğ‘‡ can be empty set . running of w.r.t. configuration ğ¶ = (ğ‘ ğ‘€, ğ‘€,ğ‘‡ , E) follows general, deterministic process below. (1) Starting from ğ‘ ğ‘€ , and at each state ğ‘ , iteratively applies operators from to update table with new attributes and tuples or mask its tuple values. This spawns set of child states. (2) For each transition ğ‘Ÿ = (ğ‘ , op, ğ‘ ) spawned from ğ‘  with result ğ‘  by applying op, (a) initializes test tuple ğ‘¡ (ğ‘€, ğ·ğ‘  , P) if ğ‘¡ and invokes estimator at runtime to valuate the performance vector of ğ‘¡; or (b) if ğ‘¡ is already in ğ‘‡ , it directly loads ğ‘¡ .P. Consistently, we say state node ğ‘  is valuated, if corresponding test ğ‘¡ (ğ‘€, ğ·ğ‘ , P) is valuated by E. We denote its evaluated performance vector as ğ‘ .P. The above process terminates at set of output states Sğ¹ , under an external termination condition, or no transition can be spawned (no new datasets can be generated with O). The result of running refers to the set of corresponding datasets Dğ¹ induced from the output states Sğ¹ . As each output state ğ‘  uniquely determines corresponding output dataset Example 4: Revisiting prior example and consider the temporal results Dğ¹ = {ğ·1, . . . , ğ·5} with the following performance vectors valuated by the estimator so far: RMSE T: (D, M, P, E) ğ‘¡1 : (ğ·1, RF, P, MO GBM) 0.48 0.41 ğ‘¡2 : (ğ·2, RF, P, MO GBM) 0.26 ğ‘¡3 : (ğ·3, RF, P, MO GBM) 0.37 ğ‘¡4 : (ğ·4, RF, P, MO GBM) 0.25 ğ‘¡5 : (ğ·5, RF, P, MO GBM) ğ‘‡ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› 0.37 0.37 0.37 0.39 0.35 Ë†ğ‘…2 0.33 0.24 0.15 0.22 0.18 Here Ë†ğ‘…2 is inversed as 1-ğ‘…2: the smaller, the better. All the measures are normalized in (0, 1] w.r.t. user-specified upper and lower bounds, and the optimal values are underlined. One can verify the following dominance relation among the datasets: (1) ğ·1 ğ·2 ğ·3, and ğ·4 ğ·5; (2) ğ·3 ğ·5 and vice versa. Hence Skyline set Dğ¹ currently contains {ğ·3, ğ·5}. We present the following hardness result. Theorem 1. Skyline data generation is (1) NP-hard; and (2) fixed-parameter tractable, if (a) is fixed, and (b) Dğ¹ is polynomially bounded by the input size D. Proof sketch: The NP-hardness can be verified by reduction from the Multiobjective Shortest Path problem (MOSP). Given an edge-weighted graph ğºğ‘¤, where each edge ğ‘’ğ‘¤ has ğ‘‘-dimensional cost vector ğ‘’ğ‘¤ .ğ‘, the cost of path ğœŒğ‘¤ in ğºğ‘¤ is defined as ğœŒğ‘¤ .ğ‘ = (cid:205)ğ‘’ğ‘¤ ğœŒğ‘¤ ğ‘’ğ‘¤ .ğ‘. The dominance relation between paths is determined by comparing their costs. MOSP is to compute Skyline set of paths from start node ğ‘  to target node ğ‘¡. Given an instance of MOSP, we construct an instance of our problem as follows. (1) We assign an arbitrally ordered index to the edges of ğºğ‘¤, say ğ‘’1, . . . ğ‘’ğ‘›. (2) We initialize configuration as follows. (a) ğ‘ ğ‘€ has single dataset ğ·0, where for each edge ğ‘’ğ‘– = (ğ‘£, ğ‘£ ), there is distinct tuple ğ‘¡ğ‘– ğ·0. (b) contains set of reduction operators, where each operator ğ‘œğ‘– removes tuple ğ‘¡ğ‘– from ğ·0, and incurs pre-defined performance measure ğ‘’ğ‘– .ğ‘. (c) ğ‘€ maps each tuple ğ‘¡ğ‘– in ğ·0 to fixed embedding in Rğ‘‘ . (d) The test set ğ‘‡ is . We enforce the running graph of to be the input ğºğ‘¤, by setting the initial state as ğ‘  with associated dataset ğ·0, unique termination state as the node ğ‘¡, and the applicable transitions as the edges in ğºğ‘¤. One can verify that solution of MOSP is Pareto set of paths from ğ‘  to ğ‘¡, each results in dataset by sequentially applying the reduction operators following the edges of the path. This yields set of datasets that constitutes corresponding skyline set Dğ¹ as solution for our problem. As MOSP is shown to be NP-hard [16, 37], the hardness of skyline data generation follows. To see the fixed-parameter tractability, we outline an exact algorithm. (1) The algorithm exhausts the runnings of skyline generator , and invokes PTIME inference process of the model ğ‘€ and valuate at most ğ‘ Dğ¹ possible states (datasets). (2) It invokes multi-objective optimizer such as Kungs algorithm [24]. This incurs ğ‘‚ (ğ‘ log ğ‘ ) 2 valuations when 4, or ğ‘‚ (ğ‘ (log ğ‘ )) if < 4. As ğ‘ Dğ¹ , and Dğ¹ is in ğ‘‚ (ğ· ), and ğ‘ƒ is fixed constant, the overall cost is in PTIME (see [43] for details). While the above exact algorithm is able to compute skyline dataset, it remains infeasible even when enlisting ğ‘ states as once-for-all cost is affordable. Moreover, solution may contain an excessive number of datasets to be inspected. We next present three feasible algorithms, that generate datasets that approximate skyline sets with bounded size and quality guarantees."
        },
        {
            "title": "5.1 Approximating Skyline Sets\nWe next present our first algorithm that generates a size-bounded\nset, which approximates a Skyline set in Dğ¹ . To characterize the\napproximation quality, we introduce a notion of ğœ–-skyline set.\nğœ–-Skyline set. Given a data discovery system T with a configu-\nration ğ¶, Let DS be a set of ğ‘ valuated datasets in the running\nof T . Given a pair of datasets (ğ·, ğ·â€²) from DS, and a constant\nğœ– > 0, we say ğ·â€² ğœ–-dominates ğ·, denoted as ğ·â€² âª°ğœ– ğ·, if for the\ncorresponding tests ğ‘¡ = (ğ‘€, ğ·) and ğ‘¡ â€² = (ğ‘€, ğ·â€²),\nâ€¢ ğ‘¡ â€².ğ‘ â‰¤ (1 + ğœ–)ğ‘¡ .ğ‘ for each ğ‘ âˆˆ P, and\nâ€¢ there exists a measure ğ‘âˆ— âˆˆ P, such that ğ‘¡ â€².ğ‘âˆ— â‰¤ ğ‘¡ .ğ‘âˆ—.\nIn particular, we call ğ‘âˆ— a decisive measure. Note that ğ‘âˆ— can",
            "content": "be any ğ‘ and may not be fixed. set of datasets Dğœ– DS is an ğœ–-Skyline set of DS, if for any dataset ğ· Dğœ– , and any performance measure ğ‘ P, there exists corresponding test ğ‘¡ ğ‘‡ , such that ğ‘¡ .ğ‘ [ğ‘ğ‘™ , ğ‘ğ‘¢ ]; and for every dataset ğ· DS, there exists dataset ğ· Dğœ– such that ğ· ğœ– ğ·. (ğ‘ , ğœ–)-approximation. We say an algorithm is an (ğ‘ , ğœ–)- approximation for MODis, if it satisfies the following: it explores and valuates at most ğ‘ states; for any constant ğœ– > 0, the system correctly outputs an ğœ–-Skyline set, as an approximation of Skyline set defined over ğ‘ valuated states; and the time cost is polynomial determined by , ğ‘ , and 1 ğœ– . Below we present our main result. (cid:18) min(ğ‘ ğ‘…ğ‘¢ Theorem 1. Given datasets D, configuration ğ¶, and number ğ‘ , there exists an (ğ‘ , ğœ–)-approximation for MODis in (cid:17) 1 time, where ğ‘…ğ‘¢ is the ğ‘‚ size of the universal schema, ğ‘ğ‘¢ = ğ‘…ğ‘¢ + adomğ‘š (adomğ‘š the largest active domain), ğ‘ğ‘š = max ğ‘ğ‘¢ ğ‘ğ‘™ as the measure ğ‘ ranges over P; and ğ¼ is the unit valuation cost per test. (cid:18)(cid:16) log(ğ‘ğ‘š ) ğœ– , ğ‘ ) + ğ¼ (cid:19) (cid:19) ğ‘¢ Remarks. The above result captures relative guarantee w.r.t. ğœ– and ğ‘ . When ğ‘ = Dğ¹ , an (ğ‘ , ğœ–)-approximation ensures to output ğœ–-Skyline set. The paradigm is feasible as one can explicitly trade the closeness of the output to Skyline set with affordable time cost, by explicitly tuning ğœ– and ğ‘ . Moreover, the worst-case factor adomğ‘š can also be tightened by bound determined by the value constraints posed by the literals. For example, an attribute ğ´ may contribute up to two necessary values in the search if the literals involving ğ´ only enforce two equality conditions A=2 and A=5, regardless of how large adom(ğ´) is (see Sections 3 and 6)."
        },
        {
            "title": "5.2 Approximation Algorithm\nAs a constructive proof of Theorem 1, we next present an (ğ‘ , ğœ–)-\napproximation algorithm, denoted as ApxMODis.\nâ€œReduce-from-Universalâ€. Algorithm ApxMODis simulates the\nrunning of T from a start state ğ‘ ğ‘ˆ . The start state is initialized\nwith a â€œuniversalâ€ dataset ğ·ğ‘ˆ , which carries the universal schema\nğ‘…ğ‘ˆ , and is populated by joining all the tables (with outer join to\npreserve all the values besides common attributes, by default).\nThis is to enforce the search from a set of rows that preserve all\nthe attribute values as much as possible to maximize the chance of",
            "content": "Algorithm 1 :ApxMODis 1: Input: Configuration ğ¶ = (ğ‘ ğ‘ˆ , O, ğ‘€,ğ‘‡ , E), number ğ‘ , 2: constant ğœ– > 0, decisive measure ğ‘ğ‘‘ ; user-specified upper bound ğ‘ğ‘¢ for ğ‘ P; 3: Output: ğœ–-Skyline set Dğ¹ . 4: Queue ğ‘„ := , integer ğ‘– := 0, ğ‘„.enqueue((ğ‘ ğ‘ˆ , 0)); 5: while ğ‘„ and number of valuated states < ğ‘ do 6: ğ‘–+1 := Dğ¹ (ğ‘ , i) := ğ‘„.dequeue(); Dğ¹ for each (ğ‘ , ğ·ğ‘  ) OpGen (ğ‘ ) do ğ‘– ; 7: 8: 9: ğ‘–+1 := UPareto (ğ‘ , Dğ¹ ğ‘„.enqueue((ğ‘ , + 1)); Dğ¹ 10: return Dğ¹ 11: procedure OpGen(s) set ğ‘„ := ; 12: for each entry ğ‘™ ğ‘ .ğ¿ do 13: ğ‘–+1, ğœ–); if ğ‘™ = 1 then ğ‘™ := 0; create new state ğ‘ ; ğ‘ .ğ¿ := ğ‘ .ğ¿; generate dataset ğ·ğ‘  accordingly; ğ‘„ .append((ğ‘ , ğ·ğ‘  )); return ğ‘„ 20: procedure UPareto(ğ‘ , Dğ¹ 21: ğ‘–+1, ğœ–) update ğ‘ .P with estimator E; for each ğ‘ do if ğ‘ .P (ğ‘) > ğ‘ğ‘¢ then return Dğ¹ ğ‘–+1; update pos(ğ‘ ) with Equation (1); retrieve state ğ‘  where pos(ğ‘ ) = pos(ğ‘ ); if no such ğ‘  exists then ğ‘–+1 {ğ· ğ‘–+1 := Dğ¹ Dğ¹ ğ‘  }; else if ğ‘ .P (ğ‘ğ‘‘ ) < ğ‘ .P (ğ‘ğ‘‘ ) then ğ‘–+1 {ğ·ğ‘  } {ğ·ğ‘  }; Dğ¹ return Dğ¹ ğ‘–+1 := Dğ¹ ğ‘–+1 14: 15: 16: 17: 18: 19: 22: 23: 24: 25: 26: 27: 28: 29: 30: Figure 3: ApxMODis: Approximating Skyline sets sequential applications of reduction only. It transforms state dominance into path dominance counterpart. For transition edge (ğ‘ , , ğ‘ ), weight is assigned to quantify the gap between the estimated model performance over datasets ğ·ğ‘  and its reduced counterpart ğ· ğ‘  . The length of path from ğ‘ ğ‘ˆ to ğ‘  aggregates the edge weights towards the estimated performance of its result. Advantage. We justify the reduce-from-universal strategy in the following context. (1) As the measures are to be minimized, we can extend shortest paths by prioritizing the valuation of datasets towards user-defined upper bounds with early pruning, to avoid unnecessary reduction. (2) Starting from universal dataset allows early exploration of dense datasets, over which the model always tends to have higher accuracy in practice. We next present the details of our algorithm. Auxiliary structure. ApxMODis follows dynamic levelwise state generation and valuation process, which yields running graph ğº with up to ğ‘ nodes. It maintains the following. (1) ğ‘„ is queue that maintains the dynamically generated and valuated state nodes. Each entry of ğ‘„ is pair (ğ‘ , ğ‘–) that records state and the level it resides. (2) Dğ¹ is list of datasets. Dğ‘– ğ¹ specifies the datasets processed at level ğ‘–. Each state node ğ‘  is associate with bitmap ğ¿ to encode if Figure 4: Reduct-from-Universal: an illustration of twolevel computation. It performs multiple level-wise spawns and updates the ğœ–-Skyline set. its schema ğ‘ .ğ‘…ğ‘  contains an attribute ğ´ in ğ·ğ‘ˆ , and if ğ·ğ‘  contains value from its active domain adom(ğ´). The map is consulted to assert the applicability of reduct operators at runtime. (3) Each state ğ‘  is associated with position ğ‘ğ‘œğ‘  (ğ‘ ) in discretized 1-ary space, which is defined as pos(ğ‘  ) = (cid:34) (cid:22) log1+ğœ– (cid:23) ğ‘ . (ğ‘1 ) ğ‘ğ‘™1 (cid:36) , . . . , log1+ğœ– (cid:37) (cid:35) ğ‘ . (ğ‘P 1 ) ğ‘ğ‘™P 1 (1) By default, we set the last measure in as decisive measure. We remark that one can choose any measure as decisive measure, and our results carry over. Algorithm. The algorithm ApxMODis is illustrated in Fig. 3. It initializes queue ğ‘„ with start state ğ‘ ğ‘ˆ , and set position to ğ‘ ğ‘ˆ (line 4). In lines 5 to 9, it update the Skyline set Dğ¹ for each level iteratively. At level ğ‘‘, for each state ğ‘  ğ‘„, procedure OpGen (line 12 to 19) explores all one-flip transitions in ğ‘ .ğ¿ and generates set with applicable reduct operators. ApxMODis enqueue new ğ‘‘+1 at next level accordingly states and update the Skyline set Dğ¹ by invoking Procedure UPareto. This process terminations until ğ‘ states are valuated, or no new state can be generated. Procedure UPareto. Given new state ğ‘ , procedure UPareto determines if ğ‘  should be included in the current Skyline set. (1) It updates ğ‘ .P by consulting the estimator (line 1), and decide an early skipping if its performance fails to satisfy the upperbound ğ‘ğ‘¢ for some measure ğ‘ P. (2) Otherwise, UPareto updates the position of state ğ‘ , and decides if ğ‘  replaces valuated state ğ‘  at the same position due to (1 + ğœ–)-dominance (lines 6-9). The Skyline set at current level is updated accordingly with dataset ğ·ğ‘  . Example 5: Fig. 4 illustrates data discovery of ApxMODis with ğ‘ =5 and ğœ–=0.3, over table set = {ğ·1, . . . , ğ·3} and measures = < ğ‘1, ğ‘2 >. The operator set ğ‘‚ contains four reduct operators {1, . . . 4}. (1) It first constructs universal dataset ğ·ğ‘ˆ with universal schema ğ‘…ğ‘ˆ . ğ·ğ‘ˆ can be obtained by optimized multi-way join [46], augmentation [28], or UDFs [8]. The bitmap ğ·ğ‘ˆ .ğ¿ is initialized accordingly. Procedure OpGen then generates applicable reductions by flipping the entries in ğ·ğ‘ˆ .ğ¿. This spawns states ğ‘ 1 and ğ‘ 2 obtained by applying reduct 1 and 2, respectively It then consults the estimator to valuate model performances, and identified that ğ·1 0.3 ğ·2 and vice versa. Thus ApxMODis sets the current 0.3-Skyline set as {ğ·1, ğ·2}. ApxMODis next spawns states with applicable reductions 3 and 4, extending ğœŒ1 that leads to ğ‘ 1, and ğœŒ2 that leads to ğ‘ 2, This generates new extended paths ğœŒ3 and ğœŒ4 with results ğ·3 and ğ·4, respectively. It verfies that D3 0.3 ğ·1, but ğ·2 0.3 ğ·4; and ğ·2 0.3 ğ·3 and vice versa. This yields an updated 0.3-Skyline set {ğ·2, ğ·3}, after valuating 5 states. Correctness & Approximability. ApxMODis terminates as it spawns ğ‘ nodes with at most ğ‘…ğ‘ˆ adomğ‘š distinct reduction, where adomğ‘š refers to the size of the largest active domain. For approximability, we present the result below. Lemma 2. For any constant ğœ–, ApxMODis correctly computes an ğœ–-Skyline set Dğ¹ as an approximated Skyline set defined on the ğ‘ states it valuated. Proof sketch: We verify the ğœ–-approximability, with reduction to the multi-objective shortest path problem (MOSP) [40]. Given an edge-weighted graph ğºğ‘¤, where each edge carries ğ‘‘-dimensional attribute vector ğ‘’ğ‘¤ .ğ‘, it computes Skyline set of paths from start node ğ‘¢. The cost of path ğœŒğ‘¤ in ğºğ‘¤ is defined as ğœŒğ‘¤ .ğ‘ = (cid:205)ğ‘’ğ‘¤ ğœŒğ‘¤ ğ‘’ğ‘¤ .ğ‘. The dominance relation between two paths is determined by the dominance relation of their cost. Our reduction (1) constructs ğºğ‘¤ as the running graph ğº with ğ‘ valuated state nodes and spawned transition edges; and (2) for each edge (ğ‘ , ğ‘ ), sets an edge weight as ğ‘’ğ‘¤ = ğ‘ .P ğ‘ .P. Given solution Î ğ‘¤ of the above instance of MOSP, for each path ğœŒğ‘¤ Î , we set corresponding path ğœŒ in ğº with result dataset ğ·, and adds it into Dğ¹ . We can verify that Î ğ‘¤ is an ğœ–-Skyline set of paths Î ğ‘¤, if and only if Dğ¹ is an ğœ–-Skyline set of Dğ‘† that contains the datasets from the set of ğ‘ valuated states in ğº . We then show that ApxMODis is an optimized process of the algorithm in [40], which correctly computes Î ğ‘¤ for ğºğ‘¤. Time cost. Let ğ‘…ğ‘¢ be the total number of attributes in the universal schema ğ‘…ğ‘¢ of ğ·ğ‘¢ , and adomğ‘š be the size of the largest active domain. ApxMODis performs ğ‘…ğ‘¢ levels of spawning, and at each node, it spawns at most ğ‘…ğ‘¢ + adomğ‘š children, given that it flips one attribute for each reduction, and for each attribute, at most one domain value to mask. Let ğ‘ğ‘¢ be ğ‘…ğ‘¢ +adomğ‘š . Thus, ApxMODis valuates at most min(ğ‘ ğ‘…ğ‘¢ , ğ‘ ) nodes (datasets), taking ğ¼ min(ğ‘ ğ‘…ğ‘¢ , ğ‘ ) time, where ğ¼ refers to polynomial time valuation cost of per test. For each node, (cid:16) (cid:106)log1+ğœ– it then takes at most (cid:206) 1 ) time to update the ğœ–-Skyline set. Given ğœ– is small, log(1 + ğœ–) ğœ–, and the total + 1(cid:17) ğ‘ğ‘¢ğ‘– ğ‘ğ‘™ğ‘– ğ‘–=1 ğ‘¢ ğ‘¢ (cid:107) cost is in ğ‘‚ min(ğ‘ ğ‘…ğ‘¢ ğ‘¢ , ğ‘ ) (cid:18) (cid:18)(cid:16) log(ğ‘ğ‘š ) ğœ– (cid:17) + ğ¼ (cid:19)(cid:19) time. Given ğœ– . Theorem 1 thus follows. ğ‘…ğ‘¢ and are small constants, the cost is polynomial in the input size ğ·ğ‘¢ , ğ‘ and 1 An FPTAS case. We next present case when ApxMODis ensures stronger optimality guarantee. We say an (ğ‘ , ğœ–)- approximation is fully polynomial time approximation (FPTAS) for MODis, if (1) it computes an ğœ–-Skyline set for Dğ‘† , where Dğ‘† refers to all possible datasets that can be generated from ğ·ğ‘ˆ , and (2) it runs in time polynomial in the size of ğ·ğ‘ˆ and 1 ğœ– . Lemma 3. Given skyline dataset generator with configuration ğ¶, if Dğ‘† has size that is polynomially bounded in ğ‘‚ (ğ‘“ (ğ·ğ‘ˆ )), then ApxMODis is an FPTAS for MODis. Proof sketch: We show this by reduction from MODis to MOSP, similarly as in the approximability analysis. MOSP is known to have fully polynomial time approximable (FPTAS) in Algorithm 2 :BiMODis 1: Input: Configuration ğ¶ = (ğ‘ ğ‘ˆ , O, ğ‘€,ğ‘‡ , E), constant ğœ– > 0; 2: Output: ğœ–-Skyline set Dğ¹ . 3: set ğ‘ ğ‘ = BackSt(ğ‘ ğ‘ˆ ); queue ğ‘„ ğ‘“ := {(ğ‘ ğ‘ˆ , 0)}, ğ‘„ğ‘ := {(ğ‘ ğ‘, 0)}; integer ğ‘– := 0; 4: while ğ‘„ ğ‘“ , ğ‘„ğ‘ and ğ‘„ ğ‘“ ğ‘„ğ‘ = do 5: (ğ‘ , ğ‘–) = ğ‘„ ğ‘“ .dequeue(); (ğ‘ , ğ‘–) = ğ‘„ğ‘ .dequeue(); Dğ¹ for all ğ‘  ğ‘“ OpGen (ğ‘ ) and ğ‘ ğ‘ OpGen (ğ‘ ) do ğ‘–+1 = Dğ¹ Forward Serach Backward Serach ğ‘– ; ğ‘–+1 = UPareto (ğ‘  ğ‘“ , Dğ¹ Dğ¹ ğ‘–+1 = UPareto (ğ‘ ğ‘ , Dğ¹ Dğ¹ if canPrune(ğ‘  ğ‘“ , ğ‘ ğ‘ ) then prune (C, ğ‘  ğ‘“ , ğ‘ ğ‘ ); ğ‘–+1, Dğ¹ ğ‘–+1, Dğ¹ ğ‘– , ğœ–); ğ‘– , ğœ–); ğ‘„ ğ‘“ .enqueue((ğ‘  ğ‘“ , ğ‘– + 1)), ğ‘„ğ‘ .enqueue((ğ‘ ğ‘ , ğ‘– + 1)); 6: 7: 8: 9: 10: 11: 12: 13: 14: return Dğ¹ Figure 5: BiMODis: Bi-directional Search terms of ğœ–-dominance. We set ApxMODis to run as (Dğ‘† , ğœ–)- approximation, which is simplified implementation of an FPTAS in [40] with multiple rounds of replacement strategy following path dominance. As Dğ‘† is bounded by polynomial of the input size ğ·ğ‘ˆ , it approximates Skyline set for all in PTME. The size bound of Dğ‘† is pragmatic and practical due to that the attributes often bear active domains that are much smaller than dataset size. Indeed, data science applications typically consider data with values under task-specific constraints. These suggest practical application of ApxMODis with affordable setting of ğ‘ and ğœ–. We present the detailed analysis in [43]."
        },
        {
            "title": "5.3 Bi-Directional Skyline Set Generation\nGiven our cost analysis, for skyline data generation with larger\n(more â€œtolerateâ€) ranges (ğ‘ğ‘™ , ğ‘ğ‘¢ ) and larger |D |, ApxMODis may\nstill need to valuate a large number of datasets. To further reduce\nvaluation cost, we introduce BiMODis, its bi-directional variant.\nOur idea is to interact both augment and reduct operators, with a\nâ€œforwardâ€ search from universal dataset, and a â€œbackwardâ€ coun-\nterpart from a single dataset in D. We also introduce a pruning\nstrategy based on an early detection of dominance relation.\nAlgorithm. Algorithm BiMODis, as shown in Fig. 5, has the fol-\nlowing steps. (1) Initialization (lines 3). It first invokes a pro-\ncedure BackSt to initialize a back-end start state node ğ‘ ğ‘ . Two\nqueues ğ‘„ ğ‘“ and ğ‘„ğ‘ are initialized, seeded with start state ğ‘ ğ‘ˆ for\nforward search, and a back state ğ‘ ğ‘ for backward search, respec-\ntively. They serve as the forward and backward frontiers, respec-\ntively. (2) Bi-directional Search (lines 4-13). BiMODis conducts an\nexploration from both directions, controlled by ğ‘„ ğ‘“ for forward\nsearch, and ğ‘„ğ‘ for backward search. Similar to ApxMODis, a Sky-\nline set Dğ¹ is maintained in a levelwise manner. The difference\nis that it invokes a revised procedure OpGen (with original coun-\nterpart in ApxMODis in Fig. 3), which generates reduct operators\nfor the forward search, and augment operators for the backward\nsearch. The search process terminates when both ğ‘„ ğ‘“ and ğ‘„ğ‘ are\nempty, or when a path is formed, the result Dğ¹ is returned.\nProcedure BackSt. This procedure initializes a backend dataset ğ·ğ‘\nfor augmentation. This procedure can be tailored to the specific\ntask. For example, for a classifier ğ‘€ with input features and a",
            "content": "target attribute ğ´ to be classified, we sample small (minimal) set of tuples in ğ·ğ‘ˆ to ğ·ğ‘ that covers all values of the active domain adom of ğ´, to ensure that no classes will be missed in dataset ğ·ğ‘ . Other task-specific strategies can also be applied here. To reduce the valuation cost, BiMODis leverages correlation analysis over historical performance ğ‘‡ , to assert non-ğœ–dominance early, without full valuation of their measures P. Correlation-Based Pruning. At runtime, BiMODis dynamically maintains correlation graph ğº C, where each node represents measure in P, and there is an edge (ğ‘ğ‘–, ğ‘ ğ‘— ) in ğº if ğ‘ğ‘– and ğ‘ ğ‘— are strongly correlated, with an associated weight corr(ğ‘ğ‘–, ğ‘ ğ‘— ) [47]. Here we say two measures are strongly correlated, if their Spearman correlation coefficient corr(ğ‘ğ‘–, ğ‘ ğ‘— ) ğœƒ , given their value distribution in the current set of tests ğ‘‡ , for user-defined threshold ğœƒ . ğº is dynamically updated, as more valuated tests are added to ğ‘‡ . Parameterized Dominance. BiMODis also parameterize any unvaluated measures in the performance vector ğ‘ .P of state ğ‘  with potential range [ Ë†ğ‘ğ‘™ , Ë†ğ‘ğ‘¢ ] [ğ‘ğ‘™ , ğ‘ğ‘¢ ]. This range is derived from the valuated measures that are most strongly correlated, by consulting ğº and test sets ğ‘‡ . The entire vector ğ‘ .P is incrementally updated, for each ğ‘ P, by setting (1) ğ‘ .P (ğ‘) as ğ‘¡ .ğ‘ (valuated), if there is corresponding test ğ‘¡ = (ğ‘€, ğ·ğ‘  ) ğ‘‡ with ğ‘¡ .ğ‘ valuated; or (2) ğ‘ .P (ğ‘) as variable with an estimated range [ğ‘ . Ë†ğ‘ğ‘™ , ğ‘ . Ë†ğ‘ğ‘¢ ], if no test over ğ‘ of ğ·ğ‘  is valuated. state ğ‘  is parameterized ğœ–-dominated by another state ğ‘ , denoted as ğ‘  ğœ– ğ‘ , if for each ğ‘ P, ğ‘ .P (ğ‘) (1 + ğœ–)ğ‘ .P (ğ‘), if both are valuated; ğ‘ . Ë†ğ‘ğ‘¢ (1 + ğœ–)ğ‘ . Ë†ğ‘ğ‘™ , if neither is valuated; or ğ‘ .P (ğ‘) (1 + ğœ–)ğ‘ . Ë†ğ‘ğ‘™ (resp. ğ‘ . Ë†ğ‘ğ‘¢ (1 + ğœ–)ğ‘ .P (ğ‘)), if ğ‘ .P (ğ‘) (resp. ğ‘ .P (ğ‘)) is valuated but ğ‘ .P (ğ‘) (resp. ğ‘ .P (ğ‘)) is not. Based on the above construction, BiMODis monitors monotonicity condition as follows. Monotonicity Condition. Given the current test set ğ‘‡ , we say state ğ‘  (resp. ğ‘ ) with performance measure ğ‘ at path ğœŒ has monotonicity property, if for any state ğ‘  reachable from ğ‘  (resp. can reach ğ‘ ) via ğœŒ, ğ‘ . Ë†ğ‘ğ‘¢ < ğ‘  . Ë†ğ‘ğ‘™ 1+ğœ– (resp. ğ‘ . Ë†ğ‘ğ‘¢ < ğ‘  . Ë†ğ‘ğ‘™ 1+ğœ– ). Given two states ğ‘  and ğ‘ , where ğ‘  ğœ– ğ‘ , state ğ‘  on path ğœŒ from ğ‘  or to ğ‘  can be pruned by Correlation-based Pruning, if for every ğ‘ P, ğ‘  has ğ‘ at ğœŒ with monotonicity property w.r.t. ğ‘  (resp. ğ‘ ). We present the following pruning rule. Lemma 4. Let ğ‘  ğ‘„ ğ‘“ and ğ‘  ğ‘„ğ‘ . If ğ‘  ğœ– ğ‘ , then for any state node ğ‘  on path from ğ‘  or to ğ‘  that can be pruned by CorrelationBased Pruning, ğ·ğ‘  is not in any ğœ–-Skyline set of the datasets that can be generated from valuated states. Proof sketch: We verify the pruning rule with case study of ğ‘  and ğ‘ , subject to the monotonicity property. Case 1: Both ğ‘ .P (ğ‘) and ğ‘ .P (ğ‘) are valuated. If ğ‘  ğœ– ğ‘ , then by definition, ğ‘ .P (ğ‘) (1 + ğœ–)ğ‘ .P (ğ‘) for all ğ‘ P. This readily leads to ğœ–dominance, i.e., ğ‘  ğœ– ğ‘ . As ğ‘  has every performance measures ğ‘ with monotonicity property w.r.t. ğ‘ , ğ‘  ğœ– ğ‘ . Hence ğ‘  can be safely pruned without valuation. Case 2: Neither ğ‘ .P (ğ‘) nor ğ‘ .P (ğ‘) is valuated. By definition, as ğ‘  ğœ– ğ‘ , then for every ğ‘ P, ğ‘ . Ë†ğ‘ğ‘¢ (1 + ğœ–)ğ‘ . Ë†ğ‘ğ‘™ . Given that ğ‘  has every performance measures ğ‘ with monotonicity property w.r.t. ğ‘ , then by definition, for each ğ‘ P, we have ğ‘ .ğ‘ ğ‘ . Ë†ğ‘ğ‘¢ ğ‘  . Ë†ğ‘ğ‘™ (1 + ğœ–)ğ‘ . Ë†ğ‘ğ‘™ (1 + ğœ–)ğ‘ . Ë†ğ‘ğ‘¢ < (1 + ğœ–) 1+ğœ– ğ‘ .ğ‘, for every ğ‘ P. By definition of state dominance, ğ‘  ğ‘ , for unevaluated ğ‘ . Following similar proof, one can infer that ğ‘  ğ‘  for state ğ‘  in the forward front of BiMODis. Hence ğ‘  can be safely pruned. Case 3: One of ğ‘ .P (ğ‘) or ğ‘ .P (ğ‘) is valuated. Given that ğ‘  ğœ– ğ‘ , we have (a) ğ‘ .P (ğ‘) (1 + ğœ–)ğ‘ . Ë†ğ‘ğ‘™ , if only ğ‘ .P (ğ‘) is valuated; or (b) ğ‘ . Ë†ğ‘ğ‘¢ (1 + ğœ–)ğ‘ .P (ğ‘), if only ğ‘ .P (ğ‘) is valuated. Consider case 3(a). As ğ‘  can reach ğ‘  via path ğœŒ, and ğ‘  satisfiies the pruning condition, we can infer that ğ‘ .P (ğ‘) (1 + ğœ–)ğ‘ . Ë†ğ‘ğ‘™ ğ‘  . Ë†ğ‘ğ‘™ (1 + ğœ–)ğ‘ . Ë†ğ‘ğ‘¢ < (1 + ğœ–) 1+ğœ– ğ‘ .ğ‘, hence ğ‘  ğ‘ . Similarly for case 3(b), we can infer that ğ‘ . Ë†ğ‘ğ‘¢ (1 + ğœ–)ğ‘ .ğ‘ (1 + ğœ–)ğ‘ . Ë†ğ‘ğ‘¢ < ğ‘  . Ë†ğ‘ğ‘™ 1+ğœ– ğ‘ .ğ‘. hence ğ‘  ğ‘ . For both cases, ğ‘  can be (1 + ğœ–) pruned without evaluation. Lemma 4 hence follows. Procedures canPrune and prune (lines 11-12; omitted) asserts the Correlation-Based Pruning condition, and perform the maintenance of ğº C, ğ‘‡ and other auxiliary structures, respectively. Note that the above rule is checkable in PTIME w.r.t. input size Dğ‘† . When Dğ‘† is large, one can generate path with its states unevaluated, and check at runtime if the condition holds between evaluated states in the forward and backward frontier. We present the detailed analysis in [43]. Example 6: We illustrate Correlation-Based Pruning in the figure below. From left to right, it depicts set of test records ğ‘…, the correlation graph ğº C, and part of the running graph ğº . ğº is constructed from ğ‘‡ with measures as nodes and Spearman correlations as edge weights. For each ğ‘ ğ‘› ğº , the associated ğ‘ Pğ‘ ğ‘› is obtained by test ğ‘¡ğ‘ ğ‘› = (ğ‘€, ğ·ğ‘ ğ‘› ). Label (1, 1, 1, 1) (1, 1, 1, 0) (1, 0, 0, 1) (0, 1, 0, 0) (0, 0, 0, 0) ğ‘1 0.42 0.4 0.5 0.45 0. ğ‘2 0.18 0.17 0.22 / 0.4 ğ‘3 0.9 0.1 / / 0.3 ğ‘ ğ‘ˆ ğ‘ 1 ğ‘ 2 ğ‘ 3 ğ‘ ğ‘ At ğœƒ = 0.8, ğ‘1 and ğ‘2 are positively correlated with each other and negatively correlated with ğ· , so Pğ‘˜ = {ğ‘1, ğ‘2}. From ğ‘ ğ‘ˆ and ğ‘ ğ‘ , the forward and backward frontiers derive states ğ‘ 1 and ğ‘ 3, respectively. To estimate ğ‘ 3.P (ğ‘2), note that ğ‘ 3.P (ğ‘1) = 0.45, which lies between ğ‘ ğ‘ˆ .P (ğ‘1) = 0.42 and ğ‘ 2.P (ğ‘1) = 0.5. Given the strong correlation between ğ‘1 and ğ‘2, we infer ğ‘ 3.P (ğ‘2) to be within the interval [0.18, 0.22], with Ë†ğ‘ğ‘™2 = ğ‘ ğ‘ˆ .P (ğ‘2) and Ë†ğ‘ğ‘¢2 = ğ‘ 2.P (ğ‘2). With ğœ– = 0.3, we find ğ‘ 3 ğœ– ğ‘ 1 because 0.45 (1 + 0.3) 0.4 and 0.22 (1 + 0.3) 0.17. For intermediate states ğ‘ 4 (bitmap entry (1, 1, 0, 0)) and ğ‘ 5 (bitmap entry (0, 1, 1, 0)), which are not recorded in ğ‘‡ and have ğ·ğ‘ 4 = ğ·ğ‘ 5 = 2, similar inference process shows they fall within the bounds set by [ğ‘ 1.P, ğ‘ 3.P]. As result, ğ‘ 4 and ğ‘ 5 can be pruned. Time Cost. BiMODis takes the same time complexity as ApxMODis. The (ğ‘ , ğœ–)-approximation holds for BiMODis given that it correctly updates the ğœ–-Skyline set by definition. Our experimental study verifies that it is much faster in practice and particularly suitable for larger ğœ– or search spaces (represented by maximum path length). It also scales more efficiently for large datasets (see Exp-3 in Section 6)."
        },
        {
            "title": "5.4 Diversified Skyline Dataset Generation\nA Skyline dataset may still contain data that largely overlap or\nare similar, hence leading to bias and reducing the generality of\nthe model if adopted. This may occur due to skewed value distri-\nbution in the active domains, common attributes, over specific\nperformance metrics in the skyline data generation process. It is",
            "content": "Algorithm 3 :Diversification step at level ğ‘– 1: Input: ğœ–-Skyline set Dğ¹ 2: Output: diversified ğ‘˜-subset of Dğ¹ ğ‘– (from UPareto), integer ğ‘˜; ğ‘– (to be passed to level ğ‘– + 1). ğ‘– ğ‘˜ then return Dğ¹ ğ‘– ğ¹ with ğ‘˜ random dataset in Dğ¹ ğ‘– ; 3: if Dğ¹ 4: initialize Dğ‘ƒ 5: score := ğ‘‘ğ‘–ğ‘£ (Dğ‘ƒ 6: for all ğ· Dğ‘ƒ 7: ğ‘– do ğ¹ ); ğ¹ do for all ğ· Dğ¹ if ğ· Dğ‘ƒ Dğ‘ƒ ğ¹ := (Dğ‘ƒ score := ğ‘‘ğ‘–ğ‘£ (Dğ‘ƒ ğ¹ ) if score > score then ğ¹ := Dğ‘ƒ Dğ‘ƒ ğ¹ then Continue; ğ¹ {ğ· }) {ğ·}; ğ¹ , score := score; 8: 9: 10: 11: 12: 13: return Dğ‘ƒ ğ¹ Figure 6: Level-wise diversification of DivMODis often desirable to explore diversified variant of skyline set generation to create varied datasets that mitigate such bias [23, 30]. Given and configuration ğ¶, set of datasets D, constants ğ‘ , ğœ– and ğ‘˜, the diversified skyline data generation is to compute set ğ¹ is an ğœ–-Skyline set of ğ‘ valuated states by an (ğ‘ , ğœ–)-approximation of MODis, and (2) among all ğœ–-Skyline sets over ğ‘ states valuated in (1), it maximizes diversification score defined as: ğ¹ of at most ğ‘˜ tables, such that (1) div(Dğ¹ ) = ğ‘˜ 1 ğ‘˜ ğ‘–=1 ğ‘—=ğ‘–+1 dis(ğ·ğ‘–, ğ· ğ‘— ) (2) where distance function dis quantifies the difference of datasets in terms of both value distributions and estimated performance, and is defined as: dis(ğ·ğ‘–, ğ· ğ‘— ) = ğ›¼ 1 cos(ğ‘ ğ‘– .ğ¿, ğ‘  ğ‘— .ğ¿) 2 + (1 ğ›¼) euc(ğ‘¡ğ‘– .P, ğ‘¡ ğ‘— .P) eucm We adopt Cosine similarity cos and Euclid Distance (euc). The latter is normalized by the maximum Euclid Distance ğ‘’ğ‘¢ğ‘ğ‘š among the historical performances in ğ‘‡ . We next outline an algorithm, denoted as DivMODis, that extends an (ğ‘ , ğœ–)-approximation to computes an diversified ğœ–-Skyline set Dğ¹ of at most ğ‘˜ datasets. Algorithm. DivMODis revises MODis by incrementally diversify ğ‘– at level ğ‘– (partially shown in Fig.6). It an input ğœ–-Skyline set Dğ¹ derives Dğ‘ƒ ğ¹ by greedy selection and replace strategy as follows. (1) It initializes Dğ‘ƒ ğ‘– , and updates ğ¹ as random ğ‘˜-set from Dğ¹ Dğ‘ƒ ğ¹ by incrementally replacing tables with the highest marginal gain in diversification, hence an improved ğ‘‘ğ‘–ğ‘£ (Dğ‘ƒ ğ¹ is passed to be processed at level ğ‘– + 1, upon the arrival of new states. DivMODis returns the diversified set Dğ¹ , following the same termination condition as in ApxMODis. ğ¹ ). (2) Dğ‘ƒ We show that the diversified MODis can be approximated, for submodular diversification function div. Our result holds for the specification of div in Equation 2. Lemma 5. Given ğ‘ and ğœ–, DivMODis achieves 1 4 approximation for diversified MODis, i.e., (1) it correctly computes ğœ–-Skyline set ğ·ğ‘ƒ ğ¹ over ğ‘ valuated datasets, and (2) div(ğ·ğ‘ƒ ğ¹ ) 4 div(D ). Dataset Sets Kaggle OpenData HF # tables 1943 2457 255 # Columns 33573 71416 # Rows 7317K 33296K 10207K Table 2: Characteristics of Datasets preserving reduction to the stream submodular maximization problem [3]. Given stream ğ¸ = {ğ‘’0, . . . ğ‘’ğ‘š }, an integer ğ‘˜, and submodular diversification function ğ‘“ , it computes ğ‘˜-set of elements ğ‘† that can maximize ğ‘“ (ğ‘†). Our reduction constructs stream of datasets Dğ‘† following the level-wise generation. We show that the function div is submodular function. (2) By integrating greedy selection and replacement policy, DivMODis keeps ğ‘˜-set with the most diverse and representative datasets to mitigate the biases in the Skyline set. DivMODis achieves 1 4 -approximation of an ğœ–-Skyline set with maximized diversity at each level ğ‘–. Please see the detailed proof in [43]. ğ‘¢ ğ‘¢ Analysis. DivMODis incurs an overhead to update the diversified ğ‘˜-set. As MODis valuates up to min(ğ‘ ğ‘…ğ‘¢ , ğ‘ ) nodes (datasets), the total additional overhead is in ğ‘‚ (min(ğ‘ ğ‘…ğ‘¢ , ğ‘ ) ğ‘˜ ğ‘‡S), where ğ‘‡S refers to the unit valuation cost for single table, which is in PTIME. As both ğ‘˜ and ğ‘‡S are relatively small, the practical overhead for DivMODis remains small (see Sec. 6). Remarks. Alternatives that solve multi-objective optimization may be applied, such as evolutionary algorithms such as NSGAII [5], or reinforcement-learning based methods [29]. The former rely on costly stochastic processes (e.g., mutation and crossover) and may require extensive parameter tuning. The latter are effective for general state exploration but require high-quality training samples and may not converge over conflicting measures. In contrast, MODis is training and tuning free. Our experiments verified that ApxMODis provides early generation of high-quality datasets from few large input datasets, due to reduce-from-Universal strategy, BiMODis enhances efficiency through bidirectional exploration and pruning, hence benefits for larger number of small-scale datasets, and DivMODis benefits most for datasets with skewed distribution."
        },
        {
            "title": "6 EXPERIMENT STUDY\nWe next experimentally verify the efficiency and effectiveness of\nour algorithms. We aim to answer three questions: RQ1: How\nwell can our algorithms improve the performance of models\nin multiple measures? RQ2: What is the impact of generation\nsettings, such as data size? RQ3: How fast can they generate\nskyline sets, and how scalable are they? We also illustrate the\napplications of our approaches with case studies1.\nDatasets. We use three sets of tabular datasets: kaggle [21],\nOpenData [1], and HF [19] (summarized in Table 2).\nTasks and Models. A set of tasks are assigned for evaluation.\nWe trained: (1) a Gradient Boosting model (GBmovie) to predict\nmovie grosses using Kaggle for Task ğ‘‡1; (2) a Random Forest\nmodel (RFhouse) to classify house prices using OpenData with\nthe same settings in [14] for Task ğ‘‡2; and (3) a Logistic Regression\nmodel (LRavocado) to predict Avocado prices using HF for Task\nğ‘‡3. (4) a LightGBM model (LGCmental) [22] to classify mental\nhealth status using Kaggle for Task ğ‘‡4. We also introduced task\nğ‘‡ 5, a link regression task for recommendation. This task takes as\ninput a bipartite graph between users and products, and links in-\ndicate their interaction. A LightGCN [17] (LGRmodel), a variant",
            "content": "Proof sketch: We show an induction on the levels. (1) We verify the guarantee at single level, by constructing an approximation 1Our codes and datasets are available at github.com/wang-mengying/modis Notation ğ‘ğ´ğ‘ğ‘ ğ‘ğ‘‡ ğ‘Ÿ ğ‘ğ¹ 1 ğ‘ğ´ğ‘ˆ ğ¶ ğ‘ğ‘ ğ‘ (ğ‘›) ğ‘ğ‘€ğ´ğ¸ , ğ‘ğ‘€ğ‘†ğ¸ ğ‘ğ‘ƒğ‘ (ğ‘›) , ğ‘ğ‘…ğ‘ (ğ‘›) ğ‘ğ¹ ğ‘ ğ‘ ğ‘ğ‘€ğ¼ Measures Model Accuracy Training Time Cost ğ¹1 score Area under the curve NDCG(@n) Mean Absolute / Squared Error Precision(@n), Recall(@n) Fisher Score [27] Mutual Information [14, 27] Used In P1, P2, P4 P1-P4 P2, P4 P4 P5 P3 P5 P1, P2 P1, Table 3: Performance Measures of graph neural networks (GNN) optimized for fast graph learning, is trained to predict top-ğ‘˜ missing edges in an input bipartite graph to suggest products to users. set of 1873 bipartite graphs is constructed from Kaggle for ğ‘‡5. The augment (resp. reduct) operators are defined as edge insertions (resp. edge deletions) to transform bipartite graph to another. We use the same training scripts for each task and all methods for fair comparison. We assigned measures P1 through P5 for tasks ğ‘‡1 to ğ‘‡5, respectively , as summarized in Table 3. We also report the size of the data (ğ‘ğ·ğ‘†ğ‘–ğ‘§ğ‘’ ) in terms of (total # of rows total # of columns), excluding attributes with all cells masked. Estimator E. We adopt MO-GBM [34] as desired model performance estimator. It outperforms other candidate models even with simple training set For example, for ğ‘‡1, MO-GBM performs inference for all objectives on one state in at most 0.2 seconds, with small MSE of 0.0003 when predicting Accuracy. Algorithms. We implemented the following methods. (1) MODis: Our multi-objective data discovery algorithms, including ApxMODis, BiMODis, and DivMODis. We also implemented NOBiMODis, counterpart of BiMODis without correlation-based pruning. (2) METAM [14]: goal-oriented data discovery algorithm that optimizes single utility score with consecutive joins of tables. We also implemented an extension METAM-MO, by incorporating multiple measures into single linear weighted utility function. (3) Starmie [12]: data discovery method that focuses on table-union search and uses contrastive learning to identify joinable tables. For METAM and Starmie, we used the code from original papers. (4) SkSFM [34]: An automated feature selection method in scikit-learns SelectFromModel, which recommends important features with built-in estimator. (5) H2O [15]: an AutoML platform; we used its feature selection module, which fits features and predictors into linear model. Construction of ğ·ğ‘ˆ and Operators. To prepare universal datasets ğ·ğ‘ˆ for MODis, we preprocess Kaggle, OpenData and HF into joinable tables and construct ğ·ğ‘ˆ with multi-way joins. This results in ğ·ğ‘ˆ datasets with size (in terms of # of columns and # of rows): (12, 3732), (27, 1178), (13, 18249) and (20, 140700), for tasks ğ‘‡1 to ğ‘‡4, respectively. Specifically, we applied ğ‘˜-means clustering over the active domain of each attribute (with maximum ğ‘˜ set as 30), and derived equality literals, one for each cluster. We then compressed the input tables by replacing rows into tuple clusters, reducing the number of rows. This pragmatically help us avoid starting from large ğ·ğ‘ˆ by only retaining the values of interests, and still yield desired skyline datasets. For ğ‘‡5, large bipartite graph is constructed with size of (7925, 34) (# of edges, # of nodes features). The generation of graphs consistently aligns with its table data counterpart, by conveniently replacing augment and reduction to their graph counterpart that performs link insertions and deletions. Figure 7: Effectiveness: Multiple Measures Evaluation metrics. We adopt the following metrics to quantify the effectiveness of data discovery approaches. Denote ğ·ğ‘€ as an initial dataset, and Dğ‘œ set of output datasets from data discovery algorithm. (1) We define the relative improvement rImp(ğ‘) for given measure ğ‘ achieved by method as ğ‘€ (ğ·ğ‘€ ).ğ‘ . As ğ‘€ (ğ·ğ‘œ ).ğ‘ all metrics are normalized to be minimized, the larger rImp(ğ‘) is, the better ğ·ğ‘ is in improving ğ‘€ w.r.t. ğ‘. Here ğ‘€ (ğ·ğ‘€ ).ğ‘ and ğ‘€ (ğ·ğ‘ ).ğ‘ are obtained by actual model inference test. This allows us to fairly compare all methods in terms of the quality of data suggestion. For efficiency, we compare the time cost of data discovery upon receiving given model or task as query. Exp-1: Effectiveness. We first evaluate MODis methods over five tasks. Results for ğ‘‡1 and ğ‘‡3 are shown in Fig. 7 (the outer, the better). While results for ğ‘‡2 and ğ‘‡4 are presented in Table 4. Results for ğ‘‡5 are in Table 5. We also report the model performance over the input tables as yardstick (Original) for all methods. As all baselines output single table, to compare MODis algorithms, we select the table in the Skyline set with the best estimated ğ‘ğ´ğ‘ğ‘ , ğ‘ƒğ¹ 1, ğ‘ƒğ‘€ğ‘†ğ¸ , ğ‘ğ´ğ‘ğ‘ and ğ‘ğ‘ƒğ‘5 for ğ‘‡1 to ğ‘‡5, respectively. As METAM optimizes single utility score, we choose the same measure for each task as the utility. We apply model inference to all the output tables to report actual performance values. We have the following observations. (1) MODis algorithms outperform all the baselines in all tasks. As shown in Table 4, for example, for ğ‘‡4, the datasets that bear best ğ‘ğ´ğ‘ğ‘ and the second best are returned by ApxMODis (0.9535) and BiMODis (0.9525), respectively, and all MODis methods generated datasets that achieve 0.87 on ğ‘ğ¹ 1 in ğ‘‡2. (2) Over the same dataset and for other measures, MODis algorithms outperform the baselines in most cases. For example, in ğ‘‡1, the result datasets that most optimize ğ‘ğ¹ğ‘ ğ‘ and ğ‘ğ‘€ğ¼ are obtained by BiMODis and ApxMODis, respectively; also in ğ‘‡2 and ğ‘‡3, NOBiMODis and BiMODis show absolute dominance in most measures. Table 5 also verifies that MODis easily generalizes to suggest graph data for GNN-based analytics, beyond tabular data. (3) Methods with data augmentation (e.g.,METAM and Starmie) enriches data to improve model accuracy, at cost of training time, while feature selection methods (e.g.,SkSFM and H2O) reduce data at the cost of accuracy with improved training efficiency. MODis methods are able to balance these trade-offs better by explicitly performing multi-objective optimization. For example, ğ‘ğ´ğ‘ğ‘ and ğ‘ğ‘‡ ğ‘Ÿğ‘ğ‘–ğ‘› in ğ‘‡4, The best result for training cost (0.2359s) is contributed from SkSFM, yet at cost of lowest model accuracy (0.8839). We also compared ğ‘ğ´ğ‘ğ‘ on ğ‘‡4 with HydraGAN, generative data augmentation method, which achieves 0.9355 with 330 rows ğ‘‡2: House ğ‘ğ¹ 1 ğ‘ğ´ğ‘ğ‘ ğ‘ğ‘‡ ğ‘Ÿğ‘ğ‘–ğ‘› ğ‘ğ¹ğ‘ ğ‘ ğ‘ğ‘€ğ¼ Output Size ğ‘‡4: Mental ğ‘ğ´ğ‘ğ‘ ğ‘ğ‘ƒğ‘ ğ‘ğ‘…ğ‘ ğ‘ğ¹ 1 ğ‘ğ´ğ‘ˆ ğ¶ ğ‘ğ‘‡ ğ‘Ÿğ‘ğ‘–ğ‘› Output Size ğ‘‡5: Model ğ‘ğ‘ƒğ‘5 ğ‘ğ‘ƒğ‘10 ğ‘ğ‘…ğ‘5 ğ‘ğ‘…ğ‘10 ğ‘ğ‘ ğ‘5 ğ‘ğ‘ ğ‘10 Output Size Original METAM METAM-MO 0.8288 0.8305 0.2000 0.0928 0.126 (1178, 27) 0.8310 0.8333 0.19 0.0894 0.1207 (1178, 28) 0.8510 0.8322 0.21 0.0889 0.1109 (1178, 28) Original METAM METAM-MO 0.9222 0.7940 0.7722 0.7829 0.9618 0.4098 (105, 14) 0.9468 0.7991 0.7846 0.7918 0.9757 0.3198 (105, 15) 0.9462 0.8070 0.7959 0.8014 0.9774 0.4027 (105, 15) Starmie 0.8351 0.8331 0.2100 0.0149 0.0243 (1178, 32) Starmie 0.9505 0.8106 0.8030 0.8068 0.9784 0.3333 (105, 16) SkSFM 0.7825 0.7826 0.2000 0. 0.2970 (1178, 4) SkSFM 0.8839 0.6577 0.7523 0.7018 0.9326 0.2359 (105 8) H2O 0.8333 0.8305 0.2000 0.0691 0.1054 (1178, 15) H2O 0.9236 0.7892 0.7879 0.7885 0.9615 0.2530 (105, 8) ApxMODis NOBiMODis 0.9044 0.9050 0.1533 0.2268 0.2039 (835, 17) 0.9125 0.9121 0.1519 0.2610 0.2018 (797, 17) ApxMODis NOBiMODis 0.9532 0.8577 0.8097 0.8330 0.9792 0.3327 (128332, 16) 0.9471 0.8454 0.8092 0.8269 0.9755 0.2818 (116048, 16) BiMODis 0.9125 0.9121 0.1519 0.2610 0.2018 (797, 17) BiMODis 0.9525 0.8549 0.8075 0.8305 0.9789 0.3201 (128332, 17) DivMODis 0.8732 0.8729 0.2128 0.2223 0.3164 (1129, 5) DivMODis 0.9471 0.8454 0.8092 0.8269 0.9755 0.2818 (116048, 16) Table 4: Comparison of Data Discovery Algorithms in Multi-Objective Setting (ğ‘‡2, ğ‘‡4) 0.8200 0. Original ApxMODis NOMODis BiMODis DivMODis 0.8000 0.7200 0.8000 0.6600 0.2022 0.1863 0.3816 0.3217 0.7875 0.6923 0.7891 0.6646 (1966, 6) (7925, 0) 0.8000 0.8000 0.2022 0.3816 0.7875 0.7891 (1966, 6) 0.8200 0.8200 0.2072 0.3977 0.7924 0.7935 0.7976 (5826, 30) 0.8033 (2869, 4) 0.2072 0. Table 5: Comparison of MODis Methods on ğ‘‡5 Figure 8: Effectiveness: Impact of Factors but fell short of data discovery methods. Increasing the number of rows further reduced performance, reflecting the limitations of generative approaches in this context, which cannot utilize verified external data sources, and synthetic data often lacks inherent reliability and contextual relevance of discovered data. Exp-2: Impact factors. We next investigate the MODis methods under the impact of two factors: ğœ– and the maximum path length (maxl), as well as the impact of ğ›¼ on DivMODis. Varying ğœ–. Fixing maxl = 6, we varied ğœ– from 0.5 to 0.1 for ğ‘‡1. As shown in Fig. 8(a), MODis algorithms are able to improve the model in ğ‘ğ‘ğ‘ğ‘ better with smaller ğœ–, as they all ensure to output ğœ–-Skyline set that better approximate Skyline set when ğœ– is set to be smaller. In all cases, they achieve relative improvement rImp(ğ‘ğ´ğ‘ğ‘ ) at least 1.07. BiMODis and NOBiMODis perform better in recognizing better solutions from both ends in reduction and augmentation as smaller ğœ– is enforced. ApxMODis, with reduction only, is less sensitive to the change of ğœ– due to that larger ğœ– may trap it to local optimal sets from one end. Adding diversification (DivMODis) is able to strike balance between ApxMODis and BiMODis by enforcing to choose difference datasets out of local optimal sets, thus improving ApxMODis for smaller ğœ–. We choose smaller range of ğœ– for ğ‘‡2 in Fig. 8(c), as the variance of ğ‘ğ¹ 1 is small. As ğœ– varies from 0.1 to 0.02, NOBiMODis improves F1 score from 0.84 to 0.91. Varying maxl. Fixing ğœ– = 0.1, we varied maxl from 2 to 6. Fig. 8(b, d) tells us that all MODis algorithms improve the task performance for more rounds of processing. Specifically, BiMODis and NOBiMODis benefit most as bi-directional search allows both to find better solution from wider search space as maxl becomes larger. ApxMODis is less sensitive, as the reduction strategy from dense tables incurs smaller loss in accuracy. DivMODis finds datasets that ensure best model accuracy when maxl = 5, yet may lose chance to maintain the accuracy, due to that the diversification step may update the Skyline set with less optimal but more different counterparts in future levels (e.g., when maxl = 6). Varying ğ›¼ in DivMODis. We demonstrate the effectiveness of DivMODis by adjusting ğ›¼. smaller ğ›¼ prioritizes performance, while larger ğ›¼ emphasizes content diversity, measured by hamming distance. Fig. 9(a) illustrates Performance Diversity, where smaller ğ›¼ results in wider accuracy range with balanced and stable distribution. Both the mean and median remain centered. As ğ›¼ increases, the accuracy distribution narrows and shifts toward higher values, reflecting the dominance of high-accuracy datasets in the Skyline set. Fig. 9(b) verifies the impact of Content Diversity, visualized as the percentage contribution of each adom. Larger ğ›¼ leads to more evenly distributed contributions. The standard deviation values above the heatmap quantify this trend, showing consistent decrease as ğ›¼ increases, indicating improved balance. Exp-3: Efficiency and Scalibility. We next report the the efficiency of MODis algorithms for task ğ‘‡1 and ğ‘‡3 over Kaggle and HF, respectively, and the impact of factors ğœ– and maxl. We also evaluate their scalability for ğ‘‡1 and ğ‘‡5 in terms of input size. Efficiency: Varying ğœ–. Fixing maxl = 6 and varying ğœ– from 0.1 to 0.5, Fig. 10 (a) verifies the following. (1) BiMODis, NOBiMODis and DivMODis take less time as ğœ– increases, as larger ğœ– provides more chance to prune unnecessary valuations. DivMODis has comparable performance with NOBiMODis, as it mainly Figure 9: Impact of ğ›¼ for DivMODis Figure 10: Efficiency and Scalabilitiy benefits from the bi-directional strategy, which exploits early pruning and stream-style placement strategy. (2) As shown in Fig. 10(a), for ğ‘‡1, BiMODis, NOBiMODis, and DivMODis are 2.5, 2, and 2 times faster than ApxMODis on average, respectively. ApxMODis takes longer time to explore larger universal table with reduct operators. It is insensitive to ğœ–. We observe that its search from the data rich end may converge faster at high-quality ğœ–-Skyline sets. Efficiency: Varying maxl. Fixing ğœ– = 0.2 for task ğ‘‡1 and ğœ– = 0.1 for task ğ‘‡3, we varied maxl from 2 to 6, all MODis algorithms take longer as maxl increases, as shown in Fig. 10 (b). Indeed, larger maxl results in more states to be valuated, and more nonğœ–-dominance relation to be resolved. ApxMODis is sensitive to maxl due to the rapid growth of the search space. In contrast, BiMODis mitigates the impact with bi-directional strategy and effective pruning. Scalability. We varied the number of total attributes ğ´ and size of the largest active domain adom. We perform ğ‘˜-means clustering over the tuples of the universal table with ğ‘˜ = adom, and extended operators with range queries to control adom. Fig. 10 (c) and (d) show that all MODis algorithms take more time for larger ğ´ and adom. BiMODis scales best due to bi-directional strategy. DivMODis remains more efficient than ApxMODis, indicating affordable overhead from diversification. While our algorithms scale well with ğ´ and adom, highdimensional datasets may present challenges due to the search space growth. Dimensionality reduction such as PCA or feature selection, or correlation-based pruning (to identify and eliminate highly correlated or redundant features), can be tailored to specific tasks to mitigate these challenges. Figure 11: Case 1 (left): Discover Datasets for Materials Peak Classification Analysis. Case 2 (right): Test Data Generation for Model Performance Benchmarking Exp-4: Case study. We next report two real-world case studies to illustrate the application scenarios of MODis. (1) Find data with models. material science team trained random forest-based classifier to identify peaks in 2D X-ray diffraction data. They seek more datasets to improve the models accuracy, training cost, and F1 score for downstream fine-tuning. Original X-ray datasets and models are uploaded to crowdsourced X-ray data platform we deployed [44] with best performance of < 0.6435, 3.2, 0.77 >. Within available X-ray datasets, BiMODis created three datasets {ğ·1, ğ·2, ğ·3} and achieved the best performance of 0.987, 2.88, and 0.91, respectively. We set METAM to optimize F1-score, and achieved performance score of < 0.972, 3.51, 0.89 > over its output dataset. Fig. 11 illustrates such case that is manually validated with ground-truth from third-party institution. (2) Generating test data for model evaluation. We configure MODis algorithms to generate test datasets for model benchmarking, where specific performance criteria can be posed [41]. Utilizing trained scientific image classifier from Kaggle, and pool of image feature datasets from HF with 75 tables, 768 columns, and over 1000 rows. We request BiMODis to generate datasets over which the classifier demonstrates: accuracy > 0.85 and training cost < 30s. BiMODis successfully generated 3 datasets to be chosen from within 15 seconds, with performance < 0.95, 0.27 >, < 0.94, 0.26 > and < 0.90, 0.25 >, as in Fig. 11."
        },
        {
            "title": "7 CONCLUSION\nWe have introduced MODis, a framework that generate skyline\ndatasets to improve data science models on multiple performance\nmeasures. We have formalized skyline data generation with trans-\nducers equipped with augment and reduction operators. We show\nthe hardness and fixed-parameter tractability of the problem. We\nhave introduced three algorithms that compute approximate Sky-\nline sets in terms of ğœ–-Skyline set, with reduce-from-universal,\nbi-directional, and diversification paradigms. Our experiments\nhave verified their effectiveness and efficiency. A future topic is\nto enhance MODis with query optimization techniques to scale\nit for larger input with high-dimensional data. Another topic is\nto extend MODis for distributed Skyline data generation.",
            "content": "ACKNOWLEDGMENTS This work is supported by NSF under OAC-2104007. Saif Khan, et al. 2023. Evolution-guided Bayesian optimization for constrained multi-objective optimization in self-driving labs. (2023). [31] Tien-Dung Nguyen, Tomasz Maszczyk, Katarzyna Musial, Marc-AndrÃ© ZÃ¶ller, and Bogdan Gabrys. 2020. Avatar-machine learning pipeline evaluation using surrogate model. In Advances in Intelligent Data Analysis XVIII: 18th International Symposium on Intelligent Data Analysis, IDA 2020, Konstanz, Germany, April 2729, 2020, Proceedings 18. Springer, 352365. [32] Xuan Vinh Nguyen, Jeffrey Chan, Simone Romano, and James Bailey. 2014. Effective global approaches for mutual information based feature selection. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. 512521. [33] Andrei Paleyes, Raoul-Gabriel Urma, and Neil Lawrence. 2022. Challenges in deploying machine learning: survey of case studies. Comput. Surveys 55, 6 (2022), 129. [34] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research 12 (2011), 28252830. [35] Hanchuan Peng, Fuhui Long, and Chris Ding. 2005. Feature selection based on mutual information criteria of max-dependency, max-relevance, and minredundancy. IEEE Transactions on pattern analysis and machine intelligence 27, 8 (2005), 12261238. [36] Yuji Roh, Geon Heo, and Steven Euijong Whang. 2019. survey on data collection for machine learning: big data-ai integration perspective. IEEE Transactions on Knowledge and Data Engineering 33, 4 (2019), 13281347. [37] Paolo Serafini. 1987. Some considerations about computational complexity for multi objective combinatorial problems. In Recent Advances and Historical Development of Vector Optimization: Proceedings of an International Conference on Vector Optimization Held at the Technical University of Darmstadt, FRG, August 47, 1986. Springer, 222232. [38] Darius Å idlauskas and Christian Jensen. 2014. Spatial joins in main memory: Implementation matters! Proceedings of the VLDB Endowment 8, 1 (2014), 97100. [39] Tom Sterkenburg and Peter GrÃ¼nwald. 2021. The no-free-lunch theorems of supervised learning. Synthese 199, 3-4 (2021), 997910015. [40] George Tsaggouris and Christos Zaroliagis. 2009. Multiobjective optimization: Improved FPTAS for shortest paths and non-linear objectives with applications. Theory of Computing Systems 45, 1 (2009), 162186. [41] Francesco Ventura, Zoi Kaoudi, Jorge Arnulfo QuianÃ©-Ruiz, and Volker Markl. 2021. Expand your training limits! generating training data for ml-based data management. In SIGMOD. [42] Mengying Wang, Sheng Guan, Hanchao Ma, Yiyang Bian, Haolai Che, Abhishek Daundkar, Alp Sehirlioglu, and Yinghui Wu. 2023. Selecting Top-k Data Science Models by Example Dataset. In CIKM. [43] Mengying Wang, Hanchao Ma, Yiyang Bian, Yangxin Fan, and Yinghui Wu. 2024. Full Version. https://wangmengying.me/papers/modis.pdf [44] Mengying Wang, Hanchao Ma, Abhishek Daundkar, Sheng Guan, Yiyang Bian, Alpi Sehirlioglu, and Yinghui Wu. 2022. CRUX: crowdsourced materials science resource and workflow exploration. In CIKM. [45] Chengrun Yang, Jicong Fan, Ziyang Wu, and Madeleine Udell. 2020. Automl pipeline selection: Efficiently navigating the combinatorial space. In proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining. 14461456. [46] Zhuoyue Zhao, Feifei Li, and Yuxi Liu. 2020. Efficient join synopsis maintenance for data warehouse. In Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data. 20272042. [47] JH Zheng, YN Kou, ZX Jing, and QH Wu. 2019. Towards many-objective optimization: Objective analysis, multi-objective optimization and decisionmaking. IEEE Access 7 (2019), 9374293751. [48] Patrick Ziegler and Klaus Dittrich. 2007. Data integrationproblems, approaches, and perspectives. In Conceptual modelling in information systems engineering. Springer, 3958. tion. REFERENCES [1] U.S. General Services Administration. 2023. Data.gov. https://www.data.gov/ [2] Ashwinkumar Badanidiyuru, Baharan Mirzasoleiman, Amin Karbasi, and Andreas Krause. 2014. Streaming submodular maximization: Massive data summarization on the fly. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. [3] Amit Chakrabarti and Sagar Kale. 2015. Submodular maximization meets streaming: matchings, matroids, and more. Mathematical Programming (2015). [4] Jan Chomicki, Paolo Ciaccia, and Niccolo Meneghetti. 2013. Skyline queries, front and back. ACM SIGMOD Record 42, 3 (2013), 618. [5] Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMT Meyarivan. 2002. fast and elitist multiobjective genetic algorithm: NSGA-II. IEEE transactions on evolutionary computation 6, 2 (2002), 182197. [6] Chance DeSmet and Diane Cook. 2024. HydraGAN: Cooperative Agent Model for Multi-Objective Data Generation. ACM Transactions on Intelligent Systems and Technology 15, 3 (2024), 121. [7] AnHai Doan, Alon Halevy, and Zachary Ives. 2012. Principles of data integra- [8] Bin Dong, Kesheng Wu, Surendra Byna, Jialin Liu, Weijie Zhao, and Florin Rusu. 2017. ArrayUDF: User-defined scientific data analysis on arrays. In Proceedings of the 26th International Symposium on High-Performance Parallel and Distributed Computing. 5364. [9] Zahra Donyavi and Shahrokh Asadi. 2020. Diverse training dataset generation based on multi-objective optimization for semi-supervised classification. Pattern Recognition 108 (2020), 107543. [10] Iddo Drori, Yamuna Krishnamurthy, Raoni Lourenco, Remi Rampin, Kyunghyun Cho, Claudio Silva, and Juliana Freire. 2019. Automatic machine learning by pipeline synthesis using model-based reinforcement learning and grammar. arXiv preprint arXiv:1905.10345 (2019). [11] Mahdi Esmailoghli, Christoph Schnell, RenÃ©e Miller, and Ziawasch Abedjan. 2023. Blend: unified data discovery system. arXiv preprint arXiv:2310.02656 (2023). [12] Grace Fan, Jin Wang, Yuliang Li, Dan Zhang, and RenÃ©e Miller. 2023. Semantics-Aware Dataset Discovery from Data Lakes with Contextualized Column-Based Representation Learning. Proceedings of the VLDB Endowment 16, 7 (2023). [13] Matthias Feurer, Katharina Eggensperger, Stefan Falkner, Marius Lindauer, and Frank Hutter. 2018. Practical automated machine learning for the automl challenge 2018. In International Workshop on Automatic Machine Learning at ICML. 11891232. [14] Sainyam Galhotra, Yue Gong, and Raul Castro Fernandez. 2023. Metam: Goaloriented data discovery. In 2023 IEEE 39th International Conference on Data Engineering (ICDE). IEEE, 27802793. [15] H2O.ai. 2022. H2O: Scalable Machine Learning Platform. https://github.com/ h2oai/h2o-3 version 3.42.0.2. [16] Pierre Hansen. 1980. Bicriterion path problems. In Multiple Criteria Decision Making Theory and Application: Proceedings of the Third Conference Hagen/KÃ¶nigswinter, West Germany, August 2024, 1979. 109127. [17] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for recommendation. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval. 639648. [18] Zezhou Huang, Pranav Subramaniam, Raul Castro Fernandez, and Eugene Wu. 2023. Kitana: Efficient Data Augmentation Search for AutoML. arXiv preprint arXiv:2305.10419 (2023). [19] Hugging Face AI 2023. Hugging Face The AI Community Building the Future. https://huggingface.co/ [20] John Hwang and Joaquim RRA Martins. 2018. fast-prediction surrogate model for large datasets. Aerospace Science and Technology 75 (2018), 7487. [21] Kaggle. 2023. Kaggle: Your Home for Data Science. https://www.kaggle.com/ [22] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. 2017. Lightgbm: highly efficient gradient boosting decision tree. Advances in neural information processing systems 30 (2017). [23] Mina Konakovic Lukovic, Yunsheng Tian, and Wojciech Matusik. 2020. Diversity-guided multi-objective bayesian optimization with batch evaluations. Advances in Neural Information Processing Systems 33 (2020), 1770817720. [24] Hsiang-Tsung Kung, Fabrizio Luccio, and Franco Preparata. 1975. On finding the maxima of set of vectors. J. ACM 22, 4 (1975), 469476. [25] Berti-Equille Laure, Bonifati Angela, and Milo Tova. 2018. Machine learning to data management: round trip. In ICDE. 17351738. [26] Maurizio Lenzerini. 2002. Data integration: theoretical perspective. In PODS. [27] Jundong Li, Kewei Cheng, Suhang Wang, Fred Morstatter, Robert Trevino, Jiliang Tang, and Huan Liu. 2017. Feature selection: data perspective. ACM computing surveys (CSUR) 50, 6 (2017), 145. [28] Yuliang Li, Xiaolan Wang, Zhengjie Miao, and Wang-Chiew Tan. 2021. Data augmentation for ml-driven data preparation and integration. Proceedings of the VLDB Endowment 14, 12 (2021), 31823185. [29] Chunming Liu, Xin Xu, and Dewen Hu. 2014. Multiobjective reinforcement learning: comprehensive overview. IEEE Transactions on Systems, Man, and Cybernetics: Systems 45, 3 (2014), 385398. [30] Andre KY Low, Flore Mekki-Berrada, Aleksandr Ostudin, Jiaxun Xie, Eleonore Vissol-Gaudin, Yee-Fun Lim, Abhishek Gupta, Qianxiao Li, Yew Soon Ong, Appendix A: Algorithms and Proof A.1 ApxMODis Proof of Lemma 2 For any constant ğœ–, ApxMODis correctly computes an ğœ–-Skyline set Î  that approximates Skyline set defined on the ğ‘ states it valuated. weights in ğºğ‘¤. Therefore, Î ğ‘¤ is an ğœ–-Skyline set of paths Î ğ‘¤ in ğºğ‘¤ if Dğ¹ is an ğœ–-Skyline set of Dğ‘† . Only If condition. Conversely, we assume the following: (1) Î ğ‘¤ is an ğœ–-Skyline set of paths Î ğ‘¤ in ğºğ‘¤, but (2) the induced Dğ¹ is not an ğœ–-Skyline set of Dğ‘† . Assumption (2) implies one of the following two cases: Proof. We establish the ğœ–-approximability of ApxMODis by constructing reduction from MODis to the multi-objective shortest path problem (MOSP) [40]. Reduction. An instance of MOSP consists of an edge-weighted graph ğºğ‘¤, where each edge ğ‘’ğ‘¤ is assigned ğ‘‘-dimensional attribute vector ğ‘’ğ‘¤ .ğ‘. The cost of path ğœŒğ‘¤ in ğºğ‘¤ is defined as ğœŒğ‘¤ .ğ‘ = (cid:205)ğ‘’ğ‘¤ ğœŒğ‘¤ ğ‘’ğ‘¤ .ğ‘. The dominance relation between two paths ğœŒğ‘¤ and ğœŒ ğ‘¤ is determined by comparing their costs. Specifically, ğœŒğ‘¤ dominates ğœŒ ğ‘¤ if ğœŒğ‘¤ has equal or lower costs than ğœŒ ğ‘¤ in all dimensions and is strictly better in at least one dimension. The objective is to compute Skyline set of paths from start node ğ‘¢ to all other nodes in the graph. We construct the reduction from our problem to MOSP. (1) We define ğºğ‘¤ as an edge weighted counterpart of running graph ğº . (a) Each vertex in ğº represents unique state ğ‘  during the execution of ApxMODis, with each state corresponding to specific dataset configuration in the data discovery process. The graph ğº contains ğ‘ vertices, corresponding to the ğ‘ states that ApxMODis has spawned and valuated. (b) Each edge (ğ‘ , ğ‘ ) in ğº represents transition from state ğ‘  to state ğ‘ , resulting from applying an operation (e.g., reduction or augmentation) that modifies the dataset. The edge is weighted by the difference in performance measures in between the two states: ğ‘’ğ‘¤ = ğ‘ .P ğ‘ .P. Here, ğ‘ .P and ğ‘ .P are the performance vectors of the states ğ‘  and ğ‘ , respectively. The edge weight ğ‘’ğ‘¤ is ğ‘‘-dimensional vector that quantifies how the performance metrics change as result of the transition. path ğœŒ ğº corresponds to sequence of transitions between states, starting from the initial state ğ‘ ğ‘ˆ . Similar to ğœŒğ‘¤ ğºğ‘¤, the cumulative cost of this path ğœŒ.ğ‘ is defined as the sum of the edge weights along the path, which represents the cumulative change in the performance measures as the dataset evolves through different states. Given solution Î ğ‘¤ of an instance of MOSP, which is an ğœ–Skyline set of paths, we construct solution for corresponding instance of MODis. For each path ğœŒğ‘¤ Î ğ‘¤, we establish corresponding path ğœŒ in ğº and identify the final state ğ‘  that the path reaches. The final state ğ‘  corresponds to specific dataset ğ·, which is the result of applying the sequence of operations from ğœŒ. We then include ğ· in the set Dğ¹ . This forms set of datasets as the solution to MODis. We next prove that Î ğ‘¤ is an ğœ–-Skyline set of paths Î ğ‘¤ in ğºğ‘¤. if and only if Dğ¹ is an ğœ–-Skyline set of Dğ‘† . If condition. Let Dğ¹ be an ğœ–-Skyline set of Dğ‘† . By the definition given in sec 4, this means that for every dataset ğ· (Dğ‘† Dğ¹ ), there exists at least one dataest ğ· Dğ¹ such that ğ· ğœ–-dominates D. Specifically, this means that ğ· has costs that are at most (1 + ğœ–) times the costs of ğ· in all performance measures in P, and ğ· has strictly lower cost in at least one measure. From the reduction, each path ğœŒğ‘¤ Î ğ‘¤ corresponds to sequence of transitions in ğº leading to final state ğ‘ , which represents dataset ğ· Dğ¹ . Similarly, ğœŒ ğ‘¤ Î ğ‘¤ corresponds to dataset ğ· (Dğ‘† Dğ¹ ). Since ğ· ğœ–-dominates ğ·, the corresponding path ğœŒğ‘¤ ğœ–-dominates ğœŒ ğ‘¤. This dominance is preserved because the performance measures P, directly corresponding to the edge Case 1: There exists dataset ğ· (Dğ‘† Dğ¹ ) that is not ğœ–-dominated by any dataset in Dğ¹ . This means there is corresponding path ğœŒ ğ‘¤ in ğºğ‘¤ that is not ğœ–-dominated by any path in Î ğ‘¤. This contradicts assumption (1) because ğœŒ ğ‘¤ should be ğœ–-dominated by at least one path in Î ğ‘¤. Case 2: There exists dataset ğ· Dğ¹ that is ğœ–-dominated by dataset ğ· (Dğ‘† Dğ¹ ). This means there exists path ğœŒ ğ‘¤ corresponding to ğ· in ğºğ‘¤ that ğœ–-dominates the path ğœŒğ‘¤ corresponding to ğ· in Î ğ‘¤. However, this would imply that Î ğ‘¤ does not fully capture the ğœ–-Skyline set because ğœŒğ‘¤ should not be in Î ğ‘¤ if it is ğœ–-dominated by ğœŒ ğ‘¤. Thus, it contradicts assumption (1). Both cases lead to contradiction with the assumption (1) that Î ğ‘¤ is an ğœ–-Skyline set of paths Î ğ‘¤ in ğºğ‘¤. Therefore, the initial assumption (2) must be false, meaning Î ğ‘¤ is an ğœ–-Skyline set of paths Î ğ‘¤ in ğºğ‘¤, only if Dğ¹ is an ğœ–-Skyline set of Dğ‘† . By proving both directions, we establish the equivalence that Î ğ‘¤ is an ğœ–-Skyline set of paths Î ğ‘¤ in ğºğ‘¤, if and only if Dğ¹ is an ğœ–-Skyline set of Dğ‘† . Correctness. We then show that algorithm ApxMODis is an optimized process of the algorithm in [40], which correctly computes Î ğ‘¤ for ğºğ‘¤. Specifically, this means that in ğºğ‘¤, for any path ğœŒ ğ‘¤ Î ğ‘¤ with corresponding state ğ‘ , there exists path ğœŒğ‘¤ Î ğ‘¤ with corresponding state ğ‘ , such that for every performance measure ğ‘ğ‘– (where 1 ğ‘– ğ‘‘, and ğ‘‘ = ), the condition ğ‘ .P (ğ‘ğ‘– ) (1 + ğœ–)ğ‘ .P (ğ‘ğ‘– ) holds. We prove the correctness of this result by induction. (cid:107) (cid:106)log1+ğœ– ğ‘ . (ğ‘ğ‘– ) ğ‘ğ‘™ğ‘– ğ‘  . (ğ‘ğ‘– ) ğ‘ğ‘™ğ‘– ğ‘ . (ğ‘ğ‘– ) = ğ‘ğ‘™ğ‘– 1 log1+ğœ– (1) Base case. After the first iteration in the main procedure of ApxMODis, and due to the merge steps in the UPareto procedure, the position ğ‘ğ‘œğ‘  (ğ‘ ) in Î 1 ğ‘¤ will be occupied by path ğœŒğ‘¤, for which: (i) ğ‘ğ‘œğ‘  (ğ‘ ) = ğ‘ğ‘œğ‘  (ğ‘ ); and (ii) ğ‘ .P (ğ‘ğ‘‘ ) ğ‘ .P (ğ‘ğ‘‘ ). From (i) and based on the Equation (1), for 1 ğ‘– ğ‘‘ 1, we have (cid:106)log1+ğœ– (cid:107). This implies log1+ğœ– , so that ğ‘ .P (ğ‘ğ‘– ) (1 + ğœ–)ğ‘ .P (ğ‘ğ‘– ) for 1 ğ‘– < ğ‘‘. Combined with (ii), we conclude that ğ‘ .P (ğ‘ğ‘– ) (1 + ğœ–)ğ‘ .P (ğ‘ğ‘– ) holds for 1 ğ‘– ğ‘‘. (2) Induction. Assume that after ğ‘– 1 iterations, ApxMODis correctly computes the ğœ–-Skyline set Î ğ‘– 1 ğ‘¤ for all paths from the source node ğ‘ ğ‘ˆ that contain up to ğ‘– 1 edges. This means that ğ‘¤ Î ğ‘– 1 for every path ğœŒ ğ‘¤ with at most ğ‘– 1 edges, there exists path ğœŒğ‘¤ Î ğ‘– 1 ğ‘¤ such that the corresponding states ğ‘  and ğ‘  satisfy: ğ‘  . (ğ‘ğ‘– ) ğ‘ğ‘™ğ‘– ğ‘ .P (ğ‘ğ‘– ) (1 + ğœ–)ğ‘ .P (ğ‘ğ‘– ), 1 ğ‘– ğ‘‘ We next prove that after ğ‘– iterations, ApxMODis correctly ğ‘¤ for all paths from the source node computes the ğœ–-Skyline set Î ğ‘– ğ‘ ğ‘ˆ that contain up to ğ‘– edges. By induction, every path in Î ğ‘– 1 ğ‘¤ ğœ–-dominates any other paths of up to ğ‘– 1 edges not included in Î ğ‘– 1 ğ‘¤ , so we only need to ensure the correctness of the ğ‘–th iteration. In this iteration, paths are expanded to include ğ‘– edges. As seen in the base case, after the merge step in procedure UPareto, ApxMODis ensures that for any state ğ‘  corresponding to path not included in Î ğ‘– ğ‘¤, there exists at least one state ğ‘  with corresponding path in Î ğ‘– ğ‘¤, such that: ğ‘ .P (ğ‘ğ‘– ) (1 + ğœ–)ğ‘ .P (ğ‘ğ‘– ), 1 ğ‘– ğ‘‘ Thus, after ğ‘– iterations, Î ğ‘– that should be included in the ğœ–-Skyline set. ğ‘¤ covers all paths with up to ğ‘– edges Putting these together, we show that ApxMODis correctly ğ‘¤ for all ğ‘– > 0. This verifies the computes the ğœ–-Skyline set Î ğ‘– correctness of ApxMODis. Proof of Lemma 3. Given with configuration ğ¶, if Dğ‘† has size in ğ‘‚ (ğ‘“ (ğ·ğ‘ˆ )), where ğ‘“ is polynomial, then ApxMODis is an FPTAS for MODis. Proof. We consider the reduction of an instance of MODis to its counterpart of MOSP as detailed in the proof of Lemma 2. MOSP is known to be solvable by an FPTAS. That is, there is an algorithm that can compute an ğœ–-Skyline set in polynomial time relative to the size of the input graph and 1 ğœ– [40]. We configure ApxMODis to run in (Dğ‘† , ğœ–)-approximation, which is simplified implementation of an FPTAS in [40] with multiple rounds of replacement strategy following path dominance. In the proof of Lemma 2, we have already shown that ApxMODis correctly computes the ğœ–-Skyline set for ğºğ‘¤, which is equivalent to the ğœ–-Skyline set for Dğ‘† in MODis. Meanwhile, as Dğ‘† is bounded by polynomial of the input size ğ·ğ‘ˆ , the time (cid:19) (cid:19) (cid:18) complexity of ApxMODis is ğ‘‚ where ğ‘“ is polynomial. This ensures that ApxMODis approximates the Skyline set for all datasets within PTIME. (cid:18)(cid:16) log(ğ‘ğ‘š ) ğœ– ğ‘“ (ğ·ğ‘ˆ ) (cid:17) 1 + ğ¼ , Space cost. We also report the space cost. (1) It takes vector of length in ğ‘‚ (ğ‘ƒ 1) to encode the position ğ‘ğ‘œğ‘  (ğœŒ). The replacement strategy in ApxMODis keeps one copy of position per path at runtime and recycles the space once it is verified to be dominated. According to Equation 1, there are at most (cid:206) 1 ğ‘–=1 paths to be remained in (P 1)- log1+ğœ– + 1 (cid:18) (cid:22) (cid:23) (cid:19) ğ‘ğ‘šğ‘ğ‘¥ ğ‘– ğ‘ğ‘šğ‘–ğ‘› ğ‘– dimensional array at runtime, hence the total space cost is in (cid:19)(cid:19) (cid:18) (cid:22) (cid:23) (cid:18) ğ‘‚ (cid:206) 1 ğ‘–=1 log1+ğœ– ğ‘ğ‘šğ‘ğ‘¥ ğ‘– ğ‘ğ‘šğ‘–ğ‘› ğ‘– + . A.2 BiMODis the details of Correlation based Pruning. We present Correlation-based Pruning. We first introduce monotonicity property as the condition for the applicability of the pruning. Monotonicity property. Given the current historical performances over valuated states, we say state ğ‘  (resp. ğ‘ ) with performance measure ğ‘ at path ğœŒ has monotonicity property, if for any state ğ‘  reachable from ğ‘  (resp. can reach ğ‘ ) via ğœŒ, ğ‘ . Ë†ğ‘ğ‘¢ < ğ‘  . Ë†ğ‘ğ‘™ 1+ğœ– (resp. ğ‘ . Ë†ğ‘ğ‘¢ < ğ‘  . Ë†ğ‘ğ‘™ 1+ğœ– ). Pruning rule. We next specify Correlation-based pruning with pruning rule as follows. First, recall that BiMODis dynamically maintains, for each performance ğ‘ and each state ğ‘ , an estimated range [ Ë†ğ‘ğ‘™ , Ë†ğ‘ğ‘¢ ] [ğ‘ğ‘™ , ğ‘ğ‘¢ ]. The bounds Ë†ğ‘ğ‘™ (resp. Ë†ğ‘ğ‘¢ are updated with runtime performance estimation of ğ‘  upon the changes of correlated performance measures. Specifically, for any state ğ‘  on path ğœŒ obtained by augmented features of its ancestor state ğ‘  on ğœŒ, where ğ‘  has performance ğ‘ learning cost ğ‘ .ğ‘ with lower bound ğ‘ . Ë†ğ‘ğ‘™ = 0.4, and an accuracy with estimated upperbound ğ‘ . Ë†ğ‘ ğ‘¢ = 0.8, then (1) ğ‘  has an estimated running cost initialized as ğ‘ . Ë†ğ‘ğ‘™ = 0.4, indicating learning cost no smaller than the counterpart ğ‘  with smaller dataset; and (2) ğ‘  has an estimated accuracy with an upperbound ğ‘ . Ë†ğ‘ ğ‘¢ = 0.8, as ğ‘ and ğ‘ are statistically negatively correlated with rule specified as: for every current valuated ğ‘ , ğ‘ as learning cost, and ğ‘ as accuracy, if ğ‘ is larger, then ğ‘ is smaller. The algorithm BiMODis dynamically maintains bounds list for all created states ğ‘  in the bidirectional search. Given two states ğ‘  and ğ‘ , where ğ‘  ğœ– ğ‘ , state ğ‘  on path ğœŒ from ğ‘  or to ğ‘  can be pruned according to Correlation-Based Pruning if for every ğ‘ P, ğ‘  has ğ‘ at ğœŒ with monotonicity property w.r.t. ğ‘  (resp. ğ‘ ). Note that the above rule is checkable in PTIME in terms of input size Dğ‘† . When Dğ‘† is large, one can generate path with all states unevaluated, and check at runtime if the condition holds between two evaluated states and any unevaluated state in betwen in PTIME, to prune the unevaluated states. We are now ready to show Lemma 4. Proof of Lemma 4 Let ğ‘  ğ‘„ ğ‘“ and ğ‘  ğ‘„ğ‘ . If ğ‘  ğœ– ğ‘ , then any state node ğ‘  on path from ğ‘  or to ğ‘ , that can be pruned according to Correlation-Based Pruning, ğ·ğ‘  is not in ğœ–-Skyline sets of the datasets from valuated states. We next perform case study of ğ‘  and ğ‘  as follows, subject to the monotonicity property. Case 1: Both ğ‘ .P (ğ‘) and ğ‘ .P (ğ‘) are valuated. If ğ‘  ğœ– ğ‘ , then by definition, ğ‘ .P (ğ‘) (1 + ğœ–)ğ‘ .P (ğ‘) for all ğ‘ P. This readily leads to ğœ–-dominance, i.e., ğ‘  ğœ– ğ‘ . As ğ‘  has every performance measures ğ‘ with monotonicity property w.r.t. ğ‘ , ğ‘  ğœ– ğ‘ . Hence ğ‘  can be safely pruned without valuation. Case 2: Neither ğ‘ .P (ğ‘) nor ğ‘ .P (ğ‘) is valuated. By definition, as ğ‘  ğœ– ğ‘ , then for every ğ‘ P, ğ‘ . Ë†ğ‘ğ‘¢ (1 + ğœ–)ğ‘ . Ë†ğ‘ğ‘™ . Given that ğ‘  has every performance measures ğ‘ with monotonicity property w.r.t. ğ‘ , then by definition, for each ğ‘ P, we have ğ‘  . Ë†ğ‘ğ‘™ ğ‘ .ğ‘ ğ‘ . Ë†ğ‘ğ‘¢ (1 + ğœ–)ğ‘ . Ë†ğ‘ğ‘™ (1 + ğœ–)ğ‘ . Ë†ğ‘ğ‘¢ < (1 + ğœ–) 1+ğœ– ğ‘ .ğ‘, for every ğ‘ P. By definition of state dominance, ğ‘  ğ‘ , for unevaluated ğ‘ . Following similar proof, one can infer that ğ‘  ğ‘  for state ğ‘  in the forward front of BiMODis. Hence ğ‘  can be safely pruned. Case 3: One of ğ‘ .P (ğ‘) or ğ‘ .P (ğ‘) is valuated. Given that ğ‘  ğœ– ğ‘ , we have (a) ğ‘ .P (ğ‘) (1 + ğœ–)ğ‘ . Ë†ğ‘ğ‘™ , if only ğ‘ .P (ğ‘) is valuated; or (b) ğ‘ . Ë†ğ‘ğ‘¢ (1 + ğœ–)ğ‘ .P (ğ‘), if only ğ‘ .P (ğ‘) is valuated. Consider case 3(a). As ğ‘  can reach ğ‘  via path ğœŒ, and ğ‘  satisfiies the pruning condition, we can infer that ğ‘ .P (ğ‘) (1 + ğœ–)ğ‘ . Ë†ğ‘ğ‘™ (1 + ğœ–)ğ‘ . Ë†ğ‘ğ‘¢ < (1 + ğœ–) ğ‘  . Ë†ğ‘ğ‘™ 1+ğœ– ğ‘ .ğ‘, hence ğ‘  ğ‘ . Similarly for case 3(b), we can infer that ğ‘ . Ë†ğ‘ğ‘¢ (1 + ğœ–)ğ‘ .ğ‘ ğ‘  . Ë†ğ‘ğ‘™ 1+ğœ– ğ‘ .ğ‘. hence ğ‘  ğ‘ . For both cases, (1 + ğœ–)ğ‘ . Ë†ğ‘ğ‘¢ < (1 + ğœ–) ğ‘  can be pruned without evaluation. Lemma 4 hence follows. We present the details of the algorithm BiMODis in Fig. 12. A.3 DivMODis 1 Proof of Lemma 5 Given ğ‘ and ğœ–, DivMODis achieves 4 approximation for diversified MODis, i.e., (1) it correctly computes ğœ–-Skyline set ğ·ğ‘ƒ ğ¹ ) 1 4 div(D ). ğ¹ over ğ‘ valuated datasets, and (2) div(ğ·ğ‘ƒ We here present detailed analysis for the lemma 5. Monotone submodularity. We first show that the diversification function div() is monotone submodular function. Given set of datasets Dğ¹ , we show that for any set of datasets ğ‘Œ ğ‘‹ Dğ¹ , Algorithm 4 BiMODis 1: Input: Configuration ğ¶ = (ğ‘ ğ‘ˆ , O, ğ‘€,ğ‘‡ , E), Records ğ‘…ğ‘’ğ‘, constant ğœ– > 0; 2: Output: ğœ–-Skyline set Dğ¹ . 3: Set Dğ¹ := , BIB := , PrunS := ; ğ‘ ğ‘ = BackSt(ğ‘ ğ¸, ğ‘ ğ‘ˆ ); 4: queue ğ‘„ ğ‘“ := {(ğ‘ ğ‘ˆ , 0)}, queue ğ‘„ğ‘ := {(ğ‘ ğ‘, 0)}; 5: D0 6: D0 7: while ğ‘„ ğ‘“ , ğ‘„ğ‘ and ğ‘„ ğ‘“ ğ‘„ğ‘ = do 8: ğ¹ [ğ‘ğ‘œğ‘  (ğ‘ ğ‘ˆ )] = CorrFP(ğ‘ ğ‘ˆ , ğ‘…ğ‘’ğ‘, E); ğ¹ [ğ‘ğ‘œğ‘  (ğ‘ ğ‘ )] = CorrFP(ğ‘ ğ‘, ğ‘…ğ‘’ğ‘, E); (ğ‘ , ğ‘‘) = ğ‘„ ğ‘“ .dequeue(), Dğ¹ ğ‘‘+1 = Dğ¹ ğ‘‘ ; Forward Serach for all ğ‘  OpGen (ğ‘ , F) do Pğ‘  = CorrFP(ğ‘ , ğ‘…ğ‘’ğ‘, E); set ğœŒğ‘  with Pğ‘  ; if ğ‘ğ‘œğ‘  (ğ‘ ) PrunS then continue; pruned = False for bound in SandwBs do pruned = SandwPrun(ğœŒğ‘  , bound, SandwBs) if pruned then break; pruned = UPareto (Dğ‘‘+1 , PrunS, pos(s), ğœ–) if not pruned then ğ‘„ ğ‘“ .enqueue((s, d+1)) ğ¹ (ğ‘ , ğ‘‘) = ğ‘„ğ‘ .dequeue(), Dğ¹ Serach ğ‘‘+1 = Dğ¹ ğ‘‘ ; Backward for all ğ‘  OpGen (ğ‘ , B) do same with line 10 to 16 in Forward Search if not pruned then ğ‘„ğ‘ .enqueue((s, d+1)) 22: procedure CorrFP(ğ‘ , ğ‘…ğ‘’ğ‘, E) 23: Build ğºğ‘ for measures recorded in ğ‘…ğ‘’ğ‘; StrongRs = GetSR(ğºğ‘ ) if in ğ‘…ğ‘’ğ‘.keys() then Pğ‘  = ğ‘…ğ‘’ğ‘ [ğ‘ ]; if ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘ (Pğ‘  ) 0.8Pğ‘  then return Pğ‘  ; for all missing ğ‘ğ‘  ğ‘– in Pğ‘  do if (ğ‘ğ‘–, ğ‘ ğ‘— ) StrongRs and ğ‘ğ‘  Case 1: By ğ‘…ğ‘’ğ‘ Case 2: By ğºğ‘ ğ‘— ğ‘…ğ‘’ğ‘ [ğ‘ ] then ğ‘— in ğ‘…ğ‘’ğ‘; ğ‘— with ğ‘ğ‘  find closed ğ‘ğ‘™ ğ‘– + ğ‘ğ‘¢ ğ‘– = (ğ‘ğ‘™ ğ‘ğ‘  ğ‘— and ğ‘ğ‘¢ ğ‘– )/2 if ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘ (Pğ‘  ) < 0.8Pğ‘  then Case 3: By fill missing ğ‘ğ‘  Pğ‘  by invoking E; update ğ‘…ğ‘’ğ‘ [ğ‘ ] = Pğ‘  return Pğ‘  ; 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: Figure 12: Complete Version of BiMODis div(ğ‘Œ ) div(ğ‘‹ ); and ğ‘¥ Dğ¹ ğ‘‹ , div(ğ‘‹ {ğ‘¥ })div(ğ‘‹ ) div(ğ‘Œ {ğ‘¥ })div(ğ‘Œ ). (1) To see div(ğ‘Œ ) div(ğ‘‹ ), we have div(ğ‘‹ ) div(ğ‘Œ ) = ğ‘˜ 1 ğ‘˜ ğ‘–=1 ğ‘— =ğ‘–+1 Given ğ‘Œ ğ‘‹ , we have dis(ğ·ğ‘‹ ğ‘– , ğ·ğ‘‹ ğ‘— ) ğ‘˜ 1 ğ‘˜ ğ‘–= ğ‘— =ğ‘–+1 dis(ğ·ğ‘Œ ğ‘– , ğ·ğ‘Œ ğ‘— ) div(ğ‘‹ ) div(ğ‘Œ ) = dis(ğ·, ğ· ) 0; ğ· ğ‘‹ ğ‘Œ ,ğ· ğ‘Œ (2) We next show the submodularity of the function div(). To simplify the presentation, we introduce notation marginal gain. For any ğ‘¥ Dğ¹ ğ‘‹ and ğ‘Œ ğ‘‹ , the marginal gain of mg(ğ‘‹ , ğ‘¥ ) = ğ‘˜ 1 ğ‘˜ ğ‘— =ğ‘–+1 ğ‘–=1 + ğ‘˜ 1 ğ‘˜ mg(ğ‘Œ , ğ‘¥ ) = ğ‘˜ 1 ğ‘˜ ğ‘— =ğ‘–+1 ğ‘–=1 + ğ‘˜ 1 ğ‘˜ diversification score for ğ‘‹ {ğ‘¥ } and ğ‘Œ {ğ‘¥ }, denoted as mg(ğ‘‹, ğ‘¥) and mg(ğ‘Œ, ğ‘¥), are defined as: ğ›¼ 1 cos(ğ‘ ğ‘– .ğ¿, ğ‘  ğ‘— .ğ¿) 2 ğ‘˜ 1 ğ‘˜ ğ‘–= ğ‘— =ğ‘–+1 ğ›¼ 1 cos(ğ‘ ğ‘™ .ğ¿, ğ‘ ğ‘š .ğ¿) 2 (1 ğ›¼ ) euc(ğ‘¡ğ‘– . P, ğ‘¡ ğ‘— . ) eucm ğ‘˜ 1 ğ‘˜ (1 ğ›¼ ) euc(ğ‘¡ğ‘™ . P, ğ‘¡ğ‘š . ) eucm ğ‘— =ğ‘–+1 ğ‘–=1 ğ‘— =ğ‘–+1 where ğ·ğ‘–, ğ· ğ‘— ğ‘‹ {ğ‘¥ } and ğ·ğ‘™ , ğ·ğ‘š ğ‘‹ . ğ‘–=1 ğ›¼ 1 cos(ğ‘ ğ‘– .ğ¿, ğ‘  ğ‘— .ğ¿) 2 ğ‘˜ 1 ğ‘˜ ğ‘–=1 ğ‘— =ğ‘–+1 ğ›¼ 1 cos(ğ‘ ğ‘™ .ğ¿, ğ‘ ğ‘š .ğ¿) 2 (1 ğ›¼ ) euc(ğ‘¡ğ‘– . P, ğ‘¡ ğ‘— . ) eucm ğ‘˜ 1 ğ‘˜ (1 ğ›¼ ) euc(ğ‘¡ğ‘™ . P, ğ‘¡ğ‘š . ) eucm ğ‘— =ğ‘–+ ğ‘–=1 where ğ·ğ‘–, ğ· ğ‘— ğ‘Œ ğ‘¥ and ğ·ğ‘™ , ğ·ğ‘š ğ‘Œ . ğ‘–=1 ğ‘— =ğ‘–+1 In our problem,we only consider ğœ–-Skyline sets with size at most ğ‘˜. With this condition, we observe that given the dataset ğ‘¥, mg(ğ‘Œ, ğ‘¥) and mg(ğ‘‹, ğ‘¥) measure the marginal gain of diversification scores by replacing dataset ğ‘¥ ğ‘Œ with ğ‘¥ and ğ‘¥ ğ‘‹ with ğ‘¥. mg(ğ‘Œ, ğ‘¥) and mg(ğ‘‹, ğ‘¥) measure the margin gain by replacing ğ‘¥ with same dataset ğ‘¥ over same ğœ–-Skyline set with size ğ‘˜. In this case, we can have ğ¹ at level ğ‘–, and new batch of datasets ğ·ğ‘– mg(ğ‘‹ , ğ‘¥ ) = mg(ğ‘Œ , ğ‘¥ ) due to DivMODis replace the same dataset ğ‘¥ that are in ğ‘‹ and ğ‘Œ . Thus, we can see that the marginal gain of ğ‘‹ is no larger than marginal gain of ğ‘Œ by including ğ‘¥. This analysis completes the proof of diversification function div() is monotone submodular function. Approximability. We next prove that DivMODis ensures 1 4 - approximation of diversified size-ğ‘˜ Skyline set. We verify this by proving an invariant that the approximation holds for any size-ğ‘˜ ğœ–-Skyline set ğ·ğ‘ƒ ğ¹ generated at every level ğ‘–. By integrating greedy selection and replacement policy, DivMODis keeps -set with the most diverse and representative datasets to mitigate the biases in the Skyline set at each level. Consider set of datasets ğ·ğ‘– 1 ğ¹ arrives. DivMODis aims to maintain the set of datasets ğ·ğ‘– ğ¹ such that, at level ğ‘–, ğ·ğ‘– ğ¹ ) is maximized. At any level ğ‘–, DivMODis approximates the global optimal solution upon the newly generated datasets. Consider the global optimal solution at level ğ‘–, over ğ·ğ‘– ğ¹ , we can show that DivMODis maintains ğ·ğ‘ƒ ğ¹ at any level ğ‘– by solving streaming submodular maximization problem [3]. Reduction. We show there exists an approximation at any level by reduction from the diversification phase of MODis problem to stream submodular maximization problem [2, 3]. Given streaming of elements ğ¸ = {ğ‘’0, . . . ğ‘’ğ‘š }, an integer ğ‘˜, and submodular score function ğ‘“ , it computes set of elements ğ‘† with size ğ‘˜ with maximized ğ‘“ (ğ‘†). Given the ğœ–-Skyline set Dğ‘ƒ ğ¹ , and integer ğ‘˜, the diversification of MODis problem aims to compute an ğœ–-Skyline ğ‘ƒ ) is maximized. set Dğ¹ Given an instance of diversification of MODis problem at any level ğ‘–, we construct an instance of stream submodular maximization problem by setting (1) ğ‘“ = div; (2) ğ¸ = ğ·ğ‘– ğ¹ ; (3) integer ğ¹ ğ‘˜, and div(ğ·ğ‘– ğ‘ƒ ğ‘˜ and div(Dğ¹ ğ‘ƒ such that (1) Dğ¹ ğ¹ as ğ·ğ‘ƒ compared to other methods. The results reinforce the scalability and practicality of BiMODis across diverse datasets and tasks, including both graph-based and tabular data. Scalability. Fig. 14 presents the scalability test results for ğ‘‡5. With universal graph size of (7925, 34), we performed ğ‘˜-means clustering on edges, setting 5 as the minimum number of clusters, 30 as the maximum, and identifying 13 as the optimal number of clusters based on performance. For node features, we leveraged the graphs structure to reduce the input feature space from 34 to 10 by aggregating attributes from similar types of relations, such as combining multiple training records of an ML model, while preserving all augmented information. Across all settings, methods applied bi-directional search (BiMODis, NOBiMODis, and DivMODis) consistently achieve superior efficiency, handling both increasing attributes and active domain sizes effectively. In contrast, ApxMODis exhibits slower performance as ğ´ and adom grows, highlighting the scalability of the bi-directional search strategy in managing large and complex graph datasets. Effectiveness. The effectiveness results for ğ‘‡1 and ğ‘‡3 are reported in Table 6, where we select the best results from the Skyline set based on the first metric for each task. These results align with those observed for other tasks, consistently showing that MODis methods outperform baseline approaches in most cases. Notably, NOBiMODis and BiMODis secure the first and second positions across the majority of metrics. Sensitivety analysis. Fig. 15 reports the impact of critical factors including the maximum length of paths, and ğœ– (as in ğœ–-Skyline set) to the accuracy measures. The larger the Percentage Change is, the better the generated Skyline set can improve the prformance of the input model. We found that all the MODis algorithms benefit from larger maximum length and smaller ğœ– in terms of percentage of accuracy improvement. This is consistent with our observation over the tests that report absolute accuracy measures. Moreover, MODis algorithms are relatively more sensitive to the maximum length, compared with the changes to ğœ–. Figure 13: Efficiency Analysis on ğ‘‡5 and ğ‘‡3 Figure 14: Scalability on ğ‘‡5 ğ¹ with ratio 1 ğ‘˜ is equal to the value of in the instance of diversification of MODis. Correctness.DivMODis approximates ğ·ğ‘ƒ 4 follows greedy selection and replacement policy that integrates the ğ‘– at level ğ‘–. DivMODis always \"replace\" strategy given the Dğ¹ terminates when no datasets are generated at level ğ‘– by procedure UPareto (See lines 1-3 in Fig. 6). Approximation. Dğ‘ƒ 4 when terminates at level ğ‘–. DivMODis exploits the greedy selection as in [3] but specifies diversification function div(Dğ‘ƒ ğ¹ ) to maintain the ğœ–-Skyline set of datasets and replaces newly arrived datasets whenever possible. It returns the ğœ–-Skyline set of datasets those corresponding elements in ğ¸ are selected by the instance stream submodular maximization problem. This ensures 1 4 approximability by this consistent construction from the solution for stream submodular maximization. ğ¹ approximates Dğ‘ƒ ğ¹ with ratio The above analysis completes the proof of Lemma 5. Appendix B: Additional Experiments We have performed more complementary experimental studies. Efficiency. Fig. 13 (a, b) evaluates the efficiency of MODis algorithms for taskğ‘‡ 5 on generating graph data for the link regression task. The observation is consistent with our findings for their counterparts over tabular data. In particular, BiMODis is quite feasible for generating graph data for GNN-based link regression task, with around 20 seconds in all settings, and consistently outperforms other MODis algorithms. Fig. 13 (c, d) presents the efficiency results for task ğ‘‡3, which involves avocado price prediction. Similar to other tasks, BiMODis demonstrates superior efficiency, maintaining significantly lower search times ğ‘‡1: Movie ğ‘ğ´ğ‘ğ‘ ğ‘ğ‘‡ ğ‘Ÿğ‘ğ‘–ğ‘› ğ‘ğ¹ ğ‘ ğ‘ ğ‘ğ‘€ğ¼ Output Size ğ‘‡3: Avocado MSE MAE Training Time Output Size Original METAM METAM-MO 0.8560 1.4775 0.0824 0.0538 (3264, 10) 0.8743 1.6276 0.0497 0.0344 (3264, 11) 0.8676 1.1785 0.0801 0.0522 (3264, 11) Original METAM METAM-MO 0.0428 0.1561 0.0280 (9999, 11) 0.0392 0.1497 0.0178 (9999, 12) 0.0312 0.1452 0.0350 (9999, 12) Starmie 0.8606 1.2643 0.1286 0.1072 (3264, 23) Starmie 0.036152 0.145259 0.043600 (9999, 12) SkSFM 0.8285 0.6028 0.7392 0.3921 (3264, 3) SkSFM 0.050903 0.173676 0.008618 (9999, 3) H2O 0.8545 0.9692 0.3110 0.1759 (3264, 8) H2O 0.0442 0.1592 0.0156 (9999, 5) ApxMODis NOBiMODis BiMODis DivMODis 0.9291 0.9947 0.6011 0.4178 (2958, 9) 0.9874 0.8766 0.7202 0.3377 (1980, 12) ApxMODis NOBiMODis 0.029769 0.127916 0.006516 (1589, 10) 0.022821 0.115326 0.003293 (817, 5) 0.9755 0.8027 0.9240 0.3839 (1835, 11) 0.9427 0.8803 0.8010 0.4165 (2176, 10) BiMODis DivMODis 0.027511 0.027511 0.123200 0.123200 0.004366 0.004366 (1310, 9) (1310, 9) Table 6: Comparison of Data Discovery Algorithms in Multi-Objective Setting (ğ‘‡1, ğ‘‡3) Figure 15: Sensitivity Analysis for Parameters on ğ‘‡"
        }
    ],
    "affiliations": [
        "Case Western Reserve University Cleveland, Ohio, USA"
    ]
}