{
    "paper_title": "DenseDPO: Fine-Grained Temporal Preference Optimization for Video Diffusion Models",
    "authors": [
        "Ziyi Wu",
        "Anil Kag",
        "Ivan Skorokhodov",
        "Willi Menapace",
        "Ashkan Mirzaei",
        "Igor Gilitschenski",
        "Sergey Tulyakov",
        "Aliaksandr Siarohin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Direct Preference Optimization (DPO) has recently been applied as a post-training technique for text-to-video diffusion models. To obtain training data, annotators are asked to provide preferences between two videos generated from independent noise. However, this approach prohibits fine-grained comparisons, and we point out that it biases the annotators towards low-motion clips as they often contain fewer visual artifacts. In this work, we introduce DenseDPO, a method that addresses these shortcomings by making three contributions. First, we create each video pair for DPO by denoising corrupted copies of a ground truth video. This results in aligned pairs with similar motion structures while differing in local details, effectively neutralizing the motion bias. Second, we leverage the resulting temporal alignment to label preferences on short segments rather than entire clips, yielding a denser and more precise learning signal. With only one-third of the labeled data, DenseDPO greatly improves motion generation over vanilla DPO, while matching it in text alignment, visual quality, and temporal consistency. Finally, we show that DenseDPO unlocks automatic preference annotation using off-the-shelf Vision Language Models (VLMs): GPT accurately predicts segment-level preferences similar to task-specifically fine-tuned video reward models, and DenseDPO trained on these labels achieves performance close to using human labels."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 7 1 5 3 0 . 6 0 5 2 : r DenseDPO: Fine-Grained Temporal Preference Optimization for Video Diffusion Models Ziyi Wu1,2,3, Anil Kag1, Ivan Skorokhodov1, Willi Menapace1, Ashkan Mirzaei1, Igor Gilitschenski2,3,, Sergey Tulyakov1,, Aliaksandr Siarohin1 1Snap Research, 2University of Toronto, 3Vector Institute"
        },
        {
            "title": "Abstract",
            "content": "Direct Preference Optimization (DPO) has recently been applied as post-training technique for text-to-video diffusion models. To obtain training data, annotators are asked to provide preferences between two videos generated from independent noise. However, this approach prohibits fine-grained comparisons, and we point out that it biases the annotators towards low-motion clips as they often contain In this work, we introduce DenseDPO, method that fewer visual artifacts. addresses these shortcomings by making three contributions. First, we create each video pair for DPO by denoising corrupted copies of ground truth video. This results in aligned pairs with similar motion structures while differing in local details, effectively neutralizing the motion bias. Second, we leverage the resulting temporal alignment to label preferences on short segments rather than entire clips, yielding denser and more precise learning signal. With only one-third of the labeled data, DenseDPO greatly improves motion generation over vanilla DPO, while matching it in text alignment, visual quality, and temporal consistency. Finally, we show that DenseDPO unlocks automatic preference annotation using off-the-shelf Vision Language Models (VLMs): GPT accurately predicts segment-level preferences similar to task-specifically fine-tuned video reward models, and DenseDPO trained on these labels achieves performance close to using human labels. Additional results are available at https://snap-research.github.io/DenseDPO/."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in diffusion models [24] have enabled high-quality text-guided video generation [2, 7, 25, 35, 56, 64, 69]. Despite tremendous progress, existing video generators still fall short on temporal coherence, visual fidelity, and prompt alignment [86], impeding their industry-level applications. Inspired by the success of learning from human feedback in language models [3, 50] and image diffusion [5, 18, 68], recent works have explored preference alignment in video diffusion [40, 58, 84]. Among them, methods based on Direct Preference Optimization (DPO) [59] stand out as they bypass the need for an explicit reward model [45, 46, 63, 78]. However, existing DPO methods for video diffusion are largely adapted from their image-based counterparts, without addressing the unique challenges inherent to video generation. Typically, these methods first generate videos from independent noise maps, followed by human preference labeling to construct comparison pairs. Yet, human preferences in video are influenced by multiple, sometimes inversely correlated, factors, such as the visual quality (i.e., pixel-level fidelity) and the dynamic degree (i.e., strength of global motion). Indeed, current video generation models excel at producing high-quality slow-motion videos, while struggling to synthesize more challenging dynamic scenes [8]. As result, when annotators are asked to express preferences, they often favor artifact-free slow-motion clips. Applying DPO training on such preference data further reinforces video generators bias toward slow-motion content, ultimately suppressing the models ability to generate dynamic and motion-rich videos. Preprint. Under review. Figure 1: Text-to-video results with our DenseDPO aligned model. Our method improves both visual quality and temporal consistency of the model, enabling generation of challenging motion. natural approach to enhance competing factors, drawing inspiration from Pareto optimization, is to fix some attributes within each video pair while varying others. Motivated by the guided image synthesis approach in SDEdit [49], we generate pair by introducing different partial noise to ground-truth video and perform denoising. The resulting videos in each pair share high-level semantics and motion trajectories while differing in local visual details [8, 70], allowing us to reduce spurious correlations. However, guided sampling inherently reduces the diversity across generated video pairs, leading to degraded DPO performance [51, 61]. straightforward solution might be to annotate more data pairs. Instead, we propose extracting richer and more accurate supervision from each video pair by collecting segment-level preference labels. Prior works show that multi-dimensional scores are superior to single label in preference alignment [42, 78, 89]. Unlike images, videos have unique temporal dimension [76]. In practice, we observe that human preferences over video pairs often vary across time, as artifacts may appear at different timestamps in each video, leading to inconsistent preferences. This issue is more severe with modern video generators as they produce longer videos. Therefore, we split videos into short segments (e.g., 1s slices), and collect per-segment preference labels. Thanks to temporally aligned videos from guided sampling, there is clear one-to-one correspondence between segment pairs, simplifying the annotation process. Segment feedback also reduces the amount of ties when both videos contain artifacts, and provides more accurate supervision. In addition, it allows us to apply existing vision-language models (VLMs) [1, 4] which can produce reliable judgment on short segments. Our main contributions are threefold: (i) DenseDPO framework tailored towards video generation, with improved data construction and preference granularity over vanilla DPO; (ii) DenseDPO retains the motion strength of the base model while matching other metrics of vanilla DPO, with significantly higher data efficiency; (iii) We show that existing VLMs fail to label preference over long videos (e.g., 5s), but they perform well in segment-level preference, achieving results close to human labels."
        },
        {
            "title": "2 Related Work",
            "content": "Preference learning for image diffusion. Inspired by the success of human feedback learning in language modeling [3, 50], similar approaches have been adapted to image generation. One major line of work focuses on training reward models from human preference labels [33, 73, 74, 77, 89], these models can be used as loss functions to optimize generators by direct gradient backpropagation [9, 13, 31, 57, 75, 77] or policy gradients [5, 18]. Another line of work uses predicted rewards on training data to re-weight the diffusion loss [38], or only trains on high-scoring samples [14, 16, 39]. However, all these methods require an explicitly trained reward model, and may suffer from the reward hacking issue [13, 77]. In contrast, Diffusion-DPO [68] and D3PO [80] directly optimize the model on 2 pre-collected human preference pairs, bypassing the need for online reward feedback. Building on this paradigm, subsequent works have explored constructing more informative comparison data [29, 32, 85], improved DPO objective [41, 71], and credit assignment over denoising timesteps [43, 90]. Preference learning for video diffusion. Early approaches to preference learning in video diffusion directly borrow techniques from image diffusion, such as direct reward optimization [40, 54, 58, 66, 84] and training loss re-weighting [20], and even relying on image reward models to provide supervision. Recent papers thus focus on developing better video reward models [45, 46, 78]. One strategy aggregates multiple video quality assessment metrics [22, 30] to final score [46, 87]. However, existing metrics are only effective for short videos [45, 78], limiting their applicability for modern video generators that produce long videos [7, 69, 81]. To address this limitation, LiFT [72], VisionReward [78], and VideoAlign [45] collect large number of videos from advanced video generators, label human feedback, and fine-tune VLMs to predict preferences. With powerful video reward model, they apply weighted training [72] or DPO [45, 78] to improve video generation. In contrast to prior works, we focus on DPO for video diffusion using direct human annotations, i.e., without an explicit reward model. Analogous to the verbosity bias observed in language model preference learning, where annotators favor longer outputs [62, 65], we identify motion bias in video preference labels, where slow-motion videos are often preferred. To mitigate this, we propose better data pair construction strategy to address this bias via guided video generation. Rich feedback for alignment. While early preference alignment methods treated human feedback as single binary label, recent works begin to exploit rich, multi-dimensional feedback [42, 78, 88]. In image generation, MPS [89] learns reward model that evaluates images on four dimensions including aesthetics, semantics, detail, and overall quality, improving its alignment with humans. On the other hand, Liang et al. [42] curates dataset that localizes regions of artifacts and misaligned words in the text prompt, leading to better DPO performance. Multi-aspect feedback is even more critical for video generation due to its inherently higher dimensionality. Recent works all explicitly model dimensions such as visual fidelity, text relevance, and motion consistency [45, 46, 78]. However, notable limitation is that they still aggregate feedback at the whole-video level, neglecting the finegrained temporal dimension of preferences. In contrast, our DenseDPO partitions videos into short, temporally aligned segments, and collects preferences for each segment. This is conceptually similar to the sentence-level preference label used in language models [34, 83]. By localizing feedback to brief windows, we obtain more accurate and denser supervision signals for DPO training."
        },
        {
            "title": "3 Method",
            "content": "In this section, we begin by reviewing the background on diffusion models and the standard Direct Preference Optimization (DPO) framework, which we refer to as VanillaDPO (Sec. 3.1). Next, we introduce StructuralDPO, method that optimizes human preferences on structurally similar video pairs (Sec. 3.2). Finally, we propose DenseDPO, which enables fine-grained human preference alignment along the temporal axis of videos (Sec. 3.3). Notation. Let RT HW denote video sample of length with spatial dimensions drawn from the real video distribution pdata. We use diffusion process that transforms real video to Gaussian noise ϵ (0, I), and reverses this transformation by learning denoiser Gθ(xt, t, c), neural network parameterized by θ, where xt is the noisy latent at time and refers to the auxiliary conditioning variable such as text prompt embeddings or an image. 3.1 Background: Video Diffusion and DPO Rectified-flow diffusion models. We follow the rectified flow framework [44, 47], which learns transport map from the distribution of real videos pdata to the standard normal distribution ϵ (0, I). The forward diffusion process produces noisy input xt at time [0, 1] via linear interpolation with noise ϵ: xt = (1 t)x0 + tϵ. The velocity estimator Gθ is optimized with the following objective: min θ Etp(t),xpdata,ϵN (0,I)(ϵ x) Gθ(xt, t, c)2, (1) where p(t) is the distribution of noise levels (following [17], we adopt the logit-normal one). VanillaDPO. In the direct preference optimization framework [59, 68], generative model is trained to align its outputs with human preferences. Typically, these preferences are defined by dataset 3 Figure 2: Comparison between VanillaDPO (top) and DenseDPO (bottom). VanillaDPO compares two videos generated from independent random noises and only assigns single binary preference, biasing the annotators toward slow-motion videos. In contrast, DenseDPO generates structurally similar videos from partially noised real videos, and label segment-level dense preferences. = {(c, x0, x1, l)}, where each sample consists of two videos {x0, x1} per input condition and their preference label {1, +1}. The preference function is defined as: l(x0, x1) = (cid:26)+1, 1, if x0 x1 if x1 x0 (i.e., x0 is preferred over x1) (i.e., x1 is preferred over x0) (2) The Bradley-Terry (BT) model [6] defines pairwise preference using reward function r(x, c) which computes the alignment score between the sample and the input condition c. The corresponding probabilistic preference can be expressed as: pBT(x0 x1c) = σ(r(x0, c) r(x1, c)); pBT(x1 x0c) = σ(r(x1, c) r(x0, c)), (3) where σ() is the sigmoid function. Rafailov et al. [59] defines the binary preference optimization as explicitly optimizing the binary reward objective log σ(l(x0, x1) (r(x0, c) r(x1, c))) in conjunction with Kullback-Leibler (KL) divergence regularization to control the deviation from reference model. Wallace et al. [68] re-formulated the preference optimization framework for diffusion models assuming the presence of reference model Gref, which is further extended to rectified flow models in [45]. Given sample (c, x0, x1, l), the denoiser Gθ, and the reference model Gref, we can define an implicit reward as: s(x, c, t, θ) = (ϵ x) Gθ(x (4) = (1 t)x + tϵ, ϵ (0, I) is noisy latent for input (either x0 or x1) at time t. where With the implicit reward function, the VanillaDPO objective is defined as follows: 2 (ϵ x) Gref(x , t, c)2 2, , t, c)2 L(θ) = (c,x0,x1,l)D, tp(t), ϵN (0,I) (cid:2)log σ (cid:0)β l(x0, x1) (cid:0)s(x0, c, t, θ) s(x1, c, t, θ)(cid:1)(cid:1)(cid:3) . (5) 3.2 StructuralDPO: Preference Learning over Structurally Similar Videos Motion bias in VanillaDPO. In the standard VanillaDPO pipeline, preference pairs are created by independently sampling two videos (x0, x1) from different noise seeds under the same conditioning (see Algo. 1), followed by human preference annotation. While this approach works reasonably well for images, its direct extension to videos introduces new issues due to the presence of the new temporal dimension. Independent noises often result in videos with significantly different motion patterns and global layouts (see Fig. 2 (a)). For example, in typical preference pair, one video may be nearly static but visually clean, while the other contains the desired motion but also introduces artifacts, such as distorted limbs or flickering. 4 Figure 3: Guided video generation with different η. Lower η means more guidance. We sample one frame per video for visualization. η = 0.75 is enough to maintain the motion trajectory and high-level semantics of the ground-truth video. For slow-motion videos (top), high η suffices to generate artifact-free videos, while videos with challenging motion (bottom) require more guidance. We empirically observe that this is common bias in generated video data. Indeed, video models often excel at producing high-quality slow-motion clips, while dynamic videos usually contain visible artifacts [8, 86]. Since humans typically perceive clean static videos as more realistic than artifactprone dynamic ones, this often leads to preference dataset that systematically over-represents static content. Consequently, model trained with DPO on this dataset would produce videos with reduced motion. In our preliminary DPO experiments, we observed substantial drop in dynamic degree using our base model (see Tab. 1 and Tab. 2). The same issue has also been observed in prior works, e.g., Tab.10 of [78] using CogVideoX [81] and Tab. 2 of [46] using VideoCrafter-v2 [11]. StructuralDPO. To address the static bias in VanillaDPO, we propose StructuralDPO, which modifies the data curation strategy using guided generation [49] to obtain pairs of videos with similar motion trajectories. Specifically, in VanillaDPO, we start from independent noise, i.e., x0 (0, I); x1 (0, I) and then denoise from step till step 1. Instead, we propose to denoise from partially noised real video to generate video pairs. Concretely, given ground truth video and the guidance level η [0, 1], we obtain the corrupted noisy video at step = round(η ) as: = (1 η)x + ηϵ0; x1 x0 = (1 η)x + ηϵ1; where ϵ0, ϵ1 (0, I). (6) The corrupted videos are then denoised from step to 1, as outlined in Algo. 2. Here, η governs the structural similarity between the two samples. Since early diffusion steps control the global motion [8], this approach preserves the overall dynamics while allowing variations in local details. We compare videos of varying motion strengths generated with different η in Fig. 3. StructuralDPO applies the standard DPO formulation (Eq. (5)) on this structurally consistent dataset, which helps to focus the preference on the temporal artifacts and visual inconsistencies, anchoring other dimensions like dynamic degree. Additionally, guided sampling simplifies the generation task, allowing the model to produce artifact-free highly dynamic videos more reliably. Finally, guided sampling reduces data construction costs as it requires fewer sampling steps. Algorithm 1 Vanilla Paired Video Generation Algorithm 2 Guided Paired Video Generation Input: Denoiser Gθ, Input Condition c, Inference Steps Init: = 1 Init: x0 (0, I) Init: x1 (0, I) for = to 1 do = i1 = x0 x0 x1 i1 = x1 end for return Video Pair (x0 + Gθ(x0 + Gθ(x1 0, x1 0) , t, c) , t, c) Input: Denoiser Gθ, Input Condition c, Inference Steps , Real Video x, Guidance Level η [0, 1] Init: = 1 , = round(η ) Init: x0 Init: x1 for = to 1 do = (1 η)x + ηϵ0, ϵ0 (0, I) = (1 η)x + ηϵ1, ϵ1 (0, I) = i1 = x0 x0 x1 i1 = x1 end for return Video Pair (x0 + Gθ(x0 + Gθ(x1 0, x1 0) , t, c) , t, c) 5 3.3 DenseDPO: Rich Temporal Feedback with Segment-Level Preferences Although StructuralDPO effectively preserves dynamic degree, models trained with it tend to generate videos with lower visual quality and weaker text alignment compared to VanillaDPO. This performance gap is because video pairs are structurally similar, which reduces diversity in the curated DPO dataset, and, as we discuss in Appendix B, can unintentionally drive the model to diverge from the real data distribution [51, 61]. straightforward solution is to obtain more labeled data, but it increases the annotation cost compared to VanillaDPO. Instead, we explore an alternative approach to increase data and annotation diversity without increasing the number of labeled video pairs. DenseDPO. In VanillaDPO and StructuralDPO, scalar preference label {1, +1} is obtained for the entire video of length . Instead, DenseDPO annotates preferences on shorter temporal segments. Since guided video generation (Algo. 2) yields structurally similar video pairs, the same time period in both videos has clear correspondence, making comparison feasible. We show an example in Fig. 2 (b) with the intervals being single frame: for frame 1 and 2, the first video is better, while for frame 3, the second video is better. Formally, given two videos (x0, x1) and the interval length s, we can break down videos into = ceil( ) temporal segments of length by splitting along the time dimension. The resulting video pairs ({x0 =1) are annotated with preferences over each segment, yielding segment-level dense preference labels {1, +1}F , i.e., l(x0, x1) = [l(x0 =1. Thus, following Eq. (5), we can formulate the DenseDPO objective as: , x1 , f )]F }F L(θ) = (c,x0,x1,l)D tp(t), ϵN (0,I) log σ β (cid:88) =1 l(x0 , x1 ) (cid:0)s(x0, c, t, θ)f s(x1, c, t, θ)f (cid:1) , (7) where s()f is the implicit reward value on the -th video segment. In our collected dense preference data, we find that over 60% of video pairs have both winning and losing labels in l. In regular preference annotation such pairs will either be treated as ties or choose the video with fewer artifacts. In the latter case, this encourages the model to minimize loss on videos with artifacts in Eq. (5), degrading the model performance. In contrast, DenseDPO assigns preference labels more accurately over time, only optimizing models on segments with clear difference. Segment preference annotation with VLMs. Another benefit of the DenseDPO is that it allows us to use off-the-shelf VLMs for automatic preference labeling. Prior works point out that existing VLMs struggle at assessing long videos (e.g., 5s) [22, 78], often requiring task-specific fine-tuning and large-scale human annotations to train effective video reward models. Instead, we show that pre-trained VLMs are already capable of processing short clips (e.g., 1s). Given two temporally aligned videos, we feed in pairs of segments into VLM, and ask it to identify the better one. As we will demonstrate in the experiments (Tab. 3), GPT-o3 [1] achieves high accuracy on segment-level preferences, leading to DPO results competitive with using human preference labels."
        },
        {
            "title": "4 Experiments",
            "content": "Our experiments aim to answer the following questions: (i) How does DenseDPO perform against VanillaDPO? (Sec. 4.2) (ii) Can we leverage existing VLMs to produce high-quality preference labels? (Sec. 4.3) (iii) What is the impact of each component in our framework? (Sec. 4.4) 4.1 Experimental Setup We list some key aspects of our experimental setup here. For full details, please refer to Appendix A. Preference learning data. We curate high-quality video dataset from existing datasets, resulting in around 55k videos. This is done by filtering the length, visual quality, and motion score of videos similar to [56], and prompting GPT-4o [3] to classify if the text prompt contains events of meaningful dynamics. This naturally gives us text prompts and corresponding ground-truth videos. Baselines. Our pre-trained text-to-video generator is DiT [53]-based latent flow model. We finetune it on the curated high-quality data, termed the SFT baseline. For VanillaDPO, we randomly select 30k text prompts from the curated dataset, generate 2 videos of 5s per prompt with Algo. 1, and ask human labelers to annotate preferences. This leads to around 10k winning-losing pairs after Table 1: Quantitative results on VideoJAM-bench [8]. We report automatic metrics from VBench [30] and VisionReward [78]. DenseDPO significantly outperforms Vanilla DPO in dynamic degree, while achieves similar performance in other dimensions. VBench Metrics VisionReward Metrics Method Aesthetic Imaging Temporal Dynamic Quality Quality Consistency Consistency Smoothness Degree Alignment Quality Consistency Degree Background Motion Dynamic Subject Visual Text Pre-trained SFT Vanilla DPO [45] Structural DPO DenseDPO 54.65 55.19 57.25 56.38 56.99 55.85 53.26 60.38 59.78 60.92 88.29 87.71 91.21 90.21 91. 91.50 91.52 93.94 92.34 93.84 92.40 92.72 93.43 92.94 93.56 84.16 83.25 80.25 84.69 85. 0.770 0.773 0.867 0.843 0.863 0.192 0.205 0.371 0.341 0.376 0.354 0.279 0.636 0.602 0. 0.680 0.675 0.535 0.652 0.680 Table 2: Quantitative results on MotionBench. We report automatic metrics from VBench [30] and VisionReward [78]. DenseDPO achieves similar motion smoothness compared to Vanilla DPO, while consistently outperforms it in visual quality, dynamic degree, and text alignment. VBench Metrics VisionReward Metrics Method Aesthetic Imaging Temporal Dynamic Quality Quality Consistency Consistency Smoothness Degree Alignment Quality Consistency Degree Background Motion Dynamic Subject Visual Text Pre-trained SFT Vanilla DPO [45] Structural DPO DenseDPO 56.21 56.16 57.51 57.46 57.54 56.26 55.54 61.20 59.84 61. 88.23 87.94 91.45 90.98 91.60 91.67 92.42 93.49 93.13 93.72 93.56 94.44 97.43 97.11 97. 83.69 84.93 72.55 79.95 84.73 0.261 0.273 0.355 0.347 0.359 0.112 0.105 0.172 0.152 0. 0.154 0.129 0.239 0.229 0.232 0.840 0.845 0.709 0.839 0.858 removing ties. For StructuralDPO, we use the same 30k prompts from VanillaDPO, and label human preferences on videos generated using Algo. 2. The guidance level η is randomly sampled from [0.65, 0.8] to obtain video pairs with similar motion. This again leads to around 10k winning-losing pairs. Both VanillaDPO and StructuralDPO apply the Flow-DPO loss in Eq. (5). Following prior works [45, 78], we set β to 500 and apply LoRA [28] with rank 128 to fine-tune the video model. We train with the AdamW optimizer [48] and global batch size of 256 for 1000 steps. DenseDPO implementation details. For fair comparison with baselines, we only take 10k video pairs from the StructuralDPO training data to label dense preferences, which costs similar amount of human annotation time. The segment length is set to 1s. Overall, more than 80% of video pairs have at least 1 non-tie segment and can be used in DPO training, greatly improving the data efficiency over using global preferences. All other hyper-parameters are the same as DPO baselines. Evaluation datasets. We utilize two benchmarks to evaluate the performance of text-to-video generation. VideoJAM-bench [8] contains 128 prompts focusing on real-world scenarios with challenging motion, ranging from human actions to physical phenomena. We also construct MotionBench, which collects more diverse prompts from existing prompt sets [35, 56, 72] such as MovieGenBench. We run GPT-4o to select prompts with dynamic human actions, resulting in 419 prompts. Evaluation metrics. We aim to measure the visual quality, text alignment, and motion quality of videos. Specifically, we want to evaluate both the smoothness and strength of the motion. Therefore, we adopt VBench [30] and state-of-the-art video quality assessment model, VisionReward [78]. 4.2 DPO with Human Labels Tab. 1 and Tab. 2 present the quantitative results on VideoJAM-bench and MotionBench. Fig. 4 shows qualitative comparison. VanillaDPO greatly improves the pre-trained model and the SFT baseline in all dimensions except dynamic degree due to the motion bias in video preference data. StructuralDPO retains the motion with paired video generation, while compromising the visual quality and text alignment. With the rich temporal feedback, DenseDPO consistently outperforms StructuralDPO. In addition, it matches all aspects of VanillaDPO and scores significantly higher dynamic degree, despite using only one-third of labeled videos (10k vs. 30k). Human evaluation. We conduct user study using all prompts from VideoJAM-bench in Fig. 5. We ask the participants to express their preference when presented with paired samples from our method Figure 4: Qualitative results. Pre-trained model generates deformed limbs. VanillaDPO fixes it but generates almost static motion. StructuralDPO retains dynamics but produces oversaturated frames. DenseDPO is the only method that generates correct limbs, large dynamics, and high quality visuals. Figure 5: Human evaluation of DenseDPO vs. StructuralDPO (left) and VanillaDPO (right). TA, VQ, TC, DD stand for text alignment, visual quality, temporal consistency, and dynamic degree. and each baseline. DenseDPO consistently outperforms StructuralDPO in all dimensions. Compared to VanillaDPO, we achieve significantly higher dynamic degree, and are on par in other aspects. 4.3 DPO with VLM Labels VLM labelers. As discussed in Sec. 3.3, we aim to evaluate the effectiveness of pre-trained VLMs in video preference learning. We take VLMs designed specifically for the video quality assessment task, VideoScore [22], LiFT [72], VideoReward [45], and VisionReward [78]. They have been fine-tuned on large-scale human preference labels. We also utilize the state-of-the-art visual reasoning model, GPT o3 [1], to explore the limits of models without task-specific training. Finally, we design GPT o3 Segment that partitions long videos into short segments to process separately, and aggregates results via majority voting. Please refer to Appendix A.6 for more implementation details. Evaluation setup. We first test the preference prediction accuracy of VLMs on two types of videos: Short Segment partitions videos into 1s clips, and compares them separately. We report the accuracy on the 10k human preference labels used in DenseDPO. Long Video directly runs the model on the entire video except for GPT o3 Segment that aggregates segment-level results. We report the accuracy on the 30k human preference labels used in StructuralDPO. In addition, we conduct DPO training using these VLM-generated labels, and report the VisionReward score on VideoJAM-bench. Notably, we run VLMs to label video pairs generated from all 55k training data for better performance. Results. Tab. 3a presents the results of preference prediction. With task-specific fine-tuning, stateof-the-art video reward models outperform the advanced GPT o3 in assessing short clips. Yet, their performance on long videos degrades drastically. Thanks to the temporal alignment of video pairs, we can run GPT to compare short segments individually, and then aggregate the results. This leads to higher accuracy in long video preference prediction, without any preference data fine-tuning. We further show the DPO alignment results in Tab. 3b. Due to higher preference accuracy, DenseDPO with GPT o3 Segment labels outperforms StructuralDPO with binary preferences from 8 Table 3: VLMs results on human preference learning. (a) Existing VLMs excel at assessing short segments, yet their performance on long videos is still unsatisfactory. In contrast, by aggregating results over short segments, GPT o3 achieves the best accuracy without task-specific fine-tuning. (b) DenseDPO training with GPT-based segment-level labels achieves performance close to using human labels, significantly outperforming other VLMs that predict single binary preferences. VLM VideoScore [22] LiFT [72] VideoReward [45] VisionReward [78] GPT o3 [1] Short Segment Long Video 61.23% 51.23% 65.03% 55.45% 71.89% 59.65% 72.45% 62.11% 70.03% 53.45% GPT o3 Segment 70.03% 70.59% Label Source TA VQ TC DD Pre-trained 0. 0.192 0.354 0.680 VisionReward [78] + StructuralDPO GPT o3 [1] + StructuralDPO GPT o3 Segment + DenseDPO 0.785 0. 0.521 0.622 0.759 0.284 0.506 0. 0.842 0.368 0.598 0.672 Human Label 0. 0.376 0.632 0.680 (a) Preference accuracy of VLMs on 1s short segments and 5s long videos. Table 4: Ablation on different variants of preference labels used for DPO training. We report VBench [30] and VisionReward [78] metrics on VideoJAM-bench [8]. (b) DPO results with VLM labels on VideoJAM-bench [8]. VBench Metrics VisionReward Metrics Method Aesthetic Imaging Temporal Dynamic Quality Quality Consistency Consistency Smoothness Degree Alignment Quality Consistency Degree Background Motion Dynamic Subject Visual Text StructuralDPO Majority Voting Flip 40% Label Flip 20% Label 0.5 Label 2 Label DenseDPO 56.38 56.48 55.10 56.23 55.62 57.56 59.78 59.72 55.42 58.10 56.13 62. 56.99 60.92 90.21 90.15 88.85 90.85 89.23 92.85 91. 92.34 92.35 92.04 93.06 92.48 95.06 93.84 92.94 92.98 92.60 92. 92.99 93.85 93.56 84.69 84.54 84.27 84.85 84.25 85.55 85. 0.843 0.847 0.782 0.846 0.806 0.882 0.341 0.342 0.250 0.368 0.305 0. 0.863 0.376 0.602 0.608 0.556 0.602 0.589 0.654 0. 0.652 0.645 0.668 0.678 0.655 0.682 0.680 other VLMs. It even matches DenseDPO trained with human labels on text alignment, visual quality, and dynamic degree. Yet, please note that GPT label has 5.5 more videos than human label. 4.4 Ablation Study We study the effect of each component in DenseDPO. All results are evaluated on VideoJAM-bench. Label bias. We first study if there is systematic bias in the annotation pipeline, e.g., labelers may produce higher quality preference labels on short segments than long videos. To verify it, we aggregate labels of all segments within video to obtain its binary preference label via majority voting, and then train StructuralDPO on these labels. Tab. 4 shows that it achieves similar results compared to StructuralDPO trained on globally labeled preferences. This proves that DenseDPOs superior performance comes from segment-level preference supervision instead of labeler bias. Label quality. We study how label noise affects DenseDPO performance. As shown in Tab. 4, we randomly flip 20% and 40% winning or losing labels. This results in clear drop in all the metrics. Label quantity. We ablate different amounts of segment preference labels used in DenseDPO. Tab. 4 shows that scaling to 2x labels leads to the best results across all metrics. Interestingly, results trained with 0.5 labels are clearly better than results trained with 40% labels flipped. This may indicate that the quality of labels has larger impact than the quantity of labels."
        },
        {
            "title": "5 Conclusion",
            "content": "We present DenseDPO, an improved preference optimization framework for video generation. We address two critical aspects of video DPOcomparison data curation and preference labeling. Our guided video generation mechanism and fine-grained segment-level labels lead to better post-training results compared to DPO baselines. We discuss our limitations in Appendix D. 9 Acknowledgments We would like to thank Zhengyang Liang, Weize Chen, and Tsai-Shien Chen for valuable discussions, and Maryna Diakonova for help with data preparation and user studies."
        },
        {
            "title": "References",
            "content": "[1] OpenAI o3. https://openai.com/index/o3-o4-mini-system-card/, 2025. [2] Kling2.0. https://klingai.com/, 2025. [3] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-VL technical report. arXiv preprint arXiv:2502.13923, 2025. [5] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. In ICLR, 2024. [6] Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 1952. [7] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. OpenAI technical reports, 2024. URL https: //openai.com/research/video-generation-models-as-world-simulators. [8] Hila Chefer, Uriel Singer, Amit Zohar, Yuval Kirstain, Adam Polyak, Yaniv Taigman, Lior Wolf, and Shelly Sheynin. VideoJAM: Joint appearance-motion representations for enhanced motion generation in video models. arXiv preprint arXiv:2502.02492, 2025. [9] Chaofeng Chen, Annan Wang, Haoning Wu, Liang Liao, Wenxiu Sun, Qiong Yan, and Weisi Lin. Enhancing diffusion models with text-encoder reinforcement learning. In ECCV, 2024. [10] Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Juncheng Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Zheng Chen, Chengchen Ma, et al. SkyReels-V2: Infinite-length film generative model. arXiv preprint arXiv:2504.13074, 2025. [11] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In CVPR, 2024. [12] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70M: Captioning 70m videos with multiple cross-modality teachers. In CVPR, 2024. [13] Kevin Clark, Paul Vicol, Kevin Swersky, and David Fleet. Directly fine-tuning diffusion models on differentiable rewards. arXiv preprint arXiv:2309.17400, 2023. [14] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, et al. Emu: Enhancing image generation models using photogenic needles in haystack. arXiv preprint arXiv:2309.15807, 2023. [15] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, et al. Emu: Enhancing image generation models using photogenic needles in haystack. arXiv preprint arXiv:2309.15807, 2023. 10 [16] Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment. TMLR, 2023. [17] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. [18] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. DPOK: Reinforcement learning for fine-tuning text-to-image diffusion models. NeurIPS, 2023. [19] Duanyu Feng, Bowen Qin, Chen Huang, Zheng Zhang, and Wenqiang Lei. Towards analyzing and understanding the limitations of dpo: theoretical perspective. arXiv preprint arXiv:2404.04626, 2024. [20] Hiroki Furuta, Heiga Zen, Dale Schuurmans, Aleksandra Faust, Yutaka Matsuo, Percy Liang, and Sherry Yang. Improving dynamic object interactions in text-to-video generation with ai feedback. arXiv preprint arXiv:2412.02617, 2024. [21] Songwei Ge, Aniruddha Mahapatra, Gaurav Parmar, Jun-Yan Zhu, and Jia-Bin Huang. On the content bias in fréchet video distance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 72777288, 2024. [22] Xuan He, Dongfu Jiang, Ge Zhang, Max Ku, Achint Soni, Sherman Siu, Haonan Chen, Abhranil Chandra, Ziyan Jiang, Aaran Arulraj, et al. VideoScore: Building automatic metrics to simulate fine-grained human feedback for video generation. In EMNLP, 2024. [23] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. [24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. [25] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. [26] Jiwoo Hong, Sayak Paul, Noah Lee, Kashif Rasul, James Thorne, and Jongheon Jeong. MarginIn ICLR aware preference optimization for aligning diffusion models without reference. Workshop, 2025. [27] Wenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Yan Wang, Yean Cheng, Shiyu Huang, Junhui Ji, Zhao Xue, et al. CogVLM2: Visual language models for image and video understanding. arXiv preprint arXiv:2408.16500, 2024. [28] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In ICLR, 2022. [29] Zijing Hu, Fengda Zhang, and Kun Kuang. D-Fusion: Direct preference optimization for aligning diffusion models with visually consistent samples. In ICML, 2025. [30] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. VBench: Comprehensive benchmark suite for video generative models. In CVPR, 2024. [31] Dongzhi Jiang, Guanglu Song, Xiaoshi Wu, Renrui Zhang, Dazhong Shen, Zhuofan Zong, Yu Liu, and Hongsheng Li. CoMat: Aligning text-to-image diffusion model with image-to-text concept matching. NeurIPS, 2024. [32] Shyamgopal Karthik, Huseyin Coskun, Zeynep Akata, Sergey Tulyakov, Jian Ren, and Anil Kag. Scalable ranked preference optimization for text-to-image generation. arXiv preprint arXiv:2410.18013, 2024. [33] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-Pic: An open dataset of user preferences for text-to-image generation. NeurIPS, 2023. [34] Aobo Kong, Wentao Ma, Shiwan Zhao, Yongbin Li, Yuchuan Wu, Ke Wang, Xiaoqian Liu, Qicheng Li, Yong Qin, and Fei Huang. SDPO: Segment-level direct preference optimization for social agents. arXiv preprint arXiv:2501.01821, 2025. [35] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. HunyuanVideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [36] PKU-Yuan Lab and Tuzhan AI etc. Open-Sora-Plan, April 2024. URL https://doi.org/10. 5281/zenodo.10948109. [37] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [38] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human feedback. arXiv preprint arXiv:2302.12192, 2023. [39] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2.5: Three insights towards enhancing aesthetic quality in text-to-image generation. arXiv preprint arXiv:2402.17245, 2024. [40] Jiachen Li, Qian Long, Jian Zheng, Xiaofeng Gao, Robinson Piramuthu, Wenhu Chen, and William Yang Wang. T2V-Turbo-v2: Enhancing video generation model post-training through data, reward, and conditional guidance design. In ICLR, 2025. [41] Shufan Li, Konstantinos Kallidromitis, Akash Gokul, Yusuke Kato, and Kazuki Kozuka. Aligning diffusion models by optimizing human utility. NeurIPS, 2024. [42] Youwei Liang, Junfeng He, Gang Li, Peizhao Li, Arseniy Klimovskiy, Nicholas Carolan, Jiao Sun, Jordi Pont-Tuset, Sarah Young, Feng Yang, et al. Rich human feedback for text-to-image generation. In CVPR, 2024. [43] Zhanhao Liang, Yuhui Yuan, Shuyang Gu, Bohan Chen, Tiankai Hang, Mingxi Cheng, Ji Li, and Liang Zheng. Aesthetic post-training diffusion models from generic preferences with step-by-step preference optimization. arXiv preprint arXiv:2406.04314, 2024. [44] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. In ICLR, 2023. [45] Jie Liu, Gongye Liu, Jiajun Liang, Ziyang Yuan, Xiaokun Liu, Mingwu Zheng, Xiele Wu, Qiulin Wang, Wenyu Qin, Menghan Xia, Xintao Wang, Xiaohong Liu, Fei Yang, Pengfei Wan, Di Zhang, Kun Gai, Yujiu Yang, and Wanli Ouyang. Improving video generation with human feedback. arXiv preprint arXiv:2501.13918, 2025. [46] Runtao Liu, Haoyu Wu, Zheng Ziqiang, Chen Wei, Yingqing He, Renjie Pi, and Qifeng Chen. VideoDPO: Omni-preference alignment for video diffusion generation. In CVPR, 2025. [47] Xingchao Liu, Chengyue Gong, et al. Flow straight and fast: Learning to generate and transfer data with rectified flow. In ICLR, 2023. [48] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. [49] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In ICLR, 2022. [50] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In NeurIPS, 2022. 12 [51] Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, and Colin White. Smaug: Fixing failure modes of preference optimisation with dpo-positive. arXiv preprint arXiv:2402.13228, 2024. [52] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: An imperative style, high-performance deep learning library. NeurIPS, 2019. [53] William Peebles and Saining Xie. Scalable diffusion models with transformers. ICCV, 2023. [54] Jun-cheng Chen Po-Hung Yeh, Kuang-Huei Lee. Training-free diffusion model alignment with sampling demons. In ICLR, 2025. [55] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [56] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. [57] Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, and Katerina Fragkiadaki. Aligning textto-image diffusion models with reward backpropagation. arXiv preprint arXiv:2310.03739, 2023. [58] Mihir Prabhudesai, Russell Mendonca, Zheyang Qin, Katerina Fragkiadaki, and Deepak Pathak. Video diffusion alignment via reward gradients. arXiv preprint arXiv:2407.08737, 2024. [59] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. NeurIPS, 2023. [60] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. JMLR, 2020. [61] Noam Razin, Sadhika Malladi, Adithya Bhaskar, Danqi Chen, Sanjeev Arora, and Boris Hanin. Unintentional Unalignment: Likelihood displacement in direct preference optimization. In ICLR, 2025. [62] Keita Saito, Akifumi Wachi, Koki Wataoka, and Youhei Akimoto. Verbosity bias in preference labeling by large language models. In NeurIPS Workshop, 2024. [63] Team Seawead, Ceyuan Yang, Zhijie Lin, Yang Zhao, Shanchuan Lin, Zhibei Ma, Haoyuan Guo, Hao Chen, Lu Qi, Sen Wang, Feng Cheng, Feilong Zuo, Xuejiao Zeng, Ziyan Yang, Fangyuan Kong, Meng Wei, Zhiwu Qing, Fei Xiao, Tuyen Hoang, Siyu Zhang, Peihao Zhu, Qi Zhao, Jiangqiao Yan, Liangke Gui, Sheng Bi, Jiashi Li, Yuxi Ren, Rui Wang, Huixia Li, Xuefeng Xiao, Shu Liu, Feng Ling, Heng Zhang, Houmin Wei, Huafeng Kuang, Jerry Duncan, Junda Zhang, Junru Zheng, Li Sun, Manlin Zhang, Renfei Sun, Xiaobin Zhuang, Xiaojie Li, Xin Xia, Xuyan Chi, Yanghua Peng, Yuping Wang, Yuxuan Wang, Zhongkai Zhao, Zhuo Chen, Zuquan Song, Zhenheng Yang, Jiashi Feng, Jianchao Yang, and Lu Jiang. Seaweed-7B: Cost-effective training of video generation foundation model, 2025. [64] Abhishek Sharma, Adams Yu, Ali Razavi, Andeep Toor, Andrew Pierson, Ankush Gupta, Austin Waters, Daniel Tanis, Dumitru Erhan, Eric Lau, Eleni Shaw, Gabe Barth-Maron, Greg Shaw, Han Zhang, Henna Nandwani, Hernan Moraldo, Hyunjik Kim, Irina Blok, Jakob Bauer, Jeff Donahue, Junyoung Chung, Kory Mathewson, Kurtis David, Lasse Espeholt, Marc van Zee, Matt McGill, Medhini Narasimhan, Miaosen Wang, Mikołaj Binkowski, Mohammad Babaeizadeh, Mohammad Taghi Saffar, Nick Pezzotti, Pieter-Jan Kindermans, Poorva Rane, Rachel Hornung, Robert Riachi, Ruben Villegas, Rui Qian, Sander Dieleman, Serena Zhang, Serkan Cabi, Shixin Luo, Shlomi Fruchter, Signe Nørly, Srivatsan Srinivasan, Tobias Pfaff, Tom Hume, Vikas Verma, Weizhe Hua, William Zhu, Xinchen Yan, Xinyu Wang, Yelin Kim, Yuqing Du, and Yutian Chen. Veo, 2024. URL https://deepmind.google/technologies/veo/. 13 [65] Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. long way to go: Investigating length correlations in rlhf. In COLM, 2024. [66] Zhiwei Tang, Jiangweizhi Peng, Jiasheng Tang, Mingyi Hong, Fan Wang, and Tsung-Hui Chang. Tuning-free alignment of diffusion models with direct noise optimization. In ICML Workshop, 2024. [67] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. [68] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In CVPR, 2024. [69] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [70] Binxu Wang and John Vastola. Diffusion models generate images like painters: an analytical theory of outline first, details later. arXiv preprint arXiv:2303.02490, 2023. [71] Fu-Yun Wang, Yunhao Shui, Jingtan Piao, Keqiang Sun, and Hongsheng Li. Diffusion-NPO: Negative preference optimization for better preference aligned generation of diffusion models. In ICLR, 2025. [72] Yibin Wang, Zhiyu Tan, Junyan Wang, Xiaomeng Yang, Cheng Jin, and Hao Li. LiFT: Leveraging human feedback for text-to-video model alignment. arXiv preprint arXiv:2412.04814, 2024. [73] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. [74] Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score: Better aligning text-to-image models with human preference. In ICCV, 2023. [75] Xiaoshi Wu, Yiming Hao, Manyuan Zhang, Keqiang Sun, Zhaoyang Huang, Guanglu Song, Yu Liu, and Hongsheng Li. Deep reward supervisions for tuning text-to-image diffusion models. In ECCV, 2024. [76] Ziyi Wu, Aliaksandr Siarohin, Willi Menapace, Ivan Skorokhodov, Yuwei Fang, Varnith Chordia, Igor Gilitschenski, and Sergey Tulyakov. Mind the time: Temporally-controlled multi-event video generation. In CVPR, 2025. [77] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. NeurIPS, 2023. [78] Jiazheng Xu, Yu Huang, Jiale Cheng, Yuanming Yang, Jiajun Xu, Yuan Wang, Wenbo Duan, Shen Yang, Qunlin Jin, Shurun Li, Jiayan Teng, Zhuoyi Yang, Wendi Zheng, Xiao Liu, Ming Ding, Xiaohan Zhang, Xiaotao Gu, Shiyu Huang, Minlie Huang, Jie Tang, and Yuxiao Dong. VisionReward: Fine-grained multi-dimensional human preference learning for image and video generation. arXiv preprint arXiv:2412.21059, 2024. [79] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Advancing high-resolution video-language representation with large-scale video transcriptions. In CVPR, 2022. [80] Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Weihan Shen, Xiaolong Zhu, and Xiu Li. Using human feedback to fine-tune diffusion models without any reward model. In CVPR, 2024. [81] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. CogVideoX: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [82] Lijun Yu, José Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. [83] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, and Tat-Seng Chua. RLHF-V: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. In CVPR, 2024. [84] Hangjie Yuan, Shiwei Zhang, Xiang Wang, Yujie Wei, Tao Feng, Yining Pan, Yingya Zhang, Ziwei Liu, Samuel Albanie, and Dong Ni. InstructVideo: Instructing video diffusion models with human feedback. In CVPR, 2024. [85] Huizhuo Yuan, Zixiang Chen, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning of diffusion models for text-to-image generation. NeurIPS, 2024. [86] Ailing Zeng, Yuhang Yang, Weidong Chen, and Wei Liu. The dawn of video generation: Preliminary explorations with sora-like models. arXiv preprint arXiv:2410.05227, 2024. [87] Jiacheng Zhang, Jie Wu, Weifeng Chen, Yatai Ji, Xuefeng Xiao, Weilin Huang, and Kai Han. OnlineVPO: Align video diffusion model with online video-centric preference optimization. arXiv preprint arXiv:2412.15159, 2024. [88] Lingzhi Zhang, Zhengjie Xu, Connelly Barnes, Yuqian Zhou, Qing Liu, He Zhang, Sohrab Amirghodsi, Zhe Lin, Eli Shechtman, and Jianbo Shi. Perceptual artifacts localization for image synthesis tasks. In ICCV, 2023. [89] Sixian Zhang, Bohan Wang, Junqiang Wu, Yan Li, Tingting Gao, Di Zhang, and Zhongyuan Wang. Learning multi-dimensional human preference for text-to-image generation. In CVPR, 2024. [90] Tao Zhang, Cheng Da, Kun Ding, Kun Jin, Yan Li, Tingting Gao, Di Zhang, Shiming Xiang, and Chunhong Pan. Diffusion model as noise-aware latent reward model for step-level preference optimization. arXiv preprint arXiv:2502.01051, 2025. [91] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-Sora: Democratizing efficient video production for all, March 2024. URL https://github.com/hpcaitech/Open-Sora."
        },
        {
            "title": "A Detailed Experimental Setup",
            "content": "In this section, we provide full details on the datasets, baselines, evaluation settings, and the training and inference implementation details of our model. A.1 Training Data Following common practice in diffusion model post-training [15, 35, 69], we curate high-quality data subset from existing large-scale video datasets [12, 79]. We mostly follow [56] to filter the resolution, duration, aesthetic quality, and motion strength of videos. Inspired by VideoAlign [45], we further apply GPT-4o [3] to only retain prompts with non-trivial motion. GPT-4o Prompt Filtering Template Please help me classify if text prompt contains challenging dynamics. Ignore camera motion and description of the background in the text prompt. Only focus on foreground objects. In general, we are looking for complex motion that requires precise, coordinated movement such as doing sports. Here are some good examples: 1. skateboarder performs jumps. 2. On rainy rooftop, pair of hip-hop dancers lock and pop in perfect sync. 3. figure skater executes powerful leap. 4. woman transitions gracefully on an aerial hoop under golden hour light. 5. An acrobat holding handstand on mat. 6. martial artist performs spinning hook kick in misty bamboo forest. Additional rules: 1. Only keep real-time videos, and remove any video that is slow-motion, time-lapse, or aerial shot; 2. Remove videos with more than five major subjects in the scene, such as sports games or group of people doing something; 3. Remove videos with any of the following content: screen-in-screen, screen recording, special effects from video editing, cartoon, animation, TV news, and video games; 4. When not violating the previous rules, you can keep videos with any of the following content: eating, cutting, or any action that causes big deformation of the main object, such as \"A person takes bite of hamburger / cuts steak / squeezes sponge / makes dough\"; 5. Be conservative. If you are uncertain about prompt, please classify it as \"no\". Please reply \"yes\" or \"no\" in the first line of your response. Then, please explain your decision in the second line. will now provide the prompt for you to classify: [PROMPT] The GPT-4o template we used is shown above. Overall, our final dataset contains around 55k high-quality text-video pairs, which we used to generate preference data. Data processing. The training dataset contains videos of different lengths, resolutions, and aspect ratios. Following common practice [55, 91], we use data bucketing, which groups videos into fixed set of sizes. Overall, we sample videos up to 512 resolution and 5s during training. A.2 Baselines Pre-trained model. Our base text-to-video model is latent Diffusion Transformer framework [53]. It leverages MAGVIT-v2 [82] as the autoencoder and stack of 32 DiT blocks as the denoiser Gθ. The autoencoder is similar to the one in CogVideoX [81], which downsamples the spatial dimensions by 8 and the temporal dimension by 4. Each DiT block is similar to the one in Open-Sora [36], which consists of 3D self-attention layer running on all video tokens, cross-attention layer between video tokens and T5 text embeddings [60] of the input prompt, and an MLP. The base model adopts the rectified flow training objective [44, 47]. We mostly follow the design choice of Stable Diffusion 3 [17], e.g., Logit-Normal distribution of noise levels. SFT fine-tunes the pre-trained model on the 55k high-quality dataset described in Appendix A.1 for 5k iterations. We did not observe clear difference between full-model and LoRA [28] fine-tuning, and thus choose LoRA fine-tuning as it is more efficient. 16 VanillaDPO follows the common direct preference optimization (DPO) practice for video diffusion models [45, 63, 78]. We randomly select 30k text prompts from the curated dataset, and generate 2 videos of 5s per prompt to obtain comparison pair. We then ask human annotators to choose better video or tie by considering text alignment, visual quality, and temporal consistency. 2 labelers are assigned to pair, with reviewer to correct any potential errors. This leads to around 10k winning-losing pairs after removing ties. Indeed, human preferences in video are influenced by multiple, sometimes inversely correlated, factors, making it hard to obtain clear preference. StructuralDPO uses the same 30k prompts as VanillaDPO to construct comparison pairs. For text prompt, we sample guidance level η U(0.65, 0.8), and 2 Gaussian noises from 2 random seeds. Then, we add the noise to the ground-truth video corresponding to the text prompt according to η, and denoise from it to generate video. We perform the same human preference annotation process, again leading to around 10k non-tie data pairs. We have tried using different η when generating the video pair. Usually, the video generated with more guidance is preferred over the other one (e.g., has fewer artifacts). However, we observe that the model quickly achieves low DPO loss when trained on this data, yet the quality of generated videos is not improved. We hypothesize that different η leads to statistical differences in generated videos, and the model learns shortcuts using such cues instead of truly understanding human preferences. DPO training hyper-parameters. Both VanillaDPO and StructuralDPO adopt the Flow-DPO loss with constant β, as prior work shows it achieves the best performance [45]. Following prior works [45, 78], we use β = 500 and apply LoRA [28] with rank 128 to fine-tune the video model. We train with the AdamW optimizer [48] and global batch size of 256 for 1k steps. The peak learning rate is set to 1 105 and it is linearly warmed up from 0 in the first 250 steps. gradient clipping of 1.0 is applied to stabilize training. We implement all models using PyTorch [52] and conduct training on 64 NVIDIA A100 GPUs, which takes around 16 hours. Inference hyper-parameters. We use the rectified flow sampler [47] with 40 sampling steps and classifier-free guidance [23] scale of 8 to generate horizontal videos of 512 288 resolution. timestep shifting [17] of 5.66 is applied to further improve results. A.3 DenseDPO Implementation Details For fair comparison with baselines, we only label dense human preferences on 10k randomly sampled video pairs from the StructuralDPO training data, which costs similar amount of human annotation time compared to labeling 30k global binary preferences. The segment length is set to 1s. Overall, more than 80% of video pairs have at least 1 non-tie segment and can be used in DPO training, greatly improving the data efficiency over using global preferences. Since the DPO loss is now averaged over all non-tie tokens within training video pair, we sample video clips that have more than 20% non-tie segments to avoid small effective batch size. All other training and inference hyper-parameters are the same as DPO baselines. A.4 Evaluation Datasets Since our main focus is improving the temporal quality of the pre-trained video model and VanillaDPO, we evaluate on two motion-rich prompt sets. VideoJAM-bench [8] contains 128 prompts focusing on real-world scenarios with challenging motion, ranging from human actions to physical phenomena. MotionBench collects more diverse prompts from existing prompt sets [35, 56, 72]. We run GPT-4o to select prompts with dynamic actions as described in Appendix A.1, resulting in 419 prompts. This prompt set contains more diverse scenes, subjects, and action types. Note that, to ensure fair comparison between methods, we use the original text prompt without any prompt rewriting or extension process. A.5 Evaluation Metrics Inspired by prior works [45, 78], we identify three key aspects in text-to-video generation: visual quality, text alignment, and motion quality. Since VanillaDPO has the motion bias issue that leads 17 to visually pleasing videos with reduced motion strength, we want to evaluate both the smoothness (i.e., temporal consistency) and the strength (i.e., dynamic degree) of the video in disentangled way. Thus, we cannot use metrics like VideoReward [45] as it only contains motion quality\" dimension. VBench [30] is comprehensive benchmark that tests different aspects of video generation model. When testing on custom prompt sets, it supports six dimensions covering the visual quality, temporal consistency, and dynamic degree. Following the official evaluation protocol, we run each model to generate 5 videos using each prompt with 5 random seeds. VisionReward [78] is state-of-the-art video reward model that fine-tunes pre-trained Vision Language Model (VLM) [27] on large-scale human video preference data. It breaks down the human preference into 64 aspects, which can be categorized into 9 dimensions. We take the Alignment\" dimension as text alignment, merge Composition\", Quality\", Fidelity\" dimensions into visual quality, merge Stability\", Physics\", Preservation\" dimensions into temporal consistency, and take the Dynamic\" dimension as dynamic degree to report in the final result. A.6 VLM-based Preference Labeling VLM labelers. We aim to evaluate the effectiveness of existing VLMs in video preference learning. This includes both off-the-shelf models and models fine-tuned for the preference prediction task. For fine-tuned VLMs, we input both videos and take the one with higher score as the winning data: VideoScore [22] is fine-tuned on human-labeled scores on 1-3s short videos. We average all five dimensions of its output as the overall score of video; LiFT [72], VideoReward [45], and VisionReward [78] are all fine-tuned on >5s long videos generated by modern video models. We average their output dimensions or take the overall dimension (if presented) as the final score of video. For off-the-shelf VLMs, we take GPT o3 [1] as we find it to outperform GPT-4o [3] due to the visual reasoning capability. We follow the official guide1 to prompt the model as follows: GPT o3 Video Preference Labeling Template Please help me compare two videos generated by our text-to-video diffusion model. will provide you with frames sampled from the two videos. The two videos are structurally similar (e.g. global layout and motion are similar), so only want to compare their details. Please assess which one has higher quality, i.e., the video that contains fewer artifacts. Pay close attention to visual artifacts such as: - Additional fingers or legs, deformed human limbs, morphing human faces or body parts; - Blurry or distorted objects, slight motion blur is fine, but the object should not be completely distorted; - Abrupt changes in the object, such as objects appearing/disappearing unexpectedly, or anything that should not happen in the real world, e.g., rigid object deforming or melting. Please only answer tie\" (two videos have equal quality), first\" (the first video has fewer artifacts), and second\" (the second video has fewer artifacts), followed by simple explanation. Be conservative in your answer. If you see similar amounts of artifacts in both videos, please choose \"tie\". Only select \"first\" or \"second\" when one video is clearly better than the other. These are frames from the first video, sampled at 8 FPS: Video 1 frames These are frames from the second video, sampled at 8 FPS: Video 2 frames Please compare the two videos and tell me which one is better. To improve accuracy, we apply self-consistency check by reversing the order of Video 1 and Video 2. If the predictions on both orders are the same, we keep it. Otherwise, we treat it as tie. This simple strategy improves the accuracy on short segments by around 10%. We note that there might be better strategies in prompt construction, such as concatenating paired video frames side-by-side, or organizing frames into grid. We leave further investigations for future work. 1https://cookbook.openai.com/examples/gpt_with_vision_for_video_understanding 18 Finally, we design GPT o3 Segment that partitions long videos into short 1s segments to process separately. This gives the dense preference labels compatible with our DenseDPO framework. To obtain global preference for the entire long video, we simply apply majority voting. Preference prediction setup. Prior work [45] pointed out that existing VLMs excel at processing short videos, while falling short on long videos. Therefore, we evaluate two cases: Short Segment that predicts human preferences on 1s clips. We report the accuracy on the 10k dense human preference labels used in DenseDPO training; Long Video that predicts human preferences on the entire video, except for GPT o3 Segment that aggregates segment-level results. We report the accuracy on the 30k binary human preference labels used in StructuralDPO training, which is superset of the previous case. When calculating the prediction accuracy, we skip tie labels and only compute results on segments or videos with non-tie ground-truth preference labels. DPO training setup. We apply StructuralDPO on binary preference labels produced by GPT o3 and VisionReward as it achieves the highest accuracy. We also apply DenseDPO on dense preference labels produced by GPT o3 Segment. Notably, we run VLMs to label video pairs generated from all 55k training data to explore the limit of automatic preference learning performance."
        },
        {
            "title": "B Caveat of Guided Sampling",
            "content": "B.1 Analyzing the Learning Signal As discussed in Sec. 3.2, guided sampling is attractive since it fixes the structure in the preference pair. This neutralizes the motion bias between videos and focuses the comparison on visual artifacts. However, StructuralDPO with guided sampling achieves inferior results. Analyzing its learning signal reveals that it can paradoxically push the model to unlearn the real data distribution. Intuitively, this happens because the learning signal from losing sample typically dominates over the winning one in those regions of video, which correspond to the real data distribution. In this section, we investigate this phenomenon and discuss potential remedies to the issue. For our analysis, we will use the original DPO notation [68] to simplify the exposition and emphasize that the argument applies for the most general diffusion DPO setup. Diffusion DPO training loss is formulated as: L(θ) = (cid:16) (cid:104) (8) 0 ,xl (xw (cid:0)q(xw t1 xw (cid:0)q(xl t1 xl 0)D, tU (0,T ), xw 0,t) pθ(xw 0,t) pθ(xl q(xw t1 xw t1 xl xw 0 ), xl )(cid:1) DKL t)(cid:1) + DKL + DKL DKL tq(xl log σ txl 0) (cid:0)q(xw t1 xw (cid:0)q(xl t1 xl βT (cid:0) 0,t) pref (xw 0,t) pref (xl t1 xw t1 xl )(cid:1) t)(cid:1) (cid:1)(cid:17)(cid:105) (9) , (10) 0 , xl where (xw 0) is the winning-losing preference pair, is the number of diffusion steps, U(0, ) is the noise level distribution, and q(xt x0) = (xt αtx0, σtI). In Diffusion-DPO [68], the objective is further simplified to: L(θ) = ϵw ϵθ(xw (xw 0 ,xl , t)2 0)D, tU (0,T ), xw 2 ϵw ϵref(xw q(xw , t)2 xw tq(xl txl 0) t, t)2 2 ϵl ϵθ(xl 0 ), xl (cid:104) (cid:16) βT ω(t)(cid:0) log σ 2 + ϵl ϵref(xl t, t)2 where ω(t) is weighting function. Lets denote: θ = ϵw ϵθ(xw ref = ϵw ϵref(xw , t)2 2 , t)2 2 Then, the loss gradient is: l θ = ϵl ϵθ(xl ref = ϵl ϵref(xl t, t)2 2 t, t)2 2 θL(θ) = θE (cid:2)log σ (cid:0)βT ω(t) (cid:0)w = (cid:2)(1 σ())βT ω(t) θ(w = (cid:2)C θ(w θ θ))(cid:3) , θ θ ref (l ref (l θ θ ref)(cid:1)(cid:1)(cid:3) ref))(cid:3) 19 (cid:1)(cid:17)(cid:105) , (11) (12) (13) (14) (15) (16) (17) where = (1 σ())βT ω(t). θ and θl One can show that > 0 since σ() (0, 1) and β, T, ω(t) > 0. Since > 0, the per-pixel direction of the incoming gradient w.r.t. the model outputs is determined entirely by the interplay between θw θ. This fact becomes crucial for StructuralDPO with guided sampling for the following reason. In guided sampling (see Algo. 2), we generate winning/losing sample = (1 η)x0 + ηϵ using real video x0. This makes them carry similar structure, which means that the winning and losing samples share many pixels. Moreover, these shared pixels normally correspond to the ground-truth data distribution. Lets split the pixels into two sets, Isame and Iunique: Isame(xw, xl) = {p xw Iunique(xw 0 [p] xl 0) = {p Isame}, where is pixel location (e.g., typically, triplet (i, j, k) denoting the frame, height, width indices). In this way, Isame stores pixel locations which remained intact during the forward diffusion process of guided sampling and subsequent denoising with Gθ. Fig. 2 and Fig. 11 illustrate this: many pixels in the winning and losing samples are identical and correspond to the original ground-truth video. In this way, one can argue that Isame corresponds to the real data distribution. 0[p] x0[p]}, 0 , xl (19) (18) Lets denote: ()[I] = ϵ[I] ϵ ()(x , t)[I]2 2, (20) i.e., the diffusion loss in particular pixel locations I. Now, if θ[Isame] (meaning that the diffusion error in ground-truth pixel locations of losing sample is higher than that of the winning one), then the gradient will be dominated by the negative contribution of θ[Isame], and the model will be unlearning real data distribution. Turns out, this is exactly what is happening in practice: θ [Isame] < (cid:0)q(xl Prior to DPO, the model undergoes extensive supervised fine-tuning (SFT), so it is reat )(cid:1) < sonable to expect that at initialization, we have DKL 0,t) pθ(xw DKL is closer in distribution to winning pairs and interprets the entire image as unlikely when there are artifacts )(cid:1) < present DKL (cid:0)q(xw the model (cid:0)q(xw θ (see Appendix 2 of [68]). The condition DKL t)(cid:1) implies θ < in some part of it. t1 xl t)(cid:1), meaning that 0,t) pθ(xw 0,t) pθ(xl 0,t) pθ(xl t1 xw t1 xw t1 xw t1 xw t1 xl t1 xl t1 xl (cid:0)q(xl The model suffers from an internal distribution shift in the presence of artifacts and its predictions in good pixel locations deteriorate in the presence of artifacts. Besides, it might be shifting its capacity towards rectifying the artifacts, so the rest of the output suffers. Given the above two observations, we conclude that the model will be unlearning the real data distribution in the StructuralDPO setting. While the above analysis is outlined for diffusion models with ϵ-prediction parametrization [24], it holds for v-prediction as well (used in both our and many contemporary works that align rectified flow models [47]) with argument derivation being basically the same. We also emphasize that even marginal domination of the gradient from losing sample over the one from the winning sample results in such unlearning behavior. Moreover, practitioners commonly use the same noise seed (i.e., ϵw = ϵl) for both the winning and losing samples, following the original DiffusionDPO implementation.2 This exacerbates the problem: the learning signal from losing sample no longer dominates merely in expectation per pixel, but at each training step, entirely suppressing the learning signal from the winning sample. Several prior works observed similar behavior in DPO on language models [19, 51, 61]. For example, DPO-Positive [51] shows that on datasets with short edit distances between winning and losing samples, DPO may lead to reduction in the models likelihood on the preferred examples. DenseDPO is natural way to eliminate this shortcoming of StructuralDPO since it is designed to provide dense per-pixel DPO objective and would allow to treat the pixels from Isame (i.e., similar pixels) as ties, thus removing any loss on them. In the ideal world, we would love to have per-pixel preference labels for DenseDPO training, but, as our work demonstrates, even coarse-level temporal ones allow us to recover and improve the DPO performance. 2https://github.com/SalesforceAIResearch/DiffusionDPO/blob/main/train.py#L1053-L1055 20 One can also investigate other strategies to mitigate the issue by taking into account the similarity between pixels (e.g., uncertainty-based or margin-aware DPO [26, 46]) or reformulating the DPO in some novel way without maximizing the loss of losing samples. We leave this for future work. B.2 Empirical Verification While the previous section presents our theoretical argument, we must verify empirically whether the assumption regarding the dominance of the losing samples loss over the winning sample indeed holds. Here, we address this question through empirical analysis using the Flux-dev [37] model. We deliberately chose popular, open-source v-prediction model to ensure our conclusions remain general and reproducible rather than specific to our internal video model. We constructed synthetic dataset containing controlled artifacts to facilitate this analysis. Specifically, we selected 5, 195 real-world images with resolution of 10242 and artificially corrupted them by applying blur to their central patches. Each image is encoded using the FluxAE encoder, resulting in latent tensor of dimensions 128 128 16. We then applied increasing levels of corruption to the central 32 32 patch (representing 6.25% of the image) with the following blur intensities: 1. No corruption (0% blur) 2. Blur with = 2 (6.25% of the patch size) 3. Blur with = 4 (12.5% of the patch size) 4. Blur with = 8 (25% of the patch size) 5. Blur with = 16 (50% of the patch size) Figure 6: Visualization of losing sample loss dominance in uncorrupted regions. Top: example images with progressively increasing blur in the central region (note, that the visualization corresponds to an equally-corrupted RGB image, rather than the decoded corrupted latent tensor). Bottom: the per-pixel loss difference δL between losing and winning samples averaged across latent channels. Positive values indicate the dominance of losing sample losses, driving the model to unintentionally degrade predictions in artifact-free areas. For clarity, we clamp maximum values in the visualization to the maximum loss difference observed in uncorrupted regions, preventing extreme outliers from dominating the heatmap. The resulting dataset comprises five image variants, progressively more corrupted, visualized in Fig. 6 (top). This setup allows separate measurement of losses in corrupted and uncorrupted regions. Subsequently, we randomly sample timestep U[0, 1], estimate velocities vFlux(xw , t) and vFlux(xl t, t), and compute per-pixel losses (averaged over the 16 latent channels C): Lw = Ll = 1 1 (cid:88) c=1 (cid:88) c=1 (vFlux(xw , t)[c] (ϵ xw 0 )[c])2 (vFlux(xl t, t)[c] (ϵ xl 0)[c])2, 21 (21) (22) Figure 7: Average loss difference mean(δL) in uncorrupted regions as function of artifact severity. Increasing blur severity amplifies the losing samples negative learning signal in good (uncorrupted) regions, illustrating the risk of inadvertently unlearning correct predictions. (a) StructuralDPO vs. VanillaDPO (b) DenseDPO vs. pre-trained model Figure 8: Human evaluation on the VideoJAM-bench dataset. TA, VQ, TC, DD stand for text alignment, visual quality, temporal consistency, and dynamic degree. where ()2 denotes element-wise squaring. Next, for each sample, we calculate the loss difference δL Rhw as follows: δL = Ll Lw. (23) δL indicates the extent to which the losing samples loss surpasses that of the winning sample. If this dominance occurs in Isame, it implies the model is unlearning these areas due to the negative contribution from the losing sample. Loss computations use the same noise seed for the corrupted and uncorrupted images (as is usually done in practice). Fig. 6 (top) illustrates representative example of corruption. Visualizations in Fig. 6 (bottom) clearly demonstrate the loss dominance of the losing samples in the uncorrupted regions. As we maximize the loss of the losing sample and minimize that of the winning sample, this results in the model increasing loss in uncorrupted image regions when trained with DPO to mitigate blurring artifacts. Fig. 7 provides quantitative analysis, demonstrating how increasing artifact severity intensifies the negative learning signal from losing samples in uncorrupted image areas."
        },
        {
            "title": "C Additional Experimental Results",
            "content": "C.1 DPO with Human Labels Human evaluation. Fig. 8 presents additional user study on VideoJAM-bench. Fig. 8a shows that StructuralDPO outperforms VanillaDPO in dynamic degree as it performs DPO on video pairs with similar motion. Yet, it underperforms in other dimensions. Fig. 8b shows that DenseDPO consistently beats the pre-trained model in all dimensions. 22 Figure 9: Text-to-video results with our DenseDPO aligned model. Here, we show generation of challenging human activities, novel animal actions, and physical phenomena. Please check out our project page for video results of baselines and our methods. Qualitative results. We show more videos generated by DenseDPO in Fig. 9. Fig. 10 compares DenseDPO with baselines. Overall, DenseDPO aligned model generates videos with high visual quality, rich motion, and precise text alignment. Please check out our project page for video results of baselines and our methods. C.2 DPO with VLM Labels Motion bias in VLMs. As discussed in Sec. 3.2, there is motion bias in human preference annotationhuman labelers tend to favor artifact-free slow-motion clips over dynamic clips with artifacts. Ge et al. [21] pointed out that video metrics such as FVD [67] are also biased towards per-frame visual quality rather than temporal motion quality. Here, we study whether more advanced VLM-based video reward models also suffer from this issue. We test two state-of-the-art models, VisionReward [78] and VideoReward [45]. We randomly sample 10k videos from the VanillaDPO training data, each having 121 frames (5s). For each video, we construct static version of it by duplicating one frame of it, and we compare this static video with the original video. Tab. 5 presents the winning rate of static vs. original video. Surprisingly, VisionReward favors static videos over original videos with sizable gap, indicating clear motion bias. In contrast, VideoReward prefers original videos in around 80% of cases. We note that both VisionReward and 23 Table 5: Winning rate of static video vs. original video measured by video reward models. The static video is constructed by duplicating frame to the video length, where we tested frame 0, 24, 48, and 96 here. lower winning rate means the video reward model is more sensitive to motion. Method Frame 0 vs. Frame 96 vs. Original video Original video Original video Original video Frame 24 vs. Frame 48 vs. VisionReward [78] VideoReward [45] 70.63% 20.33% 68.28% 17.76% 69.84% 17.72% 69.06% 17.91% Table 6: Ablation on segment length of dense preference labels. We report VBench [30] and VisionReward [78] metrics on VideoJAM-bench [8]. All models are only trained on 5k videos. VBench Metrics VisionReward Metrics Method Aesthetic Imaging Temporal Dynamic Quality Quality Consistency Consistency Smoothness Degree Alignment Quality Consistency Degree Background Motion Dynamic Subject Visual Text Pre-trained = 0.5 = 1 (Ours) = 2 54.65 55.57 55.62 55.40 55.85 57.03 56.13 56.10 88. 89.88 89.23 89.04 91.50 92.76 92.48 91.96 92.40 92.52 92.99 92.54 84. 84.25 84.25 84.15 0.770 0.811 0.806 0.795 0.192 0.326 0.305 0.291 0. 0.601 0.589 0.557 0.680 0.643 0.655 0.623 VideoReward output multi-dimensional scores (e.g., visual quality, dynamic degree), and aggregate them to predict the binary human preference. VideoReward simply averages all dimensions, and thus is not biased to any dimension. VisionReward instead first labels human preferences on video pairs, and then learns per-dimension weights via logistic regression. This inevitably inherits the motion bias in human labels. Indeed, Tab. 25 in the VisionReward paper [78] reveals that human preference is negatively correlated with object dynamics, while positively correlated with temporal smoothness. These results suggest that it is better to label per-dimension scores and predict them, instead of predicting an overall score. The bias in human preferences will be leaked into the reward model if we train the model to regress it. We note that our analysis is still preliminary. Further investigations similar to [21] are required to fully understand the bias in recent VLM-based video reward models. GPT dense preference label. We visualize some dense preferences predicted by GPT o3 Segment in Fig. 11. Overall, GPT is able to identify obvious artifacts such as distorted faces and deformed limbs. With our carefully designed prompt and self-consistency check, it only predicts preference when there is clear difference between two segments. However, it still does not understand complex motion, such as playing tennis and cartwheels. This is partially because GPT o3 Segment only has access to 1s video clips, which is too short to finish these actions. C.3 Ablation Study Dense label granularity. We study the impact of the segment length in dense preference labels. By default, = 1 is used in all our experiments. Here, we tested = 0.5 and = 2. Due to the high annotation cost, we only label 5k videos for each segment setting in this study. Tab. 6 compares DenseDPO trained on 5k videos using different s. = 1 consistently outperforms = 2 due to more fine-grained preference annotation. Interestingly, = 0.5 performs similarly to = 1. We hypothesize that this is because 0.5s is too shorta longer context window is needed to assess the temporal aspect of videos. In addition, labeling dense preference at = 0.5 is 2 expensive compared to = 1. Therefore, we choose to label 1s segments in our experiments."
        },
        {
            "title": "D Limitations and Future Works",
            "content": "Similar to prior works [45, 78], we also observed unstable training and reward hacking when finetuning the entire model. As result, we have to rely on LoRA training and early stopping. This is in stark contrast to DPO in large language models, where DPO training is relatively stable. More investigation on diffusion DPO basics is needed to resolve this issue. 24 To mitigate the motion bias in VanillaDPO, we propose guided sampling [49] to generate structurallysimilar pairs. However, this reduces the variations in comparison pairs, degrading the DPO performance. We note that image-to-video generation with the same conditioning image is another way to retain similar structure between video pairs. In addition, it allows more variations in the generated videos, which may improve StructuralDPO performance. Although this work focuses on improving visual quality and consistency while holding motion dynamics constant, the same methodology can be extended to other dimensions. For example, slightly perturbing the text prompt during guided synthesis could support better alignment with textual descriptions. It may even lead to preference labels for-free\", e.g., the video generated with the original prompt is often better than the one generated with the perturbed prompt. Similar approaches have also been studied in [10], where they intentionally distort video via inversion. 25 Figure 10: Qualitative comparison with baselines. Pre-trained model often generates deformed human body or unnatural object composition. VanillaDPO fixes these artifacts, but with significantly reduced dynamics. StructuralDPO retains the dynamics, but generates oversaturated frames or some artifacts. DenseDPO strikes the best balance over these dimensions. Please check out our project page for video results of baselines and our methods. 26 Figure 11: Uncurated samples of GPT o3 predicted dense preference labels. Each sample consists of pair of structurally similar videos generated via guided sampling. Videos are sampled at 2 FPS. top red bar means GPT prefers the first example, and bottom green bar means GPT prefers the second example, otherwise it is tie. We highlight some obvious artifacts with blue rectangles. 27 Pytorch-style Pseudo Code for StructuralDPO and DenseDPO StructuralDPO applies the same Flow-DPO objective as VanillaDPO, which is adopted from [45]: def flow_dpo_1oss ( model , ref_model , x_0 , x_1 , , , beta ) : \"\"\" # model : Flow model that takes text embeddings and timestep # as inputs and predicts velocity # ref_model : Pre - trained model that is frozen # x_0 : The first video in the pair , shape [B , , , , ] # x_1 : The second video in the pair , shape [B , , , , ] # : Text embedding of the prompt # : Preference label , shape [ ] , each item is either +1 or -1 # # beta : DPO regularization hyper - parameter # returns : Flow - DPO loss value \"\"\" # Add noise to videos = lo git _no rm al_ sa mp ler ( x_0 . shape [0]) noise = torch . randn_1ike ( x_0 ) noisy_x_0 = (1 - ) * x_0 + * noise noisy_x_1 = (1 - ) * x_1 + * noise +1 means x_0 is better than x_1 , -1 means the other way # Compute velocity prediction loss v_0_pred = model ( noisy_x_0 , , ) v_1_pred = model ( noisy_x_1 , , ) v_ref_0_pred = ref_model ( noisy_x_0 , , ) v_ref_1_pred = ref_model ( noisy_x_1 , , ) v_0_gt = noise - x_0 v_1_gt = noise - x_1 model_0_err = (( v_0_pred - v_0_gt ) ** 2) . mean ( dim =[1 , 2 , 3 , 4]) model_1_err = (( v_1_pred - v_1_gt ) ** 2) . mean ( dim =[1 , 2 , 3 , 4]) ref_0_err = (( v_ref_0_pred - v_0_gt ) ** 2) . mean ( dim =[1 , 2 , 3 , 4]) ref_1_err = (( v_ref_1_pred - v_1_gt ) ** 2) . mean ( dim =[1 , 2 , 3 , 4]) # Compute DPO loss diff_0 = model_0_err - ref_0_err diff_1 = model_1_err - ref_1_err inside_term = -0.5 * beta * * ( diff_0 - diff_1 ) loss = -1 * log ( sigmoid ( inside_term ) ) . mean () return loss # Shape [ ] # Shape [ ] DenseDPO extends the Flow-DPO loss to frame-level (or token-level for latent models): def flo w_de ns e_d po _1 oss ( model , ref_model , x_0 , x_1 , , , beta ) : \"\"\" # model : Flow model that takes text embeddings and timestep # as inputs and predicts velocity # ref_model : Pre - trained model that is frozen # x_0 : The first video in the pair , shape [B , , , , ] # x_1 : The second video in the pair , shape [B , , , , ] # : Text embedding of the prompt # : Dense preference label , shape [B , ] , can be +1 , 0 , or -1 # # # beta : DPO regularization hyper - parameter # returns : Flow - DPO loss value \"\"\" # Add noise to videos = lo git _no rm al_ sa mp ler ( x_0 . shape [0]) noise = torch . randn_1ike ( x_0 ) noisy_x_0 = (1 - ) * x_0 + * noise noisy_x_1 = (1 - ) * x_1 + * noise +1 means x_0 is better than x_1 , -1 means the other way 0 means tie # Compute velocity prediction loss v_0_pred = model ( noisy_x_0 , , ) v_1_pred = model ( noisy_x_1 , , ) v_ref_0_pred = ref_model ( noisy_x_0 , , ) v_ref_1_pred = ref_model ( noisy_x_1 , , ) v_0_gt = noise - x_0 v_1_gt = noise - x_1 model_0_err = (( v_0_pred - v_0_gt ) ** 2) . mean ( dim =[2 , 3 , 4]) model_1_err = (( v_1_pred - v_1_gt ) ** 2) . mean ( dim =[2 , 3 , 4]) ref_0_err = (( v_ref_0_pred - v_0_gt ) ** 2) . mean ( dim =[2 , 3 , 4]) ref_1_err = (( v_ref_1_pred - v_1_gt ) ** 2) . mean ( dim =[2 , 3 , 4]) # Compute DPO loss diff_0 = model_0_err - ref_0_err diff_1 = model_1_err - ref_1_err inside_term = -0.5 * beta * * ( diff_0 - diff_1 ) inside_term = inside_term [ != 0] loss = -1 * log ( sigmoid ( inside_term ) ) . mean () return loss # Shape [B , ] # Shape [B , ] # Only take non - tie frames"
        }
    ],
    "affiliations": [
        "Snap Research",
        "University of Toronto",
        "Vector Institute"
    ]
}