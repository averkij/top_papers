{
    "paper_title": "ARC-Hunyuan-Video-7B: Structured Video Comprehension of Real-World Shorts",
    "authors": [
        "Yuying Ge",
        "Yixiao Ge",
        "Chen Li",
        "Teng Wang",
        "Junfu Pu",
        "Yizhuo Li",
        "Lu Qiu",
        "Jin Ma",
        "Lisheng Duan",
        "Xinyu Zuo",
        "Jinwen Luo",
        "Weibo Gu",
        "Zexuan Li",
        "Xiaojing Zhang",
        "Yangyu Tao",
        "Han Hu",
        "Di Wang",
        "Ying Shan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Real-world user-generated short videos, especially those distributed on platforms such as WeChat Channel and TikTok, dominate the mobile internet. However, current large multimodal models lack essential temporally-structured, detailed, and in-depth video comprehension capabilities, which are the cornerstone of effective video search and recommendation, as well as emerging video applications. Understanding real-world shorts is actually challenging due to their complex visual elements, high information density in both visuals and audio, and fast pacing that focuses on emotional expression and viewpoint delivery. This requires advanced reasoning to effectively integrate multimodal information, including visual, audio, and text. In this work, we introduce ARC-Hunyuan-Video, a multimodal model that processes visual, audio, and textual signals from raw video inputs end-to-end for structured comprehension. The model is capable of multi-granularity timestamped video captioning and summarization, open-ended video question answering, temporal video grounding, and video reasoning. Leveraging high-quality data from an automated annotation pipeline, our compact 7B-parameter model is trained through a comprehensive regimen: pre-training, instruction fine-tuning, cold start, reinforcement learning (RL) post-training, and final instruction fine-tuning. Quantitative evaluations on our introduced benchmark ShortVid-Bench and qualitative comparisons demonstrate its strong performance in real-world video comprehension, and it supports zero-shot or fine-tuning with a few samples for diverse downstream applications. The real-world production deployment of our model has yielded tangible and measurable improvements in user engagement and satisfaction, a success supported by its remarkable efficiency, with stress tests indicating an inference time of just 10 seconds for a one-minute video on H20 GPU."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 9 3 9 0 2 . 7 0 5 2 : r ARC-Hunyuan-Video-7B: Structured Video Comprehension of Real-World Shorts Yuying Ge1,, Yixiao Ge1,,, Chen Li1,, Teng Wang1,, Junfu Pu1,, Yizhuo Li1,, Lu Qiu1,, Jin Ma2, Lisheng Duan2, Xinyu Zuo2, Jinwen Luo2, Weibo Gu3, Zexuan Li4, Xiaojing Zhang2, Yangyu Tao3, Han Hu3, Di Wang3, Ying Shan1 1ARC Lab, Tencent PCG, 2Search Application Department, Tencent CSIG, 3Tencent Hunyuan, 4Big Data Platform Department, Tencent PCG Core contributors, Project lead Real-world user-generated short videos, especially those distributed on platforms such as WeChat Channel and TikTok, dominate the mobile internet. However, current large multimodal models lack essential temporally-structured, detailed, and in-depth video comprehension capabilities, which are the cornerstone of effective video search and recommendation, as well as emerging video applications. Understanding real-world shorts is actually challenging due to their complex visual elements, high information density in both visuals and audio, and fast pacing that focuses on emotional expression and viewpoint delivery. This requires advanced reasoning to effectively integrate multimodal information, including visual, audio, and text. In this work, we introduce ARC-Hunyuan-Videoa, multimodal model that processes visual, audio, and textual signals from raw video inputs end-to-end for structured comprehension. The model is capable of multi-granularity timestamped video captioning and summarization, open-ended video question answering, temporal video grounding, and video reasoning. Leveraging high-quality data from an automated annotation pipeline, our compact 7B-parameter model is trained through comprehensive regimen: pre-training, instruction fine-tuning, cold start, reinforcement learning (RL) post-training, and final instruction fine-tuning. Quantitative evaluations on our introduced benchmark ShortVid-Bench and qualitative comparisons demonstrate its strong performance in real-world video comprehension, and it supports zero-shot or fine-tuning with few samples for diverse downstream applications. The real-world production deployment of our model has yielded tangible and measurable improvements in user engagement and satisfaction, success supported by its remarkable efficiency, with stress tests indicating an inference time of just 10 seconds for one-minute video on H20 GPUb. Date: July 28, 2025 Code: https://github.com/TencentARC/ARC-Hunyuan-Video-7B aThe version supports Chinese and English videos and particularly excels at Chinese. bThe reported inference time is based on deployment accelerated with vLLM."
        },
        {
            "title": "1 Introduction",
            "content": "The explosive growth of user-generated short videos on platforms such as WeChat Channel and TikTok has fundamentally reshaped mobile internet consumption. These short videos, characterized by their brevity, diversity, and high engagement, have become dominant medium for information sharing, entertainment, and social interaction. This shift has created an urgent need for structured, temporally-aware, and in-depth comprehension of real-world video content, which is essential for enabling wide range of video-centric applications, including search, recommendation, and emerging intelligent video services. Yet, the very characteristics that make these videos so appealing also present significant challenges for automated understanding. These videos typically contain dense visual elements (e.g., visual effects, text overlays), high-information audios (including speech and sound effects), and rapid pacing that emphasizes emotional expression and viewpoint delivery. Effectively comprehending such complex content requires not only multimodal integration of visual, audio, and textual information, but also advanced reasoning capabilities Figure 1 Model capabilities of ARC-Hunyuan-Video-7B, which supports multi-granular timestamped captioning (output time span and corresponding description), summarization, temporal grounding, and open-ended question answering through integrating and reasoning over both visual and audio cues in the user-generated short videos. to grasp the core intent of the content. However, current multimodal models, which are primarily designed for general video understanding, struggle to address these challenges and fall short of delivering the level of comprehension required for real-world applications. To bridge this gap, we introduce ARC-Hunyuan-Video, multimodal model for comprehensive understanding of real-world short videos. Our model processes visual, audio, and textual inputs to achieve what we term Structured Video Comprehension: the ability to decompose video into its constituent events and narrative elements with temporal precision. This includes generating multi-granularity timestamped video caption and summary, answering open-ended questions through video reasoning, and performing accurate temporal grounding of events as illustrated in Fig. 1. This structured understanding is crucial for real-world scenarios, as it allows the model to move beyond surface-level analysis and truly understand what happens in user-generated content, when it happens, why it matters, and what intentions the creator wanted to convey. Specifically, ARC-Hunyuan-Video is built on top of the Hunyuan-7B vision-language model and has two key incremental designs to meet the requirements of effective structured video comprehension: (1) an extra audio encoder with fine-grained visual-audio synchronization for temporally aligned visual-audio inputs, and (2) timestamp overlay mechanism on visual frames that explicitly provides the model with temporal awareness for accurate event localization. Additionally, we collect millions of in-house real-world videos and develop totally automated bootstrapped annotation pipeline that generates high-quality data, enabling comprehensive training regimen. This includes (i) pre-training for fundamental knowledge and atomic capability acquisition, (ii) instruction fine-tuning for task alignment, (iii) cold-start initialization, (iv) reinforcement learning (RL) post-training, and (v) final instruction fine-tuning using high-quality human-annotated data and trajectories obtained through rejection sampling. key aspect of our RL strategy is the design of objective questions such as multiple-choice questions and temporal grounding to enhance the models holistic comprehension of the video. This design is grounded in our pilot experiments, which demonstrate that verifiable tasks with RL significantly benefits the learning of high-quality subjective data (e.g., video summary). To rigorously evaluate our models ability to understand real-world short videos, we construct specialized, human-annotated benchmark named ShortVid-Bench with multiple-choice questions. Empirical evaluations demonstrate that our compact 7B-parameter model not only achieves exceptional performance in real-world video understanding on our proposed benchmark, but also excels in temporal video grounding benchmarks. Furthermore, ARC-Hunyuan-Video exhibits strong versatility, supporting zero-shot inference for range of tasks and adapting to various downstream applications, such as video abstract for search and tagging for recommendation, with minimal fine-tuning data required. The deployment of our fine-tuned model in real-world product scenarios has resulted in significant and measurable improvements in user engagement and satisfaction. This success is underpinned by the models remarkable efficiency: stress test reports show an inference time of just 10 seconds for one-minute video on NVIDIA H20 GPU, yielding an average of 500 tokens, with inference accelerated by the vLLM (Kwon et al., 2023) framework. To facilitate further research and application, we have open-sourced both the model checkpoint, API, and inference code. We hope that ARC-Hunyuan-Video will contribute to advancing the field of structured video comprehension and inspire new developments in the comprehension of real-world short videos."
        },
        {
            "title": "2 Related Work",
            "content": "Short-form video has emerged as dominant medium for communication, entertainment, and information dissemination across social media platforms. The ability to automatically understand such content is crucial for wide range of downstream applications, including content retrieval, personalized recommendation, automated video tagging, and content moderation. Despite its importance, comprehending real-world short videos presents unique challenges. These videos are characterized by dense visual elements (e.g., dynamic effects, text overlays), rich audio streams (speech, music, sound effects), and rapid narrative pacing that emphasizes emotional expression and viewpoint delivery. These characteristics necessitate joint modeling of visual, audio, and textual modalities with advanced reasoning to comprehend key events, temporal relationships, and decipher underlying intentions. Recent efforts have sought to address these challenges. Our concurrent work Keye-VL-8B (Team et al., 2025) introduces multimodal foundation model designed specifically for short-video understanding. However, Keye-VL-8B does not directly integrate raw audio signals; instead, it relies on transcripts generated by automatic speech recognition (ASR). This approach discards important non-speech audio cues, such as emotional tone and environmental sounds, and can lead to temporal misalignment between audio and visual content. Meanwhile, audio-visual LLMs (Shu et al., 2023; Zhang et al., 2023; Sun et al., 2024, 2025; Xu et al., 2025; Tang et al., 2025) have been developed to jointly process video, audio, and text input, but they focus on video understanding of general scenarios, which feature slower pacing, and lower information density. As result, they often struggle to capture the dynamic, fast-paced, and information-rich nature of user-generated short videos. In this work, we propose ARC-Hunyuan-Video, compact 7B-parameter multimodal model that achieves structured video comprehension of real-world shorts through synchronized audio-visual-text processing and sophisticated reasoning."
        },
        {
            "title": "3 Method",
            "content": "Figure 2 (a) Model architecture. Built upon the Hunyuan-7B VLM, we incorporate an audio encoder with fine-grained visual-audio synchronization to obtain temporally aligned multimodal inputs. Timestamps are overlaid on visual frames to provide the model with temporal awareness.(b) Training stages including pre-training, instruction fine-tuning, cold start initialization, RL post-training and final instruction fine-tuning using high-quality human-annotated data and trajectories selected via rejection sampling."
        },
        {
            "title": "3.1 Model Architecture\nAs shown in Fig. 2, ARC-Hunyuan-Video is built upon the Hunyuan-7B vision-language model (VLM), with\nan additional audio encoder and fine-grained visual-audio synchronization mechanism to obtain temporally\naligned multimodal features as input to the LLM. To explicitly provide the model with temporal awareness,\nwe directly overlay the timestamp of each sampled frame onto its corresponding visual frame.",
            "content": "Visual Encoding. To process the visual input, we first sample frames at rate of one frame per second (1 fps). For videos exceeding 150 seconds, we uniformly sample total of 150 frames to maintain manageable sequence length. To enhance models temporal awareness, we explicitly render the corresponding timestamp of each frame in an HH:MM:SS format onto its top-right corner. This provides the model with direct, explicit signal for temporal localization.These timestamped frames are then fed into pre-trained Vision Transformer (ViT) (Dosovitskiy et al., 2020) encoder. While the Hunyuan ViT architecture inherently supports dynamic input resolutions, we resize all frames to fixed resolution of 640640 for enabling the visual-audio synchronization detailed below. For each input frame, the ViT encoder outputs sequence of 112 visual tokens. Audio Encoding. For the audio modality, we leverage the pre-trained audio encoder from Whisper (Radford et al., 2023). The raw audio waveform is first segmented into 30-second chunks. The encoder processes each chunk to produce sequence of 1500 feature tokens. For videos longer than 300 seconds, we split the audio into exactly 150 segments and truncate each to the initial 2 seconds before encodinga design choice that optimizes temporal synchronization with visual frames. Finally, the audio features extracted by the audio encoder are passed through multi-layer perceptron (MLP). This projection layer aligns the dimensionality of the audio features with that of the visual tokens, preparing them for fusion. Visual-audio Synchronization. The fine-grained synchronization aims to obtain multimodal representation where visual and audio tokens that correspond to the same time interval are fused, ensuring that the LLM receives temporally aligned multimodal signals. We adopt an adaptive and parameter-free synchronization strategy based on video duration. Specifically, for each sampled video frame, we align and fuse the corresponding audio segment by zero-padding the audio tokens to match the number of visual tokens, then adding them to form synchronized multimodal embeddings. This approach ensures that, regardless of video length, each fused embedding consistently represents the same temporal interval. The resulting sequence of synchronized embeddings, with positional encodings added, is then input to the LLM."
        },
        {
            "title": "3.2 Pre-training",
            "content": "Figure 3 Our automated bootstrapped annotation pipeline for pre-training. It extracts timestamped speech via ASR model and frame-level descriptions via MLLM; these, along with meta information (e.g., title), are input to an LLM for initial video annotation. The annotated data is used to train an initial version of the model, whose inference results are further integrated to produce the final annotations."
        },
        {
            "title": "3.2.1 Annotation pipeline",
            "content": "To generate high-quality video descriptions and summaries that capture the essence of real-world short videos (i.e., truly understanding the content), we designed an automated bootstrapped annotation pipeline as shown in Fig. 3. This pipeline is structured to iteratively refine annotations through self-improving process, ensuring robust and accurate multimodal integration. We begin by extracting and integrating multimodal information from videos using series of specialized models. Specifically, we employ Whisper-v3 (Radford et al., 2023) to transcribe speech with precise timestamps, providing synchronized audio-text data. Concurrently, we use InternVL-2.5-8B (Chen et al., 2024) to generate detailed captions and detect textual overlays (e.g., on-screen text) for sampled video frames. These outputs, combined with video metadata such as titles, are fed into closed-source large language model (LLM) for comprehensive synthesis. To ensure the model truly understands the video, we implement Chain-ofThought (COT) prompting strategy. This guides the LLM to first output intermediate elements: step-by-step description of events, the creators attitude, and potential audience-interest tags. These elements are then integrated into final summary that encapsulates the videos core intent, emotional expression, and viewpoint delivery. Leveraging the initial annotations, we train preliminary version of our model. This model is then deployed to generate new descriptions and summaries, which are combined with the outputs from the initial pipeline (e.g., speech transcriptions, frame captions, and metadata). The aggregated data is reprocessed through the same closed-source LLM for polishing, using the COT approach to resolve inconsistencies and enrich detail. This iterative loop, where the models own outputs refine the annotations, yields high-quality, final descriptions and summaries that are used for pre-training."
        },
        {
            "title": "3.2.2 Pre-training data\nVideo description and summary. Using our automated bootstrapped annotation pipeline, we annotate 4.5M\nshort-form videos with detailed descriptions and summaries. Additionally, to ensure general video understand-\ning capabilities, we apply the same pipeline to annotate 0.2M publicly available academic videos.",
            "content": "Image caption and OCR. Frame-level image understanding, including captioning and OCR, is one of the fundamental capabilities for short-form video comprehension. Therefore, we leverage the frame-level captions and OCR results obtained during the video annotation process as training data, resulting in total of 4.7M image-text pairs. ASR. Automatic speech recognition (ASR) is another fundamental capability for understanding short-form videos. Therefore, we utilize the ASR results obtained during the video annotation process as training data. We further employ an LLM to filter out transcribed speech samples without meaningful semantics (retaining only small portion and labeling them as no speech detected), resulting in total of 3.2M audio-text pairs. Video temporal grounding. Precise temporal video grounding, aligning textual queries with specific temporal segments within video, is essential for structured comprehension, which improves the models temporal awareness. To develop this capability, we leverage diverse collection of 0.5M temporally grounded instances sourced from multiple public datasets. These instances provide paired textual queries (comprising natural language descriptions or questions) and their corresponding temporal intervals within the videos. Video multi-granular caption. To support multi-granular video captioning, our dataset includes both eventlevel and chapter-level captions, each paired with their corresponding time spans and descriptive texts. For event-level captions, we leverage timestamped annotations from public datasets, applying filters to remove videos with excessive segment overlap or insufficient caption coverage relative to video duration, resulting in 50K high-quality samples. For chapter-level captions, we automatically generate captions for the time spans in 80K in-house videos, where the time spans themselves were annotated by humans."
        },
        {
            "title": "3.2.3 Training Recipe",
            "content": "Building upon the pre-trained Hunyuan-7B VLM with established visual understanding capabilities, our training adopts progressive two-stage strategy to integrate audio modality while preserving core competencies. In the first stage, we conduct warm-up training using Automatic Speech Recognition (ASR) data to adapt the model to audio feature inputs as shown in Fig. 2. To prevent degradation of existing visual understanding, this stage simultaneously incorporates image-text pair data. When modality is missing, we feed an all-zero input into the corresponding modality encoder. The dual-task design ensures the model develops initial audio-text alignment while retaining its foundational visual understanding abilities. In the second stage, we perform full multimodal pre-training (video/audio/text) via next-token prediction. We freeze parameters of both the ViT and audio encoder to preserve their feature extraction capabilities. Only the MLP adapter layers and the full LLM backbone are updated. This phase employs learning rate of 2e-5, leverages DeepSpeed Zero Stage 1 optimization, and operates with an extended context length of 20K tokens."
        },
        {
            "title": "3.3.1 Pilot Experiments",
            "content": "To quantitatively evaluate our models capability for video summarization, we curate 140 real-world shorts with human-annotated summaries and employ LLM-as-a-judge scoring (scale: 1-10) comparing model outputs against human annotations. Initial experiments revealed that supervised fine-tuning the pre-trained model directly on this human-annotated summary data yielded no significant performance gains (pretrained 6.42 vs. fine-tuned 6.67). We further explored using Direct Preference Optimization (DPO) (Rafailov et al., 2023), treating the human annotations as positive samples and model outputs as negative samples, but similarly observed no improvement (pretrained 6.42 vs. DPO-tuned 6.50). We hypothesize that this stems from potential distribution mismatch between the human annotations and the models learned representations. Inspired by the success of approaches like DeepSeek-R1 (Guo et al., 2025), which utilized rule-based rewards on tasks with verifiable outputs (e.g., mathematics, coding) with Generalized Reinforcement Policy Optimization (GRPO) algorithm, we design two objective tasks for structured video understanding: (1) Multi-dimensional Multiple-Choice QA covering five critical aspects: (a) spatial fine-grained understanding, (b) temporal finegrained understanding, (c) timeline analysis, (d) intent comprehension (creators attitude/purpose), and (e) event reasoning; (2) Temporal Video Grounding. We found that conducting GRPO-based post-training on these verifiable tasks, followed by fine-tuning on the human-annotated summaries, led to substantial gains in comprehension performance (pretrained 6.42 vs. GRPO-sft 6.99). The MCQ task enhances the models understanding across multiple respects by explicitly targeting diverse dimensions of video comprehension, while the grounding task increases the models temporal awareness by requiring precise localization of events within the video timeline. This combination enables the model to develop more holistic and temporally sensitive understanding of video content, thereby better preparing the enhanced model to effectively learn from high-quality data. After verifying that GRPO post-training on verifiable tasks followed by fine-tuning strengthens models comprehension ability, we design comprehensive post-training regimen including an initial instruction fine-tuning for instruction alignment, cold-start phase to initialize the model for reinforcement learning, targeted reinforcement learning phase using GRPO, and final instruction fine-tuning stage using high-quality human-annotated data."
        },
        {
            "title": "3.3.2 Stage 1: Initial Instruction Fine-tuning",
            "content": "The Instruction Fine-tuning stage is adopted to equip ARC-Hunyuan-Video with robust instruction-following capabilities. To achieve this, we construct comprehensive and high-quality supervised dataset that covers wide range of tasks. Specifically, our data includes 460K open-ended question answering (QA) samples and 70K multiple-choice QA from publicly available academic datasets, as well as 20K QA samples collected from real-world short videos, ensuring both general coverage and domain-specific relevance. For temporal video grounding, we incorporate 10K samples from academic datasets and 5K samples from real-world short videos, enabling precise event localization within diverse video content. Additionally, the dataset contains 45K video description and summarization samples from real-world videos, along with 12K multi-granular captioning samples. Similar to pre-training, only the MLP adapter layers and the full LLM backbone are updated during this stage. We use learning rate of 1e-5, leverage DeepSpeed ZeRO Stage 1 optimization, and operate with an extended context length of 20K tokens."
        },
        {
            "title": "3.3.3 Stage 2: Cold Start Initialization for Reinforcement Learning",
            "content": "To prepare strong initial policy for the subsequent reinforcement learning phase, we fine-tune the model on curated dataset of 146K samples featuring Chain-of-Thought (CoT) reasoning. This cold start stage aims to teach the model how to perform step-by-step reasoning across broad spectrum of tasks, thereby building versatile reasoning foundation. For 90K multiple-choice QA samples covering both general video understanding and real-world shorts scenarios, we use powerful MLLM to generate CoT rationales, retaining only instances where the final answer was correct. Similarly, for 18K temporal grounding tasks again spanning general videos and real-world shorts, we generate CoT rationales for timestamp prediction and filtered for samples with an Intersection-over-Union (IoU) above 0.6 between the predicted and the ground truth time span. This process was extended to 20K open-ended QA samples, where an LLM judge verifies the correctness of the final answer, and to 15K video summarization and 3K chapter-level captioning tasks, where event-level captions serve as the intermediate reasoning steps. This model, trained with the same recipe as the initial instruction fine-tuning stage, serves as the starting point for reinforcement learning."
        },
        {
            "title": "3.3.4 Stage 3: Reinforcement Learning with GRPO",
            "content": "Our pilot experiments demonstrate that effective reinforcement learning on verifiable tasks benefits the learning of high-quality subjective data. Therefore, the phase narrows its focus to the two tasks with objective, verifiable reward signals: 100K multiple-choice questions and 35K temporal video grounding instances. For multiple-choice questions, binary reward of 1.0 is assigned for correct answers and 0.0 for incorrect ones. For temporal grounding tasks, the reward is determined by the Intersection over Union (IoU) between the predicted time span and the ground truth. Using the GRPO (Guo et al., 2025) algorithm, we exclusively fine-tune the parameters of the large language model (LLM). Training employs learning rate of 2e-5, leverages DeepSpeed ZeRO Stage 3 optimization, and operates with an extended context length of 20K tokens. The KL divergence coefficient within the GRPO algorithm is set to 0.1."
        },
        {
            "title": "3.3.5 Stage 4: Final Instruction Fine-tuning",
            "content": "In the final stage, we return to the high-quality, human-annotated data. Having undergone the targeted reasoning enhancement of GRPO, the model is now capable of effectively learning from this nuanced data. We use dataset of 25K human-annotated subjective questions for instruction fine-tuning including open-ended QA, video summarization and chapter-level captioning. We further leverage the enhanced capabilities of the GRPO-tuned model to generate 100K high-quality multiple-choice questions with CoT rationales and 50K temporal grounding instances with detailed reasoning traces through rejection sampling. By combining the high-quality human annotations with accurate self-generated trajectories, this final instruction fine-tuning stage polishes the models capabilities, aligning it closely with human-level comprehension of real-world short videos. The training employs learning rate of 1e-5 and utilizes DeepSpeed ZeRO Stage 1 optimization."
        },
        {
            "title": "4.1.1 Model Capability",
            "content": "To intuitively showcase the advanced capabilities of ARC-Hunyuan-Video-7B, we present series of qualitative evaluations on diverse real-world short videos as shown in Fig. 4, Fig. 5 and Fig. 6. These examples highlight our models proficiency in leveraging joint audio-visual reasoning and temporal awareness to achieve deep, structured comprehension, demonstrating its applicability in various real-world scenarios. Joint audio-visual reasoning for complex queries. Our model excels at integrating information from multiple modalities to answer complex questions that are unanswerable from single modality. It is crucial to note that although many short videos feature subtitles, these visual texts can be easily missed or only partially captured at low frame sampling rates. This makes processing the complete audio stream essential for reliable comprehension of the spoken content, which might otherwise be lost. In the outlet replacement tutorial, when asked how to verify the absence of electricity  (Fig. 4)  , the model correctly synthesizes the visual action of using voltage tester with the narrators spoken instructions to provide precise and safe procedure. Furthermore, when tasked with summarizing purchase advice from product review video  (Fig. 6)  , the model effectively extracts and organizes detailed specifications, prices, and target user profiles for different models, demonstrating its utility in structured information extraction from content where information is distributed across visuals, on-screen text, and narration. Fine-Grained temporal grounding and summarization. core strength of our model is its ability to understand the chronological flow of events. For instance, when analyzing tutorial video on replacing an electrical outlet  (Fig. 4)  , ARC-Hunyuan-Video-7B accurately segments the entire process into time-stamped, coherent steps, from turning off the breaker to testing the new installation. This demonstrates its capacity for fine-grained event localization and generating structured, step-by-step summaries. Similarly, for fast-paced video montage of different morning routines  (Fig. 6)  , the model successfully identifies and describes each distinct scene with its corresponding time range, showcasing robust scene segmentation. This temporal awareness is crucial for applications like video highlight generation and structured data extraction. Figure 4 An example of ARC-Hunyuan-Video-7B. Given an instructional short video, our model can accurately identify and summarize the content of each step along with the corresponding time spans. For specific questions, the model is also able to locate the relevant time segments within the video, thereby providing precise answers. Figure 5 An example of ARC-Hunyuan-Video-7B. Given real-world video with excellent audiovisual quality, our model can analyze the video from visual, auditory, and thematic perspectives, and through reasoning, provide fine-grained segment recommendations. Figure 6 Examples of ARC-Hunyuan-Video-7B. Given review-style short video, the model can extract the characteristics of different products based on both visuals and speech. Given short video consisting of multiple distinct scenes, our model is able to analyze the transitions between these scenes and accurately discern the main theme. High-level thematic and creative understanding. Beyond literal descriptions, ARC-Hunyuan-Video-7B demonstrates remarkable ability for thematic reasoning, which is vital for understanding content focused on emotional expression and viewpoint delivery. When analyzing real-world promotional video about environmental protection, it can identify sophisticated creative strategies  (Fig. 5)  , such as the use of strong visual contrast, symbolism and narrative progression to convey the message. Moreover, it can pinpoint the single most thematically resonant moment in the video  (Fig. 5)  , the final shot with the slogan The future is in our hands, showcasing its capacity to grasp the core intent and emotional weight of the content. In conclusion, these qualitative results validate that ARC-Hunyuan-Video-7B moves beyond surface-level perception to truly understand what happens in video, when it happens, and why it matters. This deep, structured comprehension makes it powerful and versatile tool for wide range of real-world applications."
        },
        {
            "title": "4.1.2 Model Comparison",
            "content": "To qualitatively assess the capabilities of ARC-Hunyuan-Video-7B, we conduct comparative analysis against several baseline models, including Qwen2.5-VL-7B-Instruct (Bai et al., 2025), Qwen2.5-Omni-7B (Xu et al., 2025), and Keye-VL-8B-8B (Team et al., 2025). The results, summarized across four representative cases as shown in Fig. 7, Fig. 8 and Fig. 9, highlight our models superior performance in structured video comprehension, particularly in leveraging joint audio-visual reasoning and precise temporal awareness. Superior thematic understanding through audio-visual fusion. primary limitation of video-only models is their inability to process audio, which is often crucial for understanding the context and intent of short videos. In Fig. 7, comedic skit titled POV: Parent Logic, the humor and narrative are driven by the audio narration explaining the parents illogical assumptions. Video-only models like Qwen2.5-VL-7B-Instruct and Keye-VL-8B, deprived of this audio context, misinterpret the visual cues. They describe the physical actions (e.g., child peeking, parent checking) but fail to grasp the core comedic premise. Keye-VL-8B even hallucinates non-existent dialogue like the entropy theme. In contrast, ARC-Hunyuan-Video-7B correctly identifies the skits satirical nature by integrating the audio narration with the visual scenes, accurately summarizing the central theme of parents tendency to assume the worst in his childs activities\". While the audio-visual model Qwen2.5-Omni-7B captures the basic events, its summary remains literal play-by-play, lacking the deeper thematic insight that our model provides. Deeper nuance comprehension in real-world scenarios. Beyond just understanding the plot, grasping the nuance and emotional tone is key to short video comprehension. In Fig. 8, video contrasting the imagination vs. reality\" of holding an umbrella for partner, all models identify the basic visual contrast. However, ARC-Hunyuan-Video excels in capturing the videos intended purpose and emotional impact. Its summary describes the excellent comedic effect and how the video resonates with the audience\" by showing relatable and humorous\" side of love. This demonstrates more profound level of reasoning compared to the baselines, which offer more superficial, descriptive summaries. This ability to understand why video is engaging is critical advantage for real-world applications. Enhanced temporal awareness for accurate event grounding. The fast-paced nature of short videos makes temporal localization significant challenge. This weakness is evident in the temporal grounding tasks  (Fig. 9)  . For the first example, which asks for the time range of woman cooking, the baseline models produce wildly inaccurate predictions, both completely missing the event. Similarly, in the second example, their predictions for when woman comments on food are imprecise. ARC-Hunyuan-Video-7B, however, leverages its explicit temporal awareness, achieved through our timestamp overlay mechanism, and joint visual-audio reasoning, to pinpoint the events with remarkable accuracy. It correctly identifies the cooking scene and the food commentary. This precision demonstrates that our model does not just see what happens, but understands precisely when it happens, cornerstone of structured video comprehension. In summary, these qualitative comparisons underscore the effectiveness of ARC-Hunyuan-Video-7B. By robustly fusing audio-visual information and maintaining strong sense of temporality, our model overcomes the limitations of video-only and general-purpose multimodal models, delivering more accurate, nuanced, and structured understanding of real-world short videos. Figure 7 qualitative comparison between baseline models and our model in understanding short videos where one person plays multiple roles. Our model can accurately identify the events in each scene and provide precise understanding of the main video theme. Figure 8 qualitative comparison between baseline models and our model in understanding short videos with rich visual information. Our model can accurately describe the visual content, analyze the background music, and identify the main theme of the video. Figure 9 qualitative comparison between baseline models and our model in the ability of temporal video grounding on real-world videos. Our model can effectively analyze visual and audio cues to accurately determine the start and end times of events."
        },
        {
            "title": "4.2.1 Evaluation Benchmark\nReal-world shorts understanding. Existing benchmarks often fall short in capturing the nuanced complexities\nof user-generated content. To rigorously evaluate our model’s ability to understand real-world short videos,\nwe construct a specialized benchmark named ShortVid-Bench. Specifically, we develop an automated pipeline\nto generate multi-dimensional questions for each video, targeting capabilities that signify a deep, holistic\ncomprehension through integrating both visual and audio cues. These dimensions include: (1) Temporal\nReasoning and Localization, (2) Affective Intent Classification, (3) Creator Intent Taxonomy, (4) Narrative\nComprehension, (5) Humor & Meme Deconstruction, (6) Creative Innovation Analysis as shown in Fig. 10.\nFor objective assessment, we employ a multiple-choice question (MCQ) format following previous work (Li\net al., 2023; Chen et al., 2023; Qiu et al., 2024). Each question is carefully curated by human annotators who\nprovide the ground-truth answer and design challenging, plausible distractors. Collectively, these dimensions\npush the evaluation beyond mere descriptive captioning, demanding a genuine comprehension of the video’s\ncontext, intent, and narrative.",
            "content": "Video temporal grounding. We further evaluate our model on temporal video grounding tasks including Charades-STA (Gao et al., 2017), which contains 3,720 long videos capturing indoor human activities for testing, and ActivityNet (Caba Heilbron et al., 2015), which comprises 17,031 test samples. General video understanding and reasoning. While our primary focus is the structured comprehension of real-world short videos, we also evaluated ARC-Hunyuan-Video-7B on several established general-purpose benchmarks. Specifically, we report performance on (1) MVBench (Li et al., 2024), (2) the multiple-choice task of VCR-Bench (Qi et al., 2025), and (3) Video-Holmes (Cheng et al., 2025). The first two benchmarks encompass mixture of perception and reasoning tasks across various video types, while Video-Holmes is specifically designed to test complex video reasoning, with focus on suspenseful short films."
        },
        {
            "title": "4.2.2 Evaluation Results",
            "content": "As shown in Tab. 1, ARC-Hunyuan-Video-7B achieves the highest accuracy on our proposed ShortVid-Bench, which demonstrates its superior ability to comprehend real-world short videos by integrating visual and audio signals with advanced reasoning capabilities. Furthermore, our model outperforms all baselines in temporal video grounding, which is largely attributed to our strategy of directly overlaying timestamps onto the video frames for enhancing temporal awareness. With limited general-purpose video training data, our model also shows promising results on general video understanding and reasoning benchmarks. Table 1 Quantitative evaluation on different benchmarks, which use accuracy as the evaluation metric, except for the grounding tasks, which use mIoU. Real-world Shorts Und Temporal Video Grounding General Video Und & Reasoning Model fps #frames think ShortVid-Bench Charades-STA ActivityNet MVBench VCR-Bench Video-Holmes Qwen2.5-VL-7B-Instruct Qwen2.5-Omni-7B Keye-VL-8B 1.0 1.0 1.0 ARC-Hunyuan-Video-7B 1. 150 150 150 150 67.8 68.3 53.5 74. 46.9 30.5 25.1 54.8 25.1 13. 14.9 41.7 62.9 64.8 35.7 62. 53.7 51.0 34.9 50.5 41.6 43. 35.7 40."
        },
        {
            "title": "4.3 Downstream Application\nTo demonstrate the practical utility and adaptability of ARC-Hunyuan-Video-7B, we conduct supervised\nfine-tuning on a set of downstream tasks with minimal task-specific data for real-world application scenarios.\nWe map specific supervised fine-tuning tasks to their corresponding real-world applications: (a) Brief Summary\nfor Video Retrieval, (b) Detailed Summary for comprehensive Video Tagging, and (c) Extended Browsing\nWords for Video Recommendation.",
            "content": "Figure 10 Examples from ShortVid-Bench. The questions, spanning six distinct dimensions, require integrating both visual and audio information for genuine comprehension of the real-world short videos. Figure 11 Demonstration of ARC-Hunyuan-Video-7Bs versatility through minimal fine-tuning for various downstream applications. Specific supervised fine-tuning tasks are mapped to their corresponding real-world scenarios: (a) Brief Summary for Video Retrieval, (b) Detailed Summary for comprehensive Video Tagging, and (c) Extended Browsing Words for Video Recommendation."
        },
        {
            "title": "4.3.1 Experimental Setup",
            "content": "Based on common business scenarios, we consider three typical ones as examples. The specific task definitions are as follows: Brief Summary. The concise summary of the core content such as people, location and event, condensing the key information in simple language, facilitating quick understanding of the overall picture. Brief summaries can effectively simplify the functional requirements in scenarios such as video retrieval and video aggregation, and transform traditional cross-modal or pure visual analysis into more mature plain text operations. Detailed Summary. The detailed summary of the entire content of the video, which should include complete description of the videos content, as well as information about the shooting techniques, background music, subtitles, etc., and also brief analysis of the videos meaning. Similar to brief summary, detailed summary is also compromise made to maximize utilizing the efficiency and high quality of plain text retrieval. The difference is that detailed summary pay more attention to the content details of the video and can make more accurate matches to the video content details during retrieval. Extended Browsing Words. Extended browsing words refer to search terms that users may be interested in and search for after browsing given video, and then extend their browsing. This is typical recommendation scenario. Traditional strategies may implement recommendations based on video similarity (Wray et al., 2021; Fang et al., 2021) or collaborative filtering (Wu et al., 2022) or association rules (Liao et al., 2021; Qin et al., 2021) based on user behavior, but extended browsing words given by content-based reasoning can effectively expand the scope of recommendations and have better prospects in terms of cold start and preference prediction. For each of the three tasks, we obtain 1,100 samples by manual annotation, of which 1,000 are used for supervised fine-tuning and the remaining 100 are used for evaluation. To ensure the quality of data annotation, we randomly select 10% of the labeled data for cross-validation, and the pass rate was greater than 95%. For evaluating the performance of the fine-tuned ARC-Hunyuan-Video from both qualitative and quantitative perspectives, we utilize the following two indicators. Pass Rate (PR). We manually define score scale for evaluating model outputs, which is divided into three grades from low to high, i.e., 0-2 points. Specifically, score of 0 indicates that the model output contains obvious errors, which are clearly inconsistent with the original video content or task definition; score of 1 means that the model output has acceptable minor issues that do not affect the understanding of the original video content and do not excessively violate the rules; score of 2 represents that the model output has no problems at all and fully conforms to the task definition. In actual business operations, we generally believe that score of 1 or above can be considered pass. (Zou et al., 2021; Zhao et al., 2022; Ye et al., 2023; Li et al., 2025) GSB metric Good vs. Same vs. Bad (GSB). is widely adopted in industry, and it is evaluated by experts judging the superiority or inferiority of results from different sources. Specifically, judges are given two results for single input: one generated by System A, and the other by its competitor, System B. Crucially, annotators are unaware of which system each result corresponds to. Their task is to determine which result is of higher quality based on an assessment of the overall quality of the output results."
        },
        {
            "title": "4.3.2 Implementation Details",
            "content": "We use the ARC-Hunyuan-Video-7B as the base model and optimize the parameters of the MLP adapter layers and the full LLM backbone with learning rate of 1e-5 for 3 epochs. The prompt settings in the training data consist of rule descriptions in natural language. In particular, in the reasoning of extended browsing words, we design simple thought of chain structure, which outputs the understood video content and then infers possible extended browsing words."
        },
        {
            "title": "4.3.3 Experimental Analysis\nMain Results. As shown in Table 2, compared with the current online baseline, the supervised fine-tuned\nARC-Hunyuan-Video-7B performs significantly better in the three tasks. Specifically, by comparing the\nmodel scores, it can be found that supervised fine-tuned ARC-Hunyuan-Video-7B received fewer 0 points\nand more 2 points, so the PR is also significantly higher than the baseline, proving that it can better and\nmore correctly understand the video content according to the instructions. At the same time, the relatively\nlarge difference in the GSB score based on manual evaluation further proves that even one instance can get a\nscore of 2 points. The supervised fine-tuned ARC-Hunyuan-Video-7B can better meet the preferences of the",
            "content": "Table 2 Evaluation results of three tasks, where baseline is the original online model (different for three businesses) and Ours is ARC-Hunyuan-Video-7B after supervised fine-tuning for different tasks. Our model shows marked improvements, with significantly higher Pass Rate (PR) and dominant win rate in the GSB (Good vs. Same vs. Bad) human preference comparisons. Brief Summary Detailed Summary Extended Browsing Words Model Baseline (Method A) Ours (Method B) 0 29 18 1 4 2 49 78 PR 0. 0.82 GSB 16:4:80 0 37 1 44 35 2 19 PR 0.63 0.74 GSB 8:15:77 18 12 1 36 34 46 54 PR 0.82 0.88 GSB 14:44:42 reviewers and provide better experience. Going deeper, since both are summary tasks, the main difference between brief summary and detailed summary is the level of detail in the description of the video content. By comparing the PR of the two, it can be found that the detailed summary is more difficult, which is also in line with intuition. After all, the rules of detailed summary are more difficult, and the description of video details is more prone to errors and omissions. Further observation of the distribution of 0, 1, and 2 scores of these two tasks also shows that the number of 0 points in the detailed summary is larger, and the amount of 2 points in the detailed summary is also significantly lower than that of the concise summary, which further indicates that the current model still has room for improvement. Real-world Production Benefits. The above three types of supervised fine-tuning tasks are not actually directly output to users for interaction, but can provide indirect support for different user services as intermediate products. For example, the brief summary, as description of the core content of the video, can directly serve video retrieval services. Therefore, we apply it to the video retrieval of our products as retrieval target with user query, which significantly improves the users retrieval experience. Specifically, our retrieval CTR increased by 5.88%, the landing page consumption time increased by 5.11%, the video floating layer click CTR increased by 7.26%, and the long click rate increased by 3.34%. Similarly, we also applied it to the video aggregation application. After the function was launched, the number of goals per capita increased by 0.63%, the average QV per capita increased by 0.55%, and the proportion of satisfied QV increased by 1.77%. Case Study. To more clearly compare the performance differences after supervised fine-tuning, we provide specific examples for the brief summary in Figure 12. By observing these examples, we can find that after supervised fine-tuning, ARC-Hunyuan-Video-7B is able to better incorporate task rules and achieve rule-guided video understanding."
        },
        {
            "title": "5 Conclusion",
            "content": "This paper introduces ARC-Hunyuan-Video, powerful multimodal model designed to tackle the challenges of understanding real-world short videos. Faced with the complexity of user-generated content, characterized by dense information, multimodal integration, and rapid pacing, we propose the concept of Structured Video Comprehension, which focuses on fine-grained, temporally-precise understanding of videos narrative, events, and underlying intent. Built upon on Hunyuan-7B VLM, we adopt an audio encoder for fine-grained audiovisual synchronization and timestamp overlay mechanism for explicit temporal awareness. This model is trained using multi-stage strategy on large-scale dataset of millions of real-world videos, annotated via an automated bootstrapped pipeline. core finding of our work is that grounding the model in objective tasks with RL is key to unlocking high-quality, subjective understanding. Extensive experiments demonstrate that ARC-Hunyuan-Video achieves state-of-the-art performance on short video comprehension benchmarks and exhibits strong versatility for downstream applications. We believe ARC-Hunyuan-Video represents significant step towards enabling more sophisticated, in-depth, and practical video-centric AI services, paving the way for new generation of intelligent video applications. Figure 12 Examples of brief summary. In the first case, we observe that the fine-tuned ARC-Hunyuan-Video-7B can correctly infer the relationship between the characters in the video, and infer that the person is going home from school through the behavior (carrying schoolbag into the door). In the second example, the fine-tuned ARC-Hunyuan-Video-7B also correctly identifies the relationship between the characters in the video and describes the complex interaction details between the characters in more detail."
        },
        {
            "title": "References",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: large-scale video benchmark for human activity understanding. In Proceedings of the ieee conference on computer vision and pattern recognition, pages 961970, 2015. Yi Chen, Yuying Ge, Yixiao Ge, Mingyu Ding, Bohao Li, Rui Wang, Ruifeng Xu, Ying Shan, and Xihui Liu. Egoplanbench: Benchmarking multimodal large language models for human-level planning. arXiv preprint arXiv:2312.06722, 2023. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. Junhao Cheng, Yuying Ge, Teng Wang, Yixiao Ge, Jing Liao, and Ying Shan. Video-holmes: Can mllm think like holmes for complex video reasoning? arXiv preprint arXiv:2505.21374, 2025. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. Han Fang, Pengfei Xiong, Luhui Xu, and Yu Chen. Clip2video: Mastering video-text retrieval via image clip. arXiv preprint arXiv:2106.11097, 2021. Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. Tall: Temporal activity localization via language query. In Proceedings of the IEEE international conference on computer vision, pages 52675275, 2017. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th symposium on operating systems principles, pages 611626, 2023. Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2219522206, 2024. Yuchen Li, Hao Zhang, Yongqi Zhang, Xinyu Ma, Wenwen Ye, Naifei Song, Shuaiqiang Wang, Haoyi Xiong, Dawei Yin, and Lei Chen. M2oerank: Multi-objective mixture-of-experts enhanced ranking for satisfaction-oriented web search. In 2025 IEEE 41st International Conference on Data Engineering (ICDE), pages 44414454. IEEE Computer Society, 2025. Shu-Hsien Liao, Retno Widowati, and Yu-Chieh Hsieh. Investigating online social media users behaviors for social commerce recommendations. Technology in Society, 66:101655, 2021. Yukun Qi, Yiming Zhao, Yu Zeng, Xikun Bao, Wenxuan Huang, Lin Chen, Zehui Chen, Jie Zhao, Zhongang Qi, and Feng Zhao. Vcr-bench: comprehensive evaluation framework for video chain-of-thought reasoning. arXiv preprint arXiv:2504.07956, 2025. Yuqi Qin, Pengfei Wang, and Chenliang Li. The world is binary: Contrastive learning for denoising next basket recommendation. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval, pages 859868, 2021. Lu Qiu, Yi Chen, Yuying Ge, Yixiao Ge, Ying Shan, and Xihui Liu. Egoplan-bench2: benchmark for multimodal large language model planning in real-world scenarios. arXiv preprint arXiv:2412.04447, 2024. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In International conference on machine learning, pages 2849228518. PMLR, 2023. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. Fangxun Shu, Lei Zhang, Hao Jiang, and Cihang Xie. Audio-visual llm for video understanding. arXiv preprint arXiv:2312.06720, 2023. Guangzhi Sun, Wenyi Yu, Changli Tang, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, Yuxuan Wang, and Chao Zhang. video-salmonn: Speech-enhanced audio-visual large language models. arXiv preprint arXiv:2406.15704, 2024. Guangzhi Sun, Yudong Yang, Jimin Zhuang, Changli Tang, Yixuan Li, Wei Li, Zejun Ma, and Chao Zhang. videosalmonn-o1: Reasoning-enhanced audio-visual large language model. arXiv preprint arXiv:2502.11775, 2025. Changli Tang, Yixuan Li, Yudong Yang, Jimin Zhuang, Guangzhi Sun, Wei Li, Zejun Ma, and Chao Zhang. videosalmonn 2: Captioning-enhanced audio-visual large language models. arXiv preprint arXiv:2506.15220, 2025. Kwai Keye Team, Biao Yang, Bin Wen, Changyi Liu, Chenglong Chu, Chengru Song, Chongling Rao, Chuan Yi, Da Li, Dunju Zang, et al. Kwai keye-vl technical report. arXiv preprint arXiv:2507.01949, 2025. Michael Wray, Hazel Doughty, and Dima Damen. On semantic similarity in video retrieval. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 36503660, 2021. Le Wu, Xiangnan He, Xiang Wang, Kun Zhang, and Meng Wang. survey on accuracy-oriented neural recommendation: From collaborative filtering to information-rich recommendation. IEEE Transactions on Knowledge and Data Engineering, 35(5):44254445, 2022. Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, et al. Qwen2. 5-omni technical report. arXiv preprint arXiv:2503.20215, 2025. Dezhi Ye, Bowen Tian, Jiabin Fan, Jie Liu, Tianhua Zhou, Xiang Chen, Mingming Li, and Jin Ma. Improving query correction using pre-train language model in search engines. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, pages 29993008, 2023. Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. Xue Zhao, Dayiheng Liu, Junwei Ding, Liang Yao, Mahone Yan, Huibo Wang, and Wenqing Yao. Self-supervised product title rewrite for product listing ads. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Track, pages 7985, 2022. Lixin Zou, Shengqiang Zhang, Hengyi Cai, Dehong Ma, Suqi Cheng, Shuaiqiang Wang, Daiting Shi, Zhicong Cheng, and Dawei Yin. Pre-trained language model based ranking in baidu search. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pages 40144022, 2021."
        }
    ],
    "affiliations": [
        "ARC Lab, Tencent PCG",
        "Big Data Platform Department, Tencent PCG",
        "Search Application Department, Tencent CSIG",
        "Tencent Hunyuan"
    ]
}