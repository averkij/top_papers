{
    "paper_title": "X-Part: high fidelity and structure coherent shape decomposition",
    "authors": [
        "Xinhao Yan",
        "Jiachen Xu",
        "Yang Li",
        "Changfeng Ma",
        "Yunhan Yang",
        "Chunshi Wang",
        "Zibo Zhao",
        "Zeqiang Lai",
        "Yunfei Zhao",
        "Zhuo Chen",
        "Chunchao Guo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generating 3D shapes at part level is pivotal for downstream applications such as mesh retopology, UV mapping, and 3D printing. However, existing part-based generation methods often lack sufficient controllability and suffer from poor semantically meaningful decomposition. To this end, we introduce X-Part, a controllable generative model designed to decompose a holistic 3D object into semantically meaningful and structurally coherent parts with high geometric fidelity. X-Part exploits the bounding box as prompts for the part generation and injects point-wise semantic features for meaningful decomposition. Furthermore, we design an editable pipeline for interactive part generation. Extensive experimental results show that X-Part achieves state-of-the-art performance in part-level shape generation. This work establishes a new paradigm for creating production-ready, editable, and structurally sound 3D assets. Codes will be released for public research."
        },
        {
            "title": "Start",
            "content": "X -Part: high fidelity and structure coherent shape decomposition Xinhao Yan,1,2,, Jiachen Xu,1,, Yang Li,1, , Changfeng Ma1,3, Yunhan Yang1,4, Chunshi Wang1,5, Zibo Zhao1, Zeqiang Lai1,6, Yunfei Zhao1, Zhuo Chen1, Chunchao Guo1, 1 Tencent Hunyuan, 2ShanghaiTech, 3NJU, 4HKU, 5ZJU, 6CUHK 5 2 0 2 0 ] . [ 1 3 4 6 8 0 . 9 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Generating 3D shapes at part level is pivotal for downstream applications such as mesh retopology, UV mapping, and 3D printing. However, existing part-based generation methods often lack sufficient controllability and suffer from poor semantically meaningful decomposition. To this end, we introduce -Part, controllable generative model designed to decompose holistic 3D object into semantically meaningful and structurally coherent parts with high geometric fidelity. -Part exploits the bounding box as prompts for the part generation and injects point-wise semantic features for meaningful decomposition. Furthermore, we design an editable pipeline for interactive part generation. Extensive experimental results show that - Part achieves state-of-the-art performance in part-level shape generation. This work establishes new paradigm for creating production-ready, editable, and structurally sound 3D assets. Codes will be released for public research. Equal Contributions Project Leader Corresponding Author"
        },
        {
            "title": "Tencent Hunyuan",
            "content": ""
        },
        {
            "title": "Introduction",
            "content": "3D assets are now extensively utilized across wide range of fields, including gaming, film production, 3D printing, autonomous driving, and robotic simulation. However, traditional 3D content creation remains time-consuming process that demands significant expertise. Recent advances in generative AI have substantially lowered the barriers to 3D content generation particularly with the emergence of foundational 3D models Zhang et al. (2024); Zhao et al. (2025); Lai et al. (2025). Despite this progress, most existing generative approaches are only capable of producing monolithic 3D models, which poses considerable limitations for practical 3D creation pipelines. Decomposing complete 3D shape into meaningful semantic parts would greatly facilitate various downstream tasks. For instance, breaking down complex geometry into simpler parts can significantly ease the process of mesh re-topology Weng et al. (2025) and uv-unwrapping Li et al. (2025a). Generating shapes at the part level presents two major challenges: 1) The decomposed geometry must maintain meaningful part-level semantics, and 2) The generation process must recover geometrically plausible structures for internal regions. Mainstream part-generation methods adopt the latent vecset diffusion framework Zhang et al. (2023), where each part is represented as an independent set of latent codes for diffusion. The generation process can be executed independently for individual parts (e.g., HoloPart Yang et al. (2025a)) or simultaneously for all parts (e.g., PartCrafter Lin et al. (2025), PartPacker Tang et al. (2025)) with enhanced part synchronization. Furthermore, multi-view or 3D segmentation are frequently employed for better part decomposition Chen et al. (2024); Yang et al. (2025a;b). However, these approaches are highly sensitive to inaccuracies in the segmentation results. Alternative works Lin et al. (2025); Tang et al. (2025) do not explicitly rely on segmentation, but they lack controllability and often generate parts with ambiguous boundary. Motivated by these observations, we present -Part, diffusion-based framework that decomposes holistic mesh into semantically meaningful and structurally coherent 3D parts. The method utilizes the state-of-the-art segmenter P3-SAM Ma et al. (2025) to automatically generate initial part segmentations, bounding boxes, and semantic features. Then the shape decomposition is executed within synchronized multi-part diffusion process. Specifically, 1) First, to control part decomposition, instead of directly using segmentation results as input we uses bounding boxes as prompts to indicate part locations and scales. Compared with fine-grained and point-level segmentation cues, bounding boxes provide coarser form of guidance, which mitigates overfitting to the input segmentation masks. Besides, the bounding box provides additional volume scale information for the partially visible part, benefiting generation and controllability. 2) Second, despite inaccuracies in the segmentation results, we notice that the high-dimension point-wise semantic feature is free from the information compression caused by the mask prediction head used in P3-SAM, resulting in more robust semantic representations. Therefore, we introduce the semantic features from P3-SAM into our diffusion process to guide the multi-part diffusion process. This greatly benefits the part decomposition. 3) Third, we integrate -Part into bounding box based part editing pipeline following Lugmayr et al. (2023). It supports local editing, such as splitting part into several parts and adjusting their scales, to facilitate interactive part generation. To prove the effectiveness of -Part, we conducted extensive experiments on various benchmarks. Our results show that -Part achieves state-of-the-art performance in part-level decomposition and generation. In summary, the contributions of our work are as follows: 1. We propose -Part, controllable and editable diffusion framework, capable of generating semantically meaningful and structurally coherent 3D parts. 2. We integrate -Part into an editable part generation pipeline, which supports multiple interactive editing methods. 3. Extensive experiments demonstrate that -Part achieves state-of-the-art performance in part-level decomposition and generation."
        },
        {
            "title": "2 Related Work",
            "content": "Part Segmentation. The most straightforward approach for decomposing 3D geometry is segmentation. Early conventional methods Qi et al. (2017); Zhao et al. (2021) directly predict per-point semantic labels via supervised learning. While effective within constrained settings, these methods rely heavily on extensive part-level annotations, generalize poorly beyond seen categories, and offer limited semantic scalability. Inspired by the remarkable success of 2D foundation models like SAM Kirillov et al. (2023) and GLIP Li et al. (2022) in open-vocabulary tasks, several recent approaches Abdelreheem et al. (2023); Liu et al. (2023); Tang et al. (2024); Thai et al. (2024); Umam et al. (2024); Zhong et al. (2024) attempt to lift 2D visual knowledge to 3D domains. Although these methods improve generalization, they often fail to accurately infer parts in occluded or unobserved regions. To mitigate this, PartField Liu et al. (2025) and SAMPart3D Yang et al. (2024) learn open-world 3D feature fields for semantic part decomposition. P3-SAM Ma et al. (2025) proposes native 3D part segmentation network trained on large, purely 3D dataset with part annotations, demonstrating impressive part segmentation results. Object-level Shape Generation. The remarkable success of latent diffusion models in 2D image generation has inspired new wave of methods extending this capability to 3D object generation. Dreamfusion Poole et al. (2022) introduced Score Distillation Sampling (SDS) to distill 2D priors from pre-trained diffusion models for 3D synthesis, though it often suffers from slow optimization and geometrically inconsistent outputs. Subsequent approaches Li et al. (2023); Long et al. (2024); Shi et al. (2023), reformulated 3D generation as multi-view image synthesis problem. With the release of large-scale 3D datasets such as Objaverse Deitke et al. (2023b) and Objaverse-XL Deitke et al. (2023a), native 3D generative models have become increasingly prevalent. Methods like 3DShape2VecSet Zhang et al. (2023), Michelangelo Zhao et al. (2023), Clay Zhang et al. (2024), and Dora Chen et al. (2025c) encode object point clouds into vector-set tokens using variational autoencoder (VAE) Kingma & Welling (2013) and model the distribution via Diffusion Transformer (DiT) Peebles & Xie (2023). In contrast, Trellis Xiang et al. (2025) employs an explicit voxel representation for coarse geometry and further generates both geometry and appearance from the voxel latents. Part-level Shape Generation. PartGen Chen et al. (2025a) decomposes 3D objects by solving multi-view segmentation task and subsequently completes and reconstructs each part in 3D. PhyCAGE Yan et al. (2024b) adopt physical regularization for non-rigid part decomposition. The second category exploits DiT-based generative methods to achieve partlevel generation Yang et al. (2025a); Lin et al. (2025); Tang et al. (2025); Dong et al. (2025); Yang et al. (2025b); Zhang et al. (2025). HoloPart Yang et al. (2025a) completes part geometry from initial 3D segmentation results. In contrast, PartCrafter Lin et al. (2025) and PartPacker Tang et al. (2025) operate without explicit segmentation, instead leveraging multi-instance DiTs to generate parts automatically. PartPacker Tang et al. (2025) further introduces dual-volume DiT to model complementary spatial volumes for improved efficiency. Frankenstein Yan et al. (2024a) execute similar idea by packing multiple SDFs in latent triplane space via VAE. However, these approaches often yield parts with limited geometric quality and offer minimal local controllability. CoPart Dong et al. (2025) incorporates an auxiliary 2D image diffusion model to enhance texture and detail using 2D/3D bounding box conditions, though it supports only up to 8 parts and cannot decompose an existing 3D shape. OmniPart Yang et al. (2025b) adopts an explicit representation similar to Trellis and uses bounding box prompts, yet it lacks the ability to complete occluded geometry. BANG Zhang et al. (2025) frames part generation as an object explosion process, enabling bounding-box-guided decomposition and recursive refinement, but it often fails to preserve fine geometric details throughout the process. AutoPartGen Chen et al. (2025b) employs latent diffusion model to autoregressively generate parts, which is computationally expensive and offers limited user control. Kestrel Ahmed et al. (2024) employs an LLM to reason about part segmentation, while ShapeLLM Qi et al. (2024) leverages LLMs for visual grounding and bounding box extraction. MeshCoder Dai et al. (2025) represents parts using code-based representation, injects 3D point clouds into an LLM, and generates 3D primitives expressed as Blender Python scripts."
        },
        {
            "title": "Tencent Hunyuan",
            "content": "Figure 1: Architecture of -Part. Given input point cloud, per-point feature and part bounding boxes are extracted from P3-SAM. Global and part conditions are obtained by stacking geometry token with interpolated semantic features. They and injected to multipart diffusion process to guide shape decomposition."
        },
        {
            "title": "3 Method",
            "content": "Our objective is to generate high-fidelity and structure-coherent part geometries from given object point cloud, while ensuring flexible controllability over the decomposition process. To this end, we propose -part (see Figure 1) based on the diffusion framework. In Section 3.1, In Section 3.2 we outline the foundational vecset-based 3D latent diffusion framework. we first describe our proposed bounding box-based part-level cues extraction module and point-wise semantic feature injection, then we present our overall -Part framework for synchronized part generation and training scheme. Finally, we introduce the editable part generation pipeline in Section 3.3. 3.1 Preliminary Our method builds upon pre-trained vecset-based 3D shape generation models Zhang et al. (2024); Zhao et al. (2023; 2025); Li et al. (2025b), which typically consist of 3D shape variational autoencoder (VAE) and latent diffusion model. Variational Autoencoder (VAE) Given an input mesh, we first sample point cloud RN 7 with both normal and sharp-edge flags using sharp-edge-aware sampling strategy, following Dora Chen et al. (2025c) and Hunyuan2.0 Zhao et al. (2025). The VAE adopts transformer-based architecture Zhang et al. (2023; 2024); Li et al. (2025b); Zhao et al. (2025), comprising cross-attention block followed by multiple self-attention layers, whose encoder maps the sampled point cloud into latent vectors: (1) = E(X) = SelfAttn(CrossAttn(P E(X0), E(X))) where X0 RN07 denotes the point set obtained by applying farthest point sampling (FPS) to X, and RN0C represents the N0 latent tokens of the input shape. represents position embedding for input point cloud. The decoder of the VAE similarly consists of several self-attention layers followed by final cross-attention module, mapping spatial coordinate query R3 to its corresponding signed distance value (SDF): D(q, Z) = CrossAttn(q, SelfAttn(Z)) (2) Note that, to enhance the capacity of VAE to represent part-level geometry, we further fine-tune the VAE on the part-level dataset. 3D Diffusion Model To model the latent space of encoded objects, flow-based diffusion model Lipman et al. (2022) is trained to generate latent tokens, which can subsequently be"
        },
        {
            "title": "Tencent Hunyuan",
            "content": "decoded into 3D geometries. Following Hunyuan-DiT Li et al. (2024) and TripoSG Li et al. (2025b), the core of our model is constructed using series of Diffusion Transformer (DiT) blocks. 3.2 Multi-Parts Latent Diffusion 3.2.1 Semantic-Aware Shape Conditioning In contrast to image-conditioned 3D generation, we take the point cloud as input. To incorporate holistic shape information, we directly employ the VAE encoder to encode the input point cloud X, which serves as the object-level condition fo. For the controllability over the decomposition process, we design the bounding box-based part-level cues extraction module, shown in Figure 1, to extract the part-level condition fp. First, we sample points Xinbox within the given bounding box from the object point cloud. Xinbox is then encoded by learnable encoder to form the part-level condition fp. In addition, to further improve the robustness to bounding box perturbations during inference, we apply augmentations involving random translations and moderate scaling to the bounding boxes during training. Furthermore, to facilitate more accurate decomposition of the part geometry, we enhance input conditions by concatenating point-wise semantic features encoded by P3-SAM Ma et al. (2025) with the shape tokens encoded by the VAE encoder. The enhanced object-level and part-level conditional features, p, are defined as: and = Concat(fo, Esem(X)), fo = Eo(X) = Concat(fp, Esem(Xinbox)), fp = Ep(Xinbox) where Eo denotes the object-level VAE encoder, and Ep represents the learnable encoder in part-level cues extraction module. Esem represents for the semantic encoder proposed in P3-SAM. Note that to align with the shape tokens, the semantic feature is obtained by interpolated using the down-sampled XYZ positions from the shape encoder output, c.f. Figure 1. To enhance the robustness to the high-dimensional semantic feature, we randomly mask the semantic feature for partial points. (3) Note that, when extracting the part-level condition, points belonging to other adjacent parts may be involved in the box of certain part. However, thanks to the point-wise semantic feature and inter-part attention (discussed later in Section 3.2.2), different parts could help each other exclude points that do not belong to themselves. 3.2.2 Synchronized Part Generation Our proposed -Part simultaneously generates latent tokens for all parts of the whole object 1 RnKC , where the object consists of parts and each part represented by = {zi}K latent tokens denoted as zi. Specifically, -part contains several DiT blocks and each block consists of one self-attention layer followed by two cross-attention layers (see Figure 1). Initially, self-attention is conducted within each part, providing intra-part awareness. However, this leads to the performance degradation at the boundaries between parts. Therefore, to enhance inter-part awareness, we extend the receptive field in half of the self-attention layers to encompass all part tokens, design choice aligned with Lin et al. (2025). Attnintra = softmax( Attninter = softmax( σq(zi)σk(zi)T σq(zi)σk(O)T )σv(zi) )σv(O) (4) where σ{qkv} represent projection layers for the query, key and value. denotes the hidden dim for attention tokens. Then, as shown in Figure 1, we inject the geometric condition, p, by two layers of cross-attention module to improve the structural consistency of decomposition and preserve geometric details of the input object. and"
        },
        {
            "title": "Tencent Hunyuan",
            "content": "Furthermore, to enhance the distinctiveness between latent tokens of different parts, we incorporate learnable part embedding for each individual part. Specifically, we initialize codebook RlC and assign unique embedding to each part during training. Note that, to enable the decomposition of objects that contains more parts than the maximum limit for single object in the dataset, we set much larger number, and randomly select the unique embedding to each part. Training. Building upon the conditioning framework established in Section 3.2.1, we train the model using flow matching objective Lipman et al. (2022) to transport noisy part tokens toward the target data distribution z0. Specifically, during the forward process, Gaussian noise ε (0, I) is added to the data z0 according to noise level t, resulting in zt = tz0 + (1 t)ε. The model is trained to predict the velocity field = ε z0that moves zt back toward z0, conditioned on both the object-level condition and the part-level condition p. The training objective is formulated as follows: (cid:20)(cid:13) (cid:13) (cid:13)(ε z0) vθ(zt, t, = Ez,t,ε (cid:13) (cid:13) p) (cid:13) o, (5) 2(cid:21) where vθ denotes the DiT-based neural network. Given that the geometric complexity of an individual part is substantially lower than that of complete object, we assign reduced number of tokens to each part during both the VAE fine-tuning process and - Part training process. It significantly accelerates both training and inference while remaining the performance. 3.3 Part Editing Leveraging the controllability and ease of manipulation provided by the bounding box, we further design part-level editing pipeline for interactive part generation. Following Repaint Lugmayr et al. (2022), we adopt training-free method to achieve two kinds of editing types, that is, split and adjust. The split operation refers to splitting the bounding box and generating several parts accordingly. The adjust operation means adjusting certain bounding box so that the part and adjacent parts would be re-generated accordingly. Specifically, for parts indicated by the bounding box, their latent tokens are resampled and denoised while keeping tokens of other parts unchanged."
        },
        {
            "title": "4 Experiments",
            "content": "We begin by detailing the implementation of our model, including the network architecture, hyperparameters, and training procedure. We then conduct comparative evaluations against existing part generation methods. Furthermore, we perform ablation studies to validate the design choices of our framework. Finally, we demonstrate various downstream applications enabled by our method. 4.1 Implementation Details Network Architecture The DiT module consists of 21 DiT blocks, where skip connections are implemented by concatenating latent features along the channel dimension. During training, the number of tokens per part is set to 512, consistent with the VAE fine-tuning configuration. The self-attention layers at odd indices are configured to perform inter-part attention, thereby enhancing awareness of other parts. For the cross-attention modules, both the object condition and the part condition are represented with 2,048 tokens, providing detailed guidance for the generation process. The part embedding codebook contains 50 entries, and unique embedding is randomly assigned to each part latent during both training and inference. In addition, we employ Mixture-of-Experts (MoE) model for the linear output layers of the first six network blocks to efficiently enhance the learning capacity in the latent space. Training Our model is initialized from pre-trained object generator, with its self-attention parameters loaded as the starting point. We use the Adam optimizer with learning rate of"
        },
        {
            "title": "Tencent Hunyuan",
            "content": "Method CD Fscore-0.1 Fscore-0.5 SAMPart3D 0.15 0.17 PartField 0.26 HoloPart 0.23 OmniPart 0.11 Ours 0.73 0.68 0.59 0.63 0.80 0.63 0.57 0.43 0.46 0.71 Table 1: Quantitative part decomposition results. 1e4 and apply gradient clipping with maximum norm of 1.0 to enhance training stability. The model was trained for approximately four days on 128 H20 GPUs. To further improve robustness, we randomly drop semantic features with probability of 0.3, and independently apply 0.1 dropout probability to the object condition, the part condition, or both during training. Additionally, we apply data augmentation to the bounding boxes by introducing random translations sampled from uniform distribution U(0.05, 0.05) and scaling factors sampled from the interval [0.9, 1.1]. Dataset Curation We use the part dataset introduced in P3-SAM Ma et al. (2025), which contains nearly 2.3 million objects with ground truth part segmentation. To create training pairs, each part of an object, as well as the object itself, is remeshed into watertight mesh. dataset of this scale significantly enhanced the generalizability of our diffusion-based shape decomposition method. 4.2 Comparison Existing methods can be broadly categorized into two groups: 3D Shape Decomposition and Image-to-3D Part Generation. We compare our approach against representative methods from both categories at two distinct levels: the overall object level and the decomposed part level. This two-tier evaluation comprehensively validates our models capacity for part decomposition and high-fidelity geometric generation. Evaluation Protocol We evaluate our method on 200 samples from the ObjaversePartTiny dataset, each comprising rendered images and corresponding ground-truth part geometries. To assess geometric quality, we employ Chamfer Distance (CD) and F-Score. The F-Score is computed at two different thresholds [0.1, 0.5] to capture both coarse-level and fine-level geometric alignment. Prior to metric computation, each object is normalized to the range [1, 1]. To ensure pose-agnostic evaluation, we rotate each object by [0, 90, 180, 270] degrees and report the best score among these orientations as the final metric. 3D Shape Decomposition. This experiment aims to evaluate and compare the geometric decomposition capabilities of different methods, validating that our approach achieves deeper structural understanding and decomposition of objects while generating higherquality part geometries. Our method takes ground-truth watertight surface as input and automatically generates decomposed parts; We compute metrics between the generated parts and the ground-truth parts. We first compare against segmentation-based methods such as Sampart3D Yang et al. (2024) and PartField Liu et al. (2025), which also take the same watertight mesh as input. The segmented results are directly compared to the ground truth parts. In addition, we include generative methods such as HoloPart Yang et al. (2025a) and OmniPart Yang et al. (2025b). HoloPart also uses the ground-truth watertight point cloud as input. Although OmniPart does not directly take 3D shape as input, it first generates coarse geometry and then performs part decomposition. To eliminate the influence of segmentation quality, we replace the Sampart3D segmentation used in HoloPart with our own trained segmentation model, and provide OmniPart with 2D part masks rendered from the ground-truth parts. As shown in Table 1, segmentation-based methods can decompose part points on the input watertight surface but fail to produce complete part geometries. Our method outperforms all baselines in decomposition quality, even when OmniPart is supplied with ground-truth 2D masks. Furthermore, as illustrated in Figure 2, our approach significantly surpasses other methods in the geometric quality of the generated parts."
        },
        {
            "title": "Tencent Hunyuan",
            "content": "Figure 2: Qualitative shape decomposition results. Note the input shapes for HoloPart and Ours are obtained from Hunyuan3D-2.5 Lai et al. (2025), while OmniPart leverage the shape from trellis Xiang et al. (2025). Image-to-3D Part Generation. Leveraging existing image-to-3D generative models, we extend our method to the task of image-to-3D part generation. Specifically, given reference image, we first generate watertight mesh using an off-the-shelf image-to-3D model Zhang et al. (2024); Lai et al. (2025); Li et al. (2025b), which is then fed into our pipeline for decomposition into parts. Similar to the previous experiment, we compare our approach"
        },
        {
            "title": "Tencent Hunyuan",
            "content": "Method CD Fscore-0.1 Fscore-0.5 Part123 HoloPart PartCrafter PartPacker OmniPart Ours 0.42 0.09 0.20 0.11 0.08 0.08 0.36 0.88 0.66 0.85 0.91 0.92 0.20 0.73 0.45 0.65 0.77 0. Table 2: Quantitative holistic shape generation results. Figure 3: Qualitative shape decomposition results. Note the input shapes for HoloPart and Ours are obtained from Hunyuan3D-2.5, OmniPart leverage the shape from Trellis, PartCrafter and PartPacker are image to 3D methods, they do not rely on shapes. not only against HoloPart and OmniPart, but also against methods that directly generate parts from images, such as PartPacker Tang et al. (2025), PartCrafterLin et al. (2025), and Part123Liu et al. (2024). The input to OmniPart remains consistent with the setup above, while both HoloPart and our method use the same generated mesh as input. Since different methods may produce divergent part structures, making it difficult to establish accurate correspondences with ground-truth parts. We compare only the overall object geometry composed of all generated parts. As shown in Table 2, our method produces final objects"
        },
        {
            "title": "Method",
            "content": "Part-level Overall-level CD F1-0.1 F1-0.5 CD F1-0.1 F1-0.5 W/o part embedding W/o object-cond W/o part-cond W/o semantic-feat W/o inter-part self-attn Ours 0.13 0.12 0.27 0.12 0.12 0.11 0.78 0.79 0.57 0.78 0.79 0. 0.68 0.70 0.47 0.69 0.70 0.71 0.04 0.03 0.03 0.04 0.03 0.02 0.97 0.97 0.98 0.97 0.97 0.98 0.92 0.93 0.95 0.92 0.94 0.96 Table 3: Based on the ground-truth bounding boxes, we compute part-level and object-level metrics for different modules on the ObjaversePart-Tiny dataset. with higher geometric quality and better alignment to the ground truth. Figure 2 visually demonstrates the structural plausibility and high quality of our results. Moreover, our decomposition is more refined, often generating larger number of semantically reasonable parts. 4.3 Applications Figure 4: Demonstration of two representative applications of our method. Subfigure (a) shows the results of bounding box-controlled part generation, while subfigure (b) illustrates improved UV unwrapping performance achieved through part-based decomposition. Part Editing Since our method directly takes point clouds and bounding boxes as input, it enables intuitive control over part decomposition and facilitates various part-level editing operations. As demonstrated in Figure 4(a), users can easily adjust both the position and scale of bounding boxes to influence the geometric characteristics of the generated parts. Specifically, as described in Section 3.3, part generation can be controlled in multiple ways: modifying the location and size of bounding box alters the shape and coverage of the corresponding part; merging adjacent bounding boxes results in the fusion of multiple parts into single component; and splitting bounding box leads to the decomposition of part into finer structures. Part-Aware Un-wrapping. UV unwrapping is an essential step in 3D content creation pipelines. Fig. 4 compares the UV maps generated by unwrapping holistic mesh and partdecomposed meshes respectly. Part-decomposed mesh are processed by unwrapping each of the part separatedly. Decomposing shapes into part greatly simplify Un-wrapping process and makeing UV maps more compact and semantically meanningful. 4.4 Ablation Study As shown in Table 3, we conduct series of ablation studies to validate the effectiveness of each component in our proposed framework, all of which contribute to improved model performance. We analyze the roles of individual components in detail. The intra-part and inter-part attention mechanism enhances the representation of part-level latents while maintaining global contextual view across all parts. The part embedding module introduces distinctiveness among the latent representations of different parts. The object-level condition provides priors about the overall geometry of the shape. Meanwhile, the part-level"
        },
        {
            "title": "Tencent Hunyuan",
            "content": "Figure 5: Part generation results under different module ablation settings. condition offers detailed information indicating coarse part location and scale. Additionally, the semantic point feature supplies semantic cues that facilitate structurally coherent shape decomposition. We further provide visualizations of representative results in Figure 5 to illustrate the impact of each component."
        },
        {
            "title": "5 Conclusion and Limitation",
            "content": "Conclusion We introduce -Part, purely geometry-based part generation framework that takes bounding boxes as input to decompose complete 3D objects into structured parts. Compared to existing approaches, our method better preserves geometric quality and fidelity in the generated parts, while also offering easier integration into 3D content creation pipelines, thereby significantly reducing the complexity of downstream tasks. Additionally, our method allows users to alter part decomposition strategies by adjusting bounding boxes, thereby enabling more intuitive control and flexible editing. To enhance the models structural understanding, we incorporate semantic point features that provide high-level shape semantics. Our approach supports the generation of up to 50 distinct parts, which sufficiently covers most practical application scenarios. Limitation Our method currently relies on geometric cues for decomposition and lacks guidance from physical principles, which may limit its ability to meet certain applicationspecific decomposition requirements. Additionally, since the latent codes of all parts are processed simultaneously through the diffusion model, inference time increases with the number of parts, posing challenge for real-time usage when handling high-part-count objects."
        },
        {
            "title": "References",
            "content": "Ahmed Abdelreheem, Ivan Skorokhodov, Maks Ovsjanikov, and Peter Wonka. Satr: Zeroshot semantic segmentation of 3d shapes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1516615179, 2023."
        },
        {
            "title": "Tencent Hunyuan",
            "content": "Mahmoud Ahmed, Junjie Fei, Jian Ding, Eslam Mohamed Bakr, and Mohamed Elhoseiny. Kestrel: 3d multimodal llm for part-aware grounded description. arXiv e-prints, pp. arXiv2405, 2024. Minghao Chen, Roman Shapovalov, Iro Laina, Tom Monnier, Jianyuan Wang, David Novotny, and Andrea Vedaldi. Partgen: Part-level 3d generation and reconstruction with multi-view diffusion models. arXiv preprint arXiv:2412.18608, 2024. Minghao Chen, Roman Shapovalov, Iro Laina, Tom Monnier, Jianyuan Wang, David Novotny, and Andrea Vedaldi. Partgen: Part-level 3d generation and reconstruction with multi-view diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 58815892, 2025a. Minghao Chen, Jianyuan Wang, Roman Shapovalov, Tom Monnier, Hyunyoung Jung, Dilin Wang, Rakesh Ranjan, Iro Laina, and Andrea Vedaldi. Autopartgen: Autogressive 3d part generation and discovery. arXiv preprint arXiv:2507.13346, 2025b. Rui Chen, Jianfeng Zhang, Yixun Liang, Guan Luo, Weiyu Li, Jiarui Liu, Xiu Li, Xiaoxiao Long, Jiashi Feng, and Ping Tan. Dora: Sampling and benchmarking for 3d shape variational auto-encoders. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1625116261, 2025c. Bingquan Dai, Li Ray Luo, Qihong Tang, Jie Wang, Xinyu Lian, Hao Xu, Minghan Qin, Xudong Xu, Bo Dai, Haoqian Wang, et al. Meshcoder: Llm-powered structured mesh code generation from point clouds. arXiv preprint arXiv:2508.14879, 2025. Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: universe of 10m+ 3d objects. Advances in Neural Information Processing Systems, 36: 3579935813, 2023a. Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1314213153, 2023b. Shaocong Dong, Lihe Ding, Xiao Chen, Yaokun Li, Yuxin Wang, Yucheng Wang, Qi Wang, Jaehyeok Kim, Chenjian Gao, Zhanpeng Huang, et al. From one to more: Contextual part latents for 3d generation. arXiv preprint arXiv:2507.08772, 2025. Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 4015 4026, 2023. Zeqiang Lai, Yunfei Zhao, Haolin Liu, Zibo Zhao, Qingxiang Lin, Huiwen Shi, Xianghui Yang, Mingxin Yang, Shuhui Yang, Yifei Feng, et al. Hunyuan3d 2.5: Towards highfidelity 3d assets generation with ultimate details. arXiv preprint arXiv:2506.16504, 2025. Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. arXiv preprint arXiv:2311.06214, 2023. Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded languageimage pre-training. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1096510975, 2022. Yang Li, Victor Cheung, Xinhai Liu, Yuguang Chen, Zhongjin Luo, Biwen Lei, Haohan Weng, Zibo Zhao, Jingwei Huang, Zhuo Chen, et al. Auto-regressive surface cutting. arXiv preprint arXiv:2506.18017, 2025a."
        },
        {
            "title": "Tencent Hunyuan",
            "content": "Yangguang Li, Zi-Xin Zou, Zexiang Liu, Dehu Wang, Yuan Liang, Zhipeng Yu, Xingchao Liu, Yuan-Chen Guo, Ding Liang, Wanli Ouyang, et al. Triposg: High-fidelity 3d shape synthesis using large-scale rectified flow models. arXiv preprint arXiv:2502.06608, 2025b. Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, et al. Hunyuan-dit: powerful multiresolution diffusion transformer with fine-grained chinese understanding. arXiv preprint arXiv:2405.08748, 2024. Yuchen Lin, Chenguo Lin, Panwang Pan, Honglei Yan, Yiqiang Feng, Yadong Mu, and Katerina Fragkiadaki. Partcrafter: Structured 3d mesh generation via compositional latent diffusion transformers, 2025. URL https://arxiv.org/abs/2506.05573. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Anran Liu, Cheng Lin, Yuan Liu, Xiaoxiao Long, Zhiyang Dou, Hao-Xiang Guo, Ping Luo, and Wenping Wang. Part123: part-aware 3d reconstruction from single-view image. In ACM SIGGRAPH 2024 Conference Papers, pp. 112, 2024. Minghua Liu, Yinhao Zhu, Hong Cai, Shizhong Han, Zhan Ling, Fatih Porikli, and Hao Su. Partslip: Low-shot part segmentation for 3d point clouds via pretrained image-language models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2173621746, 2023. Minghua Liu, Mikaela Angelina Uy, Donglai Xiang, Hao Su, Sanja Fidler, Nicholas Sharp, and Jun Gao. Partfield: Learning 3d feature fields for part segmentation and beyond. arXiv preprint arXiv:2504.11451, 2025. Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 99709980, 2024. Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1146111471, 2022. Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Repaint Van Gool. Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11461 11471, 2023. Changfeng Ma, Yang Li, Xinhao Yan, Jiachen Xu, Yunhan Yang, Chunshi Wang, Zibo Zhao, Yanwen Guo, Zhuo Chen, and Chunchao Guo. P3-sam: Native 3d part segmentation. arXiv preprint arXiv:2509.06784, 2025. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023. Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. Charles Qi, Hao Su, Kaichun Mo, and Leonidas Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 652660, 2017. Zekun Qi, Runpei Dong, Shaochen Zhang, Haoran Geng, Chunrui Han, Zheng Ge, Li Yi, and Kaisheng Ma. Shapellm: Universal 3d object understanding for embodied interaction. In European Conference on Computer Vision, pp. 214238. Springer, 2024. Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512, 2023."
        },
        {
            "title": "Tencent Hunyuan",
            "content": "George Tang, William Zhao, Logan Ford, David Benhaim, and Paul Zhang. Segment any mesh: Zero-shot mesh part segmentation via lifting segment anything 2 to 3d. arXiv e-prints, pp. arXiv2408, 2024. Jiaxiang Tang, Ruijie Lu, Zhaoshuo Li, Zekun Hao, Xuan Li, Fangyin Wei, Shuran Song, Gang Zeng, Ming-Yu Liu, and Tsung-Yi Lin. Efficient part-level 3d object generation via dual volume packing. arXiv preprint arXiv:2506.09980, 2025. Anh Thai, Weiyao Wang, Hao Tang, Stefan Stojanov, James Rehg, and Matt Feiszli. 3 2: 3d object part segmentation by 2d semantic correspondences. In European Conference on Computer Vision, pp. 149166. Springer, 2024. Ardian Umam, Cheng-Kun Yang, Min-Hung Chen, Jen-Hui Chuang, and Yen-Yu Lin. Partdistill: 3d shape part segmentation by vision-language model distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 34703479, 2024. Haohan Weng, Zibo Zhao, Biwen Lei, Xianghui Yang, Jian Liu, Zeqiang Lai, Zhuo Chen, Yuhong Liu, Jie Jiang, Chunchao Guo, et al. Scaling mesh generation via compressive tokenization. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1109311103, 2025. Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2146921480, 2025. Han Yan, Yang Li, Zhennan Wu, Shenzhou Chen, Weixuan Sun, Taizhang Shang, Weizhe Liu, Tian Chen, Xiaqiang Dai, Chao Ma, et al. Frankenstein: Generating semanticcompositional 3d scenes in one tri-plane. In SIGGRAPH Asia 2024 Conference Papers, pp. 111, 2024a. Han Yan, Mingrui Zhang, Yang Li, Chao Ma, and Pan Ji. Phycage: Physically plausible compositional 3d asset generation from single image. arXiv preprint arXiv:2411.18548, 2024b. Yunhan Yang, Yukun Huang, Yuan-Chen Guo, Liangjun Lu, Xiaoyang Wu, Edmund Lam, Yan-Pei Cao, and Xihui Liu. Sampart3d: Segment any part in 3d objects. arXiv preprint arXiv:2411.07184, 2024. Yunhan Yang, Yuan-Chen Guo, Yukun Huang, Zi-Xin Zou, Zhipeng Yu, Yangguang Li, Yan-Pei Cao, and Xihui Liu. Holopart: Generative 3d part amodal segmentation. arXiv preprint arXiv:2504.07943, 2025a. Yunhan Yang, Yufan Zhou, Yuan-Chen Guo, Zi-Xin Zou, Yukun Huang, Ying-Tian Liu, Hao Xu, Ding Liang, Yan-Pei Cao, and Xihui Liu. Omnipart: Part-aware 3d generation with semantic decoupling and structural cohesion. arXiv preprint arXiv:2507.06165, 2025b. Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: 3d shape representation for neural fields and generative diffusion models. ACM Transactions On Graphics (TOG), 42(4):116, 2023. Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM Transactions on Graphics (TOG), 43(4):120, 2024. Longwen Zhang, Qixuan Zhang, Haoran Jiang, Yinuo Bai, Wei Yang, Lan Xu, and Jingyi Yu. Bang: Dividing 3d assets via generative exploded dynamics. ACM Transactions on Graphics (TOG), 44(4):121, 2025. Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun. Point transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1625916268, 2021."
        },
        {
            "title": "Tencent Hunyuan",
            "content": "Zibo Zhao, Wen Liu, Xin Chen, Xianfang Zeng, Rui Wang, Pei Cheng, Bin Fu, Tao Chen, Gang Yu, and Shenghua Gao. Michelangelo: Conditional 3d shape generation based on shape-image-text aligned latent representation. Advances in neural information processing systems, 36:7396973982, 2023. Zibo Zhao, Zeqiang Lai, Qingxiang Lin, Yunfei Zhao, Haolin Liu, Shuhui Yang, Yifei Feng, Mingxin Yang, Sheng Zhang, Xianghui Yang, et al. Hunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets generation. arXiv preprint arXiv:2501.12202, 2025. Ziming Zhong, Yanyu Xu, Jing Li, Jiale Xu, Zhengxin Li, Chaohui Yu, and Shenghua Gao. Meshsegmenter: Zero-shot mesh semantic segmentation via texture synthesis. In European Conference on Computer Vision, pp. 182199. Springer, 2024."
        }
    ],
    "affiliations": [
        "CUHK",
        "HKU",
        "NJU",
        "ShanghaiTech",
        "Tencent Hunyuan",
        "ZJU"
    ]
}