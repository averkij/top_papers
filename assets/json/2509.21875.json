{
    "paper_title": "LUMINA: Detecting Hallucinations in RAG System with Context-Knowledge Signals",
    "authors": [
        "Min-Hsuan Yeh",
        "Yixuan Li",
        "Tanwi Mallick"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Retrieval-Augmented Generation (RAG) aims to mitigate hallucinations in large language models (LLMs) by grounding responses in retrieved documents. Yet, RAG-based LLMs still hallucinate even when provided with correct and sufficient context. A growing line of work suggests that this stems from an imbalance between how models use external context and their internal knowledge, and several approaches have attempted to quantify these signals for hallucination detection. However, existing methods require extensive hyperparameter tuning, limiting their generalizability. We propose LUMINA, a novel framework that detects hallucinations in RAG systems through context-knowledge signals: external context utilization is quantified via distributional distance, while internal knowledge utilization is measured by tracking how predicted tokens evolve across transformer layers. We further introduce a framework for statistically validating these measurements. Experiments on common RAG hallucination benchmarks and four open-source LLMs show that LUMINA achieves consistently high AUROC and AUPRC scores, outperforming prior utilization-based methods by up to +13% AUROC on HalluRAG. Moreover, LUMINA remains robust under relaxed assumptions about retrieval quality and model matching, offering both effectiveness and practicality."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 5 7 8 1 2 . 9 0 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "LUMINA : DETECTING HALLUCINATIONS IN RAG SYSTEM WITH CONTEXTKNOWLEDGE SIGNALS Min-Hsuan Yeh1,2, Yixuan Li1 & Tanwi Mallick2 1Department of Computer Science, University of Wisconsin-Madison {samuelyeh, sharonli}@cs.wisc.edu 2Argonne National Laboratory tmallick@anl.gov"
        },
        {
            "title": "ABSTRACT",
            "content": "Retrieval-Augmented Generation (RAG) aims to mitigate hallucinations in large language models (LLMs) by grounding responses in retrieved documents. Yet, RAG-based LLMs still hallucinate even when provided with correct and sufficient context. growing line of work suggests that this stems from an imbalance between how models use external context and their internal knowledge, and several approaches have attempted to quantify these signals for hallucination detection. However, existing methods require extensive hyperparameter tuning, limiting their generalizability. We propose LUMINA, novel framework that detects hallucinations in RAG systems through contextknowledge signals: external context utilization is quantified via distributional distance, while internal knowledge utilization is measured by tracking how predicted tokens evolve across transformer layers. We further introduce framework for statistically validating these measurements. Experiments on common RAG hallucination benchmarks and four open-source LLMs show that LUMINA achieves consistently high AUROC and AUPRC scores, outperforming prior utilization-based methods by up to +13% AUROC on HalluRAG. Moreover, LUMINA remains robust under relaxed assumptions about retrieval quality and model matching, offering both effectiveness and practicality."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language models (LLMs) are prone to hallucination, i.e., producing responses that are factually incorrect, nonsensical, or not grounded in the input or available data, while still appearing fluent and plausible (Luo et al., 2024; Huang et al., 2024). One commonly used strategy to mitigate hallucination is providing LLMs with relevant information retrieved from external knowledge bases, so-called Retrieval-Augmented Generation (RAG) (Shuster et al., 2021; Fan et al., 2024; Gao et al., 2024). However, despite having sufficient and relevant retrieved documents, RAG systems still have chance to hallucinate and produce statements that are either unsupported or contradict the retrieved information (Niu et al., 2024; Ridder & Schilling, 2025). Recent work has shown that such failures often arise from conflicts between an LLMs internal knowledge and the retrieved external context (Xu et al., 2024). In these cases, models tend to over-rely on internal knowledge regardless of correctness, undermining factual reliability (Longpre et al., 2021; Li et al., 2023; Sun et al., 2025a; Yamin et al., 2025). Inspired by this observation, recent approaches attempt to quantify hallucinations in RAG (Sun et al., 2025b; Wang, 2025; Tao et al., 2025). However, existing methods rely on mechanistic interpretability heuristicssuch as selecting specific attention heads or transformer layers to achieve the optimal hallucination detection performancewhich require heavy hyperparameter tuning and often fail to generalize across models and datasets. To overcome these limitations, we propose LUMINA, new framework for detecting hallucinations in RAG system through contextknowledge signals, namely the signals of external context utilization and internal knowledge utilization, as shown in Figure 1. Rather than targeting particular attention heads or layers, LUMINA measures these signals in layer-agnostic manner, requiring less hyperparameter"
        },
        {
            "title": "Preprint",
            "content": "Figure 1: The overview of LUMINA. For external context utilization, we propose to measure the maximum mean discrepancy between two next token probability distributions conditioned on different documents. For internal knowledge utilization, we introduce the idea of information processing rate by looking at the ratio of the most probable output tokens probability across transformer layers and use it to determine the amount of utilized internal knowledge when generating the next token. tuning. Specifically, for external context utilization, we measure the discrepancy between predictive distributions conditioned on retrieved documents vs. random documents. larger discrepancy indicates that the LLM is more sensitive to semantic changes in documents when generating the answer, implying higher reliance on the external context. For internal knowledge utilization, we track how the models internal states and token predictions evolve across layers: if the internal layers predictions do not converge to the final output until later layers, it suggests more information is added during the layer-wise process, implying stronger reliance on internal knowledge. We further validate the soundness of our measurements through statistical hypothesis testing on verifiable implications, establishing stronger link between the proposed scores and actual utilization. We conduct extensive experiments on common RAG hallucination benchmarks and across four LLMs to evaluate the performance of LUMINA on hallucination detection. The results show that the hallucination score calculated with LUMINA outperforms existing methods by significant margin. For example, LUMINA achieves more than 0.9 AUROC on the HalluRAG datasets across models, with improvements of up to +13% over prior state-of-the-art. Importantly, the decomposition into external context utilization and internal knowledge utilization provides interpretable insights: hallucinations are strongly associated with low external context scores and disproportionately high internal knowledge scores. We further demonstrate that LUMINA is robust across different retrieval settings. These results validate both the effectiveness and practicality of our framework. Our key contributions are summarized as follows: 1. We propose LUMINA, novel approach to quantify utilization of external context and internal knowledge for RAG-based hallucination detection. 2. We propose framework to statistically validate LUMINA, showing that they align with the intended results. 3. We conduct extensive experiments and show that LUMINA outperforms both score-based and learning based methods in hallucination detection, establishing new state-of-the-art."
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "2.1 PROBLEM FORMULATION AND MOTIVATION RAG systems aim to improve factuality by incorporating external documents into the generation process. However, hallucinations still occur when model over-relies on its internal parametric knowledge and under-utilizes the retrieved external context. We provide formal definition below. Conjecture 1 (External context vs. internal knowledge utilization). Let pθ be an RAG-based LLM that takes query and retrieved documents as inputs to generate response a. Assume is relevant to and contains correct and sufficient information to respond to q. Denote Epθ (aq, d), Ipθ (aq, d) be the signals of external context utilization and internal knowledge utilization of pθ, respectively, when generating a. The response is more likely to be hallucination if Ipθ (aq, d) Epθ (aq, d)."
        },
        {
            "title": "Preprint",
            "content": "Definition 2.1 (Hallucination in an RAG system). Based on Conjecture 1, we define hallucination scores at both the token and response level. Specifically, for generated answer = (a1, . . . , aT ) with tokens, let Epθ (atq, d, a<t), Ipθ (atq, d, a<t) be the signals of external context utilization and internal knowledge utilization of pθ when generating the token at, respectively. The token-level hallucination score of at is defined as Ht(atq, d, a<t) := λ Ipθ (atq, d, a<t) (1 λ) Epθ (atq, d, a<t), (1) where λ is hyperparameter. Similarly, the response-level hallucination score of the response is defined as the average of the token-level hallucination scores, i.e., Hr(aq, d) :="
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 Ht(atq, d, a<t). (2) In this paper, we focus on the core question: How to quantify the utilization of external context and internal knowledge?"
        },
        {
            "title": "2.2 RELATED WORK",
            "content": "Prior works have attempted to quantify Epθ (atq, d, a<t) and Ipθ (atq, d, a<t) using empirical metrics (Sun et al., 2025b; Wang, 2025). For example, Sun et al. (2025b) proposed ReDeEP, which measures external context utilization through cosine similarity between the generated token and tokens in context that have high attention weights w.r.t. certain attention heads. For internal knowledge utilization, it measures the Jensen-Shannon (JS) divergence between the hidden states before/after the FFN layer of certain transformer layers. The success of ReDeEP on some RAG hallucination detection datasets validates the idea of Conjecture 1. Wang (2025) combine the idea of ReDeEP with semantic entropy probes (SEP) (Han et al., 2024). They quantified external context utilization by measuring the semantic correlation between the semantic entropy of the generated token and attended tokens in the context. For internal knowledge utilization, they measured the absolute difference between the semantic entropy corresponding to hidden states before and after the FFN layer. Although these approaches effectively detect hallucinations in the RAG system, they have two major limitations. First, these approaches require selecting specific attention heads and transformer layers to compute the external context score and internal knowledge score. However, the selection process is non-trivial and requires extensive hyperparameter tuning. In addition, these hyperparameters are dataset and model-specific, limiting the generalizability across different datasets and models. Another limitation is that although these works demonstrated the correlation between their proposed scores and hallucination, they did not validate whether the scores truly reflect the utilization of external context and internal knowledge."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "Overview. To overcome the limitations of prior empirical approaches, we introduce LUMINA, new framework for quantifying both external context and internal knowledge utilization. In Section 3.1 and Section 3.2, we formalize the quantification of the two signals, which will be combined to compute the final hallucination score. In Section 3.3, we propose to validate the soundness of LUMINA through extensive hypothesis testing, addressing the challenges of score validation in previous works. 3.1 QUANTIFYING EXTERNAL CONTEXT UTILIZATION To measure LLMs external context utilization, our key idea is to assess its sensitivity to semantic changes in the input documents. If the LLM effectively incorporates the external context to generate response, then replacing relevant documents with random ones should noticeably change the token probability distribution. Formally, we propose the following measurement: Measurement 1 (External context utilization). Let be an LLM-generated answer to query with retrieved documents as input. Assume is relevant to and contains correct and sufficient information to respond to q. Let be subset of random documents irrelevant to q. The models predictive distribution over tokens induces two (approximated) distributions over embeddings: (Ev) = pθ(v q, d, a<t), Q(Ev) = pθ(v q, d, a<t), (3)"
        },
        {
            "title": "Preprint",
            "content": "where each token in the vocabulary space is associated with an embedding Ev RD. Then, the degree to which the model uses external context for generating token at is reflected in the divergence between the two distributions conditioned on versus d: where : R+ is distance function between two probability distributions. Epθ (atq, d, a<t) := (P, Q), (4) Note that we adopt (Ev) and Q(Ev) as proxies to approximate the ground truth embedding distribution, as it is challenging to estimate it over the high-dimensional vector space. We instantiate with Maximum Mean Discrepancy (MMD), which measures the distance of two probability distributions by mapping them into Reproducing Kernel Hilbert Space. Definition 3.1 (Maximum Mean Discrepancy (Gretton et al., 2012)). Given positive semi-definite kernel function k, the squared MMD between two probability distributions and is defined as MMD k(P, Q) := EA,AP [k(A, A)] + EB,BQ[k(B, B)] 2EAP,BQ[k(A, B)], (5) where A, are i.i.d. vectors randomly sampled from and B, are sampled from Q. This metric provides us with non-parametric and LLM-agnostic way to quantify the utilization of external context, making it generalizable to different models and datasets. By rewriting MMD with and we defined in Eq. (3) over token embeddings, we obtain: Epθ (atq, d, a<t) := (cid:88) (Eu)P (Ev)k(Eu, Ev) + (cid:88) u,vV Q(Eu)Q(Ev)k(Eu, Ev) u,vV (cid:88) 2 u,vV We adopt the cosine kernel: (Eu)Q(Ev)k(Eu, Ev). kcos(Eu, Ev) := (cid:18) 1 2 1 + ET Ev Eu2Ev2 (cid:19) . (6) (7) Note that the cosine kernel acts equivalent to computing cosine similarity between two token embeddings, which is commonly used to measure the semantic similarity of two pieces of text. In Section 4.4, we experiment with alternative kernels such as the Gaussian kernel, and we show that our method is not sensitive to the choice of kernels. 3.2 QUANTIFYING INTERNAL KNOWLEDGE UTILIZATION To quantify the utilization of internal knowledge, we focus on the signals in internal states of an LLM. Specifically, transformer-based autoregressive LLM has multiple layers, through which information is gradually added into residual stream that flows from the input layer to the output layer, shaping the output token representation and probability distribution (Geva et al., 2022). Studies have found that by projecting the hidden state of each layer to the token representation space, we can interpret what an LLM believes after the process of each layer (nostalgebraist, 2020). In addition, via logit lens (nostalgebraist, 2020), studies have identified the saturation event in an LLM, i.e., the top-k prediction of the LLM remains constant in all subsequent layers after certain layer called the k-th saturation layer (Geva et al., 2022; Lioubashevski et al., 2025). Inspired by these observations, we propose metric that quantifies how actively the model updates its predictions across layers. Formally, we define the rate of information processing below. Definition 3.2 (Information processing rate). Given an LLM pθ with layers, which takes x<t as the input and generate the next token xt, we denote xt,1 := arg maxv pθ(vx<t) as the most probable next token and ht,l RD as the l-th layer hidden state when generating xt. Let : RD be projection from hidden state to probability distribution over the vocabulary V. The information processing rate of pθ conditioned on x<t is defined as Rpθ (x<t) := (cid:16) (cid:80)L1 l=1 1 min (cid:110) [f (ht,l)]xt, pθ(xt,1x<t) , 1 (cid:111)(cid:17) (cid:80)L1 l=1 H(f (ht,l )) 4 , (8)"
        },
        {
            "title": "Preprint",
            "content": "where H() is the entropy function, and is the logit lens (nostalgebraist, 2020) that projects the hidden state of each layer to logits using the LayerNorm and the unembedding matrix , i.e., LogitLens(h) := LayerNorm(h)W , () := Softmax(LogitLens()). (9) Specifically, Rpθ (x<t) captures two key elements: (1) The numerator measures the extent to which each layers prediction for the most probable token differs from the final output, weighted by layer [f (ht,l)]xt,1 depth to emphasize later-layer processing. When pθ(xt,1x<t) is small, it indicates the layer has not yet converged to the final prediction, suggesting active information processing. (2) The denominator provides adaptive normalization based on each layers prediction uncertainty (entropy), giving higher relative weight to layers that exhibit confident, decisive processing patterns. Given this definition, we attribute the utilization of internal knowledge to the 1st information processing rate and propose the following measurement: Measurement 2 (Internal knowledge utilization). An LLM is considered to be more heavily utilizing its internal knowledge to generate at when it exhibits higher information processing rate. Specifically, we propose that the internal knowledge utilization of an LLM to generate at given and can be measured as Ipθ (atq, d, a<t) := Rpθ (q, d, a<t). (10) 3.3 STATISTICAL VALIDATION OF THE MEASUREMENT In this section, we validate the soundness of our approach. Previous work such as Sun et al. (2025b) primarily verified whether their scores have causal relationship with hallucination but failed to show the relationship between the scores and actual external context/internal knowledge utilization. To address this, we directly assess whether our measurements capture the intended notion of utilization. Specifically, we derive verifiable implications that must hold if our proposed measurements are valid. We then use the proposed score to verify these implications with statistical hypothesis testing. If the proposed score passes all tests, the score reflects the corresponding utilization. External context utilization. To validate Measurement 1, we examine the following implications: H1. If Measurement 1 is valid, then Epθ (atq, d, a<t) > Epθ (a <t). That is, generations with tq, , retrieved documents have stronger external context utilization than generations without. H2. If Measurement 1 is valid, then Epθ (atqsum, dsum, a<t) > Epθ (atqQA, dQA, a<t). That is, summarization tasks should exhibit higher external context utilization than question answering. Internal knowledge utilization. To validate Measurement 2, we examine the following: H3. If Measurement 2 is valid, then R1 pθ (q, d, a<t). That is, generating an answer without retrieved documents requires more internal knowledge than with retrieved documents. (qD2T, dD2T, a<t) > R1 (qsum, dsum, a<t). In other words, pθ H4. If Measurement 2 is valid, then R1 pθ (q, , a<t) > R1 pθ data-to-text generation requires more internal knowledge than summarization. Table 1: All the hypotheses pass the statistical tests. For H1, H2, H4, we report one-tailed t-statistic; for H3, we report paired-sample one-tailed t-statistic. All four implications reject their null hypothesis, validating the soundness of LUMINA. Note that the tests are run with > 65k tokens and the magnitude of the t-statistic means how easy we can distinguish the two distributions. * < 0.05; ** < 0.01; *** < 0.001. LLM H1 H2 H3 Llama2-7B 79.85*** Llama2-13B 73.49*** 94.15*** Llama3-8B 88.70*** Mistral-7B 27.67*** 20.51*** 6.35*** 6.21*** 101.20*** 91.00*** 102.44*** 109.26*** 15.36*** 7.71*** 15.85*** 9.69*** To examine H1, we utilize data in the QA set of RAGTruth (Niu et al., 2024). We use the original data to compute Epθ (atq, d, a<t), and generate additional answers without providing retrieved documents as to compute Epθ (a <t). For H2, we utilize the Summary and QA set of RAGTruth; for H4, tq, ,"
        },
        {
            "title": "Preprint",
            "content": "the Summary and Data2Text set; and for H3, the entire RAGTruth dataset. We test the hypotheses with four different instruction-tuned LLMs, including Llama2-{7B, 13B} (Llama Team, 2023), Llama3-8B (Llama Team, 2024), and Mistral-7B (Jiang et al., 2023). Results in Table 1 indicate that all four implications reject their null hypothesis, validating our measurements for external context utilization and internal knowledge utilization."
        },
        {
            "title": "4.1 EXPERIMENTAL SETTINGS",
            "content": "Baselines. We compare LUMINA with baselines across 8 different hallucination detection strategies: (1) Uncertainty-based, which detects hallucination by estimating uncertainty via token-level probability or entropy. Baselines of this category include Perplexity (Ren et al., 2023), LN-Entropy (Malinin & Gales, 2021), and Focus (Zhang et al., 2023). (2) Cross-sample consistency, which detects hallucination by sampling multiple responses for query and measuring their (logic/semantic) consistency. Approaches include SelfCKGPT (Manakul et al., 2023) and EigenScore (Chen et al., 2024). (3) Verbalization, which detects hallucinations by prompting another LLM to score the correctness of the answer. Approaches include P(True) (Kadavath et al., 2022) and RefChecker (Hu et al., 2024). (4) Utilization of external context and internal knowledge, which decouples these two signals via findings in the study of mechanistic interpretability. Baseline of this category is ReDeEP (Sun et al., 2025b). Details of each baseline are introduced in Appendix B. LLMs. To demonstrate the generalizability of LUMINA, we conduct experiments with four opensourced LLMs, including Llama2-{7B, 13B}, Llama3-8B, and Mistral-7B. Specifically, each LLM is used to detect hallucinations in responses generated by the same model. We also report the performance of proxy LLM setting, i.e., using one LLM to detect hallucinations in responses generated by another model, in Sec. 4.3. All LLMs are the instruction-tuned version. Datasets. Experiments are conducted on two representative RAG hallucination detection benchmarks: RAGTruth (Niu et al., 2024), the first high-quality RAG hallucination detection dataset, consisting of three types of RAG tasks, including question answering, data-to-text writing, and news summarization. HalluRAG (Ridder & Schilling, 2025), dataset of free-form question answering in an RAG setting. Details of these datasets are introduced in Appendix C. Evaluation metrics. We measure the performance with three metrics: AUROC, AUPRC, and Pearsons correlation coefficient (PCC). AUPRC captures precision-recall trade-offs, while AUROC evaluates the trade-offs between true and false positive rates. These metrics are threshold-agnostic and better suited for comparing scoring-based methods. We also report the optimal precision, recall, and F1 score (PrecOpt, RecallOpt, F1Opt) in Appendix E.1, where F1Opt is the optimal F1 score among all possible threshold and PrecOpt and RecallOpt are corresponding Precision and Recall. Implementation details. We adopt λ = 0.5 to compute Eq. (1) as ablations show that balancing the scores of external context and internal knowledge yields relatively strong performance (see Appendix E.3 for detailed ablations). Other implementation details and computational resources of LUMINA are reported in Appendix and G, respectively. 4.2 MAIN RESULTS LUMINA achieves state-of-the-art performance. Table 2 summarizes the experimental comparison across methods. The results show that LUMINA has consistently high performance across datasets and LLMs. In particular, it almost always outperforms ReDeEP, the previous attempt of measuring the utilization of external context and internal knowledge to detect hallucinations. The gap between them is particularly large on the HalluRAG dataset. Noticeably, LUMINA achieves more than 0.9 AUROC on the HalluRAG dataset across models, outperforming the baselines by substantial margin. We further conduct an error analysis to see when and why LUMINA fails. Specifically, we sample 20 false-negative and false-positive cases from the RAGTruth dataset, respectively, and qualitatively analyze the reason of errors. The result reveals that most of the errors stem from incorrect labels and low-quality retrieved documents of the dataset, suggesting potentially higher performance in setting with high-quality data. The details of this analysis can be found in Appendix F."
        },
        {
            "title": "Preprint",
            "content": "Table 2: LUMINA consistently achieves high performance across datasets and LLMs. The highest scores are set in bold. Note that HalluRAG dataset does not contain responses generated by Llama3-8B. LLM Approach AUROC PCC AUPRC AUROC PCC AUPRC RAGTruth HalluRAG Llama2-7B Llama2-13B Llama3-8B Mistral-7B Perplexity LN-Entropy Focus SelfCKGPT EigenScore P(True) RefChecker ReDeEP LUMINA Perplexity LN-Entropy Focus SelfCKGPT EigenScore P(True) RefChecker ReDeEP LUMINA Perplexity LN-Entropy Focus SelfCKGPT EigenScore P(True) RefChecker ReDeEP LUMINA Perplexity LN-Entropy Focus SelfCKGPT EigenScore P(True) RefChecker ReDeEP LUMINA 0.5103 0.6964 0.5633 0.4787 0.5454 0.5197 0.5869 0.7273 0.7646 0.4539 0.7677 0.5451 0.4545 0.6329 0.7543 0.6363 0.8055 0.8569 0.7130 0.7072 0.5258 0.5339 0.6001 0.5407 0.5718 0.7495 0.7446 0.6200 0.7607 0.7803 0.5680 0.5642 0.7530 0.6017 0.7615 0.7685 -0.0118 0.3318 0.0811 -0.0279 0.0717 0.0404 0.1751 0.3859 0.4546 -0.1020 0.4446 0.0130 -0.0835 0.2080 0.3821 0.2723 0.5195 0. 0.3568 0.3500 0.0375 0.0491 0.1774 0.0928 0.1494 0.4458 0.4236 0.1463 0.4386 0.4188 0.0812 0.1006 0.4334 0.2047 0.4613 0.4623 0.4836 0.6615 0.5386 0.4859 0.5183 0.5334 0.6827 0.6971 0.7491 0.3993 0.6838 0.4603 0.4106 0.5202 0.7418 0.6988 0.7792 0.8436 0.7183 0.7109 0.5380 0.5550 0.5824 0.5502 0.6874 0.7817 0.7874 0.6106 0.7377 0.7647 0.5698 0.5637 0.7494 0.7303 0.8133 0. 0.4610 0.9102 0.5652 0.4669 0.6720 0.5847 0.4907 0.6771 0.9153 0.2548 0.7826 0.6739 0.7729 0.7862 0.6914 0.5670 0.7645 0.9166 - - - - - - - - - -0.0673 0.5133 0.2415 -0.0070 0.2705 0.1143 -0.0255 0.1468 0.6554 -0.2366 0.3262 0.2563 0.2640 0.4250 0.2480 0.1390 0.2705 0.6044 - - - - - - - - - 0.2332 0.6812 0.3844 0.2377 0.4470 0.2976 0.2750 0.3378 0.7572 0.0944 0.3567 0.3181 0.3029 0.4867 0.2146 0.3169 0.3001 0.8497 - - - - - - - - - 0.5362 0.9188 0.8565 0.8275 0.8652 0.5899 0.5065 0.7870 0.9899 -0.0264 0.6076 0.4318 0.5552 0.6411 0.0886 0.0153 0.2611 0.7529 0.1261 0.7347 0.4219 0.6098 0.7337 0.1771 0.1784 0.3516 0. Comparison with supervised approach. We also compare LUMINA with SAPLMA (Azaria & Mitchell, 2023), supervised approach that trained binary classifier on the last token hidden states to detect hallucination. Since our method is unsupervised in nature and does not rely on labeled data, the supervised baseline can be viewed as performance upper bound. Results in Appendix E.2 show that LUMINA achieves competitive performance against SAPLMA and even sometimes outperforms it, all without any training, highlighting both its supreme performance and ease of deployment. 4.3 RELAXING ASSUMPTIONS In Section 3, we implicitly make two assumptions: 1) perfect context assumption: we assume the retrieved documents are correct, sufficient, and relevant to the query. 2) same LLM assumption: we assume the LLM used to compute the external context score and internal knowledge score is the same as the LLM used to generate responses. These two assumptions are usually introduced in other hallucination detection works as well (Zhang et al., 2023; Sun et al., 2025b; Park et al., 2025). Unfortunately, they are often strong and have significant impact on the performance, limiting the usability of these methods (such as for open-sourced model-generated responses only). In this section, we investigate the performance of LUMINA when relaxing these two assumptions, showing the robustness of LUMINA."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Noises in context do not largely degrade the performance of LUMINA. We add 0 30% noises to the retrieved documents and random documents and evaluate the hallucination detection performance. The experiment is conducted on the RAGTruth dataset. Figure 3: The same LLM setting is not essential for LUMINA to achieve the optimal performance. On the RAGTruth dataset, for each set of responses generated by the same LLM, we apply LUMINA with different base LLM to detect hallucination. Bars in more saturated shades indicate settings where the same LLM is used for both generation and detection. Relaxing perfect context assumption. We relax this assumption by gradually injecting noise into the retrieved documents and random documents d. Specifically, for the assumption on retrieved documents, we randomly remove {0%, 10%, 20%, 30%} sentences from d. And for the assumption on the random documents, we randomly add {0%, 10%, 20%, 30%} sentences from to d. Figure 2 shows the AUPRC of all noise injection combinations on the RAGTruth dataset. The result shows that except Llama2-13B, which has > 0.1 performance drop after injecting noises, LUMINA with other LLMs yields stable performance. Furthermore, after removing sentences from retrieved documents, LUMINA with Llama3-8B even achieves higher AUPRC. These results demonstrate the robustness of LUMINA against context noises. Relaxing the same LLM assumption. We relax this assumption by using different LLMs to compute the scores for response. Specifically, we use Llama2-7B, Llama2-13B, Llama3-8B, Mistral-7B, and Qwen2.5-7B (Qwen, 2025) to detect hallucination on the RAGTruth dataset, which contains responses generated by Llama2-7B, Llama2-13B, Llama2-70B, Llama3-8B, Mistral-7B, GPT-3.5, and GPT-4. Figure 3 shows AUROC across different generator-detector LLM pairs. The results show that the same model setting is not always necessary. Specifically, Llama2-7B achieves comparable or higher AUROC than Llama3-8B on Llama3-8B responses. Moreover, LUMINA with Llama2-7B and Llama3-8B has stable performance across different generation LLMs. Overall, LUMINA demonstrates plausible solution for generation LLM-agnostic hallucination detection, which is more practical in real-world scenarios. 4.4 ABLATION STUDY Impact of external context & internal knowledge. Our final hallucination score is the combination of the external context score and internal knowledge score. To obtain more insights into how each component contributes to the final score, we ablate on the components by considering only the"
        },
        {
            "title": "Preprint",
            "content": "Figure 4: Combining scores of external context and internal knowledge boosts the hallucination detection performance. Left: 2D kernel density estimation (KDE) of the distribution of external context score and internal knowledge score of Llama2-13B responses on the RAGTruth dataset. Right: Hallucination detection performance with external/internal score only, as well as the performance of their combination. external context score and internal knowledge score. The right plot of Figure 4 shows that combining scores of external context and internal knowledge achieves the highest AUPRC on the RAGTruth dataset for every LLM. For example, on Llama2-13B, the combination leads to more than 10% improvement. This observation justifies the effectiveness of the hallucination score introduced in Definition 2.1. In addition, the left plot of Figure 4 shows that response generated by Llama2-13B is more likely to be hallucination if it has high internal knowledge score and low external context score. This observation validates Conjecture 1 and suggests that Eq. (1) does not imply an objective function that forces LLM only using external context to answer questions. Instead, it suggests that the internal knowledge utilization should be grounded in an external context to achieve reliable generation. Further experiments are shown in Appendix E.3. 2 . (cid:16) i.e., RBFσ(Eu, Ev) Impact of kernel selection. We ablate on the selection of kernel {Cosine, RBF0.5, RBF0.7, RBF1, RBF2, RBF3}, where RBFσ := is RBF kernel, (cid:17) EuEv2 exp Figure 5 shows the 2σ2 AUPRC of different kernels on the RAGTruth dataset. The results show that the optimal setting of the RBF kernel has similar performance to the cosine kernel, suggesting our external context score is insensitive to the kernel selection. We default to the cosine kernel as it is less dependent on hyperparameters, making it easy to use in practice. Figure 5: MMD with cosine kernel performs similarly or better than with RBF kernel."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we introduce LUMINA, novel approach to quantify the utilization of external context and internal knowledge. These contextknowledge signals provide principled way to assess how LLMs balance retrieved evidence against their own parametric knowledge during generation. Experimental results on common benchmarks across four LLMs demonstrate that LUMINA has consistently high performance on hallucination detection for RAG-based generations, outperforming prior attempts of quantifying external context and internal knowledge utilization, and being competitive with supervised hallucination detection models. Analyses also show that LUMINA is robust against noise in retrieved documents and can be generalized to the proxy LLM setting, demonstrating its usability in real-world scenarios."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "This work introduce LUMINA, novel way to estimate the utilization of external context and internal knowledge when an LLM generating response with the RAG setup. LUMINA significantly improves the performance of hallucination detection, which will help increase the reliability of RAG systems in real-world deployments and reduce the risk of sharing misinformation. Through deeper analysis on LUMINA in the future, researchers may better understand how LLMs utilize external context and internal knowledge to generate responses. Such findings will help the community design approaches to mitigate hallucinations and create more reliable AI system."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We provide all details of the implementation of LUMINA in Appendix. D, including the approximation of MMD, the selection of kernel, and choice of random documents for measuring external context score, as well as the calibration of internal knowledge score. In Sec. 4.1, we illustrate the experimental settings, including baselines, datasets, LLMs, and evaluation metrics. The details of baselines and datasets are further provided in Appendix and C, respectively. Furthermore, we provide the codebase of LUMINA at https://anonymous.4open.science/r/LUMINA-E71B. These comprehensive reports will help future studies easily reproduce our experiments."
        },
        {
            "title": "ACKNOWLEDGEMENT",
            "content": "We thank Chongdae Oh and Seongheon Park for their valuable feedback on the draft. This work was supported by Laboratory Directed Research and Development funding from Argonne National Laboratory, provided by the Office of Science, U.S. Department of Energy under Contract No. DEAC02-06CH11357. Y. Li acknowledges support from AFOSR Young Investigator Program under award number FA9550-23-1-0184, National Science Foundation under awards IIS-2237037 and IIS-2331669, Office of Naval Research under grant number N00014-23-12643, Schmidt Sciences Foundation, Open Philanthropy, and Alfred P. Sloan Fellowship. The funders had no role in study design, data collection, data analysis or interpretation, or manuscript preparation."
        },
        {
            "title": "BIBLIOGRAPHY",
            "content": "Amos Azaria and Tom Mitchell. The internal state of an LLM knows when its lying. In Findings of the Association for Computational Linguistics: EMNLP 2023, 2023. Chao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Mingyuan Tao, Zhihang Fu, and Jieping Ye. INSIDE: LLMs internal states retain the power of hallucination detection. In The Twelfth International Conference on Learning Representations, 2024. Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. survey on rag meeting llms: Towards retrieval-augmented large language models. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 24, pp. 64916501, 2024. ISBN 9798400704901. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997, 2024. Mor Geva, Avi Caciularu, Kevin Wang, and Yoav Goldberg. Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 3045, 2022. Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Scholkopf, and Alexander Smola. kernel two-sample test. Journal of Machine Learning Research, 13(25):723773, 2012. ISSN 1533-7928."
        },
        {
            "title": "Preprint",
            "content": "Jiatong Han, Jannik Kossen, Muhammed Razzak, Lisa Schut, Shreshth Malik, and Yarin Gal. In ICML 2024 Semantic entropy probes: Robust and cheap hallucination detection in llms. Workshop on Foundation Models in the Wild, 2024. Xiangkun Hu, Dongyu Ru, Lin Qiu, Qipeng Guo, Tianhang Zhang, Yang Xu, Yun Luo, Pengfei Liu, Yue Zhang, and Zheng Zhang. Refchecker: Reference-based fine-grained hallucination checker and benchmark for large language models. arXiv preprint arXiv:2405.14486, 2024. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Trans. Inf. Syst., 2024. ISSN 1046-8188. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221, 2022. Daliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix Yu, and Sanjiv Kumar. Large language models with controllable working memory. In Findings of the Association for Computational Linguistics: ACL 2023, 2023. Daria Lioubashevski, Tomer Schlank, Gabriel Stanovsky, and Ariel Goldstein. Looking beyond the top-1: Transformers determine top tokens in order. In Proceedings of the 42nd International Conference on Machine Learning, 2025. AI @ Meta Llama Team. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. AI @ Meta Llama Team. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. Entity-based knowledge conflicts in question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021. Junliang Luo, Tianyu Li, Di Wu, Michael R. M. Jenkin, Steve Liu, and Gregory Dudek. Hallucination detection and hallucination mitigation: An investigation. arXiv preprint arXiv:2401.08358, 2024. Andrey Malinin and Mark Gales. Uncertainty estimation in autoregressive structured prediction. In International Conference on Learning Representations, 2021. Potsawee Manakul, Adian Liusie, and Mark Gales. SelfCheckGPT: Zero-resource black-box hallucination detection for generative large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023. Cheng Niu, Yuanhao Wu, Juno Zhu, Siliang Xu, KaShun Shum, Randy Zhong, Juntong Song, and Tong Zhang. RAGTruth: hallucination corpus for developing trustworthy retrieval-augmented language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2024. nostalgebraist. interpreting gpt: the logit lens, 2020. URL https://www.lesswrong.com/ posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens. Seongheon Park, Xuefeng Du, Min-Hsuan Yeh, Haobo Wang, and Yixuan Li. Steer LLM latents for hallucination detection. In Forty-second International Conference on Machine Learning, 2025."
        },
        {
            "title": "Preprint",
            "content": "Qwen. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2025. Jie Ren, Jiaming Luo, Yao Zhao, Kundan Krishna, Mohammad Saleh, Balaji Lakshminarayanan, and Peter Liu. Out-of-distribution detection and selective generation for conditional language models. In The Eleventh International Conference on Learning Representations, 2023. Fabian Ridder and Malte Schilling. The hallurag dataset: Detecting closed-domain hallucinations in rag applications using an llms internal states. arXiv preprint arXiv:2412.17056, 2025. Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmentation reduces hallucination in conversation. In Findings of the Association for Computational Linguistics: EMNLP 2021, 2021. Kaiser Sun, Fan Bai, and Mark Dredze. What is seen cannot be unseen: The disruptive effect of knowledge conflict on large language models. arXiv preprint arXiv:2506.06485, 2025a. ZhongXiang Sun, Xiaoxue Zang, Kai Zheng, Jun Xu, Xiao Zhang, Weijie Yu, Yang Song, and Han Li. RedeEP: Detecting hallucination in retrieval-augmented generation via mechanistic interpretability. In The Thirteenth International Conference on Learning Representations, 2025b. Yufei Tao, Adam Hiatt, Rahul Seetharaman, and Ameeta Agrawal. lost-in-the-later: Framework for quantifying contextual grounding in large language models. arXiv preprint arXiv:2507.05424, 2025. Lei Wang. Seredeep: Hallucination detection in retrieval-augmented models via semantic entropy and context-parameter fusion. arXiv preprint arXiv:2505.07528, 2025. Rongwu Xu, Zehan Qi, Zhijiang Guo, Cunxiang Wang, Hongru Wang, Yue Zhang, and Wei Xu. Knowledge conflicts for LLMs: survey. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, 2024. Khurram Yamin, Gaurav Ghosal, and Bryan Wilder. Llms struggle to perform counterfactual reasoning with parametric knowledge. arXiv preprint arXiv:2506.15732, 2025. Tianhang Zhang, Lin Qiu, Qipeng Guo, Cheng Deng, Yue Zhang, Zheng Zhang, Chenghu Zhou, Xinbing Wang, and Luoyi Fu. Enhancing uncertainty-based hallucination detection with stronger In Proceedings of the 2023 Conference on Empirical Methods in Natural Language focus. Processing, 2023."
        },
        {
            "title": "CONTENTS",
            "content": "A Broader Impacts Details of Baselines Details of Datasets Implementation Details of LUMINA Additional Experimental Results E.1 Evaluation with Other Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Compare with supervised baselines . . . . . . . . . . . . . . . . . . . . . . . . . . E.3 Performance with Hyperparameter Tuning . . . . . . . . . . . . . . . . . . . . . . Error Analysis Computational Resources"
        },
        {
            "title": "A BROADER IMPACTS",
            "content": "13 13 14 14 15 16 16 17 17 Beyond hallucination detection, LUMINA has broader impacts in interpretability and LLM understanding. Specifically, our proposed score validation framework in Sec. 3.3 suggests novel way to empirically validate the finding of mechanistic interpretability, which can be used to highlight the soundness of proposed hypotheses. In addition, our proposed information processing rate in Sec. 3.2 presents new lens for examining the internal states of LLMs. Deeper investigation of this measure could help the community better characterize how LLMs reason and leverage internal knowledge, potentially leading to more reliable training and inference processes. While our experiments focus on using LUMINA for hallucination detection, its utility extends further. For instance, it could inform the design of new training objectives or decoding algorithms aimed at mitigating hallucinations, ultimately making LLMs more reliable and trustworthy."
        },
        {
            "title": "B DETAILS OF BASELINES",
            "content": "(1) Token-level uncertainty: Perplexity: This approach measured the perplexity of the generated response as uncertainty and to detect hallucinations. LN-Entropy: This approach measured sequence-level uncertainty with entropy normalized by sequence length. higher entropy indicates greater uncertainty and higher likelihood of hallucinations. Focus: This approach used entropy and token probability as based score, and calibrated it by focusing only on key informative tokens and propagating the score according to the attention weight. (2) Cross-sample consistency:"
        },
        {
            "title": "Preprint",
            "content": "SelfCKGPT: This approach sampled multiple responses and used an NLI model to check the logistic consistency between the target generation and additional samples. In our experiment, we follow the setting of Manakul et al. (2023) to set the sample size as 20. EigenScore: Similar to SelfCKGPT, this approach sampled multiple responses and checked the semantic consistency between the additional samples and the target generation through measuring the eigenvalues of responses covariance matrix. In our experiment, we set the sample size as 20. (3) Verbalization: P(True): This approach prompted an LLM with the generated answer and asked whether the LLM think the answer is true. The approach then estimated the probability of the Yes generated by the LLM. RefChecker: This approach prompted an LLM to extract claims from generation, and prompted another LLM to verify the logical consistency between each claim and reference documents. In our experiment, we use dongyru/Mistral-7B-Claim-Extractor, the model finetuned by Hu et al. (2024), to extract claims. (4) Utilization of external context and internal knowledge: ReDeEP: For external context utilization, ReDeEP measured the cosine similarity between the generated token and topK attended tokens in retrieved documents. For internal knowledge utilization, it measured the JS divergence of the vocabulary distributions between logit lens outputs before and after FFN layers in Transformer. At the end, it weighted summed the two scores to obtain hallucination score."
        },
        {
            "title": "C DETAILS OF DATASETS",
            "content": "RAGTruth. The RAGTruth dataset is human annotated hallucination detection dataset, containing 15,090 training data and 2,700 testing data. Each data point consists of query, retrieved documents, LLM-generated answer, and span-level hallucination annotation. The dataset covers three tasks, including summarization, data to text generation, and question answering. For each query-anddocuments pair, RAGTruth provides answers generated by six different LLMs, including Llama2-7B, Llama2-13B, Llama2-70B, Mistral-7B, GPT-3.5, and GPT-4. In our experiment, we also utilize the extended test set provided by Sun et al. (2025b), who curated and annotated Llama3-8B generated responses. HalluRAG. HalluRAG is an LLM annotated hallucination detection dataset for question answering. Ridder & Schilling (2025) prompted GPT-4o to generate question given sentences from Wikipedia, then used Llama2-7B, Llama2-13B, and Mistral-7B to generate answer for each question given the relevant Wikipedia article. The hallucination labels were assigned by GPT-4o with Chain-ofThought (CoT) prompt and verified by human. HalluRAG contains both answerable and unanswerable questions, while we only use the answerable instances for evaluation."
        },
        {
            "title": "D IMPLEMENTATION DETAILS OF LUMINA",
            "content": "For external context utilization, we measure MMD with Eq. (6), which requires summing over the combinations of the entire vocabulary. In practice we approximate it with the top 100 tokens to reduce the computational cost. To obtain pctx, in our experiment we treat the retrieved documents of another data point as the of the target data point. In real-world RAG system, can be obtained by selecting random documents from the data store or retrieving less relevant documents of the query with retrieval model. For internal knowledge utilization, Eq. (10) computes the first information process rate of generating at based on the next token with the highest probability. However, due to the sampling process of generation, the generated token at is not always the highest probability token. Thus, the internal knowledge used during the generation process may not fully apply to at. To take this factor into"
        },
        {
            "title": "Preprint",
            "content": "Table 3: LUMINA consistently achieves balanced precision-recall trade-off and high F1 score across datasets and LLMs. We report the score of PrecOpt, RecallOpt, and F1Opt for LUMINA and baselines on each dataset. LLM Approach PrecOpt RecallOpt F1Opt PrecOpt RecallOpt F1Opt RAGTruth HalluRAG Llama2-7B Llama2-13B Llama3-8B Mistral-7B Perplexity LN-Entropy Focus SelfCKGPT EigenScore P(True) RefChecker ReDeEP LUMINA Perplexity LN-Entropy Focus SelfCKGPT EigenScore P(True) RefChecker ReDeEP LUMINA Perplexity LN-Entropy Focus SelfCKGPT EigenScore P(True) RefChecker ReDeEP LUMINA Perplexity LN-Entropy Focus SelfCKGPT EigenScore P(True) RefChecker ReDeEP LUMINA 0.5080 0.6303 0.5276 0.5125 0.5201 0.5079 0.5022 0.6898 0.7131 0.4926 0.6602 0.4938 0.4801 0.5389 0.6890 0.4600 0.7772 0.7816 0.6369 0.5852 0.5571 0.5657 0.5907 0.5718 0.5400 0.6621 0.6988 0.6187 0.6890 0.7175 0.5914 0.5931 0.7030 0.5578 0.6506 0. 0.9867 0.7920 0.9292 1.0000 0.9735 0.9956 1.0000 0.7478 0.7699 0.9662 0.8164 0.9565 0.9903 0.9034 0.6957 1.0000 0.7246 0.7778 0.8519 0.9465 0.9630 0.9918 0.9383 0.9342 1.0000 0.7901 0.7449 0.9243 0.9040 0.9004 0.9920 0.9522 0.8486 1.0000 0.8640 0.9320 0.6707 0.7020 0.6731 0.6777 0.6780 0.6726 0.6686 0.7176 0.7404 0.6525 0.7300 0.6513 0.6467 0.6751 0.6923 0.6301 0.7500 0. 0.7289 0.7233 0.7059 0.7205 0.7250 0.7094 0.7013 0.7205 0.7211 0.7412 0.7820 0.7986 0.7411 0.7309 0.7690 0.7161 0.7423 0.7728 0.2531 0.7143 0.3077 0.2631 0.4333 0.3065 0.2532 0.4167 0.7826 0.1519 0.5385 0.5556 0.3056 0.5833 0.2449 0.2727 0.4706 1.0000 - - - - - - - - - 0.1702 0.8571 0.7143 0.5385 1.0000 0.3333 0.1266 0.6250 0. 1.0000 0.7500 1.0000 1.0000 0.6500 0.9500 1.0000 0.7500 0.9000 1.0000 0.5833 0.4167 0.9167 0.5833 1.0000 0.2500 0.6667 0.7500 - - - - - - - - - 0.8000 0.6000 0.5000 0.7000 0.5000 0.3000 1.0000 0.5000 0.9000 0.4040 0.7317 0.4706 0.4167 0.5200 0.4634 0.4040 0.5357 0.8372 0.2637 0.5600 0.4762 0.4583 0.5833 0.3934 0.2609 0.5517 0. - - - - - - - - - 0.2807 0.7059 0.5882 0.6087 0.6667 0.3158 0.2247 0.5556 0.9000 account, we calibrate the internal knowledge score by the ratio of probability between the generated token and the highest probability token. In the end, the calibrated internal knowledge score of at is defined as Ipθ (atq, d, a<t) := pθ(atq, d, a<t) pθ(at,1q, d, a<t) R1 pθ (q, d, a<t). (11)"
        },
        {
            "title": "E ADDITIONAL EXPERIMENTAL RESULTS",
            "content": "E.1 EVALUATION WITH OTHER METRICS Table 3 shows the scores of PrecOpt, RecallOpt, and F1Opt on each dataset. The results show that LUMINA consistently has balanced precision-recall trade-off, where the differences between PrecOpt and RecallOpt are smaller than other baselines. Specifically, it achieves (PrecOpt, RecallOpt) = (0.9, 0.9) on HalluRAG with Mistral-7B. This suggests that LUMINA does not over-predict hallucinations to achieve high F1Opt score."
        },
        {
            "title": "Preprint",
            "content": "Table 4: LUMINA achieves competitive performance against supervised approaches. We report the score of AUROC (ROC), Pearsons correlation coefficient (PCC), and AUPRC (PRC) for LUMINA and baselines on each dataset. The highest scores are set in bold. RAGTruth HalluRAG LLM Approach ROC PCC PRC ROC PCC PRC Llama2-7B Llama2-13B Mistral-7B SAPLMA 0.6508 0.7646 LUMINA SAPLMA 0.8337 0.8569 LUMINA SAPLMA 0.8073 LUMINA 0.7685 0.2530 0. 0.5623 0.6041 0.5027 0.4623 0.6446 0.7491 0.8466 0.8436 0.8164 0.7942 0.8813 0. 0.8925 0.9166 0.9667 0.9899 0.6710 0.6554 0.8249 0.6044 0.7920 0.7529 0.8023 0. 0.8647 0.8497 0.9088 0.9431 Figure 6: good performance of LUMINA happens with medium λ value. We alter λ in Eq. (1) to control the weight of internal knowledge score and external context score and evaluate the resulted hallucination detection performance. We conduct the experiment on the RAGTruth dataset and report the AUPRC score. E.2 COMPARE WITH SUPERVISED BASELINES We further compare LUMINA with SAPLMA (Azaria & Mitchell, 2023), supervised approach that trained MLP model over the internal hidden states of the last generated token to classify whether the generation is hallucination or not. Following the original paper, we use hidden states at the 20th layer as input features of SAPLMA. Result in Table 4 shows that LUMINA has competitive performance against SAPLMA and even sometimes outperforms it. Note that Table 4 doesnt show the result of Llama3-8B as the training set doesnt contain responses generated by Llama3-8B. E.3 PERFORMANCE WITH HYPERPARAMETER TUNING We evaluate the hallucination detection performance with λ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9}. Figure 6 shows the AUPRC of different λ on the RAGTruth dataset. The results show that the LUMINA achieves the optimal performance with varies λ across LLMs. For Llama2-13B and Mistral-7B, setting λ = 0.5, i.e., the default setting, is the optimal. While for Llama2-7B and Llama3-8B, the optimal λ is 0.2. However, for these two models, their performance only drops less than 0.025 when setting λ = 0.5, suggesting that weighting internal knowledge and external context utilization equally is still good practice."
        },
        {
            "title": "F ERROR ANALYSIS",
            "content": "To analyze the failure of LUMINA, we sample 20 cases from the RAGTruth dataset that are (1) hallucinated with high-external context and low-internal knowledge scores (i.e., false negative) or (2) non-hallucinated with low-external context and high-internal knowledge scores (i.e., false positive). We qualitatively analyze these cases and categorize them into three groups: (1) Incorrect labels. Sometimes LLMs generate fabricated content that is not sourced from the retrieved document (e.g., detailed menu of restaurant). However, these fabricated contents are sometimes not identified by human annotators. Also, human annotators sometimes misclassify semantically equivalent content as hallucination. In these cases, the provided labels are incorrect, and LUMINA indeed correctly detects hallucination. (2) Generally low hallucination score for the summarization task. We observe that many false negative samples come from the summarization task. In these cases, the LLM does generate content that contradicts the retrieved documents and has relatively high internal knowledge score. However, since most of the generated content is still grounded in the retrieved documents, they usually have high external score as well, resulting in relatively low hallucination score. This observation suggests that different tasks might have different distributions of hallucination scores. better practice is to independently evaluate the hallucination detection performance on each task. (3) Low quality of retrieved documents. For the false positive cases, we observe that many of them are due to the quality issue of the retrieved documents. These documents often contain only irrelevant information or are too vague to concretely answer the query. Thus, the LLM has to reason over them and respond with unable to answer or use its internal knowledge to generate answers with details and examples. This results in relatively high internal knowledge score and low external context score. To address this, future direction can focus on assessing whether the utilization of internal knowledge is necessary and correct, and using that to calibrate the hallucination score."
        },
        {
            "title": "G COMPUTATIONAL RESOURCES",
            "content": "LUMINA is lightweight and efficient approach, which requires only two forward passes to obtain the necessary information to compute external context and internal knowledge scores. As LUMINA does not require generating multiple samples nor training, it is easy to scale up to large amount of data. All the experiments of LUMINA are conducted on single Nvidia H100 GPU. The execution time of computing both external context and internal knowledge scores varies depending on the length of the response. For responses around 150 tokens, the average computational time is less than 1 second."
        }
    ],
    "affiliations": [
        "Argonne National Laboratory",
        "Department of Computer Science, University of Wisconsin-Madison"
    ]
}