{
    "paper_title": "Multi-Objective-Guided Discrete Flow Matching for Controllable Biological Sequence Design",
    "authors": [
        "Tong Chen",
        "Yinuo Zhang",
        "Sophia Tang",
        "Pranam Chatterjee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Designing biological sequences that satisfy multiple, often conflicting, functional and biophysical criteria remains a central challenge in biomolecule engineering. While discrete flow matching models have recently shown promise for efficient sampling in high-dimensional sequence spaces, existing approaches address only single objectives or require continuous embeddings that can distort discrete distributions. We present Multi-Objective-Guided Discrete Flow Matching (MOG-DFM), a general framework to steer any pretrained discrete-time flow matching generator toward Pareto-efficient trade-offs across multiple scalar objectives. At each sampling step, MOG-DFM computes a hybrid rank-directional score for candidate transitions and applies an adaptive hypercone filter to enforce consistent multi-objective progression. We also trained two unconditional discrete flow matching models, PepDFM for diverse peptide generation and EnhancerDFM for functional enhancer DNA generation, as base generation models for MOG-DFM. We demonstrate MOG-DFM's effectiveness in generating peptide binders optimized across five properties (hemolysis, non-fouling, solubility, half-life, and binding affinity), and in designing DNA sequences with specific enhancer classes and DNA shapes. In total, MOG-DFM proves to be a powerful tool for multi-property-guided biomolecule sequence design."
        },
        {
            "title": "Start",
            "content": "Multi-Objective-Guided Discrete Flow Matching for Controllable Biological Sequence Design Tong Chen1,2, Yinuo Zhang1,3, Sophia Tang1,4, Pranam Chatterjee1,5,6, 1Department of Biomedical Engineering, Duke University 2Department of Computer Science, Fudan University 3Center of Computational Biology, Duke-NUS Medical School 4Management and Technology Program, University of Pennsylvania 5Department of Computer Science, Duke University 6Department of Biostatistics and Bioinformatics, Duke University Corresponding author: pranam.chatterjee@duke.edu"
        },
        {
            "title": "Abstract",
            "content": "Designing biological sequences that satisfy multiple, often conflicting, functional and biophysical criteria remains central challenge in biomolecule engineering. While discrete flow matching models have recently shown promise for efficient sampling in high-dimensional sequence spaces, existing approaches address only single objectives or require continuous embeddings that can distort discrete distributions. We present Multi-Objective-Guided Discrete Flow Matching (MOG-DFM), general framework to steer any pretrained discrete-time flow matching generator toward Pareto-efficient trade-offs across multiple scalar objectives. At each sampling step, MOG-DFM computes hybrid rank-directional score for candidate transitions and applies an adaptive hypercone filter to enforce consistent multi-objective progression. We also trained two unconditional discrete flow matching models, PepDFM for diverse peptide generation and EnhancerDFM for functional enhancer DNA generation, as base generation models for MOG-DFM. We demonstrate MOG-DFMs effectiveness in generating peptide binders optimized across five properties (hemolysis, non-fouling, solubility, half-life, and binding affinity), and in designing DNA sequences with specific enhancer classes and DNA shapes. In total, MOG-DFM proves to be powerful tool for multi-property-guided biomolecule sequence design."
        },
        {
            "title": "Introduction",
            "content": "Designing biological sequences that simultaneously satisfy multiple functional and biophysical criteria is foundational challenge in modern bioengineering [1]. For example, when engineering therapeutic proteins, one must balance high target-binding affinity with low immunogenicity and favorable pharmacokinetics [2]; CRISPR guide RNAs require both high on-target activity and minimal off-target effects [3, 4]; and synthetic promoters must achieve strong gene expression while maintaining tissue-specific activation [5]. Most existing biomolecule-design methods focus on optimizing single objective in isolation [6, 7]. For example, efforts have been made to reduce protein toxicity [8, 9] and neural networks are used to improve protein thermostability [10]. While these single-objective approaches yield high performance on their target metrics, they often produce sequences with undesirable trade-offshighaffinity peptides may be insoluble or toxic, and stabilized proteins may lose functional specificity 5 2 0 2 1 1 ] . [ 1 6 8 0 7 0 . 5 0 5 2 : r [11, 12]. Consequently, framework for multi-objective guided generation that can balance conflicting requirements is critical to meet the demands of practical biomolecular engineering. Classical multi-objective optimization (MOO) techniques, such as evolutionary algorithms and Bayesian optimization, have been successfully applied to black-box tuning of molecular libraries [1316]. More recently, controllable generative models have been developed to integrate MOO directly into the sampling process [1719]. ParetoFlow [20], for instance, leverages continuous-space flow matching to produce Pareto-optimal samples, but operates only in continuous domains. Applying such techniques to discrete sequences typically requires embedding into continuous manifold, which can distort distributions and complicate property-based guidance [21, 22]. Recently, we introduced PepTune [23], multi-objective framework based on the masked discrete diffusion language model (MDLM) architecture [24]. PepTune uses Monte Carlo Tree Search (MCTS) strategy to guide the unmasking process toward Pareto-optimal peptide SMILES, enabling optimization across multiple therapeutic properties [23]. However, MDLMs lack coherent notion of token-level velocity, making them less amenable to structured, stepwise control. Discrete flow matching has recently emerged as powerful paradigm for directly modeling and sampling from complex discrete spaces [25, 26]. Two primary variants exist: (i) continuous-time simplex methods, which diffuse discrete data through continuous embedding over the probability simplex [2729], and (ii) jump-process models that learn time-dependent transition rates for tokenlevel stochastic updates [25]. The latter is particularly well suited for controllable generation, as it naturally supports reweighting of token transitions based on scalar reward functions. Recent work has applied these models to single-objective tasks: Nisonoff et al. [30] introduced ratebased classifier guidance for pretrained samplers, while Tang et al. [29] proposed Gumbel-Softmax Flow Matching with straight-through guidance for controllable discrete generation. Yet, to our knowledge, no prior work has extended discrete flow matching to support Pareto-guided generation across multiple objectives. As such, our key contributions are as follows: 1. MOG-DFM: Multi-Objective-Guided Discrete Flow Matching, general framework that steers pretrained discrete flow matching models toward Pareto-efficient solutions via multiobjective guidance and adaptive hypercone filtering. 2. Rank-Directional Scoring and Hypercone Filtering combine rank-normalized local improvement and directional alignment with user-specified trade-off vector to reweight token-level transition velocities, followed by dynamic angular filtering mechanism that enforces directional consistency along the Pareto front. 3. Unconditional Base Models for Biomolecule Generation; we train two high-quality discrete flow matching modelsPepDFM for diverse peptide generation and EnhancerDFM for functional enhancer DNA generationdemonstrating low loss and biological plausibility. 4. Multi-Property Sequence Design; we apply MOG-DFM to two challenging biological generation tasks: (i) therapeutic peptide binder generation with five competing objectives (affinity, solubility, hemolysis, half-life, non-fouling), and (ii) enhancer DNA sequence generation guided by enhancer class and DNA shape. 5. Superior Multi-Objective Optimization; MOG-DFM significantly outperforms classical evolutionary algorithms and flow-based baselines on both peptide and DNA tasks, producing sequences with favorable trade-offs and improved downstream docking, folding, and property scores."
        },
        {
            "title": "2 Discrete Flow Matching",
            "content": "In the discrete setting, we consider data = (x1, . . . , xd) taking values in finite state space = d, where = [K] = {1, 2, . . . , K} is called the vocabulary. We model continuous-time Markov chain (CTMC) {Xt}t[0,1] whose time-dependent transition rates ut(y, x) transport probability mass from an initial distribution p0 to target distribution p1 [25]. The marginal probability at time is denoted pt(x), and its evolution is governed by the Kolmogorov forward equation ut(y, x) pt(x) . (1) dt pt(y) = (cid:88) xS 2 The learnable velocity field ut(y, x) is defined as the sum of factorized velocities: ut(y, x) = (cid:88) δ(yi, xi)ui t(yi, x), (2) where = (1, . . . , 1, + 1, . . . , d) denotes all indices excluding i. The rate conditions for factorized velocities ui t(yi, x) are required per dimension [d]: ut(y, x) 0 for all yi = xi, and (cid:88) yiT t(yi, x) = 0 for all S, ui so that for small > 0 , the one-step kernel pt+ht(y x) = δ(y, x) + ut(y, x) + o(h) (3) (4) remains proper probability mass function. The goal of training discrete flow matching model is to learn the velocity field uθ in terms of factorized velocities uθ,i the marginal velocity uθ matching loss . Representing enables the following conditional flow LCDFM(θ) = Et,Z,XtptZ (cid:88) (cid:16) Di Xt t(, Xt Z), uθ,i ui (, Xt) (cid:17) , (5) where U[0, 1], and ui t(, z), uθ,i ui t(, z), uθ,i (, x) Ωxi where, for α , we define (, x) RT satisfy the rate conditions. This means that Ωα = RT (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) v(β) 0 β {α}, and v(α) = v(β) (cid:88) β=α RT . (6) This is convex set, and Di R. x(u, v) is Bregman divergence defined by convex function Φi : Ωxi In practice, we can further parametrize the velocity field using mixture path. Specifically, one defines mixture path with scheduler κt [0, 1] so that each coordinate (i) 1 with probabilities 1 κt and κt respectively. The mixture marginal velocity is then obtained by averaging the conditional rates over the posterior of (x0, x1) given Xt = x, yielding 0 or x(i) equals x(i) t(yi, x) = ui κt 1 κt (cid:88) xi 1 (cid:2)δ(yi, xi 1) δ(yi, xi)(cid:3) pi 1t(xi 1 x), (7) where κt denotes the time derivative of κt. Therefore, the aim of discrete flow matching model training, which is to learn the velocity field ui t(yi, x), now equals to learning the marginal posterior 1t(xi pi 1 x). In this case, we can set the Bregman divergence to the generalized KL comparing general vectors u, Rm 0, D(u, v) = (cid:104) (cid:88) uj log uj + vj (cid:105) . uj vj For this choice of D, we get (cid:16) t(, xi x0, x1), uθ,i ui (, x) (cid:17) = (cid:104) κt 1 κt (δ(xi 1, xi) 1) log pθ,i 1t(xi 1 x) + δ(xi 1, xi) pθ,i 1t(xi x) (8) (cid:105) (9) which implements the loss (8) when conditioning on = (X0, X1). The generalized KL loss also provides an evidence lower bound (ELBO) on the likelihood of the target distribution (cid:17) (cid:16) log pθ 1(x1) Et,X0,Xtpt0, t(, ui 1 X0, x1), uθ,i (, Xt) , (cid:88) (10) where pθ generalized KL loss can also be used for evaluation. 1 is the marginal generated by the model at time = 1. Therefore, in addition to training, the 3 Figure 1: Visualization for MOG-DFM algorithm."
        },
        {
            "title": "3 Multi-Objective Guided Discrete Flow Matching",
            "content": "MOG-DFM (Multi-Objective Guided Discrete Flow Matching) operates under the same setting as discrete flow matching described in the previous section. Suppose we have pre-trained discrete flow matching model that defines CTMC with factorized velocity field ui t(yi, x) , which transports probability mass from an initial distribution p0 to the unknown target distribution via mixture path parametrization. In addition, we assume access to pre-trained scalar score functions sn : R, where = 1, . . . , , that assign objective scores to any sequence. Our aim is to generate novel sequences x1 whose objective vectors (cid:0)s1(x1), s2(x1), . . . , sN (x1)(cid:1) lie near the Pareto front (not guaranteed to be Pareto optimal) PF = (cid:8)x (cid:12) (cid:12) : sn(x) sn(x) n, : sm(x) > sm(x)(cid:9). To achieve this, we will guide the CTMC sampling dynamics of the discrete flow matching model using multi-objective transition scores, steering the generative process toward Pareto-efficient regions of the state space (Figure 1, Pseudocode 1, Proof in Section D). 3.1 Step 0: Initialization and Weight Vector Generation MOG-DFM begins by initializing the generative process at time = 0 by sampling an initial sequence x0 uniformly from the discrete state space = [K]d. To steer the generation towards diverse Pareto-efficient solutions, we introduce set of weight vectors {ωk}M k=1 that uniformly cover the -dimensional Pareto Front. Intuitively, each ω encodes particular trade-off among the objectives, so sampling different ω promotes exploration of distinct regions of the Pareto front. Concretely, we construct these vectors via the DasDennis simplex lattice with subdivisions, yielding components ωi = ki , ki Z0, (cid:88) i=1 ki = H, (11) and then draw one ω randomly before the following steps. This defines one direction we want to optimize in the Pareto Flow for the current run. The following three steps will then be performed in each iteration. We set the number of total iterations to be . 3.2 Step 1: Guided Transition Scoring We first randomly select one position on the sequence so that we will update the token on this position during the current iteration. At each intermediate state xt and selected position i, each possible candidate transition yi = xi is scored by combining local improvement measures with global directional alignment. The normalized rank score captures how much each individual objective improves relative to other possible token replacements, thereby encouraging exploration of promising local moves; formally, for each objective we compute In(yi, x) = rank(cid:0)sn(xnew) sn(x)(cid:1) , (12) 4 where xnew denotes the sequence obtained by replacing the ith token of with yi. The rank() function maps the raw score change into uniform scale in [0, 1]. In contrast, the directional term D(yi, x, ω) = s(yi, x) ω (13) measures the alignment of the multi-objective improvement vector with the chosen weight vector ω, ensuring that transitions not only improve individual objectives but collectively move toward the desired trade-off direction. By z-score normalizing both components and combining them as S(yi, x, ω) = Norm (cid:104) 1 N (cid:88) n=1 (cid:105) inIn(yi, x) + λ Norm(cid:2)D(yi, x, ω)(cid:3), (14) we balance rank-based exploration against direction-guided exploitation with λ > 0. An importance vector = [i1, . . . , iN ] is used to normalize the improvement values for each objective. Finally, we re-weight the original factorized velocity field from the pre-trained discrete flow matching model: guided,t(yi, ω) = (cid:40) β (cid:80) (yi, x) exp(cid:0)S(yi, x, ω)(cid:1), guided,t(yi, ω), yi=xi yi = xi yi = xi (15) where β is the strength hyperparameter. Therefore, the guided velocities satisfy the non-negativity and zero-sum rate conditions by construction, preserving valid CTMC dynamics while favoring high-utility transitions. 3.3 Step 2: Adaptive Hypercone Filtering To ensure each candidate token replacement drives the sequence towards the chosen trade-off direction, we restrict candidate transitions to lie within cone around the weight vector ω. This hypercone mechanism allows the sampler to navigate non-convex or discontinuous regions of the Pareto front by enforcing local directional consistency. Specifically, for given position and candidate token yi, we compute the angle αi = arccos (cid:18) s(yi, x) ω s(yi, x) ω (cid:19) , (16) where s(yi, x) is the multi-objective improvement vector from replacing xi with yi. We accept only those yi for which αi Φ, where Φ denotes the current hypercone angle. Denoting {xi} as the set of accepted tokens, we select the best transition as S(yi, x, ω) if = . (17) yi best = arg max yiY There are two degenerate cases that may lead to empty i: If every αi π, indicating that all possible transitions decrease performance, we will perform self-transition and retain the current state; if there exist some αi < π but none lie within the cone (i.e., Φ is temporarily too small), we still advance by choosing the best-aligned candidate yi best = arg max S(yi, x, ω), (18) {y:αi<π} allowing progress while the hypercone angle self-adjusts. As pre-defined hypercone angle may be too big or too small during the dynamic optimization process, we need to adaptively tune the angle that best balances exploration and exploitation. Specifically, we compute the rejection rate rt = #{yi : αi > Φ} total # of candidate transitions (19) and its exponential moving average (EMA) (20) where αr [0, 1) is smoothing coefficient and r0 = τ is the target rejection rate. We then update the hypercone angle via rt = αr rth + (cid:0)1 αr (cid:1) rt, Φt+h = clip (cid:16) Φt exp(cid:0)η (rt τ )(cid:1), Φmin, Φmax (cid:17) , (21) with learning rate η > 0 and bounds Φmin, Φmax to prevent the hypercone from collapsing or over-expanding. Intuitively, if too many candidates are being rejected (rt > τ ), the hypercone widens to admit more directions; if too few are rejected (rt < τ ), it narrows to focus on the most aligned transitions. 5 3.4 Step 3: Euler Sampling Once the guided transition rates guided,t(yi, ω) have been computed and the best candidate transition has been selected after hypercone filtering (if not self-transitioning), we evolve the CTMC via Euler sampling. Specifically, we denote the total outgoing rate from at time on coordinate by"
        },
        {
            "title": "R i",
            "content": "t (x) = guided,t(xi, ω) = (cid:88) yi=xi guided,t(yi, ω). (22) The one-step transition kernel for coordinate is given by the exact EulerMaruyama analogue for CTMCs: P(cid:0)X t+h = yi Xt = x(cid:1) = exp(cid:0)h guided,t(yi, ω) (x)"
        },
        {
            "title": "R i",
            "content": "guided,t(xi, ω)(cid:1) = exp(cid:0)h (x)(cid:1), (cid:0)1 exp(h (x))(cid:1), yi = xi, yi = xi. (23) Here, = 1/T is the step size in the time interval, Xt and Xt+h denotes the current state and the next state respectively. In practice, one draws uniform random number [0, 1]: if 1 exp(h (x)), xi will transition to the best selected candidate; otherwise we retain xi. After performing from step 1 to step 3 for iterations, we end with the final sample x1 whose score vectors have been steered close to the Pareto Front, with all objectives optimized."
        },
        {
            "title": "4 Experiments",
            "content": "To the best of our knowledge, there are no public datasets that serve to benchmark multi-objective optimization algorithms for biological sequences. Therefore, we develop two benchmarks to evaluate MOG-DFM: multi-objective guided peptide binder sequence generation and multi-objective guided enhancer DNA sequence generation. We first show two discrete flow matching models developed for peptide generation and enhancer DNA generation, and we then demonstrate MOG-DFMs efficacy on wide variety of tasks and examples. 4.1 PepDFM and EnhancerDFM Generate Diverse and Biologically Plausible Sequences To enable the efficient generation of peptide binders, we developed an unconditional peptide generator, PepDFM, based on the Discrete Flow Matching (DFM) framework. The model backbone of PepDFM is U-Net-style convolutional architecture. We trained PepDFM on custom dataset that includes all peptides from the PepNN and BioLip2 datasets, as well as sequences from the PPIRef dataset with lengths ranging from 6 to 49 amino acids, finally converging to training loss of 3.3134 and validation loss of 3.1051 [3133]. As described in Section 2, the low generalized KL loss during evaluation demonstrates the strong performance of PepDFM. We further investigate the diversity and biological plausibility of peptides generated by PepDFM. Specifically, PepDFM generates peptides with substantially high Hamming distances from the test set, indicating great degree of diversity and novelty in the generated sequences (Figure 3). Additionally, the Shannon entropy of the generated peptides closely matches that of the test set, highlighting the models capability to produce biologically plausible peptides with diverse sequence lengths (Figure 3). EnhancerDFM adopts the same model backbone and melanoma enhancer dataset used in the enhancer DNA design task from Stark, et al. [27]. We employed the Fréchet Biological distance (FBD) metric from [27] to evaluate the performance of EnhancerDFM  (Table 1)  . Specifically, using the same number of function evaluations (NFE), EnhancerDFM achieved comparable FBD of 5.9 compared with Dirichlet FM of 5.3, significantly lower than the FBD of random sequences, demonstrating EnhancerDFMs ability to design biologically plausible enhancer DNA sequences. Significantly, the best EnhancerDFM model is achieved within 20 training epochs, while the best EnhancerDFM is obtained only in around 1400 training epochs, highlighting discrete flow matching models superior capability of capturing the underlying data distribution. 4.2 MOG-DFM effectively balances each objective trade-off To validate that the MOG-DFM framework can balance the trade-offs between each objective, we performed two sets of experiments for peptide binder generation with three property guidance, and 6 Table 1: Evaluation of unconditional EnhancerDNA generation. Each method generates 10k sequences, and we compare their empirical distributions with the data distributions using the Fréchet Biological distance (FBD) metric. NFE refers to number of function evaluations. # Training Epochs refers to the number of training epochs needed to get the model checkpoint for this evaluation. The Random Sequence baseline shows the FBD for the same number and length of sequences with uniform randomly chosen nucleotides. Dirichlet FM refers to the Dirichlet Flow Matching model [27]"
        },
        {
            "title": "Random Sequence\nDirichlet FM\nEnhancerDFM",
            "content": "FBD NFE # Training Epochs 622.8 5.3 5.9 - 1400 20 - 100 100 Table 2: MOG-DFM generates peptide binders for 10 diverse protein targets, optimizing five therapeutic properties: hemolysis, non-fouling, solubility, half-life (in hours), and binding affinity. Each value represents the average of 100 MOG-DFM-designed binders. Target AMHR2 AMHR2 AMHR2 EWS::FLI1 EWS::FLI1 EWS::FLI1 MYC OX1R DUSP12 1B8Q 1E6I 3IDJ 5AZ8 7JVS Binder Length 8 12 16 8 12 16 8 10 9 8 6 7 11 11 Hemolysis () Non-Fouling () Solubility () Half-Life () Affinity () 0.0755 0.0570 0.0618 0.0809 0.0616 0.0709 0.0809 0.0741 0.0735 0.0744 0.0887 0.0924 0.0698 0. 0.8352 0.8419 0.7782 0.8508 0.8302 0.7787 0.8135 0.8115 0.8360 0.8334 0.7884 0.8246 0.8462 0.8390 0.8219 0.8279 0.7428 0.8296 0.8130 0.7400 0.8005 0.7969 0.8216 0.827 0.7793 0.7992 0.8420 0.8206 31.624 28.761 31.227 47.169 34.225 34.192 39.836 33.533 33.754 33.243 41.164 30.388 28.726 32.834 7.3789 7.4274 7.6099 6.2251 6.3631 6.5912 6.8488 7.4162 6.4946 5.932 4.9621 7.6304 6.6051 6.9569 in ablation experiment settings, we removed one or more objectives. In the binder design task for target 7LUL (affinity, solubility, hemolysis guidance; Table 8), omitting any single guidance causes collapse in that property, while the remaining guided metrics may modestly improve. Likewise, in the binder design task for target CLK1 (affinity, non-fouling, half-life guidance; Table 9), disabling non-fouling guidance allows half-life to exceed 80 hours but drives non-fouling near zero, and disabling half-life guidance preserves non-fouling yet reduces half-life below 2 hours. In contrast, enabling all guidance signals produces the most balanced profiles across all objectives. These results confirm that MOG-DFM precisely targets chosen objectives while preserving the flexibility to navigate conflicting requirements and push samples toward the Pareto front, thereby demonstrating the correctness and precision of our multi-objective sampling framework. 4.3 MOG-DFM generates peptide binders under five property guidance We next benchmark MOG-DFM on peptide binder generation task guided by five different properties that are critical for therapeutic discovery: hemolysis, non-fouling, solubility, half-life, and binding affinity. To evaluate MOG-DFM in controlled setting, we designed 100 peptide binders per target for ten diverse proteinsstructured targets with known binders (1B8Q, 1E6I, 3IDJ, 5AZ8, 7JVS), structured targets without known binders (AMHR2, OX1R, DUSP12), and intrinsically disordered targets (EWS::FLI1, MYC)  (Table 2)  . Across all targets and across multiple binder lengths, the generated peptides achieve low hemolysis rates (0.060.09), high non-fouling (>0.78) and solubility (>0.74), extended half-life (2847 h), and strong affinity scores (6.47.6), demonstrating both balanced optimization and robustness to sequence length. 7 Figure 2: (A), (B) Complex structures of PDB 5AZ8 with MOG-DFM-designed binder and its pre-existing binder. (C), (D) Complex structures of two target proteins without pre-existing binders (OX1R, EWS::FLI1) with MOG-DFM-designed binders. Five property scores are shown for each binder, along with the ipTM score from AlphaFold3 and docking score from AutoDock VINA. Interacting residues on the target are visualized. (E) Plots showing the mean scores for each property across the number of iterations during MOG-DFMs design of binders of length 12-aa for EWS::FLI1. (F) Density plots illustrating the distribution of predicted property scores for MOG-DFM-designed EWS::FLI1 binders of length 12 aa, compared to the peptides generated unconditionally by PepDFM. Please zoom in for better viewing. For the target proteins with pre-existing binders, we compared the property values between their known binders with MOG-DFM-designed ones (Figure 2A,B, 4). The designed binders significantly outperform the pre-existing binders across all properties without compromising the binding potential, which is further confirmed by the ipTM scores computed by AlphaFold3 [34] and docking scores calculated by AutoDock VINA [35]. Although the MOG-DFM-designed binders bind to similar target positions as the pre-existing ones, they differ significantly in sequence and structure, demonstrating MOG-DFMs capacity to explore the vast sequence space for optimal designs. For target proteins without known binders, complex structures were visualized using one of the MOG-DFM-designed binders (Figure 2C,D, 5). The corresponding property scores, as well as ipTM and docking scores, are also displayed. Some of the designed binders demonstrated extended half-life, while others excelled in non-fouling and solubility, underscoring the comprehensive exploration of the sequence space by MOG-DFM. At each iteration, we recorded the mean and standard deviation of the five property scores across all the 100 binders to evaluate the effectiveness of the guided generation strategy (Figure 2E). All five properties exhibited an improving trend over iterations, with the average score of the solubility and non-fouling properties showing significant increase from score around 0.3 to 0.8. large deviation of the final half-life values is caused by the susceptibility of the half-life value to guidance, with MOG-DFM balancing the trade-offs between half-life and other values. The improvements of Table 3: MOG-DFM outperforms traditional multi-objective optimization algorithms in designing peptide binders guided by five objectives. Each value represents the average of 100 designed binders. The table also records the average runtime for each algorithm to design single binder. The best result for each metric is highlighted in bold. Target Method Time (s) Hemolysis () Non-Fouling Solubility Half-Life Affinity 1B8Q PPP5 MOPSO NSGA-III SMS-EMOA SPEA2 MOG-DFM MOPSO NSGA-III SMS-EMOA SPEA2 MOG-DFM 8.54 33.13 8.21 17.48 43. 11.34 37.30 8.43 19.02 90.00 0.1066 0.0862 0.1196 0.0819 0.0785 0.0883 0.0479 0.1242 0.0555 0.0617 0.4763 0.5715 0.3450 0.4973 0.8445 0.4711 0.7138 0.4269 0.6221 0.7738 0.4684 0.5825 0.3511 0.5057 0. 0.4255 0.7066 0.4334 0.6098 0.751 4.449 7.324 3.023 4.126 27.227 1.769 2.901 1.031 2.613 27.775 6.0594 7.2178 5.955 7.324 5.9094 6.6958 7.3789 6.2854 7.6253 6.8197 hemolysis, non-fouling, and solubility gradually converge, demonstrating MOG-DFMs efficiency in steering the generation process to the Pareto Front within only 100 iterations. We visualized the distribution change steered by MOG-DFM by plotting the property score distribution of 100 peptides of length 12 designed for EWS::FLI1 and 100 peptides of the same length sampled unconditionally from PepDFM (Figure 2F). MOG-DFM effectively shifted and concentrated the peptide distribution so that the peptides possess improved properties for all the objectives, demonstrating MOG-DFMs ability to steer the generation so that all properties are optimized simultaneously. In Section B, we demonstrate the reliability of our score models. We now use external evaluation tools to further confirm that MOG-DFM-designed binders possess desired properties. The average solubility and half-life for each target across all 100 designed peptides were predicted using ADMETAI  (Table 6)  [36]. ADMET-AI, trained on different dataset from our solubility and half-life prediction models, predicts average LogS values around 2.5 log molL1, which is well above the conventional 4 threshold for good solubility, and confirms long half-life estimates (>15 h). These results from an orthogonal predictive model demonstrate MOG-DFMs capability to generate candidates with multiple desirable drug properties. We benchmarked MOG-DFM against four multi-objective optimizersNSGA-III [37], SMS-EMOA [38], SPEA2 [39], and MOPSO [40]on two protein targets: 1B8Q (a small protein with known peptide binders) and PPP5 (a larger protein lacking characterized binders)  (Table 3)  . For each method, we generated 100 peptide binders per target of specified length, guided by five property objectives (hemolysis, non-fouling, solubility, half-life, and binding affinity), and recorded both the average generation time for one sequence and the mean property scores. Although MOG-DFM requires longer runtimes, it consistently produces the most favorable trade-offs: reducing predicted hemolysis by more than 10%, boosting non-fouling and solubility by approximately 30-50%, and extending half-life by factor of 3 to 4 compared to the next-best method, while maintaining competitive affinity values. These results demonstrate MOG-DFMs effectiveness in navigating high-dimensional property landscapes to generate peptide binders with well-balanced, optimized profiles. We did not benchmark against ParetoFlow, another multi-objective optimization algorithm that uses flow matching, because it requires score models to take continuous inputs, which is not suitable for our task [20]. 4.4 Hyperparameter Sensitivity Benchmark There are several hyperparameters in MOG-DFM whose settings may affect generative performance. To assess this sensitivity, we evaluated peptide binder design across broad range of values for each parameter  (Table 10)  . We find that increasing the number of sampling steps consistently improves all performance metrics, as finer discretization more closely approximates the continuous-time dynamics. In contrast, setting the initial hypercone angle Φinit too small or too large both degrade results: an overly narrow cone restricts exploration, while an overly wide cone dilutes directional guidance. The importance weights also play critical role in balancing multiple objectives. Because each property can vary over different numerical range, we initialize each weight inversely proportional to the maximum observed improvement of that property, thereby normalizing all guidance signals to roughly unit scale. This allows for similar improvements for each objective, otherwise the improvements for some objectives may stagnate. By comparison, the remaining hyperparameters (i.e., β, λ, αr, η, τ , and the bounds Φmin, Φmax) exhibit only modest impact on outcomes, indicating that MOG-DFM is robust to moderate variations in these settings. 4.5 Adaptive Hypercone Filtering Enhances Multi-Objective Optimization To quantify the contribution of our adaptive hypercone mechanism, we performed an ablation study on three increasingly disordered, and thus undruggable, protein targets (3IDJ, 4E-BP2, and EWS::FLI1), generating 100 peptide binders for each target  (Table 7)  . Removing hypercone filtering entirely (w/o filtering) causes dramatic collapse in half-lifefrom roughly 3035 hours down to 413 hourswhile leaving non-fouling and solubility largely unchanged, indicating that filtering out poorly aligned moves is essential for optimizing objectives that require gradual, coordinated changes. Introducing static hypercone gating without angle adaptation (w/o adaptation) recovers much of the half-life gains (to 2337 h), but at the expense of reduced non-fouling and solubility scores and only marginal improvements in affinity. In contrast, the full MOG-DFMwith both directional hypercone filtering and adaptive angle updatessimultaneously elevates half-life and maintains strong performance across all five objectives. This effect is especially pronounced on disordered targets (4E-BP2 and EWS::FLI1), where dynamic cone adjustment is essential for navigating the irregular, non-convex Pareto landscapes. 4.6 MOG-DFM generates enhancer DNA of specific class with specified DNA shapes To demonstrate the universal capability of MOG-DFM in performing multi-objective guided generation for biological sequences, we applied MOG-DFM to design enhancer DNA sequences guided by enhancer class and DNA shape. EnhancerDFM was used as the unconditional enhancer DNA sequence generator, while Deep DNAshape was employed to predict DNA shape [41], and the enhancer class predictor from which it was sourced [27]. Two distinct tasks with different enhancer class and DNA shape guidance were carried out, and ablation results are presented in Table 4. Given the time constraints, we designed five enhancer sequences of length 100 for each setting. In the first task, we conditioned the generation to target enhancer class 1 (associated with the transcription factor binding motif ATF) and high HelT (helix twist) value, with the maximum HelT value set to 36. With both guidance criteria in place, MOG-DFM effectively steered the sequence generation towards enhancer class 1 while simultaneously ensuring that the HelT value approached its maximum  (Table 4)  . When one or both guidance criteria were removed, the corresponding properties showed significant degradation, with the probability of achieving the desired enhancer class dropping near zero  (Table 4)  . similar outcome was observed in the second task, which targeted enhancer class 16 and higher Rise shape value, with the maximum Rise value set to 3.7. Since the canonical range for the Rise shape value spans from 3.3 to 3.4, MOG-DFM ensured both high probability for the target enhancer class and an optimal DNA shape value, outperforming other ablation settings  (Table 4)  ."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we have presented Multi-Objective-Guided Discrete Flow Matching (MOG-DFM), scalable framework for generating biomolecular sequences that simultaneously optimize multiple, often conflicting properties. By guiding discrete flow matching models with multi-objective optimization, MOG-DFM enables the design of peptide and DNA sequences with improved therapeutic and structural characteristics. While MOG-DFM performs well in biological domains, future work will extend the framework to longer sequences and higher-dimensional outputs, including applications in text and image generation. From theoretical perspective, improving Pareto convergence guarantees and incorporating uncertainty-aware or feedback-driven guidance remain key directions to explore. Ultimately, MOG10 Table 4: Performance evaluation of MOG-DFM in guided DNA sequence generation. Task 1 guides the generation towards the HelT shape and enhancer class 1, while Task 2 targets the Rise shape and enhancer class 16. The table presents the predicted DNA shape values (HelT for Task 1, Rise for Task 2) and enhancer class probabilities (class 1 for Task 1, class 16 for Task 2) under various guidance conditions. The Shape column shows the predicted DNA shape values obtained using Deep DNAshape, and the Class Prob column displays the predicted enhancer class probabilities. Ablation studies were conducted by removing one or both guidance criteria, as shown by the rows corresponding to different combinations of shape and class guidance. For each setting, 5 enhancer DNA sequences were designed. Guidance Settings Shape Class Task 1 Task 2 Class Prob 0.7504 0.6507 0.6821 0.7097 0.6425 0.9999 0.9999 0.9989 0.9997 0.9998 0.0026 0.0055 0.0062 0.0186 0.0051 0.0362 0.0364 0.0309 0.0138 0.0213 Shape 36.0100 36.0100 36.0000 36.0000 36.0000 34.3274 34.4715 34.4257 34.5226 34.4210 36.0017 36.0238 36.0214 36.0396 36.0304 34.7379 34.5350 34.5720 34.3060 34. Class Prob 0.9960 0.9922 0.9864 0.9976 0.9961 1.0000 1.0000 0.9999 0.9994 1.0000 2.36E-05 0.0005 0.0114 0.0001 0.0054 0.0008 0.0057 0.0476 0.0632 0.0003 Shape 3.3640 3.3680 3.3669 3.3680 3.3623 3.3368 3.3345 3.3348 3.3357 3.3340 3.3690 3.3647 3.3705 3.3717 3.3669 3.3283 3.3258 3.3268 3.3378 3.3320 DFM offers foundation for generating the next generation of therapeuticsmolecules that are not only effective but explicitly optimized for the multifaceted properties critical to clinical success."
        },
        {
            "title": "6 Declarations",
            "content": "Acknowledgments. We thank the Duke Compute Cluster, Pratt School of Engineering IT department, and Mark III Systems, for providing database and hardware support that has contributed to the research reported within this manuscript. Author Contributions. T.C. devised model architectures and theoretical formulations, and trained and benchmarked models. Y.Z. trained and benchmarked supervised models, and performed molecular docking. Y.Z. and S.T. advised on model design and theoretical framework. T.C. drafted the manuscript, and Y.Z. and S.T. assisted in figure design and data presentation. P.C. designed, supervised, and directed the study, formulated algorithm proofs, and finalized the manuscript. Data and Materials Availability. The codebase will be freely accessible to the academic community at https://huggingface.co/ChatterjeeLab/MOG-DFM. Funding Statement. This research was supported by grants from the Hartwell Foundation, CHDI Foundation, and EndAxD Foundation to the lab of P.C. 11 Competing Interests. P.C. is co-founder of Gameto, Inc. and UbiquiTx, Inc. and advises companies involved in biologics development. P.C.s interests are reviewed and managed by Duke University in accordance with their conflict-of-interest policies. T.C., S.T., and Y.Z., have no conflicts of interest to declare."
        },
        {
            "title": "References",
            "content": "[1] Gita Naseri and Mattheos AG Koffas. Application of combinatorial optimization strategies in synthetic biology. Nature communications, 11(1):2446, 2020. [2] Masahiro Tominaga, Yoko Shima, Kenta Nozaki, Yoichiro Ito, Masataka Someda, Yuji Shoya, Noritaka Hashii, Chihiro Obata, Miho Matsumoto-Kitano, Kohei Suematsu, et al. Designing strong inducible synthetic promoters in yeasts. Nature Communications, 15(1):10653, 2024. [3] Stephanie Mohr, Yanhui Hu, Benjamin Ewen-Campen, Benjamin Housden, Raghuvir Viswanatha, and Norbert Perrimon. Crispr guide rna design for research applications. The FEBS journal, 283(17):32323238, 2016. [4] Henri Schmidt, Minsi Zhang, Dimitar Chakarov, Vineet Bansal, Haralambos Mourelatos, Francisco Sánchez-Rivera, Scott Lowe, Andrea Ventura, Christina Leslie, and Yuri Pritykin. Genome-wide crispr guide rna design and specificity analysis with guidescan2. Genome biology, 26(1):125, 2025. [5] Valentin Artemyev, Anna Gubaeva, Anastasiia Iu Paremskaia, Amina Dzhioeva, Andrei Deviatkin, Sofya Feoktistova, Olga Mityaeva, and Pavel Yu Volchkov. Synthetic promoters in gene therapy: Design approaches, features and applications. Cells, 13(23):1963, 2024. [6] Ruimin Zhou, Zhaoyan Jiang, Chen Yang, Jianwei Yu, Jirui Feng, Muhammad Abdullah Adil, Dan Deng, Wenjun Zou, Jianqi Zhang, Kun Lu, et al. All-small-molecule organic solar cells with over 14% efficiency by optimizing hierarchical morphologies. Nature communications, 10 (1):5393, 2019. [7] Atef Nehdi, Nosaibah Samman, Vanessa Aguilar-Sánchez, Azer Farah, Emre Yurdusev, Mohamed Boudjelal, and Jonathan Perreault. Novel strategies to optimize the amplification of single-stranded dna. Frontiers in Bioengineering and Biotechnology, 8:401, 2020. [8] Ryan Kreiser, Aidan Wright, Natalie Block, Jared Hollows, Lam Nguyen, Kathleen LeForte, Benedetta Mannini, Michele Vendruscolo, and Ryan Limbocker. Therapeutic strategies International journal of molecular to reduce the toxicity of misfolded protein oligomers. sciences, 21(22):8651, 2020. [9] Neelam Sharma, Leimarembi Devi Naorem, Shipra Jain, and Gajendra PS Raghava. Toxinpred2: an improved method for predicting toxicity of proteins. Briefings in bioinformatics, 23(5): bbac174, 2022. [10] Evan Komp, Christian Phillips, Lauren Lee, Shayna Fallin, Humood Alanzi, Marlo Zorman, Michelle McCully, and David AC Beck. Neural network conditioned to produce thermophilic protein sequences can increase thermal stability. Scientific Reports, 15(1):14124, 2025. [11] Alessandra Bigi, Eva Lombardo, Roberta Cascella, and Cristina Cecchi. The toxicity of protein aggregates: new insights into the mechanisms, 2023. [12] Dillon Rinauro, Fabrizio Chiti, Michele Vendruscolo, and Ryan Limbocker. Misfolded protein oligomers: Mechanisms of formation, cytotoxic effects, and pharmacological approaches against protein misfolding diseases. Molecular Neurodegeneration, 19(1):20, 2024. [13] Eckart Zitzler and Lothar Thiele. Multiobjective optimization using evolutionary algorithmsa comparative case study. In International conference on parallel problem solving from nature, pages 292301. Springer, 1998. 12 [14] Kalyanmoy Deb. Multi-objective optimisation using evolutionary algorithms: an introduction. In Multi-objective evolutionary optimisation for product design and manufacturing, pages 334. Springer, 2011. [15] Tsuyoshi Ueno, Trevor David Rhone, Zhufeng Hou, Teruyasu Mizoguchi, and Koji Tsuda. Combo: An efficient bayesian optimization library for materials science. Materials discovery, 4:1821, 2016. [16] Trevor Frisby and Christopher James Langmead. Bayesian optimization with evolutionary and structure-based regularization for directed protein evolution. Algorithms for Molecular Biology, 16(1):13, 2021. [17] Yibo Li, Liangren Zhang, and Zhenming Liu. Multi-objective de novo drug design with conditional graph generative model. Journal of cheminformatics, 10:124, 2018. [18] Tiago Sousa, João Correia, Vitor Pereira, and Miguel Rocha. Combining multi-objective evolutionary algorithms with deep generative models towards focused molecular design. In Applications of Evolutionary Computation: 24th International Conference, EvoApplications 2021, Held as Part of EvoStar 2021, Virtual Event, April 79, 2021, Proceedings 24, pages 8196. Springer, 2021. [19] Yinghua Yao, Yuangang Pan, Jing Li, Ivor Tsang, and Xin Yao. Proud: Pareto-guided diffusion model for multi-objective generation. Machine Learning, 113(9):65116538, 2024. [20] Ye Yuan, Can Chen, Christopher Pal, and Xue Liu. Paretoflow: Guided flows in multi-objective optimization. arXiv preprint arXiv:2412.03718, 2024. [21] Gleb Beliakov and Kieran Lim. Challenges of continuous global optimization in molecular structure prediction. European journal of operational research, 181(3):11981213, 2007. [22] Richard Michael, Simon Bartels, Miguel González-Duque, Yevgen Zainchkovskyy, Jes Frellsen, Søren Hauberg, and Wouter Boomsma. continuous relaxation for discrete bayesian optimization. arXiv preprint arXiv:2404.17452, 2024. [23] Sophia Tang, Yinuo Zhang, and Pranam Chatterjee. Peptune: De novo generation of therapeutic peptides with multi-objective-guided discrete diffusion. Proceedings of the 41st International Conference on Machine Learning (ICML), 2025. [24] Subham Sekhar Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. Advances in Neural Information Processing Systems, 2024. [25] Itai Gat, Tal Remez, Neta Shaul, Felix Kreuk, Ricky TQ Chen, Gabriel Synnaeve, Yossi Adi, and Yaron Lipman. Discrete flow matching. Advances in Neural Information Processing Systems, 37:133345133385, 2024. [26] Ian Dunn and David Ryan Koes. Exploring discrete flow matching for 3d de novo molecule generation. ArXiv, pages arXiv2411, 2024. [27] Hannes Stark, Bowen Jing, Chenyu Wang, Gabriele Corso, Bonnie Berger, Regina Barzilay, and Tommi Jaakkola. Dirichlet flow matching with applications to dna sequence design. Proceedings of the 41st International Conference on Machine Learning (ICML), 2024. [28] Oscar Davis, Samuel Kessler, Mircea Petrache, Ismail Ceylan, Michael Bronstein, and Joey Bose. Fisher flow matching for generative modeling over discrete data. Advances in Neural Information Processing Systems, 37:139054139084, 2024. [29] Sophia Tang, Yinuo Zhang, and Pranam Chatterjee. Gumbel-softmax flow matching with straight-through guidance for controllable biological sequence generation. arXiv preprint arXiv:2503.17361, 2025. [30] Hunter Nisonoff, Junhao Xiong, Stephan Allenspach, and Jennifer Listgarten. Unlocking guidance for discrete state-space diffusion and flow models. Proceedings of the 13th International Conference on Learning Representations (ICLR), 2025. 13 [31] Osama Abdin, Satra Nim, Han Wen, and Philip Kim. Pepnn: deep attention model for the identification of peptide binding sites. Communications biology, 5(1):503, 2022. [32] Chengxin Zhang, Xi Zhang, Peter Freddolino, and Yang Zhang. Biolip2: an updated structure database for biologically relevant ligandprotein interactions. Nucleic Acids Research, 52(D1): D404D412, 2024. [33] Anton Bushuiev, Roman Bushuiev, Petr Kouba, Anatolii Filkin, Marketa Gabrielova, Michal Gabriel, Jiri Sedlar, Tomas Pluskal, Jiri Damborsky, Stanislav Mazurenko, et al. Learning to design protein-protein interactions with enhanced generalization. arXiv preprint arXiv:2310.18515, 2023. [34] Josh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, Lindsay Willmore, Andrew Ballard, Joshua Bambrick, et al. Accurate structure prediction of biomolecular interactions with alphafold 3. Nature, 630(8016):493500, 2024. [35] Oleg Trott and Arthur Olson. Autodock vina: improving the speed and accuracy of docking with new scoring function, efficient optimization, and multithreading. Journal of computational chemistry, 31(2):455461, 2010. [36] Kyle Swanson, Parker Walther, Jeremy Leitz, Souhrid Mukherjee, Joseph Wu, Rabindra Shivnaraine, and James Zou. Admet-ai: machine learning admet platform for evaluation of large-scale chemical libraries. Bioinformatics, 40(7):btae416, 2024. [37] Kalyanmoy Deb and Himanshu Jain. An evolutionary many-objective optimization algorithm using reference-point-based nondominated sorting approach, part i: solving problems with box constraints. IEEE transactions on evolutionary computation, 18(4):577601, 2013. [38] Nicola Beume, Boris Naujoks, and Michael Emmerich. Sms-emoa: Multiobjective selection based on dominated hypervolume. European journal of operational research, 181(3):16531669, 2007. [39] Eckart Zitzler, Marco Laumanns, and Lothar Thiele. Spea2: Improving the strength pareto evolutionary algorithm. TIK report, 103, 2001. [40] CA Coello Coello and Maximino Salazar Lechuga. Mopso: proposal for multiple objective particle swarm optimization. In Proceedings of the 2002 Congress on Evolutionary Computation. CEC02 (Cat. No. 02TH8600), volume 2, pages 10511056. IEEE, 2002. [41] Jinsen Li, Tsu-Pei Chiu, and Remo Rohs. Predicting dna structure using deep learning method. Nature communications, 15(1):1243, 2024. [42] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks In Medical image computing and computer-assisted for biomedical image segmentation. interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. [43] Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, et al. Evolutionary-scale prediction of atomic-level protein structure with language model. Science, 379(6637):11231130, 2023. [44] Zeynep Kalender Atak, Ibrahim Ihsan Taskiran, Jonas Demeulemeester, Christopher Flerin, David Mauduit, Liesbeth Minnoye, Gert Hulselmans, Valerie Christiaens, Ghanem-Elias Ghanem, Jasper Wouters, et al. Interpretation of allele-specific chromatin accessibility using cell stateaware deep learning. Genome research, 31(6):10821096, 2021. [45] Jason Buenrostro, Paul Giresi, Lisa Zaba, Howard Chang, and William Greenleaf. Transposition of native chromatin for fast and sensitive epigenomic profiling of open chromatin, dna-binding proteins and nucleosome position. Nature methods, 10(12):12131218, 2013. [46] Ruochi Zhang, Haoran Wu, Yuting Xiu, Kewei Li, Ningning Chen, Yu Wang, Yan Wang, Xin Gao, and Fengfeng Zhou. Pepland: large-scale pre-trained peptide representation model for comprehensive landscape of both canonical and non-canonical amino acids. arXiv preprint arXiv:2311.04419, 2023. 14 [47] Chakradhar Guntuboina, Adrita Das, Parisa Mollaei, Seongwon Kim, and Amir Barati Farimani. Peptidebert: language model based on transformers for peptide property prediction. The Journal of Physical Chemistry Letters, 14(46):1042710434, 2023. [48] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:28252830, 2011. [49] Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: next-generation hyperparameter optimization framework. In International Conference on Knowledge Discovery and Data Mining, pages 26232631, 2019. [50] Deepika Mathur, Satya Prakash, Priya Anand, Harpreet Kaur, Piyush Agrawal, Ayesha Mehta, Rajesh Kumar, Sandeep Singh, and Gajendra PS Raghava. Peplife: repository of the half-life of peptides. Scientific reports, 6(1):36617, 2016. [51] Vera DAloisio, Paolo Dognini, Gillian Hutcheon, and Christopher Coxon. Peptherdia: database and structural composition analysis of approved peptide therapeutics and diagnostics. Drug Discovery Today, 26(6):14091419, 2021. [52] Shipra Jain, Srijanee Gupta, Sumeet Patiyal, and Gajendra PS Raghava. Thpdb2: compilation of fda approved therapeutic peptides and proteins. Drug Discovery Today, page 104047, 2024. [53] Kotaro Tsuboyama, Justas Dauparas, Jonathan Chen, Elodie Laine, Yasser Mohseni Behbahani, Jonathan Weinstein, Niall Mangan, Sergey Ovchinnikov, and Gabriel Rocklin. Mega-scale experimental analysis of protein folding stability in biology and design. Nature, 620(7973): 434444, 2023."
        },
        {
            "title": "A Base Model Details",
            "content": "A.1 PepDFM Model Architecture. The base model is time-dependent architecture based on U-Net [42]. It uses two separate embedding layers for sequence and time, followed by five convolutional blocks with varying dilation rates to capture temporal dependencies, while incorporating time-conditioning through dense layers. The final output layer generates logits for each token. We used polynomial convex schedule with polynomial exponent of 2.0 for the mixture discrete probability path in the discrete flow matching. Dataset Curation. The dataset for PepDFM training was curated from the PepNN, BioLip2, and PPIRef dataset [3133]. All peptides from PepNN and BioLip2 were included, along with sequences from PPIRef ranging from 6 to 49 amino acids in length. The dataset was divided into training, validation, and test sets at an 80/10/10 ratio. Training Strategy. The training is conducted on 2xH100 NVIDIA NVL GPU system with 94 GB of VRAM for 200 epochs with batch size 512. The model checkpoint with the lowest evaluation loss was saved. The Adam optimizer was employed with learning rate 1e-4. learning rate scheduler with 20 warm-up epochs and cosine decay was used, with initial and minimum learning rates both 1e-5. The embedding dimension and hidden dimension were set to be 512 and 256 respectively for the base model. Dynamic Batching. To enhance computational efficiency and manage variable-length token sequences, we implemented dynamic batching. Drawing inspiration from ESM-2s approach [43], input peptide sequences were sorted by length to optimize GPU memory utilization, with maximum token size of 100 per GPU. A.2 EnhancerDFM Model Architecture. The base model for EnhancerDFM applies the same architecture as the PepDFM. We also used polynomial convex schedule with polynomial exponent of 2.0 for the mixture discrete probability path in the discrete flow matching. Dataset Curation. The dataset for EnhancerDFM training is curated by [27]. The dataset contains 89k enhancer sequences from human melanoma cells [44]. Each sequence is of length 500 paired with cell class labels determined from ATAC-seq data [45]. There are 47 such classes of cells in total, with details displayed in Table 11 [44]. We applied the same dataset split strategy as [27]. Training Strategy. The training is conducted on 2xH100 NVIDIA NVL GPU system with 94 GB of VRAM for 1500 epochs with batch size 256. The model checkpoint with the lowest evaluation loss was saved. The Adam optimizer was employed with learning rate 1e-3. learning rate scheduler with 150 warm-up epochs and cosine decay was used, with initial and minimum learning rates both 1e-4. Both the embedding dimension and hidden dimension were set to be 256 for the base model."
        },
        {
            "title": "B Score Model Details",
            "content": "We collected hemolysis (9,316), non-fouling (17,185), solubility (18,453), and binding affinity (1,781) data for classifier training from the PepLand and PeptideBERT datasets [46, 47]. All sequences taken are wild-type L-amino acids and are tokenized and represented by ESM-2 protein language model [43]. B.1 Boosted Trees for Classification For hemolysis, non-fouling, and solubility classification, we trained XGBoost boosted tree models for logistic regression. We split the data into 0.8/0.2 train/validation using traitified splits from scikit-learn [48] and generated mean pooled ESM-2-650M [43] embeddings as input features to the model. We ran 50 trials of OPTUNA [49] search to determine the optimal XGBoost hyperparameters  (Table 5)  tracking the best binary classification F1 scores. The best models for each property reached F1 scores of: 0.58, 0.71, and 0.68 on the validation sets accordingly. 16 Table 5: XGBoost Hyperparameters for Classification Hyperparameter Value/Range"
        },
        {
            "title": "Objective\nLambda\nAlpha\nColsample by Tree\nSubsample\nLearning Rate\nMax Depth\nMin Child Weight\nTree Method",
            "content": "binary:logistic [1e8, 10.0] [1e8, 10.0] [0.1, 1.0] [0.1, 1.0] [0.01, 0.3] [2, 30] [1, 20] hist B.2 Binding Affinity Score Model We developed an unpooled reciprocal attention transformer model to predict protein-peptide binding affinity, leveraging latent representations from the ESM-2 650M protein language model [43]. Instead of relying on pooled representations, the model retains unpooled token-level embeddings from ESM2, which are passed through convolutional layers followed by cross-attention layers. The binding affinity data was split into 0.8/0.2 ratio, maintaining similar affinity score distributions across splits. We used OPTUNA [49] for hyperparameter optimization tracing validation correlation scores. The final model was trained for 50 epochs with learning rate of 3.84e-5, dropout rate of 0.15, 3 initial CNN kernel layers (dimension 384), 4 cross-attention layers (dimension 2048), and shared prediction head (dimension 1024) in the end. The classifier reached 0.64 Spearmans correlation score on validation data. B.3 Half-Life Score Model Dataset Curation. The half-life dataset is curated from three publicly available datasets: PEPLife, PepTherDia, and THPdb2 [5052]. Data related to human subjects were selected, and entries with missing half-life values were excluded. After removing duplicates, the final dataset consists of 105 entries. Pre-training on stability data. Given the small size of the half-life dataset, which is insufficient for training model to capture the underlying data distribution, we first pre-trained score model on larger stability dataset to predict peptide stability [53]. The model consists of three linear layers with ReLU activation functions, and dropout rate of 0.3 was applied. The model was trained on 2xH100 NVIDIA NVL GPU system with 94 GB of VRAM for 50 epochs. The Adam optimizer was employed with learning rate 1e-2. learning rate scheduler with 5 warm-up epochs and cosine decay was used, with initial and minimum learning rates both 1e-3. After training, the model achieved validation Spearmans correlation of 0.7915 and an R2 value of 0.6864, demonstrating the reliability of the stability score model. Fine-tuning on half-life data. The pre-trained stability score model was subsequently fine-tuned on the half-life dataset. Since half-life values span wide range, the model was adapted to predict the base-10 logarithm of the half-life (h) values to stabilize the learning process. After fine-tuning, the model achieved validation Spearmans correlation of 0.8581 and an R2 value of 0.5977."
        },
        {
            "title": "C Sampling Details",
            "content": "C.1 Peptide Binder Generation Tasks Score Model Settings. To align all objectives as maximization, we convert the predicted hemolysis rate into score 1 h, so that lower hemolysis yields higher value. We also cap the predicted log-scale half-life at 2 (i.e., 100 hours) to prevent it from dominating the optimization and ensure 17 balanced trade-offs across all properties. For the remaining objectivesnon-fouling, solubility, and binding affinitywe directly employ their model outputs during sampling. Hyperparameter Settings. The hyperparameters were set as follows: The number of divisions used in generating weight vectors, num_div, was set to 64, λ to 1.0, β to 1.0, αr to 0.5, τ to 0.3, η to 1.0, Φinit to 45, Φmin to 15, Φmax to 75. The total sampling step was 100. Importance Vectors. In the task with five property guidance, the importance vector was set to [1, 1, 1, 0.5, 0.2], each corresponding to hemolysis, non-fouling, solubility, half-life, and binding affinity guidance, respectively. For the two tasks with only three property guidance, the importance vector was set to [1, 1, 0.1] for solubility, hemolysis, and binding affinity guidance, respectively, and [1, 0.5, 0.2] for non-fouling, half-life, and binding affinity guidance, respectively. The rationale for setting the importance values is based on the range lengths of the properties: hemolysis, non-fouling, and solubility each have range length of 1.0, half-life has range length of 2.0, and binding affinity has range length of 10.0. The importance values were assigned inversely proportional to these range lengths. C.2 Enhancer DNA Generation Tasks Hyperparameter Settings. The hyperparameters were set the same as those in peptide binder generation tasks, except that the total sampling step was set to 800. Importance Vectors. The importance vector was set to be [1, 10] for the first task and [1, 100] for the second task, with the first value corresponding to the enhancer class guidance and the second value corresponding to the DNA shape guidance. The rationale for assigning these importance values is based on the range lengths of the properties: enhancer class probability has range length of 1.0, HelT shape values have range length of 2.0, and Rise shape values have range length of 0.1. The importance values were assigned inversely proportional to these range lengths."
        },
        {
            "title": "D Additional Proof",
            "content": "Claim: MOG-DFM directs the discrete generation process toward the Pareto front by inducing positive expected improvement in the direction of specified weight vector ω RN . Proof: Let = be the discrete sequence space over vocabulary , and let denote the current sequence state at time [0, 1]. Assume the multi-objective score function : RN is measurable, with scalar objectives. Define the improvement vector at candidate transition yi {xi} at position {1, . . . , d} as: s(yi, x) := s(x(iyi)) s(x), where x(iyi) denotes the sequence with token xi replaced by yi. Let ω RN be fixed unit-norm trade-off vector sampled uniformly from the DasDennis lattice covering the simplex 1. Define the directional improvement of transition yi as: D(yi, x; ω) := s(yi, x) ω. Define the set of feasible transitions (those within the hypercone of angle Φ (0, π)) at time as: i(x, ω, Φ) := (cid:26) yi {xi} (cid:12) (cid:12) (cid:12) (cid:12) arccos (cid:18) s(yi, x) ω (cid:19) s(yi, x) ω (cid:27) Φ . Let µi t( x, ω) be the conditional probability measure over feasible transitions defined by: exp (cid:0)S(yi, x, ω)(cid:1) Z(x, ω) t(yi x, ω) := µi 1{yiY i(x,ω,Φ)}, yiY exp (cid:0)S(yi, x, ω)(cid:1) is where S() is the rank-directional guidance score and Z(x, ω) := (cid:80) the normalizing partition function. Assume that i(x, ω, Φ) is non-empty, or else the algorithm falls back to selecting the best yi with D(yi, x; ω) > 0 by construction. 18 We now consider the expected improvement in the direction of ω over all guided transitions: iU [d], yiµi t(x,ω) (cid:2)D(yi, x; ω)(cid:3) = 1 (cid:88) (cid:88) i=1 yiY i(x,ω,Φ) D(yi, x; ω) µi t(yi x, ω). Since each yi i(x, ω, Φ) satisfies arccos D(yi, x; ω) > 0 for all yi i. Moreover, µi (cid:16) s(yi,x)ω (cid:17) s(yi,x)ω Φ < π, it follows that t(yi x, ω) > 0 by construction. Therefore, each term in the sum is strictly positive, and thus: E[s(xnew, x) ω] > 0, where xnew = x(iyi) is the updated sequence following guided and filtered transition. Hence, the MOG-DFM procedure ensures that in expectation, the sampling dynamics induce forward motion along the Pareto trade-off direction ω, thereby steering generation toward the Pareto frontier. 19 Table 6: Average solubility (LogS) and half-life (in hours) metrics computed by ADMET-AI for each target across the 100 MOG-DFM-designed binders. Target AMHR2 AMHR2 AMHR2 EWS::FLI1 EWS::FLI1 EWS::FLI1 MYC OX1R DUSP12 1B8Q 1E6I 3IDJ 5AZ8 7JVS LogS -2.3931 -2.5055 -2.5784 -2.3869 -2.3813 -2.5457 -2.4053 -2.4772 -2.4333 -2.3203 -2.0394 -2.4193 -2.5964 -2.4824 Half-Life 15.505 18.777 16.463 18.945 16.305 15.984 16.491 23.002 19.258 18.7862 19.9358 20.3586 16.3016 20.2565 Table 7: Ablation study results for the adaptive hypercone filtering module in MOG-DFM. Three settings are evaluated: w/o filtering indicates the module is completely disabled, w/o adaptation means the module is enabled but the hypercone is not adaptive, and MOG-DFM represents the complete algorithm. For each setting, 100 peptide binders were designed, with lengths of 7, 12, and 12 for the targets 3IDJ, 4E-BP2, and EWS::FLI1, respectively. Target Method Hemolysis () Non-Fouling Solubility Half-Life Affinity 3IDJ 4E-BP2 EWS::FLI1 w/o filtering w/o adaptation MOG-DFM w/o filtering w/o adaptation MOG-DFM w/o filtering w/o adaptation MOG-DFM 0.0660 0.0856 0.0924 0.0504 0.0638 0.0698 0.0450 0.0620 0.0616 0.8430 0.8060 0.8246 0.8582 0.8418 0. 0.8596 0.8444 0.8302 0.8482 0.7970 0.7992 0.8600 0.8234 0.8050 0.8570 0.8482 0.8130 12.50 37.17 30.39 12.62 23.44 34. 4.40 28.82 34.225 7.3730 7.3142 7.6304 6.5066 6.4548 6.5824 6.1392 6.2118 6.3631 20 Table 8: Ablation results for peptide binder design targeting PDB 7LUL with different guidance settings. For each setting, 100 binders of length 7 were designed. Guidance Settings Affinity Solubility Hemolysis Affinity Solubility Hemolysis () 6.3489 5. 6.9060 6.5304 5.0761 5.2434 7.4834 5. 0.8890 0.9482 0.4224 0.8975 0.7148 0. 0.1218 0.3736 0.0620 0.0406 0.0488 0. 0.0163 0.0955 0.3281 0.1567 Table 9: Ablation results for peptide binder design targeting PDB CLK1 with different guidance settings. For each setting, 100 binders of length 12 were designed. Guidance Settings Affinity Non-Fouling Half-Life Affinity Non-Fouling Half-Life 6. 6.4735 7.5360 7.4150 6.2363 6.1378 8. 5.8926 0.7401 0.8107 0.3062 0.8560 0. 0.9503 0.2439 0.3999 51.73 60.75 84. 1.24 96.44 0.94 3.15 1.94 Table 10: Hyperparameter sensitivity benchmark for MOG-DFM in peptide binder generation, guided by five objectives. For each setting, 100 peptide binders are designed with length matching that of the pre-existing binder for each target. Hyper parameter Target Value Hemolysis () Non-Fouling Solubility Half-Life Affinity 0.8088 0.8280 0.8438 0.7894 0.8388 0.8588 0.8461 0.8168 0.8362 0.8690 0.8361 0.8441 0.8529 0.8403 0. 0.8437 0.8256 0.8125 0.8285 0.8393 0.8338 0.8095 0.8139 0.8385 0.8373 0.8159 0.8252 0.8017 0.8224 0.8310 0.8360 0.7386 0.8617 0.8695 0.8799 0.5735 0.6003 0.5549 0.5939 0. 0.7924 0.8232 0.8386 0.761 0.8321 0.8582 0.8416 0.8152 0.8207 0.8461 0.8051 0.8280 0.8421 0.8377 0.8091 0.8368 0.8144 0.7887 0.8007 0.8187 0.8192 0.7970 0. 0.8200 0.8116 0.8020 0.8119 0.7835 0.8088 0.8043 0.8078 0.7219 0.8504 0.8621 0.8760 0.5485 0.5738 0.5272 0.5647 0.5007 38.39 34.91 32.97 28.10 41.78 47.65 53. 30.89 33.28 41.90 37.83 38.83 31.45 35.50 45.25 29.48 24.47 35.13 34.04 35.60 36.29 38.25 33.29 26.64 29.56 35.71 24.57 31.19 28.72 24.03 28. 15.22 30.25 41.53 57.65 28.17 21.99 33.58 34.80 29.65 6.5436 6.3260 6.4197 6.7884 7.0002 7.0505 7.0169 6.4838 6.4549 6.5317 6.0569 6.0484 6.0445 6.0839 6. 7.3657 7.3111 7.1974 7.0335 7.0251 7.0944 7.0932 7.1261 8.2201 8.1673 8.2313 7.0112 7.1067 7.0756 7.0862 7.0477 6.9155 6.9946 7.2166 7.2172 6.4190 6.3409 6.3844 6.4281 6. num_div 6MLC β λ 4IU7 1AYC αr 2Q8Y η 2LTV Φinit 5M [Φmin, Φmax] 3EQS τ 5E1C 5KRI 32 64 128 0.5 1 1.5 2 0.5 1 2 0.1 0.3 0.5 0.7 0.9 0.5 1 2 15 30 45 60 [0,90] [15,75] [30,60] 0 0.1 0.3 0.5 0.7 50 100 200 500 importance weights 4EZN [1,1,1,0.5,0.2] [1,1,1,0.5,0.1] [1,1,1,1,0.1] [1,1,1,1,0.2] [1,1,1,1,1] 0.0994 0.0863 0.0890 0.0829 0.0684 0.0585 0.0615 0.0703 0.0647 0.0587 0.0777 0.0718 0.0718 0.0688 0.0813 0.0633 0.0601 0.0624 0.0746 0.0792 0.0747 0.0813 0. 0.0572 0.0599 0.0614 0.0614 0.0650 0.0595 0.0555 0.0590 0.0757 0.0580 0.0525 0.0518 0.0877 0.0836 0.0892 0.0958 0.0960 22 Table 11: Motif clusters and associated properties of enhancer DNA sequences. In this paper, each class refers to its corresponding cluster ID. Cluster ID # of explainable ASCAVs Motif Annotation cluster_1 cluster_2 cluster_3 cluster_4 cluster_5 cluster_6 cluster_7 cluster_8 cluster_9 cluster_10 cluster_11 cluster_12 cluster_13 cluster_14 cluster_15 cluster_16 cluster_17 cluster_18 cluster_19 cluster_20 cluster_21 cluster_22 cluster_23 cluster_24 cluster_25 cluster_26 cluster_27 cluster_28 cluster_29 cluster_30 cluster_31 cluster_32 cluster_33 cluster_34 cluster_35 cluster_36 cluster_37 cluster_38 cluster_39 cluster_40 cluster_41 cluster_42 cluster_43 cluster_44 cluster_45 cluster_46 cluster_47 ATF CTCF EBOX AP1 RUNX SP ETS TEAD TFAP Other SOX CTCFL GATA Other TEAD Other Other Other ZNF Other Other NRF Other Other Other Other Other Other Other Other Other Other SOX Other Other Other Other Other Other Other Other Other Other Other Other Other Other 3278 1041 2480 4011 1165 789 1285 544 1024 334 935 1010 696 141 601 805 270 475 473 395 393 768 214 336 375 215 234 354 210 200 218 415 387 116 121 394 112 111 107 118 144 105 102 108 114 118 119 23 # of Motifs in the cluster 71 85 91 191 37 20 33 9 53 4 17 16 7 2 6 7 4 5 6 4 4 8 2 2 3 2 2 3 2 2 2 2 2 1 1 2 1 1 1 1 1 1 1 1 1 1 1 Figure 3: (A) The Hamming distance of sampled peptides of different lengths to the peptides of the same length in the test set. (B) The Shannon Entropy of sampled peptides of different lengths to the peptides of the same length in the test set. 24 Figure 4: Complex structures of target proteins with pre-existing binders. (A)-(B) 1B8Q, (C)-(D) 1E6I, (E)-(F) 3IDJ, (G)-(H) 7JVS. Each panel shows the complex structure of the target with either MOG-DFMdesigned binder or its pre-existing binder. For each binder, five property scores are provided, as well as the ipTM score from AlphaFold3 and the docking score from AutoDock VINA. Interacting residues on the target are visualized. 25 Figure 5: Complex structures of target proteins without pre-existing binders. (A)-(C) AMHR2, (D)-(E) EWS::FLI1, (F) MYC, (G) DUSP12. Each panel shows the complex structure of the target with MOG-DFMdesigned binder. For each binder, five property scores are provided, as well as the ipTM score from AlphaFold3 and the docking score from AutoDock VINA. Interacting residues on the target are visualized. 26 Algorithm 1 MOG-DFM: Multi-Objective-Guided Discrete Flow Matching Sample an initial sequence x0 uniformly from the discrete state space Generate set of weight vectors {ωk}M Select weight vector ω randomly from {ωk} 1: Input: Pre-trained discrete flow matching model, multi-objective score functions 2: Output: Sequence x1 with multi-objective optimized properties 3: Initialize: 4: 5: 6: 7: for = 0 to 1 with step size = 1 8: 9: 10: 11: 12: 13: Compute the normalized rank score In(yi, x) for each objective Compute D(yi, x, ω) based on the alignment of improvements with ω Combine rank and direction components: Select position in the sequence to update For each candidate transition yi = xi: Step 1: Guided Transition Scoring do k=1 that uniformly cover the N-dimensional Pareto front S(yi, x, ω) = Norm (cid:34) 1 (cid:88) n= (cid:35) In(yi, x) + λ Norm [D(yi, x, ω)] Re-weight the original velocity field ui(yi, x) by the combined score Step 2: Adaptive Hypercone Filtering Compute angle αi between improvement vector s(yi, x) and weight vector ω Accept transitions yi where αi Φ (hypercone angle) Select the best transition ybest Adapt Hypercone Angle: from the candidates Compute the rejection rate rt based on the number of rejected candidate transitions Compute the exponential moving average rt of rejection rate Update the hypercone angle Φ based on the moving average: 14: 15: 16: 17: 18: 19: 20: 21: 22: Φt+h = clip (Φt exp (η (rt τ )) , Φmin, Φmax) Step 3: Euler Sampling 23: 24: 25: 26: 27: end for 28: Return: Final sequence Use Eulers method to sample the next state based on the guided velocity field Transition to the new sequence Update time: +"
        }
    ],
    "affiliations": [
        "Center of Computational Biology, Duke-NUS Medical School",
        "Department of Biomedical Engineering, Duke University",
        "Department of Biostatistics and Bioinformatics, Duke University",
        "Department of Computer Science, Duke University",
        "Department of Computer Science, Fudan University",
        "Management and Technology Program, University of Pennsylvania"
    ]
}