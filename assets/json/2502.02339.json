{
    "paper_title": "Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking",
    "authors": [
        "Jinyang Wu",
        "Mingkuan Feng",
        "Shuai Zhang",
        "Ruihan Jin",
        "Feihu Che",
        "Zengqi Wen",
        "Jianhua Tao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal large language models (MLLMs) exhibit impressive capabilities but still face challenges in complex visual reasoning. While recent efforts attempt to enhance MLLMs' reasoning by incorporating OpenAI o1-like structured thinking through explicit search structures or teacher-guided distillation, they often struggle to balance performance and efficiency. A critical limitation is their heavy reliance on extensive data and search spaces, resulting in low-efficiency implicit insight extraction and data utilization. To address this, we propose AStar, an Automated Structured thinking paradigm for multimodal reasoning via Monte Carlo Tree Search (MCTS). AStar automatically derives high-level cognitive reasoning patterns from limited data using MCTS-powered hierarchical structures. Building on these explicit patterns, we design a unified reasoning framework that seamlessly integrates models' internal reasoning capabilities and external reasoning guidelines, enabling efficient inference with minimal tree iterations. This novel paradigm strikes a compelling balance between performance and efficiency. Extensive experiments demonstrate AStar's effectiveness, achieving superior accuracy (54.0$\\%$) on the MathVerse benchmark with a 7B backbone, surpassing GPT-4o (50.2$\\%$) while maintaining substantial data and computational efficiency."
        },
        {
            "title": "Start",
            "content": "Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking Jinyang Wu 1 Mingkuan Feng 1 Shuai Zhang 1 Ruihan Jin 1 Feihu Che 2 Zengqi Wen 2 Jianhua Tao 1 2 5 2 0 2 4 ] . [ 1 9 3 3 2 0 . 2 0 5 2 : r Abstract Multimodal large language models (MLLMs) exhibit impressive capabilities but still face challenges in complex visual reasoning. While recent efforts attempt to enhance MLLMs reasoning by incorporating OpenAI o1-like structured thinking through explicit search structures or teacherguided distillation, they often struggle to balance performance and efficiency. critical limitation is their heavy reliance on extensive data and search spaces, resulting in low-efficiency implicit insight extraction and data utilization. To address this, we propose AStar, an Automated Structured thinking paradigm for multimodal reasoning via Monte Carlo Tree Search (MCTS). AStar automatically derives high-level cognitive reasoning patterns from limited data using MCTS-powered hierarchical structures. Building on these explicit patterns, we design unified reasoning framework that seamlessly integrates models internal reasoning capabilities and external reasoning guidelines, enabling efficient inference with minimal tree iterations. This novel paradigm strikes compelling balance between performance and efficiency. Extensive experiments demonstrate AStars effectiveness, achieving superior accuracy (54.0%) on the MathVerse benchmark with 7B backbone, surpassing GPT-4o (50.2%) while maintaining substantial data and computational efficiency. 1. Introduction Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities across diverse tasks and domains (Yin et al., 2023; OpenAI, 2024; Chen et al., 2024c), such as autonomous driving (Cui et al., 2024), and visual question answering (Hartsock & Rasool, 2024). Their proficiency in complex multimodal reasoning, particularly in *Equal contribution 1Department of Automation, Tsinghua University, Beijing, China 2Beijing National Research Center for Information Science and Technology, Beijing, China. Correspondence to: Shuai Zhang <zhang shuai@mail.tsinghua.edu.cn>, Jianhua Tao <jhtaoo@tsinghua.edu.cn>. Preprint Figure 1. Performance comparison on the MathVerse benchmark. Our AStar framework achieves competitive results against most open-sourced MLLMs and closed-source ones, showing outstanding structured thinking and reasoning abilities. mathematical tasks involving visual content, has emerged as critical benchmark for evaluating fundamental cognitive abilities towards strong artificial intelligence (Searle, 1980; Goertzel & Pennachin, 2007; Wang et al., 2024f). Mastering multi-step visual reasoning requires the integration of multimodal information, along with rigorous adherence to complex rules and sophisticated problem-solving strategies, presenting significant challenges for existing MLLMs (Zhang et al., 2024g; Wang et al., 2024a). Inspired by recent advances in System 2 slow-thinking reasoning systems like OpenAI o1 (OpenAI, 2024) and QVQ (Team, 2024), there is growing interest in incorporating structured thinking into MLLMs (Xu et al., 2024; Dong et al., 2024d; Du et al., 2025). This direction aims to address the limitations of conventional MLLMs that often rely on simple direct prediction modes due to the scarcity of high-quality long-chain reasoning data (Xu et al., 2024; Luo et al., 2025). According to existing literature, two primary approaches have emerged for implementing slow-thinking reasoning systems: explicit search and teacher supervisionguided training. The first approach leverages explicit search structures (e.g., Monte Carlo tree search, MCTS) with specialized reward models to guide the exploration of solution paths (Dong et al., 2024a; Yao et al., 2024). The second approach focuses on distilling structured reasoning patterns AStar: Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking Figure 2. Schematic comparison between AStar and two mainstream structured reasoning methods. L() denotes the training optimization objective. (a) Search-based methods suffer from computational inefficiency due to extensive solution space iterations. (b) Teacher-guided training optimizes models using distilled rationales from powerful models like GPT-4o, but requires substantial data and computational resources, resulting in low-efficiency pattern extraction and poor data utilization. (c) Our approach effectively combines MLLMs internal implicit reasoning capabilities with explicitly extracted insights, achieving compelling balance between performance and efficiency. through long-form Chain-of-Thought (CoT) (Wei et al., 2022; Zhang et al., 2024g; Luo et al., 2025) instruction data, typically requiring supervision from closed-source models like GPT-4o for data synthesis. Despite these advances, current reasoning paradigms face three critical limitations (Figure 2). First, search-based methods (Dong et al., 2024a) suffer from computational inefficiency due to extensive solution space iterations. Second, teacher-guided training methods (Hu et al., 2024; Yao et al., 2024; Luo et al., 2025) typically require substantial training data (100K) and computational resources to implicitly extract reasoning patterns, resulting in low efficiency and poor data utilization. They also heavily depend on proprietary models like GPT-4o for data synthesis, making them impractical for researchers outside major enterprises. Third, static and predefined reasoning processes (Xu et al., 2024; Thawakar et al., 2025) constrain flexibility, leaving the reasoning potential of MLLMs underexplored. To address these challenges, we propose AStar, an Automated Structured Thinking paradigm for multimodAl Reasoning via MCTS. Our approach introduces novel mechanism for automatically deriving high-level cognitive reasoning patterns from limited data (500 samples) using MCTS-powered hierarchical structures (aiming to address the second limitation). Building on these explicit patterns, we design unified reasoning framework that seamlessly integrates internal and external advantageous attributes, enabling efficient adaptive inference with minimal tree iterations, thereby tackling the first and third limitations. This novel reasoning paradigm effectively combines MLLMs internal implicit reasoning capabilities with external explicit reasoning guidelines, achieving compelling balance between performance and efficiency. Specifically, our method comprises three steps: (1) visual reasoning action definition, (2) MCTS-powered thought card construction, and (3) adaptive reasoning and verification. First, we define six atomic reasoning actions as building blocks of chain-structured reasoning patterns (termed thought cards that serve as reference insights during inference). These actions simulate human-like cognitive behaviors, including problem decomposition and reasoning step reflection. Second, using small seed dataset (500 samples), we apply MCTS to derive reference reasoning patterns to construct multiple thought cards. Finally, in the reasoning stage, we select the five optimal thought cards most aligned with the target problems cognitive complexity. With these reasoning guidelines, we perform visual reasoning and validate the final solutions via self-consistency checks or outcome reward models. Experiments demonstrate that AStar exhibits impressive reasoning performance with enhanced efficiency, comparable to powerful closed-source models like GPT-4o (Figure 1). Our main contributions are: Automated Reasoning Paradigm: proposing an MCTSbased automated approach for generating and selecting the optimal reasoning patterns. Efficient Action-Chain Guided Reasoning: providing explicit guidance for each step of the visual reasoning process, enhancing structured thinking capabilities. Superior Performance: achieving 54.0% score on the challenging MathVerse benchmark with 7B backbone, surpassing GPT-4o (50.2%). Improved Efficiency: achieving comparable performance to recent tree-based methods while reducing inference overhead by 6.4, and matching training-based methods while requiring 520 less prior data. 2 AStar: Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking Figure 3. Flowchart of our proposed method AStar. This framework consists of three main parts: (1) Visual Atomic Reasoning Action Definition; (2) MCTS-Powered Thought Card Construction; (3) Adaptive Reasoning and Verification. 2. Related Work Multimodal Reasoning Recent advancements in MLLMs have demonstrated robust capabilities across diverse domains, including visual understanding (Zhang et al., 2024c), mathematics (Zhang et al., 2024d; Du et al., 2025), and scientific inquiries (Zong & Qiu, 2024). Despite these achievements, complex multimodal reasoning remains challenging due to its demands on both visual perception and high-level cognition. Inspired by OpenAI o1s impressive performance, recent approaches (Zhang et al., 2024e; Xu et al., 2024; Thawakar et al., 2025) attempt structured reasoning with pre-defined stages, enhancing MLLMs CoT capabilities (Zhang et al., 2024g). However, their rigid structure limits flexibility across different tasks, overlooking the importance of adaptive reasoning in unleashing multimodal reasoning potential (Wang et al., 2024e). Our approach addresses this by introducing hierarchical tree structure that enables task-specific reasoning path generation and selection. Tree-based Search Tree structures have demonstrated significant potential in language models (Zhang et al., 2024a; Qi et al., 2024; Wu et al., 2024a). Recent efforts explore applying these tree search methods to search effective reasoning paths for MLLMs. While AR-MCTS (Dong et al., 2024a) enhances multimodal reasoning by integrating MCTS with active retrieval, its extensive iteration requirements and computational overhead limit practical applications. Similarly, Mulberry (Yao et al., 2024) leverages tree structures to distill 260K long-chain reasoning data from powerful models like GPT-4o, but requires substantial computational resources and high-capacity teacher models. These methods struggle to achieve an optimal balance between performance and efficiency. To address these limitations, we propose incorporating high-level reasoning abstractions into MCTS, achieving competitive performance with higher efficiency. 3. Methodology Overview of AStar This section introduces AStar in detail. As shown in Figure 3 and Algorithm 1, our approach consists of three steps: Visual Reasoning Action Definition: Establish six humanlike reasoning actions as building blocks for chainstructured thought cards. MCTS-Powered Thought Card Construction: Leverage MCTS to systematically construct thought cards, which serve as reference insights during inference. Adaptive Reasoning and Verification: Dynamically select and execute optimal reasoning patterns based on problem complexity, followed by solution verification. 3.1. Visual Reasoning Action Definition Understanding human complex reasoning is crucial for modeling cognitive processes (Jaffe et al., 2023). Existing studies distinguish between two cognitive systems: System 1 3 AStar: Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking and System 2 (Kahneman, 2011; Da Silva, 2023). While System 1 represents fast, intuitive, yet error-prone thinking, System 2 involves slow, deliberative thinking with superior performance. With the emergence of advanced models like OpenAI o1 (OpenAI, 2024) and QVQ (Team, 2024), developing efficient System 2 approaches to emulate human cognitive processes has gained significant research attention (Xu et al., 2024; Yao et al., 2024; Thawakar et al., 2025). Inspired by this, we introduce six vision-language reasoning actions to bridge the gap between model reasoning and human cognition: Visual Parsing (VP, a1), System Analysis (SA, a2), One-Step Thought (OST, a3), Chain-of-Thought (CoT, a4), Divide and Conquer (DC, a5), Self-Reflection (SR, a6). Details are provided in Appendix B.1. 3.2. MCTS-Powered Thought Card Construction Following the action definition, we introduce thought cards as structured reasoning templates to guide inference in Sec. 3.3. Using small seed dataset, we first derive reasoning paths (Phase 1) and then distill them into high-level thought cards (Phase 2). These cards serve as prior insights during inference, providing structured guidance for efficient problem-adaptive reasoning. Phase 1: Acquire reasoning paths for seed data As shown in Figure 3, we employ MCTS to iteratively optimize the solution search process, generating high-quality reasoning paths for the seed dataset. This design leverages MCTSs systematic exploration (Ye et al., 2021) and MLLMs inherent reasoning capabilities (Yin et al., 2023; Wang et al., 2024f). We formulate each multimodal reasoning problem (consisting of input question and images) as tree search problem, where represents the root node and subsequent nodes denote reasoning steps (actions and corresponding outcomes) generated by policy MLLM πθ. We define the state St1 as the trajectory x, s1, ..., st1, where S0 = x. The next step is sampled as st πθ(St1). To guide tree expansion, we define Q(s) as the reward value for node s. Initially, all unexplored nodes are assigned Q(si) = 0. They are updated using weighted average between the parents current value and its child nodes value: Q(p) (1 α)Q(p) + αQ(s) (1) where α is discount factor for future rewards. For terminal nodes, following Zhou et al. (2024); Wu et al. (2024a), we adopt the likelihood of self-consistency majority voting as the reward value, enabling supervision-free generalization. Specifically, this phase comprises four MCTS operations: (1) Selection. This operation identifies promising nodes for expansion. Starting from the root node, we iteratively select child nodes using the Upper Confidence Bounds applied to Trees (UCT) (Kocsis & Szepesvari, 2006) until reaching Algorithm 1 Multimodal Reasoning with AStar Input: policy model πθ; multimodal test question xt; set of seed data Ds // 3.1. Visual Reasoning Action Definition Initialize action space = {a1, a2, a3, a4, a5, a6} // 3.2. MCTS-Powered Thought Card Construction [ ]; Cards {} for (x, y) Ds do {x, y, } MCTS(πθ; x) if found an valid reasoning path then Find pbest from Add {x, pbest} into Update Cards from end if end for // 3.3. Adaptive Reasoning and Verification CardMatch(Cards; xt) yt ReasonAndVerify(πθ; xt; c) Output: the optimal reasoning trajectory yt leaf node: CT (s) = Q(s) + (cid:115) lnN (p) (s) (2) where Q(s) is the reward value for node s, (s) is the visit count, is the parent node, and is the exploration weight. The node with the highest UCT value is selected for subsequent phases, balancing exploration and exploitation. (2) Expansion. The selected node is expanded by sampling actions from πθ and generating corresponding reasoning outcomes. These child nodes are added to the tree and stored in an external memory structure. (3) Simulation. Starting from the selected node, we iteratively sample and expand nodes until reaching terminal state (maximum depth or answer node). (4) Backpropagation. Upon simulation completion, node information is updated along the simulation path s0, ...sd. Visit counts are incremented (N (s) (s) + 1), and node value Q(s) is propagated backward to its parent node using Equation 1. These updated values are used to guide subsequent UCT-based node selection. Phase 2: Distill paths into thought cards After executing MCTS, we obtain tree structure for each seed dataset question, yielding multiple valid reasoning paths that constitute the path set . Inspired by the concept of Value of Computation (VOC) (Russell & Wefald, 1991), which optimizes the trade-off between computational benefits and costs, we propose VOC-inspired selection metric to identify the optimal reasoning trajectory from candidate solutions: Score(x, px) = R(pxx) (1 k) C(px) (3) 4 AStar: Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking where is the task input, px represents candidate reasoning path from , and balances computational benefits against costs. Here, for simplicity, R(pxx) denotes the paths final reward (defined as the leaf nodes Q-value), while C(px) measures the reasoning cost (defined as the number of actions in the sequence). Then, for each question in seed dataset, we select path pbest with highest Score(x, px) to build Question-path repository with one-to-one mappings. Inspired by metareasoning principles (Russell & Wefald, 1991; Roberts & Roberts, 2024), which advocate for adaptive reasoning strategies, we distill these question-path pairs into abstract thought cards using Problem Condition Complexity (PCC) (Lee & Heyworth, 2000; Embretson & Daniel, 2008), cognitive complexity metric designed for complex reasoning. Each card represents high-level reasoning pattern abstracted from multiple prior problems, enabling efficient transfer of reasoning insights during inference in the next step. 3.3. Adaptive Reasoning and Verification During inference, given multimodal test query xt, we compute its PCC and perform nearest neighbor matching (Muja & Lowe, 2014) against pre-constructed thought cards to identify the most relevant five cards that best align with its complexity level. The selection process is formalized as: N5(xt, C) = arg min Cxt C,S=5 (cid:80) cCxt d(xt, c) (4) where N5(xt, C) is the closest subset to xt, determined by the distance : R. By leveraging these reasoning templates, our method maintains the benefits of tree-structured reasoning without extensive node expansion, enabling efficient generation of candidate solutions. Identifying the optimal reasoning trajectory among these candidates represents critical challenge (Luo et al., 2025). Given the scarcity of verification models in the visual domain, we employ both self-consistency checks and textdomain outcome reward models to select the final solution. In summary, our approach can be viewed as an optimized variant of tree search algorithms. Unlike traditional MCTS methods that require extensive exploration, we strategically reduce computational complexity by incorporating prior insights through thought cards. As shown in Figure 3, this enables efficient tree traversal along promising trajectories while maintaining high performance, offering valuable insights for developing efficient reasoning strategies. 4. Experiments The section presents 4.1 experimental setup and assesses AStars effectiveness from four aspects: 4.2 performance, 4.3 data and computational efficiency, 4.4 out-of-distribution generalization, and 4.5 ablation study and analysis. 4.1. Experimental Setup Datasets We perform extensive experiments across three reasoning tasks and six datasets: (1) general visual question answering: ChartQA (Masry et al., 2022) and MMStar (Chen et al., 2024b); (2) mathematical reasoning: MathVista (Lu et al., 2023), MathVerse (Zhang et al., 2025), and MathVision (Wang et al., 2024a); (3) commonsense and scientific reasoning: GAOKAO-MM (Zong & Qiu, 2024). Models To demonstrate the versatility of AStar, we evaluate its effectiveness on both LLM and MLLM, including Qwen2.5-7B (Qwen Team, 2024), Qwen2-VL-2B (Wang et al., 2024b), and Qwen2-VL-7B. This design aims to validate that AStar can seamlessly leverage pre-trained LLM and MLLM as its inference backbone without modifications. Baselines We evaluate AStar against four strong baseline categories: (1) open-source general MLLMs, including the powerful Qwen2-VL (Wang et al., 2024b) and InternVL2 series (Chen et al., 2024d); (2) open-source MLLMs specifically optimized for mathematical reasoning, such as recent work URSA (Luo et al., 2025) and Math-LLaVA (Shi et al., 2024); (3) advanced closed-source MLLMs, including GPT4V (OpenAI, 2023), and GPT-4o (OpenAI, 2024); and (4) tree-based methods, such as AR-MCTS (Dong et al., 2024a) and Mulberry (Yao et al., 2024). 4.2. Performance Comparison with Baselines Results on Diverse Reasoning Benchmarks Table 1 and 2 presents the performance of AStar across four mainstream reasoning benchmarks. We have four key findings: AStar consistently outperforms both powerful opensource general and math-specialized MLLMs across general visual question answering and mathematical reasoning tasks. Specifically, using Qwen2.5-7B as the reasoning backbone, our method achieves 54.0% accuracy on MathVerse, surpassing InternVL2-8B by 18.1% and the recent high-quality CoT-enhanced URSA-7B model by 8.3%. AStar demonstrates strong performance across multiple reasoning capabilities. Notably, it achieves 63.1% accuracy on logical reasoning (LOG), outperforming InternVL2-8B by 50.5% and URSA-7B by 39.7%. Similar improvements are observed in statistical reasoning (STA: 68.8%) and visual question answering (VQA: 60.1%). The benefits of AStars adaptive reasoning are universal and maintain consistent performance improvements across varying degrees of multimodal information content. Notably, our method achieves consistent gains across different information distributions, from vision-intensive (VI: 46.8%) and vision-dominant (VD: 61.8%) to text-dominant (TD: 5 AStar: Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking Table 1. Evaluation of AStars reasoning capabilities on MathVista and MathVerse testmini. The best results are highlighted in bold. For MathVista, we pick five categories: ALL (overall accuracy), ARI (arithmetic reasoning), LOG (logical reasoning), STA (statistical reasoning), and VQA (visual question answering). For MathVerse, we present seven categories: ALL (overall accuracy), VI (vision intensive), VD (vision dominant), VO (vision only), TD (text dominant), TL (text lite), and TO (text only). Model Random Human #Params MathVista MathVerse ALL ARI LOG STA VQA ALL VI VD VO TD TL - - 17.9 60.3 13.8 59.2 13.4 40.7 14.3 63.9 26.3 55. 12.4 64.9 12.4 61.4 12.4 68.3 12.4 66.7 12.4 71.2 12.4 70. mPLUG-Owl2-7B (Ye et al., 2023) MiniGPT4-7B (Zhu et al., 2023) LLaVA-1.5-13B (Liu et al., 2024a) SPHINX-V2-13B (Lin et al., 2023) SPHINX-MoE (Lin et al., 2023) LLaVA-NeXT-34B (Liu et al., 2024b) InternLM-XComposer2-VL (Dong et al., 2024c) Deepseek-VL (Lu et al., 2024) InternVL2-8B (Chen et al., 2024d) Qwen2-VL (Wang et al., 2024b) Open-Source General MLLMs 7B 7B 13B 13B 87B 34B 7B 7B 8B 7B 22.2 23.1 27.7 36.7 42.6 46.5 57.6 34.9 58.3 58.9 19.2 32.0 28.6 33.4 43.0 - 51.6 38.8 56.4 57.5 13.5 10.8 10.8 24.3 14.4 - 13.5 18.9 10.8 24. 21.4 17.9 22.9 51.5 50.8 - 62.8 33.2 68.8 43.1 27.9 30.2 30.2 43.0 43.3 - 39.7 34.6 49.7 58.1 Open-Source Math MLLMs (Large-Scale Training) G-LLaVA-7B (Gao et al., 2023) Math-LLaVA-13B (Shi et al., 2024) Math-PUMA-Qwen2-7B (Zhuang et al., 2024) Math-PUMA-DeepSeek-Math (Zhuang et al., 2024) MAVIS-7B (Zhang et al., 2024d) InfiMM-Math (Han et al., 2024) MultiMath-7B (Peng et al., 2024) URSA-7B (Luo et al., 2025) AStar (Ours, Training-free Reasoning) 7B 13B 7B 7B 7B 7B 7B 7B 7B 25.1 46.6 47.9 44.7 - - 50.0 59.8 63.5 19.4 40.7 46.2 41.9 - - 42.2 53.5 63.1 15.2 23.3 21.6 8.1 - - 23.3 21. 61.3 15.1 42.3 55.8 50.8 - - 64.9 57.1 68.8 28.7 33.5 30.2 31.3 - - 49.2 40.2 60.1 10.3 12.2 12.7 16.1 22.8 34.6 25.9 19.3 35.9 33. 16.6 22.9 33.6 31.8 35.2 34.5 27.7 45.7 54.0 11.1 12.5 12.6 16.4 21.1 35.2 20.1 20.2 32.2 31.3 17.2 24.5 33.4 33.6 34.1 38.1 28.1 46.4 9.4 14.8 12.7 15.6 19.6 28.9 24.4 18.4 30.9 30.3 14.6 21.7 31.6 31.6 29.7 32.4 25.9 43. 8.0 8.7 9.0 16.2 18.3 22.4 19.8 11.8 27.7 28.1 9.4 16.1 26.0 14.7 31.8 15.8 15.0 28.6 11.6 12.3 17.1 20.8 33.3 49.0 36.9 23.0 39.0 37.4 20.9 27.3 42.1 43.4 43.2 46.7 34.8 55.3 11.4 12.9 12.0 14.1 21.9 37.6 28.3 23.2 33.8 33.5 20.7 24.9 35.0 35.4 37.2 32.4 30.8 48. 46.8 61.8 46.4 53.9 44.3 TO 12.4 41.7 13.8 13.4 22.6 14.0 23.1 30.1 42.5 23.1 36.0 35.0 21.1 27.0 39.8 47.5 - - 35.3 51.8 53.9 Table 2. Comparison with powerful baselines on general VQA datasets: MMStar and ChartQA. Model #Params MMStar ChartQA 2B-Scale Baselines Qwen2-VL (Wang et al., 2024b) Mulberry (Yao et al., 2024) AStar (Ours) 2B 2B 2B 7B-Scale Baselines Deepseek-VL (Lu et al., 2024) Qwen2-VL (Wang et al., 2024b) InternVL2 (Chen et al., 2024d) Insight-V (Dong et al., 2024d) Mulberry (Yao et al., 2024) LLaVA-CoT (Xu et al., 2024) LlamaV-o1 (Thawakar et al., 2025) AStar (Ours) 7B 7B 8B 7B 7B 11B 11B 7B 48.0 51.3 51.7 36.1 60.7 61.5 61.5 61.3 58.1 59.5 61.7 73.5 77.7 78.3 59.1 83.0 83.3 82.3 83.9 - - 83.9 53.9%) and text-only (TO: 53.9%) scenarios, demonstrating robust performance regardless of the modality balance. Beyond specialized reasoning tasks, AStar performs superior on general VQA benchmarks (MMStar and ChartQA), suggesting that enhanced reasoning capabilities also benefit general visual understanding. Results on More Challenging Datasets As shown in Figure 4, we evaluate AStar against leading models on the challenging MathVision benchmark (Wang et al., 2024a) across multiple reasoning dimensions. AStar achieves 32.3% average accuracy across all dimensions, surpassing GPT-4o (30.4%). Notably, in logical reasoning area, AStar attains 45.8% accuracy, outperforming both GPT-4o (29.4%) and Math-LLaVA-13B (16.0%). AStars effectiveness can be attributed to its adaptive tree-structured decomposition framework, which simplifies complex multimodal reasoning tasks into manageable sub-problems, enabling substantial improvements across diverse reasoning capabilities. Comparison with Leading MLLMs We further compare AStar against leading closed-source and larger open-source models. As illustrated in Figure 5, AStar-empowered models deliver competitive performance, matching or surpassing larger models. Notably, AStar-7B achieves an average score of 50.0% across three challenging benchmarks, outperforming GPT-4o (48.1%) and Qwen2-VL-72B (46.0%). Additionally, our reasoning paradigm enables Qwen2-VL-2B to surpass the larger Qwen2-VL-7B model. 4.3. Data and Computational Efficiency We further compare our approach AStar with two recent powerful multimodal tree search methods: AR-MCTS (Dong et al., 2024a) and Mulberry (Yao et al., 2024). As shown in Table 3, we compare multiple dimensions, includ6 AStar: Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking Table 3. Comparison with leading multimodal tree search methods. AStar achieves competitive performance with only 0.5K prior data and 5 search iterations per sample, demonstrating superior efficiency and effectiveness. Methods Open-Source Only Training-Free Data Volume Search Iter. MathVista Acc. MMStar Acc. GAOKAO Acc. AR-MCTS (Dong et al., 2024a) Mulberry (Yao et al., 2024) Ours 34.5K 260K 0.5K 32.0 - 5.0 64.1 63.1 63. - 61.3 61.70.4 37.4 - 49.712.3 Figure 4. Comparison with leading MLLMs across various capabilities on the challenging MathVision dataset. Our 7B-parameter model achieves comparable performance to GPT-4o. Figure 5. Comparison between AStar and powerful MLLMs across 3 challenging benchmarks: MathVista, MathVerse, and MathVision. OS and CS denote open-source and closed-source models. AStar with 7B model outperforms larger OS and CS models. ing training requirements, training data volume, computational efficiency (measured by average search iterations per sample), and accuracy on three representative benchmarks. Results demonstrate that AStar achieves competitive performance while significantly improving both data and computational efficiency. In terms of data efficiency, AStar requires only 0.5K prior samples, which represents 69-fold reduction compared to AR-MCTS (34.5K) and 520-fold reduction compared to Mulberry (260K), respectively. This efficiency stems from explicit reasoning pattern extraction, enabling performance comparable to implicit learning methods with far less training data. Such efficiency is particularly advantageous in limited-data scenarios. Regarding computational efficiency, AStar reduces the average iterations per sample by 6.4 compared to AR-MCTS, while maintaining comparable or superior accuracy across all benchmarks. 4.4. Out-of-Distribution Generalization Recent work has highlighted that distributional shifts severely affect the reliability of MLLMs (Yin et al., 2023; Zhang et al., 2024f). While these models excel in indistribution (ID) tasks, their performance often degrades in out-of-distribution (OOD) scenarios (Dong et al., 2024b; Miyai et al., 2024), challenge further compounded by the difficulty of acquiring sufficient high-quality training data. In this section, we evaluate AStars performance in OOD scenarios. Since our reasoning guidance during inference is derived from the mathematical domain, we further test on GAOKAO-MM (Zong & Qiu, 2024), Chinese human-level multimodal reasoning benchmark that includes historical, geographical, and scientific reasoning tasks. This design naturally constitutes an OOD evaluation setting. As shown in Figure 6, all models demonstrate significant improvements over baseline methods when enhanced with the AStar framework. Notably, for Qwen2-VL-7B, our method improves Geography task accuracy from 26.9% to 51.6% and achieves an average improvement of 19.5% across all subjects. These results validate AStars strong generalization across different languages, task distributions, and reasoning domains, establishing it as robust and versatile approach for both ID and OOD reasoning tasks. 4.5. Ablation Study and Analysis To investigate the contribution of each component in AStar, we conduct ablation study as shown in Table 4. Here, w/o denotes variants with specific components removed. Our analysis reveals three key findings: First, removing any component leads to performance degradation, validating the necessity of all component designs. Specifically, excluding the card construction module results in the most substantial drops (4.6% on MathVista and 5.5% on MathVerse), highlighting the critical role of thought cards 7 AStar: Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking Figure 6. Cross-distribution performance comparison on GAOKAO-MM. We test on Qwen2-VL-2B, Qwen2-VL-7B, and GPT-4o. Results highlight that AStar exhibits superior performance when testing on out-of-distribution data. Table 4. Ablation results on AStar-7B. RA, RC, RS, SC denotes random actions, random card, random selection, and self-consistency, respectively. We observe that every component is important for optimal performance. Model Setting MathVista MathVerse Average AStar w/o thought cards (RA) w/o card match (RC) w/o verification (RS) w/o verification (SC) 63.5 58.9 61.3 62.0 63.0 54.0 48.5 50.3 51.6 51.8 58.8 53.7-5.1 55.8-3.0 56.8-2.0 57.4-1.4 in providing high-level insights for test-time inference. Second, the precision of card matching also impacts performance, particularly on complex tasks like MathVerse where degradation is more pronounced than on MathVista. While this paper employs PCC-based nearest-neighbor matching, future work could explore more advanced strategies. Third, while the verification module improves performance, simpler alternatives like self-consistency (SC) or random selection (RS) show only modest degradation (2.0% and 1.4%). This resilience suggests that thought card guidance enables robust candidate solution generation even with basic selection methods. Inference Time Scaling We gradually increase the number of selected thought cards during inference to investigate whether our method follows test-time scaling laws. Figure 7 demonstrates that incorporating additional reasoning guidance via thought cards leads to consistent performance improvements in our AStar framework. Visualization of Sampling Space To analyze the diversity of solutions generated by our AStar-powered reasoning framework across varying numbers of thought cards, we conduct visualization study here. We sample 250 problems from MathVision and generate 2 candidate solutions for each problem using AStar (Qwen2-VL-2B), resulting Figure 7. Inference Time Scaling. We examine the variation in AStar performance with the number of selected reasoning guidanceproviding thought cards in Sec. 3.3. in 500 total samples. Following Dong et al. (2024a), we employ three-step analysis process using BGE-M3 (Chen et al., 2024a) for semantic embedding, followed by dimensionality reduction (PCA) and clustering techniques, DBSCAN (Ester et al., 1996). Figure 8 and 9 illustrate the visualization results across varying numbers of thought cards (k = 2 and = 5) on MathVista and MathVision, respectively. The empirical analysis reveals consistent conclusions across both standard mathematical reasoning tasks (MathVista) and challenging scenarios (MathVision). When fewer thought cards are provided (k = 2), the model generates clusters with fewer centroids for the same problem set, indicating more constrained sampling space with limited potential. Conversely, increasing the number of thought cards to = 5 leads to clusters with more centroids, suggesting richer sampling space with higher performance potential. The modest difference in cluster counts between = 2 and = 5 indicates that our framework effectively adapts to problem complexity by dynamically selecting suitable reasoning patterns from high-quality thought cards. Even with fewer thought cards, it maintains robust sampling diversity and performance through this adaptive matching mechanism. 8 AStar: Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking Figure 8. Visualization of candidate reasoning paths on MathVista with = 2 (left) and = 5 (right) thought cards. Dots represent individual reasoning steps, while colors indicate distinct solution clusters. The increasing density and diversity of clusters from left to right demonstrate the models enhanced exploration capability with more thought cards, leading to more comprehensive coverage of the solution space. Figure 9. Visualization of candidate reasoning paths on MathVision with = 2 (left) and = 5 (right) thought cards. Dots represent individual reasoning steps, while colors indicate distinct solution clusters. The increasing density and diversity of clusters from left to right demonstrate the models enhanced exploration capability with more thought cards, leading to more comprehensive coverage of the solution space. Notably, the observed lower cluster count in MathVista compared to MathVision offers valuable insights into taskspecific reasoning requirements. On MathVista, which contains relatively simpler problems, our AStar framework exhibits strong reasoning path convergence: multiple highquality thought cards typically lead to consistent solutions. This convergence allows for effective performance even with basic self-consistency verification. However, on the more challenging MathVision dataset, we observe greater path divergence, emphasizing the importance of careful validation and selection of optimal reasoning paths. This behavior closely resembles human problem-solving patterns. When addressing elementary problems, different approaches tend to naturally converge to the same answer. In contrast, for complex challenges like Olympic problems, diverse approaches often yield different conclusions, making thorough solution validation necessary. In summary, our visualization results empirically validate that AStar effectively addresses the challenge of limited diversity in candidate solutions, enabling comprehensive coverage of the problem-solving space while providing robust prior guidance for efficient inference. 5. Conclusion In this paper, we propose AStar, novel automated structured thinking paradigm for multimodal reasoning that efAStar: Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking fectively tackles both performance and efficiency challenges in visual reasoning tasks. By leveraging the hierarchical structure of MCTS and extracting high-level cognitive reasoning patterns to guide inference, our approach achieves notable advantages: (1) 32.3% accuracy on the challenging MathVision benchmark with 7B backbone, surpassing GPT-4os 30.4%; (2) 6.4 reduction in inference overhead compared to existing tree-based methods; and (3) 520 less prior data than training-based approaches while maintaining comparable performance. These results demonstrate that AStars integration of structured decomposition with efficient prior guidance offers promising pathway for advancing multimodal reasoning, making it both more powerful and accessible to the broader research community."
        },
        {
            "title": "References",
            "content": "Anthropic. Introducing the next generation of claude, URL https://www.anthropic. March 2024. com/news/claude-3-family. Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., and Zhou, J. Qwen-vl: versatile visionlanguage model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 1(2): 3, 2023. Bansal, H., Hosseini, A., Agarwal, R., Tran, V. Q., and Kazemi, M. Smaller, weaker, yet better: Training llm reasoners via compute-optimal sampling. arXiv preprint arXiv:2408.16737, 2024. Chaslot, G., Bakkes, S., Szita, I., and Spronck, P. Montecarlo tree search: new framework for game ai. In Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, volume 4, pp. 216217, 2008. Chen, J., Li, D. Z. X. S. X., Zhang, Z. L. P., Xiong, R. K. V. C. Y., and Elhoseiny, M. Minigpt-v2: Large language model as unified interface for vision-language multitask learning. arXiv preprint arXiv:2310.09478, 2023. Chen, J., Xiao, S., Zhang, P., Luo, K., Lian, D., and Liu, Z. BGE m3-embedding: Multi-lingual, multifunctionality, multi-granularity text embeddings through self-knowledge distillation. CoRR, abs/2402.03216, 2024a. Chen, L., Li, J., Dong, X., Zhang, P., Zang, Y., Chen, Z., Duan, H., Wang, J., Qiao, Y., Lin, D., and Zhao, F. Are we on the right way for evaluating large vision-language In The Thirty-eighth Annual Conference on models? Neural Information Processing Systems, 2024b. formance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024c. Chen, Z., Wu, J., Wang, W., Su, W., Chen, G., Xing, S., Zhong, M., Zhang, Q., Zhu, X., Lu, L., et al. Internvl: Scaling up vision foundation models and aligning for In Proceedings of the generic visual-linguistic tasks. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2418524198, 2024d. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Cui, C., Ma, Y., Cao, X., Ye, W., Zhou, Y., Liang, K., Chen, J., Lu, J., Yang, Z., Liao, K.-D., et al. survey on multimodal large language models for autonomous driving. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 958979, 2024. Da Silva, S. System 1 vs. system 2 thinking. Psych, 5(4): 10571076, 2023. Dong, G., Zhang, C., Deng, M., Zhu, Y., Dou, Z., and Wen, J.-R. Progressive multimodal reasoning via active retrieval. arXiv preprint arXiv:2412.14835, 2024a. Dong, H., Ding, Z., and Zhang, S. Deep Reinforcement Learning: Fundamentals, Research and Applications, volume 1 of eBook Packages: Mathematics and Statistics. Springer Singapore, 1 edition, 2020. ISBN 9789811540950. Dong, H., Zhao, Y., Chatzi, E., and Fink, O. MultiOOD: Scaling out-of-distribution detection for multiple modalities. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024b. Dong, X., Zhang, P., Zang, Y., Cao, Y., Wang, B., Ouyang, L., Wei, X., Zhang, S., Duan, H., Cao, M., et al. Internlmxcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. arXiv preprint arXiv:2401.16420, 2024c. Dong, Y., Liu, Z., Sun, H.-L., Yang, J., Hu, W., Rao, Y., and Liu, Z. Insight-v: Exploring long-chain visual reasoning with multimodal large language models. arXiv preprint arXiv:2411.14432, 2024d. Driess, D., Xia, F., Sajjadi, M. S., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023. Chen, Z., Wang, W., Cao, Y., Liu, Y., Gao, Z., Cui, E., Zhu, J., Ye, S., Tian, H., Liu, Z., et al. Expanding perDu, Y., Liu, Z., Li, Y., Zhao, W. X., Huo, Y., Wang, B., Chen, W., Liu, Z., Wang, Z., and Wen, J.-R. Virgo: AStar: Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking preliminary exploration on reproducing o1-like mllm. arXiv preprint arXiv:2501.01904, 2025. Embretson, S. E. and Daniel, R. C. Understanding and quantifying cognitive complexity level in mathematical problem solving items. Psychology Science, 50(3):328, 2008. Ester, M., Kriegel, H., Sander, J., and Xu, X. densitybased algorithm for discovering clusters in large spatial In Simoudis, E., Han, J., and databases with noise. Fayyad, U. M. (eds.), Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD-96), Portland, Oregon, USA, pp. 226231. AAAI Press, 1996. Gao, B., Cai, Z., Xu, R., Wang, P., Zheng, C., Lin, R., Lu, K., Lin, J., Zhou, C., Xiao, W., et al. Llm critics help catch bugs in mathematics: Towards better mathematical verifier with natural language feedback. CoRR, 2024. Gao, J., Pi, R., Zhang, J., Ye, J., Zhong, W., Wang, Y., Hong, L., Han, J., Xu, H., Li, Z., et al. G-llava: Solving geometric problem with multi-modal large language model. arXiv preprint arXiv:2312.11370, 2023. Ge, Z., Huang, H., Zhou, M., Li, J., Wang, G., Tang, S., and Zhuang, Y. Worldgpt: Empowering llm as multimodal world model. In Proceedings of the 32nd ACM International Conference on Multimedia, pp. 73467355, 2024. Goertzel, B. and Pennachin, C. Artificial general intelligence, volume 2. Springer, 2007. Han, X., Jian, Y., Hu, X., Liu, H., Wang, Y., Fan, Q., Ai, Y., Huang, H., He, R., Yang, Z., et al. Infimm-webmath-40b: Advancing multimodal pre-training for enhanced mathematical reasoning. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS24, 2024. Hao, S., Gu, Y., Ma, H., Hong, J., Wang, Z., Wang, D., and Hu, Z. Reasoning with language model is planning with world model. In Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 81548173, Singapore, December 2023. Association for Computational Linguistics. Hartsock, I. and Rasool, G. Vision-language models for medical report generation and visual question answering: review. Frontiers in Artificial Intelligence, 7:1430984, 2024. Hu, Y., Shi, W., Fu, X., Roth, D., Ostendorf, M., Zettlemoyer, L., Smith, N. A., and Krishna, R. Visual sketchpad: Sketching as visual chain of thought arXiv preprint for multimodal language models. arXiv:2406.09403, 2024. Jaffe, P. I., Poldrack, R. A., Schafer, R. J., and et al. Modelling human behaviour in cognitive tasks with latent dynamical systems. Nature Human Behaviour, 7:9861000, 2023. Kahneman, D. Thinking, Fast and Slow. Farrar, Straus and Giroux, New York, NY, 2011. ISBN 978-0374275631. Kocsis, L. and Szepesvari, C. Bandit based monte-carlo planning. In Furnkranz, J., Scheffer, T., and Spiliopoulou, M. (eds.), Machine Learning: ECML 2006, pp. 282293, Berlin, Heidelberg, 2006. Springer Berlin Heidelberg. ISBN 978-3-540-46056-5. Langley, P. Crafting papers on machine learning. In Langley, P. (ed.), Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 12071216, Stanford, CA, 2000. Morgan Kaufmann. Lee, F.-L. and Heyworth, R. Problem complexity: measure of problem difficulty in algebra by using computer. Education Journal, 28(1):85108, 2000. Li, B., Zhang, Y., Guo, D., Zhang, R., Li, F., Zhang, H., Zhang, K., Zhang, P., Li, Y., Liu, Z., et al. Llavaonevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. Li, Y., Lin, Z., Zhang, S., Fu, Q., Chen, B., Lou, J.-G., and Chen, W. Making language models better reasoners with step-aware verifier. In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 53155333, Toronto, Canada, July 2023. Association for Computational Linguistics. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and In The Twelfth Cobbe, K. Lets verify step by step. International Conference on Learning Representations, 2024. Lin, Z., Liu, C., Zhang, R., Gao, P., Qiu, L., Xiao, H., Qiu, H., Lin, C., Shao, W., Chen, K., et al. Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. arXiv preprint arXiv:2311.07575, 2023. Liu, H., Li, C., Li, Y., and Lee, Y. J. Improved baselines In Proceedings of the with visual instruction tuning. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2629626306, 2024a. Liu, H., Li, C., Li, Y., Li, B., Zhang, Y., Shen, S., and Lee, Y. J. Llava-next: Improved reasoning, ocr, and world knowledge, 2024b. 11 AStar: Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking Lu, H., Liu, W., Zhang, B., Wang, B., Dong, K., Liu, B., Sun, J., Ren, T., Li, Z., Yang, H., et al. Deepseek-vl: towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024. Lu, P., Bansal, H., Xia, T., Liu, J., Li, C., Hajishirzi, H., Cheng, H., Chang, K.-W., Galley, M., and Gao, J. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. Luo, R., Zheng, Z., Wang, Y., Yu, Y., Ni, X., Lin, Z., Zeng, J., and Yang, Y. Ursa: Understanding and verifying chainof-thought reasoning in multimodal mathematics. arXiv preprint arXiv:2501.04686, 2025. Masry, A., Do, X. L., Tan, J. Q., Joty, S., and Hoque, E. ChartQA: benchmark for question answering about charts with visual and logical reasoning. In Muresan, S., Nakov, P., and Villavicencio, A. (eds.), Findings of the Association for Computational Linguistics: ACL 2022, pp. 22632279, Dublin, Ireland, May 2022. Association for Computational Linguistics. Miyai, A., Yang, J., Zhang, J., Ming, Y., Lin, Y., Yu, Q., Irie, G., Joty, S., Li, Y., Li, H., et al. Generalized outof-distribution detection and beyond in vision language model era: survey. arXiv preprint arXiv:2407.21794, 2024. Muja, M. and Lowe, D. G. Scalable nearest neighbor algorithms for high dimensional data. IEEE Transactions on Pattern Analysis and Machine Intelligence, 36(11): 22272240, 2014. OpenAI. URL gpt-4v-system-card. GPT-4V(ision) 2023. https://openai.com/research/ system card, OpenAI. Learning to reason with llms, September 2024. URL https://openai.com/index/ learning-to-reason-with-llms/. OpenAI. GPT-4o system card, 2024. URL https://openai.com/research/ gpt-4o-system-card. Peng, S., Fu, D., Gao, L., Zhong, X., Fu, H., and Tang, Z. Multimath: Bridging visual and mathematical reasoning for large language models. arXiv preprint arXiv:2409.00147, 2024. Qi, Z., Ma, M., Xu, J., Zhang, L. L., Yang, F., and Yang, M. Mutual reasoning makes smaller llms stronger problemsolvers. arXiv preprint arXiv:2408.06195, 2024. Qwen Team. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm.github. io/blog/qwen2.5/. 12 Roberts, D. and Roberts, L. Smart vision-language reasoners. arXiv preprint arXiv:2407.04212, 2024. Russell, S. and Wefald, E. Principles of metareasoning. Artificial Intelligence, 49(1):361395, 1991. ISSN 00043702. Searle, J. R. Minds, brains, and programs. Behavioral and brain sciences, 3(3):417424, 1980. Shi, W., Hu, Z., Bin, Y., Liu, J., Yang, Y., Ng, S.-K., Bing, L., and Lee, R. K.-W. Math-LLaVA: Bootstrapping mathematical reasoning for multimodal large language models. In Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 46634680, Miami, Florida, USA, November 2024. Association for Computational Linguistics. Team, G., Anil, R., Borgeaud, S., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Team, Q. Qvq: To see the world with wisdom, December 2024. URL https://qwenlm.github.io/blog/ qvq-72b-preview/. Thawakar, O., Dissanayake, D., More, K., Thawkar, R., Heakl, A., Ahsan, N., Li, Y., Zumri, M., Lahoud, J., Anwer, R. M., et al. Llamav-o1: Rethinking step-by-step visual reasoning in llms. arXiv preprint arXiv:2501.06186, 2025. Wang, K., Pan, J., Shi, W., Lu, Z., Ren, H., Zhou, A., Zhan, M., and Li, H. Measuring multimodal mathematical reasoning with MATH-vision dataset. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024a. Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, X., Wang, J., Ge, W., et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024b. Wang, P., Li, L., Shao, Z., Xu, R., Dai, D., Li, Y., Chen, D., Wu, Y., and Sui, Z. Math-shepherd: Verify and reinforce LLMs step-by-step without human annotations. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 94269439, Bangkok, Thailand, August 2024c. Association for Computational Linguistics. Wang, P., Li, L., Shao, Z., Xu, R., Dai, D., Li, Y., Chen, D., Wu, Y., and Sui, Z. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 94269439, 2024d. AStar: Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking Wang, W., Chen, Z., Wang, W., Cao, Y., Liu, Y., Gao, Z., Zhu, J., Zhu, X., Lu, L., Qiao, Y., et al. Enhancing the reasoning ability of multimodal large language models via mixed preference optimization. arXiv preprint arXiv:2411.10442, 2024e. Wang, X., Wei, J., Schuurmans, D., Le, Q. V., Chi, E. H., Narang, S., Chowdhery, A., and Zhou, D. Selfconsistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023. Wang, Y., Chen, W., Han, X., Lin, X., Zhao, H., Liu, Y., Zhai, B., Yuan, J., You, Q., and Yang, H. Exploring the reasoning abilities of multimodal large language models (mllms): comprehensive survey on emerging trends in multimodal reasoning. arXiv preprint arXiv:2401.06805, 2024f. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Wu, J., Feng, M., Zhang, S., Che, F., Wen, Z., and Tao, J. Beyond examples: High-level automated reasoning paradigm in in-context learning via mcts. arXiv preprint arXiv:2411.18478, 2024a. Wu, S., Peng, Z., Du, X., Zheng, T., Liu, M., Wu, J., Ma, J., Li, Y., Yang, J., Zhou, W., et al. comparative study on reasoning patterns of openais o1 model. arXiv preprint arXiv:2410.13639, 2024b. Xu, G., Jin, P., Hao, L., Song, Y., Sun, L., and Yuan, L. Llava-o1: Let vision language models reason step-bystep. arXiv preprint arXiv:2411.10440, 2024. Yang, J., Dong, Y., Liu, S., Li, B., Wang, Z., Tan, H., Jiang, C., Kang, J., Zhang, Y., Zhou, K., et al. Octopus: Embodied vision-language programmer from environmental feedback. In European Conference on Computer Vision, pp. 2038. Springer, 2025. Yang, Y., Ma, Y., and Liu, P. Weak-to-strong reasoning. In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N. (eds.), Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 83508367, Miami, Florida, USA, November 2024. Association for Computational Linguistics. Yao, H., Huang, J., Wu, W., Zhang, J., Wang, Y., Liu, S., Wang, Y., Song, Y., Feng, H., Shen, L., et al. Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search. arXiv preprint arXiv:2412.18319, 2024. Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Cao, Y., and Narasimhan, K. Tree of thoughts: Deliberate problem solving with large language models. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 1180911822. Curran Associates, Inc., 2023. Ye, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J., Hu, A., Shi, P., Shi, Y., et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. Ye, W., Liu, S., Kurutach, T., Abbeel, P., and Gao, Y. MasIn Ranzato, M., tering atari games with limited data. Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 2547625488. Curran Associates, Inc., 2021. Yin, S., Fu, C., Zhao, S., Li, K., Sun, X., Xu, T., and Chen, E. survey on multimodal large language models. arXiv preprint arXiv:2306.13549, 2023. Zeng, Z., Cheng, Q., Yin, Z., Wang, B., Li, S., Zhou, Y., Guo, Q., Huang, X., and Qiu, X. Scaling of search and learning: roadmap to reproduce o1 from reinforcement learning perspective. arXiv preprint arXiv:2412.14135, 2024. Zhang, D., Li, J., Huang, X., Zhou, D., Li, Y., and Ouyang, W. Accessing gpt-4 level mathematical olympiad solutions via monte carlo tree self-refine with llama-3 8b. arXiv preprint arXiv:2406.07394, 2024a. Zhang, D., Zhoubian, S., Hu, Z., Yue, Y., Dong, Y., and Tang, J. Rest-mcts*: Llm self-training via process reward guided tree search. Advances in Neural Information Processing Systems, 2024b. Zhang, J., Huang, J., Jin, S., and Lu, S. Vision-language models for vision tasks: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46(8):5625 5644, 2024c. Zhang, R., Wei, X., Jiang, D., Guo, Z., Li, S., Zhang, Y., Tong, C., Liu, J., Zhou, A., Wei, B., et al. Mavis: Mathematical visual instruction tuning with an automatic data engine. arXiv preprint arXiv:2407.08739, 2024d. Zhang, R., Zhang, B., Li, Y., Zhang, H., Sun, Z., Gan, Z., Yang, Y., Pang, R., and Yang, Y. Improve vision language model chain-of-thought reasoning. arXiv preprint arXiv:2410.16198, 2024e. Zhang, R., Jiang, D., Zhang, Y., Lin, H., Guo, Z., Qiu, P., Zhou, A., Lu, P., Chang, K.-W., Qiao, Y., et al. Mathverse: Does your multi-modal llm truly see the diagrams 13 AStar: Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking in visual math problems? In European Conference on Computer Vision, pp. 169186. Springer, 2025. Zhang, X., Li, J., Chu, W., Hai, J., Xu, R., Yang, Y., Guan, S., Xu, J., and Cui, P. On the out-of-distribution generalization of multimodal large language models. arXiv preprint arXiv:2402.06599, 2024f. Zhang, Z., Zhang, A., Li, M., hai zhao, Karypis, G., and Smola, A. Multimodal chain-of-thought reasoning in language models. Transactions on Machine Learning Research, 2024g. ISSN 2835-8856. Zhou, A., Yan, K., Shlapentokh-Rothman, M., Wang, H., and Wang, Y.-X. Language agent tree search unifies reasoning, acting, and planning in language models. In Forty-first International Conference on Machine Learning, 2024. Zhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. Zhuang, W., Huang, X., Zhang, X., and Zeng, J. Math-puma: Progressive upward multimodal alignment arXiv preprint to enhance mathematical reasoning. arXiv:2408.08640, 2024. Zong, Y. and Qiu, X. GAOKAO-MM: Chinese humanlevel benchmark for multimodal models evaluation. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Findings of the Association for Computational Linguistics: ACL 2024, pp. 88178825, Bangkok, Thailand, August 2024. Association for Computational Linguistics. 14 AStar: Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking"
        },
        {
            "title": "Appendix of AStar",
            "content": "This comprehensive supplementary material provides in-depth insights into our AStar method, covering additional descriptions, experimental details, and results. The appendix is organized as follows:"
        },
        {
            "title": "Table of Contents",
            "content": "A. Preliminaries A.1. Overall Notations A.2. MLLM Reasoning A.3. Monte Carlo Tree Search A.4. Verification Methods B. Algorithm Details B.1. Action Space B.2. Reward Value in MCTS C. More Details about Experimental Setup C.1. Benchmarks and Datasets C.2. Baselines C.3. Implementation Details D. Supplementary Results D.1. Detailed Results on Multimodal Reasoning Benchmarks D.2. Comparison with Strong Baselines D.3. Integration with SFT D.4. Weak-to-Strong Generalization E. Case Study A. Preliminaries This section describes overall notations (A.1), MLLM reasoning (A.2), monte carlo tree search (A.3), and verification methods (A.4). A.1. Overall Notations The definitions for notations are in Table 5. A.2. MLLM Reasoning With the advancement of computational resources and expanded datasets, MLLMs have demonstrated remarkable capabilities across various multimodal tasks (Yin et al., 2023; Zhang et al., 2024c; Wang et al., 2024f). These range from visual reasoning tasks like chart understanding and visual question-answering (Masry et al., 2022; Hartsock & Rasool, 2024) to more complex perception-action tasks in autonomous driving and robotics (Driess et al., 2023; Cui et al., 2024; Yang et al., 2025), where models must integrate visual inputs with decision-making processes. Central to these achievements is the development of effective reasoning methods, which can substantially enhance MLLM problem-solving capabilities, enabling even smaller models to achieve sophisticated reasoning abilities (Wang et al., 2024b; Dong et al., 2024a; Thawakar et al., 2025). To formalize this reasoning process, we consider an autoregressive pre-trained MLLM serving as policy model πθ. Given 15 AStar: Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking Character Meaning Table 5. Notation Table πθ DI Ds Dt xt yd yg yt Td at st St at Q(s) pφ oψ policy MLLM task instruction demonstration examples of I, which is ϕ in zero-shot settings seed data test data multimodal test problem, consisting of input question and images decoded answer gold standard answer reasoning trajectory / solution number of reasoning steps number of tokens in decoded answer yp t-th decoded answer token of yd t-th reasoning step of trajectory yt t-th state, which consists of input and preceding reasoning steps (s1, s2, ..., st1) t-th action based on the previous state St1 node in the tree structure parent node of reward value of node process reward model outcome reward model an input problem xt, the model generates sequence output = (s0, s1, s2, ..., sT ) through iterative token prediction, where s0 := xt represents the initial state and sT corresponds to the solution yp. We define this generated sequence (s0, s1, s2, ..., sT ) as reasoning trajectory yt. The conditional probability distribution of generating the complete reasoning trajectory is: πθ(yt, yd I, xt, DI ) = (cid:89) πθ (st s<t, I, xt, DI ) t=1 (cid:124) (cid:123)(cid:122) Intermidiate Reasoning Process (cid:125) Td(cid:89) t=1 (cid:124) (cid:16) πθ <t, yt, I, xt (cid:123)(cid:122) Answer Decoding (cid:17) , (cid:125) (5) Following (Hao et al., 2023; Ge et al., 2024), we can conceptualize MLLMs as world models, with the complex reasoning process formulated as Markov decision process. Specifically, when addressing complex reasoning challenges in real-world scenarios, at each time step t, the model receives state St1, comprising the original input problem and preceding reasoning steps (s0, s1, s2, ..., st1). The policy model πθ then generates the current action at = πθ(Φ(St1)), which prompts the MLLM to produce the next reasoning step st. The entire process, from the initial step s0 to the final output sT , naturally forms complete trajectory or chain of thought. Inspired by recent advances in language model reasoning capabilities, researchers have explored OpenAI o1-like structured reasoning approaches to enhance long-chain reasoning in MLLMs. These approaches aim to develop systematic thinking patterns that enable models to perform complex multi-step reasoning. The process can be formalized under the following theoretical framework: Pπθ (yd = yg xt) = E(s0,s1, ,sT )Pπθ (ytxt) [P (yd = yg s0, s1, , sT , xt)] where (yd = yg s0, s1, , sT , x) represents the probability of obtaining an accurate final answer given the test problem xt and reasoning trajectory yt. (6) Early work explored multimodal chain-of-thought (CoT) (Zhang et al., 2024g), addressing the limitations of conventional MLLMs that typically defaulted to simple direct prediction mode due to the scarcity of high-quality long-chain reasoning 16 AStar: Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking Table 6. Method Comparison: Recent Multimodal Structured Reasoning Approaches. Method Open-Source Only Data Volume Training-Free Training Cost Inference Cost AR-MCTS (Dong et al., 2024a) Mulberry (Yao et al., 2024) LLaVA-CoT (Xu et al., 2024) URSA (Luo et al., 2025) LlamaV-o1 (Thawakar et al., 2025) AStar (Ours) 34.5K 260K 100K 1100K 118K 0.5K - High High High High Low High Low Low Low Low Low data. However, these approaches often exhibit instability in multimodal reasoning tasks due to the distribution shift between training and inference (Wang et al., 2024e). Two primary methods have emerged to address this challenge: explicit search mechanisms and teacher-guided distillation. Recent advances in these approaches are summarized in Table 6. (1) Explicit search mechanisms. These approaches leverage explicit search structures (e.g., Monte Carlo tree search, MCTS) with specialized reward models to guide the exploration of solution paths. AR-MCTS (Dong et al., 2024a) combines MCTS with active retrieval to expand the solution space and enhance performance, though its extensive iterations demand significant computational resources. Moreover, the quality and relevance of retrieved examples at each exploration step substantially impact model performance. Similarly, Mulberry (Yao et al., 2024) employs collective knowledge to enable collaborative conjecture, search, and identification of effective reasoning paths via MCTS. However, it requires substantial training data (260K samples) generated with expensive proprietary models, making it computationally intensive. (2) Teacher-guided distillation. This approach focuses on distilling structured reasoning patterns through long-form CoT (Wei et al., 2022; Zhang et al., 2024g; Luo et al., 2025) instruction data, typically requiring supervision from proprietary models for data synthesis. For instance, LLaVA-CoT (Xu et al., 2024) explicitly structures visual reasoning into four stages (Summary, Caption, Reasoning, and Conclusion), utilizing 100K synthetic data samples for downstream task fine-tuning. Similarly, URSA (Luo et al., 2025) proposes three-module data synthesis strategy integrating CoT distillation, trajectory rewriting, and format standardization, which relies on Gemini-Flash-002. A.3. Monte Carlo Tree Search As heuristic search algorithm, MCTS has demonstrated remarkable success in complex reasoning and decision-making environments (Chaslot et al., 2008; Ye et al., 2021; Zhou et al., 2024). The algorithm conceptualizes search spaces as tree structures and has achieved significant breakthroughs across various domains, most notably in game-playing AI such as AlphaGo and AlphaZero (Dong et al., 2020). The basic MCTS algorithm involves an iterative search process with four key steps: selection, expansion, simulation, and backpropagation. As an example in multimodal mathematical reasoning, Figure 10 illustrates the four phases in an iteration, expanding the tree and then updating reward values. Leveraging MCTS, recent approaches like AR-MCTS (Dong et al., 2024a) exploit MLLMs intrinsic capabilities and active retrieval for iterative exploration to enhance visual complex reasoning. However, these methods demand substantial computational resources due to extensive iterations. While approaches like Mulberry (Yao et al., 2024) attempt to integrate tree structures through model training, they require large-scale datasets and significant computational overhead. Moreover, current methods typically employ static reasoning processes, lacking the ability to adapt reasoning strategies based on problem complexity. In contrast, our approach employs MCTS only during the generation of prior reasoning patterns (referred to as thought cards in Sec. 3.2) and references these thought cards during inference to achieve efficient reasoning. This design enables AStar to adaptively match reasoning strategies to problem complexity, significantly reducing time complexity compared to traditional tree search methods while maintaining comprehensive search coverage and performance, thus achieving an optimal efficiency-effectiveness trade-off. A.4. Verification Methods After obtaining multiple valid candidate solutions for the test multimodal query xt = [q; i], selecting the most accurate reasoning trajectory among candidates presents critical challenge. In multimodal reasoning, self-consistency (Wang et al., 2023) represents simple yet effective approach, where the final answer is determined through majority voting among 17 AStar: Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking Figure 10. An illustration of the four phases in an iteration of MCTS for complex reasoning tasks (A.3). sampled paths: = arg max yY (cid:88) i=1 I(yi = y) (7) where denotes the set of all possible decoded answers, and is the indicator function. This strategy exploits the observation that complex reasoning problem typically admits multiple valid reasoning paths leading to its unique correct answer. Thus, repeatedly sampled actions at the same state likely indicate successful task completion. However, for complex reasoning tasks where models may only achieve correct reasoning with low probability, simple self-consistency might not provide stable performance. Recent advances in LLM reasoning verification have demonstrated significant improvements in reasoning capabilities through verification modules (Wang et al., 2024d; Zhang et al., 2024b). This has motivated the integration of outcome and process supervision in multimodal domains (Dong et al., 2024a; Luo et al., 2025) to enhance reasoning performance. However, the scarcity of multimodal supervision data and challenges in ensuring supervision signal quality often compromise verification effectiveness, with few high-performance open-source verification models currently available. Therefore, for simplicity, we leverage mature text-domain outcome reward models (ORM). Specifically, in the text domain, given text problem and its reasoning trajectory yt, ORM (D R) assigns single real-value to indicate whether yt is correct. ORM is usually trained with cross-entropy loss (Cobbe et al., 2021; Li et al., 2023): LORM = yg log ryt + (1 yg) log(1 ryt) (8) where yg is the golden answer (yg = 1 if traj is correct else yg = 0), and rtraj is the sigmoid score of traj assigned by ORM. The effectiveness of ORM heavily depends on training data quality. For reasoning problems with definitive answers, researchers can construct the ORM training set automatically by: (1) sampling candidate solutions, and (2) labeling solutions based on answer correctness. While this approach may introduce false positives (incorrect reasoning leading to correct answers), prior work has demonstrated its effectiveness in training robust ORM models (Lightman et al., 2024; Wang et al., 2024c). This enables the development of high-performing open-source verification models in the text domain. To leverage these powerful text-domain verification models for visual reasoning while maintaining our principle of balancing performance and efficiency, we directly employ an MLLM to translate the original visual problems into pure text format. = πθ(xt, prompttranslate) (9) where prompttranslate denotes translation prompt (e.g., please translate the question and visual image into pure text format). The resulting text question and the derived language-based reasoning steps derived from prior visual reasoning are then jointly processed by the ORM model for path selection and verification. 18 AStar: Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking Table 7. Comparison with other search-based LLM and MLLM methods. Note that, most method contain limited space. In contrast, we define rich set of reasoning actions, thus enhancing the upper bound of model reasoning capabilities."
        },
        {
            "title": "MLLM",
            "content": "Tree-of-Thought (Yao et al., 2023) RAP (Hao et al., 2023) ReST-MCTS* (Zhang et al., 2024b) MCTSr (Zhang et al., 2024a) a3: one-step thought a5: divide and conquer a3: one-step thought a4: chain-of-thought, a6: self-reflection AR-MCTS (Dong et al., 2024a) LLaVA-CoT (Xu et al., 2024) URSA (Luo et al., 2025) Ours a3: one-step thought a4: chain-of-thought a4: chain-of-thought a1, a2, a3, a4, a5, a6 Our experiments indicate that this approach achieves improvements over simple self-consistency with minimal computational overhead. Future work could explore integration with high-quality verification models for potentially better results. In this paper, we utilize off-the-shelf ORM model: Llama3.1-8B-ORM-Mistral-Data1. B. Algorithm Details B.1. Action Space Reasoning capabilities are fundamental for handling diverse tasks, encompassing multiple cognitive dimensions. Prior work has categorized reasoning into deductive, inductive, abductive, and analogical approaches (Wang et al., 2024f). While open-source MLLMs demonstrate basic task competency, advanced models necessitate more sophisticated human-like reasoning abilities to achieve their full potential (Zeng et al., 2024). key insight from recent studies suggests that model reasoning performance is bounded by their available reasoning actions (Wu et al., 2024b). Current frameworks, however, operate within constrained action space  (Table 7)  , which may limit the reasoning capabilities of MLLMs. To expand models reasoning capabilities beyond these constraints, we propose comprehensive framework with six fundamental reasoning actions: Visual Parsing (a1): Extracting and analyzing visual information from input images to support multimodal reasoning tasks. System Analysis (a2): Analyzing the overall structure of the problem and identifying the constraints and conditions before addressing it, thereby clarifying task requirements effectively. One-Step Thought (a3): Generating the next one-step thought based on the given question and the preceding reasoning steps. Chain-of-Thought (a4): Facilitating step-by-step reasoning by constructing logical sequence of intermediate thoughts, where each step incrementally builds on the previous ones. Divide and Conquer (a5): Breaking down complex reasoning problem into several smaller subproblems and progressively solving them to achieve the overall solution. Self-Reflection (a6): Engaging in timely reflection of prior solutions and implementing necessary refinement during the reasoning process to ensure accuracy. 1https://huggingface.co/RLHFlow/Llama3.1-8B-ORM-Mistral-Data 19 AStar: Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking Table 8. Comparison with other search-based LLM and MLLM methods. Note that, most method contain limited space. In contrast, we define rich set of reasoning actions, thus enhancing the upper bound of model reasoning capabilities."
        },
        {
            "title": "Visual Question Answering",
            "content": "ChartQA (Masry et al., 2022) MMStar (Chen et al., 2024b) chart understanding and reasoning 6 core capabilities, like scientific reasoning"
        },
        {
            "title": "Mathematics",
            "content": "MathVista (Lu et al., 2023) MathVerse (Zhang et al., 2025) MathVision (Wang et al., 2024a) 12 core capabilities, like arithmetic reasoning 6 distinct versions-text-dominant 16 mathematical domains Commonsense and science GAOKAO-MM (Zong & Qiu, 2024) 8 subjects, like history B.2. Reward Value in MCTS To simplify the construction of thought cards via the MCTS iteration, we avoid introducing an external reward model to score each step. Given the current skepticism regarding the self-rewarding capabilities of language models, alternative methods are necessary. Inspired by the principle that actions leading to correct answers should be rewarded more frequently, we aim to increase their likelihood of selection in future MCTS tree expansions. Following (Zhou et al., 2024; Wu et al., 2024a), we define the reward value as the likelihood (or confidence) of self-consistency via majority voting. Note that this principle applies only to leaf nodes. The Q-values of intermediate nodes are initially set to 0 and are dynamically updated during the backpropagation process, as described in Equation 1. C. More Details about Experimental Setup C.1. Benchmarks and Datasets Here are the details of the benchmarks used in our experiments. The statistics of the datasets are recorded in Table 8. Note that the evaluation metrics for all datasets are consistent with their official standards, primarily focusing on accuracy. ChartQA (Masry et al., 2022) is comprehensive benchmark for chart-based question answering that emphasizes visual and logical reasoning capabilities. The dataset comprises 9,608 human-authored questions and 23,111 machinegenerated questions derived from human-written chart summaries. It incorporates 20,882 real-world charts from diverse sources including Statista, Pew Research, Our World in Data, and OECD, spanning multiple domains and visualization styles. ChartQA addresses prior dataset limitations through its focus on human-authored questions, real-world charts, and open-vocabulary responses. We utilize this dataset to evaluate our methods performance on visual chart question-answering tasks commonly encountered in real-world scenarios. MMStar (Chen et al., 2024b) serves as vision-critical multimodal benchmark for evaluating MLLM capabilities. It addresses two fundamental limitations in existing evaluations: the redundancy of visual inputs and potential training data contamination. Comprising 1,500 meticulously curated challenge samples across six core competencies and 18 evaluation axes, the benchmark undergoes rigorous validation to ensure visual necessity and minimal data leakage. We employ this comprehensive dataset to assess our methods general reasoning capabilities. MathVista (Lu et al., 2023) is mathematical visual benchmark consisting of 6,141 examples. These examples are divided into two subsets: testmini with 1,000 annotated examples and test comprising 5,141 problems without public answers. We conduct our evaluation on the testmini subset to assess visual comprehension and compositional reasoning capabilities. MathVerse (Zhang et al., 2025) is comprehensive and specialized visual mathematics benchmark for assessing multimodal mathematical reasoning capabilities of MLLMs. The benchmark contains 2,612 visual math problems, combining 1,236 newly collected problems from public repositories with 1,376 problems from existing benchmarks. Human annotators have transformed each problem into six distinct versions-text-dominant, text-lite, text-only, visionintensive, vision-dominant, and vision-onlyeach offering different levels of multimodal information. We utilize this dataset to evaluate AStars performance across varying degrees of multimodal integration. 20 AStar: Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking MathVision (Wang et al., 2024a) constitutes meticulously compiled collection of 3,040 mathematics problems, each incorporating visual elements from authentic mathematics competitions. The dataset spans 16 mathematical domains and features five-tier difficulty classification system. We employ this dataset to evaluate our models performance on challenging reasoning tasks. GAOKAO-MM (Zong & Qiu, 2024) represents multimodal evaluation framework derived from the Chinese National College Entrance Examination (Gaokao). It encompasses eight academic subjects and includes twelve categories of images, such as diagrams, function graphs, maps, and photographs. The benchmark aims to evaluate models abilities to understand and reason over diverse multimodal content, reflecting the complexity and breadth of knowledge. We leverage this dataset to assess AStars cross-domain generalization capabilities. C.2. Baselines We evaluate AStar against four strong baseline categories: Open-source general MLLMs: Recent advances include Qwen2-VL (Wang et al., 2024b) and InternVL2 (Chen et al., 2024d), which demonstrate exceptional capabilities in complex reasoning and mathematical problem-solving. Open-source math MLLMs: Several models have been specifically optimized for mathematical reasoning in MLLMs. These models can be grouped based on their core methodologies: 1. Dataset Construction and Fine-Tuning: a. G-LLaVA (Gao et al., 2023): Introduces the Geo170K dataset, comprising over 170K geometric image-caption and question-answer pairs, to enhance MLLMs geometric problem-solving capabilities. b. Math-LLaVA (Shi et al., 2024): Develops the MathV360K dataset by collecting 40K high-quality images with question-answer pairs and synthesizing an additional 320K pairs, then fine-tunes the LLaVA-1.5 model to improve multimodal mathematical reasoning. c. MultiMath (Peng et al., 2024): Constructs the MultiMath-300K dataset, encompassing K-12 level mathematical problems with image captions and step-wise solutions, and trains the MultiMath-7B model to bridge visual and mathematical reasoning. 2. Progressive Alignment and Curriculum Learning: a. Math-PUMA (Zhuang et al., 2024): Proposes three-stage Progressive Upward Multimodal Alignment methodology to enhance MLLMs mathematical reasoning skills, focusing on aligning visual and textual modalities through structured training process. b. LlamaV-o1 (Thawakar et al., 2025): Introduces multimodal visual reasoning model trained using multi-step curriculum learning approach, organizing tasks progressively to facilitate incremental skill acquisition and problemsolving. 3. CoT Reasoning Integration: a. LLaVA-CoT (Xu et al., 2024): Develops vision-language model designed to conduct autonomous multistage reasoning, employing structured approach that includes summarization, visual interpretation, logical reasoning, and conclusion generation. b. URSA (Luo et al., 2025): Proposes three-module synthesis strategy integrating CoT distillation, trajectoryformat rewriting, and format unification, resulting in the MMathCoT-1M dataset and the URSA-7B model, which demonstrates state-of-the-art performance on multiple multimodal mathematical benchmarks. 4. Large-Scale Multimodal Pre-Training: a. InfiMM-Math (Han et al., 2024): Introduces InfiMM-WebMath-40B, high-quality dataset comprising 24 million web pages, 85 million associated image URLs, and 40 billion text tokens, aiming to improve MLLMs mathematical reasoning through large-scale multimodal pre-training. 21 AStar: Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking 5. Slow-Thinking System Implementation: a. Virgo (Du et al., 2025): Explores straightforward approach to implementing multimodal slow-thinking systems by fine-tuning capable MLLM with small amount of textual long-form thought data, demonstrating that such reasoning processes can be effectively transferred to MLLMs. Advanced closed-source MLLMs: Leading proprietary models including GPT-4V (OpenAI, 2023), GPT-4o (OpenAI, 2024), and Gemini-1.5-Pro (Team et al., 2023) demonstrate exceptional capabilities in multimodal understanding and task-solving, setting benchmarks for open-source alternatives. Multimodal tree-based methods: Recent works incorporate explicit search mechanisms into multimodal reasoning. AR-MCTS (Dong et al., 2024a) enhances reasoning by combining Monte Carlo Tree Search (MCTS) with active retrieval, improving reasoning space diversity and reliability. Mulberry (Yao et al., 2024) leverages multi-model collaboration through MCTSs four iterative operations (selection, expansion, simulation, backpropagation) to identify optimal reasoning paths. C.3. Implementation Details We utilize the vLLM framework2 with the following parameters: temperature set to 0.8, top set to 0.9, and max tokens set to 1024. All experiments were conducted on machine running Ubuntu 22.04, equipped with NVIDIA A100-80GB GPUs. We list all hyperparameters in Table 9. Table 9. All hyperparameters utilized in this paper. Hyperparameter Value Description temperature top-p top-k repetition penalty max tokens maximum tree depth dmax exploration weight predefined terminal threshold balance factor 0.8 0.9 40 1.0 1024 5 2.0 0.90 0.95 vllm inference settings MCTS VOC-based optimal path selection in Sec. 3.2 Phase 2 In the MCTS-powered prior thought card construction stage, we implement an early termination strategy based on selfconsistency (Wang et al., 2023) for enhanced efficiency. Building on the observation that repeated action sampling at the same state often indicates successful task completion, we terminate simulation early when the models consistency score exceeds threshold (i.e., SC(s) > c). In the inference stage, we evaluate AStars effectiveness across both LLM and MLLM architectures. For MLLMs, we can directly utilize their native visual understanding and reasoning capabilities for visual parsing. For traditional LLMs like Qwen2.5-7B, we employ Qwen2-72B-VL solely for visual information extraction, deliberately avoiding its visual reasoning capabilities. This design choice ensures that the inference backbone remains the primary reasoning component while maintaining computational efficiency. D. Supplementary Results This section presents supplementary results and analysis, including: D.1 Detailed Results on Multimodal Reasoning Benchmarks, D.2 Comparison with Strong Baselines, D.3 Integration with SFT, and D.4 Weak-to-Strong Generalization. 2https://github.com/vllm-project/vllm 22 AStar: Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking D.1. Detailed Results on Multimodal Reasoning Benchmarks We provide detailed test results based on various mathematical abilities using the MathVista benchmark. As shown in Table 10, AStar demonstrates notable strengths in statistics and challenging logic, whereas other models exhibit superior performance in algebraic and geometric problem-solving. Notably, with Qwen2-VL-2B as our inference backbone, our 2B model even surpasses larger models such as InternLM-XComposer2-VL-7B and Math-LLaVA, achieving performance comparable to GPT-4V. This indicates that, regardless of model size, our AStar reasoning framework effectively enhances multimodal reasoning capabilities. Table 10. Results on MathVista testmini detailed mathematics capabilities. The best results of closed-source MLLMs are highlighted in green. The best and second-best results of open-source MLLMs are highlighted in red and blue respectively. Model #Params ALL ALG ARI GEO LOG NUM SCI STA Baselines Random Choice Human Performance - - 17.9 60. 25.8 50.9 13.8 59.2 22.7 51.4 13.4 40.7 8.8 53.8 15.8 64. 14.3 63.9 Closed-source MLLMs Qwen-VL-Plus (Bai et al., 2023) GPT-4V (OpenAI, 2023) - - 43.3 49.9 39.1 53. 32.0 49.0 39.3 51.0 18.9 21.6 26.4 20.1 59.0 63.1 56.1 55. Open-source Genreral MLLMs mPLUG-Owl2-7B (Ye et al., 2023) LLaVA-1.5-13B (Liu et al., 2024b) MiniGPT-v2 (Chen et al., 2023) InternLM-XComposer2-VL-7B (Dong et al., 2024c) SPHINX-MoE (Lin et al., 2023) DeepSeek-VL (Lu et al., 2024) InternVL2-8B (Chen et al., 2024d) Qwen2-VL (Wang et al., 2024b) 7B 13B 7B 7B 8 7B 7B 8B 7B 22.2 25.7 23.1 47.8 42.3 34.9 58.3 58.9 23.6 19.6 28.1 32.0 31.7 29.2 59.8 44.1 19.2 28.6 21.0 51.6 41.6 38.8 56.4 57. Open-source Math MLLMs (Large-Scale Training) G-LLaVA (Gao et al., 2024) Math-LLaVA (Shi et al., 2024) Multimath-7B (Peng et al., 2024) Math-PUMA-Qwen2-7B (Zhuang et al., 2024) URSA-7B (Luo et al., 2025) AStar (Ours, Training-free Reasoning) 7B 13B 7B 7B 7B 7B 2B 25.1 46.6 50.0 47.9 59. 63.5 49.6 36.0 51.5 61.9 47.7 74.0 69.0 51.0 19.4 40.7 42.2 46.2 53.5 63.1 49.9 23.9 17.6 24.7 30.5 30.5 27.2 60.3 43. 37.6 56.2 64.9 47.3 77.4 71.7 53.1 13.5 10.8 16.2 13.5 16.2 18.9 10.8 24.3 15.2 23.3 23.3 21.6 21.6 61.3 34.7 12.7 27.8 16.7 43.8 27.1 43.1 30.6 41. 17.7 34.7 32.6 32.6 35.4 60.0 46.8 26.3 33.6 25.4 37.7 50.8 35.3 59.0 66.4 21.0 47.7 42.6 42.6 58.2 48.2 41.3 21.4 22.9 17.9 62.8 50.8 33.2 68.8 75. 15.1 42.3 49.2 55.8 57.1 68.8 54.3 D.2. Comparison with Strong Baselines Table 11 provides performance comparison of our method against leading open-source and closed-source models. AStar with 7B inference backbone achieves competitive performance with larger MLLMs. Notably, on the challenging MathVision dataset, our approach exhibits substantial improvements. This suggests that while simpler visual tasks may not significantly benefit from structured reasoning strategieswith performance primarily determined by model capacityour method shows increasing performance advantages on more complex datasets like MathVerse and MathVision, surpassing both InternVL2.5-26B and InternVL2-Llama3-76B models despite their larger parameter counts of 26B and 72B respectively. We also benchmark our approach against recent powerful multimodal reasoning methods, including LLaVA-CoT (Xu et al., 2024) and Virgo (Du et al., 2025). As shown in Table 12, comparative experiments across different model scales demonstrate that our AStar reasoning framework, which effectively integrates MLLMs inherent reasoning capabilities with external reasoning guidelines, achieves superior performance with minimal prior data. On challenging datasets like MathVerse, our method exhibits substantial improvements, with the 2B model achieving comparable performance to the extensively trained Virgo 7B model. Further analysis reveals two key insights: First, methods relying on vast solution spaces often struggle to identify appropriate reasoning paths, analogous to finding needle in haystack. Second, approaches dependent on AStar: Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking large-scale training data typically face difficulties in fully capturing complex long-chain reasoning patterns through implicit learning. In contrast, our method adaptively identifies suitable reasoning strategies based on problem complexity, enabling efficient inference across diverse scenarios. Table 11. Comparison with leading LLMs. The best results are highlighted in bold. Results for off-the-shelf models are sourced from corresponding official websites."
        },
        {
            "title": "MathVista MathVerse MathVision Average",
            "content": "Closed-Source Models GPT-4o-0513 (OpenAI, 2024) GPT-4V (OpenAI, 2023) Gemini-1.5-Pro (Team et al., 2023) Claude-3.5-Sonnet (Anthropic, 2024) Qwen-VL-Plus (Bai et al., 2023) 63.8 49.9 63.9 67.7 43.3 Open-Source Models Qwen2-VL-72B (Wang et al., 2024b) LLaVA-OneVision-72B (Li et al., 2024) InternVL2.5-26B (Chen et al., 2024d) InternVL2-Llama3-76B (Chen et al., 2024d) Astar (Ours) 70.5 67.5 67.7 65.5 63.5 50.2 54.4 35.3 - 21.3 - 39.1 40.1 42.8 54.0 30.4 24.8 19.2 - 10. 25.9 - 28.0 23.7 32.4 48.2 43.1 39.5 - 25.2 - - 45.3 44.0 50.0 Table 12. Comparison with recent works targeting enhanced multimodal reasoning through structured thinking. We list 2B and 7B-scale baselines. The best results in each box are highlighted in bold. Our method demonstrates significant performance improvements. Model Size MMStar ChartQA MathVista MathVerse Mulberry (Yao et al., 2024) Astar (Ours) 2B-Scale Baselines 2B 2B 51.3 51. 7B-Scale Baselines Insight-V (Dong et al., 2024d) AR-MCTS (Dong et al., 2024a) Mulberry (Yao et al., 2024) LLaVA-CoT (OpenAI, 2024) LlamaV-o1 (Thawakar et al., 2025) URSA (Luo et al., 2025) Virgo (Luo et al., 2025) Astar (Ours) 7B 7B 7B 11B 11B 7B 7B 7B 61.5 - 61.3 57.6 59.6 - - 61.7 77.7 78.3 81.5 - 83.9 - - - - 83. 51.7 49.6 59.9 64.1 63.1 54.8 54.4 59.8 - 63.5 - 33.7 - - - - - 45.7 37.5 54.0 D.3. Integration with SFT To evaluate the versatility of the AStar framework, we conducted experiments on both pretrained base models and their supervised fine-tuning (SFT) counterparts. Table 13 presents comprehensive comparison of model performance across different configurations. Our results demonstrate that the AStar framework consistently yields substantial improvements over the zero-shot performance reported in the original papers, regardless of whether it is applied to pretrained or SFT models. 24 AStar: Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking Table 13. Integration With SFT on MathVision. We evaluate the AStar framework using Qwen2-VL-2B and Qwen2-VL-7B as backbone models, comparing both pre-trained base models and SFT variants across different difficulty levels."
        },
        {
            "title": "Backbone",
            "content": "1 2 3"
        },
        {
            "title": "Average",
            "content": "Qwen2-VL-2B-Base Qwen2-VL-2B Qwen2-VL-7B-Base Qwen2-VL-7B 20.8 25.0 26.8 26.5 20.7 20.0 20.6 21. 21.4 15.6 23.7 26.2 17.8 16.7 26.7 29.8 10.2 17.6 22.4 23. 18.2 22.3 23.4 25.2 Notably, the combination of SFT and AStar achieves superior performance, suggesting synergistic effect between supervised fine-tuning and our framework. For instance, with Qwen2-VL-2B as the backbone, the SFT variant (Qwen2-VL2B-Instruct) achieves better average performance (22.3% vs 18.2%) and shows particular strength in handling problems at difficulty levels 1 and 5, where it achieves 25.0% and 17.6% accuracy, respectively. Similarly, the Qwen2-VL-7B model with AStar demonstrates robust performance across all difficulty levels, achieving an average accuracy of 25.2%. These findings suggest promising future research directions for further integrating SFT techniques into the AStar framework to enhance overall system performance. D.4. Weak-to-Strong Generalization As described in (Bansal et al., 2024; Yang et al., 2024), interactions between weak and strong models can be categorized into three primary paradigms: 1) weak-to-strong improvement, where models with limited capabilities can effectively guide the development of more advanced models, 2) self-improvement, wherein the weak and strong models are identical, focusing on designing methods to enhance the models own performance, and 3) knowledge distillation, which involves transferring the capabilities or knowledge from strong models to weak models. Through our main results, where thought cards are constructed based on Qwen2-VL-7B, we have already demonstrated the significant potential of our AStar method in knowledge distillation (Qwen2-VL-7BQwen2-VL-2B) and self-improvement (Qwen2-VL-7BQwen2-VL-7B). Therefore, in this section, we empirically evaluate AStars effectiveness in weak-to-strong generalization. We hypothesize that weaker model, when integrated with AStar, can effectively guide more powerful policy model. To test this hypothesis, we leverage the reasoning guidance (thought cards) generated by Qwen2-VL-7B to assist GPT-4o in solving challenging problems from the MathVision dataset. As illustrated in Figure 11, we present comprehensive comparison between GPT-4os zero-shot performance and its performance when enhanced with AStar (using Qwen2-VL-7Bs reasoning guidance) across multiple dimensions of the MathVision benchmark. Our results demonstrate that incorporating our reasoning paradigm enables models with limited capabilities to provide valuable supervision to stronger policy models. Notably, the Qwen2-VL-7B model, despite its relatively limited capabilities, successfully guides the more powerful, closed-source GPT-4o. This finding validates the potential of learning from prior reasoning patterns and offers promising insights for developing more scalable and sophisticated strategies to enhance AI reasoning capabilities. E. Case Study Taking geometry problem in MathVerse as an example, we provide qualitative comparison of LLaVA-NeXT-8B (Liu et al., 2024b), Qwen2-VL-7B (Wang et al., 2024b), and AStar-7B in Figure 12. The results demonstrate that LLaVA-NeXT-8B and Qwen2-VL-7B struggle to accurately parse complex geometric relationships, leading to potential errors in each step of their reasoning processes. In contrast, our AStar framework effectively combines MLLMs internal implicit reasoning capabilities with external explicit reasoning guidelines. This integration exhibits explicit and well-defined reasoning steps with comprehensive understanding, ultimately arriving at the correct solution. 25 AStar: Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking Figure 11. Weak-to-strong generalization results: Performance comparison of GPT-4o in zero-shot and AStar-enhanced (Qwen2-VL-7B guidance) settings across various dimensions of MathVision. The results demonstrate that weaker model (Qwen2-VL-7B) can effectively guide stronger model (GPT-4o) through the AStar framework, validating the frameworks potential for weak-to-strong generalization in mathematical reasoning tasks. Figure 12. Qualitative comparison of reasoning processes across different models. AStar demonstrates superior understanding through its explicit, well-structured reasoning steps, leading to accurate solution derivation. The comparison highlights AStars ability to systematically decompose complex geometric relationships while maintaining reasoning clarity throughout the problem-solving process."
        }
    ],
    "affiliations": [
        "Beijing",
        "Department of Automation, Tsinghua University, Beijing, China"
    ]
}