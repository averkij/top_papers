{
    "paper_title": "R1-Zero's \"Aha Moment\" in Visual Reasoning on a 2B Non-SFT Model",
    "authors": [
        "Hengguang Zhou",
        "Xirui Li",
        "Ruochen Wang",
        "Minhao Cheng",
        "Tianyi Zhou",
        "Cho-Jui Hsieh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently DeepSeek R1 demonstrated how reinforcement learning with simple rule-based incentives can enable autonomous development of complex reasoning in large language models, characterized by the \"aha moment\", in which the model manifest self-reflection and increased response length during training. However, attempts to extend this success to multimodal reasoning often failed to reproduce these key characteristics. In this report, we present the first successful replication of these emergent characteristics for multimodal reasoning on only a non-SFT 2B model. Starting with Qwen2-VL-2B and applying reinforcement learning directly on the SAT dataset, our model achieves 59.47% accuracy on CVBench, outperforming the base model by approximately ~30% and exceeding both SFT setting by ~2%. In addition, we share our failed attempts and insights in attempting to achieve R1-like reasoning using RL with instruct models. aiming to shed light on the challenges involved. Our key observations include: (1) applying RL on instruct model often results in trivial reasoning trajectories, and (2) naive length reward are ineffective in eliciting reasoning capabilities. The project code is available at https://github.com/turningpoint-ai/VisualThinker-R1-Zero"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 2 3 1 5 0 . 3 0 5 2 : r R1-Zeros Aha Moment in Visual Reasoning on 2B Non-SFT Model Hengguang Zhou* University of California, LA Xirui Li* University of California, LA Ruochen Wang University of California, LA Minhao Cheng Pennsylvania State University Tianyi Zhou University of Maryland Cho-Jui Hsieh University of California, LA"
        },
        {
            "title": "Abstract",
            "content": "Recently DeepSeek R1 demonstrated how reinforcement learning with simple rule-based incentives can enable autonomous development of complex reasoning in large language models, characterized by the \"aha moment\", in which the model manifest self-reflection and increased response length during training. However, attempts to extend this success to multimodal reasoning often failed to reproduce these key characteristics. In this report, we present the first successful replication of these emergent characteristics for multimodal reasoning on only non-SFT 2B model. Starting with Qwen2-VL-2B and applying reinforcement learning directly on the SAT dataset, our model achieves 59.47% accuracy on CVBench, outperforming the base model by approximately 30% and exceeding both SFT setting by 2%. In addition, we share our failed attempts and insights in attempting to achieve R1-like reasoning using RL with instruct models. aiming to shed light on the challenges involved. Our key observations include: (1) applying RL on instruct model often results in trivial reasoning trajectories, and (2) naive length reward are ineffective in eliciting reasoning capabilities. The project code is available at https://github.com/turningpoint-ai/VisualThinker-R1-Zero Work in Progress This research is still work in progress. In this report, we share our successes, failures, and insights from our preliminary exploration of potential approaches for developing R1-like reasoning in multimodal models. This is an ongoing effort and we will be committed to continuous update and improvement. This report presents only minimal version, with the full version to be released soon. Preprint. Under review. Figure 1: The training dynamics of VisualThinker-R1-Zero on Qwen2-VL-2B base model. Benchmark accuracy is measured on CV-Bench, and the average response length is calculated from rollouts on SAT training samples. Initially, we observed drop in length because the base model tended to generate HTML code. This behavior was quickly suppressed by RL, leading the model to adopt more appropriate output format and regular increase in response length. Afterwards, we observed multimodal aha momentthe emergence of self-reflection in models response, as described in the DeepSeek-R1 paper, followed by consistent positive correlation between response length and benchmark accuracy."
        },
        {
            "title": "Introduction",
            "content": "Recently, DeepSeek R1 [2] has demonstrated how Reinforcement Learning (RL) with simple rulebased incentives can enable large language model to build complex reasoning capabilities autonomously. key finding from this work was the emergence of advanced reasoning patterns without explicit supervisionwhat the researchers termed the \"aha moment,\" characterized by selfreflection, and spontaneous increase in response length during training as the model learned to explore increasingly sophisticated problem-solving strategies. Many researchers [1; 20; 13; 17] have attempted to extend this success to multimodal reasoning, where models process and reason on both visual and textual information. However, these efforts have primarily struggled to reproduce the key characteristics exhibited by DeepSeek R1 mentioned abovespecifically, the emergent \"aha moment\" and increased response length during reasoning. These implementations often failed to demonstrate the autonomous development of sophisticated reasoning strategies observed in DeepSeek R1s training. In this report, we present the first successful replication of these key characteristics for multimodal reasoning on only non-SFT 2B model. With both \"aha moment\" and increased length (Figure 1), our approach demonstrates that direct application of reinforcement learning on non-sft model can induce sophisticated reasoning capabilities even in smaller multimodal models without supervised fine-tuning. We start from the Qwen2-VL-2B [16] base model and directly perform reinforcement learning. Without any SFT, our model achieves 59.47% accuracy on CVBench [15], outperforming the base model by approximately 30% and exceeding the SFT model by 2%. In addition to our replication on non-sft model, we also share our insights and failed attempts in achieving R1-like reasoning using RL with instruct model. We observed that starting from supervised fine-tuned model often failed to reproduce the observations and findings reported by DeepSeek-R1. Upon investigation on this issue, we found that (1) Despite improved performance, 2 RL on instruct model leads to superficial reasoning rather than genuine problem-solving strategies, and (2) naive length reward are ineffective at inducing deeper reasoning capabilities. In summary, our key contributions are as follows: We are the first team to replicate the key characteristics of R1 success (\"aha moment\" and increased reasoning length) on multimodal reasoning tasks with non-SFT 2B model. We showed that vision-centric spatial reasoning tasks could also benefit from improved reasoning capabilities. We demonstrated that applying RL on instruction-tuned models leads to superficial reasoning. We open-sourced our project to facilitate future studies on multimodal reasoning."
        },
        {
            "title": "2 Related Works",
            "content": "2.1 Multimodal Reasoning Researchers have demonstrated that LLMs can be post-trained to elicit enhanced reasoning abilities [8]. With pre-trained visual encoders to understand visual content alongside textual data, multimodal LLMs reasoning abilities are investigated and enhancement typically requires sophisticated prompting designs [6] or large amounts of reasoning training data [14; 18]. The research community is increasingly interested on developing more natural methods to incentivize higher intelligence in models without relying on extensive supervised data or complex prompting techniques. 2.2 The \"Aha Moment\" Phenomenon in DeepSeek R1 recent breakthrough study, DeepSeek R1 [2], demonstrated that reinforcement learning can incentivize models reasoning abilities without any supervised reasoning data. Intriguingly, researchers discovered an \"aha moment\" when directly applying RL with rule-based reward on mathematical datasetsthe model autonomously developed advanced problem-solving strategies, including reflection and self-correction. We summarize the key characteristics that contributed to DeepSeek R1s success and compare them with our model and other multimodal replications in Table 1. Specifically, we highlight two emergent phenomena: the \"aha moment\" and increasing response length. The \"aha moment\" refers to the models autonomous development of advanced problem-solving strategies during training, while the increasing response length indicates the model naturally learns to allocate more thinking time for reasoning tasks. It remains questionable whether existing multimodal replications [4; 1; 3] without these key characteristics can be considered truly valid implementations of the R1 approach. Table 1: Comparison between DeepSeek R1 and its multimodal replications Feature DeepSeek R1 [2] VisualThinker R1 Zero (Ours) R1-V [1] R1-Multimodal-Journey [4] open-r1-multimodal [3] Base Model Modality Aha Moment Response Length Dynamics DeepSeek V3 Language Yes Qwen2-VL-2B Vision + Language Yes Qwen2-VL-2B-Instruct Vision + Language No Qwen2-VL-2B-Instruct Vision + Language Yes Qwen2-VL-2B/7B-Instruct Vision + Language No"
        },
        {
            "title": "3 VisualThinker R1 Zero",
            "content": "In this part, we demonstrate VisualThinker-R1-Zero have showed an emergence of aha moment by directly applying RL training to the non-SFT base model, leading to superior performance on vision-centric tasks on non-SFT 2B models. Base Model Our method builds on Qwen-2-VL-2B [16] as the base model, applying GRPO [12] with tailored chat template and prompting strategy to enhance its reasoning capabilities. We posit that applying GRPO to the base model may be more efficient and effective way to replicate multimodal R1 reasoning compared to training instruct fine-tuned model. As we will show in Section 5, the instruction-tuned variant, Qwen-2-VL-2B-Instruct, demands significantly more training 3 resources as base model yet struggles to replicate the emergent \"aha moment\" observed in DeepSeek R1 with notable failure modes. Training Recipe We train the model directly on the SAT [10] dataset to let the base model to explore various spatial reasoning for each question in the dataset Q, we apply the following chat template:"
        },
        {
            "title": "Prompt Template",
            "content": "A conversation between User and Assistant. The user asks question about the image, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer.n User: {QUESTION} Assistant: Let me solve this step by step.n <think> For each of the question in the dataset, the model generate an response with this prompt template and it is then optimized using RL objective. RL algorithm Existing repos applying RL on top of fine-tuned visual models failed to replicate DeepSeek r1s key characteristics. In contrast, we witness prolonged reasoning trajectory and aha moment with an overlooked approach that directly applies GRPO [12] to an non-SFT model. Our findings suggest that this setting is the key to TRUE aha moment in multimodal reasoning. Now we briefly review the GRPO algorithm we adopted for RL training. To ease the burden of training an additional value function approximation model employed by PPO [11], GRPO adopted the average reward of sampled response of the policy model as the baseline in computing the advantage. Specifically, given the an input question q, we first sample group of responses {o1, o2, , oG} and compute corresponding rewards {r1, r2, , rG} with the reward model. The advantage is then computed as: ˆAi,t = (cid:101)ri = ri mean(r) std(r) The policy model is then optimized by maximizing the following KL objective: JGRP O(θ) = qP (Q),{oi}G i=1πθold (Oq) (cid:34) 1 G (cid:88) i=1 1 oi clip (cid:18) πθ(oi,tq, oi,<t) πθold(oi,tq, oi,<t) (cid:19) , 1 ϵ, 1 + ϵ (cid:40) oi (cid:88) (cid:34) min t=1 (cid:35) πθ(oi,tq, oi,<t) πθold(oi,tq, oi,<t) ˆAi,t, (cid:41)(cid:35) ˆAi,t βDKL [πθπref ] (1) (2) where πθ and πold are the current and old policy, and ϵ and β are hyper-parameters introduced in PPO. Reward Modeling Following DeepSeek-R1, our RL approach remains elegant, avoiding the use of reward models [9] or Monte Carlo tree search (MCTS)-like techniques [19]. Specifically, we employ rule-based reward function that evaluates responses based on their format and correctness: If the response provides final answer and is correct, the model receives an accuracy reward of +1. If the response encloses its thinking in <think></think> and the final answer in <answer></answer> tags, the model receives format reward of +1. otherwise, the model receives 0 reward. Our implementation is based on DeepSeek R1s report. Initial experiments suggest that this reward function helps the policy model quickly converge towards generating responses in the desired format."
        },
        {
            "title": "4 Experiments",
            "content": "In this study, we demonstrate that small non-sft model could solve vision-centric spatial reasoning task with our method, type of benchmark often pose challenges to even larger models. We train our models on SAT [10], VQA dataset comprising 218k question-answer pairs synthesized using photo-realistic physics engine to enhance spatial intelligence. Our training focuses on the static subset, which includes questions on relative spatial relationships, relative depth, and object counting. 4.1 Evaluation To test the generalization of our method, we evaluate on CVBench [15], realistic vision-centric benchmark consists of 2,638 examples repurposed from standard vision datasets on fundamental 2D and 3D reasoning tasks. CVBench formulates natural language questions to assess spatial relationships, object counting, depth ordering, and relative distance. This setup allows us to systematically examine how well our recipe could improve spatial reasoning capability. 4.2 Implementation Details We conduct all our experiments using four NVIDIA H100 GPUs (80GB each), setting the batch size to 1 per device. The model is trained for 1500 steps with learning rate of 1 106 and temperature of 1.0. We found long response length is must for observing increasing response length during training, so we set the maximum response length as 700. During GRPO optimization, we sample 8 responses per step and apply KL coefficient of 0.04. Table 2: Hyper-parameters of VisualThinker R1 zero GRPO training. Setting Batch Size per Device Gradient Accumulation Steps Training Steps Learning Rate Temperature Maximum Response Length Number of Responses per GRPO Step KL Coefficient Value 1 2 1500 1 106 1.0 700 8 0.04 To demonstrate the superior effectiveness of directly applying GRPO on base model, we compare our method with the non-sft pre-trained mode as the baseline, and compare our method against the base model SFT on the same SAT dataset. 4.3 Main Results Benchmark Results In the experiments, we fine-tuned the Qwen2-VL-2B non-sft model and evaluated its performance on CV-Bench. During training, we can observe the model autonomously develop increasing response length and the performance comes with it in Figure 1 on CV-Bench. We can also observe that directly applying RL on base model has superior performance compared to SFT method in Figure 2. In addition, we tested our method on various spatial reasoning dataset including BLINK [5]1 and VSR [7], illustrated in Table 3. Our method demonstrated improved performance over the Qwen2VL-2B (base) by 30%, and the Qwen2-VL-2B SFT(base + SFT) by 2% on CV-Bench. We also achieve superior performance on the BLINK and VSR benchmark, with our method achieves around 27% advantage comparing against the model trained with SFT. This suggests that visual reasoning could significantly benefit from R1-Zero training, demonstrating more scalable training through RLs exploration of diverse reasoning. 1Following SAT [10] evaluation, we use two spatial splits of BLINK - Multiview reasoning, Relative Depth, and Spatial Relations. 5 Figure 2: Comparison between RL and SFT training. Our method achieves significant improvement over the base model and the instruction fine-tuned model. Specifically, Qwen2-VL-2B + R1 outperforms Qwen2-VL-2B (base model) by approximately 30%, Qwen2-VL-2B-Instruct (instruction fine-tuned model) by 5%, and Qwen2-VL-2B SFT (base model + SFT) by 2%. Table 3: Results on vision-centric benchmarks. Table shows RL training on base model has overall better performance over SFT training and the base model. Model CV-Bench BLINK VSR Qwen2-VL-2B Qwen2-VL-2B + SFT Qwen2-VL-2B + GRPO (Ours) 54.69 60.02 59.64 22.46 68.92 66. 0.16 55.00 54.16 31.66 45.83 56.66 31.38 57.84 59.47 13.70 58.06 50.80 0.69 47.55 55.94 6.74 52.43 53. 0.0 35.80 62.32 Count Acc(%) Relation Acc(%) Depth Acc(%) Distance Acc(%) Total Acc(%) Relative Depth Acc(%) Spatial Relation Acc(%) Average Acc(%) Average Acc(%) Multimodal Aha Moment particularly intriguing phenomenon observed during the training of DeepSeek-R1-Zero is the occurrence of an aha moment: ... **Wait, wait. Wait. Thats an aha moment can flag here.** Lets reevaluate this step-by-step to identify if the correct sum can be... We started with the equation: ... This aha moment indicates DeepSeek-R1-Zero spontaneously builds reasoning strategy rethinking its initial approach for improved reasoning capability. Beyond strong performance gain, we also observed our model exhibit very interesting \"aha moment\" aligned with the finding in DeepSeek R1: during training, the model spontaneously revisit its previous judgment and explore alternative options: ... Therefore, dark brown wooden bed with white blanket is not above the doorway. **But wait! can think of something else.** <think> ..."
        },
        {
            "title": "5 Challenges of Applying RL to Supervised Fine-Tuned Models",
            "content": "Starting from scratch with non-SFT multimodal model, we have demonstrated that RL empowers VisualThinker-R1-Zero to attain robust reasoning capabilities without any supervised fine-tuning data. In addition to this replication, one might be inclined to apply RL directly to supervised fine-tuned models given its stronger instruction following capability. In this section, we share our failed cases on applying GRPO algorithm described in Section 3 to the Qwen2-VL-2B-Instruct model (a supervised fine-tuned Qwen VL model) to provide insights that may benefit future research. However, note that this does not imply that these approaches are incapable of building effective visual reasoning models. Specifically, we found that starting from supervised"
        },
        {
            "title": "Trivial Reasoning Trajectory",
            "content": "Question: Which object is closer to the camera taking this photo, the box (highlighted by red box) or the desk (highlighted by blue box)? Response: <think>To determine which object is closer to the camera, will measure the distance from the camera to each object.</think> <answer>box</answer> Figure 3: Example response of applying RL to supervised fine-tuned models. fine-tuned model exposes problems of trival reasoning patterns. We then demonstrate our findings in (1) investigating the reason and implication of this phenomenon and (2) attempting to incentivize sophisticated reasoning. 5.1 Emergence of Trivial Reasoning Patterns Applying RL on an SFT model does improve the models performance on CVBench  (Table 4)  . However, it is questionable whether this method incentivizes the models to achieve higher intelligence, as Figure 3 demonstrates the degeneration of model responses into meaningless or trivial reasoning patterns. The observed reasoning trajectory follows structure: trivial and generic strategy within <think></think> tags followed by question-specific answer between <answer></answer> tags. One hypothesis is that the model enhances its performance during RL training without necessarily developing genuine reasoning capabilities. 5.2 Preliminary Investigation of Trivial Reasoning Trajectories Given the trivial reasoning trajectories, nature question is whether performance improvements occur through enhancement of the vision encoder during training. We hypothesized that when we freeze the vision encoder during RL, the model may focus more on developing sophisticated reasoning strategies; conversely, when we freeze the LLM during RL, the performance would be the same level as the full fine-tuning setting. Surprisingly, results in Table 5 demonstrate that both approaches achieve greater improvement than the vanilla implementation yet they still generate short and trivial responses. This observation suggests the complexity of training dynamics of RL on multimodal models and further analysis is needed to better understand the phenomenon. Table 4: Evaluation of applying RL to instruct models. Prompting with only <answer> achieves superior performance in the context of applying RL on instruct models. Prompting Strategy Total Acc (%) Count Acc (%) Relation Acc (%) Depth Acc (%) Distance Acc (%) Instruct Model Instruct Model + RL - with reason 55.64 66. 45.43 69.54 68.92 61.84 58.66 66.50 51.66 65.50 Table 5: Evaluation of applying RL to instruct models with different freezing components. Either freezing language or vision components of instruct model can both leads to performance improvement. Freezing Components Total Acc (%) Count Acc (%) Relation Acc (%) Depth Acc (%) Distance Acc (%) Instruct model + RL Instruct model + RL Instruct model + RL - vision encoder LLM 63.57 68.34 65.35 61.68 64.21 62.05 71.08 78.00 72.46 61.50 65.67 65. 60.00 66.00 62.17 5.3 Failed Attempts with Length Reward The key characteristics of DeepSeek R1s success is its increased response length during training. After observing our failure to replicate DeepSeek R1s reasoning capabilities when starting from instruction-tuned models, we investigated whether we could encourage more sophisticated reasoning patterns by directly rewarding longer response. 7 Figure 4: Response length across training steps for different fine-tuning settings during RL. The x-axis represents training steps, while the y-axis shows the response length. Models with different fine-tuning configurations are compared: Freeze LLM (green), Freeze Vision Encoder (blue), and Full Finetune (red). The response length drops significantly in the early training phase and stabilizes over time. However, despite improved accuracy, all three RL-based fine-tuning on Instruct Model does not necessarily enhance reasoning capabilities, as responses tend to remain short and trivial Figure 5: Model performance during training with length-rewarded RL on instruction ginetuned models. Despite implementing length rewards to encourage longer responses, model performance shows no improvement in reasoning quality as response length increases. While average response length (red line) grows substantially, benchmark accuracy (green line) remains relatively stable, suggesting that longer responses do not necessarily translate to enhanced reasoning capabilities. Length Reward Hacking Example Question: Considering the relative positions of the giraffe (annotated by the red box) and the person in the image provided, where is the giraffe (annotated by the red box) located with respect to the person? Model Response: <think> - The giraffe (annotated by the red box) is located in the foreground of the image. - The person is in the background, standing behind the giraffe. - The giraffe is closer to the viewer than the person. - The person is not directly above the giraffe. - The person is not directly below the giraffe. - The giraffe is not above the person. - The giraffe is not below the person. - The person is not in front of the giraffe. - ... [Repeats until it reaches max_new_tokens limit] ... </think> Figure 6: Response example of model hacking naive length reward. Model hacks reward by repeating trivial response. Thus, we implemented length-based reward mechanism: in addition to the vanilla accuracy and format rewards described in Section 3, we added an auxiliary reward of +0.001 for each additional token generated. However, as shown in Figure 5, naively rewarding lengthy responses does not 8 improve model performance and often leads to reward hacking behaviors, with models generating extremely long yet meaningless content as illustrated in Figure 6. This finding suggests that the emergent reasoning capabilities observed in DeepSeek R1 cannot be easily achieved through enforcing long response length during the training of instruction-tuned models."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper presents VisualThinker R1 Zero, the first successful multimodal replication of DeepSeek R1s emergent reasoning characteristics. By applying reinforcement learning directly to non-finetuned Qwen2-VL-2B model, we observed both the \"aha moment\" and increased response length during trainingkey indicators of autonomous reasoning development. Empirically, our approach achieved 59.47% accuracy on CVBench, outperforming both base and instruction-tuned models without any supervised fine-tuning. We also share our insights and failed attempts in achieving R1-like reasoning using RL with instruct model: applying RL to supervised fine-tuned models leads to trivial reasoning trajectories rather than genuine problem-solving strategies. This report is work in progress for presenting our preliminary findings, we plan to release further updates with deeper investigations and insights to continuously expand and refine this report with our ongoing exploration of the technical roadmap for realizing R1-like multimodal reasoning."
        },
        {
            "title": "References",
            "content": "[1] Liang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci. R1-v: Reinforcing super generalization ability in vision-language models with less than $3. https://github.com/Deep-Agent/ R1-V, 2025. Accessed: 2025-02-02. [2] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. [3] EvolvingLMMs-Lab. open-r1-multimodal. https://github.com/EvolvingLMMs-Lab/ open-r1-multimodal, 2025. Accessed: March 6, 2025. 9 [4] FanqingM. R1-multimodal-journey. https://github.com/FanqingM/ R1-Multimodal-Journey, 2025. Accessed: March 6, 2025. [5] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A. Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive, 2024. [6] Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. Advances in Neural Information Processing Systems, 37:139348139379, 2025. [7] Fangyu Liu, Guy Emerson, and Nigel Collier. Visual spatial reasoning, 2023. [8] OpenAI. Learning to reason with llms, 2024. [9] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. [10] Arijit Ray, Jiafei Duan, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan Plummer, Ranjay Krishna, Kuo-Hao Zeng, et al. Sat: Spatial aptitude training for multimodal language models. arXiv preprint arXiv:2412.07755, 2024. [11] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. [12] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [13] Haozhan Shen, Zilun Zhang, Qianqian Zhang, Ruochen Xu, and Tiancheng Zhao. Vlm-r1: stable and generalizable r1-style large vision-language model. https://github.com/ om-ai-lab/VLM-R1, 2025. Accessed: 2025-02-15. [14] Omkar Thawakar, Dinura Dissanayake, Ketan More, Ritesh Thawkar, Ahmed Heakl, Noor Ahsan, Yuhao Li, Mohammed Zumri, Jean Lahoud, Rao Muhammad Anwer, et al. Llamav-o1: Rethinking step-by-step visual reasoning in llms. arXiv preprint arXiv:2501.06186, 2025. [15] Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. Advances in Neural Information Processing Systems, 37:8731087356, 2025. [16] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing visionlanguage models perception of the world at any resolution, 2024. [17] Xiaodong Wang and Peixi Peng. Open-r1-video. https://github.com/ Wang-Xiaodong1899/Open-R1-Video, 2025. [18] Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. Llava-o1: Let vision language models reason step-by-step. arXiv preprint arXiv:2411.10440, 2024. [19] Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. Rest-mcts*: Llm self-training via process reward guided tree search, 2024. [20] Yaowei Zheng, Junting Lu, Shenzhi Wang, Zhangchi Feng andDongdong Kuang, and Yuwen Xiong. Easyr1: An efficient, scalable, multi-modality rl training framework. https://github. com/hiyouga/EasyR1, 2025."
        }
    ],
    "affiliations": [
        "Pennsylvania State University",
        "University of California, LA",
        "University of Maryland"
    ]
}