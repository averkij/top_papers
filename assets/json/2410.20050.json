{
    "paper_title": "AutoMIR: Effective Zero-Shot Medical Information Retrieval without Relevance Labels",
    "authors": [
        "Lei Li",
        "Xiangxu Zhang",
        "Xiao Zhou",
        "Zheng Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Medical information retrieval (MIR) is essential for retrieving relevant medical knowledge from diverse sources, including electronic health records, scientific literature, and medical databases. However, achieving effective zero-shot dense retrieval in the medical domain poses substantial challenges due to the lack of relevance-labeled data. In this paper, we introduce a novel approach called Self-Learning Hypothetical Document Embeddings (SL-HyDE) to tackle this issue. SL-HyDE leverages large language models (LLMs) as generators to generate hypothetical documents based on a given query. These generated documents encapsulate key medical context, guiding a dense retriever in identifying the most relevant documents. The self-learning framework progressively refines both pseudo-document generation and retrieval, utilizing unlabeled medical corpora without requiring any relevance-labeled data. Additionally, we present the Chinese Medical Information Retrieval Benchmark (CMIRB), a comprehensive evaluation framework grounded in real-world medical scenarios, encompassing five tasks and ten datasets. By benchmarking ten models on CMIRB, we establish a rigorous standard for evaluating medical information retrieval systems. Experimental results demonstrate that SL-HyDE significantly surpasses existing methods in retrieval accuracy while showcasing strong generalization and scalability across various LLM and retriever configurations. CMIRB data and evaluation code are publicly available at: https://github.com/CMIRB-benchmark/CMIRB."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 6 2 ] . [ 1 0 5 0 0 2 . 0 1 4 2 : r AutoMIR: Effective Zero-Shot Medical Information Retrieval without Relevance Labels Lei Li Gaoling School of Artificial Intelligence, Renmin University of China Beijing, China leil@ruc.edu.cn Xiao Zhou Gaoling School of Artificial Intelligence, Renmin University of China Beijing, China xiaozhou@ruc.edu.cn ABSTRACT Medical information retrieval (MIR) is essential for retrieving relevant medical knowledge from diverse sources, including electronic health records, scientific literature, and medical databases. However, achieving effective zero-shot dense retrieval in the medical domain poses substantial challenges due to the lack of relevancelabeled data. In this paper, we introduce novel approach called Self-Learning Hypothetical Document Embeddings (SL-HyDE) to tackle this issue. SL-HyDE leverages large language models (LLMs) as generators to generate hypothetical documents based on given query. These generated documents encapsulate key medical context, guiding dense retriever in identifying the most relevant documents. The self-learning framework progressively refines both pseudo-document generation and retrieval, utilizing unlabeled medical corpora without requiring any relevance-labeled data. Additionally, we present the Chinese Medical Information Retrieval Benchmark (CMIRB), comprehensive evaluation framework grounded in real-world medical scenarios, encompassing five tasks and ten datasets. By benchmarking ten models on CMIRB, we establish rigorous standard for evaluating medical information retrieval systems. Experimental results demonstrate that SL-HyDE significantly surpasses existing methods in retrieval accuracy while showcasing strong generalization and scalability across various LLM and retriever configurations. CMIRB data and evaluation code are publicly available at: https://github.com/CMIRB-benchmark/CMIRB CCS CONCEPTS Information systems Retrieval models and ranking. KEYWORDS Medical Information Retrieval, Large Language Models, Zero-shot Retrieval"
        },
        {
            "title": "1 INTRODUCTION\nMedical information retrieval (MIR) [7, 22] focuses on retrieving\nrelevant medical information from extensive data sources, such as\nelectronic health records, scientific papers, and medical knowledge\ndatabases, based on specific medical queries. Its applications are",
            "content": "Xiao Zhou and Zheng Liu are corresponding authors. Xiangxu Zhang Gaoling School of Artificial Intelligence, Renmin University of China Beijing, China xansar@ruc.edu.cn Zheng Liu Beijing Academy of Artificial Intelligence Beijing, China zhengliu1026@gmail.com wide-ranging, supporting doctors in clinical decision-making [30], assisting patients in finding health information [24], and aiding researchers in accessing relevant studies [46]. In information retrieval (IR), there are two mainstream paradigms: lexical-based sparse retrieval methods, such as BM25 [28], and embedding-based dense retrieval approaches [4, 10, 41]. Dense retrievers have shown strong performance, particularly in domains where large annotated datasets are available [13]. These datasets are typically created by manually matching queries with relevant documents, process that becomes impractical when dealing with collections containing millions or billions of items. As result, many retrieval tasks face the challenge of having limited or no in-domain labeled examples. Given these challenges, several studies [13, 41] have explored transfer learning for zero-shot retrieval, where dense retrievers are trained on large, high-resource datasets and evaluated on queries from different domains. MS MARCO [1], dataset containing substantial number of manually annotated query-document pairs, is commonly used for this purpose. An alternative to transfer learning is contrastive learning [6], which follows two-stage pretraining and fine-tuning pipeline to develop general-purpose text embedding models. The pretraining stage leverages weakly supervised data gathered through large-scale web crawling, while the fine-tuning stage uses high-quality text pairs derived from data mining or manual annotation [16, 40]. However, the availability of such largescale datasets or high-quality text pairs cannot always be assumed, particularly in non-English languages or specialized domains. Recently, large language models (LLMs) have demonstrated exceptional performance in zero-resource retrieval scenarios, primarily due to their extensive knowledge and robust text generation capabilities [11, 29, 36]. This makes them particularly effective in situations where labeled data is scarce or unavailable. One such approach, HyDE (Hypothetical Document Embeddings)[5], employs zero-shot prompts to guide an instruction-following language model to generate hypothetical documents, effectively narrowing the semantic gap between the query and the target document. Similarly, Query2doc[36] uses few-shot prompting of LLMs to generate pseudo-documents, which are then used to expand the original query. However, applying these methods to medical information retrieval presents three critical challenges: (1) LLMs lack the specialized medical knowledge necessary to generate highly relevant hypothetical documents. Although LLMs are trained on vast datasets drawn from wide array of general-purpose sources, they are often insufficiently equipped with domain-specific knowledge, particularly in fields like medicine. This limitation can lead to the generation of irrelevant or misleading hypothetical documents, as LLMs may produce hallucinationserroneous or fabricated informationdue to their limited understanding of complex medical concepts. (2) General text embedding models are inadequate for representing medical queries and documents effectively. These models are typically designed for multi-domain (e.g., medicine, law, economics) and multi-task (e.g., retrieval, question answering, classification) settings, with the aim of creating versatile retrievers capable of handling various fields, tasks, and languages. However, such generalization often fails to capture the nuanced and knowledge-intensive nature of the medical domain, where precise representation of medical terminology and context is crucial for accurate retrieval. (3) The medical domain suffers from scarcity of highquality, relevance-labeled datasets. In particular, relevance-labeled medical datasetsespecially in non-English languagesare extremely limited. Creating such datasets, which require the careful curation of query-document pairs, is both time-consuming and resourceintensive. The scarcity of labeled data significantly increases the cost of training and fine-tuning models to achieve high performance in medical information retrieval. In this paper, we aim to develop an effective fully zero-shot dense retrieval system that requires no relevance-labeled data for medical information retrieval. Inspired by the concept of enhancing retrieval through hypothetical documents, we design an end-to-end framework called Self-Learning Hypothetical Document Embedding (SL-HyDE). During the inference phase, the framework first employs an LLM as generator to produce relevant hypothetical document in response to medical query. retrieval model is then employed to identify the most relevant real document based on the generated hypothetical document. In the training phase, we design self-learning mechanism that enhances the retrieval performance of SL-HyDE without the need for labeled data. Specifically, this mechanism leverages the retrieval models ranking capabilities to select high-relevance hypothetical documents that align with the output of the generator, effectively injecting medical knowledge into the LLM. In turn, the generators ability to produce high-quality hypothetical documents provides pseudo-labeled data for the retrieval model, enabling it to efficiently encode medical texts. This interactive and complementary approach generates supervisory signals that enhance both the generation and retrieval capabilities of the system. Notably, SL-HyDE begins with unlabeled medical corpora and completes the training process through self-learning mechanism, thereby circumventing the heavy reliance on labeled data typically required for training both large language models and retrieval systems. To evaluate SL-HyDEs performance in Chinese medical information retrieval, we identify significant lack of suitable benchmarks. In the broader field of information retrieval, datasets like BEIR [32] offer collection of 18 datasets that assess embedding performance Li et al. across range of retrieval tasks. MTEB [26] and C-MTEB [40] provide extensive benchmarks for massive text embedding tasks (such as retrieval, ranking, and clustering) in English and Chinese, respectively. Despite these resources, Chinese medical datasets are significantly underrepresented. To bridge this gap, we develop valuable Chinese Medical Information Retrieval Benchmark (CMIRB). CMIRB is constructed from real-world medical scenarios, including online consultations, medical examinations, and literature retrieval. It comprises five tasks and ten datasets, marking the first comprehensive and authentic evaluation benchmark for Chinese medical information retrieval. This benchmark is poised to accelerate advancements toward more robust and generalizable MIR systems in the future. Through extensive experimentation on the CMIRB benchmark, we find that our proposed method significantly enhances retrieval performance. We validate SL-HyDE across various configurations involving three large language models as generators and three embedding models as retrievers. Notably, SL-HyDE surpasses the HyDE (Qwen2 as generator + BGE as retriever) combination by an average of 4.9% in NDCG@10 across ten datasets, and it shows 7.2% improvement compared to using BGE alone for retrieval. These outcomes underscore the effectiveness and versatility of SL-HyDE. In summary, our contributions are as follows: We propose Self-Learning Hypothetical Document Embeddings (SL-HyDE) for zero-shot medical information retrieval, eliminating the need for relevance-labeled data. We develop comprehensive Chinese Medical Information Retrieval Benchmark (CMIRB) and evaluate the performance of various retrieval models using this benchmark. SL-HyDE enhances retrieval accuracy across five tasks and demonstrates generalizability and scalability with different combinations of generators and retrievers."
        },
        {
            "title": "2 RELATED WORK\n2.1 Dense Retrieval\nIn recent years, with rapid advancements in deep learning and\nnatural language processing (NLP) technologies, researchers in the\nfield of Information Retrieval (IR) have increasingly focused on\nleveraging advanced text representation methods to improve re-\ntrieval system performance. Contriever [10] leverages unsupervised\ncontrastive learning for dense retrieval. BGE [40] enhances Chi-\nnese general embeddings through training on large-scale text pairs.\nGTE [16] employs multi-stage contrastive learning for multilingual\napplications. mE5 [35] uses weakly supervised pretraining and fine-\ntuning for multilingual embeddings. BMRETRIEVER [42] special-\nizes in English biomedical retrieval. These works demonstrate the\nimpact of well-structured training strategies on effective retrieval\nacross domains. Beyond embedding-based techniques, large lan-\nguage models (LLMs) have demonstrated exceptional performance\nin zero-resource retrieval scenarios, primarily due to their exten-\nsive knowledge and robust text generation capabilities. GAR [23]\nenriches query semantics with generated content. HyDE [5] gener-\nates hypothetical documents for the retriever, effectively narrow-\ning the semantic gap between the query and the target document.",
            "content": "AutoMIR: Effective Zero-Shot Medical Information Retrieval without Relevance Labels Figure 1: Training and inference pipeline of SL-HyDE. Query2doc [36] utilizes few-shot prompts to expand queries, boosting both sparse and dense retrieval. These methods have significantly enhanced retrieval in open-domain QA. However, retrieval through hypothetical documents generated by LLMs often yields suboptimal results when domain-specific knowledge is insufficient. To address these challenges, we propose self-learning framework that jointly optimizes the generator and retriever without any relevance labels, thereby enhancing retrieval performance."
        },
        {
            "title": "2.2 Information Retrieval Benchmark\nTo better guide the development of retrieval models, researchers\nhave developed various datasets and benchmarks to comprehen-\nsively evaluate retrieval model performance. For instance, DuReader [9],\na large-scale Chinese reading comprehension dataset, provides a\nvaluable question-answer resource, significantly advancing text\nunderstanding and information retrieval research. Similarly, Hot-\nPotQA [44] introduces challenges requiring complex multi-hop rea-\nsoning, driving advancements in multi-document retrieval. BEIR[32],\na zero-shot retrieval evaluation benchmark, covers diverse retrieval\ntasks and offers a unified evaluation platform. MTEB [26] estab-\nlishes a framework for evaluating multilingual text embeddings.\nMore recently, C-MTEB [40] specifically addresses Chinese text\nembedding evaluations. Collectively, these benchmarks have pro-\npelled progress in IR technologies and provided critical resources\nfor embedding research.",
            "content": "However, most benchmarks are designed for general domains, limiting their utility for specific domains such as medical retrieval. Existing medical benchmarks like CMB [37] and CMExam [19] focus primarily on medical QA and clinical reasoning, which are not suitable for medical retrieval evaluation. To bridge this gap, we compile and adapt medical corpora specifically for IR tasks thereby improving the evaluation of medical retrievers and guiding their optimization for specialized applications."
        },
        {
            "title": "3 METHODOLOGY\n3.1 Dense Retrieval\nZero-shot document retrieval is a crucial component of the search\nsystems. Given a user query 𝑞 and a document set 𝐷 = {𝑑1, ..., 𝑑𝑛 }\nwhere n represents the number of document candidates, the goal\nof a retrieval model (M𝑟 ) is to fetch documents that align with the\nuser’s genuine search intent for the current query 𝑞. These models\nmap an input query 𝑞 and a document 𝑑 into a pair of vectors\n<𝑣𝑞, 𝑣𝑑 >, using their inner product as a similarity function 𝑠 (𝑞, 𝑑):\n𝑠 (𝑞, 𝑑) =< M𝑟 (𝑞), M𝑟 (𝑑) >",
            "content": "(1) The retrieval models then identify the top-k documents, denoted as 𝐷, which have the highest similarity scores when compared to the query 𝑞. Earlier studies [4, 13] utilized dual-tower architectures to encode queries and documents separately. However, more recent research [10, 40, 41] indicates that using the same architecture for both query and document encoding provides greater robustness in low-resource information retrieval tasks. In this work, we utilize the M𝑡 model to serve as both the query encoder and the document encoder."
        },
        {
            "title": "3.2 Hypothetical Document Embedding (HyDE)\nLarge language models have achieved remarkable success in text\ngeneration across various natural language processing tasks, in-\ncluding question answering [18], text generation [3], and dialogue",
            "content": "systems. Recently, there has been growing interest in utilizing these models to generate relevant documents based on queries, thereby improving retrieval accuracy. Hypothetical Document Embeddings (HyDE) [5] decompose dense retrieval into two tasks: generative task executed by an instruction-following language model and document-document similarity task executed by retrieval model. Initially, the query is fed to the generative model with the instruction to \"write document that answers the question,\" creating hypothetical document. Specifically, large language model such as ChatGPT or InstructGPT can serve as the generative model (M𝑔). By deploying prompts, the user query is supplied, and the model generates the hypothetical documents: 𝑑 = M𝑔 (𝑞, 𝑃𝑟𝑜𝑚𝑝𝑡1) To better fuse the documents, they sample 𝑁 documents [𝑑 1 (2) , ..., 𝑑 from generated hypothetical documents. Subsequently, an unsupervised contrastive encoder (such as Contriever) is used to encode these documents into an embedding vector 𝑣𝑞. 𝑁 ] 𝑣𝑞 = 1 𝑁 + [ 𝑁 𝑘=1 M𝑟 (𝑑 𝑘 ) + M𝑟 (𝑞)] (3) The inner product is then computed between M𝑟 (𝑑) and the set of all document vectors: 𝑠 (𝑞, 𝑑) =< 𝑣𝑞, M𝑟 (𝑑) > 𝑑 𝐷 (4) This vector identifies neighborhood in the corpus embedding space, from which similar real documents are retrieved based on vector similarity. This straightforward plug-and-play paradigm makes HyDE easy to deploy in real-world applications for solving retrieval tasks."
        },
        {
            "title": "3.3 SL-HyDE\nApplying HyDE to the medical domain presents two primary chal-\nlenges: (1) LLMs often lack specialized medical domain knowledge,\nand (2) retrievers may struggle to effectively encode medical texts\ndue to inadequate training on medical corpora. These challenges\nhinder the successful implementation of HyDE technology in the\nmedical field, making it difficult to achieve significant performance\nimprovements in retrieval tasks. A common strategy to supple-\nment medical domain knowledge involves fine-tuning with labeled\nmedical data [33, 38, 42, 45]. However, these approaches rely on\nhigh-quality, manually constructed data to adapt general models\nto the medical domain. Unfortunately, obtaining such high-quality\nlabeled data in practice is particularly challenging, making the\ntraining of a medical LLM highly costly.",
            "content": "In this paper, we introduce Self-Learning Hypothetical Document Embedding (SL-HyDE) mechanism designed to leverage the potential of unlabeled corpora in the medical domain. SL-HyDE consists of two main modules: self-learning generator and selflearning retriever. The overall framework is depicted in Figure 1."
        },
        {
            "title": "3.4 Self-Learning Generator\nA common strategy for equipping a large model with medical do-\nmain knowledge involves continued pretraining on unlabeled cor-\npora, supervised fine-tuning on labeled data, and reinforcement\nlearning to align the model with human preferences. However,",
            "content": "Li et al. continued pretraining typically requires substantial computational resources and extensive datasets. Consequently, supervised finetuning with smaller datasets has gained popularity as an efficient method for adapting large models. In this paper, we utilize the generator and retriever in SL-HyDE to create labeled dataset based on an unsupervised corpus. The labels are entirely generated by SL-HyDE, eliminating the need for external labeled data collection. We begin with an unlabeled corpus, specifically utilizing medical encyclopedia dataset, Huatuo26M_encyclopedia1, as our foundational text. To construct queries, we employ robust offline LLM, Qwen2.52, leveraging in-context learning [2, 25]. This approach generates meaningful queries in specific format, using prompts such as: \"Please generate medical question based on the provided medical text. Demonstration: [DEMONSTRATION] Text: [TEXT]\". (5) 𝑞 = 𝐿𝐿𝑀 (𝑑, 𝑃𝑟𝑜𝑚𝑝𝑡2) After generating the query, we employ the generator to produce hypothetical document that encapsulates the correct information based on the true target document. We intentionally avoid using the true target document as the output label because the primary role of the generator is to craft hypothetical document that aids the retriever in locating the true target document. Expecting the generator to replicate the exact target document itself would be overly demanding and unrealistic. {𝑑 1 𝑁 } = M𝑔 (𝑞, 𝑑, 𝑃𝑟𝑜𝑚𝑝𝑡3) , ...𝑑 (6) Considering that the hypothetical documents generated by the generator may not always be ideal for retrieving the true target document. Ideally, the optimal hypothetical document is one that enhances the ranking position of the target document during retrieval. To achieve this, we employ the retriever M𝑟 to rank and select the optimal hypothetical document. Specifically, the generator creates 𝐾 hypothetical documents for given query, and each hypothetical document is utilized by the retriever to search the corpus for the most relevant documents. We then calculate the ranking position 𝑟𝑖 of the true target document for 𝑑 𝑖 : 𝑟𝑖 = 𝑟𝑎𝑛𝑘 (𝑑, M𝑟 (𝑑 𝑖 , 𝐷)), 𝑖 = 1, ..., 𝐾 (7) 𝑑 = 𝑑 (8) 𝑟𝑖 arg minK i=1 The higher the ranking, the more effective the hypothetical document. We select the generated document with the highest rank (i.e., the lowest 𝑟𝑖 ). At this stage, we have constructed questionanswer pair in the form of (𝑞, 𝑑), where the query serves as the question, and the generated document acts as the answer. We then employ supervised fine-tuning approach to train the generator using this dataset, 𝐷𝑙𝑙𝑚 = {(𝑞, 𝑑)𝑞 𝑄 }. The standard supervised fine-tuning (SFT) loss is calculated as follows: 𝑡 log M𝑔 (𝑞 Lsft = <𝑡 , 𝑞) 𝑡 𝑞 (9) 𝑞 𝑄 For the training data of the self-learning generator, there is no need to rely on supervision signals from labeled medical data. Instead, we utilize only unlabeled corpora, leveraging the generator and retriever within the SL-HyDE framework to create the supervision signals. More importantly, the self-learning generator trained 1https://huggingface.co/datasets/FreedomIntelligence/huatuo_encyclopedia_qa 2https://huggingface.co/Qwen/Qwen2.5-32B-Instruct AutoMIR: Effective Zero-Shot Medical Information Retrieval without Relevance Labels in this manner can produce hypothetical documents optimized for retrieval ranking preferences, thereby enhancing the retrievers performance in subsequent retrieval processes."
        },
        {
            "title": "3.5 Self-Learning Retriever\nThe training of retrievers typically involves two stages: unsuper-\nvised pre-training and supervised fine-tuning. Unsupervised pre-\ntraining aims to harness large volumes of unlabeled text to learn\ndomain knowledge, while supervised fine-tuning employs high-\nquality labeled datasets, often alongside hard negative mining, to\nfurther teach the model to differentiate between similar texts. For\nexample, GTE [16] utilizes ∼800M text pairs during the unsuper-\nvised pre-training stage and ∼3M pairs for fine-tuning. In this work,\nour goal is not to train a medical retriever with vast amounts of\ndata but rather to adapt a general-purpose retriever to the medical\ndomain through fine-tuning with a small amount of data.",
            "content": "Now, we have passage from the corpus 𝐷 and its corresponding query 𝑞. This < 𝑞, 𝑑 > combination already constitutes the query-document labeled data necessary for retriever fine-tuning. However, since HyDE retrieves documents by encoding both the query and the hypothetical document to locate the target document, we propose using < (𝑞, 𝑑), 𝑑 > triplet as the labeled data for retriever training. This methods advantage lies in bridging the gap between the training and inference stages for the retriever, ensuring consistency in data format across both processes, thereby enhancing retriever performance. In addition to the unlabeled medical corpus, we now have corresponding query. This (𝑞, 𝑑) combination already forms the query-document labeled data required for retriever training. However, considering that HyDE retrieves documents by simultaneously encoding both the query and the hypothetical document to locate the target document, we propose using (𝑞, 𝑑, 𝑑) triplet as the labeled data for retriever training. This method narrows the gap between the training and inference stages for the retriever, ensuring the two stages operate under the same data format. To achieve this, we utilize the fine-tuned generator M𝑔 from the previous stage to generate hypothetical documents for all queries, constructing labeled fine-tuning dataset 𝐷𝑒𝑚𝑏 = {(𝑞, 𝑑, 𝑑 >)}. Following previous research [15, 40], we enhance the complexity of the training data through hard negative mining using the retriever, which creates challenging negative examples that closely resemble the queries. The hard negative dataset 𝐷 is mined from the original corpus 𝐷, employing the ANN-style sampling strategy in [41]: 𝐷 = 𝐴𝑁 𝑁 (M𝑟 (𝑞, 𝑑), M𝑟 (𝑑)) (10) In addition to the negatives mined from the corpus, we also incorporate in-batch negatives. We then apply contrastive learning loss for supervised fine-tuning of the retriever, with the objective function formulated as follows: Lemb = min. (𝑞,𝑑 ) log 𝑒𝑠 (𝑞,𝑑 )/𝜏 𝑒𝑠 (𝑞,𝑑 )/𝜏 + (cid:205)𝐵𝐷 𝑒𝑠 (𝑞,𝑑 )/𝜏 (11) where 𝜏 is the temperature coefficient, and 𝐵 represents the negative samples within the batch. The similarity score 𝑠 (𝑞, 𝑑) incorporates the generated document, as described in Equation 4. Thus far, we have developed retriever equipped with medical domain knowledge that is coherently adapted to the characteristics of retrieval queries, incorporating hypothetical documents within the HyDE framework."
        },
        {
            "title": "3.6 SL-HyDE vs. HyDE\nOur approach, SL-HyDE, builds upon HyDE with several enhance-\nments while retaining some similarities. First, both SL-HyDE and\nHyDE follow the same inference process for information retrieval\ntasks. Each uses a large model to generate a hypothetical document\nbased on the query, which the retriever then employs to locate\nthe most relevant document. Second, neither SL-HyDE nor HyDE\nrequires labeled data, which allows for rapid deployment. HyDE\nis especially advantageous in real-world scenarios where efficient\nretrieval can be executed simply by selecting a generator and a\nretriever. However, for tasks needing domain-specific knowledge,\nsuch as medical information retrieval, deploying HyDE directly\nmay not yield optimal results. One potential strategy is to fine-tune\nthe generator and retriever separately using labeled medical data\nbefore deploying the HyDE framework. The primary challenge here\nin acquiring labeled data, and fine-tuning the models separately\noften leads to suboptimal performance.",
            "content": "SL-HyDE improves upon this by integrating self-learning mechanism, transforming HyDE into trainable end-to-end framework. This mechanism enables both the generator and the retriever to better adapt to the medical domain without relying on labeled medical data. Supervision signals for the generators training are derived from the retriever, and vice versa, facilitating mutual enhancement through this self-learning process. This holistic approach results in improved performance in retrieval tasks. Overall, SL-HyDE offers an efficient and convenient solution for enhancing HyDEs performance in the medical domain, particularly when dealing with unlabeled corpora."
        },
        {
            "title": "4.1 Task Definition\nFigure 2 provides an overview of the tasks and datasets available\nin CMIRB. The benchmark comprises the following five task types:\nMedical Knowledge Retrieval : Retrieve relevant medical knowl-\nedge snippets from textbooks or encyclopedias based on a given\nmedical entity query.\nMedical Consultation Retrieval : Extract relevant doctor’s re-\nsponses to online medical consultation questions posed by patients,\nsimulating the retrieval of expert medical advice.\nMedical News Retrieval : Focus on retrieving news articles that\naddress queries related to COVID-19.\nMedical Post Retrieval : Retrieve the content of a forum post\ncorresponding to its title.",
            "content": "Table 1: Statistics of datasets in CMIRB. Li et al. Dataset Task MedExam Medical DuBaike Knowledge Retrieval DXYDisease Medical MedicalRet. Consultation CmedqaRet. DXYConsult Retrieval CovidRet. News Ret. IIYiPost Post Ret. CSLCite Literature CSLRel Retrieval #Query 697 318 1,255 1,000 3,999 943 949 789 573 439 #Samples Avg. Word Lengths #Document Query Document 96.9 7.6 24.3 17.9 48.4 170.4 25.9 15.9 21.9 281. 27,871 56,441 54,021 100,999 100,001 12,577 100,001 27,570 36,703 36,758 493.7 403.3 191.1 122.0 307.7 370.1 332.4 150.1 269.6 292.2 For the medical consultation retrieval task, we collect three online patient-doctor consultation datasets: MedicalRetrieval, CmedqaRetrieval, and DXYConsult. The MedicalRetrieval dataset is sourced from [20], and the CmedqaRetrieval dataset comes from [27]. Additionally, we compile several doctor-patient dialogues from the DingXiangYuan, to create DXYConsult. Compared to the former two datasets, the query in DXYConsult includes more detailed patient information, such as symptoms, medication usage, and pending diagnosis questions. For the medical news retrieval task, we utilize the CovidRetrieval dataset from [27]. This dataset features manually annotated highquality questions related to COVID-19 as queries, with corresponding news articles as the documents. In the medical post retrieval task, we curate IIYiPost by crawling posts from the IIYi forum5, platform where medical professionals discuss various medical topics. This dataset includes both post titles and the full post content. For the medical literature retrieval task, we develop two datasets based on the CSL data [14]: the citation retrieval dataset CSLCite and similar literature retrieval dataset CSLRel. To construct CSLCite, we utilize journal titles from CSL as queries and extract their cited references from WanFangMedical6 as positive documents. Similarly, we select the most similar paper recommended by the website as the positive documents for the CLSRel dataset. To ensure dataset quality, we design filtering process, as outlined in Algorithm 1. We use ChatGPT to exclude non-medical data and remove low-quality query-document pairs. For the MedExam and DuBaike datasets, beyond the initial filtering, we implement query-document matching process to find the most similar positive document for each query, detailed in Appendix A.1. After processing, we compile total of 10 datasets. The statistical details of these datasets are presented in Table 1. Statistics reveal that query lengths vary significantly, ranging from single title to full article. Similarly, document lengths span from brief physician responses to extensive medical knowledge texts. This diversity ensures that our benchmark is both comprehensive and practically meaningful. Figure 2: An overview of the diverse tasks and datasets in CMIRB benchmark. Medical Literature Retrieval : Retrieve abstracts of cited references based on the title of medical paper or find similar paper based on the text of given medical paper."
        },
        {
            "title": "4.2 Data Construction\nThe data corpus for CMIRB is partially sourced from existing datasets\nand supplemented by high-quality data collected from relevant med-\nical websites.",
            "content": "For the medical knowledge retrieval task, we develop three datasets: the medical exam retrieval dataset MedExam, the medical encyclopedia knowledge retrieval dataset DuBaike, and the medical disease knowledge retrieval dataset DXYDisease. For MedExam, the queries and corpus are derived from the MEDQA(MCMLE) test dataset and MedTextbook(MCMLE) [12]. We concatenate the multiple-choice question and options as the query. For DuBaike, the queries are derived from the DuReader dataset [8], while the documents consist of medical knowledge pages from Baidu Baike3. These queries are generated by users while searching on Baidu Search or Baidu Zhidao. For DXYDisease, we collect question-document pairs from the online disease knowledge website DingXiangYuan4. The questions consist of pre-constructed inquiries about various aspects of diseases, such as definitions and symptoms. 3https://baike.baidu.com/ 4https://dxy.com/ 5https://bbs.iiyi.com/ 6https://med.wanfangdata.com.cn/ AutoMIR: Effective Zero-Shot Medical Information Retrieval without Relevance Labels Table 2: Performance of various Retrieval models on CMIRB benchmark. All scores denote NDCG@10. The best score among 10 base retrievers on given dataset is marked in bold. Post Literature Retrieval Task Dataset Text2Vec(large) mContriever BM25 OpenAI-Ada-002 M3E(large) mE5(large) piccolo(large) GTE(large) BGE(large) PEG(large) HyDE SL-HyDE Improve. Knowledge Retrieval Consulation Retrieval MedExam DuBaike DXYDis. Medical Cmedqa DXYCon. 41.39 51.50 31.95 53.48 33.29 53.96 43.11 41.22 58.61 52.78 64.39 71.49 11.03% 21.13 22.25 17.89 43.12 46.48 53.27 45.91 42.66 44.26 51.68 52.73 60.96 15.61% 41.52 44.34 40.12 58.72 62.57 72.10 70.69 70.59 71.71 77.38 73.98 75.34 1.84% 30.93 38.50 29.33 37.92 48.66 51.47 59.04 62.88 59.60 60.96 57.27 58.58 15.53 22.71 6.83 22.36 30.73 28.67 41.99 43.15 42.57 44.42 38.52 39.07 2.29% 1.43% 21.92 20.04 17.78 27.69 41.05 41.35 47.35 46.30 47.73 49.30 47.11 50.13 6.41% IIYiPost CSLCite News Covid 60.48 56.01 78.90 57.21 61.33 75.54 85.04 88.41 73.33 82.56 74.32 76.95 29.47 20.21 28.11 34.59 66.95 33.74 48.60 32.97 45.03 35.79 63.86 42.65 65.89 44.31 46.40 63.02 67.13 43.27 70.38 44.74 73.07 46.16 73.81 46.78 3.54% 1.01% 1.34% CSLRel Average 23.01 33.95 29.97 43.40 47.54 37.94 44.21 49.32 45.79 40.38 38.68 40.71 5.25% 30.56 35.20 35.35 42.55 45.25 52.08 54.75 55.40 55.40 57.46 56.62 59.38 4.87%"
        },
        {
            "title": "5 EXPERIMENTS\n5.1 Experimental Setup\nImplementation Details For the training medical corpus, we\nsample 10,000 documents from the Huatuo26_encyclopedia dataset.\nIn our training framework, we utilize Qwen2-7B-Instruct [43] as the\ngenerator and BGE-Large-zh-v1.5 [40] as the retriever. Unless oth-\nerwise stated, all subsequent experiments employ the Qwen+BGE\nsetup. Model training and evaluation are conducted on up to 5\nNVIDIA A100 GPUs with 40GB of memory each. During fine-tuning\nof the LLM, the AdamW optimizer [21] is employed alongside a\ncosine-type scheduler. Training is executed for 1 epoch with a learn-\ning rate of 1e-5 and a batch size of 2. The warmup steps for the LLM\nare set to 200, and the lora rank is configured at 8. For retriever\ntraining, we use the AdamW optimizer with an initial learning rate\nof 1e-5, applying a linear decay schedule. The batch size per GPU\nis set at 4, and the maximum input sequence length is limited to\n512. The model is trained for 1 epoch. We set the temperature to\n0.02 and mined 7 hard negatives for training.\nEvaluation Settings For simplicity, we employ the LLM to gen-\nerate a single hypothetical document for each query. The retrieval\nmodel embeds all queries, hypothetical documents, and corpus doc-\numents, with similarity scores calculated using cosine similarity.\nDocuments in the corpus are ranked for each query based on these\nscores, and nDCG@10 is used as the primary evaluation metric to\nassess retrieval effectiveness.\nBaseline Models To comprehensively evaluate CMIRB, we com-\npare our model against several popular retrieval models. These\ninclude lexical retriever BM25 [28]; dense retrieval models such as\nText2Vec-Large-Chinese7, PEG [39], BGE-Large-zh-v1.5 [40], GTE-\nLarge-zh [15], and Piccolo-Large-zh8; multilingual retrievers like\nmContriever (masmarco) [10], M3E-Large9, mE5 (multilingual-e5-\nlarge) [35]; and OpenAI’s retriever, text-embedding-ada-00210.",
            "content": "7https://github.com/chmlyrical/text2vec-large-chinese 8https://github.com/timczm/piccolo-large-zh 9https://github.com/wangyuxinwhy/uniem 10https://platform.openai.com/docs/guides/embeddings"
        },
        {
            "title": "5.2 Main Results\nThe experimental results for various retrieval models, including\nSL-HyDE, on the CMIRB benchmark are presented in Table 2. We\nmake the following observations.",
            "content": "(1) BM25 remains highly competitive in specific retrieval scenarios. As lexical retriever, it ranks documents based on TF-IDF matching scores calculated between queries and documents. Despite underperforming on the overall CMIRB benchmark, it displays strong results in tasks like medical news retrieval (78.9 vs. 73.33 for BGE) and medical post retrieval (66.95 vs. 67.13 for BGE). This can be attributed to the higher keyword overlap in these datasets, which aligns with BM25s strengths. (2) No single retrieval model achieves optimal performance across all tasks. PEG and GTE each deliver the best performance on four datasets, while BGE and mE5 each excel in achieving the top results on one dataset. Dense models with better performance often utilize contrastive learning, pretraining on large-scale unlabeled data followed by fine-tuning on labeled data. Variations in training data distribution influence model effectiveness across different datasets, suggesting the need for specialized approaches. (3) SL-HyDE consistently outperformed HyDE across all ten datasets. While HyDE shows slight overall improvements over BGE, it excels in medical knowledge retrieval but underperforms in medical consultation tasks. This discrepancy could be due to LLMs stronger handling of encyclopedia-type knowledge compared to the nuanced domain of patient-doctor consultations. In contrast, SL-HyDE achieved improvements over HyDE in all tasks, owing to its self-learning mechanism, which effectively enhances medical knowledge integration within both the generator and the retriever."
        },
        {
            "title": "5.3 Performance Analysis\nThe core components of SL-HyDE are the generative LLM and the\nretrieval model. In this section, we explore the effects of varying\nthese components.",
            "content": "Table 3: Performance of different generators. Table 5: Performance of different fusing strategies. Li et al. Literature Avg.(All) Task HyDE SL-HyDE Improve. HyDE SL-HyDE Improve. Post Know. Consu. News ChatGLM3 as Generator + BGE as Retriever 44.46 73.89 62.43 46.40 76.78 66.26 4.36% 6.14% 70.88 72.29 3.91% 1.99% 46.43 48.55 4.57% Llama2 as Generator + BGE as Retriever 72.90 77.17 72.22 71.99 14.21% 11.87% 5.86% 0.32% 55.74 63.66 40.62 45.44 45.30 45.75 0.99% 56.02 58.63 4.65% 52.48 56.80 8.23% Table 4: Performance of different retrievers. Literature Avg.(All) Task HyDE SL-HyDE Improve. HyDE SL-HyDE Improve. Post Know. Consu. News Qwen2 as Generator + mE5 as Retriever 75.92 77.59 68.15 66.81 4.31% 3.90% 2.20% 1.97% 65.77 68.60 43.15 44.83 38.58 42.33 9.72% Qwen2 as Generator + PEG as Retriever 80.49 80.89 72.51 75.93 5.96% 2.50% 0.50% 4.72% 38.87 45.03 15.86% 66.03 69.96 49.73 50.97 54.80 56.94 3.90% 57.80 60.97 5.48% Effect of Different LLMs In Table 3, we present SL-HyDEs performance with alternative LLMs fine-tuned, such as ChatGLM36B [31] and Llama2-7b-Chat [34], while keeping BGE as the retriever. Both models demonstrate performance improvements under the SL-HyDE framework compared to HyDE. For instance, we observe 4.65% improvement with ChatGLM3 and an 8.23% improvement with the Llama2 model. However, for Llama2, HyDE shows slight decline compared to BGE. This is likely due to the fact that the pseudo-documents generated by the English-based Llama2 contained English content, which the downstream BGE retriever struggled to encode effectively. After fine-tuning, SL-HyDE improves by approximately 8%, attributed to both the reduction of English content and the enhanced retrievers ability to encode medical knowledge, illustrating SL-HyDEs adaptability across different LLMs. Effect with Different Retrievers We consider fine-tuning the other two retrieval models: PEG which achieves optimal performance on CMIRB, and multilingual retrieval mE5. In Table 4, we observe that the standard HyDE method offers some improvement over using only the retriever, but the overall performance is significantly enhanced with the application of SLHyDE. For example, the top-performing PEG model on the CMIRB benchmark improved from 57.46% to 60.97%, representing substantial increase in retrieval tasks. This underscores SL-HyDEs ability to boost retrieval performance across various retriever models. How to Use Hypothetical Documents In this section, we test several methods for incorporating hypothetical documents. SLHyDE: This method encodes the original query and the hypothetical documents separately, then applies mean pooling to obtain the final query vector. SL-HyDE w/K-D: This approach generates five hypothetical documents. SL-HyDE w/D: Only the hypothetical Task SL-HyDE w/ D. w/ con. w/ K-D. Know. Consu. News 76.95 49.26 69.26 71.94 41.86 68.00 73.38 45.51 69.04 77.38 50.17 69.30 Post 73.81 68.02 69.53 74. Literature Avg.(All) 43.75 37.36 44.81 45.42 59.38 54.43 57.62 60.12 Table 6: Performance of different variants. Task HyDE SL-HyDE w/o BGE-FT w/o Qwen-FT Know. Consu. News 74.32 47.63 63.70 76.95 49.26 69.26 74.87 47.95 64.32 76.63 48.85 68. Post 73.07 73.81 72.91 74.52 Literature Avg.(All) 42.42 43.75 43.24 43.11 56.62 59.38 57.11 58.77 document is used as the query for retrieval. SL-HyDE w/con: The original query and the hypothetical document are concatenated into single string to form new query. Table 5 shows that the combination of the original query and hypothetical documents is optimal. Sole reliance on hypothetical documents significantly reduces performance, especially in medical consultation tasks, where original queries contain critical information. The string concatenation method introduces some performance degradation, indicating that the generated documents may contain noise at the string level, whereas average pooling effectively mitigates it. Generating multiple hypothetical documents increases coverage and improves performance across tasks. However, it often leads to K-fold increase in inference time. Ablation Study To further analyze the gains brought by the internal architecture of the SL-HyDE model, we conduct two sets of ablation experiments. (1) variant, SL-HyDE w/o BGE-FT, which uses the fine-tuned LLM as the generator and the raw BGE as the retriever; (2) variant, SL-HyDE w/o Qwen-FT, which utilizes raw LLM as the generator and the fine-tuned BGE as the retriever. Table 6 demonstrates that fine-tuning both components substantially enhances performance, validating the efficacy of the self-learning mechanism. Notably, fine-tuning the retriever yields greater gains, suggesting that BGE benefits significantly from domainspecific adaptation. However, our approach does not rely on labeled data and still achieves significant retrieval performance improvements."
        },
        {
            "title": "6 CONCLUSIONS\nIn this paper, we introduce an automated framework for zero-shot\nmedical information retrieval, named SL-HyDE, which operates\nwithout the need for relevance labels. Utilizing an unlabeled medi-\ncal corpus, we employ a self-learning, end-to-end training frame-\nwork where the retriever guides the generator’s training, and the\ngenerator, in turn, enhances the retriever. This process integrates\nmedical knowledge to create hypothetical documents that are more\neffective in retrieving target documents. Furthermore, we present a\ncomprehensive Chinese medical information retrieval benchmark,\nevaluating mainstream retrieval models against this new standard.",
            "content": "AutoMIR: Effective Zero-Shot Medical Information Retrieval without Relevance Labels Experimental findings demonstrate that SL-HyDE consistently improves retrieval accuracy over HyDE across ten datasets. Additionally, SL-HyDE shows strong adaptability and scalability, effectively enhancing retrieval performance across various combinations of generators and retrievers. REFERENCES [1] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et al. 2016. Ms marco: human generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268 (2016). [2] Tom Brown. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165 (2020). [3] Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. 2019. Plug and play language models: simple approach to controlled text generation. arXiv preprint arXiv:1912.02164 (2019). [4] Yingqi Qu Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. 2020. RocketQA: An optimized training approach to dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2010.08191 (2020). [5] Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2022. Precise zero-shot dense retrieval without relevance labels. arXiv preprint arXiv:2212.10496 (2022). [6] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. Simcse: Simple contrastive learning of sentence embeddings. arXiv preprint arXiv:2104.08821 (2021). [7] Lorraine Goeuriot, Gareth JF Jones, Liadh Kelly, Henning Müller, and Justin Zobel. 2016. Medical information retrieval: introduction to the special issue. Information Retrieval Journal 19 (2016), 15. [8] Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yuan Liu, Yizhong Wang, Hua Wu, Qiaoqiao She, et al. 2017. Dureader: chinese machine reading comprehension dataset from real-world applications. arXiv preprint arXiv:1711.05073 (2017). [9] Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yuan Liu, Yizhong Wang, Hua Wu, Qiaoqiao She, Xuan Liu, Tian Wu, and Haifeng Wang. 2018. DuReader: Chinese Machine Reading Comprehension Dataset from Real-world Applications. In Proceedings of the Workshop on Machine Reading for Question Answering, Eunsol Choi, Minjoon Seo, Danqi Chen, Robin Jia, and Jonathan Berant (Eds.). Association for Computational Linguistics, Melbourne, Australia, 3746. https://doi.org/10.18653/v1/W18-2605 [10] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2021. Unsupervised dense information retrieval with contrastive learning. arXiv preprint arXiv:2112.09118 (2021). [11] Rolf Jagerman, Honglei Zhuang, Zhen Qin, Xuanhui Wang, and Michael Bendersky. 2023. Query expansion by prompting large language models. arXiv preprint arXiv:2305.03653 (2023). [12] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. 2021. What disease does this patient have? large-scale open domain question answering dataset from medical exams. Applied Sciences 11, 14 (2021), 6421. [13] Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for opendomain question answering. arXiv preprint arXiv:2004.04906 (2020). [14] Yudong Li, Yuqing Zhang, Zhe Zhao, Linlin Shen, Weijie Liu, Weiquan Mao, and Hui Zhang. 2022. CSL: large-scale Chinese scientific literature dataset. arXiv preprint arXiv:2209.05034 (2022). [15] Li, Zhang, Zhang, Long, Xie, and Zhang. [n. d.]. Towards general text embeddings with multi-stage contrastive learning (2023). arXiv preprint arXiv:2308.03281 ([n. d.]). [17] [16] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. 2023. Towards general text embeddings with multi-stage contrastive learning. arXiv preprint arXiv:2308.03281 (2023). Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep, and Rodrigo Nogueira. 2021. Pyserini: Python toolkit for reproducible information retrieval research with sparse and dense representations. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 23562362. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021. What Makes Good In-Context Examples for GPT-3? arXiv preprint arXiv:2101.06804 (2021). Junling Liu, Peilin Zhou, Yining Hua, Dading Chong, Zhongyu Tian, Andrew Liu, Helin Wang, Chenyu You, Zhenhua Guo, Lei Zhu, et al. 2024. Benchmarking large language models on cmexam-a comprehensive chinese medical exam dataset. Advances in Neural Information Processing Systems 36 (2024). [18] [19] [20] Dingkun Long, Qiong Gao, Kuan Zou, Guangwei Xu, Pengjun Xie, Ruijie Guo, Jian Xu, Guanjun Jiang, Luxi Xing, and Ping Yang. 2022. Multi-cpr: multi domain chinese dataset for passage retrieval. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. 30463056. Loshchilov. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 (2017). [21] [22] Gang Luo, Chunqiang Tang, Hao Yang, and Xing Wei. 2008. MedSearch: specialized search engine for medical information retrieval. In Proceedings of the 17th ACM conference on Information and knowledge management. 143152. [23] Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen. 2021. Generation-Augmented Retrieval for Open-Domain Question Answering. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (Eds.). Association for Computational Linguistics, Online, 40894100. https://doi.org/10.18653/v1/2021.acl-long.316 Jessie McGowan, Roland Grad, Pierre Pluye, Karin Hannes, Katherine Deane, Michel Labrecque, Vivian Welch, and Peter Tugwell. 2009. Electronic retrieval of health information by healthcare providers to improve practice and patient care. Cochrane Database of Systematic Reviews 3 (2009). [24] [25] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837 (2022). [26] Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. 2023. MTEB: Massive Text Embedding Benchmark. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, Andreas Vlachos and Isabelle Augenstein (Eds.). Association for Computational Linguistics, Dubrovnik, Croatia, 20142037. https://doi.org/10.18653/v1/2023.eacl-main.148 [27] Yifu Qiu, Hongyu Li, Yingqi Qu, Ying Chen, Qiaoqiao She, Jing Liu, Hua Wu, and Haifeng Wang. 2022. Dureader_retrieval: large-scale chinese benchmark for passage retrieval from web search engine. arXiv preprint arXiv:2203.10232 (2022). [28] Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends in Information Retrieval 3, 4 (2009), 333389. [29] Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. 2023. Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy. arXiv preprint arXiv:2305.15294 (2023). [30] Sonish Sivarajkumar, Haneef Ahamed Mohammad, David Oniani, Kirk Roberts, William Hersh, Hongfang Liu, Daqing He, Shyam Visweswaran, and Yanshan Wang. 2024. Clinical information retrieval: literature review. Journal of Healthcare Informatics Research (2024), 140. [31] GLM Team, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, et al. 2024. Chatglm: family of large language models from glm-130b to glm-4 all tools. arXiv e-prints (2024), arXiv2406. [32] Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. 2021. BEIR: Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). https://openreview. net/forum?id=wCu6T5xFjeJ [33] Yuanhe Tian, Ruyi Gan, Yan Song, Jiaxing Zhang, and Yongdong Zhang. 2023. Chimed-gpt: chinese medical large language model with full training regime and better alignment to human preferences. arXiv preprint arXiv:2311.06025 (2023). [34] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023). [35] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2024. Multilingual e5 text embeddings: technical report. arXiv preprint arXiv:2402.05672 (2024). [36] Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query Expansion with Large Language Models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 94149423. https://doi.org/10.18653/v1/2023.emnlp-main.585 [37] Xidong Wang, Guiming Chen, Song Dingjie, Zhang Zhiyi, Zhihong Chen, Qingying Xiao, Junying Chen, Feng Jiang, Jianquan Li, Xiang Wan, Benyou Wang, and Haizhou Li. 2024. CMB: Comprehensive Medical Benchmark in Chinese. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), Kevin Duh, Helena Gomez, and Steven Bethard (Eds.). Association for Computational Linguistics, Mexico City, Mexico, 61846205. https://doi.org/10.18653/v1/2024.naacl-long.343 [38] Xidong Wang, Nuo Chen, Junyin Chen, Yan Hu, Yidong Wang, Xiangbo Wu, Anningzhe Gao, Xiang Wan, Haizhou Li, and Benyou Wang. 2024. Apollo: Lightweight multilingual medical llms towards democratizing medical ai to 6b people. arXiv preprint arXiv:2403.03640 (2024). [39] Tong Wu, Yulei Qin, Enwei Zhang, Zihan Xu, Yuting Gao, Ke Li, and Xing Sun. 2023. Towards Robust Text Retrieval with Progressive Learning. arXiv preprint arXiv:2311.11691 (2023). [40] Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff, Defu Lian, and Jian-Yun Nie. 2024. C-pack: Packed resources for general chinese embeddings. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. 641649. [41] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. 2020. Approximate nearest neighbor negative contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808 (2020). [42] Ran Xu, Wenqi Shi, Yue Yu, Yuchen Zhuang, Yanqiao Zhu, May Wang, Joyce Ho, Chao Zhang, and Carl Yang. 2024. Bmretriever: Tuning large language models as better biomedical text retrievers. arXiv preprint arXiv:2404.18443 (2024). [43] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2 technical report. arXiv preprint arXiv:2407.10671 (2024). [44] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: Dataset for Diverse, Explainable Multi-hop Question Answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii (Eds.). Association for Computational Linguistics, Brussels, Belgium, 23692380. https://doi.org/10. 18653/v1/D18-1259 [45] Hongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhihong Chen, Jianquan Li, Guiming Chen, Xiangbo Wu, Zhiyi Zhang, Qingying Xiao, et al. 2023. Huatuogpt, towards taming language model to be doctor. arXiv preprint arXiv:2305.15075 (2023). Jiaping Zheng and Hong Yu. 2015. Key concept identification for medical information retrieval. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. 579584. [46] CMIRB DATASETS A.1 Data Process We curated substantial dataset from various medical resources, as presented in Table 7, which details the source distribution and data volume. Our data preprocessing pipeline, depicted in Algorithm 1, employs prompt templates outlined in Tables 11 and 12. Initially, we use ChatGPT11 to perform medical relevance detection on the texts, eliminating non-medical content (lines 3-8). Subsequently, ChatGPT assesses query-document relevance, filtering out low-relevance examples (lines 27-33). Our relevance assessment considers semantic alignment and the practical significance of data samples for their respective tasks, as highlighted in prompt 12. For the MedExam and DuBaike datasets, the direct query-document signal isnt initially provided. Both queries and documents in the MedExam dataset originate from Work [12], where 100 randomly selected questions have corpus documents containing evidence sufficient to answer them, verified manually by the authors. In the DuBaike dataset, queries from Baidu Search and Baidu Zhidao often match the content distribution of Baidu Baike. These factors allow us to design query-matching algorithm to locate the valuable document. We leverage ChatGPTs capabilities to identify the most relevant documents. Starting with query, we use the BM25 to retrieve the top 20 relevant documents, which GPT then ranks to identify the top 3 most relevant. Ideally, these documents should be semantically related and provide sufficient answers or evidence for the query. Therefore, ChatGPT extracts document segments as evidence details for the query. 11https://openai.com/chatgpt Li et al. To verify the sufficiency of this evidence, GPT generates an answer to the query based on the extracted evidence fragment. self-verification step follows: if the GPT-generated answer aligns with the document, the document is deemed positive match for the query. For MedExam, where queries are multiple-choice questions, we verify model answers against correct ones. For DuBaike, queries are medical knowledge questions, and answers are encyclopedic. GPT scores the generated and reference answers for consistency in expressing the same medical knowledge. This detailed process is outlined in lines 10-26. Through this iterative loop of self-ranking, evidence searching, answering, and verification, combined with ChatGPTs advanced knowledge capabilities, we ensure high-quality, highly relevant query-document pairs. Post-processing statistical details of these datasets are displayed in Table 1. Algorithm 1 Data Preprocessing Pipeline 1: Input: Query set 𝑄, Document set 𝐷, large language model LLM (e.g., ChatGPT) 2: Output: High-quality, highly relevant query-document pair collection 3: // Step 1: Filter out medically irrelevant queries and documents 4: for each query 𝑞 𝑄, 𝑑 𝐷 do 5: 𝑚𝑒𝑑𝑠𝑐𝑜𝑟𝑒 LLM.med_score(𝑞/𝑑) if 𝑚𝑒𝑑𝑠𝑐𝑜𝑟𝑒 < 𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑 then 6: 7: Remove 𝑞/𝑑 end if 8: 9: end for 10: // Step 2: Matching positive query-document pair 11: if query-document matching is required then 12: for each query 𝑞 𝑄 do 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: // Retrieve top-k relevant documents 𝑑1, 𝑑2, . . . , 𝑑𝑀 BM25(𝑞, 𝐷) 𝑑1, 𝑑2, . . . , 𝑑𝑘 LLM.reranking(𝑞; 𝑑1, 𝑑2, . . . , 𝑑𝑀 ) // Extract evidence snippets 𝑒1, 𝑒2, . . . , 𝑒𝑘 LLM.extract_evidence(𝑞, 𝑑1𝑑2 . . . 𝑑𝑘 ) // Generate answers 𝑎1, 𝑎2, . . . , 𝑎𝑘 LLM.answer(𝑞, 𝑒1𝑒2 . . . 𝑒𝑘 ) for each document 𝑑𝑖 do if LLM.validate(𝑎𝑖, 𝑑𝑖 ) is correct then Store (𝑞, 𝑑𝑖 ) as matched pair end if end for end for 25: 26: end if 27: // Step 3: Filter out pseudo-relevant match pairs 28: for each matched pair (𝑞, 𝑑) do 29: 𝑟𝑒𝑙𝑠𝑐𝑜𝑟𝑒 LLM.filter_score(𝑞, 𝑑) if 𝑟𝑒𝑙𝑠𝑐𝑜𝑟𝑒 < 𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑 then Remove (𝑞, 𝑑) 30: 31: end if 32: 33: end for AutoMIR: Effective Zero-Shot Medical Information Retrieval without Relevance Labels Table 7: Dataset Collection Sources and Quantity Statistics. Dataset MedExam DuBaike DXYDisease MedicalRetrieval CmedqaRetrieval DXYConsult CovidRetrieval IIYiPost CSLCite CSLRel Query URL https://github.com/jind11/MedQA https://github.com/baidu/DuReader https://dxy.com/diseases https://huggingface.co/datasets/C-MTEB/MedicalRetrieval https://huggingface.co/datasets/C-MTEB/CmedqaRetrieval https://dxy.com/questions/ https://huggingface.co/datasets/C-MTEB/CovidRetrieval https://bbs.iiyi.com/ https://github.com/ydli-ai/CSL https://github.com/ydli-ai/CSL #Samples 3,426 20,000 61,840 1,000 3,999 13,057 949 37,065 934 Document URL https://github.com/jind11/MedQA https://baike.baidu.com/ https://dxy.com/diseases https://huggingface.co/datasets/C-MTEB/MedicalRetrieval https://huggingface.co/datasets/C-MTEB/CmedqaRetrieval https://dxy.com/questions/ https://huggingface.co/datasets/C-MTEB/CovidRetrieval https://bbs.iiyi.com/ https://med.wanfangdata.com.cn/ https://med.wanfangdata.com.cn/ #Samples 27,871 56,441 61,840 100,999 100,001 13,057 100,001 37,065 36,783 36,783 A.2 Data Example The datasets we constructed encompass various real-world medical scenarios, with examples from 10 different datasets illustrated in Figure 3. Queries can take the form of medical paper title, patients symptom description, or an exam question. Corresponding documents include abstracts of medical papers, doctor-patient diagnostic conversations, and reference materials for exam questions. MODELS B.1 Baselines To comprehensively evaluate the performance of existing retrievers on CMIRB, we selected 10 representative models, all of which have achieved strong results on the MTEB leaderboard12. Table 8: Evaluation Prompts for LLMs. Q2P Prompt Please generate medical content paragraph to answer this question. Question: [QUESTION] Paragraph: T2P Prompt Please generate medical content paragraph based on this title. Title: [TITLE] Paragraph: P2P Prompt Please generate similar medical paragraph for the following text. Text: [TEXT] Similar Paragraph: BM25. BM25 [28] is commonly used baseline retriever which uses bag-of-words and TF-IDF to perform lexical retrieval. In this paper, BM25 is implemented with Pyserini [17] using the default hyperparameters to index snippets from all corpora. Text2Vec13. motivated pre-trained language model (LERT). It is cosine sentence model based on linguisticallyContriever 18. It is multilingual dense retriever with contrastive learning, which fine-tunes the pre-trained mContriever model on MS MARCO dataset. M3E 19. M3E (Moka Massive Mixed Embedding) is bilingual text embedding model trained on over 22 million Chinese sentence pairs, supporting tasks like cross-lingual text similarity and retrieval. PEG14. Wu et al., [39] proposes the PEG, which is trained on more than 100 million data, encompassing wide range of domains and covering various tasks. mE5 20. Multilingual E5 text embedding models that are trained with multi-stage pipeline, involving contrastive pre-training on 1 billion multilingual text pairs, and fine-tuning on labeled datasets. BGE15. It takes compound recipe to train general-purpose text embedding, including, embedding-oriented pre-training, contrastive learning with sophisticated negative sampling, and instructionbased fine-tuning. GTE16. It presents multi-stage contrastive learning approach to develop text embedding model that can be applied to various tasks. Piccolo17. Piccolo is general-purpose Chinese embedding model trained using two-stage process with weakly supervised and manually labeled text pairs. OpenAI-Ada-002 21. It is highly efficient text embedding model that converts natural language into dense vectors for wide range of applications, including semantic search and similarity tasks. For the generator, we selected three highly powerful large language models. Qwen222. Qwen2 is comprehensive suite of foundational and instruction-tuned language models, encompassing parameter range from 0.5 to 72 billion, featuring dense models and Mixtureof-Experts model. 12https://huggingface.co/spaces/mteb/leaderboard 13https://huggingface.co/GanymedeNil/text2vec-large-chinese 14https://huggingface.co/TownsWu/PEG 15https://huggingface.co/BAAI/bge-large-zh-v1.5 16https://huggingface.co/thenlper/gte-large-zh 17https://huggingface.co/sensenova/piccolo-large-zh 18https://huggingface.co/facebook/mcontriever-msmarco 19https://huggingface.co/moka-ai/m3e-large 20https://huggingface.co/intfloat/multilingual-e5-large 21https://openai.com/index/new-and-improved-embedding-model/ 22https://huggingface.co/Qwen/Qwen2-7B-Instruct Table 9: Performance of various Retrieval models on CMIRB benchmark. All scores denote Recall@100. The best score on given dataset is marked in bold. Li et al. Task Dataset BM25 Text2Vec(large) mContriever mE5(large) M3E(large) GTE(large) piccolo(large) PEG(large) BGE(large) Knowledge Retrieval Consulation Retrieval News MedExam DuBaike DXYDis. Medical Cmedqa DXYCon. Covid 96.47 88.83 84.93 97.05 93.26 99.47 99.47 98.74 98.10 72.91 78.01 84.06 96.02 93.55 95.86 96.81 98.01 96.81 17.26 42.99 53.40 57.95 70.61 84.95 84.81 84.64 82. 44.20 52.80 61.50 70.90 74.00 87.00 82.80 83.70 81.20 56.92 79.25 86.48 98.43 98.43 96.54 99.06 98.74 98.74 75.61 89.81 93.40 93.83 86.08 87.52 89.67 95.41 97.42 37.33 64.58 62.67 80.38 86.96 89.50 91.09 89.50 91.30 Post Literature Retrieval IIYiPost CSLCite 89.98 74.78 70.72 91.64 88.97 93.41 95.69 96.83 95.69 67.19 61.96 72.25 77.31 76.09 83.25 83.07 81.15 80.80 CSLRel Average 72.66 70.39 84.97 91.12 96.58 96.58 92.25 92.25 96.36 63.05 70.34 75.44 85.46 86.45 91.41 91.47 91.90 91.90 Table 10: Performance of different combinations of generators and retrievers on CMIRB benchmark. Task Dataset Knowledge Retrieval Consulation Retrieval MedExam DuBaike DXYDis. Medical Cmedqa DXYCon. News Covid ChatGLM3 as Generator + BGE as Retriever Post IIYiPost CSLCite Literature Retrieval CSLRel Average HyDE SL-HyDE Improve. 61.96 67.12 8.33% 54.25 59.40 9.49% 71.07 72.25 1.66% 56.32 57.16 1.49% 37.73 38.77 2.76% 45.23 49.71 9.90% 73.89 76. 70.88 45.11 72.29 45.81 3.91% 1.99% 1.55% 43.80 46.98 7.26% 56.02 58.63 4.65% HyDE SL-HyDE Improve. 53.10 64.88 22.18% 45.78 56.30 22.98% 68.34 69.81 2.15% HyDE SL-HyDE Improve. 65.18 71.36 9.48% HyDE SL-HyDE Improve. 64.87 72.04 11.05% 56.35 59.50 5.59% 55.04 60.26 9.48% 75.77 74.95 1.08% 78.18 77.59 0.75% Llama2 as Generator + BGE as Retriever 72.90 77.17 44.19 44.62 2.19% 18.02% 20.64% 5.86% 0.32% 0.97% 53.51 54. 31.29 36.93 37.07 44.72 72.22 71.99 46.41 46.88 1.01% 52.48 56.80 8.23% Qwen2 as Generator + mE5 as Retriever 75.92 77. 54.31 54.68 0.68% 32.02 33.95 6.03% 43.12 45.87 6.38% 45.66 45.65 2.20% 1.97% 0.02% 23.84% 3.90% 54.80 56.94 68.15 66. 31.50 39.01 Qwen2 as Generator + PEG as Retriever 80.49 80.89 58.47 59.81 2.29% 41.47 40.43 2.51% 49.25 52.68 6.96% 43.56 47.53 0.50% 4.72% 9.11% 24.47% 5.48% 57.80 60.97 72.51 75.93 34.17 42.53 ChatGLM323. ChatGLM3-6B is next-generation conversational pre-trained model with strong performance across tasks like semantics, reasoning, and code execution, and supports complex scenarios such as tool use and function calls. Llama224. Llama2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions utilize supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. B.2 Evaluation Prompt We use the C-MTEB25 framework to evaluate the performance of various retrieval models on CMIRB. To ensure stability, we set the temperature coefficient to 0 when generating hypothetical documents for LLMs, producing deterministic results. For each dataset, 23https://huggingface.co/THUDM/chatglm3-6b 24https://huggingface.co/meta-llama/Llama-2-7b-chat-hf 25https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB the prompts used to generate pseudo-documents are shown in Figure 8. The IIYIPost and CSLCite datasets utilize the T2P template to prompt LLMs to generate documents based on the given title. For the CSLRel dataset, we employ the P2P template to instruct the model to produce similar text. As for the other datasets, the Q2P template is employed by the LLM to generate answers to medical questions. MORE EXPERIMENT RESULTS Table 9 presents the performance of 10 retrieval models on CMIRB in terms of Recall@100. In Table 10, we present more detailed breakdown of the performance of various LLM and retriever combinations across the 10 datasets. AutoMIR: Effective Zero-Shot Medical Information Retrieval without Relevance Labels Table 11: Prompts for Data Processing (1). Medical Relevance Prompt (LLM.med_score) You will receive question-answer pair from Baidu Search. Your task is to evaluate whether the Q&A is related to the medical field and output the result in JSON format. The JSON object must include the following keys: - \"reason\": string explaining the reason for your judgment. - \"label\": an int, 0/1. Please adhere to the following steps: - If the content mentioned in the question and answer includes medical information and is related to the medical field, the label should be 1. - If most of the content in the question and answer is unrelated to the medical field, the label should be 0. You need to make judgment and provide reason. Please output the result as required, and do not output any other content. Here is the text: Question: [QUESTION] Answer: [ANSWER] Passage Reranking Prompt (LLM.reranking) You will be given medical question-answer pair (Q-A) and its Top-20 most relevant documents. Your task is to select up to 3 most relevant and independent reference documents to verify or support the given answer accurately and efficiently. Your output is JSON object, which must contain the following keys: - 𝑑𝑜𝑐_𝑖𝑑: list, the document IDs you selected, ranked by importance, such as 𝑖𝑑1. - \"explanation\": string, the reasons for your selection of these documents. Please adhere to the following steps: - 1. Read the medical question and its answer. - 2. Examine the Top-20 retrieved documents to identify which documents independently contain the essential information needed to support or verify the provided answer. - 3. If suitable documents are found, select up to 3 documents that can independently support the answer and explain why these documents are relevant. If none of the documents are suitable, do not select any. The \"explanation\" should be in Chinese. and your output must always be JSON object, do not output anything else. Now here are the medical Q-A pair and Top-20 documents: Q-A pair: [Q-A PAIR] Documents: [DOCUMENTS] Evidence Extracting Prompt (LLM.extract_evidence) You will be given medical question, its answer and related document. Your task is to extract evidence spans from the document that directly or indirectly support the answer to the medical question. Your output is JSON object, which must contain the following keys: - \"evidence_spans\": list, list of passages. Please adhere to the following steps: - 1. Carefully read the medical question and its answer. - 2. Review the content of the provided document. or verify the provided answer. - 3. Identify and extract the passage from the document that directly supports the correct answer to the question. - 4. If no passage in the document can directly support the correct answer or answer the question, return an empty list. The \"explanation\" should be in Chinese. and your output must always be JSON object, do not output anything else. Here is the medical question, its answer, and the related document Question: [QUESTION] Answer: [ANSWER] Document: [DOCUMENT] Table 12: Prompts for Data Processing (2). Li et al. Answer by Evidence Prompt (LLM.answer) You will be given medical exam question and one or more evidence spans that were extracted from related documents. Your task is to provide detailed and comprehensive answer to the question based solely on the provided evidence spans. Your output is JSON object, which must contain the following keys: - \"answer\": string, the answer you derive from the reference documents. - \"reason\": detailed explanation of your reasoning process leading to the answer. Please adhere to the following steps: - 1. Review the exam question. - 2. Review the provided evidence spans. - 3. Based solely on the information contained in the evidence spans, provide detailed and comprehensive answer to the question. - 4. If the evidence spans do not provide sufficient information to answer the question, state \"The evidence passage can not answer the question.\" in \"answer\" and explain why. If you dont know the answer, dont guess. You must not use any common knowledge, personal knowledge, or external information beyond the provided evidence spans. The \"answer\" and \"reason\" should be in Chinese. and your output must always be JSON object, do not output anything else. Now here are the exam question and reference documents. Question: [QUESTION] Evidence Spans: [EVIDENCE SPANS] Validate Answer Prompt (LLM.validate) You will be given medical question, reference (standard) answer, and model-generated answer. Your task is to evaluate the content similarity between the reference answer and the model-generated answer to determine whether they are conveying the same meaning. Your output is JSON object, which must contain the following keys: - \"similarity_score\": number between 0 and 1 indicating the content similarity between the two answers. - \"explanation\": detailed explanation of the similarities or differences that justify your similarity score. Please adhere to the following steps: - 1. Carefully read the medical question. - 2. Review the reference answer and the model-generated answer. - 3. Compare the two answers, focusing on content similaritywhether they convey the same meaning, and lead to the same conclusion. - 4. Provide similarity score between 0 and 1, where 1 indicates that the answers are identical in meaning, and 0 indicates different. - 5. Justify your score by explaining the similarities or differences between the two answers. The \"explanation\" should be in Chinese. and your output must always be JSON object, do not output anything else. Now here are the question, standard answer, and generated answer. Question: [QUESTION] Reference Answer: [REFERENCE ANSWER] Model-generated Answer: [MODEL-GENERATED ANSWER] Query-Document Relevance Prompt (LLM.filter_score) You will be given medical search query and its associated passage. Your task is to evaluate the quality of query-passage pairs intended for use in medical encyclopedia knowledge retrieval evaluation dataset. Your output is JSON object, which must contain the following keys: - \"quality_score\": an integer, score from 1 to 5. - \"explanation\": string, providing brief rationale for the given score. Please adhere to the following steps: - 1. Carefully read the query to understand the users information need. - 2. Review the passage to assess its relevance and targeted content in relation to the query. - 3. Assign quality score from 1 to 5 and explain your reasoning. The \"explanation\" should be in Chinese. and your output must always be JSON object, do not output anything else. Now here are the query and passage. Query: [QUERY] Passage: [PASSAGE] AutoMIR: Effective Zero-Shot Medical Information Retrieval without Relevance Labels Figure 3: An overview of the diverse datasets in CMIRB benchmark."
        }
    ],
    "affiliations": [
        "Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China"
    ]
}