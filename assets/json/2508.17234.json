{
    "paper_title": "ClaimGen-CN: A Large-scale Chinese Dataset for Legal Claim Generation",
    "authors": [
        "Siying Zhou",
        "Yiquan Wu",
        "Hui Chen",
        "Xavier Hu",
        "Kun Kuang",
        "Adam Jatowt",
        "Ming Hu",
        "Chunyan Zheng",
        "Fei Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Legal claims refer to the plaintiff's demands in a case and are essential to guiding judicial reasoning and case resolution. While many works have focused on improving the efficiency of legal professionals, the research on helping non-professionals (e.g., plaintiffs) remains unexplored. This paper explores the problem of legal claim generation based on the given case's facts. First, we construct ClaimGen-CN, the first dataset for Chinese legal claim generation task, from various real-world legal disputes. Additionally, we design an evaluation metric tailored for assessing the generated claims, which encompasses two essential dimensions: factuality and clarity. Building on this, we conduct a comprehensive zero-shot evaluation of state-of-the-art general and legal-domain large language models. Our findings highlight the limitations of the current models in factual precision and expressive clarity, pointing to the need for more targeted development in this domain. To encourage further exploration of this important task, we will make the dataset publicly available."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 4 3 2 7 1 . 8 0 5 2 : r ClaimGen-CN: Large-scale Chinese Dataset for Legal Claim Generation Siying Zhou1, Yiquan Wu1, Hui Chen1, Xavier Hu1, Kun Kuang1 Adam Jatowt2, Ming Hu1, Chunyan Zheng1, Fei Wu1 1Zhejiang University, Hangzhou, China 2University of Innsbruck, Innsbruck, Austria {zhousiying, wuyiquan, 22402119, kunkuang,hm606, boxzheng}@zju.edu.cn xavier.hu.research@gmail.com, adam.jatowt@uibk.ac.at, wufei@cs.zju.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Legal claims refer to the plaintiffs demands in case and are essential to guiding judicial reasoning and case resolution. While many works have focused on improving the efficiency of legal professionals, the research on helping non-professionals (e.g., plaintiffs) remains unexplored. This paper explores the problem of legal claim generation based on the given cases facts. First, we construct ClaimGenCN, the first dataset for Chinese legal claim generation task, from various real-world legal disputes. Additionally, we design an evaluation metric tailored for assessing the generated claims, which encompasses two essential dimensions: factuality and clarity. Building on this, we conduct comprehensive zero-shot evaluation of state-of-the-art general and legaldomain large language models. Our findings highlight the limitations of the current models in factual precision and expressive clarity, pointing to the need for more targeted development in this domain. To encourage further exploration of this important task, we will make the dataset publicly available."
        },
        {
            "title": "Introduction",
            "content": "Over the past decades, the advancement of natural language processing (NLP) techniques has advanced the field of Legal Artificial Intelligence (Legal AI). Legal AI is an important subfield of artificial intelligence, with the goal of supporting individuals across various legal tasks, including legal judgment prediction (Zhong et al., 2018; Xu et al., 2020; Wu et al., 2023), court view generation (Wu et al., 2020), similar case matching (Bhattacharya et al., 2020), legal language understanding (Chalkidis et al., 2022), and legal question answering (Zhong et al., 2020b). As Legal AI systems become more advanced, it is essential to reflect not only on what these systems can do, but also on what *Corresponding Authors. they ought to contribute to society. If we could push the boundaries of what we envision AI to be, to the point where we explicitly require it to have positive impact on people and communities, ensuring our definition of success explicitly includes that, AI could make the world better place (Li, 2023). Yet, the existing research predominantly targets courtroom trials and judge assistance (Ma et al., 2021; Malik et al., 2021; Feng et al., 2022; Zhang et al., 2023; Le et al., 2024; Li et al., 2025), with limited attention to pre-trial contexts or non-professional needs, such as legal claim generation. To promote the rule of law and make legal support more accessible, we advocate exploring the problem of legal claim generation. As Figure 1 shows, different legal scenarios play different roles. The pre-court scenario is primarily dedicated to preparing claims for the plaintiffs, whereas the incourt scenario is where those claims are utilized. Our work is concentrated on the pre-court phase, focusing on the interests of the parties involved. To the best of our knowledge, we are the first to explore the problem of legal claim generation and to reaffirm its significance in civil litigation. Technically, legal claim generation introduces two key challenges that set it apart from prior Legal AI tasks. First, it is inherently open-ended: models must generate legal claims directly from factual narratives, without the guidance of predefined templates, as is common in judgment generation. Second, the input is typically authored by non-experts. Unlike court view generation, which builds on legally structured texts written by judges, this task requires interpreting informal, often unstructured, and emotional descriptions. The model must extract relevant facts, infer the underlying legal intent, and articulate it as clear and valid claim. To support research on legal AI in the field of civil litigation, several legal datasets have been constructed (Wang et al., 2018; Long et al., 2019; Xiao Figure 1: Conceptual overview of the differences between the pre-court scenario (left part) and the in-court scenario (right). In the pre-court scenario, claims are generated and prepared, which are then addressed and resolved during the in-court scenario. It is important to note that unreasonable claims may lead to loss. et al., 2021). However, existing work still faces the following challenges: 1) Insufficient Data Coverage: Many existing datasets are limited to one or few causes of action. For instance, both AC-NLG (Wu et al., 2020) and MSJudge (Ma et al., 2021) focus only on the private lending category in civil cases. 2) Lack of diverse and fine-grained metrics: Commonly used metrics like BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) focus mainly on n-gram overlap and are not comprehensive enough for evaluating the completeness of key information in legal texts. To address the limited scope and diversity of existing legal datasets, we manually construct dataset from real-world cases covering one hundred causes of action. Moreover, to better evaluate model performance in claim generation, we introduce several fine-grained criteria, which include two essential dimensions: factuality and clarity. To better understand current model capabilities, we further conduct comprehensive zero-shot evaluation of state-of-the-art general and legal-domain large language models on the legal claim generation task. To summarize, our main contributions are as follows: We explore legal AI from the perspective of non-professionals (e.g., plaintiffs), and extend its scope to novel and significant problem: legal claim generation. We construct large-scale Chinese dataset, ClaimGen-CN, built from raw civil legal documents. Furthermore, we introduce two key aspects for assessing claims: factuality and clarity. We conduct comprehensive zero-shot evaluation of state-of-the-art general and legaldomain large language models on the legal claim generation task. Our detailed error analysis reveals notable limitations in current models, particularly in terms of factual accuracy and linguistic clarity. To motivate other scholars to investigate this novel and important problem, we make the ClaimGen-CN dataset publicly available1."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Legal AI Legal AI aims to enhance tasks within the legal domain through the utilization of artificial intelligence techniques (Zhong et al., 2020a; Katz et al., 2023). The field has evolved from foundational developments in legal question answering and entity recognition, as explored by early researchers like Monroy et al. (2009) and Cardellino et al. (2017), to more advanced applications. The integration of AI in areas such as court view generation and legal summarization has shown its potential to process and interpret complex legal texts (Wu et al., 2020; Hachey and Grover, 2006). Recent advancements in legal language understanding and predictive models for legal judgments demonstrate the significant strides made in applying AI to legal analysis (Bommarito et al., 2018; Zhong et al., 2018; Liu et al., 2023a). The ongoing innovations in the field, particularly in the development of legal pre-trained models and event detection in legal documents, are further evidenced in the research by Chalkidis et al. (2020), showcasing the transformative impact of AI on legal research and practice. 2.2 Legal Datasets Recent advancements in Legal AI have been significantly driven by the development and use of 1https://github.com/JosieZhou00/ClaimGen-CN specialized legal datasets. For instance, the European Court of Human Rights (ECHR) dataset provided rich text data for human rights case law research, further diversifying the applications of AI in legal research (Aletras et al., 2016). LEXGLUE dataset provided comprehensive benchmark for various legal NLP tasks, offering an invaluable resource for testing and improving AI models in the legal domain (Chalkidis et al., 2020). Additionally, the Contract Understanding Atticus Dataset (CUAD) was designed for AI-based contract review and analysis, enabling more accurate and efficient processing of legal contracts (Hendrycks et al., 2021). TWLJP dataset was collected using indictments to assist prosecutors in indexing charges and crime factual descriptions (Chien et al., 2024). However, existing works primarily aim to assist experts such as judges and prosecutors, often overlooking common individuals in need of help with regards to their infringed rights. Our work shifts the research perspective from aiding legal professionals to assisting the non-expert public."
        },
        {
            "title": "3 Problem Formulation",
            "content": "We first clarify the definition of the terms as follows. Facts and Reasons consist of two parts: brief narrative detailing the disputes background, including the legal relationship, time, place, specific case details, cause, process, circumstances, and consequences; and the grounds upon which the plaintiff seeks court relief. Evidence is encompassed in the plaintiffs presentation of facts and reasons. We define facts and reasons as token sequence . Claims can be singular or multiple, each representing specific demand to the court for the protection of civil rights. claim is defined as token sequence c. For given case, there can be several claims, which can be denoted as = {c1, c2, ..., cn}, where is the number of claims. Common types of legal claims include: 1) Confirmation of Legal Relationships. This claim seeks court confirmation for certain legal relationships, such as the confirmation of the status of missing or deceased individual. 2) Enforcement of Obligations. This claim compels the opposing party to fulfill payment obligations. Examples include seeking compensation for losses, demanding the repayment of loan principal and interest, or demanding compliance with contractual obligations. 3) Alteration or Termination of Civil Legal Relationships. This claim covers demands for changes or termination of specific civil legal relationships. Examples include filing for divorce or demanding cancellations of contracts. Then the problem can be defined as: Problem 1 (Claim generation). Given the facts and reasons , the task is to predict the claims C."
        },
        {
            "title": "4 Dataset",
            "content": "4.1 Dataset Construction We construct ClaimGen-CN, Chinese dataset for legal claim generation, from 207,748 civil documents sourced from China Judgments Online2. While these documents span civil, criminal, and administrative cases, our work specifically focuses on civil cases for two main reasons. Civil cases, distinct from criminal or administrative cases, often involve private litigants and wider variety of claims, making them more relevant for our research on claim generation. In criminal cases, since most claims are brought by professionals (prosecutors) and private prosecutions are rare (Krauss, 2009; Xiong, 2021), the generated claims carry lesser significance in promoting the accessibility of judicial assistance. In administrative litigation, the courts decisions are not strictly bound by the plaintiffs claims (He, 2022); unlike civil litigation, which follows the plaintiff requests, and the court adjudicates accordingly approach, the generated claims do not have as significant an impact on judges in administrative cases as they do in civil cases. Civil cases constitute the majority of the three categories of cases, accounting for 87% of all cases, followed by criminal cases at 10%, and administrative cases at 3%, according to publicly available data3. Furthermore, civil litigation is the most common legal proceeding encountered in everyday life. 4.1.1 Raw Data Processing Each initial document contains unstructured content over the entire document along with additional information, including the cause of action, title, 2 http://wenshu.court.gov.cn/ 3The proportions of the three case types are calculated based on statistics from the China Judgments Online website as of December 6, 2023: 87,618,891 civil documents, 10,010,356 criminal documents, and 3,031,224 administrative documents. start time, and end time. To maintain material homogeneity, we only select first-instance civil judgment documents and filter out those that are not publicly available for some reason. To ensure completeness, we segment the content, retaining only the task-related segments. To assess completeness, we use sequential inclusion criterion with the following keywords: file lawsuit with this court, facts and reasons, argument, this court believes, and judgment as follows. The content has been then segmented into specific sections, including introduction, plaintiffs facts, plaintiffs claims, defendants arguments, courts findings, and judgment. For our task, we exclusively utilize plaintiffs facts as input and plaintiffs claims as output. 4.1.2 Structured Data Among all the sampled data, there are 134 causes of action. Due to insufficient data in some categories, we retain the top 100 most common civil causes of action for our main dataset, which we call ClaimGen-CN. In addition, we constructed test set, ClaimGen-CN-test, by exclusively selecting cases where the court fully supports the plaintiffs claims. This allows us to obtain reference claims that are both reasonable and legally grounded. This test set still follows the original distribution of causes of action from the sampled data. Table 1 presents the statistics of the processed datasets; all the experiments are conducted on the same datasets. Type ClaimGen-CN ClaimGen-CN-test # Cause of Action # Sample Avg. # Fact Tokens Avg. # Claim Tokens 100 207,748 353.4 134.3 100 1,000 379.2 120.1 Table 1: Statistics of datasets. 4.2 Dataset Characteristics The comparison of ClaimGen-CN with other openaccess legal datasets is presented in Table 2. We discuss the characteristics of ClaimGen-CN based on the following aspects. Diverse. Prior research mainly centered on private lending disputes, the most common cause of action. Among the existing open-source datasets, ClaimGen-CN stands out as the only one with more than 10 distinct civil case categories. ClaimGenCN encompasses and openly shares diverse range of case categories that were absent in prior work, including private lending disputes, divorce disputes, sales contract disputes, labor disputes, residential lease contract disputes, maintenance disputes, education and training contract disputes, and more. Large-scale. As Table 2 illustrates, ClaimGenCN is currently the largest civil litigation dataset, comprising 207k records and covering wide variety of case types. This diversity in data provides researchers with numerous options for drafting legal claims. The number of tokens in the claims and facts in this dataset is much higher than in the previous datasets, which also introduces greater complexity. Plaintiff-centered. ClaimGen-CN is plaintiffcentered dataset that emphasizes the demands of plaintiffs. This dataset primarily focuses on the connection between the plaintiffs facts and their claims. Additionally, it shifts the focus from supporting judges to helping the non-expert public whose rights have been violated."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Baseline Models We implement the following baseline models for comparison4: GPT-4o (Hurst et al., 2024) is an optimized version of GPT-4 (Achiam et al., 2023), featuring superior natural language processing capabilities and multimodal interaction functions. It can handle various data types including text, images, and audio, and is suitable for wide range of complex tasks and application scenarios. LLaMA3.1 (Grattafiori et al., 2024) possesses powerful multilingual dialogue and code generation capabilities after pre-training and instruction fine-tuning. Claude3.5 (Anthropic, 2024) has enhanced performance, accuracy, and excels at high-level understanding and reasoning, providing reliable and consistent performance. Qwen2.5 (Yang et al., 2024) has strong instruction following ability, improved coding and mathematical ability, can generate long text, understand structured data, and generate structured output. Deepseek-R1 (Guo et al., 2025) is efficient and flexible, and performs well on reasoning, coding, and mathematics tasks with lower training cost. At the same time, it can be applied to various application scenarios that require multimodal processing such as image description genera4To our best knowledge, we are the first to propose the claim generation task, and there are no dedicated previous methods that could be directly applied for this task. Dataset # Samples # Causes Avg. # Claim Tokens Avg. # Fact Tokens Availability Plaintiff-centered AutoJudge (Long et al., 2019) AC-NLG (Wu et al., 2020) MSJudge (Ma et al., 2021) LK (Gan et al., 2021) CCJudge (Zhao et al., 2021) CPEE (Zhao et al., 2022) ClaimGen-CN (Ours) 100,000 66,904 70,482 61,611 123,048 158,625 207,748 1 1 1 1 238 10 100 23.9 77.9 109.8 99 134.3 100.1 158 143 198.7 156 353. Table 2: Comparison of legal datasets. means the value is not accessible. tion. Farui (Alibaba Cloud, 2024) is large-modelbased AI legal advisor trained on professional legal data, capable of legal knowledge understanding, reasoning, and generation. We apply 0-shot settings to GPT-4o (Hurst et al., 2024), LLaMA3.1 (Grattafiori et al., 2024), Claude3.5 (Anthropic, 2024), Qwen2.5 (Yang et al., 2024), DeepSeek-R1 (Guo et al., 2025) and Farui (Alibaba Cloud, 2024). Details of LLMs are provided in Appendix A. 5.2 Experiment Settings Here we describe the implementation of the claim generation method used in our experiments. Note that all LLMs are replaceable in this method. For all models (GPT-4o, Claude 3.5, DeepSeek-R1, Farui, LLaMA3.1, and Qwen2.5), we used the official APIs or open-source checkpoints available via OpenAI, Anthropic, Deepseek, Aliyun, or Hugging Face. See Appendix for full API URLs. We adopt zero-shot setting for all models evaluated in this study. Each model is prompted to generate the plaintiffs legal claims based solely on the factual description of given case. We provide the full factual context as input to the model, without any additional fine-tuning, demonstrations, or retrieval-based augmentation. The prompt used across all models is as follows: Please generate the plaintiffs claims based on the following facts. 5.3 Metrics When constructing our dataset, we have considered including common NLG metrics such as ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), and BERT SCORE (Zhang et al., 2020) to help users evaluate text quality. However, these metrics do not provide good measure of the overall claim quality (Liu et al., 2016; Novikova et al., 2017; Chaganty et al., 2018; Sellam et al., 2020; Deutsch and Roth, 2021; Liu et al., 2023b). To better assess claim quality, we introduce two metrics: factuality and clarity for evaluating the claims. We employ LLM, specifically GPT-4o (Hurst et al., 2024), for scoring. GPT-4o evaluates claims using defined prompts for these metrics, as detailed in Appendix C. The robustness and reliability of our proposed metrics are detailed in Appendix D.2. ROUGE (Lin, 2004) is set of metrics used in the NLP task. We keep the results of ROUGE1, ROUGE-2, and ROUGE-L. ROUGE-1 and ROUGE-2 refer to the overlap of unigram and bigram between the generated and reference documents, respectively. ROUGE-L is Longest Common Subsequence (LCS) based statistics. BLEU (Papineni et al., 2002) is used for automatic text-generation evaluation and correlates well with human judgment. We evaluate using BLEU-1, BLEU-2, and average scores from BLEU-1 through BLEU-4. BERT SCORE (Zhang et al., 2020) measures similarity using contextual embeddings. We calculate the precision (p), recall (r), and f1-score to evaluate the information matching degree. Factuality refers to the facts stated in the claims being truthful, accurate, and based on objectively existing circumstances. Clarity means that claims should be specific, providing details such as the amount of compensation for losses and specifying the manner and scope of issuing an apology. Total represents the average of factuality and clarity scores. 5.4 Experimental Results We report the performance of several state-of-theart models on the CLAIMGEN-CN-TEST set using both automatic and human-aligned evaluations. The automatic metrics include BLEU, ROUGE, and BERT SCORE  (Table 3)  , while GPT-4o-based human-aligned scores focus on factuality and clarity  (Table 4)  . Automatic Evaluation. As shown in Table 3, Claude3.5 achieves the highest performance across most automatic metrics, including BLEU, ROUGE, and BERT SCORE f1. Farui ranks second in Method GPT-4o (Hurst et al., 2024) LLaMA3.1 (Grattafiori et al., 2024) Claude3.5 (Anthropic, 2024) Qwen2.5 (Yang et al., 2024) DeepSeek-R1 (Guo et al., 2025) Farui (Alibaba Cloud, 2024) BLEU BB-N R-1 ROUGE R-2 7.25 5.64 9.59 7.81 5.61 8.54 7.85 6.82 9.92 8.22 5.99 9.08 48.98 43.14 49.26 46.26 42.32 48. 22.40 19.12 24.21 22.10 18.29 23.10 R-L 43.78 36.05 42.95 41.00 37.38 42.55 BERT SCORE f1 76.06 69.38 76.56 77.76 77.59 76.28 65.86 70.58 67.70 64.46 60.56 66.45 70.46 69.64 71.66 70.34 67.91 70.85 B-1 19.07 16.95 22.07 18.59 14.16 20.81 Table 3: Automatic evaluation results on the CLAIMGEN-CN-TEST set using ROUGE, BLEU, and BERT SCORE. The best is bolded and the second best is underlined. Method GPT-4o SCORE Factuality Clarity Total GPT-4o (Hurst et al., 2024) LLaMA3.1 (Grattafiori et al., 2024) Claude3.5 (Anthropic, 2024) Qwen2.5 (Yang et al., 2024) DeepSeek-R1 (Guo et al., 2025) Farui (Alibaba Cloud, 2024) 48.31 51.32 54.18 54.34 62.14 42. 56.23 55.18 64.16 59.61 69.43 46.28 52.27 53.25 59.17 56.97 65.79 44.56 Table 4: Evaluation results on the CLAIMGEN-CN-TEST set using GPT-4o. Total represents the average of factuality and clarity scores. The best is bolded and the second best is underlined. BLEU and BERT SCORE f1, while GPT-4o leads in ROUGE-L but is outperformed by Claude3.5 and Qwen2.5 on other metrics. These results suggest that Claude3.5 generates outputs with higher n-gram overlap and semantic similarity to the references, as captured by both lexical and contextual metrics. absolute error (MAE) and consistency score between GPT-4o and human annotations over 100 randomly sampled cases generated by DeepSeekR1. We adopt Factuality and Clarity as evaluation dimensions. Three annotators are asked to assign scores on 1-to-5 scale, where 1 denotes the worst and 5 the best. GPT-4o Evaluation. To better understand the alignment between generated claims and human judgments, we use GPT-4o to evaluate outputs based on Factuality and Clarity  (Table 4)  . DeepSeek-R1 significantly outperforms other models with total score of 65.79, showing the best factual accuracy and clarity. Claude3.5 again demonstrates strong performance with the second-best total score, while Qwen2.5 and LLaMA3.1 achieve moderate scores. While GPT-4o performs well on automatic metrics, its scores in GPT-4o-based evaluations are lower than those of several other models. Overall, Claude3.5 stands out in terms of automatic metrics, whereas DeepSeek-R1 excels in GPT-4o evaluation. This discrepancy underscores the importance of incorporating human-centric evaluation to complement automatic metrics, especially for complex legal text generation tasks. 5.5 Human Evaluation To assess the alignment between GPT-4o evaluations and human judgments, we calculate the mean Dimension MAE Consistency Factuality Clarity 0.19 0. 81.05 73.68 Table 5: MAE and consistency between GPT-4o scores and human annotations on 100 DeepSeek-generated samples. As shown in Table 5, GPT-4o achieves MAE of 0.19 and consistency of 81.05 on the factuality dimension, and MAE of 0.20 with 73.68 consistency on clarity. These results suggest that GPT-4o evaluations are closely aligned with human judgments, as evidenced by the low MAE and high consistency scores across both dimensions. In particular, the model demonstrates stronger agreement with human ratings on factuality, achieving consistency of 81.05, compared to 73.68 on clarity. This indicates that while GPT-4o can serve as trustworthy proxy for human annotation overall, its performance is more stable and reliable in assessing factual correctness than in evaluating clarity, where subjectivity may play greater role. purple parts indicate clarity mistakes in prediction, while the Figure 2: The claim generation of given case. The (cid:58)(cid:58)(cid:58)(cid:58)(cid:58) pink parts are factual mistakes in prediction. Green checkmarks indicate acceptable outputs, while red crosses mark outputs with factual or clarity errors. detailed analysis of human evaluation is provided in Appendix D.1, Appendix D.3 and Appendix D.4. 5.6 Case Study Figure 2 illustrates representative case from the ClaimGen-CN-test set, showcasing the comparative outputs of different large language models (LLMs) when generating legal claims. The factual background involves plaintiff injured in dispute, seeking compensation for subsequent treatment costs not covered by the defendant. The ground truth specifies that the plaintiff requests the defendant to pay total of 16,000 covering medical expenses, lost wages, transportation costs, and litigation expenses. Among the models evaluated, GPT-4o, Qwen2.5, and DeepSeek-R1 generate legally accurate and contextually faithful claims, aligning well with the facts and compensation details. In particular, GPT-4o excels in both factuality and clarity, offering detailed yet concise articulation of the plaintiffs request. In contrast, models like Claude3.5 and LLaMA3.1 either misidentify the defendants or fabricate salary agreements, resulting in factual inconsistencies. Faruis output, although structurally rich, suffers from vague and overly general phrasing, leading to reduced clarity and failure to directly match the required compensation scope. This case highlights the importance of grounding legal claim generation not only in accurate fact extraction but also in the precise mapping of legal entitlements, where minor deviations can significantly impact the validity of generated content."
        },
        {
            "title": "6 Discussion",
            "content": "6.1 Error Analysis Current models exhibit multidimensional deficiencies in legal document generation, primarily manifested through four core aspects: inadequate legal knowledge comprehension, disconnects in legalmathematical logic, polarized deviations in claim generation, and systemic instability in output precision. The following case examples are provided in Appendix to supplement the error analysis discussed in this section. We include full input facts, ground truth and model outputs for all referenced instances: CaseID 1, 2, 3, 11, 93, 98, 110, and 116, in the order of appearance. Lack of Legal Knowledge. We identify two common types of legal knowledge gaps that cause models to generate incorrect or incomplete claims. First, models often fail to reconstruct legally relevant facts from the case description. In CaseID 1, for instance, the model mistakenly assumed that interest began accruing month too early. This shows the lack of ability to track and reason over event timelines based on contractual terms and payment recordsan essential skill for accurate legal fact recognition. Second, models lack domainspecific legal knowledge needed for handling different types of cases. For example, in loan disputes, the validity of claim often depends on understanding the sequence of eventsloan issuance, due date, default, and interestalong with the corresponding legal rules. Without knowledge of such patterns, the model may generate structurally invalid or legally irrelevant claims. 4 = Legal-Mathematical Disconnects. LLMs often struggle with multi-step quantitative legal reasoning. In CaseID 2, for example, most models failed to calculate the correct inheritance share in co-ownership scenario. The legally precise logic50% 1 8 -was reduced to vague statements like proportional division, with no explanation of how the entitlement was derived. This error points to deeper gap between legal texts and computational understanding. Specifically, models lack the ability to interpret and apply statutory provisions such as Articles 1122 and 1130 of the Civil Code of the Peoples Republic of China, which define the structure and sequence of inheritance allocation. As result, they fail to generate legally valid and numerically grounded claims. Polarized Claim Generation. Models exhibit two opposite types of errors when generating claims: some produce redundant requests that are not supported by the facts, while others omit essential claims required by the legal context. In CaseID 3 and CaseID 11, GPT-4o and Qwen2.5 inserted mental damage compensation and interest claims that the plaintiff never requested, likely reflecting patterns overrepresented in debt-related training examples. On the other hand, Qwen2.5 and LLaMA3.1 omitted essential claims such as the confirmation of contract validity in CaseID 11, which is legal prerequisite for pursuing the associated property transfer. These errors reflect two distinct but co-existing problems. First, some models tend to hallucinate legally implausible claims by defaulting to common case patterns rather than the actual fact statements. Second, others fail to identify necessary legal elements that are implied by the factual background or logically required by the cause of action. Consequently, claims that are either excessive or incomplete are produced, which reduces the factual and legal accuracy of the output. Systemic Instability. LLaMA3.1 shows repeated output problems in two ways: (1) In CaseID 93, it copied and pasted over 180 consumer protection law articles without selecting the relevant ones, making the response bloated and meaningless; (2) In CaseID 98, 110, and 116, LLaMA3.1 repeated similar legal claims more than five times, showing lack of control over output length and content. In practical terms, this hurts the clarity and coherence of generated claims, making it difficult for users to identify distinct legal requests. More importantly, these repeated and redundant outputs obscure the logical structure of the plaintiffs position, potentially causing critical claims to be overlooked amid excessive or noisy output. This undermines the functional purpose of legal claim generation as concise, actionable summary of the plaintiffs demands. 6.2 Future Work in Legal Claim Generation Our error analysis reveals deeper challenges in legal claim generation, pointing to several scenariodriven directions for future research. First, largesmall model collaboration can improve factual grounding, such as using lightweight modules to identify key events or legal rules before invoking larger model for structured claim generation. Second, long-chain reasoning techniques may help track complex legal timelinese.g., loan issuance, default, and interest accrualenhancing logical completeness. Third, reinforcement learning with legal-specific feedback can be used to optimize the claim generation process by designing taskspecific reward functions. For instance, models can be penalized for producing claims that lack factual support, and rewarded for generating claims that follow legally valid reasoning paths. Such techniques should be developed in close alignment with real-world legal tasks to ensure reliability and practical value."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we make pioneering step in Legal AI by focusing on generating claims for civil litigation. Our dataset, ClaimGen-CN, covers wide range of legal cases, offering solid foundation for future Legal AI research and application. To evaluate model performance, we propose two dimensions, namely factuality and clarity, which are tailored to the specific requirements of legal text generation. Our error analysis reveals that current LLMs struggle with generating factually accurate and concise claims, suggesting key directions for future improvements."
        },
        {
            "title": "Limitations",
            "content": "In this section, we discuss the limitations of our work: We only interact with the LLMs one round per time. The LLMs are capable of multi-round interaction, which may help the LLMs to better understand the claim generation task. The reliance on LLMs for claim generation raises questions about the transparency and interpretability of the generated claims. The decisionmaking process of these models is often opaque, which could lead to challenges in understanding and justifying the basis on which claims are generated. The exploration of legal claim generation tasks beyond the Chinese legal context remains limited."
        },
        {
            "title": "Ethics Statement",
            "content": "The corpus we use is released by the Chinese government and has been anonymized wherever necessary5. Our dataset thus does not involve any personally private information. Besides, the corpus is in public domain and licensed for use within the legal scope6. With the increasing use of Legal AI for justicerelated tasks, there is growing concern about its ethical implications, including the risk of biases and errors that could have serious repercussions. To address these concerns, its important to clarify that our work is an exploration of new data source and the development of an algorithm, which is not meant to be immediately and directly used in practical settings. Our goal is to assist non-professionals by providing recommendations, not to make final decisions. Measures for releasing the dataset: Our dataset is sourced from publicly available court documents and will undergo further validation to ensure its security. When releasing the dataset, we will provide usage limitations, appropriate scenarios, and guidelines. To minimize the risk of misuse, we will also implement access restrictions, 5Provisions of the Supreme Peoples Court on Publication of Judgment Documents by the Peoples Courts on the http://gongbao.court.gov.cn/Details Internet 2016. /415f49dd8baaa04b479d57af9616ef.html 6See Article 3.3 of User Protocol of China Judgments Online. https://g.alicdn.com/onlineCourt/static/0. 6.97/akan-wenshu-protocol.html including requiring applicants to provide their real identities. Measures for future use: The proposed framework will include warning statements, such as adding the phrase This answer may be incorrect and is for reference only in the output. We will also insert declarations in the prompt instructing the model not to provide answers when confidence is low or requirements are not met, and we will consider additional validation after generation. The proposed framework will also advise users to seek professional legal advice, for instance, adding Before making final decision, you should consult qualified lawyer in the output."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, and 1 others. 2023. GPT-4 technical report. arXiv preprint arXiv:2303.08774. Nikolaos Aletras, Dimitrios Tsarapatsanis, Daniel Preotiuc-Pietro, and Vasileios Lampos. 2016. Predicting judicial decisions of the european court of human rights: natural language processing perspective. PeerJ computer science, 2:e93. Alibaba Cloud. 2024. Tongyi farui: Legal large language model. Accessed May 18, 2025. Anthropic. 2024. Claude 3.5 sonnet model card addendum. https://www-cdn.anthropic.com/f ed9cc193a14b84131812372d8d5857f8f304c52/ Model_Card_Claude_3_Addendum.pdf. Accessed May 18, 2025. Paheli Bhattacharya, Kripabandhu Ghosh, Arindam Pal, and Saptarshi Ghosh. 2020. Hier-spcnet: legal statute hierarchy-based heterogeneous network for computing legal case document similarity. In Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval, pages 16571660. MJ Bommarito, Daniel Martin Katz, and Detterman. 2018. Lexnlp: Natural language processing and information extraction for legal and regulatory texts. Research Handbook on Big Data Law. Cristian Cardellino, Milagro Teruel, Laura Alonso Alemany, and Serena Villata. 2017. Legal NERC with ontologies, wikipedia and curriculum learning. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2017, Valencia, Spain, April 3-7, 2017, Volume 2: Short Papers, pages 254259. Association for Computational Linguistics. Arun Chaganty, Stephen Mussmann, and Percy Liang. 2018. The price of debiasing automatic metrics in natural language evalaution. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 643653. Ilias Chalkidis, Manos Fergadiotis, Prodromos Malakasiotis, Nikolaos Aletras, and Ion Androutsopoulos. 2020. LEGAL-BERT: the muppets straight out of law school. CoRR, abs/2010.02559. Ilias Chalkidis, Abhik Jana, Dirk Hartung, Michael Bommarito, Ion Androutsopoulos, Daniel Katz, and Nikolaos Aletras. 2022. LexGLUE: benchmark dataset for legal language understanding in English. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 43104330, Dublin, Ireland. Association for Computational Linguistics. Kuo-Chun Chien, Chia-Hui Chang, and Ren-Der Sun. 2024. Legal knowledge management for prosecutors based on judgment prediction and error analysis from indictments. Computer Law & Security Review, 52:105902. Daniel Deutsch and Dan Roth. 2021. Understanding the extent to which content quality metrics measure the information quality of summaries. In Proceedings of the 25th Conference on Computational Natural Language Learning, CoNLL 2021, Online, November 10-11, 2021, pages 300309. Association for Computational Linguistics. Alexander R. Fabbri, Wojciech Kryscinski, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir R. Radev. 2021. Summeval: Re-evaluating summarization evaluation. Trans. Assoc. Comput. Linguistics, 9:391409. Yi Feng, Chuanyi Li, and Vincent Ng. 2022. Legal judgment prediction via event extraction with constraints. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 648664. Leilei Gan, Kun Kuang, Yi Yang, and Fei Wu. 2021. Judgment prediction via injecting legal knowledge into neural networks. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, ThirtyThird Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pages 1286612874. AAAI Press. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, and 1 others. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Ben Hachey and Claire Grover. 2006. Extractive summarisation of legal texts. Artif. Intell. Law, 14(4):305 345. Haibo He. 2022. Administrative Litigation Law (3rd Edition). Law Press. Dan Hendrycks, Collin Burns, Anya Chen, and Spencer Ball. 2021. CUAD: an expert-annotated NLP dataset In Proceedings of the for legal contract review. Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, and 1 others. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Daniel Martin Katz, Dirk Hartung, Lauritz Gerlach, Abhik Jana, and Michael Bommarito II. 2023. Natural language processing in the legal domain. arXiv preprint arXiv:2302.12039. Rebecca Krauss. 2009. The theory of prosecutorial discretion in federal law: Origins and developments. Seton Hall Cir. Rev., 6:1. Richard Landis and Gary Koch. 1977. The measurement of observer agreement for categorical data. biometrics, pages 159174. Yuquan Le, Sheng Xiao, Zheng Xiao, and Kenli Li. 2024. Topology-aware multi-task learning framework for civil case judgment prediction. Expert Systems with Applications, 238:122103. Ang Li, Yiquan Wu, Ming Cai, Adam Jatowt, Xiang Zhou, Weiming Lu, Changlong Sun, Fei Wu, and Kun Kuang. 2025. Legal judgment prediction based on knowledge-enhanced multi-task and multi-label text classification. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 69576970, Albuquerque, New Mexico. Association for Computational Linguistics. Dr Fei-Fei Li. 2023. The Worlds See: Curiosity, Exploration, and Discovery at the Dawn of AI. Flatiron Books: Moment of Lift Book, New York. Chin-Yew Lin. 2004. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain. Association for Computational Linguistics. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Chia-Wei Liu, Ryan Lowe, Iulian Serban, Michael D. Noseworthy, Laurent Charlin, and Joelle Pineau. 2016. How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pages 21222132. The Association for Computational Linguistics. Yifei Liu, Yiquan Wu, Yating Zhang, Changlong Sun, Weiming Lu, Fei Wu, and Kun Kuang. 2023a. MLLJP: multi-law aware legal judgment prediction. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 10231034. Yiqi Liu, Nafise Sadat Moosavi, and Chenghua Lin. 2023b. Llms as narcissistic evaluators: When arXiv preprint ego inflates evaluation scores. arXiv:2311.09766. Shangbang Long, Cunchao Tu, Zhiyuan Liu, and Maosong Sun. 2019. Automatic judgment prediction via legal reading comprehension. In Chinese Computational Linguistics: 18th China National Conference, CCL 2019, Kunming, China, October 1820, 2019, Proceedings 18, pages 558572. Springer. Luyao Ma, Yating Zhang, Tianyi Wang, Xiaozhong Liu, Wei Ye, Changlong Sun, and Shikun Zhang. 2021. Legal judgment prediction with multi-stage case representation learning in the real court setting. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 9931002. Vijit Malik, Rishabh Sanjay, Shubham Kumar Nigam, Kripabandhu Ghosh, Shouvik Kumar Guha, Arnab Bhattacharya, and Ashutosh Modi. 2021. ILDC for CJPE: Indian legal documents corpus for court judgIn Proceedings ment prediction and explanation. of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 40464062, Online. Association for Computational Linguistics. Alfredo Monroy, Hiram Calvo, and Alexander Gelbukh. 2009. NLP for shallow question answering of legal documents using graphs. In International Conference on Intelligent Text Processing and Computational Linguistics, pages 498508. Springer. Mavuto Mukaka. 2012. guide to appropriate use of correlation coefficient in medical research. Malawi medical journal, 24(3):6971. Jekaterina Novikova, Ondrej Dusek, Amanda Cercas Curry, and Verena Rieser. 2017. Why we need new evaluation metrics for NLG. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, September 9-11, 2017, pages 22412252. Association for Computational Linguistics. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318. Thibault Sellam, Dipanjan Das, and Ankur P. Parikh. 2020. BLEURT: learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 78817892. Association for Computational Linguistics. Pengfei Wang, Ze Yang, Shuzi Niu, Yongfeng Zhang, Lei Zhang, and ShaoZhang Niu. 2018. Modeling dynamic pairwise attention for crime classification over legal articles. In the 41st international ACM SIGIR conference on research & development in information retrieval, pages 485494. Yiquan Wu, Kun Kuang, Yating Zhang, Xiaozhong Liu, Changlong Sun, Jun Xiao, Yueting Zhuang, Luo Si, and Fei Wu. 2020. De-biased courts view generation with causality. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 763780. Association for Computational Linguistics. Yiquan Wu, Siying Zhou, Yifei Liu, Weiming Lu, Xiaozhong Liu, Yating Zhang, Changlong Sun, Fei Wu, and Kun Kuang. 2023. Precedent-enhanced legal judgment prediction with LLM and domain-model collaboration. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 1206012075. Association for Computational Linguistics. Chaojun Xiao, Xueyu Hu, Zhiyuan Liu, Cunchao Tu, and Maosong Sun. 2021. Lawformer: pre-trained language model for chinese legal long documents. AI Open, 2:7984. Qiuhong Xiong. 2021. On relationship of public prosecution and private prosecution. Criminal Science. Nuo Xu, Pinghui Wang, Long Chen, Li Pan, Xiaoyan Wang, and Junzhou Zhao. 2020. Distinguish confusing law articles for legal judgment prediction. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 30863095. Association for Computational Linguistics. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, and 22 others. 2024. Qwen2.5 technical report. CoRR, abs/2412.15115. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with BERT. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Zhuo Zhang, Xiangjing Hu, Jingyuan Zhang, Yating Zhang, Hui Wang, Lizhen Qu, and Zenglin Xu. 2023. FEDLEGAL: The first real-world federated learning In Proceedings of the benchmark for legal NLP. 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 34923507, Toronto, Canada. Association for Computational Linguistics. Lili Zhao, Linan Yue, Yanqing An, Ye Liu, Kai Zhang, Weidong He, Yanmin Chen, Senchao Yuan, and Qi Liu. 2021. Legal judgment prediction with multiple perspectives on civil cases. In Artificial Intelligence: First CAAI International Conference, CICAI 2021, Hangzhou, China, June 56, 2021, Proceedings, Part 1, pages 712723. Springer. Lili Zhao, Linan Yue, Yanqing An, Yuren Zhang, Jun Yu, Qi Liu, and Enhong Chen. 2022. CPEE: civil case judgment prediction centering on the trial mode of essential elements. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management, pages 26912700. Haoxi Zhong, Zhipeng Guo, Cunchao Tu, Chaojun Xiao, Zhiyuan Liu, and Maosong Sun. 2018. Legal judgment prediction via topological learning. In Proceedings of the 2018 conference on empirical methods in natural language processing, pages 35403549. Haoxi Zhong, Chaojun Xiao, Cunchao Tu, Tianyang Zhang, Zhiyuan Liu, and Maosong Sun. 2020a. How does NLP benefit legal system: summary of leIn Proceedings of the gal artificial intelligence. 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 52185230. Association for Computational Linguistics. Haoxi Zhong, Chaojun Xiao, Cunchao Tu, Tianyang Zhang, Zhiyuan Liu, and Maosong Sun. 2020b. JECQA: legal-domain question answering dataset. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 97019708. AAAI Press."
        },
        {
            "title": "A Details of LLMs Used in This Paper",
            "content": "As shown in Table 6, we employ six LLMs: GPT4o (Hurst et al., 2024), LLaMA3.1 (Grattafiori et al., 2024), Claude3.5 (Anthropic, 2024), Qwen2.5 (Yang et al., 2024), DeepSeek-R1 (Guo et al., 2025) and Farui (Alibaba Cloud, 2024). The version of GPT-4o is gpt-4o-2024-08-06, the snapshot of gpt-4o from August 6th 2024. As for LLaMA3.1, we use the llama-3.1-8b-instruct version.The version of Claude3.5 that we use is Claude-3-5-sonnet-20241022, and the model was last updated in April 2024. We use Qwen2.5-7binstruct as the version of Qwen, which is an instruction tuning model that performs well on reasoning, coding and mathematics tasks.The version of DeepSeek-R1 that we utilize is deepseek-reasoner, and Faruis version is farui-plus. GPT-4o has context window of 128,000 tokens. LLaMA3.1 (specifically the llama-3.1-8b-instruct version) has context window of 131,072 tokens, surpassing that of Claude3.5 (200,000 tokens) and Qwen2.5 (128,000 tokens). DeepSeek-R1 features context window of 64,000 tokens, and Farui has the smallest context window among them with 12,000 tokens. Each models version and characteristics are tailored to meet different application needs and scenarios."
        },
        {
            "title": "B API Access URLs",
            "content": "GPT-4o (OpenAI): https://platform.o penai.com/docs/models/gpt-4o Claude 3.5 (Anthropic): https: //docs.anthropic.com/en/docs/about-c laude/models/overview DeepSeek-R1 (Deepseek): https://api-d ocs.deepseek.com/ Farui (Aliyun): https://tongyi.aliyun. com/farui/guide/api_description_doc LLaMA3.1 (Hugging Face): https://huggingface.co/meta-lla ma/Llama-3.1-8B-Instruct Qwen2.5 (Hugging Face): https://hugg ingface.co/Qwen/Qwen2.5-7B-Instruct GPT-4o Scoring Prompts The detailed prompts for two metrics are as follows. For factuality. Factuality in claims requires fact be truthful, precise, that statements of and based on objectively existing circumstances. Please rate from 1 to 5 based on factual accuracy. Deduct points for errors by the plaintiff or defendant, discrepancies between factual descriptions and real scenarios, and inconsistencies between claims and actual events, whether adding or omitting details. Model Version Context Window GPT-4o (Hurst et al., 2024) LLaMA3.1 (Grattafiori et al., 2024) Claude3.5 (Anthropic, 2024) Qwen2.5 (Yang et al., 2024) DeepSeek-R1 (Guo et al., 2025) Farui (Alibaba Cloud, 2024) gpt-4o-2024-08-06 llama-3.1-8b-instruct Claude-3-5-sonnet-20241022 Qwen2.5-7b-instruct deepseek-reasoner farui-plus 128,000 tokens 131,072 tokens 200,000 tokens 128,000 tokens 64,000 tokens 12,000 tokens Table 6: Details of used LLMs. For clarity. Clarity refers to statements that should be clear, concise, and unambiguous, avoiding vagueness, ambiguity, and unnecessary redundancy. Please rate from 1 to 5 based on clarity. Deduct points for repetitive or redundant reasoning sections and claims. Excessive additional information will result in deduction of points."
        },
        {
            "title": "D Details about Human Annotation",
            "content": "D.1 Full Annotation Guidelines Provided to Annotators To ensure consistency and fairness in human evaluation, we provided all annotators with standardized set of instructions. Below, we report the full text of the annotation guidelines given to participants, in accordance with ethical best practices. This includes the task description, scoring rubric, and any disclaimers or risk notices if applicable. Annotation Task Description Annotators were instructed to evaluate each modelgenerated legal claim based on two criteria: Factuality and Clarity. Definitions of each are given below. Factuality (15): Factuality in claims requires that statements of fact be truthful, precise, and based on objectively existing circumstances. Please rate from 1 to 5 based on factual accuracy. Deduct points for errors by the plaintiff or defendant, discrepancies between factual descriptions and real scenarios, and inconsistencies between claims and actual events, whether adding or omitting details. Clarity (15): Clarity refers to statements that should be clear, concise, and unambiguous, avoiding vagueness, ambiguity, and unnecessary redundancy. Please rate from 1 to 5 based on clarity. Deduct points for repetitive or redundant reasoning sections and claims. Excessive additional information will result in deduction of points. Scoring Scale Annotators were instructed to assign integer scores from 1 (very poor) to 5 (excellent), with intermediate scores indicating partial satisfaction of the criterion. Disclaimer Annotators were informed that the task involved reading real legal case descriptions and that no personally identifiable information (PII) was included. They were told they could opt out at any time. No personal risks were identified for participants in this task. Figure 3 shows the interface provided to annotators for scoring legal claims generated by the model. D.2 Evaluation Robustness and Metric Validity Meta-evaluation of the proposed metrics on SummEval (Fabbri et al., 2021) includes the Spearman correlation coefficient and Pearson correlation coefficient. The Spearman correlation coefficient and Pearson correlation coefficient between human and GPT-4o scores are respectively 0.5197 and 0.5248, which means moderate correlation (Mukaka, 2012). D.3 Annotation Protocol and Inter-Annotator Agreement The inter-annotator agreement (IAA) between annotators and the correlation between human and GPT-4o scores are detailed as follows. The Fleiss indicatKappa among 3 annotators is 0.6823, ing substantial agreement according to Landis and Kochs interpretation scale (Landis and Koch, 1977), which validates the human evaluation process. The Spearman correlation coefficient between human and GPT-4o scores is 0.5197, and the Pearson correlation coefficient is 0.5248, both of which indicate moderate correlation (Mukaka, 2012). Figure 3: Interface used for human annotation. D.4 Metric Definitions and Scoring Procedure We provide the formal definitions of the evaluation metrics used in Section 5.5, including the normalization process and the calculation of mean absolute error (MAE) and consistency. Normalization. Given raw human and model scores Ji, Pi [smin, smax], we normalize them to the [0, 1] interval as follows: Ji = Ji smin smax smin , Pi = Pi smin smax smin Metric Computation. Let be the number of evaluated samples. The metrics are computed as: MAE = 1 (cid:88) i=1 (cid:12) (cid:12) (cid:12) Ji Pi (cid:12) (cid:12) (cid:12) Consistency = 1 (cid:88) i=1 (cid:18)(cid:12) Ji Pi (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 (cid:19) δ smax smin In our experiments, we use 5-point rating scale where smin = 1, smax = 5, and the tolerance threshold δ = 1. Thus, consistency corresponds to the proportion of samples where the absolute difference between model and human scores does not exceed 1."
        },
        {
            "title": "E Case Examples for Error Analysis",
            "content": "We provide the full input and model-generated claims for the representative cases referenced in Section 6.1. For each case, we list the input facts (prompt), the ground truth legal claim written by human experts (ground truth), and the responses generated by each model. The selected examples include CaseID 1, 2, 31, 93, 98, 110, and 116, covering diverse types of factual patterns and model errors. To facilitate close comparison, we highlight key elements in color: Blue text indicates critical factual expressions within the prompt. Green text marks the key elements in the ground truth claim that correspond to the legal intent. Red text is used to highlight incorrect or hallucinated content within the model outputs. Purple text highlights correct legal reasoning or factual reproduction by the models. These annotated examples serve as the basis for our multi-dimensional error analysis in the main text, enabling fine-grained understanding of how different models succeed or fail in legal claim generation. Figure 4: CaseId 1. Figure 5: CaseId 2. Figure 6: CaseId 3. Figure 7: CaseId 11-part1. Figure 8: CaseId 11-part2. Figure 9: CaseId 93-part1. Figure 10: CaseId 93-part2. Figure 11: CaseId 98. Figure 12: CaseId 110-part1. Figure 13: CaseId 110-part2. Figure 14: CaseId 116-part1. Figure 15: CaseId 116-part2."
        }
    ],
    "affiliations": [
        "University of Innsbruck, Innsbruck, Austria",
        "Zhejiang University, Hangzhou, China"
    ]
}