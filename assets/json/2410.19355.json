{
    "paper_title": "FasterCache: Training-Free Video Diffusion Model Acceleration with High Quality",
    "authors": [
        "Zhengyao Lv",
        "Chenyang Si",
        "Junhao Song",
        "Zhenyu Yang",
        "Yu Qiao",
        "Ziwei Liu",
        "Kwan-Yee K. Wong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel training-free strategy designed to accelerate the inference of video diffusion models with high-quality generation. By analyzing existing cache-based methods, we observe that \\textit{directly reusing adjacent-step features degrades video quality due to the loss of subtle variations}. We further perform a pioneering investigation of the acceleration potential of classifier-free guidance (CFG) and reveal significant redundancy between conditional and unconditional features within the same timestep. Capitalizing on these observations, we introduce FasterCache to substantially accelerate diffusion-based video generation. Our key contributions include a dynamic feature reuse strategy that preserves both feature distinction and temporal continuity, and CFG-Cache which optimizes the reuse of conditional and unconditional outputs to further enhance inference speed without compromising video quality. We empirically evaluate FasterCache on recent video diffusion models. Experimental results show that FasterCache can significantly accelerate video generation (\\eg 1.67$\\times$ speedup on Vchitect-2.0) while keeping video quality comparable to the baseline, and consistently outperform existing methods in both inference speed and video quality."
        },
        {
            "title": "Start",
            "content": "Preprint. FASTERCACHE: TRAINING-FREE VIDEO DIFFUSION MODEL ACCELERATION WITH HIGH QUALITY Zhengyao Lv1 Chenyang Si2 Yu Qiao3 Ziwei Liu2 Kwan-Yee K. Wong1 1The University of Hong Kong 3Shanghai Artificial Intelligence Laboratory Code: https://github.com/Vchitect/FasterCache Junhao Song3 Zhenyu Yang3 2S-Lab, Nanyang Technological University"
        },
        {
            "title": "ABSTRACT",
            "content": "In this paper, we present FasterCache, novel training-free strategy designed to accelerate the inference of video diffusion models with high-quality generation. By analyzing existing cache-based methods, we observe that directly reusing adjacent-step features degrades video quality due to the loss of subtle variations. We further perform pioneering investigation of the acceleration potential of classifier-free guidance (CFG) and reveal significant redundancy between conditional and unconditional features within the same timestep. Capitalizing on these observations, we introduce FasterCache to substantially accelerate diffusion-based video generation. Our key contributions include dynamic feature reuse strategy that preserves both feature distinction and temporal continuity, and CFG-Cache which optimizes the reuse of conditional and unconditional outputs to further enhance inference speed without compromising video quality. We empirically evaluate FasterCache on recent video diffusion models. Experimental results show that FasterCache can significantly accelerate video generation (e.g., 1.67 speedup on Vchitect-2.0) while keeping video quality comparable to the baseline, and consistently outperform existing methods in both inference speed and video quality. 4 2 0 2 5 2 ] . [ 1 5 5 3 9 1 . 0 1 4 2 : r (Lat denotes latency, measured on single A100 GPU. Video synthesis configuration: 192 frames at 480P for Open-Sora, 65 frames at 512512 for Open-Sora-Plan, and 16 frames at 512 512 for Latte.) Figure 1: Comparison of visual quality and inference speed with competing methods."
        },
        {
            "title": "INTRODUCTION",
            "content": "Diffusion transformers (DiT) (Peebles & Xie, 2023) have achieved notable success in image (Chen et al., 2023; 2024b; Esser et al., 2024) and video generation (Ma et al., 2024a; Zheng et al., 2024; Corresponding authors. Project leader. The work was done during an internship at Shanghai AILab. 1 Preprint. Figure 2: Vanilla cache-based acceleration method. Lab & etc., 2024), attracting significant attention for their potential. Although iterative denoising, classifier-free guidance (CFG) (Ho & Salimans, 2022), and transformer attention mechanisms have significantly improved the generative capabilities of diffusion models, they also lead to substantial computational costs and increased memory requirements for inference, especially for video generation which typically takes 2-5 minutes to synthesize 6-second 480P video, limiting their practical use. This calls for the development of new techniques that require less computational cost for diffusion models (Salimans & Ho, 2022; Ma et al., 2024b; Chen et al., 2024c; Zhao et al., 2024c). Among the recently proposed solutions, cache-based acceleration has emerged as one of the most widely adopted approaches. This approach speeds up the sampling process by reusing intermediate features across timesteps, thereby reducing redundant computations and significantly improving computational efficiency. Besides, it requires no additional training costs for inference acceleration and offers straightforward generalization to other video diffusion models. Examples include the cache-based methods for U-Net based diffusion models (Ma et al., 2024b; Li et al., 2023b), residual caching in -DiT (Chen et al., 2024c) for the transformer based diffusion models, and hierarchical attention caching of PAB (Zhao et al., 2024c) for video generation. Despite their proven effectiveness, there exist two critical concerns: 1) Whether directly reusing intermediate features aligns with the iterative denoising mechanism, considering the inherent feature variations between timesteps. 2) Current cache-based methods focus primarily on the attention features within the transformer networks, with limited exploration of accelerating different parts of the pipeline. In this work, we aim to address these two concerns. To thoroughly investigate the acceleration potential of DiT inference for video generation, we delve into the feature reuse process of existing cache-based methods. As shown in Fig. 2, these methods typically assume high degree of feature similarity between adjacent timesteps in the iterative denoising process, and achieve accelerated inference by sharing features across consecutive timesteps. However, our investigation reveals that while features in the same attention module (e.g., spatial attention) appear to be nearly identical between adjacent timesteps, there exist some subtle yet discernible differences. As result, naive feature caching and reuse strategy often leads to degradation of details in generated videos, as shown in Fig. 3 (a). Following this analysis, we further extend the scope of our investigation to explore potential redundancy within the classifier-free guidance (CFG). As shown in Fig. 3 (b), compared to internal network modules (e.g., spatial attention and temporal attention), CFG almost doubles the inference time due to the additional computation required for unconditional outputs. Our experiments reveal notable difference from our earlier conclusion regarding In CFG, the conditional and unattention modules. conditional outputs at the same timestep exhibit very high degree of similarity, suggesting significant information redundancy. In contrast, the similarity of unconditional features between adjacent timesteps is relatively weak. We further discover that the differences between the conditional and unconditional outputs are predominantly concentrated in lowto mid-frequency features during the mid-sampling phase, shifting to highfrequency features in the late-sampling phase, with these differences evolving gradually. Figure 3: (a) Vanilla cache-based methods typically lead to detail loss. (b) Time overhead proportions of different components in video models. Based on the above insights, we propose novel strategy, termed FasterCache, to accelerate the inference of video diffusion models while ensuring high-quality generation and remaining trainingfree. Specifically, we first introduce dynamic feature reuse strategy for attention modules which Preprint. dynamically adjusts the reused features across different timesteps, ensuring both distinction and continuity of features between adjacent timesteps are maintained. This strategy preserves the subtle variations essential for the iterative denoising process while ensuring temporal consistency, resulting in accelerated inference with minimal loss of details in the generated videos. Furthermore, we introduce CFG-Cache, an innovative technique that stores the residuals between conditional and unconditional outputs, dynamically enhancing their high-frequency and low-frequency components before reuse. This significantly accelerates inference while preserving details in generated videos. We evaluate our FasterCache on various video diffusion models, including Open-Sora 1.2 (Zheng et al., 2024), Open-Sora-Plan (Lab & etc., 2024), Latte Ma et al. (2024a), CogVideoX (Yang et al., 2024), and Vchitect-2.0 (Vchitect, 2024). Experimental results demonstrate that FasterCache can significantly accelerate inference while preserving high-quality video generation across all tested models. Specifically, on Vchitect-2.0, FasterCache achieves 1.67 speedup, with performance comparable to the baseline (VBench: baseline 80.80% FasterCache 80.84%). Furthermore, our method outperforms existing approaches in both inference speed and video generation quality, highlighting its effectiveness and efficiency in real-world applications. Overall, the contributions of this work are as follows: We analyze the feature reuse process in cache-based methods and discover that while adjacent-step features in attention modules appear to be similar, their subtle differences can degrade output quality if ignored. We conduct pioneering investigation of CFGs potential for acceleration, finding high redundancy within the same timestep but weaker similarity across adjacent timesteps, revealing new acceleration opportunities. We propose FasterCache, training-free strategy that dynamically adjusts feature reuse, preserving both feature distinction and continuity. It also introduces CFG-Cache to accelerate inference while preserving details in generated videos. We empirically evaluate our approach on various video diffusion models, demonstrating significant improvement in inference speed while maintaining high video quality."
        },
        {
            "title": "2 METHODOLOGY",
            "content": "2.1 PRELIMINARY Diffusion model is generative model consisting of forward process and reverse process. Specifically, its forward diffusion process progressively adds noise to the data x0 pdata(x0), eventually destroying the signal. This can be formulated as: q(xtx0) = (xt; αtx0, 1 αtI), (1) where {αt}T t=1 controls the noise schedules and represents the total number of diffusion timesteps. The reverse process is typically parameterized as UNet or transformer architecture ϵθ which is trained to predict the noise with the following loss function: LDM = Ex,ϵN (0,1),t[ϵ ϵθ(xt, t)2 2]. (2) clean signal x0 can be recovered through iterative inference steps which predict xt1 from xt using ϵθ. This can formulated as: p(xt1xt) = (xt1; µθ(xt, t), Σθ(xt, t)), (3) where µθ and Σθ are the mean and variance parameterized with learnable θ. Video diffusion models recently employ diffusion transformers as the backbone for noise prediction. This work explores video synthesis acceleration based on Open-Sora 1.2 (Zheng et al., 2024). This model is composed of 56 stacked transformer layers, with alternating spatial and temporal layers. Each layer contains not only spatial or temporal attention module but also cross-attention and feed-forward network. Latte (Ma et al., 2024a) and Open-Sora-Plan (Lab & etc., 2024) also adopt similar architecture as their noise prediction networks. 3 Preprint. Classifier-Free Guidance (CFG) has proven to be powerful technique for enhancing the quality of synthesized images/videos in diffusion models. During the sampling process, CFG computes two outputs, namely ϵθ(xt, c) for the conditional input and ϵθ(zt, ) for the unconditional input (often an empty or negative prompt). The final output is given by: ϵθ(xt, c) = (1 + g)ϵθ(xt, c) gϵθ(zt, ), (4) where is the guidance scale. As shown in Fig. 3 (b), while CFG significantly enhances visual quality, it also increases computational cost and inference latency due to the additional computation required for unconditional outputs."
        },
        {
            "title": "2.2 RETHINKING ATTENTION FEATURE REUSE",
            "content": "Attention feature reuse has become primary focus for cache-based acceleration methods in video generation (e.g., pyramid attention reuse of PAB). In video diffusion models, features of attention modules (e.g., spatial attention and temporal attention) exhibit high similarity between adjacent timesteps, as illustrated in Fig. 4. Hence, existing methods completely bypass the attention computations in subsequent timesteps by reusing the cached attention features, thereby significantly reducing computational costs. To gain better understanding of the implications of attention feature reuse in video generation, we first visualize the videos generated with the same random seed and observe that existing feature reuse methods result in noticeable loss of details in the output. For example, as illustrated in Fig. 5, compared to the original video generated without feature reuse, the video generated with vanilla feature reuse exhibits smoother sky, with lack of visible stars, indicating noticeable degradation in fine details. Figure 4: Comparison of the mean squared error (MSE) of attention features between the current and previous diffusion steps. Smaller values indicate higher similarity. Figure 5: Visual quality degradation caused by Vanilla Feature Reuse (left) and feature differences between adjacent timesteps (right). To investigate the underlying causes of this phenomenon, we subsequently visualize the attention features between adjacent timesteps and analyze their differences. The results indicate that while the attention features between adjacent timesteps are highly similar, there exist noticeable differences between them. These subtle variations between timesteps are essential for preserving fine details in video generation. Therefore, directly reusing features without accounting for these differences leads to the loss of important visual information, resulting in smoother but less detailed outputs. i.e., one that can retain This highlights the need for more refined approach to feature reuse, computational efficiency while preserving key inter-step variations. 2.3 FEATURE REDUNDANCY IN CFG Following the observation of feature redundancy in attention modules across adjacent timesteps, we further extend our investigation into other critical components of the diffusion models. Through this 4 Preprint. Figure 6: (a) The MSE between conditional and unconditional outputs at the same timestep as well as across adjacent timesteps. (b) Directly reusing unconditional outputs from previous timesteps will lead to significantly degraded visual quality. broader analysis of the entire denoising process, we find that classifier-free guidance (CFG) significantly increases inference time, as it requires the computation of both conditional and unconditional outputs at every timestep. While CFG has been widely adopted for enhancing visual quality, there is little exploration to reduce its computational burden, leaving this aspect largely uncharted. To explore potential redundancy within CFG, we first conduct quantitative analysis of the similarity between conditional and unconditional outputs at the same timestep as well as across adjacent timesteps based on mean squared error (MSE). As shown in Fig. 6 (a), the results reveal that, in the mid to later stages of sampling, the similarity between conditional and unconditional outputs at the same timestep is remarkably high, significantly surpassing that of adjacent steps. Hence, as illustrated in Fig. 6 (b), directly reusing unconditional outputs from adjacent timesteps, as suggested in existing methods, leads to significant error accumulation, resulting in decline in video quality. These results indicate substantial redundancy in the CFG process and highlight the necessity for new strategy to accelerate CFG without compromising the quality of the generated outputs. Figure 7: (a) Simply reusing the conditional output from the same time step results in the poor generation of intricate details. (b) Trend curves of high and low-frequency biases between conditional and unconditional outputs change as sampling progresses. 2.4 FASTERCACHE FOR VIDEO DIFFUSION MODEL Capitalizing on the above discoveries, we introduce an innovative approach, FasterCache, which accelerates inference for video diffusion models while preserving high-quality generation. This is accomplished through Dynamic Feature Reuse Strategy that maintains feature distinction and temporal continuity. Furthermore, we introduce CFG-Cache to optimize the reuse of conditional and unconditional outputs, further enhancing inference speed without compromising visual quality. Dynamic Feature Reuse Strategy As discussed in Section 2.2, vanilla attention feature reuse strategy neglects the feature differences between adjacent timesteps which leads to visual quality degradation. Hence, instead of directly reusing previously cached features at the current timestep, we propose Dynamic Feature Reuse Strategy that can more effectively capture and preserve critical details in the generated videos. Specifically, for the attention modules in diffusion models, we compute the attention module outputs at every alternate timestep. For example, we calculate the attention outputs for each layer at + 2 and timesteps, denoted as Ft+2 and Ft, and store them in the feature cache as t+2 cache. To dynamically adjust feature reuse, we compute the difference between the adjacent cached features. This serves as bias for approximating the feature variation trend and enables the reused features to more accurately capture the evolving details across cache and 5 Preprint. timesteps. For the intermediate 1 timestep, its features can be computed as: Figure 8: Overview of the CFG-Cache. Ft1 = cache + (F cache t+2 (5) where w(t) is weighting function that modulates the contribution of the feature difference to account for variation between adjacent timesteps, ensuring both efficiency and the preservation of fine details in the generated videos. In our experiments, w(t) gradually increases as the sampling process progresses, allowing the model to place greater emphasis on the feature differences at later stages of generation. Consequently, our approach significantly accelerates inference while preserving the visual quality of the synthesized videos. cache) w(t), CFG-Cache As analyzed in Section 2.3, the conditional and unconditional outputs at the same timestep exhibit high similarity in CFG, indicating significant information redundancy. naive approach to take advantage of this would be to directly reuse the conditional features for the corresponding unconditional outputs at the same timestep. However, this often leads to noticeable degradation in detail generation. As illustrated in Fig 7 (a), this approach results in poor generation of intricate details, such as the texture of the spacesuit which shows lack of details and clarity. Since both the conditional and unconditional outputs in CFG represent predicted noise, and drawing inspiration from the Dynamic Feature Reuse Strategy and FreeU (Si et al., 2024), we analyze the differences between these two outputs in the frequency domain. As shown in Fig 7 (b), we observe that, in the early and mid-stages of the sampling process, the conditional and unconditional outputs at the same timestep exhibit significant bias in the low-frequency components, which progressively shifts to the high-frequency components in the later steps. This observation suggests that despite their overall similarity, key differences in frequency components must be addressed to avoid the degradation of fine details. Building on this discovery, we propose CFG-Cache, novel approach designed to account for both highand low-frequency biases, coupled with timestep-adaptive enhancement technique. Specifically, as shown in Fig. 8, at timestep t, full inference is performed to obtain both the conditional output ϵθ(xt, t, c) and the unconditional output ϵθ(xt, t, ). We then separately calculate the biases for the high-frequency (HF ) and low-frequency (LF ) components between these two outputs: LF = FFT (ϵθ(xt, t, ))low FFT (ϵθ(xt, t, c))low (6) HF = FFT (ϵθ(xt, t, ))high FFT (ϵθ(xt, t, c))high. (7) These biases ensure that both highand low-frequency differences are accurately captured and compensated during the reuse process. In the subsequent timesteps (from 1 to n), we infer only the outputs of the conditional branches and compute the unconditional outputs using the cached HF and LF as follows: ˆϵθ(xti, i, ) = IFFT (Flow, Fhigh), (8) Flow = LF w1 + FFT (ϵθ(xti, i, c))low (9) Fhigh = HF w2 + FFT (ϵθ(xti, i, c))high (10) Here, w1 and w2 are adaptively adjusted based on the sampling timestep t, with greater emphasis on different frequency components at distinct sampling phases. The weighting scheme is defined as: w1 = 1 + α1 I(t > t0), w2 = 1 + α2 I(t <= t0), (11) where α1 and α2 are hyperparameter weights, t0 is the manually set switching timestep, and I() is the indicator function. This formulation ensures that mid-low frequencies are prioritized in the mid-sampling phase, while high-frequency components receive more attention in the later phase. 6 Preprint."
        },
        {
            "title": "3.1 EXPERIMENTAL SETTINGS",
            "content": "Base models and compared methods To demonstrate the effectiveness of our method, we apply our acceleration technique to various video diffusion models, including Open-Sora 1.2 (Zheng et al., 2024), Open-Sora-Plan (Lab & etc., 2024), Latte (Ma et al., 2024a), CogVideoX (Yang et al., 2024), and Vchitect-2.0 (Vchitect, 2024). We compare our base models with recent efficient video synthesis techniques, including PAB (Zhao et al., 2024c) and -DiT (Chen et al., 2024c), to highlight the benefits of our approach. Notably, -DiT was originally designed as an acceleration method for image synthesis. Here we have adapted it for video synthesis to facilitate comparison. Please refer to the Appendix for more details of the base models and compared methods. Evaluation metrics and datasets To assess the performance of video synthesis acceleration methods, we focus primarily on two aspects, namely inference efficiency and visual quality. To evaluate inference efficiency, we employ Multiply-Accumulate Operations (MACs) and inference latency as metrics. We utilize VBench (Huang et al., 2024), LPIPS (Zhang et al., 2018), PSNR, and SSIM for visual quality evaluation. VBench is comprehensive benchmark suite for video generative models. It is well-aligned with human perceptions and capable of providing valuable insights from multiple perspectives. LPIPS, PSNR, and SSIM measure the similarity between videos generated by the accelerated sampling method and those from the original model. PSNR quantifies pixel-level fidelity between outputs, LPIPS measures perceptual consistency, and SSIM assesses structural similarity. In general, higher similarity scores indicate better fidelity and visual quality. Implementation details All experiments conduct full attention inference for spatial and temporal attention modules every 2 timesteps to facilitate dynamic feature reuse. The weight w(t) increases linearly from 0 to 1 starting from the beginning of dynamic feature reuse until the end of sampling. For CFG output reuse, full inference is conducted every five timesteps, starting from 1/3 of the total sampling steps (e.g., for Open-Sora 1.2, which has 30 total sampling steps, this begins at step 10). The hyperparameters α1 and α2 are set to default value of 0.2, which performs well for most models. All experiments are carried out on NVIDIA A100 80GB GPUs using PyTorch, with FlashAttention (Dao et al., 2022) enabled by default. Method Table 1: Comparison of efficiency and visual quality on single GPU. Visual Quality Efficiency Speedup Latency (s) VBench LPIPS MACs (P) SSIM PSNR Open-Sora 1.2 (T = 30) -DiT (Nc = 14, = 2) -DiT (Nc = 28, = 2) PAB Ours Open-Sora-Plan (T = 150) -DiT (Nc = 14, = 3) -DiT (Nc = 28, = 3) PAB Ours Latte (T = 50) -DiT (Nc = 14, = 2) -DiT (Nc = 28, = 2) PAB Ours CogVideoX (T = 50) -DiT (Nc = 4, = 2) -DiT (Nc = 8, = 2) -DiT (Nc = 12, = 2) Ours Vchitect-2.0 (T = 100) -DiT (Nc = 6, = 3) -DiT (Nc = 12, = 3) Ours Open-Sora 1.2 (192 frames, 480P) 1 1.14 1.34 1.23 1.62 192.07 168.69 143.14 156.73 118.44 78.79% 77.43% 76.60% 78.15% 78.46% Open-Sora-Plan (65 frames, 512512) 1 1.19 1.46 1.32 1.68 103.76 86.88 70.99 78.72 61.68 Latte (16 frames, 512512) 1 1.23 1.43 1.28 1.54 29.22 23.80 20.38 22.84 18. CogVideoX (48 frames, 480P) 1 1.08 1.15 1.26 1.62 78.48 72.72 68.19 62.50 48.44 Vchitect-2.0 (40 frames, 480P) 1 1.11 1.24 1.67 260.32 233.59 209.78 156. 80.16% 78.12% 77.71% 80.06% 80.19% 77.05% 76.27% 76.01% 76.70% 76.89% 80.18% 79.61% 79.31% 79.09% 79.83% 80.80% 79.98% 79.50% 80.84% 6.30 5.51 4.72 5.33 4.13 10.30 8.60 6.90 7.39 5. 3.05 2.67 2.29 2.24 1.97 6.03 5.62 5.23 4.82 3.71 14.57 13.00 11.79 8.67 7 - 0.2834 0.3321 0.1041 0.0835 - 0.4515 0.4819 0.2423 0. - 0.1731 0.2245 0.2904 0.0817 - 0.3319 0.3822 0.4053 0.0766 - 0.4153 0.4534 0.0282 - 0.7403 0.7092 0.8821 0.8932 - 0.4813 0.4467 0.7126 0.8138 - 0.8107 0.7620 0.7083 0. - 0.6612 0.6277 0.6126 0.9066 - 0.5837 0.5519 0.9224 - 17.77 16.24 26.43 27.03 - 16.08 15.42 20.29 23.72 - 22.69 21.00 18.98 28.21 - 17.93 16.69 16.15 28. - 14.26 13.68 31.45 Preprint."
        },
        {
            "title": "3.2 MAIN RESULTS",
            "content": "Quantitative comparison Table 1 presents quantitative comparison of our method with -DiT and PAB in terms of efficiency and visual quality. We synthesize videos with prompts provided by VBench and use the synthesized videos to compute the VBench metrics as well as calculate LPIPS, SSIM, and PSNR with videos sampled by the original model. The results demonstrate that our method achieves stable acceleration efficiency and superior visual quality across different base models, sampling schedulers, video resolutions, and lengths. Figure 9: Comparison of visual result quality among different methods. Visual quality comparison Fig. 9 compares the videos generated by our method against those by the original model, PAB, and -DiT. The results demonstrate that our method can effectively preserve the original quality and fine details. More visual results can be found in the Appendix. 3.3 ABLATION STUDY To comprehensively assess the effectiveness and efficiency of our method, we perform extensive ablation studies based on Open-Sora, synthesizing videos of 48 frames at 480P. Efficiency Table 2 compares the efficiency of the original Open-Sora and its variants with different acceleration components. There are two key observations. (1) The Dynamic Feature Reuse Strategy and CFG-Cache independently contribute to significant reductions in inference costs. When combined, they fur- (2) Compared to ther minimize inference overhead. Vanilla Feature Reuse, the proposed Dynamic Feature Reuse strategy has negligible impact on efficiency. Table 2: Impact on inference efficiency. Variants Vanilla FR Dynamic FR CFG-Cache MACs (P) Latency (s) (s) 1.54 1.33 1.33 1.16 1.01 41.28 33.25 33.50 31.32 26.12 - -8.03 -7.78 -9.96 -15.16 (Vanilla FR denotes Vanilla Feature Reuse, and represents the reduction in latency compared to the original model.) Visual quality Table 3 compares the visual quality of the original Open-Sora with its variants implementing different acceleration components. Note that vanilla feature reuse leads to performance drop in VBench and LPIPS. The introduction of the dynamic feature reuse strategy mitigates the loss of information and thereby improves the performance of these metrics (e.g., VBench: 78.34% 78.69%). Fig. 10 (a) provides visual comparison of the results. It can be observed that vanilla feature reuse shows reduced details (e.g., the moon and snowflakes), whereas dynamic feature reuse strategy can significantly alleviate this problem. The Feature MSE curves show that adding the bias term can lower the MSE between intermediate features from the original and accelerated sampling process, aligning with the visual results. Table 3: Impact on visual quality. Variants Original Open-Sora Vanilla FR Full (w/ Dynamic FR) CFG-Cache w/o Enhancement Enhance LF only Enhance HF only Full (w/ full CFG-Cache) Open-Sora ( 192 frames, 480P) 72.82 (2.64) 58.11(3.31) 42.18(4.55) Open-Sora-Plan(221 frames, 512512) 169.21 (1.87) 127.30 (2.49) 104.37 (3.03) VBench 78.99% 78.34% 0.0657 78.69% 0.0590 78.42% 0.0709 78.58% 0.0617 78.49% 0.0686 78.69% 0.0590 Table 4: Scaling to multiple GPUs with DSP. 2 A100 - 0.8785 0.8938 0.8727 0.8894 0.8834 0.8938 - 28.20 28.41 27.97 28.29 28.08 28.41 21.62(8.89) 17.21 (11.16) 12.57 (15.28) 192.07 (1) 156.73 (1.23) 118.44 (1.62) 316.71 (1) 243.33 (1.30) 187.91 (1.69) 89.10 (3.55) 71.17 (4.45) 57.70 (5.49) 39.09 (4.92) 30.91 (6.21) 22.55 (8.52) 49.13(6.44) 37.13(8.53) 31.82(9.95) SSIM PSNR LPIPS - Open-Sora-Plan Open-Sora 4 A100 1 A100 8 Method Ours Ours PAB PAB (FR denotes Feature Reuse.) Referring to Table 3, it can be seen that introducing CFG-Cache without enhancement reduces the visual quality. On the other hand, CFG-Cache with dynamic enhancement of either the lowor high8 Preprint. Figure 10: Comparison of Feature MSE curves and visual results from the ablation study. frequency bias helps to improve the visual quality, and their combined effect achieves the best visual quality. Fig. 10 (b) shows that enhancing low-frequency bias improves the fidelity of low-frequency components (e.g., clouds, tornado outlines) while enhancing high-frequency bias enriches highfrequency details (e.g., lightning). The Feature MSE curve of CFG-Cache without enhancement aligns with the reduced visual quality. Dynamic enhancement helps to mitigate error accumulation, leading to higher visual fidelity. 3.4 SCALABILITY AND GENERALIZATION Scaling to multiple GPUs To evaluate the sampling efficiency of our method on multiple GPUs, we adopt the approach used in PAB and integrate Dynamic Sequence Parallelism (DSP) (Zhao et al., 2024b) to distribute the workload across GPUs. Table 4 illustrates that, as the number of GPUs increases, our method consistently enhances inference speed across different base models, surpassing the performance of the compared methods. Performance at different resolutions and lengths To evaluate the effectiveness of our method in accelerating sampling for videos of varying sizes, we conduct tests across different video lengths and resolutions and report the results in Fig. 11. Our method maintains stable acceleration performance when faced with increasing resolutions and frame counts in videos, demonstrating its potential to accelerate sampling longer and higher-resolution videos in line with practical demands. Figure 11: Acceleration efficiency of our method at different video resolutions and lengths. I2V and image synthesis performance We integrate our acceleration method to the state-of-theart image-to-video model DynamiCrafter (Xing et al., 2023) and image synthesis model PixArtsigma (Chen et al., 2024a). As shown in Fig. 12, our method significantly accelerates sampling while maintaining visual fidelity, demonstrating its potential for extension to various base models."
        },
        {
            "title": "4 RELATED WORK",
            "content": "4.1 DIFFUSION MODELS FOR VIDEO SYNTHESIS Diffusion models have demonstrated potential in high-quality image synthesis (Ho et al., 2020; Rombach et al., 2022; Chen et al., 2023; 2024b), attracting significant attention. Subsequent work has adapted these models for video synthesis to generate high-fidelity videos (Ho et al., 2022). Motivated by advancements in image synthesis, early studies typically employed the diffusion UNet architecture (Blattmann et al., 2023; Wang et al., 2023; Zhang et al., 2023; Wu et al., 2023). As the scalability of diffusion transformer (Peebles & Xie, 2023) was validated in image synthesis, 9 Preprint. Figure 12: Visual results and inference time of our method on I2V and image synthesis models. an increasing number of works have adopted the diffusion transformer as the noise estimation network (Ma et al., 2024a; Zheng et al., 2024; Lab & etc., 2024; Yang et al., 2024)."
        },
        {
            "title": "5 CONCLUSION AND DISCUSSION\nIn this work, we present FasterCache, a training-free strategy that significantly accelerates video\nsynthesis inference while preserving high-quality generation. Through analysis of existing cache-\nbased methods, we find that directly reusing adjacent-step features in attention modules can degrade\nvideo quality. Additionally, we investigate the acceleration potential of CFG, identifying redundancy\nbetween conditional and unconditional features at the same timestep. Leveraging these insights,\nFasterCache integrates a dynamic feature reuse strategy that maintains feature distinction and tem-\nporal continuity, and CFG-Cache which optimizes the reuse of conditional and unconditional out-\nputs to further boost speed without sacrificing detail quality. Extensive experiments demonstrate its\nstrong performance in both efficiency and synthesis quality across diverse video models, sampling\nschedules, video lengths and resolutions, highlighting its potential for real-world applications.\nLimitation Despite the effectiveness shown by our method, certain limitations remain. When the\nsynthesis quality of the model is suboptimal, our acceleration method is unlikely to yield satisfactory\nresults either. We believe that advancements in base video models will mitigate this issue. Addi-\ntionally, in complex scenes with substantial video motion, our method may occasionally produce\ndegraded results. At present, this can be remedied through manual adjustments of hyperparameters.\nIn the future, we plan to investigate strategies for adaptive caching to further enhance performance.",
            "content": "10 Preprint."
        },
        {
            "title": "REFERENCES",
            "content": "Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Daniel Bolya and Judy Hoffman. Token merging for fast stable diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 45994603, 2023. Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis, 2023. Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation, 2024a. Junsong Chen, Yue Wu, Simian Luo, Enze Xie, Sayak Paul, Ping Luo, Hang Zhao, and Zhenguo Li. Pixart-δ: Fast and controllable image generation with latent consistency models, 2024b. Pengtao Chen, Mingzhu Shen, Peng Ye, Jianjian Cao, Chongjun Tu, Christos-Savvas Bouganis, Yiren Zhao, and Tao Chen. δ-dit: training-free acceleration method tailored for diffusion transformers. arXiv preprint arXiv:2406.01125, 2024c. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:1634416359, 2022. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. Yefei He, Luping Liu, Jing Liu, Weijia Wu, Hong Zhou, and Bohan Zhuang. Ptqd: Accurate posttraining quantization for diffusion models. Advances in Neural Information Processing Systems, 36, 2024. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:8633 8646, 2022. Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. PKU-Yuan Lab and Tuzhan AI etc. Open-sora-plan, April 2024. URL https://doi.org/10. 5281/zenodo.10948109. Lijiang Li, Huixia Li, Xiawu Zheng, Jie Wu, Xuefeng Xiao, Rui Wang, Min Zheng, Xin Pan, Fei Chao, and Rongrong Ji. Autodiffusion: Training-free optimization of time steps and architectures for automated diffusion model acceleration. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 71057114, 2023a. Senmao Li, Taihang Hu, Fahad Shahbaz Khan, Linxuan Li, Shiqi Yang, Yaxing Wang, Ming-Ming Cheng, and Jian Yang. Faster diffusion: Rethinking the role of unet encoder in diffusion models. arXiv preprint arXiv:2312.09608, 2023b. Preprint. Yanjing Li, Sheng Xu, Xianbin Cao, Xiao Sun, and Baochang Zhang. Q-dm: An efficient low-bit quantized diffusion model. Advances in Neural Information Processing Systems, 36, 2024a. Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Snapfusion: Text-to-image diffusion model on mobile devices within two seconds. Advances in Neural Information Processing Systems, 36, 2024b. Shanchuan Lin and Xiao Yang. Animatediff-lightning: Cross-model diffusion distillation. arXiv preprint arXiv:2403.12706, 2024. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:57755787, 2022a. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022b. Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, arXiv preprint Latte: Latent diffusion transformer for video generation. and Yu Qiao. arXiv:2401.03048, 2024a. Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache: Accelerating diffusion models for free. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1576215772, 2024b. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022. Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. arXiv preprint arXiv:2311.17042, 2023. Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and Yan Yan. Post-training quantization on diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 19721981, 2023. Chenyang Si, Ziqi Huang, Yuming Jiang, and Ziwei Liu. Freeu: Free lunch in diffusion u-net. In CVPR, 2024. Junhyuk So, Jungwon Lee, Daehyun Ahn, Hyungjun Kim, and Eunhyeok Park. Temporal dynamic quantization for diffusion models. Advances in Neural Information Processing Systems, 36, 2024a. Junhyuk So, Jungwon Lee, and Eunhyeok Park. Frdiff : Feature reuse for universal training-free acceleration of diffusion models, 2024b. URL https://arxiv.org/abs/2312.03517. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint arXiv:2303.01469, 2023. Yang Sui, Yanyu Li, Anil Kag, Yerlan Idelbayev, Junli Cao, Ju Hu, Dhritiman Sagar, Bo Yuan, Sergey Tulyakov, and Jian Ren. Bitsfusion: 1.99 bits weight quantization of diffusion model. arXiv preprint arXiv:2406.04333, 2024. 12 Preprint. Vchitect. Vchitect-2.0: Parallel transformer for scaling up video diffusion models, 2024. URL https://github.com/Vchitect/Vchitect-2.0. Hongjie Wang, Difan Liu, Yan Kang, Yijun Li, Zhe Lin, Niraj Jha, and Yuchen Liu. Attentiondriven training-free efficiency enhancement of diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1608016089, 2024. Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. arXiv preprint arXiv:2309.15103, 2023. Felix Wimbauer, Bichen Wu, Edgar Schoenfeld, Xiaoliang Dai, Ji Hou, Zijian He, Artsiom Sanakoyeu, Peizhao Zhang, Sam Tsai, Jonas Kohler, et al. Cache me if you can: AcceleratIn Proceedings of the IEEE/CVF Conference on ing diffusion models through block caching. Computer Vision and Pattern Recognition, pp. 62116220, 2024. Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 76237633, 2023. Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Xintao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicrafter: Animating open-domain images with video diffusion priors. arXiv preprint arXiv:2310.12190, 2023. Shuai Yang, Yukang Chen, Luozhou Wang, Shu Liu, and Yingcong Chen. Denoising diffusion step-aware models. arXiv preprint arXiv:2310.03337, 2023. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. Dingkun Zhang, Sijia Li, Chen Chen, Qingsong Xie, and Haonan Lu. Laptop-diff: Layer pruning and normalized distillation for compressing diffusion models. arXiv preprint arXiv:2404.11098, 2024a. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. Wentian Zhang, Haozhe Liu, Jinheng Xie, Francesco Faccio, Mike Zheng Shou, and Jurgen Schmidhuber. Cross-attention makes inference cumbersome in text-to-image diffusion models. arXiv preprint arXiv:2404.02747, 2024b. Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng Zhang, Wangmeng Zuo, and Qi Tian. Controlvideo: Training-free controllable text-to-video generation. arXiv preprint arXiv:2305.13077, 2023. Tianchen Zhao, Tongcheng Fang, Enshu Liu, Wan Rui, Widyadewi Soedarmadji, Shiyao Li, Zinan Lin, Guohao Dai, Shengen Yan, Huazhong Yang, et al. Vidit-q: Efficient and accurate quantization of diffusion transformers for image and video generation. arXiv preprint arXiv:2406.02540, 2024a. Xuanlei Zhao, Shenggan Cheng, Zangwei Zheng, Zheming Yang, Ziming Liu, and Yang You. arXiv preprint Dsp: Dynamic sequence parallelism for multi-dimensional arXiv:2403.10266, 2024b. transformers. Xuanlei Zhao, Xiaolong Jin, Kai Wang, and Yang You. Real-time video generation with pyramid attention broadcast. arXiv preprint arXiv:2408.12588, 2024c. Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, March 2024. URL https://github.com/hpcaitech/Open-Sora. 13 Preprint. APPENDIX A.1 FURTHER DETAILS OF BASE MODELS In this work, we applied our FasterCache to various video synthesis models, including OpenSora 1.2 (Zheng et al., 2024), Open-Sora-Plan (Lab & etc., 2024), Latte (Ma et al., 2024a), CogVideoX (Yang et al., 2024), and Vchitect 2.0 Vchitect (2024). Open-Sora 1.2 (Zheng et al., 2024) integrates 2D-VAE and 3D-VAE to enhance video compression and employs ST-DiT blocks for the diffusion process. Open-Sora-Plan (Lab & etc., 2024) adopts CausalVideoVAE to compress visual representations better and 3D full attention architecture to capture joint spatial and temporal features. Latte Ma et al. (2024a) extracts spatio-temporal tokens from input videos and then adopts series of transformer blocks to model video distribution in the latent space. CogVideoX (Yang et al., 2024) employs 3D VAE to compress videos along spatial and temporal dimensions and an expert transformer with the expert adaptive LayerNorm to facilitate the fusion between the two modalities. A.2 FURTHER DETAILS OF COMPARED METHODS PAB (Zhao et al., 2024c) employs pyramid-style broadcasting mechanism to propagate attention outputs across subsequent steps. It optimizes efficiency by applying distinct broadcast strategies to each attention layer based on their respective variances. Additionally, the method introduces broadcast sequence parallelism to enhance the efficiency of distributed inference. This paper follows the default parameter configuration of PAB. -DiT (Chen et al., 2024c) accelerates inference by caching feature offsets instead of the full feature maps while preventing input information loss. It caches the residuals of the blocks in the latter part of DiT for approximation during early-stage sampling and caches the residuals of the blocks in the earlier part during later-stage sampling. In -DiT, the parameters that need to be configured are the residual cache interval , the number of cached blocks Nc, and the timestep boundary for determining the position of the cached blocks. Since the source code of -DiT is not publicly available, we implemented its method based on the paper for accelerating video synthesis. Following the guidelines in -DiT, we experimented with different configurations of Nc and to balance visual quality and inference speed, allowing for fair evaluation of the method. A.3 ADDITIONAL QUALITATIVE EXPERIMENTS More visual results on Text-to-Video models The additional visual comparison results for OpenSora 1.2 (Zheng et al., 2024), Open-Sora-Plan (Lab & etc., 2024), and Latte (Ma et al., 2024a) are presented in Fig. 13, Fig. 14, and Fig. 15, while further comparisons for CogVideoX (Yang et al., 2024) and Vchitect-2.0 Vchitect (2024) are shown in Fig. 16. Our method demonstrates reliable fidelity across various models and styles or content in video synthesis, while simultaneously achieving acceleration. More visual results on Image-to-Video models We conducted image-to-video sampling acceleration experiments based on DynamiCrafter (Xing et al., 2023), achieving 1.52 speedup on single GPU. Additional visual results are provided in Fig. 17. Our method demonstrates good fidelity in the acceleration of image-to-video models, indicating broad potential for practical applications. A.4 ADDITIONAL QUANTITATIVE EXPERIMENTS User preference study To assess the effectiveness of our FasterCache, we additionally conduct human evaluation. We randomly selected 30 videos for each model. Each rater receives text prompt and two generated videos from different sampling acceleration methods (in random order). They are then asked to select the video with better visual quality. Five raters evaluate each sample, and the voting results are summarized in Table 5. As one can see, compared to other acceleration methods, the raters strongly prefer the videos generated by our method. Method comparison Open-Sora 1.2 Open-Sora-Plan Ours vs. -DiT Ours vs. PAB 78.00% 72.67% 80.67% 69.33% Latte 77.33% 74.00% Table 5: User preference study. The numbers represent the percentage of raters who favor the videos synthesized by our method. Preprint. Figure 13: More visual results on Open-Sora (480P 192 frames). Zoom in for details. 15 Preprint. Figure 14: More visual results on Open-Sora-Plan (512512 65 frames). Zoom in for details. Preprint. Figure 15: More visual results on Latte (512512 16 frames). Zoom in for details. 17 Preprint. Figure 16: More visual results on CogVideoX (480P 48 frames) & Vchitect-2.0 (480P 40frames). Preprint. Figure 17: More visual results on DynamiCrafter (1024576 16frames). Zoom in for details."
        }
    ],
    "affiliations": [
        "S-Lab, Nanyang Technological University",
        "Shanghai Artificial Intelligence Laboratory",
        "The University of Hong Kong"
    ]
}