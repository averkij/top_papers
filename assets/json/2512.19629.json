{
    "paper_title": "LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry",
    "authors": [
        "Jiaqi Peng",
        "Wenzhe Cai",
        "Yuqiang Yang",
        "Tai Wang",
        "Yuan Shen",
        "Jiangmiao Pang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Trajectory planning in unstructured environments is a fundamental and challenging capability for mobile robots. Traditional modular pipelines suffer from latency and cascading errors across perception, localization, mapping, and planning modules. Recent end-to-end learning methods map raw visual observations directly to control signals or trajectories, promising greater performance and efficiency in open-world settings. However, most prior end-to-end approaches still rely on separate localization modules that depend on accurate sensor extrinsic calibration for self-state estimation, thereby limiting generalization across embodiments and environments. We introduce LoGoPlanner, a localization-grounded, end-to-end navigation framework that addresses these limitations by: (1) finetuning a long-horizon visual-geometry backbone to ground predictions with absolute metric scale, thereby providing implicit state estimation for accurate localization; (2) reconstructing surrounding scene geometry from historical observations to supply dense, fine-grained environmental awareness for reliable obstacle avoidance; and (3) conditioning the policy on implicit geometry bootstrapped by the aforementioned auxiliary tasks, thereby reducing error propagation.We evaluate LoGoPlanner in both simulation and real-world settings, where its fully end-to-end design reduces cumulative error while metric-aware geometry memory enhances planning consistency and obstacle avoidance, leading to more than a 27.3\\% improvement over oracle-localization baselines and strong generalization across embodiments and environments. The code and models have been made publicly available on the \\href{https://steinate.github.io/logoplanner.github.io/}{project page}."
        },
        {
            "title": "Start",
            "content": "LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry Jiaqi Peng,1,2, Wenzhe Cai,2, Yuqiang Yang,2, Tai Wang2,, Yuan Shen1,2, and Jiangmiao Pang2 5 2 0 2 2 2 ] . [ 1 9 2 6 9 1 . 2 1 5 2 : r Abstract Trajectory planning in unstructured environments is fundamental and challenging capability for mobile robots. Traditional modular pipelines suffer from latency and cascading errors across perception, localization, mapping, and planning modules. Recent end-to-end learning methods map raw visual observations directly to control signals or trajectories, promising greater performance and efficiency in open-world settings. However, most prior end-to-end approaches still rely on separate localization modules that depend on accurate sensor extrinsic calibration for self-state estimation, thereby limiting generalization across embodiments and environments. We introduce LoGoPlanner, localization-grounded, end-to-end navigation framework that addresses these limitations by: (1) finetuning long-horizon visual-geometry backbone to ground predictions with absolute metric scale, thereby providing implicit state estimation for accurate localization; (2) reconstructing surrounding scene geometry from historical observations to supply dense, fine-grained environmental awareness for reliable obstacle avoidance; and (3) conditioning the policy on implicit geometry bootstrapped by the aforementioned auxiliary tasks, thereby reducing error propagation. We evaluate LoGoPlanner in both simulation and real-world settings, where its fully endto-end design reduces cumulative error while metric-aware geometry memory enhances planning consistency and obstacle avoidance, leading to more than 27.3% improvement over oracle-localization baselines and strong generalization across embodiments and environments. The code and models have been made publicly available on the project page. I. INTRODUCTION Autonomous navigation, requiring robots to reliably reach specified goals in unstructured environments, remains fundamental challenge in robotics. Traditional navigation pipelines are typically modular, decomposing the task into perception, localization, mapping, and planning [1], [2], [3]. While this factorization improves interpretability and allows for component-level optimization, it often introduces compounding latency and suffers from cascading errors between modules [4]. These issues become particularly acute in real deployments, such as legged robots, where gait-induced vibrations in both cameras and IMUs reduce the accuracy of odometry and mappingwhich in turn destabilizes downstream planning [3]. End-to-end learning-based methods [5], [6], [7], [8], [9] have recently emerged as promising alternative, offering compact pipelines that map raw visual observations directly to control signals or trajectories. Beyond mitigating cascading errors, such approaches also demonstrate high efficiency 1Jiaqi Peng, Yuan Shen are with the Department of Electronic Engineering, Tsinghua University, Corresponding author. 2Jiaqi Peng, Wenzhe Cai, Yuqiang Yang, Tai Wang, Jiangmiao Pang and Yuan Shen are all with Shanghai AI laboratory, *Equal contribution. Fig. 1. (a) Traditional modular planners decompose tasks into modules, introducing cascading errors. (b) Existing end-to-end frameworks directly map observations to control signals but still rely on explicit localization modules. (c) LoGoPlanner integrates implicit state estimation and metricaware geometry perception into policy for fully end-to-end planning. and ease of deployment in open-world scenarios. However, most approaches mainly replace perception, mapping, and planning modules but still rely on explicit localization modules such as SLAM or visual odometry [6], [9] for self-state updates, which requires precise extrinsic calibration between the camera and the robot chassis. This reliance arises because these planners typically process only single frame [5] or short clips [10], [9], lacking the ability to summarize longterm histories for consistent state updates. Without temporal grounding, short-term estimates accumulate errors over time, leaving trajectory planning vulnerable to drift and inconsistency. Also, single frame perception lacks geometric memory needed for robust metric reasoning [11]. Most methods reconstruct only partial or scale-ambiguous geometry, and fails to capture broader spatial context including occluded and rear-view regions, limiting the fidelity of spatial reasoning. To address these challenges, we propose LoGoPlanner, localization-grounded end-to-end navigation framework that integrates temporal visual geometry estimation with diffusion-based trajectory generation. Specifically, localization, we design an implicit state estimation module to overcome the reliance on explicit that operates on long visual sequences. Consecutive image histories are processed using pretrained visual-geometry backbones [12] for camera extrinsic estimation. We further decouple chassis and camera extrinsics: perception is tied to the camera viewpoint while control is executed at the chassis level. This separation prevents viewpoint-dependent perception errors from propagating into the control space, enabling stable action generation even under varying sensor placements. Training with camera data captured under varying heights and pitch angles enables cross-embodiment and cross-view ego-motion estimation. Additionally, implicit state features are used to project the goal back into the current frame, ensuring consistent goal alignment. Furthermore, we aggregate multi-frame visual features and pose embeddings to generate an implicit reconstruction of the surrounding scene, thereby providing fine-grained geometric priors for planning. Visual geometry backbones such as VGGT [12] output relative-scale reconstructions, which cannot be directly aligned with planning trajectories. To resolve this, we introduce an efficient post-training procedure that incorporates depth information as scene-scale prior, enabling the prediction of dense, metric-scale point clouds relative to the robots current position. Inspired by planningoriented frameworks [13], we adopt query-based policy architecture that fuses implicit state[14], geometry, and goal features into unified planning context. Unlike approaches that pass explicit poses or point clouds downstream and thereby risk error accumulation, our query-driven design enables the diffusion head to operate directly on geometrically grounded representations. This allows the policy to iteratively refine trajectories toward feasible, collision-free solutions, with planning as the ultimate optimization target. In simulation, LoGoPlanner achieves 27.3% relative improvement over baselines that rely on oracle localization, demonstrating the effectiveness of implicit self-state estimation. Furthermore, in diverse real-world scenarios, our framework exhibits robust generalization across different embodiments and environments. II. RELATED WORK A. Learning-based planner Recent end-to-end visual navigation frameworks aim to directly map visual inputs to control commands, eliminating the need for traditional modular pipelines. By leveraging semantic and geometric cues, these methods demonstrate the ability to plan across varied terrains. Some prior works adopt supervised learning paradigm, where robots are trained to imitate expert trajectories or human demonstrations, reasoning about expert actions from vision-based observations [15], [16], [11], [17]. Beyond imitation learning, reinforcement learning has been applied to end-to-end navigation by optimizing policies through trial-and-error in simulation [8], [18], but its high sample complexity and sparse rewards make training costly. To overcome these challenges, Yang et al. [6] proposed iPlanner, which reformulates planning as an offline bi-level optimization problem, improving efficiency but only relying on single frame to capture the robots surrounding geometry. These methods are still trained in an open-loop manner, predicting the entire trajectory from start to goal. As result, they lack explicit estimation of intermediate states and typically assume access to metric geometry, treating localization as an external input. Our work builds on these advances by further integrating state estimation and geometry understanding into navigation planning. B. Video-geometric model [20], Recent progress in video-based geometry models [12], [21] has significantly advanced multi-frame [19], 3D scene understanding. For instance, Video Depth Anything [22] extends monocular depth estimation to long video sequences, exploiting temporal information to preserve geometric quality and consistency over time while retaining generalization. Beyond depth estimation, models such as VGGT [12] perform full video-based reconstruction. VGGT is feed-forward neural network that jointly predicts dense 3D attributes, including depth maps, 3D point tracks, and camera extrinsics, from one or more views of scene. By leveraging long temporal windows, these models provide geometrically consistent reconstructions and explicit camera pose estimates, offering fine-grained priors that are particularly valuable for downstream tasks such as environment perception and navigation [23]. C. Monocular Visual Odometry Monocular visual odometry (VO) and SLAM methods inherently suffer from scale ambiguity. Classical geometric methods such as MonoSLAM [24] and PTAM [25] rely on handcrafted features and epipolar constraints but require strong motion or scene priors, and are prone to drift accumulation. Direct methods such as DSO [26] and LSD-SLAM [27] optimize photometric consistency, reducing reliance on features but still needing scale initialization and remaining sensitive to illumination changes. Learningbased methods introduce data-driven priors. Depth-assisted approaches such as MonoDepth [28] and ORB-SLAM2 [29] recover scale from monocularly estimated depth, but often generalize poorly across domains. End-to-end networks like PoseNet [30] regress pose and scale directly, while transformer-based models such as MonoRec [31] alleviate drift through long-range dependencies; however, both remain vulnerable to dynamic or fast-motion scenarios. Fusionbased approaches combine multiple cues: BEV-ODOM[32] exploits spatial regularities, CodedVO [33] improves scale estimation via coded apertures, and VINS-Mono [34] integrates IMU data to enhance robustness, albeit at the cost of additional sensors. Despite these advances, most monocular VO methods still depend on external priorswhether geometric assumptions, scale initialization, or auxiliary hardwarelimiting their scalability and generalization. III. PROBLEM FORMULATION We study the problem of point-goal navigation using only RGB-D observations. An agent must navigate from its start pose to designated target point R3 while avoiding collisions without relying on additional modules. Time proceeds in discrete steps = 1, . . . , . At each step, the agent receives an RGB-D observation Oi = (Ii, Di), where Ii RHW 3 is the RGB image and Di RHW is the corresponding depth map. To successfully reach the goal, the agent must continuously estimate its own state from long-term visual history while simultaneously perceiving the surrounding environment to ensure safe navigation. The predicted state of the agent up to time is given by ˆs1:i = (O1:i), which further enables the transformation of the global goal into the current coordinate frame, yielding the relative goal ˆgi = (ˆs1:i, g). Based on this goal and the perceived environment, the agent plans an obstacle-avoiding trajectory τ1:T = {p1, . . . , pT } toward the goal expressed in the chassis coordinate frame. Unlike existing end-to-end approaches that depend on explicit localization modules, our framework performs implicit, closed-loop state estimation directly from visual sequences. At each step, the agent maintains (i) its estimated chassis pose (cid:98)pi in metric scale, (ii) the relative goal position (cid:98)gi in the current frame, and (iii) dense local point cloud (cid:98)Pi that captures its surrounding environment. IV. METHODOLOGY A. Overall Architecture As illustrated in Fig. 2, LoGoPlanner is unified end-toend navigation framework that jointly learns metric-aware perception, implicit localization, and trajectory generation. The system builds on pretrained video geometry backbone augmented with depth-derived scale priors. Through auxiliary supervision on point prediction and pose estimation, the model encodes both fine-grained structures and long-term ego-motion into compact world point embeddings aligned with the planning coordinate system. querybased design allows task-specific queries to extract state and geometry features through cross-attention and fuse them with goal embeddings into unified planning context. diffusion-based policy head then conditions on this context to iteratively refine noisy actions into collision-free trajectories. B. Metric-aware Visual Geometry Learning Recent video geometry models such as VGGT [12] can recover dense 3D scene geometry from image sequences, but their predictions are only defined up to an unknown scale, limiting their applicability to navigation. To address this limitation, we fine-tune the backbone by injecting scale priors from depth maps, enabling metric-scale scene reconstruction. Concretely, given causal sequence of RGB images I1:N from the same navigation scene, the video geometric model uses vision transformer [35] to patchify each image Ii into set of initial patch tokens tI . The alternating-attention mechanism alternates between intraframe and inter-frame attention, improving both local fidelity and long-horizon consistency. To inject scale priors into the semantic patches, we employ lightweight variant of the same vision transformer [35] to encode input depth maps into geometric tokens tD , which are then fused at the RKCI RKCD patch level with initial semantic tokens tI . transformer decoder module, integrating attention mechanisms with Rotary Position Embedding(RoPE) [36], is further used to integrate information within frames, producing per-frame features that are enriched with metric-scale information. tmetric = Attention(cid:0)RoPE((tI , tD ), pos)(cid:1), (1) where pos RK2 denotes the spatial position coordinates of image patches, RoPE(, pos) applies position-dependent rotations to tokens using 2D coordinates in pos, thereby augmenting the intra-frame attention mechanism to better capture spatial positional relationships between patch tokens, and tmetric representing the fused feature embedding at time with metric-scale awareness. To improve the accuracy of point cloud prediction, we introduce auxiliary tasks that provide additional supervision during training. Specifically, given the multi-frame feature representation = {tmetric } from the context RGB frames, we branch the features into two task-specific heads: local point head and camera pose head. Local point prediction. For each frame i, the local point head ϕp maps metric-aware tokens tmetric to latent feature representation hp , which is further decoded to predict canonical local 3D points in the camera coordinate system: , . . . , tmetric 1 hp = ϕp(tmetric i,j }M = {ˆplocal ), (cid:98)P local = fp(hp ) (2) where (cid:98)P local j=1 denotes set of predicted local points for frame i, which is supervised by local points in the camera coordinate system using the pinhole model for each pixel (u, v) in images: pcam,i(u, v) = Di(u, v) 1[u 1] (3) Unlike directly using noisy depth maps for local point projection, our data-driven prediction improves reconstruction robustness and provides implicit features for world point prediction. Camera pose prediction. In parallel, the camera pose head ϕc maps tmetric to another task-specific feature hc , which is decoded into predicted camera-to-world transformation: = ϕc(tmetric hc ), (cid:98)Tc,i = fc(hc ) (4) where the world coordinate system is defined with respect to the chassis frame of the last time step, ensuring consistency between the predicted camera trajectory and the planning coordinate system. World point prediction. Rather than directly encoding the predicted local points or poses, we exploit their intermediate task-specific features to perform implicit transformations, thereby obtaining compact representation of world points for downstream planning. For each frame i, we concatenate the local-point feature hp . These perframe fused features are then aggregated across the patches by context fusion module A: and the pose feature hc = A([hp hw , hc ]). (5) Fig. 2. Architecture overview. Our method injects scale priors into the image patches that are encoded by ViT [35], and finetunes the video geometry model to metric scale prediction. We adopt query-based design in which ego state representation and environment geometry are implicitly aggregated through task-specific queries. diffusion policy head is detached to generate feasible and collision-free trajectories. Finally, the aggregated representation hw is passed through point-cloud decoder ψ to upsample to the target resolution. The intermediate output zi is further processed to enforce scale-invariant range and preserve sign information: zi = ψ(hw ), (cid:98)P world = sign(zi) (exp(zi) 1) (6) where exp( ) 1 is used to enhance expressiveness of large coordinate values while avoiding saturation, (cid:98)P world represents the reconstructed metric-scale scene point cloud expressed in the last frame to align the coordinate with planning trajectory. C. Localization Grounded Navigation Policy is executed at In navigation, perception is tied to the camera viewpoint, whereas control the chassis level. This mismatch leads to alignment errors, particularly in cross-embodiment settings where camera height and pitch may vary significantly. Traditional methods rely on explicit extrinsic calibration between the camera and chassis to align perception with planning, but such calibration is fragile and fails to generalize across different views. In contrast, we decouple the estimation of camera and chassis poses into separate prediction tasks and leverage implicit feature interaction to bridge perception and planning without explicit calibration. Specifically, we assume the robot-mounted camera has no yaw rotation relative to the chassis. The model also predicts the chassis pose and latest goal position from pose estimation task-specific feature hc : (cid:98)Tb,i = fb(hc ) (cid:98)gi = fg(hc , g) (7) which is defined on the ground plane as (xi, yi, θi), where (xi, yi) denotes the current position relative to the starting point, and θi is the yaw angle. Formally, the camera pose at time is obtained from the chassis pose through the extrinsic transformation: where Tb,i is the chassis transformation with respect to the start point, and Text denotes the fixed extrinsic transformation capturing the cameras height and pitch angle relative to the chassis. To endow the model with robustness across embodiments, we construct training data under arbitrary camera heights and varying pitch angles, thereby enabling the system to generalize across diverse camera configurations. To achieve end-to-end planning, our approach does not explicitly feed the predicted extrinsics or point clouds into the network for goal transformation or trajectory optimization. Instead, inspired by UniAD [13], we adopt querybased design in which different modules are aggregated through task-specific queries. Interactions across modules are realized via query cross attention. We set state queries Qs to extract implicit state representation from pose specific tokens and geometric queries Qg to extract implicit environment geometry from world point specific tokens, thus providing sufficient information for trajectory planning: (9) (10) QS = CrossAttn(Qs, hc) QG = CrossAttn(Qg, hp) where the generated QS, QG and goal embedding are concatenated and passed to transformer decoder to produce the planning context query QP : These implicit features, which encode states and geometric properties, serve as conditioning signals for planning. This strategy avoids cascading errors that would otherwise arise from directly applying upstream predictions to downstream tasks, while ensuring that the final optimization target remains the trajectory planning error. For navigation trajectory planning, we attach diffusion policy head to generate action chunks {at = t=1. Starting from aK sampled from Gaus- (xt, yt, θt)}T sian noise, the model predicts noise from noisy action sequences, performs iterative steps of denoising to produce series of intermediate actions with decreasing levels of noise: Tb,i = Tc,i Text, (8) ak1 = α(ak γϵθ(QP , ak, k) + (0, σ2I)) (11) where ak denotes the noisy action at step k, ϵθ is the noise prediction network conditioned on planning context query QP , α and γ are the standard diffusion schedule parameters. This formulation enables the policy to iteratively refine actions toward feasible, collision-free trajectories. V. EXPERIMENTS A. Datasets and Implementation Details We use large-scale navigation dataset [37], generated with simulation pipeline designed to efficiently generate diverse robot trajectories across variety of 3D environments. The robot is modeled as cylindrical rigid body with differential-drive two-wheel configuration and is equipped with an RGB-D camera mounted on the top. To simulate embodiment variations across different robotic platforms, the robots height is randomized between 0.25 and 1.25 m, while the cameras pitch angle is randomized between 0 and 30. Initial paths between randomly sampled start and goal points are generated using the A* algorithm. These paths are refined through greedy search and subsequently smoothed via cubic spline interpolation to ensure collisionfree navigation. The dataset comprises over 200k trajectories and approximately 10 million rendered images. We adopt two-stage training paradigm. In the first stage, we fine-tune the decoder of the video geometry model and the task-specific head with batch size of 12 for 24 hours. During this process, depth-based scale priors are injected, and supervision is provided by metric-scale scene point clouds and camera extrinsics. In the second stage, we jointly train the diffusion head together with the task-specific head while keeping the decoder of the backbone frozen. This stage uses batch size of 32 and runs for three days, ensuring both robust state estimation and stable perception capabilities. B. Main Results We evaluate the performance of different learning-based planners in both simulation and real-world environments. The test environments are unseen during training, requiring the robot to continuously estimate its state, plan collision-free trajectories, and navigate toward the designated goal. Planning performance is measured using Success Rate (SR) and Success weighted by Path Length (SPL). The experimental results are summarized in Table and Table II. 1) Simulation Experiments: To simulate realistic environments, we selected 40 scenes from the InternScenes dataset [38], including 20 home and 20 commercial scenes. Home scenes are characterized by narrow passages and cluttered semantic layouts, while commercial scenes cover representative categories such as hospitals, supermarkets, restaurants, schools, libraries, and offices. In each scene, 100 startgoal pairs are randomly sampled in unoccupied spaces with distances of 410 meters, and initial orientations are determined through path planning to avoid collisions. Table reports navigation performance for two scene categories. In the table, the Localization column indicates whether the planner has access to ground-truth localization Fig. 3. Home scenes are characterized by narrow passages and cluttered semantic layouts, while commercial scenes cover representative categories such as hospitals, supermarkets, restaurants, schools, libraries, and offices. TABLE SIMULATION RESULTS Planner Localization DD-PPO [8] iPlanner [6] ViPlanner [5] LoGoPlanner Home Commercial SR SPL SR SPL 0.4 41.7 43. 44.0 45.0 57.3 0.4 40.2 40.6 42.8 43.2 52. 5.3 53.1 54.6 61.3 63.7 67.1 5.2 51.8 52. 60.1 61.9 63.9 With explicit or implicit localization. With oracle localization from simulator. from the simulator. means the planner uses simulatorprovided poses, while indicates no access to groundtruth localization: for iPlanner and ViPlanner, an external visual odometry module (ORB-SLAM3 [39] with RGB-D input) is used, whereas LoGoPlanner performs implicit state estimation without any external localization. Reinforcement-learning-based planners like DD-PPO [8] typically require massive interaction data and careful reward shaping. They can underperform in unseen environments due to overfitting to training distributions and sparse, noisy reward signals. The rule-based nature of imperative planning [6], [5] leads to failure to adjust to novel spatial configurations. Moreover, both methods operate on singleframe input, which restricts their ability to capture holistic scene geometry and leads to poor adaptation in cluttered or unstructured settings. In contrast, LoGoPlanner achieves stronger robustness by jointly incorporating ego-state information and multi-frame geometric reconstruction. This design ensures greater consistency in trajectory generation while providing richer spatial perception, which in turn enhances obstacle avoidance and overall navigation performance. Compared with baselines Fig. 4. Visualization of LoGoPlanner in real-world scenarios on different robot platforms. The green curves are the planned trajectories of LoGoPlanner. Blue and grey clouds are the obstacles of the current frame and the previous frame respectively. that benefit from external or oracle localization, LoGoPlanner improves Home SR by 27.3% points and Home SPL by 21.3% relative to ViPlanner, demonstrating that integrating implicit self-localization with geometry-aware planning yields superior closed-loop navigation. 2) Real-world Experiments: To evaluate the crossplatform, cross-scene, and cross-view generalization of LoGoPlanner in real-world settings, we deploy the system on three distinct robotic platforms under diverse environment configurations. For quantitative evaluation of vision-based navigation methods in real-world scenarios, we test TurtleBot in an office scene with structured obstacles, Unitree Go2 in cluttered home scene containing arbitrarily shaped obstacles, and Unitree G1 in an industrial scene with road-block obstacles, evaluating 20 trajectories. Quantitative results are shown in Table II. All algorithms run on an NVIDIA RTX 4090 GPU for cloud-based inference, with control commands transmitted to the robots in real time. iPlanner performs relatively poorly across different embodiments because its trajectory inconsistency during obstacle avoidance often leads to collisions. In contrast, ViPlanner demonstrates better performance; however, constrained by the training data and network design of single-frame-based navigation policy, it exhibits poor performance in challenging scenarios (e.g., industrial environments with Unitree G1). LoGoPlanner can be deployed directly without requiring visual odometry or SLAM. Despite camera jitter caused by the quadruped platform, LoGoPlanner achieves accurate selflocalization and generates reliable collision-free trajectories towards the goal. By leveraging point clouds as implicit intermediate representations, the model further reduces the sim-to-real gap, demonstrating the frameworks robust generalization, reduced deployment complexity, and readiness for direct application in scenarios with varying scene structure and camera viewpoints. For detailed demonstrations of our system, please refer to the demo video. TABLE II REAL-WORLD RESULTS Planner Office Home Industrial TurtleBot Unitree Go2 Unitree SR SR SR iPlanner [6] ViPlanner [5] LoGoPlanner 10 (2/20) 50 (10/20) 85 (17/20) 15 (3/20) 45 (9/20) 70 (14/20) 0 (0/20) 0 (0/20) 50 (10/20) C. Ablation Study To endow the model with self-state estimation and metricaware perception, we introduce three auxiliary tasks in the first stage. Odometry supervises ego-motion estimation, Goal provides dynamic target updates based on self-state, and Point Cloud transforms historical image observations into geometric point clouds using estimated camera extrinsics. Without any auxiliary tasks, the model trained solely with end-to-end supervision exhibits basic path-planning capability. However, intermediate features lack explicit guidance, and the model may lose track of the goal when self-state estimation is inaccurate. Incorporating Odometry Fig. 5. Visualization of reconstruction results: the first row shows the scene point cloud of the ground truth, and the second row shows the predicted scene point cloud. The point cloud at the metric scale is predicted with the chassis of the last frame as the coordinate origin. and Goal supervision improves ego-motion estimation and enhances trajectory consistency. Further adding Point Cloud supervision allows the model to capture the spatial relationships of obstacles beyond 2D semantics, significantly improving obstacle avoidance in trajectory generation. TABLE III ABLATION EXPERIMENTS ON KEY MODULES After incorporating scale priors, our model not only achieves higher planning success rates but also demonstrates improved planning accuracy. Therefore, metric-scale supervision is still required for real-world applications. PERFORMANCE UNDER DIFFERENT VIDEO GEOMETRY BACKBONES TABLE IV Modules Home Commercial SR SPL NE PE SR SPL NE PE Backbone Home Commercial Odometry Goal Point Cloud SR SPL SR SPL 49.5 51.3 52.4 57.3 47.0 49.7 50.1 52.4 59.4 61.2 63.3 67.1 57.0 59.2 60.3 63. Our model grounds the pose estimation and reconstruction capabilities of the video geometry model into the planning policy, making the choice of geometric backbone crucial for the task. We experimented with several backbones: singleframe geometric backbones, DepthAnything [40], multiframe geometric backbones, Video DepthAnything [22], VGGT [12] without metric scale, and scale-injected version of VGGT [12]. During these experiments, all auxiliary tasks are retained in training. To quantify pose estimation and planning accuracy, we define two metrics: Navigation Error (NE), the Euclidean distance between the robots final stopping position and the goal, and Planning Error (PE), the distance between the endpoint of the planned trajectory and the goal. Single-frame backbones provide per-frame depth perception to for obstacle avoidance but lack temporal consistency. Multi-frame pretrained geometric models capture temporal correlations, yet without supervision on camera poses, they struggle to model accurate sequential pose relationships and keep planning consistency. Existing reconstruction-pretrained models offer reliable ego-motion estimation; however, after finetuning without depth prior, the estimated camera poses follow the correct trends but exhibit scale discrepancies. DA [40] VDA [22] VGGT [12] VGGT [12] : without injecting depth as the scale prior. 47.1 48.2 50.4 52.4 1.48 1.08 0.87 0.55 2.51 2.43 2.35 2. 49.9 51.5 54.9 57.3 59.9 61.4 62.4 67.1 57.4 58.8 57.7 63.9 2.49 2.15 2.31 2.07 1.49 1.08 1.18 0.59 VI. CONCLUSIONS We proposed LoGoPlanner, localization-grounded endto-end navigation framework that unifies metric-aware pose estimation, long-horizon scene reconstruction, and featurelevel policy conditioning. By integrating implicit selfstate estimation with fine-grained environmental perception, our method overcomes limitations of traditional modular pipelines and prior end-to-end approaches that rely on external localization. Experiments in both simulation and real-world scenarios demonstrate that LoGoPlanner achieves superior trajectory planning and obstacle avoidance, while generalizing robustly across diverse embodiments, viewpoints, and environments. This work highlights the potential of grounding navigation policies in geometric and metricaware priors, pointing toward more autonomous, reliable, and adaptable robotic navigation in unstructured real-world settings. Due to the limited number(2k) of available navigation scenes, the reconstruction performance in real-world environments remains unsatisfactory. We are currently training on real world datasets in metric-scale, to enhance the performance for practical deployment."
        },
        {
            "title": "REFERENCES",
            "content": "[1] C. Cao, H. Zhu, F. Yang, Y. Xia, H. Choset, J. Oh, and J. Zhang, Autonomous exploration development environment and the planning algorithms, in 2022 International Conference on Robotics and Automation (ICRA). IEEE, 2022, pp. 89218928. [2] D. D. Fan, K. Otsu, Y. Kubo, A. Dixit, J. Burdick, and A.-A. AghaMohammadi, Step: Stochastic traversability evaluation and planning for risk-aware off-road navigation, arXiv preprint arXiv:2103.02828, 2021. [3] L. Wellhausen and M. Hutter, Rough terrain navigation for legged robots using reachability planning and template learning, in 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2021, pp. 69146921. [4] I. Gog, S. Kalra, P. Schafhalter, M. A. Wright, J. E. Gonzalez, and I. Stoica, Pylot: modular platform for exploring latency-accuracy tradeoffs in autonomous vehicles, in 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2021, pp. 8806 8813. [5] P. Roth, J. Nubert, F. Yang, M. Mittal, and M. Hutter, Viplanner: Visual semantic imperative learning for local navigation, in 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024, pp. 52435249. [6] F. Yang, C. Wang, C. Cadena, and M. Hutter, iPlanner: Imperative Path Planning, in Proceedings of Robotics: Science and Systems, Daegu, Republic of Korea, July 2023. [7] D. Shah, A. Sridhar, A. Bhorkar, N. Hirose, and S. Levine, Gnm: to drive any robot, in 2023 IEEE IEEE, general navigation model International Conference on Robotics and Automation (ICRA). 2023, pp. 72267233. [8] E. Wijmans, A. Kadian, A. Morcos, S. Lee, I. Essa, D. Parikh, M. Savva, and D. Batra, Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames, arXiv preprint arXiv:1911.00357, 2019. [9] W. Cai, J. Peng, Y. Yang, Y. Zhang, M. Wei, H. Wang, Y. Chen, T. Wang, and J. Pang, Navdp: Learning sim-to-real navigation diffusion policy with privileged information guidance, arXiv preprint arXiv:2505.08712, 2025. [10] A. Sridhar, D. Shah, C. Glossop, and S. Levine, Nomad: Goal masked diffusion policies for navigation and exploration, in 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024, pp. 6370. [11] D. Shah, A. Sridhar, N. Dashora, K. Stachowicz, K. Black, N. Hirose, and S. Levine, Vint: foundation model for visual navigation, in Conference on Robot Learning. PMLR, 2023, pp. 711733. [12] J. Wang, M. Chen, N. Karaev, A. Vedaldi, C. Rupprecht, and D. Novotny, Vggt: Visual geometry grounded transformer, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 52945306. [13] Y. Hu, J. Yang, L. Chen, K. Li, C. Sima, X. Zhu, S. Chai, S. Du, T. Lin, W. Wang et al., Planning-oriented autonomous driving, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2023, pp. 17 85317 862. [14] J. Peng, T. Wang, J. Pang, and Y. Shen, Towards latency-aware 3d streaming perception for autonomous driving, arXiv preprint arXiv:2504.19115, 2025. [15] A. Loquercio, E. Kaufmann, R. Ranftl, M. Muller, V. Koltun, and D. Scaramuzza, Learning high-speed flight in the wild, Science Robotics, vol. 6, no. 59, p. eabg5810, 2021. [16] A. Sadat, S. Casas, M. Ren, X. Wu, P. Dhawan, and R. Urtasun, Perceive, predict, and plan: Safe motion planning through interpretable semantic representations, in European Conference on Computer Vision. Springer, 2020, pp. 414430. [17] M. Wei, C. Wan, J. Peng, X. Yu, Y. Yang, D. Feng, W. Cai, C. Zhu, T. Wang, J. Pang et al., Ground slow, move fast: dual-system foundation model for generalizable vision-and-language navigation, arXiv preprint arXiv:2512.08186, 2025. [18] Z. Xu, X. Han, H. Shen, H. Jin, and K. Shimada, Navrl: Learning safe flight in dynamic environments, IEEE Robotics and Automation Letters, 2025. [19] Y. Wang, J. Zhou, H. Zhu, W. Chang, Y. Zhou, Z. Li, J. Chen, J. Pang, C. Shen, and T. He, π3: Scalable permutation-equivariant visual geometry learning, arXiv preprint arXiv:2507.13347, 2025. [20] Q. Wang, Y. Zhang, A. Holynski, A. A. Efros, and A. Kanazawa, Continuous 3d perception model with persistent state, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 10 51010 522. [21] J. Yang, A. Sax, K. J. Liang, M. Henaff, H. Tang, A. Cao, J. Chai, F. Meier, and M. Feiszli, Fast3r: Towards 3d reconstruction of 1000+ images in one forward pass, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 21 92421 935. [22] S. Chen, H. Guo, S. Zhu, F. Zhang, Z. Huang, J. Feng, and B. Kang, Video depth anything: Consistent depth estimation for super-long videos, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 22 83122 840. [23] O. Simeoni, H. V. Vo, M. Seitzer, F. Baldassarre, M. Oquab, C. Jose, V. Khalidov, M. Szafraniec, S. Yi, M. Ramamonjisoa et al., Dinov3, arXiv preprint arXiv:2508.10104, 2025. [24] A. J. Davison, I. D. Reid, N. D. Molton, and O. Stasse, Monoslam: Real-time single camera slam, IEEE transactions on pattern analysis and machine intelligence, vol. 29, no. 6, pp. 10521067, 2007. [25] G. Klein and D. Murray, Parallel tracking and mapping on camera phone, in 2009 8th IEEE International Symposium on Mixed and Augmented Reality. IEEE, 2009, pp. 8386. [26] J. Engel, V. Koltun, and D. Cremers, Direct sparse odometry, IEEE transactions on pattern analysis and machine intelligence, vol. 40, no. 3, pp. 611625, 2017. [27] J. Engel, T. Schops, and D. Cremers, Lsd-slam: Large-scale direct monocular slam, in European conference on computer vision. Springer, 2014, pp. 834849. [28] C. Godard, O. Mac Aodha, M. Firman, and G. J. Brostow, Digging into self-supervised monocular depth estimation, in Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 38283838. [29] R. Mur-Artal, J. M. M. Montiel, and J. D. Tardos, Orb-slam: versatile and accurate monocular slam system, IEEE transactions on robotics, vol. 31, no. 5, pp. 11471163, 2015. [30] A. Kendall, M. Grimes, and R. Cipolla, Posenet: convolutional network for real-time 6-dof camera relocalization, in Proceedings of the IEEE international conference on computer vision, 2015, pp. 29382946. [31] F. Wimbauer, N. Yang, L. Von Stumberg, N. Zeller, and D. Cremers, Monorec: Semi-supervised dense reconstruction in dynamic environments from single moving camera, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 61126122. [32] Y. Wei, S. Lu, F. Han, R. Xiong, and Y. Wang, Bev-odom: Reducing scale drift in monocular visual odometry with bev representation, in 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2024, pp. 349356. [33] S. Shah, N. Rajyaguru, C. D. Singh, C. Metzler, and Y. Aloimonos, Codedvo: Coded visual odometry, IEEE Robotics and Automation Letters, 2024. [34] T. Qin, P. Li, and S. Shen, Vins-mono: robust and versatile monocular visual-inertial state estimator, IEEE transactions on robotics, vol. 34, no. 4, pp. 10041020, 2018. [35] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby et al., Dinov2: Learning robust visual features without supervision, arXiv preprint arXiv:2304.07193, 2023. [36] J. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu, Roformer: Enhanced transformer with rotary position embedding, Neurocomputing, vol. 568, p. 127063, 2024. [37] I.-N. D. contributors, Interndata-n1 dataset, https://huggingface.co/ datasets/InternRobotics/InternData-N1, 2025, accessed: 2025-09-15. [38] H. Wang, J. Chen, W. Huang, Q. Ben, T. Wang, B. Mi, T. Huang, S. Zhao, Y. Chen, S. Yang, P. Cao, W. Yu, Z. Ye, J. Li, J. Long, Z. Wang, H. Wang, Y. Zhao, Z. Tu, Y. Qiao, D. Lin, and P. Jiangmiao, Grutopia: Dream general robots in city at scale, in arXiv, 2024. [39] C. Campos, R. Elvira, J. J. Gomez, J. M. M. Montiel, and J. D. Tardos, ORB-SLAM3: An accurate open-source library for visual, visual-inertial and multi-map SLAM, IEEE Transactions on Robotics, vol. 37, no. 6, pp. 18741890, 2021. [40] L. Yang, B. Kang, Z. Huang, Z. Zhao, X. Xu, J. Feng, and H. Zhao, Depth anything v2, Advances in Neural Information Processing Systems, vol. 37, pp. 21 87521 911, 2024."
        }
    ],
    "affiliations": [
        "OpenGVLab",
        "Tsinghua University"
    ]
}