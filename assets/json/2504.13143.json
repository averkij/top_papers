{
    "paper_title": "$\\texttt{Complex-Edit}$: CoT-Like Instruction Generation for Complexity-Controllable Image Editing Benchmark",
    "authors": [
        "Siwei Yang",
        "Mude Hui",
        "Bingchen Zhao",
        "Yuyin Zhou",
        "Nataniel Ruiz",
        "Cihang Xie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce $\\texttt{Complex-Edit}$, a comprehensive benchmark designed to systematically evaluate instruction-based image editing models across instructions of varying complexity. To develop this benchmark, we harness GPT-4o to automatically collect a diverse set of editing instructions at scale. Our approach follows a well-structured ``Chain-of-Edit'' pipeline: we first generate individual atomic editing tasks independently and then integrate them to form cohesive, complex instructions. Additionally, we introduce a suite of metrics to assess various aspects of editing performance, along with a VLM-based auto-evaluation pipeline that supports large-scale assessments. Our benchmark yields several notable insights: 1) Open-source models significantly underperform relative to proprietary, closed-source models, with the performance gap widening as instruction complexity increases; 2) Increased instructional complexity primarily impairs the models' ability to retain key elements from the input images and to preserve the overall aesthetic quality; 3) Decomposing a complex instruction into a sequence of atomic steps, executed in a step-by-step manner, substantially degrades performance across multiple metrics; 4) A straightforward Best-of-N selection strategy improves results for both direct editing and the step-by-step sequential approach; and 5) We observe a ``curse of synthetic data'': when synthetic data is involved in model training, the edited images from such models tend to appear increasingly synthetic as the complexity of the editing instructions rises -- a phenomenon that intriguingly also manifests in the latest GPT-4o outputs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 3 4 1 3 1 . 4 0 5 2 : r Complex-Edit: CoT-Like Instruction Generation for Complexity-Controllable Image Editing Benchmark Siwei Yang1 Mude Hui1 Bingchen Zhao2 Yuyin Zhou1 Nataniel Ruiz3 Cihang Xie1 1 University of California, Santa Cruz 2 University of Edinburgh 3 Google Project Page: https://ucsc-vlaa.github.io/Complex-Edit/ Dataset: https://huggingface.co/datasets/UCSC-VLAA/Complex-Edit Figure 1. An illustration of our Complex-Edit Benchmark. This figure presents structured progression of instruction complexity in image editing tasks, highlighting the transition from atomic edits to highly intricate transformations."
        },
        {
            "title": "Abstract",
            "content": "We introduce Complex-Edit, comprehensive benchmark designed to systematically evaluate instruction-based image editing models across instructions of varying complexity. To develop this benchmark, we harness GPT-4o to automatically collect diverse set of editing instructions at scale. Our approach follows well-structured Chainof-Edit pipeline: we first generate individual atomic editing tasks independently and then integrate them to form cohesive, complex instructions. Additionally, we introduce suite of metrics to assess various aspects of editing performance, along with VLM-based auto-evaluation pipeline that supports large-scale assessments. Our benchmark yields several notable insights: 1) Opensource models significantly underperform relative to proprietary, closed-source models, with the performance gap widening as instruction complexity increases; 2) Increased instructional complexity primarily impairs the models ability to retain key elements from the input images and to preserve the overall aesthetic quality; 3) Decomposing complex instruction into sequence of atomic steps, executed in step-by-step manner, substantially degrades performance across multiple metrics; 4) straightforward Best-of-N selection strategy improves results for both direct editing and the step-by-step sequential approach; and 5) We observe curse of synthetic data: when synthetic data is involved in model training, the edited images from such models tend to appear increasingly synthetic as the complexity of the editing instructions rises phenomenon that intriguingly also manifests in the latest GPT-4o outputs. 1 1. Introduction Since InstructPix2Pix [3] introduced instruction-based image editingwhere images are directly modified through textual commandsthe field has witnessed remarkable progress. Noteworthy developments include enhancements in the quality of training images [2, 10, 12, 24, 32, 33], the evolution of model architectures from CNN-based to transformer-based diffusion models [16], the refinement of training methodologies [20, 21], and the on-going exploration of test-time scaling [7, 17]. Despite these strides, much of the available training data and evaluation benchmarks rely on relatively simple editing instructions [3, 14, 18, 24, 3033]. Yet, real-world scenarios often demand the ability to handle instructions that vary considerably in complexity. For instance, while one user might simply request to remove the car, another might need more elaborate transformation, such as replace the car with blue bus featuring an iPhone advertisement on its side. The current lack of evaluation benchmarks capable of handling such diverse instruction complexities not only impedes our ability to rigorously assess existing editing models but also hampers the evaluation of emerging, more powerful systems [7, 17]. To bridge this gap, we present novel data generation pipeline that leverages advanced GPT models to create diverse, scalable, and complexity-controllable evaluation dataset for image editing (see Fig. 2 for an illustration). Our approach follows chain-of-thought-like [25] paradigm, unfolding in three key stages. First, in the Sequence Generation stage, an input image prompts GPT-4o to generate series of simple instructions corresponding to predefined atomic operations (e.g., Add an Object, Change the Object Color), with each serving as an intermediate step toward more complex editing task. Next, recognizing that these GPT-generated atomic instructions may contain superfluous/unnecessary detailssuch as added commentary on the operations intentionwe then move to Simplification stage, where each instruction is trimmed to retain only its core editing intent. In this final Instruction Compounding stage, the simplified atomic instructions are merged into single, coherent complex instruction (again, via GPT-4o), using the input image as contextual guidance. We name this new dataset Complex-Edit. Importantly, this structured process of creating Complex-Edit allows us to quantitatively control the editing instructions complexity by simply adjusting the number of merged atomic instructions. To evaluate performance on these complex instructions, we developed comprehensive evaluation framework that measures three critical dimensions: (1) Instruction Following, which assesses whether the intended modifications are correctly applied; (2) Identity Preservation, which ensures that unspecified elements remain unchanged; and (3) Perceptual Quality, which evaluates the overall aesthetic quality and absence of artifacts. To support large-scale evaluations, we implement vision language model (VLM)-based autograding system to quantify these dimensions. Our investigation further identifies important considerations in enhancing this evaluation pipeline. For example, enabling chain-of-thought reasoning in VLM evaluations does not necessarily enhance evaluation qualitycontrary to previous findings [10, 13]while providing detailed rubrics and using direct numeric scoring [4, 28, 29, 34] consistently improves autograder performance. Our extensive evaluation results reveal five key insights that were previously difficult to capture with existing benchmarks. First, open-source models significantly underperform compared to proprietary, closed-source models, with this disparity growing as editing complexity increases. Second, the increasing instruction complexity mainly affects editing models perceptual quality (especially in identity preservation), while its impact on instruction following varies across models. Thirdly, unlike the benefits observed with chain-of-thought (CoT) reasoning in text generation, CoT-inspired sequential editing yields much worse results than directly executing complex instructions we note this is because sequential editing significantly degrades outputs in instruction following, identity preservation, and perceptual quality. Fourth, simple techniques such as Best-of-N sampling improve direct editing outcomes across all three dimensions and, when applied to sequential editing, notably enhance identity preservation and perceptual quality. Lastly, our Complex-Edit reveals an emerging curse of synthetic data when training utilizes synthetic images, models tend to produce edited outputs with overly synthetic qualities under complex instructions, often resembling oil paintings or animations. More interestingly, this phenomenon is also observed with the latest GPT-4o models, potentially suggesting that the incorporation of synthetic data may contribute to GPT-4os advanced image generation capabilities. We this hope newly developed benchmark, Complex-Edit, will not only deepen the communitys understanding of instruction-based image editing models but also serve as valuable framework for the rigorous evaluation of next-generation image editing systems, particularly those with test-time scalability. 2. Collection of Complex-Edit As illustrated in Fig. 2, the collection of Complex-Edit is organized into three stages: 1) Stage #1 Sequence Generation: sequence of atomic instructions are generated for each image; 2) Stage #2 Simplification: each atomic instruction is simplified to remove unnecessary information other than the description of the editing operation; 3) Stage #3 Instruction Compounding: multiple atomic instructions are combined into single, complex instruction. 2 Figure 2. An overview of our data collection pipeline. The pipeline consists of three distinct stages: 1) Stage #1 Sequence Generation: for each image, series of atomic instructions is produced; 2) Stage #2 Simplification: each fundamental instruction is refined to eliminate extraneous details, preserving only the essential description of the editing process; 3) Stage #3 Instruction Compounding: several atomic instructions are integrated into one comprehensive instruction. input image, are fed into GPT-4o to generate sequence of atomic instructions. Note that each generated instruction adheres to predetermined length and is associated with one of the predefined atomic operations. We also record the atomic operation type for each instruction, with its distribution illustrated in Fig. 3. This metadata can facilitate the future development of specialized image editing models [24, 30]. 2.2. Simplification In practice, these GPT-generated instructions sometimes are unnecessarily rich in detail and may include extraneous commentary on the intentions of the editing operations, as shown in Fig. 2. Because our benchmark is designed to evaluate the performance of image-editing models on clear and concise instructions, we employ dedicated simplification stage to eliminate any superfluous content. Specifically, each generated atomic instruction is examined by GPT-4o to determine whether it requires further simplification; if so, GPT-4o outputs simplified version adhering to the predefined response format, and the original instruction is replaced accordingly. Figure 3. The distribution of atomic instructions among 9 operation categories. The arc thickness between two categories shows the number of adjacent instructions from these two categories. 2.1. Sequence Generation 2.3. Instruction Compounding In this first stage, we begin by defining 24 distinct atomic operations that represent the most basic actions of image editing. These operations can be grouped into 9 categories Object Manipulation and Transformation, Color and Tone Adjustments, Texture and Material Adjustments, Background and Environment, Lighting and Shadows, Text and Symbols, Composition and Cropping, Pose and Expression, Special Effects each of which captures unique aspect of image editing, as shown in Fig. 4. We provide detailed descriptions of each operation type in the supplementary materials Sec. 8. These categorized atomic operation types and their descriptions, along with an Lastly, we combine these simplified atomic instructions into compound instructions. Specifically, given sequence of atomic instructions, we progressively generate compound instructions corresponding to different levels of complexity: compound instruction at complexity level Ci is formed by combining the first atomic instructions in the sequence, with the simplest level (C1) being identical to the first atomic instruction and the hardest level (CN ) integrating all atomic instructions. In implementation, rather than simply concatenating the instructions, GPT-4o is employed to seamlessly integrate them, potentially reordering or merging operations to make 3 Figure 4. An illustration of 24 types of atomic editing operations in 9 categories. the final outcome more natural and coherent. For example, as shown in Fig. 2, rather than sequentially executing add ball of yarn and change the color of the yarn to red, the instructions are compounded into single directive: add red ball of yarn. 2.4. Implementation Details To ensure that GPT-4o fully understands our objectives, we carefully construct multiple few-shot examples for each of the three stages. Additionally, CoT reasoning [25] is enabled in both Stage #1 (Sequence Generation) and Stage #3 (Instruction Compounding) to enhance generation quality. Furthermore, to improve data diversity, we slightly increase the sampling temperature from 1.0 to 1.15 during Stage #1 (Sequence Generation). Although this adjustment promotes diversity, it also increases the likelihood of generating garbled text. To address this, we implement rule-based filtration mechanism at each stage that detects and regenerates any flawed outputs. 3. Evaluation Following prior works [10, 13], we employ VLM-based autograders to facilitate large-scale evaluation. However, we have observed that these frameworks do not fully capture the nuances of instruction-guided image editing. In response, we introduce several enhancements to address these limitations and improve the overall autograding framework. 3.1. Metric Design Our evaluation framework focuses on two primary dimensions: 1) the alignment dimensionwhether the output image reflects the changes specified by the editing instruction, and 2) the perceptual quality dimensionwhether the output image looks aesthetically pleasing and is devoid of visual artifacts. 3.1.1. Alignment Unlike HQ-Edit [10], which simply measures alignment as single metric, we hereby decompose it into two complementary criteria: Instruction Following (IF ): Measures whether the specified modifications are present in the output image. Identity Preservation (IP ): Assesses whether elements of the input image that should remain unchanged are indeed preserved. We would like to point out that these two criteria roughly correspond to the directional CLIP similarity (CLIPdir) and image-wise CLIP similarity (CLIPimg) metrics used in earlier studies [3, 31, 32]. When accessing IF and IP , we feed the VLM with triplet containing {an input image, an output image, and an editing instruction}. For better visual 4 Figure 5. Examples of evaluation results for Instruction Following and Identity Preservation. Figure 6. Examples of evaluation results for Perceptual Quality. understanding, we provide series of example results of IF and IP in Fig. 5. 3.1.2. Perceptual Quality Beyond alignment, we also evaluate the overall visual quality of the generated images. Specifically, our Perceptual Quality (PQ) metric examines factors such as consistency in lighting and shadows, style coherence, and the seamless integration of elements. Because users may sometimes request bizarre or non-aesthetic edits, such as add motion blur to these vehicles or enlarge the pet to match the height of the owner, one might argue that it is essential to include editing instructions when evaluating perceptual quality. However, as discussed in Sec. 4.1.4, our empirical results indicate that providing editing instructions to VLMs actually reduces the correlation between VLM and human evaluations. Consequently, for evaluation, we only supply the VLM autograder with the edited image. Sample results are provided in Fig. 6. 3.1.3. Overall Score Lastly, we define the overall score, O, simply as the arithmetic mean of the three metrics to summarize performance: 3.2. Metric Calculation 3.2.1. Numeric Scoring v.s. Token Probability In line with VIEScore and HQ-Edit, we instruct the VLM to assign score between 0 and 10 for each metric. We refer to this method as numeric scoring. In addition, we explore another approach for metric computation: token probalility, which is widely used in test-time scaling and VLM post-training [4, 28, 29, 34]. Specifically, rather than directly asking the VLM for numerical score, we reformulate the evaluation as binary classification task by posing yes-or-no question to the VLM, e.g., Do the specified changes appear in the output image?. The score is then calculated as the normalized token probability for the affirmative response Yes using ProbYes/(ProbYes + ProbNo). 3.2.2. Detailed Rubric In contrast to existing frameworks like VIEScore and HQEdit, we design comprehensive rubrics for each metric to guide the evaluation process and improve result interpretability. Detailed descriptions of these rubrics are provided in the supplementary material Sec. 9.1. In addition, we discuss the impact of these rubrics and different scoring approaches in Sec. 4.1.2. 3.3. Per-sample Variance = IF + IP + 3 (1) Previous evaluations of image-editing models using advanced proprietary VLMs, such as GPT-4o [11] and Gemini"
        },
        {
            "title": "Deterministic",
            "content": ""
        },
        {
            "title": "Correlation",
            "content": "IF IP"
        },
        {
            "title": "P Q",
            "content": "0.468 0.461 0.530 0.507 0.234 0.215 Table 1. Random sampling vs Discriminative sampling. Scoring Method CoT Rubric Token Prob Token Prob Numeric Numeric Numeric Correlation IF 0.411 0.447 0.434 0.446 0. IP 0.366 0.460 0.450 0.451 0.530 0.136 0.158 0.207 0.234 0.208 CLIPdir CLIPimg 0. 0.523 Table 2. The results from different scoring methods. We can observe that w/ COT and w/ Rubric are the best for IF and IP, and w/o COT and w/ rubric are the best for PQ. [22, 23], have encountered issues with score stability, leading to score variations for individual samples across different runs. Although averaging over many samples generally mitigates these discrepancies at the dataset level, persample stability is crucial for test-time scaling, where accurate and consistent evaluation is required for each individual sample [7, 17, 29]. straightforward approach to enhance stability utilizes multiple measurements to average the resultant scores, though this approach may considerably escalate computational costs. Alternatively, one can attempt to make VLM outputs more deterministic by using greedy random sampling techniques (e.g., by setting the sampling probability mass to an extremely low value such as 1e-7). However, empirically, this deterministic approach slightly lower the correlation with human evaluations as shown in Tab. 1. Consequently, we adopted the first approach by default, utilizing the average score from 20 independent evaluations for each sample. Further discussion on reducing per-sample variance is included in Sec. 4.2. 4. Meta-Evaluation for Metrics In this section, we detail our meta-evaluation of the VLM-based auto-evaluation framework, focusing on design choices and their effectiveness in assessing image editing model outputs. We expect an effective auto-evaluation pipeline to satisfy two core criteria: 1) High correlation with human judgments, and 2) Low variance for ensuring reproducibility. Next, we first examine how various design decisions impact correlation with human evaluations (Section 4.1) and then describe our strategies for reducing persample variance (Section 4.2). 4.1. Correlation with Human Evaluation 4.1.1. Meta-Evaluation Settings Meta-evaluation for instruction-based image editing metrics [10, 13] is typically performed by correlating humanassigned scores with metric values. However, we found that expecting human raters to assign consistent numeric scores across diverse samples is challenging. Instead, for each input image and editing instruction, we present raters with pair of output images and ask them to compare the outputs with respect to each metric. We then compute the correlation between these human comparisons and the differences in the corresponding metric scores. Additionally, we ask raters to choose their preferred output based on overall impression. This comparative approach allows us to align our metric aggregation with human intuition and to determine the best way to combine the three proposed metrics into single score. To ensure that our meta-evaluations generalize across varying levels of instruction complexity, we sampled 100 output images for each complexity level from C1 (i.e., the simplest level, just single atomic instruction) to C8 (i.e., the hardest level, compounding from 8 atomic instructions) using both Imagen3 [2] and SeedEdit [20], resulting in 800 image pairs. Each pair was annotated independently by at least two human raters. We use GPT-4o as our VLM evaluator. Since the per-sample variance is yet to be discussed in Sec. 4.2, for now, we naively evaluate each sample 40 times and average the scores to reduce the per-sample variance. 4.1.2. Scoring Method and Rubric As shown in Tab. 2, our experiments indicate that numeric scoring yields higher correlation with human evaluation than token probability methods. Furthermore, supplementing the numeric scoring with detailed rubric consistently improves this correlation, emphasizing the importance of clear evaluation guidelines. 4.1.3. Chain-of-Thought We additionally investigate the impact of CoT on our evaluation by adding an explicit instruction (e.g., explain your reasoning before answering the question) to encourage GPT to articulate its reasoning prior to delivering the final score. The complete prompt for evaluation is provided in Sec. 9.2. However, as shown in Table 2, our results indicate that CoT fails to enhance metric correlation consistently. In particular, CoT negatively affects the correlation for Perceptual Quality when using numeric scoring. Consequently, we utilize numeric scoring with detailed rubrics and CoT only for Instruction Following IF and Identity Preservation IP , while CoT is disabled during Perceptual Quality evaluation. 6 Figure 7. Relationship between per-sample variance and the number of measurements (a) Real-life input images. (b) Synthetic input images. Figure 8. Evaluation results of direct and sequential editing. The instruction complexity level ranges from C1 to C8. This indicates that IP and consistently dropped as the instruction complexity level grows, while it influence on IF varies among different models. Also, the performance gap between models is enhanced with increasing complexity. 4.1.4. Instruction Input for Perceptual Quality 4.1.5. Different Averaging Formula for Overall Score One might initially hypothesize that including the editing instructions alongside the output image could help the VLM discount aesthetically inferior modifications that are explicitly required by the instructions during Perceptual Quality evaluation. However, our experiments reveal that providing the editing instructions in this context dramatically reduces the correlation with human evaluationsfrom 0.234 to 0.046. Consequently, we exclude the editing instructions when evaluating Perceptual Quality Q, ensuring that the VLMs assessments of image quality better align with human perception. 7 Previous studies [13, 24] advocate using the geometric mean to combine individual metrics to penalize low scores. In our case, switching from the geometric mean to the arithmetic mean resulted in modest correlation increase from 0.334 to 0.386. Therefore, we compute the overall score as the arithmetic mean of the individual metrics. 4.1.6. Comparison with CLIP Scores We further compare the correlation of Directional CLIP Score CLIPdir and Image-wise CLIP Score CLIPimg with human evaluation, as these scores similarly assess aspects of Instruction Following IF and Identity Preservation IP . As reported in Tab. 2, these CLIP scores exhibit lower correlations with human evaluations compared to our metrics. (a) real input images. Figure 9. Outputs from open-source and proprietary models via direct editing. See results with more models in Figs. 17 and 18. (b) Synthetic input images. 8 Figure 10. real image edited with C8 instruction. Outputs from certain models tend to lose the realistic style completely. 4.2. Per-Sample Variance As discussed in Sec. 3.3, one way to reduce per-sample variance is to set the probability mass for sampling to an extremely small value, e.g., 1e-7, thereby approximating the determinism of proprietary VLMs. However, as shown in Table 1, this approach slightly diminishes the correlation with human evaluations. Consequently, we disable determinstic sampling and instead perform multiple evaluations per sample, using the average score as the final metric. To determine the required number of measurements per sample, we study the relationship between the number of measurements per sample and the variation in final scores across independent runs. Specifically, given measurements for one sample, we compute average scores four times, each averaged from distinct measurements, denoted as S1, S2, S3, S4. The variation for this sample is defined as max{S1,S2,S3,S4}min{S1,S2,S3,S4} . Figure 7 shows that the variation amplitudes for all three metrics converge when = 20. Therefore, we adopt 20 measurements per sample in all subsequent experiments. 5. Experiments 5.1. Experiment Setup Dataset. Our Complex-Edit dataset comprises both realistic and synthetic images. Specifically, we select 531 deduplicated images from the EMU-Edit test set [19] to serve as our realistic image collection. In parallel, we use FLUX.1 [15] to generate an equivalent set of 531 synthetic images based on the captions associated with these realworld images, also sourced from EMU-Edit. Following Sec. 2, we create editing instructions with the complexity ranging from C1 to C8 for these images. five involve experiments Models. Our advanced instruction-based image editing models: three open-source models (UltraEdit [33], OmniGen [26], AnyEdit [30]) and two proprietary models (Imagen3 [2], SeedEdit [20]). We evaluate each model on both realistic and synthetic images using editing instructions spanning complexity levels from C1 to C8. Note that, owing to usage policies for the proprietary models, results are reported on approximately 60% of the images for Imagen3 and 95% for SeedEdit. Model Prop. of Real Images Realistic Style IF IP UltraEdit OmniGen AnyEdit SeedEdit Imagen3 GPT4o ? ? 6.56 6.25 1.60 8.49 7.56 9.29 5.93 6.42 8.15 6.91 6.55 7.51 PQ 7.29 7.54 7.25 8.74 7.67 9. 6.59 6.74 5.67 8.04 7.26 8.76 Table 3. Evaluation results with direct editing on real images with the instruction complexity at C8. Additionally, we perform preliminary evaluation of the newly launched GPT-4o, which now features image generation capabilities. Without API access, we obtain output images via the GPT-4o web interface and center-crop them to match the input aspect ratio. Our evaluation of GPT-4o is limited to roughly 30% of realistic images at the hardest instruction complexity level C8. 5.2. Effect of Increasing Complexity Figure 8 illustrates the overall performance trends as instruction complexity increases. In general, higher complexity leads to consistent decline in both Identity Preservation and Perceptual Quality across models, while Instruction Following tends to fluctuate depending on the specific model. For instance, although AnyEdit exhibits substantial drop in Instruction Following as complexity increases, its Perceptual Quality improves moderately. Additionally, proprietary models outperform open-source alternatives on both realistic and synthetic images, and the performance gap becomes more pronounced at higher complexity levels. Furthermore, as shown in Tabs. 4 and 5, open-source models performance drops are notably more severe than proprietary models. For example, when editing synthetic images, all open-source models have drops larger than 1 in Overall score while the Overall scores drop of Imagen3 and SeedEdit are 0.66 and 0.95 respectively, implying that stronger models are not more resilient towards the negative impact of increased instruction complexity. Sample qualitative results are shown in Figure 9. Our preliminary evaluation of GPT-4o, as detailed in Table 3, demonstrates that it significantly outperforms all other models at the hardest instruction complexity level C8, particularly in terms of Instruction Following and Perceptual Quality. 9 Figure 11. real image edited with C8 instruction by GPT-4o. Outputs from GPT-4o severely lose the realistic style. See additional results in Fig. 22. Model UltraEdit OmniGen AnyEdit Imagen3 SeedEdit C8 C8 C1 IF IP 7.13 6.76 5.94 7.67 9.31 7.76 8.69 8.50 8.93 9. 7.45 7.99 6.78 7.90 8.71 IF IP 7.45 7.82 7.07 8.17 9. 6.56 6.25 1.61 7.56 8.50 5.93 6.42 8.15 6.55 6.91 7.29 7.55 7.25 7.68 8.74 IF IP O 6.59 6.74 5.67 7.27 8.05 -0.57 -0.50 -4.33 -0.11 -0.81 -1.82 -2.27 -0.34 -2.38 -2.10 -0.16 -0.45 +0.47 -0.22 +0.02 -0.85 -1.07 -1.40 -0.90 -0.96 Table 4. Performance comparison between C1 and C8 on real images. All models almost consistently underperform on all metrics when the complexity increases from C1 to C8. 5.3. Curse of Synthetic data Our qualitative analysis reveals an intriguing phenomenon: when applying extremely complex editing instructions (C8) to real input images, the resulting outputs frequently lose their realistic appearance and adopt distinctly synthetic aesthetic. Notably, UltraEdit is particularly susceptible to this effect compared to OmniGen and AnyEdit, as illustrated in Figure 10. We attribute this trend to the composition of UltraEdits training data, which contains significantly higher proportion of synthetic images than the datasets used by OmniGen and AnyEdit (see Table 3). This observation aligns with the curse of synthetic data phenomenon discussed in [6]. This phenomenon also extends to highly capable proprietary models, including SeedEdit, Imagen3, and even GPT4o. Specifically, as illustrated in Figure 11, the edited images produced by GPT-4o completely lose their realistic style, even though its overall performance surpasses that of all other models evaluated. This observation may indicate that, although the training sources for Imagen3 and GPT-4o are undisclosed, these models likely incorporate synthetic data into their training sets, which in turn contributes to their impressive performance as well as the emergence of distinctly synthetic aesthetic. 5.4. Test-Time Scaling Approaches 5.4.1. CoT-like Sequential Editing Our data pipeline composes complex instructions by combining sequences of atomic editing operations, suggesting that sequential application of these atomic steps should be equivalent to executing the complex instruction directly. Moreover, this way of decomposing complex task and 10 Model UltraEdit OmniGen AnyEdit Imagen3 SeedEdit C1 C8 C1 IF IP 7.82 7.80 5.79 8.27 9.33 8.61 9.34 9.17 9.25 9.60 9.02 9.05 8.61 8.86 9.29 IF IP 8.48 8.73 7.86 8.79 9.41 6.51 6.49 1.47 7.97 8. 6.83 7.47 8.73 7.54 7.94 8.72 8.74 8.72 8.88 9.20 IF IP O 7.35 7.57 6.31 8.13 8. -1.30 -1.31 -4.32 -0.30 -1.08 -1.78 -1.87 -0.44 -1.72 -1.66 -0.30 -0.31 +0.11 +0.03 -0.10 -1.13 -1.16 -1.55 -0.66 -0.95 Table 5. Performance comparison between C1 and C8 on synthetic images. All models almost consistently underperform on all metrics when the complexity increases from C1 to C8. (a) real input images. (b) Synthetic input images. Figure 12. Qualitative results of direct and sequential editing. The complexity level ranges from C1 to C8. It can be seen that IF and are severely hurt by the growing instruction complexity. The instructions are the same ones in Fig. 9. Qualitative results with more models are shown in the supplementary Figs. 17 to 20. then sequentially executing it has been proven effective in both language generation [25] and text-to-image generation [7]. Formally, given an input image x, complex instruction at complexity level Ci, and corresponding sequence of atomic instructions t1, . . . , ti, we define the outputs for direct and sequential editing as follows: ydirect = (x, ) ysequence = i(x, {t1, . . . , ti}) = (f (f (. . . (x, t1) . . . , ti1), ti)) (2) (3) (4) 11 Figure 13. Qualitative results of sequential editing with and without Best-of-4 with OmniGen on real input images. The complexity level is at C8. This shows that sequential editing can benefit lot from Best-of-N in terms of IF and Q. The instructions are the same ones in Fig. 9a. Qualitative results with more models are shown in the supplementary Fig. 21. Figure 14. Direct and Sequential Editing results w/ Best-of-N of open-source models on real input images. The complexity level is at C8. This shows that Best-of-N can improve IF and especially for sequential editing. However, sequential editing with Best-of-N can still barely surpass direct editing without Best-of-N . We evaluate sequential editing on both real and synthetic images. Our results (summarized in Figure 8 and illustrated qualitatively in Figure 12) reveal that sequential editing yields steady decline in performance across all three metrics, with accumulating visual artifacts and distortionseven for strong proprietary models such as Imagen3 and SeedEdit. Notably, AnyEdit demonstrates improved Instruction Following with sequential editing compared to direct editing, although this improvement is offset by reduction in Identity Preservation. 5.4.2. Best-of-N We also experiment with Best-of-N, which is another simple test-time-scaling method commonly used for text-toimage generation [7, 17]. For direct editing, we generate candidate outputs for each input image and select the one with the highest overall score O. For sequential editing, we generate candidates at each step and choose the candidate with the highest overall score to proceed to the next editing operation. When evaluating an intermediate candidate at the i-th step during sequential editing, instead of considering it as the output of single-step edit produced with the predecessor output and an atomic instruction, e.g. (yi1, ti) with yi1 = i1(x, {t1, . . . , ti1}), we evaluate it as the output from the original input image with compound instruction Ti. This compound instruction Ti has Ci complexity and encompasses all the atomic instructions up to the i-th step: {t1, . . . , ti}. The rationale here is that the intermediate result should best represent the cumulative effects of all preceding atomic steps on the original image, ensuring the final output best reflects all atomic steps and consequently the final compound instruction. Our evaluations with open-source models on real-life images (see Figure 14) indicate that increasing gradually improves direct editing performance across all metrics. For sequential editing, even Best-of-N strategy with = 2 produces significant gains in Identity Preservation and Perceptual Quality; however, the improvement in Instruction Following is less consistent. Qualitative comparisons are provided in Figure 13. 6. Related Works instructions, Instruction-Based Image Editing InstructPix2Pix [3] established the paradigm of instruction-based image editing, whereby images are directly modified through texthus enhancing user interaction effitual ciency. This approach necessitated the creation of substantial dataset for instruction-based image editing, constructed through the utilization of fine-tuned GPT-3 [1] and the Prompt-to-Prompt [8] Diffusion Model. Nevertheless, the editing capabilities were constrained by the limited generation quality of these underlying models. Subsequent research endeavors have sought to address these innovations. limitations through various methodological MagicBrush [12] proposed manually curated annotation methodology, while HQ-Edit[10] employed sophisticated black-box large language models in conjunction with textto-image generation systems to produce datasets of superior quality. To enhance instruction granularity, MGIE [5] and SmartEdit [9] implemented multilevel language learning models (MLLLM), facilitating more precise editing operations. Furthermore, UltraEdit [33] introduced advanced masking techniques to enable fine-grained image manipulation. OmniGen [26] expanded both the model architecture and data corpus, implementing unified Transformer framework capable of concurrent processing and comprehension of textual and visual inputs while inferring editing targets from linguistic descriptions. UIP2P [21] introduced an innovative cyclic editing methodology, enabling unsupervised instruction-driven image editing without the requirement for paired edited images during the training phase. The domain of instruction-based image editing has witnessed substantial advancements through these diverse methodological contributions. Test-Time Scaling for Image Generation Recently, inspired by Large Reasoning models such as OpenAIs GPT4o and DeepSeek-R1, substantial efforts have been dedicated to enhancing image generation results through Test-Time Scaling methodologies. Ma et al. [17] employed verifiers to provide critical feedback while utilizing various search algorithms to identify optimal noise parameters, thereby enhancing the quality of generated samples during the inference stage. Xie et al. [27] implemented Repeated Sampling techniques in conjunction with VLM-based Evaluation mechanisms to improve both the overall aesthetic quality of generated images and their semantic alignment with textual prompts. Guo et al. [7] introduced test-time verification and preference alignment within the autoregressive image generation process, significantly improving the quality of text-to-image generation. Although Guo et al. [7] have successfully implemented CoT-like sequential generation in text-to-image generation tasks, maintaining image identity remains critical challenge in image editing contexts, imposing substantially higher constraints and requirements on editing models. The preservation of original image elements while executing targeted modifications necessitates more sophisticated approaches than those employed in other less-constrained image generation tasks, thus presenting distinctive set of technical challenges for instruction-based editing systems. In this paper, rather than proposing more powerful instruction-based image editing models, we introduce challenging instruction editing dataset designed to systematically assess how existing models perform under complex instructions. 7. Conclusion In this work, we introduced Complex-Edit, comprehensive benchmark designed to systematically evaluate instruction-based image editing models across varying levels of instruction complexity. Through extensive experiments, Complex-Edit enable us to uncover multiple key insights regarding the limitations and capabilities of current models. We hope this benchmark will drive future advancements in developing more powerful instruction-based image editing models, especially those with test-time scaling ability."
        },
        {
            "title": "Acknowledgement",
            "content": "We would like to thank Google Cloud Research Credits Program, and the Microsoft Accelerate Foundation Models Research Program for supporting our computing needs."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 13 [2] Jason Baldridge, Jakob Bauer, Mukul Bhutani, Nicole Brichtova, Andrew Bunner, Lluis Castrejon, Kelvin Chan, Yichang Chen, Sander Dieleman, Yuqing Du, et al. Imagen 3. arXiv preprint arXiv:2408.07009, 2024. 2, 6, 9 [3] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1839218402, 2023. 2, 4, 13 [4] Chenhang Cui, An Zhang, Yiyang Zhou, Zhaorun Chen, Gelei Deng, Huaxiu Yao, and Tat-Seng Chua. Finegrained verifiers: Preference modeling as next-token pre13 diction in vision-language alignment. arXiv:2410.14148, 2024. 2, arXiv preprint [5] Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang, and Zhe Gan. Guiding instruction-based image editing via multimodal large language models. arXiv preprint arXiv:2309.17102, 2023. 13 [6] Matthias Gerstgrasser, Rylan Schaeffer, Apratim Dey, Rafael Rafailov, Henry Sleight, John Hughes, Tomasz Korbak, Rajashree Agrawal, Dhruv Pai, Andrey Gromov, et al. Is model collapse inevitable? breaking the curse of recursion arXiv preprint by accumulating real and synthetic data. arXiv:2404.01413, 2024. 10 [7] Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Peng Gao, Hongsheng Li, and Pheng-Ann Heng. Can we generate images with cot? lets verify and reinforce image generation step by step. arXiv preprint arXiv:2501.13926, 2025. 2, 6, 11, 12, 13 [8] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt imarXiv preprint age editing with cross attention control. arXiv:2208.01626, 2022. 13 [9] Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan, Xiaodong Cun, Yixiao Ge, Jiantao Zhou, Chao Dong, Rui Huang, Ruimao Zhang, et al. Smartedit: Exploring complex instruction-based image editing with multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8362 8371, 2024. [10] Mude Hui, Siwei Yang, Bingchen Zhao, Yichun Shi, Heng Wang, Peng Wang, Cihang Xie, and Yuyin Zhou. HQ-edit: high-quality dataset for instruction-based image editing. In The Thirteenth International Conference on Learning Representations, 2025. 2, 4, 6, 13 [11] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 5 [12] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60076017, 2023. 2, 13 [13] Max Ku, Dongfu Jiang, Cong Wei, Xiang Yue, and Wenhu Chen. Viescore: Towards explainable metrics for conditional image synthesis evaluation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1226812290, 2024. 2, 4, 6, 7 [14] Max Ku, Tianle Li, Kai Zhang, Yujie Lu, Xingyu Fu, Wenwen Zhuang, and Wenhu Chen. Imagenhub: Standardizing the evaluation of conditional image generation models. In The Twelfth International Conference on Learning Representations, 2024. 2 [15] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. [16] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. SiT: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In ECCV, 2024. 2 [17] Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, YuChuan Su, Mingda Zhang, Xuan Yang, Yandong Li, Tommi Jaakkola, Xuhui Jia, et al. Inference-time scaling for diffusion models beyond scaling denoising steps. arXiv preprint arXiv:2501.09732, 2025. 2, 6, 12, 13 [18] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. arXiv preprint arXiv:2311.10089, 2023. 2 [19] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8871 8879, 2024. 9 [20] Yichun Shi, Peng Wang, and Weilin Huang. Seededit: Align image re-generation to image editing. arXiv preprint arXiv:2411.06686, 2024. 2, 6, [21] Enis Simsar, Alessio Tonioni, Yongqin Xian, Thomas Hofmann, and Federico Tombari. Uip2p: Unsupervised instruction-based image editing via cycle edit consistency. arXiv preprint arXiv:2412.15216, 2024. 2, 13 [22] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 6 [23] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 6 [24] Cong Wei, Zheyang Xiong, Weiming Ren, Xeron Du, Ge Zhang, and Wenhu Chen. Omniedit: Building image editing generalist models through specialist supervision. In The Thirteenth International Conference on Learning Representations, 2025. 2, 3, 7 [25] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 2, 4, 11 [26] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. 9, [27] Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, Han Cai, et al. Sana 1.5: Efficient scaling of training-time and inference-time compute in linear diffusion transformer. arXiv preprint arXiv:2501.18427, 2025. 13 [28] Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, and Michael Xie. Self-evaluation guided beam search for reasoning. Advances in Neural Information Processing Systems, 36:4161841650, 2023. 2, 5 14 [29] Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy Lillicrap, Kenji Kawaguchi, and Michael Shieh. Monte carlo tree search boosts reasoning via iterative preference learning. arXiv preprint arXiv:2405.00451, 2024. 2, 5, 6 [30] Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Anyedit: Mastering unified arXiv preprint high-quality image editing for any idea. arXiv:2411.15738, 2024. 2, 3, 9 [31] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instructionguided image editing. Advances in Neural Information Processing Systems, 36:3142831449, 2023. [32] Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, et al. Hive: Harnessing human feedback for instructional visual editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 90269036, 2024. 2, 4 [33] Haozhe Zhao, Xiaojian Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained In The Thirty-eight Conference on image editing at scale. Neural Information Processing Systems Datasets and Benchmarks Track, 2024. 2, 9, 13 [34] Yiyang Zhou, Zhiyuan Fan, Dongjie Cheng, Sihan Yang, Zhaorun Chen, Chenhang Cui, Xiyao Wang, Yun Li, Linjun Zhang, and Huaxiu Yao. Calibrated self-rewarding vision language models. arXiv preprint arXiv:2405.14622, 2024. 2, 5 15 Complex-Edit: CoT-Like Instruction Generation for Complexity-Controllable Image Editing Benchmark"
        },
        {
            "title": "Supplementary Material",
            "content": "8. Atomic Editing Operations Here we present the descriptions for all the 24 atomic operation types. These operation types are also fed to GPT-4o during Stage #1 (Sequence Generation) of our data pipeline, as discussed in Sec. 2.1. Object Manipulation and Transformation Add an Object: Insert new element into the image. Remove an Object: Eliminate an existing element from the image. Replace an Object: Swap one element with another. Move an Object: Change the position of an existing element within the image. Resize an Object: Adjust the size of an existing element. Rotate an Object: Rotate an element to specified angle. Duplicate an Object: Create copy of an existing element. Color and Tone Adjustments Change Color: Replace the color of an element with specified color. Apply Filter/Weather: Add color filter or weather effect to the entire image or specific parts. Texture and Material Adjustments Zoom In/Out: Adjust the zoom level to focus on specific elements or show broader view. Special Effects Add Special Effects: Introduce effects like glow, motion blur, or lens flare. Remove Special Effects: Eliminate existing special effects from the image. Add Particles: Insert particles like dust. Remove Particles: Remove existing particles from the image. 9. Prompts for Evaluation 9.1. Rubrics In the rubric for each metric we propose, we specify the detailed textual description for every score from 0 to 10, and most scores have textual example to reduce ambiguity. All the rubrics for evaluation in are listed in Fig. 15. 9.2. System Prompts The system prompts that we use for evaluation are provided in Fig. 16. 10. Additional Qualitative Results Change Texture: Apply texture to an element (e.g., Here we present more comprehensive qualitative results. change from metal to wood). Background and Environment Change Background: Replace the background with different scene or color. Lighting and Shadows Adjust Lighting: Change the overall lighting or lighting of specific elements. Text and Symbols Add Text: Insert text into the image. Remove Text: Eliminate existing text from the image. Change Text Properties: Modify font, color, size, or position of existing text. Pose and Expression Change Pose: Modify the stance or posture of person or object. Change Facial Expression: Alter the facial expression of character. Composition and Cropping Crop Image: Adjust the framing of the image by removing outer areas. Reframe Composition: Change the focus or arrangement of elements within the image. 10.1. Direct Editing Additional qualitative results of direct editing with more models are shown in Figs. 17 and 18, indicating that the gap between open-source and proprietary models expands with increasing complexity of the instruction. Also, Identity Preservation and Perceptual Quality continue to drop as instruction complexity increases, while the effect on Instruction Following varies from model to model. One can refer to the discussion in Sec. 5.2 for more detailed insights. 10.2. Sequential Editing We illustrate additional qualitative results of sequential editing with more models in Figs. 19 and 20, showing that all three aspects, particularly Identity Preservation and Perceptual Quality, consistently degrade, as visual artifacts and distortions accumulate with an increase in the number of intermediate steps. Even advanced proprietary models, i.e. Imagen3 and SeedEdit, demonstrate limitations in maintaining the identity of key elements and the overall quality of output images. More in-depth discussions are given in Sec. 5.4.1. 1 10.3. Best-of-N Additional qualitative results of sequential editing combined with Best-of-4 with more open-source models are shown in Fig. 21. It can been noticed that the Identity Preservation and Perceptual Quality of sequential editings results can be substantially improved with Best-of-4. more comprehensive analysis of this topic is provided in Sec. 5.4.2. The Identity Preservation and Perceptual Quality of sequential editing improve significantly with Best-of-4, as analyzed in detail in Sec. 5.4.2. 10.4. Direct Editing with GPT-4o We provide more qualitative results with GPT-4o via direct editing real-life images in Fig. 22. These results imply that while GPT-4o is capable of generating edited images of exceptional quality, output images with very complex instructions prune to lose the realistic feeling in the original images. This may imply lack of real-life images in GPT-4os training data. See Sec. 5.3 for more in-depth analysis. 2 Rubric for Instruction Following. * 10 (Perfect Instruction Following): All the required changes occur in the output image. * 9 (Near Perfect Instruction Following with negligible deviations): Almost all instructed changes are present but negligible deviations exist (e.g., tiny color variation such as the cat in the image is now black but the ears are grey). * 7-8 (Strong Instruction Following with minor deviations): Most required changes are applied accurately. Minor deviations exist but do not substantially alter the intended modification (e.g., car is changed to blue as instructed, but the reflection on its surface still contains red tint). * 5-6 (Moderate Instruction Following with noticeable deviations): The output reflects an attempt to follow instructions but with moderate errors (e.g., adding required element but with incorrect attributes like color or shape). * 3-4 (Weak Instruction Following with major deviations): Most required modifications are missing, incorrect, or only vaguely implemented. Significant elements from the instruction are misrepresented (e.g., when instructed to add hat, small, barely visible accessory is added to the head, but it does not resemble proper hat). * 1-2 (Minimal Instruction Following with severe deviations): vague attempt is made, but the required modifications are either incorrect or so minimal that they do not fulfill the instruction (e.g., the instruction asks to remove person from the image, but they are still visible, just slightly blurred or faded instead of being properly erased.). * 0 (Complete failed Instruction Following): The output image is entirely unrelated to the instruction. Rubric for Identity Preservation. (a) Instruction Following. * 10 (Perfect Identity Preservation): All key elements that should remain unchanged are completely preserved and indistinguishable from the input (e.g., persons face, expression, and proportions remain completely unchanged except for the required edits). * 9 (Near Perfect Identity Preservation with negligible distortion): Key elements that should remain unchanged are preserved with negligible distortion (e.g., persons face is identical except for tiny, imperceptible variation in hair texture). * 7-8 (Strong Identity Preservation with minor distortion): Small details of the key elements may have changed, but they do not significantly disrupt the overall identity (e.g., pets fur pattern remains mostly accurate, but minor detail like stripe or spot is different). * 5-6 (Moderate Identity Preservation with noticeable distortion): Most of the key elements remain recognizable but with noticeable distortions (e.g., the instruction asks to change cars color, but the cars shape or size is modified along with the color). * 3-4 (Weak Identity Preservation with major distortion): Key elements maintain general resemblance but noticeable changes are present (e.g., the instruction asks to brighten the sky, but additional buildings in the background appear or disappear). * 1-2 (Minimal Identity Preservation with severe distortion): Most key elements are significantly altered or replaced. The key elements in the output retain only minor aspects of the original, but major features are incorrect (e.g., persons face is still human face, but it no longer resembles the original person at all). * 0 (Complete failed Identity Preservation): All key elements that should remain unchanged are altered, distorted, or missing. Rubric for Perceptual Quality. (b) Identity Preservation. * 10 (Perfect Perceptual Quality): The image appears flawlessly natural, and all objects are seamlessly integrated into the environment with consistent lighting and shadows. There is no visual artifact at all. * 9 (Near Perfect Perceptual Quality with negligible incoherence): The image is very close to perfect, but tiny, almost imperceptible inconsistency exists. Seamless integration, but one might notice an extremely subtle flaw. (e.g., person added to group photo blends in perfectly, but upon close examination, their shadow is slightly softer than others.) * 7-8 (Strong Perceptual Quality with minor incoherence): Minor incoherence and artifacts are present but they do not significantly detract from the overall harmony. (e.g., sunset scene where the added reflections on water are slightly off in intensity, but the image still looks highly realistic.) * 5-6 (Moderate Perceptual Quality with noticeable incoherence): There is noticeable visual artifacts affecting the images harmony. Lighting and shadows may be misaligned or inconsistent. (e.g., an animal is distorted in size or shape, making it appear out of place in the scene.) * 3-4 (Weak Perceptual Quality with major incoherence): Disharmonious elements are prominent, greatly disturbing the visual harmony. (e.g., an animals shape or persons face is greatly distorted, only showing some resemblance of the animal species or human face.) * 1-2 (Minimal Perceptual Quality with severe incoherence): The whole scene is distorted, making it difficult to recognize the objects or subjects in the image. * 0 (Complete failed Perceptual Quality): The image is completely random and makes no sense at all. 3 Figure 15. Rubrics for evaluating Instruction Following, Identity Preservation and Perceptual Quality. (c) Perceptual Quality. Rubric for Instruction Following. You are required to evaluate the result of an instruction-based image editing model. Given an input image, an output image and text instruction, you are required to access the output image based on whether the changes made to the input image align with the text instruction. You are required to give two integer scores in [0, 10] based on the following criteria: 1. Instruction Following: whether the required changes occur in the output image, regardless of whether unnecessary changes are also made. 10 means that all the changes required by the instruction occur in the output image, 0 means that no changes required by the instruction occur in the output image. 2. Identity Preservation: whether elements that should not be changed stay the same in the output image, regardless of whether required changes occur. 10 means that no unnecessary changes occur in the output image, 0 means that all elements in the input image that should be kept the same are changed in the output image. Here is the detailed rubric for Instruction Following: <INSTRUCTION FOLLOWING RUBRIC> Here is the detailed rubric for Identity Preservation: <IDENTITY PRESERVATION RUBRIC> Note that these two scores should be graded independently, and low score for one criterion should not affect the score for the other criterion. For example, an output image that is identical to the input image should have an Instruction Following score of 0, but an Identity Preservation score of 10. Also, an output image that has no relevance with the input image should have an Identity Preservation score of 0 unless the instruction specifically orders the model to create whole different image, but it should not affect the Instruction Following score as long as changes required by the instruction occur in the output. If the instruction contains several atomic operations, evaluate the Instruction Following for each atomic operation separately and then average the scores as the assessment for Instruction Following. Explain your reasoning before answering the question. (a) Instruction Following and Identity Preservation. Rubric for Instruction Following. You are required to evaluate model-generated image. Given an output, you are required to access the output images Perceptual Quality. You are required to give one integer score in [0, 10] with 0 indicating extreme disharmony characterized by numerous conflicting or clashing elements, and 10 indicating perfect harmony with all components blending effortlessly. These are the criteria: 1. Consistency in lighting and shadows: The light source and corresponding shadows are consistent across various elements, with no discrepancies in direction or intensity. 2. Element cohesion: Every item in the image should logically fit within the scenes context, without appearing misplaced or extraneous. 3. artificially inserted or poorly integrated. 4. Aesthetic uniformity and visual flow: The image should not only be aesthetically pleasing but also facilitate natural visual journey, without abrupt interruptions caused by disharmonious elements. Integration and edge smoothness: Objects should blend seamlessly into their surroundings, with edges that do not appear Here is the detailed rubric for scoring: <PERCEPTUAL QUALITY RUBRIC> Figure 16. System prompt for evaluating Instruction Following, Identity Preservation and Perceptual Quality. (b) Perceptual Quality. 4 Figure 17. Additional qualitative results with direct editing on real-life images. As instruction complexity increases, the gap between opensource and proprietary models widens, and both Identity Preservation and Perceptual Quality decline, with variable effects on Instruction Following across models. See Sec. 5.2 for more details. 5 Figure 18. Additional qualitative results with direct editing on synthetic images. As instruction complexity increases, the gap between opensource and proprietary models widens, and both Identity Preservation and Perceptual Quality decline, with variable effects on Instruction Following across models. See Sec. 5.2 for more details. 6 Figure 19. Additional qualitative results with sequential editing on real-life images. Identity Preservation and Perceptual Quality, degrade as visual artifacts and distortions increase with more intermediate steps. Even advanced proprietary models, i.e. Imagen3 and SeedEdit, struggle to maintain element identity and quality in output images. More in-depth analysis is in Sec. 5.4.1. 7 Figure 20. Additional qualitative results with sequential editing on synthetic images. Identity Preservation and Perceptual Quality, degrade as visual artifacts and distortions increase with more intermediate steps. Even advanced proprietary models, i.e. Imagen3 and SeedEdit, struggle to maintain element identity and quality in output images. More in-depth analysis is in Sec. 5.4.1. 8 Figure 21. Qualitative results with sequential editing with Best-of-4 on real-life images. Identity Preservation and Perceptual Quality of sequential editing improve significantly with Best-of-4. Refer to Sec. 5.4.2 for more detailed discussions. 9 Figure 22. More real-life images edited with C8 instruction by GPT-4o. Outputs from GPT-4o severely lose the realistic style. Refer to Sec. 5.3 for detailed discussion."
        }
    ],
    "affiliations": [
        "Google",
        "University of California, Santa Cruz",
        "University of Edinburgh"
    ]
}