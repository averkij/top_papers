{
    "paper_title": "ReCode: Updating Code API Knowledge with Reinforcement Learning",
    "authors": [
        "Haoze Wu",
        "Yunzhi Yao",
        "Wenhao Yu",
        "Huajun Chen",
        "Ningyu Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) exhibit remarkable code generation capabilities but falter when adapting to frequent updates in external library APIs. This critical limitation, stemming from reliance on outdated API knowledge from their training data, even with access to current documentation, impedes reliable code generation in dynamic environments. To tackle this issue, we propose ReCode (rule-based Reinforcement learning for Code Update), a novel framework that mimics human programmer adaptation to API changes. Specifically, we construct a dataset of approximately 2,000 data entries to train the LLMs to perform version migration based on updated information. Then, we introduce a modified string similarity metric for code evaluation as the reward for reinforcement learning. Our experiments demonstrate that ReCode substantially boosts LLMs' code generation performance in dynamic API scenarios, especially on the unseen CodeUpdateArena task. Crucially, compared to supervised fine-tuning, ReCode has less impact on LLMs' general code generation abilities. We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and DAPO), all achieving consistent improvements. Notably, after training, Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned model and the reasoning model with the same architecture. Code is available at https://github.com/zjunlp/ReCode."
        },
        {
            "title": "Start",
            "content": "ReCode: Updating Code API Knowledge with Reinforcement Learning Haoze Wu1, Yunzhi Yao1, Wenhao Yu2*, Huajun Chen1, Ningyu Zhang1 1Zhejiang University 2Tencent AI, Seattle Lab wuhz1020@gmail.com, yyztodd@zju.edu.cn, wenhaoyu97@gmail.com, huajunsir@zju.edu.cn, zhangningyu@zju.edu.cn 5 2 0 2 5 2 ] . [ 1 5 9 4 0 2 . 6 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) exhibit remarkable code generation capabilities but falter when adapting to frequent updates in external library APIs. This critical limitation, stemming from reliance on outdated API knowledge from their training data, even with access to current documentation, impedes reliable code generation in dynamic environments. To tackle this issue, we propose ReCode (rulebased Reinforcement learning for Code Update), novel framework that mimics human programmer adaptation to API changes. Specifically, we construct dataset of approximately 2,000 data entries to train the LLMs to perform version migration based on updated information. Then, we introduce modified string similarity metric for code evaluation as the reward for reinforcement learning. Our experiments demonstrate that ReCode substantially boosts LLMs code generation performance in dynamic API scenarios, especially on the unseen CodeUpdateArena task. Crucially, compared to supervised fine-tuning, ReCode has less impact on LLMs general code generation abilities. We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and DAPO), all achieving consistent improvements. Notably, after training, Qwen2.5-Coder7B outperforms that of the 32B parameter code instruction-tuned model and the reasoning model with the same architecture1."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have recently demonstrated remarkable code generation abilities (Chen et al., 2021b; Zan et al., 2023; Rozière et al., 2024; Guo et al., 2024; Hui et al., 2024; Team et al., 2024; OpenAI et al., 2024; Jiang et al., 2024; Team et al., 2025a; Yang et al., 2025). This capacity enables LLMs to solve data science-related tasks by *Corresponding authors 1https://github.com/zjunlp/ReCode Figure 1: Top: LLMs cannot be aware of API updates that occur after their release date, which may lead to code errors. Bottom: Simply incorporating update information into the prompt cannot effectively alleviate the issue of outdated APIs. ReCode enhances their ability to migrate code to new versions through rule-based RFT. calling external libraries in the generated code (Lai et al., 2022; Wang et al., 2025d; Hong et al., 2025). However, the APIs of external libraries are updated very frequently, while the model still has outdated information in their parameters (Wu et al., 2024; Islah et al., 2024; Wang et al., 2025a; Yao et al., 2023; Liu et al., 2025). As shown in Figure 1-top, the LLM released before would generate code containing outdated APIs, leading to task failure in the upto-date environment. This poses challenge for the application of LLMs in scenarios that require highquality code generation, such as AI4Research (Lu et al., 2024; Starace et al., 2025), Software Engineering (Zhang et al., 2023), and Human-Computer Interaction (Zhang et al., 2024). The root cause of these challenges lies in the fact that LLMs are trained on static datasets, making it difficult for them to adapt to dynamically evolving scenarios such as API updates. While supervised fine-tuning (SFT) can partially alleviate this issue (Liu et al., 2025), the high frequency of updates Figure 2: Training Reward and Test Pass Rate during RL Fine-Tuning. It demonstrates that Qwen2.5-Coder-7BInstruct can enhance performance on the unseen CodeUpdateArena, even surpassing 32B code model and reasoning model after training. The two dashed lines in the figure represent the Pass@1 of the corresponding models on CodeUpdateArena. may result in exorbitant costs and catastrophic forgetting of previously learned knowledge (Biderman et al., 2024; Luo et al., 2025). An alternative approach involves embedding updated information directly into the prompt as dynamic knowledge supplement (Liu et al., 2025; Wang et al., 2025a). This method can be further enhanced by integrating it with retrieval-augmented generation (RAG), offering greater potential in handling dynamic scenarios (Lewis et al., 2021; Gao et al., 2024; Gupta et al., 2024). Furthermore, inputting updated knowledge through prompts avoids the risk of erasing other unrelated knowledge. Nevertheless, significant room for improvement remains in version-related code evaluation benchmarks, particularly for open-source code models (Liu et al., 2025). This may be attributed to the conflict between inherent (parameters) and external (prompt) knowledge, where LLMs tend to prioritize their internal knowledge. In summary, while prompting is better suited for this scenario than SFT, it is necessary to address the challenge of knowledge conflicts. Reinforcement Fine-Tuning (RFT) has been shown to enhance the models ability to integrate retrieval in RAG systems, boosting response accuracy (Jin et al., 2025). Inspired by this, we design ReCode, rule-based Reinforcement learning for Code Update approach (Kaelbling et al., 1996; Ghasemi et al., 2025) to enhance the models code migration capabilities when encountering conflicting information. To take data science as an example, programmers first learn specific version of the library, such as NumPy. Then, after being informed of the API updates, they can map the old version of the code in their minds to the new version. Our goal is to enable LLMs to use new APIs to complete tasks based on the updated information provided in the prompt. Similar to the VersionAware Code Migration task in Versicode (Wu et al., 2024), we train the model to migrate code from an old version to new version based on the update information. As shown in Figure 1-bottom, we put the updated document in the prompt and fine-tune the model to better understand the prompt by using reinforcement learning based on the improved string similarity reward for code evaluation. We evaluate the models performance on CodeUpdateArena (Liu et al., 2025), more challenging task, where the model must solve practical tasks using API update information. Our experimental results indicate that ReCode significantly enhances LLMs code generation capabilities in dynamic API scenarios. As shown in Figure 2, the trained Qwen2.5-Coder-7B model outperforms Qwen2.5-Coder-32B and achieves higher Pass@1 score than DeepSeek-R1-DistillQwen-32B. Additionally, ReCodes impact on the models general code generation abilities is less pronounced than that of supervised fine-tuning (SFT). In summary, our contributions are as follows: ReCode is the first to explore the application of rule-based RFT in dynamic API scenarios. Particularly, we combine the use of prompts with RFT. We construct training dataset comprising approximately 2,000 data entries, specifically designed to train models in performing version migration based on updated information. Through extensive experiments and analyses, we highlight the potential of ReCode in code generation and knowledge update scenarios."
        },
        {
            "title": "2 API Knowledge Update",
            "content": "Problem formula. During the pre-training phase, the LLM accumulates code knowledge within its parameter θ via an autoregressive approach (Yenduri et al., 2023). However, θ remains static postpre-training and thus fails to incorporate subsequent API updates. Consequently, when tasked with question q, the model may produce code containing outdated APIs: code with outdated API Pθ(q). One basic strategy is to embed updated API information within the prompt cupdate. The model then generates code based on both the question and the embedded update details: code Pθ(q, c). Nevertheless, this method introduces potential conflicts between the models internal knowledge (housed in θ) and external information (provided in the prompt c). Such conflicts can lead to the model overlooking the updated API details in c, even when additional updates are supplied. Unlike directly writing the updated knowledge into the models parameter θ, we employ reinforcement learning to fine-tune θ θ, enabling the model to more effectively leverage the updated API information presented in the prompt. This refined approach allows the model to generate code that better aligns with current API standards: update code Pθ(q, c). CodeUpdateArena. CodeUpdateArena (Liu et al., 2025) is designed to evaluate the ability of LLMs to handle API updates. The dataset comprises 670 program synthesis tasks, covering updates to 54 functions across seven different Python packages. Unlike other datasets, CodeUpdateArena is synthetic dataset generated with LLM assistance. It prevents overlap with the models training data and allows for the assessment of LLMs adaptability to completely new updates. In addition, each entry in the dataset includes at least three test cases that can be directly executed for verification."
        },
        {
            "title": "3 ReCode",
            "content": "3.1 Overview As we described in Section 1, providing API update information to LLMs via prompts is the most promising solution, as it is more aligned with the behavior of human programmers and can be seamlessly integrated with RAG. However, current LLMs usually miss the information provided in the instructions. Our goal is to enhance the models ability to follow the updated information provided in the prompt. , c(target) Training. For training, we train models to perform version migration based on updated information. As shown in Figure 3-right, given data entry ei = [di, vi, ui, c(old) ], which cori responds to [Dependency, Target Version, Update Info, Old Code, and Target Code], the input of training task is xi = [di, vi, ui, c(old) ] and the output is = c(target) . The actual prompts with templates we used are provided in the Appendix C. We provide detailed discussion of the training dataset construction in Section 3.2. Moreover, we detail the rewards utilized in the training process in Section 3.3. Testing. During testing, each piece of data in CodeUpdateArena (Liu et al., 2025) includes: Dependency and API update doc ui, real-world question qi, and function signature to be implemented si. The test task requires the model to generate the complete function code for si based on the above three pieces of information. Since the dataset provides test cases, we use Pass@k as the evaluation metric. The prompts with templates are also provided in the Appendix C."
        },
        {
            "title": "3.2 Training Dataset Construction",
            "content": "Since no existing dataset provides input-output pairs, we construct our own training dataset. As shown in Figure 3-left, the data construction process is as follows: 1. We access the release notes of major data science libraries (e.g., Numpy, Pandas, PyTorch, matplotlib) to identify paragraphs that detail specific API updates. 2. We leverage GPT-4 to generate two code snippets with equivalent functionality: one using the old API and the other utilizing the updated API. Figure 3: The pipeline of data collection and training task with running example. 3. Human experts review these code snippets to ensure they incorporate the updated API correctly. More details can be seen in Appendix B. 4. Only those code snippets that pass the expert review are included in the dataset. training objective is to migrate correctly generated code to new version, with the focus on migration rather than the inherent correctness of the code itself. Following previous works (Wu et al., 2024; Wang et al., 2025a), we use string matching metrics to evaluate the codes cross-version migration capability: The final dataset comprises approximately 2K entries, with detailed statistics provided in Appendix B. It is worth noting that the API updates in our dataset encompass diverse range of changes, including but not limited to API renaming, parameter addition, and functionality modification. detailed discussion of these API update types is also included in Appendix B."
        },
        {
            "title": "3.3 Reward Design",
            "content": "Similar to the existing works (DeepSeek-AI, 2025; Xie et al., 2025), the reward in ReCode consists of two parts: format and correctness. Format Reward. We hope that the output of our model meets the format: <think>...</think><answer>...</answer>. That is, to output the thinking process within the <think> tag, and to output the target code within the <answer> tag. The format reward is defined as follows: Rf ormat(x) = (cid:26)+1, 1, if meets the format else (1) where is the output of the model. Correctness Reward. Code, unlike math problem (Hendrycks et al., 2021; Liu et al., 2024a; Chernyshev et al., 2025), does not have verifiable, unique standard answer. When improving coding skills, code quality can be verified by the pass rate of test cases (Liu and Zhang, 2025). However, we argue that the pass rate of test cases is not suitable reward metric for code migration task. Our Edit Similarity (ES): ES assesses the similarity between predicted completions and target codes by analyzing the edit operations needed to transform one into the other. Exact Match (EM): EM calculates the rate at which predicted completions exactly match the target codes after normalizing return values. Versicode (Wu et al., 2024) introduced the Critical Diff Check (CDC) metric by adding rules based on EM. However, since it only considered API name updates, CDC is not applicable to diverse API updates. Drawing on Logic-RLs (Xie et al., 2025) reward mechanisms, we incorporated code syntax checking into the string matching metrics, as illustrated in Figure 4. Taking EM as an example: EM (x) = +2.0, 1.5, 2.0, if match the target elif is syntactically valid else (2) Similarly, ES is: ES(x) = (cid:26)2.0, if isnt syntactically valid ES(x) 3.5 1.5, else (3) The reward range when the syntax is correct is also [1.5, 2.0]. Our reward is applicable to any reinforcement learning algorithm that uses policy gradient updates. In our experiments, we select GRPO (Shao et al., 2024) and its modified version, DAPO (Yu et al., 2025), as the training algorithms. Figure 4: Correctness Reward and Training Pipeline (taking GRPO as an example). The dashed box shows the correctness reward of our design, which includes two parts: syntax checking and string matching. It is worth mentioning that ReCode can be adapted to any reinforcement learning algorithm and is not limited to GRPO."
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Experimental Setup Model. In our experiments, we utilize two code models to evaluate our method: Qwen-2.5-Coder7B-Instruct (Hui et al., 2024) and DeepSeekv1.5-Coder-7B-Instruct (Guo et al., 2024). Both models demonstrate superior performance among models with fewer than 10B parameters. Notably, instruction-tuned models are found to retain R1 characteristics (Xie et al., 2025). Additionally, in the experimental section, we employ Qwen2.5-Coder-Instruct-32B (Hui et al., 2024) and DeepSeek-R1-Distill-Qwen-32B (DeepSeekAI, 2025) as baseline models. The performance metrics for these two models are obtained via the API of SiliconFlow2. Training. In our experiment, we utilize the DoRA algorithm (Liu et al., 2024b) to update the model due to the computational limits. The hyperparameters are configured as = 64, α = 64. For the RL component, both GRPO and DAPO are set with = 8, while in GRPO, the parameter β is assigned value of 0.001. The training procedure spans 5000 steps, targeting the training tasks with batch size set at 8 and learning rate of 5 105. Specifically, the initial 150 training steps incorporate learning rate warm-up schedule, which is subsequently followed by cosine schedule. Test Dataset and Metric. We select the CodeUpdateArena benchmark (Liu et al., 2025) as our test task and filter inaccurate cases from this public dataset3. detailed account of these errors is provided in Appendix D. We evaluate our corrected data using the Pass@k metric, with values of 1 and 5 in our experiments. All tests are conducted using the inputs with updated information as de2https://www.siliconflow.cn/ 3https://github.com/leo-liuzy/CodeUpdateArena/ blob/main/data/arena-ungrouped.jsonl scribed in Section 3.1. To examine how additional post-training affects the models general code capabilities, we employed HumanEval+ (Chen et al., 2021a; Liu et al., 2023). 4.2 Main Result Table 1 presents the results obtained from CodeUpdateArena after training. ReCode enhances the models pass rate within the arena. We conduct an ablation study on the rewards in Section 4.3 and select ES* as our training reward. As indicated in Table 1, both GRPO and DAPO contribute to improving the models performance in the arena. Specifically, regarding the Qwen2.5-Coder-7B-Instruct model, its final Pass@1 exceeds that of the 32B parameter code instruction-tuned model as well as the distilled reasoning model with an identical architecture. Although the Pass@5 is marginally lower than that of DeepSeek-R1-Distill-Qwen-32B, this aligns with prior observations that the reasoning enhancement derived from RL is constrained by the base model (Yue et al., 2025). Conversely, it is observed that the benefits accrued by Qwen2.5Coder from ReCode are substantially greater than those of DeepSeekCoder-v1.5. This discrepancy may be intrinsically linked to the extensive adoption of Qwen2.5 by the community for reproducing R1 work. SFT exhibits limited generalization capabilities when transitioning from the code migration task to real-world code generation tasks. As evidenced in Table 1, the performance boost provided by SFT is inferior to that of ReCode (Qwen), and in some cases, it may even diminish the pre-trained models performance (DS). The performance drop on DS is consistent with Liu et al., 2025s observations. To comprehensively highlight ReCodes practical utility in dynamic API settings, we explicitly differentiate between training and testing tasks. Model Method CodeUpdateArena Pass@ 1() Pass@ 5() HumanEval+() Qwen2.5-Coder-32B-Instruct DeepSeek-R1-Distill-Qwen-32B Untrained Untrained DS-v1.5-Coder-7B-Instruct Qwen2.5-Coder-7B-Instruct Untrained SFT ReCode GRPO ReCode DAPO Untrained SFT ReCode GRPO ReCode DAPO 75.7 (+0.0) 78.2 (+0.0) 59.1 (+0.0) 53.4 (-5.7) 63.6 (+4.5) 63.6 (+4.5) 67.3 (+0.0) 69.4 (+2.1) 74.6 (+7.4) 78.7 (+11.3) 84.3 (+0.0) 86.1 (+0.0) 72.5 (+0.0) 67.3 (-5.3) 77.3 (+4.7) 78.2 (+5.6) 74.0 (+0.0) 78.2 (+4.1) 82.1 (+8.0) 84.3 (+10.2) - - 71.3 (+0.0) 64.0 (-7.3) 67.7 (-3.6) 68.9 (-2.4) 84.1 (+0.0) 70.2 (-11.7) 82.3 (-1.8) 81.7 (-2.4) Table 1: The performance results using the GRPO and DAPO algorithms on CodeUpdateArena and HumanEval+. This approach is also adopted in other studies, such as Logic-RL (Xie et al., 2025), which trains on logic puzzles and evaluates on math competition problems. However, SFT frequently falls short in this regard. ReCode has less impact on the general capabilities of LLMs than SFT. We assess the models performance on HumanEval+ (Liu et al., 2023), benchmark designed to evaluate the models general code generation abilities, after training. Table 1 compares the pre-training and post-training test results, revealing that ReCodes impact on general code generation capabilities is significantly less pronounced than that of SFT. In conclusion, we maintain that ReCode represents the most viable and promising solution for dynamic API scenarios."
        },
        {
            "title": "4.3 Reward Design Ablation",
            "content": "In Section 3.3, we outline the design space for the correctness reward, which encompasses EM, ES, EM*, and ES* metrics. Using Qwen2.5-Coder in conjunction with GRPO (Shao et al., 2024), we conduct comparative analysis to evaluate how these different reward mechanisms influence the final performance. format +EM +ES +EM* +ES* Pass@1() Pass@5() -2.3 -3. -1.2 -3.2 +5.4 +5.2 +1.1 +2.0 +7.4 +8.0 Table 2: Ablation study on reward design using Qwen2.5-Coder and GRPO, comparing the impact of format reward, exact match (EM), edit similarity (ES), and their syntax-checked variants (EM*, ES*) on final performance metrics (Pass@1 and Pass@5). Table 2 illustrates the variations in the final metrics when training with different rewards. Below are three key findings: Using only the format reward does not lead to improvement. Without correctness rewards, the policy model cannot receive feedback on whether the code migration is correct. Moreover, instruction-tuned models can follow instructions well without additional training; further training using only format rewards would be redundant. This underscores the necessity of introducing correctness rewards in the API update scenario. The inclusion of syntax checking is necessary. It is evident that regardless of whether using the strict matching metric EM or the loose similarity measure ES, the inclusion of syntax checking enhances performance. We speculate that pure string matching may result in the models understanding of the task degrading. Additionally, Versicode (Wu et al., 2024) also observed that when the target code length increases, the correlation coefficient between EM and the pass rate of test cases decreases. Similarity measurement is better than strict matching. We find that although ES and EM ultimately share the same goal of aligning the output with the target code, ES and ES* achieve superior results. This may be related to the calculation of the inter-group advantage in naive GRPO, as shown in Equation 5. When none of the outputs in group strictly match the target code, each output receives the same reward, leading to zero advantage for each output. DAPO (Yu et al., 2025) also highlighted this issue. In contrast, ES offers more flexible values. Even if none of the outputs in group strictly match the target code, different outputs can still receive varying rewards. To verify this, we calculate the proportion of data with inter-group reward variance being 0 before training started. To better highlight the differences, we discard the data with an initial reward of 2. As shown in Figure 5-left, ES* has lower proportion than EM*. This means the model can get reward feedback on more data it couldnt handle before (reward < 2) when using GRPO. Unless otherwise specified, we employ ES* as the correctness reward in all other experiments."
        },
        {
            "title": "5 Analysis",
            "content": "If not specifically emphasized otherwise, the analyses are based on the best-performing model, Qwen2.5-Coder-7B-Instruct w/ DAPO ReCode. 5.1 Training Dynamics Reward and Pass Rate. As depicted in Figure 2, during the initial few hundred training steps, the reward undergoes decline rather than an increase. This may appear counterintuitive, but our analysis reveals that the primary driver of this earlystage reduction is the decrease in format reward. Instruction-tuned models inherently possess robust instruction-following abilities, enabling them to readily comply with reasoning formats without undergoing training. Nevertheless, this very strength in adhering to instructions might concurrently restrict the exploration space of reinforcement learning algorithms, potentially entrenching specific modes of thinking. We posit that the initial reward drop reflects the models reluctant shift as it commences exploration. It discards its pre-existing instruction-following capabilities and established thought processes to embark on new learning phase. The subsequent reward increase is attributed to the model having sufficiently expanded its exploration space, which in turn enables the gradual enhancement of both reward and performance metrics. However, in our experimental observations, even with this initial reward decline, instructiontuned models demonstrated faster learning pace compared to their base model counterparts. Response Length. As shown in Figure 5-right, the models output length tends to increase during training. However, we do not believe there is direct relationship between the models reasoning ability and response length. In our training tasks, the length of the models Chain of Thought (CoT) output is inherently limited (around 300), and even after training, the models output length only increases to approximately 400. This may be due to the limited thinking required for the migration task, with the model not engaging in excessive thinking Figure 5: Left: the proportion of data with inter-group reward variance being 0. Right: The changes in response length during the training process. throughout the process. 5.2 Case Study: the impact of RL? natural question arises: Why does ReCode result in improvements? To explore this question, we divide the error cases before training into two categories. One category includes cases that are corrected after traing, and the other includes those that remain wrong. ReCode can help LLMs overcome their laziness in acquiring external knowledge. We observe that the pretrained model exhibits \"laziness\" regarding the updates in the prompt. Specifically, it either ignores the mentioned updated APIs or overlooks the new API parameters. Appendix E.1 shows an erroneous case where the model disregards the updated API math.sin and misinterprets the angle unit \"gradian,\" producing incorrect code. Hypothesis. When generating code, an LLM draws from two information sources: internal knowledge (parameters) and provided prompts. Similar to humans, the model tends to rely more on its internal knowledge. This propensity limits improvements when using prompts as the sole information source in dynamic scenarios. This is also one of the challenges faced by RAG systems. Longpre et al. 2022 found that when encountering conflicting information between context and internal knowledge, QA models tend to rely on internal knowledge. Xu et al. 2024 emphasized that knowledge obsolescence can lead to conflict between parameters and prompts. Fortunately, ReCode corrects these issues, as demonstrated in Appendix E.2, where the model accurately uses the updated API information. ReCode enables the LLM to overcome its tendency to be lazy in utilizing external knowledge. The capacity to address problems is fundamentally constrained by the pre-trained model. Most uncorrected cases are unrelated to API updates. Appendix E.3 shows case where the model is supposed to develop pendulum simulation program. It ignores the physical quantities in the parameters and fails to describe the physical laws as required in the prompt. Unsurprisingly, even after training, the model still couldnt write this function correctly under the condition of Pass@1. Our method doesnt focus on enhancing physical reasoning abilities. Thus, Qwen2.5-Coder-7BInstruct, which originally lacked this ability, still couldnt complete the task after training. 5.3 Evaluation under Complex Scenarios Since our training and testing tasks only involve single API update at time, we explore two more complex scenarios: Multiple updates to single API. torch.gels() was deprecated in version 1.2 in favor of torch.lstsq(). Later, torch.lstsq() was also deprecated in version 1.9, replaced by torch.linalg.lstsq(). To update code from torch.gels to torch.linalg.lstsq, one could do it in two steps following the version sequence. However, as shown in Appendix F.1, with update-aware prompting, the model can directly migrate from torch.gels to torch.linalg.lstsq without going through torch.lstsq, which is more align with human programmers behavior. Model Pass@1 Pass@5 DeepSeek-R1-Distill-Qwen-32B Qwen2.5-Coder-7B-Instruct +ReCode 65 35 60 80 55 75 Table 3: The results tested on 20 questions involving multiple API updates. It demonstrates that ReCode enhances the performance of Qwen2.5-Coder-7B-Instruct in handling the questions with multiple API updates. Multiple API Updates. Can the model handle updates to multiple APIs at the same time? Appendix F.2 provides an example where the model correctly generated code when facing multiple API updates. It also doesnt show laziness in using the knowledge from the prompt. To demonstrate the improvement achieved by ReCode, we construct 20 similar questions involving multiple API updates to test models performance. The results can be seen in Table 3, ReCode enhances the performance of Qwen2.5-Coder-7B, bringing 7B model closer to the performance of 32B reasoning model."
        },
        {
            "title": "6 Related Work",
            "content": "6.1 Benchmarking Code Generation Most current code completion benchmarks (Chen et al., 2021a; Austin et al., 2021; Cassano et al., 2023; Nijkamp et al., 2023; Zheng et al., 2024; Zhuo et al., 2025) fail to test models cross-version code migration skills. To address this issue, several version-related code completion benchmarks have been proposed recently. Versicode (Wu et al., 2024) includes over 9,000 code samples from websites but only covers API name changes and lacks executable test cases. In contrast, GitChameleon (Islah et al., 2024) collects over 100 real-world update samples to evaluate whether models can generate correct code based on library versions. CodeUpdateArena (Liu et al., 2025) is dataset of synthetic data with 670 samples generated by LLMs that includes executable test cases."
        },
        {
            "title": "6.2 Reinforcement Fine-Tuning",
            "content": "Reinforcement Learning (RL) remains crucial for LLM post-training, with RLHF being key to GPTs evolution into ChatGPT (Ouyang et al., 2022). With the emergence of slow-thinking reasoning models (OpenAI, 2024; DeepSeek-AI, 2025; Team et al., 2025b), the community has recognized the importance of rule-based RL. We provide more comprehensive context in Appendix A."
        },
        {
            "title": "7 Discussion and Conclusion",
            "content": "To the best of our knowledge, ReCode is the first to adopt the rule-based RFT post-training method for dynamic API scenarios. By training models on the code migration task, weve addressed the issue of AI programmers struggling to migrate code based on updates without test data. ReCode helps models make correct choices when encountering knowledge conflicts. AI programmers are tireless and efficient but lack dynamic adaptability to new knowledge, reasoning ability once thought unique to humans. How to endow AI with this ability remains significant challenge. ReCode represents an exploratory step in code generation. Through ReCode, Qwen2.5-Coder-7BInstruct has achieved cross-scale performance, surpassing the strong reasoning model with 32B parameters in the Pass@1 metric. In the future, we aim to explore this paradigms application in other knowledge update scenarios and further enhance reward design and training algorithms."
        },
        {
            "title": "Limitations",
            "content": "Due to experimental constraints, we only perform efficient fine-tuning (DoRA) on the 7B models and dont explore models with more parameters. Furthermore, our training and testing are limited to Python code in the data science field. In the future, we hope to explore whether the ReCode posttraining paradigm can be applied to more dynamic knowledge scenarios."
        },
        {
            "title": "References",
            "content": "Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. 2021. Program synthesis with large language models. Preprint, arXiv:2108.07732. Dan Biderman, Jacob Portes, Jose Javier Gonzalez Ortiz, Mansheej Paul, Philip Greengard, Connor Jennings, Daniel King, Sam Havens, Vitaliy Chiley, Jonathan Frankle, Cody Blakeney, and John P. Cunningham. 2024. Lora learns less and forgets less. Preprint, arXiv:2405.09673. Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Feldman, Arjun Guha, Michael Greenberg, and Abhinav Jangda. 2023. Multipl-e: scalable and polyglot approach to benchmarking neural code generation. IEEE Transactions on Software Engineering, 49(7):36753691. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, and 39 others. 2021a. Evaluating large language models trained on code. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, and 1 others. 2021b. Evaluating large language models trained on code. Preprint, arXiv:2107.03374. Konstantin Chernyshev, Vitaliy Polshkov, Ekaterina Artemova, Alex Myasnikov, Vlad Stepanov, Alexei Miasnikov, and Sergei Tilga. 2025. U-MATH: university-level benchmark for evaluating mathematical skills in LLMs. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V. Le, Sergey Levine, and Yi Ma. 2025. Sft memorizes, rl generalizes: comparative study of foundation model post-training. Preprint, arXiv:2501.17161. DeepSeek-AI. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Preprint, arXiv:2501.12948. Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, and Xiangyu Yue. 2025a. Video-r1: Reinforcing video reasoning in mllms. Preprint, arXiv:2503.21776. Zhaopeng Feng, Shaosheng Cao, Jiahan Ren, Jiayuan Su, Ruizhe Chen, Yan Zhang, Zhe Xu, Yao Hu, Jian Wu, and Zuozhu Liu. 2025b. Mt-r1-zero: Advancing llm-based machine translation via r1-zero-like reinforcement learning. Preprint, arXiv:2504.10160. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2024. Retrieval-augmented generation for large language models: survey. Preprint, arXiv:2312.10997. Majid Ghasemi, Amir Hossein Moosavi, and Dariush Ebrahimi. 2025. comprehensive survey of reinforcement learning: From algorithms to practical challenges. Preprint, arXiv:2411.18892. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y.K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. 2024. Deepseek-coder: When the large language model meets programming the rise of code intelligence. Shailja Gupta, Rajesh Ranjan, and Surya Narayan Singh. 2024. comprehensive survey of retrievalaugmented generation (rag): curPreprint, rent landscape and future directions. arXiv:2410.12837. Evolution, Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. Preprint, arXiv:2103.03874. Sirui Hong, Yizhang Lin, Bang Liu, Bangbang Liu, Binhao Wu, Ceyao Zhang, Chenxing Wei, Danyang Li, Jiaqi Chen, Jiayi Zhang, Jinlin Wang, Li Zhang, Lingyao Zhang, Min Yang, Mingchen Zhuge, Taicheng Guo, Tuo Zhou, Wei Tao, Xiangru Tang, and 8 others. 2025. Data interpreter: An LLM agent for data science. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, and 1 others. 2024. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186. Nizar Islah, Justine Gehring, Diganta Misra, Eilif Muller, Irina Rish, Terry Yue Zhuo, and Massimo Caccia. 2024. Gitchameleon: Unmasking the version-switching capabilities of code generation models. Preprint, arXiv:2411.05830. Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. 2024. survey on large language models for code generation. Preprint, arXiv:2406.00515. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. 2025. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. Preprint, arXiv:2503.09516. L. P. Kaelbling, M. L. Littman, and A. W. Moore. 1996. Reinforcement learning: survey. Preprint, arXiv:cs/9605103. Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. 2025. An empirical study of catastrophic forgetting in large language models during continual fine-tuning. Preprint, arXiv:2308.08747. Peixian Ma, Xialie Zhuang, Chengjin Xu, Xuhui Jiang, Ran Chen, and Jian Guo. 2025. Sql-r1: Training natural language to sql reasoning model by reinforcement learning. Preprint, arXiv:2504.08600. Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Scott Wen tau Yih, Daniel Fried, Sida Wang, and Tao Yu. 2022. Ds-1000: natural and reliable benchmark for data science code generation. Preprint, arXiv:2211.11501. Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2023. Codegen: An open large language model for code with multi-turn program synthesis. Preprint, arXiv:2203.13474. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2021. Retrieval-augmented generation for knowledgeintensive nlp tasks. Preprint, arXiv:2005.11401. OpenAI. 2024. Introducing openai o1. https:// openai.com/o1/. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, and 1 others. 2024. Gpt-4 technical report. Preprint, arXiv:2303.08774. Xuefeng Li, Haoyang Zou, and Pengfei Liu. 2025. Preprint, Scaling tool-integrated rl. Torl: arXiv:2503.23383. Hongwei Liu, Zilong Zheng, Yuxuan Qiao, Haodong Duan, Zhiwei Fei, Fengzhe Zhou, Wenwei Zhang, Songyang Zhang, Dahua Lin, and Kai Chen. 2024a. Mathbench: Evaluating the theory and application proficiency of llms with hierarchical mathematics benchmark. Preprint, arXiv:2405.12209. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and LINGMING ZHANG. 2023. Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation. In Thirty-seventh Conference on Neural Information Processing Systems. Jiawei Liu and Lingming Zhang. 2025. Code-r1: Reproducing r1 for code with reliable rewards. Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Dora: Cheng, and Min-Hung Chen. 2024b. Weight-decomposed low-rank adaptation. Preprint, arXiv:2402.09353. Zeyu Leo Liu, Shrey Pandit, Xi Ye, Eunsol Choi, and Greg Durrett. 2025. Codeupdatearena: Benchmarking knowledge editing on api updates. Preprint, arXiv:2407.06249. Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. 2022. Entity-based knowledge conflicts in question answering. Preprint, arXiv:2109.05052. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. 2024. The AI Scientist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. Preprint, arXiv:2203.02155. Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tür, Gokhan Tur, and Heng Ji. 2025. Toolrl: Reward is all tool learning needs. Preprint, arXiv:2504.13958. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. 2024. Direct preference optimization: Your language model is secretly reward model. Preprint, arXiv:2305.18290. Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, and 7 others. 2024. Code llama: Open foundation models for code. Preprint, arXiv:2308.12950. John Schulman, Filip Wolski, Prafulla Dhariwal, ProxPreprint, Alec Radford, and Oleg Klimov. 2017. imal policy optimization algorithms. arXiv:1707.06347. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Preprint, arXiv:2402.03300. Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, Johannes Heidecke, Amelia Glaese, and Tejal Patwardhan. 2025. Paperbench: Evaluating ais ability to replicate ai research. Preprint, arXiv:2504.01848. CodeGemma Team, Heri Zhao, Jeffrey Hui, Joshua Howland, Nam Nguyen, Siqi Zuo, Andrea Hu, Christopher A. Choquette-Choo, Jingyue Shen, Joe Kelley, Kshitij Bansal, Luke Vilnis, Mateo Wirth, Paul Michel, Peter Choy, Pratik Joshi, Ravin Kumar, Sarmad Hashmi, Shubham Agrawal, and 8 others. 2024. Codegemma: Open code models based on gemma. Preprint, arXiv:2406.11409. Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy Lillicrap, Angeliki Lazaridou, and 24 others. 2025a. Gemini: family of highly capable multimodal models. Preprint, arXiv:2312.11805. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, and 1 others. 2025b. Kimi k1.5: Scaling reinforcement learning with llms. Preprint, arXiv:2501.12599. Chong Wang, Kaifeng Huang, Jian Zhang, Yebo Feng, Lyuye Zhang, Yang Liu, and Xin Peng. 2025a. Llms meet library evolution: Evaluating deprecated api usage in llm-based code completion. Preprint, arXiv:2406.09834. Hongru Wang, Cheng Qian, Wanjun Zhong, Xiusi Chen, Jiahao Qiu, Shijue Huang, Bowen Jin, Mengdi Wang, Kam-Fai Wong, and Heng Ji. 2025b. Otc: Optimal tool calls via reinforcement learning. Preprint, arXiv:2504.14870. Junke Wang, Zhi Tian, Xun Wang, Xinyu Zhang, Weilin Huang, Zuxuan Wu, and Yu-Gang Jiang. 2025c. Simplear: Pushing the frontier of autoregressive visual generation through pretraining, sft, and rl. Preprint, arXiv:2504.11455. Zifeng Wang, Benjamin Danek, Ziwei Yang, Zheng Chen, and Jimeng Sun. 2025d. Can large language models replace data scientists in biomedical research? Preprint, arXiv:2410.21591. Tongtong Wu, Weigang Wu, Xingyu Wang, Kang Xu, Suyu Ma, Bo Jiang, Ping Yang, Zhenchang Xing, Yuan-Fang Li, and Gholamreza Haffari. 2024. Versicode: Towards version-controllable code generation. Preprint, arXiv:2406.07411. Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. 2025. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. Preprint, arXiv:2502.14768. Rongwu Xu, Zehan Qi, Zhijiang Guo, Cunxiang Wang, Hongru Wang, Yue Zhang, and Wei Xu. 2024. Knowledge conflicts for llms: survey. Preprint, arXiv:2403.08319. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, and 41 others. 2025. Qwen3 technical report. arXiv preprint arXiv:2505.09388. Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu Zhang. 2023. Editing large language models: Problems, methods, and opportunities. arXiv preprint arXiv:2305.13172. Gokul Yenduri, Ramalingam M, Chemmalar Selvi G, Supriya Y, Gautam Srivastava, Praveen Kumar Reddy Maddikunta, Deepti Raj G, Rutvij Jhaveri, Prabadevi B, Weizheng Wang, Athanasios V. Vasilakos, and Thippa Reddy Gadekallu. 2023. Generative pre-trained transformer: comprehensive review on enabling technologies, potential applications, emerging challenges, and future directions. Preprint, arXiv:2305.10435. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, and 16 others. 2025. Dapo: An open-source llm reinforcement learning system at scale. Preprint, arXiv:2503.14476. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, and Gao Huang. 2025. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? Preprint, arXiv:2504.13837. Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan, Yongji Wang, and JianGuang Lou. 2023. Large language models meet nl2code: survey. Preprint, arXiv:2212.09420. Quanjun Zhang, Chunrong Fang, Yang Xie, Yaxin Zhang, Yun Yang, Weisong Sun, Shengcheng Yu, and Zhenyu Chen. 2023. survey on large language models for software engineering. arXiv preprint arXiv:2312.15223. Ziyin Zhang, Chaoyu Chen, Bingchang Liu, Cong Liao, Zi Gong, Hang Yu, Jianguo Li, and Rui Wang. 2024. Unifying the perspectives of NLP and software engineering: survey on language models for code. Transactions on Machine Learning Research. Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang. 2024. Codegeex: pre-trained model for code generation with multilingual benchmarking on humaneval-x. Preprint, arXiv:2303.17568. Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, Simon Brunner, Chen Gong, Thong Hoang, Armel Randy Zebaze, Xiaoheng Hong, Wen-Ding Li, Jean Kaddour, Ming Xu, Zhihan Zhang, and 14 others. 2025. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. Preprint, arXiv:2406.15877."
        },
        {
            "title": "A Context of Reinforcement Learning",
            "content": "Reinforcement Learning (RL) training remains an important post-training component for LLMs. Reinforcement Learning from Human Feedback (RLHF) is key contributor to the upgrade of GPT to ChatGPT (Ouyang et al., 2022). By training reward model that aligns with human preferences to provide reward signals for RL training, it makes LLMs more interactive. Subsequent algorithms like Direct Preference Optimization (DPO) (Rafailov et al., 2024) do not explicitly use RL algorithms, but the underlying idea is consistent with RLHF. With the emergence of slow-thinking reasoning models (OpenAI, 2024; DeepSeek-AI, 2025; Team et al., 2025b), the community has recognized the importance of rulebased RL. Compared to SFT, rule-based Reinforcement Fine-Tuning (rule-based RFT) demonstrates stronger generalization capabilities (Chu et al., 2025). Particularly, the \"Aha moments\" observed in mathematical tasks are quite fascinating. Recently, the community has found that RFT is also effective in other domain tasks, such as video understanding (Feng et al., 2025a), image generation (Wang et al., 2025c), code generation (Liu and Zhang, 2025; Ma et al., 2025), tool using (Li et al., 2025; Qian et al., 2025; Wang et al., 2025b), machine translation (Feng et al., 2025b), and others (Jin et al., 2025). At the same time, the underlying reinforcement learning algorithms used for model optimization are also being continuously updated. PPO (Proximal Policy Optimization) (Schulman et al., 2017) algorithm is one of the most popular RL algorithms in recent years. An additional model of criticism needs to be trained to predict the advantages of the actions taken. GRPO (Group Reward Policy Optimization) algorithm, introduced by DeepSeekMATH (Shao et al., 2024), leverages group-wise advantages as rewards, eliminating the need for the critic model in PPO. Specifically, the training objective of GRPO is: arg maxθJGRP O(θ) = EqP (Q),{oi}G i=1πθk (Oq)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 oi oi (cid:88) t=1 {min[ πθ(oi,tq, oi,<t) πθk (oi,tq, oi,<t) ˆAi,t, clip( πθ(oi,tq, oi,<t) πθk (oi,tq, oi,<t) , 1 ϵ, 1 + ϵ) ˆAi,t] βDKL[πθπref ]} (4) where πθ is the policy model (LLM to be optimized), πref is the reference model, πθk is the old policy model, is the question dataset. GRPO algorithm simultaneously generates multiple responses from the policy model {oi}G i=1 and calculates rewards {ri}G i=1, thereby obtaining the intergroup advantage as the advantage function: ˆAi,t = ˆAi = ri avg({ri}N std({ri}N i=1}) i=1}) (5) DAPO (Dynamic Advantage Policy Optimization) algorithm (Yu et al., 2025) analyzes the shortcomings in GRPO and makes improvements, filling the gaps in the details of the GRPO algorithm as mentioned in the DeepSeek-R1 technical report (DeepSeek-AI, 2025). DAPO modifies the training objective to: arg maxθJDAP O(θ) = E(q,a)D,{oi}G i=1πk(q)[ 1 i=1 oi (cid:80)G (cid:88) oi (cid:88) i=1 t=1 min( ri,t(θ) ˆAi,t, clip(ri,t(θ), 1 ϵlow, 1 + ϵhigh) ˆAi,t)] s.t. 0 < {oiis_equivalent(a, oi)} < (6) where ri,t(θ) = πθ(oi,tq,oi,<t) πk(oi,tq,oi,<t) . It includes three modifications: 1) the KL divergence is removed, 2) the upper bound of the clip is relaxed ϵhigh > ϵlow), 3) using token-level policy gradient Loss and 4) the inter-group advantage being zero is avoided by dynamic sampling. DAPO also takes into account that reward noise is introduced when the output length exceeds the set maximum length. Therefore, the algorithm also introduces an additional length penalty: Rlen(x) = 0.0, Lmax Lcache 1.0, > Lmax (LmaxLcache)y Lcache , otherwise (7)"
        },
        {
            "title": "B Collected Dataset",
            "content": "B.1 Human Expert Review We recruit five students with background in computer software to review the generated code. They focus on checking whether the generated content includes the updated API and on verifying the codes correctness. When errors occur, we provide GPT-4 with human feedback about the errors and request it to regenerate the content again. B.2 Statistics Table 4 presents the dataset statistics we collected. All data are collected from mainstream data science libraries. The diversity of our dataset is not only reflected in the variety of libraries, but also in the diversity of API update types. In addition to common API updates such as name changes and added parameters, our dataset also includes wider range of update types. Several examples are shown below: squares the input .\" 1 # Example 1 2 # _add_newdoc_ufunc is now deprecated . 3 # ufunc . doc = newdoc should be used . 4 # Old Code 5 import numpy as np 6 7 = np . frompyfunc ( lambda x: **2 ,1 ,1) 8 np . _add_newdoc_ufunc (f , \"A custom ufunc that squares the input .\") 9 10 11 # New Code 12 import numpy as np 13 14 = np . frompyfunc ( lambda x: **2 ,1 ,1) 15 f. __doc__ = \"A custom ufunc that 16 17 18 19 # Example 2 20 # Arrays of 2dimensional vectors for 21 # np . cross have been deprecated . 22 # Use arrays of 3dimensional vectors . 23 # Old Code 24 import numpy as np 25 26 = np . array ([1 , 2]) 27 = np . array ([3 , 4]) 28 result = np . cross (a , b) 29 30 # New Code 31 import numpy as np 32 33 = np . array ([1 , 2, 0]) 34 = np . array ([3 , 4, 0]) 35 result = np . cross (a , b)"
        },
        {
            "title": "C Prompt with Template",
            "content": "The prompt template we use during training is as follows: C.1 Training Task (Code Migration) Prompt Template python old code c(old) Show your work in <think> </think> tags. And return the final code in <answer> </answer>, the code within <answer></answer> should be enclosed in python tags. Assistant: Let me solve this step by step. <think> And the prompt template we use during testing is as follows: C.2 Testing Task (CodeUpdateArena) Prompt Template System: You are helpful code assistant. You first think about the reasoning process in the mind and then provide Python solution to problem in real-world scenario. User: Update Note: Theres ui[update_api_path] ui[update_description]. update recent an to function The function now has new function signature ui[update_signature]. Here is detailed documentation about the update: <doc> </doc> ui[update_docstring] Scenario: qi[Scenario] Problem: qi[problem] Solution Signature: qi[signature] Show your work in <think> </think> tags. And return the final code in <answer> </answer>, the code within <answer></answer> should be enclosed in python tags. Assistant: Let me solve this step by step. <think>"
        },
        {
            "title": "D Test Dataset",
            "content": "System: You are helpful coding assistant. Your task is to transform the old version of the code into the new version specified, based on the update information. You first thinks about the reasoning process in the mind and then provides the solution. User: Dependency di performed an API update in version vi, and the update content includes: <doc> When using the CodeUpdateArena dataset, we find that some of the test cases have issues. In our experiments, we make modifications to these unreasonable aspects. Here are the three examples that we have identified: Example 1: In some test cases, two dictionaries are directly compared using == to check if they are the same. update info ui 1 assert result == expected_result </doc> The old version of the code is: We replace such test cases with correct Python code to compare two dictionaries. Library Versions Number of Data Entries NumPy Pandas PyTorch Matplotlib Scikit-Learn Scipy Scrapy Seaborn Jax Total 1.3.0 - 2.2.0 0.24.0 - 2.2.3 1.0.0 - 2.6.0 1.0 - 3.10 0.13 - 1.6 0.10.0 - 1.15.0 1.0.0 - 2.11.0 0.8.0 - 0.13.2 0.3.5 - 0.6.0 199 243 273 175 192 203 274 142 85 1786 Table 4: Statistics of our collected dataset. Example 2: The role of certain parameter in the solution signature is not clear. The solution function signature to be implemented is: 1 def convert_gradian_to_degree ( 2 3 4 ): 5 angle_list : List [ float ], flag_list : List [ bool ] pass However, the meaning of the flag parameter is not clearly specified in the update prompt. We supplement the meaning of the flag in the prompt based on the logic of the test cases. Example 3: The value of the solution parameter is in conflict with the test case. In the update prompt, the newly introduced angle unit values are specified as radians and degrees, but the test cases use the singular form. angle_sequence = [0 , 60 , 180 , 300] unit = degree result = calculate_sine_of_angles ( # Conflict angle_sequence , unit expected_results = [] ) import math 1 def test_angles_in_degrees () : 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 math . sin ( rad ) ) ) result [I], expected_results [I], rel_tol =1e -9 for in range ( len ( result )): assert math . isclose ( rad = math . radians ( angle ) expected_results . append ( for angle in angle_sequence : We unify the singular and plural forms of the units. After correcting these errors, the Pass@1 rate of DS-Coder-1.5 increases from 46.8 (value in CodeUpdateArena (Liu et al., 2025) paper) to 59.1."
        },
        {
            "title": "E Case Study",
            "content": "E.1 Error case 1 E.1 Error Case System: You are helpful code assistant. You first think about the reasoning process in the mind and then provide Python solution to problem in real-world scenario. User: Update Note: Theres an recent update to function math.degrees The math.degrees function is updated to enable conversion from radians and another unit through an additional parameter. The function now has new function signature math.degrees(x, input_unit=radian). Here is detailed documentation about the update: <doc> The function now includes an additional parameter input_unit which defaults to radian. The input_unit can be either radian or gradian. When input_unit is radian, the function converts the specified radian into degrees. When input_unit is gradian, the function converts the specified gradian into degrees. If the input_unit is neither radian nor gradian, ValueError is raised with the message input_unit should be either radian or gradian. </doc> Scenario: You are working on scientific computing project, where angle calculations are crucial part. However, not all your data sources provide angles in the same measurement units, some are providing angle values in radians and others in gradians. You are aiming to standardize all angles to degrees for improved interoperability. Problem: Given an angle value and the unit its provided in (either radian or gradian), write function to convert it into degrees. signature: Solution vert_angle_to_degrees(angle: float: function float, unit: con- -> str) Show your work in <think> </think> tags. And return the final code in <answer> </answer>, the code within <answer></answer> should be enclosed in python tags. Assistant: Let me solve this step by step. <think> 1. First, need to understand how to convert radians to degrees. The formula is degrees = radians * (180 / π). 2. Next, need to understand how to convert gradians to degrees. The formula is degrees = gradians * (90 / 200). 3. Now, need to create function that takes two parameters: the angle value and its unit (radian or gradian). 4. Inside the function, Ill check the unit parameter. If its radian, Ill use the radians to degrees formula. If its gradian, Ill use the gradians to degrees formula. 5. If the unit parameter is neither radian nor gradian, Ill raise ValueError with the appropriate message. 6. Finally, Ill return the converted angle value. </think> <answer> if unit == radian : angle : float , unit : str 1 import math 2 3 def convert_angle_to_degrees ( 4 5 ) -> float : 6 7 8 9 10 11 12 13 elif unit == gradian : else : return angle * (90 / 200) raise ValueError ( input_unit should be either radian or gradian ) return angle * (180 / math . pi ) </answer> E.2 Correct Case E.2 Correct Case System: You are helpful code assistant. You first think about the reasoning process in the mind and then provide Python solution to problem in real-world scenario. User: Update Note: Theres an recent update to function math.degrees The math.degrees function is updated to enable conversion from radians and another unit through an additional parameter. The function now has new function signature math.degrees(x, input_unit=radian). Scenario: You are working on scientific computing project, where angle calculations are crucial part. However, not all your data sources provide angles in the same measurement units, some are providing angle values in radians and others in gradians. You are aiming to standardize all angles to degrees for improved interoperability. Problem: Given an angle value and the unit its provided in (either radian or gradian), write function to convert it into degrees. signature: Solution vert_angle_to_degrees(angle: float: function float, unit: con- -> str) Show your work in <think> </think> tags. And return the final code in <answer> </answer>, the code within <answer></answer> should be enclosed in python tags. Assistant: Let me solve this step by step. <think> To solve this problem, we need to create function that leverages the updated math.degrees function to convert angles from either radians or gradians to degrees. The function will take two parameters: the angle value and the unit of that angle (radian or gradian). Heres the step-by-step thought process: We will define function named cona 1. vert_angle_to_degrees arguments: takes that angle (a float representing the angle value) and unit (a string indicating whether the angle is in radian or gradian). two 2.Inside the function, we will call the updated math.degrees function, passing the angle and unit as arguments. This function will handle the conversion based on the unit provided. 3. Since the math.degrees function already raises ValueError if the input_unit is not radian or gradian, we dont need to add any additional error handling in our function. 4. Finally, we will return the result of the math.degrees function call, which will be the angle converted to degrees. Well also import the math module at the beginning of our script since were using its degrees function. </think> <answer> 1 import math 2 3 def convert_angle_to_degrees ( 4 5 ) -> float : 6 angle : float , unit : str return math . degrees ( angle , unit ) Here is detailed documentation about the update: </answer> <doc> The function now includes an additional parameter input_unit which defaults to radian. The input_unit can be either radian or gradian. When input_unit is radian, the function converts the specified radian into degrees. When input_unit is gradian, the function converts the specified gradian into degrees. If the input_unit is neither radian nor gradian, ValueError is raised with the message input_unit should be either radian or gradian. </doc> E.3 Error case When simulating pendulum, the model lacks an understanding of physical knowledge. In the generated code, it completely ignores the physical quantities that need to be used. 1 def get_pendulum_position ( 2 3 4 ): angle_in_degrees , gravity , length , time 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Convert from degrees to radians angle_in_radians = math . radians ( angle_in_degrees ) # Calculate the sine of the angle sine_value = math . sin ( angle_in_radians ) # Calculate the pendulum position position = angle_in_degrees * sine_value return position"
        },
        {
            "title": "F Results under Complex Scenarios",
            "content": "F.1 Multiple updates to single API The prompt template we use during training is as follows: F.1 Training Task (Code Migration) Prompt Template System: You are helpful coding assistant. Your task is to transform the old version of the code into the new version specified, based on the update information. You first thinks about the reasoning process in the mind and then provides the solution. Theres ui[update_api_path] ui[update_description]. update recent an to function The function now has new function signature ui[update_signature]. Here is detailed documentation about the update: <doc> </doc> ui[update_docstring] Scenario: qi[Scenario] Problem: qi[problem] Solution Signature: qi[signature] Show your work in <think> </think> tags. And return the final code in <answer> </answer>, the code within <answer></answer> should be enclosed in python tags. Assistant: Let me solve this step by step. <think> F.2 Multiple APIs Update F.3 Error Case System: You are helpful code assistant. You first think about the reasoning process in the mind and then provide Python solution to problem in real-world scenario. User: Dependency di performed an API update in version vi, and the update content includes: User: Update Note: old code c(old) <doc> <doc> update info ui </doc> The old version of the code is: python Show your work in <think> </think> tags. And return the final code in <answer> </answer>, the code within <answer></answer> should be enclosed in python tags. Assistant: Let me solve this step by step. <think> And the prompt template we use during testing is as follows: F.2 Testing Task (CodeUpdateArena) Prompt Template System: You are helpful code assistant. You first think about the reasoning process in the mind and then provide Python solution to problem in real-world scenario. User: Update Note: Theres an recent update to function math.degrees The math.degrees function is updated to enable conversion from radians and another unit through an additional parameter. The function now has new function signature math.degrees(x, input_unit=radian). Here is detailed documentation about the update: The function now includes an additional parameter input_unit which defaults to radian. The input_unit can be either radian or gradian. When input_unit is radian, the function converts the specified radian into degrees. When input_unit is gradian, the function converts the specified gradian into degrees. If the input_unit is neither radian nor gradian, ValueError is raised with the message input_unit should be either radian or gradian. </doc> Theres an other update to function math.log Add boolean parameter inverse to math.log to control whether to perform logarithm or anti-logarithm calculation. The function now has new function signature math.log(x, base=math.e, inverse=False) <doc> An optional third parameter inverse has been added to the log function. If inverse is set to True, the function will instead calculate the anti-logarithm (exponentiation) of to the base base. The value of inverse defaults to False, i.e., the function will perform logarithm calculation as before if inverse is not explicitly set to True. 10 11 12 13 14 15 16 17 Note: If inverse is set to True and base is not specified, the function will calculate the exponential of x. </doc> Scenario: John has angle (with specific unit) and base b. He also has boolean flag flag. If flag is true, he wants to perform an anti-logarithm operation with base on the degree of angle x. If flag is false, he wants to perform logarithm operation with base on the degree of x. He needs Python function that can perform these operations for him based on the values of x, and flag. Problem: Given an angle value and the unit its provided in (either radian or gradian), write function to convert it into log or anti-log. # Perform log or anti - log operation result = math . log ( angle_in_degrees , base =b , inverse = flag ) return result </answer> Solution vert_angle_to_log_or_antilog(x, b, flag, input_unit): signature: def conShow your work in <think> </think> tags. And return the final code in <answer> </answer>, the code within <answer></answer> should be enclosed in python tags. Assistant: Let me solve this step by step. <think> Lets break down the problem and the requirements: Conversion of Angle : We need to convert the given angle from either radians or gradians to degrees. This can be done using the math.degrees function with the input_unit parameter. Logarithm or Anti-Logarithm Calculation : Depending on the flag, we need to either compute the logarithm or the anti-logarithm of the converted angle. This can be handled using the math.log function with the inverse parameter. Function Signature : function The vert_angle_to_log_or_antilog(x, b, flag, input_unit). signature should def be conSteps : First, convert the angle to degrees based on the input_unit. Then, use the math.log function with the inverse parameter set according to the flag. Edge Cases : Ensure that input_unit is either radian or gradian. Otherwise, raise ValueError. Handle cases where might not be specified when inverse is True. </think> <answer> 1 import math 2 3 def convert_angle_to_log_or_antilog ( 4 5 ): 6 7 8 9 # Convert angle to degrees angle_in_degrees = math . degrees ( , input_unit = input_unit , , flag , input_unit )"
        }
    ],
    "affiliations": [
        "Tencent AI, Seattle Lab",
        "Zhejiang University"
    ]
}