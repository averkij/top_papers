{
    "paper_title": "Esoteric Language Models",
    "authors": [
        "Subham Sekhar Sahoo",
        "Zhihan Yang",
        "Yash Akhauri",
        "Johnna Liu",
        "Deepansha Singh",
        "Zhoujun Cheng",
        "Zhengzhong Liu",
        "Eric Xing",
        "John Thickstun",
        "Arash Vahdat"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion-based language models offer a compelling alternative to autoregressive (AR) models by enabling parallel and controllable generation. Among this family of models, Masked Diffusion Models (MDMs) achieve the strongest performance but still underperform AR models in perplexity and lack key inference-time efficiency features--most notably, KV caching. In this work, we introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms, enabling smooth interpolation between their perplexities while overcoming their respective limitations. Eso-LMs set a new state of the art on standard language modeling benchmarks. Crucially, we are the **first to introduce KV caching for MDMs** while preserving parallel generation, significantly improving inference efficiency. Combined with an optimized sampling schedule, our method achieves up to **65x** faster inference than standard MDMs and **4x** faster inference than prior semi-autoregressive approaches. We provide the code and model checkpoints on the project page: [http://s-sahoo.github.io/Eso-LMs](http://s-sahoo.github.io/Eso-LMs)"
        },
        {
            "title": "Start",
            "content": "Subham Sekhar Sahoo1 Zhihan Yang2 Yash Akhauri1 Johnna Liu1 Deepansha Singh1 Zhoujun Cheng3 Zhengzhong Liu3 Eric Xing3 John Thickstun2 Arash Vahdat4 Joint First Authors Joint Second Authors 1 Cornell Tech 2 Cornell University 3MBZUAI 4 NVIDIA 5 2 0 2 2 ] . [ 1 8 2 9 1 0 . 6 0 5 2 : r {ssahoo, zhihany}@cs.cornell.edu"
        },
        {
            "title": "Abstract",
            "content": "Diffusion-based language models offer compelling alternative to autoregressive (AR) models by enabling parallel and controllable generation. Among this family of models, Masked Diffusion Models (MDMs) achieve the strongest performance but still underperform AR models in perplexity and lack key inference-time efficiency featuresmost notably, KV caching. In this work, we introduce Eso-LMs, new family of models that fuses AR and MDM paradigms, enabling smooth interpolation between their perplexities while overcoming their respective limitations. Eso-LMs set new state of the art on standard language modeling benchmarks. Crucially, we are the first to introduce KV caching for MDMs while preserving parallel generation, significantly improving inference efficiency. Combined with an optimized sampling schedule, our method achieves up to 65 faster inference than standard MDMs and 4 faster inference than prior semi-autoregressive approaches. We provide the code and model checkpoints on the project page: https://s-sahoo.com/Eso-LMs"
        },
        {
            "title": "Introduction",
            "content": "We are at pivotal moment in language modeling. paradigm shift is underwayautoregressive (AR) language models, long considered the gold standard, are now being rivaled by diffusion language models for standard language generation. Recent works [26, 29] show that Masked Diffusion Models (MDMs) are closing the gap with AR models on small-scale language benchmarks, and even outperform them on tasks involving discrete structures, such as molecular generation [28, 11] and graph generation [14]. When scaled to larger sizes (e.g., 8B parameters), MDMs match models like LLaMA on challenging datasets in math, science, and tasks such as reverse poem completion [18]. These results make MDMs compelling alternative to AR models. However, they suffer from two key limitations: (1) Inference speed: Despite supporting parallel generation, MDMs are significantly slower than AR models in practice, largely due to the lack of KV cachinga crucial optimization for real-time applications like chat systems. (2) Generation quality: MDMs still show noticeable likelihood gap on more complex language modeling tasks [26]. Recently proposed BD3-LMs [1] address the speed issue by introducing semi-autoregressive generation strategy. These models perform diffusion over fixed-length blocks of text sequentially. Because previously denoised blocks can be cached, BD3-LMs partially support KV caching and are faster than standard MDMs. However, we identify two key shortcomings in BD3-LMs: (1) Mode collapse at low sampling steps: When the number of denoising steps is reduced for faster inference, BD3-LMs exhibit severe degradation in sample qualityworse than both AR (at high Number of Function Evaluations (NFEs)) and diffusion models (at low NFEs) (Sec. 5.3). (2) Incomplete Preprint. Under review. Figure 1: Efficient generation of an example sequence with our flagship model Eso-LM (B). During Diffusion Phase, Eso-LMs denoise one or more, potentially non-neighboring mask tokens (M) per step. During Sequential Phase, Eso-LMs denoise the remaining mask tokens one at time from left to right. Eso-LM (B) allows for KV caching in both phases using just single unified KV cache: blue bounding boxes enclose transformer cells that are building their KV cache; cell becomes blue once its KV cache is built. The sequences below the transformers depict tokens in their natural order. caching: While KV caching is possible across blocks, intra-block diffusion still lacks KV support, limiting overall speed gains. To address these challenges, we propose new language modeling paradigm that fuses autoregressive and masked diffusion approaches. Our model is trained with hybrid lossa combination of AR and MDM objectiveswhich allows it to interpolate smoothly between the two paradigms in terms of perplexity. This requires two key innovations: (1) revised attention mechanism in the denoising transformer to support both AR and MDM styles of generation. (2) new training and sampling procedure that enables KV caching within the diffusion phase, feature previously unavailable in MDMs. Due to the unconventional nature of this hybrid design, we name our method Esoteric Language Models (Eso-LMs). Our method sets new state of the art among diffusion models on language modeling benchmarks and enables fine-grained control over interpolation between AR and MDM perplexities. Unlike prior approaches, it supports KV caching during diffusion, yielding significantly faster inference. In summary, our contributions are as follows: (1) We propose new framework for language modeling: one that fuses AR and MDM paradigms and outperforms the previous hybrid approach, BD3-LMs. (2) Ours is the first approach to enable KV caching for MDMs while preserving parallel generation, achieving up to 65 faster inference than standard MDMs and 3 4 faster inference than KVcached semi-autoregressive baselines (Sec. 5.2). (3) We show that BD3-LMs degrade at low sampling steps, while our method remains competitive with MDMs in the low NFE regime and with AR in the high NFE regime (Sec. 5.3). (4) Our results establish new state-of-the-art perplexity for discrete diffusion, narrowing the gap to autoregressive models (Sec. 5.1)."
        },
        {
            "title": "2 Background",
            "content": "Notation We represent scalar discrete random variables that can take values as one-hot column vectors and define {x {0, 1}K i=1 xi = 1} as the set of all such vectors. Let be special mask vector such that mK = 1. Define Cat(; π) as the categorical distribution over classes with probabilities given by π K, where denotes the K-simplex. Additionally, let a, denote the dot product between vectors and b. We use parentheses () to denote ordered sets (tuples) and curly brackets {} to denote unordered sets. denotes the cardinality of the set A. MDMs feature two salient orderings: the sequence order and the denoising order. We use permutation σ to describe the relationship between these orderings. Let PL denote the set of all permutations of [L] = {1, . . . , L}. σ PL is an ordered set (tuple) and also serves as bijective function: σ(l) is the item at the lth position in σ, and σ1(i) is the position of the item in σ. For example, (2, 4, 1, 3) denotes an ordering of {1, 2, 3, 4}, with σ(2) = 4 mapping the position 2 to its corresponding item 4, and σ1(4) = 2 mapping the item 4 to its corresponding position 2. 2 = m} denote token indices of mask tokens in zt and C(zt) = {ℓ zℓ Let denote sequence of length with no mask tokens, and let xℓ denote the ℓth entry in x. We use the term token index to refer to the position of token in the original ordering, e.g., the token index for xl is l. Let (zt)t[0,1] denote sequence of length that may contain mask tokens. Let M(zt) = {ℓ zℓ m} denote token indices of clean tokens in zt. Let n m+n denote concatenation operator on two sequences = (x1, x2, . . . , xm) and = (z1, z2, . . . , zn) of length and n. When the concatenated sequence is fed into the transformer, and carry the same positional embeddings as they would if they were fed into transformer independently. Let n denote substitution operator; for any and with > n, the output = is given by: y1n = and yn+1m = zn+1m. 2.1 Autoregressive Models Given sequence qdata in L, autoregressive (AR) models define the following factorization of the ℓ=1 log pθ(xℓ x<ℓ), where the model pθ is usually parameterized joint distribution: log pθ(x) = by causal transformer [34] model. Sampling takes steps but is efficient due to KV caching. AR models achieve the best likelihood and generation quality. 2.2 Masked Diffusion Models Diffusion models learn to reverse forward corruption process q, which transforms clean data qdata in into sequence of latent variables zt for [0, 1], each representing an increasingly noisy version of [8, 31, 32]. In MDMs [26, 30, 20], the forward masking process factors independently across the sequence x, i.e., qt(.x) = Πℓqt(.xℓ) and each token xl is progressively interpolated with fixed target distribution Cat(.; m). The marginal of zℓ qt(.xℓ) at time is given by: qt(.xℓ) = Cat(.; αtxℓ + (1 αt)m), (1) where αt [0, 1] is strictly decreasing function in with α0 1 and α1 0. Sahoo et al. [26] show that the reverse posterior qst(.zℓ for < is given by t, xℓ) over zℓ qst(.zℓ t, xℓ) = Cat(.; zℓ t) (1αs)m+(αsαt)xℓ Cat (.; 1αt zℓ m, ) zℓ = m. (2) Given denoising model xθ (K)L, the reverse unmasking process pθ(.zt)st over the sequence zs is parameterized by pθ(.zt)st = ℓ pℓ θ(.zt)st = ℓ st(.zℓ qℓ t, xℓ = xℓ θ(zt)). (3) Sahoo et al. [26], Shi et al. [30], Ou et al. [20] show that Negative Evidence Lower Bound (NELBO) for this method is LMDM(x) = Eq,t[0,1] α 1 αt ℓM(zt) logxℓ θ(zt), xℓ, (4) which is weighted average of masked language modeling losses [5] computed only on the masked tokens M(zt). To generate sequence of length L, the reverse diffusion process starts from zt=1, where zℓ t=1 = for ℓ = 1, . . . , L. It proceeds for steps, with each zℓ independently sampled from pθ(.zt)st as defined in (3); once position is unmasked, it remains fixed. Since multiple tokens can be denoised in parallel, the total number of network function evaluations (NFEs) can be less than L, enabling faster generation. However, each NFE is expensive due to the bidirectional transformer in xθ(zt). 2.3 Block Denoising Diffusion Discrete Language Models Block Denoising Diffusion Discrete Language Models (BD3-LMs) [1] autoregressively model blocks of tokens and perform masked diffusion modeling (Sec. 2.2) within each block. By changing the 3 size of blocks, BD3-LMs interpolate AR models and MDMs. BD3-LMs group tokens in into blocks of consecutive tokens with = L/L, where is assumed to be an integer. The b=1 log pθ(xb x<b) likelihood over factorizes autoregressively over blocks as log pθ(x) = b=1 LMDM(xb; x<b), where pθ(xb x<b) is conditional MDM and LMDM(xb; x<b) is the NELBO for MDLM as defined in (4), applied sequentially across the blocks."
        },
        {
            "title": "3 Esoteric Language Models",
            "content": "We propose new paradigm for language modeling: Esoteric Language Models (Eso-LMs), which form symbiotic combination of AR models and MDMs. Our approach has two main components: (1) we introduce family of models that supports both sequential (AR-style) and parallel (MDM-style) generation; and (2) in Sec. 4, we detail architectural changes to the transformer [34] to enable this hybrid generation. While AR models currently lead in language modeling performance, they generate tokens sequentially, making them slow at inference. In contrast, MDMs generate multiple tokens in parallel and are more controllable [29, 19], but they typically yield higher perplexity than AR models [26, 27]. The key question is: can we combine their strengths? We introduce hybrid approach where some tokens are generated in parallel via MDMs and the rest sequentially in left-to-right fashion. This raises two key questions. First, can we compute the likelihood of such generative model? We show that Eso-LMs admit principled bound on the true likelihood. Second, how can we adapt the attention mechanism in the transformer to support both styles of generation? During training, in AR models, causal attention is enforced on the clean sequence, whereas MDMs rely on bidirectional attention over both masked and unmasked tokens. In Sec. 4, we reconcile these two distinct architectures by designing custom attention mechanisms for the transformer. Additionally, we introduce sparsity into the attention matrix, enabling KV-caching for MDMs which significantly improves the generation speed for long-context generation. 3.1 Fusing Autoregressive Models and Masked Diffusion Let qdata(x) in be sample from the data distribution, and let pθ be our model distribution parameterized by θ. Eso-LMs decompose pθ into two components: an AR model pAR and an MDM θ pMDM (z0), and the AR model θ finishes the remaining unmasking steps in an auto-regressive left-to-right manner: pAR θ (xz0). The marginal likelihood of such hybrid generative process is: . The MDM generates partially masked sequence z0 pMDM θ pθ(x) = z0V θ (xz0)pMDM pAR θ (z0). (5) Although this sum is intractable, we can compute variational bound on the true likelihood using posterior q(z0x) [10]. Since pMDM models masked sequences, we choose to be simple masking distribution. Specifically, we set to q0(z0x) as defined in (1), which independently masks each token (xℓ)ℓ[L] with probability 1 α0, where α0 [0, 1]. This leads to the following variational bound: θ log pθ(x) Ez0q0(.x)[log pAR θ (xz0)] + DKL(q0(z0x)pMDM θ (z0)) = Ez0q0(.x) ℓM(z0) log pAR θ (xℓz0, x<ℓ) + DKL(q0(z0x)pMDM θ (z0)). (6) Here, pAR θ (xz0) is the joint AR likelihood over masked positions ℓ M(z0), conditioned on the clean tokens in z0. For AR, the denoising network xθ (K)L operates on the input z0 x<ℓ, where the substitution operator replaces the first 1 tokens in z0 with x<ℓ. For each ℓ M(z0), θ(z0 x<ℓ) approximates the distribution of the clean token xℓ given x<ℓ and z0, which may include xℓ clean tokens beyond position ℓ. In Suppl. B.1, we analyze the KL term and show that the NELBO is: LNELBO(x) = Ez0q0 logxℓ θ(z0 x<ℓ), xℓ ℓM(z0) AR loss t=1 α 1 αt + Eztqt t=0 ℓM(zt) MDM loss θ(zt), xℓ logxℓ dt . 4 (7) Interpolating between AR and Diffusion When α0 = 1, the posterior sample z0 = x, and all tokens are generated by the MDM; hence, the AR loss is zero in (7), and LNELBO reduces to the MDM loss. Conversely, when α0 = 0, all tokens are masked, and the MDM loss vanishes, reducing LNELBO to the AR loss. Thus, Eso-LMs interpolate between AR and MDM paradigms, controlled by the hyperparameter α0. 3.2 Sampling We perform the two-stage sampling defined by (5): to sample from pθ(x), we first draw partially masked sequence z0 pMDM and then complete it using pAR θ . θ Diffusion Phase Starting from fully masked sequence zt=1 = m1L, standard ancestral sampling (Sec. 2.2) updates only subset of masked tokens at each step while doing forward pass over the full sequencewasting FLOPs. To improve efficiency, we propose two key changes to the standard sampling and the training procedure. During sampling, we pre-compute the diffusion denoising schedule MDM = (S1, . . . , S1/T ), where St is the set of mask token indices to be denoised at diffusion step t. Instead of processing the entire sequence, we perform forward pass only over the subsequence {zℓ tℓ C(zt) St}i.e., the clean tokens and those scheduled for denoisingsubstantially reducing computation, especially on long sequences. To match this sampling scheme, the model must avoid bidirectional attention over mask tokens in training, as discussed in Sec. 4. This approach also enables efficient KV-caching during diffusion, as discussed in Sec. 4.2. Sequential Phase We initialize the final output as z0. The AR model then fills in mask tokens left-to-right with the sequential denoising schedule AR = ((i)i M(z0)) for which we require the single-element sets to be sorted ascendingly by their only element. Unlike standard AR decoding, each xℓ conditions on both its left context, which consists entirely of clean tokens, and rightward clean tokens in z0, enabling richer generation. We skip evaluation on rightward mask tokens, reducing unnecessary computation. The sequential phase naturally supports KV caching. We denote the unified denoising schedule by = MDM AR, which concatenates the two sampling schedules to partition [L]. When α0 = 1, all tokens are generated by diffusion, so = MDM and AR = (null set); when α0 = 0, all tokens are generated sequentially, so = AR and MDM = . See Suppl. B.4 for the full sampling algorithm and an illustrative example."
        },
        {
            "title": "4 Designing Attention Mechanisms for the Shared Denoising Transformer",
            "content": "In this section, we introduce unified modeling framework that supports both sequential (AR) and parallel (MDM) generation modes using shared transformer architecture. Our core technical contribution is flexible attention mechanism that reconciles the architectural mismatch between AR modelswhich require causal attention and shift-by-one decodingand MDMswhich rely on bidirectional attention. To achieve this, we introduce an attention bias matrix {, 0}LL , where is the sequence length, that modulates the standard attention as: SELF-ATTENTION(Q, K, V, A) = softmax ( QK + A) where Q, K, RLd denote the query, key, and value matrices. Entries of control information flow: Ai,j = 0 permits and Ai,j = blocks attention from token to j. This mechanism enables single transformer to emulate both causal and bidirectional behaviors as needed. Building on this unified attention scheme, we propose two variants: Eso-LM (A) and Eso-LM (B). Eso-LM (A) focuses on reducing computation by sparsifying attention and selectively applying the denoising transformer to subsets of mask tokens at each diffusion step. Eso-LM (B) further extends this idea by introducing causal masking not only over mask tokens but also over clean tokens, enabling KV-caching for even greater speedupsat the cost of slight drop in perplexity. In the rest of this section, we describe the attention formulation that enables this unification and detail the differences between these two variants in both diffusion and sequential phases. Diffusion Phase Standard MDMs suffer from two inefficiencies during sampling: (i) full-sequence forward passes even for partial updates, and (ii) no KV caching across steps due to bidirectional 5 Figure 2: Comparison of attention biases for diffusion-phase training. Orange represents 0 (attention) and gray represents (no attention). The clean sequence is = (A, B, C, D, E, ). After random masking, we obtain zt = (A, M, C, M, M, ). The integers denote the position indices with M(zt) = {2, 4, 5} and C(zt) = {1, 3, 6}. The random ordering is σ = (3, 1, 6, 4, 5, 2) P6 with clean tokens before mask tokens. Red highlights differences between Eso-LM (A) and Eso-LM (B). attention. As briefly discussed in Sec. 3.2, we address (i) by evaluating xθ only on {zi ti C(zt) St} at step t. To unlock this possibility, we adopt key idea from AO-ARMs [20]: mask tokens can be revealed in any order. Hence, during training, we sample random orderings σ PL and enforce causal attention over the mask tokens {zi ti M(zt)} per σ. Specifically, we enforce mask token (zi t)iM(zt) to only attend to clean tokens and prior masked tokens per σ. Eso-LM (A) (Sec. 4.1) uses this strategy to significantly reduce computation during sampling while maintaining performance. Eso-LM (B) (Sec. 4.2) extends this by enforcing similar causal masks on clean tokens, enabling KV caching. Though slightly worse in perplexity, Eso-LM (B) offers major speed-ups (up to 65) during sampling (Sec. 5.2). θ 0)ℓM(z0) produced by z0 pMDM Sequential Phase During sequential generation, the model needs to denoise mask tokens (zℓ (.) from left to right. This is straightforward during generation but challenging during training. Unlike standard AR models where each token attends only to its left, each mask token zi 0 must attend to (i) itself (for positional embedding), (ii) the clean left context x<i (assuming that all prior mask tokens in z<i 0 have been filled), (iii) and clean tokens in the right context {xjj C(z0), > i} or equivalently {zj 0j C(z0), > i}. Handling (ii) during training is non-trivial because each mask token in z0 assumes the tokens to its left are already denoised by pAR θ . To simulate this behavior, we feed the concatenated sequence z0 into the transformer during training and construct custom attention mask that enforces the correct information flow for all mask tokens. The loss is computed only on the logits corresponding to the masked tokens in z0 as per (7). During generation, this issue is naturally resolved, and no concatenation is required. 4.1 Eso-LM (A) We introduce Eso-LM (A), which removes bidirectional attention over mask tokens in the denoising transformer during the diffusion phase. This modification enables the use of the sampling schedule described in Sec. 3.2, allowing for faster inference. Eso-LM (A) only supports KV caching during sequential sampling but performs significantly less computation than standard MDMs during diffusion sampling. 4.1.1 Training Diffusion Phase The denoising transformer receives zt qt(.x) as input, which contains the mask tokens to denoise, and as target. random ordering σ PL is sampled with the natural constraint that clean tokens in zt precede mask tokens in zt in σ. We define the attention bias by Ai,j = 0 otherwise. (i, j) C(zt) C(zt) if σ1(i) σ1(j) (i, j) M(zt) [L] (8) (9) (10) Clean tokens {zi (zi ti C(zt)} have bidirectional attention among them (8), while mask token t)iM(zt) attends to clean tokens, itself and prior mask tokens per σ (9). Note that we ignore the 6 ordering among clean tokens in σ and simply use of bidirectional attention among them. See Fig. 2 for an example. Interestingly, becomes Prefix-LM [25] attention bias if we sort the rows and columns of by σ  (Fig. 7)  ; we exploit this property in our implementation (Suppl. B.5). Sequential Phase The denoising transformer receives the concatenated sequence z0 2L as input, where z0 q0(.x) contains the mask tokens to denoise, and as target. We define the 2L 2L attention bias by Ai,j = 0 Ai,j+L = 0 Ai,j+L = 0 Ai+L,j+L = 0 Ai+L,j+L = 0 Ai+L,j+L = 0 Ai,j = if = j(i, j) M(z0) M(z0) (i, j) M(z0) C(z0) if > j(i, j) M(z0) M(z0) (i, j) C(z0) C(z0) (i, j) M(z0) C(z0) if j(i, j) M(z0) M(z0) otherwise. (11) (12) (13) (14) (15) (16) (17) The attention mechanism for tokens in the noisy context z0 is described as follows. mask token 0)iM(z0) attends to (i) itself (11), (ii) the clean tokens {xjj C(z0)} (12) and (iii) the clean (zi versions of mask tokens on its left {xjj M(z0), > j} (13). clean token (zi 0)iC(z0) can attend to anything because no other token attends to them (17). The attention mechanism for tokens in the clean context x0 is described as follows. Tokens {xii C(z0)} have bidirectional attention (14). clean token corresponding to mask token,(xi)iM(z0), attends to {xjj C(z0)} (15) and {xjj M(z0), j} (16). See Fig. 6 for an example. Let σ be an ordering with two properties: (i) clean tokens in z0 precede mask tokens in z0 in σ and (ii) mask tokens in z0 are in natural order in σ. Again, the ordering among clean tokens {xii C(z0)} is ignored and we use bidirectional attention among them. When the rows and columns of each of the four L-by-L blocks are sorted by σ, displays sparse and patterned block structure  (Fig. 8)  that we exploit in our implementation (Suppl. B.5). 4.1.2 Sampling During diffusion or sequential sampling, given partially masked sequence zk, the denoising model is required to denoise the mask tokens {zi ki Sk} for Sk = {S1, . . . , SK} where = S. We perform forward pass on the subset of tokens {zi ki C(zk) Sk}. It is crucial to note that while performing forward pass on subset of tokens, the positional embeddings of these tokens in the actual sequence are preserved. Below we discuss the attention bias used in the forward pass. be the set of indices of tokens decoded in the diffusion phase prior to step and DAR Let DMDM be that for the sequential phase. Let ordering σ be the order in which we denoise tokens defined by S. We define the attention bias at step by Ai,j = 0 0 0 (i, j) DMDM DMDM DMDM (i, j) DAR if (i, j) DAR (i, j) Sk (DMDM if σ1(i) σ1(j) (i, j) Sk Sk DAR DAR ) (18) (19) (20) (21) kj DMDM 0 otherwise. ki DMDM k)iDAR } (19), itself, and prior clean tokens decoded sequentially {zj Clean tokens decoded during diffusion {zi clean token decoded sequentially (zi {zj mask token to denoise (zi itself, and prior mask tokens to denoise per σ: {zj scheduled to denoise (zi (22) (23) } have bidirectional attention among them (18). attends to clean tokens decoded during diffusion , > j} (20). } (21), kj Sk, σ1(i) > σ1(j)} (22). Mask tokens not k)iS>k can attend to anything because no other token attends to them (23). Since (i) (zi ki S>k} and (ii) we only use the transformer output over {zi ki S>k} can be avoided, leading to significantly less computation during the diffusion phase. Fig. 9 shows how this sampling procedure generates an example sequence, with KV caching during the sequential phase. ki Sk} for sampling clean tokens, any computation over {zi k)iSk attends to all decoded clean tokens {zj k)iC(zk)Sk does not attend to {zi kj DAR kj DMDM DAR 7 4.2 Eso-LM (B) Eso-LM (A) can perform KV caching during sequential sampling but cannot do so during diffuson sampling because clean tokens have bidirectional attention among them. Hence, after set of mask tokens is denoised, the KVs for all clean tokens, now including the newly denoised tokens, must be updated. To enable KV caching during diffusion sampling, we enforce causal attention among clean tokens, which leads to Eso-LM (B). We also enable shared KV cache between the two phases. 4.2.1 Training Diffusion Phase The denoising transformer receives zt qt(.x) as input, which contains the mask tokens to denoise, and as target. We leverage the connection of MDMs with AO-ARMs [20], which establishes that not only can mask tokens {zi ti M(zt)} be generated in any random order (Sec. 4.1.1), clean tokens {zi ti C(zt)} also could have been generated in any random order. Hence, we sample random ordering σ PL with the natural constraint that clean tokens in zt precede mask tokens in zt in σ. Unlike in Sec. 4.1.1, we now constrain clean token (zi t)iC(zt) to only attend to itself and prior clean tokens per σ. Similarly and as in Sec. 4.1.1, mask token (zi t)iM(zt) attends to clean tokens, itself, and prior mask tokens per σ. Hence we define the attention bias by (24) (25) See Fig. 2 for an example. Further, becomes causal attention bias if we sort the rows and columns of by σ  (Fig. 7)  ; we exploit this property in our implementation (Suppl. B.5). 0 otherwise. if σ1(i) σ1(j) (i, j) [L] [L] Ai,j = { Sequential Phase The denoising transformer receives the concatenated sequence z0 2L as input, where z0 q0(.x) contains the mask tokens to denoise, and as target. During sampling, the clean context in z0 pMDM , on which causal attention is applied and KV cache is built, is provided by the diffusion phase. To effectively use the KV cache built from the diffusion phase, we must enforce similar causal attention among clean tokens {xii C(z0)} during training for the sequential phase. To achieve this, we first sample σ PL with the constraints that (i) clean tokens in z0 precede mask tokens in z0 in σ and (ii) mask tokens are in natural order in σ. Then, we reuse (11)-(17) but replace the bidirectional attention over {xii C(z0)} (14) with causal attention per σ (29): θ Ai,j = 0 Ai,j+L = 0 Ai,j+L = 0 if = (i, j) M(z0) M(z0) (i, j) M(z0) C(z0) if > (i, j) M(z0) M(z0) if σ1(i) σ1(j) (i, j) C(z0) C(z0) (29) (30) (31) (32) See Fig. 6 for an example. When the rows and columns of each of the four blocks are sorted by σ, displays structures  (Fig. 8)  that we exploit in implementation (Suppl. B.5). Ai+L,j+L = 0 Ai+L,j+L = 0 (i, j) M(z0) C(z0) Ai+L,j+L = 0 Ai,j = if (i, j) M(z0) M(z0) otherwise. 4.2.2 Sampling Following the same setup and notation in Sec. 4.1.2, we reuse (18)-(23) but replace the bidirectional attention over DMDM 0 (18) with causal attention per σ, the denoising ordering (33): if σ1(i) σ1(j) (i, j) DMDM (i, j) DAR DMDM if (i, j) DAR (i, j) Sk (DMDM if σ1(i) σ1(j) (i, j) Sk Sk DAR DAR ) DMDM Ai,j = 0 0 otherwise. (37) (38) KV Caching in Both Phases Here, as in Sec. 4.1.2, we do not perform computation over future mask tokens {zi ki S>k}. Further, since previously decoded tokens, regardless of whether they were decoded by diffusion or sequentially, have causal attention among them, newly generated tokens at step can be used to build the KV cache at step + 1 and have their cached KV values used starting at step + 2. Fig. 1 shows how this sampling procedure generates an example sequence. (26) (27) (28) (33) (34) (35) (36)"
        },
        {
            "title": "5 Experiments",
            "content": "We evaluate Eso-LMs on two standard language modeling benchmarks: the One Billion Words dataset (LM1B) [3] and OpenWebText (OWT) [6]. For LM1B, we use the bert-base-uncased tokenizer [5] and context length of 128, reporting perplexity on the test split. For OWT, which lacks standard split, we follow [26, 27, 1] and reserve the last 100K documents for validation, using the GPT2 tokenizer [24] and context length of 1024. All models use the transformer architecture from Lou et al. [15], which builds on the diffusion transformer [22] with rotary embeddings [33]. Following Arriola et al. [1], Austin et al. [2], and unlike Sahoo et al. [26], Lou et al. [15], He et al. [7], we apply sequence packing to LM1B, making our setup more challenging and resulting in higher perplexities given the same model  (Table 1)  . Models are trained for 1M steps on LM1B and 250K steps on OWT, without timestep conditioning. Models are trained on H200 GPUs and benchmarked on A6000 GPUs. Full hyperparameters and training details are provided in Suppl. B.6. 5.1 Likelihood Evaluation Our experiments show that Eso-LMs achieve state-of-the-art perplexities among diffusion models on LM1B and OWT, while providing finer-grained interpolation between MDM and AR perplexities (Table 1 and Table 2). Experimental Setup We compare Eso-LMs against leading masked diffusion modelsMDLM [26], SEDD [15], D3PM [2], and DiffusionBERT [7]as well as uniform state models DUO [27], UDLM [29], and specifically BD3-LMs [1], which also interpolate between MDM and AR and support KV caching. All models are trained with batch_size=512, consistent with prior work. We split each batch evenly: half trained with the AR loss and half with the MDM loss (7). Refer to Table 5 for an ablation on the split proportion. Refer to Algo. 1 for the training procedure. Attention biases are configured as described in Sec. 4. When training Eso-LMs as pure MDM (α0 = 1), the full batch is trained with the MDM loss. For this setting only, we replace the diffusion coefficient α t/(1 αt) with 1, which empirically reduced training variance and improved convergence. Results For our method, PPL is computed using (7). On LM1B, we train Eso-LM (A) with α0 {0.0625, 0.125, 0.25, 0.5, 1.0}, and Eso-LM (B) with α0 = 1. We find that Eso-LM (A) effectively interpolates between MDLM and AR perplexities. Surprisingly, Eso-LM (A), which modifies MDMs by replacing bidirectional attention over mask tokens with causal mask, outperforms MDLM by 1 PPL at α0 = 1. This improvement can be attributed to the low-variance training loss as described earlier. In contrast, Eso-LM (B) introduces causal dependencies among clean tokens, which degrades performance by 4 PPL compared to Eso-LM (A) at α0 = 1. This is expected because bidirectional attention on the clean tokens in Eso-LM (A) is strictly more expressive than causal attention in Eso-LM (B). On OWT, Eso-LM (B) is particularly compelling as it is the only modelBD3-LMs includedthat supports KV caching during the diffusion phase. Trained with α0 {0.0625, 0.125, 0.25, 0.5, 1.0}, it shows smooth interpolation between AR and MDM perplexities  (Table 2)  . As with LM1B, Eso-LM (A) remains competitive with MDLM on OWT. 5.2 Generation Speed Our qualitative evaluation experiments show that Eso-LMs are 3 4 faster than prior diffusion based methods that support KV caching and about up to 65 faster than MDMs that dont support KV caching. Experimental Setup Diffusion models like MDLM do not support KV caching due to their use of bidirectional attention. Semi-autoregressive models such as BD3-LMs enable partial KV caching by generating blocks of tokens autoregressively and caching completed blocks, but they still lack caching for the current block under diffusion. In contrast, our method uniquely supports KV caching during the diffusion phase. We compare inference times of our method, Eso-LM (B), against MDLM and BD3-LMs with context lengths {2048, 8192}, using the first-hitting sampler [37], and batch size of 1. To simulate the worst-case scenario, we set to ensure all methods have approximately NFEs: = 1M for MDLM and Eso-LM (B), = 5000 (number of sampling steps per block) for BD3-LMs. We found that nucleus sampling yields non-negligible overhead for all methods, and hence disable it to focus on the relation between sampling speed and sequence length. 9 Table 1: Test perplexities (PPL; ) on LM1B. Reported in He et al. [7]. Denotes the dataset didnt incorporate sentence packing. Reported in Arriola et al. [1]. For diffusion models, we report the bound on the likelihood. Reported in Sahoo et al. [27]. Autoregressive Transformer Diffusion D3PM Uniform [2] D3PM Absorb [2] Diffusion-LM [12] DiffusionBert [7] SEDD Absorb [15] SEDD Uniform [15] MDLM [26] UDLM [29] DUO [27] PPL () 22.83 137.90 76.90 118.62 63.78 32.71 40.25 31.78 36.71 33.68 Interpolating diffusion and autoregressive BD3-LMs [1] = 16 = 8 = 4 Eso-LM (B) (Ours) α0 = 1.0 Eso-LM (A) (Ours) α0 = 1.0 α0 = 0.5 α0 = 0.25 α0 = 0.125 α0 = 0. 30.60 29.83 28.23 35.00 30.96 30.51 28.44 25.97 24.51 Table 2: Test perplexities (PPL; ) on OWT for models trained for 250K. Reported in [1]. Denotes retrained models. Intermediate checkpoints were provided by Sahoo et al. [26]. Autoregressive Transformer Diffusion SEDD Absorb [15] MDLM [26] UDLM [29] DUO [27] PPL () 17.90 26.81 25.76 30.52 27.14 Interpolating diffusion and autoregressive BD3-LMs [1] = Eso-LM (A) (Ours) α0 = 1 Eso-LM (B) (Ours) α0 = 1 α0 = 0.5 α0 = 0.25 α0 = 0.125 23.57 26. 30.14 27.85 24.73 21.87 Table 3: Sampling time () in seconds for sequence lengths {2048, 8192} with NFEs set to for all methods. Reported values are meanstd over 5 runs. Method = 2048 = 8192 AR (Oracle) 13.30. 54.00.2 MDLM BD3-LM (L = 4) BD3-LM (L = 16) Eso-LM (B) 201.30.4 24.30.7 21.30.1 14.60.3 5438.33.3 312.01.7 268.11.2 82.10.3 Results As shown in Table 3, as compared to MDLM which lacks KV caching, Eso-LM (B) is 14 faster for = 2048, and 65 faster for = 8192. Compared to BD3-LMs, which partially support caching, Eso-LM (B) is 3.2 faster than BD3-LM (L = 16) and 3.8 faster than BD3-LM (L = 4) at = 8192. This speedup stems from KV-caching and the scheduler that restricts the forward pass to the masked tokens that are supposed to be denoised and previously predicted tokens, avoiding redundant computationa feature BD3-LMs lack. As we restrict the NFEs to L, our method is slower than AR models due to delayed KV reuseonly possible from the penultimate denoising stepits overall efficiency still surpasses prior diffusion-based baselines (see Sec. 4.2.2). 5.3 Generation Quality Our sampling experiments show that (1) Eso-LMs establish new SOTA on the Pareto frontier of sampling speed and quality  (Fig. 4)  , (2) generate higher-quality samples than all previous diffusion models at high NFEs, and (3) dont suffer mode collapse at low NFEs unlike the previous interpolating diffusion method BD3-LMs  (Fig. 3)  . Experimental Setup We train Eso-LM (B) with αtrain {0.125, 0.25, 1} and generate samples with varying (αeval 0 , ) {0.0625, 0.25, 0.5, 1} {16, 128, 1024} to control NFEs (NFEs = S). MDLM and BD3-LMs use ancestral sampling as proposed in Sahoo et al. [26], with {16, 128, 256, 512, 1024, 4096} for MDLM and {16, 128, 256, 512, 1024, 2048, 4096} for BD3-LMs. All generations are = 1024 tokens long. BD3-LMs are evaluated with block sizes {4, 8, 16} and = /(1024/L). Consequently, = 128 is not applicable to BD3-LMs with = 4 and = 16 is not applicable to all BD3-LMs considered, since these would result in values 0 10 Figure 3: Gen. PPL vs. NFEs for models trained on OWT. Entropy values are in Tables (8, 9, 10). Figure 4: Eso-LM (B) establishes SOTA on the Pareto frontier of sampling speed and quality. less than 1. We measure Gen. Perplexity (via GPT-2 Large) for sample quality and average entropy for diversity [37], using nucleus sampling with = 0.9 [35]. Pareto frontier of Gen. PPL vs. NFEs As shown in Fig. 3, at high NFEs ( 1024), Eso-LM (B) outperforms MDLM and BD3-LMs and nearly matches AR model quality. As NFEs decrease, BD3-LMs degrade significantly due to their teacher-forced training: the future blocks are conditioned on clean prior blocks during training but during evalution, any errors in the prediction of prior blocks lead to significant degradation in sample quality. Eso-LMs avoid this issue. As expected, in the low NFE regime, higher αtrain = 1 performs best among αtrain = {0.125, 0.25, 1}, while remaining competitive with MDLM. Overall, Eso-LM (B) is competitive with MDLM at low NFEs and with AR models at high NFEsunlike BD3-LMs. (more diffusion during training) improves quality: αtrain 0 0 Pareto frontier of Gen. PPL vs. Speed To compare sampling efficiency, we record the median sampling duration in seconds (across 5 trials) taken by each method to generate single sample and plot Gen. PPL vs. sampling duration in Fig. 4. At the limit of infinite trials, median sampling duration is an increasing function of NFEs, modulated by the method and sampling hyperparameters used. For each method, we construct Pareto frontier over all its configurations: Eso-LM (B) (over αtrain , αeval 0 , and ), BD3-LM (over and ), and MDLM (over ). We find that Eso-LM (B) establishes new state of the art on the speed-quality Pareto frontier. Under low budgets on sampling time, models trained with αtrain = 0.125 models produce near-AR quality. 0 Intermediate values (e.g., αtrain = {0.25, 0.5}) offer the best trade-off between speed and quality. = 1 are the best; given high budgets, αtrain 0"
        },
        {
            "title": "6 Related Work, Discussion, and Conclusion",
            "content": "AR models AR models generate tokens left-to-right and remain state-of-the-art in quality, but suffer from slow, sequential inference and limited controllability. In contrast, Eso-LMs combines AR-like generation in sequential phase with any-order, parallel generation in an initial diffusion phase. During diffusion, Eso-LM (B) supports KV caching [23], previously exclusive to AR models, matching their inference speed. Its quality approaches AR models as the sequential phase increases. Masked diffusion MDMs [26, 30] generate multiple tokens per step but rely on expensive bidirectional attention. Eso-LMs improve their efficiency in two ways: Eso-LM (A) restricts attention to clean and scheduled-to-denoise tokens, and Eso-LM (B), uses the connection to AO-ARMs [20] for KV caching. Even without sequential phase, Eso-LM (A) is competitive while being more efficient. Though Eso-LM (B) may initially underperform as standalone diffusion model, adding sequential phase lets it surpass MDMs in both quality and speed. Block diffusion BD3-LMs [1] use AR over blocks of tokens and apply MDM within each. They interpolate between AR and MDMs by changing block size, whereas Eso-LMs interpolate by varying the proportion of diffusion generation α0. Both support KV caching differently: BD3-LMs cache block-level conditioning, while Eso-LM (B) caches clean-token KV values across denoising steps. BD3-LMs short blocks (L 16) increase token conflicts [13]; poor samples in one block also 11 severely affect the sample quality of subsequent blocks due to the use of teacher forcing during training. Eso-LMs do not suffer from this problem. Conclusion We introduce new paradigm for language modeling that seamlessly fuses autoregressive (AR) models and masked diffusion models (MDMs), enabling effective interpolation between the two in both sample quality and generation speed. Our method is the first to enable KV caching in MDMs while preserving parallel generation, significantly accelerating inference. It outperforms block diffusion methods in both speed and accuracy, setting new state of the art on language modeling benchmarks. Given we are working on language modeling, we carry the inherent risks and opportunities in this line of research."
        },
        {
            "title": "References",
            "content": "[1] Marianne Arriola, Subham Sekhar Sahoo, Aaron Gokaslan, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Justin Chiu, and Volodymyr Kuleshov. Block diffusion: Interpolating between autoregressive In The Thirteenth International Conference on Learning and diffusion language models. Representations, 2025. URL https://openreview.net/forum?id=tyEyYT267x. [2] Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:1798117993, 2021. [3] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. One billion word benchmark for measuring progress in statistical language modeling, 2014. [4] Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. discourse-aware attention model for abstractive summarization of long documents. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), 2018. doi: 10.18653/v1/n18-2097. URL http://dx.doi.org/10.18653/ v1/n18-2097. [5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. [6] Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. Openwebtext corpus. http://Skylion007.github.io/OpenWebTextCorpus, 2019. [7] Zhengfu He, Tianxiang Sun, Kuanning Wang, Xuanjing Huang, and Xipeng Qiu. Diffusionbert: Improving generative masked language models with diffusion models. arXiv preprint arXiv:2211.15029, 2022. [8] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [9] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Advances in neural information processing systems, 34:2169621707, 2021. [10] Diederik Kingma and Max Welling. Auto-encoding variational {Bayes}. In ICLR, 2014. [11] Seul Lee, Karsten Kreis, Srimukh Prasad Veccham, Meng Liu, Danny Reidenbach, Yuxing Peng, Saee Paliwal, Weili Nie, and Arash Vahdat. Genmol: drug discovery generalist with discrete diffusion. arXiv preprint arXiv:2501.06158, 2025. [12] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. Diffusion-lm improves controllable text generation. Advances in Neural Information Processing Systems, 35:43284343, 2022. [13] Anji Liu, Oliver Broadrick, Mathias Niepert, and Guy Van den Broeck. Discrete copula diffusion. arXiv preprint arXiv:2410.01949, 2024. [14] Chengyi Liu, Wenqi Fan, Yunqing Liu, Jiatong Li, Hang Li, Hui Liu, Jiliang Tang, and Qing Li. Generative diffusion models on graphs: Methods and applications. arXiv preprint arXiv:2302.02591, 2023. [15] Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834, 2024. [16] Mitch Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313330, 1993. [17] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016. [18] Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025. [19] Hunter Nisonoff, Junhao Xiong, Stephan Allenspach, and Jennifer Listgarten. Unlocking guidance for discrete state-space diffusion and flow models. arXiv preprint arXiv:2406.01572, 2024. [20] Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan Li. Your absorbing discrete diffusion secretly models the conditional distributions of clean data. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=sMyXP8Tanm. [21] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The LAMBADA In Proceedings of the 54th dataset: Word prediction requiring broad discourse context. Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 15251534, Berlin, Germany, August 2016. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/P16-1144. [22] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. [23] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference, 2022. URL https://arxiv.org/abs/2211.05102. [24] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. [25] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. [26] Subham Sekhar Sahoo, Marianne Arriola, Aaron Gokaslan, Edgar Mariano Marroquin, Alexander Rush, Yair Schiff, Justin Chiu, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=L4uaAR4ArM. [27] Subham Sekhar Sahoo, Justin Deschenaux, Aaron Gokaslan, Guanghan Wang, Justin Chiu, and Volodymyr Kuleshov. The diffusion duality. In ICLR 2025 Workshop on Deep Generative Model in Machine Learning: Theory, Principle and Efficacy, 2025. URL https://openreview.net/forum?id=CB0Ub2yXjC. [28] Yair Schiff, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, and Volodymyr Kuleshov. Caduceus: Bi-directional equivariant long-range dna sequence modeling. arXiv preprint arXiv:2403.03234, 2024. 13 [29] Yair Schiff, Subham Sekhar Sahoo, Hao Phung, Guanghan Wang, Sam Boshar, Hugo Dallatorre, Bernardo de Almeida, Alexander Rush, Thomas PIERROT, and Volodymyr Kuleshov. Simple guidance mechanisms for discrete diffusion models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum? id=i5MrJ6g5G1. [30] Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis K. Titsias. Simplified and generalized masked diffusion for discrete data, 2025. URL https://arxiv.org/abs/2406. 04329. [31] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pp. 22562265. PMLR, 2015. [32] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. [33] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021. [34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [35] Guanghan Wang, Yair Schiff, Subham Sahoo, and Volodymyr Kuleshov. Remasking discrete diffusion models with inference-time scaling. arXiv preprint arXiv:2503.00307, 2025. [36] Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In NIPS, 2015. [37] Kaiwen Zheng, Yongxin Chen, Hanzi Mao, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Masked diffusion models are secretly time-agnostic masked models and exploit inaccurate categorical sampling. arXiv preprint arXiv:2409.02908, 2024."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Background"
        },
        {
            "title": "2.3 Block Denoising Diffusion Discrete Language Models . . . . . . . . . . . . . . . . . .",
            "content": "3 Esoteric Language Models"
        },
        {
            "title": "4 Designing Attention Mechanisms for the Shared Denoising Transformer",
            "content": "4.1 Eso-LM (A) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Eso-LM (B) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Experiments 5.1 Likelihood Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Generation Speed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 3 3 3 4 5 5 6 8 9 9 5.3 Generation Quality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 6 Related Work, Discussion, and Conclusion Appendices 11 Appendix Background 16 A.1 BD3-LMs hyperparameter nd num_tries . . . . . . . . . . . . . . . . . . . . . . . 16 Appendix Esoteric Language Models 17 B.1 MDM loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 B.2 Training procedure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Attention biases for the sequential phase . . . . . . . . . . . . . . . . . . . . . . . . . . 18 B.4 Sampling schedule and sampler . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 B.5 Implementation details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 B.6 Experimental details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 Appendix Additional Experiments C.1 Ablation study on split proportion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 C.2 OWT Perplexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 C.3 Zero-Shot Likelihood Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 C.4 Gen. Perplexities and Entropies of Generated Samples by Models Trained on OWT ."
        },
        {
            "title": "Appendix A Background",
            "content": "A.1 BD3-LMs hyperparameter nd num_tries In the original codebase of BD3-LMs [1], the number of diffusion sampling steps for each block is set to 5000. This is an extremely high considering the fact that the number of tokens in each block is at most 16. Having 16 and = 5000 means that off-the-shelf BD3-LMs are not performing parallel generation because tokens are denoised one at time. Further, we found that BD3-LMs codebase cherry-picks its samples. More specifically, to generate single sample, the codebase keeps generating new samples (up to num_tries times) until one sample passes some quality-control test. By default, num_tries = 10 and the codebase reports sampling failure when the 10 tries are exhausted with no samples passing the test. Empirically, we found that sampling failures dont occur for = 5000. To investigate the true performance of BD3-LMs for parallel generation, we set num_tries = 1, disable the quality-control test and evaluate samples from BD3-LMs across wide range of values (Figure 5). Here and in Figure 5, means the sum of sampling steps across all blocks for BD3-LMs, e.g., = 16 and = 4096 means that = 4096/(1024/16)) = 64 sampling steps is used per block. In contrast, BD3-LMs codebase uses = 5000 by default, which corresponds to = in Figure 5. For MDLM, can be interpreted normally because it has no blocks. As shown in Figure 5, as is decreased to enable more parallel generation, both sample quality and sample diversity of BD3-LMs becomes significantly worse than MDLM. This drop in performance is due to the mode collapse issue as discussed in Section 6. We also found that increasing num_tries can somewhat improve the sample entropy of BD3-LMs (second row of Table 4) and mitigate mode collapse, but doing so provides less or no improvements for AR and MDLM. Figure 5: Gen. Perplexity () with nucleus sampling (p = 0.9) against the number of sampling steps for AR, MDLM and BD3-LMs trained for 1 million steps. The number of sampling steps for AR is always 1024; we extend it to other values for easier comparison. The number next to each data point records its sample entropy (); value less than 5 usually indicates mode collapse. Table 4: Gen. PPL () and entropies () (in parenethesis) for BD3-LMs = 16, MDLM, and AR with nucleus sampling = 0.9. We observe that the num_tries parameter introduced in [1] for BD3-LMs selectively helps BD3-LMs but not the baselines. AR is not affected by . BD3LM MDLM AR num_tries = 1024 = 256 1 72.80 (5.35) 356.02 (5.11) 10 77.71 (5.41) 440.69 (5.28) 1 41.92 (5.36) 45.07 (5.40) 10 41.79 (5.37) 44.57 (5.39) 1 13.03 (5.26) 13.03 (5.26) 10 13.76 (5.32) 13.76 (5.32)"
        },
        {
            "title": "Appendix B Esoteric Language Models",
            "content": "B.1 MDM loss The NLL is given as: log pθ(x) Ez0q0(.x) log pAR θ (xz0) + DKL(q0(z0x)pMDM θ (z0)) = Ez0q0(.x) ℓM(z0) log pAR θ (xℓz0, x<ℓ) + DKL(q0(z0x)pMDM θ (z0)). (39) Note that z0 may contain clean tokens at indices exceeding the index ℓ. As discussed in Sec. 3.1, the AR log-likelihood is given as: ℓM(z0) log pAR θ (xℓz0, x<ℓ) = ℓM(z0) logxℓ θ(z0 x<ℓ), xℓ, (40) where we compute the loss only at the masked indices M(z0). To compute the KL term in (39), we define masked diffusion process over z0. For this diffusion process, its forward marginal zℓ qt(xℓ) at time [0, 1] is the same as (1) but uses noise schedule with scaled-down range (αt)t[0,1] [0, α0], strictly decreasing function in with αt=0 = α0 such that zt=0 = z0 and αt=1 = 0 such that zt=1 = m1L. With diffusion steps, we have: DKL(q0(z0x)pMDM θ (z0)) = Ez0 [log q0(z0x) pMDM (z0) θ ] = Ez0 [log Ez Ez01 [log ]] 1 [ q(z01x) pMDM (z01) θ q(z01x) pMDM (z01) θ ] t{ 1 log ,...,1} , 2 Ezt [DKL(q(zt 1 = Ez01 = q(zt 1 pMDM θ zt, x) zt) (zt 1 zt, x)pMDM θ (zt 1 zt))] Sahoo et al. [26] show that, as , the above simplifies to: = EtU [0,1],ztqt α 1 αt ℓM(zt) logxℓ θ(zt), xℓ . (41) Finally, combining (40) and (41), we get the desired result: LNELBO(x; θ) = Ez0q0 ℓM(z0) logxℓ θ(z0 x<ℓ), xℓ AR loss + t=1 α 1 αt ℓM(zt) MDM loss θ(zt), xℓ Eztqt logxℓ t=0 dt . 17 (42) B.2 Training procedure Algo. 1 outlines the complete training procedure. Algorithm 1 Eso-LMs Training Input: dataset D, batch size bs, forward noise process qt(x), model xθ, learning rate η while not converged do x1, x2, . . . , xbs for 1 to bs/2 do z0 q0(x) σ PL with constraints Li ℓM(z0) logxℓ Used to construct the attention bias in xθ (Sec. 4) Estimator of Sequential Loss in (7) If α0 = 1, loop through 1 to bs. θ(z0, x<ℓ), xℓ If α0 = 1, skip this loop. Used to construct the attention bias in xθ (Sec. 4) θ(zt), xℓ Estimator of MDM Loss in (7) end for for bs/2 + 1 to bs do Sample U[0, 1] zt qt(x) σ PL with constraints Li α 1αt ℓM(zt) logxℓ end for θ θ ηθ bs i=1 Li end while B.3 Attention biases for the sequential phase Fig. 6 shows the comparison of attention biases for sequential-phase training. Figure 6: Comparison of attention biases for sequential-phase training. Orange represents 0 (attention) and gray represents (no attention). The clean sequence is = (A, B, C, D, E, ) and hence = 6. After random masking, we obtain z0 = (A, M, C, M, M, ). The integers denote the position indices with M(z0) = {2, 4, 5} and C(z0) = {1, 3, 6}. The random ordering among C(z0) is (3, 1, 6). Red highlights the differences between Eso-LM (A) and Eso-LM (B). B.4 Sampling schedule and sampler Eso-LMs perform two phases of sampling: the diffusion phase and the sequential phase. Within the diffusion phase, tokens are denoised in random order and potentially in parallel. Within the sequential phase, remaining mask tokens are denoised sequentially from left to right and one at time. First, to determine (i) the total number of tokens to denoise during the diffusion phase and (ii) the number of tokens to denoise per diffusion step, we run modified version of the first-hitting algorithm proposed in [37]. Suppose the sequence to generate has length L, the number of discretization steps is , and the noise schedule is α (with α0 0). Let dt = 1/T . We iterate from = 1 to 1 dt (inclusive) 18 for steps. For each step, we compute the number of tokens to denoise at time as nt = Binom (n = nremaining , = αs αt 1 αt ) , (43) where = dt and nremaining = t>t nt. When is large, some nts could be zero. All the nts produced by this algorithm are collected in an ordered list, except for the nts that are zeros. We denote the sum of all nts as nMDM and define nAR = nMDM. We select nMDM token indices from [L] to denoise by diffusion and use the complementing subset of token indices to denoise sequentially. For example, suppose = 8 and the token indices are [1, 2, . . . , 8]. Suppose we obtained nMDM = 5 from the algorithm above. Then, the diffusion indices we may select are (1, 3, 4, 6, 7) and the complementing sequential indices are (2, 5, 8). We further randomly permute the diffusion indices to be, e.g., (3, 1, 6, 4, 7), for random-order denoising. Given the list of non-zero nts and the permuted ordered set of diffusion indices, we create the sampling schedule for diffusion by partitioning the diffusion indices per the nts. Suppose the list of non-zero nts is (2, 1, 2). Using it to partition the permuted set of diffusion indices (3, 1, 6, 4, 7), we obtain the following sampling schedule for the diffusion phase: MDM = ((3, 1), (6), (4, 7)). The denoising schedule for the sequential phase is simply AR = ((2), (5), (8)). The unified sampling schedule is the concatenation of MDM and AR. In this example, = (S1, S2, S3, S4, S5, S6) where S1 = (3, 1), S2 = (6), S3 = (4, 7), S4 = (2), S5 = (5) and S6 = (8). This corresponds to 6 NFEs. Finally, is passed to Algo. 2, which handles the rest of the sampling procedure. Connecting back to the denoising ordering σ discussed in Sec. 4.1.2 and Sec. 4.2.2, we have σ = (3, 1, 6, 4, 7, 2, 5, 8) in this example. Algorithm 2 Eso-LMs Sampling Input: sequence length L, unified sampling schedule = [MASK_INDEX, . . . , MASK_INDEX] = {} for 1 to do logits xθ(z[C Si]) logits select logits corresponding to Si z[Si] categorical_sample(logits, dim=-1) Si Indices of clean tokens Sequential happens automatically when nMDM See Remark logits has shape (Si, V) end for Return: Remark. z[C Si] denotes the subset of the tokens in that are fed into the denoising model xθ. The position embeddings for token zℓ z[C Si] is ensured to be the same as that in the original sequence z. Refer to Sec. 4.1.2 and Sec. 4.2.2 for computing the sampling attention bias for Eso-LM (A) and Eso-LM (B) respectively. For Eso-LM (B), due to the use of causal attention, xθ is able to cache the KV-values of clean token the first time it is processed. B.5 Implementation details B.5.1 Training Diffusion Phase For both Eso-LM (A) and Eso-LM (B), we sort the input zt as well as the rows and columns of by σ. This results in classic attention biases  (Fig. 7)  that can be efficiently implemented. We also sort the positional embeddings of zt by σ so tokens keep their original positional embeddings. When calculating loss, we additionally sort the target by σ. Sequential Phase For both Eso-LM (A) and Eso-LM (B), we sort z0, (before concatenating them as the input z0 x), and the rows and columns of each of the four blocks in by σ. We further add specific extra attention connections for clean tokens in z0 they dont affect the transformer output because no other token attends to clean tokens in z0. These two transformations result in attention biases with sparse and patterned block structures  (Fig. 8)  that can be efficiently implemented. We also sort the positional embeddings of z0 and by σ so tokens keep their original positional embeddings. When calculating loss, we additionally sort the target by σ. 19 Figure 7: Comparison of attention biases for diffusion-phase training, before and after sorting the rows and columns by σ. Orange represents 0 (attention) and gray represents (no attention). The clean sequence is = (A, B, C, D, E, ) and hence = 6. After random masking, we obtain zt = (A, M, C, M, M, ). The integers denote the position indices with M(zt) = {2, 4, 5} and C(zt) = {1, 3, 6}. The random ordering sampled is σ = (3, 1, 6, 4, 5, 2) P6 with clean tokens before mask tokens. Red highlights the differences between Eso-LM (A) and Eso-LM (B). Figure 8: Comparison of attention biases for sequential-phase training, before and after sorting the rows and columns of each of the four blocks by σ. Orange represents 0 (attention) and gray represents (no attention). The clean sequence is = (A, B, C, D, E, ) and hence = 6. After random masking, we obtain z0 = (A, M, C, M, M, ). The integers denote the position indices with M(z0) = {2, 4, 5} and C(z0) = {1, 3, 6}. The random ordering among C(z0) is (3, 1, 6). Red highlights the differences between Eso-LM (A) and Eso-LM (B). Green highlights the extra connections added for clean tokens in z0 so that the attention biases display useful patterns after sorting they dont affect the transformer output because no other token attends to clean tokens in z0. B.5.2 Sampling Fig. 9 shows how the sampling procedure of Eso-LM (A) generates an example sequence. Fig. 10 shows how the sampling procedure of Eso-LM (B) generates an example sequence. B.6 Experimental details B.6.1 Low discrepancy sampler To reduce variance during training we use low-discrepancy sampler, similar to that proposed Kingma et al. [9]. Specifically, when processing minibatch of samples, instead of independently sampling from uniform distribution, we partition the unit interval and sample the time step for each sequence {1, . . . , } from different portion of the interval ti [ i1 ]. This ensures that our sampled timesteps are more evenly spaced across the interval [0, 1], reducing the variance of the ELBO. , B.6.2 Likelihood evaluation We use single monte-carlo estimate for to evaluate the likelihood. We use low discrepancy sampler [9] to reduce the variance of the estimate. 20 Figure 9: Efficient generation of an example sequence with Eso-LM (A). During Diffusion Phase, Eso-LMs denoise one or more, potentially non-neighboring mask tokens (M) per step. During Sequential Phase, Eso-LMs denoise the remaining mask tokens one at time from left to right. Eso-LM (A) doesnt process mask tokens not scheduled for denoising and allows for KV caching in sequential phase: blue bounding boxes enclose transformer cells that are building their KV cache; cell becomes blue once its KV cache is built. The sequences below the transformers depict tokens in their natural order. Figure 10: (Copy of Fig. 1) Efficient generation of an example sequence with Eso-LM (B). During Diffusion Phase, Eso-LMs denoise one or more, potentially non-neighboring mask tokens (M) per step. During Sequential Phase, Eso-LMs denoise the remaining mask tokens one at time from left to right. Eso-LM (B) allows for KV caching in both phases using just single unified KV cache: blue bounding boxes enclose transformer cells that are building their KV cache; cell becomes blue once its KV cache is built. The sequences below the transformers depict tokens in their natural order. B.6.3 Language modeling We detokenize the One Billion Words dataset following [15, 26], whose code can be found here1. We tokenize the One Billion Words dataset with the bert-base-uncased tokenizer, following Austin et al. [2], He et al. [7]. We concatenate and wrap sequences (also known as sequence packing) to length of 128 [25]. When wrapping, we add the [CLS] token in-between concatenated sequences. The final preprocessed sequences also have the [CLS] token as their first and last token. We tokenize OpenWebText with the GPT2 tokenizer. We concatenate and wrap them to length of 1,024. When wrapping, we add the eos token in-between concatenated sequences. Unlike for One Billion Words, the final preprocessed sequences for OpenWebText do not have special tokens as their first and last token. Since OpenWebText does not have validation split, we leave the last 100k docs as validation. Eso-LMs shares the same parameterization as our autoregressive baselines, SEDD, MDLM, UDLM, and DUO: modified diffusion transformer architecture [22] from Lou et al. [15], Sahoo et al. [26]. We use 12 layers, hidden dimension of 768, 12 attention heads. Eso-LMs do not use timestep embedding used in uniform diffusion models (SEDD Uniform, UDLM, DUO). Word embeddings are not tied between the input and output. We train BD3-LMs using the original code provided by their authors. 1https://github.com/louaaron/Score-Entropy-Discrete-Diffusion/blob/main/data.py 21 We use the log-linear noise schedule αt = α0(1 t). We use the AdamW optimizer with batch size of 512, constant learning rate warmup from 0 to learning rate of 3e-4 for 2,500 steps. We use constant learning rate for 1M steps on One Billion Words and for 250K steps for OpenWebText. We use dropout rate of 0.1. We train models on H200 GPUs. On One Billon Words for 1M steps, training takes 21 hours when α0 = 1 and 34 hours when α0 < 1. On OpenWebText for 250K steps, training takes 50 hours when α0 = 1 and 69 hours when α0 < 1."
        },
        {
            "title": "Appendix C Additional Experiments",
            "content": "C.1 Ablation study on split proportion See Table 5. Table 5: Test perplexities () on LM1B for Eso-LMs (A) trained for 500K vs. the proportion of examples in each batch used for evaluating the MDM loss in (7) during training. Remaining examples in each batch are used for evaluating the AR loss in (7) during training. 0.75 0.5 0. 0.125 Eso-LM (A) α0 = 0.5 α0 = 0.25 α0 = 0.125 α0 = 0.0625 32.25 30.49 27.76 25.92 31.53 Diverged Diverged 29.33 Diverged Diverged 26.73 Diverged Diverged 25.07 Diverged Diverged C.2 OWT Perplexity See Table 6. Table 6: Test perplexities (PPL; ) on OWT for models trained for 250K and 1M steps. We report bounds for diffusion models and interpolation methods. Reported in [1]. Denotes retrained models. Intermediate checkpoints were provided by Sahoo et al. [27], Arriola et al. [1]. Autoregressive Transformer Diffusion SEDD Absorb [15] MDLM [26] UDLM [29] DUO [27] PPL 250K () PPL 1M () 17.90 26.81 25.76 30.52 27.14 17.54 24.10 22.98 27.43 25.20 Interpolating diffusion and autoregressive BD3-LMs [1] = Eso-LM (A) (Ours) α0 = 1 Eso-LM (B) (Ours) α0 = 1 α0 = 0.5 α0 = 0.25 α0 = 0.125 23.57 22.27 26.21 30.14 27.85 24.73 21.87 - - - - - C.3 Zero-Shot Likelihood Evaluation We also explore models ability to generalize by taking models trained on OWT and evaluating how well they model unseen datasets  (Table 7)  . We compare the perplexities of our Eso-LMs with SEDD 22 [2], MDLM [26], BD3-LMs [1], and an AR Transformer language model. Our zero-shot datasets include the validation splits of Penn Tree Bank (PTB; [16]), Wikitext [17], LM1B, Lambada [21], AG News [36], and Scientific Papers (Pubmed and Arxiv subsets; [4]). Table 7: Zero-shot perplexities () of models trained for 250K steps on OWT. We report bounds for diffusion models and interpolation methods. Numbers for AR were taken from [1]. PTB Wikitext LM1B Lambada AG News Pubmed Arxiv AR MDLM SEDD Absorb BD3-LM (L = 16) Eso-LM (B) (Ours) α0 = 1 α0 = 0.5 α0 = 0.25 α0 = 0.125 81.07 93.82 99.59 90.63 126.29 110.70 105.19 97. 25.32 36.89 38.55 33.14 45.08 39.57 37.32 35.65 51.14 69.45 72.51 64.88 82.01 75.75 67.69 60. 52.13 53.05 52.16 53.09 61.37 57.33 60.15 69.13 52.11 67.33 72.62 62.5 98.22 86.65 75.74 65. 48.59 49.47 47.07 43.25 62.37 60.20 62.45 65.27 41.22 43.84 41.18 39.82 55.76 53.78 55.31 57. C.4 Gen. Perplexities and Entropies of Generated Samples by Models Trained on OWT In Fig. 3 we study how the sample quality changes by varying NFEs. The numerical values for Gen. PPL and entropy can be found in Table 8 (Eso-LMs), Table 9 (MDLM), and Table 10 (BD3-LMs). Table 8: Gen. PPL () and entropies () of samples by Eso-LM (B) trained for 250K steps on OWT. αtrain 0 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 αeval 0.0625 0.0625 0.0625 0.25 0.25 0.25 1.0 1.0 1.0 0.0625 0.0625 0.0625 0.25 0.25 0.25 1.0 1.0 1.0 0.0625 0.0625 0.0625 0.25 0.25 0.25 1.0 1.0 1.0 16 128 1024 16 128 1024 16 128 1024 16 128 1024 16 128 1024 16 128 1024 16 128 1024 16 128 1024 16 128 1024 NFE Gen. PPL () Entropy () 23.15 25.51 26.85 46.68 44.19 49.55 279.63 151.77 159.31 23.95 26.13 27.96 42.31 38.48 43.71 152.16 72.74 72.36 31.33 30.9 31.11 50.14 43.59 43.6 118.92 54.36 49.4 5.37 5.41 5.43 5.45 5.42 5.47 5.54 5.4 5.42 5.36 5.39 5.41 5.43 5.39 5.44 5.55 5.4 5.42 5.21 5.19 5.19 5.4 5.33 5.32 5.53 5.37 5.33 977.18 1011.37 1022.13 806.47 896.41 999.24 16.0 127.96 647.06 977.18 1011.37 1022.13 806.47 896.41 999.24 16.0 127.96 647.06 977.18 1011.37 1022.13 806.47 896.41 999.24 16.0 127.96 647.06 23 Table 9: Gen. PPL () and entropies () of samples by MDLM trained for 1M steps on OWT. 16 128 256 512 1024 4096 NFE Gen. PPL () Entropy () 16 127.96 251.35 442.87 647.91 906.32 148.08 48.66 45.07 42.58 41.92 41. 5.61 5.43 5.4 5.37 5.36 5.36 Table 10: Gen. PPL () and entropies () of samples by BD3-LM trained for 1M steps on OWT. Block size NFE Gen. PPL () Entropy () 4 4 4 4 4 8 8 8 8 8 8 16 16 16 16 16 16 4096 2048 1024 512 256 4096 2048 1024 512 256 128 4096 2048 1024 512 256 128 925.32 830.93 672.91 480.21 256.0 915.14 814.07 646.69 439.32 254.9 128 910.37 811.01 642.25 427.46 245.77 128 41.04 88.4 199.99 186.05 199.22 39.88 59.84 137.99 258.95 278.57 466.1 38.74 47.02 73.84 189.26 351.63 447.24 5.31 5.19 4.79 4.23 4.39 5.33 5.34 5.24 4.92 4.39 4.91 5.33 5.34 5.36 5.33 5.1 4."
        }
    ],
    "affiliations": [
        "Cornell Tech",
        "Cornell University",
        "MBZUAI",
        "NVIDIA"
    ]
}