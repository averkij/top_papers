{
    "paper_title": "Asking like Socrates: Socrates helps VLMs understand remote sensing images",
    "authors": [
        "Run Shao",
        "Ziyu Li",
        "Zhaoyang Zhang",
        "Linrui Xu",
        "Xinran He",
        "Hongyuan Yuan",
        "Bolei He",
        "Yongxing Dai",
        "Yiming Yan",
        "Yijun Chen",
        "Wang Guo",
        "Haifeng Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent multimodal reasoning models, inspired by DeepSeek-R1, have significantly advanced vision-language systems. However, in remote sensing (RS) tasks, we observe widespread pseudo reasoning: models narrate the process of reasoning rather than genuinely reason toward the correct answer based on visual evidence. We attribute this to the Glance Effect, where a single, coarse perception of large-scale RS imagery results in incomplete understanding and reasoning based on linguistic self-consistency instead of visual evidence. To address this, we propose RS-EoT (Remote Sensing Evidence-of-Thought), a language-driven, iterative visual evidence-seeking paradigm. To instill this paradigm, we propose SocraticAgent, a self-play multi-agent system that synthesizes reasoning traces via alternating cycles of reasoning and visual inspection. To enhance and generalize these patterns, we propose a two-stage progressive RL strategy: first, RL on fine-grained Grounding tasks to enhance RS-EoT capabilities, followed by RL on RS VQA to generalize to broader understanding scenarios. Experiments show RS-EoT achieves state-of-the-art performance on multiple RS VQA and grounding benchmarks. Analyses reveal clear iterative cycles of reasoning and evidence seeking, confirming RS-EoT mitigates the Glance Effect and enables genuine evidence-grounded reasoning. Our code, data, and models are available at https://geox-lab.github.io/Asking_like_Socrates"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 6 9 3 2 2 . 1 1 5 2 : r Asking like Socrates: Socrates helps VLMs understand remote sensing images Run Shao1,2, Ziyu Li1, Zhaoyang Zhang1 Linrui Xu1 Xinran He2 Hongyuan Yuan1,2 Bolei He2 Yongxing Dai2 Yiming Yan3 Yijun Chen3 Wang Guo1 Haifeng Li1, 1School of Geosciences and Info-Physics, Central South University, Changsha, China 2Baidu Inc., Beijing, China 3School of Earth Sciences, Zhejiang University, Hangzhou, China Figure 1. Illustration of the pseudo reasoning problem and our RS-EoT solution. (a) Existing models show pseudo reasoning: explicit thinking (blue bars) degrades performance below the non-reasoning base model (red bar). (b) We attribute this to the Glance Effectreasoning based on single, coarse perception. We propose RS-EoT, an iterative evidence-seeking loop. (c) Our model, RS-EoT-7B, successfully solves the task by iteratively reasoning and seeking visual evidence."
        },
        {
            "title": "Abstract",
            "content": "inspired reasoning models, Recent multimodal by DeepSeek-R1, have significantly advanced visionlanguage systems. However, in remote sensing (RS) tasks, we observe widespread pseudo reasoning: models narrate the process of reasoning rather than genuinely reason toward the correct answer based on visual evidence. We attribute this to the Glance Effect, where single, coarse perception of large-scale RS imagery results in incomplete understanding and reasoning based on linguistic self-consistency instead of visual evidence. To address this, we propose RS-EoT (Remote Sensing Evidence-of-Thought), language-driven, iterative visual evidence-seeking paradigm. To instill this paradigm, we propose SocraticAgent, self-play reasoning traces multi-agent system that synthesizes inspecvia alternating cycles of reasoning and visual tion. To enhance and generalize these patterns, we propose two-stage progressive RL strategy: first, RL on fine-grained Grounding tasks to enhance RS-EoT capabilities, followed by RL on RS VQA to generalize to broader understanding scenarios. Experiments show RS-EoT achieves state-of-the-art performance on multiple RS VQA and grounding benchmarks. Analyses reveal clear iterative cycles of reasoning and evidence seeking, confirming RS-EoT mitigates the Glance Effect and enables genuine evidence-grounded reasoning. Our code, data, and models are available at https : // geox - lab.github.io/Asking_like_Socrates. 1. Introduction *Equal contribution. Corresponding author. Recent deep reasoning models, such as DeepSeek-R1 [8], Qwen3 [41], and Doubao-Seed-1.6 [2], show the SFTRL 1 paradigm substantially enhances long-chain reasoning [10, 23, 32, 39], enabling breakthroughs in math, code, and agent planning. This paradigm was then extended to multimodal settings, training visionlanguage models to generate structured reasoning [3, 10, 23, 34, 42, 43]. This progress has significantly advanced multimodal systems, stimulating interest in applying deep reasoning to complex visual domains [19, 25, 30, 45, 47]. However, owing to the unique properties of remote sensing (RS) imagery, such as wide spatial extents, large scale variations, and sparse and subtle visual clues, these models often exhibit pseudo reasoning [15, 30] in RS tasks (Fig. 1a). Despite producing explicit reasoning chains, their performance on RS tasks shows no gain [3, 6, 7, 28], and even degrades compared to the base model. In other words, they merely describe reasoning process rather than performing one. We attribute this to the Glance Effect (Fig. 1b). Current models perform only single, coarse perception pass (a glance) before reasoning. This is insufficient, leading to reasoning built on incomplete visual evidence. Consequently, the model drifts toward linguistically self-consistent narratives rather than evidence-based logic, which can even degrade performance. This suggests RS reasoning requires iterative, not static, evidence acquisition [18, 30]. Human analysts exemplify this, using repeated cycles of inspection and refinement. To emulate this, models must adopt paradigm where reasoning guides perception to dynamically seek new visual evidence, rather than relying on fixed initial view. To this end, we propose RS-EoT (Remote Sensing Evidence-of-Thought), language-driven iterative evidence-seeking reasoning paradigm. It frames reasoning as reasoningperception loop where the model continuously revisits the image, seeking new visual cues guided by the evolving reasoning. Thus, perceptual grounding is progressively refined, not fixed at the outset. To instill RSEoT, we follow Deepseek-R1s methodology: first, Supervised Fine-Tuning (SFT) cold-starts the reasoning mode, and then Reinforcement Learning (RL) enhances and generalizes these patterns. [8, 10, 23, 43]. In the SFT phase, we propose novel data synthesis method called SocraticAgent to equip the model with the aforementioned self-iterative evidence-seeking capability. Inspired by the Socratic Method, which fosters knowledge acquisition through step-by-step questioning process rather than direct instruction, we design SocraticAgent as self-play multi-agent system to synthesize RS-EoT reasoning traces. Specifically, SocraticAgent contains Reasoner and Perceiver. The former is responsible for purelanguage reasoning and posing perceptual questions about the image as necessitated by the reasoning, while the latter is responsible for perceiving the image and answering the questions posed by the Reasoner. They engage in multiturn dialogue, simulating the iterative reasoning-perception loop. Through Socratic self-play strategy, wherein each agent is prompted with the presumed limited capability of the other, the Reasoner is guided to pose simple, incremental questions and the Perceiver to return accurate yet concise results, ensuring detailed and progressive traces. This synthesis method yielded the RS-EoT-4K dataset, which is used for the initial SFT cold-start to instill this self-iterative evidence-seeking reasoning in the model. In the RL stage, we propose two-stage progressive training pipeline to further enhance and generalize the RSEoT capability. First, adhering to the principle that iron sharpens iron, we perform RL on fine-grained grounding tasks, which are precisely those that most demand finegrained visual evidence, to specifically enhance the RS-EoT capability. Building on this foundation, we then conduct RL on general RS VQA tasks to generalize this capability to broader remote sensing scenarios. However, considering that existing RS VQA data is often simple (e.g., Yes/No types), it is highly susceptible to reward hacking and thus unsuitable for RL training. To address this, we propose multiple-choice VQA reconstruction strategy and tailored reward function, enabling stable RL training. Experiments show RS-EoT achieves state-of-the-art performance on multiple RS VQA and grounding benchmarks. Case studies and attention analyses confirm the model employs clear iterative cycles of reasoning and evidence seeking, validating the effectiveness of RS-EoT. Our main contributions are summarized as follows: We propose RS-EoT (Remote Sensing Evidence-ofThought), language-driven iterative evidence-seeking reasoning paradigm for remote sensing. We introduce complete framework to instill this (1) SocraticAgent to synthesize the RSparadigm: EoT-4K dataset for SFT cold-starting. (2) two-stage progressive Grounding+VQA RL strategy, featuring novel VQA data reconstruction method that enables stable RL training on simple RS VQA datasets. We train RS-EoT-7B, which achieves SOTA on multiple RS VQA and grounding benchmarks. Analyses confirm it mitigates the Glance Effect via clear iterative reasoning and evidence-seeking cycles. 2. Related Work 2.1. Vision-language Model for Remote Sensing The automated interpretation of RS images is crucial for applications like environmental monitoring, urban planning, and disaster response [11, 18]. This has spurred development of VLMs for RS, with models like GeoChat [14], Skysense [9], and RingmoGPT [36] demonstrating strong image understanding capabilities. More recently, the field has advanced toward complex reasoning, with models like Geo2 R1 [40] and VHM-RL [13] adopting the SFT-RL paradigm to output explicit reasoning chains. However, key issue persists: these models often rely on single-pass, global perception, neglecting detailed information and regional cues. This is particularly problematic for RS images with their wide geographical ranges and large scale variations. In contrast, our work introduces RSEoT, an iterative evidence-seeking paradigm that replaces this single-pass approach with dynamic loop of linguistic deduction and targeted visual inspection. 2.2. Large Language Reasoning Model Reasoning in LLMs was reshaped by the Chain-of-Thought (CoT) paradigm [37], which showed that eliciting multistep traces improves complex reasoning. This inspired variants like Least-to-Most [50], Program-of-Thought [4], and advanced models like OpenAIs o1 [12]. pivotal development is the SFTRL paradigm, popularized by DeepSeek-R1 [8]. This approach uses SFT to cold-start reasoning patterns, then applies RL to refine and generalize them. Algorithms like Group Relative Policy Optimization (GRPO) [27] and its variants [39, 44] offer more stable optimization for long-sequence reasoning than PPO [26]. Our methodology builds upon this SFT-RL paradigm: we first employ SFT to cold-start the iterative evidence-seeking pattern, then utilize two-stage progressive RL process to enhance and generalize this capability. 2.3. Multimodal reasoning model The SFTRL paradigm was rapidly adopted in multimodal learning, spawning models like Vision-R1 [10], WeThink [42], and R1-OneVision [43]. These models, with subsequent refinements [3, 23, 34], transferred this methodology to visual-language contexts. However, they largely inherit language-oriented patterns, assuming reasoning can occur over static, global visual representation. This assumption fails in remote sensing, where tasks demand iterative, region-specific inspection. To address this gap, we propose RS-EoT, an iterative, evidence-seeking paradigm tailored for remote sensing. We introduce complete framework to instill this capability, featuring: (1) Socratic-inspired mechanism to synthesize RS-specific reasoning traces, and (2) multi-stage progressive training pipeline to instill this behavior. This design overcomes the limitations of single-pass, language-oriented reasoning frameworks. 3. Method We aim to enable multimodal models to perform evidenceseeking reasoning over remote sensing imagery. To introduce RS-EoT (Remote Sensthis end, we first ing Evidence-of-Thought), which characterizes the dereasoning is sired reasoning pattern for remote sensing: language-driven, iteratively seeking visual evidence to refine the reasoning chain and ultimately converge on the correct answer. Following the Deepseek-R1, we first inject the RS-EoT reasoning mode via SFT cold-starting, and then enhance and generalize it through RL. An overview of our method is presented in Fig. 2. To instill this reasoning pattern, primary challenge is synthesizing the cold-start data. To this end, we propose SocraticAgent, self-play multi-agent system that synthesizes RS-EoT reasoning traces. SocraticAgent simulates an iterative process of reasoning (via Reasoner) and visual evidence seeking (via Perceiver), ensuring each reasoning step is grounded in verifiable visual evidence. These synthesized traces are then used for SFT to cold-start the RS-EoT reasoning mode. Building on this SFT foundation, we adopt two-stage progressive RL strategy to further enhance and generalize this capability. The first stage performs RL on fine-grained grounding tasks. Since these tasks naturally demand iterative evidence seeking, this stage effectively strengthens the RS-EoT behavior learned during SFT. The second stage performs RL on general remote sensing VQA, allowing the learned RS-EoT pattern to generalize to broader RS understanding scenarios. The final model obtained from this process is termed RS-EoT-7B. 3.1. SFT: RS-EoT Cold-Start 3.1.1. RS-EoT The RS-EoT reasoning paradigm is defined by two core principles. First, reasoning is orchestrated by natural language, which serves as the primary medium for formulating hypotheses, planning evidence-seeking steps, and refining intermediate conclusions. Language is not merely description of multimodal information but the active controller that determines when and how to probe for multimodal evidence. Second, visual information functions as on-demand evidence within the reasoning chain. Instead of relying on single, static global perception, the model actively seeks, verifies, and integrates localized visual evidence at each reasoning step. Through this iterative loop of language-driven reasoning and targeted evidence retrieval, RS-EoT enables more faithful, interpretable, and evidencegrounded decision-making in remote sensing tasks. 3.1.2. Asking like Socrates core challenge is how to synthesize RS-EoT reasoning traces from scratch, given that existing models do not possess this reasoning mode. We draw inspiration from the Socratic method, which encourages knowledge discovery through structured process of questioning rather than direct instruction, proposing that the iterative process of reasoning and evidence seeking can be framed as continuous, incremental process of inquiry. 3 Figure 2. Overview of our method to instill the RS-EoT paradigm. (Left) SFT: RS-EoT Cold-Start: We propose SocraticAgent to synthesize reasoning traces. Reasoner (text-only) and Perceiver (image-aware) engage in an iterative dialogue, guided by selfplay prompting mechanism. (Right) RL: Enhancing and Generalizing RS-EoT: two-stage progressive RL pipeline. Stage 1 (RLGrounding) enhances fine-grained evidence-seeking via an IoU-based reward. Building on this, Stage 2 (RL-VQA) generalizes reasoning by converting simple VQA datasets into multiple-choice format with graded reward for stable training. For instance, when addressing the problem, What is the heading of the ship in this remote sensing image? one might first naturally ask, Is there ship in the image? This is followed by coarse visual inspection to confirm its presence. Next, one would reason based on common sense: if the ship is moving, its wake might indicate the direction of travel. This hypothesis, derived from reasoning, leads to new, more specific question: In which direction is the ships wake pointing? This prompts detailed visual inspection, and by observing the wake, one can finally infer the ships heading (as the opposite direction of the wake). By emulating this Socratic questioning, an iterative process of posing questions, seeking visual evidence, and refining the reasoning chain, we can progressively correct and complete our logical path to correctly solve the problem. Simulating the aforementioned process, we propose SocraticAgent, self-play multi-agent system capable of synthesizing RS-EoT reasoning traces from remote sensing VQA datasets. It primarily comprises two roles: the Reasoner and the Perceiver. Reasoner. Instantiated by GPT-5-mini [24], the Reasoner operates solely on textual queries and basic image metadata (e.g., modality, count, size), lacking direct access to the RS images. It performs pure-text reasoning and is prompted to pose perceptual questions to the Perceiver when visual information is needed. Upon receiving the Perceivers answers, it continues to reason or pose further questions until it can formulate final answer. Throughout the interaction, the Reasoner continuously analyzes, summarizes, and integrates the Perceivers feedback to build coherent semantic understanding, ultimately producing the final answer. Perceiver. Implemented using powerful multimodal model (Gemini-2.5-flash [5] in our setup), the Perceivers input consists solely of the RS image and the questions posed by the Reasoner, without access to the original task query. Its sole task is to receive the Reasoners questions and provide accurate answers based on the remote sensing image. They engage in multi-turn dialogue, culminating in the Reasoner providing the final answer. We then introduce Verifier (instantiated by doubao-seed-1.6-thinking [2]) to validate the Reasoners answer against the ground truth. The design rationale is: if the Reasoner, despite having no direct visual access, arrives at the correct solution, the intermediate dialogue is deemed reliable reasoning trace. This dialogue history is then concatenated based on the following template and formatted into self QA style reasoning trajectory. Fig. 3 illustrates an actual case. SocraticAgent EoT Trace Concatenation Template thought & question} <think> Alright, will reason in self QA style and give the final reply. {Reasoner (round 1): {Perceiver (round 1): ... {Reasoner (round ): {Perceiver (round ): {Reasoner (round + 1): </think> {Final answer from Reasoner} thought & question} final answer} response} response} 3.1.3. Self-play Prompting Mechanism However, in practice, we found that the Reasoner tends to forgo detailed reasoning, often defaulting to sending the original query directly to the Perceiver, resulting in an overly jumpy reasoning process. The Perceiver, in turn, 4 is prone to returning overly exhaustive visual information, much of it irrelevant to the specific question. This distracts the Reasoner, leading it to produce hasty responses. Therefore, we designed an elegant self-play prompting mechanism. Specifically, we prompt each agent that its collaborator is weak. The Reasoner is told, The Perceiver you are collaborating with cannot understand complex questions. The Perceiver is told, The Reasoner you are collaborating with has weak reasoning abilities. This strategy forces the Reasoner to perform detailed problem decomposition and pose simple, incremental questions. Concurrently, it compels the Perceiver to provide answers that are accurate yet concise, without extraneous information. This self-play mechanism ensures their multi-turn dialogue converges into detailed, progressively structured reasoning trace. Ultimately, we synthesized multi-modal RS-EoT dataset, termed RS-EoT-4K, (including RGB, infrared, and SAR imagery) for SFT cold-starting. Data statistics and training details are provided in Sec. 4. 3.2. RL: Enhancing and Generalizing RS-EoT To ensure the models RS-EoT reasoning capability is sufficiently powerful, we build upon the SFT stage with twostage progressive reinforcement learning pipeline designed to first enhance and subsequently generalize this capability. 3.2.1. Stage#1: Fine-grained Grounding RL Starting with the SFT model, adhering to the principle that iron sharpens iron, we first perform reinforcement learning on fine-grained grounding tasks. These tasks are selected precisely because they most demand fine-grained visual evidence-seeking, thereby specifically strengthening the models RS-EoT reasoning capability. Implementation-wise, we prompt the model to output the target objects location in the format [x1, y1, x2, y2]. We then calculate the Intersection over Union (IoU) with the ground truth and use the IoU score directly as the reward. We also apply strict format-based reward during this process to ensure output consistency. 3.2.2. Stage#2: General RS VQA RL To generalize the models reasoning to broader RS scenarios, we further perform RL on general RS VQA datasets. However, existing RS VQA datasets predominantly contain Yes/No or low-reasoning-intensity questions [16, 21, 22], which are highly susceptible to reward hacking. To address this, we propose multiple-choice RL data reconstruction strategy that converts existing simple RS VQA datasets into format that stably supports RL training. Specifically, we observe that while current RS VQA datasets feature simple questions and answers, they often map single image to multiple QA pairs (stemming from their construction process, often being programmatically converted from traditional classification or detection datasets). Leveraging this property, we convert them into multiple-choice questions. Without loss of generality, the reconstruction process for single sample is as follows: set {(Qi, Ai)}m 1. Collect remote sensing image and its associated QA i=1 with 10 < < 15. 2. Randomly invert the answers of QA pairs to create incorrect QA pairs, for example by changing Yes to No or adding/subtracting random integer from numeric answer, with 1 < < m. 3. Formulate multiple-choice question (MCQ) sample using Which of the following QA pairs match this remote sensing image? as the query, and use the QA pairs as the options. The following is an example: Multiple-Choice RL Data Reconstruction Example Query:<image>Which of the following QA pairs match this remote sensing image? A. Is there square in the image? B. Are there cars in the image? # Inverted; cars are actually present ... J. How many ships are in the image? 3 7 K. How many trees are in the image? # Inverted; there are actually 4 trees Ground truth: A, ..., E, yes no Reward design. Given the large number of options, the model often cannot produce perfectly correct answer on this reconstructed data. To avoid training collapse, we design tailored graded reward function. Specifically, we assign an option-level, equal-weight, symmetric accuracy reward: selecting correct option and correctly rejecting an incorrect option both receive positive reward, whereas selecting an incorrect option or failing to select correct option receive zero reward. Let {0, 1}N denote groundtruth labels (1 = correct, 0 = incorrect) and ˆy {0, 1}N the models binary choices (1 = chosen, 0 = not chosen). The per-question reward is rqa = 1 1 (cid:88) i=1 yi ˆyi . (1) This reward provides stable, graded training signal by symmetrically penalizing both misses and false selections and equally weighting all options. This design effectively mitigates reward hacking and necessitates multi-round reasoning with evidence aggregation, forcing the model to verify each distinct option against the visual evidence to maximize its score. 5 Table 1. Main comparison on remote sensing general VQA and fine-grained grounding. We compare RS-EoT-7B with recent multimodal reasoning models (all based on Qwen2.5-VL-7B) as well as two remote-sensing-oriented reasoning models (Geo-R1 and VHM-RL). RS-EoT-7B consistently achieves the best results across both VQA and grounding tasks, demonstrating the effectiveness of the RS-EoT paradigm in remote sensing scenarios. Abbreviations: VLAA-T = VLAA-Thinker, VL-R = VL-Rethinker, Vis-R1 = Vision-R1, MM-E = MM-Eureka, R1-OV = R1-OneVision. Benchmark Metric RS-EoT-7B Qwen2.5VL [1] WeThink [42] VLAA-T [3] VL-R [34] Vis-R1 [10] MM-E [23] R1-OV [43] Geo-R1 [40] VHM-RL [13] RSFG-VQA [22] RSFG-SC [22] VRSBench [17] RSVQA [20] DIOR-RSVG [46] VRSBench-Ref [17] Avg@5 Conv@5 Pass@5 Scene@acc Object@F1 Avg@5 Conv@5 Pass@5 Avg@5 Conv@5 Pass@5 IoU@50 IoU@70 mIoU IoU@50 IoU@70 mIoU 67.85 68.90 85.28 64.05 56.52 63.09 64.12 83.54 75.16 78.29 92.51 47.00 33.32 45.29 54.71 32.40 48. 4. Experiments 62.45 62.46 77.16 57.42 36.78 62.45 62.58 75.62 67.20 67.45 77.95 35.40 20.84 35. 19.10 7.58 21.99 Remote Sensing General QA 55.72 37.87 77.88 56.50 32.91 60.54 61.16 77.57 42.18 47.76 70. 58.80 57.88 76.37 58.82 34.84 55.04 55.49 71.27 65.57 66.31 79.20 Fine-grained Grounding 7.16 0.93 12. 26.46 11.95 27.55 24.28 13.35 25.48 24.69 11.83 25.29 55.04 41.07 61.43 60.12 38.35 62.17 62.36 73. 40.74 41.10 54.86 34.51 20.76 33.96 35.56 18.74 34.07 39.95 41.94 57.81 21.63 6.27 37.72 37.39 48. 57.99 61.03 76.71 3.21 0.43 8.91 3.5 0.63 8.62 60.98 60.82 71.22 56.98 31.57 55.25 55.37 66. 67.78 67.97 75.86 2.73 0.20 6.87 14.64 6.33 15.69 47.70 48.00 77.66 47.04 24.65 44.59 46.36 73. 46.34 51.12 82.34 4.37 1.29 7.82 1.97 0.36 5.48 45.03 39.76 71.96 52.46 20.82 57.00 58.65 81. 34.50 45.78 65.43 17.67 7.21 20.97 17.18 13.77 4.51 53.04 51.06 61.56 64.90 30.89 57.01 56.76 70. 67.29 67.81 72.14 44.63 29.37 40.32 33.13 35.40 18.74 Experimental Design. Our experiments are designed to (1) evaluate the effectiveness of the RS-EoT paradigm and (2) provide in-depth analyses of its mechanisms. For evaluation, we conduct quantitative comparisons against SOTA models on VQA and grounding tasks (Sec. 4.1). We also compare our SocraticAgent data against traditional distillation methods (Sec. 4.2). For analysis, we provide case studies illustrating RS-EoT-7Bs iterative evidence-seeking (Sec. 4.3) and analyze attention dynamics to show its reasoning-perception cycles (Sec. 4.4). We also present the VQA-RL reward curve to validate our data strategy (Sec. 4.5). Finally, we conduct stage-wise ablation study to verify the contribution of each component (Sec. 4.6). Implementation details. We set 6-round dialogue limit for SocraticAgent and synthesize 4.3K SFT samples from the datasets listed in Tab. 2. SFT training is conducted for 5 epochs with learning rate of 3 105 using LLaMAFactory [48]. The two-stage RL procedure is implemented with EasyR1 [49] and GRPO [27]. In Stage 1, the model is trained on DIOR-RSVG [46] and VRSBench [17]. In Stage 2, the model is trained on RS-VQA [21] and FIT-RS [22]. Both RL stages are trained for 2 epochs with batch size of 512 and learning rate of 106. Benchmark. We evaluate on Remote Sensing General QA (FiT-RSFG-VQA [22], FiT-RSFG-SC [22], VRSBenchVQA [17], RSVQA [20]) and Fine-grained Grounding (DIOR-RSVG [46], VRSBench-Ref [17]). Metrics. For VQA, we report Avg@5, Conv@5, and Pass@5, which evaluate the average correctness, majority correctness, and any-correct correctness among 5 generated 6 Table 2. Statistics of the data sources used for synthesizing our RS-EoT dataset. Data Source Modality Count FIT-RS [22] VRSBench [17] DroneVehicle [29] SARLang-1M [38] EarthVQA [35] RSVQA [20] RGB RGB RGB & INF SAR RGB RGB 1.9K 1.1K 0.2K 0.2K 0.6K 0.3K Total RGB/INF/SAR 4.3K Table 3. SFT performance comparison of reasoning trace synthesis strategies on FIT-RSFG-VQA. Traces generated by SocraticAgent yield stronger reasoning performance than direct distillation from frontier models (Qwen3VL, Doubao, GLM-4.5V). Method Avg@5 Conv@ Pass@5 SocraticAgent (ours) Qwen3-VL-plus [33] Doubao-seed-1-6-vision [2] GLM-4.5V [31] 64.82 63.78 51.80 66.25 67.35 65.15 17.17 66.58 85.89 76.87 66.81 77.99 answers, respectively. We use accuracy for classification and F1 score for multi-object recognition. For grounding, we report mIoU and IoU@N (N=50, 70), the percentage of predictions with IoU > 0.5 or 0.7, respectively. Baseline. We compare against SOTA reasoning models (e.g., WeThink [42], Vision-R1 [10], R1-OneVision [43]) built on Qwen2.5-VL-7B [1], and RS-specific models (GeoR1 [40], VHM-RL [13]). For data comparison, we test distillation from Qwen3-VL-plus, Doubao-seed-1-6-vision, and GLM-4.5V. We exclude Gemini/GPT-5 models as their internal processes are inaccessible. Figure 3. Case studies comparing RS-EoT-7B with prior multimodal reasoning models on (top) Remote Sensing General QA and (bottom) Fine-grained Grounding. Unlike previous models, RS-EoT-7B follows the RS-EoT paradigm: it iteratively self-questions, gathers additional visual evidence during reasoning, and uses that evidence to verify or adjust its conclusions. 4.1. Comparison with Prior Reasoning Models Tab. 1 compares RS-EoT-7B with recent multimodal reasoning models and two RS specific reasoning models across both remote sensing VQA and fine-grained grounding benchmarks. Except for Geo-R1 and VHM-RL, all competing models are built on Qwen2.5-VL-7B, ensuring fair comparison focused on the effectiveness of the reasoning paradigm rather than the capacity of the base model. On the remote sensing VQA benchmarks, RS-EoT-7B consistently achieves the best performance across all three evaluation metrics (Avg@5, Conv@5, Pass@5), indicating stronger reasoning ability and more stable answer quality. In contrast, existing multimodal reasoning models exhibit noticeable instability: their performance fluctuates significantly relative to the base model and fails to deliver consistent gains on remote sensing tasks, suggesting that their generated reasoning traces primarily reflect pseudo reasoning rather than genuine evidence-guided reasoning. For fine-grained grounding tasks, RS-EoT-7B also surpasses all baselines by large margin in IoU@50, IoU@70, and mIoU. These improvements demonstrate that the iterative evidence-seeking mechanism effectively guides the models attention to and improves its ability to localize visual cues. 4.2. Comparison with Frontier Model Distillation We compare our SocraticAgent against the common practice of directly distilling reasoning traces from frontier models. We sampled 2K queries from the FIT-RSFG-VQA training set and generated reasoning traces in two ways: (1) using our SocraticAgent, and (2) via direct distillation from Qwen3-VL-plus, Doubao-seed-1-6-vision, and GLM-4.5V. We then fine-tuned models on each dataset and evaluated them on FIT-RSFG-VQA, with results shown in Tab. 3. The results show that models trained on SocraticAgentgenerated traces consistently outperform those trained on directly distilled traces. This suggests that frontier models often do not expose reasoning processes aligned with remote sensing cognition. In contrast, SocraticAgent synthesizes structured, iterative, and evidence-seeking trajectories, leading to more reliable and transferable reasoning behavior in remote sensing. 4.3. Case Study We present two qualitative examples in Fig. 3 to illustrate the RS-EoT paradigm. In the VQA example, the baseline performs only single, coarse scene interpretation and fails to find an available jet bridge, missing contradictory visual evidence. In contrast, RS-EoT-7B performs iterative verification: it first establishes the airport context, then explicitly searches for an unoccupied gate, and finally identifies an available one. In the grounding example, the baseline fails to detect the small, irregular water body. RS-EoT-7B, however, uses iterative inspection guided by self-questioning: it first performs global scan, then identifies candidate region (lower-left), and re-examines the image to rule out 7 Figure 4. Token-wise attention visualization on eight randomly sampled cases. The y-axis represents the proportion of attention allocated to image tokens, and the x-axis represents the token index during the decoding step. Clear periodic patterns emerge: attention peaks on visual tokens (evidence-seeking phases) and then drops during language-based reasoning (reasoning phases). This alternating cycle reflects the iterative reasoning mechanism instilled by the RS-EoT paradigm. Table 4. Ablation study of training stages on VQA and Grounding and the results validate the complementary role of each stage. Stage VQA Grounding Avg@5 Conv@5 Pass@ IoU@50 IoU@70 mIoU Base +SFT +RL-IoU +RL-VQA 67.20 70.73 69.51 75.16 67.45 74.05 72.01 78.29 77.95 91.96 90.63 92. 35.40 8.81 48.41 48.23 20.84 2.19 33.51 33.27 35.64 13.95 45.57 45.52 4.5. VQA-RL Training Stability Analysis Fig. 5 illustrates the reward curve during the VQA RL stage (Stage#2). We observe smooth and consistent upward trajectory, with the reward steadily increasing from an initial value of approximately 0.62 and converging around 0.84. This stable progression provides strong evidence for the effectiveness of our multiple-choice data reconstruction strategy. It demonstrates that our method, combined with the tailored graded reward function, successfully mitigates reward hacking and provides stable, effective learning signal, enabling the model to learn from simple RS VQA datasets without training collapse. 4.6. Stage-wise Training Analysis We conduct stage-wise ablation to analyze the contribution of each training step, as shown in Tab. 4. Starting from the Base model, SFT with RS-EoT trajectories substantially improves VQA accuracy but causes significant drop in grounding ability. The subsequent RL-IoU stage effectively restores and dramatically enhances grounding performance, confirming that explicit spatial feedback is crucial for localization. Finally, the RL-VQA stage further improves VQA reasoning while successfully maintaining the strong grounding performance. These results highlight the complementary role of each stage: SFT injects the reasoning pattern, RL-IoU enhances the visual evidence-seeking capability, and RL-VQA allows the model to be applied to broader remote sensing scenarios. Figure 5. The reward curve for the VQA RL stage. The stable upward trend validates that our multiple-choice data reconstruction strategy provides an effective learning signal and successfully mitigates reward hacking. other water bodies. This sequence of targeted visual checks allows the model to refine both what is present and where it is located, ultimately producing accurate coordinates supported by its grounded reasoning. 4.4. Attention Dynamics During EoT Reasoning To understand how RS-EoT guides the model to alternate between linguistic reasoning and visual evidence seeking, we visualize the token-wise attention distribution during decoding. For each generated token, we compute the proportion of its attention allocated to image tokens (y-axis) plotted against its index in the generated sequence (x-axis). Fig. 4 presents visualizations on eight randomly sampled test cases, all revealing clear periodic pattern. Attention periodically peaks on visual tokens, which we interpret as evidence-seeking phases where the model actively retrieves new visual cues from the image. These peaks are followed by troughs where attention shifts back to language tokens, corresponding to reasoning phases for linguistic deduction. This alternating behavior aligns precisely with our RS-EoT paradigm, demonstrating that the model dynamically coordinates between image inspection and high-level inference, rather than merely generating text sequentially. These findings confirm that RS-EoT enables structured, iterative reasoning process where the model repeatedly examines the image, updates its hypothesis, and advances its reasoning. 8 5. Conclusion In this work, we introduce RS-EoT, an iterative evidenceseeking paradigm for remote sensing understanding. RSEoT structures reasoning as an alternating loop of linguistic deduction and targeted visual evidence seeking. We instill this paradigm using an SFT and progressive RL (Grounding+VQA) pipeline. Experiments show our approach surpasses SOTA baselines in reasoning robustness and grounding accuracy. Analyses coninternalizes this iterative pattern, valfirm the model idating structured reasoning for trustworthy geospatial AI."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 6 [2] ByteDance / Volcengine. Seed 1.6 doubao (seed) 1.6. Online, 2025. 1, 4, 6 [3] Hardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, and Cihang Xie. Sft or rl? an early investigation into training r1-like reasoning large vision-language models. arXiv preprint arXiv:2504.11468, 2025. 2, 3, 6 [4] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Program of thoughts prompting: Disentangling Cohen. computation from reasoning for numerical reasoning tasks. Transactions on Machine Learning Research, 2023. 3 [5] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 4 [6] Chenrui Fan, Ming Li, Lichao Sun, and Tianyi Zhou. Missing premise exacerbates overthinking: Are reasonarXiv preprint ing models losing critical thinking skill? arXiv:2504.06514, 2025. [7] Gongfan Fang, Xinyin Ma, and Xinchao Wang. Thinkless: Llm learns when to think. Advances in neural information processing systems, 2025. 2 [8] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et al. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081):633 638, 2025. 1, 2, 3 [9] Xin Guo, Jiangwei Lao, Bo Dang, Yingying Zhang, Lei Yu, Lixiang Ru, Liheng Zhong, Ziyuan Huang, Kang Wu, Dingxiang Hu, Huimei He, Jian Wang, Jingdong Chen, Ming Yang, Yongjun Zhang, and Yansheng Li. Skysense: multimodal remote sensing foundation model towards universal interpretation for earth observation imagery. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2767227683, 2024. 2 [10] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. 2, 3, 6 [11] Ziyue Huang, Hongxi Yan, Qiqi Zhan, Shuai Yang, Mingming Zhang, Chenkai Zhang, YiMing Lei, Zeming Liu, Qingjie Liu, and Yunhong Wang. survey on remote sensing foundation models: From vision to multimodality. arXiv preprint arXiv:2503.22081, 2025. 2 [12] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [13] Aybora Koksal and Aydın Alatan. Few-shot visionlanguage reasoning for satellite imagery via verifiable rewards. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 69016910, 2025. 3, 6 [14] Kartik Kuckreja, Muhammad Sohail Danish, Muzammal Naseer, Abhijit Das, Salman Khan, and Fahad Shahbaz Khan. Geochat: Grounded large vision-language model for remote sensing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 27831 27840, 2024. 2 [15] Haodong Li, Xiaofeng Zhang, and Haicheng Qu. Ddfav: Remote sensing large vision language models dataset and evaluation benchmark. Remote Sensing, 17(4):719, 2025. 2 [16] Kun Li, George Vosselman, and Michael Ying Yang. Hrvqa: visual question answering benchmark for high-resolution ISPRS Journal of Photogrammetry and Reaerial images. mote Sensing, 214:6581, 2024. 5 [17] Xiang Li, Jian Ding, and Mohamed Elhoseiny. Vrsbench: versatile vision-language benchmark dataset for remote sensing image understanding. Advances in Neural Information Processing Systems, 37:32293242, 2024. 6 [18] Xiang Li, Congcong Wen, Yuan Hu, Zhenghang Yuan, and Xiao Xiang Zhu. Vision-language models in remote sensing: IEEE Geoscience and Current progress and future trends. Remote Sensing Magazine, 12(2):3266, 2024. [19] Chenyang Liu, Jiafan Zhang, Keyan Chen, Man Wang, Zhengxia Zou, and Zhenwei Shi. Remote sensing spatiotemporal visionlanguage models: comprehensive survey. IEEE Geoscience and Remote Sensing Magazine, 2025. 2 [20] Sylvain Lobry, Diego Marcos, Jesse Murray, and Devis Tuia. Rsvqa: Visual question answering for remote sensing data. IEEE Transactions on Geoscience and Remote Sensing, 58 (12):85558566, 2020. 6 [21] Sylvain Lobry, Begum Demir, and Devis Tuia. Rsvqa meets bigearthnet: new, large-scale, visual question answering In 2021 IEEE International dataset for remote sensing. Geoscience and Remote Sensing Symposium IGARSS, pages 12181221, 2021. 5, 6 [22] Junwei Luo, Zhen Pang, Yongjun Zhang, Tingzhu Wang, Linlin Wang, Bo Dang, Jiangwei Lao, Jian Wang, Jingdong Chen, Yihua Tan, and Yansheng Li. Skysensegpt: finegrained instruction tuning dataset and model for remote sensing vision-language understanding, 2024. 5, 6 [23] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, Ping Luo, Yu Qiao, Qiaosheng Zhang, and Wenqi Shao. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning, 2025. 2, 3, 6 [24] OpenAI. Gpt-5 system card. https://cdn.openai. com/gpt-5-system-card.pdf, 2025. 4 [25] Pranav Saxena, Nishant Raghuvanshi, and Neena Goveas. Uav-vln: End-to-end vision language guided navigation for In 2025 European Conference on Mobile Robots uavs. (ECMR), page 16. IEEE, 2025. 2 [26] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 3 [27] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. 3, [28] Chuming Shen, Wei Wei, Xiaoye Qu, and Yu Cheng. Satorir1: Incentivizing multimodal reasoning with spatial grounding and verifiable rewards, 2025. 2 [29] Yiming Sun, Bing Cao, Pengfei Zhu, and Qinghua Hu. Drone-based rgb-infrared cross-modality vehicle detection via uncertainty-aware learning. IEEE Transactions on Circuits and Systems for Video Technology, pages 11, 2022. 6 [30] Lijie Tao, Haokui Zhang, Haizhao Jing, Yu Liu, Dawei Yan, Guoting Wei, and Xizhe Xue. Advancements in vision language models for remote sensing: Datasets, capabilities, and enhancement techniques. Remote Sensing, 17(1):162, 2025. 2 [31] GLM-V Team. Glm-4.5v and glm-4.1v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning, 2025. 6 [32] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. 2 [33] Qwen Team. Qwen3-vl: Sharper vision, deeper thought, broader action. Qwen Blog. Accessed, pages 1004, 2025. [34] Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vl-rethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. arXiv preprint arXiv:2504.08837, 2025. 2, 3, 6 [35] Junjue Wang, Zhuo Zheng, Zihang Chen, Ailong Ma, and Yanfei Zhong. Earthvqa: Towards queryable earth via relational reasoning-based remote sensing visual question answering. Proceedings of the AAAI Conference on Artificial Intelligence, 38(6):54815489, 2024. 6 [36] Peijin Wang, Huiyang Hu, Boyuan Tong, Ziqi Zhang, Fanglong Yao, Yingchao Feng, Zining Zhu, Hao Chang, Wenhui Diao, Qixiang Ye, and Xian Sun. Ringmogpt: unified remote sensing foundation model for vision, language, and grounded tasks. IEEE Transactions on Geoscience and Remote Sensing, 63:120, 2025. 2 [37] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 3 [38] Yimin Wei, Aoran Xiao, Yexian Ren, Yuting Zhu, Hongruixuan Chen, Junshi Xia, and Naoto Yokoya. Sarlang-1m: benchmark for vision-language modeling in sar image understanding, 2025. 6 [39] Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Tanglifu Tanglifu, Xiaowei Lv, Haosheng Zou, Yongchao Deng, Shousheng Jia, and Xiangzheng Zhang. Light-r1: Curriculum SFT, DPO and RL for long COT from scratch and beyond. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 6: Industry Track), pages 318327, Vienna, Austria, 2025. Association for Computational Linguistics. 2, [40] Chenhui Xu, Fuxun Yu, Michael J. Bianco, Jacob Kovarskiy, Raphael Tang, Qi Zhang, Zirui Xu, Will LeVine, Brandon Dubbs, Heming Liao, Cassandra Burgess, Suvam Bag, Jay Patravali, Rupanjali Kukal, Mikael Figueroa, Rishi Madhok, Nikolaos Karianakis, and Jinjun Xiong. Geo-r1: Unlocking vlm geospatial reasoning with cross-view reinforcement learning, 2025. 3, 6 [41] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 1 [42] Jie Yang, Feipeng Ma, Zitian Wang, Dacheng Yin, Kang Rong, Fengyun Rao, and Ruimao Zhang. Wethink: Toward general-purpose vision-language reasoning via reinforcement learning, 2025. 2, 3, 6 [43] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, Bo Zhang, and Wei Chen. R1-onevision: Advancing generalized multimodal reasoning through crossarXiv preprint arXiv:2503.10615, modal formalization. 2025. 2, 3, 6 [44] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, YaQin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale, 2025. 3 [45] Ruifeng Yuan, Chenghao Xiao, Sicong Leng, Jianyu Wang, Long Li, Weiwen Xu, Hou Pong Chan, Deli Zhao, Tingyang Xu, Zhongyu Wei, et al. Vl-cogito: Progressive curriculum reinforcement learning for advanced multimodal reasoning. arXiv preprint arXiv:2507.22607, 2025. [46] Yang Zhan, Zhitong Xiong, and Yuan Yuan. Rsvg: Exploring data and models for visual grounding on remote sensing data. IEEE Transactions on Geoscience and Remote Sensing, 61: 113, 2023. 6 [47] Yuhang Zhang, Haosheng Yu, Jiaping Xiao, and Mir Fer10 oskhan. Grounded vision-language navigation for uavs with open-vocabulary goal understanding, 2025. 2 [48] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. 6 [49] Yaowei Zheng, Junting Lu, Shenzhi Wang, Zhangchi Feng, Dongdong Kuang, and Yuwen Xiong. Easyr1: An efficient, scalable, multi-modality rl training framework. https:// github.com/hiyouga/EasyR1, 2025. [50] Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models, 2023. 3 11 Asking like Socrates: Socrates helps VLMs understand remote sensing images"
        },
        {
            "title": "Supplementary Material",
            "content": "6. System Prompts Details In this section, we provide the exact system prompts used in our SocraticAgent framework to synthesize the RS-EoT-4K dataset. As described in the main paper, SocraticAgent operates as self-play multi-agent system consisting of three distinct roles: the Reasoner, the Perceiver, and the Verifier. The Reasoner  (Fig. 6)  serves as the cognitive core. It is text-only agent designed to simulate the human coarseto-fine visual interpretation process. By following PlanIntegrateDecide paradigm, it decomposes complex queries into atomic visual questions, iteratively gathering evidence before concluding. The Perceiver  (Fig. 7)  acts as the visual interface. To enforce explicit and detailed reasoning from the Reasoner, we employ self-play prompting mechanism where the Perceiver is instructed to collaborate with weakreasoning partner. Its role is to provide accurate, descriptive visual observations in natural inner monologue style without performing high-level logical reasoning. The Verifier  (Fig. 8)  functions as the quality control gatekeeper. It compares the final answer generated by the Reasoner against the ground truth to filter out incorrect or hallucinated reasoning traces during the data synthesis phase. The complete prompts for these agents are presented in Fig. 6, Fig. 7, and Fig. 8, respectively. 7. SFT Training Settings We perform SFT on the base model Qwen2.5-VL-7BInstruct using the RS-EoT-4K dataset. The training is implemented based on the LLaMA-Factory framework. We train the model for 5 epochs with learning rate of 3105, using the AdamW optimizer and cosine learning rate scheduler. The global batch size is set to 64, and the maximum sequence length is limited to 4096 tokens to accommodate the detailed reasoning traces. The entire training process was executed on 4 A100 GPUs and required approximately 40 minutes to complete. empirical finding in our experiments concerns the use of system prompts. We observed that including explicit reasoning triggers in the system prompt (e.g., instructions like Please reason step-by-step) during training caused the model to develop strong dependency on these specific triggers. Consequently, the model often failed to initiate reasoning or exhibited abnormal behaviors when such prompts were absent during inference. To address this and ensure robust, spontaneous reasoning capabilities, we exclude all reasoning-specific instructions from the system prompt during SFT and RL. Instead, we hard-code the start-of-thought token <think> directly into the assistants response field within the chat template. This design forces the model to automatically enter the reasoning mode immediately upon generation, ensuring stable output trajectories without relying on specific user or system prompts. 8. RL Training Settings All reinforcement learning experiments are conducted using the EasyR1 framework, which provides production-ready implementation of GRPO with KL regularization. We fix the KL coefficient to β = 1.0 102. For each input, the model generates 4 rollout samples using sampling temperature 1.0, with maximum response length of 4096 tokens. Policy optimization is performed with the AdamW optimizer, using learning rate of 1.0 106, weight decay of 1.0 102, and linear warm-up over the first 3We apply gradient clipping with threshold of 1.0. All models are trained in bf16 precision under full-sharded data parallelism (FSDP) with gradient checkpointing enabled. For both RL-Grounding and RL-VQA, we use global batch size of 512 (covering rollout, training, and validation batches) and train for 240 steps. Experiments are run on single node equipped with 8 A100 GPUs. The RL-VQA stage requires approximately 2.3 days to complete, while the RL-Grounding stage takes about 2.5 days. 9. RL Reward Function 9.1. Grounding Reward For the grounding task, the model is required to output bounding box in the form [x1, y1, x2, y2] after complete <think></think> block. Our reward contains two components: an IoU-based accuracy term and lightweight format term. Format reward. For the grounding task, we apply lightweight format reward to encourage the model to produce both well-formed reasoning block and structured bounding-box output. We evaluate two binary indicators: (i) whether the response contains complete, matched <think></think> block, and (ii) whether valid fournumber bounding-box list of the form [x1, y1, x2, y2] appears after the closing </think> tag. We denote these indicators as sthink and sbbox, respectively. The format reward is then defined as rfmt = sthink + sbbox 2 , 1 System Prompt for the Reasoner You are reasoning model that follows PlanIntegrateDecide paradigm, collaborating with weak-perception visual model to complete general remote sensing tasks (such as classification/attribute recognition, localization/counting, relation/change detection, and VQA). The perception model can only answer very simple, atomic visual facts and cannot perform reasoning. Therefore, you must decompose the perception process into coarse-to-fine sequence of steps, simulating how humans visually interpret remote sensing imagery. [Coarse-to-Fine Perception Chain] 1) Global Observation Stage (Overall Understanding): - Begin with broad, holistic examination of the entire image, forming an initial impression of its overall layout main land-cover types, spatial organization, scene functionality, distribution of major objects, and possible visual interferences (e.g., shadows, fog, noise, or occlusion). - While questions at this stage should remain broad, general, and high-level, they must be context-aware i.e., lightly tailored to the task/query so they inform later reasoning for this specific problem. 2) Focused and Detailed Observation Stage (Targeted Analysis): - After forming general understanding of the scene, use the task objective (query) and global observations to focus attention on potentially relevant local regions or objects. - Naturally shift attention from overall impressions to specific, task-relevant areas, similar to how humans visually focus. - Ask more detailed and targeted questions, typically focusing on: Local details (shape, texture, boundaries, orientation, color features, etc.); Relationships and differences (changes, similarities, transitions between regions, etc.); Task-critical elements (e.g., presence, quantity, or arrangement of specific targets). 3) Integration and Verification Stage: - Integrate the facts collected from the global and detailed observation stages into consistent intermediate conclusion. - If contradictions or uncertainties remain, ask verification questions. - Ensure that the reasoning covers all key regions and that the logic is consistent. 4) Final Review and Confirmation Stage: - Before giving the final answer, perform quick overall review of the image to confirm that no small anomalies, marginal areas, or potential clues have been overlooked. - Check whether the final answer meets the querys requirements regarding format, length, and structure. - The final answer must only output the direct answer to the query itself, such as Yes/No, specific number, or concise conclusion. Do not include any explanations, reasoning, or additional commentary. - If necessary, ask one final targeted question for confirmation. [Questioning and Iteration Constraints] - Never forward the users original query directly to the perception model; each question must concern only one atomic visual fact. - Each new question should provide maximum information gain and must not repeat previous questions (avoid paraphrasing). - You have {MAX LOOP-1} questioning rounds available: the early rounds focus on global perception, the middle rounds gather key evidence, and the final rounds perform verification questioning. [Output Format (Strict Requirements)] - If further questioning is needed: Start with <thinking>...</thinking> (briefly explain the reasoning and purpose of the next question), <question>...</question>. then output only one - If ready to give the final answer: Start with <thinking>...</thinking> (summarize key evidence and note that final checks have been completed), {FINAL PREFIX EN} .... then output - Each round must begin with <thinking>...</thinking> and followed <question>...</question> or {FINAL PREFIX EN} .... No other content is allowed. be by exactly one of the two options: - Inside <thinking>, do not mention external entities such as the perception model, user, or conversation. - Use English for internal reasoning and questioning, but ensure that the final answer matches the input querys language. Figure 6. The system prompt for the Reasoner in SocraticAgent. IoU accuracy. We first normalize the response and verify that well-formed reasoning block (i.e., matched pair <think>...</think>) exists. Only the text appearing after the closing tag </think> is used for prediction parsing. From this tail segment, we extract the first valid four-number list [x1, y1, x2, y2]; if no valid list is found, the prediction is treated as invalid. The accuracy score is then computed using the IoU between the predicted and ground-truth boxes directly in the original pixel coordinate system: racc = (cid:40) IoU(bboxpred, bboxgt), 0, if valid box is parsed, otherwise. Overall reward. The final grounding reward combines IoU accuracy and the format term: roverall = (1 λ) racc + λ rfmt, 2 System Prompt for the Perceiver You are an image interpretation expert collaborating with reasoning model that has very weak logical ability. Together, through multi-turn dialogue, you will complete general remote sensing tasks (classification/attribute, localization/counting, relation/change analysis, VQA, etc.). The reasoning model can only understand the textual descriptions of your perception results it cannot see the image directly. Therefore, you must respond to each of its questions about the image accurately and completely, without adding any information that is irrelevant to the question. Your tone should resemble natural inner monologue of person carefully observing an image. Always begin your response with: Lets look at the image, and then continue with your detailed observation. Figure 7. The system prompt for the Perceiver in SocraticAgent. System Prompt for the Verifier You are strict answer evaluator. Given Query, Answer, and GT, output only: 1) \"ACCEPT\" 2) \"REJECT: <brief reason>\" simply check whether the model produces complete and matched <think></think> reasoning block. If the block is present, the model earns format score of 1; otherwise, the score is 0. Figure 8. The system prompt for the Verifier in SocraticAgent. Overall reward. The final VQA reward is convex combination of accuracy and format: where we use λ = 0.1 in all experiments. Each sample is scored independently, and we log roverall, IoU, racc, and rfmt during training. 9.2. Multiple-Choice VQA Reward For the multiple-choice VQA task, the model predicts subset of option letters (e.g., A, C, D) after completing the <think></think> reasoning block. Only the text following the last </think> tag is considered for answer extraction. We first identify the last line in this tail that contains independent letter tokens; if none are found, we treat the prediction as invalid. Given the ground-truth correct set and the models predicted set A, where is the set of all allowed options, we compute symmetric, option-level accuracy score. Each option contributes equally: selecting correct option and correctly rejecting an incorrect option both receive positive credit, while selecting an incorrect option or failing to select correct one is penalized. The accuracy reward is therefore: racc = 1 1 (cid:88) aA (cid:12) (cid:12)1{a G} 1{a }(cid:12) (cid:12). If the model outputs any letter not contained in A, we apply hard-zero accuracy (i.e., racc = 0), matching our implementation. Format reward. The VQA setting uses the same lightweight format reward as the grounding task. Because empty outputs are allowed and interpreted as selecting no options, the format reward does not require the predicted letters to appear after the </think> tag. Instead, we roverall = (1 λ) racc + λ rfmt, with λ = 0.1 throughout all experiments. This reward design provides graded, stable learning signal across all options without relying on task-specific heuristics, while the matched <think></think> constraint ensures structured reasoning outputs. 10. RL Training Dynamics Curves Figure 9 visualizes the evolution of key optimization statistics during the two RL stages in our pipeline: RLGrounding and RL-VQA. The top block corresponds to the RL Grounding stage and the bottom block to the RL-VQA stage; in both cases we plot the same set of metrics, including mean advantage, actor gradient norm, entropy loss, KL loss, policy gradient loss, and average reward. For each quantity, we plot both the raw measurements and their Gaussian-smoothed trends, providing compact view of how the GRPO optimization behaves over the training steps in both stages. 11. Difference Between Multiple-Choice VQA and Standard VQA To assess the effectiveness of our proposed multiple-choice reformulation of VQA, we additionally perform an ablation study using the original dataset and model settings, but applying reinforcement learning directly on the standard freeform VQA answers. This experiment allows us to isolate and compare the impact of our multiple-choice VQA design against the conventional VQA supervision. We first evaluate the final model performance on the RSVQA. As presented in Table 5, our multiple-choice reconstruction strategy achieves consistently superior results Table 5. Ablation study on VQA data reconstruction strategies on the RSVQA. We compare our proposed Multiple-Choice VQA reconstruction against the Standard VQA (Free-form) supervision. Our method achieves consistently superior performance across all metrics. Method Avg@5 Conv@5 Pass@5 Standard VQA (Free-form) Ours (Multiple-Choice) 74.73 75.16 76.86 78. 90.91 92.51 Figure 10. Ablation comparing reinforcement learning on the original free-form VQA answers (red) versus our reconstructed multiple-choice VQA reconstruction (blue). Standard VQA produces unstable and oscillatory training dynamics, while the multiple-choice approach yields smooth and efficient reward improvement. achieving stable reinforcement learning in our setting. 12. Case Study We provide additional qualitative examples to further demonstrate the effectiveness of RS-EoT-7B in complex remote sensing reasoning scenarios. Specifically, we present extended case studies covering both Remote Sensing General VQA tasks (Fig. 11, Fig. 12, and Fig. 13) and Finegrained Grounding tasks (Fig. 14 and Fig. 15). These visualizations illustrate how the model employs the iterative evidence-seeking paradigm to achieve robust reasoning and precise localization across diverse queries. Figure 9. RL training dynamics for our two-stage pipeline. Top: RL-Grounding stage; bottom: RL-VQA stage. Each panel reports the evolution of different statistic ( advantages mean, entropy loss, gradient norm, KL loss, policy gradient loss, and rewards mean), with light curves showing raw values and dashed curves showing Gaussian-smoothed trends over training steps. across all metrics compared to the standard free-form supervision. To understand the underlying cause of this performance gap, we visualize the training stability in Figure 10. When trained with standard free-form VQA answers, the model exhibits slow reward improvement and frequent large-magnitude oscillations during GRPO optimization. This indicates that the original VQA supervision produces an unstable and inefficient reward signal for RL, making it difficult for the model to learn consistent behaviors. In contrast, our multiple-choice VQA reformulation leads to much smoother and steadily increasing reward curve under the same RL setup. This demonstrates that transforming VQA into structured multi-option prediction task greatly stabilizes the reward landscape and improves training efficiency. Overall, the ablation confirms that the multiple-choice VQA formulation provides more reliable and effective RL objective compared to standard VQA, and is essential for 4 Figure 11. Reasoning cases of RS-EoT-7B (Part 1). 5 Figure 12. Reasoning cases of RS-EoT-7B (Part 2). 6 Figure 13. Reasoning cases of RS-EoT-7B (Part 3). 7 Figure 14. CReasoning cases of RS-EoT-7B (Part 4). 8 Figure 15. Reasoning cases of RS-EoT-7B (Part 5)."
        }
    ],
    "affiliations": [
        "Baidu Inc., Beijing, China",
        "School of Earth Sciences, Zhejiang University, Hangzhou, China",
        "School of Geosciences and Info-Physics, Central South University, Changsha, China"
    ]
}