{
    "paper_title": "DeMo: Decoupled Momentum Optimization",
    "authors": [
        "Bowen Peng",
        "Jeffrey Quesnelle",
        "Diederik P. Kingma"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Training large neural networks typically requires sharing gradients between accelerators through specialized high-speed interconnects. Drawing from the signal processing principles of frequency decomposition and energy compaction, we demonstrate that synchronizing full optimizer states and model parameters during training is unnecessary. By decoupling momentum updates and allowing controlled divergence in optimizer states across accelerators, we achieve improved convergence compared to state-of-the-art optimizers. We introduce {\\textbf{De}}coupled {\\textbf{Mo}}mentum (DeMo), a fused optimizer and data parallel algorithm that reduces inter-accelerator communication requirements by several orders of magnitude. This enables training of large neural networks even with limited network bandwidth and heterogeneous hardware. Our method is topology-agnostic and architecture-independent and supports scalable clock-synchronous distributed training with negligible compute and memory overhead. Empirical results show that models trained with DeMo match or exceed the performance of equivalent models trained with AdamW, while eliminating the need for high-speed interconnects when pre-training large scale foundation models. An open source reference PyTorch implementation is published on GitHub at https://github.com/bloc97/DeMo"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 9 2 ] . [ 1 0 7 8 9 1 . 1 1 4 2 : r DeMo: Decoupled Momentum Optimization Bowen Peng1 Jeffrey Quesnelle1 Diederik P. Kingma 1Nous Research"
        },
        {
            "title": "Abstract",
            "content": "Training large neural networks typically requires sharing gradients between accelerators through specialized high-speed interconnects. Drawing from the signal processing principles of frequency decomposition and energy compaction, we demonstrate that synchronizing full optimizer states and model parameters during training is unnecessary. By decoupling momentum updates and allowing controlled divergence in optimizer states across accelerators, we achieve improved convergence compared to state-of-the-art optimizers. We introduce Decoupled Momentum (DeMo), fused optimizer and data parallel algorithm that reduces inter-accelerator communication requirements by several orders of magnitude. This enables training of large neural networks even with limited network bandwidth and heterogeneous hardware. Our method is topologyagnostic and architecture-independent and supports scalable clock-synchronous distributed training with negligible compute and memory overhead. Empirical results show that models trained with DeMo match or exceed the performance of equivalent models trained with AdamW, while eliminating the need for high-speed interconnects when pre-training large scale foundation models. An open source reference PyTorch implementation is published on GitHub at https://github.com/bloc97/DeMo."
        },
        {
            "title": "1 Introduction",
            "content": "Large-scale neural networks, particularly language models, are characterized by high parameter counts. In fact, it is not uncommon to talk about models with trillions of parameters. Training these models requires multiple accelerators (e.g. GPUs, TPUs) to achieve tractable training times. Common strategies for distributing training across accelerators include Distributed Data Parallelism [5] and Fully Sharded Data Parallelism [13]. These techniques work by having accelerators split the weights and synchronize the gradients (sometimes multiple times per step), with communication volumes proportional to the model size itself. This gradient synchronization between accelerators traditionally requires specialized high-speed interconnects (e.g. Inﬁniband). Such interconnects represent expensive localized networking topologies, constraining all accelerators to be present in the same data center. However, if the volume of synchronized data could be substantially reduced, these hardware constraints could potentially be relaxed. In this paper, we demonstrate that gradients and optimizer states during the training of large neural networks exhibit signiﬁcant redundancy and are highly compressible. Building on this insight, we develop DeMo, an optimizer that takes advantage of this compressibility to reduce inter-accelerator communication needs by several orders of magnitude. We evaluated DeMo by training standard X: @bloc97 X: @theemozilla Email: emozilla@nousresearch.com This work was done in the authors personal capacity as an independent researcher before joining Anthropic Email: bloc@nousresearch.com LLM architecture (decoder-only Transformer [9]) using both the baseline optimizer (AdamW [6]) in traditional high-speed interconnects and DeMo in bandwidth-constrained scenarios. Our results show that models trained with DeMo meet or exceed the performance of their conventional counterparts. The remainder of this paper is organized as follows. Section 2 reviews the relevant background and related work. Section 3 presents our methodology and theoretical foundations. Section 4 details our experimental setup and results. Finally, Section 5 concludes with implications and future directions."
        },
        {
            "title": "2 Background and Related Work",
            "content": "Various strategies have been developed to mitigate communication overhead in distributed training. For centralized and clock-synchronous training, the most effective techniques can be categorized into three main approaches: Quantization and sparsiﬁcation of gradients. Low-rank projection of gradients. Federated averaging (also known as Local-SGD). We focus exclusively on centralized, clock-synchronous methods, excluding asynchronous and decentralized approaches. Although these latter methods represent important areas of research, they introduce signiﬁcant analytical complexity, often lack generalizability, and depend on speciﬁc network topologies or architectures. Our study instead concentrates on developing generalizable centralized clock-synchronous distributed optimizer. 2.1 Quantization and Sparsiﬁcation Previous work, such as [10], has primarily explored the compressibility of gradients through quantization and sparsiﬁcation, assuming that gradient values are uncorrelated and tolerant of compression errors. However, quantization-based approaches face fundamental limits: 16-bit gradient can be compressed to no fewer than one bit. Although sparsiﬁcation offers theoretically unbounded compression, achieving high compression ratios without degrading training performance remains challenging, making it most suitable for ﬁne-tuning rather than pre-training. 2.2 Low Rank Projection Recent work [12] demonstrated that LLM gradients exhibit very low rank structure during training. This enables the use of Singular Value Decomposition (SVD) to project gradients onto lowerdimensional spaces that preserve the most signiﬁcant directions, substantially reducing storage and communication requirements. However, computing SVD for very large models is computationally expensive, and the projection matrices must be shared or recomputed across nodes. While this overhead can be reduced by computing SVDs less frequently, it remains signiﬁcant bottleneck that scales poorly with model size. Nevertheless, this approachs success in achieving convergence4 parity with full-rank optimizers provides valuable insight: low-rank projection offers advantages over sparsiﬁcation and warrants further investigation."
        },
        {
            "title": "2.3 Federated averaging",
            "content": "Federated averaging [7] reduces communication by allowing nodes to train independently for multiple steps before synchronizing through weight averaging. Essentially, each accelerator node trains independently for ﬁxed number of steps, then synchronizes the accelerator nodes by averaging their weights. While this eliminates the need for per-step gradient communication, it still requires sharing full model parameters during synchronization, incurring bandwidth costs comparable to standard training. Moreover, increasing the steps between synchronizations creates fundamental trade-off: more steps reduce communication but slow convergence. This results in either fast iterations with poor convergence or good convergence with prohibitively slow iterations. Finding optimal hyperparameters becomes challenging as they depend heavily on system-speciﬁc variables (node count, 4Rate of loss decrease per iteration, or per minibatch of data. 2 network bandwidth, architecture, batch sizes, etc.), making federated averaging difﬁcult to deploy as general-purpose optimization solution."
        },
        {
            "title": "3 Methodology",
            "content": "Rather than incrementally modifying existing optimization algorithms, we propose novel decoupled momentum optimization algorithm that intentionally allows and leverages divergent optimizer states across accelerators."
        },
        {
            "title": "3.1 Assumptions",
            "content": "Our method is built on three key conjectures that, while currently lacking formal proofs, are supported by empirical evidence from large-scale neural network training: Conjecture 3.1 The fast-moving components of momentum exhibit high spatial auto-correlation, with most of their energy concentrated in small number of principal components. Conjecture 3.2 Fast-moving momentum components show low temporal variance and should be applied to parameter updates immediately, while slow-moving components exhibit high temporal variance and beneﬁt from temporal smoothing over longer periods. Conjecture 3.3 Slow-moving momentum components, despite their high variance, are crucial for long-term convergence and should be preserved rather than ﬁltered out. We leave formal proofs of these conjectures to later work, but highlight that the optimizer we present was made with all of these assumptions in mind. We hope that by proposing this novel method, it can help develop these ideas further in future research. large leap of faith was needed. 3.2 Algorithm Starting from SGD with Momentum, we make two key modiﬁcations: ﬁrst, we remove the all-reduce operation on gradients gk, decoupling momentum across the accelerators. Second, after updating the momentum, we extract and remove its fast components q, which can be efﬁciently synchronized with minimal communication. Algorithm 1 presents the complete method: Algorithm 1 Decoupled Momentum Optimization Input: learning rate η, decay β (0, 1), parameters xt, momentum mt, hyperparameters s, gt LocalStochasticGradient(xt) mt βmt + gt qt ExtractFastComponents(mt, s, k) mt+1 mt qt Qt Synchronize(qt) xt+1 xt ηQt Get local gradient without all-reduce Accumulate gradient in momentum Extract fast components from Remove from Synchronize across all accelerators Parameter update step"
        },
        {
            "title": "3.2.1 Efﬁcient Extraction of Fast Moving Components",
            "content": "Our goal is to efﬁciently identify and extract the most signiﬁcant momentum components while minimizing computational overhead. While optimal decomposition methods exist, we prioritize practical efﬁciency for large-scale training. For our method to work, we must ﬁrst decorrelate, separate, and extract the principal components from the momentum during training. Assuming Conjecture 3.1 holds, one approach would be to apply spatial KosambiKarhunenLo`eve Transform (KLT) to separate faster-moving components from slower ones. However, computing KLT on momentum tensors for neural networks with billions or trillions of parameters is computationally prohibitive. Alternatively, taking cues from signal processing work, the Discrete Cosine Transform (DCT) can act as an approximation of the KLT, if used for the purpose of energy compaction, as they are both 3 decorrelating transforms. For highly spatially correlated signals, the DCT approximation approaches the KLT [8]. The DCT provides an excellent practical approximation to the theoretically optimal KLT for our purposes, offering three key advantages: Firstly, the DCT is highly parallelizable and is extremely fast to compute on modern GPUs. Furthermore, it being separable transform ensures that its computational complexity scales linearly for 2D, 3D or even n-dimensional signals. Finally, the DCT has ﬁxed orthogonal basis, which means it is possible to perfectly decode DCT-encoded signal without any auxiliary information; the transform matrix and its inverse are known in advance. While not perfect, this approximation enables practical implementation at scale. If 3.1 holds, the DCT should effectively extract fast-moving components from our momentum, as we assume the momentum is spatially auto-correlated. Empirically, we ﬁnd the DCT alone sufﬁciently approximates the principal components. During training, we treat each momentum tensor as d-dimensional auto-correlated signal, and we chunk each momentum tensor of shape (n0, n1, ..., nd1) into contiguous chunks with shape (s0, s1, ..., sd1), where each si is divisor of ni, and apply separable d-dimensional decorrelating DCT transform on all chunks. Next, we ﬁnd the top-k DCT frequencies with the biggest amplitudes in each chunk and treat them as if they were the principal components of the momentum. Both (s0, s1, ..., sd1) and are hyperparameters and control the size of the effective rank of the frequencies that we extract. After extracting the highest energy frequencies, we are left with two tensors of size ( n0 s0 , n1 , k), one tensor representing the discrete frequency bins as integer indices, the other representing the amplitudes as ﬂoating point number. s1 , ..., nd1 sd1 Effectively what we have done here is create fast transform that tries to maximize energy compaction. This way, most of the movement described by the momentum can be compressed down to fewer numbers without resorting to sparsity or quantization, which is similar idea to but is not exactly the same as low rank projection. For ease of reference, we will deﬁne the transform here as follows, where is the vector representing the chunk sizes for each dimension of mt: mt req, mt ampl = p(mt, s, k) (1) We can reverse this transform by ﬁrst scattering both frequency and amplitude tensors onto sparse tensor chunked the same way as before, then apply the inverse DCT transform, obtaining something close to the fast moving components of the original momentum: qt = p1( mt req, mt ampl) (2) The next steps momentum is then set to be equal to the residual, which represents the slow moving components of the original momentum: mt+1 = mt qt (3) Note that because the principal components are removed from the momentum at each step, the momentum decay rate should be lowered in general. For example, β = 0.999 would be more reasonable value for pre-training LLM, instead of the usual β = 0.9. Also, since the DCT is computed on relatively small static chunks of shape (s0, s1, ..., sd1), the required transition matrices can be pre-computed in advance and reused at each iteration, which makes the memory and computational overhead almost negligible if implemented correctly."
        },
        {
            "title": "3.2.2 Low Bandwidth Synchronization",
            "content": "f req, mt ampl from the momentum mt, we perform an all-gather operation along After extracting mt the last dimension of the extracted tensors. This enables each accelerator to perform the same inverse DCT operation by scattering both frequency and amplitude tensors the same way as before, but this time we average the amplitude of any duplicate frequencies. When hyperparameters and are ampl can be orders of magnitude smaller than the model chosen appropriately, tensors mt size, enabling efﬁcient synchronization across accelerators with minimal communication overhead. req, mt Given Conjecture 3.2 and 3.3, here we are effectively averaging all of the fast moving components of the momentum at each step, while letting the slow moving components be decoupled from eachother. If we assume that slow moving components in the gradient are high variance, they will be 4 accumulated over time in the momentum. As such, slow moving components can slowly overtake fast moving components in strength, which is then transmitted and removed from the momentum. From this, we can conclude that slow moving components are gradually transmitted alongside the immediate transmission of fast components. Finally, the gradient descent step is described here, where η is the learning rate and Qt the fast moving components of the momentum accumulated from all accelerators: θt+1 = θt ηQt (4)"
        },
        {
            "title": "3.3 Signum",
            "content": "In order to improve convergence when training LLMs, signum [1] variant of DeMo can be used instead, where the gradient descent step is replaced by: θt+1 = θt η sign(Qt) (5) Since the second moment is not computed here, this variant of DeMo uses less memory for optimizer states as compared to AdamW."
        },
        {
            "title": "4 Experimental results",
            "content": "We evaluated the signum variant of DeMo using OLMo [4], highly reproducible large language model pre-training framework. Adapting OLMo to use DeMo required only including the DeMo optimizer class and disabling gradient synchronization in PyTorch Distributed Data Parallelism [5]. We provide the modiﬁed OLMo code as well as the conﬁguration ﬁles for all experiments in the supplementary material. Our experiments used the Dolma v1.55 dataset for pre-training. As baseline we used the publicly released OLMo-1B6, standard decoder-only Transformer model consisting of 1.18 billion parameters using the AdamW optimizer (β1 = 0.9, β2 = 0.95, weight decay = 0.1) as compared to using the DeMo optimizer (β = 0.999). The learning rate and the AdamW hyperparameters were untouched and set with the suggested defaults. Due to computational constraints, we trained models for 100 billion total tokens rather than the full 3 trillion tokens in Dolma. For complete comparability, we re-trained OLMo-1B with these same 100 billion tokens and adjusted the learning rate schedule accordingly. We also repeated the experiments on smaller 300M model identical to the 1B except halving the models hidden size. All experiments were performed on 64 H100 GPUs with global batch size of 2048 with sequence length 2048 tokens, resulting in per-GPU batch size of 32. Figure 1 shows the cross-entropy training loss of DeMo as compared to the reference AdamW model for various values of the hyperparameters and ﬁxed shape7 of = 64. Additionally we report the ﬁnal training loss, per-GPU communication requirements, and downstream evaluation scores of the Hellaswag [11], ARC-Easy [3], and PiQA [2] tasks for these conﬁgurations as well as = 128 in Table 1."
        },
        {
            "title": "5 Conclusion",
            "content": "In conclusion, we have shown that our proposed DeMo optimization algorithm can act as drop-in replacement to AdamW when training LLMs, with no noticeable slowdown in convergence while reducing communication requirements by several orders of magnitude. The signum variant of DeMo is more memory efﬁcient than AdamW and has negligible compute overhead if we use small precomputed DCT transition matrices. Finally, the LLMs pre-trained with DeMo have equivalent or better scores on multiple standard benchmarks compared to their equivalents trained with AdamW. 5https://huggingface.co/datasets/allenai/dolma/blob/main/urls/v1_5.txt 6https://huggingface.co/allenai/OLMo-1B 7For brevity, = 64 means shape of (64, 64) for 2D parameter tensor and (64, ..., 64) for n-D tensor. 5 300M 1B 3. 3 2.9 AdamW 0 10,000 20, Training Steps 2.9 2.8 2.7 2.6 AdamW 10,000 20,000 Training Steps = 32 = = 8 = 4 = 2 = 1 Figure 1: Convergence of training cross-entropy loss across model sizes trained on 100B tokens of reference AdamW and DeMo with = 64 for various values of the hyperparameter k."
        },
        {
            "title": "Hellaswag",
            "content": "ARC-Easy"
        },
        {
            "title": "PIQA",
            "content": "300M 3 2.95 2.9 2.85 2. 2.8 2.75 2.7 2.65 2.6 1 2 4 8 16 32 k"
        },
        {
            "title": "Loss",
            "content": "1 2 4 8 16 32 0.4 0.38 0.36 0.34 0.32 0.3 0.5 0.48 0.46 0.44 0.42 0.4 0.5 0.48 0. 0.44 1 2 4 8 16 32 0.68 0.66 0.64 1 2 4 8 16 32 1 2 4 8 16 32 1B"
        },
        {
            "title": "Hellaswag",
            "content": "ARC-Easy"
        },
        {
            "title": "PIQA",
            "content": "0.56 0.54 0.52 0.5 0.72 0. 0.68 1 2 4 8 16 32 1 2 4 8 16 32 1 2 4 8 16 32 Figure 2: Final loss and downstream evaluation scores of model sizes trained on 100B tokens for various values of the hyperparameter with = 64. The red line represents the reference AdamW training run."
        },
        {
            "title": "5.1 Reproducibility Statement",
            "content": "As described in Section 4 we chose the OLMo framework and references to maximize reproducability and comparability of our experiments. We have provided as publicly available supplementary material standalone PyTorch implementation of DeMo, as well as the minimal patch to OLMo and conﬁguration ﬁles used for the experiments. We do this in hopes of encouraging independent reproduction and improvement of our method."
        },
        {
            "title": "Model",
            "content": "DeMo 300M = 64, = 32 = 64, = 16 = 64, = 8 = 64, = 4 = 64, = 2 = 64, = 1 = 128, = 32 = 128, = 16 = 128, = 8 = 128, = 4 = 128, = 2 = 128, = 1 AdamW-DDP 300M DeMo 1B = 64, = 32 = 64, = 16 = 64, = 8 = 64, = 4 = 64, = 2 = 64, = = 128, = 32 = 128, = 16 = 128, = 8 = 128, = 4 Final Loss Hellaswag ARC-Easy acc norm acc PIQA acc norm Data Tx MB/step 2.87 2.87 2.88 2.89 2.93 2. 2.88 2.90 2.93 2.98 3.06 3.16 2.98 2.63 2.63 2.64 2.67 2.71 2.76 2.65 2.67 2.72 2.76 0.37 0.38 0.38 0.37 0.36 0.35 0.37 0.37 0.36 0.35 0.33 0. 0.35 0.48 0.47 0.47 0.45 0.44 0.41 0.46 0.46 0.44 0.41 0.46 0.50 0.47 0.47 0.46 0.45 0.50 0.47 0.49 0.46 0.45 0.45 0. 0.55 0.53 0.52 0.52 0.51 0.52 0.53 0.50 0.52 0.50 0.67 0.67 0.67 0.67 0.65 0.65 0.66 0.67 0.66 0.64 0.65 0.63 0.65 0.70 0.70 0.69 0.70 0.69 0. 0.69 0.70 0.68 0.67 29.9 14.9 7.49 3.74 1.87 0.93 7.49 3.74 1.87 0.93 0.46 0.23 636.9 110.32 55.16 27.58 13.79 6.89 3.44 27.58 13.79 6.89 3. AdamW-DDP 1B"
        },
        {
            "title": "0.43\nTable 1: Results of training loss, downstream evaluation scores, and per-GPU communication re-\nquirements of the model sizes and reference trained on 100B tokens",
            "content": "2416.6 0.68 2.73 0."
        },
        {
            "title": "References",
            "content": "[1] J. Bernstein, Y.-X. Wang, K. Azizzadenesheli, and A. Anandkumar. signsgd: compressed In International Conference on Machine Learning, optimisation for non-convex problems. 2018. URL https://api.semanticscholar.org/CorpusID:7763588. [2] Y. Bisk, R. Zellers, R. Le bras, J. Gao, and Y. Choi. Piqa: Reasoning about physical commonsense in natural language. Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 34(05):74327439, Apr. 2020. doi: 10.1609/aaai.v34i05.6239. URL https://ojs.aaai.org/index.php/AAAI/article/view/6239. [3] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018. URL https://arxiv.org/abs/1803.05457. [4] D. Groeneveld, I. Beltagy, P. Walsh, A. Bhagia, R. Kinney, O. Tafjord, A. Jha, H. Ivison, I. Magnusson, Y. Wang, S. Arora, D. Atkinson, R. Authur, K. R. Chandu, A. Cohan, J. Dumas, Y. Elazar, Y. Gu, J. Hessel, T. Khot, W. Merrill, J. D. Morrison, N. Muennighoff, A. Naik, C. Nam, M. E. Peters, V. Pyatkin, A. Ravichander, D. Schwenk, S. Shah, W. Smith, E. Strubell, N. Subramani, M. Wortsman, P. Dasigi, N. Lambert, K. Richardson, L. Zettlemoyer, J. Dodge, K. Lo, L. Soldaini, N. A. Smith, and H. Hajishirzi. Olmo: Accelerating the science of language models. arXiv preprint, 2024. URL https://api.semanticscholar.org/CorpusID:267365485. 7 [5] S. Li, Y. Zhao, R. Varma, O. Salpekar, P. Noordhuis, T. Li, A. Paszke, J. Smith, B. Vaughan, P. Damania, and S. Chintala. Pytorch distributed: experiences on accelerating data parallel training. Proc. VLDB Endow., 13(12):30053018, Aug 2020. ISSN 2150-8097. [6] I. In https://openreview.net/forum?id=Bkg6RiCqY7. Loshchilov International Conference F. Hutter. on Learning and Decoupled weight decay Representations, 2019. regularization. URL [7] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y. Arcas. CommunicationEfﬁcient Learning of Deep Networks from Decentralized Data. In A. Singh and J. Zhu, editors, Proceedings of the 20th International Conference on Artiﬁcial Intelligence and Statistics, volume 54 of Proceedings of Machine Learning Research, pages 12731282. PMLR, 2022 Apr 2017. URL https://proceedings.mlr.press/v54/mcmahan17a.html. [8] N. Roma and L. Sousa. tutorial overview on the properties of the discrete cosine Signal Processing, 91(11):2443 ISSN 0165-1684. doi: https://doi.org/10.1016/j.sigpro.2011.04.015. URL transform for encoded image and video processing. 2464, 2011. https://www.sciencedirect.com/science/article/pii/S0165168411001137. [9] A. Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. [10] J. Wang, Y. Lu, B. Yuan, B. Chen, P. Liang, C. De Sa, C. Re, and C. Zhang. CocktailSGD: Fine-tuning foundation models over 500Mbps networks. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 3605836076. PMLR, 2329 Jul 2023. URL https://proceedings.mlr.press/v202/wang23t.html. [11] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. HellaSwag: Can mathe 57th Annual Meeting of chine really ﬁnish your sentence? the Association for Computational Linguistics, pages 47914800, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1472. URL https://aclanthology.org/P19-1472."
        },
        {
            "title": "In Proceedings of",
            "content": "[12] J. Zhao, Z. Zhang, B. Chen, Z. Wang, A. Anandkumar, and Y. Tian. Galore: Memory-efﬁcient llm training by gradient low-rank projection, 2024. [13] Y. Zhao, A. Gu, R. Varma, L. Luo, C.-C. Huang, M. Xu, L. Wright, H. Shojanazeri, M. Ott, S. Shleifer, A. Desmaison, C. Balioglu, B. Nguyen, G. Chauhan, Y. Hao, and S. Li. PyTorch FSDP: Experiences on scaling fully sharded data parallel, 2023. arXiv: 2304.11277."
        }
    ],
    "affiliations": [
        "Nous Research"
    ]
}