{
    "paper_title": "SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations",
    "authors": [
        "Wentao Guo",
        "Mayank Mishra",
        "Xinle Cheng",
        "Ion Stoica",
        "Tri Dao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Mixture of Experts (MoE) models have emerged as the de facto architecture for scaling up language models without significantly increasing the computational cost. Recent MoE models demonstrate a clear trend towards high expert granularity (smaller expert intermediate dimension) and higher sparsity (constant number of activated experts with higher number of total experts), which improve model quality per FLOP. However, fine-grained MoEs suffer from increased activation memory footprint and reduced hardware efficiency due to higher IO costs, while sparser MoEs suffer from wasted computations due to padding in Grouped GEMM kernels. In response, we propose a memory-efficient algorithm to compute the forward and backward passes of MoEs with minimal activation caching for the backward pass. We also design GPU kernels that overlap memory IO with computation benefiting all MoE architectures. Finally, we propose a novel \"token rounding\" method that minimizes the wasted compute due to padding in Grouped GEMM kernels. As a result, our method SonicMoE reduces activation memory by 45% and achieves a 1.86x compute throughput improvement on Hopper GPUs compared to ScatterMoE's BF16 MoE kernel for a fine-grained 7B MoE. Concretely, SonicMoE on 64 H100s achieves a training throughput of 213 billion tokens per day comparable to ScatterMoE's 225 billion tokens per day on 96 H100s for a 7B MoE model training with FSDP-2 using the lm-engine codebase. Under high MoE sparsity settings, our tile-aware token rounding algorithm yields an additional 1.16x speedup on kernel execution time compared to vanilla top-$K$ routing while maintaining similar downstream performance. We open-source all our kernels to enable faster MoE model training."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 0 8 0 4 1 . 2 1 5 2 : r SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations Wentao Guo1, Mayank Mishra2, Xinle Cheng1, Ion Stoica2, and Tri Dao1,3 1Princeton University 2University of California, Berkeley 3Together AI Correspondence to: wg0420@princeton.edu, tri@tridao.me December 17,"
        },
        {
            "title": "Abstract",
            "content": "Mixture of Experts (MoE) models have emerged as the de facto architecture for scaling up language models without significantly increasing the computational cost. Recent MoE models demonstrate clear trend towards high expert granularity (smaller expert intermediate dimension) and higher sparsity (constant number of activated experts with higher number of total experts), which improve model quality per FLOP. However, fine-grained MoEs suffer from increased activation memory footprint and reduced hardware efficiency due to higher IO costs, while sparser MoEs suffer from wasted computations due to padding in Grouped GEMM kernels. In response, we propose memory-efficient algorithm to compute the forward and backward passes of MoEs with minimal activation caching for the backward pass. We also design GPU kernels that overlap memory IO with computation benefiting all MoE architectures. Finally, we propose novel token rounding method that minimizes the wasted compute due to padding in Grouped GEMM kernels. As result, our method SonicMoE reduces activation memory by 45% and achieves 1.86x compute throughput improvement on Hopper GPUs compared to ScatterMoEs BF16 MoE kernel for fine-grained 7B MoE. Concretely, SonicMoE on 64 H100s achieves training throughput of 213 billion tokens per day comparable to ScatterMoEs 225 billion tokens per day on 96 H100s for 7B MoE model training with FSDP-2 using the lm-engine codebase1. Under high MoE sparsity settings, our tile-aware token rounding algorithm yields an additional 1.16x speedup on kernel execution time compared to vanilla top-K routing while maintaining similar downstream performance. We open-source all our kernels2 to enable faster MoE model training."
        },
        {
            "title": "1 Introduction",
            "content": "Mixture of Experts (MoE) (Shazeer et al. 2017) models have emerged as key technique for scaling up parameters (Kimi et al. 2025; Zhao et al. 2025a) without increasing the training computational requirements. Modern transformers often have layers comprised of sequence mixer block (e.g. Multi-head Attention (Vaswani et al. 2017)), followed by channel mixer block (e.g. dense MLPs) where MoEs are an excellent substitute for dense MLPs for FLOPs efficiency. MoE block is typically composed of token router and multiple smaller and often equal-sized subnetworks, called experts. MoEs can reduce FLOPs consumption during training by only activating subset of all experts per token. However, reducing FLOPs does not directly translate to better hardware utilization since MoE computation features more dynamic IO accesses when each expert needs to gather token embeddings from different positions, and also scatter the results back to the original positions. Moreover, such hardware-unfriendliness becomes worse as experts become more granular (experts have smaller intermediate sizes) and sparser (experts are increased while keeping the number of activated experts constant), shown in Table 1. MoE scaling laws (Clark et al. 2022; Krajewski et al. 2024; Tian et al. 2025) predict better model quality per FLOP with increasing expert granularity (ratio between the models embedding dimension and each experts intermediate size) and sparsity. Recent MoE models like DeepSeek V3 (DeepSeek-AI et al. 2024), Qwen3 MoE (QwenLM 2025) and gpt-oss-120b (OpenAI 2025), have demonstrated superior performance of fine-grained MoEs over coarse-grained MoEs at scale. Besides granularity, the pursuit of MoEs with better model quality while keeping computational requirements constant has also led to modern MoEs becoming sparser. For example, Kimi K2 (Kimi et al. 2025) has the same amount of activated parameters as DeepSeek V3 (DeepSeek-AI et al. 2024) but much larger total parameter count. Overall, granularity and sparsity for MoEs have only increased over time as shown in Table 1. We also note that the pursuit of granularity and sparsity is also adopted by recent alternative architectures to MoE such as PEER (He 2024), Memory Layers (Berges et al. 2024), and 1https://github.com/open-lm-engine/lm-engine 2https://github.com/Dao-AILab/sonic-moe 1 Ultra-Mem (Huang et al. 2025). Though more granular and sparser MoEs increase model quality per FLOP, they suffer from hardware inefficiency due to: (1) larger activation memory footprint for granular MoE models as activation size typically scales linearly with the number of activated experts, (2) lower arithmetic intensity and increased IO cost due to granular experts and (3) wasted computations due to tile quantization effects of grouped GEMM for highly sparse MoEs. The high granularity and sparsity both push MoE training towards the memory-bound regime requiring carefully designed MoE kernels to hide the increased IO costs. Existing state-of-the-art MoE kernels such as ScatterMoE (Tan et al. 2024) and MoMoE (Costin et al. 2025) are not designed to handle these high IO costs and they suffer significant training throughput degradation. Figure 1: SonicMoEs per-layer activation memory footprint (left) stays constant even when expert granularity (d/n where is the embedding dimension and is the expert intermediate dimension) increases, and is 0.20-1.59x more memory-efficient than other baselines. SonicMoEs forward computation throughput (right) reaches an average of 88% (max 91%, min 86%) of the upper bound (cuBLAS BMM + activation + cuBLAS BMM + aggregation on H100). Note that the cuBLAS upper bound baseline does not include the router computation. Here we use 30B MoE configuration with microbatch size of 32768 tokens, and we vary the activated experts / total number of experts as 2/32, 4/64, 8/128, and 16/256 from left to right. We propose to co-design the MoE architecture with GPU kernel tailored to NVIDIA Blackwell and Hopper generation GPUs and novel routing method. (1) We derive an algorithm to compute the MoE backward pass more efficiently leading to much smaller activation memory footprint that does not increase with increasing expert granularity. (2) We leverage new hardware features on Blackwell and Hopper GPUs to overlap memory IO with computation which can benefit all MoEs, and, in particular, fine-grained MoEs. (3) We propose hardware-aware token rounding routing method where the routed number of tokens to an expert is always multiple of the GEMM tile size. Using extensive experiments, we show that token rounding routing is 16% faster than the baseline token-choice routing when we scale up the number of experts 4 times from 30B MoE. We also validate that TR preserves the MoE inference quality on 2B parameter scale. With (1) and (2), we can increase the end-to-end training throughput of 7B MoE model by 50% (without changing the top-K token choice routing). Our token rounding routing method further improves training throughput by 16% when we scale up the number of experts without any accuracy loss. Summary of contributions. We propose SonicMoE, hardware and model architecture co-design solution to address MoE training efficiency problems, making the following contributions: MoE training with minimum possible activation memory footprint without increasing FLOPs: We analyze the impact of MoE granularity on the MoE layers forward and backward passes and observe that increasing MoE granularity while maintaining constant FLOPs leads to linear increase in activation memory required by the backward pass. Leveraging this observation, we carefully redesign the computation graph to avoid caching the activations for the router gradient computation while maintaining the mathematical equivalence to the original MoE formulation. As result, for fine-grained 7B MoE, SonicMoE reduces activation memory usage per layer by up to 45%. Efficient MoE kernel that overlaps IO with computation to yield SOTA training throughput: We show that increasing both granularity and sparsity leads to MoEs becoming increasingly memory bandwidth bound. To alleviate this bottleneck, we exploit the asynchrony of the GEMM and IO operations by overlapping them to maximize throughput. For the same fine-grained 7B MoE model, our approach increases relative speedup by 43% on the forward pass compared to highly optimized DeepGEMM baseline, and by 83% and 115% on the backward pass compared to the state-of-the-art MoE baselines ScatterMoE and MoMoE, respectively. To evaluate the performance of these techniques, we conduct an extensive performance analysis through comprehensive kernel-level profiling and an IO-aware exploration of the MoE computational paths. Token rounding routing that eliminates wasted FLOPs from sparse MoEs: We introduce drop-in routing algorithm that rounds the per-expert token counts to multiples of the tile size (e.g., 128) used by grouped GEMM in MoE kernels. This rounding reduces compute wasted on padding while preserving the original token-to-expert 2 assignment as much as possible. The algorithm ensures that, for each expert, the maximum deviation from the original top-K token-choice result is bounded by one tile. This method effectively eliminates padding waste in grouped GEMM while maintaining the same total number of tokens in expectation, and it delivers robust token-choice accuracy even under highly sparse MoE training regimes. We validate the performance of this token-rounding strategy in 1.4B-parameter sparse training setting, demonstrating that its compute throughput consistently exceeds that of the vanilla top-K token-choice routing. In highly sparse regimes, the improvement reaches up to 16% higher TFLOPS for end-to-end MoE computation. We release SonicMoE, mainly written in CuTe-DSL (NVIDIA 2025c) with PyTorch interface, with permissive license to benefit researchers and practitioners. The GitHub link is https://github.com/Dao-AILab/sonic-moe. Table 1: MoE Scaling Trends: Here, we show the activation ratio as experts activated per token / total experts and expert granularity is shown as model embedding dimension (d) / expert intermediate size (n) for frontier open source models. We do not include the shared experts for the MoE sparsity calculation. The trend indicates new open-source MoE models tend to be more granular and sparser."
        },
        {
            "title": "Model",
            "content": "Release date Parameters Expert activation ratio (K/E) Expert granularity (d/n) Mixtral 8x22B (Mistral 2024) DBRX (The Mosaic Research Team 2024) Phi-3.5-MoE (Microsoft 2024) OLMoE (Muennighoff et al. 2025) Granite 3.1-MoE (Granite 2024) DeepSeek-V3 (DeepSeek-AI et al. 2024) Qwen3 MoE (QwenLM 2025) QWen3-30B-A3B (Qwen 2025) Kimi K2 (Kimi et al. 2025) gpt-oss-120b (OpenAI 2025) GLM-4.5-Air (Zeng et al. 2025) Qwen3-Next-80B-A3B-Instruct (Qwen 2025) DeepSeek-V3.2-Exp (DeepSeek-AI 2025) 11/23 03/24 09/24 09/24 12/24 12/24 04/25 05/25 07/25 08/25 08/25 09/25 10/25 131B 132B 42B 7B 3B 671B 235B 30.5B 1.04T 120B 106B 81B 685B 25.0% (2/8) 25.0% (4/16) 12.5% (2/16) 12.5% (8/64) 20.0% (8/40) 3.13% (8/256) 6.25% (8/128) 6.25% (8/128) 2.08% (8/384) 3.13% (4/128) 6.25% (8/128) 1.95% (10/512) 3.13% (8/256) 6144/16384 = 0.38 6144/10752 = 0.57 4096/6400 = 0.64 2048/1024 = 2.00 1536/512 = 3.00 7168/2048 = 3.50 2048/1536 = 1.33 2048/768 = 2.67 7168/2048 = 3.50 2880/2880 = 1.00 4096/1408 = 2.91 2048/512 = 4.00 7168/2048 = 3."
        },
        {
            "title": "2 Background",
            "content": "We first provide an overview of the MoE architecture and standard MoE kernel employing grouped GEMM in Section 2.1. In Section 2.2, we discuss how granularity and MoE sparsity will affect MoEs training efficiency. We then examine the impact of MoE routing method on the MoE model quality and training efficiency in Section 2.3."
        },
        {
            "title": "2.1 MoE using Grouped GEMM",
            "content": "Modern GPUs support Tensor Cores; specialized hardware units with high matrix multiplication throughput (NVIDIA 2022). GEMM (general matrix multiply) (Lawson et al. 1979) kernel often has 3 stages: prologue (start input loading), mainloop (keep loading inputs and compute GEMM) and epilogue (miscellaneous IO/math operations on GEMM outputs). The kernel tiles computations (dividing large matrices into small tiles), and optionally pads dimensions so computation aligns with hardware-permissible tile sizes. In this paper, we follow standard GEMM notations in most BLAS (Lawson et al. 1979) libraries: we have RMK, RKN, RMN for = AB with problem shape (M, N, K). This notation is adopted by CUTLASS (NVIDIA 2025a) which implements efficient GEMM on CUDA. On NVIDIA Hopper GPUs, GEMM is performed asynchronously with producer-consumer paradigm (Shah et al. 2024) where producers are dedicated to load tile of data from High Bandwidth Memory (HBM), or global memory (GMEM) logically, to shared memory (SMEM) while consumer warpgroups3 are responsible for GEMM computation (Shah et al. 2024). In prologue and mainloop, producer warpgroups fetch tile of data and cache to dedicated pipeline while the consumer warpgroups read from the cached tile from this pipeline, perform tiled matrix multiply (MMA) and accumulate over the dimension of GEMM. After the mainloop, we enter the epilogue stage where the consumer warpgroups apply post-processing (activation function and write results back to HBM) on the final MMA results. MoE block is typically composed of token router and multiple smaller and often equal-sized subnetworks, called experts. The router is responsible for dispatching tokens to the experts which are subsequently used by the specific expert for 3A warp is the basic execution unit on an NVIDIA GPU. warpgroup in Hopper GEMM consists of 4 contiguous warps (128 threads). Hopper GPUs provide high-throughput WGMMA instruction for MMA which is issued collectively by warpgroup. Algorithm 1 MoE forward with Grouped GEMM"
        },
        {
            "title": "Input",
            "content": "d, W1 = {W1,e}e : RT {W2,e}e {0, 1}T sents whether token is routed to expert e. 2n, W2 = d, routing scores RT [E] Rn , π as binary-valued mask matrix where πt,e repre- [E] Rd Output :output activation RT Parallel for [E] do // up-proj Xe Gather(X, π:,e) // varlen-M Grouped GEMM He XeW1,e // apply activation function, e.g. SwiGLU Ae act func(He) // down-proj, varlen-M Grouped GEMM Ye AeW2,e Parallel for [T ] do // expert aggregation Ot = (cid:80) [E] πt,eSt,e Ye,t Figure 2: MoE computation often requires Grouped GEMM. Each expert gathers inputs from different positions on an input tensor (top) or reads contiguous chunk on grouped input array (bottom). This figure is adapted from Tan et al. (2024)s Figure 2. computation. The outputs from all experts in the layer are then aggregated and passed onto the next layer. MoE computation4 can be performed using Grouped GEMM (a list of GEMMs with possibly different {M, N, K} dimensions). Algorithm 1 illustrates running MoE forward with Grouped GEMM. As shown in Algorithm 1, during the forward pass (and backward activation gradient computation), we have variable number of tokens routed to every expert. Grouped GEMM operation with fixed (N, K) dim (as the expert weight matrix) but variable (token dim) is then performed. We refer to this Grouped GEMM as varlen-M Grouped GEMM. During the backward weight gradient computation, the embedding dimension (M for backward) and intermediate hidden size (N for backward) are constant and instead, we reduce over the token dimension (K), which we refer to as varlen-K Grouped GEMM. For each Grouped GEMM, we often have inputs gathered from different positions or contiguously-packed, as illustrated in Figure 2. For example in Algorithm 1, the inputs to up-proj are gathered while the inputs to down-proj are already contiguously-packed."
        },
        {
            "title": "2.2 MoE computation",
            "content": "Arithmetic intensity, defined as the ratio of FLOPs over the number of transferred bytes (IO), is metric to quantify whether kernel is memory-bound (kernel runtime dominated by memory IO cost) or compute-bound (kernel runtime dominated by compute throughput). The standard MoE computation for an expert with SwiGLU activation can be broken down into following components: He = up-projection(Xe) = XeW1,e :RTed RTe2n Ae = SwiGLU(He) :RTe2n RTen Ye = down-projection(Ae) = AeW2,e :RTen RTed (1) (2) (3) where Xe RTed denotes the input received by expert e. Here, the up-projection uses 2Te 2n FLOPs and 2Ted + 2 2n + 2Ten HBM memory transfer bytes (we ignore the writes for He here). Similarly, down-projection uses 2Tend FLOPs with 2Ten + 2nd + 2Ted bytes. Assuming ρ = as the MoE activation ratio, = as the granularity and uniform routing i.e Te = ρ, the arithmetic intensity (ignoring the writes for He) for the forward pass of an expert is Figure 3: IO cost of MoEs forward pass for one layer w.r.t. expert granularity across MoE configurations under isoFLOPs training from 1.4B to 120B (configurations in Table 9a). We keep MoE activation ratio ρ = K/E and the number of parameters of each MoE layer 3dnE constant. When we scale up expert granularity d/n, we scale down expert intermediate size while keeping both nE and nK constant. 2Te 2n + 2Tend 4Ten + 6nd + 4Ted = 3 2 + 3 + 2 Te = 3 2+2G + 3 ρ (4) 4We refer the computation that decides the activated expert for each token and relevant routing metadata as MoE routing, and how each expert process the routed tokens and expert aggregation as MoE computation. Algorithm 2, 3, and 5 are SonicMoEs MoE computation components that are compatible with arbitrary routing algorithms. 4 For specific model size (constant d), it can be seen that increasing granularity (increasing G) or increasing sparsity (decreasing ρ) leads to decreasing arithmetic intensity. This is caused by the linear scaling of IO cost w.r.t. expert granularity, as illustrated in Figure 3. Therefore for the case of fine-grained MoEs (high G)5, it becomes increasingly important to address the increased IO cost by maximally reducing IO access and hiding IO latency. We examine memoryefficient MoE kernel design in Section 3 and discuss techniques to reduce IO access and latency in Section 4. Existing MoE kernel designs. There are multiple MoE implementations available: ScatterMoE (Tan et al. 2024), MoMoE (Costin et al. 2025), MegaBlocks (Gale et al. 2023), and Megatron (Shoeybi et al. 2019). However, they do not specialize for the setting of fine-grained MoEs that have linearly-increasing IO cost w.r.t. increasing expert granularity illustrated in Figure 3. In contrast, our kernel design, SonicMoE, minimizes the impact of IO cost on the training throughput. In Section 4 and Figure 14, we show that when expert granularity increases, SonicMoE demonstrates greater relative speedup over existing MoE kernel designs due to the IO-aware optimizations. We elaborate on the technical differences between SonicMoE and prior MoE kernels in Appendix and include an overview in Table 2."
        },
        {
            "title": "2.3 MoE routing methods",
            "content": "In MoE, routing determines which experts to activate for each token. Token choice (TC) routing where each token independently selects the activated expert is often the default routing method for MoE models (Shazeer et al. 2017). We often have top-K TC routing where the routing decision for token is TopKe[E](St,e, K) and St,e is the expert score for token t. Besides top-K, Huang et al. (2024) introduce token-choice top-P routing to flexibly allocate compute during training. However, it introduces nondeterminism in the number of activated experts and consumed FLOPs per token. Zeng et al. (2024b) also propose similar idea that uses null experts to dynamically adjust the number of activated experts. Besides TC routing, expert choice (EC) routing is developed to avoid load imbalance for expert parallelism (Zhou et al. 2022) by letting experts choose the tokens. However, EC routing is not directly usable for inference because it is incompatible with autoregressive decoding, and switching back to TC at inference time leads to mismatch. In addition, EC breaks causality by future token information leakage (Wang et al. 2024). To address the inference issue of EC routing, Raposo et al. (2024) introduce an auxiliary loss to promote the agreement between TC and EC routing results, or train an auxiliary router to explicitly predict the routing result of EC router and use this auxiliary router during inference. In this paper, we propose novel Grouped GEMM tile-aware token rounding method that rounds the number of received tokens per expert (expert frequency) to nearby multiples of Grouped GEMM tile sizes and alters at most one tile of tokens per expert. This approach effectively reduces wasted FLOPs caused by Grouped GEMM padding during sparse MoE training while preserving inference quality of trained MoE models. There are similar works that propose to drop and reroute tokens, including Rectify-Router (Zeng et al. 2024a), but they do not focus on the tile structure of Grouped GEMM. Other works such as TMA-adaptive FP8 Grouped GEMM (Fu et al. 2025) focus on reducing padding-related load traffic but the FLOPs wasted by non-aligned tile size in GEMM computation is not addressed."
        },
        {
            "title": "3 Memory-efficient MoE algorithm",
            "content": "We first describe SonicMoEs high-level kernel design in Section 3.1 that illustrates SonicMoEs MoE computation4 as shown in Algorithm 2, 3, and 5. We then focus on activation memory usage of SonicMoE in Section 3.2."
        },
        {
            "title": "3.1 Overview of SonicMoE’s MoE kernels",
            "content": "The MoE computation4 in SonicMoE launches 8 kernels: during forward, we have up-proj (A), down-proj (Y ), and expert aggregation (O) kernels; during backward, we have activation gradient kernels for dH (down-proj), (up-proj), dX (aggregating across experts), and weight gradient kernels dW1 and dW2. Figure 4 illustrates the computational workflow of these 8 kernels. We provide an efficient TC top-K router, and an interface that accepts arbitrary routing input. However, it should be noted that SonicMoEs MoE computation is independent of the MoE router choice and is thus compatible with arbitrary router logic. The implementation of SonicMoEs MoE computation is highly modularized: it only consists of (1) optimized grouped GEMM kernel with modularized fusion (2) optimized expert aggregation kernel. The host dispatches to the best GEMM config and load/store strategies to launch the 8 kernels listed above. Besides such high modularity, SonicMoE still exhibits state-of-the-art training throughput and minimum activation memory usage which we describe below. 5Here we refer fine-grained MoE as MoE with small granularity i.e. is smaller than d. We assume the setting of both iso-FLOPs and iso-params. 5 Figure 4: Computational workflow of SonicMoEs 8 launched kernels, grouped by yellow boxes. 3 and 5 kernels are launched during forward and backward computation respectively. The incoming arrows to yellow circle indicate variable loaded from HBM to SRAM, and an outgoing arrow represents variable stored to HBM. We color the boxes of all variables on HBM, with purple boxes indicating the output of forward and backward while blue boxes indicate intermediate variables or weights (W1, W2). We color all cached activations X, H, π, in red. Algorithm 2 formally describes SonicMoEs forward pass, and Algorithm 3 and 5 describe the backward pass."
        },
        {
            "title": "3.2 Activation memory efficiency",
            "content": "The FLOPs of MoE forward and backward computation is (6 + 12)T nKd. For given T, d6, we need to keep nK constant for constant FLOPs. Therefore, increasing granularity requires decreasing and proportionally increasing K. Hence, any activations with memory O(T Kd) should not be cached for backward computation to avoid activation memory scaling with granularity. For current MoE kernels like ScatterMoE, activations scale linearly with expert granularity. Activations (down-proj output) and Xe (gathered X) have size Kd and avoiding caching them eliminates activation memory dependency on granularity. We avoid writing dY (gradient for ) and dOe (gathered dO) to HBM as they increase the peak activation memory during the backward computation: For and dO, fusing the gather operation with the HBM load eliminates the need for materialization and activation caching in HBM. We show in Figures 6 and 20 that this gather fusion significantly improves the throughput for fine-grained MoEs. naive implementation to compute dS and dH would need and dY . Instead, we identify an alternative computation path to compute dS and dH without increasing FLOPs. This is achieved via expanding dS and dH into an equation that does not involve using and dY , as illustrated in Appendix C. SonicMoEs dH kernel is shown in Algorithm 3. As result, we only cache and along with routing metadata for total size 2T + 4T Kn7 bytes per layer. This activation memory usage is the same as dense model with same number of activated parameters, which is the minimum activation memory required for backward computation without doing activation recomputation with GEMM.89 In Figure 13, we profile SonicMoEs activation memory for 7B MoE training configuration and demonstrate that the activation memory of SonicMoE is independent of expert granularity. More results from 1.4B to 120B are included in Figure 13. 6Embedding dimension is often picked independently of MoE layer so we always assume is fixed. 7Routing metadata is negligible in size compared to activations. 8SonicMoEs only recomputation is from H, and this is directly on registers during epilogue of dH. 9Although SonicMoE still materializes temporary variable, we can recycle after each layer. As long as the number of MoE layers (typically 32+ for 7B+ MoE) is larger than K, the transient memory usage of will be overshadowed. Removing such materialization requires an atomic add (Figure 9 right) to global memory which creates new issue with determinism (He and Machines 2025), numerical accuracy (for BF16 atomic add), and incompatibility with expert parallelism all2all or all-gather communication. Algorithm 2 SonicMoEs MoE kernel forward pass. Variables stored in HBM are colored blue. load and store means load from / store into HBM respectively. Input :X, S, π, W1, W2 same as Algorithm 1. Output :MoE layer output Up-proj kernel (X, W1, π) (H, A): // Gather + varlen-M Grouped GEMM + SwiGLU Algorithm 3 SonicMoEs MoE kernel backward pass of down projection. Input :S, π, W2, dO. Output :dH, dW2, dS. Down-proj act dH kernel (dO, W2, S, π) (dH, dS, A): // Gather + varlen-M Grouped GEMM + dSwiGLU + dS // Appendix elaborates this algorithm in more detail Parallel for [E] do Xe, W1,e, π:,e load(Xe, W1,e, π:,e) Xe Gather(X, π:,e) He XeW1,e // apply activation function, e.g. SwiGLU Ae act func(He) He, Ae store(He, Ae) Down-proj kernel (A, W2) : // varlen-M Grouped GEMM Parallel for [E] do Ae, W2,e load(Ae, W2,e) Ye AeW2,e Ye store(Ye) Expert aggregation Kernel (Y, S, π) O: // Gather and sum Parallel for [T ] do Ye,t, St,e, πt,e load(Ye,t, St,e, πt,e) Ot (cid:80) [E] πt,eSt,eYe,t Ot store(Ot) Parallel for [E] do dOe, W2,e, S, π:,e load(dOe, W2,e, S, π:,e) dOe Gather(dO, π:,e) // dA is temp variable for computing dA, dS, and // dA dAe dOe 2,e se Gather(S, π:,e) dAe Broadcast(se) dAe // compute fwd act and bwd act grad simultaneously in dAct call Ae, dHe dAct func(dAe, He) Ae Broadcast(se) Ae // RTen, input for dW2 // reduce over dim dSe,t dAe,t, Ae,t dHe, dS, Ae store(dHe, dS, Ae) RTen Down-proj weight dW2 kernel (dO, A, π) dW2: // Gather + varlen-K Grouped GEMM Parallel for [E] do dOe, Ae, π:,e load(dOe, Ae, π:,e) dOe Gather(dO, π:,e) dW2,e Ae dOe dW2,e store(dW2,e) 4 IO-aware kernel design The expressivity of fine-grained MoE comes from the diversity of every tokens expert selection, which in turn leads to linearly-scaled IO cost w.r.t. expert granularity (Figure 3). To sustain high throughput, we need to maximally (1) reduce IO access via fusion (2) overlap the IO latency with compute. We first examine the token gather fusion with computation, and math and IO fusion with epilogue in Section 4.1.1 and 4.1.2 respectively. We then describe the techniques to overlap MMA with IO in Section 4.2. We examine SonicMoEs top-K sorting kernel in Section 4.3 respectively. In Appendix B, we compare SonicMoE with other MoE kernel designs with summary in Table 2. Features Methods"
        },
        {
            "title": "SonicMoE ScatterMoE MoMoE MegaBlocks Megatron DeepGEMM",
            "content": "Gather fused with GMEM-to-SMEM (HBM) load (Sec. 4.1.1) SwiGLU and dSwiGLU fused with epilogue (Sec. 4.1.2) dS computed as dAe,t, Ae,t (Sec. 4.1.2, App. C.1) Backward epilogue that computes dH, dS together (Sec. 4.1.2) Overlap MMA with epilogue/IO (Sec. 4.2) Do not need separate scatter kernel Efficient top-K sorting (Sec. 4.3) Do not need shape-alignment efforts outside GEMM kernels fwd , bwd fwd , bwd NA NA NA NA NA NA Table 2: Comparison between SonicMoE and prior MoE kernels. means that the kernel implements the feature or functionality similar in semantics, and means the feature is missing from the kernel. NA means that the feature is out of the expected scope. We use the GroupedMLP for Megatron and ParallelDroplessMLP for MegaBlocks. More discussion is included in Appendix B."
        },
        {
            "title": "4.1 SonicMoE’s Grouped GEMM",
            "content": "SonicMoE is built on top of an efficient varlen-M and varlen-K Grouped GEMM. Inside the Grouped GEMM, we fuse the gather operations with the activation loading (4.1.1), and fuse SwiGLU/dSwiGLU/dS with epilogue (4.1.2). The gather fusion helps SonicMoE to be faster than MoE kernel designs that require separate gather kernel such as MegaBlocks, Megatron, and DeepGEMM++, an optimized MoE forward pass implementation built on top of DeepGEMM library (Zhao et al. 2025b). The epilogue fusion boosts SonicMoE to be faster than ScatterMoE in the backward pass. These fusions reduce unnecessary IO access and can be overlapped with compute MMA, as we discuss in Section 4.2."
        },
        {
            "title": "4.1.1 Gather fusion with HBM load",
            "content": "SonicMoEs Grouped GEMM accepts either contiguously-packed inputs or inputs gathered from different positions, illustrated in Figure 2. For the latter case, we fuse the input gather with the input loads from global memory (GMEM, often the HBM) to shared memory (SMEM) so we can batch them to perform GEMM on Tensor Core (Costin et al. 2025; Tan et al. 2024). This involves (1) fetching the routed token indices for each expert and then (2) using these indices to gather activations via Blackwell and Hoppers cp.async instruction. We often have no better alternatives for the second step10, but synchronous index fetching is still optimizable by prefetching and cooperative fetching among producer warps. We illustrate our strategies in Figure 18. Hopper GPUs. As shown in Figure 6, Gather fusion provides SonicMoE with major advantage over existing MoE kernel designs on H100 such as DeepGEMM. Although DeepGEMMs varlen-M Grouped GEMM kernel is highly optimized11, DeepGEMM assumes the inputs are already contiguously packed and padded to multiples of 128, which requires separate kernel launch for gather and pad (either all2all in expert parallelism, or gather kernel on single GPU in our case) before the Grouped GEMM. In Figure 6, even though we can provide an optimized gather kernel and DeepGEMMs varlen-M Grouped GEMM is also highly-optimized, the large amount of IO to gather (2T Kd bytes) still makes DeepGEMM++ (for definition of DeepGEMM++, refer to Figure 6) slower than SonicMoE. In the backward pass, weight gradients for up-proj and down-proj (dW1 and dW2 respectively) need to gather and dO, and the activation gradient for dH also needs to gather dO. Despite the backward having more kernels requiring the gather operation, existing approaches including ScatterMoE (Tan et al. 2024)12 and MoMoE (Costin et al. 2025)13 fuse the gather during forward but still launch separate gather kernel during backward. Fusing this gather reduces the IO cost by 2T Kd bytes and cuts down major portion of fine-grained MoE training time. For example in Figure 6, the 2 gathers in backward of ScatterMoE and MoMoE consume 19.6% and 20.6% total backward time which is even longer than their up-proj weight gradient dW1 kernel time. Blackwell GPUs. SonicMoE supports the varlen-M Grouped GEMM and its gather fusion for Blackwell GPUs at the time of writing this paper. On Blackwell GPUs, the gather fusion with cp.async encounters an architectural challenge when using 2-CTA clusters (Figure 5) for GEMM computation. The cp.async instruction, introduced in the Ampere generation, can only signal completion within the same CTA. However, Blackwells 2-CTA GEMM requires the MMA instruction in the leader CTA (CTA 0) to wait for gather completion from both CTAs. To work around this limitation, CTA 1 requires dedicated relay warp that receives the cp.async completion signal and forwards it to CTA 0s MMA warp using cluster-level synchronization primitives (e.g., mbarrier with cluster scope). This relay mechanism adds scheduling complexity but enables efficient gather fusion across the 2-CTA cluster, maintaining high throughput for Grouped GEMM. Figure 5: Pipeline structure for gather fusion with cp.async on Blackwell GPUs using 2-CTA clusters."
        },
        {
            "title": "4.1.2 Epilogue fusion",
            "content": "We exploit the epilogue computation to maximally reduce unnecessary IO accesses with the following design choices: SwiGLU and dSwiGLU fusion: We fuse the SwiGLU and backward of SwiGLU with the epilogue of forward up-proj and backward down-proj activation gradient kernel respectively (Costin et al. 2025). In Figure 6, even though DeepGEMM++ has highly-optimized Grouped GEMM and SwiGLU kernel, the total time (0.60ms) of DeepGEMM++s up-proj (0.49ms, 629TFLOPS) and SwiGLU (0.11ms, 2.88TB/s) is still longer than SonicMoEs up-proj (0.55ms, 559TFLOPS) despite SonicMoE having an additional Gather fusion besides SwiGLU. Computing dH and dS in backward down-proj activation gradient (dH) kernels epilogue: This heavy epilogue fusion benefits SonicMoE with major speedup over alternative designs. Our dH kernel (0.47ms, 328TFLOPS) produces the same output with far less total time than ScatterMoEs down-proj act (0.43ms, 364TFLOPS), dS (0.24ms), 10On both Hopper and Blackwell GPUs, we have either legacy cp.async instruction or TMA for loading from GMEM to SMEM. The gather on dim would require 1D TMA load with each size 64 4 = 256 (assume 4 pipeline stages and Ktile as 64) that are often slower than direct cp.async call with 4 producer warps due to TMAs extra hardware overhead. The TMA overhead is illustrated in Luo et al. (2025)s Figure 2 and 3. 11https://github.com/deepseek-ai/DeepGEMM/blob/38f8ef73a48a42b1a04e0fa839c2341540de26a6/csrc/jit_kernels/impls/ sm90_bf16_gemm.hpp#L 12https://github.com/shawntan/scattermoe/blob/47b5e1502e5a10e82c8e5945d761b877849871e7/scattermoe/parallel_experts. py#L92 13https://github.com/tilde-research/MoMoE-impl/blob/d6e2d683185bfe4030265c3ca062564356faa61e/momoe/momoe.py#L299 8 Figure 6: Runtime breakdown of different MoE kernels (ms ) on the 7B MoE training with (T, d, n, E, K) = (24576, 1536, 256, 128, 8) on H100. We annotate the model memory bandwidth (TB/s ) for memory-bound kernels (gather, SwiGLU/dSwiGLU, and expert aggregation kernel) and compute throughput (TFLOPS , abbr as TF/s in the figure) for grouped GEMM kernels. Note that this profile is grouped by kernel runtime semantics and one block can contain multiple actual kernel timing results. For example, the router related on left subfigure includes both router GEMM and routing metadata computation time. In addition, we do not consider the CUDA stream bubble time across kernels in this figure. We use the GroupedMLP for Megatron, and ParallelDroplessMLP for MegaBlocks. DeepGEMM does not provide an efficient router implementation, gather and expert aggregation kernels during the forward pass, where we use standard PyTorch implementation (DeepGEMM-pt) or our highly optimized kernels (DeepGEMM++) for them. During the backward pass, both DeepGEMM++ and DeepGEMM-pt use the same computational path as SonicMoE, except we launch separate kernel(s) that compute dS, A, and dSwiGLU together. DeepGEMM++ is effectively the best possible MoE implementation built on top of DeepGEMM SM90 BF16 Grouped GEMM kernels without modifying DeepGEMMs source code. and dSwiGLU (0.33ms) combined together (0.99ms) during 7B MoE training in Figure 6. In addition, SonicMoE also demonstrates speedup over DeepGEMM++ (0.57ms), where we launch an efficient Grouped GEMM (0.32ms, 480TFLOPS) and separate optimized kernel (0.25ms, 2.43TB/s) that computes dSwiGLU and dS together. In Appendix C.1, we show that SonicMoEs dS = dA, = dA, Broadcast(s)A is the computationally and activation memory-efficient choice for fine-grained MoEs. However, both ScatterMoE and MoMoE choose to compute dS as dO, , which requires an additional 2T Kd HBM load cost and requires caching 2T Kd bytes of activation memory. In Figure 6 (right subfigure), ScatterMoE14 launches separate kernel for dS (0.24ms) while MoMoE15 fuses dS with up-proj activation gradient (in total 1.58ms, 196TFLOPS) which takes much longer time than SonicMoEs up-proj activation gradient (0.50ms, 618TFLOPS). The throughput of heavy epilogue fusion on backward down-proj activation gradient dH kernel is boosted by the overlap of asynchronous IO and MMA, which we will elaborate on Section 4.2. Such overlap helps SonicMoE to sustain reasonable training throughput (328TFLOPS) and memory bandwidth (2.14TB/s) simultaneously even with the heavy epilogue fusion (load and S, compute dH, dS, and as inputs to dW2) in dH kernel."
        },
        {
            "title": "4.2 GEMM MMA Overlapping with Asynchronous IO",
            "content": "Hopper GPUs. In NVIDIA Hopper GPUs, GEMM is performed asynchronously with producer-consumer paradigm (Shah et al. 2024). Suppose we have 2 consumer warpgroups, we can either let them cooperatively issue the WGMMA instruction with large tile size, or overlap the IO of 1 warpgroup with GEMM of another warpgroup with smaller tile size. Once this is finished, we switch the roles of the warpgroups (effectively interleaving IO and GEMM). This is often referred to as Ping-Pong scheduling (Shah et al. 2024; Wright and Hoque 2024b) on Hopper GPUs in Figure 7. Ping-Pong scheduling16 is particularly useful to maintain high Tensor Core throughput with heavy epilogue. For example, the down-proj forward kernels epilogue has heavy HBM store IO (2T Kd bytes) relative to the mainloop. In the down-proj activation gradient (dH) kernels epilogue, we need to load (4T Kn bytes) and execute multiple activation and reduction operations to compute and store dH, dS, and as inputs for dW2. We note that the concept of overlapping MMA with IO and Ping-Pong scheduling is known in other places such as Flash Attention 3 (Shah et al. 2024), but the application of 14https://github.com/shawntan/scattermoe/blob/47b5e1502e5a10e82c8e5945d761b877849871e7/scattermoe/parallel_experts. py#L79 15https://github.com/tilde-research/MoMoE-impl/blob/d6e2d683185bfe4030265c3ca062564356faa61e/momoe/momoe.py#L702 16The selection between cooperative and Ping-Pong scheduling is largely determined by the selection of larger tile size or more epilogue overlap. Fine-grained MoE needs both: the dW1, dW2 kernels often have long mainloop and we often need the largest tile so cooperative scheduling nearly always win, while , dH kernels have heavy epilogue and Ping-Pong is usually more favorable. more detailed comparison is included in Vijay Thakkar and Pradeep Ramanis lecture on CUTLASS with page 92-93 in https://drive.google.com/file/d/18sthk6IUOKbdtFphpm_jZNXoJenbWR8m/view. Figure 7: SonicMoEs Ping-Pong warpgroup scheduling on Hopper GPUs. The green arrows indicate that consumer warpgroup signals the start of the epilogue and the other consumer warpgroup can proceed with the MMA. Once this step is complete, the roles of 2 consumer warpgroups is switched. SonicMoE mainly uses Ping-Pong for forward down-proj kernel and backward down-proj activation gradient dH kernel as they both have heavy epilogue. In dH kernel, SonicMoE has an asynchronous TMA load during epilogue, and producer warps need to issue cp.async for gathering dO and load expert weights with TMA. This figure is adapted from Wright and Hoque (2024a)s blog on Ping-Pong scheduling. Ping-Pong scheduling to address the increasing IO costs of fine-grained MoE kernel design is novel. DeepGEMM SM90 BF16 varlen-M Grouped GEMM kernel11 does not implement Ping-Pong scheduling. This design choice is suitable for the case of lightweight epilogue (e.g. up-proj forward) but underperforms (413 TFLOPS and 2.15 TB/s vs. SonicMoEs 485 TFLOPS and 2.52 TB/s) for the case of heavy epilogue in down-proj forward, as shown in Figure 6. In Figure 19, SonicMoEs down-proj has 10.0% higher TFLOPS than DeepGEMM on average. Besides Ping-Pong scheduling, SonicMoE also relies on asynchronous TMA operations to perform GMEM-to-SMEM load and SMEM-to-GMEM store. We overlap the following asynchronous IO with the MMA operations: Asynchronous TMA load during dH kernels epilogue: In the dH kernels epilogue, we need to load to compute dH from dA. We create dedicated pipeline for asynchronous TMA load of to overlap with other epilogue operations across epilogue stages. In Figure 7, the transparent TMA block in the consumer warpgroups illustrates such asynchronous epilogue load. Asynchronous TMA store in forward down-proj and backward up-proj activation gradient kernel: SonicMoE applies asynchronous TMA store for all 6 Grouped GEMMs. In forward down-proj and backward up-proj activation gradient, SonicMoE does not fuse the scatter with HBM store where ScatterMoE17 and MoMoE18 both choose to fuse the HBM store with scatter. This is because the scatter fusion (1) has more synchronous index fetching and address calculations19 and (2) requires synchronous SMEM-to-GMEM store instruction on Hopper GPUs.20 The synchronous GMEM store blocks the execution of MMA of next tile and largely degrade the TFLOPS (20%) in the case of heavy HBM store during forward down-proj and backward up-proj activation gradient kernel computation for fine-grained MoEs, as illustrated by Figure 8. We also note that Ping-Pong warpgroup scheduling cannot fully restore the throughput degradation for synchronous epilogue IO operations as the epilogue consumer warpgroup would be blocked and cannot switch the role with MMA warpgroup until the current synchronous GMEM store is finished. Blackwell GPUs. On NVIDIA Blackwell GPUs, GEMM kernels use the same Ping-Pong scheduling in spirit, but the implementation differs from Hopper. Blackwell introduces Tensor Memory (TMEM), dedicated 256KB on-chip memory per SM organized as 512 columns 128 rows of 32-bit cells (NVIDIA 2025b; Research 2024). The accumulator results from matrix multiplication are stored directly in TMEM rather than in registers, with the 512-column structure naturally enabling two-stage accumulator pipeline. Each stage uses 256 columns: while one stage performs MMA operations via the new UMMA (Unified Matrix Multiply-Accumulate) instruction, the other stage executes the epilogue. Unlike Hoppers WGMMA which required warpgroup-level coordination and consumed significant register memory, Blackwells UMMA is single-threaded asynchronous operation that eliminates register pressure for accumulation. This architectural change allows 17https://github.com/shawntan/scattermoe/blob/47b5e1502e5a10e82c8e5945d761b877849871e7/scattermoe/kernels/ops.py# L56 18https://github.com/tilde-research/MoMoE-impl/blob/d6e2d683185bfe4030265c3ca062564356faa61e/momoe/momoe.py#L166 19For tiled GEMM size of (Mtile, Ntile), we have to compute scatter index for Kd/Ntile times for each output token while for TMA store, we do not need to fetch scatter index in GEMM epilogue. Instead, we will perform gather on the aggregation kernel and in this case, fully reuse the same gather index over the entire row and the number of synchronous index fetching per token is just K. Index reuse over for GEMM w. scatter requires each CTA persistently handle all GEMM tiles over before proceed to next Mtile, which can easily cause under-utilization of Streaming Processors if is small. 20Synchronous st.global is the only available PTX instruction for scatter fusion with HBM store on Hopper GPUs if we do not use TMA 1D store. This is different from the gather case as cp.async is asynchronous but it cannot be used for SMEM-to-GMEM store. Although asynchronous st.async.release.global PTX instruction is available on Blackwell GPUs, the repeated index fetching will still make scatter less favorable option. Figure 8: Illustration to show asynchronous TMA store (top) has higher memory bandwidth and can naturally overlap with TensorCore MMA while synchronous st.global (bottom) PTX instruction, necessary for scatter fusion on Hopper GPUs, blocks the execution of next Tensor Core MMA tile and leads to longer kernel runtime. This figure is supported by the 20.1% speedup on average of SonicMoE (gemm + gth w. sum) (TMA store) over SonicMoE (gemm w. sct + sum) (st.global) in Figure 22s transparent bars. As result, SonicMoE does not fuse scatter with HBM store and instead, lets each token gather the expert results in the expert aggregation kernel. Both ScatterMoE and MoMoE do not adopt such design and SonicMoE can achieve 1.75x and 3.11x speedup respectively on average during forward down-proj kernel in Figure 6. epilogue warps to read and process results from one TMEM stage concurrently with MMA warps accumulating into the other stage, enabling better overlap of epilogue and MMA operations compared to Hoppers ping-pong scheduling. Figure 9: Possible strategies for storing the results and aggregating the results for each token. SonicMoE chooses the first strategy (left) in which each expert directly stores contiguously-packed outputs via TMA in the GEMM epilogue. In the expert aggregation kernel, each token gathers and sums over activated expert outputs. ScatterMoE and MoMoE (middle) choose to fuse HBM store with scatter in epilogue and launch summation kernel afterwards. We note that each token gathering (left) the Grouped GEMM result is equivalent to each expert scattering (middle) the Grouped GEMM outputs. In Figure 22, we implement both strategies on SonicMoE and observe the left strategy can have 17% speedup over the middle strategy. It is also possible to fuse atomic add in the epilogue to circumvent the requirement of an expert aggregation kernel as the right subfigure illustrated. However, this atomic add operation creates new issues like non-determinism (He and Machines 2025) and numerical accuracy (for BF16 atomic add). This figure is adapted from Tan et al. (2024)s Figure 2."
        },
        {
            "title": "4.3 Efficient top-K sorting kernel for MoE",
            "content": "Existing MoE approaches such as ScatterMoE21, MoMoE22, and MegaBlocks23 use the PyTorch top-K (torch.topk) to compute the expert assignments for each token. We find that the PyTorch top-K kernel can take approximately 40% of the routers computation time. We implement an efficient top-K kernel in SonicMoE to reduce the overhead due to PyTorch top-K. Our top-K kernel supports and when 4096 and 1624 and is optimized for the case of large number of tokens . We also offer an optional softmax fusion on top-K values within the top-K kernel. The top-K kernel accepts the router output with shape (T, E) and parallelizes over . The kernel uses bitonic sort (Batcher 1968) over every row (sorts values) and selects the first columns as the sort output. After loading the input, we pack the column indices of the first columns (for argtopK) to the lower log2(E) mantissa bits of FP32 values in registers25, except that we specialize the base sorting cases (number of values 64) to follow the comparison strategies obtained from optimal low-latency sorting networks (Dobbelaere 2025), which provide the minimum number of parallel operation steps and required compare and swap calls. 21https://github.com/shawntan/scattermoe/blob/47b5e1502e5a10e82c8e5945d761b877849871e7/tests/test_mlp.py#L50 22https://github.com/tilde-research/MoMoE-impl/blob/d6e2d683185bfe4030265c3ca062564356faa61e/momoe/topk_router.py# L101 23https://github.com/databricks/megablocks/blob/78eea65fda01e638af36ae38853bc51efb04a4b4/megablocks/layers/router. py#L 24We restrict this and so that we only need thread and warp-level primitives. Larger > 4k requires SMEM buffer and block-level synchronization and is often too large for most MoE models. 25This design assumes that the lower mantissa bits (up to log2(E) with maximum as 12 bits) of FP32 values do not significantly affect the sorting 11 = 0.05% for the top positions. An alternative design for this bit packing is to use FP64 results, or equivalently, the relative sorting gap is larger than 2 values at cost of all slower swapping operations and each thread hold less values. 11 The bitonic compare and merging occurs within the same thread or the same warp via warp-shuffle. Therefore, every swap and merge operation only uses intra-thread or intrawarp registers. This achieves higher memory bandwidth for the kernel over alternative kernel designs such as PyTorch TopK (Paszke et al. 2019), the Triton (Tillet, Kung, and Cox 2019) and Tilelang (Wang et al. 2025) official example, and RTop-K (Xie et al. 2025) in Figure 23. Since the assigned column indices for values on each row are always unique, there will not be any equal numbers after we pack the column index to the lower mantissa bits. Therefore, SonicMoEs top-K kernel is always stable as there will not be any tie-breaking scenarios during bitonic compare and merge. Figure 10: The sorting is conducted over values after we pack the column index bits into lower mantissa bits. This value format ensures stable sorting result. Tritons official top-K kernel follows similar format."
        },
        {
            "title": "5 Token rounding routing",
            "content": "In this section, we analyze the hardware efficiency under sparse MoE training regime and identify that as MoEs become sparser, the wasted compute on padded GEMM tiles accumulate to nontrivial amount, known as tile quantization effects. In response, we propose novel routing method token rounding to eliminate tile quantization effects."
        },
        {
            "title": "5.1 Training efficiency of sparse MoE",
            "content": "Algorithm 4 Token rounding routing"
        },
        {
            "title": "Output",
            "content": ": RT d; number of experts and expected activated number of experts per token; tile size Mtile; router scores [0, 1]T . round and sparsify that determines rounding up or down. : Mtile-rounded router scores SMtile , (1) Top-K token choice sorting (StopK, ItopK) TopK(S, K) (2) Calculate each experts received token frequencies and its Mtilee { ItopK, t} rounded multiples fe (cid:80) 1 feMtile fe/Mtile Mtile feMtile fe/Mtile Mtile (3) Build Top-K-preferred for expert-wise ranking // ensure non-top-K entries are smaller Se Se 1 for [T ] & [K] in parallel do Figure 11: Wasted FLOPs by padding during MoE forward & backward pass with = 16k, = 4k, = 1k, = 4 as illustrated in the bottom right 2 subfigures of Figure 16. St,ItopK(t,k) StopK,t,k (4) Token rounding per expert for [E] do // token ordering and sorted scores πe, se sort(Se) // token rounding for expert πe, se round and sparsify(πe, se, fe, feMtile , feMtile ) SMtile,e Gather(S, πe) Figure 12: demonstration of tile quantization effect for sparse MoE. The rounding subroutine in TR makes binary decision for discarding or padding tokens to guarantee that each expert receives Mtile-multiple number of tokens. Besides granularity, the arithmetic intensity of MoE also depends on the MoE activation ratio ρ as shown in Equation 4. When we scale down ρ, the expected number of received tokens per expert Ee[E]Te = Te = ρ will also linearly decrease and the GEMM computation shifts towards memory-bound regime. Tile quantization effect. GEMM on modern GPUs is often computed in tiles (NVIDIA 2022) and we always need to pad to the next tile-sized multiples if any dimensions of M, N, are not fully divisible by tile sizes. Once the size of input (e.g. token dimension per expert) is small, the wasted TFLOPS by padding can be nontrivial. Therefore, we propose to use token rounding to avoid launching such extra tiles, thereby leading to more efficient training. We also empirically show that our token rounding method does not affect model quality while achieving much higher training throughput."
        },
        {
            "title": "5.2 Token rounding routing",
            "content": "As such, we propose to use token rounding (TR) method as 2-step sorting algorithm as shown in Algorithm 4. The token rounding algorithm first computes the vanilla token-choice (TC) routing results and applies sorting of the router score over each experts tokens, similar to EC sorting step. We then choose to either discard tokens selected in the first step of TC top-K routing or pad additional tokens on the second step of sorting. Between these 2 steps, we process the routing weight matrix such that the TC tokens are always preferred over EC tokens. This is done so that both discarding or padding only affects the last input tile for each expert. Token rounding requires round and sparsify subroutine for making binary decision for discarding or padding. Our default choice for such subroutine is to round expert frequency to the nearest Mtile multiples: we choose to pad EC selected tokens if feMtile fe is smaller than fe feMtile . 26 We further conduct an ablation in Table 6 and find that (1) our TR algorithm is quite robust w.r.t. the underlying rounding subroutine (2) this simple strategy of nearest rounding on expert frequency is often sufficient to yield excellent task performance. More detailed discussion on different rounding subroutines are included in Appendix F.2. MoE training & inference quality. This simple algorithm guarantees that for each expert, the maximum deviation from token-choice routing is at most 1 tile. We find that this property has surprisingly robust performance even under sparse MoE training regime and can serve as substitute for token-choice under sparse MoE training settings, which is shown in Table 3. We also conduct an ablation study on the effect of microbatch size and tile size Mtile on the quality of trained MoE model with TR in Table 7 and 8, and we find token rounding routing is generally robust when Te/Mtile 2. MoE training throughput. TR guarantees no tile quantization effects and in Section 6.3.3, we show that TR training throughput over vanilla TC top-K is consistently higher when in the highly sparse MoE training regime and can achieve 16% higher TFLOPS for the kernel runtime as we scale up while keeping constant."
        },
        {
            "title": "6 Experiments",
            "content": "Figure 13: Peak activation memory usage per layer across different model scales (1.4B120B). MegaBlocks does not support small n. The benchmark configurations are listed in Table 9b. We only cache X, gathered Xe, He for each expert and routing metadata which is the minimum amount of activation memory required for backward computation without GEMM recomputation. We evaluate SonicMoEs activation memory footprint (Section 6.1) and training throughput (Section 6.2) compared to other baseline MoE implementations. We also demonstrate the efficacy of token rounding routing strategy and show that it is possible to use token choice as drop-in replacement after training with token rounding in Section 6.3.1. We also show that token rounding can maintain the training throughput under sparse MoE configuration in Section 6.3.3."
        },
        {
            "title": "6.1 SonicMoE’s activation memory",
            "content": "We demonstrate that the peak activation memory for SonicMoE has the lowest activation memory footprint for single MoE layer as shown in Figure 13 across all scales. For the 7B model with = 256, our approach reduces memory usage by 45% compared to ScatterMoE, and more significantly compared to MoMoE. For 30B and 120B models, the gap becomes even wider: at 120B scale, our method saves more than 3GiB memory per layer compared to MoMoE. We also validate that SonicMoEs activation memory stays constant w.r.t. expert granularity as shown in Figure 1. 26For simplicity, we always use Mtile as 128 in Table 3 and Figure 16, but we acknowledge that Mtile is GPU-dependent and in some cases (d, n)- dependent e.g., when we have small d, on N, dimension, we might prefer to have larger Mtile to saturate the tensor pipes. Here we focus on varlen-M Grouped GEMM as it consists 12/18 FLOP fraction during MoE training. The other 6/18 FLOP are consumed by varlen-K Grouped GEMM in weight gradient kernel, and in this case padding occurs on Ktile instead of Mtile (Ktile is often smaller than Mtile so we do not focus on this in this paper)."
        },
        {
            "title": "6.2.1 Entire forward and backward throughput",
            "content": "Figure 14 reports the compute throughput of forward and backward pass of one MoE layer in various MoE training configurations. Across all model scales, our method consistently achieves the highest TFLOPS. In 1.4B and 7B settings, our approach improves TFLOPS by 40% compared to ScatterMoE and MoMoE. This throughput gap becomes wider for 30B and 120B MoE as SonicMoE achieves over 500 TFLOPS in forward and backward passes, whereas other baselines either fail to support certain sizes (MegaBlocks) or suffer from significant performance degradation (MoMoE). SonicMoE also demonstrates speedup over DeepGEMM++ in the forward pass, which mainly arises from the gather kernel and Ping-Pong scheduling. The effect of both features increases as the MoE becomes more fine-grained (as we move from right to left across all configs in Figure 14) and thus SonicMoEs relative speedup over DeepGEMM++ becomes larger. We further measure the real training throughput of 7B MoE model with FSDP-2: SonicMoE on 64 H100s gets 213 billion tokens per day which achieves similar throughput as ScatterMoE on 96 H100s with 225 billion tokens per day. The throughput for this is measured using the lm-engine codebase27 (Mishra 2024). We shard the model using ZeRO-3 within single node (8x H100s) and replicate this sharded unit across nodes for this experiment. Figure 14: Forward & backward TFLOPS for different MoE kernels on H100. DeepGEMM does not provide an efficient router implementation, gather and expert aggregation kernels, where we use standard PyTorch implementation (DeepGEMM-pt) or our highly optimized kernels (DeepGEMM++) for them. During the backward pass, both DeepGEMM++ and DeepGEMM-pt use the same computational path as SonicMoE, except we launch separate kernel(s) that compute dS, A, and dSwiGLU together. The MoE configurations are the same as in Figure 13. In addition, we measure the training throughput of single MoE layer with configurations from recent open-source MoEs in Figure 15. SonicMoE generally achieves more than 550 TFLOPS during both forward and backward pass, and consistently surpasses all baselines. We note that ScatterMoE, MoMoE, DeepGEMM-pt, and DeepGEMM++ all fail to run at the configuration for DeepSeek-V3.2-Exp, 685B MoE model, while SonicMoE successfully runs on single H100 GPU without expert parallelism achieving 534.8 TFLOPS during forward and 480.1 TFLOPS during backward. We also note that SonicMoEs IO-aware kernel design can achieve greater relative speedup over baselines (61% over ScatterMoE, 92% over MoMoE during forward, 85% over ScatterMoE, 120% over MoMoE during backward) for sparse and fine-grained MoEs (e.g. ρ = 10/512 and = 2048/512 for Qwen3-Next-80B-A3B-Thinking, 4th column for Firgure 15)."
        },
        {
            "title": "6.3.1 Token rounding’s general task evaluation",
            "content": "In this section, we assess the quality of trained MoEs using our token rounding (TR) algorithm. We use TR for training and during evaluation we switch to token-choice top-K (TC top-K) routing. This assesses the capability of replacement of TR with TC after training.28 We use the OLMoE codebase and construct MoE models with OLMoE base architecture 27https://github.com/open-lm-engine/lm-engine 28Token rounding is not token-choice routing method which creates difficulty for autoregressive generation. Here we do not apply any adaptation and switch to vanilla token choice top-K routing during evaluation/validation. 14 Figure 15: Forward & backward TFLOPS of single MoE layer for different MoE kernels for different configurations ranging from 7B to 685B parameters on H100. The MoE configurations from left to right adopt the model size of OLMoE-1B-7B-0125 (Muennighoff et al. 2025), gpt-oss-20b (OpenAI 2025), Kimi-Linear-48B-A3B-Base (Zhang et al. 2025), Qwen3-Next-80B-A3B-Thinking (Qwen 2025), Qwen3-235B-A22B-Thinking-2507 (Qwen 2025), and DeepSeek-V3.2-Exp (DeepSeek-AI 2025). For fair comparison, we do not consider shared experts and expert biases, and we always use TC top-K router with softmax scores. ScatterMoE, MoMoE, DeepGEMM-pt, and DeepGEMM++ all fail to run (either due to index overflow or CUDA OOM errors) for the DeepSeek-V3.2-Exp configuration. (Muennighoff et al. 2025). We use deduplicated version of FineWeb-Edu (Ben Allal et al. 2024) for training all our models. More details are included in Appendix H. We consistently use Mtile = 128 in Table 3, and the round and sparsify subroutine always rounds the expert frequency to the nearest multiple of Mtile (NR-f, see Appendix F.2). We also use softmax renormalization for TR. We compare TR to token-choice (TC) top-K routing and expert-choice (EC) routing (Zhou et al. 2022). However, EC routing results in future token leakage causing problems for autoregressive generation resulting in performance drop during evaluation (Raposo et al. 2024; Wang et al. 2024). To address this issue, we consider MoDs approach (Raposo et al. 2024) that trains an auxiliary router to predict the EC routers selection during inference29. This baseline is referred as EC (aux router) in each subtable in Table 3. We also adapt EC routing to TC routing by finetuning learned TC top-K router and compare its task performance against TRs task performance without any adaptation. This is the EC (ft TC router) baseline in Table 3. Finally, we consider token dropping baseline in which we set the capacity of each expert as the largest multiple of Mtile not exceeding its token frequencies and we discard the tokens with lowest scores. This is the TC (token drop) baseline, and we note that this is equivalent as we always round down in TR. TRs train-test gap. We validate TRs performance on 0.5B (subtable 3a) and 1.4B (subtable 3c) MoE model. We then increase the MoE sparsity by either decreasing while keeping constant (from 3a to 3b, from 3d to 3e) or increasing while keeping constant (from 3a to 3c). Across these sparse MoE configurations, we consistently observe similar model quality between TR and TC. In fact, TR achieves slightly lower validation perplexity and higher average accuracy under the extremely sparse MoE (K/E < 1/32) settings for the 3c and 3e. There is noticeable discrepancy between EC and TC as the train and val PPL for EC can have 3 gap for 3c,3d and 3e compared to TC and TRs usual 0.3 gap. TC finetuning is more effective than the auxiliary router to close this gap, but TRs task evaluation is still always better. In addition, when we compare TR with the token dropping baseline, we also find TR consistently yields lower validation perplexity, and has higher average task accuracy for 3a, 3c, 3e. In this case, TR can serve as an in-place substitute for TC during training."
        },
        {
            "title": "6.3.2 Ablation studies on token rounding routing",
            "content": "There are 3 variables that can affect the trained MoE quality with token rounding routing: (1) rounding subroutine round and sparsify (2) microbatch size , and (3) tile size Mtile for rounding. We analyze their impacts: Choice of rounding subroutine: In Table 6, we assess the choice of different routing subroutines to train MoEs using TR. We find that our token rounding algorithm in general is robust to the specific rounding subroutines, and nearest rounding expert frequency to multiples of Mtile (NR-f in Table 6) is often sufficient for providing an excellent downstream task performance despite its simplicity. Therefore, we choose NR-f as the default rounding subroutine. Effect of microbatch size and tile size Mtile: The token rounding is applied on the microbatch level so varying the microbatch size will result in different qualitative results for TR. This also holds true for EC routing. For example, 29However, for MoE we are solving harder E-label prediction problem instead of MoDs(Raposo et al. 2024) binary prediction problem. This is because EC router can activate arbitrary number of experts for each token, and we have to independently predict the label for each expert. This approach is likely not scalable for MoE as the prediction problem size scales with E. 15 Table 3: Comparison of different routing methods task evaluation. Train and Val refer to the perplexity towards the end of training and on the validation set respectively. The next 11 columns are downstream tasks evaluated at the end of training and we report the accuracy for each. Avg is the mean accuracy across these 11 downstream tasks. We use TC top-K routing for TR, token dropping, and EC baselines when evaluating validation perplexity and task performance. Te represents the average number of received tokens in each microbatch per expert. (a) 0.5B params, 20B tokens, 8/64 activated ( Te = 4096, Mtile = 128) COPA CSQA BoolQ ArcE ArcC Method Train Val Wino SIQA SciQ PIQA OBQA TR TC top-K TC (token drop) EC EC (aux router) EC (ft TC router) 15.91 16.04 16.52 16.25 16.25 16.34 15.94 16.01 16.46 17.23 17.40 16.40 51.9 51.0 51.1 51.0 52.6 49. 41.3 41.4 41.1 41.0 41.5 41.4 80.8 79.2 79.5 78.3 77.3 78.0 65.5 65.5 64.6 63.8 64.4 64.4 35.0 31.6 30.2 33.4 31.4 33.4 HS 38.7 38.4 37.3 37.5 37.5 37. 63.0 66.0 63.0 69.0 65.0 67.0 31.2 31.5 31.8 31.4 30.9 30.8 TR TC top-K TC (token drop) EC EC (aux router) EC (ft TC router) TR TC top-K TC (token drop) EC EC (aux router) EC (ft TC router) TR TC top-K TC (token drop) EC EC (aux router) EC (ft TC router) TR TC top-K TC (token drop) EC EC (aux router) EC (ft TC router) 16.22 16.34 16.44 16.83 16.80 16.81 13.34 13.51 13.62 14.92 14.94 14.81 13.51 13.50 13.52 14.41 14.34 14.67 13.31 13.50 13.35 14.08 14.01 14.24 (b) 0.5B params, 40B tokens, 2/64 activated ( Te = 512, Mtile = 128) 15.92 15.94 16.10 18.61 21.80 16. 51.4 51.0 51.1 49.6 50.0 50.0 41.6 41.9 41.4 41.4 40.9 41.7 78.4 78.5 78.7 79.1 75.2 79.7 65.4 64.8 64.9 64.4 63.7 64.9 31.6 33.0 31.6 33.4 28.2 31.6 38.1 38.1 38.0 36.9 35.2 36. 65.0 67.0 62.0 62.0 61.0 63.0 31.0 30.8 32.8 32.8 31.5 32.1 (c) 1.8B params, 40B tokens, 8/256 activated ( Te = 512, Mtile = 128) 13.10 13.12 13.19 19.82 18.01 15.01 53.4 50.1 55.4 51.9 50.6 52.7 42.1 42.9 41.6 40.8 41.8 41. 81.7 81.3 82.2 77.7 79.8 79.6 69.6 69.8 68.6 65.8 65.8 66.9 35.2 33.8 34.8 30.0 31.6 30.6 45.3 45.2 45.0 39.8 39.3 40.2 70.0 71.0 69.0 67.0 62.0 66.0 33.2 34.1 34.0 30.9 31.8 31. (d) 1.4B params, 50B tokens, 8/128 activated ( Te = 2048, Mtile = 128) 13.28 13.32 13.30 17.37 26.96 14.90 52.6 51.8 51.8 51.4 49.8 51.9 42.6 41.7 42.2 42.0 41.5 41.8 81.5 81.5 84.1 79.7 79.1 80.1 69.6 69.3 69.2 66.3 63.1 66. 33.6 32.4 34.4 32.2 30.2 32.6 45.4 45.3 45.2 40.7 37.6 41.1 67.0 68.0 70.0 64.0 61.0 65.0 34.8 34.5 35.1 31.8 31.0 32.4 (e) 1.4B params, 100B tokens, 2/128 activated ( Te = 512, Mtile = 128) 13.22 13.32 13.29 24.79 37.52 14. 52.8 51.3 50.0 51.5 49.7 52.2 41.8 42.0 42.2 41.7 40.2 42.6 80.8 83.2 81.7 81.0 73.6 79.4 68.7 68.2 68.3 66.1 57.5 65.7 33.0 34.0 31.2 33.2 27.6 32.8 43.4 43.4 43.3 40.6 33.2 40. 67.0 66.0 66.0 64.0 61.0 64.0 33.6 35.4 34.3 34.0 27.8 34.9 61.4 60.2 58.2 54.4 55.4 56.1 61.1 54.7 61.9 60.2 57.2 60.7 61.4 56.7 54.4 60.7 59.7 60.5 57.3 56.6 61.2 59.0 60.9 57. 60.2 57.9 56.6 56.3 58.8 58.3 58.9 57.5 57.9 56.1 55.8 55.4 27.1 25.7 28.4 29.4 30.4 29.4 57.4 55.8 58.9 55.8 53.3 54.6 63.0 64.6 63.5 56.0 55.8 57.2 63.7 63.2 64.2 57.4 46.7 57. 60.7 61.6 59.5 56.5 45.2 57.2 29.1 30.1 30.8 29.1 24.7 27.4 33.4 31.1 31.4 28.4 29.8 30.8 28.1 28.4 31.4 27.4 25.1 27.8 29.8 29.4 30.8 27.4 24.2 27.1 Avg 50.4 49.8 49.4 49.7 49.3 49.3 50.0 49.6 50.2 49.5 47.4 49.3 53.5 52.8 52.7 49.9 49.8 50.7 52.4 52.1 53.5 50.2 47.8 50.4 52.0 52.0 51.3 50.2 45.3 50.5 EC over sequence will result in different model quality as EC over text segment. Nevertheless, in Table 7, we find that TR perserves its trained MoE quality when Te/Mtile 2, and even if Te/Mtile = 1 (the last row in both subtables), the trained MoE inference quality is still better than training with EC and finetuning with TC top-K routing. Similarly in Table 8, we can find that TR is generally robust w.r.t. Mtile when Te/Mtile 2. However, when Te/Mtile = 1 there is noticeable degradation compared to TC baseline but the model quality is still better than the EC baseline."
        },
        {
            "title": "6.3.3 Token rounding’s training throughput",
            "content": "In Figure 16, we benchmark the token roundings MoE main kernel runtime (without router) against top-K token choice routing. We focus on the iso-FLOPs setting by keeping , and constant. We linearly increase the number of experts while keeping constant to increase MoE sparsity. As we linearly increase E, we observe drop in TFLOPS for token-choice routing. This is due to the (1) tile quantization effect as the wasted FLOPs spent on padding roughly linearly increases with the MoE sparsity as shown in Figure 11 and (2) the linearly increased IO due to more expert weights. We observe drop in both TC and TR drop FLOPs as we increase E, but the drop is more pronounced for TC as shown in Figure 16. For the 3rd and 4th column in the top row in Figure 16, an MoE model with 128 experts (K/E = 1/64) and = 1k with 16 Figure 16: Forward & backward model TFLOPS for SonicMoE MoE kernels with different routing methods. We compare TR equipped with nearest rounding to Mtile-multiples via expert frequency subroutine against TC top-K routing. Configuration details are in Appendix G. token rounding routing achieves 16.5% model TFLOPS30 improvement in forward and 6.1% in backward resulting in an end-to-end improvement of 9.4%. For the 3rd and 4th column on the bottom row in Figure 16, when we have MoE with 256 experts (K/E = 1/128), token rounding routing achieves 25.7% TFLOPS improvement in forward and 11.8% in backward resulting in an end-to-end improvement of 15.9%. In general, we observe that as we move to larger intermediate sizes (more compute-bound) and higher MoE sparsity, the gap between TR and TC top-K becomes larger. This trend also holds with configurations from recent open-source MoEs in Figure 17. When we equip SonicMoEs MoE kernel with TR router instead of TC top-K router, we observe larger relative speedup for highly sparse MoEs such as Qwen3-Next-80B-A3B-Thinking (K/E = 10/512), where TR achieves 19.6% and 7.9% speedup over TC top-K router during forward and backward pass respectively. Figure 17: Forward & backward TFLOPS of single MoE layer of SonicMoE equipped with different routing methods for different configurations ranging from 7B to 685B parameters on H100. The MoE configurations from left to right adopt the model size of OLMoE-1B-7B-0125, gpt-oss-20b, Kimi-Linear-48B-A3B-Base, Qwen3-Next-80B-A3B-Thinking, Qwen3-235B-A22B-Thinking-2507, and DeepSeek-V3.2-Exp (configurations identical to Figure 15). We compare TR equipped with nearest rounding to Mtile-multiples via expert frequency subroutine against TC top-K routing."
        },
        {
            "title": "7 Conclusion",
            "content": "We present SonicMoE, co-design solution that jointly optimizes MoE architecture and GPU kernels to address the training challenges of granular and sparse MoEs. Our contributions include: (1) memory-efficient algorithm that minimizes 30Note that the consumed FLOPs are calculated from (6 + 12)dn((cid:80) fe = for TC top-K routing) as model FLOPs rather than hardware FLOPs. The speedup behind TR is to preserve the model FLOPs consumption on expectation but save the hardware FLOPs consumption by removing padding wastes, which in turn leads to model TFLOPS speedup. fe) (note that (cid:80) 17 activation size as MoEs become more fine-grained, (2) GPU kernels that overlap IO with computation for throughput improvement, and (3) tile-aware token rounding that yields additional speedup without quality loss. Future directions include extending to low-precision and microscaling formats (FP8, MXFP8, MXFP4) for further memory savings, and overlapping communication with computation in distributed settings like expert parallelism. We envision future model architecture designs that optimize for quality per compute hour rather than just quality per FLOPjointly considering algorithmic and hardware efficiency."
        },
        {
            "title": "Acknowledgment",
            "content": "We gratefully acknowledge the support of Schmidt Sciences AI2050 fellowship, the Google ML and Systems Junior Faculty Awards, and the Google Research Scholar program. We also thank the Princeton Language Intelligence program for the computing resources support. We thank Shawn Tan for his generous support on our experiments. We also thank Songlin Yang, Yilong Zhao, Bharat Runwal, Xinyu Yang, Andrew Sheinberg, Lijie Yang, Yongye Zhu, Zhuoqing Song and numerous anonymous reviewers for providing valuable feedback. References [1] K. E. Batcher. Sorting networks and their applications. In: Proceedings of the April 30May 2, 1968, Spring Joint Computer Conference. AFIPS 68 (Spring). Atlantic City, New Jersey: Association for Computing Machinery, 1968, 307314. ISBN: 9781450378970. DOI: 10.1145/1468075.1468121. URL: https://doi.org/10.1145/ 1468075.1468121. [2] Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von Werra. SmolLM-Corpus. 2024. URL: https://huggingface.co/datasets/HuggingFaceTB/smollm-corpus. [3] Vincent-Pierre Berges, Barlas Oguz, Daniel Haziza, Wen-tau Yih, Luke Zettlemoyer, and Gargi Ghosh. Memory layers at scale. In: arXiv preprint arXiv:2412.09764 (2024). [4] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: Reasoning about Physical Commonsense in Natural Language. In: Thirty-Fourth AAAI Conference on Artificial Intelligence. 2020. [5] Aidan Clark, Diego de Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Unified scaling laws for routed language models. In: International conference on machine learning. PMLR. 2022, pp. 40574086. [6] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019, pp. 29242936. [7] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. In: arXiv:1803.05457v1 (2018). [8] Feder Cooper, Wentao Guo, Duc Khiem Pham, Tiancheng Yuan, Charlie Ruan, Yucheng Lu, and Christopher De Sa. Coordinating distributed example orders for provably accelerated training. In: Advances in Neural Information Processing Systems 36 (2023), pp. 5619856210. [9] NVIDIA Corporation. 2025. URL: https://docs.nvidia.com/cutlass/media/docs/cpp/cutlass_ 3x_backwards_compatibility.html. [10] Bobby Costin, Timor Averbuch, Dhruv Pai, Nathan Chen, and Ben Keigwin. MoMoE: Memory optimized Mixture of Experts. In: Tilde Research Blog (July 2025). Blog post. URL: https://www.tilderesearch.com/blog/ momoe. [11] DeepSeek-AI. DeepSeek-V3.2-Exp: Boosting Long-Context Efficiency with DeepSeek Sparse Attention. 2025. [12] DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, and et al. DeepSeek-V3 Technical Report. 2024. arXiv: 2412.19437 [cs.CL]. URL: https://arxiv.org/abs/2412.19437. [13] Bert Dobbelaere. 2025. URL: https://bertdobbelaere.github.io/sorting_networks.html. [14] Raaz Dwivedi and Lester Mackey. Kernel thinning. In: Journal of Machine Learning Research 25.152 (2024), pp. 177. [15] Efficient GEMM in CUDA. 2025. URL: https : / / docs . nvidia . com / cutlass / media / docs / cpp / efficient_gemm.html#hopper-warp-specialization. [16] William Fedus, Barret Zoph, and Noam Shazeer. Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. 2022. arXiv: 2101.03961 [cs.LG]. URL: https://arxiv.org/abs/2101. 03961. 18 [17] Rong Fu, Weihan Cao, Jianfei Gao, Minxi Jin, Hui Wang, et al. TMA-Adaptive FP8 Grouped GEMM: Eliminating Padding Requirements in Low-Precision Training and Inference on Hopper. In: ES-FoMo III: 3rd Workshop on Efficient Systems for Foundation Models. 2025. [18] Trevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia. Megablocks: Efficient sparse training with mixture- [19] of-experts. In: Proceedings of Machine Learning and Systems 5 (2023), pp. 288304. IBM Granite. Granite 3.1 Language Models. https : / / github . com / ibm - granite / granite - 3 . 1 - language-models. GitHub repository. 2024. [20] Horace He and Thinking Machines. Defeating Nondeterminism in LLM Inference. 2025. URL: https://thinkingmachines. ai/blog/defeating-nondeterminism-in-llm-inference/#true-on-policy-rl. [21] Xu Owen He. Mixture of million experts. In: arXiv preprint arXiv:2407.04153 (2024). [22] Quzhe Huang, Zhenwei An, Nan Zhuang, Mingxu Tao, Chen Zhang, Yang Jin, Kun Xu, Liwei Chen, Songfang Huang, and Yansong Feng. Harder Task Needs More Experts: Dynamic Routing in MoE Models. In: Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2024, pp. 1288312895. [23] Zihao Huang, Qiyang Min, Hongzhi Huang, Yutao Zeng, Defa Zhu, Ran Guo, et al. Ultra-Sparse Memory Network. In: The Thirteenth International Conference on Learning Representations. 2025. [24] Matt Gardner Johannes Welbl Nelson F. Liu. Crowdsourcing Multiple Choice Science Questions. In: 2017. [25] Team Kimi, Yifan Bai, Yiping Bao, Guanduo Chen, and et al. Kimi K2: Open Agentic Intelligence. 2025. arXiv: [26] 2507.20534 [cs.LG]. URL: https://arxiv.org/abs/2507.20534. Jakub Krajewski, Jan Ludziejewski, Kamil Adamczewski, Maciej Pioro, Michał Krutul, Szymon Antoniak, Kamil Ciebiera, Krystian Krol, Tomasz Odrzygozdz, Piotr Sankowski, et al. Scaling laws for fine-grained mixture of experts. In: arXiv preprint arXiv:2402.07871 (2024). [27] Chuck Lawson, Richard J. Hanson, David Kincaid, and Fred T. Krogh. Basic linear algebra subprograms for Fortran usage. In: ACM Transactions on Mathematical Software (TOMS) 5.3 (1979), pp. 308323. [28] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding. In: International Conference on Learning Representations. 2024. [29] Yucheng Lu, Wentao Guo, and Christopher De Sa. Grab: Finding provably better data permutations than random reshuffling. In: Advances in Neural Information Processing Systems 35 (2022), pp. 89698981. [30] Weile Luo, Ruibo Fan, Zeyu Li, Dayou Du, Hongyuan Liu, Qiang Wang, and Xiaowen Chu. Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and Multiple Level Analysis. In: arXiv preprint arXiv:2501.12084 (2025). [31] Microsoft. Announcing the Availability of PHI-3.5 MoE in Azure AI Studio and GitHub. https://techcommunity. microsoft.com/blog/azure-ai-foundry-blog/announcing-the-availability-of-phi3-5-moe-in-azure-ai-studio-and-github/4256278. Microsoft Tech Community. 2024. [32] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can Suit of Armor Conduct Electricity? New Dataset for Open Book Question Answering. In: EMNLP. 2018. [33] Mayank Mishra. LM Engine: Hyper-Optimized Library for Pretraining and Finetuning. June 2024. URL: https: //github.com/ibm/lm-engine. [34] Mistral. Mixtral of Experts: high quality Sparse Mixture-of-Experts. https://mistral.ai/news/mixtralof-experts. Mistral AI News. 2024. [35] Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Evan Pete Walsh, Oyvind Tafjord, Nathan Lambert, et al. OLMoE: Open Mixture-of-Experts Language Models. In: The Thirteenth International Conference on Learning Representations. 2025. [36] NVIDIA. NVIDIA H100 Tensor Core GPU Architecture: Exceptional Performance, Scalability, and Security for the Data Center. Whitepaper V1.01. Grace Hopper Hopper Architecture. NVIDIA, 2022. URL: https://www. advancedclustering . com / wp - content / uploads / 2022 / 03 / gtc22 - whitepaper - hopper . pdf. [37] NVIDIA. CUTLASS: CUDA Templates for Linear Algebra Subroutines. https://github.com/NVIDIA/ cutlass. Version 4.2.0, Accessed: 2025-09-19. 2025. [38] NVIDIA. NVIDIA Blackwell Architecture Technical Brief. Whitepaper. Blackwell Architecture. NVIDIA, 2025. URL: https://resources.nvidia.com/en-us-blackwell-architecture?ncid=no-ncid. [39] NVIDIA. NVIDIA CUTLASS Documentation. 2025. URL: https://docs.nvidia.com/cutlass/media/ docs/pythonDSL/cute_dsl_general/dsl_introduction.html. [40] OpenAI. gpt-oss-120b & gpt-oss-20b Model Card. 2025. arXiv: 2508.10925 [cs.CL]. URL: https://arxiv. org/abs/2508.10925. [41] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zach DeVito, Martin Raison, 19 Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style, High-Performance Deep Learning Library. 2019. arXiv: 1912.01703 [cs.LG]. URL: https: //arxiv.org/abs/1912.01703. [42] Team Qwen. Qwen3 Technical Report. 2025. arXiv: 2505.09388 [cs.CL]. URL: https://arxiv.org/ abs/2505.09388. [43] QwenLM. Qwen3: Think Deeper, Act Faster. https://qwenlm.github.io/blog/qwen3/. Official Blog. 2025. [44] David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter Conway Humphreys, and Adam Santoro. Mixture-of-depths: Dynamically allocating compute in transformer-based language models. In: arXiv preprint arXiv:2404.02258 (2024). [45] Colfax Research. CUTLASS Tutorial: Writing GEMM Kernels Using Tensor Memory For NVIDIA Blackwell GPUs. Accessed: 2025-09-21. 2024. URL: https : / / research . colfax - intl . com / cutlass - tutorial - writing-gemm-kernels-using-tensor-memory-for-nvidia-blackwell-gpus/. [46] Melissa Roemmele, Cosmin Adrian Bejan, and Andrew Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In: 2011 AAAI Spring Symposium Series. 2011. [47] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An Adversarial Winograd Schema Challenge at Scale. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. 2020, pp. 87328740. [48] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. SocialIQA: Commonsense Reasoning [49] about Social Interactions. In: Conference on Empirical Methods in Natural Language Processing. 2019. Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision. 2024. arXiv: 2407.08608 [cs.LG]. URL: https: //arxiv.org/abs/2407.08608. [50] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer. 2017. arXiv: 1701.06538 [cs.LG]. URL: https://arxiv.org/abs/1701.06538. [51] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatronlm: Training multi-billion parameter language models using model parallelism. In: arXiv preprint arXiv:1909.08053 (2019). [52] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: Question Answering Challenge Targeting Commonsense Knowledge. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Minneapolis, Minnesota: Association for Computational Linguistics, June 2019, pp. 41494158. DOI: 10.18653/v1/N19-1421. arXiv: 1811.00937 [cs]. URL: https://aclanthology.org/N19-1421. [53] Shawn Tan, Yikang Shen, Rameswar Panda, and Aaron Courville. Scattered mixture-of-experts implementation. In: arXiv preprint arXiv:2403.08245 (2024). [54] Databricks The Mosaic Research Team. Introducing DBRX: New State-of-the-Art Open LLM. https://www. databricks.com/blog/introducing-dbrx-new-state-art-open-llm. Databricks Blog, March 27, 2024. 2024. [55] Changxin Tian, Kunlong Chen, Jia Liu, Ziqi Liu, Zhiqiang Zhang, and Jun Zhou. Towards greater leverage: Scaling laws for efficient mixture-of-experts language models. In: arXiv preprint arXiv:2507.17702 (2025). [56] Philippe Tillet, H. T. Kung, and David Cox. Triton: an intermediate language and compiler for tiled neural network computations. In: Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages. MAPL 2019. Phoenix, AZ, USA: Association for Computing Machinery, 2019, 1019. ISBN: 9781450367196. DOI: 10.1145/3315508.3329973. URL: https://doi.org/10.1145/3315508. 3329973. [57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention Is All You Need. 2017. arXiv: 1706.03762 [cs.CL]. URL: https://arxiv.org/ abs/1706.03762. [58] Lean Wang, Huazuo Gao, Chenggang Zhao, Xu Sun, and Damai Dai. Auxiliary-loss-free load balancing strategy for mixture-of-experts. In: arXiv preprint arXiv:2408.15664 (2024). [59] Lei Wang, Yu Cheng, Yining Shi, Zhengju Tang, Zhiwen Mo, Wenhao Xie, Lingxiao Ma, Yuqing Xia, Jilong Xue, Fan Yang, et al. TileLang: Composable Tiled Programming Model for AI Systems. In: arXiv preprint arXiv:2504.17577 (2025). [60] Less Wright and Adnan Hoque. Deep dive on Cutlass Ping-Pong Gemm Kernel. 2024. URL: https://pytorch. org/blog/cutlass-ping-pong-gemm-kernel/. 20 [61] Less Wright and Adnan Hoque. Deep Dive on CUTLASS Ping-Pong GEMM Kernel. Accessed: 2025-09-21. 2024. URL: https://docs.pytorch.org/blog/cutlass-ping-pong-gemm-kernel/. [62] Xi Xie, Yuebo Luo, Hongwu Peng, and Caiwen Ding. RTop-K: Ultra-Fast Row-Wise Top-K Selection for Neural Network Acceleration on GPUs. In: The Thirteenth International Conference on Learning Representations. 2025. URL: https://openreview.net/forum?id=PHg4rAXFVH. [63] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can Machine Really Finish Your Sentence? In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019. [64] Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, et al. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. In: arXiv preprint arXiv:2508.06471 (2025). [65] Zhiyuan Zeng, Qipeng Guo, Zhaoye Fei, Zhangyue Yin, Yunhua Zhou, Linyang Li, Tianxiang Sun, Hang Yan, Dahua Lin, and Xipeng Qiu. Turn Waste into Worth: Rectifying Top-k Router of MoE. In: EMNLP. 2024. [66] Zihao Zeng, Yibo Miao, Hongcheng Gao, Hao Zhang, and Zhijie Deng. AdaMoE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models. In: Findings of the Association for Computational Linguistics: EMNLP 2024. Ed. by Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen. Miami, Florida, USA: Association for Computational Linguistics, Nov. 2024, pp. 62236235. DOI: 10.18653/v1/2024.findings-emnlp.361. URL: https://aclanthology.org/2024.findings-emnlp.361/. [67] Yu Zhang, Zongyu Lin, Xingcheng Yao, and et al. Kimi Linear: An Expressive, Efficient Attention Architecture. 2025. arXiv: 2510.26692 [cs.CL]. [68] Chenggang Zhao, Chengqi Deng, Chong Ruan, Damai Dai, Huazuo Gao, Jiashi Li, Liyue Zhang, Panpan Huang, Shangyan Zhou, Shirong Ma, Wenfeng Liang, Ying He, Yuqing Wang, Yuxuan Liu, and Y.X. Wei. Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures. In: Proceedings of the 52nd Annual International Symposium on Computer Architecture. ISCA 25. New York, NY, USA: Association for Computing Machinery, 2025, 17311745. ISBN: 9798400712616. DOI: 10.1145/3695053.3731412. URL: https://doi.org/10.1145/3695053.3731412. [69] Chenggang Zhao, Liang Zhao, Jiashi Li, and Zhean Xu. DeepGEMM: clean and efficient FP8 GEMM kernels with fine-grained scaling. 2025. URL: https://github.com/deepseek-ai/DeepGEMM. [70] Chenggang Zhao, Shangyan Zhou, Liyue Zhang, Chengqi Deng, Zhean Xu, Yuxuan Liu, Kuai Yu, Jiashi Li, and Liang Zhao. DeepEP: an efficient expert-parallel communication library. https://github.com/deepseekai/DeepEP. 2025. [71] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai, Quoc Le, James Laudon, et al. Mixture-of-experts with expert choice routing. In: Advances in Neural Information Processing Systems 35 (2022), pp. 71037114. [72] Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William Fedus. St-moe: Designing stable and transferable sparse expert models. In: arXiv preprint arXiv:2202.08906 (2022)."
        },
        {
            "title": "Appendix",
            "content": "We provide table listing all notations and their explanations in Table 4. In Section B, we compare SonicMoEs kernel design with other open-source MoE kernel designs. In Section C, we elaborate on SonicMoEs computational path for dS and dH that does not use and dY . In Section C.1, we justify SonicMoEs computational path for dS is both activation memory and computationally-efficient. In Section D, we illustrate SonicMoEs up-projection backward is included in Algorithm 5. In Section E, we present ablation studies of training throughput for SonicMoEs MoE computation kernels to examine the impact of each design choice made for SonicMoE. In Section F, we assess the quality improvements of MoE models trained by varying expert granularity. We then focus on various ablation studies on our token rounding routing algorithm to assess the quality difference of the trained MoE models from the choice of rounding subroutine. We also study the effect of microbatch size and tile size Mtile on token rounding. In Section G, we describe the configurations for benchmarking the memory and training throughput. In Section H, we include the details of model training and the evaluation setup."
        },
        {
            "title": "A Notations",
            "content": "In Table 4, we describe the notations used in this paper."
        },
        {
            "title": "Explanation",
            "content": "Table 4: Notations and their explanations"
        },
        {
            "title": "T\nd",
            "content": "n ρ Te M, N, Mtile, Ntile, Ktile Re W1 W2 π A dO dA dA dY dS dH dX dW dW2 kernel kernel kernel dH kernel dW2 kernel kernel dW1 kernel number of tokens in microbatch model embedding dimension (hidden size) each experts intermediate dimension total number of experts number of activated experts for AB = [E][Te] = ρ represents the expected number of received tokens in each microbatch by expert K, RK N, and RM d, input token embeddings for an MoE layer 2n, output of up projection n, output of SwiGLU d, output of down projection 2n, weight of up projection d, weight of down projection , binary-valued matrix where πt,e represents if token is routed to expert , router scores. In practice, we only materialize the sparsified instead of the full ρ = K/E represents MoE activation ratio Te = Ee = d/n represents the MoE expert granularity. Greater means MoE is more fine-grained Dimensions for GEMM in CUTLASS. We define RM tile size of M, N, dimension for single GEMM tile tile quantization residue Re := Te mod Mtile RT W1 RE W2 RE π {0, 1}T RT RT RT RT RT dO RT dA = dO 2 RT dA = Broadcast(s) dA RT dY = Broadcast(s) dO RT dS RT = Broadcast(s) RT dH RT RT dX RT dW1 RE dW1 RE forward up-proj kernel d, output of expert aggregation, also output of the entire MoE layer d, activation gradient for d, activation gradient for after aggregation 2n, weight gradient for W1 d, weight gradient for W2 d, activation gradient for . dY is not used in SonicMoE. n, intermediate result and input for computing dW2 d, activation gradient for before aggregation 2n, activation gradient for n, activation gradient for , activation gradient for n, GEMM output of dO and W2. Intermediate result for computing dA and dS forward down-proj kernel forward expert aggregation kernel where each token aggregates all routed experts result as the final forward output backward down-proj activation gradient kernel backward down-proj weight gradient kernel backward up-proj activation gradient kernel backward up-proj weight gradient kernel backward expert aggregation kernel where each token aggregates the routed experts dX kernel feMtile , feMtile Mtile-rounded multiples of expert frequency fe. feMtile = fe/Mtile Mtile SMtile , feMtile feMtile is Mtile-rounded multiples of expert frequency fe. feMtile {feMtile , feMtile }, and SMtile is the score after rounding in Algorithm 4. feMtile = fe/Mtile Mtile 22 SonicMoEs comparison with existing MoE kernel design Existing efficient MoE kernels also frame MoE computation as Grouped GEMM, but their ingredients are different from SonicMoE. Here we provide an overview (but not complete list) of key differences: ScatterMoE (Tan et al. 2024)31 implements gather fusion for varlen-M Grouped GEMM but not for varlen-K Grouped GEMM. ScatterMoE also does not overlap MMA computation with memory IO. Moreover, ScatterMoE is also built on older versions of Triton where TMA is not supported. ScatterMoE computes dS as dS = dO, which requires caching . This results in large IO cost and activation memory requirement. ScatterMoEs both forward and backward pass have limited fusion and hence it is much slower than SonicMoE, especially for backward computation. MoMoE (Costin et al. 2025)32 also implements the gather fusion for varlen-M but not varlen-K Grouped GEMM similar to ScatterMoE. Although fused with up-proj activation gradient, the dS computation still utilizes dS = dO, . Similar to ScatterMoE, MoMoE does not use TMA for IO. The scatter operation in MoMoE is (much) slower than SonicMoE, as shown in Figure 22. MegaBlocks (Gale et al. 2023) has multiple MoE implementations and we focus on ParallelDroplessMLP33 which is built on top of block-sparse matrix multiplication34. ParallelDroplessMLP first gathers and pads the tokens and then launches block-sparse GEMM for up and down-proj. Then, it launches scatter kernel before reducing across the expert results. These sparse matrix multiplications usually take longer time than the highly-optimized Grouped GEMM, as shown in Figure 6, and the gather and scatter kernel have total IO cost of 8T Kd bytes which can be bottleneck for fine-grained MoEs. We consider MegaBlockss ParallelDroplessMLP as block-sparse GEMM baseline in our benchmark and find that MoE implemented via Grouped GEMMs often have higher training throughput than MoEs implemented via block-sparse GEMM. Megatron-LM (Shoeybi et al. 2019) also have multiple MoE implementations and we focus on GroupedMLP35, which uses Grouped GEMM36 from the CUTLASS library (Corporation 2025) with JIT epilogue fusion as the GEMM backend. Similar to DeepGEMM, GroupedMLP does not fuse gather with the prologue (it assumes contiguouslypacked inputs). recent memory-efficient patch37 fuses weighting with SwiGLU computation during forward, and during backward which allows the PyTorch autograd engine (Paszke et al. 2019) to follow similar computational path as SonicMoE. Megatron-LM also implements TEGroupedMLP38 which launches 4 CUDA streams to execute list of GEMM (without contiguously-packed inputs, and without persistent tile scheduler). In this case, each expert independently launches new GEMM kernel leading to bubbles on the CUDA streams. This leads to underutilization of the GPU resources. We empirically find that TEGroupedMLP runs slower slower than GroupedMLP and so we use GroupedMLP across all benchmarks. DeepGEMM (Zhao et al. 2025b) design Grouped GEMM kernel for contiguously-packed inputs. They also dont implement any other fusion for SM90 (Hopper) BF16 Grouped GEMM. DeepGEMM specializes more on distributed training with expert parallelism (Zhao et al. 2025c), and it is common to launch separate all2all kernel (Lepikhin et al. 2024) which is then followed by contiguous Grouped GEMM. DeepGEMM SM90 BF16 kernel also assumes that each expert receives multiple of Mtile tokens as it does not implement the TMA tensor descriptor online update during the Grouped GEMM computation. DeepGEMMs BF16 GEMM on SM9011 also does not employ Ping-Pong scheduling. Additionally, ScatterMoE and MoMoE are both implemented in Triton (Tillet, Kung, and Cox 2019) for the ease of development at the expense of losing full programmability of the asynchronous compute and memory IO of Hopper and Blackwell GPUs (NVIDIA 2022, 2025b). For example, they cannot implement fine-grained control of asynchronous load and store during the GEMMs epilogue. They also cannot overlap MMA with heavy epilogue operations using Ping-Pong scheduling. It becomes increasingly important to overlap IO operations in epilogue when the GEMM computations are small 31https://github.com/shawntan/scattermoe/blob/47b5e1502e5a10e82c8e5945d761b877849871e7/scattermoe/mlp.py#L51 32https://github.com/tilde-research/MoMoE-impl/blob/d6e2d683185bfe4030265c3ca062564356faa61e/momoe/momoe.py#L914 33https://github.com/databricks/megablocks/blob/78eea65fda01e638af36ae38853bc51efb04a4b4/megablocks/layers/dmoe. py#L18 34https://github.com/databricks/megablocks/blob/78eea65fda01e638af36ae38853bc51efb04a4b4/megablocks/layers/mlp.py# 35https://github.com/NVIDIA/Megatron-LM/blob/610a75ef3a4a80c2ce2da436c19244e5362978d4/megatron/core/transformer/ moe/experts.py#L100 36https://github.com/fanshiqing/grouped_gemm/blob/main/csrc/grouped_gemm.cu 37https://github.com/NVIDIA/Megatron-LM/commit/2659630721ac87237c8cb772b1c2f1b34176f443 38https://github.com/NVIDIA/Megatron-LM/blob/610a75ef3a4a80c2ce2da436c19244e5362978d4/megatron/core/transformer/ moe/experts.py#L746 23 in size (as in the case of fine-grained MoEs) to achieve high GPU utilization."
        },
        {
            "title": "C Gradient computation",
            "content": "For an expert e, let Xe RTed, W1,e Rd2n, W2,e Rnd The forward activation computation is given by: He = XeW1,e RTe2n, Ae = SwiGLU(He) RTen, Ye = AeW2,e RTed. The token aggregation with scores = {st,e} is given by Ot = (cid:88) st,e Ye,t, dOt RT as the gathered results from dO."
        },
        {
            "title": "We know",
            "content": "dYe,t = st,e dOt = dYe = Broadcast(se) dOe. Define the Grouped GEMM output as dA := dOe 2,e RTen. Then from Equation 8 dAe = dYe 2,e = Broadcast(se) dA e. The activation gradient for score dS is39 dSt,e = dOt, Ye,t = (cid:10)dOt 2,e, Ae,t (cid:11) = (cid:10)dA e,t, Ae,t (cid:11) . In addition, we can derive dHe from dAe and Ae (recomputed from He) as: dHe = dSwiGLU(dAe, He). Using Equation 8, dW2,e = dYe = e (cid:0)Broadcast(se) dOe C.1 Computational choices for dS (cid:1) = (cid:0) Broadcast(se) Ae (cid:125) (cid:124) (cid:123)(cid:122) (cid:1) dOe. (5) (6) (7) (8) (9) (10) (11) (12) If we do not implement custom kernels and rely solely on PyTorchs autograd (AD) engine, we can add the expert weighting (S) either (1) before down-proj forward or (2) after down-proj forward. Both yield identical results for forward and backward, but the computation for dS is different. For (1), we need to compute (cid:10)dA (cid:11) which is used by SonicMoE and Megatron40. MoMoE41, ScatterMoE42, and MegaBlocks4344 compute dOt, Ye,t as required in (2). Note that dS can be computed as any of dSt,e = (cid:10)dA (cid:10)dA (cid:11) is computationally and activation memory-efficient choice due to the following reasons: (cid:11) = dOt, Ye,t, however computing it as dSt,e = e,t, Ae,t e,t, Ae,t e,t, Ae,t 39If we also consider expert biases, we will have dSe,t = dAe,t, Ae,t + dOe,t, Broadcast(Se,t). The activation memory efficiency can still be preserved but we need to perform separate computation for dOe,t, Broadcast(Se,t) either via separate kernel (SonicMoEs current choice) or fuse it with dH Grouped GEMM mainloop. Please refer to the released code of SonicMoE. 40https://github.com/NVIDIA/Megatron-LM/blob/44af130cc4568d324860646996b6a5bfd6c5e3e6/megatron/core/transformer/ moe/experts.py#L 41https://github.com/tilde-research/MoMoE-impl/blob/d6e2d683185bfe4030265c3ca062564356faa61e/momoe/momoe.py#L702 42https://github.com/shawntan/scattermoe/blob/47b5e1502e5a10e82c8e5945d761b877849871e7/scattermoe/parallel_experts. py#L79 43https://github.com/databricks/megablocks/blob/78eea65fda01e638af36ae38853bc51efb04a4b4/megablocks/ops/binned_ scatter.py#L12 44https://github.com/databricks/megablocks/blob/78eea65fda01e638af36ae38853bc51efb04a4b4/megablocks/ops/binned_ scatter.py#L 24 Figure 18: Index prefetching strategies for gathering on dim of varlen-M Grouped GEMM (left) and dim of varlen-K Grouped GEMM (right) on H100 GPU. For gather on dim (left), we let each thread independently prefetch indices to their own registers before the mainloop. For gather on dim (right), we create buffer on SMEM and let 4 producer warps cooperatively prefetch indices to SMEM and each producer thread read from this SMEM buffer to their registers. Additional HBM traffic (0 vs. 2T Kd bytes): (cid:10)dA dH kernel, so we can avoid extra unnecessary loads. e,t, Ae,t (cid:11) requires dA e,t and Ae,t are already computed during the Extra cached activation memory (0 vs. 2T Kd bytes): One of the reasons why the cached activation memory for ScatterMoE, MoMoE and MegaBlocks fails to stay constant w.r.t. expert granularity is the required caching of for computing dS. Parallel reduction rounds (log2(n) vs. log2(d)): (cid:10)dA e,t, Ae,t This difference saves at least log2(d/n) rounds of reduction. (cid:11) reduces over while dOt, Ye,t reduces over d."
        },
        {
            "title": "D SonicMoE algorithm",
            "content": "In this section, we present the referenced diagrams and algorithms in the main paper. We further describe the index prefetching strategies incorporated in our group GEMM kernel on Hopper GPUs, as illustrated in Figure 18. Algorithm 5 SonicMoEs MoE kernel backward pass of up-proj. Input :X, π, W1, dH Output :dX, dW1. Up-proj act kernel (dH, W1) X: // varlen-M Grouped GEMM Parallel for [E] do dHe, W1,e load(dHe, W1,e) Xe dHe 1,e Xe store(d Xe) Up-proj weight dW1 kernel (X, dH, π) dW1: // Gather + varlen-K Grouped GEMM Parallel for [E] do X, π:,e, dHe load(X, π:,e, dHe) Xe Gather(X, π:,e) dW1,e dHe dW1,e store(dW1,e) Expert aggregation dX kernel (d X, π) dX: // Gather and sum Parallel for [T ] do X, πt,e load(d X, πt,e) dXt (cid:80) dXt store(dXt) [E] πt,ed Xe,t 25 SonicMoEs ablation study on kernel-level throughput In this section, we present kernel-level ablation studies on training throughput to examine the impact of each of the implemented features on SonicMoE. In Section E.1, we investigate on the Grouped GEMM throughput with and without the gather fusion on Hopper GPUs. In Section E.2, we profile the memory bandwidth of expert aggregation kernels. In Section E.3, we compare SonicMoEs top-K kernels with other efficient top-K implementations. E.1 Grouped GEMM on Hopper GPUs We also benchmark SonicMoEs base Grouped GEMM kernel with both contiguously-packed inputs and gathered inputs without any epilogue fusion on H100 GPU. For contiguously-packed inputs, we mainly compare with DeepGEMM kernels (sm90 grouped bf16 gemm contiguous45 and sm90 bf16 grouped gemm46). We note that at the time of writing, DeepGEMMs Grouped GEMM kernel only accepts the form of = AB + for GEMM47 where for correctness, we use zero-filled FP32 weight gradient buffer as accumulator input, but we use as uninitialized weight gradient buffer during benchmarking. For inputs requiring gather operation, we mainly compare with ScatterMoE and MoMoE. We also benchmark cuBLAS dense BMM (CUDA toolkit v12.9) assuming each expert receives the same amount of tokens. In Figure 19, we compare SonicMoE with DeepGEMM on varlen-M Grouped GEMM with contiguously-packed inputs. Grouped GEMM on H100 GPU without any other fusion. We also benchmark cuBLAS dense BMM (perfect load balance and no tensormap update needed) as reference for the TFLOPS upper bound for Grouped GEMM. We find that SonicMoEs up-proj has 2.7% higher TFLOPS while down-proj has 10.0% higher TFLOPS than DeepGEMM. SonicMoEs relative TFLOPS speedup over DeepGEMM is 57.4%, 14.0%, and 5.3% for 30B down-proj config. We use Ping-Pong scheduling for down-proj with < 1024 while DeepGEMM (Zhao et al. 2025b) uses cooperative scheduling (Efficient GEMM in CUDA 2025; Zhao et al. 2025b). Figure 19: Varlen-M Grouped GEMM with contiguously-packed inputs to up and down-proj during forward pass on H100 GPU. We use the same configurations as in Figure 13. cuBLAS BMM is dense BMM baseline equivalent to all expert receiving equal number of tokens (perfectly load balanced), whose TFLOPS can be considered as an upper bound for any Grouped GEMM kernel. Grouped GEMM with gather fusion. In Figure 20, we report SonicMoE, ScatterMoE, MoMoE, and DeepGEMM with and without gather fusion (as opaque and transparent bars for each method) on Hopper GPU. ScatterMoE and MoMoE both have gather fusion for varlen-M but not varlen-K Grouped GEMM, so we benchmark their gather with varlen-K Grouped GEMM time (opaque bar) by adding up the time of their contiguously-packed weight gradient kernel (transparent bar) with the time of their own gather kernel. We benchmark the gather with Grouped GEMM time of DeepGEMM for both varlen-M and varlen-K cases using the same approach. We also provide cuBLAS dense BMM (transparent bar) baseline and the gather with BMM GEMM time (opaque bar) by adding up the time with heavily-tuned gather kernels time with the same 45https://github.com/deepseek-ai/DeepGEMM/blob/9b680f428484625f4f35dc3617f134187c6bcd4a/csrc/jit_kernels/impls/ sm90_bf16_gemm.hpp#L127 46https://github.com/deepseek-ai/DeepGEMM/blob/9b680f428484625f4f35dc3617f134187c6bcd4a/csrc/jit_kernels/impls/ sm90_bf16_gemm.hpp#L 47This formula naturally fits with the gradient accumulation. 26 Figure 20: Forward pass up-proj (gather on dim) and backward up-proj weight gradient dW1 (gather on dim) kernel on H100 GPU. SonicMoE supports both inputs gathered from different positions (opaque bars) and contiguously-packed inputs (transparent bars). ScatterMoE and MoMoE both have gather fusion for varlen-M but not varlen-K, so we benchmark their gather with varlen-K Grouped GEMM time (opaque bar) by adding up the time of their contiguously-packed weight gradient kernel (transparent bar) with the time of their gather kernel. DeepGEMM does not have gather fusion for both varlen-M and varlen-K Grouped GEMM, so we provide an optimized gather kernel in both cases. We also provide cuBLAS dense BMM (transparent bar) baseline and the gather with GEMM time (opaque bar) by adding up the time with heavily-tuned gather kernels time with the same input shape, which can be considered as the upper bound TFLOPS for any Grouped GEMM kernel without gather fusion. input shape, which can be considered as the upper bound of the TFLOPS for any Grouped GEMM kernel without gather fusion. We discuss the effect gather on and dimension for SonicMoE: Gather on dimension. The average relative TFLOPS difference of SonicMoE with and without gather fusion on dim is 6.3% in Figure 20. SonicMoE consistently achieves higher TFLOPS than ScatterMoE (avg 9.7%), MoMoE (avg 30.9%), and DeepGEMM (avg 38.3%) with gather fusion. Gather on dimension. The average relative TFLOPS difference of SonicMoE with and without gather fusion on dim is 8.5% in Figure 20. SonicMoE already achieves higher TFLOPS than ScatterMoE (avg 23.5% higher), MoMoE (avg 4.3% higher), and DeepGEMM (avg 41.4% higher) without gather fusion (transparent bars). When we compare SonicMoE with gather fusion (opaque bars) against ScatterMoE and MoMoE with their gather kernel together, we observe wider gap as expert granularity increases (from right to left on each 3 bar groups). On average, SonicMoE with gather fusion achieves 55.1%, 42.4%, 71.8% higher TFLOPS than ScatterMoE, MoMoE, and DeepGEMM with gather fusion respectively. E.2 Expert aggregation on Hopper GPUs We benchmark the bandwidth of SonicMoEs aggregation kernel on H100 in Figure 21. We compare SonicMoEs gatherand-sum aggregation (Figure 9 left) with ScatterMoEs torch.bmm and MoMoE torch.sum aggregation (Figure 9 middle). In addition, we implement highly-optimized triton aggregation kernel (triton sum (contig. )) with extensive kernel configuration tuning. This kernel achieves 2.85+ TB/s (85%+ peak) for most MoE configurations so we consider it as an upper bound for any aggregation kernel on H100. Although SonicMoEs aggregation kernel requires gather fusion during HBM load, the memory bandwidth of SonicMoEs still surpasses ScatterMoE (2.92x on average) and MoMoE (1.05x on average), and is only slightly slower (0.98x on average) than the triton upper bound of summing over contiguous . 27 Figure 21: Expert aggregation kernels (O kernel) during forward pass of MoE. Same configurations as in Figure 13. ScatterMoE (contig. ) is the expert aggregation strategy employed by ScatterMoE. The implementation uses torch.bmm call to reduce over K, MoMoE (contig. ) is torch.sum call for MoMoE. We take the maximum bandwidth between PyTorch eager and PyTorch compile on default mode with PyTorch 2.9.0. We also implement an optimized triton kernel for summing over contiguously-packed inputs with Triton 3.3.1 as triton sum (contig. ). Figure 22: Throughput of Grouped GEMM and expert aggregation kernel on H100. SonicMoE (gemm + gth w. sum) is the final design choice for SonicMoE as illustrated in Figure 9 left strategy. We compare this design against SonicMoE (gemm w. sct + sum) that implements the Figure 9 middle strategy on SonicMoE. We use identical tile sizes and other GEMM configs for both SonicMoE (gemm + gth w. sum) and SonicMoE (gemm w. sct + sum). We also compare with ScatterMoEs design (fused scatter with GEMM + torch.bmm, labeled as ScatterMoE (gemm w. sct + BMM)) and MoMoEs design (fused scatter with GEMM + torch.sum, labeled as MoMoE (gemm w. sct + sum)). For each method, we report the GEMM TFLOPS in transparent bars and TFLOPS of total runtime of GEMM and expert aggregation in the opaque bars. E.3 Top-K sorting on Hopper GPUs We benchmark the bandwidth of SonicMoEs top-K kernel on H100 in Figure 23. We compare SonicMoE with PyTorch48, triton official example49, tilelang official example50, and RTop-K51 (Xie et al. 2025) on BF16 and FP32 inputs. PyTorch single block Top-K: PyTorch (Paszke et al. 2019) implements radix-select followed by gather algorithm for top-K, and it dispatches to single or multiple block version depending on T, E, K. For large with modest and K, PyTorch uses the single-block version that performs 2 SMEM scans. In this case, SonicMoEs sorting networks with pure register-based communication is much faster. Triton official example: Triton (Tillet, Kung, and Cox 2019) provides top-K example kernel that is also based on bit packing and bitonic merge. The main algorithmic difference is that SonicMoE relies on optimal sorting networks on the base cases while the Triton implementation directly calls triton.language.topk. During the top-K benchmark in Figure 23, we observe that Triton is much faster than PyTorch torch.topk but it is still consistently slower than SonicMoEs top-K across all configurations. Tilelang official example: Tilelang (Wang et al. 2025) provides top-K example kernel that performs K-pass 48https://github.com/pytorch/pytorch/blob/b54e466fd04e5e736662a6206d81ab0d5fe85d91/aten/src/ATen/native/cuda/ TensorTopK.cu#L 49https://github.com/triton-lang/triton/blob/de8e71503fea971dfb65308147798657e18f8568/python/triton_kernels/ triton_kernels/topk_details/_topk_forward.py#L90 50https://github.com/tile-ai/tilelang/blob/5c62d00a64f2f52cf6b2536a2492a29fc5323723/examples/topk/example_topk.py# L18 51https://github.com/xiexi51/RTopK/blob/952f515321a4bdc5c4a57944bca0d32641052460/rtopk_kernel.cu#L7 Figure 23: Top-K kernels with BF16 inputs (1st row) and FP32 inputs (2nd row) during forward pass of MoE. Same configurations as in Figure 13. torch is direct torch.topk call. triton and tilelang are taken from their official examples with slight modifications to support BF16 inputs. For the triton official kernel, we remove the unnecessary bit matrix store and disable the softmax fusion in this example for fair comparison. RTop-K(Xie et al. 2025) only supports FP32 inputs. We set ϵ = 0 and maximum iteration as 8 for RTop-K. maximum reduction. This design is more targeted for small and we observe that as both and become larger, the Tilelang top-K kernels throughput decreases compared to other baselines (and SonicMoE)s increasing trend. Such trend makes Tilelangs example top-K kernel (K-pass top-K kernel) unsuitable for fine-grained MoEs. RTop-K: RTop-K (Xie et al. 2025) follows threshold-based binary search. Each bisection step utilizes warp-level primitives and follows selection-by-count method instead of SonicMoEs sorting network. RTop-K is also an iterative algorithm with iterations dependent on the value range and vector size. In addition, RTop-K heavily utilizes SMEM for scanning while SonicMoE solely relies on registers for its compare and swap subroutine. We find that SonicMoEs top-K consistently achieves higher throughput for 1.4B and 7B model configurations. For 30B and 120B, SonicMoE is slower when (E, K) = (256, 16) but still outperforms when (E, K) = (128, 8), (64, 4)."
        },
        {
            "title": "F More experiments",
            "content": "In this section, we investigate the qualitative improvement from fine-grained MoE in Section F.1. We also investigate the effect of rounding subroutines round and sparsify in Algorithm 4 and the effects from microbatch size and tile size Mtile for token rounding in Section F.3. F.1 Effect of expert granularity Here we validate the effectiveness of adopting fine-grained MoE. We fix the MoE activation ratio ρ = K/E for the 0.5B and 1.4B model and we proportionally scale up and while linearly decreasing from row 1 to row 3 in Table 5a and 5b. In general, we observe better performance for = 256 than = 1024 which is also consistent with the MoE scaling trends mentioned in Table 1. In Figure 1 right subfigure, we find both SonicMoE and cuBLAS can still sustain the throughput from = 1024 to = 256 under iso-FLOPs, but starting from = 256 FLOPs will drop linearly w.r.t. granularity. Therefore, we choose = 256 for all experiments in Table 3. F.2 Ablation study on different rounding subroutines for token rounding We conduct ablation studies to study the effect of the different routing subroutines on the trained MoE by TR. We compare token rounding with nearest rounding (NR) with per-expert token counts with other rounding methods. Specifically, we compare against stochastic rounding with per-expert token count (SR), always round up (UP), and always round down (DOWN). The results are shown in Table 6 and we find that our token rounding algorithm in general is robust to the specific rounding subroutines. 29 Table 5: Evaluation of MoE w.r.t. granularity with iso-FLOPs (nK is constant) and iso-params (nE is constant) settings. PPL refers to the validation perplexity at the end of training. Avg is the mean accuracy across the 11 downstream tasks. The dense, iso-FLOPs refers to dense model with nK as the intermediate size, while the dense, iso-params refers to dense model with nE as the intermediate size. (a) 0.5B params, 20B tokens, 8/64 activated"
        },
        {
            "title": "OBQA",
            "content": "HS"
        },
        {
            "title": "Avg",
            "content": "(E, K, n) 16, 2, 1024 64, 8, 256 256, 32, 64 Dense, iso-FLOPs Dense, iso-params 16.23 16.01 16.13 19.90 15.46 53.0 51.0 51. 48.9 52.1 41.3 41.4 41.5 41.4 41.5 79.8 79.2 78.9 74.9 78.9 65.0 65.5 65. 62.2 65.3 32.6 31.6 34.2 30.2 34.0 37.8 38.4 38.4 32.6 39.2 66.0 66.0 63. 62.0 69.0 (b) 1.4B params, 50B tokens, 8/128 activated 32, 2, 1024 128, 8, 256 512, 32, 64 Dense, iso-FLOPs Dense, iso-params 13.38 13.32 13.50 17.90 12. 52.2 51.8 52.5 52.2 52.2 41.7 41.7 41.2 41.0 42.6 81.7 81.5 82.9 79.2 83. 69.2 69.3 68.9 63.4 70.1 33.6 32.4 34.4 31.0 34.8 44.3 45.3 44.7 34.7 46. 64.0 68.0 69.0 61.0 67.0 32.2 31.5 32.4 31.6 32.2 33.5 34.5 33.6 30.5 35. 55.8 60.2 60.6 61.7 58.5 61.1 56.6 58.7 60.3 61.7 53.9 57.5 59.5 53.2 59. 60.9 63.2 62.6 51.8 63.5 29.1 25.7 28.1 27.1 28.8 29.8 28.4 30.1 25.1 31. 49.7 49.8 50.3 47.8 50.8 52.0 52.1 52.6 48.2 53.5 Following Algorithm 4, for expert e, we denote the expert frequency from the TC sorting as fe, and the last Mtile-divisible expert frequency as feMtile , and the next Mtile-divisible expert frequency as feMtile. We also denote the expert scores from TC sorting as se, the expert scores from the selected tokens in πe[: feMtile] as seMtile and the scores for πe[: feMtile] as seMtile . We note that all rounding algorithms only make binary decision between discarding TC tokens and padding EC tokens for each expert. Following are simple heuristics to perform rounding: NR-f: nearest rounding to Mtile-multiples via expert frequency: We pad EC tokens if feMtile fe < fefeMtile. NR-f is our default choice of token rounding and we use it for Table 3, 7, 8, and Figures 11 and 16. SR-f: stochastic rounding to Mtile-multiples via expert frequency: We sample from Bernoulli distribution for deciding whether to pad EC tokens for expert e. (cid:18) fe feMtile Mtile (cid:19) NR-s: nearest rounding to Mtile-multiples via expert scores: We sample from the following distribution for deciding whether to pad EC tokens for expert e:"
        },
        {
            "title": "Bernoulli",
            "content": "(cid:18) (cid:80) (cid:80) se,t (cid:80) tse,tMtile (cid:80) tse,tMtile tse,tMtile (cid:19) (13) Balance-f: balanced rounding to Mtile-multiples via expert frequency: The Balance algorithm (Cooper et al. 2023; Dwivedi and Mackey 2024; Lu, Guo, and De Sa 2022) can be adapted to ensure the total number of routed tokens to all experts after tile-rounding is preserved regardless of the number of experts E. Algorithm 6 is such an example that ensures max e[E] feMtile fe Mtile/2, feMtile fe Mtile/ (14) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:88) e=1 (cid:88) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:16) e="
        },
        {
            "title": "Mtile",
            "content": "(cid:17) for (cid:80)E e=1feMtile . where the other rounding subroutine will have an expected deviation of UP: always round up expert frequency as feMtile: We always pad EC tokens chosen in the second step of sorting in Algorithm 4. This gives model TFLOPS lower-bound for Figure 16. DOWN: always round down expert frequency as feMtile : We always discard TC top-K tokens chosen in the first step of sorting in Algorithm 4. This gives model TFLOPS upper-bound for Figure 16. Always discarding TC tokens (DOWN). DOWN is baseline in which we always drop the last TC tile if the expert frequency is not Mtile-divisible. This idea is similar to the idea of token dropping in expert parallelism where the expert will sort and drop the token with the lowest scores when it receives too many tokens (Fedus, Zoph, and Shazeer 2022). We note that DOWN produces the shortest MoE kernel runtime for any rounding algorithm. However, in Table 6, we observe that DOWN yields much higher validation perplexity than NR-f, SR-f and NR-s. Although we can expect shorter MoE kernel runtime by always discarding TC tokens, such quality degradation might not be acceptable in practice."
        },
        {
            "title": "Input",
            "content": "Algorithm 6 Balanced rounding to Mtile-multiples via expert frequency (Balance-f in Table 6). (cid:12) (cid:12) Mtile/2 and This algorithm satisfies maxe[E] (cid:12) (cid:12) (cid:12) Mtile/2 :f TC = {fe}e [E] as list of expert frequency with TC top-K routing and with potential EC padding to ensure each expert receives multiple of Mtile tokens, {feMtile }e [E] as list of expert frequency with TC top-K routing and with potential token dropping to ensure each expert receives multiple of Mtile tokens; We should ensure maxe [E] as list of expert frequency with TC top-K routing, {feMtile }e e=1feMtile (cid:80)E (cid:12) (cid:12)feMtile fe (cid:16) feMtile feMtile e=1 fe Mtile. (cid:80)E (cid:12) (cid:12) (cid:12) [E] (cid:17) [E] as list of expert frequency that ensures each expert receives multiple of Mtile tokens Output :f TR = {feMtile }e // an accumulator that ensures the preservation of total expert frequency 0; for [E] do // calculate the residual error of both rounding choice rup feMtile fe; rdown feMtile fe; (cid:12) (cid:12) if (cid:12) (cid:12)rup (cid:12) then + + (cid:12) < // choose to pad with EC tokens feMtile feMtile ; + rup ; (cid:12) (cid:12)rdown else // choose to discard TC tokens feMtile feMtile ; + rdown ; Table 6: Evaluation of token rounding algorithms equipped with different round and sparsify subroutines in Algorithm 4. PPL refers to the validation perplexity at the end of training. Avg is the mean accuracy across the 11 downstream tasks. (a) 0.5B params, 40B tokens, 2/64 activated ( Te = 512, Mtile = 128)"
        },
        {
            "title": "OBQA",
            "content": "HS"
        },
        {
            "title": "ArcC",
            "content": "TR (NR-f) TR (SR-f) TR (NR-s) TR (Balance-f) TR (UP) TR (DOWN) TC top-K TR (NR-f) TR (SR-f) TR (NR-s) TR (Balance-f) TR (UP) TR (DOWN) TC top-K 15.92 15.93 15.91 15.93 15.89 16.10 15. 13.10 13.08 13.09 13.08 13.07 13.19 13.12 51.4 50.8 51.3 51.9 50.5 51.1 51.0 41.6 40.9 40.9 41.8 40.9 41.4 41. 78.4 77.4 80.3 78.8 78.6 78.7 78.5 65.4 66.9 65.4 65.9 64.5 64.9 64.8 31.6 33.0 30.8 32.6 32.2 31.6 33. 38.1 38.4 37.7 38.4 38.2 38.0 38.1 65.0 64.0 67.0 66.0 68.0 62.0 67.0 31.0 31.1 31.0 31.6 29.9 32.8 30. 61.1 60.7 61.6 60.3 55.2 61.9 54.7 (b) 1.8B params, 40B tokens, 8/256 activated ( Te = 512, Mtile = 128) 53.4 52.7 54.1 52.5 50.4 55.4 50.1 42.1 41.6 42.3 42.0 41.7 41. 42.9 81.7 82.6 82.8 82.7 81.4 82.2 81.3 69.6 69.4 69.3 70.0 68.4 68.6 69.8 35.2 34.4 33.8 33.2 37.2 34. 33.8 45.3 45.6 45.7 45.3 45.4 45.0 45.2 70.0 70.0 70.0 68.0 69.0 69.0 71.0 33.2 33.0 34.1 34.6 31.9 34. 34.1 61.4 59.1 59.0 59.4 51.7 54.4 56.7 57.4 55.8 55.4 56.8 54.2 58.9 55.8 63.0 62.5 64.6 63.3 62.2 63. 64.6 29.1 28.1 28.4 27.1 30.1 30.8 30.1 33.4 34.8 32.4 33.4 33.4 31.4 31."
        },
        {
            "title": "Avg",
            "content": "50.0 49.7 50.0 50.1 49.3 50.2 49.6 53.5 53.2 53.5 53.1 52.1 52.7 52.8 Always padding EC tokens (UP). UP is baseline in which we always pad extra EC tokens to the last TC tile if the expert frequency is not Mtile-divisible. Contrary to DOWN, UP produces the longest MoE kernel runtime for any rounding algorithm. In Table 6, we find that UP often produces lower validation perplexity, but the average downstream task accuracy is not necessarily higher than other rounding algorithms. Given the longer MoE kernel runtime but not necessarily better trained MoE quality, we do not recommend the usage of always rounding up. We speculate this is due to the train-test gap between TC and EC routing and UP reinforces the bias towards EC more strongly than other TR algorithms. For balance between training efficiency and trained MoE quality, neither always discarding TC tokens nor padding EC tokens is the right solution. In Table 3, we pick NR-f as the round and sparsify subroutine for TRs main experiments. F.3 Ablation study on the effects of microbatch size and tile size Mtile Effect of microbatch size . Since the token rounding is applied on the microbatch level, the choice of microbatch size will result in different qualitative results for TR. Note that this also holds true for EC routing. For example, EC over sequence 31 will result in different model quality as EC over text segment. In Table 7, we vary the microbatch size while keeping the minibatch size (consumed tokens per optimization step) constant. We find that TR will preserve its trained MoE quality when Te/Mtile 2, but if Te/Mtile = 1 (the last row in both subtables), there is noticeable quality degradation for both validation perplexity and downstream task performance. However, the trained MoE quality with Te/Mtile = 1 is still better than training with EC and finetuning with TC top-K routing. Effect of the tile quantization size Mtile. Similarly in Table 8, we can find that TR is generally robust w.r.t. Mtile when Te/Mtile 2, and when Te/Mtile = 1 there is noticeable degradation but the overall result is still better than EC baseline. Table 7: Evaluation of token rounding algorithms when we vary microbatch size to change average number of tokens per expert ( Te). For each trial, we vary the microbatch size from 4 ( Te = 512) to 1 ( Te = 128) and keep minibatch size constant. The Mtile is always kept as 128. PPL refers to the validation perplexity at the end of training. Avg is the mean accuracy across the 11 downstream tasks. (a) 0.5B params, 40B tokens, 2/64 activated (Mtile = 128) Method PPL Wino SIQA SciQ PIQA OBQA HS COPA CSQA BoolQ ArcE ArcC TR ( Te = 1024) TR ( Te = 512) TR ( Te = 256) TR ( Te = 128) TC top-K EC (ft TC router) TR ( Te = 1024) TR ( Te = 512) TR ( Te = 256) TR ( Te = 128) TC top-K EC (ft TC router) 15.91 15.92 15.98 16. 15.94 16.98 13.08 13.10 13.12 13.55 13.12 15.01 52.9 51.4 52.2 51.7 51.0 50.0 51.5 53.4 51.9 51. 50.1 52.7 41.0 41.6 41.4 41.7 41.9 41.7 80.1 78.4 77.7 77.9 78.5 79.7 65.1 65.4 66.1 66. 64.8 64.9 31.0 31.6 32.2 30.8 33.0 31.6 37.9 38.1 37.9 37.7 38.1 36.8 63.0 65.0 66.0 67. 67.0 63.0 32.3 31.0 31.0 31.9 30.8 32.1 (b) 1.8B params, 40B tokens, 8/256 activated (Mtile = 128) 42.0 42.1 41.2 41.5 42.9 41. 81.7 81.7 81.8 82.0 81.3 79.6 68.9 69.6 69.7 69.2 69.8 66.9 34.8 35.2 33.6 32.8 33.8 30. 45.7 45.3 45.2 44.4 45.2 40.2 72.0 70.0 73.0 69.0 71.0 66.0 32.6 33.2 34.2 34.4 34.1 31. 59.3 61.1 59.6 61.2 54.7 60.7 59.5 61.4 56.9 59.8 56.7 60.5 54.9 57.4 57.2 54.7 55.8 54. 61.4 63.0 63.2 64.0 64.6 57.2 28.1 29.1 30.1 29.1 30.1 27.4 32.1 33.4 34.1 30.4 31.1 30. Avg 49.6 50.0 50.1 50.0 49.6 49.3 52.9 53.5 53.2 52.7 52.8 50.7 Table 8: Evaluation of token rounding algorithms when we vary the size of tile Mtile for token rounding. PPL refers to the validation perplexity at the end of training. Avg is the mean accuracy across the 11 downstream tasks. (a) 0.5B params, 40B tokens, 2/64 activated ( Te = 512)"
        },
        {
            "title": "OBQA",
            "content": "HS"
        },
        {
            "title": "ArcC",
            "content": "TR (Mtile = 64) TR (Mtile = 128) TR (Mtile = 256) TR (Mtile = 512) TC top-K EC (ft TC router) TR (Mtile = 64) TR (Mtile = 128) TR (Mtile = 256) TR (Mtile = 512) TC top-K EC (ft TC router) 15.90 15.92 16.00 16.17 15.94 16. 13.07 13.10 13.13 13.56 13.12 15.01 51.3 51.4 51.7 52.5 51.0 50.0 52.3 53.4 52.0 53.0 50.1 52. 41.7 41.6 41.4 41.2 41.9 41.7 78.1 78.4 78.7 80.2 78.5 79.7 65.6 65.4 66.3 65.2 64.8 64. 31.4 31.6 32.4 32.0 33.0 31.6 37.9 38.1 37.7 37.9 38.1 36.8 67.0 65.0 67.0 62.0 67.0 63. 32.4 31.0 31.3 31.0 30.8 32.1 (b) 1.8B params, 40B tokens, 8/256 activated ( Te = 512) 42.9 42.1 41.6 41.8 42.9 41.1 82.7 81.7 82.1 81. 81.3 79.6 69.4 69.6 69.2 68.4 69.8 66.9 35.4 35.2 35.4 34.0 33.8 30.6 45.6 45.3 45.3 44. 45.2 40.2 70.0 70.0 69.0 68.0 71.0 66.0 32.4 33.2 34.2 33.3 34.1 31.9 59.8 61.1 60.1 59. 54.7 60.7 56.6 61.4 58.0 58.1 56.7 60.5 57.2 57.4 58.2 57.2 55.8 54.6 64.4 63.0 65.6 59. 64.6 57.2 28.8 29.1 29.1 30.4 30.1 27.4 31.4 33.4 32.1 30.1 31.1 30."
        },
        {
            "title": "Avg",
            "content": "50.1 50.0 50.4 49.9 49.6 49.3 53.0 53.5 53.1 52.0 52.8 50."
        },
        {
            "title": "G Activation memory and training throughput benchmark configurations",
            "content": "The configurations for calculating IO cost in Figure 3 are presented in Table 9a and the configurations of Figure 13 and 14 are included in Table 9b. The configurations for the 4 subfigures in Figure 16 are listed below as list. Notice that we consistently use Mtile as 128 when we benchmark the TRs speed. 32 Table 9: Benchmark configurations for Figure 3, 13 and 14. (a) Benchmark configurations for memory IO cost in Figure 3. Model Size n 1.4B 7B 30B 120B 40960 40960 40960 40960 40960 768 768 768 768 768 64 128 256 512 1024 64 24576 1536 128 24576 1536 256 24576 1536 24576 1536 512 24576 1536 64 32768 4096 128 32768 4096 256 32768 4096 32768 4096 512 32768 4096 1024 128 32768 4096 256 32768 4096 32768 4096 512 32768 4096 1024 32768 4096 2048 512 256 128 64 32 512 256 128 64 32 32 16 8 4 2 32 16 8 4 1024 64 32 512 16 256 8 128 4 64 1024 64 32 512 16 256 8 128 4 64 (b) Benchmark configurations used by Figure 13 and 14, and all other kernel-level ablation studies. Model Size n 1.4B 7B 30B 40960 40960 768 768 768 512 1024 24576 1536 24576 1536 256 24576 1536 1024 128 64 32 128 32 8 4 2 8 2 32768 4096 32768 4096 256 512 256 128 32768 4096 1024 64 8 4 32768 512 256 16 120B 32768 4096 1024 128 32768 4096 2048 8 4 Top-left 2 subfigures: We use (T, d, n, K) = (16384, 1536, 256, 8) and we vary from 64 to 512. Top-right 2 subfigures: We use (T, d, n, K) = (16384, 1536, 1024, 2) and we vary from 16 to 128. Bottom-left 2 subfigures: We use (T, d, n, K) = (16384, 4096, 512, 8) and we vary from 64 to 512. Bottom-right 2 subfigures: We use (T, d, n, K) = (16384, 4096, 1024, 4) and we vary from 32 to 256."
        },
        {
            "title": "H Hyperparameter details for LM training",
            "content": "We use the OLMoE codebase (Muennighoff et al. 2025) and its downstream tasks in the default configuration52 except for MMLU: WinoGrande (wino) (Sakaguchi et al. 2020), Social IQA (SIQA) (Sap et al. 2019), SciQ (Johannes Welbl 2017), PIQA (Bisk et al. 2020), OpenBookQA (OBQA) (Mihaylov et al. 2018), HellaSwag (HS) (Zellers et al. 2019), COPA (Roemmele, Bejan, and Gordon 2011), CommonsenseQA (CSQA) (Talmor et al. 2019), BoolQ (Clark et al. 2019), Arc-Easy and Arc-Challenge (ArcE and ArcC) (Clark et al. 2018) datasets. We use deduplicated version of FineWeb-Edu (Ben Allal et al. 2024)53 for pretraining corpus, and train all models with context length of 4096 tokens. We always use MoE with SwiGLU for the MoE layers and we use an auxiliary load balancing loss (Shazeer et al. 2017) with coefficient 0.01 but we do not use the router loss (Zoph et al. 2022). Our attention block architecture is identical to OLMoEs attention block. We always tie the weight of the LM head with the weight of the token embedding matrix. Table 10: Common configurations for MoE pretraining experiment Config name in Tables 3 and 6 # layers # attn heads n # tokens in minibatch LR WD LR scheduler 0.5B params, 20B tokens, 8/64 activated 0.5B params, 40B tokens, 2/64 activated 1.8B params, 40B tokens, 8/256 activated 1.4B params, 50B tokens, 8/128 activated 1.4B params, 100B tokens, 2/128 activated 12 12 12 18 18 12 12 12 12 12 64 768 256 64 768 256 768 256 256 768 256 128 768 256 8 2 8 8 2 0.5M 1M 1M 1M 2M 6e-4 0.01 cosine w/. warmup (10% steps) 6e-4 0.01 cosine w/. warmup (10% steps) 6e-4 0.01 cosine w/. warmup (10% steps) 4e-4 0.01 cosine w/. warmup (10% steps) 4e-4 0.01 cosine w/. warmup (10% steps) For all EC with finetuned TC router experiments in Table 3, we use an additional 4B tokens and we only finetune the router weights with TC top-K routing (all other parameters are frozen). We always use learning rate of 2e-4, weight decay of 0.01 52https://github.com/allenai/OLMoE/blob/357454f4f647385839c0ff6b99a688dc7cd9c13f/configs/OLMoE-1B-7B-0924.yml 53https://huggingface.co/datasets/HuggingFaceTB/smollm-corpus and cosine learning rate scheduler with 10% warmup steps. The number of tokens per minibatch during finetuning is 1M. We do disable auxiliary load balancing loss during TC finetuning. For all EC with auxiliary router experiments, we use 2-layer MLP (each linear layer has size with SiLU activation) which takes as input the raw router logits and make independent binary predictions for all experts. We compute the averaged binary cross entropy loss over labels using the multi-label prediction loss, and scale the loss by 0.01. During the evaluation, we will let EC router compute the raw logits and raw scores and let the auxiliary router mask the token-expert pair with its own confidence score. We implement TC (token drop) by discarding tokens selected from the TC top-K sorting, or always round down."
        }
    ],
    "affiliations": [
        "Princeton University",
        "Together AI",
        "University of California, Berkeley"
    ]
}