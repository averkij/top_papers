{
    "paper_title": "Align Your Flow: Scaling Continuous-Time Flow Map Distillation",
    "authors": [
        "Amirmojtaba Sabour",
        "Sanja Fidler",
        "Karsten Kreis"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion- and flow-based models have emerged as state-of-the-art generative modeling approaches, but they require many sampling steps. Consistency models can distill these models into efficient one-step generators; however, unlike flow- and diffusion-based methods, their performance inevitably degrades when increasing the number of steps, which we show both analytically and empirically. Flow maps generalize these approaches by connecting any two noise levels in a single step and remain effective across all step counts. In this paper, we introduce two new continuous-time objectives for training flow maps, along with additional novel training techniques, generalizing existing consistency and flow matching objectives. We further demonstrate that autoguidance can improve performance, using a low-quality model for guidance during distillation, and an additional boost can be achieved by adversarial finetuning, with minimal loss in sample diversity. We extensively validate our flow map models, called Align Your Flow, on challenging image generation benchmarks and achieve state-of-the-art few-step generation performance on both ImageNet 64x64 and 512x512, using small and efficient neural networks. Finally, we show text-to-image flow map models that outperform all existing non-adversarially trained few-step samplers in text-conditioned synthesis."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 3 0 6 4 1 . 6 0 5 2 : r Align Your Flow: Scaling Continuous-Time Flow Map Distillation Amirmojtaba Sabour1,2,3 Sanja Fidler1,2,3 Karsten Kreis1 1 NVIDIA 2 University of Toronto 3 Vector Institute Project Page: https://research.nvidia.com/labs/toronto-ai/AlignYourFlow/ Figure 1: Four-step samples by our distilled text-conditioned flow map model (prompts in Appendix)."
        },
        {
            "title": "Abstract",
            "content": "Diffusionand flow-based models have emerged as state-of-the-art generative modeling approaches, but they require many sampling steps. Consistency models can distill these models into efficient one-step generators; however, unlike flowand diffusion-based methods, their performance inevitably degrades when increasing the number of steps, which we show both analytically and empirically. Flow maps generalize these approaches by connecting any two noise levels in single step and remain effective across all step counts. In this paper, we introduce two new continuous-time objectives for training flow maps, along with additional novel training techniques, generalizing existing consistency and flow matching objectives. We further demonstrate that autoguidance can improve performance, using lowquality model for guidance during distillation, and an additional boost can be achieved by adversarial finetuning, with minimal loss in sample diversity. We extensively validate our flow map models, called Align Your Flow, on challenging image generation benchmarks and achieve state-of-the-art few-step generation performance on both ImageNet 64x64 and 512x512, using small and efficient neural networks. Finally, we show text-to-image flow map models that outperform all existing non-adversarially trained few-step samplers in text-conditioned synthesis. Preprint. Under review. Figure 2: Overview of Flow Maps. Flow maps generalize both consistency models and flow matching by connecting any two noise levels (s, t) in single step. When = 0, flow maps reduce to consistency models; when theyre equivalent to standard flow matching models. Our proposed EMD objective (see Theorem 3.2) similarly generalizes the continuous-time consistency and flow matching losses. For detailed derivations, please see the Appendix."
        },
        {
            "title": "Introduction",
            "content": "Diffusion and flow-based generative models have revolutionized generative modeling [25, 70, 45, 62, 14, 8], but they rely on slow iterative sampling. This has led to the development of approaches to accelerate generation. Advanced, higher-order samplers [68, 50, 51, 12, 89, 34, 61] help, but cannot produce high quality outputs with <10 steps. Distillation techniques [63, 67, 87, 72], in contrast, can successfully distill models into few-step generators. In particular, consistency models [71, 69, 49] and variety of related techniques [38, 79, 80, 42, 93, 17, 22] have gained much attention recently. Consistency models learn to transfer samples that lie on teacher-defined deterministic noise-to-data paths to the same, consistent clean outputs in single prediction. These approaches excel in few step generation, but have been empirically shown to degrade in performance as the number of steps increases. In this work, we analytically show that consistency models are inherently incompatible with multi-step sampling. Specifically, we show that their objective of strictly predicting clean outputs inevitably leads to error accumulation over multiple denoising steps. Motivated by this limitation, we turn to the flow map formulation as unifying and more robust alternative. The flow map framework - also known as Consistency Trajectory Models - was introduced in [38, 5] and encompasses diffusion and flowbased models [45], consistency models [71, 69, 49], and other distillation variants [80, 17, 93, 94] within single coherent formulation. Flow maps allow connecting any two noise levels in single step, enabling efficient few-step sampling as well as flexible multi-step sampling. As flow maps, figuratively speaking, learn mapping that aligns the teacher flow into few-step sampler, we call our approach Align Your Flow (AYF). We propose two new continuous-time training objectives, which can be interpreted as AYFs versions of the Eulerian and Lagrangian losses described by Boffi et al. [5]. The new objectives use consistency condition at either the beginning or the end of denoising interval. Notably, the first of our objectives generalizes both the continuous-time consistency loss [71, 49] and the flow matching loss [45]. While regular consistency models only perform well for singleor two-step generation and degrade for multi-step sampling, e.g. for 4 steps or more, flow map models such as AYF produce high-quality outputs in this multi-step setting, too. To scale AYF to high performance, we leverage the recently proposed autoguidance [35], where low-quality guidance model checkpoint is used together with the regular model to produce model with enhanced quality. Specifically, we propose to distill an autoguided teacher model into an AYF student and introduce several practical techniques that stabilize flow map training and push performance further. Moreover, unlike prior distillation approaches that rely on adversarial training to boost quality at the expense of sample diversity [67, 66, 87, 86, 38], we show that short finetuning of pretrained AYF model with combination of our proposed flow map objective and an adversarial loss is sufficient to yield significantly sharper images with minimal impact on diversity. We validate AYF on popular image generation benchmarks and achieve state-of-the-art performance among few-step generators on both ImageNet 64x64 and 512x512, while using only small and efficient neural networks  (Fig. 4)  . For instance, 4-step sampling of AYFs ImageNet models is as fast or faster than previous works single step generation. Additionally, our adversarially finetuned AYF also achieves significantly higher diversity compared to other adversarial training approaches. We further distill the popular FLUX.1 model [41] and obtain text-to-image AYF flow map models that significantly outperform all existing non-adversarially trained few-step generators in text-conditioned 2 Figure 3: Samples (4 steps): LCM [54], TCD [93], FLUX.1 [schnell] [41], AYF (view zoomed in). synthesis  (Fig. 1)  . For these experiments, we use an efficient LoRA [27] framework, avoiding the overhead of many previous text-to-image distillation approaches. Contributions. (i) We prove that consistency models inherently suffer from error accumulation in multi-step sampling. (ii) We propose Align Your Flow, high-performance few-step flow map model with new theoretical insights. (iii) We introduce two new training objectives and stabilization techniques for flow map learning. (iv) We apply autoguidance for distillation for the first time and show that adversarial finetuning further boosts performance with minimal loss in diversity. (v) We achieve state-of-the-art few-step generation performance on ImageNet, and we also show fast high-resolution text-to-image generation, outperforming all non-adversarial methods in this task."
        },
        {
            "title": "2 Background",
            "content": "Diffusion Models and Flow Matching. Diffusion models are probabilistic generative models that inject noise into the data with forward diffusion process and generate samples by learning and simulating time-reversed backward diffusion process, initialized by pure Gaussian noise. Flow matching [45, 48, 2, 1, 39] is generalization of these methods that eliminates the requirement of the noise being Gaussian and allows learning continuous flow between any two distributions p0, p1 that converts samples from one to the other. Denote the data distribution by x0 pdata and the noise distribution by x1 pnoise. Let xt = (1 t) x0 + x1 indicate the noisy samples of the data for time [0, 1], corresponding to the rectified flow [48] or conditional optimal transport [45] formulation. The flow matching training (cid:3); w(t) is weighting function (cid:2)w(t)vθ(xt, t) (x1 x0)2 objective is then given by Ex0,x1,t and vθ is neural network parametrized by θ. The standard sampling procedure starts at = 1 by sampling x1 pnoise. Then the probability flow ODE (PF-ODE), defined by dxt dt = vθ(xt, t)dt, is simulated from = 1 to = 0 to obtain the final outputs. We will assume to be in the flow matching framework from this point on of the paper. 2 Consistency Models. Consistency models (CM) [71] train neural network fθ(xt, t) to map noisy inputs xt directly to their corresponding clean samples x0, following the PF-ODE. Consequently, fθ(xt, t) must satisfy the boundary condition fθ(x, 0) = x, which is typically enforced by parameterizing fθ(xt, t) = cskip(t)xt + cout(t)Fθ(xt, t) with cskip(0) = 1, cout(0) = 0. CMs are trained to have consistent outputs between adjacent timesteps. They can be trained from scratch or distilled from given diffusion or flow models. In this work, we are focusing on distillation. Depending on how time is dealt with, CMs can be split into two categories: Discrete-time CMs. The training objective is defined between adjacent timesteps as Ext,t [w(t)d(fθ(xt, t), fθ(xtt, t))] , (1) where θ denotes stopgrad(θ), w(t) is weighting function, > 0 is the distance between adjacent timesteps, and d(., .) is distance function. Common choices include ℓ2 loss d(x, y) = y2 2, Pseudo-Huber loss d(x, y) = (cid:112)x y2 2 + c2 [69], and LPIPS loss [90]. Discrete-time CMs are sensitive to the choice of t, and require manually designed annealing schedules [71, 18]. The noisy sample xtt at the preceding timestep is often obtained from xt by numerically solving the PF-ODE, which can cause additional discretization errors. 3 Continuous-time CMs. When using d(x, y) = y2 [71] show that the gradient of Eq. (1) with respect to θ converges to (cid:21) θExt,t (cid:20) w(t)f θ (xt, t) dfθ (xt, t) dt 2 and taking the limit 0, Song et al. , (2) dt where dfθ (xt,t) = xtfθ (xt, t) dxt dt +tfθ (xt, t) is the tangent of fθ at (xt, t) along the trajectory of the PF-ODE dxt dt . This means continuous-time CMs do not need to rely on numerical ODE solvers which avoids discretization errors and offers better supervision signals during training. Recently, Lu and Song [49] successfully stabilized and scaled continuous-time CMs and achieved significantly better results compared to the discrete-time approach."
        },
        {
            "title": "3 Continuous-Time Flow Map Distillation",
            "content": "Flow maps generalize diffusion, flow-based and consistency models within single unified framework by training neural network fθ(xt, t, s) to map noisy inputs xt directly to any point xs along the PF-ODE in single step. Unlike consistency models, which only perform well for singleor two-step generation but degrade in multi-step sampling, flow maps remain effective at all step counts. In Sec. 3.1, we first show that standard consistency models are incompatible with multi-step sampling, leading to inevitable performance degradation beyond certain step count. Next, in Sec. 3.2, we introduce two novel continuous-time objectives for distilling flow maps from pretrained flow model. Finally, in Sec. 3.3, we explain how we leverage autoguidance to sharpen the flow map. Sec. 3.4 addresses implementation details. The detailed training algorithm for AYF is provided in the Appendix. Figure 4: Two-step AYF samples on ImageNet512. 3.1 Consistency Models are Flawed Multi-Step Generators CMs are powerful approach to turn flow-based models into one-step generators. To allow CMs to trade compute for sample quality, multi-step sampling procedure was introduced by Song et al. [71]. This process sequentially denoises noisy xt by first removing all noise to estimate the clean data and then reintroducing smaller amounts of noise. However, in practice, this sampling procedure performs poorly as the number of steps increases and most prior works only demonstrate 1or 2-step results. To understand this behavior, we analyze simple case where the initial distribution is an isotropic Gaussian with standard deviation c, i.e. pdata(x) = (0, c2I). The following theorem shows that regardless of how accurate (non-optimal) CM is, increasing the number of sampling steps beyond certain point will lead to worse performance due to error accumulation in that setting. Theorem 3.1 (Proof in Appendix). Let pdata(x) = (0, c2I) be the data distribution, and let (xt, t) denote the optimal consistency model. For any δ > 0, there exists suboptimal consistency model (xt, t) such that Extp(x,t) (cid:2)f (xt, t) (xt, t)2 2 (cid:3) < δ for all [0, 1], and there is some integer for which increasing the number of sampling steps beyond increases the Wasserstein-2 distance of the generated samples to the ground truth distribution (i.e. worse approximation of the ground truth). This suggests that CMs, by design, are not suited for multi-step generation. Interestingly, when = 0.5a common choice in diffusion model training, where the data is often normalized to this std. dev. [34]multi-step CM sampling with non-optimal CM produces the best samples at two steps  (Fig. 5)  . This is in line with common observations in the literature [49]. This behavior is the opposite of standard diffusion models, which improve as the number of steps increases. Prior works have attempted to address this issue (see Sec. 4), and they all ultimately reduce to special cases of flow maps. 4 3.2 Learning Flow Maps Flow maps are neural networks fθ(xt, t, s) that generalize CMs by mapping noisy input xt directly to any other point xs by following the PF-ODE from time to s. When = 0, they reduce to standard CMs. When performing many small steps, they become equivalent to regular flow or diffusion model sampling with the PF-ODE and Euler integration. valid flow map fθ(xt, t, s) must satisfy the general boundary condition fθ(xt, t, t) = xt for all t. As is done in prior work, this is enforced in practice by parameterizing the model as fθ(xt, t, s) = cskip(t, s)xt + cout(t, s)Fθ(xt, t, s) where cskip(t, t) = 1 and cout(t, t) = 0 for all t. In this work, we set cskip(t, s) = 1 and cout(t, s) = (s t) for simplicity and to align it with an Euler ODE solver. Figure 5: Wasserstein-2 distance between multi-step consistency samples and data distribution (c=0.5). Unlike CMs, which perform poorly in multi-step sampling, flow maps are designed to excel in this scenario. Additionally, their ability to fully traverse the PF-ODE enables them to accelerate tasks such as image inversion and editing by directly mapping images to noise [43]. As we are interested in distilling diffusion or flow matching model, we assume access to pretrained velocity model vϕ(xt, t). The flow map model is trained by aligning its single-step predictions with the trajectories generated by the teachers PF-ODE, i.e. dxt dt = vϕ(xt, t). We propose two primary methods for training flow maps. The first training objective aims to ensure that for fixed s, the output of the flow map remains constant as we move (xt, t) along the PF-ODE. Let θ = stopgrad(θ). The theorem below summarizes the approach. We call this loss AYF-Eulerian Map Distillation (AYFEMD), as it can also be interpreted as variant of the Eulerian loss of Boffi et al. [5]. The AYF-EMD loss naturally generalizes the loss used to train continuous-time consistency models [71, 49], as it reduces to the same objective when = 0. Interestingly, it also generalizes the standard flow matching loss, to which it reduces in the limit as t. See Appendix for details. Theorem 3.2 (Proof in Appendix). Let fθ(xt, t, s) be the flow map. Consider the loss function defined between two adjacent starting timesteps and = + ϵ(s t) for small ϵ > 0, Ext,t,s (cid:2)w(t, s)fθ(xt, t, s) fθ(xt, t, s)2 2 (cid:3) , where xt is obtained by applying 1-step Euler solver to the PF-ODE from to t. In the limit as ϵ 0, the gradient of this objective with respect to θ converges to: θExt,t,s (cid:20) w(t, s)sign(t s) θ (xt, t, s) dfθ(xt, t, s) dt (cid:21) , where w(t, s) = w(t, s) s. The second approach ensures consistency at timestep instead. This method tries to ensure that for fixed (xt, t), the trajectory fθ(xt, t, ) is aligned with that points PF-ODE. We call this loss AYF-Lagrangian Map Distillation (AYF-LMD), as it is related to the Lagrangian loss of Boffi et al. [5]. The theorem below formalizes this approach. Theorem 3.3 (Proof in Appendix). Let fθ(xt, t, s) be the flow map. Consider the loss function defined between two adjacent ending timesteps and = + ϵ(t s) for small ϵ > 0, Ext,t,s (cid:2)w(t, s)fθ(xt, t, s) ODEss[fθ(xt, t, s)]2 2 (cid:3) , where ODEts(x) refers to running 1-step Euler solver on the PF-ODE starting from at timestep to timestep s. In the limit as ϵ 0, the gradient of this objective with respect to θ converges to: (cid:20) θExt,t,s w(t, s) sign(s t) θ (xt, t, s) (cid:18) dfθ (xt, t, s) ds vϕ(fθ (xt, t, s), s) , (cid:19)(cid:21) where w(t, s) = w(t, s) s. In our 2D toy experiments, comparing the two objectives above, we found the AYF-LMD objective to be more stable. However, when applied to image datasets, it leads to overly smoothened samples that drastically reduce the output quality (see Appendix for detailed ablation studies). 5 3.3 Sharpening the Distribution with Autoguidance The training objective of diffusionand flow-based models strongly encourages the model to cover the entire data distribution. Yet it lacks enough data to learn how to generate good samples in the tails of the distribution. The issue is even worse in distilled models which use fewer sampling steps. As result, many prior distillation methods rely on adversarial objectives to achieve peak performance, often sacrificing diversity and ignoring low-probability regions altogether. The most commonly used technique to partially address this in conditional diffusion and flow-based models is classifier-free guidance (CFG) [24]. CFG trains flow or diffusion model for both conditional and unconditional generation and steers samples away from the unconditional regions during sampling. Prior works [57, 49] have explored distilling CFG with great success. However, CFG struggles with overshooting the conditional distribution at large guidance scales, which leads to overly simplistic samples [40]. Recently, Karras et al. [35] introduced autoguidance as better alternative for CFG. Unlike CFG, this technique works for unconditional generation as well. Autoguidance uses smaller, less trained version of the main model for guidance, essentially steering samples away from low-quality sample regions in the probability distribution, where the weaker guidance model performs particularly poorly. We found that distilling autoguided teacher models can significantly improve performance compared to standard CFG. To the best of our knowledge, we are the first to demonstrate the distillation of autoguided teachers. Specifically, during flow map distillation we define the guidance scale λ and use the autoguided teacher velocity vguided ϕ (xt, t) = λvϕ(xt, t) + (1 λ)vweak ϕ (xt, t), (3) ϕ where vweak represents the weaker guidance model. In summary, we use autoguidance in the teacher as mechanism to sharpen the distilled flow map model. See Appendix for visual comparison between autoguidance and CFG on 2D toy distribution. 3.4 Training Tricks Training continuous-time CMs has historically been unstable [69, 18]. Recently, sCM [49] addressed this issue by introducing techniques focused on parameterization, network architectures, and modifications to the training objective. Following their approach, we stabilize time embeddings and apply tangent normalization, while also introducing few additional techniques to further improve stability. Our image models are trained with the AYF-EMD objective objective in Theorem 3.2, which relies on the tangent function dfθ (xt,t,s) . Under our parametrization, this tangent function is computed by dt dfθ (xt, t, s) dt = (cid:18) dxt dt (cid:19) Fθ (xt, t, s) + (s t) dFθ (xt, t, s) dt , (4) where dxt dt = vϕ(xt, t) represents the direction given by the pretrained diffusion or flow model along the PF-ODE. We find that most terms in this formulation are relatively stable, except for dFθ(xt,t,s) dt + tFθ(xt, t, s). Among these, the instability originates mainly dt = xtFθ(xt, t, s) dxt from tFθ(xt, t, s), which can be decomposed into tFθ(xt, t, s) = cnoise(t) emb(cnoise) cnoise Fθ emb , where emb() refers to the time embeddings, most commonly in the form of positional embeddings [25, 78] or Fourier embeddings [70, 73]. sCM [49] proposes several techniques to stabilize this term including tangent normalization, adaptive weighting, and tangent warmup. We use tangent normalization [49], i.e. dfθ dt dfθ + c) with = 0.1, as we find it to be critical for stable training. However, in our experiments, adaptive weighting had no meaningful impact and can be removed. We make few tweaks to the time embeddings and tangent warmup to ensure compatibility with flow matching and better training dynamics which we describe below. Stabilizing the Time Embeddings The time embedding layers are one of the causes for the instability of tFθ(xt, t, s). As noted in [49], the cnoise parameterization used in most CMs is based on the EDM [34] framework, where the noise level is defined as cnoise(σ) = log(σ). In the flow matching framework, which we use, the noise level for timestep is given by σt = 1t , which can lead to instabilities when passing through log operation as 0 or 1. To address this, we modify the dt /( dfθ dt 6 time parameterization by setting cnoise(t) = t, ensuring stable partial derivatives. To utilize pretrained teacher model checkpoints trained with different time parameterizations, we first finetune the students time embedding module to align with the outputs of the original checkpoints. For example, if we want to adapt EDM2 checkpoints, which use σt = 1t , we minimize the following objective: (cid:105) Etp(t) (cid:104) embnew(t) emboriginal (log (σt))2 2 . This approach enables us to re-purpose nearly any checkpoint, making it compatible with our flow matching framework with minimal finetuning, rather than training new models from scratch. Regularized Tangent Warmup We initialize the student model with pretrained flow matching or diffusion model weights, following prior work to speed up training [49, 71]. Lu and Song [49] proposed gradual warmup procedure for the second term in Eq. (4), i.e., dfθ (xt,t,s) . Specifically, they introduced coefficient that linearly increases from 0 to 1 over the first 10k training iterations, gradually incorporating the term. This warmup has clear intuitive motivation. When considering only the first term in Eq. (4) (i.e., the = 0 case), the objective simplifies to regularization term that encourages flow maps to remain close to straight lines (please see the Appendix for the derivation): dt (cid:20) sign(t s)f θ (xt, t, s) θ (cid:18) dxt dt (cid:19)(cid:21) Fθ(xt, t, s) θ[Fθ(xt, t, s) vϕ(xt, t)2 2]. (5) Therefore, for < 1, the warmed-up loss with coefficient is equivalent to weighted sum of the actual loss and this regularization term: (cid:20) sign(t s)f θ (xt, t, s) θ (cid:18) dxt dt Fθ (xt, t, s) + r(s t) = rθ (cid:20) sign(t s)f θ (xt, t, s) (cid:21) dfθ (xt, t, s) ds + (1 r)θ (cid:19)(cid:21) dFθ (xt, t, s) dt (cid:2)t Fθ(xt, t, s) vϕ(xt, t)2 (cid:3) . 2 In our experiments, training these models for too long after the warmup phase can cause destabilization. simple fix is to clamp to value smaller than 1, ensuring some regularization remains. We found rmax = 0.99 to be effective in all cases. Timestep scheduling As in standard diffusion, flow-based, and consistency models, selecting an effective sampling schedule for (t, s) during training is crucial. Similar to standard consistency models, where information must propagate from = 0 to = 1 over training, flow map models propagate information from small intervals t = 0 to large ones t = 1. For details on our practical implementation of the schedules, as well as complete training algorithms, please see the Appendix."
        },
        {
            "title": "4 Related Work",
            "content": "Consistency Models. Flow Map Models generalize the seminal CMs, introduced by Song et al. [71]. Early CMs were challenging to train and several subsequent works improved their stability and performance, using new objectives [69], weighting functions [18] or variance reduction techniques [79], among other tricks. Truncated CMs [42] proposed second training stage, focusing exclusively on the noisier time interval, and Lu and Song [49] successfully implemented continuous-time CMs for the first time. Flow Map Models. Consistency Trajectory Models (CTM) [38] can be considered the first flow map-like models. They combine the approach with adversarial training. Trajectory Consistency Distillation [93] extends CTMs to text-to-image generation, and Bidirectional CMs [43] train additionally on timestep pairs with t<s, also accelerating inversion and tasks such as inpainting and blind image restoration. Kim et al. [37] trained CTMs connecting arbitrary distributions. Multistep CMs [22] split the denoising interval into sub-intervals and train CMs within each one, enabling impressive generation quality using 2-8 steps. Phased CMs [80] use similar interval-splitting strategy combined with an adversarial objective. These methods can be seen as learning flow maps by training on (t, s) pairs, where is the start of the sub-interval containing t. Flow Map Matching [5] provides rigorous analysis of the continuous-time flow map formulation and proposes several continuous-time losses. Shortcut models [17] adopt similar flow map framework, but these two works struggle to produce high-quality imagesin contrast to our novel AYF, the first high-performance continuous-time flow map model. Accelerating Diffusion Models. Early diffusion distillation approaches are knowledge distillation [52] and progressive distillation [63, 57]. Other methods include adversarial distillation [67, 66], variational score distillation (VSD) [87, 86], operator learning [92] and further 7 Table 1: Sample quality on classconditional ImageNet 64x64. Recall metric is also included. Method Diffusion Models & Fast Samplers ADM [11] RIN [29] DisCo-Diff [83] DPM-Solver [50] EDM (Heun) [34] EDM2 (Heun) [36] EDM2 + Autoguidance [35] Adversarial & Joint Training BigGAN-deep [7] StyleGAN-XL [65] Diff-Instruct [55] DMD [87] DMD2 [86] SiD [97] CTM [38] Moment Matching [64] GDD-I [91] SiDA [96] AYF + adv. loss (ours) Diffusion Distillation without Adversarial Objectives DFNO [92] PID [74] TRACT [4] BOOT [20] PD [63] (reimpl. from Heek et al. [22]) EMD [82] CD [71] MultiStep-CD [22] sCD [49] AYF (ours) Consistency Training iCT [69] iCT-deep [69] ECT [18] sCT [49] TCM-S [42] TCM-XL [42] NFE () FID () Recall () 250 1000 623 20 79 63 63 1 1 1 1 1 1 1 2 1 2 1 1 1 2 4 8 1 1 1 1 1 2 1 1 2 1 2 1 2 4 8 1 1 1 1 2 1 2 1 2.07 1.23 1.22 3.42 2.44 1.33 1.01 4.06 1.52 5.57 2.62 1.28 1.52 1.92 1.73 3.00 3.86 1.16 1.11 1.32 1.17 1.10 1.07 7.83 8.51 7.43 16.3 10.70 4.70 2.20 6.20 1.90 2.44 1.66 2.98 1.25 1.15 1.12 4.02 3.25 2.77 2.04 1.48 2.88 2.31 2.20 1.62 0.63 - - - 0.68 0.68 0.69 0.48 - - - - 0.63 0.57 - - - 0.60 0.62 0.65 0.65 0.65 0. 0.61 - - 0.36 0.65 - 0.59 - - 0.66 0.66 0.65 0.66 0.66 0.66 0.63 0.63 - - - - - - - Table 2: Sample quality on class-conditional ImageNet 512x512. For additional baselines, which AYS all outperforms, please see the Appendix. NFE () FID () Method #Params Gflops Time (s) Teacher Diffusion Model EDM2-S [36] EDM2-XXL [36] EDM2-S + Autoguid. [35] EDM2-XXL + Autoguid. [35] Adversarial & Joint Training SiDA-S [96] (best adv. baseline) AYF-S + adv. loss (ours) Consistency Distillation sCD-S [49] sCD-M [49] sCD-L [49] sCD-XL [49] sCD-XXL [49] AYF-S (ours) Consistency Training sCT-S [49] sCT-M [49] sCT-L [49] sCT-XL [49] sCT-XXL [49] 632 632 632 632 1 1 2 4 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 2.23 1.81 1.34 1.25 1.69 1.92 1.81 1.64 3.07 2.50 2.75 2.26 2.55 2.04 2.40 1.93 2.28 1.88 3.32 1.87 1.70 10.13 9.86 5.84 5.53 5.15 4.65 4.33 3.73 4.29 3.76 280M 1.5B 280M 1.5B 280M 280M 280M 280M 280M 280M 498M 498M 778M 778M 1.1B 1.1B 1.5B 1.5B 280M 280M 280M 280M 280M 498M 498M 778M 778M 1.1B 1.1B 1.5B 1.5B 12852 69552 12852 69552 8.31 31.50 8.31 31.50 102 102 204 102 204 181 362 282 564 406 812 552 1104 102 204 408 102 204 181 362 282 564 406 812 552 1104 0.06 0.06 0.12 0.24 0.06 0.13 0.10 0.20 0.14 0.28 0.19 0.38 0.25 0.50 0.06 0.12 0.24 0.06 0.13 0.10 0.20 0.14 0.28 0.19 0.38 0.25 0.50 techniques [55, 20, 4, 74, 85, 48, 82, 46, 97, 95], many of them relying on adversarial losses, too. However, although popular, adversarial methods introduce training complexities due to their GAN-like objectives. VSD exhibits similar properties and does not work well at high guidance levels. Moreover, these methods can produce samples with limited diversity. For these reasons we avoid such objectives and instead rely on autoguidance to achieve crisp high-quality outputs. Finally, many training-free methods efficiently solve diffusion models generative differential equations [50, 51, 32, 34, 12, 89, 61], but they are unable to perform well when using <10 generation steps."
        },
        {
            "title": "5 Experiments",
            "content": "We train AYF flow maps on ImageNet [10] at resolutions 64 64 and 512 512, measuring sample quality using Fréchet Inception Distance (FID) [23], as previous works. We also use our AYF framework to distill FLUX.1 [dev] [41], the best text-to-image diffusion model, using an efficient LoRA [27] framework and reduce sampling steps to just 4. Experiment details explained in the Appendix. ImageNet Flow Maps. We adopt the EDM2 [36] using their framework, small models, and modify network parametrization and time embedding layer as detailed in Sec. 3. Pretrained checkpoints available online are used both as teacher network and as flow map initialization. We incorporate autoguidance into the flow map model by introducing an additional input, λ, corresponding to the guidance scale [49, 57]. During training, λ is uniformly sampled from [1, 3] and applied to the teacher model via autoguidance. At inference, we leverage the γ-sampling algorithm from [38] for stochastic multistep sampling of flow map models. Results are reported using the optimal γ and λ values. For ImageNet 512 512, the teacher and distilled models are in latent space [60]. Figure 6: FID as function of wall clock time. (b) ImageNet-512 (a) ImageNet-64 In Tab. 1 we show ImageNet 64 64 results, reporting FID and recall scores along with number of neural function evaluations (NFE). Our flow maps achieve the best sample quality among all 8 non-adversarial few-step methods, given only 2 sampling steps by sacrificing optimal 1-step quality. This is because learning flow map is more challenging task compared to only consistency model. In Tab. 2 we compare AYF against the state-of-the-art consistency model sCD/sCT [49] on ImageNet 512 512, also reporting total wall sampling clock time, Gflops, and #parameters. We show that although our small-sized model achieves slightly worse one step sample quality, it is on par with the best sCD model at only two steps while using only 18% of the larger models compute. Increasing the sampling steps to four improves the quality even further while still being over twice as fast as the large 1-step sCM model (wallclock time). We further analyze the performance vs. sampling speed trade-off in Fig. 6, showing that AYF is much more efficient than sCD/sCT (also see Appendix for additional comparison). Autoguidance allows AYF to use small network and still achieve strong performance and the efficient network results in 2-step or 4-step synthesis still being lightning fast. Adversarial finetuning of AYF. Given pretrained AYF flow map model, we found that short finetuning stage using combination of the EMD objective and an adversarial loss can significantly boost the performance across the board, especially for 1-step generation, with minimal impact to sample diversity as measured by recall scores. Using this approach, we achieve state-of-the-art performance on few-step generation on ImageNet64 (see Tab. 1). For implementation details, please see Appendix. Additional GAN and diffusion model baselines on ImageNet 512 512 can be found in the Appendix; AYF outperforms all of them. Text-to-Image Flow Maps. We apply AYF to distill the open-source text-to-image model FLUX.1 [dev] [41] into few-step generator, finetuning FLUX.1 base model into flow map model using LoRA [27] with the objective in Theorem 3.2 for 10,000 steps. This distillation process took approx. four hours on 8 NVIDIA A100 GPUs, which is highly efficient, in contrast to several previous large-scale text-to-image distillation methods. Samples from the model are shown in Fig. 1. We compare to LCM [53, 54] and TCD [93], two consistency-distilled LoRAs trained on top of SDXL [59] without adversarial objectives. To evaluate quality we ran user study. The results  (Fig. 7)  show clear preference for our method. We also provide qualitative comparisons in Fig. 3. Compared to LCM and TCD, our images are more aesthetically pleasing with finer details. We also included FLUX.1 [schnell] [41], commercially distilled model trained with Latent Adversarial Diffusion Distillation [66]. Our method achieves comparable image quality to the [schnell] model, while requiring only four sampling steps and 32 GPU hours without the use of adversarial losses. In conclusion, AYF achieves state-of-the-art fewstep text-to-image generation performance among non-adversarial methods. Detailed ablation studies on different components of AYF (EMD vs. LMD; autoguidance vs. CFG, AYF vs. Shortcut) are presented in the Appendix. Additional qualitative examples of images generated by AYF are shown in the Appendix, too. Figure 7: User preferences comparing LoRA-based consistency and flow map models (4-step samples). LCM and TCD use SDXL and AYF uses FLUX.1 [dev] as base model, respectively."
        },
        {
            "title": "6 Conclusions",
            "content": "We have presented Align Your Flow (AYF), novel continuous-time distillation method for training flow maps, which generalizes flow-matching and consistency-based models. Importantly, flow maps remain effective generators across all denoising step counts, unlike standard consistency models; fact we prove analytically for the first time. In addition, we use autoguidance to enhance the quality of the teacher model, resulting in an improved distilled student, and an additional boost can be achieved by adversarial finetuning, with minimal impact in sample diversity. We achieve state-of-the-art performance among non-adversarial distillation methods on both ImageNet64 and ImageNet512 generation. Since AYF requires only relatively small neural networks, which further reduces the computational burden and boosts sampling efficiency, even 2-step or 4-step sampling from AYF is as fast or faster than previous single-step generators. We also distill FLUX.1 using an efficient LoRA framework, resulting in state-of-the-art text-to-image generation performance among non-adversarial distillation approaches. 9 Future work could explore applying AYF to video model distillation or in other domains, for instance in drug discovery for efficient molecule or protein modeling."
        },
        {
            "title": "References",
            "content": "[1] Michael S. Albergo, Nicholas M. Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023. [2] Michael Samuel Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In The Eleventh International Conference on Learning Representations (ICLR), 2023. [3] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2266922679, 2023. [4] David Berthelot, Arnaud Autef, Jierui Lin, Dian Ang Yap, Shuangfei Zhai, Siyuan Hu, Daniel Zheng, Walter Talbott, and Eric Gu. Tract: Denoising diffusion models with transitive closure time-distillation. arXiv preprint arXiv:2303.04248, 2023. [5] Nicholas Boffi, Michael Albergo, and Eric Vanden-Eijnden. Flow map matching. arXiv preprint arXiv:2406.07507, 2024. [6] Nicholas Boffi, Michael Albergo, and Eric Vanden-Eijnden. How to build consistency model: Learning flow maps via self-distillation. arXiv preprint arXiv:2505.18825, 2025. [7] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis, 2019. [8] Tim Brooks, Bill Peebles, Connor Homes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Wing Yin Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. URL https://openai.com/research/ video-generation-models-as-world-simulators. [9] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1131511325, 2022. [10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: largescale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248255, 2009. doi: 10.1109/CVPR.2009.5206848. [11] Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion Models Beat GANs on Image Synthesis. In Advances in Neural Information Processing Systems, 2021. [12] Tim Dockhorn, Arash Vahdat, and Karsten Kreis. GENIE: Higher-Order Denoising Diffusion Solvers. In Advances in Neural Information Processing Systems (NeurIPS), 2022. [13] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. [14] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In International Conference on Machine Learning (ICML), 2024. [15] Zhengcong Fei, Mingyuan Fan, Changqian Yu, and Junshi Huang. Scalable diffusion models with state space backbone. arXiv preprint arXiv:2402.05608, 2024. [16] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, and Junshi Huang. Diffusion-rwkv: Scaling rwkv-like architectures for diffusion models. arXiv preprint arXiv:2404.04478, 2024. 10 [17] Kevin Frans, Danijar Hafner, Sergey Levine, and Pieter Abbeel. One step diffusion via shortcut models. ArXiv, abs/2410.12557, 2024. [18] Zhengyang Geng, Ashwini Pokle, William Luo, Justin Lin, and Zico Kolter. Consistency models made easy. arXiv preprint arXiv:2406.14548, 2024. [19] Zhengyang Geng, Mingyang Deng, Xingjian Bai, Zico Kolter, and Kaiming He. Mean flows for one-step generative modeling. arXiv preprint arXiv:2505.13447, 2025. [20] Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Lingjie Liu, and Joshua Susskind. Boot: Data-free distillation of denoising diffusion models with bootstrapping. In ICML 2023 Workshop on Structured Probabilistic Inference & Generative Modeling, 2023. [21] Ali Hatamizadeh, Jiaming Song, Guilin Liu, Jan Kautz, and Arash Vahdat. Diffit: Diffusion vision transformers for image generation. In European Conference on Computer Vision, pages 3755. Springer, 2024. [22] Jonathan Heek, Emiel Hoogeboom, and Tim Salimans. Multistep consistency models. arXiv preprint arXiv:2403.06807, 2024. [23] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. [24] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. [25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems (NeurIPS), 2020. [26] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution images. In International Conference on Machine Learning, pages 13213 13232. PMLR, 2023. [27] J. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations (ICLR), 2022. [28] Nick Huang, Aaron Gokaslan, Volodymyr Kuleshov, and James Tompkin. The gan is dead; long live the gan! modern gan baseline. Advances in Neural Information Processing Systems, 37:4417744215, 2024. [29] A. Jabri, David J. Fleet, and Ting Chen. Scalable adaptive computation for iterative generation. In International Conference on Machine Learning, 2022. [30] Jackyhate. Text-to-image-2m, 2025. https://huggingface.co/datasets/jackyhate/ text-to-image-2M. [31] Alexia Jolicoeur-Martineau. The relativistic discriminator: key element missing from standard gan. arXiv preprint arXiv:1807.00734, 2018. [32] Alexia Jolicoeur-Martineau, Ke Li, Rémi Piché-Taillefer, Tal Kachman, and Ioannis Mitliagkas. Gotta go fast when generating data with score-based models. arXiv preprint arXiv:2105.14080, 2021. [33] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 81108119, 2020. [34] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, 35: 2656526577, 2022. 11 [35] Tero Karras, Miika Aittala, Tuomas Kynkäänniemi, Jaakko Lehtinen, Timo Aila, and Samuli Laine. Guiding diffusion model with bad version of itself. arXiv preprint arXiv:2406.02507, 2024. [36] Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the training dynamics of diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2417424184, 2024. [37] Beomsu Kim, Jaemin Kim, Jeongsol Kim, and Jong Chul Ye. Generalized consistency trajectory models for image manipulation. arXiv preprint arXiv:2403.12510, 2024. [38] Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ode trajectory of diffusion. ArXiv, abs/2310.02279, 2023. [39] Diederik Kingma and Ruiqi Gao. Understanding diffusion objectives as the ELBO with simple data augmentation. In Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS), 2023. [40] Tuomas Kynkäänniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, and Jaakko Lehtinen. Applying guidance in limited interval improves sample and distribution quality in diffusion models. arXiv preprint arXiv:2404.07724, 2024. [41] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2023. [42] Sangyun Lee, Yilun Xu, Tomas Geffner, Giulia Fanti, Karsten Kreis, Arash Vahdat, and Weili Nie. Truncated consistency models. arXiv preprint arXiv:2410.14895, 2024. [43] Liangchen Li and Jiajun He. Bidirectional consistency models. arXiv preprint arXiv:2403.18035, 2024. [44] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024. [45] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [46] Hongjian Liu, Qingsong Xie, Zhijie Deng, Chen Chen, Shixiang Tang, Fueyang Fu, Zheng jun Zha, and Haonan Lu. Scott: Accelerating diffusion models with stochastic consistency distillation. arXiv preprint arXiv:2403.01505, 2024. [47] Qihao Liu, Zhanpeng Zeng, Ju He, Qihang Yu, Xiaohui Shen, and Liang-Chieh Chen. Alleviating distortion in image generation via multi-resolution diffusion models. arXiv preprint arXiv:2406.09416, 2024. [48] Xingchao Liu, Chengyue Gong, and qiang liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations (ICLR), 2023. [49] Cheng Lu and Yang Song. Simplifying, stabilizing and scaling continuous-time consistency models. arXiv preprint arXiv:2410.11081, 2024. [50] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:57755787, 2022. [51] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpmsolver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022. [52] Eric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved sampling speed. arXiv preprint arXiv:2101.02388, 2021. [53] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference. ArXiv, abs/2310.04378, 2023. 12 [54] Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick von Platen, Apolinario Passos, Longbo Huang, Jian Li, and Hang Zhao. Lcm-lora: universal stable-diffusion acceleration module. ArXiv, abs/2311.05556, 2023. [55] Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diffinstruct: universal approach for transferring knowledge from pre-trained diffusion models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [56] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740, 2024. [57] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1429714306, 2023. [58] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. [59] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [60] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. HighResolution Image Synthesis with Latent Diffusion Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [61] Amirmojtaba Sabour, Sanja Fidler, and Karsten Kreis. Align your steps: Optimizing sampling schedules in diffusion models. arXiv preprint arXiv:2404.14507, 2024. [62] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, Seyedeh Sara Mahdavi, Raphael Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. ArXiv, abs/2205.11487, 2022. [63] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022. [64] Tim Salimans, Thomas Mensink, Jonathan Heek, and Emiel Hoogeboom. Multistep distillation of diffusion models via moment matching. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [65] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse datasets. In ACM SIGGRAPH 2022 conference proceedings, pages 110, 2022. [66] Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast high-resolution image synthesis with latent adversarial diffusion distillation. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. [67] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. In European Conference on Computer Vision, pages 87103. Springer, 2025. [68] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. [69] Yang Song and Prafulla Dhariwal. Improved techniques for training consistency models. arXiv preprint arXiv:2310.14189, 2023. [70] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-Based Generative Modeling through Stochastic Differential Equations. In International Conference on Learning Representations (ICLR), 2021. [71] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint arXiv:2303.01469, 2023. 13 [72] Yanke Song, Jonathan Lorraine, Weili Nie, Karsten Kreis, and James Lucas. Multi-student diffusion distillation for better one-step generators. arXiv preprint arXiv:2410.23274, 2024. [73] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. Advances in neural information processing systems, 33:75377547, 2020. [74] Joshua Tian Jin Tee, Kang Zhang, Hee Suk Yoon, Dhananjaya Nagaraja Gowda, Chanwoo Kim, and Chang D. Yoo. Physics informed distillation for diffusion models. arXiv preprint arXiv:2411.08378, 2024. [75] Yao Teng, Yue Wu, Han Shi, Xuefei Ning, Guohao Dai, Yu Wang, Zhenguo Li, and Xihui Liu. Dim: Diffusion mamba for efficient high-resolution image synthesis. arXiv preprint arXiv:2405.14224, 2024. [76] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. arXiv preprint arXiv:2404.02905, 2024. [77] Yuchuan Tian, Zhijun Tu, Hanting Chen, Jie Hu, Chao Xu, and Yunhe Wang. U-dits: Downsample tokens in u-shaped diffusion transformers. arXiv preprint arXiv:2405.02730, 2024. [78] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. [79] Fu-Yun Wang, Zhengyang Geng, and Hongsheng Li. Stable consistency tuning: Understanding and improving consistency models. arXiv preprint arXiv:2410.18958, 2024. [80] Fu-Yun Wang, Zhaoyang Huang, Alexander William Bergman, Dazhong Shen, Peng Gao, Michael Lingelbach, Keqiang Sun, Weikang Bian, Guanglu Song, Yu Liu, Xiaogang Wang, and Hongsheng Li. Phased consistency models. arXiv preprint arXiv:2405.18407, 2024. [81] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in Neural Information Processing Systems, 36:84068441, 2023. [82] Sirui Xie, Zhisheng Xiao, Diederik Kingma, Tingbo Hou, Ying Nian Wu, Kevin Murphy, Tim Salimans, Ben Poole, and Ruiqi Gao. Em distillation for one-step diffusion models. Advances in Neural Information Processing Systems, 37:4507345104, 2024. [83] Yilun Xu, Gabriele Corso, Tommi Jaakkola, Arash Vahdat, and Karsten Kreis. Disco-diff: Enhancing continuous diffusion models with discrete latents. In International Conference on Machine Learning, 2024. [84] Jing Nathan Yan, Jiatao Gu, and Alexander Rush. Diffusion models without attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82398249, 2024. [85] Ling Yang, Zixiang Zhang, Zhilong Zhang, Xingchao Liu, Minkai Xu, Wentao Zhang, Chenlin Meng, Stefano Ermon, and Bin Cui. Consistency flow matching: Defining straight flows with velocity consistency. arXiv preprint arXiv:2407.02398, 2024. [86] Tianwei Yin, Michaël Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and William Freeman. Improved distribution matching distillation for fast image synthesis. arXiv preprint arXiv:2405.14867, 2024. [87] Tianwei Yin, Michaël Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 66136623, 2024. [88] Lijun Yu, José Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. [89] Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. arXiv preprint arXiv:2204.13902, 2022. [90] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. [91] Bowen Zheng and Tianming Yang. Diffusion models are innate one-step generators. arXiv preprint arXiv:2405.20750, 2024. [92] Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, and Anima Anandkumar. Fast sampling of diffusion models via operator learning. In International conference on machine learning, pages 4239042402. PMLR, 2023. [93] Jianbin Zheng, Minghui Hu, Zhongyi Fan, Chaoyue Wang, Changxing Ding, Dacheng Tao, and Tat-Jen Cham. Trajectory consistency distillation, 2024. [94] Linqi Zhou, Stefano Ermon, and Jiaming Song. Inductive moment matching. arXiv preprint arXiv:2503.07565, 2025. [95] Mingyuan Zhou, Zhendong Wang, Huangjie Zheng, and Hai Huang. Long and short guidance in score identity distillation for one-step text-to-image generation. ArXiv 2406.01561, 2024. [96] Mingyuan Zhou, Huangjie Zheng, Yi Gu, Zhendong Wang, and Hai Huang. Adversarial score identity distillation: Rapidly surpassing the teacher in one step. arXiv preprint arXiv:2410.14919, 2024. [97] Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In International Conference on Machine Learning, 2024."
        },
        {
            "title": "Appendix",
            "content": "A Broader Impact Limitations Proofs, Derivations and Theoretical Details C.1 Theorem: Consistency Models are Flawed Multi-step Generators . C.2 Theorems: Flow Map Objectives . . . . . . . . . . C.3 Derivation: Tangent Warmup as Linearity Regularization C.2.1 AYF-EMD . C.2.2 AYF-LMD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Connections to Existing Methods . . . . . . . . . . D.1 Flow Matching . . D.2 Continuous-Time Consistency Models . D.3 Consistency Trajectory Models . . . D.4 Flow Map Matching . . . . . D.4.1 EMD . . . . . D.4.2 LMD . . . . D.5 MeanFlow Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Flow Maps in Prior and Concurrent Works . . . E.1 Discrete-time Flow Maps . . E.2 Continuous-time Flow Maps . . E.3 Concurrent Works . . . . . . . . . . . . . . . . . Experiment Details F.1 ImageNet Experiments . F.2 Adversarial Finetuning . . F.3 Training Algorithms . F.4 Text-to-Image Experiments . . F.5 User Study Details . . . . . . . . . . . . Ablation Studies Additional Samples H.1 Text-to-Image . H.2 ImageNet-512 . . H.3 ImageNet-64 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Text Prompts for Generated Images Licenses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 17 18 18 21 21 22 23 23 24 24 24 24 25 26 26 26 27 27 28 28 29 29 30 31 31 33 33 34"
        },
        {
            "title": "A Broader Impact",
            "content": "Generative models have recently seen major advances and one can now routinely synthesize realistic and highly aesthetic images, videos and other modalities. Align Your Flow represents universal approach to significantly accelerate model sampling from diffusion and flow-based generative models through distillation. Real-time generation can unlock new interactive applications and help artistic and creative expression enabled by generative modeling tools. Moreover, accelerated generation also makes inference workloads more computationally efficient, thereby reducing generative models energy footprint. Although we evaluated Align Your Flow on image generation benchmark, our proposed methodology is in principle broadly applicable. For instance, one could also imagine future applications in drug discovery, where fast generative models can propose novel molecules rapidly and enable efficient in-silico drug candidate screening. However, generative models like diffusion and flow models, and their distilled versions, can also be used for malicious purposes and, for instance, produce deceptive imagery. Hence, they generally need to be applied with an abundance of caution."
        },
        {
            "title": "B Limitations",
            "content": "As shown in our ablation studies (App. G; also see Fig. 8), our AYF models stabilize performance across multi-step sampling. However, this comes at the cost of slightly degraded one-step performance compared to methods focused solely on one-step generation (e.g. sCD [49] or SiDA [96]). Note that adversarial finetuning can mitigate this and improve performance across the board, as shown Fig. 8 and with minimal loss in diversity. We believe that it would be possible to further improve upon that with more carefully tuned post-training stage, possibly leveraging recent variational score distillation techniques [87, 81]. Additionally, small gap remains between the AYF model and its multi-step teacher flow-based model, regardless of the number of sampling steps used. This is expected, given that both models have roughly the same number of parameters, yet AYF is trained on more challenging task and preserves the same noise-to-data mapping. Scaling up model capacity may help bridge this performance gap. Finally, our work focuses on distillation and assumes access to pretrained flow-based teacher. Prior works [49] suggest that direct consistency training can outperform distillation in certain scenarios. Since our AYF-EMD loss is directly compatible with this paradigm, exploring flow map training without distillation is promising direction for future work. Figure 8: FID versus number of sampling steps on ImageNet 512x512 (lower is better). Diffusion models require dozens of steps to reach good quality, and consistency models deteriorate after only few, whereas our AYF flow maps maintain low FID across the board. AYF is slightly weaker at single step generation, but brief adversarial fine-tuning stage closes this gap and improves quality for all numbers of sampling steps. 17 Proofs, Derivations and Theoretical Details C.1 Theorem: Consistency Models are Flawed Multi-step Generators Theorem C.1 (Restated from Theorem 3.1). Let pdata(x) = (0, c2I) be the data distribution. Then the optimal consistency model (xt, t) is given by (xt, t) = xt (cid:112)t2 + (1 t)2c2 . For any ϵ > 0 consider the following non-optimal consistency model fϵ(xt, t) = (c + ϵ)xt (cid:112)t2 + (1 t)2c . Then, there exists an integer for which increasing the number of sampling steps during multistep CM sampling beyond increases the Wasserstein-2 distance of the generated samples to the ground truth distribution. Proof. In this Gaussian setting, all intermediate noisy distributions p(xt), where xt = x1 + (1 t) x0, x0 pdata, x1 (0, I), and [0, 1], remain Gaussian. As result, the error of the non-optimal consistency model fϵ is: Ext[fϵ(xt, t) (xt, t)2 2] = Ext[t2ϵ2 xt (cid:112)t2 + (1 t)2c 2 2] = t2ϵ2 t2 + (1 t)2c2 Ex0,x1 [t x1 + (1 t) x02 2] = t2ϵ2 ϵ2, (6) and satisfies the error bound in Theorem 3.1 by choosing small enough ϵ that satisfies ϵ2 < δ. Due to the Gaussian setting, both the optimal velocity from flow matching, v(xt, t), and the optimal denoiser, D(xt, t), have closed-form solutions. We first derive the denoiser. Following the framework of [34], let p(x; σ) denote the distribution obtained by adding independent Gaussian noise with standard deviation σ to the data. Then, p(x; σ) = (0, (c2 + σ2)I) log p(x; σ) = c2 + σ2 D(x, σ) = c2 c2 + σ2 x, where the final step follows from the identity log p(x; σ) = (D(x; σ) x)/σ from [34]. Using this, the optimal velocity from flow matching, v(xt, t), can be computed as v(xt, t) = E[x1 x0 xt] = (cid:20) xt x0 (cid:21) xt = xt 1 E[x0 xt]. Substituting (cid:16) xt 1t , 1t (cid:17) for E[x0 xt], we obtain v(xt, t) = xt 1 D (cid:18) xt 1 , 1 (cid:19) . Using the closed-form expression for D(x, σ), v(xt, t) = xt 1 (cid:17)2 xt 1 . c2 (cid:16) 1t c2 + Simplifying further, v(xt, t) = xt (cid:18) 1 c2(1 t) c2(1 t)2 + t2 (cid:19) = (cid:18) c2(1 t) t2 + (1 t)2c2 (cid:19) xt. 18 Integrating this velocity along the PF-ODE, dxt = v(xt, t)dt, from to 0 yields the optimal consistency model: (xt, t) = xt (cid:112)t2 + (1 t)2c2 . Consider the non-optimal consistency model fϵ(xt, t) = (c + ϵ)xt (cid:112)t2 + (1 t)2c2 . (7) Note that fϵ(x, 0) = satisfies the boundary condition and is valid consistency model. Let us analyze single multistep transition from timestep to timestep s. This process consists of two steps: 1. The noise is removed by predicting the clean data using the consistency model, yielding 0 = fϵ(xt, t). 2. The estimated clean data x xs = + (1 s) 0, 0 is then noised back to timestep using (0, I). Assuming that xt (0, σ2 is obtained as follows: I), xs will also follow an isotropic zero-mean Gaussian distribution and xs = + (1 s) Therefore, the variance of xs is given by (c + tϵ)xt (cid:112)t2 + (1 t)2c2 . Var(xs) = s2 + (1 s)2(c + tϵ)2 t2 + (1 t)2c2 Var(xt). (8) Using this recurrence relation, we can compute the variance of the distribution obtained by running multistep CM sampling on uniform n-step schedule: [0, 1 , . . . , 1 , 1]. Since both the ground truth distribution and the distribution of x0 derived by n-step sampling are isotropic zero-mean Gaussians, the Wasserstein-2 distance between them has the following closed form solution: W2(p(x0); pdata(x)) = ((cid:112)Var(x0) c)2 Let Var(s) := Var(xs) for convenience. We will show that as , the variance Var(0) when computed via the recurrence defined in Eq. (8) on the uniform step schedule diverges, i.e. Var(0) . This means performing multi-step sampling with the consistency model will result in accumulated errors beyond certain point. Define Plugging this into Eq. (8) gives: h(s) := Var(s) s2 + (1 s)2c2 . h(s) = s2 + (1 s)2(c + tϵ)2h(t) s2 + (1 s)2c . (9) We know h(1) = 1 and h(0) = Var(0)/c2. So, its enough to show that h(0) as . Its easy to see by induction that h(t) 1. Define g(t) := h(t) 1 to measure how much it grows. Then: g(s) = (1 s)2(c + tϵ)2(g(t) + 1) (1 s)2c2 s2 + (1 s)2c (cid:0)2ctϵ + t2ϵ2 + (c + tϵ)2g(t)(cid:1) (10) (1 s)2 s2 + (1 s)2c2 (1 s)2 = s2 + (1 s)2c2 (2ctϵ + c2g(t)). 19 All terms are positive, so lets lower bound g(t) by defining simpler sequence: g(s) := (1 s)2 s2 + (1 s)2c2 (2ctϵ + c2g(t)). Clearly g(s) g(s), so its enough to show g(0) as . Now define: Then the recurrence becomes: r(s) := g(s) 2cϵ . r(s) = (1 s)2 s2 + (1 s)2c2 (t + c2r(t)), with r(1) = 0 and r(0) = g(0)/(2cϵ). So we just need to show r(0) as . To analyze this, fix and t, and consider the function: (1 s)2 fs,t(x) := s2 + (1 s)2c2 (t + c2x). This is an affine map with unique fixed point: Subtracting o(s, t) from both sides, we get: o(s, t) = t(1 s)2 s2 . where: (x) o(s, t) = λ(s)(x o(s, t)). λ(s) := 1 (1s)c )2 + 1 ( < 1. (11) (12) (13) (14) (15) So fs,t(x) pulls every point toward its fixed point o(s, t), with pull factor of λ(s). As 0, since o(s, t) (1s)2 , the fixed point o(s, t) , and the pull factor λ(s) approaches 1, meaning the pull gets weaker. This means the recurrence in Eq. (12) is applying sequence of weaker and weaker pulls toward bigger and bigger targets. To prove r(0) , we now proceed by contradiction. Assume there exists some δ > 0 and an infinite sequence n1 < n2 < . . . such that r(0) < δ when using the schedule [0, 1 ni , . . . , 1]. Since o(s, t) (1s)2 all [0, t]: and the function (1x) as 0, we can pick [0, 1] such that for (1 t)2 = 2δ o(s, t) (1 s)2 2δ. (16) So every fixed point in [0, t] is at least 2δ. Now we split the problem into two cases: Case 1: There exists some [0, t] where r(s) δ. Since all future pulls are toward values > 2δ, r() will stay above δ, so r(0) δa contradiction. Case 2: For all [0, t], we have r(s) < δ. Pick any (0, t). Let ℓ := λ(s) < 1, which is the maximum pull factor (corresponding to the weakest pull) on [s, t]. Then for < t: 2δ r(s) = (o(s, t) r(s)) + (2δ o(s, t)) = (o(s, t) fs,t(r(t))) + (2δ o(s, t)) = λ(s)(o(s, t) r(t)) + (2δ o(s, t)) λ(s)(o(s, t) r(t)) + (2δ o(s, t)) + (2δ o(s, t)) = ℓ(o(s, t) r(t)) = ℓ(2δ r(t)) + (1 ℓ)(2δ o(s, t)) ℓ(2δ r(t)). 20 Applying this inequality times (where is the number of steps between and on the schedule), we get: δ 2δ r(s) 2δℓM . (17) If is large enough so that 2δℓM < δ, we get contradiction. This happens when > M/(t s). Since both cases lead to contradiction, we conclude that r(0) as , completing the proof. Intuition: When doing multi-step sampling with consistency models we first perform denoising by removing all noise from noisy image to obtain clean one, and then re-add smaller amount of noise. But because the model is not perfect, the denoised image drifts slightly off the true data manifold. When noise is added back in, the resulting image is now slightly off the noisy manifold the model was trained on. This mismatch compounds: each denoising step starts from slightly worse input, pushing the sample further off-manifold over time. As result, errors accumulate with more sampling steps, leading to degrading image quality beyond certain point. C.2 Theorems: Flow Map Objectives C.2.1 AYF-EMD Theorem C.2 (Restated from Theorem 3.2). Let fθ(xt, t, s) be the flow map and vϕ(xt, t) denote the pretrained flow matching model. Define θ = stopgrad(θ). Let and = + ϵ(s t) denote two adjacent starting timesteps for small ϵ > 0. Note that [t, s] is derived by taking small step from towards s. Consider the following consistency loss function defined (cid:2)w(t, s)fθ(xt, t, s) fθ (xt, t, s) EM D(θ) = Ext,t,s Lϵ (18) (cid:3) , 2 where xt is obtained by using 1-step euler solver on the PF-ODE, i.e. dxt = vϕ(xt, t)dt, from to t. In the limit as ϵ 0, the gradient of this objective with respect to θ converges to: dfθ(xt, t, s) dt (cid:20) w(t, s)(t s)f EM D(θ) = θExt,t,s (1/ϵ)θLϵ θ (xt, t, s) lim ϵ0 (19) (cid:21) . Proof. For this proof, we follow similar logic as the proof of Theorem 5 in [71]. We start by computing the gradient of Eq. (18) with respect to θ and simplifying the result to obtain 1 ϵ (cid:2)w(t, s)fθ(xt, t, s) fθ (xt, t, s)2 EM D(θ) = θExt,t,s θLϵ 1 ϵ (cid:3) 2 = = = 1 ϵ 1 ϵ 1 ϵ Ext,t,s Ext,t,s (cid:2)w(t, s)θfθ(xt, t, s) (fθ(xt, t, s) fθ(xt, t, s))(cid:3) (cid:18) (cid:20) w(t, s)θfθ(xt, t, s) fθ(xt, t, s) fθ(xt, t, s) + (cid:18) fθ (xt xt)+ (t t) + O(ϵ2) (cid:19)(cid:19)(cid:21) fθ (ϵ(s t) vϕ(xt, t)) + (cid:19)(cid:19)(cid:21) (ϵ(s t)) + O(ϵ) fθ (cid:20) w(t, s)θfθ(xt, t, s) Ext,t,s (cid:20) w(t, s)(t s)θfθ(xt, t, s) (cid:18) = Ext,t,s (cid:18) fθ (cid:18) fθ = Ext,t,s (cid:20) w(t, s)(t s)θfθ(xt, t, s) (cid:18) dfθ(xt, t, s) dt (cid:19)(cid:21) Taking the limit of both sides as ϵ 0 completes the proof. vϕ(xt, t) + + O(ϵ) (cid:19)(cid:21) fθ + O(ϵ), Corollary C.3. Theorem 3.2 assumes that the step size is proportional to the interval length, i.e. t s, leading to the introduction of (t s) term in the weighting function. This can be eliminated by using = + sign(s t) ϵ and leads to the following objective: θExt,t,s (cid:20) w(t, s)sign(t s)f θ (xt, t, s) dfθ (xt, t, s) dt (cid:21) . (20) 21 C.2.2 AYF-LMD Theorem C.4 (Restated from Theorem 3.3). Let fθ(xt, t, s) be the flow map, vϕ(xt, t) be the pretrained flow matching model, and define θ = stopgrad(θ). Let and = + ϵ(t s) denote two adjacent ending timesteps for small ϵ > 0. Note that [t, s] is obtained by taking small step from towards t. Consider the following consistency loss function defined LM D(θ) = Ext,t,s Lϵ (cid:2)w(t, s)fθ(xt, t, s) ODEss(fθ (xt, t, s))2 (cid:3) , (21) where ODEss(x) refers to running 1-step euler solver on the PF-ODE, i.e. dxt = vϕ(xt, t)dt, starting from at timestep to timestep s. In the limit as ϵ 0, the gradient of this objective with respect to θ converges to: (1/ϵ)θLϵ LM D(θ) = θExt,t,s lim ϵ (cid:20) w(t, s)(s t)f θ (xt, t, s) (cid:18) dfθ (xt, t, s) ds vϕ(fθ (xt, t, s), s) . (cid:19)(cid:21) (22) Proof. We start by computing the gradient of Eq. (21) with respect to θ and simplifying the results to obtain 1 ϵ (cid:2)w(t, s)fθ(xt, t, s) ODEss(fθ (xt, t, s))2 LM D(θ) = θExt,t,s θLϵ 1 ϵ (cid:3) 2 = = = = 1 ϵ 1 ϵ 1 ϵ 1 ϵ Ext,t,s (cid:2)w(t, s)θfθ(xt, t, s) (fθ(xt, t, s) ODEss(fθ(xt, t, s)))(cid:3) Ext,t,s Ext,t,s (cid:2)w(t, s)θfθ(xt, t, s) (fθ(xt, t, s) (fθ (xt, t, s) + (s s) vϕ(fθ (xt, t, s), s)))(cid:3) (cid:20) w(t, s)θfθ(xt, t, s) (s s) + O(ϵ2) fθ(xt, t, s) fθ (xt, t, s) + (cid:18) (cid:18) fθ (xt, t, s) +(s s) (vϕ(fθ (xt, t, s), s) + O(ϵ)))] (cid:20) w(t, s)θfθ(xt, t, s) Ext,t,s (cid:20) w(t, s)(s t)θfθ(xt, t, s) (cid:18) = Ext,t,s (cid:18) fθ (xt, t, s) (cid:18) fθ (xt, t, s) (s s) + (s s)vϕ(fθ(xt, t, s), s) + O(ϵ) (cid:19)(cid:19)(cid:21) vϕ(fθ (xt, t, s), s) + O(ϵ) (cid:19)(cid:21) Taking the limit of both sides as ϵ 0 completes the proof. Corollary C.5. Theorem 3.3 assumes that the step size is proportional to the interval length, i.e. s s, leading to the introduction of (t s) term in the weighting function. This can be eliminated by using = + sign(t s) ϵ which leads to the following objective: θExt,t,s (cid:20) w(t, s)sign(s t)f θ (xt, t, s) (cid:18) dfθ (xt, t, s) ds vϕ(fθ(xt, t, s), s) . (23) (cid:19)(cid:21) C.3 Derivation: Tangent Warmup as Linearity Regularization Here, we derive Eq. (5). This equation shows the equivalence between the tangent warmup technique and regularization term on flow maps that encourages linearity. (cid:20) sign(t s)f θ (xt, t, s) θ (cid:18) dxt dt (cid:19)(cid:21) Fθ (xt, t, s) = θ [sign(t s)(xt + (s t)Fθ(xt, t, s)) (vϕ(xt, t) Fθ (xt, t, s))] = θ [t Fθ(xt, t, s) (vϕ(xt, t) Fθ (xt, t, s))] (i) θ [Fθ(xt, t, s) (Fθ (xt, t, s) vϕ(xt, t))] = (θFθ(xt, t, s)) (Fθ (xt, t, s) vϕ(xt, t)) (ii) = (θFθ(xt, t, s)) (Fθ(xt, t, s) vϕ(xt, t)) (iii) = (θ[Fθ(xt, t, s) vϕ(xt, t)]) (Fθ(xt, t, s) vϕ(xt, t)) = 0.5 θFθ(xt, t, s) vϕ(xt, t, s)2 2, (24) where (i) is because we discard s 0, which doesnt change the gradient direction, (ii) is because Fθ = Fθ , and (iii) is because θvϕ(xt, t) = 0."
        },
        {
            "title": "D Connections to Existing Methods",
            "content": "In this section, we highlight the connections between AYF and existing methods. We show how AYF generalizes several prior approaches and discuss its relationship to recent concurrent works. See Fig. 9 for schematic overview of these connections. In the following subsections, we will derive these relationships in detail. Figure 9: AYF can be seen generalization of many prior works such as Flow Matching [45], Continuous-time Consistency Models [49], Flow Map Matching [5], Consistency Trajectory Distillation [93], and the concurrent MeanFlow Models [19]. D.1 Flow Matching We show that the AYF-EMD objective introduced in Theorem 3.2 generalizes the standard flow matching objective. In particular, we prove that the gradient of the AYF-EMD objective reduces to the flow matching gradient in the limit as t, up to constant factor. Recall the flow map parameterization: fθ(xt, t, s) = xt + (s t) Fθ(xt, t, s). (25) Substituting into the AYF-EMD objective gives: θExt,t,s (cid:20) w(t, s) sign(t s) θ (xt, t, s) (cid:21) dfθ (xt, t, s) dt = θExt,t,s (cid:20) w(t, s) sign(t s) (xt + (s t)Fθ) (cid:18) dxt dt Fθ + (s t) (cid:19)(cid:21) dFθ dt = θExt,t,s (cid:20) w(t, s) s θ (cid:18) dxt dt Fθ + (s t) (cid:19)(cid:21) dFθ dt θExt,t,s (cid:18) (cid:20) θ Fθ dxt dt + (t s) (cid:19)(cid:21) . dFθ dt (26) Taking the limit as gives: θExt,t (cid:20) θ (xt, t, t) (cid:18) Fθ (xt, t, t) (cid:19)(cid:21) dxt dt = θExt,t (cid:34)(cid:13) (cid:13) (cid:13) (cid:13) Fθ(xt, t, t) (cid:35) , dxt dt (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 (27) which is exactly the standard flow matching loss. 23 D.2 Continuous-Time Consistency Models We show that the AYF-EMD objective introduced in Theorem 3.2 also generalizes the continuoustime consistency model objective. Specifically, we prove that the AYF-EMD objective reduces to the continuous CM objective when = 0, up to constant factor. Consider the AYF-EMD objective: θExt,t,s (cid:20) w(t, s) sign(t s) θ (xt, t, s) dfθ (xt, t, s) dt (cid:21) . Setting = 0 gives: θExt,t (cid:20) w(t, 0) θ (xt, t, 0) dfθ (xt, t, 0) dt (cid:21) , (28) (29) since sign(t) = 1 for [0, 1]. Noting that fθ(xt, t, 0) corresponds to the CM prediction at noise level t, this recovers the standard continuous-time CM objective [49]. D.3 Consistency Trajectory Models In this part, we discuss the connection to Consistency Trajectory Models (CTMs) [38, 93]. Recall from the derivation of the AYF-EMD objective in App. C.2.1 that its gradient corresponds to the continuous-time limit of discrete consistency loss. Interestingly, TCD uses this exact same discrete consistency loss to train its flow map, with fixed discretization schedule. Therefore, the TCD objective can be seen as discrete approximation of the AYF-EMD loss. D.4 Flow Map Matching In this part, we highlight the similarities and differences between our AYF objectives and the losses proposed by Boffi et al. [5]. D.4.1 EMD Here, we will show the connection between our AYF-EMD objective and the EMD loss proposed by Boffi et al. [5]. Recall the EMD loss from their work: (cid:34) θExt,t,s w(t, s) (cid:13) (cid:13) (cid:13) (cid:13) tfθ(xt, t, s) + xfθ(xt, t, s) dxt dt (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 (cid:35) (cid:34) = θExt,t,s w(t, s) (cid:35) . (cid:13) (cid:13) (cid:13) (cid:13) dfθ(xt, t, s) dt (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 (30) This loss can be derived in similar fashion as our AYF-EMD loss by introducing modification to Eq. (18). Specifically, by replacing the second term fθ (xt, t, s) with fθ(xt, t, s) and allowing gradients to flow through both terms, we recover the EMD loss from Boffi et al. [5]. Concretely, we can show that in the limit as ϵ 0, the gradient of this objective with respect to θ converges to: lim ϵ0 1 ϵ2 θExt,t,s (cid:2)w(t, s)fθ(xt, t, s) fθ(xt, t, s)2 2 (cid:3) = θExt,t,s where w(t, s) = w(t, s) s2. The proof is as follows: (cid:34) w(t, s) (cid:13) (cid:13) (cid:13) (cid:13) dfθ(xt, t, s) dt (cid:35) , (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 (31) 1 ϵ2 θExt,t,s 1 ϵ2 θExt,t,s (cid:34) = = θExt,t,s w(t, s) (cid:2)w(t, s)fθ(xt, t, s) fθ(xt, t, s)2 (cid:34) 2 (cid:3) w(t, s) ϵ(t s) (cid:13) (cid:13) (cid:13) (cid:13) dfθ(xt, t, s) dt (cid:35) (cid:13) 2 (cid:13) + O(ϵ2) (cid:13) (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) (t s) dfθ(xt, t, s) dt (cid:35) + O(ϵ) (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 = θExt,t,s (cid:34) w(t, s) (cid:13) (cid:13) (cid:13) (cid:13) dfθ(xt, t, s) dt (cid:35) (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 + O(ϵ), 24 which converges to the gradient of the EMD loss as ϵ 0. Note that when deriving AYF-EMD, taking the limit of the discrete consistency loss when ϵ 0 only required dividing by ϵ, since the relevant term decays linearly in ϵ. In contrast, the gradient in this case decays quadratically with respect to ϵ, which is why we divide by ϵ2 before taking the limit. The small difference in the AYF-EMD derivation, namely applying stop-gradient operation on the second term in Eq. (18), has significant effect on training dynamics. Without it, one must backpropagate through the Jacobian-vector product (JVP) used to compute dfθ(xt,t,s) . This often introduces instability and slows down training. However, with the stop-gradient operation, one must only compute the JVP without needing to backpropagate through it. Fortunately, modern autograd libraries like PyTorch support forward-mode automatic differentiation, which allows computing the JVP efficiently with minimal overhead. dt Intuitively, applying the stop-gradient means the output of large step with the flow map is pushed toward the output of following the PF-ODE trajectory for bit and then doing smaller step with the flow map. Without the stop-gradient, the small step is also encouraged to match the outcome of the large step, which is counterintuitive. This is because learning flow map becomes more difficult as the interval length increases. Smaller steps are typically more reliable and offer better approximations. Ultimately, the decision to include or omit the stop-gradient operation in Eq. (18) leads to two fundamentally different derivations and objectives. In our experiments, only the AYF-EMD variant, where the stop-gradient is applied, was able to scale effectively to large-scale image datasets and produce high-quality outputs. D.4.2 LMD An analogous connection can also be made between our AYF-LMD objective and the LMD loss from Boffi et al. [5]. Recall the LMD loss from their work: θExt,t,s (cid:104) w(t, s) sfθ(xt, t, s) vϕ(fθ(xt, t, s), s)2 2 (cid:105) . (32) Similar to before, we will show that by removing the stop-gradient operation from Eq. (21) and allowing gradients to flow through both terms, we recover the LMD loss from Boffi et al. [5]. Concretely, we can show that as ϵ 0, the gradient of this objective with respect to θ converges to: lim ϵ0 Ext,t,s 1 ϵ2 = θExt,t,s (cid:2)w(t, s)fθ(xt, t, s) ODEss(fθ(xt, t, s))2 2 (cid:105) w(t, s) sfθ(xt, t, s) vϕ(fθ(xt, t, s), s)2 2 (cid:104) (cid:3) , (33) where w(t, s) = w(t, s) s2. The proof is as follows: ϵ0 1 lim ϵ2 ϵ0 1 ϵ2 lim 1 ϵ2 lim 1 ϵ2 lim 1 ϵ2 lim ϵ0 ϵ0 = = = = ϵ0 Ext,t,s = lim ϵ0 = Ext,t,s Ext,t,s (cid:2)w(t, s)fθ(xt, t, s) ODEss(fθ(xt, t, s))2 2 (cid:3) Ext,t,s (cid:2)w(t, s)fθ(xt, t, s) (fθ(xt, t, s) + (s s) vϕ(fθ(xt, t, s), s))2 (cid:3) Ext,t,s (cid:2)w(t, s)(fθ(xt, t, s) fθ(xt, t, s)) (ϵ(s t) vϕ(fθ(xt, t, s), s))2 2 (cid:3) Ext,t,s (cid:2)w(t, s)((s s)sfθ(xt, t, s) + O(ϵ2)) (ϵ(t s) vϕ(fθ(xt, t, s), s) + O(ϵ2))2 2 Ext,t,s (cid:2)w(t, s)(ϵ(t s)sfθ(xt, t, s) + O(ϵ2)) (ϵ(t s) vϕ(fθ(xt, t, s), s) + O(ϵ2))2 2 (cid:3) (cid:3) (cid:2)w(t, s)(sfθ(xt, t, s) + O(ϵ)) (vϕ(fθ(xt, t, s), s) + O(ϵ))2 2 (cid:3) (cid:2)w(t, s)sfθ(xt, t, s) vϕ(fθ(xt, t, s), s)2 (cid:3) . As before, applying the stop-gradient operation allows us to avoid backpropagating through the JVP, which speeds up training. Unlike the EMD case though, we found that the LMD loss from Boffi et al. 25 [5] was already stable in practice and did not introduce training instabilities. Since both versions performed similarly in our experiments, we recommend using AYF-LMD for its improved training efficiency (note, however, that in all image generation experiments, we found AYF-EMD to perform better; see ablation study Tab. 5). Intuitively, the stop-gradient in this loss plays similar role as in the other objective: we want the output of large step of the flow map to match the result of taking smaller step with the flow map, followed by integrating the PF-ODE. Another way to understand this is through Eq. (21). This loss fixes the smaller step fθ(xt, t, s) and optimizes the larger step fθ(xt, t, s) so that it aligns with the velocity field at the smaller step. As s, we can think of fθ(xt, t, s) as fixed. The model then adjusts the slope of the flow map with respect to so that it matches the teacher flow at that point. Without the stop-gradient, the endpoint is no longer fixed and can also move to match the teacher, which changes the optimization behavior. D.5 MeanFlow Models In this section, we will show the connection to MeanFlow Models [19], which are concurrent work focused on training flow maps from scratch. We will show that the AYF-EMD objective reduces to the MeanFlow loss assuming an Euler parametrization of the flow map fθ(xt, t, s) = xt + (s t)Fθ(xt, t, s). This parametrization is inspired by the first-order DDIM [68] solver of diffusion models, which uses Euler Integration to solve the probability flow ODE. Recall the MeanFlow objective: LMeanFlow(θ) = Ext,t,s (cid:34)(cid:13) (cid:13) (cid:13) (cid:13) Fθ(xt, t, s) (cid:18) dxt dt (t s) dFθ (xt, t, s) dt (cid:35) . (cid:19)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 (34) Taking the gradient with respect to θ: (cid:34)(cid:13) (cid:13) (cid:13) (cid:13) θLMeanFlow(θ) = θExt,t,s Fθ(xt, t, s) (cid:18) dxt dt (t s) dFθ(xt, t, s) dt (cid:35) (cid:19)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 = Ext,t,s = Ext,t,s (cid:20) 2θF θ (cid:20) 2θF θ (cid:18) Fθ (cid:18) Fθ + (t s) + (t s) dxt dt dxt dt dxt dt (cid:19)(cid:21) (cid:19)(cid:21) dFθ dt dFθ dt dFθ dt (cid:19)(cid:21) , = 2θExt,t,s (cid:20) θ (cid:18) Fθ + (t s) (35) which matches the AYF-EMD objective using an Euler parametrization up to constant, as shown in Eq. (26)."
        },
        {
            "title": "E Flow Maps in Prior and Concurrent Works",
            "content": "Flow maps (or specific instances of them) have appeared in various prior works. In the previous section, we have derived relations to some prior and concurrent works already. In this section, we provide broader overview. Broadly, prior works on flow maps can be grouped into discrete-time and continuous-time methods. E.1 Discrete-time Flow Maps Flow maps were initially proposed as natural generalization of consistency models (CMs), and early work primarily focused on discrete-time formulations built on discrete consistency losses. Consistency Trajectory Models (CTM) [38] were the first to explicitly introduce and study flow maps. CTM trains discrete-time flow maps using combination of discrete consistency loss, flow matching loss, and an adversarial loss. Notably, CTM was also the first to introduce γ-sampling for stochastic sampling of flow maps. While the models produced high-quality samples, their performance depended heavily on the adversarial component, with significant FID drop when it was removed. 26 Trajectory Consistency Distillation [93] builds on CTM, extending it to text-to-image generation. They empirically demonstrate that standard CMs degrade in quality as the number of function evaluations (NFEs) increases, problem which is solved when using flow maps. These models can be viewed as discrete variant of AYF, as shown in App. D.3. Bidirectional CMs [43] observe that most prior work only trained flow maps in the denoising direction (s < t). They propose training on unordered timestep pairs, accelerating application like inversion, inpainting, interpolation, and blind restoration. Kim et al. [37] trained CTMs connecting arbitrary distributions by operating within the flow matching framework instead of diffusions. In parallel, Multistep CMs [22] attempted to address the poor multi-step behavior of standard CMs. They divide the denoising trajectory into subintervals and train separate CMs within each, achieving strong performance with as few as 28 steps. Phased CMs [80] adopt similar idea but add adversarial training within each subinterval. These models effectively learn flow maps by training on (t, s) pairs where is the start of fixed subinterval containing t. This requires the inference step count to be pre-determined during training, and cannot be changed to trade off compute and quality. E.2 Continuous-time Flow Maps More recently, several methods have tackled training flow maps in continuous time. Flow Map Matching [5] provides formal and rigorous analysis of continuous-time flow maps and proposes several continuous-time losses. However, their empirical validation is limited to small-scale experiments. In App. D.3, we discuss the connection between AYF and Flow Map Matching. Shortcut Models [17] also operate in the continuous-time flow map setting. They propose an objective combining flow matching and self-consistency loss. Their full loss (up to constants) is: (cid:34)(cid:13) (cid:13) (cid:13) (cid:13) (cid:19) (cid:18) (cid:18) + fθ xt, t, (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 dxt dt + 2 Fθ(xt, t, t) L(θ) = Ext,t,s (cid:13) (cid:13) fθ(xt, t, s) fθ (cid:13) (cid:13) (cid:19)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 (36) where the flow map is parameterized as fθ(xt, t, s) = xt + (s t)Fθ(xt, t, s). Intuitively, the self-consistency term encourages agreement between one large step and two smaller intermediate steps along the PF-ODE denoising path. To train from scratch, they use the empirical estimate (x1 x0) in place of dxt dt .Although their results on ImageNet-256 and CelebAHQ-256 are promising, they use suboptimal architectures and achieve significantly worse FID scores than state-of-the-art methods. The Shortcut loss can also be used for distillation by replacing dxt velocity model. We include this variant in our ablations (see Tab. 5 for details). dt with outputs from pretrained + 2 , , , (cid:35) Inductive Moment Matching [94] is recent method for training flow maps from scratch. It proposes using an MMD loss to align the distributions of fθ(xt, t, s) and fθ(xr, r, s) over tuples (s, r, t) satisfying < < t. E.3 Concurrent Works Two concurrent works have also investigated training continuous-time flow maps. MeanFlow Models [19] study training flow maps from scratch using loss closely related to our AYF-EMD objective. In fact, their formulation corresponds to special case of AYF-EMD under an Euler flow map parameterization: fθ(xt, t, s) = xt + (s t) Fθ(xt, t, s), as shown in App. D.5. They refer to Fθ(xt, t, s) as the Mean Flow and use their objective to train flow maps from scratch, thereby being complementary to our work, which focuses on distillation. Their method achieves strong oneand two-step results on ImageNet 256x256, showing that the AYF-EMD objective is effective not just for distillation, but also for training from scratch. How to Build Consistency Model [6] extends the authors prior work on Flow Map Matching [5], offering deeper analysis of the EMD and LMD objectives. They also investigate the role of higher-order derivatives, finding them helpful in low-dimensional settings but largely ineffective in high-dimensional ones. 27 Table 3: Optimal sampling hyperparameters. Model NFE Stochasticity γ Guidance scale λ ImageNet-64, AYS-S ImageNet-512, AYS-S 1 2 4 1 2 4 1.0 1.0 0.8 1.0 1.0 0. 2.0 2.0 2.0 2.0 2.1 2.0 In contrast to these prior and concurrent works, AYF proposes new training objectives, leverages autoguidance for distillation, and shows how brief adversarial fine-tuning can boost performance without reducing diversity. Moreover, we provide new theoretical insights and for the first time analytically prove CMs deterioration with increased numbers of sampling steps. Finally, we overall scale flow map models to text-to-image generation and state-of-the-art few-step performance on ImageNet benchmarks."
        },
        {
            "title": "F Experiment Details",
            "content": "F.1 ImageNet Experiments For our ImageNet experiments, we use publicly available checkpoints from EDM2 [36]. These models are first fine-tuned to align with the flow matching framework (see Sec. 3.4 for details) before being used as teacher models to distill flow map. We run this finetuning stage for 10, 000 steps using learning rate of 0.001. For the teacher model, we use checkpoints corresponding to the and XS models, trained on 2147 million and 134 million images, respectively. During training, we randomize the guidance scale by sampling λ uniformly from the range [1, 3]. In all experiments, we apply tangent normalization and tangent warmup, following the approach introduced in sCM [49], setting = 0.1 and = 10000. To ensure stable training, we define w(t, s) = 1 ts2 , which removes the (s t)2 termone arising from the proportional assumption (see App. C.2.1 for details) and another from the (s t) coefficient of Fθ(xt, t, s) in our flow map parameterization. We use learning rate of 104 and batch size of 2048 for all experiments for total of 50, 000 iterations. These experiments were performed using 32 NVIDIA A100 gpus and took approximately 24-48 hours to converge. detailed algorithm can be seen in Algorithm 1. Timestep scheduling We sample the interval distance s from normal distribution (Pmean, 2 std), followed by sigmoid transformation. This prioritizes medium-length intervals and improves overall training stability. Once the interval length is determined, random interval of that length is uniformly selected, with > set as the two endpoints. Since we are only concerned with generation, we do not train on (t < s) pairs. We find (Pmean, Pstd) = (0.8, 1.0) works well for ImageNet-512, while (Pmean, Pstd) = (0.6, 1.6) works well for ImageNet-64. Sampling hyperparameters At inference time, we sweep the guidance scale λ in the range [1, 3] and the sampling stochasticity γ in [0, 1] to determine optimal hyperparameters. Tab. 3 summarizes the selected values. For n-step sampling, intermediate timesteps ti are uniformly distributed over the interval [0, 1]. Additional ImageNet512 baselines: For completeness, we include the additional baselines from Table 2 of sCM [49] in Tab. 4. Our AYF models are able to outperform all methods using only 4 sampling steps. 28 Table 4: Sample quality on class-conditional ImageNet 512512. This is an extension of Tab. 2 with further baseline methods. Method NFE () FID () #Params Diffusion Models ADM-G [11] RIN [29] U-ViT-H/4 [3] DiT-XL/2 [58] SimDiff [26] VDM++ [39] DiffIT [21] DiMR-XL/3R [47] DiFFUSSM-XL [84] DiM-H [75] U-DiT [77] SiT-XL [56] MaskDiT [92] Dis-H/2 [15] DRWvK-H/2 [16] EDM2-S [36] EDM2-M [36] EDM2-L [36] EDM2-XL [36] EDM2-XXL [36] GANs & Masked Models BigGAN [7] StyleGAN-XL [65] VQGAN [13] MaskGIT [9] MAGVIT-V2 [88] MAR [44] VAR-d36-s [76]) AYF-S (ours) AYF-S + adv. loss (ours) 250 250 250 250 5122 5122 2502 2502 2502 2502 2502 2502 792 2502 2502 632 632 632 632 632 1 12 1024 642 642 642 102 1 2 4 1 2 7.72 4.05 4.05 3.04 3.02 3.15 2.67 2.50 3.41 3.78 3.50 2.62 2.24 2.88 2.95 2.23 2.00 1.87 1.80 1.73 8.31 3.92 12.57 9.24 9.11 1.95 2.63 3.32 1.87 1.70 1.92 1.81 1.64 559M 501M 501M 675M 2B 2B 561M 725M 673M 860M 604M 675M 736M 900M 879M 280M 498M 778M 1.1B 1.5B 160M 266M 232M 284M 1B 2.3B 2.3B 280M 280M 280M 280M 280M 280M F.2 Adversarial Finetuning For adversarial finetuning, we use the StyleGAN2 discriminator [33] and follow the relativistic pairing GAN (RpGAN) formulation [28, 31]. The complete algorithm is provided in Algorithm 2. The flow map is optimized by minimizing combination of the AYF-EMD loss and weighted RpGAN objective. To compute the adversarial loss, we perform one-step sampling with the flow map (i.e. setting (t, s) = (1, 0)) to generate negative samples. Following prior work [13], we apply an adaptive weighting scheme to balance the AYF-EMD and adversarial terms. Additionally, we multiply the adversarial loss by fixed coefficient α = 0.1 to ensure stable training. For the discriminator, we apply R1 and R2 regularization [28] with regularization weight of β = 0.1. We use learning rate of 2 105 for both networks and batch size of 1024. Finetuning is run for approximately 3000 iterations using 32 NVIDIA A100 GPUs, taking around 4 hours in total. F.3 Training Algorithms We provide the full AYF algorithm in Algorithm 1, and the variant with adversarial finetuning in Algorithm 2. 29 Algorithm 1 Flow Map Distillation with AYF-EMD Loss. 1: Input: dataset with std. σd, autoguided pretrained flow model vguided (xt, t, λ) with guidance weight λ, model Fθ(xt, t, s, λ), learning rate η, distance schedule (Pmean, Pstd), guidance interval [λmin, λmax], constant c, warmup iteration H. ϕ 2: Init: Iters 0 3: repeat 4: 5: 6: 7: ϕ std), λ Unif(λmin, λmax) (xt, t, λ) x0 D, x1 (0, σ2 dI), τ (Pmean, 2 σ(τ ), Unif(0, 1 d), + d, xt (1 t)x0 + tx1 dt vguided dxt min(0.99, Iters/H) (cid:0)Fθ (xt, t, s, λ) dxt g/(g + c) L(θ) Fθ (xt, t, s, λ) Fθ (xt, t, s, λ) + g2 2 θ θ ηθL(θ) Iters Iters + 1 (cid:1) + r(t s) dFθ (xt,t,s,λ) dt dt 8: 9: 10: 11: 12: 13: until convergence Tangent warmup Tangent normalization ϕ (xt, t, λ) if Generator step then std), λ Unif(λmin, λmax) Algorithm 2 Adversarial Flow Map Finetuning with AYF-EMD and Adversarial losses. 1: Input: dataset with std. σd, autoguided pretrained flow model vguided (xt, t, λ) with guidance weight λ, model Fθ(xt, t, s, λ), learning rates ηG, ηD, distance schedule (Pmean, Pstd), guidance interval [λmin, λmax], constant c, warmup iteration H, discriminator Dψ(x), adversarial weights α, β. 2: repeat 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: until convergence x0 D, x1 (0, σ2 dI), τ (Pmean, 2 σ(τ ), Unif(0, 1 d), + d, xt (1 t)x0 + tx1 dt vguided dxt ϕ 0.99 (cid:0)Fθ (xt, t, s, λ) dxt g/(g + c) 0 fθ(x1, 1, 0, λ) = x1 Fθ(x1, 1, 0, λ) LEM D(θ) Fθ (xt, t, s, λ) Fθ (xt, t, s, λ) + g2 2 LADV (θ) Softplus(Dψ(x wadaptive adaptive_weight(LADV , LEM D) L(θ) LAdv + α wadaptive LEM θ θ ηGθL(θ) else if Discriminator step then x0 D, x1 (0, σ2 dI) L(ψ) Softplus(Dψ(x0) Dψ(x ψ ψ ηDψL(ψ) Tangent warmup Tangent normalization 0 fθ(x1, 1, 0, λ) = x1 Fθ(x1, 1, 0, λ) (cid:1) + r(t s) dFθ (xt,t,s,λ) 0)) + β (xDψ(x0)2 2 + xDψ(x 0) Dψ(x0)) 0)2 2) dt dt F.4 Text-to-Image Experiments For our text-to-image experiments, we distill FLUX.1-[dev] [41] into few-step generator by finetuning LoRA [27] on top of the FLUX base model. We use the AYF-EMD objective and train the LoRA for 10,000 iterations on 8 NVIDIA A100 GPUs. Each GPU fits single 512 512 image, resulting in total batch size of 8. To manage memory, gradient checkpointing and gradient partitioning is used. We also warm up the tangent over the first 2000 iterations. Since FLUX.1-[dev] is guidance-distilled, we cannot use autoguidance and instead rely solely on the distilled guidance. 30 We train our model using the text-to-image-2M dataset [30] from Hugging Face, which contains over 2 million real and synthetic images. We filter this dataset and train only on the 100K images generated by FLUX.1-[dev] using text prompts from GPT-4o. F.5 User Study Details To evaluate our AYF-LoRA and compare it against TCD-LoRA [93] and LCM-LoRA [53] (both based on SDXL [59]), we conduct user study with 47 participants. We use holdout set of 200 prompts generated by GPT-4o from the text-to-image-2M dataset. For each prompt, we generate five images using each of the three methods, all sampled with four steps from the same random seed, resulting in 1000 sets of three generated images. Each participant is given text prompt and one image from each method, with the image order randomized to prevent bias. Participants are asked to select the best image based on quality and text alignment or indicate tie. Fig. 7 summarizes the results. See Fig. 10 for screenshot of the instruction. The participants were paid 0.10$ per evaluation with an average evaluation time of 15-30s per decision, with an hourly average pay of 18$/hr. Figure 10: Screenshot of instructions provided to the participants for the human evaluation study."
        },
        {
            "title": "G Ablation Studies",
            "content": "In this section, we isolate the effects of AYFs core design choices and compare our method against recent baseline consistency model and flow map approaches (on ImageNet512). summary of the results is provided in Tab. 5. We begin by analyzing two key design decisions behind AYF: (1) using the AYF-EMD loss instead of AYF-LMD, and (2) applying autoguidance on the teacher model instead of classifier-free guidance (CFG), which is used by existing CM methods. To compare AYF-EMD and AYF-LMD, we first evaluate both qualitatively on 2D toy dataset  (Fig. 11)  . In this setting, AYF-LMD significantly outperforms AYF-EMD. However, the trend reverses on image datasets, where AYF-EMD consis31 tently yields better results. As shown in Tab. 5, AYF-EMD leads to significantly improved generation quality. We also evaluate the impact of autoguidance. Replacing it with CFG consistently degrades performance across all sampling steps, highlighting the benefit of autoguidance during distillation. This difference can also be visually seen in the 2D setting  (Fig. 12)  . Next, we compare AYF against baseline methods. Against sCD [49], the current state-of-the-art consistency model, we observe that increasing the number of steps consistently degrades performance beyond 4 steps. In contrast, AYFs performance stabilizes after 8 steps and remains consistently better at 4 steps, narrowing the gap between the few-step student and the multi-step teacher. AYF achieves this improved multi-step performance by having slightly worse single-step generations. This is expected considering that both models share the same parameter budget but AYF solves more difficult task. However, performing short adversarial finetuning on top of pretrained AYF model significantly boosts performance across all sampling steps (see Fig. 8), overcoming this limitation. Note that also with autoguidance replaced by CFG, we still outperform sCD for all steps except for single-step generation. This trend remains when re-training sCD with an autoguided teacher and comparing with the full AYF with autoguided teacher. These analyses show that while autoguidance boosts performance, our model is superior to the previous state-of-the-art CM for all sampling step settings except single-step generation also when comparing under the same guidance scheme. This is due to our novel objectives for state-of-the-art continuous-time flow map training. Compared to Shortcut models [17], which we re-implemented, AYF again outperforms them consistently. While Shortcut models improve steadily with increasing NFEs, their few-step performance is significantly worse than both AYF and sCD. Unlike shortcut models, AYF reaches strong performance earlier and plateaus around 8 steps. Figure 11: Four-step samples from distilled AYF flow maps trained using the AYF-EMD and AYFLMD objectives for 2D distribution. Figure 12: Four-step samples from distilled AYF flow maps using no guidance, autoguidance, and CFG (scale 3) for 2D distribution. Table 5: Ablation study on ImageNet 512x512. indicates our reproduction of prior methods. Also see Fig. 8 for visualization of the key results. Training configurations 1-step 2-step 4-step 8-step 16-step 24-step 32-step 48-step 64-step FID() AYF-S (with autoguided teacher and AYF-EMD objective) - with AYF-LMD objective instead of AYF-EMD - with CFG teacher instead of autoguidance sCD-S (with autoguided teacher) - with CFG teacher instead of autoguidance Shortcut model (with autoguided teacher) 3.32 12.45 4. 2.90 3.26 1.87 8.90 2.57 1.87 2.68 47.60 13.12 1.70 6.70 2. 1.84 2.72 5.37 1.69 6.10 2.29 2.08 2.81 2.31 EDM-S + autoguidance (teacher) 566.13 298.60 89.88 26.41 1.72 5.85 2.31 2.32 3. 2.05 6.08 1.75 - - 2.68 - 1.92 3. 1.73 - - 2.91 - 1.85 2.21 1.74 - - 3.72 - 1.83 1.65 1.75 - - 4.62 - 1.81 1. Adversarial finetuning AYF-S + adv. loss 1.92 1.81 1.64 1. 1.63 1.61 1.63 1.62 1."
        },
        {
            "title": "H Additional Samples",
            "content": "H.1 Text-to-Image In Fig. 13, we show additional text-to-image samples generated by our FLUX.1 [dev]-based AYF flow map model using efficient LoRA fine-tuning. We also show the effect of increasing the number of sampling steps of this model in Fig. 15. Additionally, we show some side-by-side comparisons between our model and prior LoRA based consistency models in Fig. 14. We find that our model produces sharper and more detailed images with better prompt adherence. Figure 13: Selected 4-step samples generated by our FLUX.1 [dev]-based AYF flow map model using efficient LoRA fine-tuning. 33 Figure 14: Qualitative comparison between 4-step samples from LCM [54], TCD [93], and AYF (view zoomed in). H. ImageNet-512 In Figs. 17 to 26, we show additional oneand two-step samples generated by our ImageNet-512 AYF model. We also show the effect of increasing the number of sampling steps of this model in Fig. 16. Only very tiny quality differences are visible. H.3 ImageNet-64 In Figs. 27 and 28, we show additional oneand two-step samples generated by our ImageNet-64 AYF model. Figure 15: The effect of increasing number of steps when sampling from the text-to-image AYF model. Figure 16: The effect of increasing the number of steps when sampling from the AYF model on ImageNet512 (best viewed zoomed-in). 35 Figure 17: Selected one-step samples generated by our ImageNet512 AYF-S model, shown for classes 1 (goldfish) and 22 (bald eagle). Figure 18: Selected two-step samples generated by our ImageNet512 AYF-S model, shown for classes 1 (goldfish) and 22 (bald eagle). Figure 19: Selected one-step samples generated by our ImageNet512 AYF-S model, shown for classes 29 (axolotl) and 88 (macaw). Figure 20: Selected two-step samples generated by our ImageNet512 AYF-S model, shown for classes 29 (axolotl) and 88 (macaw). 37 Figure 21: Selected one-step samples generated by our ImageNet512 AYF-S model, shown for classes 270 (white wolf) and 978 (coast). Figure 22: Selected two-step samples generated by our ImageNet512 AYF-S model, shown for classes 270 (white wolf) and 978 (coast). Figure 23: Selected one-step samples generated by our ImageNet512 AYF-S model, shown for classes 979 (valley) and 980 (volcano). Figure 24: Selected two-step samples generated by our ImageNet512 AYF-S model, shown for classes 979 (valley) and 980 (volcano). 39 Figure 25: Selected one-step samples generated by our ImageNet512 AYF-S model, shown for classes 89 (sulphur-crested cockatoo) and 985 (daisy). Figure 26: Selected two-step samples generated by our ImageNet512 AYF-S model, shown for classes 89 (sulphur-crested cockatoo) and 985 (daisy). Figure 27: Uncurated one-step samples generated by our ImageNet64 AYF-S model, with randomly chosen class labels. 41 Figure 28: Uncurated two-step samples generated by our ImageNet64 AYF-S model, with randomly chosen class labels."
        },
        {
            "title": "I Text Prompts for Generated Images",
            "content": "Here we will list all the text prompts used to generate the images in Figs. 1, 3 and 13. \"A surreal and dreamlike scene featuring small island with lush green mountain encased in giant, translucent sphere. The sphere reflects warm golden and green tones, blending harmoniously with the soft orange hues of serene sunset. The scene is set on calm, reflective body of water, with gentle ripples creating perfect reflections of the sphere and the sky. lone wooden boat floats peacefully in the foreground, adding sense of scale and solitude. Distant mountains frame the horizon, completing the ethereal and otherworldly atmosphere.\" \"A dark, atmospheric forest shrouded in mist, with towering, shadowy trees. At the center stands hooded warrior clad in black armor and flowing cloak, holding massive, glowing crimson sword. The blade emits an intense, fiery red light that contrasts sharply with the cool blue tones of the misty forest. Scattered red flowers grow on the forest floor, their vivid color echoing the swords glow. Fiery red embers float through the air, adding to the ominous and mystical mood. The scene is cinematic and otherworldly, with strong sense of power and mystery.\" \"A mystical forest bathed in moonlight, with glowing blue and green bioluminescent plants. gentle mist rolls through the trees, and faint magical runes glow on ancient stone pillars scattered throughout the scene. In the background, shimmering waterfall cascades into crystal-clear pool. The atmosphere is serene, with soft light beams piercing through the canopy above.\" \"A hyper-realistic photograph of delicate yet powerful creature, rabbit with the striped fur of tiger, combined with the soft, powdery wings and antennae of moth. The rabbits body is small and fluffy, with bold orange and black stripes covering its fur, and its back is adorned with large, soft moth wings that shimmer in muted tones. The scene is set in moonlit garden, with the creature nestled among flowers, its wings gently fluttering as it sniffs at bloom.\" \"A luminous koi fish with translucent fins and shimmering galaxy-like patterns on its scales, gracefully swimming in mystical pond under the soft light of glowing full moon. The setting features delicate lotus flowers, glowing orbs, and vibrant foliage with intricate golden details weaving through the scene. The water reflects celestial ambiance, with tiny stars and glowing accents creating dreamlike atmosphere. The composition is highly detailed, with rich, deep colors of navy and gold contrasted by the vivid reds and oranges of the koi and flowers. The overall style combines fantasy realism with touch of ornate elegance.\" \"A serene bald monk in vibrant orange robes meditating and levitating above pristine swimming pool, with modern minimalist house and lush trees in the background. The atmosphere is calm and sunny, with bright daylight casting clean shadows. The scene captures surreal and harmonious balance of spirituality and luxury, with vibrant colors, sharp details, and reflective pool surface.\" \"A cinematic and hyper-detailed and hyper-realistic portrait photograph captured in the rugged beauty of the scottish moor, gorgeous young woman with wild, fiery red hair sits gracefully under large, gnarled, ancient tree, leaning against the rough bark, her vibrant locks catching the golden hour light and flowing freely around her shoulders, her piercing green eyes sparkle with life and joy as she smiles warmly, her expression radiates youthful energy.\" \"A weathered, yellow robotic cat kneels gently in muddy alley, its mechanical paws tenderly holding small, pure white rabbit. The robots design is mix of sleek and worn, with scratches and rust marks telling story of resilience. Its glowing blue eyes exude warmth and curiosity as it carefully examines the rabbit, which has vibrant yellow flower sprouting from its back. Around them, abandoned concrete buildings loom in the background, shrouded in soft mist, while faint droplets of rain fall, creating tiny ripples in the puddles. The scene is quiet and melancholic, with subtle touch of hope.\" \"A lone samurai stands at the edge of crimson maple forest, his silhouette dark against the golden light of dusk. Fallen leaves swirl around him as gentle breeze carries the scent of 43 autumn through the air. narrow wooden bridge stretches across tranquil koi pond, its surface reflecting the fiery hues of the trees above. In the distance, the curved rooftop of hidden temple peeks through the dense foliage, bathed in the last light of the setting sun.\" \"A carnival floats above the clouds, its colorful tents and twisting roller coasters suspended in midair. The Ferris wheel glows with radiant neon lights, each cabin holding different surreal sightone with floating goldfish, another with miniature city inside. Hot air balloons drift lazily between the attractions, carrying visitors who gaze down at the soft, cotton-like clouds below. The air is filled with the sounds of distant laughter and the smell of caramel popcorn carried by the wind.\" \"A warm and inviting cottage interior, lit by crackling fireplace. The room features rustic wooden furniture, patchwork quilt on cozy armchair, and shelves lined with books and potted plants. Sunlight streams through window, casting soft golden light across the space. cat naps on woven rug in front of the fire.\" \"A charming, bright-eyed octopus sports miniature, pointed witchs hat, its brim adorned with delicate, sparkling broom. The octopuss eyes, shining like two bright, black jewels, sparkle with excitement as it gazes out from beneath its witchy disguise. One of its tentacles grasps tiny, wooden broom, its bristles perfectly proportioned to the octopuss miniature size. Another tentacle holds cauldron-shaped candy pail, its metal surface adorned with miniature, glowing jack-o-lantern face. The atmosphere is playful and lighthearted, with hint of spooky, Halloween fun. The octopuss very presence seems to radiate sense of joyful, mischievous energy, as if its ready to cast spell of delight on all who encounter it. highest-Quality, intricate details, visually stunning, Masterpiece\" \"A baby dragon with vibrant, golden-yellow scales sits on forest floor, its enormous, glossy black eyes sparkling with curiosity. Its tiny horns curve gently upward, and soft tuft of orange fur runs along its head, adding to its charm. The dragons delicate claws rest on its chest as it tilts its head slightly, radiating an innocent, playful energy. The dappled sunlight filters through the trees, casting soft shadows around the creature while emphasizing the intricate textures of its scales and the gentle details of the forest floor.\" \"A towering, moss-covered golem with glowing blue eyes trudges gently through an enchanted forest. Tiny birds nest in the cracks of its ancient stone body, and wildflowers bloom along its shoulders. Each step it takes leaves behind shimmering footprints, as if the earth itself is waking beneath it. small, mischievous fairy perches on its shoulder, whispering secrets into its ear.\" \"A celestial bard with flowing, star-speckled robes strums crystalline harp that hums with the music of the cosmos. Their silver hair drifts as if caught in an eternal breeze, and their eyes shine like twin galaxies. As they play, glowing constellations dance around them, weaving stories of forgotten legends. The air vibrates with an ethereal melody, bending reality itself to their song.\" \"A tiny mushroom spirit with cap like spotted toadstool scurries through moonlit meadow, carrying lantern made from firefly trapped in crystal jar. Their tiny hands clutch satchel filled with enchanted spores, which they scatter as they run, causing luminescent fungi to sprout in their wake. The night air is filled with soft, magical glow as they embark on their secret midnight errand.\" \"A mischievous clockwork cat made of brass and polished wood prowls through an ancient library, its glowing emerald eyes scanning the towering bookshelves. Gears whir softly as it moves, its tail ticking like pocket watch. Whenever it pounces, time seems to stutter for just fraction of second, as if reality itself is playing along with its game.\" \"A wandering candy alchemist, dressed in coat of shimmering, sugar-spun fabric, mixes glowing elixirs in crystal vials. Their hair is cascade of molten caramel, and their eyes sparkle like rock candy. Each step they take leaves behind trail of edible blossoms, and their belt is lined with tiny jars of potions that fizz, swirl, and pop with magical flavors.\" \"A towering jellyfish queen glides gracefully through an underwater kingdom, her translucent tendrils trailing behind her like an elegant gown. Bioluminescent patterns ripple across her ethereal body, pulsing in sync with the deep ocean currents. Tiny fish swim in mesmerizing formations around her, drawn to the soft, hypnotic glow that follows her every movement.\" 44 \"A giant, four-armed baker made entirely of gingerbread hums deep, rumbling tune as he kneads dough in cozy, fire-lit kitchen. His icing-swirled eyebrows lift in delight as he pulls tray of enchanted pastries from the oveneach one shaped like tiny, dancing creature. The warm scent of cinnamon and sugar fills the air as his candy-button eyes twinkle with pride.\" \"In the heart of an ancient cathedral, Excalibur rests upon an altar of marble, encased in shimmering, ethereal light. The stained-glass windows cast multicolored beams across the blade, illuminating the intricate runes carved into its steel. quiet reverence fills the chamberno one dares to approach, for legends say that only the true king may grasp its hilt without being turned to dust.\" \"A mischievous minion transformed into dark side warrior, inspired by Darth Vader, stands menacingly in dimly lit chamber. Its yellow, cylindrical body is painted matte black, with glossy red accents glowing faintly. It wears flowing black cape, custom helmet with sharp edges and single menacing goggle-eye glowing red. In its hand, tiny yet powerful red lightsaber hums with energy. The minions expression is mix of determination and its usual playful mischief, as if ready to wreak havoc while still being adorably chaotic. The dark background is illuminated by faint red and blue lights, evoking the ominous atmosphere of Sith lair.\" \"A cunning anthropomorphic wolf wearing coat made of sheeps wool stands amidst flock of sheep. The wolfs face is sharp and expressive, with piercing golden eyes and sly, toothy grin. The wool coat is textured and fluffy, blending seamlessly into the surrounding flock, while curved ram-like horns add an unusual twist to the disguise. The sheep in the background look curious but unaware of the predator among them. The scene is set in misty, overcast countryside, with soft lighting emphasizing the eerie yet whimsical mood.\" \"A bustling cyberpunk metropolis at night, filled with towering neon-lit skyscrapers, hovering vehicles, and busy streets lined with holographic advertisements. The city is alive with vibrant pink, purple, and blue lights reflecting off wet pavement. People in futuristic attire walk below, while drones fly overhead. giant screen displays an AI figure speaking to the crowd.\" \"A vibrant, cherry-red 1970s muscle car, gleaming under warm afternoon sun. Chrome accents sparkle, reflecting the azure sky. The car is parked on winding asphalt road, surrounded by lush green countryside. Show the cars powerful engine and classic design details. Capture sense of nostalgia and freedom.\""
        },
        {
            "title": "J Licenses",
            "content": "Our models and code are built upon the following codebases and datasets: EDM2 (https://github.com/NVlabs/edm2): Used for our ImageNet experiments. Licensed under CC BY-NC-SA 4.0. StyleGAN3 (https://github.com/NVlabs/stylegan3): We use its metric calculation logic to compute recall scores. Licensed under the NVIDIA Source Code License. StyleGAN2 (https://github.com/NVlabs/stylegan2): We use its discriminator implementation for our adversarial finetuning experiments. Licensed under the NVIDIA Source Code License. Diffusers (https://github.com/huggingface/diffusers): Used for our text-toimage experiments. Licensed under the Apache License 2.0. We fine-tune LoRA on top of FLUX.1-dev, which is under proprietary non-commercial license (https: //huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md). Text-to-Image-2M Dataset (https://huggingface.co/datasets/jackyhate/ text-to-image-2M): Used to train our distilled text-to-image LoRAs. Licensed under the MIT License. ImageNet Dataset: Used for our main experiments. Distributed under non-commercial research license."
        }
    ],
    "affiliations": [
        "NVIDIA",
        "University of Toronto",
        "Vector Institute"
    ]
}