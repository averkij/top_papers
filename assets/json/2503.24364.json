{
    "paper_title": "Query and Conquer: Execution-Guided SQL Generation",
    "authors": [
        "Łukasz Borchmann",
        "Marek Wydmuch"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose a novel approach for generating complex outputs that significantly improves accuracy in text-to-SQL tasks. Our method leverages execution results to select the most semantically consistent query from multiple candidates, enabling smaller, cost-effective models to surpass computationally intensive reasoning methods such as o1, o3-mini, and DeepSeek R1 while reducing inference cost by as much as 30 times. It integrates effortlessly with existing models, offering a practical and scalable pathway to state-of-the-art SQL generation."
        },
        {
            "title": "Start",
            "content": "Query and Conquer: Execution-Guided SQL Generation Łukasz Borchmann and Marek Wydmuch Snowflake AI Research lukasz.borchmann@snowflake.com 5 2 0 2 1 3 ] . [ 1 4 6 3 4 2 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We propose novel approach for generating complex outputs that significantly improves accuracy in text-to-SQL tasks. Our method leverages execution results to select the most semantically consistent query from multiple candidates, enabling smaller, cost-effective models to surpass computationally intensive reasoning methods such as o1, o3-mini, and DeepSeek R1 while reducing inference cost by as much as 30 times. It integrates effortlessly with existing models, offering practical and scalable pathway to state-of-the-art SQL generation."
        },
        {
            "title": "Introduction",
            "content": "Large language models frequently produce correct outputs when multiple samples are considered (pass@k), yet their one-shot accuracy (pass@1) remains significantly lower. This gap motivates exploring methods that leverage multiple outputs to reliably identify the most promising one. prime example is the self-consistency method, which generates diverse set of reasoning paths from the model and then employs majority voting on the final answers (e.g. numerical values for mathematical problems) to select the most likely correct output (Wang et al., 2023). While practical for short, well-defined answers, this strategy quickly breaks down when outputs have multiple correct yet structurally distinct representations, such as in code generation tasks. Consider simple SQL query retrieving all unique department IDs from an employees table: SELECT DISTINCT department_id FROM employees ; This query can be restructured in variety of different yet equivalent forms, including: SELECT department_id FROM employees GROUP BY department_id ; Alternatively, it can be expressed in an entirely different way, such as: Figure 1: Cost-accuracy analysis for Qwen 2.5 Coder 7B, with or without self-consistency (10-20 samples), compared alongside OpenAI models. SELECT department_id FROM ( SELECT department_id , ROW_NUMBER () OVER ( PARTITION BY department_id ORDER BY department_id ) AS rn FROM employees ) AS subquery WHERE rn = 1; Each of these variations produces the same result but differs in structure, rendering majority voting strategies ineffective. We argue that overcoming this challenge demands methods that measure equivalence at the execution level rather than depending on structural comparison and propose such strategy substantially narrowing the gap between pass@1 and pass@k accuracy. Specifically, we propose novel self-consistency approach tailored to SQL generation, leveraging exact and approximate execution-based similarity metrics to assess semantic equivalence directly from query outputs (Figure 2). We further frame the problem of self-consistency within the Minimum Bayes Risk (MBR) decoding framework, providing theoretical justification for our method 1 Figure 2: Execution-Guided SQL Generation. and extending self-consistency to output spaces defined by execution behavior rather than superficial syntactic forms. Finally, we exploit the prefix executability property inherent in specific SQL dialects to incrementally apply execution-based selfconsistency during intermediate query generation stages, enabling more robust refinement of complex queries. These methodological advancements yield substantial empirical improvements. Notably, we demonstrate that by applying execution-based selfconsistency, smaller, less expensive models can match the performance of much larger models, highlighting significant improvement in costefficiency. In particular, the 7B Qwen 2.5 Coder employing our method improves the accuracy by nearly 10%, reaching the level of O1 despite yielding 30 times lower inference cost (Figure 1). These findings underscore our methods efficiency, and scalability, positioning it as strong candidate for real-world SQL generation tasks."
        },
        {
            "title": "2 Related Works",
            "content": "Previous authors addressed SQL generation consistency through heuristics primarily focused on textual and structural similarities among queries (Hong et al., 2025; Li et al., 2024; Chen et al., 2021; Li et al., 2024). Such inherently struggle when queries differ structurally yet produce identical outputs, and thus, are semantically equivalent. To address this limitation, we propose evaluating equivalence directly at the execution level. Precisely, we execute candidate queries (or approximate their execution) and compare resulting outputs, thus capturing semantic correctness based on actual behavior rather than superficial query structure alone (Figure 2). Significantly, the proposed method distinguishes itself from recent sequential, multi-step errorcorrection methods (Lee et al., 2024; Cen et al., 2025; Wang et al., 2024). These involve iterative refinement, inherently increasing complexity and latency. Conversely, our execution-guided similarity selection enables parallelizable generation processes and relies on efficient comparisons that primarily leverage CPU memory, typically underutilized in standard large language model deployment infrastructures (Oliaro et al., 2024)."
        },
        {
            "title": "3 Execution-Guided SQL Generation",
            "content": "The justification for execution-based similarity stems from the concept of Minimum Bayes Risk decoding (Nadas, 1983, 1985; Kumar and Byrne, 2004; Eikema and Aziz, 2020)."
        },
        {
            "title": "3.1 MBR Decoding and Code Utility",
            "content": "In MBR decoding, the objective is to find hypothesis that minimizes the expected loss (maximizes the expected utility) relative to the set of possible outputs H. If we assume the posterior distribution is derived from the empirical distribution obtained through sampling, it can be described as: = arg max hH (cid:88) ˆhH c(ˆh) (h, ˆh) where c(ˆh) denote the count of particular hypothesis ˆh in these samples, and assesses the utility of choosing when the true hypothesis is ˆh. In other words, we select the generation closest on average to all the likely generations, where closeness is measured under some function appropriate for the considered domain. 2 Concerning majority voting, such utility function is simply: (h, ˆh) = (cid:40) 1, 0, if = ˆh, otherwise. and the objective simplifies to: = arg max c(h) hH . The zero-one utility is justified for short and unambiguous hypotheses, such as numerical answers for mathematical problems faced by LLM, because these are commonly evaluated with exact match accuracy. However, what does it mean for the two distinct SQL statements to be, to some extent, equivalent? Evaluation practices in the text-to-SQL domain answer this question by resorting to executionbased similarities (Zhong et al., 2020). Thus, the similarity used in self-consistency should be based on the execution equivalence rather than the shallow form of the query or semantic interpretation lacking the information from the execution plan."
        },
        {
            "title": "3.2 Execution Similarity",
            "content": "We follow the abovementioned observation and the notion that comparing two codes behavior is the natural way to measure their similarity. Such similarity results from yielding the same output under the same circumstances and naturally captures semantic equivalence, overcoming structural differences in queries (Cheers et al., 2021; Simpson and Voorneveld, 2018). Exact Execution Similarity. Upon execution, SQL queries we consider return values (cells) structured in horizontal rows and vertical columns identifiable by name. For the purposes of experiments, we define execution similarity for such SELECT statements based on the resulting tables and as form of recall of cells with respect to the larger column. Take an example of the following two tables considered column-by-column: 1 2 1 2 3 We retrieved two out of three values in column (1 and 2), and missed two out of two values in 3 column (, ), yielding similarity of 2/5 = 0.4 (refer to Appendix B.1 for formal definition). It is one of the many suitable functions that grow monotonically as we approach the desired outcome. Because it relies on the execution result, it fulfills our assumption of measuring behavior. Approximate Execution Similarity. Though in most cases, computing execution similarity is cheap, we consider variant without the actual execution. Here, logical execution plans for considered statements are compared, that is, the operations (e.g. table scans and joins) that the database engine would perform to execute the query. As the execution plan, such as returned by the EXPLAIN query, can be represented as table, we use the same metric for table-to-table comparison as in the proposed execution similarity. However, this time, it compares the execution plans rather than the actual results."
        },
        {
            "title": "3.3 Decoding of Partially-Executable SQL",
            "content": "Parts of standard SQL code, such as subqueries or Common Table Expressions, are executable independently, without the context of the entire query. This property could further improve accuracy without increasing the inference cost, as one could refine the decoding trajectories in the middle of the generation process based on the self-consistency of the considered SQL part. Due to the property of prefix executability, PipeSQL (Shute et al., 2024) appears to be even better aligned with this objective. In this dialect, each query prefix (up to the pipe sequence >) is also valid query. Consider the example of FROM users > WHERE views > 10 > AGGREGATE COUNT ( id ); The first pipe would produce the complete users table upon execution, the first two pipesusers filtered by the number of views, whereas the complete SQL returns the count of filtered users. Consequently, one can sample continuations at each step until the > sequence and apply execution-based self-consistency on partial completion. Figure 3 presents an example of such an approach. In the middle of generation, we select consensus continuation between two WHERE clauses and SELECT based on the similarity matrix: Figure 3: PipeSQL dialect has property that each query prefix (up to the pipe sequence >) is also valid query, making it possible to apply execution-based self-consistency in the middle of the generation process. Instead of sampling complete SQL sequences, we sample pipes and stop the generation process. Then, we pick the most consistent pipe and continue the generation sampling variants of the next pipe. There is tie because WHERE clauses are equivalent, so we pick the first and proceed. In the next step, there are three SELECTs considered: The second one is pickedthe most average since it produces union of the remaining ones and the generation finishes. Following this idea, we consider text-to-SQL tasks using partially executable PipeSQL dialect in addition to self-consistency based on complete SQL generation. Such partial-execution-guided decoding can potentially reduce errors early in query generation, improving efficiency and accuracy. Patience. In practice, information carried by single pipe can be too fine-grained for efficient execution-based consistency. Take an example of join statements that can begin with any of two tables leading to equivalent code when complete SQL sequences are considered: FROM emp > JOIN dept ON emp . did = dept . did > SELECT emp . name , dept . name ; FROM dept > JOIN emp ON emp . did = dept . did > SELECT emp . name , dept . name ; pairs are not. To accommodate this property, we introduce the patience parameter, which keeps the generation unless it was selected for rejection at least times in pipe-to-pipe comparisons."
        },
        {
            "title": "4 Text-to-SQL Experiments",
            "content": "We perform primary experiments on the BIRDIn all cases, the SQL (Li et al., 2023) dataset. input is the same fixed prompt with the serialized database schema, and the model is requested to generate SQL preceded by CoT (Appendix C.1). wide range of families and parameter counts is considered, including general-purpose models, such as Llama 3 (Grattafiori et al., 2024), Gemma 3 (Gemma Team, 2025), Mistral Large (Mistral AI, 2024b), GPT-4o (OpenAI, 2024), and Gemini 2.0 (Gemini Team, 2024), as well as code-specialized Qwen 2.5 Coder (Hui et al., 2024), DeepSeek Coder (Guo et al., 2024), and Codestral (Mistral AI, 2024a). Baselines. Compute-matched baselines are provided for each model considered in addition to the SQL obtained under greedy decoding. These include the majority vote with SQL normalization1 and beam search (in fact, beam search with beams yields even higher cost than sampling completions, but we assume that they are comparable for simplicity). Heavy Reasoners. Additionally, we provide results of computationally intensive reasoning approaches such as o1 and o3, DeepSeek R1, and Gemini 2.0 Flash Thinking, which require significantly more compute (OpenAI, 2025; DeepSeek AI, 2025; Gemini Team, 2024). The first pipes from each query are dissimilar, even though the sequences of the first and second 1Using SQLGlot to reformat code: https://github. com/tobymao/sqlglot.git 4 Model Llama 3.2 3B Qwen 2.5 Coder 3B Qwen 2.5 Coder 7B Llama 3.1 8B Gemma 3 12B Qwen 2.5 Coder 14B Codestral 22B v0.1 Gemma 3 27B Qwen 2.5 Coder 32B DeepSeek Coder 33B Llama 3.3 70B Mistral Large 2411 Llama 3.1 405 GPT-4o 2024-11-20 GPT-4o mini 2024-11-20 Gemini 2.0 Flash 001 Gemini 2.0 Flash-Lite 02-05 Gemini 2.0 Thinking 01-26 DeepSeek R1 o1 2024-12-17 o3-mini 2025-01-31 Bound Pass@10 Greedy Maj@10 Beam@10 Approx. Exact Approx. Exact Approx. Exact Baseline Scores Exec@10 Exec@20 Exec@30 43.5 57.7 67.9 62.1 64.9 71.5 63.5 66.3 71.1 63.4 67.9 66.1 68.2 62.2 62.1 70.9 69.4 18.6 30.2 44.1 32.9 49.7 54.7 45.6 53.1 55.0 40.1 53.7 53.1 54.2 51.6 46.9 60.6 56. 59.1 52.5 53.9 52.1 20.2 32.5 45.4 34.3 52.2 54.9 48.6 55.5 55.2 43.7 55.8 53.2 54.8 51.6 49.3 61.8 57.9 20.0 34.9 45.4 34.9 49.2 52.9 46.2 54.6 54.9 41.8 54.7 52.5 29.1 42.6 51.3 43.1 53.2 56.8 50.6 56.0 56.3 46.6 56.1 53.8 55.6 51.6 50.5 61.7 57. 25.6 40.2 51.7 43.6 53.0 57.2 51.3 55.6 57.1 47.7 56.6 54.2 56.5 52.4 50.5 61.9 57.5 32.2 45.6 52.6 44.6 54.7 57.4 51.0 56.6 56.9 48.5 56.9 53.7 56.6 52.4 50.9 62.2 57.8 31.8 45.4 53.8 47.5 53.9 58.3 52.3 56.3 57.6 49.7 57.4 54.6 57.3 52.6 51.3 62.1 58. 34.5 46.1 53.1 45.0 54.8 57.8 51.6 56.6 57.8 49.9 56.7 54.0 56.7 52.6 51.2 62.0 57.9 34.8 47.6 54.8 48.8 54.6 58.3 52.8 56.7 57.6 50.5 57.2 54.7 57.2 52.9 51.6 62.1 59.2 Table 1: BIRD-SQL Accuracy (Text-to-SQLite). The proposed MBR decoding with execution similarity (exec@n), compared to baselines: greedy decoding, majority voting with normalization (maj@10), beam search (beam@10), theoretical maximum (pass@10), and heavy reasoning LLMs. Samplings with temp = 0.7, validation subset. Upper bound. Pass@10 score is provided for reference as theoretical upper bound for all methods based on sampling complete SQL sequences. It is score with an oracle judge always selecting the best available SQL."
        },
        {
            "title": "4.1 Overall Accuracy Improvements",
            "content": "Results in Table 1 show that execution-guided generation consistently outperforms greedy decoding and compute-matched baselines, holding across diverse set of language modelsfrom smaller general-purpose LLMs to code-specific and proprietary state-of-the-art systems. Smaller open-source models (3B7B parameters) can see substantial boosts, often improving accuracy by 10 points or more with around 30 samples. These improvements highlight that even relatively modest model scales can achieve competitive accuracy when combined with our self-consistency strategy. Larger open-source models, such as Qwen 2.5 Coder 34B, Mistral, or Llama 70B, typically gain 1.53.5 points, while huge ones (Llama 405B) still slightly benefit. Closed-source systems also show considerable improvementsGPT-4o mini gains around 5 points, whereas more powerful proprietary models improve by 12 points. While improvements for larger models are modest, these results serve primarily as confirmation that applying self-consistency does not negatively impact their already strong performance. Notably, 7B Qwen Coder with 30 samples surpasses most heavier reasoning-intensive models, with only Gemini 2.0 Thinking maintaining an edge. Since details on these proprietary systems remain limited, Figure 1 treats cost as proxy for inference complexity. It compares Qwen 2.5 Coder 7B to two families of OpenAI models in terms of cost and accuracy, evaluated with and without self-consistency. The results demonstrate that selfconsistency significantly increases accuracy at moderate computational cost, offering an effective balance between quality and resource expenditure. Although exact variants typically provide superior accuracy, approximate methods remain highly attractive in latency-sensitive or resourceconstrained production environments. Overall, these findings underscore the robust effectiveness of execution-guided generation. Even at more minor model scales, self-consistency offers notable gains in the quality-cost tradeoff, matching or exceeding the performance of more complex proprietary solutions. Model Greedy Exec + Part. + Pat. Qwen Coder 7B Llama 8B Gemma 12B Qwen Coder 14B Gemma 27B Qwen Coder 32B Codestral Llama 70B Mistral Large LLama 405B 27.1 11.6 21.6 38.9 31.2 40.3 33.6 31.3 44.3 37. 41.6 14.8 42.0 51.2 47.3 53.8 46.8 51.2 50.4 54.0 42.8 22.8 42.0 49.6 46.8 53.2 47.4 48.7 50.8 53.4 44.3 24.7 45.3 53.0 49.1 55.2 53.0 52.0 53.0 56.7 Table 2: BIRD-SQL Accuracy (Text-to-PipeSQL). Greedy decoding results compared to ten samples budged with standard execution-based self-consistency, partial self-consistency, or its variant with patience."
        },
        {
            "title": "4.3 Leveraging Partial Executability",
            "content": "Since we rely on the PipeSQL dialect for partial executability (see Section3.3), these experiments required transpiling ground-truth BIRD-SQL queries, converting the underlying databases, and establishing separate baseline scores (Appendix B.3). Moreover, unlike the widely used SQLite dialect in previous experiments, PipeSQL is relatively novel and not recognized by default, necessitating thorough in-prompt documentation with examples. Consequently, our inputs in this setup reached approximately 15k tokens, so certain opensource models with limited context windows could not be evaluated. Commercial API-based models were similarly excluded, as their interfaces typically do not allow mid-generation refinement. Despite few-shot prompting and including detailed documentation on the new dialect, models typically perform substantially worse with PipeSQL compared to standard SQL. Switching from SQLite yields accuracy drops of 520 points depending on model capabilities, underscoring limited generalizability to novel query dialects and problems with instruction-following. Table 2 summarizes the results in multiple settings: (1) Greedy decoding, (2) Standard selfconsistency without leveraging partial executability, (3) Self-consistency enhanced by partial, pipe-bypipe executability, and (4) Partial executability with patience parameter (n = 3), accommodating temporary divergences in intermediate SQL steps. Standard self-consistency notably improves accuracy over greedy decoding (typically by around 15 points), because non-executable or failing SQL generations, more common with unfamiliar diFigure 4: Self-consistency gains for various sample sizes, temperatures, and models (Gemini 2.0 Flash, Llama 3.3 70B, Codestral, Qwen 2.5 Coder 7B)."
        },
        {
            "title": "4.2 Number of Samples and Temperature",
            "content": "Figure 4 examines how quickly improvements from execution-based self-consistency begin to plateau given different models and temperatures. Notably, gains become visible with as few as three samples, and using 15 samples proves to be strong balance between accuracy and computational cost. Although improvements tend to flatten around 50 samples, steady growth continues beyond that pointreflecting how each additional sample helps approximate the distribution of SQL queries generated by the model more accurately. Sampling temperature is key to balancing immediate accuracy with the potential gains from larger sampling budgets. While increased noise from higher temperatures undermines small-sample performance, it also fosters greater diversity in generated SQLsultimately boosting accuracy once the sampling budget is large enough to absorb occasional failures. In broad terms, weaker models benefit more from the proposed method. Interestingly, high pass@1 score does not always translate into superior self-consistency accuracy. E.g., while Qwen Coder 7B outperforms Codestral in high-sample regime, it underperforms until around 10 samples. 6 Figure 5: Effect of replacing outputs produced under greedy decoding by self-consistency outputs. Valid and invalid refer to executability, whereas correct and incorrectconforming to the gold standard. Figure 6: Top problems explaining why BIRD-SQL generations of DeepSeek Coder and GPT-4o mini were incorrect. Greedy decoding compared to selfconsistency outputs. alects, naturally receive low similarity scores and are thus filtered out. Partial pipe-by-pipe executability alone provides mixed results. Depending on the model, accuracy may slightly increase or decrease (12 points) compared to standard self-consistency. Finally, introducing the patience parameter generally yields further accuracy gains (110 points over standard self-consistency), demonstrating the benefit of tolerating intermediate divergences. We hypothesize considerable potential in the partial executability approach but fully realizing it requires models capable of generating high-quality PipeSQL. Currently, limitations include insufficient training data on such dialects and the lack of robust transpilers for existing SQL datasets."
        },
        {
            "title": "5 Qualitative Analysis",
            "content": "To gain deeper insights into how execution-based self-consistency enhances text-to-SQL generation, we analyze the specific error types it effectively addresses and present key statistical comparisons between greedy-decoded outputs and those refined through self-consistency. The starting point for this part is the previously established taxonomy of SQL generation errors (Shen et al., 2025) and linguistic ambiguities in text-to-SQL tasks (Huang et al., 2023). Specifically, we consider common categories of mistakes, including dialect mismatches, schema linking failures, data type mismatches, incorrect aggregation, logical form inaccuracies, improper table joins, and projection errors (see Appendix B.4 for details). Figure 5 examines the per-example impact of 7 adopting self-consistency compared to greedy decoding. While self-consistency occasionally yields inferior SQL queries, its overall effect is beneficial. Most improvements occur because the method successfully replaces executable but incorrect queries with queries that correctly address the users intent. closer inspection of the error categories reveals that most text-to-SQL failures stem from flawed logical forms, schema linking mistakes, and incorrect projections. These arise when queries are not logically coherent when the model confuses or hallucinates database elements (e.g., referencing nonexistent columns), and when it selects columns that do not match the intent of the user query. Execution-based self-consistency mitigates these issues by filtering out candidates that fail under execution or yield outlier data frames, thus promoting solutions that produce similar outputs across multiple independent samples. For example, applying self-consistency in DeepSeek Coder reduces schema linking errors by 40%, projection, and table join mistakes by 20%, and logical form errors by 11% (see Figure 6). Models with fewer initial errors tend to benefit less; for instance, GPT-4o mini still sees notable 30% drop in schema linking mistakes. However, the low incidence of such errors in its greedy baseline limits the overall improvement. Hence, this error-reduction pattern also aligns with the findings described in the earlier sections, where models with lower baseline performance benefited disproportionately more from executionbased self-consistency."
        },
        {
            "title": "6 Limitations",
            "content": "While our self-consistency provides substantial accuracy gains, it requires generating multiple candidate queries and assessing their behavior. In some cases, actual query execution can be expensive or time-consuming. Still, the proposed approximate approach significantly reduces this concern since EXPLAIN queries typically incur negligible costs. Moreover, generating multiple solutions introduces additional compute time compared to single pass, yet the approach remains easily parallelizable. By contrast, step-by-step agentic solutions can inflate latency significantly because their sequential reasoning is far more challenging to distribute across multiple processes. Next, even though our preliminary evaluation with the partially executable SQL dialect suggests promising results, direct comparison to standard 8 SQLite tasks remains an area for further studies. Finally, models with fewer initial errors or high baseline accuracy may exhibit diminishing returns, limiting the offered improvement."
        },
        {
            "title": "7 Summary",
            "content": "A family of self-consistency techniques relying on sampling multiple SQL queries and comparing their execution results has been introduced. Additionally, partial-execution variant allowing stepby-step refinement of intermediate query fragments was proposed, enabling further precision improvements when partial queries are reliably generated. The proposed methods robustly identify semantically equivalent queries even when there are structural variations, allowing smaller and inexpensive models to achieve accuracy typically reserved for larger and costlier LLMsall while avoiding substantial processing time overhead typically associated with iterative refinement strategies. The presented analysis reveals that offered improvements can be attributed to effectively addressing common SQL generation errors, yielding 20 40% reductions in schema linking errors, projection and table join mistakes, and logical form errors. While the presented experiments focused on SQL generation, the underlying principle of leveraging execution-based similarity naturally extends to other programming languages and codegeneration tasks. Supplementary results on text-tocode benchmarks provided in Appendix A.1 suggest the broad applicability of execution-guided self-consistency and highlight its potential in program synthesis across diverse domains. We believe that further exploration of executionguided generation will open promising avenues toward robust, efficient, and universally applicable code-generation models."
        },
        {
            "title": "Acknowledgments",
            "content": "We sincerely thank the colleagues whose insightful discussions and feedback shaped this paper. Special thanks to Anupam Datta, Michał Zaj ac, Filip Gralinski, Zhewei Yao, Andrzej Szwabe, Michał Pietruszka, Jakub Swi atkowski, Wojciech Jaskowski, and Tomasz Dwojak for generously sharing their perspectives, inspiring research questions, and impacting the evolution of our ideas."
        },
        {
            "title": "References",
            "content": "Jipeng Cen, Jiaxin Liu, Zhixu Li, and Jingjing Wang. 2025. SQLFixAgent: Towards SemanticAccurate Text-to-SQL Parsing via ConsistencyEnhanced Multi-Agent Collaboration. Preprint, arXiv:2406.13408. Hayden Cheers, Yuqing Lin, and Shamus P. Smith. 2021. Academic Source Code Plagiarism Detection by Measuring Program Behavioral Similarity. IEEE Access, 9:5039150412. Mark Chen et al. 2021. guage Models Trained on Code. arXiv:2107.03374. Evaluating Large LanPreprint, DeepSeek AI. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. Preprint, arXiv:2501.12948. Bryan Eikema and Wilker Aziz. 2020. Is MAP decoding all you need? the inadequacy of the mode in neural machine translation. In Proceedings of the 28th International Conference on Computational Linguistics, pages 45064520, Barcelona, Spain (Online). International Committee on Computational Linguistics. Gemini Team. 2024. Highly Capable Multimodal Models. arXiv:2312.11805. Gemini: Family of Preprint, Gemma Team. 2025. Gemma 3 Technical Report. Aaron Grattafiori et al. 2024. The Llama 3 Herd of Models. Preprint, arXiv:2407.21783. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. 2024. DeepSeek-Coder: When the Large Language Model Meets Programming The Rise of Code Intelligence. Preprint, arXiv:2401.14196. Shankar Kumar and William Byrne. 2004. Minimum Bayes-Risk Decoding for Statistical Machine Translation. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004, pages 169176, Boston, Massachusetts, USA. Association for Computational Linguistics. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient Memory Management for Large Language Model Serving with PagedAttention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Dongjun Lee, Choongwon Park, Jaehyuk Kim, and Heesoo Park. 2024. MCS-SQL: Leveraging Multiple Prompts and Multiple-Choice Selection For Text-toSQL Generation. Preprint, arXiv:2405.07467. Boyan Li, Yuyu Luo, Chengliang Chai, Guoliang Li, and Nan Tang. 2024. The Dawn of Natural Language to SQL: Are We Fully Ready? Proceedings of the VLDB Endowment, 17(11):33183331. Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Rongyu Cao, Ruiying Geng, Nan Huo, Xuanhe Zhou, Chenhao Ma, Guoliang Li, Kevin C. C. Chang, Fei Huang, Reynold Cheng, and Yongbin Li. 2023. Can LLM Already Serve as Database Interface? BIg Bench for Large-Scale Database Grounded Text-to-SQLs. Preprint, arXiv:2305.03111. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2023. Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation. In Thirtyseventh Conference on Neural Information Processing Systems. Mistral AI. 2024a. Codestral: Hello, World! Mistral AI. 2024b. Large Enough. Zijin Hong, Zheng Yuan, Qinggang Zhang, Hao Chen, Junnan Dong, Feiran Huang, and Xiao Huang. 2025. Next-Generation Database Interfaces: Preprint, Survey of LLM-based Text-to-SQL. arXiv:2406.08426. A. Nadas. 1983. decision theorectic formulation of training problem in speech recognition and comparison of training by unconditional versus conditional maximum likelihood. IEEE Transactions on Acoustics, Speech, and Signal Processing, 31(4):814817. Zezhou Huang, Pavan Kalyan Damalapati, and Eugene Wu. 2023. Data Ambiguity Strikes Back: How Documentation Improves GPTs Text-to-SQL. Preprint, arXiv:2310.18742. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, Kai Dang, Yang Fan, Yichang Zhang, An Yang, Rui Men, Fei Huang, Bo Zheng, Yibo Miao, Shanghaoran Quan, Yunlong Feng, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, and Junyang Lin. 2024. Qwen2.5-Coder Technical Report. Preprint, arXiv:2409.12186. A. Nadas. 1985. Optimal solution of training problem in speech recognition. IEEE Transactions on Acoustics, Speech, and Signal Processing, 33(1):326329. Gabriele Oliaro, Zhihao Jia, Daniel Campos, and Aurick Qiao. 2024. SuffixDecoding: Model-Free Approach to Speeding Up Large Language Model Inference. Preprint, arXiv:2411.04975. OpenAI. 2024. GPT-4 Technical Report. Preprint, arXiv:2303.08774. OpenAI. 2025. Competitive Programming with Large Reasoning Models. Preprint, arXiv:2502.06807. 9 Jiawei Shen, Chengcheng Wan, Ruoyi Qiao, Jiazhen Zou, Hang Xu, Yuchen Shao, Yueling Zhang, Weikai Miao, and Geguang Pu. 2025. Study of In-Context-Learning-Based Text-to-SQL Errors. Preprint, arXiv:2501.09310. Jeff Shute, Shannon Bales, Matthew Brown, JeanDaniel Browne, Brandon Dolphin, Romit Kudtarkar, Andrey Litvinov, Jingchi Ma, John Morcos, Michael Shen, David Wilhite, Xi Wu, and Lulan Yu. 2024. SQL Has Problems. We Can Fix Them: Pipe Syntax In SQL. pages 40514063. Alex Simpson and Niels Voorneveld. 2018. Behavioural Equivalence via Modalities for Algebraic Effects. In Programming Languages and Systems, pages 300 326, Cham. Springer International Publishing. Transaction Processing Performance Council. 2014. TPC Benchmark (Decision Support) Standard Specification Revision 2.17.1. Bing Wang, Changyu Ren, Jian Yang, Xinnian Liang, Jiaqi Bai, Linzheng Chai, Zhao Yan, Qian-Wen Zhang, Di Yin, Xing Sun, and Zhoujun Li. 2024. MACSQL: Multi-Agent Collaborative Framework for Text-to-SQL. Preprint, arXiv:2312.11242. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-Consistency Improves Chain of Thought Reasoning in Language Models. Preprint, arXiv:2203.11171. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-Art Natural Language Processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 3845, Online. Association for Computational Linguistics. Ruiqi Zhong, Tao Yu, and Dan Klein. 2020. Semantic Evaluation for Text-to-SQL with Distilled Test Suites. Preprint, arXiv:2010.02840."
        },
        {
            "title": "A Supplementary Experiments",
            "content": "A.1 Beyond SQL Generation The execution-based self-consistency we proposed has applications beyond SQL generation, e.g., for ordinary Python code. We consider two functions similar if they produce the same outputs for the same inputs. The more inputs we test and outputs we compare, the more we know about their similarity. For example, we would expect similarity of 0.5 with Python functions: Model Greedy Exec@10 Mistral Large 2407 Codestral 22B Llama 3.1 8B Llama 3.1 405B 84.8 73.8 61.6 79.9 85.2 78.6 66.1 83.8 (a) HumanEval+ Model Greedy Exec@10 Mistral Large 2407 Codestral 22B Llama 3.1 8B Llama 3.1 405B 65.9 62.4 55.3 70.4 75.3 72.4 68.0 77.5 (b) MBPP+ Table 3: Python generation accuracy. Greedy decoding compared to execution-based self-consistency. def A(x: int ) -> int : return def B(x: int ) -> int : return if % 2 == 0 else 0 as they yield the same output for half of the inputs. Specifically, given functions A, : (cid:55) we sample arguments {x1, . . . , xn} and measure the expected fraction of agreement between A(x) and B(x) over the domain X. Similarity is defined as: S(A, B) = 1 (cid:88) i=1 1[A(xi) = B(xi)], where 1[A(xi) = B(xi)] is the indicator function that checks if A(xi) equals B(xi). Experiments. In practice, using random input values is sample-inefficient, and having source of somewhat reasonable inputs concerning the intended behavior is better. For simplicity, in this section, we assume such source is given and rely on real test cases from HumanEval+ and MBPP+ benchmarks (Liu et al., 2023). Note that we do not leverage function definitions or expected outputs but merely the tested function arguments. We observe consistent accuracy gains under execution-based self-consistency in both HumanEval+ and MBPP+  (Table 3)  . On HumanEval+, the improvements tend to be modeste.g., Mistral Large rises from 84.8% to 85.2%, while Llama 3.1 8B increases by nearly five points (61.6% to In contrast, MBPP+ shows more pro66.1%). nounced gains: Mistral Large jumps by almost ten points (65.9% to 75.3%), and Llama 3.1 8B improves by over twelve (55.3% to 68.0%). 10 Model Bound Pass@10 @10 @20 @30 Sample Budget DeepSeek Coder 33B Qwen Coder 7B Codestral 22B v0.1 + DeepSeek Coder 33B + Qwen Coder 7B Llama 70B Gemini Flash 001 Qwen Coder 32B + Llama 70B + Gemini Flash 63.4 67.9 63.5 66.5 70.1 67.9 70.9 71.1 72.2 74.6 47.7 51.7 51.3 51.4 52.7 56.6 61.9 57.1 57.4 61. 49.7 53.8 52.3 53.3 54.9 57.4 62.1 57.6 58.1 62.1 50.5 54.8 52.8 54.2 55.9 57.2 62.1 57.6 58.4 62.6 beam-search baselines, which relied on the Transformers library (Wolf et al., 2020). In both setups, inference runs were performed on Nvidia DGX nodes with 8H100 GPUs. Nearly all inferences relied on bf16 precision, except Llama 405B, which was evaluated in fp8 due to hardware constraints. OpenAI models were accessed through their public API, Gemini models through the proprietary Google interface, and DeepSeek R1 via Snowflake Cortex. When reasoning-intensive models were examined, up to 32k tokens were allowed for their chain-of-thought processes. Table 4: Impact of cross-model consistency on BIRDSQL Accuracy (Text-to-SQLite). B.1 Data Frame Similarity Specifically, we define similarity as: These results indicate that execution-based comparisons become increasingly valuable for more challenging code generation benchmarks, where model outputs show more significant variability, and there is more room for error correction by filtering out inconsistent or failing candidates. A.2 Cross-Model SQL Consistency Another way to broaden the range of generated solutions is to sample from multiple LLMs rather than relying on single one. Results presented in Table 4 indicate that combining samples from multiple LLMs, even without weighting them by expected accuracy, yields modest but consistent accuracy improvements typically on the order of 0.5 to 1 point. In the larger model group, for instance, pairing stronger model (Qwen Coder 32B) with weaker one (Llama 70B) provides slight boost over using only the stronger model for the same total number of samples. Adding the more capable Gemini model to this mix further improves results. Similar trends emerge among smaller and midsized models (e.g. Qwen 2.5 Coder 7B, Deepseek Coder, and Codestral), where combining their outputs slightly enhances performance without increasing the total sample count in self-consistency. Despite these incremental gains, the approach is appealing for its simplicity and low overhead, underscoring the potential of straightforward crossmodel ensembling."
        },
        {
            "title": "B Details of SQL Experiments",
            "content": "All open-source language models were evaluated using vLLM (Kwon et al., 2023), except for the S(A, B) = max (A, B) (cid:88) (cid:88) = min (cid:0)fAc(v), fBc(v)(cid:1), cC vVc where and denote the total number of cells in tables, is the set of all column names, Vc is the set of unique values in column c, whereas fAc(v) and fBc(v) denote the frequency of value in column of and B, respectively. B."
        },
        {
            "title": "Inference Cost",
            "content": "All cost estimates reflect actual billing under nonbatch inference. Specifically, OpenAIs usagebased pricing was applied to the OpenAI models, while Together.ais pricing was used to obtain Qwen 2.5 Coder costs. Because the Qwen 2.5 Coder 7B was unavailable on Together.ai, its cost was approximated based on the non-coder Qwen 2.5 7B Instruct Turbo pricing tier. B.3 BIRD for PipeSQL For simplicity, gold standard queries were converted to the regular BigQuery format rather than an explicit pipe-based syntax. Since the same execution engine powers both dialects and produces identical results for semantically equivalent queries, the choice does not affect correctness. Each original query was first transpiled from SQLite to BigQuery using the SQLGlot library. Both forms were executedone in SQLite, the other in BigQueryand their resulting data frames were compared for consistency. If the SQLGlottranspiled query failed or did not match the original querys result, the query was processed by an LLM 11 (o3-mini) for manual repair. The corrected query was again tested for matching data frames. For fewer than 100 instances, neither automatic nor LLM-based transpilation yielded matching results. These queries were removed from the final PipeSQL dataset. All other queries were successfully converted, ensuring that the PipeSQL variant of BIRD-SQL closely mirrors the original in both content and accuracy. B.4 SQL Error Taxonomy For the purposes of qualitative analysis, we simplify the previously established taxonomy of SQL generation errors (Shen et al., 2025) and linguistic ambiguities in text-to-SQL tasks (Huang et al., 2023) distinguishing the following categories. Dialect. Errors arising from dialect differences between SQL variants. generated query might be semantically and syntactically correct in general but fail when executed due to differences specific to the target database. Schema Linking. Errors due to incorrect or failed mappings between natural language references and corresponding database schema elements. This includes hallucinating non-existent tables or columns that appear relevant. Data Type. Errors occurring when queries fail or produce incorrect results due to mismatches or unexpected data types within the database. Aggregation. Errors involving incorrect use or omission of aggregation functions (e.g. COUNT, SUM), leading to inaccurate summary calculations or improperly grouped results. Logical Form and Condition. Errors resulting from incorrect logical structures or incomplete query specifications. This includes missing or incorrect conditions, inappropriate filters, erroneous ordering logic, or improperly scoped constraints. Table Joins. Errors involving incorrect table relationships, such as joining irrelevant tables, omitting necessary joins, or misidentifying join conditions. Projection. The model may choose the wrong columns or expressions to return. Sometimes, it might select computed value when the question asks for an entity or vice versa."
        },
        {
            "title": "C Prompts",
            "content": "C.1 SQLite Generation For BIRD-SQL, we provide straightforward instruction, serialised database, and questions concatenated with evidence. You are an AI assistant helping data analyst (cid:44) write SQL queries to answer questions. (cid:44) Below will provide DB schema with (cid:44) example values and question that can (cid:44) be answered by querying the provided DB (cid:44) . You will then write out your thought (cid:44) process in detail followed by single (cid:44) SQL query enclosed in sql ... (cid:44) that answers the question. SQLite database schema: Table: alignment : id : integer, primary key, example values: ( 1 (cid:44) , 2 , 3 ) alignment : text, example values: ( Good , (cid:44) Bad , Neutral ) Table: attribute : id : integer, primary key, example values: ( 1 (cid:44) , 2 , 3 ) attribute_name : text, example values: ( (cid:44) Intelligence , Strength , Speed ) Table: colour : id : integer, primary key, example values: ( (cid:44) , 2 , 3 ) colour : text, example values: ( No Colour , (cid:44) Amber , Auburn ) Table: gender : id : integer, primary key, example values: ( 1 (cid:44) , 2 , 3 ) gender : text, example values: ( Male , (cid:44) Female , N/A ) Table: publisher : id : integer, primary key, example values: ( 1 (cid:44) , 2 , 3 ) publisher_name : text, example values: ( , (cid:44) ABC Studios , Dark Horse Comics ) Table: race : id : integer, primary key, example values: ( 1 (cid:44) , 2 , 3 ) race : text, example values: ( - , Alien , (cid:44) Alpha ) Table: superhero : id : integer, primary key, example values: ( 1 (cid:44) , 2 , 3 ) superhero_name : text, example values: ( 3-D (cid:44) Man , A-Bomb , Abe Sapien ) full_name : text, example values: ( Charles (cid:44) Chandler , Richard Milhouse Jones , (cid:44) Abraham Sapien ) gender_id : integer, foreign key, references (cid:44) gender, example values: ( 1 , 2 , 3 ) eye_colour_id : integer, foreign key, (cid:44) references colour, example values: ( 9 (cid:44) , 33 , 7 ) hair_colour_id : integer, foreign key, 12 (cid:44) references colour, example values: ( 13 (cid:44) , 1 , 4 ) skin_colour_id : integer, foreign key, (cid:44) references colour, example values: ( 1 (cid:44) , 7 , 23 ) race_id : integer, foreign key, references (cid:44) race, example values: ( 1 , 24 , 33 ) publisher_id : integer, foreign key, (cid:44) references publisher, example values: ( (cid:44) 13 , 3 , 4 ) alignment_id : integer, foreign key, (cid:44) references alignment, example values: ( (cid:44) 1 , 2 ) height_cm : integer, example values: ( 188 , (cid:44) 203 , 191 ) - Each pipe operator in pipe syntax consists (cid:44) of the pipe symbol, >, an operator (cid:44) name, and any arguments. - Pipe syntax works anywhere standard syntax (cid:44) is supported: in queries, views, table- (cid:44) valued functions (TVFs), and other (cid:44) contexts. - Pipe syntax can be mixed with standard (cid:44) syntax in the same query. E.g., (cid:44) subqueries can use different syntax (cid:44) from the parent query. - query can start with FROM clause, and (cid:44) pipe operators can optionally be added (cid:44) after the FROM clause. weight_kg : integer, example values: ( 90 , Pipe operators have the following semantic (cid:44) 441 , 65 ) (cid:44) behavior Table: hero_attribute : hero_id : integer, foreign key, references (cid:44) superhero, example values: ( 1 , 2 , 3 (cid:44) ) attribute_id : integer, foreign key, (cid:44) references attribute, example values: ( (cid:44) 1 , 2 , 3 ) attribute_value : integer, example values: ( (cid:44) 80 , 75 , 95 ) Table: superpower : id : integer, primary key, example values: ( 1 (cid:44) , 2 , 3 ) power_name : text, example values: ( Agility (cid:44) , Accelerated Healing , Lantern (cid:44) Power Ring ) Table: hero_power : hero_id : integer, foreign key, references (cid:44) superhero, example values: ( 1 , 2 , 3 (cid:44) ) power_id : integer, foreign key, references (cid:44) superpower, example values: ( 1 , 18 , (cid:44) 26 ) - Each pipe operator performs self-contained (cid:44) operation. - pipe operator consumes the input table (cid:44) passed to it through the pipe symbol, (cid:44) >, and produces new table as output. - pipe operator can reference only columns (cid:44) from its immediate input table. Columns (cid:44) from earlier in the same query arent (cid:44) visible. Inside subqueries, correlated (cid:44) references to outer columns are still (cid:44) allowed. Operator list - SELECT: Produces new table with the listed (cid:44) columns. - EXTEND: Propagates the existing table and (cid:44) adds computed columns. - SET: Replaces the values of columns in the (cid:44) current table. - DROP: Removes listed columns from the (cid:44) current table. - RENAME: Renames specified columns. - AS: Introduces table alias for the input (cid:44) table. - WHERE: Filters the results of the input (cid:44) table. The question: Who is the publisher of Sauron? - LIMIT: Limits the number of rows to return (cid:44) (the publisher refers to publisher_name (cid:44) ; Sauron refers to superhero_name = (cid:44) Sauron)\" (cid:44) in query, with an optional OFFSET (cid:44) clause to skip over rows. - AGGREGATE: Performs aggregation on data C.2 PipeSQL Generation For PipeSQL generation, we provide quite elaborate prompt. It consists of simplified dialect documentation2 with complex examples of transpiled TPC-H queries3 (Transaction Processing Performance Council, 2014). You will be asked to generate SQL using (cid:44) BigQuerys pipe query syntax. # Pipe Query Syntax Pipe syntax has the following key (cid:44) characteristics 2https://github.com/google/zetasql/blob/ master/docs/pipe-syntax.md 3https://github.com/google/zetasql/tree/ master/zetasql/examples/tpch/pipe_queries 13 (cid:44) across groups of rows or the full input (cid:44) table. - DISTINCT: Returns distinct rows from the (cid:44) input table, while preserving table (cid:44) aliases. - ORDER BY: Sorts results by list of (cid:44) expressions. - UNION: Combines the results of the input (cid:44) queries to the left and right of the (cid:44) pipe operator by pairing columns from (cid:44) the results of each query and (cid:44) vertically concatenating them. - INTERSECT: Returns rows that are found in (cid:44) the results of both the input query to (cid:44) the left of the pipe operator and all (cid:44) input queries to the right of the pipe (cid:44) operator. - EXCEPT: Returns rows from the input query to (cid:44) the left of the pipe operator that (cid:44) arent present in any input queries to (cid:44) the right of the pipe operator. - JOIN: Joins rows from the input table with (cid:44) rows from second table provided as an (cid:44) argument. - CALL: Calls table-valued function (TVF), (cid:44) passing the pipe input table as table (cid:44) argument. - WINDOW: Adds columns with the result of (cid:44) computing the function over some window (cid:44) of existing rows - TABLESAMPLE: Selects random sample of rows (cid:44) from the input table. - PIVOT: Rotates rows into columns. - UNPIVOT: Rotates columns into rows. - ASSERT: Evaluates that an expression is true (cid:44) for all input rows, raising an error (cid:44) if not. ## Examples ### 1. Pricing Summary Report Query How can generate pricing summary report (cid:44) that shows, for each combination of (cid:44) return flag and line status, the total (cid:44) quantity shipped, total base price, (cid:44) total discounted price, total charge ( (cid:44) discounted price plus tax), average (cid:44) quantity, average extended price, (cid:44) average discount, and the count of (cid:44) orders? The report should only include (cid:44) line items shipped on or before the (cid:44) date obtained by subtracting 74 days (cid:44) from December 1, 1998, and it should be (cid:44) ordered by return flag and line status (cid:44) in ascending order. sql SELECT l_returnflag, l_linestatus, sum(l_quantity) AS sum_qty, sum(l_extendedprice) AS sum_base_price, sum(l_extendedprice * (1 - l_discount)) AS (cid:44) sum_disc_price, sum(l_extendedprice * (1 - l_discount) * (1 + (cid:44) l_tax)) AS sum_charge, avg(l_quantity) AS avg_qty, avg(l_extendedprice) AS avg_price, avg(l_discount) AS avg_disc, COUNT(*) AS count_order FROM lineitem WHERE l_shipdate <= date_sub(date 1998-12-01, (cid:44) INTERVAL 74 day) GROUP BY l_returnflag, l_linestatus ORDER BY l_returnflag, l_linestatus; ### 2. Minimum Cost Supplier Query Which supplier in the Middle East should (cid:44) select to order parts of size 19 that (cid:44) are of type containing COPPER, (cid:44) based on the lowest available supply (cid:44) cost? If multiple suppliers offer the (cid:44) part at the same minimum cost, want (cid:44) to consider only the top 100 suppliers (cid:44) with the highest account balances. For (cid:44) each supplier, please provide their (cid:44) account balance, name, nation, the part 14 (cid:44) number, manufacturer, address, phone (cid:44) number, and any additional comments, (cid:44) and sort the results by account balance (cid:44) (highest first), then by nation, (cid:44) supplier name, and part number. sql FROM part, supplier, partsupp, nation, region > WHERE p_partkey = ps_partkey AND s_suppkey = ps_suppkey AND p_size = 19 AND p_type LIKE %COPPER AND s_nationkey = n_nationkey AND n_regionkey = r_regionkey AND r_name = MIDDLE EAST AND ps_supplycost = ( FROM partsupp, supplier, nation, region > WHERE p_partkey = ps_partkey AND s_suppkey = ps_suppkey AND s_nationkey = n_nationkey AND n_regionkey = r_regionkey AND r_name = MIDDLE EAST > AGGREGATE min(ps_supplycost)) > SELECT s_acctbal, s_name, n_name, p_partkey, p_mfgr, s_address, s_phone, s_comment > ORDER BY s_acctbal DESC, n_name, s_name, p_partkey > LIMIT 100; ### 3. Order Priority Checking Query Can you help me determine how many orders, (cid:44) placed in the quarter starting June 1, (cid:44) 1997, had at least one lineitem (cid:44) delivered after its committed date? (cid:44) need the results grouped by order (cid:44) priority, with the count of such orders (cid:44) sorted in ascending order by order (cid:44) priority. sql FROM orders > WHERE o_orderdate >= date 1997-06-01 AND o_orderdate < date_add(date (cid:44) 1997-06-01, INTERVAL 3 month) AND EXISTS( FROM lineitem > WHERE l_orderkey = o_orderkey AND l_commitdate < l_receiptdate) > AGGREGATE COUNT(*) AS order_count GROUP AND ORDER BY o_orderpriority; ### 4. Potential Part Promotion Query Which suppliers in Peru supply parts whose (cid:44) names begin with tan and have an (cid:44) excess inventory of these parts-where (cid:44) excess is defined as having available (cid:44) quantity greater than 50% of the total (cid:44) quantity shipped in 1996? Please return (cid:44) the suppliers name and address, (cid:44) sorted in alphabetical order by name. sql FROM supplier, nation > WHERE s_suppkey IN ( FROM partsupp, part > WHERE p_name LIKE tan% > WHERE ps_partkey = p_partkey AND ps_availqty > ( FROM lineitem > WHERE l_partkey = ps_partkey AND l_suppkey = ps_suppkey AND l_shipdate >= date (cid:44) 1996-01-01 AND l_shipdate < date_add( (cid:44) date 1996-01-01, (cid:44) INTERVAL 1 year) > AGGREGATE 0.5 * sum(l_quantity) (cid:44) ) > SELECT ps_suppkey) AND s_nationkey = n_nationkey AND n_name = PERU > SELECT s_name, s_address > ORDER BY s_name; ### 5. Customer Distribution Query Can you generate report that shows the (cid:44) distribution of customers based on the (cid:44) number of orders they have placed? (cid:44) Include customers with zero orders, and (cid:44) make sure to exclude any orders where (cid:44) the comment contains the text unusual (cid:44) packages. The output should list, for (cid:44) each order count, how many customers (cid:44) have that many orders, sorted by the (cid:44) number of customers (in descending (cid:44) order) and then by the order count ( (cid:44) also in descending order). sql FROM customer > LEFT OUTER JOIN orders ON c_custkey = (cid:44) o_custkey AND o_comment NOT LIKE % (cid:44) unusual%packages% > AGGREGATE COUNT(o_orderkey) c_count GROUP BY c_custkey 15 > AGGREGATE COUNT(*) AS custdist GROUP BY c_count > ORDER BY custdist DESC, c_count DESC; ### 6. Discounted Revenue Query Can you calculate the gross discounted revenue (cid:44) for orders where the parts meet any of (cid:44) the following criteria? - Parts of brand Brand#53 contained in SM (cid:44) CASE, SM BOX, SM PACK, or SM PKG (cid:44) , with quantity between 5 and 15 and (cid:44) size between 1 and 5. - Parts of brand Brand#41 contained in MED (cid:44) BAG, MED BOX, MED PKG, or MED (cid:44) PACK, with quantity between 15 and (cid:44) 25 and size between 1 and 10. - Parts of brand Brand#21 contained in LG (cid:44) CASE, LG BOX, LG PACK, or LG PKG (cid:44) , with quantity between 29 and 39 (cid:44) and size between 1 and 15. Additionally, only consider orders that were (cid:44) shipped by air (i.e., with shipping (cid:44) mode of AIR or AIR REG) and were (cid:44) delivered in person. The revenue should (cid:44) be computed as the sum of (cid:44) l_extendedprice * (1 - l_discount) for (cid:44) all orders that qualify. sql FROM lineitem, part > WHERE # Added this because optimizer is needed (cid:44) to pull this out of the OR. p_partkey = l_partkey AND ( ( p_partkey = l_partkey AND p_brand = Brand#53 and p_container in (SM CASE, SM BOX (cid:44) , SM PACK, SM PKG) AND l_quantity >= 5 AND l_quantity <= 5 + 10 AND p_size BETWEEN 1 AND 5 and l_shipmode in (AIR, AIR REG) AND l_shipinstruct = DELIVER IN (cid:44) PERSON) OR ( p_partkey = l_partkey AND p_brand = Brand#41 and p_container in (MED BAG, MED (cid:44) BOX, MED PKG, MED PACK) AND l_quantity >= 15 AND l_quantity <= 15 + 10 AND p_size BETWEEN 1 AND 10 and l_shipmode in (AIR, AIR REG) AND l_shipinstruct = DELIVER IN (cid:44) PERSON) OR ( p_partkey = l_partkey AND p_brand = Brand#21 and p_container in (LG CASE, LG BOX (cid:44) , LG PACK, LG PKG) AND l_quantity >= 29 AND l_quantity <= 29 + 10 AND p_size BETWEEN 1 AND 15 and l_shipmode in (AIR, AIR REG) AND l_shipinstruct = DELIVER IN (cid:44) PERSON)) > AGGREGATE sum(l_extendedprice * (1 - l_discount)) AS (cid:44) revenue; > WHERE s_suppkey = l1.l_suppkey AND o_orderkey = l1.l_orderkey AND o_orderstatus = AND l1.l_receiptdate > l1.l_commitdate AND EXISTS( FROM lineitem l2 > WHERE ### 7. Global Sales Opportunity Query Can you identify the potential global sales l2.l_orderkey = l1.l_orderkey AND l2.l_suppkey <> l1.l_suppkey) (cid:44) opportunities by finding customers from (cid:44) specific regions-where the country (cid:44) code is defined as the first two digits (cid:44) of their phone number (i.e., one of (cid:44) 10, 19, 14, 22, 23, 31, (cid:44) 13) - who have not placed any orders (cid:44) and whose account balance is greater (cid:44) than the average positive account (cid:44) balance for these regions? For each (cid:44) country code, please return the number (cid:44) of such customers and the total account (cid:44) balance, sorting the results by the (cid:44) country code. sql FROM customer > WHERE AND NOT EXISTS( FROM lineitem l3 > WHERE l3.l_orderkey = l1.l_orderkey AND l3.l_suppkey <> l1.l_suppkey AND l3.l_receiptdate > l3. (cid:44) l_commitdate) AND s_nationkey = n_nationkey AND n_name = PERU > AGGREGATE COUNT(*) AS numwait GROUP BY s_name > ORDER BY numwait DESC, s_name > LIMIT 100; substr(c_phone, 1, 2) IN (10, 19, (cid:44) 14, 22, 23, 31, 13) # Task Your task is to generate SQL in BigQuerys AND c_acctbal > ( SELECT avg(c_acctbal) FROM customer WHERE (cid:44) pipe query syntax described above based (cid:44) on questions in natural language and (cid:44) the presented database structure. c_acctbal > 0.00 AND substr(c_phone, 1, 2) IN (10, (cid:44) 19, 14, 22, 23, 31, (cid:44) 13) You will then write out your thought process (cid:44) in detail followed by single SQL (cid:44) query enclosed in sql ... that (cid:44) answers the question. ) AND NOT EXISTS( FROM orders > WHERE o_custkey = c_custkey ) > AGGREGATE COUNT(*) AS numcust, sum(c_acctbal) AS totacctbal GROUP AND ORDER BY substr(c_phone, 1, 2) AS (cid:44) cntrycode; ### 8. Suppliers Who Kept Orders Waiting Query Which suppliers in Peru were solely (cid:44) responsible for delaying shipments in (cid:44) multi-supplier orders with final (cid:44) status of F? For each supplier, count (cid:44) the number of orders where they failed (cid:44) to meet the committed delivery date- (cid:44) while every other supplier on the same (cid:44) order delivered on time. Please list (cid:44) the supplier names along with the count (cid:44) of such delayed orders, ordered from (cid:44) the highest number of delays to the (cid:44) lowest, and show only the top 100 (cid:44) suppliers. sql FROM supplier, lineitem l1, orders, nation BigQuery database schema: song : singer_id [number] (1, 2, 4), title [ (cid:44) text] (Do They Know Its Christmas, (cid:44) F**k It (I Dont Want You Back), Cha (cid:44) Cha Slide), song_id [number] (1, 2, 3) (cid:44) , sales [float] (1094000.0, 552407.0, (cid:44) 300000.0), highest_position [float] (cid:44) (1.0, 3.0) singer : birth_year [float (cid:44) ] (1944.0, 1948.0, 1949.0), citizenship (cid:44) [text] (France, United States, (cid:44) Chile), name [text] (Liliane (cid:44) Bettencourt, Christy Walton, Alice (cid:44) Walton), singer_id [number] (1, 2, 3), (cid:44) net_worth_millions [float] (30.0, (cid:44) 28.8, 26.3) The question: List the name of singers in (cid:44) ascending order of net worth. C.3 Error Classification The prompt used for Section 5 simply outlines the taxonomy described in Appendix B.4. You are provided with the following (cid:44) information: - **Natural Language Question:** question (cid:44) posed by user. 16 - **Predicted SQL Query:** SQL query generated (cid:44) by text-to-SQL model. - **Execution Results of Predicted Query:** (cid:44) The results of executing the predicted (cid:44) query (or an error message). - **Gold Standard SQL Query:** The correct SQL (cid:44) query. - **Gold Standard Query Execution Results:** (cid:44) Execution results of the correct (gold (cid:44) standard) SQL query. Analyze the predicted SQL query and determine (cid:44) the type of error based on provided (cid:44) execution results and query structures. (cid:44) Classify the error into one of the (cid:44) following categories: - **Dialect:** Issues due to differences (cid:44) between SQL dialects. - **Schema Linking:** Incorrect matching of (cid:44) natural language terms to schema (cid:44) elements (e.g., hallucinated or (cid:44) incorrect columns or tables). - **Data Type:** Issues arising from (cid:44) mismatches or unexpected data types (cid:44) within the database. - **Aggregation:** Incorrect use or omission (cid:44) of aggregation functions (e.g., COUNT, (cid:44) SUM), leading to inaccurate (cid:44) summarization. - **Logical Form and Condition:** Incorrect (cid:44) logical query structures, missing (cid:44) conditions, inappropriate filters, or (cid:44) incorrect ordering logic. - **Table Joins:** Incorrect or missing table (cid:44) joins, irrelevant tables, or (cid:44) misidentified join conditions. - **Projection:** Overall correct queries with (cid:44) incorrect columns selected. - **Other:** Does not fit into any of the (cid:44) categories above. Provide your analysis in the following format: json { \"error_category\": \"Projection\" \"Dialect\" (cid:44) \"Schema Linking\" \"Logical Form and (cid:44) Condition\" \"Data Type\" \" (cid:44) Aggregation\" \"Table Joins\" \"Other (cid:44) \", \"reasoning\": \"Brief explanation supporting (cid:44) your classification.\" }"
        }
    ],
    "affiliations": [
        "Snowflake AI Research"
    ]
}