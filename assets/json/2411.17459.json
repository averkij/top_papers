{
    "paper_title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent Video Diffusion Model",
    "authors": [
        "Zongjian Li",
        "Bin Lin",
        "Yang Ye",
        "Liuhan Chen",
        "Xinhua Cheng",
        "Shenghai Yuan",
        "Li Yuan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional latent space, becoming a key component of most Latent Video Diffusion Models (LVDMs) to reduce model training costs. However, as the resolution and duration of generated videos increase, the encoding cost of Video VAEs becomes a limiting bottleneck in training LVDMs. Moreover, the block-wise inference method adopted by most LVDMs can lead to discontinuities of latent space when processing long-duration videos. The key to addressing the computational bottleneck lies in decomposing videos into distinct components and efficiently encoding the critical information. Wavelet transform can decompose videos into multiple frequency-domain components and improve the efficiency significantly, we thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages multi-level wavelet transform to facilitate low-frequency energy flow into latent representation. Furthermore, we introduce a method called Causal Cache, which maintains the integrity of latent space during block-wise inference. Compared to state-of-the-art video VAEs, WF-VAE demonstrates superior performance in both PSNR and LPIPS metrics, achieving 2x higher throughput and 4x lower memory consumption while maintaining competitive reconstruction quality. Our code and models are available at https://github.com/PKU-YuanGroup/WF-VAE."
        },
        {
            "title": "Start",
            "content": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent Video Diffusion Model Zongjian Li1,3,*, Bin Lin1,3,*, Yang Ye1,3, Liuhan Chen1,3, Xinhua Cheng1,3, Shenghai Yuan1,3, Li Yuan1,2, 1Peking University, 2Peng Cheng Laboratory, 3Rabbitpre Intelligence 4 2 0 2 7 2 ] . [ 2 9 5 4 7 1 . 1 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Video Variational Autoencoder (VAE) encodes videos into low-dimensional latent space, becoming key component of most Latent Video Diffusion Models (LVDMs) to reduce model training costs. However, as the resolution and duration of generated videos increase, the encoding cost of Video VAEs becomes limiting bottleneck in training LVDMs. Moreover, the block-wise inference method adopted by most LVDMs can lead to discontinuities of latent space when processing long-duration videos. The key to addressing the computational bottleneck lies in decomposing videos into distinct components and efficiently encoding the critical information. Wavelet transform can decompose videos into multiple frequency-domain components and improve the efficiency significantly, we thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages multi-level wavelet transform to facilitate lowfrequency energy flow into latent representation. Furthermore, we introduce method called Causal Cache, which maintains the integrity of latent space during block-wise inference. Compared to state-of-the-art video VAEs, WFVAE demonstrates superior performance in both PSNR and LPIPS metrics, achieving 2 higher throughput and 4 lower memory consumption while maintaining competitive reconstruction quality. Our code and models are available at https://github.com/PKU-YuanGroup/WF-VAE. 1. Introduction The release of Sora [5], video generation model developed by OpenAI, has pushed the boundaries of synthesizing photorealistic videos, drawing unprecedented attention to the field of video generation. Recent advancements in Latent Video Diffusion Models (LVDMs), such as OpenSora Plan [19], Open-Sora [45], CogVideoX [39], EasyAnimate [38], Movie Gen [25], and Allegro [46], have led to substantial improvements in video generation quality. *Equal contribution Corresponding author. Figure 1. Performance comparison of video VAEs. Bubble area indicates the memory usage during inference. All measurements are conducted on 33 frames with 256256 resolution videos. Chn represents the number of latent channels. Higher PSNR and throughput indicate better performance. These methods establish compressed latent space [27] using pretrained video Variational Autoencoder (VAE), where the generative performance is fundamentally determined by the compression quality. Current video VAEs remain constrained by fully convolutional architectures inherited from image era. They attempt to address video flickering and redundant information by incorporating spatio-temporal interaction layers and spatio-temporal compression layers. Several recent works, including OD-VAE [6, 19], CogVideoX [39], CV-VAE [44], and Allegro [46] adopt dense 3D structure to achieve highquality video compression. While these methods demonstrate impressive reconstruction performance, they require In conprohibitively intensive computational resources. trast, alternative approaches such as Movie Gen [25], and Open-Sora [45] utilize 2+1D architecture, resulting in reduced computational requirements at the cost of lower reconstruction quality. Previous architectures have not fully exploited video temporal redundancy, leading to requirements for redundant spatio-temporal interaction layers to enhance video compression quality. Many attempts employ block-wise inference strategies to trade computation time for memory, thus addressing computational bottlenecks in processing high-resolution, longduration videos. EasyAnimate [38] introduced Slice VAE to encode and decode video frames in groups, but this approach leads to discontinuous output videos. Several methods, including Open-Sora [45], Open-Sora Plan [19], Allegro [46], and Movie Gen [25], implement tiling inference strategies, but often produce spatio-temporal artifacts in overlapping regions. Although CogVideoX [39] employ caching to ensure convolution continuity, its reliance on group normalization [36] disrupts the independence of temporal feature, thus preventing lossless block-wise inference. These limitations highlight two core challenges faced by video VAEs: (1) excessive computational demands due to redundant architecture, and (2) compromised latent space integrity resulting from existing tiling inference strategies, which cause artifacts and flickering in reconstructed videos. Wavelet transform [23, 26] decomposes videos into multiple frequency-domain components. This decomposition enables prioritization strategies for encoding crucial video components. In this work, we propose Wavelet Flow VAE (WF-VAE), novel autoencoder that utilizes multi-level wavelet transforms for extracting multi-scale pyramidal features and establishes main energy flow pathway for these features to flow into latent representation. This pathway bypasses low-frequency video information to latent space, skipping the backbone network. Our WF-VAE enables simplified backbone design with reduced 3D convolutions, significantly reducing computational costs. To address the potential latent space disruption, we propose Causal Cache mechanism. This approach leverages the properties of causal convolution and maintains the continuity of the convolution sliding window through caching strategy, which ensuring numerical identity between block-wise inference and direct inference results. Experimental results show that WF-VAE achieves state-of-the-art performance in both reconstruction quality and computational efficiency. To summarize, major contributions of our work include: We propose WF-VAE, which leverages multi-level wavelet transforms to extract pyramidal features and establishes main energy flow pathway for video information flow into latent representation. We introduce lossless block-wise inference mechanism called Causal Cache which maintains identical performance as direct inference across videos of any duration. Extensive experimental evaluations of video reconstruction and generation demonstrate that WF-VAE achieves state-of-the-art performance, as validated through comprehensive ablation studies. 2. Related Work [18] Variational Autoencoders. introduced the VAE based on variational inference, establishing novel generative network structure. Subsequent research [24, 30, 34] demonstrated that training and inference in VAE latent space could substantially reduce computational costs for diffusion models. [27] further proposed two-stage image synthesis approach by decoupling perceptual compression from diffusion model. After that, numerous studies explored video VAEs with focus on more efficient video compression including Open-Sora Plan [19], CogVideoX [39], and other models [3, 6, 40, 44, 46, 47]. Current video VAE architectures largely derive from earlier image VAE design [10, 27] and use convolutional backbone. To address the challenges of inference on large videos, current video VAEs employ block-wise inference. Latent Video Diffusion Models. In the early stages of Latent Video Diffusion Models (LVDMs) development, models like AnimateDiff [13], and MagicTime [41, 42] primarily utilized the U-Net backbone [28] for denoising, without temporal compression in VAEs. Following the paradigm introduced by Sora, recent open-source models such as Open-Sora [45], Open-Sora Plan [19, 20], and CogVideoX [39] have adopted DiT backbone with spatiotemporally compressed VAE. Some methods including Open-Sora [45] and Movie Gen [25], employ 2.5D design in either the DiT backbone or the VAE to reduce training costs. For LVDMs, the upper limit of video generation quality is largely determined by the VAEs reconstruction quality. As overhead increase rapidly in scale, optimizing the VAE becomes crucial for handling large-scale data and enabling extensive pre-training. 3. Method 3.1. Wavelet Transform Preliminary. The Haar wavelet transform [23, 26], fundamental form of wavelet transform, is widely used in signal processing [11, 12, 15]. It efficiently captures spatiotemporal information by decomposing signals through two complementary filters. The first is the Haar scaling filter = 1 [1, 1], which acts as low-pass filter that captures 2 the average or approximation coefficients. The second is the Haar wavelet filter = 1 [1, 1], which functions as 2 high-pass filter that extracts the detail coefficients. These orthogonal filters are designed to be simple yet effective, with the scaling filter smoothing the signal and the wavelet filter detecting local changes or discontinuities. Multi-level Wavelet Transform. For video signal Rcthw, where c, t, h, and denote the number of chanFigure 2. Overview of WF-VAE. Our architecture consists of backbone and main energy flow pathway. The pathway functions as highway for main flow of video energy, channeling this energy into the backbone through concatenations, allowing more critical video information to be preserved in the latent representation. nels, temporal frames, height, and width respectively, the 3D Haar wavelet transform at layer is defined as: S(l) ijk = S(l1) (fi fj fk), (1) hhg, S(l) where fi, fj, fk {h, g} represent the filters applied along each dimension, and represents the convolution operation. The transform begins with S(0) = V, and layers, S(l) = S(l1) for subsequent indicating that hhh , each layer operates on the low-frequency component from the previous layer. At each decomposition layer l, the transform produces eight sub-band components: (l) = hhh, S(l) hgg, S(l) {S(l) ggg}. Here, S(l) hhh represents the low-frequency component across all dimensions, while S(l) ggg captures high-frequency details. To implement different downsampling rates in the temporal and spatial dimensions, combination of 2D and 3D wavelet transforms can be implemented. Specifically, to obtain compression rate of 488 (temporalheightwidth), we can employ combination of two-layer 3D wavelet transform followed by one-layer 2D wavelet transform. hgh, S(l) ghh, S(l) ggh, S(l) ghg, S(l) 3.2. Architecture Design of WF-VAE Through analyzing different sub-bands, we found that video energy is mainly concentrated in the low-frequency subband S(1) hhh. Based on this observation, we establish an energy flow pathway outside the backbone, as show in Fig. 2, so that low-frequency information can smoothly flow from video to latent representation during encoding process, and then flow back to the video during the decoding process. This would inherently allow the model to attention more on low-frequency information, and apply higher compression rates to high-frequency information. With the additional path, we can reduce the computational cost brought by the dense 3D convolutions in the backbone. Specifically, given video V, we apply multi-level wavelet transform to obtain pyramid features (1), (2) and (3). We utilize (1) as the input for the encoder and the target output for the decoder. The convolutional backbone and multi-level wavelet transform layer downsample the feature maps simultaneously at every downsampling layer, enabling the concatenation of features from two branches in the backbone. We employ Inflow Block to transform the channel numbers of (2) and (3) to Cf low, which are then concatenated with feature maps from backbone. We compare Cf low to the width of the energy flow pathway, as analyzed in Sec. 4.4. On the decoder side, we maintain structure symmetrical to the encoder. We extract feature maps with Cf low channels from the backbone and process them through Outflow Block to obtain ˆW (2), ˆW (3). In order to allow information to flow from the lower layer to the hhh sub-band of the next layer, we have: hhh = IW ( ˆW (3)) + ˆS(2) ˆS(2) outf low,hhh. Similarly, at the decoder output layer: hhh = IW ( ˆW (2)) + ˆS(1) ˆS(1) outf low,hhh. (2) (3) Overall, we create shortcuts for low-frequency information, ensuring its greater emphasis in latent representation. 3.3. Causal Cache (a) Illustration of Casual Cache. (b) Qualitative comparison of tiling inference and Causal Cache Figure 3. (a) Causal Cache with temporal kernel size of 3 and stride 1. (b) Comparison of tiling inference and Causal Cache, highlighting how tiling causes locally color and shape distortions at overlaps, leading to global flickering in reconstructed videos. We replace regular 3D convolutions with causal 3D convolutions [40] in WF-VAE. The causal convolution applies kt 1 temporal padding at the start with kernel size kt. This padding strategy ensures the first frame remains independent from subsequent frames, thus enabling the processing of images and videos within unified architecture. Furthermore, we leverage the causal convolution properties to achieve lossless inference. We first extract the initial frame from video with frames. The remaining 1 frames are then partitioned into temporal chunks, where Tchunk represents the chunk size. Let st denote the temporal convolutional stride, and = 0, 1, 2, represents the chunk block index. To maintain the continuity of convolution sliding windows, each chunk caches its tail frames for the next chunk. The number of cached frames is given by: Tcache(m) = kt + mTchunk st mTchunk st + 1. (4) For example, kt = 3, st = 1, Tchunk = 4, the equation yields Tcache(m) = 2, as illustrated in Fig. 9 (a). Similarly, with kt = 3, st = 2, Tchunk = 4, we obtain Tcache(m) = 1, indicating only the last frame requires to be cached. Special cases exist, such as when kt = 4, st = 3, Tchunk = 4, which results in Tcache(m) = (m mod 3 + 1). Fig. 9(b) provides qualitative comparison between Causal Cache and the tiling strategy, illustrating how it effectively mitigates significant distortions in both color and shape. 3.4. Training Objective Following the training strategies of [9, 27], our loss function combines multiple components, including reconstruction loss (comprising L1 and perceptual loss [43]), adversarial loss, and KL regularization [18]. Our model is characterized by low-frequency energy flow and symmetry between the encoder and decoder. To maintain this architectural principle, we introduce regularization term denoted as LWL (WL loss), which enforces structural consistency by penalizing deviations from the intended energy flow: LWL = ˆW (2) (2) + ˆW (3) (3). (5) The final loss function is formulated as: = Lrecon + λadvLadv + λKLLKL + λWLLWL. (6) We examine the impact of the weighting factor λwl in Sec. 4.4. Following [9], we implement dynamic adversarial loss weighting to balance the relative gradient magnitudes between adversarial and reconstruction losses: λadv = 1 2 (cid:18) GL[Lrecon] GL [Ladv] + δ (cid:19) , (7) where GL[] denotes the gradient with respect to last layer of decoder, and δ = 106 is used for numerical stability. 4. Experiments 4.1. Experimental Setup Baseline Models. To assess the effectiveness of WFVAE, we perform comprehensive evaluation, comparing its performance and efficiency against several state-of-theart VAE models. The models considered are: (1) ODVAE [6], 3D causal convolutional VAE used in OpenSora Plan 1.2 [19]; (2) Open-Sora VAE [45]; (3) CVVAE [44]; (4) CogVideoX VAE [39]; (5) Allegro VAE [46]; (6) SVD-VAE [4], which does not compress temporally and (7) SD-VAE [27], widely used image VAE. Among these, CogVideoX VAE has latent dimension of 16, while others use latent dimension of 4. Notably, all VAEs except CVVAE and SD-VAE, have been validated on LVDMs previously, making them highly representative for comparison. Dataset & Evaluation. We utilize the Kinetics-400 dataset [16] for both training and validation. For testing, we employ the Panda70M [7] and WebVid-10M [2] datasets. To comprehensively evaluate the models reconstruction performance, we select Peak Signal-to-Noise Ratio (PSNR) [14], Learned Perceptual Image Patch Similarity (LPIPS) [43], and Structural Similarity Index Measure (SSIM) [35] as primary evaluation metrics. Additionally, we use Frechet Video Distance (FVD) [33] to assess visual quality and temporal coherence. To assess our models performance in generating results with the diffusion model, we Method TCPR Chn WebVid-10M Panda-70M PSNR () SSIM() LPIPS () FVD () PSNR () SSIM () LPIPS() FVD () SD-VAE[27] SVD-VAE[4] CV-VAE[44] OD-VAE[6] 64(1 8 8) 64(1 8 8) 256(4 8 8) 256(4 8 8) Open-Sora VAE [45] 256(4 8 8) Allegro [46] 256(4 8 8) WF-VAE-S (Ours) 256(4 8 8) WF-VAE-L (Ours) 256(4 8 8) CogVideoX-VAE[39] 256(4 8 8) WF-VAE-L (Ours) 256(4 8 8) 4 4 4 4 4 4 16 16 30.19 31.18 30.76 30. 31.14 32.18 31.39 32.32 35.72 35.76 0.8377 0. 0.8566 0.8635 0.8572 0.8963 0.8737 0.8920 0. 0.9430 0.0568 0.0546 0.0803 0.0553 0. 0.0524 0.0517 0.0513 0.0277 0.0230 284.90 188.74 369. 255.92 475.23 209.68 188.04 186.00 59.83 54.36 30. 31.04 30.18 30.31 31.37 31.70 31.27 32. 35.79 35.87 0.8896 0.9059 0.8796 0.8935 0.8973 0. 0.9025 0.9142 0.9527 0.9538 0.0395 0.0379 0. 0.0439 0.0662 0.0421 0.0420 0.0411 0.0198 0.0175 182. 137.67 296.28 191.23 298.47 172.72 146.91 146. 43.23 39.40 Table 1. Quantitative metrics of reconstruction performance. Results demostrate that WF-VAE achieves state-of-the-art on reconstrcution performance comparing with other VAEs on WebVid-10M[2] and Panda70M[7] datasets. TCPR represents the token compression rate and Chn indicates the number of latent channels. The highest result is highlighted in bold, and the second highest result is underlined. Figure 4. Computational performance of encoding and decoding. We evaluate the encoding and decoding time and memory consumption across 33 frames with 256256, 512512, and 768768 resolutions (benchmark models without causal convolution are tested with 32 frames). WF-VAE surpasses other VAE models by large margin on both inference speed and memory efficiency. utilize the UCF-101 [32] and SkyTimelapse [37] datasets for conditional and unconditional training 100,000 steps. Following [22, 31], we extract 16-frame clips of 2,048 videos to compute FVD16. Additionally, we evaluate the Inception Score (IS) [29] exclusively on the UCF-101 dataset, as suggested by [22]. We select Latte-L [22] as the denoiser. Since our primary focus is not on the final generative performance but rather on whether the latent spaces of various video VAEs facilitate effective diffusion model training, we chose not to use the higher-performing Latte-XL. Training Strategy. We employ the AdamW [17, 21] optimizer with parameters β1 = 0.9 and β2 = 0.999, and set fixed learning rate of 1 105. Our training process comprises three stages: (I) the first stage aligns with [6], where we preprocess videos to 25 frames with 256 256 resolution, and total batch size of 8. (II) we refresh the discriminator, increase the number of frames to 49, and reduce the FPS by half to enhance motion dynamics. (III) we observe that large λlpips significantly affects video stability; therefore, we refresh the discriminator once more and set λlpips to 0.1. All three stages employ L1 loss: the initial stage is trained for 800,000 steps, while the subsequent stages are each trained for 200,000 steps. The training process utilizes 8 NVIDIA H100 GPUs. We implement 3D discriminator and initiate GAN training from the start. All training hyperparameters are detailed in the appendix. 4.2. Comparison With Baseline Models We compare WF-VAE with baseline models in three key aspects: computational efficiency, reconstruction performance, and performance in diffusion-based generation. To ensure fairness in comparing metrics and model efficiency, we disable block-wise inference strategies across all VAEs. Computational Efficiency. The computational efficiency evaluations are conducted using an H100 GPU with float32 precision. Performance evaluations are performed at 33 frames across multiple input resolutions. Allegro VAE is evaluated using 32-frame videos to maintain consistency due to its non-causal convolution architecture. For fair comparison, all benchmark VAEs do not employ block-wise inference strategies and are directly inferred. As shown in Fig. 4, WF-VAE demonstrates superior inference performance compared to other VAEs. For instance, WF-VAEL requires 7170 MB of memory for encoding video at Figure 5. Qualitative comparison of reconstruction performance. We select two scenarios to comprehensively evaluate the visual quality of videos reconstructed by existing VAEs. Top: scenario contains rich details. Bottom: scenario contains fast motion. 512512 resolution, whereas OD-VAE demands approximately 31944 MB (445% higher). Similarly, CogVideoX consumes around 35849.33 MB (499% higher), and Allegro VAE requires 55664 MB (776% higher). These results highlight WF-VAEs significant advantages in largeIn terms of encoding scale training and data processing. speed, WF-VAE-L achieves an encoding time of 0.0513 seconds, while OD-VAE, CogVideoX, and Allegro VAE exhibit encoding times of 0.0945 seconds, 0.1810 seconds, and 0.3731 seconds, respectively, which are approximately 184%, 352%, and 727% slower. From Fig. 4, it can also be observed that WF-VAE has significant advantages in decoding computational efficiency. Video Reconstruction Performance. We present quantitative comparison of reconstruction performance between WF-VAE and baseline models in Tab. 1 and qualitative reconstruction results in Fig. 5. As shown, despite having the lowest computational cost, WF-VAE-S outperforms popular open-source video VAEs such as OD-VAE [6] and OpenSora VAE [45] on both datasets. When increasing the model complexity, WF-VAE-L competes well with Allegro [46], outperforming it in PSNR, LPIPS and FVD, but slightly lagging in SSIM. However, WF-VAE-L is significantly more computationally efficient than Allegro. Additionally, we compare WF-VAE-L with CogVideoX [39] using 16 latent channels. Except for slightly lower SSIM on the Webvid10M, WF-VAE-L outperform CogVideoX across all other metrics. These results indicate that WF-VAE achieves competitive reconstruction performance compared to state-ofthe-art open-source video VAEs, while substantially improving computational efficiency. Video Generation Evaluation. Fig. 6 and Tab. 2 qual-"
        },
        {
            "title": "SkyTimelapse",
            "content": "UCF101 FVD16 FVD16 IS Allegro [46] OD-VAE [6] WF-VAE-S (Ours) WF-VAE-L (Ours) CogVideoX [39] WF-VAE-L (Ours) 4 4 4 16 16 117.28 130.79 103.44 113.67 109.20 108. 1045.66 1109.87 1005.10 929.55 1117.57 947.18 67.16 58. 65.89 70.53 57.47 71.86 Table 2. Quantitative evaluation of different VAE models for video generation. We assess video generation quality using FVD16 on both SkyTimelapse and UCF-101 datasets, and IS on UCF-101 following prior work [22]. Figure 6. Generated videos using WF-VAE with Latte-L. Top: results trained with the SkyTimelapse dataset. Bottom: results trained with the UCF-101 dataset. (a) Number of latent channels. (b) WL Loss weights λWL. (c) Number of energy flow path channels Cf low. Figure 7. Training dynamics under different settings. itatively and quantitatively demonstrate the video generation results of the diffusion model using WF-VAE. WFVAE achieves the best performance in terms of FVD and IS metrics. For instance, in the SkyTimelapse dataset, among models with 4 latent channels, WF-VAE-S achieves the best FVD score, which is 10.23 lower than WF-VAEL and 27.35 lower than OD-VAE. For models with 16 latent channels, WF-VAE-Ls FVD score is 0.51 lower than CogVideoXs. This might be because the higher dimensionality of the latent space makes convergence more difficult, resulting in slightly inferior performance compared to WFVAE with 4 latent channels under the same training steps. 4.3. Ablation Study Increasing the latent dimension. Recent works [8, 10] show that the number of latent channel significantly impacts reconstruction quality. We conduct experiments with 4, 8, 16, and 32 latent channels. Fig. 7a illustrates that reconstruction performance improves significantly as the number of latent channels increases. However, larger latent channels may increase convergence difficulty in training diffusion model, as evidenced by the results presented in Tab. 2. Exploration of WL loss weight λWL. To ensure structural symmetry in WF-VAE, we introduce WL loss. As shown in Fig. 7b, the model exhibits substantial decrease in PSNR performance when λWL = 0. Our experiments demonstrate optimal results for both PSNR and LPIPS metrics when λWL = 0.1. Furthermore, our analysis of different loss functions reveals that utilizing L1 loss produces superior results compared to L2 loss. Expanding the energy flow path. The low-frequency information flows into the latent representation through the energy flow pathway, and the number of channels in this path, Cf low, determines the intensity of low-frequency information injection into the backbone. We conducted experiments with Cf low values of 64, 128, and 256. As shown in Fig. 7c, we find that when Cf low is 128, it can balance reconstruction and computational performance. CogVideoX [39] implements temporal caching strategy. As demonstrated in Tab. 5, both tiling strategies and conventional caching methods exhibited performance degradation, while Causal Cache achieves lossless inference with performance metrics identical to direct inference. Method Chn BWI Allegro [46] OD-VAE [6] WF-VAE-L (Ours) CogVideoX [39] WF-VAE-L (Ours) 4 4 4 16 Panda70M PSNR LPIPS 31.71 25.31(-6.40) 30.31 28.51(-1.80) 32.10 32.10(0.00) 35.79 35.41(-0.38) 35.87 35.87(0.00) 0.0422 0.1124(+0.0702) 0.0439 0.0552(+0.0113) 0.0411 0.0411(0.0000) 0.0198 0.0218(+0.0020) 0.0175 0.0175(0.0000) Table 5. Quantitative analysis of visual quality degradation induced by block-wise inference. Values in red indicate degradation compared to direct inference, while values in green demonstrate preservation of quality. BWI denotes Block-Wise Inference. Experiments are conducted on 33 frames with 256256 resolution. 5. Conclusion In this paper, we propose WF-VAE, an innovative autoencoder that utilizes multi-level wavelet transform to extract pyramidal features, thus creating primary energy flow pathway for encoding low-frequency video information into latent representation. Additionally, we introduce lossless block-wise inference mechanism called Causal Cache, which completely resolves video flickering associated with prior tiling strategies. Our experiments demonstrate that WF-VAE achieves state-of-the-art reconstruction performance while maintaining low computational costs. WF-VAE significantly reduces the expenses associated with large-scale video pre-training, potentially inspiring future designs of video VAEs. Limitations and Future Work. The initial design of the decoder incorporated insights from [27], employing highly complex structure that resulted in more parameters in the backbone of the decoder compared to the encoder. Although the computational cost remains manageable, we consider these parameters redundant. Consequently, we aim to streamline the model in future work to fully leverage the advantages of our architecture."
        },
        {
            "title": "Model",
            "content": "BC Params (M) Kinetics-"
        },
        {
            "title": "Dec",
            "content": "PSNR LPIPS WF-VAE-S 128 WF-VAE-M 160 WF-VAE-L 38 58 84 108 164 28.21 28.44 28.66 0.0779 0.0699 0.0661 Table 3. Scalability of WF-VAE. We evaluated PSNR and LPIPS on Kinetics-400 [16]. Reconstruction performance improves as model complexity increases. Increasing the number of base channels. To further exploit the capabilities of the WF-VAE architecture, we increase the model complexity by expanding the number of base channels. The channel dimensionality increases by one base channel width after each downsampling layer, starting from the number of base channels. As shown in Tab. 3, we experiment with three configurations: 128, 160, and 192 base channels. The results demonstrate that the models performance improves as the number of base channels increases. Despite the corresponding rise in model parameters, the computational cost remains comparatively low compared to benchmark models, as shown in Fig. 4. Ablation studies of model architecture. First, We evaluate the effectiveness of the energy pathway by examining its impact when removed from layer 3 alone and both layers 2 and 3. This analysis demonstrates the benefits of incorporating low-frequency video energy into the backbone. Second, we investigate the significance of our proposed WL Loss in regularizing the encoder-decoder Third, we examine the impact of modifying the normalization method to layer normalization [1] for Causal Cache. The results of these ablation studies are shown in Tab. 4."
        },
        {
            "title": "Settings",
            "content": "Kinetics-400 L1 L2 L3 WL Loss NM PSNR LPIPS 0.0737 0.0737 0.0692 0.0690 0.0684 27.85 27.94 27.90 28.21 28."
        },
        {
            "title": "L\nL\nL\nL\nG",
            "content": "Table 4. Ablation studies on model architecture. We evaluate the impact of three key components: energy flow pathways across network layers, WL loss, and normalization methods (L: layer normalization [1], G: group normalization [36]). 4.4. Causal Cache To validate the lossless inference capability of Causal Cache, we compared our approach with existing blockwise inference methods implemented in several opensource LVDMs. OD-VAE [6] and Allegro [46] provide spatio-temporal tiling inference implementations, and"
        },
        {
            "title": "References",
            "content": "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016. 8 [2] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), 2021. 4, 5 [3] Fan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min Zhao, Shilong Liu, Yaole Wang, and Jun Zhu. Vidu: highly consistent, dynamic and skilled text-to-video generator with diffusion models, 2024. 2 [4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets, 2023. 4, 5 [5] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. [6] Liuhan Chen, Zongjian Li, Bin Lin, Bin Zhu, Qian Wang, Shenghai Yuan, Xing Zhou, Xinghua Cheng, and Li Yuan. Od-vae: An omni-dimensional video compressor for imarXiv preprint proving latent video diffusion model. arXiv:2409.01199, 2024. 1, 2, 4, 5, 6, 7, 8 [7] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, and Sergey Tulyakov. Panda-70m: Captioning 70m videos In Proceedings of with multiple cross-modality teachers. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1332013331, 2024. 4, 5 [8] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, Matthew Yu, Abhishek Kadian, Filip Radenovic, Dhruv Mahajan, Kunpeng Li, Yue Zhao, Vladan Petrovic, Kumar Singh, Simran Motwani, Yi Wen, Yiwen Song, Roshan Sumbaly, Vignesh Ramanathan, Zijian He, Peter Vajda, Devi Parikh, Meta Genai, Eric Alamillo, Andres Alvarado, Giri Anantharaman, StuartAn Derson, Snesha Arumugam, Chris Bray, Matt Butler, An-Thony Chen, Lawrence Chen, Jessica Cheng, LaurenCo Hen, Jort Gemmeke, Freddy Gottesman, Nader Hamekasi, Zecheng He, Jiabo Hu, Praveen Krishnan, Carolyn Krol, Tianhe Li, Mo Metanat, Vivek Pai, Guan Pang, Albert Pumarola, Ankit Ramchandani, Stephen Roylance, Kalyan Saladi, Artsiom Sanakoyeu, Dev Satpathy, AlexSchneid Man, Edgar Schoenfeld, Shubho Sengupta, Hardik Shah, Shivani Shah, Yaser Sheikh, Karthik Sivakumar, Lauren Spencer, Fei Sun, Ali Thabet, Mor Tzur, Mike Wang, Mack Ward, Bichen Wu, Seiji Yamamoto, Licheng Yu, Hector Yuen, Luxin Zhang, Yinan Zhao, Jessica Zhong, Connor Finally, Manohar Hayes, Ahmad Paluri, and Al Al. Emu: Enhancing image generation models using photogenic needles in haystack. 7 [9] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming In Protransformers for high-resolution image synthesis. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1287312883, 2021. 4 [10] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis, 2024. 2, 7 [11] Shahaf E. Finder, Roy Amoyal, Eran Treister, and Oren Freifeld. Wavelet convolutions for large receptive fields, 2024. 2 [12] Rinon Gal, Dana Cohen Hochberg, Amit Bermano, and Daniel Cohen-Or. Swagan: style-based wavelet-driven generative model. ACM Transactions on Graphics (TOG), 40(4):111, 2021. [13] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-toInternaimage diffusion models without specific tuning. tional Conference on Learning Representations, 2024. 2 [14] Alain Hore and Djemel Ziou. Image quality metrics: Psnr vs. ssim. In 2010 20th International Conference on Pattern Recognition, 2010. 4 [15] Huaibo Huang, Ran He, Zhenan Sun, and Tieniu Tan. Wavelet-srnet: wavelet-based cnn for multi-scale face super resolution. In Proceedings of the IEEE international conference on computer vision, pages 16891697, 2017. 2 [16] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, and Andrew Zisserman. The kinetics human action video dataset, 2017. 4, 8 [17] DiederikP. Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv: Learning,arXiv: Learning, 2014. 5 [18] DiederikP. Kingma and Max Welling. Auto-encoding variational bayes. arXiv: Machine Learning,arXiv: Machine Learning, 2013. 2, 4 [19] PKU-Yuan Lab and Tuzhan AI etc. Open-sora-plan, 2024. 1, 2, [20] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. 2 [21] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019. 5 [22] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. 5, 7 [23] S.G. Mallat. theory for multiresolution signal decompoIEEE Transactions on sition: Pattern Analysis and Machine Intelligence, page 674693, 1989. 2 the wavelet representation. [24] Gautam Mittal, Jesse Engel, Curtis Hawthorne, and Ian Simon. Symbolic music generation with diffusion models. arXiv: Sound,arXiv: Sound, 2021. 2 [25] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat Singh, Mary Williamson, Matt Le, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Simran Motwani, Tao Xu, Tianhe Li, Tingbo Hou, Wei-Ning Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu, Arun Mallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh Wood, Ce Liu, Cen Peng, Dimitry Vengertsev, Edgar Schonfeld, Elliot Blanchard, Felix Juefei-Xu, Fraylie Nord, Jeff Liang, John Hoffman, Jonas Kohler, Kaolin Fire, Karthik Sivakumar, Lawrence Chen, Licheng Yu, Luya Gao, Markos Georgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li, Simone Parmeggiani, Steve Fine, Tara Fowler, Vladan Petrovic, and Yuming Du. Movie gen: cast of media foundation models, 2024. 1, 2 [26] Aleˇs Prochazka, Lucie Grafova, Oldrich Vyˇsata, and Neurocenter Caregroup. Three-dimensional wavelet transform in multi-dimensional biomedical volume processing. In Proc. of the IASTED International Conference on Graphics and Virtual Reality, Cambridge, page 268, 2011. 2 [27] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 2, 4, 5, 8 [28] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. UNet: Convolutional Networks for Biomedical Image Segmentation, page 234241. 2015. 2 [29] Masaki Saito, Eiichi Matsumoto, and Shunta Saito. Temporal generative adversarial nets with singular value clipping. In 2017 IEEE International Conference on Computer Vision (ICCV), 2017. [30] Abhishek Sinha, Jiaming Song, Chenlin Meng, and Stefano Ermon. D2c: Diffusion-denoising models for few-shot conditional generation. arXiv: Learning,arXiv: Learning, 2021. 2 [31] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. Stylegan-v: continuous video generator with the price, image quality and perks of stylegan2. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 36263636, 2022. 5 [32] Khurram Soomro, Amir Zamir, and Mubarak Shah. Ucf101: dataset of 101 human actions classes from videos in the wild. arXiv: Computer Vision and Pattern Recognition,arXiv: Computer Vision and Pattern Recognition, 2012. 5 [33] Thomas Unterthiner, Sjoerdvan Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Fvd: new metric for video generation. International Conference on Learning Representations,International Conference on Learning Representations, 2019. 4 [34] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space. Neural Information Processing Systems,Neural Information Processing Systems, 2021. 2 [35] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. [36] Yuxin Wu and Kaiming He. Group normalization, 2018. 2, 8 [37] Wei Xiong, Wenhan Luo, Lin Ma, Wei Liu, and Jiebo Luo. Learning to generate time-lapse videos using multi-stage dynamic generative adversarial networks. Cornell University - arXiv,Cornell University - arXiv, 2017. 5 [38] Jiaqi Xu, Xinyi Zou, Kunzhe Huang, Yunkuo Chen, Bo Liu, MengLi Cheng, Xing Shi, and Jun Huang. Easyanimate: high-performance long video generation method based on transformer architecture. arXiv preprint arXiv:2405.18991, 2024. 1, 2 [39] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 1, 2, 4, 5, 6, 7, 8 [40] Lijun Yu, Jose Lezama, Nitesh B. Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, Alexander G. Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa, David A. Ross, and Lu Jiang. Language model beats diffusion tokenizer is key to visual generation, 2024. 2, 4 [41] Shenghai Yuan, Jinfa Huang, Yujun Shi, Yongqi Xu, Ruijie Zhu, Bin Lin, Xinhua Cheng, Li Yuan, and Jiebo Luo. Magictime: Time-lapse video generation models as metamorphic simulators. arXiv preprint arXiv:2404.05014, 2024. 2 [42] Shenghai Yuan, Jinfa Huang, Yongqi Xu, Yaoyang Liu, Shaofeng Zhang, Yujun Shi, Ruijie Zhu, Xinhua Cheng, Jiebo Luo, and Li Yuan. Chronomagic-bench: benchmark for metamorphic evaluation of text-to-time-lapse video generation. arXiv preprint arXiv:2406.18522, 2024. [43] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of In 2018 IEEE/CVF deep features as perceptual metric. Conference on Computer Vision and Pattern Recognition, 2018. 4 [44] Sijie Zhao, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Muyao Niu, Xiaoyu Li, Wenbo Hu, and Ying Shan. Cv-vae: compatible video vae for latent generative video models. arXiv preprint arXiv:2405.20279, 2024. 1, 2, 4, 5 [45] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, 2024. 1, 2, 4, 5, 6 [46] Yuan Zhou, Qiuyue Wang, Yuxuan Cai, and Huan Yang. Allegro: Open the black box of commercial-level video generation model, 2024. 1, 2, 4, 5, 6, 7, 8 [47] Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, HongFa Wang, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, et al. Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment. arXiv preprint arXiv:2310.01852, 2023. 2 WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent Video Diffusion Model"
        },
        {
            "title": "The supplementary materials include further details as",
            "content": "follows: We present additional notations in Sec. 6. We analyze subband energy and entropy of wavelet transform in Sec. 7, which further validates our motivation. We present our training parameters in Sec. 8 We present the derivation of the Causal Cache formulation in Sec. 9. 6. Notations The notations and their descriptions in the paper are shown in Tab. 6. Notations () IW () S(l) (l) Descriptions Wavelet transform Inverse wavelet transform Wavelet subband within layer l, where specifies the type of filtering (high or low pass) applied in three dimensions. The set of all subbands within layer Table 6. Notations symbols and their descriptions. 7. Wavelet Subband Analysis We analyze the energy and entropy distributions across the subbands obtained after wavelet transform. As illustrated in Fig. 8b, the energy and entropy of the video are primarily concentrated in the hhh low-frequency subband. This concentration suggests that low-frequency components carry more significant information and necessitate lower compression rates to ensure superior reconstruction performance. This observation further validates the rationale behind our proposed approach. 8. Training Details The training hyperaparameters are shown in Tab. 7. 9. Derivation of Causal Cache Let us define convolution with sliding window index N0 and chunk index N0. Given convolutional stride and kernel size k, as shown in Fig. 9, the starting and ending frame indices for each sliding window are expressed as follows: (a) Visualization of the eight subbands obtained after wavelet transform of the video. (b) Energy and entropy of each subband. Figure 8. Visualization of the subbands and their respective energy and entropy."
        },
        {
            "title": "Setting",
            "content": "Stage - 800k step Learning Rate Total Batch Size Peceptual(LPIPS) Weight WL Loss Weight (λWL) KL Weight (λKL) Learning Rate Resolution Num Frames EMA Decay Stage II - 200k step"
        },
        {
            "title": "Num Frames",
            "content": "Stage III - 200k step Peceptual(LPIPS) Weight 1e-5 8 1.0 0.1 1e-6 1e-5 256256 25 0.999 49 0.1 Table 7. Training hyperparameters across three stages. For chunk boundaries, we define: tchunk,end(m) = 1 + mTchunk (10) twindow,start(n) = ns, twindow,end(n) = twindow,start(n) + 1. (8) (9) where Tchunk denotes the chunk size. For given chunk index m, the maximum sliding index nmax(m) is determined by the constraint twindow,end(n) tchunk,end(m): Figure 9. Illustration of Causal Cache with parameters k=3, s=2, and chunk size Tchunk=4. nmax(m) = (cid:22) mTchunk (cid:23) + 1 . (11) Consequently, the required cache size Tcache(m) for chunk is: Tcache(m) = tchunk,end(m) twindow,start(nmax(m)) +1 = mTchunk + (cid:22) mTchunk (cid:23) + 1 (12)"
        }
    ],
    "affiliations": [
        "Peking University",
        "Peng Cheng Laboratory",
        "Rabbitpre Intelligence"
    ]
}