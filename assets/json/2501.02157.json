{
    "paper_title": "Personalized Graph-Based Retrieval for Large Language Models",
    "authors": [
        "Steven Au",
        "Cameron J. Dimacali",
        "Ojasmitha Pedirappagari",
        "Namyong Park",
        "Franck Dernoncourt",
        "Yu Wang",
        "Nikos Kanakaris",
        "Hanieh Deilamsalehy",
        "Ryan A. Rossi",
        "Nesreen K. Ahmed"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As large language models (LLMs) evolve, their ability to deliver personalized and context-aware responses offers transformative potential for improving user experiences. Existing personalization approaches, however, often rely solely on user history to augment the prompt, limiting their effectiveness in generating tailored outputs, especially in cold-start scenarios with sparse data. To address these limitations, we propose Personalized Graph-based Retrieval-Augmented Generation (PGraphRAG), a framework that leverages user-centric knowledge graphs to enrich personalization. By directly integrating structured user knowledge into the retrieval process and augmenting prompts with user-relevant context, PGraphRAG enhances contextual understanding and output quality. We also introduce the Personalized Graph-based Benchmark for Text Generation, designed to evaluate personalized text generation tasks in real-world settings where user history is sparse or unavailable. Experimental results show that PGraphRAG significantly outperforms state-of-the-art personalization methods across diverse tasks, demonstrating the unique advantages of graph-based retrieval for personalization."
        },
        {
            "title": "Start",
            "content": "Personalized Graph-Based Retrieval for Large Language Models Steven Au1, Cameron J. Dimacali1, Ojasmitha Pedirappagari1, Namyong Park 2, Franck Dernoncourt3, Yu Wang4, Nikos Kanakaris5, Hanieh Deilamsalehy3 Ryan A. Rossi3, Nesreen K. Ahmed6 1University of California Santa Cruz, 2Meta AI 3Adobe Research, 4University of Oregon, 5University of Southern California, 6Cisco AI Research 5 2 0 2 4 ] . [ 1 7 5 1 2 0 . 1 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "As large language models (LLMs) evolve, their ability to deliver personalized and contextaware responses offers transformative potential for improving user experiences. Existing personalization approaches, however, often rely solely on user history to augment the prompt, limiting their effectiveness in generating tailored outputs, especially in coldstart scenarios with sparse data. To address these limitations, we propose Personalized Graph-based Retrieval-Augmented Generation (PGraphRAG), framework that leverages usercentric knowledge graphs to enrich personalization. By directly integrating structured user knowledge into the retrieval process and augmenting prompts with user-relevant context, PGraphRAG enhances contextual understanding and output quality. We also introduce the Personalized Graph-based Benchmark for Text Generation, designed to evaluate personalized text generation tasks in real-world settings where user history is sparse or unavailable. Experimental results show that PGraphRAG significantly outperforms state-of-the-art personalization methods across diverse tasks, demonstrating the unique advantages of graph-based retrieval for personalization."
        },
        {
            "title": "Introduction",
            "content": "The recent development of large language models (LLMs) has unlocked numerous applications in natural language processing (NLP), including advanced conversational agents, automated content creation, and code generation. For instance, models like GPT-4 (OpenAI, 2024) have been employed to power virtual assistants capable of answering complex queries, summarizing lengthy documents, and engaging in human-like conversations. These advancements highlight the transformative potential of LLMs to automate and enhance tasks across various domains (Brown et al., 2020). As LLMs continue to evolve, their ability to deliver highly personalized and context-aware Figure 1: Overview of the proposed personalized graph-based retrieval-augmented generation framework, PGraphRAG. We first construct user-centric graphs from user history and interactions. Then, the resulting structured data is utilized for retrieval. The retrieved information is provided to the language models for context in generating text tailored to user i. responses opens new possibilities for transforming user experiences (Salemi et al., 2024b). Personalization enables these models to adapt outputs to individual preferences, contexts, and goals, fostering richer and more meaningful interactions (Huang et al., 2022). For example, personalized text generation allows AI systems to provide responses that are more relevant, contextually appropriate, and aligned with the style and preferences of individual users (Zhang et al., 2024)."
        },
        {
            "title": "The",
            "content": "concept of personalization is wellestablished in AI and has been extensively explored across various fields, including information retrieval, human-computer interaction (HCI), In information reand recommender systems. trieval, personalization techniques are employed to tailor search results based on user profiles and past interactions, enhancing the relevance of retrieved documents (Xue et al., 2009). HCI research has focused on creating adaptive user interfaces and interactions that cater to individual needs, improving usability and accessibility (Fowler et al., 2015). Recommender systems utilize personalization to suggest products, services, or content that match user interests, driving engagement in applications ranging from e-commerce to entertainment (Naumov et al., 2019; Lyu et al., 2024a). Despite the widespread acknowledgment of the importance of personalization in these domains, the development and evaluation of large language models (LLMs) for generating personalized responses remain relatively understudied. One of the key challenges in advancing personalized LLMs is the lack of suitable benchmarks that adequately capture personalization tasks. Popular natural language processing (NLP) benchmarks (e.g., (Wang et al., 2019b), (Wang et al., 2019a), (Gehrmann et al., 2021)) primarily focus on general language understanding and generation capabilities, with limited emphasis on personalization aspects. As result, researchers and practitioners lack standardized datasets and evaluation metrics to develop and assess models designed for personalized text generation. Recently, some efforts have been made towards personalized LLM benchmarks. The LaMP benchmark offers comprehensive evaluation framework focusing on personalized text classification and generation including email subject generation, news headline generation, paper title generation, product rating and movie tagging (Salemi et al., 2024b). LongLaMP extended this scope with four tasks emphasizing long text generation, such as email completion and paper abstract generation (Kumar et al., 2024). Unfortunately, these recently developed personalized LLM benchmarks rely exclusively on user history to model personalization. While user history is undoubtedly valuable for capturing users preferences and behaviors, this approach has significant limitations. In scenarios where user data is sparse or entirely unavailable such as with new users in cold-start situations models that depend solely on user history fail to generate personalized outputs effectively. This dependency restricts the applicability of such benchmarks in evaluating personalized LLMs for real-world use cases, where the availability and quality of user history can vary greatly. For example, Figure 2 shows the user profile distribution for Amazon user-product reviews (Ni and McAuley, 2018) where 99.99% of users have only one or two reviews in their profile. Interestingly, other personalized LLM benchmarks such as LaMP and LongLaMP limited their datasets to users with sufficient profile size. To address these challenges, we propose Personalized Graph-based Retrieval-Augmented GeneraFigure 2: The user profile distribution for Amazon userproduct dataset which highlights how most users have small profile size with few reviews. The red vertical line marks the minimum profile size in other benchmarks (e.g., LaMP, LongLaMP). tion (PGraphRAG), novel framework that leverages user-centric knowledge represented as structured graphs to enhance personalized text generation. By incorporating user-centric knowledge graphs directly into the retrieval process and augmenting the generation context or prompt with structured user-specific information, PGraphRAG provides richer and more comprehensive understanding of the users context, preferences, and relationships (see Figure 1 for an overview of the framework). This approach transcends the limitations of relying solely on user history by integrating diverse and structured user knowledge, enabling the model to generate more accurate and personalized responses even when user history is sparse or unavailable. The use of structured graphs allows PGraphRAG to represent complex user information, such as interests and past interactions, in structured and interconnected manner. By augmenting the prompt with this structured knowledge during the generation process, PGraphRAG facilitates more effective retrieval and integration of relevant user-centric information, significantly enhancing the models ability to produce contextually appropriate and personalized outputs. In cold-start scenarios, where traditional models fail due to the lack of user history, PGraphRAG leverages available structured knowledge to deliver meaningful personalization. To evaluate our approach, we introduce the Personalized Graph-based Benchmark for Text Generation, novel evaluation benchmark designed to fine-tune and assess LLMs on twelve personalized text generation tasks including long and short text generation, as well as classification. This benchmark addresses the limitations of existing personalized LLM benchmarks by providing datasets that specifically target personalization capabilities in real-world settings where user history is sparse. In addition, the benchmark enables more comprehensive assessment of models ability to personalize outputs based on structured user information. Our contributions can be summarized as follows: the set of reviews written by user i, and the set of reviews for item written by other users where = i. We provide summary of all task statistics and their associated graphs in Table 1 and Table 2 respectively. 1. Benchmark. We propose Personalized Graph-based Benchmark for Text Generation with 12 distinct tasks. To support further research, we make it available 1. 2. Problem. Current approaches to personalized text generation struggle with cold-start users, who have only minimal history data. To address this problem, we propose PGraphRAG by augmenting the context with structured user-specific information. 3. Effectiveness. We demonstrate the state-ofthe-art performance of PGraphRAG across the new benchmark in producing personalized outputs using user-centric knowledge graphs."
        },
        {
            "title": "2 Personalized Graph-based Benchmark",
            "content": "for LLMs Here, we discuss the proposed Personalized GraphBased Benchmark to evaluate LLMs in their ability to produce personalized text generations for twelve personalized tasks including long text generation, short text generation, and ordinal classification. The benchmark datasets were collected from several real-world datasets from various domains. LLMs typically take an input and predict the most likely sequence of tokens that follows x. As such, each data entry in the benchmark consists of: (1) an input sequence that serves as the input to LLMs, (2) target output sequence that the LLM is expected to generate, and (3) user-centric bipartite graph. Given an input sample (x, y) for any user i, the goal is to generate personalized output ˆy that matches the target output conditioning on the user profile Pi. We represent the user-centric graph as bipartite knowledge graph = (U, V, E), such that denotes user nodes, denotes item nodes, and denotes the interaction edges among users and items. For example, an edge (i, j) may represent review written by user for item j, including all details such as the review text, title, and rating. In this benchmark, we define the user profile Pi as 1https://github.com/ PGraphRAG-benchmark/PGR-LLM Long Text Generation 1. User Product Review Generation 2. Hotel Experience Generation 3. Stylized Feedback Generation 4. Multi-lingual Review Generation Short Text Generation 5. User Product Review Title Generation 6. Hotel Experience Summary Generation 7. Stylized Feedback Title Generation 8. Multi-lingual Review Title Generation Ordinal Classification 9. User Product Review Ratings 10. Hotel Experience Ratings 11. Stylized Feedback Ratings 12. Multi-lingual Product Ratings"
        },
        {
            "title": "2.1 Task Definitions",
            "content": "Task 1: User Product Review Generation. Personalized review text generation has progressed from incorporating user-specific context to utilizing LLMs for generating fluent and contextually relevant reviews and titles (Ni and McAuley, 2018). This task aims to generate target product review itext given the target users product review title ititle and set of additional reviews Pi from their profile. We use the Amazon Reviews 2023 dataset (Hou et al., 2024) to construct data splits and bipartite graphs across multiple product categories. Task 2: Hotel Experience Generation. Hotel reviews often contain detailed narratives reflecting users personal experiences, making personalization crucial for capturing individual preferences and accommodations (Kanouchi et al., 2020). This task focuses on generating personalized hotel experience story itext based on the target users hotel review summary ititle and set of additional reviews Pi. The Hotel Reviews dataset, subset of Datafinitis Business Database (Datafiniti, 2017), is used to construct data splits and user-hotel graph. Task 3: Stylized Feedback Generation. User writing style, influenced by grammar, punctuation, and spelling, reflects individual preferences and is shaped by geographic and cultural factors, making"
        },
        {
            "title": "Type",
            "content": "Avg. Input Length Avg. Output Length Avg. Profile Size # Classes User-Product Review Generation Hotel Experiences Generation Stylized Feedback Generation Multilingual Product Review Generation"
        },
        {
            "title": "Long Text Generation\nLong Text Generation\nLong Text Generation\nLong Text Generation",
            "content": "Short Text Generation User-Product Review Title Generation Short Text Generation Hotel Experiences Summary Generation Stylized Feedback Title Generation Short Text Generation Multilingual Product Review Title Generation Short Text Generation User-Product Review Ratings Hotel Experiences Ratings Stylized Feedback Ratings Multilingual Product Ratings"
        },
        {
            "title": "Ordinal Classification\nOrdinal Classification\nOrdinal Classification\nOrdinal Classification",
            "content": "3.754 2.71 4.29 2.57 3.35 2.02 2.9 2.40 30.34 37.95 90.40 99.17 37.42 38.17 22.17 20.15 34.10 38.66 94.69 99.62 40.77 38.69 25.15 20.75 47.90 19.28 76.26 22.39 51.80 20.07 34.52 12.55 7.02 1.14 7.64 0.92 7.16 1.11 7.15 1.09 - - - - 1.05 0.31 1.14 0.61 1.09 0.47 1.08 0.33 1.05 0.31 1.14 0.61 1.09 0.47 1.08 0.33 1.05 0.31 1.14 0.61 1.09 0.47 1.08 0.33 - - - - - - - - 5 5 5 Table 1: Data Statistics for PGraph Benchmark. The table reports the average input length and average output length in words (done for the test set on GPT-4o-mini on BM25 back on all methods). The average profile size for each task is by user review size."
        },
        {
            "title": "Users",
            "content": "Items Edges/Reviews Average Degree User-Product Review Graph Hotel Experiences Graph Stylized Feedback Graph Multilingual Product Review Graph 184,771 51,376 2,975 15,587 58,087 600 112,993 55,930 198,668 19,698 71,041 131,075 1.68 2.12 2.42 1.55 Table 2: Graph statistics for the datasets used in the personalized tasks. The table provides the number of users, items, edges (reviews), and the average degree for each dataset: User-Product Graph, Multilingual Product Graph, Stylized Feedback Graph, and Hotel Experiences Graph. it critical for personalized text generation (Alhafni et al., 2024). This task involves generating target feedback itext based on the target users feedback title ititle and set of additional feedback Pi from their profile. We use the Grammar and Online Product dataset, subset of the Datafiniti Business dataset (Datafiniti, 2018), which highlights writing quality across multiple platforms. Task 4: Multi-lingual Review Generation. Personalization in multilingual review generation presents unique challenges due to variations in linguistic structures, cultural nuances, and stylistic conventions (Cortes et al., 2024). In this task, we generate target product reviews itext in Brazilian Portuguese based on the target users review title ititle and additional reviews Pi in their profile. The B2W-Reviews dataset (Real et al., 2019), collected from Brazils largest e-commerce platform, is used to create data splits. Task 5: User Product Review Title Generation. Short text generation for personalized review titles is particularly challenging due to the need for summarization, sentiment dissemination, and capturing user behavior styles. This task generates target review title ititle using the target users review text itext and additional reviews Pi from their profile, without relying on parametric user information (Xu et al., 2023). We construct the dataset from the Amazon Reviews dataset (Hou et al., 2024). Task 6: Hotel Experience Summary Generation. Consolidating hotel information to help guests make informed decisions and personalize their experience is crucial (Kamath et al., 2024). This task focuses on generating the target users hotel experience summary ititle using their experience text itext and additional experiences Pi. We leverage the Datafiniti Business Database on Hotel Reviews (Datafiniti, 2017). Task 7: Stylized Feedback Title Generation. Opinion datasets often lack review titles and rely on comparing reviews with desirable feedback to generate Stylized Opinion Summarization (Iso et al., 2024). This task benchmarks stylized feedback across domains such as music, groceries, and household items. The goal is to generate the target users feedback title ititle based on their feedback text itext and additional feedback Pi. The dataset is constructed from the Datafiniti Products dataset (Datafiniti, 2018). Task 8: Multi-lingual Review Title Generation. Brazilian Portuguese presents unique challenges in simplifying review text (Scalercio et al., 2024), particularly in multilingual approach to generating review titles. This short task generates the target users product review title ititle using their review text itext and additional user reviews Pi. The dataset is created from the B2W-Reviews dataset (Real et al., 2019). Task 9: User Product Review Ratings. Recent advancements in sentiment analysis have utilized graph structures to enhance sentiment prediction (Zhang et al., 2023; Kertkeidkachorn and Shirai, 2023). This task focuses on predicting ratings within an ordinal classification framework, assigning values from 1 to 5. To generate user-product review rating irating, we use the target users product review itext, the corresponding title ititle, and additional reviews Pi as context. The dataset is constructed from the Amazon Reviews dataset (Hou et al., 2024). Task 10: Hotel Experience Ratings. Guest reviews often address multiple aspects of hotel experiences, which are typically framed as multi-label classification problems (Fehle et al., 2023). This task adapts this aspect to evaluating personalized bias lodging scores. We define users hotel experience rating irating based on their hotel experience story itext and the summary ititle, with additional context from Pi. The dataset is derived from the Hotel Reviews dataset (Datafiniti, 2017). Task 11: Stylized Feedback Ratings. Exploring sentiment across different domains highlights variations in writing quality and the factors influencing sentiment (Yu et al., 2021). This task investigates domain-specific variations by assigning numerical feedback rating irating to target stylized user review. The input includes the stylized review text itext and title ititle. The dataset is constructed from the Datafiniti Product Database on Grammar and Online Product Reviews (Datafiniti, 2018). Task 12: Multi-lingual Product Ratings. Sentiment analysis has proven effective at the sentence level when applied in Portuguese (de Araujo et al., 2024). However, this task extends beyond simple sentences to explore variability in Brazilian product reviews by generating Portuguese user-product rating irating for targeted review by considering both the review text itext and the review title ititle as context. We construct the dataset from B2WReviews (Real et al., 2019)."
        },
        {
            "title": "3 Dataset Splits",
            "content": "Datasets are split into train, validation, and test sets where the users review history exists solely in one split. Splits separate users, as we cannot reduce the dimensionality of the user review size when they are already sparse. We preserve the user profile review size distribution from the original dataset by stratifying across splits. We maintain product density distribution by randomly sampling across all user reviews and ensuring that neighbor exists for that product. The neighbor distribution follows the same distribution trend across splits where the graph statistics are seen in Table 2. Data statistics are shown in Table 1 and data split size in Table 3. Dataset Train Size Validation Size Test Size User-Product Review Multilingual Product Review Stylized Feedback Hotel Experiences 20,000 20,000 20,000 9, 2,500 2,500 2,500 2,500 2,500 2,500 2,500 2,500 Table 3: Dataset split sizes for training, validation, and testing across four datasets: User-Product Review, Multilingual Product Review, Stylized Feedback, and Hotel Experiences."
        },
        {
            "title": "4 PGraphRAG Framework",
            "content": "In this section, we present PGraphRAG, our proposed approach for personalizing large language models (LLMs). PGraphRAG enhances personalization by prompting shared model with userspecific context, effectively integrating structured user-specific knowledge to enable tailored and context-aware text generation. As discussed in Section 2, PGraphRAG leverages rich user-centric bipartite graph that enables our approach to broader context beyond the user history. Specifically, for any user i, we define the user profile Pi as the set of previous texts written by user (i.e., {(i, j) E}), and the set of texts written by other users for the same items connected to user (i.e., {(k, j) (i, j) E}). As such, the user profile Pi is defined as follows, Pi = {(i, j) E} {(k, j) (i, j) E} (1) V, U, = Considering the context length limitations of certain LLMs and the computational costs of processing contexts, we utilize retrieval augmentation to extract only the most relevant information from the user profile with respect to the input query. This retrieved information is then used to condition the models predictions for the current unseen test case. Given an input sample (x, y) for user i, we follow few steps to generate ˆy, which includes query function, graph-based retrieval model, and prompt construction function seen in Figure 1. 1. Query Function (ϕq): The query function transforms the input into query for retrieving from the user profile. 2. Graph-Based Retrieval (R): The retrieval function R(q, G, k) takes as input the query q, the bipartite graph G, and threshold k. First, the retrieval function leverages the graph to construct the user profile Pi. Then, it retrieves the k-most relevant entries from the user profile. 3. Prompt Construction (ϕp): The prompt construction assembles personalized prompt for user by combining the input with the retrieved entries. We define the constructed input using as x: = ϕp(x, R(ϕq(x), G, k)) (2) Then, we use (x, y) to train or evaluate LLMs."
        },
        {
            "title": "5 Experiments",
            "content": "Setup. We evaluated the performance of our proposed method on both validation and test datasets in zero-shot setting, utilizing the LLaMA-3.18B-Instruct model (Touvron et al., 2023) and the GPT-4o-mini model (OpenAI, 2024). For the outputs generated by both models, we perform evaluations using the evaluate library. The LLaMA3.1-8B-Instruct model is implemented using the Huggingface transformers library and configured to produce outputs with maximum length of 512 tokens. All experiments are conducted on an NVIDIA A100 GPU with 80GB of memory. We access GPT-4o-mini via the Azure OpenAI Service (Services, 2023), using the AzureOpenAI class. The temperature is configured at 0.4. Retrievers. We evaluated BM25 (Robertson and Zaragoza, 2009) and Contriever (Lei et al., 2023) using NLTKs word_tokenize and Contrievers tokenizer for tokenization without additional hyperparameters. Both models retrieved the top-5 documents per query, applying mean pooling to token embeddings. Baseline Methods. We compare our method against several non-personalized and personalized (1) No-Retrieval serves as nonapproaches. personalized baseline where the prompt is constructed without any retrieval augmentation. The LLM generates the target text solely based on the query. (2) Random-Retrieval serves as nonpersonalized baseline where the prompt is constructed with augmentation using random item from all user profiles. (3) LaMP (Salemi et al., 2024b) is personalized baseline where the prompt is constructed with augmentation with user-specific input or context, such as previous reviews written by the user. Evaluation. For evaluation, we assess each method by providing task-specific inputs and measuring performance based on the generated outputs. For long and short text generation tasks, we utilize the ROUGE-1, ROUGE-L (Lin, 2004), and METEOR (Banerjee and Lavie, 2005) metrics. For rating prediction tasks, we evaluate performance using MAE and RMSE as metrics."
        },
        {
            "title": "5.1 Baseline Comparison",
            "content": "Here, we discuss our baseline comparison across the different tasks described in Section 2. Long Text Generation. Tables 4 and 5 show the results for long text generation where PGraphRAG consistently outperforms the baselines methods in order of no-retrieval, random retrieval, and LaMP across several metrics. PGraphRAG showed the greatest improvement in Hotel Experience Generation over the LaMP baseline in both models, with gains of +32.1% in ROUGE-1, +21.7% in ROUGE-L, and +25.7% in METEOR (LLaMA3.1-8B). This shows the benefits gained by incorporating broader context from user-centric graphs. Short Text Generation. Table 6 and 7 show the results for short text generation. PGraphRAG outperforms the baselines in most cases, but the gains are more modest compared to the long text generation tasks. In User Product Review Title Generation PGraphRAG achieves small, consistent improvements in ROUGE-1 (+5.6%), ROUGE-L (+5.9%), and METEOR (+6.8%) over LaMP for LLaMA3.1-8B-Instruct . The smaller gains in performance are attributed to the brevity of the task, which limits the influence of the retrieved context. Ordinal Classification As shown in Tables 9 and 8, the PGraphRAG method leads in Hotel Experience Ratings and Multi-lingual Product Ratings,"
        },
        {
            "title": "Metric",
            "content": "PGraphRAG LaMP No-Retrieval Random-Retrieval Task 1: User-Product Review Generation Task 2: Hotel Experiences Generation Task 3: Stylized Feedback Generation Task 4: Multilingual Product Review Generation ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR 0.178 0.129 0.151 0.263 0.157 0.191 0.217 0.158 0. 0.188 0.147 0.145 0.173 0.129 0.138 0.199 0.129 0.152 0.186 0.134 0.177 0.176 0.141 0.125 0.172 0.123 0. 0.231 0.145 0.153 0.190 0.131 0.167 0.174 0.136 0.131 0.124 0.094 0.099 0.216 0.132 0.152 0.184 0.108 0. 0.146 0.116 0.109 Table 4: Zero-shot test set results for long text generation using LLaMA-3.1-8B. The choice of retriever and were tuned using the validation set."
        },
        {
            "title": "Metric",
            "content": "PGraphRAG LaMP No-Retrieval Random-Retrieval Task 1: User-Product Review Generation Task 2: Hotel Experiences Generation Task 3: Stylized Feedback Generation Task 4: Multilingual Product Review Generation ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR 0.189 0.130 0.196 0.263 0.152 0.206 0.211 0.140 0. 0.194 0.144 0.171 0.171 0.117 0.176 0.221 0.135 0.164 0.185 0.123 0.183 0.168 0.125 0.154 0.169 0.116 0. 0.223 0.135 0.166 0.187 0.123 0.189 0.170 0.128 0.152 0.159 0.114 0.153 0.234 0.139 0.181 0.177 0.121 0. 0.175 0.133 0.149 Table 5: Zero-shot test set results for long text generation using GPT-4o-mini. The choice of retriever and were tuned using the validation set."
        },
        {
            "title": "Metric",
            "content": "PGraphRAG LaMP No-Retrieval Random-Retrieval Task 5: User Product Review Title Generation Task 6: Hotel Experience Summary Generation Task 7: Stylized Feedback Title Generation Task 8: Multi-lingual Product Review Title Generation ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR 0.131 0.125 0.125 0.127 0.118 0.102 0.149 0.142 0. 0.124 0.116 0.108 0.124 0.118 0.117 0.126 0.117 0.106 0.140 0.134 0.136 0.121 0.122 0.094 0.121 0.115 0. 0.122 0.114 0.101 0.136 0.131 0.129 0.125 0.117 0.092 0.103 0.098 0.096 0.118 0.110 0.093 0.133 0.123 0. 0.120 0.110 0.103 Table 6: Zero-shot test set results for short text generation using LLaMA-3.1-8B. The choice of retriever and were tuned using the validation set. particularly excelling in Task 10 (LLaMA-3.1-8BInstruct ). It demonstrates notable improvements over LaMP, with an MAE improvement of +2.16% and RMSE improvement of +3.17%. The effectiveness of sentiment analysis tasks depends heavily on the dataset, as the performance of personalized methods varies based on the importance of contextual information."
        },
        {
            "title": "Metric",
            "content": "PGraphRAG LaMP No-Retrieval Random-Retrieval Task 5: User Product Review Title Generation Task 6: Hotel Experience Summary Generation Task 7: Stylized Feedback Title Generation Task 8: Multi-lingual Product Review Title Generation ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR 0.115 0.112 0.099 0.116 0.111 0.081 0.122 0.118 0. 0.111 0.105 0.083 0.108 0.105 0.091 0.108 0.104 0.075 0.113 0.109 0.096 0.115 0.107 0.088 0.113 0.110 0. 0.114 0.109 0.079 0.114 0.110 0.097 0.118 0.110 0.089 0.102 0.099 0.085 0.112 0.107 0.076 0.115 0.111 0. 0.108 0.102 0.078 Table 7: Zero-shot test set results for short text generation using GPT-4o-mini. The choice of retriever and were tuned using the validation set."
        },
        {
            "title": "Metric",
            "content": "PGraphRAG LaMP No-retrieval Random-retrieval Task 9: User Product Review Ratings Task 10: Hotel Experience Ratings Task 11: Stylized Feedback Ratings Task 12: Multi-lingual Product Ratings MAE RMSE MAE RMSE MAE RMSE MAE RMSE 0.3400 0.7668 0.3688 0.6771 0.3476 0. 0.4928 0.8367 0.3132 0.7230 0.3492 0.6527 0.3268 0.6803 0.5016 0.8462 0.3212 0. 0.3340 0.6372 0.3256 0.6806 0.5084 0.8628 0.3272 0.7616 0.3804 0.6971 0.3704 0. 0.5096 0.8542 Table 8: Zero-shot test set results on ordinal classification on Tasks 9-12 on BM25 using MAE and RMSE metrics for LLaMA-3.1-8B-Instruct ."
        },
        {
            "title": "Metric",
            "content": "PGraphRAG LaMP No-retrieval Random-retrieval Task 9: User Product Review Ratings Task 10: Hotel Experience Ratings Task 11: Stylized Feedback Ratings Task 12: Multi-lingual Product Ratings MAE RMSE MAE RMSE MAE RMSE MAE RMSE 0.3832 0.7392 0.3284 0.6083 0.3476 0. 0.4348 0.7367 0.3480 0.7065 0.3336 0.6197 0.3448 0.6669 0.4444 0.7608 0.3448 0. 0.3336 0.6197 0.3416 0.6711 0.4564 0.7718 0.4188 0.8082 0.3524 0.6384 0.4080 0. 0.4700 0.8112 Table 9: Zero-shot test set results on ordinal classification on Tasks 9-12 on BM25 using MAE and RMSE metrics for GPT-4o-mini ."
        },
        {
            "title": "5.2 Ablation Study",
            "content": "In this section, we perform several ablation studies on PGraphRAG."
        },
        {
            "title": "5.2.1 PGraphRAG Ablation Study",
            "content": "We investigate the impact of incorporating different information in the retrieved context of PGraphRAG. Specifically, we explore the following variants of PGRaphRAG: (1) User-Only: where only userspecific history is included in the user profile Pi, (2) Neighbors-Only: where history from other neighboring users (two-hop away users) is included in the user profile Pi. Across long-text and short-text generation tasks, PGraphRAG and its \"Neighbors Only\" variant consistently outperform PGraphRAG (User Only), emphasizing the value of retrieving information from neighboring users that are two hops away in the context. In Tables 11, 10, 13, and 12 both PGraphRAG and PGraphRAG (Neighbors Only) retrieval methods consistently outperform LaMP, contrasting the impact of retrieving neighboring-user context with that of retrieving target-user history as context. PGraphRAG generally matches or slightly exceeds the performance of PGraphRAG (Neighbors Only), suggesting that the additional target-user history portion of the context contributes minimally to the personalized text generation task for these long and short text generation. 5."
        },
        {
            "title": "Impact of the Retrieved Items k",
            "content": "To evaluate the impact of the number of retrievedcontext reviews (k) on model performance, we conducted experiments with = 1, = 2, and = 4. Results for these experiments using GPT-4o-mini are summarized in Table 15 for long-text tasks and Table 17 for short-text tasks. Results for LLaMA3.1-8B-Instruct are shown in Tables 14 and 16. Retrieving more context reviews improves performance by enhancing the diversity and relevance of the context. While = 4 is the best, = 1 can outperform in cases where single highly relevant review is sufficient. However, due to data sparsity, many user profiles lack sufficient neighboring or target-user reviews to meet the desired k. For instance, when configured to retrieve = 4, PGraphRAG may fall back to retrieving only one or two reviews if that is all that is available. This behavior reflects the practical challenge of handling cold-start users with limited data, key focus of our study. Long Text Generation Metric = 1 = = 4 Task 1: User-Product Review Generation Task 2: Hotel Experiences Generation Task 3: Stylized Feedback Generation Task 4: Multilingual Product Review Generation ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR 0.176 0.121 0.168 0.250 0.146 0.188 0.196 0.136 0. 0.163 0.134 0.113 0.184 0.125 0.180 0.260 0.150 0.198 0.200 0.136 0.192 0.169 0.137 0.122 0.186 0.126 0. 0.265 0.152 0.206 0.205 0.139 0.203 0.174 0.139 0.133 Table 15: Ablation study results showing the impact of varying (number of retrieved neighbors) on PGraphRAGs performance. Zero-shot validation set results GPT-4o-mini on the long text generation Task 1-4. Short Text Generation Metric = 1 = 2 = 4 Task 5: User Product Review Title Generation Task 6: Hotel Experience Summary Generation Task 7: Stylized Feedback Title Generation Task 8: Multi-lingual Product Review Title Generation ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR 0.128 0.121 0. 0.122 0.112 0.104 0.129 0.124 0.129 0.129 0.120 0.117 0.123 0.118 0.118 0.121 0.114 0.102 0.132 0.126 0. 0.126 0.119 0.116 0.125 0.119 0.117 0.121 0.113 0.099 0.132 0.128 0.129 0.131 0.123 0.118 Table 16: Ablation study results showing the impact of varying (number of retrieved neighbors) on PGraphRAGs performance. Zero-shot validation set results LLaMA-3.1-8B-Instruct on the short text generation Task 5-9. Long Text Generation Metric = 1 = 2 = 4 Short Text Generation Metric = 1 = 2 = 4 Task 1: User-Product Review Generation Task 2: Hotel Experiences Generation Task 3: Stylized Feedback Generation Task 4: Multilingual Product Review Generation ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR 0.160 0.121 0.125 0.230 0.141 0.152 0.200 0.158 0.154 0.163 0.134 0.113 0.169 0.125 0.138 0.251 0.151 0. 0.214 0.165 0.171 0.169 0.137 0.122 0.173 0.124 0.150 0.263 0.156 0.191 0.226 0.171 0.192 0.174 0.139 0. Task 5: User Product Review Title Generation Task 6: Hotel Experience Summary Generation Task 7: Stylized Feedback Title Generation Task 8: Multi-lingual Product Review Title Generation ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR 0.111 0.106 0.093 0.114 0.109 0.082 0.100 0.098 0.087 0.104 0.098 0. 0.110 0.105 0.094 0.114 0.109 0.082 0.103 0.101 0.090 0.104 0.098 0.078 0.111 0.106 0.097 0.118 0.112 0. 0.109 0.107 0.096 0.108 0.104 0.082 Table 14: Ablation study results showing the impact of varying (number of retrieved neighbors) on PGraphRAGs performance. Zero-shot validation set results LLaMA-3.1-8B-Instruct on the long text generation Task 1-4. Table 17: Ablation study results showing the impact of varying (number of retrieved neighbors) on PGraphRAGs performance. Zero-shot validation set results GPT-4o-mini on the long-text generation Task 5-9."
        },
        {
            "title": "PGraphRAG",
            "content": "PGraphRAG (Neighbors Only) PGraphRAG (User Only) Task 1: User-Product Review Generation Task 2: Hotel Experience Generation Task 3: Stylized Feedback Generation Task 4: Multi-lingual Review Generation ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR 0.173 0.124 0.150 0.263 0.156 0. 0.226 0.171 0.192 0.174 0.139 0.133 0.177 0.127 0.154 0.272 0.162 0.195 0.222 0.165 0.186 0.172 0.137 0. 0.168 0.125 0.134 0.197 0.128 0.121 0.181 0.134 0.147 0.174 0.141 0.125 Table 10: Ablation study results using LLaMA-3.1-8B-Instruct on the validation set for the long text generation Tasks 1 - 4. 5."
        },
        {
            "title": "Impact of Retriever method R",
            "content": "In Table 19, we compare the performance of PGraphRAG using BM25 and Contriever across models and Tasks 1-9. The results demonstrate that PGraphRAGs performance remains stable across datasets and tasks, showing minimal sensitivity to the choice of retriever. Both methods achieve comparable results, with BM25 showing slight improvements in most cases. This stability highlights the robustness of PGraphRAG in adapting to different retrieval contexts while maintaining consistent performance regardless of the retrieval method used."
        },
        {
            "title": "5.5 GPT Experiments",
            "content": "We perform ablation studies on GPT models and output length constraints to determine the bestperforming configuration. GPT-4o-mini is selected for test set experiments due to its superior performance, speed, and cost efficiency compared to GPTo1 in Fig 3. For short-text generation, we evaluate length constraints of 3, 5, and 10 words, finding that 5-word constraint provides the best balance between precision and informativeness in Fig 4."
        },
        {
            "title": "6 Related Work",
            "content": "Personalization in natural language processing (NLP) tailors responses to individual user preferences, behaviors, and contexts, significantly enhancing user interaction and satisfaction. Early work in personalization focused on tasks such as text generation, leveraging attributes like review sentiment (Zang and Wan, 2017) and stylistic features (Dong et al., 2017). These methods, based on neural networks and encoder-decoder models, laid the foundation for personalization in text-based systems. Recent advancements have expanded personalization techniques to incorporate retrieval-augmented generation (RAG) strategies. For example, methods such as in-context prompting (Lyu et al., 2024b), retrieval-based summarization (Richardson et al., 2023), and optimization techniques like reinforcement learning and knowledge distillation (Salemi et al., 2024a) have further refined personalized models. Benchmarks like LaMP (Salemi et al., 2024b) and LongLaMP (Kumar et al., 2024) have been developed to evaluate personalized tasks, emphasizing user-specific history for text generation tasks such as email completion and abstract writing. Retrieval-based approaches, such as (Kim et al., 2020), have also explored personalization by enhancing retrieval pipelines for long-form personalized content generation. However, most existing methods for personalization rely heavily on user history to augment the context or prompt, limiting their effectiveness in scenarios where user history is sparse or unavailable. This reliance poses challenges in real-world applications, particularly for cold-start users. Furthermore, these approaches often overlook the potential of integrating structured data, such as knowledge graphs, to provide richer and more diverse user-specific contexts. Figure 3: Comparison of GPT-4o-mini and GPT-o1 performance across all datasets and metrics for the long-text generation task. Figure 4: Impact of length constraints of 3, 5, and 10 on short-text generation tasks using PGraphRAG, evaluated on the validation set."
        },
        {
            "title": "PGraphRAG",
            "content": "PGraphRAG (Neighbors Only) PGraphRAG (User Only) Task 1: User-Product Review Generation Task 2: Hotel Experience Generation Task 3: Stylized Feedback Generation Task 4: Multilingual Product Review Generation ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR 0.186 0.126 0.187 0.265 0.152 0. 0.205 0.139 0.203 0.191 0.142 0.173 0.185 0.125 0.185 0.268 0.153 0.209 0.204 0.138 0.198 0.190 0.140 0. 0.169 0.114 0.170 0.217 0.132 0.161 0.178 0.121 0.178 0.164 0.123 0.155 Table 11: Ablation study results using GPT-4o-mini on the validation set for long text generation tasks across Tasks 1-4."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we presented PGraphRAG, framework that leverages user-centric knowledge graphs to enhance personalized text generation. Unlike approaches relying solely on user history, PGraphRAG integrates structured knowledge for richer understanding of user context, improving performance, especially in cold-start scenarios. We also introduced the Personalized Graph-based RAG Benchmark for evaluating language models on personalized text generation tasks. Experiments show that PGraphRAG outperforms state-of-the-art methods, highlighting the benefits of graph-based retrieval for personalization and paving the way for more adaptive AI systems."
        },
        {
            "title": "PGraphRAG",
            "content": "PGraphRAG (Neighbors Only) PGraphRAG (User Only) Task 5: User Product Review Title Generation Task 6: Hotel Experience Summary Generation Task 7: Stylized Feedback Title Generation Task 8: Multi-lingual Review Title Generation ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR 0.125 0.119 0.117 0.121 0.113 0. 0.132 0.128 0.129 0.131 0.123 0.118 0.129 0.123 0.120 0.124 0.115 0.103 0.135 0.130 0.132 0.131 0.122 0. 0.115 0.109 0.111 0.119 0.111 0.105 0.128 0.124 0.124 0.124 0.114 0.098 Table 12: Ablation study results using LLaMA-3.1-8B-Instruct on the validation set for short text generation on Tasks 4-8."
        },
        {
            "title": "PGraphRAG",
            "content": "PGraphRAG (Neighbors Only) PGraphRAG (User Only) Task 5: User Product Review Title Generation Task 6: Hotel Experience Summary Generation Task 7: Stylized Feedback Title Generation Task 8: Multi-lingual Product Review Title Generation ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR 0.111 0.106 0.097 0.118 0.112 0. 0.109 0.107 0.096 0.108 0.104 0.082 0.116 0.111 0.099 0.119 0.113 0.085 0.107 0.105 0.094 0.109 0.104 0. 0.112 0.108 0.095 0.109 0.104 0.077 0.108 0.104 0.091 0.116 0.109 0.091 Table 13: Ablation study results using GPT-4o-mini on the validation set for the short-text generation task on Task 4 - 8. Long Text Generation Metric Contriever BM25 Task 1: User-Product Review Generation Task 2: Hotel Experiences Generation Task 3: Stylized Feedback Generation Task 4: Multilingual Product Review Generation ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR 0.172 0.122 0. 0.262 0.155 0.190 0.195 0.138 0.180 0.172 0.134 0.135 0.173 0.124 0.150 0.263 0.156 0.191 0.226 0.171 0. 0.174 0.139 0.133 Table 18: Ablation study results evaluating the effect of the retriever method, comparing BM25 and Contriever, on PGraphRAGs performance. Results reflect the zeroshot performance of LLaMA-3.1-8B-Instruct for long text generation Tasks 14 on the validation set."
        },
        {
            "title": "Contriever",
            "content": "BM25 Short Text Generation Metric Contriever BM25 Task 1: User-Product Review Generation Task 2: Hotel Experiences Generation Task 3: Stylized Feedback Generation Task 4: Multilingual Product Review Generation ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR 0.182 0.122 0.184 0.264 0.152 0.207 0.194 0.128 0.201 0.190 0.141 0.174 0.186 0.126 0. 0.265 0.152 0.206 0.205 0.139 0.203 0.191 0.142 0.173 Task 5: User Product Review Title Generation Task 6: Hotel Experience Summary Generation Task 7: Stylized Feedback Title Generation Task 8: Multi-lingual Product Review Title Generation ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR 0.113 0.108 0. 0.113 0.107 0.080 0.108 0.106 0.094 0.108 0.103 0.082 0.111 0.106 0.097 0.118 0.112 0.085 0.109 0.107 0. 0.108 0.104 0.082 Table 19: Ablation study results evaluating the effect of the retriever method, comparing BM25 and Contriever, on PGraphRAGs performance. Results reflect the Zero-shot performance of GPT-4o-mini for long text generation Task 1-4 on the validation set. Table 21: Ablation study results evaluating the effect of the retriever method, comparing BM25 and Contriever, on PGraphRAGs performance. Results reflect the zero-shot performance of GPT-4o-mini for short text generation Tasks 58 on the validation set. Short Text Generation Metric Contriever BM25 Task 5: User Product Review Title Generation Task 6: Hotel Experience Summary Generation Task 7: Stylized Feedback Title Generation Task 8: Multi-lingual Product Review Title Generation ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR 0.122 0.116 0.115 0.117 0.110 0.095 0.125 0.121 0. 0.126 0.118 0.112 0.125 0.119 0.117 0.121 0.113 0.099 0.132 0.128 0.129 0.131 0.123 0.118 Table 20: Ablation study results evaluating the effect of the retriever method, comparing BM25 and Contriever, on PGraphRAGs performance. Results reflect the zeroshot performance of LLaMA-3.1-8B-Instruct for short text generation Tasks 58 on the validation set."
        },
        {
            "title": "References",
            "content": "Bashar Alhafni, Vivek Kulkarni, Dhruv Kumar, and Vipul Raheja. 2024. Personalized text generation with fine-grained linguistic control. In Proceedings of the 1st Workshop on Personalization of Generative AI Systems (PERSONALIZE 2024), pages 88101, St. Julians, Malta. Association for Computational Linguistics. Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 6572, Ann Arbor, Michigan. Association for Computational Linguistics. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS 20, Red Hook, NY, USA. Curran Associates Inc. Eduardo G. Cortes, Ana Luiza Vianna, Mikaela Martins, Sandro Rigo, and Rafael Kunst. 2024. LLMs and translation: different approaches to localization between Brazilian Portuguese and European Portuguese. In Proceedings of the 16th International Conference on Computational Processing of Portuguese - Vol. 1, pages 4555, Santiago de Compostela, Galicia/Spain. Association for Computational Lingustics. Datafiniti. 2017."
        },
        {
            "title": "Hotel",
            "content": "reviews, Retrieved September 15, 5. https://www.kaggle.com/datasets/ datafiniti/hotel-reviews/data. version 2024 from 2018. reviews, 15, Datafiniti. product September 2024 kaggle.com/datasets/datafiniti/ grammar-and-online-product-reviews. online Retrieved from https://www. Grammar 1. version and Gladson de Araujo, Tiago de Melo, and Carlos Maurício S. Figueiredo. 2024. Is ChatGPT an effective solver of sentiment analysis tasks in Portuguese? preliminary study. In Proceedings of the 16th International Conference on Computational Processing of Portuguese - Vol. 1, pages 1321, Santiago de Compostela, Galicia/Spain. Association for Computational Lingustics. Li Dong, Shaohan Huang, Furu Wei, Mirella Lapata, Ming Zhou, and Ke Xu. 2017. Learning to generate product reviews from attributes. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 623632, Valencia, Spain. Association for Computational Linguistics. Jakob Fehle, Leonie Münster, Thomas Schmidt, and Christian Wolff. 2023. Aspect-based sentiment analysis as multi-label classification task on the domain of German hotel reviews. In Proceedings of the 19th Conference on Natural Language Processing (KONVENS 2023), pages 202218, Ingolstadt, Germany. Association for Computational Lingustics. Andrew Fowler, Kurt Partridge, Ciprian Chelba, Xiaojun Bi, Tom Ouyang, and Shumin Zhai. 2015. Effects of language modeling and its personalization on touchscreen typing performance. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems, CHI 15, page 649658, New York, NY, USA. Association for Computing Machinery."
        },
        {
            "title": "Pawan",
            "content": "Sebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Sasanka Ammanamanchi, Anuoluwapo Aremu, Antoine Bosselut, Khyathi Raghavi Chandu, Miruna-Adriana Clinciu, Dipanjan Das, Kaustubh Dhole, Wanyu Du, Esin Durmus, Ondˇrej Dušek, Chris Chinenye Emezue, Varun Gangal, Cristina Garbacea, Tatsunori Hashimoto, Yufang Hou, Yacine Jernite, Harsh Jhamtani, Yangfeng Ji, Shailza Jolly, Mihir Kale, Dhruv Kumar, Faisal Ladhak, Aman Madaan, Mounica Maddela, Khyati Mahajan, Saad Mahamood, Bodhisattwa Prasad Majumder, Pedro Henrique Martins, Angelina McMillan-Major, Simon Mille, Emiel van Miltenburg, Moin Nadeem, Shashi Narayan, Vitaly Nikolaev, Andre Niyongabo Rubungo, Salomey Osei, Ankur Parikh, Laura Perez-Beltrachini, Niranjan Ramesh Rao, Vikas Raunak, Juan Diego João Sedoc, Rodriguez, Sashank Santhanam, Thibault Sellam, Samira Shaikh, Anastasia Shimorina, Marco Antonio Sobrevilla Cabezudo, Hendrik Strobelt, Nishant Subramani, Wei Xu, Diyi Yang, Akhila Yerukola, and Jiawei Zhou. 2021. The GEM benchmark: Natural language generation, In Proceedings of the its evaluation and metrics. 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 96120, Online. Association for Computational Linguistics. Yupeng Hou, Jiacheng Li, Zhankui He, An Yan, Xiusi Chen, and Julian McAuley. 2024. Bridging language and items for retrieval and recommendation. arXiv preprint arXiv:2403.03952. Xiaolei Huang, Lucie Flek, Franck Dernoncourt, Charles Welch, Silvio Amir, Ramit Sawhney, and Diyi Yang. 2022. Usernlp22: 2022 international workshop on user-centered natural language processing. In Companion Proceedings of the Web Conference 2022, WWW 22, page 11761177, New York, NY, USA. Association for Computing Machinery. Hayate Iso, Xiaolan Wang, and Yoshi Suhara. 2024. Noisy pairing and partial supervision for stylized opinion summarization. In Proceedings of the 17th International Natural Language Generation Conference, pages 1323, Tokyo, Japan. Association for Computational Linguistics. Srinivas Ramesh Kamath, Fahime Same, and Saad Mahamood. 2024. Generating hotel highlights from In Proceedings of unstructured text using LLMs. the 17th International Natural Language Generation Conference, pages 280288, Tokyo, Japan. Association for Computational Linguistics. Shin Kanouchi, Masato Neishi, Yuta Hayashibe, Hiroki Ouchi, and Naoaki Okazaki. 2020. You may like this hotel because ...: Identifying evidence for explainIn Proceedings of the 1st able recommendations. Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 890899, Suzhou, China. Association for Computational Linguistics. Natthawut Kertkeidkachorn and Kiyoaki Shirai. 2023. Sentiment analysis using the relationship between users and products. In Findings of the Association for Computational Linguistics: ACL 2023, pages 86118618, Toronto, Canada. Association for Computational Linguistics. Jihyeok Kim, Seungtaek Choi, Reinald Kim Amplayo, and Seung-won Hwang. 2020. Retrieval-augmented controllable review generation. In Proceedings of the 28th International Conference on Computational Linguistics, pages 22842295, Barcelona, Spain (Online). International Committee on Computational Linguistics. Ishita Kumar, Snigdha Viswanathan, Sushrita Yerra, Alireza Salemi, Ryan A. Rossi, Franck Dernoncourt, Hanieh Deilamsalehy, Xiang Chen, Ruiyi Zhang, Shubham Agarwal, Nedim Lipka, Chien Van Nguyen, Thien Huu Nguyen, and Hamed Zamani. 2024. Longlamp: benchmark for personalized long-form text generation. Yibin Lei, Liang Ding, Yu Cao, Changtong Zan, Andrew Yates, and Dacheng Tao. 2023. Unsupervised dense retrieval with relevance-aware contrastive pretraining. In Findings of the Association for Computational Linguistics: ACL 2023, pages 1093210940, Toronto, Canada. Association for Computational Linguistics. Chin-Yew Lin. 2004. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain. Association for Computational Linguistics. Hanjia Lyu, Song Jiang, Hanqing Zeng, Yinglong Xia, Qifan Wang, Si Zhang, Ren Chen, Chris Leung, Jiajie Tang, and Jiebo Luo. 2024a. LLM-rec: Personalized recommendation via prompting large language models. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 583612, Mexico City, Mexico. Association for Computational Linguistics. Hanjia Lyu, Song Jiang, Hanqing Zeng, Yinglong Xia, Qifan Wang, Si Zhang, Ren Chen, Christopher Leung, Jiajie Tang, and Jiebo Luo. 2024b. Llm-rec: Personalized recommendation via prompting large language models. Maxim Naumov, Dheevatsa Mudigere, HaoJun Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu, Alisson G. Azzolini, Dmytro Dzhulgakov, Andrey Mallevich, Ilia Cherniavskii, Yinghai Lu, Raghuraman Krishnamoorthi, Ansha Yu, Volodymyr Kondratenko, Stephanie Pereira, Xianjie Chen, Wenlin Chen, Vijay Rao, Bill Jia, Liang Xiong, and Misha Smelyanskiy. 2019. Deep learning recommendation model for personalization and recommendation systems. CoRR, abs/1906.00091. Jianmo Ni and Julian McAuley. 2018. Personalized review generation by expanding phrases and attending on aspect-aware representations. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 706711, Melbourne, Australia. Association for Computational Linguistics. OpenAI. 2024. Gpt-4o system card. Livy Real, Marcio Oshiro, and Alexandre Mafra. 2019. B2w-reviews01: an open product reviews corpus. In STIL-Symposium in Information and Human Language Technology. Chris Richardson, Yao Zhang, Kellen Gillespie, Sudipta Kar, Arshdeep Singh, Zeynab Raeesy, Omar Zia Khan, and Abhinav Sethy. 2023. Integrating summarization and retrieval for enhanced personalization via large language models. Stephen Robertson and Hugo Zaragoza. 2009. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends in Information Retrieval, 3:333389. Alireza Salemi, Surya Kallumadi, and Hamed Zamani. 2024a. Optimization methods for personalizing large language models through retrieval augmentation. Alireza Salemi, Sheshera Mysore, Michael Bendersky, and Hamed Zamani. 2024b. LaMP: When large language models meet personalization. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics, pages 73707392, Bangkok, Thailand. Association for Computational Linguistics. Arthur Scalercio, Maria Finatto, and Aline Paes. 2024. Enhancing sentence simplification in Portuguese: Leveraging paraphrases, context, and linguistic features. In Findings of the Association for Computational Linguistics: ACL 2024, pages 1507615091, Bangkok, Thailand. Association for Computational Linguistics."
        },
        {
            "title": "A Validation Results",
            "content": "We conduct comprehensive experiments on the validation set for all tasks, testing all combinations of language models, retrieval methods, and top-k retrieval settings for each method. Tables 22, 23, 24, 25,26, 27 show the selected configurations yielding the best results on the validation set which are then selected for subsequent test set experiments. The trends observed in the test set are consistent with those seen in the validation set of k=4 and BM25 in most cases."
        },
        {
            "title": "B Prompt Construction",
            "content": "Figure 5 shows the prompt construction for text generation and ordinal classification for PGraphRAG. The prompts pass the corresponding context in order to get the desired output. We adhere to modular prompt to allow for consistency between the tasks Azure AI Services. 2023."
        },
        {
            "title": "Openai",
            "content": "mini-20240718) https://learn.microsoft.com/en-us/ azure/ai-services/openai. language [large (gpt-4omodel]. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019a. SuperGLUE: stickier benchmark for general-purpose language understanding systems. Curran Associates Inc., Red Hook, NY, USA. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019b. GLUE: multi-task benchmark and analysis platform for natural language understanding. In International Conference on Learning Representations. Hongyan Xu, Hongtao Liu, Zhepeng Lv, Qing Yang, and Wenjun Wang. 2023. Pre-trained personalized review summarization with effective salience estimation. In Findings of the Association for Computational Linguistics: ACL 2023, pages 1074310754, Toronto, Canada. Association for Computational Linguistics. Gui-Rong Xue, Jie Han, Yong Yu, and Qiang Yang. 2009. User language model for collaborative personalized search. ACM Trans. Inf. Syst., 27(2). Jianfei Yu, Chenggong Gong, and Rui Xia. 2021. Crossdomain review generation for aspect-based sentiment analysis. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 47674777, Online. Association for Computational Linguistics. Hongyu Zang and Xiaojun Wan. 2017. Towards automatic generation of product reviews from aspectsentiment scores. In Proceedings of the 10th International Conference on Natural Language Generation, pages 168177, Santiago de Compostela, Spain. Association for Computational Linguistics. Xin Zhang, Linhai Zhang, and Deyu Zhou. 2023. Sentiment analysis on streaming user reviews via dualchannel dynamic graph neural network. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 72087220, Singapore. Association for Computational Linguistics. Zhehao Zhang, Ryan A. Rossi, Branislav Kveton, Yijia Shao, Diyi Yang, Hamed Zamani, Franck Dernoncourt, Joe Barrow, Tong Yu, Sungchul Kim, Ruiyi Zhang, Jiuxiang Gu, Tyler Derr, Hongjie Chen, Junda Wu, Xiang Chen, Zichao Wang, Subrata Mitra, Nedim Lipka, Nesreen Ahmed, and Yu Wang. 2024. Personalization of large language models: survey."
        },
        {
            "title": "Metric",
            "content": "PGraphRAG LaMP No-Retrieval Random-Retrieval Task 1: User-Product Review Generation Task 2: Hotel Experiences Generation Task 3: Stylized Feedback Generation Task 4: Multilingual Product Review Generation ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR 0.173 0.124 0.150 0.263 0.156 0.191 0.226 0.171 0. 0.174 0.139 0.133 0.168 0.125 0.134 0.197 0.128 0.121 0.181 0.134 0.147 0.174 0.141 0.125 0.172 0.121 0. 0.224 0.141 0.148 0.177 0.125 0.168 0.173 0.134 0.130 0.126 0.095 0.101 0.211 0.130 0.147 0.142 0.104 0. 0.146 0.117 0.110 Table 22: Zero-shot Validation set results for long text generation using LLaMA-3.1-8B on Tasks 1-4."
        },
        {
            "title": "Metric",
            "content": "PGraphRAG LaMP No-Retrieval Random-Retrieval Task 1: User-Product Review Generation Task 2: Hotel Experiences Generation Task 3: Stylized Feedback Generation Task 4: Multilingual Product Review Generation ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR 0.186 0.126 0.187 0.265 0.152 0.206 0.205 0.139 0. 0.191 0.142 0.173 0.169 0.114 0.170 0.217 0.132 0.161 0.178 0.121 0.178 0.164 0.123 0.155 0.168 0.113 0. 0.222 0.133 0.164 0.177 0.119 0.184 0.167 0.125 0.153 0.157 0.112 0.148 0.233 0.138 0.164 0.168 0.117 0. 0.171 0.131 0.150 Table 23: Zero-shot Validation set results for long text generation using GPT-4o-mini on Tasks 1-"
        },
        {
            "title": "Metric",
            "content": "PGraphRAG LaMP No-Retrieval Random-Retrieval Task 5: User Product Review Title Generation Task 6: Hotel Experience Summary Generation Task 7: Stylized Feedback Title Generation Task 8: Multi-lingual Product Review Title Generation ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR 0.125 0.119 0.117 0.121 0.113 0.105 0.132 0.128 0. 0.132 0.128 0.129 0.114 0.108 0.111 0.119 0.111 0.105 0.128 0.124 0.124 0.128 0.124 0.124 0.111 0.105 0. 0.115 0.108 0.100 0.127 0.122 0.118 0.108 0.104 0.103 0.101 0.095 0.094 0.115 0.107 0.094 0.108 0.104 0. 0.127 0.122 0.118 Table 24: Zero-shot Validation set results for short text generation using LLaMA-3.1-8B for Tasks 5-"
        },
        {
            "title": "Metric",
            "content": "PGraphRAG LaMP No-Retrieval Random-Retrieval Task 5: User Product Review Title Generation Task 6: Hotel Experience Summary Generation Task 7: Stylized Feedback Title Generation Task 8: Multi-lingual Product Review Title Generation ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR ROUGE-1 ROUGE-L METEOR 0.114 0.107 0.119 0.115 0.105 0.105 0.105 0.102 0. 0.108 0.099 0.101 0.106 0.100 0.115 0.115 0.106 0.106 0.101 0.097 0.111 0.106 0.098 0.102 0.109 0.103 0. 0.114 0.106 0.106 0.105 0.101 0.118 0.108 0.099 0.103 0.107 0.102 0.109 0.112 0.103 0.099 0.098 0.093 0. 0.103 0.095 0.095 Table 25: Zero-shot Validation set results for short text generation using GPT-4o-mini on Tasks 5 -"
        },
        {
            "title": "Metric",
            "content": "PGraphRAG LaMP No-retrieval Random-retrieval Task 9: User Product Review Ratings Task 10: Hotel Experience Ratings Task 11: Stylized Feedback Ratings Task 12: Multi-lingual Product Ratings MAE RMSE MAE RMSE MAE RMSE MAE RMSE 0.3272 0.7531 0.3868 0.6989 0.3356 0. 0.5228 0.8483 0.3220 0.7280 0.3685 0.6750 0.3368 0.6859 0.5216 0.8395 0.3200 0. 0.3614 0.6643 0.3372 0.6826 0.5282 0.8519 0.3516 0.7972 0.4008 0.7178 0.3812 0. 0.5392 0.8704 Table 26: Zero-shot validation set results on ordinal classification on Tasks 9-12 on BM25 using MAE and RMSE metrics for LLaMA-3.1-8B-Instruct ."
        },
        {
            "title": "Metric",
            "content": "PGraphRAG LaMP No-retrieval Random-retrieval Task 9: User Product Review Ratings Task 10: Hotel Experience Ratings Task 11: Stylized Feedback Ratings Task 12: Multi-lingual Product Ratings MAE RMSE MAE RMSE MAE RMSE MAE RMSE 0.3652 0.7125 0.3308 0.6056 0.3340 0. 0.4568 0.7414 0.3508 0.6943 0.3472 0.6394 0.3364 0.6545 0.4832 0.7808 0.3484 0. 0.3528 0.6475 0.3356 0.6484 0.4908 0.7897 0.4176 0.7792 0.3640 0.6627 0.3972 0. 0.4820 0.7917 Table 27: Zero-shot validation set results on ordinal classification on Tasks 9-12 on BM25 using MAE and RMSE metrics for GPT-4o-mini . Figure 5: Examples of different prompt configurations used in each task type for PGraphRAG. Teletype text is replaced with realistic data for each task."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "Cisco AI Research",
        "Meta AI",
        "University of California Santa Cruz",
        "University of Oregon",
        "University of Southern California"
    ]
}