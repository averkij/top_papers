{
    "paper_title": "Self-Taught Agentic Long Context Understanding",
    "authors": [
        "Yufan Zhuang",
        "Xiaodong Yu",
        "Jialian Wu",
        "Ximeng Sun",
        "Ze Wang",
        "Jiang Liu",
        "Yusheng Su",
        "Jingbo Shang",
        "Zicheng Liu",
        "Emad Barsoum"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Answering complex, long-context questions remains a major challenge for large language models (LLMs) as it requires effective question clarifications and context retrieval. We propose Agentic Long-Context Understanding (AgenticLU), a framework designed to enhance an LLM's understanding of such queries by integrating targeted self-clarification with contextual grounding within an agentic workflow. At the core of AgenticLU is Chain-of-Clarifications (CoC), where models refine their understanding through self-generated clarification questions and corresponding contextual groundings. By scaling inference as a tree search where each node represents a CoC step, we achieve 97.8% answer recall on NarrativeQA with a search depth of up to three and a branching factor of eight. To amortize the high cost of this search process to training, we leverage the preference pairs for each step obtained by the CoC workflow and perform two-stage model finetuning: (1) supervised finetuning to learn effective decomposition strategies, and (2) direct preference optimization to enhance reasoning quality. This enables AgenticLU models to generate clarifications and retrieve relevant context effectively and efficiently in a single inference pass. Extensive experiments across seven long-context tasks demonstrate that AgenticLU significantly outperforms state-of-the-art prompting methods and specialized long-context LLMs, achieving robust multi-hop reasoning while sustaining consistent performance as context length grows."
        },
        {
            "title": "Start",
            "content": "Self-Taught Agentic Long-Context Understanding Yufan Zhuang1,2, Xiaodong Yu1, Jialian Wu1, Ximeng Sun1, Ze Wang1, Jiang Liu1, Yusheng Su1, Jingbo Shang2, Zicheng Liu1, Emad Barsoum1 1AMD, 2UC San Diego 5 2 0 2 1 2 ] . [ 1 0 2 9 5 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Answering complex, long-context questions remains major challenge for large language models (LLMs) as it requires effective question clarifications and context retrieval. We propose Agentic Long-Context Understanding (AgenticLU), framework designed to enhance an LLMs understanding of such queries by integrating targeted self-clarification with contextual grounding within an agentic workflow. At the core of AgenticLU is Chainof-Clarifications (CoC), where models refine their understanding through self-generated clarification questions and corresponding contextual groundings. By scaling inference as tree search where each node represents CoC step, we achieve 97.8% answer recall on NarrativeQA with search depth of up to three and branching factor of eight. To amortize the high cost of this search process to training, we leverage the preference pairs for each step obtained by the CoC workflow and perform two-stage model finetuning: (1) supervised finetuning to learn effective decomposition strategies, and (2) direct preference optimization to enhance reasoning quality. This enables AgenticLU models to generate clarifications and retrieve relevant context effectively and efficiently in single inference pass. Extensive experiments across seven long-context tasks demonstrate that AgenticLU significantly outperforms stateof-the-art prompting methods and specialized long-context LLMs, achieving robust multi-hop reasoning while sustaining consistent performance as context length grows."
        },
        {
            "title": "Introduction",
            "content": "Large language models have achieved notable milestones in natural language processing, demonstrating exceptional performance in tasks such as mathematical reasoning, code generation, and conversational understanding (OpenAI, 2023; DeepSeekCode and data is available at: https://github.com/ EvanZhuang/AgenticLU. 1 AI, 2025). However, effectively comprehending and utilizing long-context inputs remains major challenge. Complex queries often require models to retrieve multiple relevant pieces of information from extensive contexts and synthesize them coherently. While recent advancements have extended context windows to 128K and even 2M tokens (Dubey et al., 2024; Touvron et al., 2023; Reid et al., 2024), these models still struggle to fully integrate and reason over large-scale contextual information. Recent studies (Liu et al., 2024; Gao et al., 2024) highlight fundamental challenge in long-context understanding: the disparity between models nominal context sizethe theoretical maximum input lengthand its effective context window, the portion of the input the model actively utilizes for reasoning. This gap significantly impacts the understanding performance, limiting the models ability to fully comprehend and integrate long-context information. We introduce novel framework AgenticLU to enhance long-context comprehension in LLMs. As illustrated in fig. 1, the core of AgenticLU is Chainof-Clarifications (CoC), process where models enhance their understanding by generating clarification questions, retrieving relevant information from the long context and answering their own clarification questions based on the gathered evidence. Rather than relying on direct response, CoC helps models refine their reasoning iteratively, resolving uncertainties along the way. We structure the framework into the following two stages. CoC Path Construction. To collect reliable CoC understanding path, we structure data collection as tree search, where each CoC step represents node. We leverage extended inference time to determine the effective clarification questions to ask and the relevant evidence to retrieve. With search depth of three and branching factor of eight, AgenticLU successfully retrieves 97.8% of Figure 1: Overview of the AgenticLU pipeline: The model iteratively refines its understanding of long-context inputs through an agentic workflow. At each step, it raises self-clarifications, retrieves relevant context via the pointback mechanism, and updates its reasoning trace. The framework integrates CoC Path Construction to generate diverse reasoning paths, followed by two-stage fine-tuning (SFT and DPO) to enhance long-context understanding. the correct answers in NarrativeQA (Koˇciský et al., 2018), demonstrating its capability to tackle complex questions that require multi-step reasoning over long-context inputs. CoC Path Distillation. Once the dataset is collected from the tree-search process, we train the model to generate effective clarifications and contextual groundings in single pass, eliminating the need for scaling at inference time. This is achieved by distilling these collected paths into LLMs through supervised finetuning (SFT) and direct preference optimization (DPO) (Rafailov et al., 2024), effectively amortizing the computational cost from inference to training. Our method AgenticLU significantly improves models long-context understanding capabilities without relying on laborious human annotations or stronger teacher models for data generation. Instead, the base models self-generated CoC paths enables it to teach itself to process longcontext inputs more effectively. This approach harnesses the models inherent long-context capabilitiespreviously only accessible through an additional LLM agentallowing it to independently refine its reasoning and retrieval processes. Empirically, we demonstrate that AgenticLU consistently boosts performance across set of questionanswering tasks up to 128K tokens, outperforming both prompting-based approaches and other long-context-finetuned LLMs. By integrating selfclarification and context grounding in an agentic manner, we take step further toward enabling LLMs to comprehend long contexts."
        },
        {
            "title": "2 Related Work",
            "content": "Challenges in Long Context Understanding LLMs struggle with long contexts despite supporting up to 2M tokens (Dubey et al., 2024; Reid et al., 2024). The lost-in-the-middle effect (Liu et al., 2024) and degraded performance on longrange tasks (Li et al., 2023) highlight these issues. To address this, ProLong (Gao et al., 2024) finetunes base models on large, carefully curated long-context corpus. While this approach improves performance on long-range tasks, it comes at significant cost, requiring training with an additional 40B tokens and long-input sequences. Inference-time Scaling for Long-Context The Self-Taught Reasoner (STaR) framework (Zelikman et al., 2022) iteratively generates rationales to refine reasoning, with models evaluating answers and finetuning on correct reasoning paths. Wang et al. (2024b) introduced Model-induced Process Supervision (MiPS), automating verifier training by generating multiple completions and assessing accuracy, boosting PaLM 2s performance on math and coding tasks. Li et al. (2024) proposed an inference scaling pipeline for long-context tasks using Bayes Risk-based sampling and fine-tuning, though their evaluation is limited to shorter contexts (10K tokens) compared to ours (128K tokens). Agentic Workflow for Long-Context Agentic workflows (Yao et al., 2022) enable LLMs to au2 tonomously manage tasks by generating internal plans and refining outputs iteratively. The LongRAG framework (Zhao et al., 2024b) enables an LLM and an RAG module to collaborate on long-context tasks by breaking down the input into smaller segments, processing them individually, and integrating the results to form coherent output. Chain-of-Agents (CoA) (Zhang et al., 2024b) tackles long-context tasks through decomposition and multi-agent collaboration. In CoA, the input text is divided into segments, each handled by worker agent that processes its assigned portion and communicates its findings to the next agent in the sequence. Unlike these, our approach employs single LLM that orchestrates its own reasoning and retrieval without relying on multiple components. By dynamically structuring its process and iteratively refining long-context information, our model reduces complexity while maintaining efficiency."
        },
        {
            "title": "3 The Context Size Gap",
            "content": "State-of-the-art LLMs have made strong claims about their context lengths, supporting hundreds of thousands of input tokens. However, recent studies (Gao et al., 2024; Yen et al., 2024; Shang et al., 2024) have shown that the effective context size of an LLM (the length over which it can reliably perform tasks such as information retrieval and complex reasoning) often diverges from its claimed, or nominal, context length. To illustrate this gap, we evaluate Llama3.1-8BInstruct, which supports 128K-token context, on the HotPotQA dataset to test multi-hop QA performance at various input lengths (8K, 16K, 32K, 64K, and 128K). We artificially expand the input by adding irrelevant context and measure the accuracy of its answers using GPT-4o as judge. As shown in fig. 2, The models performance degrades substantially as increasing context length, demonstrating the discrepancy between nominal and effective context sizes. While expanding nominal context capacity is undoubtedly important, we argue that it is not sufficient for solving all long-context problems. By analogy with computer memory, simply having more capacity does not guarantee efficient or accurate computation; one must also manage the loading of relevant information in and out of this memory. Therefore, we propose an agentic workflow aimed at helping LLMs process and interpret extended contexts more intelligently. Figure 2: Effective context size is smaller than nominal context size. Performance of Llama3.1-8BInstruct (advertised 128K-token context) on the HotPotQA dataset drops sharply as input length increases (8K, 16K, 32K, 64K, 128K), illustrating the gap between nominal and effective context capacities."
        },
        {
            "title": "4 Chain-of-Clarifications Workflow",
            "content": "Our approach centers on enhancing long-context comprehension through an iterative, self-refining process that blends inference-time scaling with agentic reasoning. We coin this agentic workflow Chain-of-Clarifications (CoC). In this section, we detail its key components, including the selfclarification process and the pointback mechanism, as illustrated in fig. 1. Our proposed CoC framework is designed to mitigate the gap between nominal and effective context sizes in large language models. Rather than processing the entire long context and potentially multi-hop questions in single pass, our methodology decomposes the task into sequence of targeted sub-tasks. At each CoC step, the model autonomously: Generates clarifying questions by identifying areas of the long input that require further elaboration or are prone to misinterpretation. Pointbacks to relevant context by using pointback mechanism that highlights critical segments of the context by naming the index of relevant paragraphs. In the data collection phase, this is done by iteratively querying the LLM about the relevance of each paragraph with respect to the question. After training, the model is finetuned to generate the related paragraph indexes directly in single pass. Answers clarifying questions by integrating highlighted context into consideration to build more accurate and contextually grounded understanding of the long document. Answers the original question by combining all newly gathered clarifications, the model attempts to generate valid answer to the original question. It is important to note key distinction between CoC path generation during data collection and the actual task deployment of the agentic workflow. In the data generation phase, we prompt the LLM to iteratively process each chunk of input text along with its self-generated clarifying questions, ensuring accurate retrieval of relevant context. During training, rather than relying on repeated inference calls, we finetune the model to directly generate the indexes of relevant paragraphs using pointback examples, effectively amortizing the computational cost into training. This enables the model to internalize the retrieval process, allowing it to dynamically synthesize relevant clarifications and contextual references at inference time without requiring extensive additional prompting."
        },
        {
            "title": "5 Data Generation & Model Training",
            "content": "Dataset We use the NarrativeQA (Koˇciský et al., 2018) dataset to facilitate long-context QA and generate agentic workflow traces with 14.7K QA pairs in the training set. NarrativeQA is designed for reading comprehension over narrative texts, such as books and movie scripts, where each example includes full story and set of corresponding QA pairs. This dataset emphasizes deeper reasoning and long-context understanding, as many questions require synthesizing information from multiple parts of the narrative rather than focusing solely on particular local context. Its relatively long passages make NarrativeQA particularly suitable for testing and refining agentic reasoning in large language models, as the answers often depend on weaving together details spanning the entire text. is Llama3.1Base Model Our base model 8B-Instruct (Dubey et al., 2024), an 8-billionparameter instruction-tuned Llama model. This model is built on the same transformer architecture as Llama3, but with additional fine-tuning data to improve its performance on multi-turn dialogue and instruction-following tasks. Table 1: Statistics of the generated traces dataset used in finetuning derived from NarrativeQA. We left out 11.9K traces for validation."
        },
        {
            "title": "Num of Traces\nAvg Context Length\nAvg Chosen Response Length\nAvg Rejected Response Length\nTotal Generation Tokens",
            "content": "# 107,550 67,812 165 164 17M In our experiments, we use branching factor of 8 at each depth and select the most promising trace based on an evaluation score that combines: Semantic similarity, measured by the RougeL (Lin, 2004) score relative to the ground truth. Discrete correctness, evaluated by binary verification using GPT4o-mini. In the data construction process, the relevant context is found by iteratively querying the LLM about the relevance of all chunked passages. Here we use 512 as the chunk size. This process is computeintensive but only happens in data collection. After the training, the LLM will directly generate the paragraph numbers of the relevant context as shown in the lower right of fig. 1. For most long-context tasks, single clarification question suffices because the required reasoning is not highly complex. 92% of the questions in our experiments are resolved correctly with just one round of clarification. More challenging tasks may require multiple rounds of clarification: two rounds resolve 53% of the remaining 8%, and three rounds resolve 35% of the remaining 4%. Because of the exponentially increasing costand given that 97.4% of the training questions are already solvedwe limit the maximum depth of our inference scaling to 3. The statistics of the collected dataset are shown in table 1. The total number of conditional generation tokens that the LLM trained on is 17M tokens, with input that has an average length of 67K and max length of 128K tokens."
        },
        {
            "title": "5.2 CoC Path Distillation",
            "content": "We employ test-time scaling approach to generate CoC paths. For each question, we construct tree of search paths where each node represents distinct clarification question posed by the LLM. We employ two-stage finetuning recipe: Supervised Fine-Tuning (SFT) followed by Direct Preference Optimization (DPO) (Rafailov et al., 2024), to convert our base model into long-context under4 standing agent. The dataset statistics is described in table 1, with input length up to 128K tokens. Supervised Fine-Tuning In the first phase, we finetune Llama3.1-8B-Instruct using the generated CoC paths. Each training example includes (1) the full context from NarrativeQA, (2) the question, and (3) the step-by-step reasoning trace leading to the final answer. By exposing the model to these traces, we encourage it to internalize multi-step reasoning strategies and context grounding for the long-context inputs. The SFT stage uses standard cross-entropy loss on the next-token prediction task, ensuring the model learns how to produce consistent and complete reasoning sequences. Direct Preference Optimization In the second phase, we apply Direct Preference Optimization to further refine the models output quality. To create preference pairs, we sample incorrect workflow traces as negative examples with using GPT4omini as the judge for answer correctness from the test-time scaling. DPO explicitly optimizes the model to generate higher-ranked responses more frequently, thus aligning the agents outputs with desirable characteristics, such as clarity, correctness, and coherence. This stage ensures that even among valid reasoning paths, the model learns to prioritize the most instructive reasoning. The details for the two-phase training are listed in appendix A."
        },
        {
            "title": "6 Evaluation",
            "content": "In this section, we assess our method AgenticLU using suite of evaluation tasks drawn from the HELMET long-context benchmark (Yen et al., 2024). Our experiments focus on testing models ability to retain, process, and reason over extended contexts ranging from 8K to 128K tokens."
        },
        {
            "title": "6.1 Tasks and Metrics",
            "content": "We evaluate our models and baselines on the Helmet (Yen et al., 2024) long-context evaluation benchmarks retrieval-augmented generation (RAG) and long-range QA (LongQA) tasks ranging from 8K, 16K, 32K, 64K, to 128K. We use GPT-4o as the judge for answer correctness, with the prompt template shown in appendix E. We report accuracies for all datasets. The RAG test suite includes: (1) HotpotQA (Yang et al., 2018), multi-hop reasoning dataset over Wikipedia; (2) Natural Questions (Kwiatkowski et al., 2019), real user queries with Wikipedia-based short and long answers; (3) TriviaQA (Joshi et al., 2017), large-scale trivia dataset with question-answer pairs linked to evidence documents; (4) PopQA (Mallen et al., 2022), dataset testing model memorization with factbased questions from popular culture. The LongQA test suite includes: (1) NarrativeQA (Koˇciský et al., 2018), reading comprehension dataset with Wikipedia summaries and story-based Q&A; (2) InfiniteBench QA (Zhang et al., 2024a), long-range QA benchmark re- (3) quiring reasoning over extended contexts; InfiniteBench Multiple-Choice (Zhang et al., 2024a), multiple-choice variant of the previous evaluating reading comprehension over long documents. For the four RAG tasks, each question is put alongside set of relevant contexts, and the overall input length is increased by appending irrelevant context. Consequently, these tasks become strictly more difficult as the context window expands. In contrast, for the three LongQA tasks, the relevant context may not appear in the truncated input (the first 8K, 16K, or 128K tokens). Hence, performance might improve at longer input lengths simply because the necessary information becomes available only after including more tokens."
        },
        {
            "title": "6.2 Baselines",
            "content": "We compare AgenticLU against diverse set of strong baselines representing different approaches for handling long-context tasks. Our comparisons include two main categories. Under prompting methods we consider techniques that require no additional model training. In particular, we evaluate (a) the chain-of-thought approach (Kojima et al., 2022), which encourages models to decompose complex questions into intermediate reasoning steps; (b) fact-andreflection prompting (Zhao et al., 2024c), which iteratively verifies and refines factual claims to enhance consistency; (c) plan-and-solve prompting (Wang et al., 2023), where the model first outlines high-level plan before sequentially executing it to address structured reasoning tasks; and (d) LongRAG (Zhao et al., 2024a) where hybrid RAG system is used to retrieve relevant context to generate global summaries and local details 1. In the fine-tuning category, we focus on models 1Note that LongRAG provided finetuned models as well. But the SFT-ed Llama3-8B only supports 8K context length. Thus we did not include it in our comparison. 5 Table 2: Performance difference of AgenticLU and its base, Llama3.1-8B-Instruct (δ =AgenticLU-8B minus Llama3.1-8B), on long context (the 128K tasks) and short-context benchmarks (6 regular tasks including ARC, GSM8K, and MMLU), the details of the short-context performance can be found in appendix B. Scores represent accuracy, with AgenticLU demonstrating significantly improved performance across long-context tasks with minimal effect on regular task performance."
        },
        {
            "title": "Short Avg HotpotQA Natural Questions TriviaQA PopQA NarrativeQA InfiniQA InfiniChoice Long Avg",
            "content": "Llama3.1-8B AgenticLU (δ) 62.3 -0.6 40.0 +31.1 56.1 +21.7 80.6 +7.7 56.1 +9. 38.0 +18.0 48.0 +2.0 55.0 +13.0 53.4 +14.7 Figure 3: Main results on 7 long-context tasks across context lengths from 8K to 128K. Our AgenticLU-8B (dotted orange) achieves significant improvements on all tasks over our base model Llama3.1-8B (solid orange). We also compare with the prompting methods (Step-by-Step, Plan-and-Solve, Fact-and-Reflect, LongRAG) and the state-of-the-art ProLong-8B model. AgenticLU-8B consistently maintains strong performance across most tasks and context lengths. that have been specifically adapted for extended context data. For substantial comparison, we employ Prolong-8B-512K (Gao et al., 2024)a model based on the Llama3 8B architecture that has been further trained on an additional 40B tokens of long-context data."
        },
        {
            "title": "6.3 Main Results",
            "content": "The performance of AgenticLU and baseline models is shown in fig. 3. Self-clarification significantly improves multihop reasoning. AgenticLU-8B consistently surpasses other methods in HotpotQA. By iteratively refining its understanding, resolving ambigu6 ities, and verifying intermediate steps, the model achieves higher accuracy, particularly as context length increases. Table 3: We evaluate the performance of adding additional self-clarification and contextual grounding rounds at inference time. The gain from self-clarification is close to optimal at the initial round. Robust performance across diverse datasets. Unlike baseline models, AgenticLU-8B maintains consistently strong performance across RAG and LongQA benchmarks, demonstrating its ability to adapt effectively to different long-context tasks. Reduced performance degradation with longer contexts. While most models experience significant accuracy drops as context length increases, AgenticLU-8B remains stable. Its self-clarification and pointback mechanisms effectively filter noise from irrelevant information, allowing the model to extract and prioritize essential evidence. Fine-tuning vs. prompting trade-offs. While structured prompting techniques like plan-andsolve improve short-context reasoning, they struggle with extreme context lengths (e.g., 128K tokens). In contrast, AgenticLU-8B, through targeted finetuning with self-clarification and pointback, maintains robust long-context reasoning without relying on complex prompting strategies. Although ProLong-8B, another finetuned model, achieves strong results, it comes with significantly higher training costs. AgenticLU-8B, by contrast, is more data-efficient and generalizes better to novel tasks, making it more practical and effective solution for long-context reasoning. Overall, these results underscore the effectiveness of AgenticLU-8B in tackling long-context understanding challenges. The integration of selfclarification plays crucial role in improving grounding, reasoning, and comprehension in longcontext settings."
        },
        {
            "title": "6.4 Performance on Short-Context Tasks",
            "content": "To demonstrate that our fine-tuning process preserves the models general capabilities while enhancing long-context understanding, we evaluated the finetuned model on diverse set of standard benchmarks. These include elementary and advanced reasoning tasks ARC Easy and ARC Challenge (Clark et al., 2018), mathematical problem-solving GSM8K (Cobbe et al., 2021), MathQA (Amini et al., 2019), and broad knowledge assessment MMLU (Hendrycks et al., 2021b,a), MMLU-Pro (Wang et al., 2024a). We report the average performance across shortcontext tasks in table 2, and each individual task Model HotpotQA NaturalQ PopQA TriviaQA Avg Llama-3.1-8B AgenticLU-8B (w/ 2 rounds) (w/ 3 rounds) 40.0 71.1 71.1 75. 56.1 77.8 76.7 78.8 56.1 65.5 67.2 68.3 80.6 88.3 91.7 91.1 58.2 75.7 76.7 78.4 result can be found in appendix B. We find that the short-context performance is well preserved, demonstrating that AgenticLUs core reasoning and problem-solving abilities remain strong and are not compromised by the significant improvements to its long-context understanding powers."
        },
        {
            "title": "7 Analyses & Ablation Studies",
            "content": "In this section, we take closer look at how each part of our approach affects long-context understanding and retrieval. Specifically, we study three main questions: (1) Can the finetuned system benefit from multi-round CoC? (2) Does adding clarifications and pointing back to the original document help the model understand and utilize the context more accurately? (3) How much additional compute overhead does AgenticLU add to the process?"
        },
        {
            "title": "7.1 How many rounds of CoC are needed?",
            "content": "Setup. We add additional rounds of reasoning in the evaluation and see if the LLM can benefit from multi-rounds of reasoning at test-time. Analysis. The results, presented in Table 3, indicate that additional rounds of agentic reasoning do provide performance improvements. This suggests that while significant benefits of self-clarification are achieved in the first round, additional rounds still contribute to further improvements. One possible explanation is the nature of our dataset: approximately 92% of the questions are resolved within single round of clarification. However, for the remaining cases, extended reasoning allows the model to refine its understanding, leading to measurable gains in performance with more clarification and reasoning."
        },
        {
            "title": "7.2 Do Self-Clarifications and Pointback Help",
            "content": "in Long-Context Understanding? Setup. To evaluate the impact of each component in our agentic workflow, we compare the full AgenticLU-8B model against two variants: 7 Table 4: We test the agentic workflow with AgenticLU8B when taking out the self-clarification steps and the contextual grounding (pointback) step. The tasks are with 128K context length. Model HotpotQA NaturalQ PopQA TriviaQA Avg Llama-3.1-8B AgenticLU-8B (w/o Clarification) (w/o Pointback) 40.0 71.1 57.8 53.3 56.1 77.8 56.7 59.4 56.1 65.5 55.5 52.7 80.6 88.3 78.3 83. 58.2 75.7 62.1 62.2 Table 5: Performance Overhead Comparison between direct answering baseline and AgenticLU."
        },
        {
            "title": "Runtime Overhead\nAvg Tokens Generated in One Round",
            "content": "100% 76.28 101.93% 1205.38 one without the self-clarification step and another without the contextual grounding (pointback) step. We use the four RAG datasets with 128K context length as the evaluation benchmark, and compare the performance alongside the original model. Analysis. Table 4 shows the results on four QA benchmarks with 128K context length. Removing self-clarification leads to an absolute performance drop of at least 10 points across most tasks (e.g., from 71.1% to 57.8% on HotpotQA), confirming that the model benefits from clarifying its own uncertainties when the context is long. Meanwhile, omitting pointback yields degenerate results, indicating that pinpointing relevant information at each stage is crucial for long-context QA. Overall, these findings highlight the importance of both clarifications and context-grounding to maximize retrieval accuracy and robustness in lengthy documents."
        },
        {
            "title": "7.3 How much additional compute cost does\nAgenticLU impose in generation?",
            "content": "Since additional generation steps are introduced in the QA process, we assess the overhead in inference time. Naïvely, long-context inference and multi-round conversations could significantly amplify compute costs. However, by leveraging prefix caching to store computed KV caches, the additional cost scales linearly with the number of newly generated tokens rather than exponentially. To quantify this overhead, we conduct runtime evaluation on 100 queries with 128K context size. The results, summarized in table 5, demonstrate that the additional computational overhead remains minimal when using prefix caching."
        },
        {
            "title": "8 Conclusion",
            "content": "In this work, we introduce Agentic Long-Context Understanding (AgenticLU), framework designed to enhance large language models ability to process and reason over long-context inputs with self-generated data. By incorporating an agentic workflow (CoC) that dynamically refines model reasoning through self-clarifications and contextual grounding, AgenticLU significantly improves LLMs long context understanding capabilities. Through combination of trace data collection and two-stage post-training, our approach enables models to autonomously explore multiple reasoning paths, distill the most effective clarification strategies, and improve their understanding of lengthy documents. Extensive evaluations on long-context benchmarks demonstrate that AgenticLU outperforms existing prompting techniques and finetuned baselines, maintaining strong performance across context lengths up to 128K tokens. Additionally, ablation studies confirm that self-clarification and pointback mechanisms play crucial role in improving retrieval and reasoning over long-contexts."
        },
        {
            "title": "Limitations",
            "content": "Despite its effectiveness in long-context reasoning, AgenticLU has notable limitations. One key drawback is its inability to autonomously determine when to stop multi-round reasoning. While additional rounds of self-clarification can improve performance, the model follows fixed number of reasoning steps rather than dynamically assessing when further refinement is necessary. This can lead to inefficiencies, where the model either stops too early, missing potential improvements, or continues reasoning unnecessarily, expending computational resources without significant gains. Developing fully agentic mechanism remains an open challenge. Ideally, the model should assess its confidence in an intermediate response and decide whether further clarification is needed. Future work should explore approaches that enable AgenticLU to regulate its reasoning depth dynamically, optimizing both efficiency and performance."
        },
        {
            "title": "References",
            "content": "Aida Amini, Saadia Gabriel, Peter Lin, Rik KoncelKedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019. Mathqa: Towards interpretable math word 8 problem solving with operation-based formalisms. Preprint, arXiv:1905.13319. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv, abs/1803.05457. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. Preprint, arXiv:2110.14168. Tri Dao. 2023. Flashattention-2: Faster attention with arXiv better parallelism and work partitioning. preprint arXiv:2307.08691. DeepSeek-AI. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Preprint, arXiv:2501.12948. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Tianyu Gao, Alexander Wettig, Howard Yen, and Danqi Chen. 2024. How to train long-context arXiv preprint language models (effectively). arXiv:2410.02660. Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. 2021a. Aligning ai with shared human values. Proceedings of the International Conference on Learning Representations (ICLR). Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199 22213. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453 466. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Jiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. 2023. Loogle: Can long-context language models understand long contexts? arXiv preprint arXiv:2311.04939. Siheng Li, Cheng Yang, Zesen Cheng, Lemao Liu, Mo Yu, Yujiu Yang, and Wai Lam. 2024. Large language models can self-improve in long-context reasoning. arXiv preprint arXiv:2411.08147. Chin-Yew Lin. 2004. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain. Association for Computational Linguistics. Hao Liu, Matei Zaharia, and Pieter Abbeel. 2023. Ring attention with blockwise transformers for nearinfinite context. arXiv preprint arXiv:2310.01889. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021b. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR). Nelson Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173. Jian Hu, Xibin Wu, Weixun Wang, Dehao Zhang, Yu Cao, et al. 2024. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143. Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Vancouver, Canada. Association for Computational Linguistics. Tomáš Koˇciský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. 2018. The NarrativeQA reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317328. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, and Daniel Khashabi. 2022. When not to trust language models: Investigating effectiveness and limitations of parametric and nonparametric memories. arXiv preprint. OpenAI. 2023. GPT-4 technical report. Preprint, arXiv:2303.08774. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th 9 ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 35053506. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530. Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Hao, Xu Han, Zhen Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun. 2024a. Bench: Extending long context evaluation beyond In Proceedings of the 62nd Annual 100K tokens. Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15262 15277, Bangkok, Thailand. Association for Computational Linguistics. Jingbo Shang, Zai Zheng, Jiale Wei, Xiang Ying, Felix Tao, and Mindverse Team. 2024. Ai-native memory: pathway from llms towards agi. arXiv preprint arXiv:2406.18312. Yusen Zhang, Ruoxi Sun, Yanfei Chen, Tomas Pfister, Rui Zhang, and Sercan Ö Arik. 2024b. Chain of agents: Large language models collaborating on longcontext tasks. arXiv preprint arXiv:2406.02818. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023. Planand-solve prompting: Improving zero-shot chain-ofthought reasoning by large language models. arXiv preprint arXiv:2305.04091. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. 2024a. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. Preprint, arXiv:2406.01574. Zihan Wang, Yunxuan Li, Yuexin Wu, Liangchen Luo, Le Hou, Hongkun Yu, and Jingbo Shang. 2024b. Multi-step problem solving through verifier: An empirical analysis on model-induced process supervision. arXiv preprint arXiv:2402.02658. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23692380, Brussels, Belgium. Association for Computational Linguistics. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629. Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat, and Danqi Chen. 2024. Helmet: How to evaluate longcontext language models effectively and thoroughly. arXiv preprint arXiv:2410.02694. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488. Qingfei Zhao, Ruobing Wang, Yukuo Cen, Daren Zha, Shicheng Tan, Yuxiao Dong, and Jie Tang. 2024a. LongRAG: dual-perspective retrieval-augmented generation paradigm for long-context question answering. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 2260022632, Miami, Florida, USA. Association for Computational Linguistics. Xiaowei Zhao, Yong Zhou, and Xiujuan Xu. 2024b. Dual encoder: Exploiting the potential of syntactic and semantic for aspect sentiment triplet extraction. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 54015413, Torino, Italia. ELRA and ICCL. Xinran Zhao, Hongming Zhang, Xiaoman Pan, Wenlin Yao, Dong Yu, Tongshuang Wu, and Jianshu Chen. 2024c. Fact-and-reflection (FaR) improves confidence calibration of large language models. In Findings of the Association for Computational Linguistics: ACL 2024, pages 87028718, Bangkok, Thailand. Association for Computational Linguistics."
        },
        {
            "title": "A Training Configurations",
            "content": "We employed the DeepSpeed (Rasley et al., 2020) framework for distributed training across four GPU nodes, each equipped with four AMD MI250 GPUs. We used vLLM (Kwon et al., 2023) for inference. Our implementation builds upon OpenRLHF (Hu et al., 2024) for both SFT and DPO. Given the input sequence length of up to 128K tokens, we leveraged FlashAttention-2 (Dao, 2023) alongside Ring Attention (Liu et al., 2023) to efficiently process extremely long sequences. The detailed hyperparameters for SFT and DPO are provided in table 6 and table 7."
        },
        {
            "title": "B Short Context Performance",
            "content": "As shown in table 8, we evaluate the shortcontext perfromance across six tasks: ARC Easy, ARC Challenge, GSM8K, MathQA, MMLU, and MMLU Pro. AgenticLU performs on par with 10 Table 6: Hyperparameters for SFT. Chain-of-Clarifications Workflow Prompts your with building responses conversations, an AI context assistant reasoning. [System Prompt] specialized are You in Analyze long information thoroughly while maintaining clarity and focus. Track the full context of connections between concepts and flagging when context Break down complex review is needed. showing your problems into components, reasoning steps and stating key assumptions. clear Structure headers and periodic summaries. Present evidence for your conclusions, acknowledge uncertainties, and request clarification when needed. Keep your analysis organized, explicit, and focused on addressing the core question. [Long-Context Input] <para 1> [chunk 1] </para 1> <para 2> [chunk 2] </para 2> ... {Question} [Self Clarification - Raise Question] In order to answer this question, ask one question about what you want to know in order to better answer it. [Contextual Grounding - Pointback] Help me find relevant context to answer the previous clarifying question. [Self Clarification - Answer Question] Based on the relevant context, answer the previous clarifying question. [Answer the Original Question] Now, lets answer the final question. Be concise in your answer."
        },
        {
            "title": "E Evaluation Template",
            "content": "We use GPT-4o (OpenAI, 2023) to judge if the models answer is correct. The specific prompt template with the structured output class is shown below."
        },
        {
            "title": "Verification Prompts",
            "content": "Please verify the following answer: Question: question Ground Truth Answers: ground truth Predicted Answer: answer Your task is to determine whether the predicted answer correctly matches the ground truth. Focus on overall correctness and provide detailed explanation in the following format: class VerificationResult: explanation: str # Justification confidence: float # Confidence score in the range [0,1] correct_answer: prediction is correct, otherwise False"
        },
        {
            "title": "True",
            "content": "bool the if #"
        },
        {
            "title": "Value",
            "content": "Learning Rate Learning Rate Schedule Optimizer β1 β2 Training dtype Batch Size Max Length 5e-7 Cosine Annealing Adam 0.9 0.95 bf16 128 131,072 Table 7: Hyperparameters for DPO."
        },
        {
            "title": "Value",
            "content": "Learning Rate Learning Rate Schedule Optimizer β1 β2 Training dtype Batch Size β Max Length 5e-7 Cosine Annealing Adam 0.9 0.95 bf16 128 0.1 131,072 the base model Llama3.1-8B-Instruct on shortcontext benchmarks, demonstrating that AgenticLU preserves the original short-context ability while greatly enhancing long-context performance."
        },
        {
            "title": "Tasks",
            "content": "As shown in table 9, table 10, table 11, table 12, table 13, table 14 and table 15, we evaluate the long-context performance across seven tasks: HotpotQA, Natural Questions, TriviaQA, PopQA, NarrativeQA, InfiniteBench QA and InfiniteBench Multiple-Choice. AgenticLU provides significant improvement for all tasks, especially for those that require multi-hop reasoning such as HotPotQA. Chain-of-Clarifications Workflow The input was first processed into chunks and grouped with paragraph tags. We list the example prompts used in AgenticLU workflow below. In training, we sampled 100 variations of the same prompt text and use them randomly to avoid training collapse. 11 Table 8: Performance comparison of AgenticLU and Llama3.1-8B-Instruct on short-context benchmarks. Scores represent accuracy percentages, with AgenticLU demonstrating matching results across tasks."
        },
        {
            "title": "Model",
            "content": "ARC Easy ARC Challenge GSM8k MathQA MMLU MMLU Pro Avg Llama3.1-8B AgenticLU-8B 84.80 83.96 59.64 58.36 80.13 80.51 42.88 41. 68.72 68.38 37.71 37.51 62.31 61.74 HotpotQA Model 8K 16K 32K 64K 128K Llama3.1-8B Llama3.1-8B+step-by-step Llama3.1-8B+plan&solve Llama3.1-8B+fact&reflect ProLong-8B Llama3.1-8B+LongRAG AgenticLU-8B 63.3 60.0 71.1 58.9 62.2 61.1 81.1 56.7 66.7 66.7 58.9 65.6 58.9 75.6 61.1 56.7 72.2 62.2 57.8 73.3 78.9 47.8 58.9 62.2 61.1 53.3 56.7 75. 40.0 56.7 50.0 48.9 58.9 57.8 71.1 Table 9: Long-context performance on HotpotQA. Model InfbenchQA 8K 16K 32K 64K 128K Llama3.1-8B Llama3.1-8B+step-by-step Llama3.1-8B+plan&solve Llama3.1-8B+fact&reflect ProLong-8B Llama3.1-8B+LongRAG AgenticLU-8B 17.0 21.0 17.0 19.0 16.0 16.7 25.0 31.0 36.0 26.0 30.0 31.0 23.3 39.0 36.0 36.0 32.0 40.0 29.0 36.7 42.0 40.0 45.0 41.0 42.0 31.0 43.3 47.0 48.0 43.0 40.0 37.0 45.0 36.7 50. Table 14: Long-context performance on InfbenchQA. Model InfbenchChoice 8K 16K 32K 64K 128K Llama3.1-8B Llama3.1-8B+step-by-step Llama3.1-8B+plan&solve Llama3.1-8B+fact&reflect ProLong-8B Llama3.1-8B+LongRAG AgenticLU-8B 9.0 15.0 27.0 20.0 22.0 16.7 45.0 12.0 13.0 15.0 14.0 27.0 30.0 46.0 24.0 41.0 48.0 38.0 37.0 43.3 47.0 39.0 41.0 55.0 51.0 48.0 53.3 64.0 55.0 44.0 58.0 56.0 58.0 63.3 68.0 Table 15: Long-context performance on InfbenchChoice. Model Nature Questions 8K 16K 32K 64K 128K Llama3.1-8B Llama3.1-8B+step-by-step Llama3.1-8B+plan&solve Llama3.1-8B+fact&reflect ProLong-8B Llama3.1-8B+LongRAG AgenticLU-8B 71.7 66.7 67.8 63.3 83.3 65.6 91. 69.4 66.1 71.7 63.3 82.2 76.1 91.1 70.6 58.9 66.7 61.7 83.9 79.4 85.0 73.9 55.6 62.2 59.4 90.0 77.2 85.0 56.1 38.9 50.6 40.0 77.8 73.9 77.8 Table 10: Long-context performance on Nature Questions. TriviaQA Model 8K 16K 32K 64K 128K Llama3.1-8B Llama3.1-8B+step-by-step Llama3.1-8B+plan&solve Llama3.1-8B+fact&reflect ProLong-8B Llama3.1-8B+LongRAG AgenticLU-8B 82.8 84.4 78.9 87.8 71.1 77.2 88.3 86.7 86.1 88.3 83.9 88.3 79.4 92. 85.6 90.0 89.4 84.4 78.9 83.9 91.1 81.1 82.2 87.2 86.7 82.8 83.9 93.3 80.6 57.2 86.7 84.4 78.3 83.3 88.3 Table 11: Long-context performance on TriviaQA. PopQA Model 8K 16K 32K 64K 128K Llama3.1-8B Llama3.1-8B+step-by-step Llama3.1-8B+plan&solve Llama3.1-8B+fact&reflect ProLong-8B Llama3.1-8B+LongRAG AgenticLU-8B 61.1 61.7 62.2 65.0 67.8 47.8 82.2 62.8 58.9 63.3 64.4 68.3 54.4 82.2 57.2 55.0 58.9 58.9 70.0 54.4 78. 58.3 58.9 55.0 53.3 64.4 57.2 76.7 56.1 60.6 61.1 65.0 65.6 50.6 65.6 Table 12: Long-context performance on PopQA."
        },
        {
            "title": "Model",
            "content": "8K 16K 32K 64K 128K Llama3.1-8B Llama3.1-8B+step-by-step Llama3.1-8B+plan&solve Llama3.1-8B+fact&reflect ProLong-8B Llama3.1-8B+LongRAG AgenticLU-8B 15.0 23.0 22.0 18.0 18.0 23.3 27.0 19.0 30.0 25.0 35.0 27.0 23.3 35.0 27.0 36.0 38.0 37.0 28.0 50.0 41. 35.0 51.0 41.0 42.0 38.0 50.0 49.0 38.0 43.0 39.0 46.0 42.0 46.7 56.0 Table 13: Long-context performance on NarrativeQA."
        }
    ],
    "affiliations": [
        "AMD",
        "UC San Diego"
    ]
}