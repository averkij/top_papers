{
    "paper_title": "LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents",
    "authors": [
        "Hyesung Jeon",
        "Hyeongju Ha",
        "Jae-Joon Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Role specialization in multi-LLM agent systems is often realized via multi-LoRA, where agents share a pretrained backbone and differ only through lightweight adapters. Despite sharing base model weights, each agent independently builds and stores its own KV cache for the same long, tool-augmented trajectories, incurring substantial memory and compute overhead. Existing KV cache sharing methods largely overlook this multi-LoRA setting. We observe that, across agents, cache differences are dominated by adapter outputs, while activations from the shared pretrained backbone remain highly similar. Based on this observation, we propose LRAgent, a KV cache sharing framework for multi-LoRA agents that decomposes the cache into a shared base component from the pretrained weights and an adapter-dependent component from LoRA weights. LRAgent reduces memory overhead by sharing the base component and storing the adapter component in its inherent low-rank form, and further reduces compute overhead, enabled by shared-$A$ multi-LoRA architectures, by also sharing the low-rank cache and avoiding redundant computations for contexts already processed by other agents. To efficiently reconstruct adapter contributions at runtime, we introduce Flash-LoRA-Attention, a kernel that reorders attention computation to avoid materializing the low-rank cache to full dimension. LRAgent achieves throughput and time-to-first-token latency close to fully shared caching, while preserving accuracy near the non-shared caching baseline across agentic question-answering benchmarks."
        },
        {
            "title": "Start",
            "content": "LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents Hyesung Jeon 1 Hyeongju Ha 1 Jae-Joon Kim"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction 6 2 0 2 1 ] . [ 1 3 5 0 1 0 . 2 0 6 2 : r Role specialization in multi-LLM agent systems is often realized via multi-LoRA, where agents share pretrained backbone and differ only through lightweight adapters. Despite sharing base model weights, each agent independently builds and stores its own KV cache for the same long, tool-augmented trajectories, incurring substantial memory and compute overhead. Existing KV cache sharing methods largely overlook this multi-LoRA setting. We observe that, across agents, cache differences are dominated by adapter outputs, while activations from the shared pretrained backbone remain highly similar. Based on this observation, we propose LRAgent, KV cache sharing framework for multiLoRA agents that decomposes the cache into shared base component from the pretrained weights and an adapter-dependent component from LoRA weights. LRAgent reduces memory overhead by sharing the base component and storing the adapter component in its inherent lowrank form, and further reduces compute overhead, enabled by shared-A multi-LoRA architectures, by also sharing the low-rank cache and avoiding redundant computations for contexts already processed by other agents. To efficiently reconstruct adapter contributions at runtime, we introduce Flash-LoRA-Attention, kernel that reorders attention computation to avoid materializing the low-rank cache to full dimension. LRAgent achieves throughput and time-to-first-token latency close to fully shared caching, while preserving accuracy near the non-shared caching baseline across agentic question-answering benchmarks. Code available at https://github. com/hjeon2k/LRAgent. 1Department of Electrical and Computer Engineering, Seoul National University, Seoul, Korea. Correspondence to: Jae-Joon Kim <kimjaejoon@snu.ac.kr>. Preprint. February 3, 2026. 1 Recently, LLMs have been widely adopted in agent systems due to their long-context understanding ability (Dubey et al., 2024; Jiang et al., 2023; Yang et al., 2025a), reasoning ability (Wei et al., 2022; Yao et al., 2023a; Snell et al., 2024), and external tool interaction capabilities (Yao et al., 2023b; Shen et al., 2024; Qin et al., 2024). In particular, multi-LLM agent systems have gained increasing attention for their ability to assign specialized roles to multiple agents that collaboratively decompose and solve complex tasks (Talebirad & Nadiri, 2023; Wu et al., 2024; Rasal, 2024; Zhang et al., 2025). These agents retrieve information from external tools, augment it with generated outputs, and pass the accumulated context, referred to as trajectories, to other agents for subsequent steps. To improve accuracy, common approach is to fine-tune pretrained model separately for each agent role. These fine-tuned models are typically trained on pre-generated trajectories that reflect role-specific behavior and tool usage patterns (Shinn et al., 2023; Bo et al., 2024; Liu et al., 2025a; Bai et al., 2025; Fu et al., 2025a). Parameter-efficient fine-tuning (PEFT) methods, such as low-rank adaptation (LoRA) (Hu et al., 2022), further enhance scalability by reducing the number of trainable parameters from the full model to pair of low-rank matrices. As result, multi-LoRA architectures enable agents to share the large pretrained backbone during inference while retraining lightweight, role-specific adapters (Wang et al., 2023; Xia et al., 2024). This design has proven effective in practice, consistently outperforming single-model agents and non-fine-tuned baselines in agentic tasks (Qiao et al., 2024; Yu et al., 2024; Liu et al., 2025b; Li et al., 2025). Due to the long trajectories in LLM agent systems, KV cache overhead and compute overhead become more severe in multi-agent systems than in single-agent settings, because each agent maintains its own KV cache and redundant prefills occur even though large portion of the context is shared. This redundancy increases both memory usage and inference latency. To mitigate the memory issue, recent work has explored KV cache sharing across agents. However, existing approaches either require architectural modifications and additional training for cache fusion (Woo et al., 2025; Fu et al., 2025b), focus mainly on handling positional misalignment caused by agent-specific prefixes (Yang et al., 2025b; Pan et al., 2025; Ye et al., 2025), or rely on selective LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents recomputation of certain tokens or layers (Yao et al., 2025; Liu et al., 2026). Furthermore, these works focus primarily on memory reduction but still incur redundant computation to build hidden state for context that has already been processed by other agents. Importantly, KV cache sharing schemes that explicitly exploit the multi-LoRA architecture remain largely unexplored. In this work, we make key observation that, for the same context, cache discrepancies across agents are dominated by task-specific LoRA-induced outputs, while activations produced by the shared pretrained backbone remain highly similar. Motivated by this observation, we propose LRAGENT, KV cache sharing framework tailored to multi-LoRA agent systems. We decompose the cache into two parts: shared component computed from the pretrained weights, which we call the base cache, and an agent-dependent component induced by the LoRA weights, which we call the adapter outputs. The next key property we exploit is that the adapter output naturally admits low-rank representation. Specifically, we store the intermediate activations produced right after the LoRA down-projection, which have small rank dimension. We refer to these activations as the LR cache. At runtime, we reconstruct the full-dimension adapter contribution from LR cache by multiplying it with the LoRA up-projection matrix only when needed. As result, we compress multiple KV caches into single shared base cache with lightweight LR caches. Based on this concept, we introduce two cache sharing schemes. BaseShared shares the base cache across all agents while maintaining separate LR cache per agent, substantially reducing KV cache memory. Furthermore, motivated by recent multi-LoRA variants that share the down-projection matrix across tasks (Tian et al., 2024; Yang et al., 2025c), we extend our idea to BaseLRShared, which also shares the LR cache as well as the base cache by aligning agents to use common down-projection. This extension further reduces both memory usage and the amount of computation for previously seen contexts. To minimize the runtime overhead of reconstructing adapter contribution using LR cache, we design Flash-LoRA-Attention, which reorders attention computation to avoid materializing low-rank caches to full dimension and implements this strategy efficiently on top of FlashAttention (Dao et al., 2022; 2023). Overall, our approach enables KV cache sharing tailored to the multi-LoRA architecture, achieving memory and inference efficiency close to fully shared KV caching while preserving accuracy near the non-shared KV baseline. 2. Background 2.1. Multi-LoRA Architecture LoRA LoRA (Hu et al., 2022) is PEFT method that adapts model weights by adding pair of low-rank matrices to the frozen pretrained base weights. Formally, LoRA parameterizes the weight update as: = W0 + W, = AB, (1) where W0 Rdindout is the pretrained base weight, Rdinr is the down-projection matrix, and Rrdout is the up-projection matrix, with rank min(din, dout). Since only and are optimized instead of W0, it significantly reduces the number of trainable parameters, leading to lower memory usage and computation compared to full fine-tuning. In particular, applying LoRA to the query and value projections yields the best accuracy for given number of parameters and is therefore widely used in practice. Unless otherwise stated, we apply LoRA to the query and value projections in all experiments. Multi-LoRA typical multi-LoRA system augments pretrained base weight with multiple task-specific lowrank weights (Xia et al., 2024). For each task index {0, 1, . . . , 1}, where is the number of tasks, and given LoRA weights Ai Rdinr and Bi Rrdout, the task-specific weight used for inference is: Wi = W0 + Wi, Wi = AiBi. (2) Given an input activation tensor for task i, Xi Rldin , with sequence length l, the output Yi Rldout and the adapter output Yi are computed as: Yi = XiWi = XiW0 + (XiAi)Bi, Yi = XiWi = (XiAi)Bi, where XiAi Rlr is the intermediate activation produced by the down-projection. (3) Multi-LoRA with Shared-A Recent works report that task-specific differences in multi-LoRA systems are driven primarily by the up-projection matrices Bi, while the downprojection matrices Ai encode highly similar intrinsic information across tasks and datasets (Tian et al., 2024; Yang et al., 2025c). Building on this observation, sharing downprojection matrix can improve accuracy compared to conventional multi-LoRA systems, since is trained to be more generalizable across tasks. For example, HydraLoRA (Tian et al., 2024) introduces trainable router that produces token-wise mixture weights over multiple up-projections for subtasks within dataset. In our setting, agent roles are predefined and do not require sequence-dependent routing. We therefore adopt the shared-A design and simplify the router to static, sequence-wise assignment. This yields the same overall architecture as conventional multi-LoRA, except that the down-projections share weights, and we also find that it improves accuracy. Based on this, we design cachesharing strategies that support both standard multi-LoRA and the shared-A variant, with the latter further leveraging the efficiency benefits of our approach. Overall, our method reduces memory usage and latency while preserving each agents role-specific behavior. 2 LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents 2.2. Multi-LLM Agent KV Cache Sharing Multi-LLM Agent Multi-LLM agent systems can be implemented either with single model that plays different roles via role-specific system prompts, which can be viewed as form of in-context learning, or with multiple models, often by fine-tuning the same backbone model for different roles (Talebirad & Nadiri, 2023; Wu et al., 2024; Shinn et al., 2023; Liu et al., 2025a; Fu et al., 2025a). These systems commonly use three agent roles: planning agent for reasoning, an action agent for tool use, and reflection agent for revising the answer. Methods that incorporate fine-tuning for role specialization often synthesize agent-trajectory datasets using instruction-following models (Touvron et al., 2023; OpenAI et al., 2024; DeepSeek-AI et al., 2024), and then fine-tune with PEFT methods such as LoRA. These approaches have been shown to outperform both single-model agents and non-fine-tuned baselines (Qiao et al., 2024; Yu et al., 2024; Liu et al., 2025b), leading to broad adoption in practice. In this work, we follow AutoAct (Qiao et al., 2024) and fine-tune LoRA adapters for the plan, action, and reflection agents using the provided agent trajectory dataset. KV Cache Sharing Meanwhile, due to long trajectories from multi-step reasoning and multiple retrieval of large contexts from external tools, memory and compute overhead becomes much more pronounced in multi-agent settings than in single-agent scenarios. This is because each agent maintains its own KV cache even though much of the context is shared. Moreover, the same context is processed independently by multiple agents, leading to substantial computation. Together, these effects introduce memory and compute redundancy, increasing memory usage and latency. To mitigate the memory issue, recent works have explored KV cache sharing for context that is shared across agents. Most approaches either introduce new model architecture that necessitates additional training to enable cache sharing, or handle positional misalignment so that precomputed KV caches for overlapping context chunks can be reused within single model. ICaRus (Woo et al., 2025) proposes decoder architecture that fine-tunes only the query projections for downstream tasks, enabling agents to share an encoder-generated cache and reconstruct task-specific additive caches at runtime. Cache2Cache (Fu et al., 2025b) adds trainable linear layer that project and fuse source models KV cache into target model. KVLink (Yang et al., 2025b), KVFlow (Pan et al., 2025), and KVComm (Ye et al., 2025) address positional misalignment from agentspecific prefixes by aligning cache offsets and adjusting positional embedding at runtime, allowing reuse of overlapping context despite divergent prefixes within single model. Other works rely on selective recomputation. For instance, CacheBlend (Yao et al., 2025) reuses KV caches by recomputing small subset of tokens that are critical for accuracy. DroidSpeak (Liu et al., 2026) enables KV Table 1. Average cosine similarity across agent pairs for the base cache, full cache, and adapter output, on the same context. Model Full cache Base cache Adapter output LLaMA-3.1-8B-Instruct Ministral-8B-Instruct 0.9576 0.9200 0.9726 0. 0.0538 0.0225 cache reuse across fine-tuned LLMs that share the same backbone by selectively recomputing KV cache of predefined set of critical layers while reusing the KV cache for the remaining layers, reporting higher accuracy than token-wise recomputation methods such as CacheBlend. In addition, DroidSpeak introduces hidden state cache to skip recomputation for the initial few non-recomputed layers and directly feed to the first recomputed layer. However, although DroidSpeak is the closest to our work, it still processes hidden states for already seen context, similar to other approaches, so it reduces only the key and value projections for cache updates while leaving most computation unchanged. This highlights that fully reusing KV caches to eliminate computation for redundant context across models remains challenging. Moreover, KV cache sharing tailored to multi-LoRA systems remains largely overlooked despite their wide deployment in practice. To the best of our knowledge, this is the first work that explicitly tailors KV cache sharing to multi-LoRA agent settings and our work is complementary to prior cache-sharing methods. 3. Methodology 3.1. Cache Similarity Across the Agents We first demonstrate that, for the same context, differences in cache values mainly arise from the adapter output, which is small in magnitude (see Appendix A.1) but contains critical agent-specific information. Applying Equation 3 to the value projections, where the input Xi corresponds to the hidden states obtained by running agent on given context, Figure 1 and Table 1 show that the base cache Ybase,i = XiW0 remains highly similar across agents on the same context. In contrast, the adapter output Yi = XiWi acts as largely decorrelated perturbation, with cosine similarity close to zero. With these small, decorrelated adapter output, the expected cosine similarity of the full cache Yi = Ybase,i + Yi is lower than that of the base cache, empirically by about 3% on average, where concrete derivation is provided in Appendix A.2. This motivates sharing only the base cache, rather than the full cache, to better preserve the small but critical agent-specific contributions from the adapters. Otherwise, the discrepancy accumulates over iterative agent executions and can lead to noticeable accuracy drop. We also note that the cosine similarity of the key cache remains above 0.98 on average (see Appendix A.3), suggesting that value cache management is the key factor for preserving accuracy in multi-agent inference. LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents Figure 1. (Left) Relationship between the full cache, base cache, and adapter output. (Right) Layer-wise pairwise cosine similarity of the base and full caches, measured on the same context across three agent pairs using 128 samples of 2k tokens from the HotpotQA dataset. Here, we share single base cache across agents even though the layer inputs Xi are not exactly identical, motivated by the observations in Section 3.1, which show that the base cache XiW0 remains highly similar across agents on the same context and that the remaining differences are dominated by the adapter contribution. Concretely, for each newly appended context, the agent that processes the context first materializes the base cache and stores it in memory. When another agent later processes the same context, it reuses the stored base cache without recomputation of value projections as illustrated in Figure 2(b). We refer to this scheme as BaseShared. For the adapter output, naively storing it in full-dimension form would largely negate the benefit of sharing, since keeping full-dimension cache per agent in addition to the base cache increases the total cache size to 1 + 1/N times that of the default non-shared scheme. Instead, we store the adapter output in its inherent low-rank form as the intermediate output Ylr,i = XiAi, which we call the LR cache, and reconstruct the required contribution at runtime via the upprojection as Ylr,iBi. In addition, since key cache similarity is sufficiently high across agents, we fully share the key cache. While the same idea can also be applied to the key projection when LoRA is used, LoRA is typically applied to the query and value projections for the best accuracy, so we focus on the value projection in our main implementation. Further accuracy and efficiency analysis for LoRA applied to the key projection is presented in Appendix D.1. Figure 3 illustrates forward pass that agent processes context segment of length Lc given prefilled context of length Lp by agent i. Here, the LR cache is accumulated over the sequence, and later expanded into the full-dimension adapter contribution via the up-projection Bi, then added to the base cache. In terms of memory, BaseShared maintains single shared base cache along with lightweight per-agent LR caches. Because each LR cache is smaller than the full cache by factor of r/dout 1, the total KV cache size is reduced to 1/N + r/dout 1/N of the non-shared scheme. However, in terms of computation, since only the base cache is shared, switching agents requires constructing the LR cache Figure 2. Agent iteration and cache accumulation for Non-Shared, BaseShared, and BaseLRShared. T0 denotes the system prompt shared across agents, and Ti denotes trajectory context blocks, formed by concatenating model-generated tokens and retrieved context from external sources. BaseShared shares only the base cache and maintains separate LR cache per agent, whereas BaseLRShared shares both the base and LR caches. 3.2. BaseShared and BaseLRShared In this section, we present KV cache sharing schemes tailored to the multi-LoRA architecture. Our key idea is to decouple the value cache into shared component (base cache) produced by the pretrained weights, and an adapter-dependent component. We reuse the base cache across agents without recomputation, and store the adapterdependent component in compact low-rank form (LR cache) that is expanded to full-dimension form on demand at runtime. We first introduce BaseShared, which primarily reduces memory usage, and then extend it to BaseLRShared, which leverages shared-A multi-LoRA architecture and further reduces both computation and memory usage. Figure 2 illustrates the agent execution flow and the corresponding cache management in our methods, which we discuss in detail in the following paragraphs. BaseShared We first decouple the value cache into base component Ybase and an adapter-dependent component Yi. 4 LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents Figure 3. Diagram of base and LR cache computation with an initial context of length Lp prefilled by agent i, followed by an additional context of length Lc processed by agent j, under (a) BaseShared and (b) BaseLRShared. BaseShared maintains per-agent LR caches and computes the LR cache using hidden states for all context tokens not yet processed by the current agent, whereas BaseLRShared shares single LR cache and uses hidden states only for newly appended tokens. Both methods first compute the base cache from the pretrained weights ( 1, 4). They then compute the LR cache via the LoRA down-projection ( 2, 5), and later expand it to the full dimension via the LoRA up-projection over the full sequence ( 3, 6). Efficient LR cache expansion is described in Section 3.3. Table 2. Average cosine similarity of the LR cache across agent pairs for the same context in shared-A multi-LoRA architectures. Model (Plan, Action) (Action, Reflect) (Reflect, Plan) LLaMA-3.1-8B-Instruct Ministral-8B-Instruct 0.9486 0.9473 0.9634 0.9526 0.9607 0.9498 for the entire accumulated context that the new agent has not yet processed; we refer to this as the LR prefill process, as illustrated in Figure 2(b). For example, in Figure 3(a), the hidden states span sequence of length at least Lp + Lc, and the LR cache for agent must be newly computed via the down-projection Aj. At this step, computation of the key and value projection that produces the shared base cache for context segment Lp can be skipped, but the majority of computation (e.g., proceeding MLP layers after the Attention layers) is still required. As result, the amount of computation remains similar to the non-shared setting, scaling as O(N L2dmodel), where is the total trajectory length and dmodel is the model hidden dimension. We note that this is consistent with prior KV cache sharing methods including DroidSpeak, since selective recomputation requires hidden states for the contexts that were previously processed only by other agents. In summary, BaseShared serves as robust and memory-efficient solution applicable to standard multi-LoRA systems. It significantly reduces KV cache memory usage compared to the non-shared baseline while preserving higher accuracy than existing prior KV cache sharing methods. As such, BaseShared is particularly well-suited for conventional multi-LoRA agents when memory efficiency is primary concern. BaseLRShared Building on this foundation, we next introduce BaseLRShared to achieve computational acceleration as well as memory savings. By leveraging the shared-A multi-LoRA architecture, BaseLRShared further eliminates the redundant prefill computation, enabling substantially higher throughput and lower latency than prior approaches. As discussed in Section 2.1, task-specific differences in multi-LoRA systems mainly arise from the upprojection matrices Bi, making it effective to share for improving both parameter efficiency and accuracy (Tian et al., 2024; Yang et al., 2025c). In this setting, we further observe that the LR cache AXi produced by the shared down-projection can also be shared across agents. As shown in Table 2, the LR cache in shared-A multi-LoRA exhibits high cosine similarity across agents, analogous to the base cache, which motivates sharing the LR cache as well. We therefore maintain single base cache and single LR cache for the entire system, and construct agent-specific outputs via their task-specific up-projections Bi. Here, the memory usage is reduced by factor of 1/N + r/(N dout) 1/N relative to the non-shared implementation. Moreover, because both the base cache and the LR cache are available for all previously seen tokens in BaseLRShared, an agent does not require recomputation over context processed by previous agents to construct either cache or the hidden states. For example, in Figure 3(b), the base and LR caches for the Lp tokens that are already available, so the forward pass only needs to compute activations for newly appended Lc contexts. Therefore, across agents over the length-L trajectory, BaseLRShared avoids the separate prefills required by the non-shared implementation. As result, the overall computational complexity becomes even comparable to full cache sharing, scaling as O(L2dmodel), considering that the LR cache expansion cost is small (discussed in the Section 3.3). 3.3. Flash-LoRA-Attention Naive runtime expansion of the LR cache introduces nontrivial computational overhead compared to fully shared caching because it scales with both the accumulated sequence length and the full output dimension dout. Unlike 5 LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents Initialize Oi 0 RBrdhead, Olr,i 0 RBrr Initialize mi RBr , ℓi 0 RBr Load Qi to ShrMem for 0 < Tc do Algorithm 1 Flash-LoRA-Attention Forward (head-wise) Require: = Lp + Lc, RLcdhead Require: RLdhead , base cache Vbase RLdhead Require: LR cache Vlr RLr, LoRA Rrdhead Require: Key, Value block size Bc, Tc = L/Bc Require: Query block size Br, Tr = Lc/Br Ensure: Attn output block RLcdhead 1: Q, Tr blocks Qi, Oi (i = 0, . . . Tr 1). 2: K, Vbase Tc blocks Kj, Vbase,j (j = 0, . . . Tc 1). 3: Vlr Tc blocks Vlr,j (j = 0, . . . Tc 1). 4: for 0 < Tr do 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: Oi Oi + Olr,iB 20: Write Oi Oi/ℓi 21: end for Load Kj, Vbase,j, Vlr,j to ShrMem Mask(QiK / max(cid:0)mi, rowmax(S)(cid:1) mnew α exp(mi mnew ) Pi exp(S mnew ) ℓi α ℓi + rowsum(Pi) Oi α Oi + PiVbase,j Olr,i α Olr,i + PiVlr,j mi mnew end for dhead) i prior low-rank cache compression methods (Yuan et al., 2023; Tomar et al., 2025; Chang et al., 2025) that treat the expansion of the compressed caches as an inevitable overhead, our approach explicitly reduces it via reordered computation with the attention weight. Specifically, we reorder the matrix multiplications so that the attention-weighted multiplication is performed directly on the LR cache first, and the up-projection is applied afterward, making the overhead scale with the small rank dimension rather than dout. Concretely, suppose the base cache Vbase RLdout and LR cache Vlr RLr for the value projection. Given the up-projection matrix B, the adapter contribution is VlrB RLdout . straightforward implementation is to first reconstruct the adapter contribution and then applies attention with attention weights to produce the attention output O: = (Vbase + VlrB). This expands the LR cache to the full-dimension dout for all tokens, so the computation overhead scales with both and dout. Instead, we exploit associativity and compute = Vbase + (P Vlr)B, so the length-L multiplication is performed in the low6 rank space, and the multiplication by is applied afterward. For instance, in decoding step where the accumulated context length was L, the naive implementation adds Θ(L dout) computation to form VlrB already before the attention computation. With reordering, we compute the low-rank intermediate Vlr R1r in Θ(L r) and apply the up-projection in Θ(r dout), for total of Θ(L + dout) computation. As result, since is the dominant term that grows over agent trajectories, our approach reduces the computation of LR cache expansion by approximately factor of r/dout 1. Algorithm 1 realizes this idea by augmenting FlashAttention (Dao et al., 2022; 2023). Here, dhead is used instead of dout to show the head-wise computation explicit. generalized analysis of the computation overhead, along with further algorithmic details, is provided in Appendix B. 4. Experiments 4.1. Implementation Detail Agent Setup We evaluate the accuracy and efficiency of our cache sharing schemes in multi-hop agent execution framework, following AutoAct (Qiao et al., 2024) which is adapted from FastChat Engine. We fine-tune three rolespecific agents, plan, action, and reflect. The plan agent performs reasoning and selects which external tool to invoke based on the reasoning. The action agent produces tool-specific arguments and executes the selected API calls, including web search (Google Developers, 2025), Wikipedia lookup in the ReAct-style tool-use setting (Yao et al., 2023b). The reflect agent reviews the accumulated trajectory, including tool outputs and reasoning, and decides whether to terminate with final answer or to continue the interaction. Models and Datasets We evaluate on LLaMA-3.1-8BInstruct and Ministral-8B-Instruct. We fine-tune and evaluate on split of 2.5k HotpotQA (Yang et al., 2018) and 2.0k ScienceQA (Lu et al., 2022) datasets, which require external knowledge and multi-step reasoning to answer. For training, we use the synthetic and filtered agent trajectories released by AutoAct, generated with LLaMA-2-70B-Chat. For evaluation, we run multi-hop inference on the test set with three difficulty levels of HotpotQA and ScienceQA questions, using 20 iterations for each level. Agent prompts and trajectory templates are provided in the Appendix C.1. Training Settings For shared-A multi-LoRA, we simplify the dynamic token-wise routers used in prior work (Tian et al., 2024) into static assignment that selects agentspecific LoRA weights for each predefined role. Under this setting, we observe an accuracy gain over naive multiLoRA, both with and without cache sharing methods (see Appendix C.2). We therefore use the shared-A weights for our main accuracy evaluations. Since shared-A is implemented by duplicating the same weights across agents, LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents Table 3. Benchmark accuracy (%) of the default non-sharing scheme and cache sharing schemes on HotpotQA and ScienceQA at each level. For each model, the tiny value in the Avg. column denotes the difference from the corresponding Non-Shared baseline. Model Method Easy Medium Hard Avg. 1-4 5-8 9-12 Avg. HotpotQA ScienceQA LLaMA-3.1-8B-Instruct Ministral-8B-Instruct Non-Shared FullShared DroidSpeak BaseShared BaseLRShared Non-Shared FullShared DroidSpeak BaseShared BaseLRShared 42.80 41.15 40.60 42.70 42.40 41.30 39.60 40.85 40.95 41.10 41.95 39.15 39.55 41.95 40.80 37.75 33.95 35.65 37.60 37.70 31.90 28.90 30.15 31.15 30.55 28.75 24.80 26.95 29.00 27. 38.88 0.00 36.40 -2.48 36.77 -2.12 38.60 -0.28 37.92 -0.97 35.93 0.00 32.78 -3.15 34.48 -1.45 35.85 -0.08 35.32 -0.62 70.63 68.00 68.79 70.38 70.42 71.50 68.83 68.88 70.75 69.71 60.54 55.67 59.25 60.75 60.25 63.83 57.33 59.54 63.25 62. 76.75 72.00 74.42 76.58 76.71 70.92 64.25 69.96 70.25 70.17 69.31 0.00 65.22 -4.08 67.49 -1.82 69.24 -0.07 69.13 -0.18 68.75 0.00 63.47 -5.28 66.13 -2.63 68.08 -0.67 67.32 -1.43 it does not change the multi-LoRA architecture, and therefore does not affect efficiency comparisons. Unless stated otherwise, we use rank = 8 on query and value projections. Additional training hyperparameters and loss curves are reported in the Appendix C.3. Additional accuracy and efficiency results for LoRA applied to the query, key, value, and output projections with the same number of trainable parameters are provided in Appendix D.1. Our method still performs better than the LoRA variations. Baselines We compare against the default Non-Shared baseline, and several cache sharing methods. These include fully shared baseline, denoted as FullShared, which shares the entire KV cache across agents without any recomputation, and DroidSpeak. For DroidSpeak, we follow the official configuration and recompute the top 33% most sensitive layers at runtime, selected by probing HotpotQA accuracy, matching the Pareto-optimal setting reported in (Liu et al., 2026). The selected layers are listed in the Appendix C.4. We note that prefix-aware positional embedding matching methods such as KVLink, KVFlow, and KVComm reduce to FullShared in our setup because the prefixes are identical across agents. Efficiency Evaluation We observe that latency in agent systems depends on both the cache sharing methods efficiency and the system accuracy, since lower-accuracy methods tend to take more steps and accumulate longer contexts. To measure the intrinsic efficiency of each cache sharing scheme, we evaluate latency under controlled trace of sequence lengths, similar to the evaluation protocol of KVComm (Ye et al., 2025). We construct this trace by varying the amount of context retrieved from external tools from 1k to 64k tokens, and we feed the same trace to all schemes. This yields total sequence lengths ranging from 2k to 66k tokens. The detailed construction of the emulated trace and the latency analysis on the HotpotQA benchmark are described in Appendix C.5 and Appendix D.2, respectively, where our schemes achieve the best results. We conducted Figure 4. System throughput (tokens per second) of BaseShared and BaseLRShared, with Flash-LoRA-Attention (FLA). all experiments on single NVIDIA A6000 48GB GPU. 4.2. Benchmark Accuracy We demonstrate that both of our cache sharing schemes preserve accuracy more effectively than prior baselines, BaseShared stays close to as shown in Table 3. Non-Shared, with an average accuracy drop of at most 0.7%. BaseLRShared also maintains strong accuracy, with an average drop of at most 1.5%. In contrast, FullShared and DroidSpeak exhibit larger average drops, up to 5.3% and 2.6%, respectively. Overall, these results indicate that decoupling the cache into shared base component and low-rank component is key factor for robust KV cache sharing, compared with selective recomputation such as DroidSpeak. Additionally, we provide further analysis of out-of-function incident ratios where the system fails to produce an answer, rank ablations, and deviation of scores in Appendix D.3, D.4, and D.5. 4.3. System Efficiency We report system throughput and time-to-first-token (TTFT) in Table 4 and Table 5, respectively. We define system throughput as the total processed sequence length divided by the end-to-end latency. TTFT is the sum of prefill latencies across agent steps, since each step incurs new 7 LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents Table 4. System throughput (tokens per second) under each total sequence length of the traces. OOM indicates out-of-memory. Model Method 1.9k 3.0k 5.0k 9.1k 17.3k 33.7k 66.4k LLaMA-3.1-8B-Instruct Ministral-8B-Instruct Non-Shared FullShared DroidSpeak BaseShared BaseLRShared Non-Shared FullShared DroidSpeak BaseShared BaseLRShared 176.2 193.7 182.4 169.0 188. 158.9 169.9 160.9 157.0 164.1 262.4 293.4 263.5 257.0 279.4 231.0 251.4 251.4 227.4 247.9 401.3 468.5 412.3 392.3 463.8 361.5 420.3 360.3 364.0 410.9 592.1 808.1 633.5 617.3 785. 541.2 711.2 570.2 552.1 703.2 669.2 1246.1 844.2 862.8 1239.4 610.7 1163.9 785.1 796.5 1159.2 683.2 1697.6 931.0 969.6 1678.1 638.4 1538.6 856.0 885.0 1521.9 OOM 1826.5 813.1 823.2 1790. OOM 1768.3 OOM 870.5 1757.0 Table 5. TTFT (second) under each total sequence length of the traces. OOM indicates out-of-memory. Lower is better. Model LLaMA-3.1-8B-Instruct Ministral-8B-Instruct Method Non-Shared FullShared DroidSpeak BaseShared BaseLRShared Non-Shared FullShared DroidSpeak BaseShared BaseLRShared 1.9k 1.94 1.13 1.62 1.62 1.13 2.02 1.19 1.65 1.66 1.20 3.0k 2.55 1.28 2.17 2.12 1.28 2.65 1.35 2.22 2.19 1.35 5.0k 3.72 1.63 3.22 3.06 1.64 3.85 1.69 3.28 3.17 1.71 9.1k 6.79 2.40 5.55 5.26 2.43 6.84 2.50 5.69 5.43 2. 17.3k 16.38 4.19 11.15 10.51 4.24 17.67 4.37 11.53 10.85 4.42 33.7k 38.87 9.13 25.43 23.91 9.19 41.67 9.30 26.71 25.57 9.38 66.4k OOM 23.28 67.80 67.80 23.35 OOM 20.84 OOM 59.62 20.98 duce KV cache memory by nearly 1/3 compared to Non-Shared baseline, as shown in Figure 5, which is comparable to other cache-sharing baselines and only marginally higher than FullShared within 1GB. This is because the LR caches in BaseShared and BaseLRShared, as well as the hidden-state cache in DroidSpeak, are negligible in size. However, under groupquery attention (GQA), the hidden state cache used by DroidSpeak is larger than the KV cache for layer, which led to near out-of-memory (OOM) behavior in some cases of our experiments. We provide detailed memory analysis in Appendix D.6. 5. Conclusion In this work, we introduce LRAGENT, KV cache sharing framework for multi-LoRA agent systems that decouples the value cache into shared base cache and an adapterdependent LR cache. BaseShared reduces KV memory by sharing the base cache, and BaseLRShared further reduces computation by sharing the LR cache under sharedA multi-LoRA variants while preserving role-specific behaviors. We validate that these methods preserve accuracy close to the non-shared baseline. Flash-LoRA-Attention provides substantial efficiency gains by avoiding full-dimension materialization of the LR cache, enabling throughput and TTFT improvements close to fully shared caching. Overall, LRAGENT consistently outperforms prior cache sharing baselines in both accuracy and efficiency. Figure 5. Memory usage (GB) of cache sharing methods on total sequence length of 66.4k on Ministral-8B-Instruct. model generation in multi-hop scenarios. We first demonstrate the impact of Flash-LoRA-Attention in Figure 4. It yields up to 1.24 gain in throughput for BaseShared and up to 1.35 gain for BaseLRShared. This shows that reducing LR cache expansion overhead enables substantial speedups. With Flash-LoRA-Attention enabled, BaseShared achieves up to 1.42 gain and BaseLRShared achieves up to 2.46 gain in throughput, approaching the upper bound of full cache sharing with FullShared. DroidSpeak reaches similar throughput gain to BaseShared, up to 1.36, since both methods compute hidden states for tokens that have not been processed by the current agent. For TTFT, BaseShared and BaseLRShared provide up to 1.63 and 4.44 reductions, respectively, both exceeding DroidSpeak which achieves up to 1.56 reduction. BaseLRShared achieves TTFT reductions close to those of FullShared. Additionally, BaseShared and BaseLRShared re8 LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents"
        },
        {
            "title": "Software and Data",
            "content": "We provide file upload that reproduces the main results in this paper, including training, evaluation, and latency measurements under the same experimental settings. Detailed descriptions and step-by-step guidelines, such as environment setup and commands to run each experiment, are included in the uploaded file."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of deep learning and large language models. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Bai, T., Yang, L., Wong, Z. H., Sun, F., Zhuang, X., Peng, J., et al. Efficient pretraining data selection for language models via multi-actor collaboration. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 9465 9491, Vienna, Austria, jul 2025. Association for Computational Linguistics. doi: 10.18653/v1/2025.acl-long. 466. URL https://aclanthology.org/2025. acl-long.466/. Bo, X., Chen, X., Dai, Q., Feng, X., Li, R., Wang, L., et al. Reflective multi-agent collaboration based on large language models. In Advances in Neural Information Processing Systems, volume 37, pp. 138595138631, 2024. doi: 10.52202/079017-4397. Chang, C.-C., Lin, C.-Y., Akhauri, Y., Lin, W.-C., Wu, K.-C., Ceze, L., et al. xkv: Cross-layer svd for kvcache compression. arXiv preprint arXiv:2503.18893, 2025. doi: 10.48550/arXiv.2503.18893. URL https: //arxiv.org/abs/2503.18893. Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and Re, C. Flashattention: Fast and memory-efficient exact attention with io-awareness. In Advances in Neural Information Processing Systems, volume 35, pp. 1634416359, 2022. doi: 10.5555/3600270.3601459. URL https://github. com/Dao-AILab/flash-attention. Paper: arXiv:2205.14135, code repository linked in url field. Dao, T., Haziza, D., Massa, F., and Sizov, G. Flashdecoding for long-context inference. PyTorch Blog, October 2023. URL https://pytorch.org/blog/ flash-decoding/. Updated Nov 16, 2024. DeepSeek-AI, Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. doi: 10.48550/arXiv.2412. 19437. URL https://arxiv.org/abs/2412. 19437. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. doi: 10.48550/arXiv.2407. 21783. Fu, D., He, K., Wang, Y., Hong, W., Gongque, Z., Zeng, W., et al. Agentrefine: Enhancing agent generalization through refinement tuning. arXiv preprint arXiv:2501.01702, 2025a. Fu, T., Min, Z., Zhang, H., Yan, J., Dai, G., Ouyang, W., et al. Cache-to-Cache: Direct semantic communication between large language models. arXiv preprint arXiv:2510.03215, 2025b. doi: 10.48550/arXiv.2510. 03215. URL https://arxiv.org/abs/2510. 03215. Google Developers. Custom Search JSON API: Introduction. Programmable Search Engine Documentation, 2025. https://developers.google.com/ URL custom-search/v1/introduction?hl=ko. Last updated 2025-08-31. Accessed 2026-01-06. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., et al. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. doi: 10.48550/arXiv.2106. 09685. URL https://openreview.net/forum? id=nZeVKeeFYf9. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., de las Casas, D., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. doi: 10.48550/arXiv. 2310.06825. Li, B., Wang, Y., Ma, H., Chen, L., Xiao, J., and Wang, S. Mobilora: Accelerating lora-based llm inference on mobile devices via context-aware kv cache optimizaIn Proceedings of the 63rd Annual Meeting of tion. the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2340023410, Vienna, Austria, July 2025. Association for Computational Linguistics. doi: 10.18653/v1/2025.acl-long.1140. URL https:// aclanthology.org/2025.acl-long.1140/. Liu, S., Chen, T., Liang, Z., Lyu, X., and Amato, C. Llm collaboration with multi-agent reinforcement learning. arXiv preprint arXiv:2508.04652, 2025a. Liu, Y., Lin, K. Q., Chen, C. W., and Shou, M. Z. Videomind: chain-of-LoRA agent for long video reasoning. arXiv preprint arXiv:2503.13444, 2025b. doi: 10.48550/arXiv.2503.13444. URL https://arxiv. org/abs/2503.13444. 9 LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents Liu, Y., Huang, Y., Yao, J., Feng, S., Gu, Z., Du, K., et al. DroidSpeak: KV cache sharing for cross-LLM communication and multi-LLM serving. In Proceedings of the 23rd USENIX Symposium on Networked Systems Design and Implementation (NSDI 26). USENIX Association, 2026. doi: 10.48550/arXiv.2411.02820. URL https: //arxiv.org/abs/2411.02820. Accepted for NSDI 2026. Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu, S.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to explain: Multimodal reasoning via thought chains for science question answering. In Advances in Neural Information Processing Systems, volume 35, pp. 25072521, 2022. OpenAI, Hurst, A., Lerer, A., Goucher, A. P., Perelman, A., Ramesh, A., et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. doi: 10.48550/arXiv.2410. 21276. URL https://arxiv.org/abs/2410. 21276. Pan, Z., Patel, A. D., Shen, Y., Hu, Z., Guan, Y., Li, W.-L., et al. KVFlow: Efficient prefix caching for accelerating LLM-based multi-agent workflows. In Advances in Neural Information Processing Systems (NeurIPS 2025), 2025. URL https://arxiv.org/ abs/2507.07400. Qiao, S., Zhang, N., Fang, R., Luo, Y., Zhou, W., Jiang, Y., et al. AutoAct: Automatic agent learning from scratch for QA via self-planning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 30033021, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long. 165. URL https://aclanthology.org/2024. acl-long.165/. Qin, Y., Liang, S., Ye, Y., Zhu, K., Yan, L., Lu, Y., et al. Toolllm: Facilitating large language models to master In The Twelfth International 16000+ real-world apis. Conference on Learning Representations, 2024. Rasal, S. Llm harmony: Multi-agent communication for problem solving. arXiv preprint arXiv:2401.01312, 2024. Shen, W., Li, C., Chen, H., Yan, M., Quan, X., Chen, H., et al. Small llms are weak tool learners: multi-llm agent. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 16658 16680, 2024. Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K., and Yao, S. Reflexion: Language agents with verbal reinforcement learning. In Advances in Neural Information Processing Systems, volume 36, pp. 86348652, 2023. Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm testtime compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. doi: 10.48550/arXiv.2408.03314. Talebirad, Y. and Nadiri, A. Multi-agent collaboration: Harnessing the power of intelligent llm agents. arXiv preprint arXiv:2306.03314, 2023. Tian, C., Shi, Z., Guo, Z., Li, L., and Xu, C. Hydralora: An asymmetric lora architecture for efficient fine-tuning. In Advances in Neural Information Processing Systems, volume 37, pp. 95659584, 2024. doi: 10.5555/3737916. 3738220. URL https://proceedings.neurips. cc/. Tomar, A., Hooper, C., Lee, M., Xi, H., Tiwari, R., Kang, W., et al. Xquant: Breaking the memory wall for llm inference with kv cache rematerialization. arXiv preprint arXiv:2508.10395, 2025. doi: 10.48550/arXiv.2508. 10395. URL https://arxiv.org/abs/2508. 10395. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023. doi: 10.48550/arXiv.2307.09288. URL https: //arxiv.org/abs/2307.09288. Wang, Y., Lin, Y., Zeng, X., and Zhang, G. Multilora: Democratizing lora for better multi-task learnarXiv preprint arXiv:2311.11501, 2023. ing. doi: 10.48550/arXiv.2311.11501. URL https://arxiv. org/abs/2311.11501. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., et al. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, volume 35, pp. 2482424837, 2022. Woo, S., Kim, H., Kil, J., Kim, M., Kim, J., Seo, A., et al. Icarus: Identical cache reuse for efficient multi-model inference. OpenReview, ICLR 2026 Conference Submission, 2025. URL https://openreview.net/ forum?id=qrMo6R7lOS. Accessed 2026-01-02. Wu, Q., Bansal, G., Zhang, J., Wu, Y., Li, B., Zhu, E., et al. Autogen: Enabling next-gen llm applications via multiagent conversations. In First Conference on Language Modeling, jul 2024. URL https://openreview. net/forum?id=BAakY1hNKS. Xia, Y., Fu, F., Zhang, W., Jiang, J., and Cui, B. Efficient multi-task LLM quantization and serving for multiple LoRA adapters. In Advances in Neural Information Processing Systems, volume 37, pp. 6368663714, 2024. doi: 10.5555/3737916.3739950. 10 LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents Yu, X., Luo, T., Wei, Y., Lei, F., Huang, Y., Peng, H., et al. Neeko: Leveraging dynamic LoRA for efficient In Proceedings of multi-character role-playing agent. the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 1254012557, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main. 697. URL https://aclanthology.org/2024. emnlp-main.697/. Yuan, Z., Shang, Y., Song, Y., Wu, Q., Yan, Y., Sun, G., et al. Asvd: Activation-aware singular value decomposition for compressing large language models. arXiv preprint arXiv:2312.05821, 2023. doi: 10.48550/arXiv.2312. 05821. URL https://arxiv.org/abs/2312. 05821. Zhang, C., Goh, X. D., Li, D., Zhang, H., and Liu, Y. Planning with multi-constraints via collaborative language agents. In Proceedings of the 31st International Conference on Computational Linguistics, pp. 1005410082, Abu Dhabi, UAE, jan 2025. Association for Computational Linguistics. URL https://aclanthology. org/2025.coling-main.672/. Yang, A., Yu, B., Li, C., Liu, D., Huang, F., Huang, H., et al. Qwen2.5-1m technical report. arXiv preprint arXiv:2501.15383, 2025a. doi: 10.48550/arXiv.2501. 15383. Yang, J., Hou, B., Wei, W., Bao, Y., and Chang, S. KVLink: Accelerating large language models via efficient KV cache reuse. arXiv preprint arXiv:2502.16002, 2025b. doi: 10.48550/arXiv.2502.16002. URL https: //arxiv.org/abs/2502.16002. Yang, Y., Muhtar, D., Shen, Y., Zhan, Y., Liu, J., Wang, Y., et al. Mtl-lora: Low-rank adaptation In Proceedings of the AAAI for multi-task learning. Conference on Artificial Intelligence, volume 39, pp. 2201022018, 2025c. 10.1609/aaai.v39i20. 35509. URL https://ojs.aaai.org/index. php/AAAI/article/view/35509. doi: Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W., Salakhutdinov, R., and Manning, C. D. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 2369 2380, Brussels, Belgium, 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1259. URL https://aclanthology.org/D18-1259/. Yao, J., Li, H., Liu, Y., Ray, S., Cheng, Y., Zhang, Q., et al. CacheBlend: Fast large language model serving In Proceedfor RAG with cached knowledge fusion. ings of the Twentieth European Conference on Computer Systems (EuroSys 25), pp. 94109, New York, NY, USA, 2025. Association for Computing Machinery. doi: 10.1145/3689031.3696098. URL https: //arxiv.org/abs/2405.16444. Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Cao, Y., et al. Tree of thoughts: Deliberate problem solving with large language models. In Advances in Neural Information Processing Systems, volume 36, pp. 1180911822, 2023a. Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., et al. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2023b. Ye, H., Gao, Z., Ma, M., Wang, Q., Fu, Y., Chung, M.- Y., et al. KVCOMM: Online cross-context KV-cache communication for efficient LLM-based multi-agent systems. In Advances in Neural Information Processing Systems (NeurIPS 2025), 2025. doi: 10.48550/arXiv.2510. 12872. URL https://arxiv.org/abs/2510. 12872. Accepted for publication in NeurIPS 2025. 11 LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents A. Base Cache and Adapter Output A.1. Cache L1 norm As discussed in Section 3.1, the output contributions from the pretrained base weights and the low-rank adapters differs in magnitude. We analyze the average output magnitude of the value projection in multi-LoRA system with three agents, where the pretrained base-weight contribution is treated as the base cache and the LoRA contribution is treated as the adapter output. As shown in Figure 6, the base cache and adapter output magnitudes follow similar trends across layers, but the adapter outputs are much smaller, by factors of 27.3 and 14.77 for LLaMA-3.1-8B-Instruct and Ministral-8B-Instruct, respectively. Given that the base cache remains highly similar across agents while the adapter outputs are largely decorrelated, the adapter output can be viewed as small but non-trivial, approximately random perturbation to the base cache. This motivates sharing the base cache rather than the full cache. Figure 6. L1 norm of the base cache and adapter output across model layers. 12 LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents A.2. Cosine Similarity Bound As shown in Section 3.1, the base cache remains highly similar across agents for the same context, while the adapter outputs are largely decorrelated across agent pairs. This implies that the full cache can be viewed as the base cache perturbed by an approximately random adapter component. In this section, we formalize the resulting effect on similarity: we show that, under the approximate orthogonality assumptions on adapter outputs consistent with our empirical observations, the cosine similarity of the base cache is higher than that of the full cache. Based on the empirical observations on the similarity of the base cache and adapter contributions in Tables 1 and 6, we derive the following bound showing that the full cache exhibits lower cosine similarity. Following the multi-LoRA convention in Section 3.1, the base cache and adapter output are defined as: Ybase,i := XiW0, Yi := XiWi, Yi := Ybase,i + Yi. (4) Assuming that the adapter outputs are approximately orthogonal to the base cache and decorrelated across agents: base,iYi = 0, base,jYj = 0, base,iYj = i Ybase,j = Yj = 0, (i = j). (5) This satisfies the following: Furthermore, we have Yj = (Ybase,i + Yi)(Ybase,j + Yj) = base,iYbase,j. Yi2 = Ybase,i + Yi2 = Ybase,i2 + Yi2 Ybase,i2, and similarly Yj Ybase,j. Therefore, cos(Yi, Yj) = Yj Yi Yj = base,iYbase,j Yi Yj base,iYbase,j Ybase,i Ybase,j = cos(Ybase,i, Ybase,j), and taking expectation over contexts yields E(cid:2) cos(Ybase,i, Ybase,j)(cid:3) E(cid:2) cos(Yi, Yj)(cid:3), which states that the base cache cosine similarity is higher than the full cache cosine similarity. Table 6. Cosine similarity between base cache and adapter outputs across agent pairs. (6) (7) (8) (9) Contribution Yplan Yaction Yreflect Ybase,plan Ybase,action Ybase,reflect 0.0033 0.0188 -0.0076 0.0050 0.0985 0.0037 -0.0003 0.0006 0. 13 LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents A.3. Key Cache Cosine Similarity As noted in Section 3.1, in typical multi-LoRA settings the key cache remains highly similar across agents. In particular, the minimum key cache similarity already exceeds the average base-cache similarity reported in Table 1. Figure 7 reports pairwise cosine similarity of the key cache for each model. The average similarity is 0.9922 for LLaMA3.1-8B-Instruct and 0.9840 for Ministral-8B-Instruct, and even the minimum similarity across agent pairs is higher than the corresponding average base-cache similarity: 0.9726 and 0.9530, respectively. This indicates that the primary cross-agent differences come from the value cache, mainly through the adapter-induced component. Therefore, we simply share the entire key cache across agents in all of our schemes. Figure 7. cache Cosine similarity of each agent pairs across the model layers. 14 LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents B. Flash-LoRA-Attention In this section, we provide detailed description of Flash-LoRA-Attention, which is introduced in Section 3.3, and analyze its computational overhead in general form, following the notation in Section 3.2 and Figure 3. Setup. We follow the notation in Algorithm 1 and describe the forward pass for single attention head. Let = Lp + Lc, where Lp is the accumulated context length and Lc is the context length of the current prefill/decoding step. We denote the query and key as RLcdhead and RLdhead, and the base value cache as Vbase RLdhead. For the LoRA update on the value projection, we save the LR cache Vlr RLr and keep the up-projection matrix Rrdhead , where dhead. With attention weight = softmax(cid:0)QK / dhead (cid:1) RLcL, the output is = (cid:0)Vbase + VlrB(cid:1) = Obase + Olr, Obase = Vbase, Olr = VlrB (10) Matrix Multiplication Reordering Based on Associativity. straightforward implementation materializes the fulldimension adapter contribution VlrB RLdhead for all tokens and then applies attention: Olr = (VlrB). (11) This expands the LR cache to the head dimension over the entire trajectory, so the computation grows with both and dhead. Instead, we exploit associativity and reorder the computation as Olr = (cid:0)P Vlr (cid:1)B, (12) so the length-L accumulation is performed in rank r, and the head-dimension multiplication by is applied only once per query block. Compute Overhead. We report multiply-add counts up to constant factors and omit the shared base-attention cost for Obase. Without reordering, we first form VlrB and then multiply by : W/o reorder: dhead (cid:125) (cid:124) (cid:123)(cid:122) VlrB + Lc dhead (cid:125) (cid:123)(cid:122) (VlrB) (cid:124) = O(cid:0)L dhead + LcL dhead (cid:1). With reordering, we first accumulate = Vlr RLcr and then apply B: Reorder: Lc (cid:124) (cid:123)(cid:122) (cid:125) Vlr + Lc dhead (cid:125) (cid:123)(cid:122) (P Vlr)B (cid:124) = O(cid:0)LcL + Lcr dhead (cid:1). (13) (14) Since dhead, reordering replaces the dominant LcL dhead-scaled expansion with an LcL r-scaled low-rank accumulation, and the dhead-dependent multiplication appears only once after the accumulation. Flash-LoRA-Attention Kernel Implementation. Algorithm 1 implements Eq. (12) by extending FlashAttention with one additional low-rank accumulator. For each query block Qi RBrdhead, the kernel streams over key/value blocks (Kj, Vbase,j) and computes = Mask(QiK dhead), maintaining the online-softmax statistics (mi, ℓi). Using the same / block-wise weights, it accumulates both Oi Oi + PiVbase,j and the low-rank intermediate Olr,i Olr,i + PiVlr,j, where Olr,i RBrr remains in rank throughout the streaming pass. After all blocks are processed, the kernel applies single post multiplication Oi Oi + Olr,iB and then normalizes by ℓi. This preserves FlashAttentions memory-efficient I/O pattern while ensuring that the LR cache expansion computation scales primarily with the rank, directly reducing the runtime overhead in both BaseShared and BaseLRShared. 15 LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents C. Implementation Details C.1. Agent Prompts and Trajectory Templates We present an example agent prompt and trajectory from our multi-LoRA agent system, based on the AutoAct implementation (Qiao et al., 2024). As shown in Figure 8, trajectory accumulates predefined system prompt, the user question, and multiple rounds of agent-generated tokens interleaved with rule-based context inserts, such as tool outputs retrieved from external sources. The example is from HotpotQA, and ScienceQA follows the similar template except for an explanation on the additional image caption lookup tool. The system prompt, which specifies the thought, action, and observation format, is identical for all agents and thus fully shared. As result, prefix positional alignment based KV cache sharing methods (Yang et al., 2025b; Pan et al., 2025; Ye et al., 2025) reduce to FullShared in our setup. The highlighted parts indicate agent-generated outputs, where agents execute in predefined order (plan-plan-action). The plan agent first produces reasoning and selects tool, then the action agent generates the tool arguments. If the selected tool is either Web search API, Wikipedia lookup, or image caption lookup that retrieves predefined image caption, the retrieved context is appended to the trajectory. If the selected tool is Finish, the action agent outputs final answer and the reflect agent is invoked to decide whether the answer is sufficient or whether another information retrieval round is needed. The reflection step is divided into two stages, and it can override an incorrect Finish decision and return control to the plan agent when the retrieved evidence is insufficient. The total number of agent iterations is limited to 45 per question. We train the agents using filtered trajectories generated by single LLaMA-2-70B-Chat model, provided by AutoAct. Figure 8. Agent prompts and an example of an accumulated trajectory on HotpotQA. 16 LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents C.2. Shared-A Multi-LoRA Architecture We observe that simply sharing the LoRA down-projection matrix yields higher accuracy than conventional multi-LoRA training with independent (Ai, Bi) pairs, consistent with prior findings (Tian et al., 2024; Yang et al., 2025c). Table 7 reports HotpotQA accuracy with and without shared-A. We note that we use the same training conditions and hyperparameters listed in Appendix C.3, which yield the best results in both settings. Across all methods, the shared-A variant improves the original (Non-Shared) accuracy and also benefits cache sharing schemes such as FullShared, BaseShared, and BaseLRShared. In particular, BaseLRShared degrades when is not shared, since sharing an LR cache computed with different matrices and expanding it with mismatched introduces large errors. In our implementation, we duplicate the shared across agents, which is the same implementation with conventional multi-LoRA architecture and therefore inference efficiency is unchanged. Since shared-A improves accuracy in all settings without introducing change of model structure or inference overheads, we conduct our main experiments using shared-A multi-LoRA trained weights. We note that sharing also reduces the number of trainable parameters by 33%, providing efficiency benefit in training. Table 7. Accuracy (%) comparison between non-shared-A and shared-A multi-LoRA variants on HotpotQA easy benchmark. Model Architecture Non-Shared FullShared BaseShared BaseLRShared LLaMA-3.1-8B-Instruct Ministral-8B-Instruct Non-shared 42.05 Shared-A 42.80 +0.75 Non-shared 41.10 Shared-A 41.30 +0. 40.30 41.15 +0.85 37.40 39.60 +2.20 41.85 42.70 +0.85 40.80 40.95 +0.15 36.25 42.40 +6.15 36.95 41.10 +4. 17 LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents C.3. Hyperparameter and Loss Curve We report the training hyperparameters and loss curves for multi-LoRA agents in Table 8 and Figure 9, respectively. Most hyperparameters, including the optimizer, scheduler, and weight decay, follow AutoAct (Qiao et al., 2024), and we perform grid search over learning rates and the number of training epochs. The sum of training time across all agents is 3.9 hours for HotpotQA and 3.4 hours for ScienceQA on single 48GB NVIDIA A6000 GPU. We also note that the HotpotQA and ScienceQA test sets consist of 300 and 360 questions, respectively, and we run 20 iterations for each accuracy evaluation. Table 8. Hyperparameter settings for multi-LoRA training. Hyperparameter LLaMA-3.1-8B-Instruct Ministral-8B-Instruct"
        },
        {
            "title": "Optimizer\nBatch Size\nLR Scheduler\nMax Sequence Length\nEpochs\nWarmup Ratio\nWeight Decay\nRank\nLoRA Dropout\nLoRA Scale",
            "content": "Learning Rate AdamW 32 cosine 32786 10 0.05 0 8 0.05 16 Plan: 5e-5 Action: 6e-5 Reflect: 6e-5 Plan: 5e-5 Action: 9e-5 Reflect: 9e-5 Figure 9. Train loss and L2 norm of the gradient for each agent types. LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents C.4. DroidSpeak Recomputation Layer Selection We implement DroidSpeak as baseline, following the methodology described in (Liu et al., 2026). In the original implementation, KV caches are selectively recomputed for set of critical layers, identified by probing the benchmark accuracy drop when the KV cache is directly reused in each layer while the remaining layers are recomputed. DroidSpeak provides Pareto-optimal configuration that balances accuracy degradation and the inference efficiency gains from cache sharing, corresponding to recomputing 33% of the model layers. Following this guideline, we probe critical layers on HotpotQA and select 11 layers for LLaMA-3.1-8B-Instruct (32 layers total) and 12 layers for Ministral-8B-Instruct (36 layers total). In addition, since the first layer typically does not require recomputation in Ministral-8B-Instruct, we enable hidden state caching that can be passed directly from the previous model to the current model to eliminate computation for these layers. We note that recent models often use group-query attention (GQA), where the hidden state dimension is typically four times larger than the output dimension of the key or value projections, so the hidden state cache can be roughly twice as large as the KV cache of single layer. This additional memory overhead is discussed in Appendix D.6. Table 9. Selected layers for recomputation in DroidSpeak. Model Selected layers LLaMA-3.1-8B-Instruct Ministral-8B-Instruct 0, 2, 16, 19, 20, 22, 23, 24, 26, 30, 31 1, 4, 5, 12, 14, 15, 17, 21, 22, 25, 29, 31 C.5. Emulated Trace for Efficiency Analysis In the experiments on HotpotQA or ScienceQA benchmarks, methods with lower accuracy tend to produce longer trajectories and thus accumulate longer contexts. Therefore, to enable fair efficiency comparisons of only the cache sharing method itself under the same context length, we construct fixed trace of context lengths and an agent schedule. This trace is based on profiled trajectories, including the concatenated context length at each agent step and the number of steps per iteration. On average, each iteration consists of 17 steps, comprising five plan-plan-action cycles and two reflect steps at the end of the iteration. We vary the retrieved context length Lctx from 0.25k to 16k, which results in total trajectory lengths ranging from 1.9k to 66.4k, as reported in Table 4 and Table 5. We note that the detailed agent trajectory templates are described in Appendix C.1. Table 10. Agent iteration trace with prefill and generation lengths. Agent Type Step Prefill Generation plan plan action plan plan action plan plan action plan plan action plan plan action reflect reflect 32 8 8 32 8 8 32 8 8 32 8 8 32 8 8 32 8 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 512 8 8 Lctx 8 8 Lctx 8 8 Lctx 8 8 Lctx 8 8 32 19 LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents D. Experiments D.1. Ablation on LoRA Application Although LoRA is most commonly applied to the query and value projections, which we denote as the qv setting, we also evaluate an alternative configuration with the same parameter budget by applying LoRA to the query, key, value, and output projections with rank = 4, which we denote as qkvo. Following Section 4.3, we measure HotpotQA accuracy along with system throughput and TTFT under the same emulated trace, where the results are presented in Table 11, 12, and 13. We find that the non-shared baseline in qkvo achieves lower accuracy than in qv, and this degradation carries over to all cache sharing methods. Nevertheless, our methods still achieve the best accuracy among the cache sharing approaches, indicating that decoupling the base cache and the LR cache remains effective when LoRA is applied to the key projection. In terms of system throughput, the qkvo setting is inherently less favorable because it introduces additional LoRA computation paths on multiple projections. Moreover, in our schemes, adapter contribution reconstruction from key LR cache must be performed in the head dimension before applying rotary positional embeddings (RoPE), which limits the same associativity-based reordering we exploit for the value cache. Concretely, letting the post-RoPE query be and the pre-RoPE lrB with = RoPE(K ), the attention score is QK = Q(cid:0)RoPE(K )(cid:1) key be = base + base) + (cid:17) , so the low-rank reordering from Q(BK RoPE(K is not directly applicable because RoPE() applies position-dependent rotation on the head dimension. As result, both BaseShared and BaseLRShared under qkvo achieve lower throughput than their qv counterparts reported in Table 4. Still, BaseLRShared remains more efficient than DroidSpeak. On the other hand, because the adapter output reconstruction primarily affects the generation stage rather than prefill, TTFT under qkvo increases marginally overall compared to Table 5. Similarly with the previous results, BaseLRShared remains close to FullShared in TTFT, while BaseShared remains comparable to DroidSpeak. lr ) to (QB)K lr RoPE(K lrB) = (cid:16) Overall, our approach maintains the strongest accuracy among cache sharing baselines, and BaseLRShared retains clear efficiency advantage, demonstrating the generality and scalability of our design across LoRA configurations, while qv setting used in this paper is favorable across the as mentioned in Section 4.1. Table 11. LLaMA-3.1-8B-Instruct average HotpotQA benchmark accuracy (%) under the qkvo scheme. Method Non-Shared FullShared DroidSpeak BaseShared BaseLRShared Accuracy (%) 38.43 34.88 36.28 38. 37.57 Table 12. LLaMA-3.1-8B-Instruct system throughput (tokens per second) under each total sequence length of the traces in qkvo setting. Method 1.9k 3.0k 5.0k 9.1k 17.3k 33.7k 66.4k Non-Shared FullShared DroidSpeak BaseShared BaseLRShared 123.1 155.8 145.9 127.2 150. 184.4 215.1 211.8 195.6 219.7 297.1 357.2 326.2 325.7 361.3 467.1 626.3 519.6 484.9 560.3 525.0 1080.6 739.1 679.3 806.7 556.0 1544.9 887.2 755.6 998.8 OOM 1699.4 792.9 673.2 1051. Table 13. LLaMA-3.1-8B-Instruct TTFT (second) under each total sequence length of the traces in qkvo setting. Method 1.9k 3.0k 5.0k 9.1k 17.3k 33.7k 66.4k Non-Shared FullShared DroidSpeak BaseShared BaseLRShared 4.79 1.23 1.77 1.78 1.27 5.00 1.40 2.29 2.34 1. 4.94 1.81 3.37 3.29 1.92 10.52 2.70 5.84 5.59 2.89 20.73 4.82 11.54 10.99 4.93 50.04 9.57 25.43 26.20 10.30 OOM 24.05 67.80 70.57 25.30 LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents D.2. Latency on HotpotQA Benchmark We report the end-to-end system latency on the HotpotQA benchmark. In real-world scenarios, additional latency other than model inference arises from function calls such as web search and Wikipedia retrieval. We therefore split latency into model latency, which includes only the inference (prefill and generation), and end-to-end (E2E) latency, which additionally includes function call latency and the latency of data processing the retrieved context. We also report time-to-first-token (TTFT), defined as the sum of model prefill latencies across agent steps, consistent with Table 5 in Section 4.3. As shown in Table 14 and Table 15, methods with lower accuracy, such as FullShared and DroidSpeak, tend to produce longer sequences and incur higher latency, sometimes even exceeding the Non-Shared baseline. This occurs despite their strong efficiency which they have demonstrated in trace-based emulations. These results highlight that overall latency depends not only on the cache sharing efficiency, but also on accuracy. When cache sharing degrades generation quality, the agent is more likely to take additional steps to re-reason and retrieve more external context, which increases sequence length and, in turn, increases E2E latency. Overall, FullShared achieves low TTFT but incurs substantial E2E latency overhead compared to Non-Shared on LLaMA-3.1-8B-Instruct. DroidSpeak and BaseShared exhibit end-to-end latency similar to Non-Shared. BaseLRShared achieves the best efficiency while preserving strong accuracy, making it empirically optimal for agentic systems. Table 14. End-to-end (E2E) latency and its breakdown. The lowest latency is highlighted in bold. Model Latency Model Latency (s) LLaMA-3.1-8B-Instruct E2E Latency (s) TTFT (s) Model Latency (s) Ministral-8B-Instruct E2E Latency (s) TTFT (s) Level Hard Medium Easy Avg. Hard Medium Easy Avg. Hard Medium Easy Avg. Hard Medium Easy Avg. Hard Medium Easy Avg. Hard Medium Easy Avg. Non-Shared FullShared DroidSpeak BaseShared BaseLRShared 5.90 5.60 5.04 5.51 13.63 12.10 11.10 12.28 1.02 1.01 1.05 1.03 6.64 6.36 6.86 6.62 14.65 12.66 15.22 14.18 1.26 1.17 1.22 1. 7.39 6.70 6.22 6.77 19.73 18.48 14.87 17.69 0.81 0.77 0.72 0.77 7.23 6.43 6.05 6.57 14.36 14.30 12.85 13.83 1.30 1.07 1.12 1.16 5.75 5.30 4.69 5.24 13.75 11.85 10.64 12.08 1.00 0.98 0.97 0.98 6.82 6.23 6.11 6.38 14.98 12.41 12.98 13.46 1.44 1.26 1.24 1.31 5.97 5.35 4.76 5.36 13.73 12.05 10.89 12.23 1.07 1.02 1.03 1.04 6.79 6.39 5.86 6.35 15.06 12.25 12.92 13.41 1.33 1.17 1.20 1.23 5.70 5.13 4.54 5.12 13.59 11.73 10.54 11.96 0.77 0.66 0.88 0.77 6.39 6.00 5.77 6.05 13.91 11.45 12.79 12.72 1.26 1.12 1.08 1.15 Table 15. Average of total sequence length (tokens) accumulated during multi-agent execution. Model LLaMA-3.1-8B-Instruct Ministral-8B-Instruct Level Hard Medium Easy Avg. Hard Medium Easy Avg. Non-Shared FullShared DroidSpeak BaseShared BaseLRShared 1223 1108 1134 1155 1355 1347 1414 1372 1039 996 1077 1038 1202 1144 1142 1302 1229 1154 1228 1468 1406 1363 1412 1099 1092 1088 1093 1120 1019 1154 1098 1584 1475 1483 1514 1460 1398 1393 1417 21 LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents D.3. Out-of-Function Ratio We define cases where the agent system fails to produce an answer before reaching the maximum number of iterations (e.g., 45) as out-of-function (OOF). In the benchmark accuracy evaluation, these cases are counted as incorrect. However, from user-experience perspective, returning no answer can be qualitatively different from returning an incorrect answer, and may be considered more severe failure. We therefore report OOF incidents in addition to benchmark accuracy. As shown in Table 16, which reports the OOF ratio and its difference from the Non-Shared baseline, methods with lower accuracy generally exhibit higher OOF ratios. Consistent with the accuracy results where BaseShared and BaseLRShared achieve the best accuracy among cache sharing methods, our schemes also yield lower OOF ratios in most cases, except for Ministral-8B-Instruct on ScienceQA. Table 16. Out-of-function (OOF) rate (%) for each benchmark and difficulty level. The underlying value in the Avg. column denotes the difference from the corresponding Non-Shared baseline. Lower is better."
        },
        {
            "title": "Easy Medium Hard",
            "content": "Avg. 1-4 5-8 9-12 Avg."
        },
        {
            "title": "ScienceQA",
            "content": "Non-Shared FullShared DroidSpeak BaseShared BaseLRShared Non-Shared FullShared DroidSpeak BaseShared BaseLRShared 1.50 2.05 2.10 1.35 1.25 3.90 7.15 7.05 6.45 4.45 1.80 2.05 3.10 1.50 1.80 5.15 7.65 9.50 6.35 6. 1.65 3.25 5.15 2.65 2.25 5.80 10.95 8.20 9.50 7.60 1.65 0.00 2.45 +0.80 3.45 +1.80 1.83 +0.18 1.77 +0.12 4.95 0.00 8.58 +3.63 8.25 +3.30 7.43 +2.48 6.20 +1.25 0.00 0.29 0.58 0.21 0.29 0.38 4.17 1.79 2.75 1. 0.13 2.46 1.29 1.83 2.50 0.17 2.96 2.17 1.92 1.67 0.21 0.54 1.71 0.13 0.25 0.83 4.92 3.33 3.25 3.38 0.11 0.00 1.10 +0.99 1.19 +1.08 0.72 +0.61 1.01 +0.90 0.46 0.00 4.01 +3.56 2.43 +1.97 2.64 +2.18 2.29 +1. LLaMA-3.1-8B-Instruct Ministral-8B-Instruct D.4. Rank Ablations We report the accuracy of the Non-Shared baseline and our schemes, BaseShared and BaseLRShared, under ranks from 4 to 32, as shown in Table 17. Since the agent-trajectory training data are relatively small and easy to adapt, we observe marginal accuracy differences across ranks, and ranks above 8 do not yield noticeable accuracy improvements. Therefore, to minimize both training and inference overhead, we use rank = 8 in all experiments. Table 17. Rank ablation on HotpotQA average accuracy (%). Rank 4 8 16 32 Non-Shared BaseShared BaseLRShared 38.25 37.88 37. 38.88 38.60 37.92 39.00 38.43 37.95 38.93 38.50 37.97 22 LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents D.5. Accuracy Score Deviation We report the standard deviation of the average accuracy in Table 3 of Section 4.2 for each baseline and our methods in Table 18. We note that all experiments use single random seed (42), but accuracy can still vary due to subtle nondeterministic characteristics in external tool usage. For each benchmark level, we run 20 iterations. The accuracy gaps between methods are larger than the observed deviations and therefore we see that the comparisons remain reliable. Table 18. Standard deviation of average benchmark accuracy (%) with 20 iterations. Dataset Non-Shared FullShared DroidSpeak BaseShared BaseLRShared"
        },
        {
            "title": "HotpotQA\nScienceQA",
            "content": "0.16 0.20 0.32 0.45 0.21 0.25 0.19 0.26 0.24 0.31 D.6. Memory Usage We report the memory usage of each method across traces with diverse trajectory lengths on Ministral-8B-Instruct. We note that the pretrained model weights consume 14.95 GB of memory, and the three LoRA weights add 0.11 GB of memory. Beyond these components, KV cache memory becomes severe in long-context scenarios where retrieved contexts accumulate. Since KV cache sharing methods typically maintain single shared KV cache for three agents and recompute and overwrite it when needed, their memory usage is similar within 1 GB difference overall. In particular, FullShared has the lowest memory usage because it directly reuses the KV cache without additional components. DroidSpeak additionally maintains hidden state cache. Since the first layer typically does not require recomputation, its hidden states can be transferred directly from the previous model to the current model, eliminating computation for these layers. However, this cache becomes an overhead in modern group-query attention (GQA) models. The hidden state dimension is often about four times larger than the key or value projection dimension, so the hidden state cache can be roughly twice as large as the KV cache of single layer. We note that the OOM observed in Section 4.3 mainly arises from memory fragmentation, despite the gap between the GPUs peak capacity and the actual allocated usage. For BaseShared and BaseLRShared, there exists an additional LR cache, which is three times larger in BaseShared than in BaseLRShared, but it remains negligible due to its small dimension relative to the base cache. As result, our schemes achieve memory usage close to FullShared as well as other cache sharing methods. Table 19. Memory usage (GB) for each total sequence length trace. Total Seq. Len. Non-Shared FullShared DroidSpeak BaseShared BaseLRShared 1.9k 3.0k 5.0k 9.1k 17.3k 33.7k 66.4k 132.0k 15.72 16.10 16.87 18.40 21.46 27.59 39.84 64. 15.25 15.38 15.65 16.18 17.24 19.36 23.61 32.11 15.26 15.40 15.68 16.25 17.37 19.62 24.12 33.12 15.26 15.40 15.67 16.23 17.34 19.56 23.99 32.87 15.25 15.39 15.65 16.19 17.27 19.43 23.74 32."
        }
    ],
    "affiliations": [
        "Department of Electrical and Computer Engineering, Seoul National University, Seoul, Korea"
    ]
}