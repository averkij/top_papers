{
    "paper_title": "EasyControl: Adding Efficient and Flexible Control for Diffusion Transformer",
    "authors": [
        "Yuxuan Zhang",
        "Yirui Yuan",
        "Yiren Song",
        "Haofan Wang",
        "Jiaming Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in Unet-based diffusion models, such as ControlNet and IP-Adapter, have introduced effective spatial and subject control mechanisms. However, the DiT (Diffusion Transformer) architecture still struggles with efficient and flexible control. To tackle this issue, we propose EasyControl, a novel framework designed to unify condition-guided diffusion transformers with high efficiency and flexibility. Our framework is built on three key innovations. First, we introduce a lightweight Condition Injection LoRA Module. This module processes conditional signals in isolation, acting as a plug-and-play solution. It avoids modifying the base model weights, ensuring compatibility with customized models and enabling the flexible injection of diverse conditions. Notably, this module also supports harmonious and robust zero-shot multi-condition generalization, even when trained only on single-condition data. Second, we propose a Position-Aware Training Paradigm. This approach standardizes input conditions to fixed resolutions, allowing the generation of images with arbitrary aspect ratios and flexible resolutions. At the same time, it optimizes computational efficiency, making the framework more practical for real-world applications. Third, we develop a Causal Attention Mechanism combined with the KV Cache technique, adapted for conditional generation tasks. This innovation significantly reduces the latency of image synthesis, improving the overall efficiency of the framework. Through extensive experiments, we demonstrate that EasyControl achieves exceptional performance across various application scenarios. These innovations collectively make our framework highly efficient, flexible, and suitable for a wide range of tasks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 7 2 0 7 0 . 3 0 5 2 : r EasyControl: Adding Efficient and Flexible Control for Diffusion Transformer Yuxuan Zhang1, Yirui Yuan2,1, Yiren Song3,1, Haofan Wang4, Jiaming Liu 1 1 Tiamat AI, 2ShanghaiTech University, 3National University of Singapore, 4Liblib AI https://github.com/Xiaojiu-z/EasyControl Figure 1. Our proposed framework, EasyControl, is lightweight and efficient plug-and-play module specifically designed for diffusion transformer. This solution not only enables spatial control and subject/face control under single conditions but also demonstrates remarkable zero-shot multi-condition generalization capability after single-condition training, achieving sophisticated multi-condition control."
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in Unet-based diffusion models, such as ControlNet and IP-Adapter, have introduced effective spatial and subject control mechanisms. However, the DiT (Diffusion Transformer) architecture still struggles with efficient and flexible control. To tackle this issue, we propose EasyControl, novel framework designed to unify condition-guided diffusion transformers with high efficiency and flexibility. Our framework is built on three key innovations. First, we introduce lightweight Condition Injection LoRA Module. This module processes conditional signals in isolation, acting as plug-and-play solution. It avoids modifying the base models weights, ensuring compatibility with customized models and enabling the flexible injection of diverse conditions. Notably, this module also supports harmonious and robust zero-shot multi-condition generalization, even when trained only on single-condition data. Second, we propose Position-Aware Training Paradigm. This approach standardizes input conditions to fixed resolutions, allowing the generation of images with arbitrary aspect ratios and flexible resolutions. At the same time, it optimizes computational efficiency, making the framework more practical for real-world applications. Third, we develop Causal Attention Mechanism combined with the KV Cache technique, adapted for conditional generation tasks. This innovation significantly reduces the latency of image synthesis, improving the overall efficiency of the framework. Through extensive experiments, we demonstrate that EasyControl achieves exceptional performance across various application scenarios. These innovations collectively make our framework highly efficient, flexible, and suitable for wide range of tasks. 1. Introduction In recent years, image generation systems based on diffusion models have undergone significant architectural evolution. The technical trajectory has gradually shifted from the early UNet-based architecture[48, 53] to the Transformer-based DiT (Diffusion Transformer) model[9, 29, 46]. During the UNet-based era, pre-trained models such as SD 1.5/XL established thriving ecosystem, giving rise to series of plug-and-play conditional generation extension modules, such as ControlNet[83], IP-Adapter[80], and so on[33, 43, 88]. These modules achieved flexible expansion of pre-trained models by freezing pre-trained parameters and introducing additional adapters or Encoder architectures, thereby promoting the widespread application of text-to-image generation technology. The DiT architectures rise catalyzed paradigm shift in the conditional generation, marking transition toward token-based approaches in the field. These methods typically convert conditional images into image token sequences through VAE encoder, concatenate them with noise latent representations, and finetune model[20] to achieve conditional-guided generation. This approach has demonstrated significant potential in applications such as virtual try-on[12, 21, 44], image editing[7, 14, 21, 24, 73, 75, 76], spatial control generation[7, 14, 69, 73, 75, 76, 81], and subject-driven generation[7, 14, 21, 69, 73, 76]. However, despite notable advancements in existing methods, the DiT-based conditional generation systems still face several critical challenges compared to the mature UNet-based ecosystem. First, there is the issue of computational efficiency bottlenecks. When additional image tokens are introduced, the self-attention mechanism incurs quadratic time complexity relative to the input length due to its attention mechanism. Specifically, the model parameter count is proportional to the square of the number of tokens, which significantly increases inference latency and limits the expansion of practical application scenarios. Second, there is the challenge of multi-condition collaborative control. Existing methods struggle to achieve stable coordination under multi-condition guidance within singlecondition training paradigm. The representational conflicts of different conditional signals in the latent space lead to degradation in generation quality, particularly in zero-shot multi-condition combination scenarios, where the model lacks effective cross-condition interaction mechanisms. Finally, there are limitations in model adaptability. Although current Parameter-Efficient Fine-Tuning methods[20] can keep the backbone network parameters frozen, there is parameter conflict between the fine-tuning modules and the custom models in the community. This design flaw results in feature degradation during style transfer, limiting the plug-and-play characteristics of the modules. Therefore, our goal is to construct novel DiT conditional generation framework, providing the community with DiT extension solution that combines efficient inference, flexible control, and plug-and-play features, thereby promoting the smooth migration of the diffusion model ecosystem to more efficient Transformer architectures. This paper presents EasyControl, an efficient and flexible condition-guided DiT framework that achieves significant improvements in efficiency and flexibility through innovations at three fundamental levels. At the model architecture level, we propose lightweight and plug-and-play Condition Injection LoRA Module, whose core innovation lies in the isolated injection of condition signals. This module is integrated into pre-trained models through parallel branch mechanism, where lowrank projection is exclusively applied to condition branch tokens while keeping the weights of text and noise branches frozen. This design ensures seamless compatibility with customized models while supporting harmonious and robust zero-shot multi-condition generalization, significantly expanding its applicability across diverse visual generation tasks. At the token processing level, we introduce novel Position-Aware Training Paradigm that enhances resolution flexibility and computational efficiency through two key innovations: (1) resolution normalization of control conditions to reduce input sequence length, and (2) PositionAware Interpolation to maintain spatial consistency between condition tokens and noise tokens. This mechanism enables the model to effectively learn arbitrary aspect ratios and multi-resolution representations, even under lowresolution control signals. At the attention mechanism level, we implement transformation by replacing the conventional full attention with Causal Attention, integrated with the KV Cache technique. This represents the first implementation of condition feature caching strategy based on the KV Cache mechanism. Specifically, at the initial diffusion time step (t = 0), the system precomputes and persistently stores key-value pairs of all condition features, which are then reused throughout subsequent time steps (t1), achieving substantial computational savings. Through these three-level innovations, EasyControl establishes new paradigm in conditional generation, demonstrating superior efficiency and flexibility. In summary, our contributions are as follows: We EasyControl, new paradigm in conditional generation for DiT models. In EasyControl, each condition undergoes an isolated condition branch, which is adapted from the pre-trained DiT model via the Condition Injection LoRA module. This design enables seamless integration with custom models. This approach supports flexible condition injection and effectively fuses multiple conditions. Efficiency: Our framework achieves high efficiency through two key innovations. First, the Position-Aware Training Paradigm standardizes input conditions to fixed resolutions, ensuring adaptability and computational efficiency. Second, we integrate the Causal Attention Mechanism with the KV Cache technique, marking the first successful application of KV Cache in conditional generation tasks. This combination significantly reduces latency and boosts overall efficiency. Flexiblity: EasyControl offers exceptional flexibility by enabling the generation of images with varying resolutions and aspect ratios. This design ensures robust performance across diverse scenarios, balancing high-quality generation with adaptability to different requirements. 2. Related Works 2.1. Diffusion Models Diffusion models have emerged as powerful class of generative models, evolving from their initial formulation in denoising probabilistic frameworks to state-of-the-art architectures like DDPM[18], DDIM[62], and LDM[53]. These models have demonstrated remarkable success across wide range of generative tasks, including text-to-image synthesis[9, 29, 48, 52, 53, 55], text-to-video generation[1, 13, 19, 27, 61, 63, 66, 67, 70, 79], text-to-3D creation[6, 36, 39, 49, 50, 59], and image editing[2, 3, 5, 24, 26, 34, 42, 45, 60, 72, 82, 86, 87]. By iteratively refining noise into structured outputs, diffusion models have achieved unprecedented fidelity and diversity in generated content. Recently, the architecture of diffusion models has shifted from the traditional U-Net-based design to transformer-based frameworks, marking significant milestone in the field. Models like SD3[9] and FLUX[29] have adopted DiT[46] structures which have enabled improved image quality, higher resolution synthesis, and better handling of complex conditional inputs. 2.2. Conditional Image Generation Condition-guided diffusion models aim to enhance the generation process by incorporating additional conditions, enabling precise control over the output. These models can be broadly categorized into two types: pixel-aligned and non-pixel-aligned methods. Pixel-aligned approaches, such as ControlNet[83] and so on[33, 41, 43, 64, 65, 69, 88], directly align conditional inputs (e.g., edges, depth maps) with the generated output at the pixel level, ensuring fine-grained spatial control. On the other hand, nonpixel-aligned methods, often referred to as subject generation, focus on high-level semantic control. Representative works include IP-Adapter[80], SSR-Encoder[84], and so on[10, 11, 22, 23, 28, 32, 40, 47, 54, 74, 85], which leverage adapters or encoders to inject subject-specific conditions into the diffusion process. Recent advancements in DiT-based conditional generation have demonstrated significant progress. These approaches primarily encode visual conditions into tokens and integrate them with text tokens. For instance, methods[7, 31, 73, 75] trained DiT-based foundation model that tokenizes conditional images and jointly trains them with text tokens for diverse conditional visual tasks within single framework. Methods like In-context LoRA[21], OminiControl[69], and so on[12, 24, 76] introduce novel approach by concatenating images or tokens and applying task-specific LoRA within pre-trained DiT tuning with taskspecific datasets for various applications. While these methods represent notable progress in unifying control mecha- (1) Token nisms, they exhibit several critical limitations. concatenation incurs high computational costs. (2) Conventional LoRA designs struggle to handle flexible combinations of multiple conditions effectively. (3) Most methods lack open-source or plug-and-play support, hindering community adoption. Our work addresses these limitations by proposing unified DiT-based framework that ensures efficiency and flexibility. 3. Method In this section, we present the technical details of EasyControl, and the overall framework of the method is illustrated in figure 2. EasyControl is built on FLUX.1 dev. It comprises several key components: the Condition Injection LoRA Module (Sec. 3.1), the Causal Attention in EasyControl (Sec. 3.2), the Position-Aware Training Paradigm (Sec. 3.3), and the KV Cache for Inference (Sec. 3.4). The preliminaries on DiT are detailed in supplementary (Sec.A). 3.1. Condition Injection LoRA Module To efficiently incorporate conditional signals while preserving the pre-trained models generalization ability, we extend the FLUX architecture by introducing an additional Condition Branch. Unlike conventional approaches that introduce separate control modules, our method seamlessly integrates conditional information into the existing architecture while avoiding redundant parameters and computational overhead. Figure 2. The illustration of EasyControl framework. The condition signal is injected into the Diffusion Transformer (DiT) through newly introduced condition branch, which encodes the condition tokens in conjunction with lightweight, plug-and-play Condition Injection LoRA Module. During training, each individual condition is trained separately, where condition images are resized to lower resolution and trained using our proposed Position-Aware Training Paradigm. This approach enables efficient and flexible resolution training. The framework incorporates Causal Attention mechanism, which enables the implementation of Key-Value (KV) Cache to substantially improve inference efficiency. Furthermore, our design facilitates the seamless integration of multiple Condition Injection LoRA Modules, enabling robust and harmonious multi-condition generation. In Transformer-based architectures, input feature representations are first projected into query (Q), key (K), and value (V ) features before being processed by the self-attention mechanism. Given input representations Zt, Zn, Zc corresponding to the Denoising (Text, Noise), and Condition Branches, the standard QKV transformation is defined as follows: Qi, Ki, Vi = WQZi, WKZi, WV Zi, {t, n, c} (1) where WQ, WK, WV are the shared projection matrices across all branches. While this design allows for efficient parameter sharing, it does not explicitly optimize the representation of conditional signals. To address this limitation, we introduce LoRA (Low-Rank Adaptation) to adaptively enhance the conditional representations while keeping other branches unmodified: Qc, Kc, Vc = BQAQZc, BKAKZc, BV AV Zc (2) Thus, Branch are: the updated QKV features for the Condition c, c, = Qc + Qc, Kc + Kc, Vc + Vc (3) where Ai, Bi Rdr (with d, {Q, K, }) are low-rank matrices that parameterize the LoRA transformation. Notably, the Text and Noise branches remain unchanged: = Qi, = Ki, = Vi, {t, n} (4) By applying LoRA-based adaptation exclusively to the Condition Branch, we ensure that conditional signals are efficiently injected into the model without disrupting the pretrained text and noise representations. This targeted adaptation allows the model to flexibly integrate conditional information while maintaining the integrity of its original feature space, thereby achieving more controllable and high-fidelity image generation. 3.2. Causal Attention in EasyControl Causal Attention is unidirectional attention mechanism designed to restrict information flow in sequence models by allowing each position to attend only to previous positions and itself, thereby enforcing temporal causality. This is achieved by applying mask with values of 0 and to the attention logits before the Softmax operation, mathematically expressed as: = [Q t; n; c], = [K t; n; c], = [V ; n; ] (5) Softmax(QK/ (cid:112) dk + )V (6) where enforces the causality constraint and QKV are the concatenated features from the noise, text, and condition branches. To improve the efficiency of inference and effectively integrate multiple conditional signals, we design two specialized causal attention mechanisms: Causal Conditional Attention and Causal Mutual Attention. These mechanisms govern information flow through distinct masking strategies to balance conditional aggregation and isolation. 3.2.1. Causal Conditional Attention This mechanism enforces two rules: (1) intra-condition computation within each condition branch and (2) an attention mask that prevents condition tokens from querying denoising (text&noise) tokens during training. Formally, we define the input sequence in single-condition training as: = [Zt&n; Zc] (7) where Zt&n denotes the noise and text tokens, and Zc represents the condition tokens, we define an attention mask {0, }nn to regulate attention flow. Specifically, the mask is formulated as: (cid:40) Mij = , 0, if / nt&nandj nt&n otherwise (8) where = nt&n + nc represents the total sequence length. This design blocks unidirectional attention from the condition branch to the denoising (noise&text) branch while allowing denoising branch tokens to freely aggregate conditional signals. By strictly isolating condition-to-denoising queries, the design enables decoupled KV Cache states for each branch during inference, thereby reducing redundant computation and significantly improving image generation efficiency. 3.2.2. Causal Mutual Attention Our model is trained exclusively on single-condition inputs, each condition token learns optimized interactions with denoising tokens. During multi-condition inference, while all conditions interact normally with denoising tokens, inter-condition interference emerges due to untrained cross-condition token interactions (see Fig. 5). This mechanism effectively integrates multiple conditional signals while avoiding interference during inference, Formally, we define the input sequence in multiple conditions inference as: = [Zt&n; Zc1 ; Zc2; . . . ; Zcm ] (9) define an attention mask {0, }nn to regulate attention flow. Specifically, the mask is formulated as: Mij = if nt&n or nt&n if i, belong to the same condition block 0, 0, , otherwise (cross-condition blocking) (10) where = nt&n + (cid:80)m i=1 nci represents the total sequence length. This structured masking ensures that while the image tokens aggregate information from all conditions, distinct conditions remain isolated, preventing mutual interference. 3.3. Position-Aware Training Paradigm To improve computational efficiency and resolution flexibility in conditional image generation, we propose Position-aware Training Paradigm. This paradigm is based on naive approach: downscaling high-resolution control signals from their original dimensions to lower target resolution . In our experiments, we set = = 512. The resized image is then encoded into latent space via VAE encoder, followed by Patchify operation to extract Condition Tokens. These tokens are combined with Noise Tokens and Text Tokens from the original DiT model and processed through iterative denoising. While this naive downscaling approach works well for subject conditions (e.g., face images), it fails to preserve spatial alignment for spatial conditions (e.g., canny maps), limiting the models ability to generate images at arbitrary resolutions. To address this, we introduce two tailored strategies: (1) Position-Aware Interpolation (PAI) for spatial conditions, which maintains pixel-level alignment during resizing, and (2) PE Offset Strategy(see in supplementary Section B) for subject conditions, which applies fixed displacement to position encodings in the height dimension. 3.3.1. Position-Aware Interpolation To maintain spatial consistency between condition tokens and noise tokens, we introduce the Position-Aware Interpolation (PAI) strategy, which interpolates position encodings during the resizing process of conditional signals. This ensures the model accurately captures spatial relationships between control conditions and generated image pixels."
        },
        {
            "title": "Given the original conditional",
            "content": "image dimensions (M, ) and the resized dimensions (H, ), the scaling factors are computed as: Sh ="
        },
        {
            "title": "M\nH",
            "content": ", Sw ="
        },
        {
            "title": "N\nW",
            "content": "(11) where Zt&n denotes the noise and text tokens, and Zci represents the tokens corresponding to the i-th condition, we where Sh and Sw denote the scaling factors in height and width directions, respectively. For given patch (i, j) in the resized conditional image, its corresponding position (Pi, Pj) in the original image is mapped as: Pi = Sh, Pj = Sw (12) where [0, H] and [0, ]. This mapping aligns any patch in the resized image to its corresponding position in the original image."
        },
        {
            "title": "The sequence of position encodings in the original image",
            "content": "is represented as: (0, 0), (0, 1), . . . , (M, ) (13) while the interpolated sequence for the resized image is: (0, 0), (0, Sw), . . . , (H Sh, Sw) (14) This ensures that spatial relationships are preserved in the resized image. 3.3.2. Loss Function Our loss function utilizes flow-matching loss. The mathematical expression is as follows: LRF = Et,ϵN (0,I)vθ(z, t, ci) (ϵ x0) 2 2 (15) where represents the image feature at time t,ci is the input condition, vθ denotes the velocity field, x0 refers to the original image feature, and ϵ is the predicted noise. 3.4. Efficient Inference via KV Cache By leveraging the Causal Attention mechanism, our framework isolates the conditioning branch as computation-independent module that remains agnostic to denoising timesteps. This unique design enables novel application of the KV Cache technique during inference. Since the conditioning branchs computations are independent of the denoising timestep, we precompute and store the Key-Value (KV) pairs of all conditional features only once during the initial timestep. These cached KV pairs are reused across all subsequent timesteps, eliminating redundant recomputation of identical conditional features. This approach reduces inference latency by avoiding -fold recomputation (for denoising steps) while preserving generation quality and model flexibility (detailed in supplementary Sec. D). 4. Experiments This section begins with description of the implementation details of EasyControl, followed by an overview of the evaluation metrics. We then present the experimental results, including both qualitative and quantitative analyses, as well as ablation experiments. Cond. Method Time(s) Params ControlNet OminiControl Single Ours(w.o. PATP&KVCache) Ours(w.o. KVCache) Ours(w.o. PATP) Ours(Full) ControlNet+IPA Multi-ControlNet Double Ours(w.o. PATP&KVCache) Ours(w.o. KVCache) Ours(w.o. PATP) Ours(Full) 16.5 31.6 38.9 22.4(-42%) 25.0(-36%) 16.3(-58%) 16.8 20.3 72.4 29.9(-59%) 36.7(-50%) 18.3(-75%) 3B 15M - - - 15M 4B 6B - - - 30M Table 1. Quantitative efficiency comparison with baseline methods and different settings. The inference time is calculated when generating 10241024 resolution image with 25 denoising steps. The parameters refer exclusively to those of the additional module, excluding the parameters of the base model. 4.1. Implementation Details We employed FLUX.1 dev as the pre-trained DiT. For each spatial or subject condition training, we utilize 4 A100 GPUs(80GB), batch size of 1 per GPU, and learning rate of 1e-4, training over 100,000 steps. During inference, the flow-matching sampling is applied with 25 sampling steps. (Training Data is detailed in supplementary Sec. C.) 4.2. Experimental Settings Visual Comparisons: We evaluate the following settings: (1) Single-condition generation, (2) Single-condition adaptation with customized models, (3) Multi-condition integration (illustrated in Figures 3 and 4, we also compare with several ID customization methods [15, 35, 71] in supplementary Sec. for details), and (4) Resolution adaptability (detailed in supplementary Sec. G). Quantitative Comparisons: We assess the following aspects: (1) Inference time and model parameter count under singleand dual-condition generation (to evaluate efficiency, as shown in Table 1), (2) Controllability, generation quality, and text consistency using face + OpenPose as multi-conditions (detailed in supplementary Sec. F), and (3) Controllability, generation quality, and text consistency under singlecondition settings (detailed in supplementary Sec. E). Comparison Methods For single-condition, we comand pare with Controlnet[83], OminiControl[69], For multi-condition settings, we Uni-ControlNet[88]. evaluate our approach against several plug-and-play baseline methods including Controlnet+IP-Adapter[80], Controlnet+Redux[30], and Uni-Controlnet[88]. We also compare several ID customization methods[15, 35, 71] integrated with ControlNet. Figure 3. Visual comparison between different methods in single condition control. Figure (a) shows the results of each method under different control conditions and Figure (b) shows the adaptation of each method with different Style LoRA[38, 5658] under control. Figure 4. Visual comparison of different methods under multi-condition control. 4.3. Experiment Results 4.3.1. Qualitative Comparison Fig. 3 (a) compares the performance of different methods under single-control conditions. Under Canny control, Uni-ControlNet and ControlNet exhibit color inconsistencies, causing deviations from the input text. Under Depth Control, Uni-ControlNet fails to generate coherent images, while ControlNet and OmniControl introduce artifacts, such as the fusion of the dog and the sofa. Under OpenPose control, our method preserves text rendering, whereas others weaken or lose this ability. In Subject Control, IP-Adapter and Uni-ControlNet fail to align with the reference. Overall, our method ensures text consistency and high-quality generation across diverse control conditions. Fig 3 (b) compares the Plug-and-play capability of different methods in generating images on four custom models. The leftmost column shows the original text-to-image (T2I) results from the LoRA fine-tuned Flux.1 Dev model. Both ControlNet and OminiControl sacrifice stylization, and suffer from quality degradation. In contrast, our method demonstrates the ability to minimize stylization loss without losing controllability which demonstrates the plug-andcantly lower than ControlNets 3B parameters. For doublecondition tasks, our full model achieves an inference time of 18.3 seconds, which is 75% faster than the ablated version without PATP and KV Cache. This performance is competitive with ControlNet+IPA (16.8 seconds) while maintaining much smaller model size (30M parameters compared to 4B for ControlNet+IPA). The results highlight the effectiveness of our proposed PATP and KV Cache mechanisms in improving inference efficiency without compromising model compactness. 4.3.3. Ablation Study In our ablation study, we analyze the impact of removing each module. First, replacing the Condition Injection LoRA (CIL) with the standard LoRA structure (W.O. CIL) enables single-condition control but fails to generalize to multicondition control in zero-shot manner. For the PositionAware Training Paradigm (PATP), we trained model W.O. PATP, where both the control signal and noise were fixed at 512512 resolution while keeping other training settings unchanged. This model exhibits artifacts and quality degradation when generating high-resolution (e.g., 10241024) or non-square aspect ratio (e.g., 1024768) images. In contrast, our PATP-based training effectively mitigates these issues. For Causal Attention, removing Causal Mutual Attention(CMA) still allows image generation due to the adaptive nature of attention. However, conflicts between conditions reduce control accuracy, leading to deviations such as altered human poses in Multi-Control scenarios and shifts in object positions, such as the moon. When all modules are used together, our method achieves the highest controllability, generation quality, and adaptability to varying resolutions and aspect ratios. 5. Conclusion In conclusion, we present EasyControl, highly efficient and flexible framework for unified condition-guided diffusion models. Our framework leverages three key innovations:(1) lightweight Condition Injection LoRA Module, which enables seamless integration of diverse condition signals without altering the core models functionality. (2) Position-Aware Training Paradigm, which ensures adaptability to various resolutions and aspect ratios. (3) novel Causal Attention Mechanism combined with the KV Cache technique, which significantly improves efficiency. Together, these components address the challenges of efficiency and flexibility in controllable image generation. EasyControl achieves strong controllability and highquality results across wide range of visual tasks. Extensive experiments demonstrate its ability to handle complex, multi-condition scenarios while scaling to diverse resolutions and aspect ratios. Our framework offers powerful and adaptable solution for conditional image generation. Figure 5. Visual ablation on different settings. play capability of our method. Fig. 4 presents visual comparison of different methods under multi-condition control. For OpenPose and Face, our approach achieves superior identity consistency and controllability. In contrast, other methods exhibit conflicts between control conditions. While the combination of ControlNet and IP-Adapter maintains controllability, it compromises identity consistency. ControlNet+Redux and UniControlNet fail to preserve both identity consistency and controllability, which is also observed in subject-depth control scenarios (Right third/fourth row). For OpenPoseCanny and Depth-Canny combinations, both our method and Uni-ControlNet generate images that satisfy the control conditions. However, Uni-ControlNet struggles to align with textual inputs and produces lower-quality images. Multi-ControlNet fails to satisfy both conditions simultaneously. These results demonstrate the flexibility of our method in seamlessly integrating multiple conditions. 4.3.2. Quantitative Comparison Table 1 presents the inference time and corresponding model parameter counts for various algorithms on single A100 GPU with 20 sampling steps. In single-condition settings, our full model achieves the best performance with an inference time of 16.3 seconds, representing 58% reduction compared to the ablated version without the PositionAware Training Paradigm (PATP) and KV Cache. Notably, our method achieves this efficiency while maintaining minimal parameter count of 15M, which is signifiAppendix. A. Preliminary: Diffusion Transformer Currently, The state-of-the-art text-to-image Diffusion model architectures are based on Diffusion Transformers (DiT)[46], including models such as SD3[9], FLUX[29]. These models integrate diffusion processes with Transformer architectures to improve text-to-image generation, yielding high-quality and accurate text-to-image synthesis. Our approach is based on the FLUX.1 pre-trained model, which consists of three key components: T5 as the text encoder, VAE for image compression, and Transformerbased denoising network. Specifically, The denoising network divides the latent noise into several patches and treats each patch as noise token, denoted as RN d, where is the number of noise tokens and is the dimensionality of each token. To effectively introduce spatial position information in different noise patches, FLUX.1 employs rotary position encoding (RoPE)[68] to encode spatial information within each noise patch. Meanwhile, the text prompts are encoded into text tokens CT RM through the T5 text encoder, where represents the number of text tokens. These image and text tokens are then fused by concatenation to form joint representation. After feature fusion, the image-text token sequence is fed parallel into transformer-based denoising model. The model iteratively denoises the image, progressively restoring clear image and ultimately generating high-quality image that aligns with the textual description. B. Position Encoding Offset The PE offset strategy was proposed by method[69], which applies fixed displacement to position encodings and was proved to lead to faster convergence. This offset is uniform across all encodings within the subject condition image. In our experiments, we set this offset to 64 in the height dimension. Mathematically, for each position encoding E(i, j) in the subject condition image, the adjustment is: E(i, j) E(i, j) + eh (16) where eh is the unit vector along the height dimension, and = 64 ensures distinct separation between spatial and subject conditions. C. Trianing Data For spatial control tasks such as depth, canny, and OpenPose, we employ the MultiGen-20M dataset[88] as our primary training resource. Regarding subject control, our training is conducted using the Subject200K dataset[69]. For face control, we utilize curated subset of the LAIONFace dataset[89], supplemented by collected private multiview human dataset (See Fig 6), where all human images Figure 6. Visualization of samples in private Multi-view Human Dataset. are preprocessed through InsightFace[8] for precise cropping and alignment to ensure consistency and accuracy in our training inputs. D. Details of KV Cache More details of KV Cache for Efficient Conditional Image Generation is shown in the algorithm 1. E. Single Condition Quantitative Comparison Settings. In this section, we compare our method with Controlnet[83], OmniControl[69], and Uni-Control[88] using two types of conditioning: Depth and Canny. For the subject condition, We compare our method with OmniControl[69], IP-Adapter(IPA)[80], and Uni-ControlNet[88]. (To ensure fair and consistent comparison, all methods are implemented using FLUX.1 dev as the base model, with configurations and parameters sourced from publicly available official[30, 69, 83] and community resources[25, 77] with recommended parameters, while Uni-ControlNet employs its official implementation[88] based on the SD1-5 architecture.) Data. For the Depth map and Canny edge conditions, comprehensive evaluations were conducted on the COCO 2017[37] validation set comprising 5,000 images. All generated outputs strictly preserved the original image resolutions and aspect ratios, with textual prompts derived from the corresponding ground-truth captions of the dataset. For subject control scenarios, we adopted the Concept-101 benchmark dataset[28] to assess model performance. Each reference image was paired with its semantically aligned Condition Method Controllability F1 /MSE Generative Quality FID MAN-IQA Text Consistency CLIP-Score Canny Depth ControlNet OminiControl Uni-ControlNet Ours ControlNet OminiControl Uni-ControlNet Ours 0.232 0.314 0.201 0.311 1781 1103 1685 1092 20.325 17.237 17.375 16. 23.968 18.536 21.788 20.394 0.420 0.471 0.402 0.503 0.319 0.431 0.423 0.469 0.271 0.283 0.279 0.286 0.265 0.285 0.279 0.289 Table 2. Quantitative comparison with baseline methods on single condition tasks. Condition Method Identity Preservation CLIP-I DINO-I Subject IP-Adapter OminiControl Uni-ControNet Ours 0.700 0.663 0.641 0.667 0.429 0.445 0.417 0.443 Generative Quality FID 79.277 72.298 86.369 71. MAN-IQA 0.511 0.579 0.439 0.595 Text Consistency CLIP-Score 0.266 0.276 0.204 0.283 Table 3. Quantitative comparison with baseline methods on single condition tasks. Condition Method ID Preservation Controllability Face Sim. MJPE Generative Quality FID MAN-IQA Text Consistency CLIP-Score Openpose+Face ControlNet+IPA ControlNet+Redux Uni-ControlNet ControlNet+InstantID ControlNet+PhotoMaker ControlNet+Uni-portrait Ours 0.049 0.027 0.048 0.521 0.343 0.456 0.530 166.7 141.5 258.8 83.9 86.3 46.0 36.7 227.06 200.70 203.31 203.17 213.83 203.07 184.93 0.229 0.293 0.481 0.345 0.420 0.564 0. 0.156 0.217 0.147 0.250 0.281 0.253 0.285 Table 4. Quantitative comparison with baseline methods on multi-condition tasks. textual description as the conditioning prompt. Metrics. To comprehensively evaluate the performance of each algorithm, we assess four key aspects: 1. Controllability: We extract structural information from the generated images using the corresponding structure extractor, obtaining the structure map. The F1 Score is computed between the extracted and input edge maps in the edge-conditioned generation, and the MSE is calculated between the extracted and original condition maps for the depth task. 2. Text Consistency: We use the CLIP-Score[16, 51] to evaluate the consistency between the generated images and the input text. 3. Generative Quality: The diversity and quality of the generated images are assessed using FID[17], MANIQA[78].4. Identity Preservation: For the subject condition, we use CLIP-I[51] and DINO-I[4] to evaluate identity preservation. Specifically, CLIP-I computes the cosine similarity between image embeddings extracted by the CLIP image encoder for both generated and reference images. Similarly, DINO-I measures identity preservation by calculating the cosine similarity of image embeddings obtained through the DINO encoder framework. Quantitative Analysis. As shown in Table 2, our proposed method demonstrates superior performance across multiple evaluation metrics. Under the Canny condition, our approach outperforms all comparative methods in terms of generation quality and text consistency, while achieving the second-highest score in controllability. In the depth condition, our method exhibits dominant performance in both controllability and text consistency. Regarding generation quality, while our method ranks second position in the FID metric, it achieves first according to the MAN-IQA metric. These comprehensive results substantiate the superiority of our approach across most evaluation criteria, particularly highlighting its exceptional performance in controllability, generation quality, and text consistency. As shown in Table 3, Under the Subject condition, our approach outperAlgorithm 1 KV Cache for Efficient Conditional Image Generation Require: 1: Conditional features {condi}m 2: Denoising steps = {t0, t1, ..., tT } i=1 for conditions Ensure: Generated image x0 with reduced latency 3: Initialize KV cache dictionary 4: Generate initial noisy image xN (0, I) 5: for timestep {tT , tT 1, ..., t0} do 6: 7: 8: 9: if = tT (First step) then Compute keys/values: KCi, VCi = fθ(condi) Cache KV pairs: D[i] (KCi, VCi) for each condition branch {1, ..., m} do 10: 11: 12: 13: 14: end for end if Retrieve cached KV pairs: {(KCi, VCi)}m Compute self-attention using current noise and text features: Qdenoising, Kdenoising, Vdenoising = fθ(xt, t) Fuse conditions via cached K/V pairs: i=1 Q = Qdenoising, = Concat (Kdenoising, KC1 , . . . , KCm ) , = Concat (Vdenoising, VC1, . . . , VCm ) Output = Softmax (cid:18) QK (cid:19) Update latent: xt1 = Denoise(xt, t, Output) 15: 16: end for 17: return Final image x0 forms all comparative methods in terms of generation quality and text consistency and achieves competitive results on identity preservation. Qualitative Analysis. We show some results about spatial control in figure 9. Under identical conditional input configurations, both ControlNet and OminiControl demonstrate significant blurring artifacts in the synthesized images. In contrast, our framework consistently preserves superior visual fidelity across all evaluated scenarios. This qualitative advantage is particularly pronounced in the preservation of fine-grained details and structural integrity, thereby substantiating the enhanced performance of our Position-Aware Training Paradigm. We have also visualized several subject control results in Figure 10 to demonstrate the effectiveness of our method in terms of identity preservation, generative quality, and text consistency. (The prompts utilized in the generated images include: in the forest, in the library, on snow-covered mountain, in the city, in room, in front of castle, floating on water, on the beach, on mountain, in the desert, and on snowy day.) F. Multi-Condition Quantitative Comparison and Settings. In this section, we conduct comparisons using face + OpenPose as multi-condition configurations, against several plug-and-play baseline methods including: Controlnet+Redux[30], Controlnet+IP-Adapter[80], and Uni-Controlnet[88] SOTA playseveral and-plug identity customization methods, including Controlnet+InstantID[71], Controlnet+PhotoMaker[35], and ControlNet+Uni-portrait[15]. (For several plug-andplay baseline methods, we utilize official FLUX-based implementations such as OminiControl and communitydriven implementations including ControlNet, IPA, and Redux. For identity customization methods, we adopt official implementations based on SD1-5 (e.g., Uni-portrait) and SDXL (e.g., PhotoMaker, InstantID) as base models, along with their corresponding ControlNet modules.) Data. For evaluation, we constructed comprehensive dataset comprising three components: (1) 1,000 randomly sampled face images from the FFHQ dataset for face control inputs; (2) 1,000 full-body or half-body human images crawled from Laion face dataset[89], from which OpenPose information was extracted for spatial control inputs; and (3) 1,000 text prompts generated by GPT, each describing person with specific characteristic and locations. Each algorithm generated 1,000 images based on these inputs for evaluation. This diverse dataset ensures thorough assessment of the models capabilities in handling various control conditions. Metrics. To comprehensively evaluate the performance of each algorithm, we assess several key aspects: 1. Controllability: The Mean Joint Position Error (MJPE) metrics computed between the extracted and input openpose maps in the pose generation. 2. Text Consistency: We use the CLIP-Score to evaluate the consistency between the generated images and the input text. 3. Generative Quality: The diversity and quality of the generated images are assessed using FID[17], MAN-IQA[78]. 4. Identity Preservation: For the face condition, we use face similarity[8] to evaluate identity preservation. For the OpenPose condition, our controllability metric is quantitatively assessed through the Mean Joint Position Error (MJPE) metrics, which measure the spatial consistency between the generated image and the input OpenPose map. The evaluation procedure involves three sequential steps: initially, key point information is extracted from both the generated image and the input condition using OpenPose. Subsequently, The Euclidean distance for each joint is computed and averaged to obtain the MJPE for single image. This process is repeated for all images in the test set, and the average MJPE serves as the models controllability metric. By quantifying joint position deviations, MJPE effectively evaluates the consistency between generated images and input conditions. It is noteworthy that lower MJPE indicates superior spatial alignment and better pose consistency between the generated image and the input condition, thus reflecting higher controllability in the pose generation process. F.1. Quantitative Comparison The quantitative results are presented in Table 4. Our method achieves state-of-the-art performance across all metrics. Specifically, it obtains the best Face Similarity, demonstrating superior ID preservation. For controllability, our approach achieve the lowest MJPE score and significantly outperforms others. In terms of generative quality, our method achieves the lowest FID and highest MANIQA, indicating better image quality and diversity. Additionally, it maintains strong text consistency with the highest CLIP score. These results collectively demonstrate the effectiveness of our framework in balancing control precision, identity preservation, and generation quality under multi-condition combination. It is noteworthy that in Table 4, certain algorithms exhibit significantly inferior performance in terms of Face Similarity (Face Sim) and Mean Joint Position Error (MJPE) metrics compared to other methods. This is primarily attributed to the fact that many competing methods fail to effectively transfer facial or pose features from the control images, often resulting in generated images that are blurry, distorted, or lack detectable facial or pose features. Consequently, these methods are unable to accurately compute the metrics required for face similarity or pose alignment. In contrast, our approach ensures robust feature transfer and precise alignment, enabling the generation of high-quality images with clearly detectable facial and pose attributes, which contributes to the superior performance reflected in the metrics. F.2. Visual Comparison As illustrated in the figure 8, we present visual comparison with ID customization methods. Our method demonstrates superior performance in facial similarity, controllability, and image quality compared to other approaches. This indicates that our framework, despite being trained on single conditions, exhibits strong plug-and-play adaptability, effectively integrating multiple conditions without conflicts. In contrast, other methods often suffer from incompatibility between different modules, leading to degraded facial similarity, controllability, and poor generation quality. The visual results further validate the robustness and versatility of our approach in handling complex multi-condition generation tasks. G. Visual Comparison of Resolution Adaptability As shown in the figure 11, we compare the controllability of our method with DiT-based controllable baseline methFigure 7. Visualization of results (1) under conflicting condition inputs (2) under very high-resolution generation. ods, including ControlNet and OmniControl, across different resolutions. Clearly, our approach consistently demonstrates strong controllability, high text consistency, and superior image quality across resolutions ranging from low to high. However, at lower resolutions, ControlNet exhibits image distortion, while at higher resolutions, OmniControl also suffers from image degradation. This demonstrates that our method exhibits strong adaptability across different resolutions. H. Limitations While the proposed framework demonstrates significant improvements in flexibility and computational efficiency compared to existing DiT-based approaches, certain technical limitations remain and warrant further investigation. For example, in multi-conditional scenarios involving conflicting inputs, the model may generate artifacts characterized by overlapping layers, as illustrated in Figure 7. Additionally, our method cannot indefinitely upscale generated resolutions. When the resolution becomes extremely high, there is decrease in the ability to control the output. Figure 8. Visual comparison with Identity customization methods under multi-condition generation setting. Figure 9. Visualization of spatial control generation. Figure 10. Visualization of subject control generation. Figure 11. Visual comparison with baseline methods under different resolution generation settings.(zoom in for better view)"
        },
        {
            "title": "References",
            "content": "[1] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 3 [2] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1839218402, 2023. 3 [3] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2256022570, 2023. 3 [4] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. EmergIn ing properties in self-supervised vision transformers. Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 10 [5] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM transactions on Graphics (TOG), 42(4):110, 2023. 3 [6] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for highIn Proceedings of the quality text-to-3d content creation. IEEE/CVF international conference on computer vision, pages 2224622256, 2023. 3 [7] Xi Chen, Zhifei Zhang, He Zhang, Yuqian Zhou, Soo Ye Kim, Qing Liu, Yijun Li, Jianming Zhang, Nanxuan Zhao, Yilin Wang, et al. Unireal: Universal image generation and arXiv preprint editing via learning real-world dynamics. arXiv:2412.07774, 2024. 2, [8] Jiankang Deng, Jia Guo, Xue Niannan, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In CVPR, 2019. 9, 11 [9] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 2, 3, 9 [10] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toarXiv preprint image generation using textual inversion. arXiv:2208.01618, 2022. 3 [11] Rinon Gal, Moab Arar, Yuval Atzmon, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. Encoder-based domain tuning for fast personalization of text-to-image models. ACM Transactions on Graphics (TOG), 42(4):113, 2023. 3 [12] Hailong Guo, Bohan Zeng, Yiren Song, Wentao Zhang, Chuang Zhang, and Jiaming Liu. Any2anytryon: Leveraging adaptive position embeddings for versatile virtual clothing tasks. arXiv preprint arXiv:2501.15891, 2025. 2, 3 [13] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 3 [14] Zhen Han, Zeyinzi Jiang, Yulin Pan, Jingfeng Zhang, Chaojie Mao, Chenwei Xie, Yu Liu, and Jingren Zhou. Ace: Allround creator and editor following instructions via diffusion transformer. arXiv preprint arXiv:2410.00086, 2024. [15] Junjie He, Yifeng Geng, and Liefeng Bo. Uniportrait: unified framework for identity-preserving singleand multi-human image personalization. arXiv preprint arXiv:2408.05939, 2024. 6, 11 [16] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. 10 [17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 10, 11 [18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 3 [19] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, Cogvideo: Large-scale pretraining for arXiv preprint and Jie Tang. text-to-video generation via transformers. arXiv:2205.15868, 2022. 3 [20] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 2 [21] Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, Yutong Feng, Yu Liu, and Jingren Zhou. In-context lora for diffusion transformers. arXiv preprint arXiv:2410.23775, 2024. 2, 3 [22] Mengqi Huang, Zhendong Mao, Mingcong Liu, Qian He, and Yongdong Zhang. Realcustom: Narrowing real text word for real-time open-domain text-to-image customizaIn Proceedings of the IEEE/CVF Conference on tion. Computer Vision and Pattern Recognition, pages 7476 7485, 2024. 3 [23] Qihan Huang, Siming Fu, Jinlong Liu, Hao Jiang, Yipeng Yu, and Jie Song. Resolving multi-condition confusion arXiv for finetuning-free personalized image generation. preprint arXiv:2409.17920, 2024. 3 [24] Shijie Huang, Yiren Song, Yuxuan Zhang, Hailong Guo, Xueyin Wang, Mike Zheng Shou, and Jiaming Liu. Photodoodle: Learning artistic image editing from few-shot pairwise data. arXiv preprint arXiv:2502.14397, 2025. 2, 3 https : FLUX.1-dev-Controlnet-Union. / / huggingface . co / InstantX / FLUX . 1 - dev - Controlnet-Union. [25] InstantX. [26] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 60076017, 2023. 3 [27] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin Long, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Dax Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, and Caesar Zhong. Hunyuanvideo: systematic framework for large video generative models, 2025. 3 [28] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19311941, 2023. 3, 9 [29] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 2, 3, 9 [30] Black Forest Labs. https : / / huggingface . co / black - forest - labs / FLUX . 1-Redux-dev, n.d. 6, 9, 11 FLUX.1-Redux-dev. [31] Duong Le, Tuan Pham, Sangho Lee, Christopher Clark, Aniruddha Kembhavi, Stephan Mandt, Ranjay Krishna, and Jiasen Lu. One diffusion to generate them all. arXiv preprint arXiv:2411.16318, 2024. 3 [32] Dongxu Li, Junnan Li, and Steven Hoi. Blip-diffusion: Pretrained subject representation for controllable text-to-image generation and editing. Advances in Neural Information Processing Systems, 36, 2024. [33] Ming Li, Taojiannan Yang, Huafeng Kuang, Jie Wu, Zhaoning Wang, Xuefeng Xiao, and Chen Chen. Controlnet++: Improving conditional controls with efficient consistency In European Conference on Computer Vision, feedback. pages 129147. Springer, 2025. 2, 3 [34] Shanglin Li, Bohan Zeng, Yutang Feng, Sicheng Gao, Xiuhui Liu, Jiaming Liu, Lin Li, Xu Tang, Yao Hu, Jianzhuang Liu, et al. Zone: Zero-shot instruction-guided local editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 62546263, 2024. 3 [35] Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, Ming-Ming Cheng, and Ying Shan. Photomaker: Customizing realistic human photos via stacked id embedding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 86408650, 2024. 6, 11 [36] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 300309, 2023. 3 [37] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence In Zitnick. Microsoft coco: Common objects in context. Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pages 740755. Springer, 2014. 9 [38] linoyts. yarn-art-Flux-LoRA. https://huggingface. co/linoyts/yarn_art_Flux_LoRA, n.d. 7 [39] Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang, Chao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Jiayuan Gu, and Hao Su. One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10072 10083, 2024. 3 [40] Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu. Subjectdiffusion: Open domain personalized text-to-image generation without test-time fine-tuning. In ACM SIGGRAPH 2024 Conference Papers, pages 112, 2024. 3 [41] Sicheng Mo, Fangzhou Mu, Kuan Heng Lin, Yanli Liu, Freecontrol: Bochen Guan, Yin Li, and Bolei Zhou. Training-free spatial control of any text-to-image diffusion model with any condition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 74657475, 2024. 3 [42] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real imIn Proceedings of ages using guided diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 60386047, 2023. 3 [43] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI conference on artificial intelligence, pages 42964304, 2024. 2, [44] Haifeng Ni. Itvton: Virtual try-on diffusion transformer model based on integrated image and text. arXiv preprint arXiv:2501.16757, 2025. 2 [45] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-toimage translation. In ACM SIGGRAPH 2023 conference proceedings, pages 111, 2023. 3 [46] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4195 4205, 2023. 2, 3, 9 [47] Single-subject Personalization. Ms-diffusion: Multi-subject zero-shot image personalization with layout guidance. 3 [48] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 2, 3 [49] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. [50] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123: One image to high-quality 3d object generation using both 2d and 3d diffusion priors. arXiv preprint arXiv:2306.17843, 2023. 3 [51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 87488763. PMLR, 2021. 10 [52] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 8821 8831. Pmlr, 2021. 3 [53] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, 3 [54] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF Conference generation. on Computer Vision and Pattern Recognition, pages 22500 22510, 2023. [55] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 3 [56] Shakker-Labs. FLUX.1-dev-LoRA-Text-Poster. https: / / huggingface . co / Shakker - Labs / FLUX . 1 - dev-LoRA-Text-Poster, . 7 [57] Shakker-Labs. FLUX.1-dev-LoRA-Vector-Journey. https : / / huggingface . co / Shakker - Labs / FLUX . 1 - dev - LoRA - Vector - Journey, . FLUX.1-dev-LoRA-Children-Simple- [58] Shakker-Labs. https : / / huggingface . co / Shakker - Sketch. Labs / FLUX . 1 - dev - LoRA - Children - Simple - Sketch, . 7 [59] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512, 2023. [60] Yichun Shi, Peng Wang, and Weilin Huang. Seededit: Align image re-generation to image editing. arXiv preprint arXiv:2411.06686, 2024. 3 [61] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 3 [62] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 3 and Stefano Ermon. arXiv preprint [63] Yiren Song, Shijie Huang, Chen Yao, Xiaojun Ye, Hai Ci, Jiaming Liu, Yuxuan Zhang, and Mike Zheng Shou. Processpainter: Learn painting process from sequence data. arXiv preprint arXiv:2406.06062, 2024. 3 [64] Yiren Song, Shengtao Lou, Xiaokang Liu, Hai Ci, Pei Yang, Jiaming Liu, and Mike Zheng Shou. Anti-reference: Universal and immediate defense against reference-based generation. arXiv preprint arXiv:2412.05980, 2024. 3 [65] Yiren Song, Pei Yang, Hai Ci, and Mike Zheng Shou. to protect arXiv preprint Idprotector: An adversarial noise encoder against arXiv:2412.11638, 2024. 3 id-preserving image generation. [66] Yiren Song, Danze Chen, and Mike Zheng Shou. Layertracer: Cognitive-aligned layered svg synthesis via diffusion transformer. arXiv preprint arXiv:2502.01105, 2025. 3 [67] Yiren Song, Cheng Liu, and Mike Zheng Shou. Makeanyfor multiHarnessing diffusion transformers arXiv preprint thing: domain procedural sequence generation. arXiv:2502.01572, 2025. 3 [68] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 9 [69] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. arXiv preprint arXiv:2411.15098, 3, 2024. 2, 3, 6, 9 [70] Cong Wan, Xiangyang Luo, Zijian Cai, Yiren Song, Yunlong Zhao, Yifan Bai, Yuhang He, and Yihong Gong. Grid: Visual layout generation. arXiv preprint arXiv:2412.10718, 2024. 3 [71] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, and Anthony Chen. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024. 6, 11 [72] Rui Wang, Hailong Guo, Jiaming Liu, Huaxia Li, Haibo Zhao, Xu Tang, Yao Hu, Hao Tang, and Peipei Li. Stablegarment: Garment-centric generation via stable diffusion. arXiv preprint arXiv:2403.10783, 2024. 3 [73] Bin Xia, Yuechen Zhang, Jingyao Li, Chengyao Wang, Yitong Wang, Xinglong Wu, Bei Yu, and Jiaya Jia. Dreamomni: Unified image generation and editing. arXiv preprint arXiv:2412.17098, 2024. 2, [74] Guangxuan Xiao, Tianwei Yin, William Freeman, Fredo Fastcomposer: Tuning-free Durand, and Song Han. multi-subject image generation with localized attention. International Journal of Computer Vision, pages 120, 2024. 3 [75] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, and arXiv Zheng Liu. Omnigen: Unified image generation. preprint arXiv:2409.11340, 2024. 2, 3 [76] Ming Xie, Chenjie Cao, Yunuo Cai, Xiangyang Xue, and Yu-Gang Jiang. Anyrefill: unified, data-efficient framework for left-prompt-guided vision tasks. arXiv preprint arXiv:2502.11158, 2025. 2, 3 [77] XLabs-AI. flux-ip-adapter. https://huggingface. co/XLabs-AI/flux-ip-adapter-v2. 9 [78] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and Yujiu Yang. Maniqa: Multi-dimension attention network for no-reference image quality assessment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11911200, 2022. 10, [79] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 3 [80] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 2, 3, 6, 9, 11 [81] Hong Zhang, Zhongjie Duan, Xingjun Wang, Yingda Chen, and Yu Zhang. Eligen: Entity-level controlled image generation with regional attention. arXiv preprint arXiv:2501.01097, 2025. 2 [82] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instructionguided image editing. Advances in Neural Information Processing Systems, 36:3142831449, 2023. 3 [83] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. 2, 3, 6, 9 [84] Yuxuan Zhang, Yiren Song, Jiaming Liu, Rui Wang, Jinpeng Yu, Hao Tang, Huaxia Li, Xu Tang, Yao Hu, Han Pan, and Zhongliang Jing. Ssr-encoder: Encoding selective subject representation for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 80698078, 2024. 3 [85] Yuxuan Zhang, Yiren Song, Jinpeng Yu, Han Pan, and Zhongliang Jing. Fast personalized text to image synthesis with attention injection. In ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 61956199, 2024. [86] Yuxuan Zhang, Lifu Wei, Qing Zhang, Yiren Song, Jiaming Liu, Huaxia Li, Xu Tang, Yao Hu, and Haibo Zhao. Stablemakeup: When real-world makeup transfer meets diffusion model. arXiv preprint arXiv:2403.07764, 2024. 3 [87] Yuxuan Zhang, Qing Zhang, Yiren Song, Jichao Zhang, Hao Tang, and Jiaming Liu. Stable-hair: Real-world hair transfer via diffusion model. arXiv preprint arXiv:2407.14078, 2024. 3 [88] Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and Kwan-Yee Wong. Uni-controlnet: All-in-one control to text-to-image diffusion models. Advances in Neural Information Processing Systems, 36, 2024. 2, 3, 6, 9, 11 [89] Yinglin Zheng, Hao Yang, Ting Zhang, Jianmin Bao, Dongdong Chen, Yangyu Huang, Lu Yuan, Dong Chen, Ming Zeng, and Fang Wen. General facial representation learnIn Proceedings of the ing in visual-linguistic manner. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1869718709, 2022. 9,"
        },
        {
            "title": "References",
            "content": "[1] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 3 [2] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1839218402, 2023. 3 [3] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2256022570, 2023. 3 [4] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. EmergIn ing properties in self-supervised vision transformers. Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 10 [5] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM transactions on Graphics (TOG), 42(4):110, 2023. 3 [6] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for highIn Proceedings of the quality text-to-3d content creation. IEEE/CVF international conference on computer vision, pages 2224622256, 2023. 3 [7] Xi Chen, Zhifei Zhang, He Zhang, Yuqian Zhou, Soo Ye Kim, Qing Liu, Yijun Li, Jianming Zhang, Nanxuan Zhao, Yilin Wang, et al. Unireal: Universal image generation and arXiv preprint editing via learning real-world dynamics. arXiv:2412.07774, 2024. 2, [8] Jiankang Deng, Jia Guo, Xue Niannan, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In CVPR, 2019. 9, 11 [9] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 2, 3, 9 [10] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toarXiv preprint image generation using textual inversion. arXiv:2208.01618, 2022. 3 [11] Rinon Gal, Moab Arar, Yuval Atzmon, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. Encoder-based domain tuning for fast personalization of text-to-image models. ACM Transactions on Graphics (TOG), 42(4):113, 2023. 3 [12] Hailong Guo, Bohan Zeng, Yiren Song, Wentao Zhang, Chuang Zhang, and Jiaming Liu. Any2anytryon: Leveraging adaptive position embeddings for versatile virtual clothing tasks. arXiv preprint arXiv:2501.15891, 2025. 2, 3 [13] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 3 [14] Zhen Han, Zeyinzi Jiang, Yulin Pan, Jingfeng Zhang, Chaojie Mao, Chenwei Xie, Yu Liu, and Jingren Zhou. Ace: Allround creator and editor following instructions via diffusion transformer. arXiv preprint arXiv:2410.00086, 2024. [15] Junjie He, Yifeng Geng, and Liefeng Bo. Uniportrait: unified framework for identity-preserving singleand multi-human image personalization. arXiv preprint arXiv:2408.05939, 2024. 6, 11 [16] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. 10 [17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 10, 11 [18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 3 [19] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, Cogvideo: Large-scale pretraining for arXiv preprint and Jie Tang. text-to-video generation via transformers. arXiv:2205.15868, 2022. 3 [20] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 2 [21] Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, Yutong Feng, Yu Liu, and Jingren Zhou. In-context lora for diffusion transformers. arXiv preprint arXiv:2410.23775, 2024. 2, 3 [22] Mengqi Huang, Zhendong Mao, Mingcong Liu, Qian He, and Yongdong Zhang. Realcustom: Narrowing real text word for real-time open-domain text-to-image customizaIn Proceedings of the IEEE/CVF Conference on tion. Computer Vision and Pattern Recognition, pages 7476 7485, 2024. 3 [23] Qihan Huang, Siming Fu, Jinlong Liu, Hao Jiang, Yipeng Yu, and Jie Song. Resolving multi-condition confusion arXiv for finetuning-free personalized image generation. preprint arXiv:2409.17920, 2024. 3 [24] Shijie Huang, Yiren Song, Yuxuan Zhang, Hailong Guo, Xueyin Wang, Mike Zheng Shou, and Jiaming Liu. Photodoodle: Learning artistic image editing from few-shot pairwise data. arXiv preprint arXiv:2502.14397, 2025. 2, 3 https : FLUX.1-dev-Controlnet-Union. / / huggingface . co / InstantX / FLUX . 1 - dev - Controlnet-Union. [25] InstantX. [26] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 60076017, 2023. 3 [27] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin Long, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Dax Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, and Caesar Zhong. Hunyuanvideo: systematic framework for large video generative models, 2025. 3 [28] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19311941, 2023. 3, 9 [29] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 2, 3, 9 [30] Black Forest Labs. https : / / huggingface . co / black - forest - labs / FLUX . 1-Redux-dev, n.d. 6, 9, 11 FLUX.1-Redux-dev. [31] Duong Le, Tuan Pham, Sangho Lee, Christopher Clark, Aniruddha Kembhavi, Stephan Mandt, Ranjay Krishna, and Jiasen Lu. One diffusion to generate them all. arXiv preprint arXiv:2411.16318, 2024. 3 [32] Dongxu Li, Junnan Li, and Steven Hoi. Blip-diffusion: Pretrained subject representation for controllable text-to-image generation and editing. Advances in Neural Information Processing Systems, 36, 2024. [33] Ming Li, Taojiannan Yang, Huafeng Kuang, Jie Wu, Zhaoning Wang, Xuefeng Xiao, and Chen Chen. Controlnet++: Improving conditional controls with efficient consistency In European Conference on Computer Vision, feedback. pages 129147. Springer, 2025. 2, 3 [34] Shanglin Li, Bohan Zeng, Yutang Feng, Sicheng Gao, Xiuhui Liu, Jiaming Liu, Lin Li, Xu Tang, Yao Hu, Jianzhuang Liu, et al. Zone: Zero-shot instruction-guided local editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 62546263, 2024. 3 [35] Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, Ming-Ming Cheng, and Ying Shan. Photomaker: Customizing realistic human photos via stacked id embedding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 86408650, 2024. 6, 11 [36] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 300309, 2023. 3 [37] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence In Zitnick. Microsoft coco: Common objects in context. Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pages 740755. Springer, 2014. 9 [38] linoyts. yarn-art-Flux-LoRA. https://huggingface. co/linoyts/yarn_art_Flux_LoRA, n.d. 7 [39] Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang, Chao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Jiayuan Gu, and Hao Su. One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10072 10083, 2024. 3 [40] Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu. Subjectdiffusion: Open domain personalized text-to-image generation without test-time fine-tuning. In ACM SIGGRAPH 2024 Conference Papers, pages 112, 2024. 3 [41] Sicheng Mo, Fangzhou Mu, Kuan Heng Lin, Yanli Liu, Freecontrol: Bochen Guan, Yin Li, and Bolei Zhou. Training-free spatial control of any text-to-image diffusion model with any condition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 74657475, 2024. 3 [42] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real imIn Proceedings of ages using guided diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 60386047, 2023. 3 [43] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI conference on artificial intelligence, pages 42964304, 2024. 2, [44] Haifeng Ni. Itvton: Virtual try-on diffusion transformer model based on integrated image and text. arXiv preprint arXiv:2501.16757, 2025. 2 [45] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-toimage translation. In ACM SIGGRAPH 2023 conference proceedings, pages 111, 2023. 3 [46] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4195 4205, 2023. 2, 3, 9 [47] Single-subject Personalization. Ms-diffusion: Multi-subject zero-shot image personalization with layout guidance. 3 [48] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 2, 3 [49] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. [50] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123: One image to high-quality 3d object generation using both 2d and 3d diffusion priors. arXiv preprint arXiv:2306.17843, 2023. 3 [51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 87488763. PMLR, 2021. 10 [52] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 8821 8831. Pmlr, 2021. 3 [53] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, 3 [54] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF Conference generation. on Computer Vision and Pattern Recognition, pages 22500 22510, 2023. [55] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 3 [56] Shakker-Labs. FLUX.1-dev-LoRA-Text-Poster. https: / / huggingface . co / Shakker - Labs / FLUX . 1 - dev-LoRA-Text-Poster, . 7 [57] Shakker-Labs. FLUX.1-dev-LoRA-Vector-Journey. https : / / huggingface . co / Shakker - Labs / FLUX . 1 - dev - LoRA - Vector - Journey, . FLUX.1-dev-LoRA-Children-Simple- [58] Shakker-Labs. https : / / huggingface . co / Shakker - Sketch. Labs / FLUX . 1 - dev - LoRA - Children - Simple - Sketch, . 7 [59] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512, 2023. [60] Yichun Shi, Peng Wang, and Weilin Huang. Seededit: Align image re-generation to image editing. arXiv preprint arXiv:2411.06686, 2024. 3 [61] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 3 [62] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 3 and Stefano Ermon. arXiv preprint [63] Yiren Song, Shijie Huang, Chen Yao, Xiaojun Ye, Hai Ci, Jiaming Liu, Yuxuan Zhang, and Mike Zheng Shou. Processpainter: Learn painting process from sequence data. arXiv preprint arXiv:2406.06062, 2024. 3 [64] Yiren Song, Shengtao Lou, Xiaokang Liu, Hai Ci, Pei Yang, Jiaming Liu, and Mike Zheng Shou. Anti-reference: Universal and immediate defense against reference-based generation. arXiv preprint arXiv:2412.05980, 2024. 3 [65] Yiren Song, Pei Yang, Hai Ci, and Mike Zheng Shou. to protect arXiv preprint Idprotector: An adversarial noise encoder against arXiv:2412.11638, 2024. 3 id-preserving image generation. [66] Yiren Song, Danze Chen, and Mike Zheng Shou. Layertracer: Cognitive-aligned layered svg synthesis via diffusion transformer. arXiv preprint arXiv:2502.01105, 2025. 3 [67] Yiren Song, Cheng Liu, and Mike Zheng Shou. Makeanyfor multiHarnessing diffusion transformers arXiv preprint thing: domain procedural sequence generation. arXiv:2502.01572, 2025. 3 [68] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 9 [69] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. arXiv preprint arXiv:2411.15098, 3, 2024. 2, 3, 6, 9 [70] Cong Wan, Xiangyang Luo, Zijian Cai, Yiren Song, Yunlong Zhao, Yifan Bai, Yuhang He, and Yihong Gong. Grid: Visual layout generation. arXiv preprint arXiv:2412.10718, 2024. 3 [71] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, and Anthony Chen. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024. 6, 11 [72] Rui Wang, Hailong Guo, Jiaming Liu, Huaxia Li, Haibo Zhao, Xu Tang, Yao Hu, Hao Tang, and Peipei Li. Stablegarment: Garment-centric generation via stable diffusion. arXiv preprint arXiv:2403.10783, 2024. 3 [73] Bin Xia, Yuechen Zhang, Jingyao Li, Chengyao Wang, Yitong Wang, Xinglong Wu, Bei Yu, and Jiaya Jia. Dreamomni: Unified image generation and editing. arXiv preprint arXiv:2412.17098, 2024. 2, [74] Guangxuan Xiao, Tianwei Yin, William Freeman, Fredo Fastcomposer: Tuning-free Durand, and Song Han. multi-subject image generation with localized attention. International Journal of Computer Vision, pages 120, 2024. 3 [75] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, and arXiv Zheng Liu. Omnigen: Unified image generation. preprint arXiv:2409.11340, 2024. 2, 3 [76] Ming Xie, Chenjie Cao, Yunuo Cai, Xiangyang Xue, and Yu-Gang Jiang. Anyrefill: unified, data-efficient framework for left-prompt-guided vision tasks. arXiv preprint arXiv:2502.11158, 2025. 2, 3 [77] XLabs-AI. flux-ip-adapter. https://huggingface. co/XLabs-AI/flux-ip-adapter-v2. 9 [78] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and Yujiu Yang. Maniqa: Multi-dimension attention network for no-reference image quality assessment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11911200, 2022. 10, [79] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 3 [80] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 2, 3, 6, 9, 11 [81] Hong Zhang, Zhongjie Duan, Xingjun Wang, Yingda Chen, and Yu Zhang. Eligen: Entity-level controlled image generation with regional attention. arXiv preprint arXiv:2501.01097, 2025. 2 [82] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instructionguided image editing. Advances in Neural Information Processing Systems, 36:3142831449, 2023. 3 [83] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. 2, 3, 6, 9 [84] Yuxuan Zhang, Yiren Song, Jiaming Liu, Rui Wang, Jinpeng Yu, Hao Tang, Huaxia Li, Xu Tang, Yao Hu, Han Pan, and Zhongliang Jing. Ssr-encoder: Encoding selective subject representation for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 80698078, 2024. 3 [85] Yuxuan Zhang, Yiren Song, Jinpeng Yu, Han Pan, and Zhongliang Jing. Fast personalized text to image synthesis with attention injection. In ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 61956199, 2024. [86] Yuxuan Zhang, Lifu Wei, Qing Zhang, Yiren Song, Jiaming Liu, Huaxia Li, Xu Tang, Yao Hu, and Haibo Zhao. Stablemakeup: When real-world makeup transfer meets diffusion model. arXiv preprint arXiv:2403.07764, 2024. 3 [87] Yuxuan Zhang, Qing Zhang, Yiren Song, Jichao Zhang, Hao Tang, and Jiaming Liu. Stable-hair: Real-world hair transfer via diffusion model. arXiv preprint arXiv:2407.14078, 2024. 3 [88] Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and Kwan-Yee Wong. Uni-controlnet: All-in-one control to text-to-image diffusion models. Advances in Neural Information Processing Systems, 36, 2024. 2, 3, 6, 9, 11 [89] Yinglin Zheng, Hao Yang, Ting Zhang, Jianmin Bao, Dongdong Chen, Yangyu Huang, Lu Yuan, Dong Chen, Ming Zeng, and Fang Wen. General facial representation learnIn Proceedings of the ing in visual-linguistic manner. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1869718709, 2022. 9,"
        }
    ],
    "affiliations": [
        "Liblib AI",
        "National University of Singapore",
        "ShanghaiTech University",
        "Tiamat AI"
    ]
}