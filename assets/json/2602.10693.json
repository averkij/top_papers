{
    "paper_title": "VESPO: Variational Sequence-Level Soft Policy Optimization for Stable Off-Policy LLM Training",
    "authors": [
        "Guobin Shen",
        "Chenxiao Zhao",
        "Xiang Cheng",
        "Lei Huang",
        "Xing Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Training stability remains a central challenge in reinforcement learning (RL) for large language models (LLMs). Policy staleness, asynchronous training, and mismatches between training and inference engines all cause the behavior policy to diverge from the current policy, risking training collapse. Importance sampling provides a principled correction for this distribution shift but suffers from high variance; existing remedies such as token-level clipping and sequence-level normalization lack a unified theoretical foundation. We propose Variational sEquence-level Soft Policy Optimization (VESPO). By incorporating variance reduction into a variational formulation over proposal distributions, VESPO derives a closed-form reshaping kernel that operates directly on sequence-level importance weights without length normalization. Experiments on mathematical reasoning benchmarks show that VESPO maintains stable training under staleness ratios up to 64x and fully asynchronous execution, and delivers consistent gains across both dense and Mixture-of-Experts models. Code is available at https://github.com/FloyedShen/VESPO"
        },
        {
            "title": "Start",
            "content": "VESPO: Variational Sequence-Level Soft Policy Optimization for Stable Off-Policy LLM Training Guobin Shen 1 Chenxiao Zhao 1 Xiang Cheng 1 Lei Huang 1 Xing Yu 1 6 2 0 2 1 1 ] . [ 1 3 9 6 0 1 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Training stability remains central challenge in reinforcement learning (RL) for large language models (LLMs). Policy staleness, asynchronous training, and mismatches between training and inference engines all cause the behavior policy to diverge from the current policy, risking training collapse. Importance sampling provides principled correction for this distribution shift but suffers from high variance; existing remedies such as token-level clipping and sequence-level normalization lack unified theoretical foundation. We propose Variational sEquence-level Soft Policy Optimization (VESPO). By incorporating variance reduction into variational formulation over proposal distributions, VESPO derives closedform reshaping kernel that operates directly on sequence-level importance weights without length normalization. Experiments on mathematical reasoning benchmarks show that VESPO maintains stable training under staleness ratios up to 64 and fully asynchronous execution, and delivers consistent gains across both dense and Mixtureof-Experts models. Code is available at https: //github.com/FloyedShen/VESPO. 1. Introduction Reinforcement learning (RL) has become key technique for tackling complex problem-solving tasks with large language models (LLMs), enabling capabilities such as multistep mathematical reasoning and code generation (OpenAI, 2024; Anthropic, 2025; DeepSeek-AI et al., 2025; Yang et al., 2025). In practice, off-policy updates arise naturally in RL pipelines for LLMs. common source is that systems split large rollout batches into mini-batches for sequential updates (Yang 1Xiaohongshu Inc. Shen <floyed shen@outlook.com>, shan2@xiaohongshu.com>. Preprint. February 12, 2026. Correspondence to: Guobin Xing Yu <yuanFigure 1. Left: VESPO reformulates IS weight reshaping as finding proposal that balances proximity to µ and π under variance constraint. Right: Training reward (gbs/mbs=4) on Qwen3-30B-A3B-Base. et al., 2025), causing later batches to become stale relative to the evolving policy. Asynchronous systems (Fu et al., 2025; Zhong et al., 2025; Noukhovitch et al., 2025) amplify this by decoupling rollout from training entirely. Traininginference mismatches introduce further discrepancies, especially in MoE models where routing decisions compound through layers. To stabilize training under such distribution mismatch, existing works adopt truncated importance sampling (TIS) (Liu et al., 2025), mask out off-policy samples from training (Hu, 2025), or replay expert routing for target policy inference (Zheng et al., 2025b; Ma et al., 2025). While PPO (Schulman et al., 2017) enforces trust region constraints via clipping, it does not directly address the variance challenge of sequence-level importance sampling (IS). Existing methods address this challenge through various importance weight transformations. Most operate at the token level: GRPO (Shao et al., 2024; DeepSeek-AI et al., 2025) applies PPO-style clipping to per-token ratios; other methods such as SAPO (Gao et al., 2025) design heuristic transformations for off-policy importance weights (Xi et al., 2025; Zheng et al., 2025c; Dwyer et al., 2025; Roux et al., 2025; Hu et al., 2025). However, token-level transformations are compromise to avoid the multiplicative variance explosion of sequence-level weights, and have been shown to be merely first-order approximation to their sequencelevel counterparts (Zheng et al., 2025b). Sequence-level approaches (Zheng et al., 2025b; Zhao et al., 2025) introduce length normalization to control variance, but this normal1 VESPO: Variational Sequence-Level Soft Policy Optimization ization makes the IS estimator biased; hard clipping is still required on top. Despite these efforts, principled guidance for designing importance weight transformations remains limited. We propose Variational sEquence-level Soft Policy Optimization (VESPO), which takes fundamentally different approach: rather than designing reshaping heuristics, we explicitly incorporate variance reduction for off-policy importance sampling into variational formulation, yielding principled closed-form solution (Figure 1). Additionally, we find that the resulting transformation is particularly friendly to sequence-level optimization: unlike previous methods that rely on length normalization to avoid variance explosion, VESPO operates directly on sequence-level importance weights without approximation or normalization. Our contributions are summarized as follows: We explicitly incorporate variance reduction into variational formulation for importance weight reshaping and derive principled closed-form solution. The resulting algorithm, VESPO, operates directly on sequence-level importance weights, preserving intertoken dependencies while remaining free from lengthdependent bias. Experiments on mathematical reasoning benchmarks demonstrate that VESPO remains stable under staleness ratios up to 64 and in fully asynchronous training, delivering consistent improvements across both dense and MoE architectures. 2. Preliminaries Notation. We consider an autoregressive language model parameterized by θ as policy πθ. Let denote query sampled from dataset D. In off-policy settings, responses = (y1, . . . , yT ) are sampled from behavior policy µ (e.g., an earlier checkpoint or different inference engine). The likelihood of generating given factorizes as: πθ(y x) = (cid:89) t=1 πθ(yt x, y<t). (1) We write τ = (x, y) for query-response pair, and R(τ ) for the sequence-level reward assigned to the complete response. Policy Gradient with Off-Policy Correction. The goal is to maximize the expected reward under the current policy: (θ) = Eτ πθ (cid:2)R(τ )(cid:3). (2) When samples are drawn from behavior policy µ instead, importance sampling provides an unbiased correction. Taking the gradient yields the off-policy policy gradient: θJ (θ) = Eτ µ [W (τ ) R(τ ) θ log πθ(τ )] , (3) where (τ ) = πθ(τ )/µ(τ ) is the importance weight. This classical policy gradient view (Sutton et al., 1999) reveals that the importance weight serves as gradient weighting factor: it determines how much each sample contributes to the parameter update. More generally, any modification to the importance weight can be understood as defining reshaping function ϕ(W ) that reweights the gradient: θ (θ) = Eτ µ [ϕ(W (τ )) R(τ ) θ log πθ(τ )] . (4) This gradient-centric view will be central to our analysis: the practical effect of any weight transformation must ultimately be understood through how it reweights the policy gradient. The Variance Challenge of Sequence-Level IS. Expanding the importance weight in terms of token-level ratios reveals fundamental structural tension. Define the token-level importance ratio as ρt = πθ(ytx,y<t) µ(ytx,y<t) . The sequence-level weight is then product: (τ ) = (cid:89) t=1 ρt, (5) while the log-policy gradient is sum: θ log πθ(τ ) = (cid:88) t=1 θ log πθ(yt x, y<t). (6) This product-sum structure creates tension: the gradient contribution of each token is weighted by global factor (τ ) that compounds across all positions. Even small per-token deviations accumulate multiplicatively, causing (τ ) to exhibit extreme values for long sequences. The variance of grows exponentially with , rendering naive importance sampling impractical. To tame this variance, existing methods define specific reshaping functions ϕ that modify the gradient weighting. GRPO (Shao et al., 2024) operates at the token level with PPO-style clipped surrogate. From the gradient perspective, the effective weight function depends on the sign of the advantage A: ϕGRPO(ρt; A) = ρt, ρt, 0, if > 0 and ρt 1+ε, if < 0 and ρt 1ε, otherwise (gradient zeroed). (7) This breaks the product structure and treats each token update independently, yielding only first-order approximation (Zheng et al., 2025a). GSPO (Zheng et al., 2025b) operates at the sequence level, defining the gradient weight as the geometric mean of tokenlevel ratios (i.e., normalizing by sequence length): ϕGSPO(W ) = (cid:33)1/T (cid:32) (cid:89) t=1 ρt = exp (cid:32) 1 (cid:88) t=1 (cid:33) log ρt , (8) 2 VESPO: Variational Sequence-Level Soft Policy Optimization followed by clipping mechanism similar to GRPO. This normalization introduces length-dependent bias: the implicit proposal distribution varies with , and sequences with identical per-token statistics but different lengths receive identical weights despite having different true importance weights (see Section for formal analysis). These approaches all define ϕ heuristically; the question of what constitutes principled choice of ϕ motivates the variational framework we develop next. 3. VESPO: Variational Sequence-Level Soft"
        },
        {
            "title": "Policy Optimization",
            "content": "We develop principled framework for designing importance weight transformations. We first show that any reshaping function ϕ(W ) implicitly defines proposal distribution, then formulate the design of ϕ as variational problem with variance constraints, and finally derive closed-form solution. 3.1. Weight Reshaping as Measure Change Standard importance sampling performs an unbiased measure change µ π, while any transformation ϕ(W ) induces different measure change µ for some implicit proposal Q. We now formalize this perspective. For any function G(τ ): Eτ µ (cid:2)ϕ(W (τ )) G(τ )(cid:3) = Eτ (cid:2)G(τ )(cid:3), (9) where = Eµ[ϕ(W )] is normalization constant, and is defined by Q(τ ) = 1 µ(τ ) ϕ(W (τ )). (10) The reshaped gradient (Equation (4)) is thus equivalent, up to the scalar Z, to an on-policy gradient under the proposal distribution Q. This is the key insight: any sequence-level reshaping function ϕ(W ) implicitly defines proposal distribution Q. This perspective provides unified lens to analyze existing importance weight transformations (see Section for detailed analysis of specific methods). Rather than handcrafting ϕ directly, we can specify desirable properties of the proposal and derive the corresponding ϕ. good proposal should remain close to µ for sampling efficiency, incorporate π to guide optimization, and control variance. The following subsections formalize these desiderata as variational objective and derive closed-form ϕ. 3.2. Variational Objective that encourages to lie between µ and π in the space of distributions: (Q) = (1 α) DKL(Qµ) + α DKL(Qπ), (11) where α controls the trade-off. The first term penalizes deviation from the sampling distribution, ensuring that samples from µ remain informative for estimating expectations under Q. The second term penalizes deviation from the target policy, reducing bias in the gradient estimate. Variance Constraint. Proximity alone is insufficient: finite sample sizes demand variance control. In importance sampling, the variance of the estimator scales with the second moment Eµ[W 2], classical result that also underlies the effective sample size (ESS) diagnostic. Under the measure-change view, this second moment can be related to an expectation under Q. By Equation (10), we have Q(τ ) µ(τ )ϕ(W (τ )), so Eτ Q[W (τ )] Eτ µ[ϕ(W ) ]. (12) When ϕ(W ) (approaching unbiased IS), this recovers Eµ[W 2]. Bounding EQ[W ] therefore provides principled way to control the variance induced by the measure change: Eτ Q[W (τ )] C. (13) Constrained Optimization. Combining the dual-proximity objective with the variance constraint: (1 α) DKL(Qµ) + α DKL(Qπ) min s.t. EQ[W ] C, (cid:82) = 1. (14) Introducing Lagrange multipliers λ 0 for the moment constraint and γ for normalization, we obtain the Lagrangian: L(Q, λ, γ) = (1α)DKL(Qµ) + αDKL(Qπ) + λ(cid:0)EQ[W ]C(cid:1) + γ(cid:0)(cid:82) Q1(cid:1). (15) 3.3. Closed-Form Solution Taking the functional derivative δL tion B): δQ = 0 yields (see SecQ(τ ) µ(τ )1α π(τ )α exp(λW (τ )). (16) Using = π/µ, this simplifies to Q(τ ) µ(τ ) (τ )α exp(λW (τ )). Comparing with Equation (10), we identify the reshaping function: ϕ(W ) = α exp(λW ). (17) Dual Proximity. We seek proposal that remains close to µ for sample efficiency, while also approaching π to reduce estimation bias. We formalize this via mixed KL objective This kernel has two components: the power term α and the exponential term exp(λW ) for soft suppression. It is 3 VESPO: Variational Sequence-Level Soft Policy Optimization 3.4. The VESPO Algorithm In practice, we use the shifted form ϕ(W ) = c1 exp(c2(1 )) to ensure ϕ(1) = 1, so that on-policy samples receive unit weight and the update magnitude aligns with standard on-policy methods under the same learning rate. We adopt different hyperparameters (c1, c2) for positive and negative advantages, mirroring the asymmetric clipping in PPO. Recent work (Tang et al., 2025) has shown that positive and negative samples exhibit different gradient dynamics during training; accordingly, we apply stronger suppression for < 0 with < 1 to prevent excessive penalization of samples the policy already dislikes, as shown in Figure 2. We treat (c1, c2) as tunable hyperparameters, which allows flexibility beyond the specific values implied by the variational derivation. Substituting the shifted kernel into Equation (4), the VESPO gradient estimator becomes: JVESPO = Eτ µ (cid:2)W c1 exp(c2(1W )) A(τ ) log πθ(τ )(cid:3), (21) where A(τ ) = R(τ ) is the advantage with baseline (computed as the mean reward within each prompt group, following GRPO (Shao et al., 2024)). Connection to Existing Methods. The kernel exhibits adaptive behavior: when 1, ϕ(W ) 1 and gradients are preserved; when 1, the exponential term decays rapidly; when 1, the power term naturally down-weights unlikely samples. Compared to hard clipping, VESPO provides smooth approximation that gradually attenuates extreme weights (Figure 2). Compared to existing soft transformations that operate at the token level, VESPO applies directly to sequence-level importance weights without length normalization, and the factorized form offers flexible control: c1 primarily governs behavior for < 1, while c2 controls the decay for > 1. Implementation. We implement VESPO in REINFORCE the reshaping weight ϕ(W ) is detached from the style: computation graph and serves purely as gradient scaling coefficient. The sequence-level importance weight is computed in log-space for numerical stability: Figure 2. Surrogate objectives (w) (top) and gradient scaling factors ϕ(w) = (w) (bottom) for positive and negative advantages. Hard clipping zeros ϕ abruptly at the boundary; VESPO peaks near w=1 and decays smoothly. smooth and differentiable everywhere, avoiding the discontinuities of hard clipping. Surrogate Objective Interpretation. Recall that our goal is to maximize the expected reward (θ) = Eτ πθ [R(τ )] (Equation (2)). With off-policy samples from µ, the reshaped gradient (Equation (4)) can be viewed as optimizing surrogate objective. Observe that θW = θ log πθ, so the gradient estimator can be rewritten as Eτ µ[ϕ(W )A log πθ] = Eτ µ (cid:2) ϕ(W ) AW (cid:3). (18) Defining (W ) such that (W ) = ϕ(W )/W , the righthand side equals θEτ µ[f (W ) A], since A(τ ) does not depend on θ. Thus VESPO implicitly maximizes surrogate objective: JVESPO(θ) = Eτ µ (cid:2)f (W (θ)) A(τ )(cid:3). (19) For ϕ(W ) = α exp(λW ), we have (W ) = α1 exp(λW ), which integrates to the lower incomplete gamma function: log = (cid:88) (cid:0)log πθ(ytx, y<t) log µ(ytx, y<t)(cid:1). (22) t=1 (W ) = 1 λα γ(α, λW ), where γ(a, x) = (cid:90) ta1et dt. 0 (20) This form is smooth and infinitely differentiable, and saturates as , providing principled soft alternative to hard clipping. The reshaping c1 log + c2(1 ) is also computed in log-space; exponentiation is performed only at the final step, avoiding overflow from extreme importance weights. This requires storing only the per-token log-probabilities under both π and µ, with no additional memory overhead. The complete pseudocode is provided in Section E. 4 VESPO: Variational Sequence-Level Soft Policy Optimization 4. Experiments We evaluate VESPO on mathematical reasoning benchmarks, focusing on practical sources of off-policy distribution shift: (1) policy staleness from batched rollouts, where later mini-batches are updated using samples from an outdated policy, including fully asynchronous setting where rollout and training run on separate nodes; and (2) train-inference mismatch, where different implementations between training and inference engines produce different outputs for the same inputan effect exacerbated in MoE models due to routing inconsistencies. quential policy gradient updates. We measure progress in policy update steps to ensure all methods process the same amount of data. Our primary experiments use = 8, with ablations exploring {4, 8, 16, 32, 64}. We additionally evaluate under fully asynchronous setting where rollout and training run on separate node groups. The trainer synchronizes updated weights to the rollout engine every 4 local updates (analogous to gbs/mbs=4 in the synchronous setting), and in-flight rollouts generated under stale parameters are preserved across synchronization boundaries rather than discarded (Zhou et al., 2025). 4.1. Experimental Setup 4.2. Main Results Training. All experiments are conducted on 32 NVIDIA H20 GPUs using the veRL framework (Sheng et al., 2024), with vLLM 0.11.0 (Kwon et al., 2023) for inference and FSDP (Zhao et al., 2023) for training dense models (Megatron (Shoeybi et al., 2019) for MoE). The training dataset is the unfiltered version of DAPO-Math (Yu et al., 2025), which provides sufficient data to support extended training. For each query, we sample 8 responses and compute rewards using Math-Verify (Hugging Face, 2025). We set the maximum context length to 16,384 tokens for both training and evaluation. We train for 1,500 gradient steps across all methods. See Table 3 for more details. Models. We evaluate on three model scales: Llama-3.23B-Instruct (Grattafiori et al., 2024), Qwen3-8B-Base, and Qwen3-30B-A3B-Base (Yang et al., 2025). The MoE architecture amplifies train-inference mismatch due to routing decisions that compound across layers. Evaluation. We evaluate on four mathematical reasoning benchmarks: AIME 2024, AIME 2025, AMC 2023, and MATH-500 (Hendrycks et al., 2021). We report avg@k accuracy (average over sampled responses per problem), with = 32 for AIME 2024/2025, = 16 for AMC 2023, and = 4 for MATH-500. We select the best checkpoint based on the average performance across all four benchmarks. Baselines. We compare against three representative methods: GRPO (Shao et al., 2024), which applies PPO-style clipping at the token level with clip ratios (0.2, 0.28) following DAPO (Yu et al., 2025); GSPO (Zheng et al., 2025b), which operates at the sequence level with 1/T length normalization and clip ratios (3e-4, 4e-4); and SAPO (Gao et al., 2025), which uses soft adaptive gating with τpos = 1.0 and τneg = 1.05. For VESPO, we use (c1, c2) = (2.0, 3.0) for > 0 and (c1, c2) = (3.0, 2.0) for < 0. To simulate policy staleness, we fix the mini-batch size (mbs) at 256 and vary the global batch size (gbs) to control the degree of off-policy updates. With gbs/mbs = , each rollout of gbs samples is split into mini-batches for seTable 1. Mathematical reasoning accuracy (%) with gbs/mbs = 8. We report avg@32 for AIME24/25, avg@16 for AMC23, and avg@4 for MATH500. Best results per model in bold. Model Method AIME25 AIME24 AMC23 MATH500 Avg Llama3.2-3BInstruct Qwen38B-Base Qwen330B-A3BBase 0.5 GRPO 0.2 GSPO 0.7 SAPO VESPO 0. 27.4 GRPO 28.8 GSPO 36.7 SAPO VESPO 33.5 28.2 GRPO 24.6 GSPO SAPO 21.4 VESPO 34.2 14.5 14.7 12.2 13.9 40.0 37.7 49.0 49.4 40.0 34.1 27.9 44.1 40.6 43.9 34.1 47. 74.5 80.8 80.0 82.2 81.4 80.5 73.0 80.3 43.8 41.4 41.3 45.3 68.6 70.4 70.3 71.1 69.9 68.8 68.7 70.2 24.8 25.1 22.1 26. 52.6 54.4 59.0 59.0 54.9 52.0 47.7 57.2 Table 1 presents the main results with gbs/mbs = 8 across three model scales. VESPO achieves the best average accuracy on all three models, demonstrating the generality of our approach. Notably, the improvements are most pronounced on Qwen3-30B-A3B-Base, where VESPO outperforms the best baseline by 2.3% average accuracy. This suggests that VESPOs soft suppression of extreme importance weights is particularly beneficial for MoE architectures, where routing inconsistencies amplify distribution shift and make training stability more challenging. Given these observations, we focus our detailed analysis on Qwen3-30B-A3B-Base in the following sections. 4.3. Robustness to Policy Staleness We examine robustness to policy staleness by varying the staleness ratio = gbs/mbs from 4 to 64. Figure 3 provides direct comparison of training stability across methods. VESPO exhibits remarkable consistency: all five curves (N =4 to =64) follow nearly identical trajectories, converging to similar final rewards around 0.7, with minimal sensitivity to policy staleness. In contrast, GRPO saturates early at suboptimal reward levels, and convergence VESPO: Variational Sequence-Level Soft Policy Optimization Table 2. Effect of staleness ratio (gbs/mbs) on Qwen3-30BA3B-Base. We report avg@k (%). VESPO maintains strong performance across all values. Method AIME25 AIME24 AMC23 MATH500 Avg 4 8 32 64 GRPO GSPO SAPO VESPO GRPO GSPO SAPO VESPO GRPO GSPO SAPO VESPO GRPO GSPO SAPO VESPO GRPO GSPO SAPO VESPO 22.1 27.6 38.4 43.1 28.2 25.1 21.4 44.3 20.3 24.3 19.6 40.2 21.8 18.8 12.4 37.7 14.6 15.4 3.3 34. 33.1 43.3 51.4 60.3 40.0 43.3 27.9 59.6 31.4 41.6 26.0 53.2 33.4 27.3 7.2 51.4 28.0 24.1 7.3 46.2 76.4 83.1 85.2 91. 81.4 83.0 73.0 91.4 71.4 83.1 72.2 90.8 73.4 73.4 29.5 85.2 69.4 73.9 23.8 83.6 67.7 70.4 71.2 71.1 69.9 69.6 68.7 72. 67.9 71.4 68.0 71.5 68.3 70.6 30.2 71.3 66.8 70.0 39.4 69.9 49.8 56.1 61.5 66.4 54.9 55.3 47.7 66.9 47.7 55.1 46.5 63. 49.2 47.5 19.8 61.4 44.7 45.8 18.4 58.5 slows as increases. GSPO shows clear correlation between staleness and final performance: larger leads to lower converged rewards. At =4, GSPO exhibits catastrophic collapse around step 1,200, with reward dropping to zero. SAPO displays unstable training at =4 and complete collapse at 8, pattern we analyze further below in connection with Figure 2. These differences in training dynamics translate directly to downstream performance, as shown in Table 2: VESPO achieves the best average accuracy at every gbs/mbs setting and maintains strong results even at =64, while GRPO and GSPO degrade to 44.7% and 45.8% respectively, and SAPO collapses to baseline level (18.4%). Training dynamics analysis. Figure 4 provides comprehensive view of training dynamics across six metrics (note: response length y-axis scales differ across rows), revealing distinct patterns for each method (see Section for additional visualizations). GRPO saturates early at suboptimal reward levels across all values. The entropy column reveals rapid entropy drop, limiting exploration before discovering high-reward behaviors. GSPO exhibits length bias amplification. At =4, response lengths peak to nearly 3,000 tokens before the catastrophic collapse around step 1,200. The 1/T normalization makes longer sequences less likely to be clipped (Section C); combined with token-sum loss aggregation, this creates feedback loop biasing the model toward longer outputs. Figure 3. Training reward across staleness levels (N {4, 8, 16, 32, 64}) on Qwen3-30B-A3B-Base. Each panel shows one method with different values. VESPO maintains stable, consistent training curves across all staleness levels. SAPO shows rapid response length growth in early training, reaching up to 15k tokens at =4. As shown in Figure 2, SAPOs design lacks sufficient down-weighting for negative-advantage samples with ρt<1, leading to excessive penalization. This aligns with recent observations that negative-advantage samples can cause length explosion and training collapse (Tang et al., 2025). VESPO demonstrates consistent stability across all metrics. Benchmark scores show steady improvement throughout training; entropy remains at stable, relatively high level; response lengths stay around 56k tokens, indicating room for further scaling; and PG loss remains remarkably stable across all values. Fully asynchronous training. The experiments above simulate staleness within synchronous framework by varying . We further evaluate under the fully asynchronous setting described in Section 4.1, where the behavior policy can lag behind the training policy by multiple gradient updates. Figure 5 shows training dynamics under this regime. SAPO collapses early, consistent with its synchronous-setting instability. GRPO exhibits highly unstable training: rollout log-perplexity escalates above 2.0, PG loss and gradient norm show frequent large spikes, and response length oscillates sharply in tandem, indicating that stale rollouts severely disrupt gradient estimation. GSPO remains stable but converges to lower training reward and accuracy. VESPO maintains stable training throughout: rollout KL, log-perplexity, PG loss, and gradient norm all remain near zero with minimal variance, while achieving the highest training reward and strongest AIME25 and AIME24 accuracy, confirming its robustness under realistic asynchronous training. VESPO: Variational Sequence-Level Soft Policy Optimization Figure 4. Training dynamics across staleness levels (N = gbs/mbs {4, 8, 16, 32, 64}) on Qwen3-30B-A3B-Base. Each row corresponds to different ; columns show training reward, AIME25 accuracy, response length, KL divergence, entropy, and PG loss. VESPO (red) maintains stable training across all conditions, while baselines exhibit characteristic failure modes. 4.4. Robustness to Train-Inference Mismatch Beyond policy staleness, second source of off-policy shift arises from train-inference mismatch: different numerical implementations between training (e.g., FSDP/Megatron) and inference (e.g., vLLM/SGLang (Zheng et al., 2024)) engines produce slightly different outputs for the same input. This effect is amplified in MoE models, where routing decisions compound across layers and small numerical differences can lead to entirely different expert assignments. Two engineering techniques have been proposed to stabilize training under such mismatch: Truncated Importance Sampling (TIS) (Liu et al., 2025) zeros token-level gradients when the importance ratio indicates significant divergence; Routing Replay (R2) (Ma et al., 2025) records router assignments during log-probability recomputation with the training engine, ensuring consistent expert selection throughout training. Figure 6 compares training stability on Qwen3-30B-A3BBase. Vanilla GRPO exhibits suboptimal training under mismatch conditions, with training reward plateauing around 0.60. Adding TIS or R2 improves GRPOs stability and final reward. Notably, VESPO without any specialized fixes 7 maintains stable training comparable to GRPO+R2, suggesting that the soft suppression of extreme importance weights helps tolerate the distribution shift induced by mismatch. Furthermore, VESPO can be combined with R2 for additional gains: VESPO+R2 achieves the highest training reward and best AIME25 accuracy among all variants. This demonstrates that VESPO is complementary to engineering techniques like R2 and TIS, enabling improved baseline stability and higher performance ceilings when combined. 4.5. Ablation: Length Normalization key design choice in VESPO is operating at the sequence level without length normalization. Methods like GSPO normalize importance weights by 1/T (sequence length) to reduce variance, but we hypothesize this creates length bias that destabilizes training. To test this, we compare VESPO with two normalized variants: VESPOsqrt (normalize by ) and VESPOlin (normalize by ). Figure 7 reveals striking differences in training dynamics. VESPOlin exhibits severe instability: around step 350, KL divergence spikes dramatically, followed by gradient exVESPO: Variational Sequence-Level Soft Policy Optimization Figure 5. Training dynamics under fully asynchronous training on Qwen3-30B-A3B-Base. VESPO maintains stable training and achieves the highest reward and benchmark accuracy. Figure 6. Training stability under train-inference mismatch on Qwen3-30B-A3B-Base. VESPO maintains stable training without specialized fixes; combining VESPO with R2 achieves the best performance. plosion and reward collapse. VESPOsqrt shows moderate instabilitytraining reward saturates around 0.58 and begins to slowly decline, accompanied by periodic gradient norm spikes. In contrast, VESPO without normalization maintains stable KL divergence, controlled gradient norms, and achieves the highest training reward. These results validate our analysis: length normalization causes longer sequences to dominate batch gradients (since they are less likely to be down-weighted), creating feedback loop that biases the model toward even longer outputs until training collapses. VESPOs sequence-level soft shaping without length normalization avoids this failure mode entirely. 4.6. Ablation: Asymmetric Hyperparameters As discussed in Section 3.4, VESPO adopts different hyperparameters for positive and negative advantages: c+ = (2, 3) and = (3, 2). We ablate this design by comparing against symmetric variants (Figure 8). Using (2, 3) for both positive and negative advantages provides insufficient Figure 7. Ablation on length normalization. VESPO without norT or normalization malization achieves stable training; adding causes instability and collapse. suppression for < 0, leading to training instability and reward drop. Using (3, 2) for both applies excessive suppression to positive-advantage samples, resulting in slower learning and slightly lower final performance. The asymmetric design balances these trade-offs: stronger exponential suppression (c2 = 3) for negative advantages prevents destabilizing updates, while milder suppression (c2 = 2) for positive advantages preserves learning signal. More broadly, we find that 8 Figure 8. Ablation on asymmetric hyperparameters. VESPO: Variational Sequence-Level Soft Policy Optimization VESPO is robust to moderate hyperparameter variations as long as sufficient down-weighting is applied to negativeadvantage samples with < 1. 5. Related Work Policy Gradient Methods for LLMs. PPO (Schulman et al., 2017) has been the workhorse for RLHF, using clipped surrogate objective to stabilize updates. Valuefree alternatives have gained traction for LLM fine-tuning: GRPO (Shao et al., 2024) normalizes rewards within sample groups and clips token-level ratios; GSPO (Zheng et al., 2025b) operates at the sequence level with geometric mean normalization; DAPO (Yu et al., 2025) introduces decoupled clipping and dynamic sampling; SAPO (Gao et al., 2025) replaces hard clipping with soft adaptive gating. These methods control variance via heuristic clipping or normalization. Our measure-change view reveals them as specific choices of implicit proposal distribution, and VESPO derives the weight function from variational principle rather than manual design. Importance Sampling in RL. Classical IS techniques truncation, self-normalization, weighted IStrade bias for variance reduction. In autoregressive LLMs, the product structure of sequence likelihoods amplifies variance, and token-level IS has been shown to be first-order approximation to sequence-level IS (Zheng et al., 2025a). Our perspective reframes IS reshaping as measure change, revealing that any ϕ(W ) implicitly defines proposal distribution and enabling principled variational design. Trust Region and Clipping. TRPO (Schulman et al., 2015) constrains updates via KL bound; PPO approximates this with hard clipping. When applied at the token level, clipping may conflict with sequence-level rewards. Our kernel provides soft alternative that gradually attenuates extreme weights rather than truncating abruptly. Stabilizing LLM RL Training. Recent work addresses specific instability sources through engineering solutions: routing replay (Ma et al., 2025) ensures consistent expert assignments in MoE models; truncated IS (Liu et al., 2025) detects and zeros gradients from mismatched samples. VESPO provides complementary algorithmic approach that can be combined with these techniques for further gains. 6. Conclusion We introduced measure-change perspective revealing that any importance weight transformation implicitly defines proposal distribution. This insight enables principled variational formulation that derives closed-form reshaping kernel from desirable distributional properties. The resulting algorithm, VESPO, operates on sequence-level importance weights without length normalization, preserving intertoken dependencies while avoiding the length-dependent bias present in existing methods. Experiments demonstrate stable training under staleness ratios up to 64 and in fully asynchronous training, with consistent improvements across dense and MoE architectures and particularly notable gains on MoE models where training stability is more challenging. Looking ahead, we see several promising directions: scaling to larger asynchronous clusters, extending to agentic RL settings with multi-turn interactions and tool use, and applying the framework to on-policy distillation and offline training."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. Specifically, we propose principled approach to stabilize reinforcement learning for large language models under off-policy conditions. While improved RL training methods may accelerate the development of more capable language models, we do not foresee direct negative societal impacts beyond those inherent to advancing language model capabilities in general. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Anthropic. Introducing Claude 4. anthropic.com/news/claude-4, 2025. cessed: 2025-05-22. https://www. AcDeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J.-M., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B.-L., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Chen, D., Ji, D.-L., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H., Ding, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J., Ni, J., Liang, J., Chen, J., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L., Zhang, M., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R., Zhang, R., Pan, R., Wang, R., Chen, R. J., Jin, R., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S., Li, S. S., Zhou, S., Wu, S.-K., Yun, T., Pei, T., Sun, T., Wang, T., Zeng, W., Zhao, W., Liu, W., Liang, W., Gao, W., Yu, W.-X., Zhang, W., Xiao, W., An, W., Liu, X., Wang, X., Chen, X., Nie, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yang, X., Li, X., Su, X., Lin, X., Li, X. Q., Jin, X., Shen, X.-C., Chen, X., Sun, 9 VESPO: Variational Sequence-Level Soft Policy Optimization X., Wang, X., Song, X., Zhou, X., Wang, X., Shan, X., Li, Y. K., Wang, Y. Q., Wei, Y. X., Zhang, Y., Xu, Y., Li, Y., Zhao, Y., Sun, Y., Wang, Y., Yu, Y., Zhang, Y., Shi, Y., Xiong, Y., He, Y., Piao, Y., Wang, Y., Tan, Y., Ma, Y., Liu, Y., Guo, Y., Ou, Y., Wang, Y., Gong, Y., Zou, Y.-J., He, Y., Xiong, Y., Luo, Y.-W., mei You, Y., Liu, Y., Zhou, Y., Zhu, Y. X., Huang, Y., Li, Y., Zheng, Y., Zhu, Y., Ma, Y., Tang, Y., Zha, Y., Yan, Y., Ren, Z., Ren, Z., Sha, Z., Fu, Z., Xu, Z., Xie, Z., guo Zhang, Z., Hao, Z., Ma, Z., Yan, Z., Wu, Z., Gu, Z., Zhu, Z., Liu, Z., Li, Z.-A., Xie, Z., Song, Z., Pan, Z., Huang, Z., Xu, Z., Zhang, Z., and Zhang, Z. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645:633 638, 2025. URL https://api.semanticscholar. org/CorpusID:275789950. Dwyer, M., Sobey, A., and Chapman, A. Its not you, its clipping: soft trust-region via probability smoothing for llm rl, 2025. URL https://arxiv.org/abs/ 2509.21282. Fu, W., Gao, J., Shen, X., Zhu, C., Mei, Z., He, C., Xu, S., Wei, G., Mei, J., JIASHU, W., Yang, T., Yuan, B., and Wu, Y. AREAL: large-scale asynchronous reinforcement learning system for language reasoning. 2025. URL https://openreview.net/forum? id=X9diEuva9R. Gao, C., Zheng, C., Chen, X.-H., Dang, K., Liu, S., Yu, B., Yang, A., Bai, S., Zhou, J., and Lin, J. Soft adaptive policy optimization, 2025. URL https://arxiv.org/ abs/2511.20347. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., Yang, A., Fan, A., Goyal, A., Hartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A., Rodriguez, A., Gregerson, A., Spataru, A., Roziere, B., Biron, B., Tang, B., Chern, B., Caucheteux, C., Nayak, C., Bi, C., Marra, C., McConnell, C., Keller, C., Touret, C., Wu, C., Wong, C., Ferrer, C. C., Nikolaidis, C., Allonsius, D., Song, D., Pintz, D., Livshits, D., Wyatt, D., Esiobu, D., Choudhary, D., Mahajan, D., Garcia-Olano, D., Perino, D., Hupkes, D., Lakomkin, E., AlBadawy, E., Lobanova, E., Dinan, E., Smith, E. M., Radenovic, F., Guzman, F., Zhang, F., Synnaeve, G., Lee, G., Anderson, G. L., Thattai, G., Nail, G., Mialon, G., Pang, G., Cucurell, G., Nguyen, H., Korevaar, H., Xu, H., Touvron, H., Zarov, I., Ibarra, I. A., Kloumann, I., Misra, I., Evtimov, I., Zhang, J., Copet, J., Lee, J., Geffert, J., Vranes, J., Park, J., Mahadeokar, J., Shah, J., van der Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J., Huang, J., Liu, J., Wang, J., Yu, J., Bitton, J., Spisak, J., Park, J., Rocca, J., Johnstun, J., Saxe, J., Jia, J., Alwala, K. V., Prasad, K., Upasani, K., Plawiak, K., Li, K., Heafield, K., Stone, K., El-Arini, K., Iyer, K., Malik, K., Chiu, K., Bhalla, K., Lakhotia, K., Rantala-Yeary, L., van der Maaten, L., Chen, L., Tan, L., Jenkins, L., Martin, L., Madaan, L., Malo, L., Blecher, L., Landzaat, L., de Oliveira, L., Muzzi, M., Pasupuleti, M., Singh, M., Paluri, M., Kardas, M., Tsimpoukelli, M., Oldham, M., Rita, M., Pavlova, M., Kambadur, M., Lewis, M., Si, M., Singh, M. K., Hassan, M., Goyal, N., Torabi, N., Bashlykov, N., Bogoychev, N., Chatterji, N., Zhang, N., Duchenne, O., elebi, O., Alrassy, P., Zhang, P., Li, P., Vasic, P., Weng, P., Bhargava, P., Dubal, P., Krishnan, P., Koura, P. S., Xu, P., He, Q., Dong, Q., Srinivasan, R., Ganapathy, R., Calderer, R., Cabral, R. S., Stojnic, R., Raileanu, R., Maheswari, R., Girdhar, R., Patel, R., Sauvestre, R., Polidoro, R., Sumbaly, R., Taylor, R., Silva, R., Hou, R., Wang, R., Hosseini, S., Chennabasappa, S., Singh, S., Bell, S., Kim, S. S., Edunov, S., Nie, S., Narang, S., Raparthy, S., Shen, S., Wan, S., Bhosale, S., Zhang, S., Vandenhende, S., Batra, S., Whitman, S., Sootla, S., Collot, S., Gururangan, S., Borodinsky, S., Herman, T., Fowler, T., Sheasha, T., Georgiou, T., Scialom, T., Speckbacher, T., Mihaylov, T., Xiao, T., Karn, U., Goswami, V., Gupta, V., Ramanathan, V., Kerkez, V., Gonguet, V., Do, V., Vogeti, V., Albiero, V., Petrovic, V., Chu, W., Xiong, W., Fu, W., Meers, W., Martinet, X., Wang, X., Wang, X., Tan, X. E., Xia, X., Xie, X., Jia, X., Wang, X., Goldschlag, Y., Gaur, Y., Babaei, Y., Wen, Y., Song, Y., Zhang, Y., Li, Y., Mao, Y., Coudert, Z. D., Yan, Z., Chen, Z., Papakipos, Z., Singh, A., Srivastava, A., Jain, A., Kelsey, A., Shajnfeld, A., Gangidi, A., Victoria, A., Goldstand, A., Menon, A., Sharma, A., Boesenberg, A., Baevski, A., Feinstein, A., Kallet, A., Sangani, A., Teo, A., Yunus, A., Lupu, A., Alvarado, A., Caples, A., Gu, A., Ho, A., Poulton, A., Ryan, A., Ramchandani, A., Dong, A., Franco, A., Goyal, A., Saraf, A., Chowdhury, A., Gabriel, A., Bharambe, A., Eisenman, A., Yazdan, A., James, B., Maurer, B., Leonhardi, B., Huang, B., Loyd, B., Paola, B. D., Paranjape, B., Liu, B., Wu, B., Ni, B., Hancock, B., Wasti, B., Spence, B., Stojkovic, B., Gamido, B., Montalvo, B., Parker, C., Burton, C., Mejia, C., Liu, C., Wang, C., Kim, C., Zhou, C., Hu, C., Chu, C.-H., Cai, C., Tindal, C., Feichtenhofer, C., Gao, C., Civin, D., Beaty, D., Kreymer, D., Li, D., Adkins, D., Xu, D., Testuggine, D., David, D., Parikh, D., Liskovich, D., Foss, D., Wang, D., Le, D., Holland, D., Dowling, E., Jamil, E., Montgomery, E., Presani, E., Hahn, E., Wood, E., Le, E.-T., Brinkman, E., Arcaute, E., Dunbar, E., Smothers, E., Sun, F., Kreuk, F., Tian, F., Kokkinos, F., Ozgenel, F., Caggioni, F., Kanayet, F., Seide, F., Florez, G. M., Schwarz, G., Badeer, G., Swee, G., Halpern, G., Herman, G., Sizov, G., Guangyi, Zhang, Lakshminarayanan, G., Inan, H., Shojanazeri, H., Zou, H., Wang, H., Zha, H., Habeeb, H., Rudolph, H., Suk, H., Aspegren, H., Goldman, H., Zhan, H., Damlaj, I., Molybog, I., Tufanov, I., Leontiadis, I., Veliche, I.-E., Gat, I., Weissman, J., Geboski, J., Kohli, VESPO: Variational Sequence-Level Soft Policy Optimization J., Lam, J., Asher, J., Gaya, J.-B., Marcus, J., Tang, J., Chan, J., Zhen, J., Reizenstein, J., Teboul, J., Zhong, J., Jin, J., Yang, J., Cummings, J., Carvill, J., Shepard, J., McPhie, J., Torres, J., Ginsburg, J., Wang, J., Wu, K., U, K. H., Saxena, K., Khandelwal, K., Zand, K., Matosich, K., Veeraraghavan, K., Michelena, K., Li, K., Jagadeesh, K., Huang, K., Chawla, K., Huang, K., Chen, L., Garg, L., A, L., Silva, L., Bell, L., Zhang, L., Guo, L., Yu, L., Moshkovich, L., Wehrstedt, L., Khabsa, M., Avalani, M., Bhatt, M., Mankus, M., Hasson, M., Lennie, M., Reso, M., Groshev, M., Naumov, M., Lathi, M., Keneally, M., Liu, M., Seltzer, M. L., Valko, M., Restrepo, M., Patel, M., Vyatskov, M., Samvelyan, M., Clark, M., Macey, M., Wang, M., Hermoso, M. J., Metanat, M., Rastegari, M., Bansal, M., Santhanam, N., Parks, N., White, N., Bawa, N., Singhal, N., Egebo, N., Usunier, N., Mehta, N., Laptev, N. P., Dong, N., Cheng, N., Chernoguz, O., Hart, O., Salpekar, O., Kalinli, O., Kent, P., Parekh, P., Saab, P., Balaji, P., Rittner, P., Bontrager, P., Roux, P., Dollar, P., Zvyagina, P., Ratanchandani, P., Yuvraj, P., Liang, Q., Alao, R., Rodriguez, R., Ayub, R., Murthy, R., Nayani, R., Mitra, R., Parthasarathy, R., Li, R., Hogan, R., Battey, R., Wang, R., Howes, R., Rinott, R., Mehta, S., Siby, S., Bondu, S. J., Datta, S., Chugh, S., Hunt, S., Dhillon, S., Sidorov, S., Pan, S., Mahajan, S., Verma, S., Yamamoto, S., Ramaswamy, S., Lindsay, S., Lindsay, S., Feng, S., Lin, S., Zha, S. C., Patil, S., Shankar, S., Zhang, S., Zhang, S., Wang, S., Agarwal, S., Sajuyigbe, S., Chintala, S., Max, S., Chen, S., Kehoe, S., Satterfield, S., Govindaprasad, S., Gupta, S., Deng, S., Cho, S., Virk, S., Subramanian, S., Choudhury, S., Goldman, S., Remez, T., Glaser, T., Best, T., Koehler, T., Robinson, T., Li, T., Zhang, T., Matthews, T., Chou, T., Shaked, T., Vontimitta, V., Ajayi, V., Montanez, V., Mohan, V., Kumar, V. S., Mangla, V., Ionescu, V., Poenaru, V., Mihailescu, V. T., Ivanov, V., Li, W., Wang, W., Jiang, W., Bouaziz, W., Constable, W., Tang, X., Wu, X., Wang, X., Wu, X., Gao, X., Kleinman, Y., Chen, Y., Hu, Y., Jia, Y., Qi, Y., Li, Y., Zhang, Y., Zhang, Y., Adi, Y., Nam, Y., Yu, Wang, Zhao, Y., Hao, Y., Qian, Y., Li, Y., He, Y., Rait, Z., DeVito, Z., Rosnbrick, Z., Wen, Z., Yang, Z., Zhao, Z., and Ma, Z. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Hu, J. Stabilizing moe rl without router replay: The online icepop solution. https://hijkzzz.notion. site/online-ice-pop, 12 2025. Accessed: February 12, 2026. Hu, J., Liu, J. K., Xu, H., and Shen, W. Reinforce++: 11 Stabilizing critic-free policy optimization with global advantage normalization, 2025. URL https://arxiv. org/abs/2501.03262. Hugging Face. Math-verify: rule-based math verification library. https://github.com/huggingface/ Math-Verify, 2025. Accessed: 2025-01-26. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient memory management for large language model serving In Proceedings of the 29th symwith pagedattention. posium on operating systems principles, pp. 611626, 2023. Liu, J., Li, Y., Fu, Y., Wang, J., Liu, Q., and Shen, Y. When speed kills stability: Demystifying RL collapse from the training-inference mismatch, September 2025. URL https://richardli.xyz/rl-collapse. Ma, W., Zhang, H., Zhao, L., Song, Y., Wang, Y., Sui, Z., and Luo, F. Stabilizing moe reinforcement learning by aligning training and inference routers, 2025. URL https://arxiv.org/abs/2510.11370. Noukhovitch, M., Huang, S., Xhonneux, S., Hosseini, A., Agarwal, R., and Courville, A. Asynchronous RLHF: Faster and more efficient off-policy RL for language In The Thirteenth International Conference models. on Learning Representations, 2025. URL https:// openreview.net/forum?id=FhTAG591Ve. OpenAI. LLMs. learning-to-reason-with-llms/, Accessed: 2024-09-12. Learning with https://openai.com/index/ 2024. reason to Roux, N. L., Bellemare, M. G., Lebensold, J., Bergeron, A., Greaves, J., Frechette, A., Pelletier, C., Thibodeau-Laufer, E., Toth, S., and Work, S. Tapered off-policy reinforce: Stable and efficient reinforcement learning for llms, 2025. URL https://arxiv.org/abs/2503.14286. Trust region policy optimization. Schulman, J., Levine, S., Abbeel, P., Jordan, M., and In Moritz, P. Bach, F. and Blei, D. (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 18891897, Lille, France, 0709 Jul 2015. PMLR. URL https://proceedings.mlr.press/v37/ schulman15.html. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. 2017. URL https://arxiv.org/abs/ 1707.06347. VESPO: Variational Sequence-Level Soft Policy Optimization 2025. URL https://openreview.net/forum? id=2a36EMSSTp. Zhao, Y., Gu, A., Varma, R., Luo, L., Huang, C.-C., Xu, M., Wright, L., Shojanazeri, H., Ott, M., Shleifer, S., et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023. Zhao, Y., Liu, Y., Liu, J., Chen, J., Wu, X., Hao, Y., Lv, T., Huang, S., Cui, L., Ye, Q., Wan, F., and Wei, F. Geometric-mean policy optimization, 2025. URL https://arxiv.org/abs/2507.20673. Zheng, C., Dang, K., Yu, B., Li, M., Jiang, H., Lin, J., Liu, Y., Lin, H., Wu, C., Hu, F., Yang, A., Zhou, J., and Lin, J. Stabilizing reinforcement learning with llms: Formulation and practices, 2025a. URL https: //arxiv.org/abs/2512.01374. Zheng, C., Liu, S., Li, M., Chen, X.-H., Yu, B., Gao, C., Dang, K., Liu, Y., Men, R., Yang, A., Zhou, J., and Lin, J. Group sequence policy optimization. 2025b. URL https://arxiv.org/abs/2507.18071. Zheng, H., Zhao, J., and Chen, B. Prosperity before collapse: How far can off-policy RL reach with stale data on LLMs? In NeurIPS 2025 Workshop on Efficient Reasoning, 2025c. URL https://openreview.net/ forum?id=osEqHuHQWK. Zheng, L., Yin, L., Xie, Z., Sun, C. L., Huang, J., Yu, C. H., Cao, S., Kozyrakis, C., Stoica, I., Gonzalez, J. E., et al. Sglang: Efficient execution of structured language model programs. Advances in neural information processing systems, 37:6255762583, 2024. Zhong, Y., Zhang, Z., Song, X., Hu, H., Jin, C., Wu, B., Chen, N., Chen, Y., Zhou, Y., Wan, C., Zhou, H., Jiang, Y., Zhu, Y., and Jiang, D. Streamrl: Scalable, heterogeneous, and elastic rl for llms with disaggregated stream generation. 2025. URL https://arxiv.org/abs/ 2504.15930. Zhou, Y., Li, J., Su, Y., Ramesh, G., Zhu, Z., Long, X., Zhao, C., Pan, J., Yu, X., Wang, Z., et al. April: Active partial rollouts in reinforcement learning to tame long-tail generation. arXiv preprint arXiv:2509.18521, 2025. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y. K., Wu, Y., and Guo, D. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300. Sheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. Megatron-lm: Training multibillion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. Sutton, R. S., McAllester, D., Singh, S., and Mansour, Y. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems, 12, 1999. Tang, X., Zhan, Y., Li, Z., Zhao, W. X., Zhang, Z., Wen, Z., Zhang, Z., and Zhou, J. Rethinking sample polarity in reinforcement learning with verifiable rewards, 2025. URL https://arxiv.org/abs/2512.21625. Xi, Z., Guo, X., Nan, Y., Zhou, E., Shen, J., Chen, W., Liu, J., Huang, J., Zhang, Z., Guo, H., Deng, X., Lei, Z., Zheng, M., Wang, G., Zhang, S., Sun, P., Zheng, R., Yan, H., Gui, T., Zhang, Q., and Huang, X. Bapo: Stabilizing off-policy reinforcement learning for llms via balanced policy optimization with adaptive clipping, 2025. URL https://arxiv.org/abs/2510.18927. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., Zheng, C., Liu, D., Zhou, F., Huang, F., Hu, F., Ge, H., Wei, H., Lin, H., Tang, J., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Zhou, J., Lin, J., Dang, K., Bao, K., Yang, K., Yu, L., Deng, L.-C., Li, M., Xue, M., Li, M., Zhang, P., Wang, P., Zhu, Q., Men, R., Gao, R., Liu, S.-Q., Luo, S., Li, T., Tang, T., Yin, W., Ren, X., Wang, X., Zhang, X., Ren, X., Fan, Y., Su, Y., Zhang, Y.-C., Zhang, Y., Wan, Y., Liu, Y., Wang, Z., Cui, Z., Zhang, Z., Zhou, Z., and Qiu, Z. Qwen3 technical report. ArXiv, abs/2505.09388, 2025. URL https://api.semanticscholar. org/CorpusID:278602855. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., YuYue, Dai, W., Fan, T., Liu, G., Liu, J., Liu, L., Liu, X., Lin, H., Lin, Z., Ma, B., Sheng, G., Tong, Y., Zhang, C., Zhang, M., Zhang, R., Zhang, W., Zhu, H., Zhu, J., Chen, J., Chen, J., Wang, C., Yu, H., Song, Y., Wei, X., Zhou, H., Liu, J., Ma, W.-Y., Zhang, Y.-Q., Yan, L., Wu, Y., and Wang, M. DAPO: An open-source LLM reinforcement In The Thirty-ninth Annual learning system at scale. Conference on Neural Information Processing Systems, 12 VESPO: Variational Sequence-Level Soft Policy Optimization VESPO: Variational Sequence-Level Soft Policy Optimization Supplementary Material Table of Contents Implicit Proposal Distributions of Existing Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 Derivation of the Proposal Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 Length Normalization Introduces Bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 Additional Analysis of Baseline Failure Modes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 Algorithm Pseudocode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Training Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 A. Implicit Proposal Distributions of Existing Methods We analyze the implicit proposal distributions induced by existing reshaping strategies under the measure-change framework of Section 3.1. Token-level methods (GRPO). GRPO applies clipping independently at each token position, yielding gradient of the form JGRPO = (cid:88) t=1 Eτ µ (cid:2)ϕt(ρt) A(τ ) log π(ytx, y<t)(cid:3), (23) where ϕt(ρt) is the clipping function applied to the t-th token ratio. The key observation is that different tokens within the same trajectory receive different weights ϕt(ρt). In contrast, sequence-level methods assign single weight ϕ(W ) to the entire trajectory: Jseq = Eτ µ (cid:2)ϕ(W ) A(τ ) (cid:88) t=1 log π(ytx, y<t)(cid:3). (24) The token-level formulation cannot be expressed as importance sampling toward any single proposal distribution Q, because such representation requires uniform weight across all token gradients within each trajectory. This token-wise weighting breaks the coherence of sequence-level credit assignment: tokens in the same trajectory that share common outcome receive inconsistent gradient signals. Token-level IS as First-Order Approximation. We now show that token-level IS is first-order approximation to sequence-level IS, following Zheng et al. (2025a). Consider the unclipped case where ϕt(ρt) = ρt and ϕ(W ) = . The sequence-level gradient (without advantage for clarity) is Jseq = Eτ µ (cid:104) (cid:88) t= (cid:105) log π(yt) = Eτ µ (cid:104) (cid:89) (cid:88) ρs s= t=1 (cid:105) log π(yt) . When ρt 1 for all (near on-policy), we can expand = (cid:81) ρs via Taylor series: = (cid:89) s=1 ρs 1 + (cid:88) s=1 (ρs 1) + (cid:88) s<s (ρs 1)(ρs 1) + 13 (25) (26) VESPO: Variational Sequence-Level Soft Policy Optimization Substituting into the gradient and keeping only first-order terms in (ρs 1): Jseq Eτ µ (cid:104)(cid:16) 1 + (cid:88) (cid:17) (cid:88) (cid:105) log π(yt) (ρs1) s=1 t=1 (cid:104) (cid:88) = Eτ µ log π(yt) (cid:124) t=1 (cid:123)(cid:122) REINFORCE (no IS) (cid:105) (cid:125) (cid:88) + Eτ µ (cid:2)(ρs1) log π(yt)(cid:3) . s,t (cid:124) (cid:123)(cid:122) first-order IS correction (cid:125) (27) (28) The token-level gradient is Jtok = (cid:88) t=1 Eτ µ (cid:2)ρt log π(yt)(cid:3) = Eτ µ (cid:105) log π(yt) + (cid:104) (cid:88) t=1 (cid:88) t=1 Eτ µ (cid:2)(ρt1) log π(yt)(cid:3). (29) Comparing, we see that token-level IS only retains the diagonal terms (s = t) of the first-order correction, discarding cross-token interactions where = t. The approximation error is Jseq Jtok (cid:88) s=t Eτ µ (cid:2)(ρs 1) log π(yt)(cid:3) + O(cid:0)(ρ 1)2(cid:1). (30) This error captures the fact that changing the policy at position affects the importance of the gradient at position ta cross-token dependency that token-level methods ignore. Remark A.1 (When Token-Level Approximation is Reasonable). The approximation Jtok Jseq is reasonable when: 1. Near on-policy: ρt 1 for all t, so higher-order terms and cross-token terms are small. 2. Short sequences: Fewer cross-token pairs (s, t) with = means smaller accumulated error. In practice, RL for LLMs often violates both conditions: off-policy updates are common, and reasoning tasks require long sequences. This motivates sequence-level methods that preserve the full product structure of importance weights. Length normalization (GSPO). GSPO uses the geometric mean ϕ(W ) = 1/T , which induces proposal distribution that explicitly depends on sequence length: QGSPO(τ ) µ(τ )11/T π(τ )1/T . (31) As , QGSPO µ, meaning longer sequences receive vanishingly small corrections toward the target policy. See Section for detailed analysis of the length-dependent bias this introduces. Sequence-level hard clipping. reshaping function would be ϕ(W ) = min(W, c), inducing If one were to apply hard clipping at the sequence level (truncating at threshold c), the Qclip(τ ) min(cid:0)π(τ ), µ(τ )(cid:1). (32) This is truncated distribution: trajectories with > are capped rather than weighted proportionally. The discontinuity at = can cause optimization difficulties when trajectories cross the boundary during training. Note that standard PPO applies clipping at the token level rather than the sequence level, combining the issues of both token-level weighting and hard truncation. B. Derivation of the Proposal Distribution We derive the closed-form solution to the constrained optimization problem in Equation (14). Proposition B.1 (Solution to the Constrained Problem). The solution to (1 α) DKL(Qµ) + α DKL(Qπ) min s.t. EQ[W ] C, (cid:82) = 1 14 (33) VESPO: Variational Sequence-Level Soft Policy Optimization is given by Q(τ ) ="
        },
        {
            "title": "1\nZ",
            "content": "µ(τ )1α π(τ )α exp(λW (τ )), where λ 0 is the Lagrange multiplier for the moment constraint and is the normalization constant. Proof. The Lagrangian for this problem is: Expanding the KL divergences: L(Q, λ, γ) = (1α)DKL(Qµ) + αDKL(Qπ) + λ(cid:0)EQ[W ] C(cid:1) + γ(cid:0)(cid:82) 1(cid:1). (cid:90) = Q(τ )(cid:2) log Q(τ ) (1α) log µ(τ ) α log π(τ ) + λW (τ )(cid:3)dτ + const. Taking the functional derivative with respect to and setting it to zero: δL δQ = log Q(τ ) + 1 (1α) log µ(τ ) α log π(τ ) + λW (τ ) + γ = 0. Solving for Q: Using = π/µ: log Q(τ ) = (1α) log µ(τ ) + α log π(τ ) λW (τ ) + const Q(τ ) µ(τ )1α π(τ )α exp(λW (τ )). Q(τ ) µ(τ )1α µ(τ )α (τ )α exp(λW (τ )) = µ(τ ) (τ )α exp(λW (τ )). Comparing with Q(τ ) = 1 µ(τ )ϕ(W (τ )) from Equation (10), we identify ϕ(W ) = α exp(λW ). (34) (35) (36) (37) (38) (39) (40) (41) (42) In practice, we use the shifted form ϕ(W ) = c1 exp(c2(1 )), which satisfies ϕ(1) = 1. This ensures Shifted Form. that on-policy samples receive unit weight. We treat (c1, c2) as tunable hyperparameters, allowing flexibility beyond the specific values implied by the variational derivation. C. Length Normalization Introduces Bias We analyze how length normalization in sequence-level importance sampling introduces length-dependent bias that conflates distinct trajectories. Length-Dependent Bias in GSPO. GSPO uses ϕ(W ) = 1/T , which induces proposal distribution that explicitly depends on the sequence length : QGSPO(τ ) µ(τ )11/T π(τ )1/T . (43) As , QGSPO µ: the normalized weight converges to exp(E[log ρt]), constant independent of the specific trajectory. This causes two fundamental problems: 1. Signal dissipation: All weights collapse toward constant, losing discriminative power. 15 VESPO: Variational Sequence-Level Soft Policy Optimization 2. Conflation of distinct sequences: Sequences with identical per-token statistics but different lengths receive identical weights, despite having different true importance weights. Proposition C.1 (Conflation under Length Normalization). For any two sequences τ1, τ2 with lengths T1 = T2, if log W1 , then GSPO assigns identical weights: ϕGSPO(W1) = ϕGSPO(W2). However, their true importance weights = log W2 T2 differ: W1 = eT1 vs W2 = eT2 for = log W1 = log W2 T2 . T1 This conflation is problematic: short sequence that is moderately off-policy and long sequence that is severely off-policy may receive identical gradient weights, even though their contributions to the policy gradient should differ substantially. VESPO Avoids Length-Dependent Bias. VESPO uses ϕ(W ) = c1 exp(c2(1W )) without any length normalization. The reshaping function depends only on the sequence-level importance weight , not on the sequence length . Two sequences with the same average per-token log-ratio but different lengths receive different weights: the longer sequence has = eT r, which is appropriately transformed by ϕ. This preserves the discriminative power of the importance weight while the soft-shaping kernel controls variance through exponential suppression of extreme weights. D. Additional Analysis of Baseline Failure Modes This section provides supplementary details on the failure modes discussed in Section 4.3, with additional visualizations comparing =4 vs =8 for each baseline. Figure 9. GRPO: = 4 (blue) vs = 8 (orange). Entropy decreases more rapidly at = 4, limiting exploration. 16 VESPO: Variational Sequence-Level Soft Policy Optimization Figure 10. GSPO: = 4 (blue) vs = 8 (orange). Response length grows to 3,000 tokens at = 4 before collapsing around step 1,200. E. Algorithm Pseudocode 3 4 5 7 8 9 10 11 13 14 15 16 17 19 20 21 22 23 25 26 1 def compute_policy_loss_vespo(log_pi, log_mu, advantages, mask, c_pos, c_neg): 2 \"\"\" Args: log_pi: log probs from current policy, shape (batch, seq_len) log_mu: log probs from behavior policy, shape (batch, seq_len) advantages: sequence-level advantages, shape (batch,) mask: response mask, shape (batch, seq_len) c_pos: (c1, c2) for positive advantages c_neg: (c1, c2) for negative advantages \"\"\" # Sequence-level IS ratio in log-space (true product, no length norm) log_ratio = log_pi - log_mu seq_log_w = (log_ratio * mask).sum(dim=-1) = exp(seq_log_w) # (batch,) # Asymmetric hyperparameter selection c1 = where(advantages >= 0, c_pos[0], c_neg[0]) c2 = where(advantages >= 0, c_pos[1], c_neg[1]) # VESPO kernel in log-space, normalized so phi(1) = 1 log_phi = c2 + c1 * log(W) - c2 * phi = exp(log_phi).detach() # gradient scaling only # Policy gradient loss: grad = -phi * * grad_log_pi loss = -phi.unsqueeze(-1) * advantages.unsqueeze(-1) * log_pi return aggregate(loss, mask) VESPO Policy Loss F. Training Hyperparameters Table 3 lists the training hyperparameters shared across all methods. Method-specific hyperparameters are described in Section 4.1. 17 VESPO: Variational Sequence-Level Soft Policy Optimization Figure 11. SAPO: = 4 (blue) vs = 8 (orange). Training collapses at = 8 due to insufficient suppression for negative advantages. Table 3. Training hyperparameters on Qwen3-30B-A3B-Base. Parameters shared between the synchronous and fully asynchronous settings are shown with merged cells. Hyperparameter Sync Fully Async Learning rate Mini-batch size (mbs) Responses per prompt Max prompt length Max response length Gradient steps Train temperature Eval temperature / top-p KL loss coefficient Entropy loss coefficient Training engine Inference engine 1 106 256 8 1,024 15,360 1,500 1.0 1.0 / 0.7 0 0 Megatron vLLM Global batch size (gbs) GPUs 256 32 (colocated) - 48 / 16 (rollout / train) Parameter sync interval Staleness threshold 4 1.0 - -"
        }
    ],
    "affiliations": [
        "Xiaohongshu Inc."
    ]
}