{
    "paper_title": "Context Learning for Multi-Agent Discussion",
    "authors": [
        "Xingyuan Hua",
        "Sheng Yue",
        "Xinyi Li",
        "Yizhe Zhao",
        "Jinrui Zhang",
        "Ju Ren"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual contexts.In this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive mechanism.It enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 ] . [ 1 0 5 3 2 0 . 2 0 6 2 : r Under review as conference paper at ICLR CONTEXT LEARNING FOR MULTI-AGENT DISCUSSION Xingyuan Hua1, Sheng Yue2, Xinyi Li3, Yizhe Zhao1, Jinrui Zhang1, Ju Ren1,4 1Department of Computer Science and Technology, Tsinghua University, 2School of Cyber Science and Technology, Sun Yat-sen University, 3College of Computer Science, Northwest University, 4Zhongguancun Laboratory"
        },
        {
            "title": "ABSTRACT",
            "content": "Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistencyLLMs fail to reach coherent solutiondue to the misalignment between their individual contexts. In this paper, we introduce multi-LLM context learning method (M2CL) that learns context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via carefully crafted self-adaptive mechanism. It enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%50%, while enjoying favorable transferability and computational efficiency."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Language Models (LLMs) have demonstrated transformative impact across large number of real-world domains, including education, healthcare, and scientific research, where an LLM instance is employed to automate the process of content generation, reasoning, or decision-making (Zheng et al., 2023; Imani et al., 2023; Zhang et al., 2024; Goyal et al., 2024). Yet, it has been recognized that single-LLM-built systems often struggle in the problems requiring complex multi-step reasoning or multi-tool using, such as complicated proof (Cobbe et al., 2021), large-scale code generation (Wang et al., 2025), and embodied agentic tasks (Ahn et al., 2022; Shen et al., 2024), because its single viewpoint of problem easily limits the ability to explore multiple reasoning paths, leverage external tools effectively, or adapt to dynamic task requirements (Li et al., 2023; Smit et al., 2024). Very recently, research has shifted towards Multi-Agent Discussion (MAD) (Smit et al., 2024), where multiple LLM instances collaboratively solve problems via structured discussions (Du et al., 2023; Liu et al., 2024). In typical MAD framework, each LLM instance is pre-assigned set of crafted contexts that represent diverse solution perspectives of the problem to be solved. Equipped with these distinct context instructions, LLMs continue to discuss with each other for solution consensus (Park et al., 2023; Shanahan et al., 2023; Wei et al., 2023; Lu et al., 2024; Liu et al., 2024). Such society-ofmind paradigms are expected to improve reasoning accuracy by enhancing creativity and expanding the search space for possible solutions, and have shown great potential across various complex tasks, including software engineering (Gu, 2023) and scientific discovery (Sprueill et al., 2024). Albeit achieving improved performance over single-LLM settings, we find that current multi-agent collaboration approaches typically suffer from discussion inconsistency, that is, the majority of LLM instances fail to reach an agreement on coherent solution (as showcased in Fig. 2), easily making the collaborative decision dominated by noise rather than principled reasoning. The underlying reason primarily lies in context misalignment between LLMs. On one hand, the pre-assigned contexts or Corresponding author: yuesh5@mail.sysu.edu.cn 1Code is available at https://github.com/HansenHua/M2CL-ICLR26. 1 Under review as conference paper at ICLR Figure 1: An illustration of context misalignment of an existing method (Debate Du et al. (2023)) on multi-step proof task. Pre-assigned context instructions (in the blue and yellow boxes of the left part) provide insufficient guidance on information fusion, leading to conflict in reasoning. Figure 2: The discrepancy between the answers of participating LLM instances. The discrepancy is characterized by the maximum distance between participating LLMs output embeddings. role-based instructions lack nuanced understanding of the task; they are often rigid, incomplete or biased, which would misguide the reasoning of individual LLMs (Jang et al., 2025). On the other hand, these contexts often fall short in effective fusion of information exchanged among LLMs, and thus hardly steer the discussion towards coherent solutions. As illustrated in Fig. 1, for multi-step mathematical proof task, one LLM agent may correctly derive an intermediate result, yet another LLMdespite receiving this result in the extended contextdoes not effectively incorporate it into its own reasoning chain. Since the context instruction does not explicitly enforce leveraging conclusions from other LLMs, other LLMs may redundantly re-derive the step or even give an inconsistent argument (see Section H) for full results). This paper aims to answer: how can we obtain the contexts for MAD that can continually guide multi-LLM discussion towards correct consensus? straightforward solution is to manually adapt instructions in contexts as the discussion progresses. It, however, is labor-intensive and requires extra expert knowledge, rendering it impractical for complex tasks or large-scale collaboration. Instead, more reasonable approach is to develop context learning mechanisms that enable evolving the context instructions based on the intermediate discussion results. Although promising, it is highly challenging to evaluate the contribution of LLMs contexts to the final solution and control the coherence among interand intra-LLM outputs. To tackle these challenges, we propose multi-LLM context learning (M2CL) method for efficient MAD, which learns context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. First, we characterize the impact of initial and evolving contexts on the discussion performance. Building upon the analytical insights, we train the generators to control context coherence and output discrepancies. To strike the right tradeoff therein, we devise self-adaptive balancing mechanism, enabling LLMs to progressively align on correct consensus while avoiding premature convergence on majority noise. Further, we develop lightweight context initialization approach, which tends to assign LLMs with diverse initial instructions that are approximately orthogonal in the latent space, enabling sufficient coverage of complementary solution perspectives. We systematically evaluate the proposed method across 9 challenging benchmarks, including LLM reasoning, embodied agentic tasks, and mobile GUI control. The results demonstrate that M2CL 2 Under review as conference paper at ICLR 2026 significantly enhances the MAD performance under various numbers of participating LLMs, consistently outperforming existing methods by 20%50%, particularly in complex GUI control tasks. M2CL enjoys more favorable MAD scaling law and exhibits great efficiency, where runtime overhead of at most 10% suffices to achieve more than 20% performance gains. Further, we find that the learned context generators can be migrated to different LLM architectures with consistent performance improvement. Our main contributions are summarized as follows: We propose M2CL, principled multi-LLM context learning method that learns context generator for each agent, capable of dynamically generating context instructions per discussion round. We devise lightweight context initialization approach that can assign LLMs with diverse initial instructions, thereby enabling sufficient coverage of complementary solution perspectives. We systematically evaluate the proposed method across range of challenging benchmarks and corroborate the efficacy and efficiency of the proposed method."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Multi-agent framework. Multi-agent frameworks have been proposed to enhance the reasoning capabilities of single-LLM prompting methods. Several works utilize two LLMs to iteratively generate and evaluate to refine the final answer (Raman et al., 2022; Yao et al., 2023; Paul et al., 2023; Madaan et al., 2024). However, scaling them to larger number of LLMs to make full use of the wisdom of crowd is challenging due to the pairwise dependency of generation and evaluation. To incorporate more LLMs, several studies have explored another multi-agent framework in which each LLM has access to the history of all preceding LLMs responses (Chan et al., 2023; Du et al., 2023; Liang et al., 2023; Smit et al., 2024; Liu et al., 2024; Zhuge et al., 2024). Most existing methods remain constrained by manually defined inter-LLM topologies or workflows. More recently, researches have attempted to overcome this limitation by automatically optimizing workflows (Zhuge et al., 2024; Liu et al., 2024; Zhang et al., 2025b). However, such evolution is often one-shot. For instance, Zhuge et al. (2024) describe language agent systems as optimizable computational graphs and enables automatic improvements of LLM prompts and inter-LLM orchestration. Zhang et al. (2025b) employs Monte Carlo Tree Search to construct complex Multi-LLM systems tailored to specific task domain. Liu et al. (2024) propose multi-LLM collaboration method which constructs dynamic communication structure by scoring other LLMs. While these approaches allow for scaling the number of LLMs, designing appropriate contexts for them to collaboratively solve problems remains difficult, as single-viewpoint context instructions fall short in guidance on how to organize and refine information across agents. Context learning. Context learning has recently attracted significant attention, as it allows LLMs to adjust their behavior at inference time by modifying the input context, without requiring any gradient-based training Von Oswald et al. (2023); Todd et al. (2024); Li et al. (2024a). One line of work focuses on context selection, which identifies the most relevant examples or information to include in the context Zhang et al. (2022); Lu et al. (2023); Xiong et al. (2024); Purohit et al. (2025). To eliminate manual selection, Lu et al. (2023) leverage reinforcement learning to learn context selection policy. Xiong et al. (2024) query LLM to obtain knowledge and then query retriever to obtain the final context. Beyond selection, some recent methods have proposed to generate or evolve context (Zhuge et al., 2024; Li et al., 2024b; Zhang et al., 2025a). Madaan et al. (2024) leverage feedback generated by LLMs as extra prompt and iteratively incorporate it into revised drafts, aiming to enhance the coherence of the generated text. Pandita et al. (2025) dynamically refines prompts during inference using textual feedback from prior outputs, aiming to improve contextual alignment and generate more consistent responses. Albeit with promising results, these methods are grounded in single-LLM formulations and struggle with inter-LLM inconsistency during multi-agent dscussion, where it is crucial to guide LLMs to make full use of others intermediate results."
        },
        {
            "title": "3 PRELIMINARIES",
            "content": "In this section, we provide necessary backgrounds and definitions of our investigated problem. 3 Under review as conference paper at ICLR 2026 Context learning. To formally define context learning, we begin with the standard probabilistic model of an autoregressive LLM. An autoregressive model, parameterized by ϕ, generates an output sequence = (x1, x2, . . . , xL) given an input context by maximizing the conditional probability: Pϕ(XC) = (cid:89) l=1 Pϕ(xlx<l, C). (1) Historically, in the paradigm of prompt engineering, the context was treated as static string of text. This view is insufficient for complex agentic tasks which leverage dynamic, structured, and multifaceted information stream. To address this, context learning is redefined as dynamically organized collection of information, denoted as {c1, c2, . . . , cn}. Each component ck can be instructions, external knowledge (Lewis et al., 2020), available external tools (Qin et al., 2023), and memory (Zhang et al., 2025a). These components are sourced, filtered, and formatted by set of functions fk, and orchestrated into coherent representation by high-level assembly function A: = A(f1(c1), f2(c2), . . . , fn(cn)). (2) Multi-LLM context learning. MAD involves set of LLM instances collaboratively solving task via multi-round inter-LLM discussion (debate). Each LLM is endowed with an evolving instruction context in terms of the task description, available external tools and current knowledge aggregation. At the t-th round, we use the concatenation function as the assembly function and construct the context with three components: (i) the task goal Rdmodeln, (ii) the concatenation of responses from all other LLMs in the previous round t1 is the response Rdmodeln (dmodel refers of LLM at round 1, and (iii) the current instruction context to the dimension of embedding). The task goal represents the ultimate objective of collaboration and remains invariant across rounds. The second component serves as dynamic memory that incorporates cross-LLM interaction history. For the third component, in constrast to existing efforts relying on static preassigned roles, we employ an instruction generator G, parameterized by θi, to adaptively refine it into per-step instruction , conditioned on the task goal , the initial instruction , and the concatenated response t1 ]j=i Rdmodeln, where t1 = [X t1 : Given context = [I , t1 = Gθi ([P ; t ; t ]). , ], each LLM i, parameterized by ϕi, generate its response by: = arg max Pϕi(XC ). (3) (4) After rounds of interaction, the final result is obtained by majority vote on the LLMs outputs generated in the final round."
        },
        {
            "title": "4 MOTIVATION",
            "content": "In this section, we investigate the quantified impact of the contexts on the MAD performance. First, we introduce the formulation of attention activation, denoted as a(), as follows (Vaswani, 2017): a(C ) . = WV [I ; t1 ; ]softmax (cid:18) (WK[I ; t1 ; ])T WQP (cid:19) (5) where mechanism. a(C d is scaling factor. WQ, WK, and WV denote the parameter weight matrices in the attention ) Rdmodeln is matrix with the same shape as [P ; X; I] From this, we have the following theorem characterizing the total distance between the activations of the correct answer ac and that induced by context j. Here, we utilize activation distance instead of token embedding distance as attention activation captures deep representational similarity learned through the models internal reasoning process, making it more robust to superficial linguistic variations. 4 Under review as conference paper at ICLR 2026 Theorem 4.1. Assume that the attention activation function is La-smooth and define the weight ; ]. Then, the following fact holds: vector as ω . = [ω1, ω2, . . . , ωN ], with the initial context . = [I (cid:88) i=1 ac a(C ) (cid:88) (cid:16) (cid:88) i=1 j=1 a(C ) a(C j) + (N + 1)LaC b (cid:17) + min ω ac (cid:88) i=1 ωia(C ). (6) Proof. For detailed proof, please refer to Section B. ) a(C j) + (N + 1)LaC Theorem 4.1 corroborates the necessity of multi-LLM context learning, consisting of both initialization and evolution. The first term, a(C , captures the divergence among LLMs activations as well as the deviation from their initial contexts. This indicates contexts must be continuously evolved to reduce inter-LLM discrepancies while keeping coherent, promoting consistency of reasoning chains. The second term, minω ac (cid:80)N ), depends solely on the initial contexts . It implies that orthogonality among initial activations provides comprehensive basis, allowing the contexts to approximate the correct activation more effectively. This highlights the importance of diverse solution perspectives at the initialization stage. Together, the result motivates two-stage design: initializing contexts to ensure diverse solution perspectives and evolving intermediate contexts to drive consensus across LLMs. i=1 ωia(C b"
        },
        {
            "title": "5 MULTI-LLM CONTEXT LEARNING",
            "content": "In this section, we introduce multi-LLM context learning method that can select appropriate contexts for each LLM and dynamically adapt the context according to the evolving task completion status. 5.1 CONTEXT INITIALIZATION The context initialization serves as the foundation of the entire multi-agent interaction process, as it frames the capability scope of LLMs and fundamentally shapes the chain of subsequent information exchange. Therefore, we propose to select initial contexts from predefined pool {I i=1 containing prompts with diverse perspectives. }M Motivated by Theorem 4.1, we provide the following context initialization mechanism: = {I 1, . . . , } = arg min (cid:110) min ω (cid:88) i=1 ωia([I ; ] ac) (cid:111) . (7) Eq. (7) identifies subset of contexts whose activation best reconstruct the target activation ac. ; ]) Rdmodeln is far larger than the number of As the dimension of the activation matrix a([I selected contexts , set of matrices that aims to best reconstruct the correct activation ac naturally tends toward forming set of basis-like directions. The resulting near-orthogonal activations form compact basis, ensuring each context contributes unique, non-overlapping information for subsequent discussion. However, Eq. (7) is impractical since the correct activation ac is not accessible during initialization. Inspired by prior work (Yang et al., 2024b; 2025), we project the activation into latent space with function : = {I 1, . . . , } = arg min (cid:110) min ω (cid:88) i=1 ωif (a([I ; ])) (ac) (cid:111) , (8) Then, we use the problem space as the selected contexts are solely dependent on the problem. This projection preserves the original orthogonality properties of activations, ensuring that diverse perspectives remain distinguishable in other spaces. Therefore, we reformulate the initialization 5 Under review as conference paper at ICLR 2026 mechanism: = {I 1, . . . , } = arg min (cid:110) min ω (cid:88) i=1 ωif (a([I ; ])) vP (cid:111) , (9) where vP denotes the sentence vector of the question . To obtain the projection () (parameterized by ϕf ), we utilize the activation of inputting the answer with the question as the correct activation ac = a([A; ]) and provide the loss function as: L(ϕf ) = vP (a([A; ])). (10) However, directly leveraging Eq. (9) to initialize context is computational costly as it requires all the context activations in the context pool. Therefore, we distill lightweight F() (parameterized by ϕF ) which directly projects the initial context into the problem space through the following loss function: Then, we provide the final context initialization as: L(ϕF ) = F([I ; ]) (a([I i ; ])). = {I 1, . . . , } = arg min (cid:110) min ω (cid:88) i=1 ωiF([I ; ]) vP (cid:111) . (11) (12) Eq. (12) provides computational efficient context initialization (as illustrated in Fig. 23) that aligns with Eq. (7) to encourage orthogonality of selected context activations, thereby providing diverse reasoning perspectives and expanding the search space for solutions. 5.2 CONTEXT EVOLUTION As discussed in Section 5.1, context initialization aims to select contexts with diverse perspectives to solve the problem. Nevertheless, these individual contexts address the problem from single viewpoint while lacking explicit instructions on how to incorporate perspectives provided by other LLMs. Directly using them often results in inconsistent reasoning across LLMs due to misaligned inter-LLM guidance. To address this, we iteratively refine the instruction to integrate collaborative instructions that guide the LLMs more effectively toward generating the desired solution. 5.2.1 EVALUATE THE CONTRIBUTION OF CONTEXTS To achieve efficient context evolution, it is crucial to quantify the contribution of different contexts (refer to utility function). natural solution is to assign utilities only at the final round. However, this leads to inefficient and unstable training process due to its sparsity Liu et al. (2025). On the other hand, current methods typically use correctness as the only criterion (Zelikman et al., 2022; Rafailov et al., 2023; Guo et al., 2025). Albeit with promising performance in single-LLM question answering, this simple criterion ignores the complex dependency among LLMs with different contexts in MAD. As result, an LLM that provides crucial insights which enable others to reach the correct answer may still receive low utility, simply because it fails to produce the correct final answer itself. To tackle these challenges, we introduce novel round-wise criterion to evaluate the contribution of in multi-agent interaction: (cid:110) max j[N ] αC b a(C i ) a(C j) (cid:111) , (13) where α 0 is weighting parameter and a(C between contexts. ) a(C j) represents the activation difference Remarks. Of note, if removing the second term of Eq. (13), the context generation reduces to that of prompt engineering: minI , which utilize fixed instructions in contexts to LLMs throughout the discussion (Liu et al., 2024; Lu et al., 2024). As mentioned in Section 1, this easily results in inconsistency across LLMs. Thus, we introduce a(C j) into the objective to encourage LLMs to remain aligned with each other. i[N ] αC ) a(C (cid:80) 1:N This design addresses the two aforementioned challenges. First, this round-wise criterion provides denser feedback, mitigating the inefficiency and instability caused by sparse final-round utilities. 6 Under review as conference paper at ICLR 2026 Second, the activation-based alignment term captures inter-LLM dependencies, thereby encouraging them to offer guidance on how to integrate others responses. Applying Eq. (13) as the criterion for LLM requires the contexts of all other LLMs. However, since these contexts are simultaneously being optimized, bias inevitably arises as each LLM is updated based on stale snapshots of others contexts. Such outdated information introduces biased utility, which in turn hinders consistency and degrades the quality of the learned contexts. To overcome this issue, we decouple the inter-LLM dependencies by designing an alternative per-LLM criterion: αC i a([I , ]) a([X t1 , ]). (14) b , ]) a([X t1 In Theorem C.1, we can prove that the summation of Eq. (14) over all LLMs serves as an upper bound of the summation of Eq. (13) over all LLMs. Intuitively, this criterion conveys clear meaning: the first term, αC , preserves the fundamental problem-solving capability endowed by the initialization, while the second term, a([I , ]), enforces consistency between an LLMs current instruction and its own previous response. This local consistency constraint implicitly aligns all LLMs, because when every LLM evolves its context in temporally coherent way, the divergence among them is gradually reduced. Hence, the collective effect of all LLMs following this rule is that their contexts evolve coherently across rounds, progressively shrinking activation differences and ultimately driving the MAD toward consistency in their final answers. = t1 To intuitively show the tightness of the bound, consider the case where holds, i.e., the instruction of each LLM in the next round equals its response in the previous round. That means a(I ). Given that all LLMs receive the same context, the second term of both Eq. (13) and Eq. (14) becomes zero. In this case, the two formulations coincide. ) = a(X t1 i 5.2.2 MULTI-ROUND CONTEXT EVOLVING We slightly abuse notation by denoting a(I provide the multi-round context evolution objective by accumulating contributions as follows: ; ]) and a(X t1 ) = a([X t1 ) = a([I i ; ]), and (cid:88) t=1 (cid:110) max αC b a(I ) a(X t1 (cid:111) . ) (15) However, selecting the optimal value for the weight α is non-trivial, as it must be carefully tuned for the evolving discussion process. To alleviate the need for manual tuning, we recast Eq. (15) into constrained optimization problem, where the context adjustment i is treated as constraint: (cid:18) min 1 s.t. a(I 1 ) a(X 0 ) + min 2 b β, t, i. (cid:16) a(I 2 ) a(X 1 ) + + min a(I i ) a(X 1 (cid:17)(cid:19) ) (16) According to the detailed derivation in Section E, we prove that taking the dual of this problem recovers Eq. (15) and yields an auxiliary update for the dual variable α. Consequently, the optimization of Problem (16) can be implemented using an approximate dual gradient descent procedure, alternating gradient updates of L(θi) and L(αi). L(θi) = (cid:13) L(αi) = αi (cid:13)a(Gθi(P, (cid:0)β (cid:13) , t1 (cid:13)Gθi(P, )) a(X t1 , t1 ) )(cid:13) (cid:13) + α(cid:13) (cid:13) (cid:1). (cid:13) (cid:13)C b (cid:13) (cid:13) (17) At the beginning of the discussion, the initial contexts are intentionally diverse to encourage multiperspective reasoning of the problem. At this stage, when the answers differ greatly, α decreases rapidly, thereby weakening the constraint on the distance between the generated contexts and their initial contexts. This guides the generation of contexts towards promoting faster convergence on unified solution. As the discussion progresses and the LLMs gradually reach agreement, α will be kept at certain level. This adjustment prevents premature consensus and supports richer, more comprehensive final solution by shifting the contexts focus toward preserving multiple perspectives and exploring nuanced differences. Overall, we name our proposed algorithm M2CL, with its pseudocode detailed in Section F.4. 7 Under review as conference paper at ICLR 2026 Table 1: Accuracy (%) on different datasets. The number of LLMs is 4 for all dataset. We exhibit the performance advantage with BoN and highlight the best result. Model Method MMLU MATH GPQA Code ALFWorld SciWorld GAIA Qwen-7B Qwen-14B Qwen-72B Single BoN Debate DyLAN GPTSwarm MacNet M2CL (ours) Single BoN Debate DyLAN GPTSwarm MacNet M2CL (ours) Single BoN Debate DyLAN GPTSwarm MacNet M2CL (ours) 61.213.0 74.20.0 71.13.1 74.30.1 76.32.1 71.52.7 92.518.3 67.212.5 79.70.0 77.22.5 86.87.1 87.07.3 78.90.8 93.714.0 72.511.7 84.20.0 82.71.5 91.57.3 91.57.3 83.80.4 95.110.9 12.912.0 24.90.0 19.95.0 26.71.8 26.21.3 21.43.5 47.822.9 21.66.2 27.80.0 27.40.4 31.23.4 31.33.5 28.70.9 51.723.9 31.619.4 51.00.0 48.42.6 63.112.1 64.713.7 52.91.9 72.521. 20.216.2 36.40.0 28.77.7 35.41.0 35.60.8 30.85.6 66.129.7 21.211.6 32.80.0 30.32.5 39.46.6 38.75.9 31.90.9 66.233.4 34.911.0 45.90.0 43.42.5 51.65.7 52.46.5 46.20.3 78.933.0 51.211.3 62.50.0 60.02.5 63.40.9 62.70.2 59.33.2 80.317.8 56.712.7 69.40.0 66.92.5 76.67.2 75.96.5 70.41.0 91.121.7 59.113.1 72.20.0 69.13.1 80.48.2 79.67.4 70.51.7 90.718. 23.87.7 31.50.0 30.60.9 29.81.7 29.91.6 33.62.1 39.98.4 30.17.4 37.50.0 36.80.7 34.82.7 34.92.6 39.21.7 48.210.7 48.29.3 57.50.0 60.42.9 55.12.4 56.80.7 61.84.3 79.021.5 25.210.1 35.30.0 32.23.1 29.45.9 30.84.5 37.01.7 45.310.0 31.310.3 41.60.0 39.42.2 36.05.6 35.95.7 46.04.4 56.114.5 50.411.7 62.10.0 65.23.1 58.04.1 60.21.9 68.46.3 88.926. 15.65.5 21.10.0 21.00.1 18.42.7 20.60.5 21.50.4 33.612.5 18.76.9 25.60.0 26.40.8 22.82.8 25.00.6 32.26.6 42.016.4 31.110.1 41.20.0 42.91.7 40.40.8 40.30.9 46.45.2 67.226.0 PDDL 21.05.3 26.30.0 24.81.5 23.42.9 24.51.8 29.53.2 34.78.4 25.72.8 28.50.0 30.01.5 28.00.5 28.40.1 32.74.2 43.014. 41.010.0 51.00.0 49.11.9 45.55.5 49.71.3 53.72.7 70.519."
        },
        {
            "title": "6 EXPERIMENT",
            "content": "In this section, we conduct experiments to evaluate the performance of M2CL by answering the following research questions: Q1. How does M2CL perform compared to existing methods across various benchmarks, especially in complex agentic tasks? Q2. How does the performance of MAD scale with the number of LLMs? Q3. How is the performance affected by factors such as context constraint and components such as context initialization and context evolution? Q4. How do contexts promote consensus, and are they transferable to other models? 6.1 EXPERIMENTAL SETUP Dataset. We run experiments with 3 domains including 9 datasets: 1) LLM reasoning, including MMLU (Hendrycks et al., 2021), MATH (Hendrycks et al., 2021), and GPQA (Rein et al., 2023), HumanEval (Chen et al., 2021). 2) Embodied Agentic, including ALFWorld Shridhar et al. (2021), SciWorld Wang et al. (2022), GAIA (Mialon et al., 2024), and PDDL Chang et al. (2024). 3) Mobile GUI AndroidWorld Rawles et al. (2025). Details on datasets can be found in Section F.1. Baselines. We evaluate our method against six strong baseline methods: 1) Single execution, querying single LLM to solve the task. 2) Best-of-N, querying single LLM times and sampling the most correct answer. 3) Debate (Du et al., 2023), multi-agent framework where LLMs discuss their responses and reasoning processes over multiple rounds. 4) DyLAN (Liu et al., 2024), multi-agent framework where LLMs score each other and collaborate dynamically. 5) GPTSwarm (Zhuge et al., 2024), multi-agent framework that refines LLM prompts and improves LLM orchestration by changing their connectivity. 6) MacNet (Jiang et al., 2023), recent multiLLM framework where LLMs are invoked between LLM interactions to provide actionable instructions to the next LLM based on the previous LLMs outputs. Reproducibility. All details of our experiments are provided in the appendices in terms of the tasks, network architectures, hyperparameters, etc. We conduct experiment on two series of LLM (Llama-2 (Touvron et al., 2023) and Qwen-2.5 (Yang et al., 2024a)) and three model sizes each for LLM reasoning, and Qwen2.5-VL (Bai et al., 2025) for GUI reasoning. All the experiments are run on Ubuntu 22.04.4 LTS with 8 NVIDIA H800 GPUs. Under review as conference paper at ICLR 2026 Figure 3: Performance versus runtime under different settings. Circles closer to the lower-left corner indicate higher efficiency. Figure 4: Performance of varying the numbers of LLMs. Uncertainty intervals depict standard deviation over three seeds. 6.2 EXPERIMENTAL RESULTS Comparative results. To answer the first question, we evaluate M2CLs performance across all datasets, with varying base models and number of LLMs (ranging from 4 to 64). We select Qwen series models and 4 LLMs participating in Table 1 and provide full results in Sections G.2 and G.3. We find M2CL consistently outperforms baselines in all 9 datasets, often by significant margin in terms of performance. Of note, BoN outperforms most of the baselines especially in complex multi-round agentic tasks, revealing the drawback of fixed contexts, which, despite expanding the exploration space, do not converge and thus hinder LLMs from achieving true cooperative reasoning. In contrast, M2CL can adapt contexts and enhance the relevance of responses and questions while ensuring creativity, indicating that M2CL well avoids that LLMs with different contexts easily influence each other and successfully brings LLMs into cooperation by reaching consensus through discussion. In addition, we evaluate the efficiency of M2CL by visualizing the performance versus runtime. As shown in Fig. 3, M2CL consistently delivers the highest performance improvement (more than 20%) while maintaining modest increase in runtime (less than 10%). This clearly demonstrates the efficiency of M2CL due to its lightweight context generator. Multi-agent scaling law. To answer the second question, we run experiments with varying numbers of LLMs (ranging from 4 to 64). The data and parameter setup adhere to that of Table 2. We present selected results in Fig. 4 and full results in Tables 4 to 8 and Figs. 5 to 10 of Section G.2. Scaling our method reveals more efficient scaling law as the performance grows logarithmically before saturation and improves faster than baselines. We speculate this arises because collaborative instructions in our generated contexts enable genuine inter-LLM cooperation, thereby unleashing the multidimensional reasoning capabilities of the MAD. Context constraint. To answer the third question, we vary the context constraint β from 0 to 10 and run experiments across all datasets. Full results are shown in Figs. 12 to 16 of Section G.4. The results clearly indicate that as β increases, there is an initial improvement in performance; once it reaches sufficiently large value, performance tends to drop. strict (refers to smaller β) context constraint leads to generated contexts getting closer to the initial contexts, thereby causing discussion inconsistency. On the other hand, loose context constraint leads to naive consistency, where LLMs tend to generate the same answers, resulting in insufficient creativity. Ablation studies. We assess the effect of key components by ablating them on all datasets under the same setting. 1) Importance of context initialization. 2) Importance of tuning α. 3) Importance of context evolution. As illustrated in Tables 11 to 13, LLMs struggle to specialize and coordinate efficiently, whereas context initialization enables them to acquire high-impact contexts, significantly 9 Under review as conference paper at ICLR 2026 enhancing the foundational capabilities of MAD. Without tuning α during discussion rounds, LLMs tend to reach an agreement in the first round, leading to responses that lack creativity and diversity, which ultimately reduces problem-solving ability. Without context evolution, LLMs lack collaborative guidance considering previous responses, which prevents them from effectively leveraging the outputs of other LLMs. Discrepancy intensity. To answer the fourth question, we explored the change of Discrepancy intensity (maxi,j[N ] ai aj2) with rounds to verify whether the generated context can help the LLMs gradually reach agreement with the discussion. Complete results are shown in Figs. 17 to 21, of Section G.5. We find that discrepancy intensity from M2CL decreases faster than other methods, corroborating its efficacy and superiority in converging the search space of multiple LLMs. Transferability of contexts. To answer the fourth question, we also conduct experiments on transferring contexts directly into stronger LLMs to verify whether the generated contexts have better interpretation ability and efficacy. Full results are depicted in Table 10 of Section G.6. The results show that the transferred contexts deliver consistent improvement, indicating that the trained context generator can directly adapt to wide range of models without additional retraining."
        },
        {
            "title": "7 LIMITATION AND DISCUSSION",
            "content": "In this paper, we propose novel context learning method, designed to expand LLMs horizons and reach consensus in MAD. By initializing and adjusting LLMs contexts based on the problem and discussion states, our method significantly improves problem-solving capabilities across diverse benchmarks while maintaining computational efficiency. limitation of M2CL is the MAD framework in which diversity is brought about by the number of LLMs with heterogeneous characteristics which is computationally inefficient. An avenue for future work is to enable LLMs to truly capture the sub-tasks they are interested in or excel at."
        },
        {
            "title": "8 ETHICS STATEMENT",
            "content": "This work advances the field of large language model (LLM) research by introducing context learning method for multi-agent discussion, demonstrating significant improvements in accuracy, diversity, and consensus across multiple benchmarks. However, the broader implications of deploying such systems warrant careful consideration. Multiagent discussion, while powerful, may inadvertently propagate biases introduced during context initialization or amplify errors during consensus-building. These risks are particularly critical in high-stakes domains like legal, financial, and healthcare applications, where erroneous outputs could lead to significant ethical, social, or economic consequences."
        },
        {
            "title": "9 REPRODUCIBILITY STATEMENT",
            "content": "The full algorithmic details of M2CL are presented in the main paper, with additional implementation details and hyperparameter settings included in Section 6. To facilitate replication, we provide anonymized source code at https://anonymous.4open.science/r/ 68dccdebea24497f-3BD6, which includes scripts for training, evaluation, and reproducing all reported experiments. For theoretical results, we include complete proofs in Sections to D."
        },
        {
            "title": "REFERENCES",
            "content": "Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as can, not as say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 10 Under review as conference paper at ICLR 2026 Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. Chateval: Towards better llm-based evaluators through multi-agent debate. In Proceedings of The 11st International Conference on Learning Representations, 2023. Ma Chang, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang, Yaohui Jin, Zhenzhong Lan, Lingpeng Kong, and Junxian He. Agentboard: An analytical evaluation board of multi-turn llm agents. In Proceedings of Advances in Neural Information Processing Systems, volume 37, pp. 7432574362, 2024. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Yilun Du, Shuang Li, Antonio Torralba, Joshua Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. In Proceedings of The 40th International Conference on Machine Learning, 2023. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Sagar Goyal, Eti Rastogi, Sree Prasanna Rajagopal, Dong Yuan, Fen Zhao, Jai Chintagunta, Gautam Naik, and Jeff Ward. Healai: healthcare llm for effective medical documentation. In Proceedings of The 17th ACM International Conference on Web Search and Data Mining, pp. 11671168, 2024. Qiuhan Gu. Llm-based code generation method for golang compiler testing. In Proceedings of The 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, pp. 22012203, 2023. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Proceedings of The 35th Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. Shima Imani, Liang Du, and Harsh Shrivastava. Mathprompter: Mathematical reasoning using large language models. arXiv preprint arXiv:2303.05398, 2023. Doohyuk Jang, Yoonjeon Kim, Chanjae Park, Hyun Ryu, and Eunho Yang. Reasoning model is stubborn: Diagnosing instruction overriding in reasoning models. arXiv preprint arXiv:2505.17225, 2025. Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. Llm-blender: Ensembling large language models with pairwise ranking and generative fusion. arXiv preprint arXiv:2306.02561, 2023. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. In Proceedings of Advances in Neural Information Processing Systems, volume 33, pp. 94599474, 2020. Dongfang Li, Zhenyu Liu, Xinshuo Hu, Zetian Sun, Baotian Hu, and Min Zhang. In-context learning In Proceedings of Advances in Neural state vector with inner and momentum optimization. Information Processing Systems, 2024a. Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for\" mind\" exploration of large language model society. In Proceedings of Advances in Neural Information Processing Systems, volume 36, pp. 5199152008, 2023. Under review as conference paper at ICLR 2026 Zhuowei Li, Zihao Xu, Ligong Han, Yunhe Gao, Song Wen, Di Liu, Hao Wang, and Dimitris Metaxas. Implicit in-context learning. arXiv preprint arXiv:2405.14660, 2024b. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, and Zhaopeng Tu. Encouraging divergent thinking in large language models through multi-agent debate. arXiv preprint arXiv:2305.19118, 2023. Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang. dynamic llm-powered agent network for task-oriented agent collaboration. In Proceedings of The 1st Conference on Language Modeling, 2024. Ziru Liu, Cheng Gong, Xinyu Fu, Yaofang Liu, Ran Chen, Shoubo Hu, Suiyun Zhang, Rui Liu, Qingfu Zhang, and Dandan Tu. Ghpo: Adaptive guidance for stable and efficient llm reinforcement learning. arXiv preprint arXiv:2507.10628, 2025. Li-Chun Lu, Shou-Jen Chen, Tsung-Min Pai, Chan-Hung Yu, Hung-yi Lee, and Shao-Hua Sun. Llm discussion: Enhancing the creativity of large language models via discussion framework and role-play. arXiv preprint arXiv:2405.06373, 2024. Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. In Proceedings of The 11st International Conference on Learning Representations, 2023. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. In Proceedings of Advances in Neural Information Processing Systems, volume 36, 2024. Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: benchmark for general ai assistants. In Proceedings of The 12th International Conference on Learning Representations, 2024. Deepak Pandita, Tharindu Cyril Weerasooriya, Ankit Parag Shah, Isabelle Diana May-Xin Ng, Christopher Homan, and Wei Wei. Prorefine: Inference-time prompt refinement with textual feedback. arXiv preprint arXiv:2506.05305, 2025. Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of The 36th Annual ACM Symposium on User Interface Software and Technology, pp. 122, 2023. Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi Faltings. Refiner: Reasoning feedback on intermediate representations. arXiv preprint arXiv:2304.01904, 2023. Kiran Purohit, Venktesh, Sourangshu Bhattacharya, and Avishek Anand. Sample efficient demonstration selection for in-context learning. In Proceedings of The 42nd International Conference on Machine Learning, 2025. Chen Qian, Zihao Xie, YiFei Wang, Wei Liu, Kunlun Zhu, Hanchen Xia, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, et al. Scaling large language model-based multi-agent collaboration. In Proceedings of The 13th International Conference on Learning Representations, 2025. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. In Proceedings of The 12th International Conference on Learning Representations, 2023. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In Proceedings of Advances in Neural Information Processing Systems, volume 36, pp. 5372853741, 2023. Under review as conference paper at ICLR 2026 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. Shreyas Sundara Raman, Vanya Cohen, Eric Rosen, Ifrah Idrees, David Paulius, and Stefanie Tellex. Planning with large language models via corrective re-prompting. In Proceedings of The 36th Conference on Neural Information Processing System Foundation Models for Decision Making Workshop, 2022. Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, et al. Androidworld: dynamic benchmarking environment for autonomous agents. In Proceedings of The Thirteenth International Conference on Learning Representations, 2025. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023. Murray Shanahan, Kyle McDonell, and Laria Reynolds. Role play with large language models. Nature, 623(7987):493498, 2023. Weizhou Shen, Chenliang Li, Hongzhan Chen, Ming Yan, Xiaojun Quan, Hehong Chen, Ji Zhang, and Fei Huang. Small llms are weak tool learners: multi-llm agent. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 1665816680, 2024. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. In Proceedings of The 10th International Conference on Learning Representations, 2021. Andries Petrus Smit, Nathan Grinsztajn, Paul Duckworth, Thomas Barrett, and Arnu Pretorius. Should we be going mad? look at multi-agent debate strategies for llms. In Proceedings of The 41st International Conference on Machine Learning, 2024. Henry Sprueill, Carl Edwards, Khushbu Agarwal, Mariefel Olarte, Udishnu Sanyal, Conrad Johnston, Hongbin Liu, Heng Ji, and Sutanay Choudhury. Chemreasoner: Heuristic search over large language models knowledge space using quantum-chemical feedback. In Proceedings of The 41st International Conference on Machine Learning, 2024. Eric Todd, Millicent Li, Arnab Sen Sharma, Aaron Mueller, Byron Wallace, and David Bau. Function vectors in large language models. In Proceedings of The 12th International Conference on Learning Representations, 2024. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Vaswani. Attention is all you need. In Proceedings of Advances in Neural Information Processing Systems, 2017. Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In Proceedings of The 41st International Conference on Machine Learning, pp. 3515135174. PMLR, 2023. Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu. Scienceworld: Is your agent smarter than 5th grader? In Proceedings of The 2022 Conference on Empirical Methods in Natural Language Processing, pp. 1127911298, 2022. Sizhe Wang, Zhengren Wang, Dongsheng Ma, Yongan Yu, Rui Ling, Zhiyu Li, Feiyu Xiong, and Wentao Zhang. Codeflowbench: multi-turn, iterative benchmark for complex code generation. arXiv preprint arXiv:2504.21751, 2025. 13 Under review as conference paper at ICLR 2026 Jimmy Wei, Kurt Shuster, Arthur Szlam, Jason Weston, Jack Urbanek, and Mojtaba Komeili. Multiparty chat: Conversational agents in group settings with humans and models. arXiv preprint arXiv:2304.13835, 2023. Jing Xiong, Zixuan Li, Chuanyang Zheng, Zhijiang Guo, Yichun Yin, Enze Xie, Zhicheng YANG, Qingxing Cao, Haiming Wang, Xiongwei Han, Jing Tang, Chengming Li, and Xiaodan Liang. DQlore: Dual queries with low rank approximation re-ranking for in-context learning. In Proceedings of The 12th International Conference on Learning Representations, 2024. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024a. Hao Yang, Qianghua Zhao, and Lei Li. Chain-of-thought in large language models: Decoding, projection, and activation. arXiv preprint arXiv:2412.03944, 2024b. Jingyuan Yang, Rongjun Li, Weixuan Wang, Ziyu Zhou, Zhiyong Feng, and Wei Peng. Lf-steering: Latent feature activation steering for enhancing semantic consistency in large language models. arXiv preprint arXiv:2501.11036, 2025. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In Proceedings of The 11th International Conference on Learning Representations, 2023. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. In Advances in Neural Information Processing Systems, volume 35, pp. 1547615488, 2022. Guibin Zhang, Muxin Fu, Guancheng Wan, Miao Yu, Kun Wang, and Shuicheng Yan. G-memory: Tracing hierarchical memory for multi-agent systems. arXiv preprint arXiv:2506.07398, 2025a. Jiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng, Xiong-Hui Chen, Jiaqi Chen, Mingchen Zhuge, Xin Cheng, Sirui Hong, Jinlin Wang, et al. Aflow: Automating agentic workflow generation. In Proceedings of The 13th International Conference on Learning Representations, 2025b. Yiming Zhang, Shi Feng, and Chenhao Tan. Active example selection for in-context learning. In Proceedings of The 2022 Conference on Empirical Methods in Natural Language Processing, pp. 91349148, 2022. Zheyuan Zhang, Daniel Zhang-Li, Jifan Yu, Linlu Gong, Jinchang Zhou, Zhanxin Hao, Jianxiao Jiang, Jie Cao, Huiqin Liu, Zhiyuan Liu, et al. Simulating classroom education with llm-empowered agents. arXiv preprint arXiv:2406.19226, 2024. Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Lei Shen, Zihan Wang, Andi Wang, Yang Li, et al. Codegeex: pre-trained model for code generation with multilingual In Proceedings of The 29th ACM SIGKDD Conference on benchmarking on humaneval-x. Knowledge Discovery and Data Mining, pp. 56735684, 2023. Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, and Jürgen Schmidhuber. Gptswarm: Language agents as optimizable graphs. In Proceedings of The 41st International Conference on Machine Learning, 2024. 14 Under review as conference paper at ICLR"
        },
        {
            "title": "A USAGE OF LLMS",
            "content": "We use large language models (LLMs) solely as an assistive tool for polishing the writing and improving clarity of exposition. All content generated by LLMs was carefully reviewed, verified, and, where necessary, revised by the authors. The authors take full responsibility for the correctness and integrity of the final manuscript. PROOF OF THEOREM 4.1 Proof. Define the optimal ω = arg minω ac (cid:80)N = (cid:80)N a(C i=1 ω triangle inequality: i=1 ωia(C ) and its corresponding activation ). Then, we can derive the upper bound of the activation difference using the (cid:88) i= ac a(C ) (cid:88) i=1 a(C ) + ac. (18) Using the triangle inequality, we can bound the first term in Eq. (18): (cid:88) i=1 a(C ) = (cid:88) (cid:88) i=1 j=1 (a(C ω ) a(C )) (cid:88) (cid:88) i=1 j=1 (cid:88) (cid:88) i= j=1 (cid:88) (cid:88) i=1 j=1 ω a(C ) a(C ) (cid:88) (cid:88) i= j=1 a(C ) a(C ) a(C ) a(C j) + a(C ) a(C j) + (cid:88) (cid:88) i=1 j=1 (cid:88) (cid:88) i=1 j= a(C j) a(C ) LaC b . Then, we bound the second term in Eq. (18): ac (cid:88) i=1 a(C ω ) ac = (cid:88) i=1 a(C ω ) + (cid:88) i= (a(C ω ) a(C )) ac min ω ac min ω ac min ω ac (cid:88) i=1 (cid:88) i=1 (cid:88) i= ωia(C ) + ωia(C ) + (cid:88) i= (cid:88) i=1 ω (a(C ) a(C )) a(C ) a(C ) ωia(C ) + La (cid:88) i=1 i . (19) (20) In the first line, we add and subtract (cid:80)N ). In the second line, we use the triangle inequality and the definition of ω. In the third line, we scale ω to 1. Finally, we utilize the smoothness property of the activation function to bound the second term Substituting into Eq. (18), we can derive: i=1 a(C (cid:88) i=1 ac a(C ) (cid:88) (cid:16) (cid:88) i=1 j=1 a(C ) a(C j) + (N + 1)LaC b (cid:17) + min ω ac (cid:88) i=1 ωia(C ), (21) Under review as conference paper at ICLR"
        },
        {
            "title": "C DECOUPLED CRITERION FUCNTION",
            "content": "Lemma C.1. Under the assumption of one-block transformer, LLMs activation diversity can be bound by the activation difference between their instructions and responses. a(C ) a(C j) a(I ) a(X ) + a(I j) a(X j) + 18LV nN exp(2ρ2). (22) Proof. We first decompose the activation of context into combination of its components: a(C ) = a(C i ) [a(I ) + a(C j) = a(C j) [a(I j) + (cid:88) k=i (cid:88) k=j a(X t1 ) (N 1)a(P )] + [a(I i ) + a(X t1 ) (N 1)a(P )] + [a(I j) + (cid:88) k=i (cid:88) k=j a(X t1 ) (N 1)a(P )] a(X t1 ) (N 1)a(P )]. (23) By using Theorem D.3, we can derive the upper bound of the activation difference as: a(C ) a(C j) a(C i ) [a(I ) + (cid:88) k=i a(X t1 ) (N 1)a(P )] + a(I i ) a(X t1 ) + a(C j) [a(I j) + (cid:88) a(X t1 ) (N 1)a(P )] + a(I j) a(X t1 a(I i ) a(X t1 k=j ) + a(I j) a(X t1 ) + 18LV nN exp(2ρ2). ) (24)"
        },
        {
            "title": "D USEFUL LEMMA",
            "content": "Lemma D.1. Suppose WV LV and (WKX)T WQX ρ2, the difference between the activation of softmax attention a(X) and linear attention a(X) can be bounded by: a(X) a(X) 3LV exp(2ρ2) (25) where a(X) = WV Xsoftmax(cid:0) (WK X)T WQX (cid:1) and a(X) = WV X(WKX)T WQX. Proof. Define = D1 exp(S) exp(S), where = (WKX)T WQX and = diag(exp(S), 1n). Then, we can derive the upper bound of each element of . ij = exp(Sij) Dii exp(Sij) exp(Sij)( 1 Dii + 1) exp(ρ2)(1 + exp(ρ2)) 2 exp(2ρ2) (26) Summing them up, we can derive: D1 exp(S) exp(S) D1 exp(S) exp(S)F = (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) (cid:88) i=1 j=1 2 ij 2n exp(2ρ2) (27) Define δ = exp(S) S, where = (WKX)T WQX. Then, we derive the upper bound of δ by finding the upper bound of each element of δ. δij = exp(Sij) Sij max{exp(ρ2) ρ2, ρ2 + exp(ρ2)} exp(ρ2) (28) 16 Under review as conference paper at ICLR 2026 Summing them up, we can derive: exp(S) exp(S) SF = (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) (cid:88) i=1 j= exp(Sij) Sij exp(ρ2) (29) After bounding D1 exp(S) exp(S) and exp(S) S, we can derive the upper bound of D1 exp(S) by basic algebra as: D1 exp(S) =D1 exp(S) exp(S) + exp(S) D1 exp(S) exp(S) + exp(S) 2n exp(2ρ2) + exp(ρ2) 3n exp(2ρ2) (30) Therefore, we derive the difference between the activation of softmax attention and linear attention as: WV Xsoftmax (cid:18) (WKX)T WQX (cid:19) WV X(WKX)T WQX WV softmax (cid:18) (WKX)T WQX (cid:19) (WKX)T WQX LV D1 exp(S) 3LV exp(2ρ2) (31) Lemma D.2. Define the activation of linear attention: . = WV [Y, ](WK[Y, ])T WQY . = WV (WKP )T WQP, (32) then the activation of long prompt = [Y1, Y2, . . . , YN ] can be derived by the activation of its component: a(Y ) a(P ) a(Y ) = (cid:88) i=1 a(Yi) (N 1)a(P ). (33) Proof. (cid:16) a(Y ) = WV [Y1, Y2, . . . , YN , ] WK[Y1, Y2, . . . , YN , ] (cid:17)T WQP = WV [Y1, Y2, . . . , YN , ][Y1, Y2, . . . , YN , ]T KWQP = WV = WV = = = (cid:88) i=1 (cid:88) i=1 (cid:88) i=1 (cid:88) (cid:0) i=1 (cid:104) (cid:88) i=1 YiY + (cid:1)W KWQP (YiY + ) (N 1)P (cid:105) KWQP WV (YiY + )W KWQP WV (N 1)P T KWQP WV [Yi, ](WK[Yi, ])T WQP (N 1)WKP (WKP )T WQP a(Yi) (N 1)a(P ). (34) 17 Under review as conference paper at ICLR 2026 Lemma D.3. We derive the difference between the activation of long prompt and combination of the activations of its components = [Y1, Y2, . . . , YN , ] as: a(Y ) (cid:88) i=1 a(Yi) + (N 1)a(P ) 9LV nN exp(ρ2). (35) Proof. Denote the activation of linear attention as a, we can complete the proof by using Theorem D.2 to simplify the relationship of linear attention and Theorem D.1 to bound the difference: a(Y ) (cid:88) i=1 a(Yi) + (N 1)a(P ) =a(Y ) a(Y ) + a(Y ) [ (cid:88) a(Yi) (N 1)a(P )] i=1 a(Yi) (N 1)a(P )] [ + [ (cid:88) i= (cid:88) i=1 a(Yi) (N 1)a(P )] a(Y ) a(Y ) + (cid:88) i= a(Yi) a(Yi) + (N 1)a(P ) a(P ) exp(2ρ2)[3LV nN + 3LV + 3LV (N 1)n] 9LV nN exp(2ρ2) (36) Lemma D.4. If ρ, we have exp(S), 1n exp(ρ) Proof. exp(S), 1n = (cid:88) i=1 exp(Si) min i[n] exp(Si) min i[n] exp(Si) = exp( max i[n] Si) = exp(S) exp(S2) exp(ρ) (37) MULTI-ROUND CONTEXT LEARNING First, we define: h(I ) (I ) . = β (cid:26)a(I +, . = ) a(X t1 ), ) if h(I otherwise. (38) (39) To solve the minimization optimization with inequality constraint, we can construct Lagrangian expression with Lagrange multiplier αT : minimize (I ) s.t. h(I i ) 0 L(I , αt) = (I ) αth(I ). The optimization changes to: min i (I ) = max αt0 min L(I , αt). (40) (41) Under review as conference paper at ICLR 2026 Therefore, to minimize (I is properly minimized and would not become +, the constraint has to be satisfied. ), the dual problem is listed as below. Note that to make sure minI f (I ) (cid:104) min a(I ) a(X t1 (cid:105) ) (I ) = min = max αt0 = max αt0 L(I i , αt) (I ) αth(I ) min min = max αT 0 min i = max αT 0 min (cid:104) a(I ) a(X t1 (cid:104) a(I i ) a(X t1 (cid:105) ) αt (cid:16) β (cid:13) (cid:13)I b (cid:17) (cid:13) (cid:13) ) αT β + αt (cid:13) (cid:13)I i (cid:105) (cid:13) (cid:13) . (42) Define the Q-function for this multi-round multi-agent collaboration as: Q(a(I i ), a(X t1 )) = a(I ) a(X t1 ) + min i a(I t+1 ) a(X ) + αt+1I t+1, i . (43) Here, t+1, one round further back to the round 1: denotes the optimal t+1 . Therefore, the expected return is as follows, when we take (cid:16) (cid:16) min 1 = min 1 a(I 1 ) a(X ) + min a(I ) a(X 1 (cid:17) ) Q(a(I 1 ), a(X 2 )) α T, b (cid:17) = max αT 1>0 = max αT 1> (cid:16) (cid:16) min 1 min 1 Q(a(I 1 ), a(X 2 )) αT 1 (cid:0)β (cid:13) (cid:13)I 1 (cid:13) (cid:1) α (cid:13) T, b (cid:17) Q(a(I 1 ), a(X 2 )) αT 1β + αT 1 (cid:13) (cid:13)I 1 b (cid:17) (cid:13) (cid:13) α T, i . Similar to the previous round, = arg min (cid:104) a(I ) a(X t1 ) αtβ + αt (cid:13) (cid:13)I b (cid:105) (cid:13) (cid:13) α = arg max αt (cid:104) αT (cid:13) (cid:13)I b (cid:13) (cid:13) αtβ (cid:105) . (44) (45) By repeating this process, we can learn the optimal temperature parameter in every round. Hence, the loss functions for θ and α are as follows: (cid:13)a(I (cid:13) (cid:13)I b (cid:13) (cid:13) i) = (cid:13) L(θt L(α) = α(cid:0)β (cid:13) )(cid:13) ) a(X t1 (cid:13) + αt (cid:13) (cid:1). (cid:13) (cid:13)I (46) 19 Under review as conference paper at ICLR"
        },
        {
            "title": "F EXPERIMENT SETUP",
            "content": "F.1 DATASETS We evaluate our method on three areas with 7 datasets which are widely used in prior studies (Dubey et al., 2024; Qian et al., 2025). We elaborate on what follows. MMLU (Hendrycks et al., 2021), comprehensive benchmark covering diverse subjects and difficulty levels, designed to test world knowledge and logical reasoning through multiple-choice questions. MATH (Hendrycks et al., 2021), dataset of challenging competition-level math problems requiring multi-step symbolic reasoning and advanced problem-solving ability. GPQA (Rein et al., 2023), benchmark of graduate-level multiple-choice science questions that assesses deep domain knowledge and reasoning under uncertainty. HumanEval (Chen et al., 2021), widely recognized benchmark for function-level code generation, designed to evaluate fundamental programming skills. ALFWorld (Shridhar et al., 2021), text-based embodied environment featuring household tasks, where agents navigate and interact with objects via natural language commands. SciWorld (Wang et al., 2022), text-based embodied environment for interactive science tasks, requiring agents to navigate rooms, conduct experiments, and perform procedural reasoning. GAIA (Mialon et al., 2024), benchmark of real-world question answering tasks that integrate knowledge retrieval, reasoning, and multi-step tool use. PDDL (Chang et al., 2024), an environment comprising diverse strategic games, where agents must employ PDDL expressions to plan and execute complex tasks. AndroidWorld (Rawles et al., 2025), an environment with 116 dynamic tasks across 20 real-world Android apps, designed to evaluate mobile agents capabilities in app navigation and system-level control. We use 20% of the questions to construct the training dataset and the rest as testing dataset. F.2 BASELINES We test our method against six baselines. We implement them based on their publicly available implementations. Single Execution (Single), querying single LLM to solve the task. Best-of-N sampling (BoN), querying single LLM times and sampling the most correct answer. We set to 32, as increasing further does not yield additional performance benefits and aligns closely with the number of LLM calls used for multi-agent methods. Discussion (Du et al., 2023), multi-agent framework in which LLMs are assigned predefined distinct contexts and iteratively exchange their reasoning processes as additional prompts over multiple rounds, before producing final answer via majority voting. Dynamic LLM-Powered Agent Network (Liu et al., 2024) (DyLAN), discussion-style framework which incorporates an LLM selection algorithm based on an unsupervised metric, namely the Agent Importance Score, which identifies the most contributive LLMs through preliminary trial tailored to the specific task. GPTSwarm (Zhuge et al., 2024), formalizing swarm of autonomous agents as computational graphs, with nodes as manually-customized functions and edges facilitating information flow, adaptively optimizing node prompts and modifying graph connectivity during collective reasoning. Multi-LLM Collaboration Network (Qian et al., 2025) (MacNet), representative framework for decentralized and scalable multi-LLM systems. It introduces edge agents that mediate interactions by generating actionable instructions for the next agent based on the outputs of the previous one. F.3 IMPLEMENTATION DETAILS The context pool is constructed using GPT-4o, where we prompt it to generate large collection of high-quality initial contexts across diverse domains, including mathematics, science, coding, and 20 Under review as conference paper at ICLR embodied reasoning, and various domain-specific sub-contexts are included in each domain. This ensures that the pool provides broad coverage of reasoning perspectives and is shared across all tasks. The context generators are implemented by T5-small model (Raffel et al., 2020) for all the tasks. The dimension of the generated context vectors is 512. The learning rates for the context generators and α are both 1e 4. We use 20% of the questions to train the context initialization and generator. Table 2: Hyperparameters (identical across datasets). Hyperparameter Sentence vector dimension Optimizer Batchsize Learning rate of context Learning rate of α Maximum rounds for discussion Training rounds Size of context pool Value 512 Adam 32 1e-4 1e-4 8 100 100 We implement our code using Pytorch 2.3.0, built upon the open-source parameters of the llama2, Qwen2.5, and Qwen2.5-VL. Models provided at https://huggingface.co/meta-llama and https://huggingface.co/Qwen. All the experiments are run on Ubuntu 22.04.4 LTS with 8 NVIDIA H800 GPUs. F.4 PSEUDOCODE OF M2CL We present the pseudocode of training M2CL in Algorithm 1. It begins with training the context initialization, then the context generators are trained along with the weight α during the discussion. i=1, and {αi}N i=1; Algorithm 1: Pseudocode of M2CL 1 Initialize parameters ϕf , ϕF , {θi}N 2 for each question do 3 4 end 5 for each epoch do 6 Obtain initial contexts {I for each round do }N i=1 via Eq. (12); ϕf ϕf ηf L(ϕf ), ϕF ϕF ηF L(ϕF ); 7 8 10 11 for = 1 to do Generate instructions θi θi ηθL(θi), αi αi ηαL(αi); via Eq. (3) and obtain responses via Eq. (4); end end 12 13 end 21 Under review as conference paper at ICLR"
        },
        {
            "title": "G ADDITIONAL RESULTS",
            "content": "G.1 COMPARISON WITH MORE MODELS To further investigate M2CLs performance, we compare it with best-of-N using stronger closesourced LLMs, including Qwen2.5-Max and GPT-4. As shown in Table 3, llama series models perform worse when fewer LLMs participate but achieve higher accuracy as the number of LLMs increases, demonstrating that M2CL can collaborate weaker models to achieve comparable performance. Table 3: Accuracy using different base models on different datasets. We exhibit the performance advantage with Qwen-max and highlight the best result. Code ALFWorld SciWorld MMLU MATH GPQA Model PDDL GAIA n=1 n= n=8 n=16 n=32 n=64 Qwen-max GPT-4 Llama-7B Llama-13B Llama-70B Qwen-max GPT-4 Llama-7B Llama-13B Llama-70B Qwen-max GPT-4 Llama-7B Llama-13B Llama-70B Qwen-max GPT-4 Llama-7B Llama-13B Llama-70B Qwen-max GPT-4 Llama-7B Llama-13B Llama-70B Qwen-max GPT-4 Llama-7B Llama-13B Llama-70B 86.50.0 84.32.2 45.341.2 54.831.7 68.917.6 87.50.0 86.41.1 59.128.4 86.90.6 95.68. 88.50.0 86.52.0 63.824.7 92.74.2 95.46.9 88.50.0 87.90.6 71.517.0 94.56.0 93.95.4 89.00.0 89.10.1 79.19.9 95.86.8 97.08.0 89.50.0 88.51.0 81.58.0 96.87.3 96.36.8 43.60.0 41.12.5 2.940.7 3.939.7 31.612.0 44.20.0 43.90.3 17.426.8 23.420.8 40.33. 45.00.0 44.60.4 24.920.1 23.022.0 43.51.5 45.00.0 44.80.2 23.921.1 32.712.3 51.56.5 45.50.0 45.40.1 28.816.7 41.44.1 54.59.0 46.00.0 45.40.6 35.410.6 40.06.0 58.012.0 35.70.0 33.32.4 16.818.9 25.010.7 27.68.1 39.20.0 38.80.4 44.75.5 58.919.7 78.739. 40.50.0 38.91.6 59.318.8 70.730.2 87.647.1 40.50.0 40.10.4 69.929.4 88.548.0 91.350.8 41.00.0 41.50.5 84.543.5 94.753.7 95.154.1 41.50.0 41.80.3 82.541.0 93.151.6 96.955.4 65.90.0 62.73.2 15.850.1 23.742.2 35.530.4 67.50.0 67.00.5 32.934.6 47.520.0 70.63. 68.80.0 67.11.7 38.830.0 55.413.4 81.012.2 68.80.0 68.50.3 48.320.5 66.12.7 97.228.4 69.50.0 69.70.2 56.513.0 75.66.1 93.724.2 70.00.0 70.50.5 60.69.4 82.512.5 92.022.0 73.10.0 70.22.9 22.550.6 28.544.6 46.127.0 74.00.0 72.81.2 35.138.9 42.931.1 69.54. 75.00.0 73.21.8 37.237.8 46.128.9 78.93.9 75.00.0 74.30.7 39.835.2 48.726.3 84.99.9 75.50.0 75.00.5 42.533.0 52.223.3 88.513.0 76.00.0 75.20.8 44.231.8 54.821.2 90.814.8 68.70.0 65.43.3 20.148.6 24.544.2 40.228.5 69.80.0 68.01.8 33.036.8 39.929.9 65.54. 71.00.0 69.51.5 36.134.9 44.726.3 76.25.2 71.00.0 70.50.5 38.432.6 47.123.9 81.710.7 71.50.0 71.00.5 40.730.8 50.720.8 86.014.5 72.00.0 71.50.5 42.929.1 52.819.2 88.916.9 61.50.0 58.72.8 15.346.2 18.642.9 29.931.6 62.50.0 61.01.5 25.537.0 30.532.0 49.613. 63.50.0 61.02.5 27.136.4 33.629.9 58.74.8 63.50.0 63.00.5 28.934.6 36.227.3 61.52.0 64.00.0 63.50.5 31.432.6 38.525.5 65.71.7 64.50.0 64.00.5 32.931.6 40.324.2 68.23.7 76.40.0 73.52.9 24.252.2 30.446.0 48.428.0 77.50.0 76.01.5 33.144.4 40.936.6 66.511. 79.00.0 76.03.0 34.744.3 43.835.2 73.65.4 79.00.0 78.20.8 37.042.0 45.533.5 78.10.9 79.50.0 78.51.0 38.940.6 48.031.5 79.80.3 80.00.0 79.20.8 40.939.1 49.830.2 82.22.2 22 Under review as conference paper at ICLR G.2 NUMBER OF LLMS We evaluate M2CLs performance with varying number of LLMs, ranging from 4 to 64. The comparative results are shown in Tables 4 to 8 and Figs. 5 to 9. Summary of key findings. The results show that M2CL significantly improves MAD performance, consistently surpassing existing methods by 20% 50%, particularly in complex tasks like math and tool-using. This highlights its ability to tackle intricate reasoning tasks. M2CL also exhibits more effective multi-agent scaling law, where performance consistently improves as the number of LLMs increases to 64, especially in agentic domains where larger amount of LLMs enhances problem-solving accuracy and efficiency. Table 4: Accuracy with varying number of LLMs on different datasets. The number of LLMs is 4 for all datasets. We exhibit the performance advantage with BoN and highlight the best result. Model Method MMLU MATH GPQA Code ALFWorld SciWorld GAIA Llama-7B Llama-14B Llama-70B Qwen-7B Qwen-14B Qwen-72B Single BoN Debate DyLAN GPTSwarm MacNet M2CL (ours) Single BoN Debate DyLAN MacNet MacNet M2CL (ours) Single BoN Debate DyLAN GPTSwarm MacNet M2CL (ours) Single BoN Debate DyLAN GPTSwarm MacNet M2CL (ours) Single BoN Debate DyLAN GPTSwarm MacNet M2CL (ours) Single BoN Debate DyLAN GPTSwarm MacNet M2CL (ours) 45.35.2 50.50.0 47.33.2 48.12.4 48.02.5 50.80.3 59.18.6 54.816.2 71.00.0 64.56.5 69.71.3 70.30.7 70.30.7 86.915.9 68.914.1 83.00.0 74.88.2 82.10.9 81.41.6 72.910.1 95.612.6 61.213.0 74.20.0 69.34.9 74.20.0 74.50.3 68.06.2 88.714.5 67.212.5 79.70.0 71.78.0 77.52.2 80.10.4 71.28.5 91.411.7 72.511.7 84.20.0 76.67.6 84.60.4 84.90.7 80.33.9 93.59. 2.94.0 6.90.0 6.40.5 6.60.3 5.81.1 7.91.0 17.410.5 3.94.9 8.80.0 8.40.4 10.31.5 9.20.4 9.20.4 23.414.6 31.63.2 34.80.0 29.85.0 31.53.3 31.53.3 32.72.1 40.35.5 12.912.0 24.90.0 17.97.0 23.31.6 24.10.8 20.34.6 40.815.9 21.66.2 27.80.0 24.73.1 26.90.9 26.90.9 24.63.2 43.615.8 31.619.4 51.00.0 48.03.0 61.510.5 60.99.9 48.12.9 69.318. 16.810.7 27.50.0 26.21.3 34.57.0 26.01.5 30.73.2 44.717.2 25.012.7 37.70.0 36.31.4 46.58.8 40.93.2 40.93.2 58.921.2 27.632.0 59.60.0 50.29.4 53.85.8 54.84.8 49.110.5 78.719.1 20.216.2 36.40.0 28.77.7 34.22.2 37.20.8 25.011.4 58.422.0 21.211.6 32.80.0 24.88.0 31.90.9 33.00.2 27.65.2 64.631.8 34.911.0 45.90.0 39.66.3 45.50.4 46.60.7 41.24.7 73.127. 15.86.0 21.80.0 19.82.0 22.20.4 18.92.9 24.02.2 32.911.1 23.78.3 32.00.0 27.94.1 32.50.5 35.23.2 35.23.2 47.515.5 35.512.5 48.00.0 43.24.8 44.53.5 43.74.3 40.87.2 70.622.6 51.211.3 62.50.0 54.58.0 62.50.0 60.42.1 57.15.4 76.814.3 56.712.7 69.40.0 62.76.7 67.91.5 68.11.3 62.96.5 86.517.1 59.113.1 72.20.0 66.75.5 71.30.9 69.72.5 68.14.1 88.316. 22.52.9 25.40.0 25.30.1 24.70.7 25.80.4 27.82.4 35.19.7 28.52.7 31.20.0 30.80.4 31.30.1 31.50.3 31.50.3 42.911.7 46.14.8 50.90.0 50.40.5 49.31.6 51.00.1 52.71.8 69.518.6 24.36.6 30.90.0 29.51.4 29.51.4 29.41.5 29.81.1 36.75.8 30.26.0 36.20.0 35.60.6 33.62.6 34.61.6 36.10.1 45.08.8 49.37.3 56.60.0 58.31.7 54.22.4 55.21.4 57.20.6 73.917. 20.14.8 24.90.0 23.81.1 21.83.1 25.10.2 26.51.6 33.08.1 24.57.2 31.70.0 29.32.4 28.73.0 31.80.1 31.80.1 39.98.2 40.26.5 46.70.0 47.81.1 45.11.6 45.21.5 55.99.2 65.518.8 25.96.6 32.50.0 31.41.1 28.34.2 29.53.0 33.81.3 41.79.2 31.77.7 39.40.0 38.50.9 34.94.5 34.84.6 41.42.0 51.311.9 51.18.7 59.80.0 62.52.7 57.82.0 59.10.7 64.14.3 81.922. 15.32.9 18.20.0 16.51.7 16.81.4 18.80.6 18.80.6 25.57.3 18.63.2 21.80.0 19.82.0 19.42.4 23.41.6 23.41.6 30.58.7 29.92.8 32.70.0 32.40.3 31.51.2 32.50.2 36.53.8 49.616.9 15.34.7 20.00.0 20.60.6 19.01.0 20.00.0 20.50.5 30.210.2 19.24.6 23.80.0 25.51.7 22.71.1 24.50.7 28.64.8 37.213.4 31.47.2 38.60.0 40.72.1 38.20.4 39.00.4 42.03.4 60.321. PDDL 24.22.0 26.20.0 26.20.0 25.50.7 27.91.7 29.33.1 33.16.9 30.42.8 33.20.0 31.91.3 30.42.8 35.32.1 35.32.1 40.97.7 48.44.4 52.80.0 52.90.1 50.32.5 51.51.3 57.04.2 66.513.7 21.14.1 25.20.0 24.30.9 23.02.2 23.91.3 26.61.4 33.07.8 26.42.4 28.80.0 29.91.1 27.81.0 28.30.5 29.40.6 39.710. 41.88.0 49.80.0 47.82.0 45.93.9 48.81.0 49.80.0 65.315.5 23 Under review as conference paper at ICLR 2026 Table 5: Accuracy with varying number of LLMs on different datasets. The number of LLMs is 8 for all datasets. We exhibit the performance advantage with BoN and highlight the best result. Model Method MMLU MATH GPQA Code ALFWorld SciWorld GAIA Llama-7B Llama-14B Llama-70B Qwen-7B Qwen-14B Qwen-72B Single BoN Debate DyLAN GPTSwarm MacNet M2CL (ours) Single BoN Debate DyLAN MacNet MacNet M2CL (ours) Single BoN Debate DyLAN GPTSwarm MacNet M2CL (ours) Single BoN Debate DyLAN GPTSwarm MacNet M2CL (ours) Single BoN Debate DyLAN GPTSwarm MacNet M2CL (ours) Single BoN Debate DyLAN GPTSwarm MacNet M2CL (ours) 45.35.2 50.50.0 49.51.0 54.23.7 49.21.3 56.56.0 63.813.3 54.816.2 71.00.0 66.94.1 82.111.1 81.710.7 81.710.7 92.721.7 68.914.1 83.00.0 82.20.8 93.510.5 93.610.6 82.01.0 95.412.4 61.213.0 74.20.0 71.13.1 74.30.1 76.32.1 71.52.7 92.518.3 67.212.5 79.70.0 77.22.5 86.87.1 87.07.3 78.90.8 93.714.0 72.511.7 84.20.0 82.71.5 91.57.3 91.57.3 83.80.4 95.110. 2.94.0 6.90.0 6.20.7 9.72.8 6.80.1 11.04.1 24.918.0 3.94.9 8.80.0 8.00.8 12.43.6 14.25.4 14.25.4 23.014.2 31.63.2 34.80.0 34.30.5 37.12.3 36.61.8 36.92.1 43.58.7 12.912.0 24.90.0 19.95.0 26.71.8 26.21.3 21.43.5 47.822.9 21.66.2 27.80.0 27.40.4 31.23.4 31.33.5 28.70.9 51.723.9 31.619.4 51.00.0 48.42.6 63.112.1 64.713.7 52.91.9 72.521. 16.810.7 27.50.0 24.82.7 357.5 28.71.2 39.612.1 59.331.8 25.012.7 37.70.0 34.53.2 46.79.0 53.916.2 53.916.2 70.733.0 27.632.0 59.60.0 58.11.5 72.813.2 71.211.6 62.22.6 87.628.0 20.216.2 36.40.0 28.77.7 35.41.0 35.60.8 30.85.6 66.129.7 21.211.6 32.80.0 30.32.5 39.46.6 38.75.9 31.90.9 66.233.4 34.911.0 45.90.0 43.42.5 51.65.7 52.46.5 46.20.3 78.933. 15.86.0 21.80.0 19.22.6 264.2 20.71.1 30.08.2 38.817.0 23.78.3 32.00.0 30.81.2 37.55.5 43.311.3 43.311.3 55.423.4 35.512.5 48.00.0 47.10.9 56.88.8 57.19.1 48.80.8 81.033.0 51.211.3 62.50.0 60.02.5 63.40.9 62.70.2 59.33.2 80.317.8 56.712.7 69.40.0 66.92.5 76.67.2 75.96.5 70.41.0 91.121.7 59.113.1 72.20.0 69.13.1 80.48.2 79.67.4 70.51.7 90.718. 22.63.6 26.20.0 25.90.3 25.01.2 26.20.0 30.44.2 37.211.0 27.84.1 31.90.0 31.70.2 31.70.2 33.41.5 33.41.5 46.114.2 44.18.3 52.40.0 51.70.7 50.22.2 51.41.0 57.34.9 78.926.5 23.87.7 31.50.0 30.60.9 29.81.7 29.91.6 33.62.1 39.98.4 30.17.4 37.50.0 36.80.7 34.82.7 34.92.6 39.21.7 48.210.7 48.29.3 57.50.0 60.42.9 55.12.4 56.80.7 61.84.3 79.021. 19.75.3 25.00.0 24.70.3 22.03.0 26.41.4 30.25.2 36.111.1 24.18.2 32.30.0 30.32.0 29.62.7 35.63.3 35.63.3 44.712.4 39.19.7 48.80.0 50.11.3 46.42.4 46.52.3 64.015.2 76.227.4 25.210.1 35.30.0 32.23.1 29.45.9 30.84.5 37.01.7 45.310.0 31.310.3 41.60.0 39.42.2 36.05.6 35.95.7 46.04.4 56.114.5 50.411.7 62.10.0 65.23.1 58.04.1 60.21.9 68.46.3 88.926. 14.94.0 18.90.0 15.93.0 17.21.7 20.01.1 20.61.7 27.18.2 18.63.9 22.50.0 20.02.5 19.43.1 24.92.4 24.92.4 33.611.1 29.13.8 32.90.0 32.20.7 31.71.2 32.20.7 40.77.8 58.725.8 15.65.5 21.10.0 21.00.1 18.42.7 20.60.5 21.50.4 33.612.5 18.76.9 25.60.0 26.40.8 22.82.8 25.00.6 32.26.6 42.016.4 31.110.1 41.20.0 42.91.7 40.40.8 40.30.9 46.45.2 67.226. PDDL 23.63.1 26.70.0 26.90.2 25.41.3 29.32.6 31.75.0 34.78.0 29.45.4 34.80.0 32.02.8 31.33.5 37.02.2 37.02.2 43.89.0 47.15.7 52.80.0 53.30.5 49.73.1 52.20.6 62.69.8 73.620.8 21.05.3 26.30.0 24.81.5 23.42.9 24.51.8 29.53.2 34.78.4 25.72.8 28.50.0 30.01.5 28.00.5 28.40.1 32.74.2 43.014. 41.010.0 51.00.0 49.11.9 45.55.5 49.71.3 53.72.7 70.519.5 24 Under review as conference paper at ICLR 2026 Table 6: Accuracy with varying number of LLMs on different datasets. The number of LLMs is 16 for all datasets. We exhibit the performance advantage with BoN and highlight the best result. Model Method MMLU MATH GPQA Code ALFWorld SciWorld GAIA Llama-7B Llama-14B Llama-70B Qwen-7B Qwen-14B Qwen-72B Single BoN Debate DyLAN GPTSwarm MacNet M2CL (ours) Single BoN Debate DyLAN MacNet MacNet M2CL (ours) Single BoN Debate DyLAN GPTSwarm MacNet M2CL (ours) Single BoN Debate DyLAN GPTSwarm MacNet M2CL (ours) Single BoN Debate DyLAN GPTSwarm MacNet M2CL (ours) Single BoN Debate DyLAN GPTSwarm MacNet M2CL (ours) 45.35.2 50.50.0 510.5 54.54.0 51.61.1 59.38.8 71.521.0 54.816.2 71.00.0 72.21.2 83.212.2 86.815.8 86.815.8 94.523.5 68.914.1 83.00.0 82.90.1 93.210.2 93.010.0 86.73.7 93.910.9 61.213.0 74.20.0 73.40.8 77.43.2 78.03.8 76.82.6 94.620.4 67.212.5 79.70.0 78.71.0 88.08.3 88.08.3 83.13.4 95.916.2 72.511.7 84.20.0 83.90.3 92.78.5 92.48.2 88.13.9 96.612. 2.94.0 6.90.0 6.40.5 9.93.0 6.90.0 12.55.6 23.917.0 3.94.9 8.80.0 8.20.6 12.43.6 16.07.2 16.07.2 32.723.9 31.63.2 34.80.0 34.60.2 37.22.4 37.42.6 38.94.1 51.516.7 12.912.0 24.90.0 24.10.8 26.01.1 27.62.7 27.82.9 47.422.5 21.66.2 27.80.0 26.71.1 31.94.1 31.84.0 30.83.0 52.724.9 31.619.4 51.00.0 56.15.1 65.814.8 66.015.0 60.69.6 74.323. 16.810.7 27.50.0 26.11.4 35.58.0 29.11.6 45.017.5 69.942.4 25.012.7 37.70.0 37.00.7 47.09.3 59.722.0 59.722.0 88.550.8 27.632.0 59.60.0 58.51.1 73.213.6 69.710.1 68.38.7 91.331.7 20.216.2 36.40.0 35.60.8 38.92.5 41.14.7 40.33.9 72.836.4 21.211.6 32.80.0 33.40.6 41.08.2 40.47.6 35.12.3 69.036.2 34.911.0 45.90.0 44.11.8 53.67.7 53.37.4 49.23.3 80.034. 15.86.0 21.80.0 21.30.5 26.24.4 23.01.2 33.711.9 48.326.5 23.78.3 32.00.0 32.00.0 38.16.1 49.117.1 49.117.1 66.134.1 35.512.5 48.00.0 46.81.2 57.29.2 55.57.5 49.81.8 97.249.2 51.211.3 62.50.0 62.10.4 64.82.3 66.23.7 69.46.9 84.922.4 56.712.7 69.40.0 70.10.7 78.28.8 78.49.0 74.85.4 93.323.9 59.113.1 72.20.0 71.60.6 81.49.2 81.18.9 72.60.4 94.422. 22.44.3 26.70.0 25.71.0 24.62.1 27.20.5 32.86.1 39.813.1 27.45.6 33.00.0 32.01.0 31.91.1 34.81.8 34.81.8 48.715.7 43.710.2 53.90.0 52.21.7 50.53.4 51.72.2 61.67.7 84.931.0 23.49.5 32.90.0 31.71.2 30.42.5 30.12.8 35.42.5 41.88.9 28.39.8 38.10.0 37.70.4 34.93.2 35.52.6 42.14.0 51.213.1 47.710.8 58.50.0 60.82.3 56.12.4 57.31.2 65.87.3 82.924. 19.66.1 25.70.0 25.30.4 22.73.0 26.81.1 33.47.7 38.412.7 24.09.5 33.50.0 30.92.6 30.23.3 37.84.3 37.84.3 47.113.6 38.311.5 49.80.0 51.21.4 46.83.0 47.52.3 70.821.0 81.731.9 24.910.5 35.40.0 32.62.8 28.76.7 30.45.0 39.74.3 47.612.2 30.911.4 42.30.0 40.51.8 35.76.6 36.85.5 49.37.0 59.417.1 50.112.8 62.90.0 66.73.8 59.13.8 59.83.1 74.711.8 94.631. 14.74.5 19.20.0 16.62.6 17.22.0 20.81.6 22.63.4 28.99.7 18.35.0 23.30.0 19.63.7 20.03.3 26.73.4 26.73.4 36.212.9 28.64.6 33.20.0 33.00.2 31.12.1 32.40.8 43.810.6 61.528.3 15.66.4 22.00.0 22.10.1 19.22.8 22.10.1 24.62.6 36.514.5 18.57.6 26.10.0 26.70.6 23.92.2 25.11.0 35.59.4 43.917.8 31.212.1 43.30.0 44.10.8 41.12.2 41.32.0 50.37.0 71.928. PDDL 24.22.1 26.30.0 26.40.1 25.80.5 29.73.4 34.27.9 37.010.7 29.24.6 33.80.0 32.71.1 31.22.6 39.96.1 39.96.1 45.511.7 46.86.2 53.00.0 52.80.2 49.73.3 51.61.4 67.114.1 78.125.1 20.66.5 27.10.0 24.22.9 22.74.4 24.42.7 30.73.6 37.410.3 25.53.8 29.30.0 30.41.1 28.31.0 28.70.6 33.84.5 46.016. 40.811.5 52.30.0 49.33.0 46.36.0 50.61.7 58.15.8 74.322.0 25 Under review as conference paper at ICLR 2026 Table 7: Accuracy with varying number of LLMs on different datasets. The number of LLMs is 32 for all datasets. We exhibit the performance advantage with BoN and highlight the best result. Model Method MMLU MATH GPQA Code ALFWorld SciWorld GAIA Llama-7B Llama-14B Llama-70B Qwen-7B Qwen-14B Qwen-72B Single BoN Debate DyLAN GPTSwarm MacNet M2CL (ours) Single BoN Debate DyLAN MacNet MacNet M2CL (ours) Single BoN Debate DyLAN GPTSwarm MacNet M2CL (ours) Single BoN Debate DyLAN GPTSwarm MacNet M2CL (ours) Single BoN Debate DyLAN GPTSwarm MacNet M2CL (ours) Single BoN Debate DyLAN GPTSwarm MacNet M2CL (ours) 45.35.2 50.50.0 51.81.3 54.74.2 52.41.9 62.712.2 79.128.6 54.816.2 71.00.0 73.12.1 83.612.6 90.719.7 90.719.7 95.824.8 68.914.1 83.00.0 85.72.7 94.511.5 96.413.4 90.07.0 97.014.0 61.213.0 74.20.0 76.82.6 78.24.0 79.65.4 80.66.4 96.021.8 67.212.5 79.70.0 82.73.0 89.39.6 89.09.3 87.88.1 96.116.4 72.511.7 84.20.0 86.52.3 92.88.6 93.18.9 92.58.3 97.513. 2.94.0 6.90.0 7.70.8 10.13.2 7.91.0 14.07.1 28.821.9 3.94.9 8.80.0 9.50.7 12.73.9 17.68.8 17.68.8 41.432.6 31.63.2 34.80.0 35.30.5 37.32.5 37.83.0 41.26.4 54.519.7 12.912.0 24.90.0 26.81.9 29.54.6 29.95.0 31.56.6 47.122.2 21.66.2 27.80.0 29.11.3 32.64.8 32.64.8 33.75.9 58.030.2 31.619.4 51.00.0 62.911.9 67.716.7 68.117.1 72.121.1 74.723. 16.810.7 27.50.0 29.52.0 36.28.7 30.83.3 50.322.8 84.557.0 25.012.7 37.70.0 40.73.0 47.710.0 67.229.5 67.229.5 94.757.0 27.632.0 59.60.0 63.43.8 74.615.0 73.413.8 73.914.3 95.135.5 20.216.2 36.40.0 40.03.6 40.33.9 41.85.4 45.89.4 74.938.5 21.211.6 32.80.0 35.52.7 42.29.4 42.29.4 40.17.3 70.537.7 34.911.0 45.90.0 48.32.4 54.88.9 54.78.8 55.29.3 82.036. 15.86.0 21.80.0 23.51.7 26.74.9 23.71.9 37.215.4 56.534.7 23.78.3 32.00.0 34.32.3 38.76.7 53.921.9 53.921.9 75.643.6 35.512.5 48.00.0 51.23.2 57.89.8 56.48.4 54.16.1 93.745.7 51.211.3 62.50.0 64.31.8 66.54.0 67.85.3 73.711.2 87.324.8 56.712.7 69.40.0 71.82.4 78.99.5 78.99.5 83.013.6 94.925.5 59.113.1 72.20.0 74.52.3 82.710.5 82.210.0 79.97.7 95.923. 22.44.4 26.80.0 26.40.4 24.91.9 27.30.5 35.78.9 42.515.7 27.05.8 32.80.0 31.90.9 32.10.7 37.14.3 37.14.3 52.219.4 43.110.0 53.10.0 52.60.5 50.13.0 52.20.9 64.511.4 88.535.4 24.28.6 32.80.0 31.71.1 30.72.1 31.21.6 38.65.8 44.411.6 29.48.0 37.40.0 39.01.6 35.42.0 36.41.0 44.87.4 54.917.5 46.712.6 59.30.0 62.83.5 56.03.3 58.31.0 70.511.2 89.330. 18.88.1 26.90.0 25.61.3 23.13.8 27.91.0 37.510.6 40.713.8 23.511.3 34.80.0 31.73.1 30.84.0 41.76.9 41.76.9 50.715.9 38.611.5 50.10.0 52.12.0 47.22.9 48.41.7 76.626.5 86.035.9 24.513.3 37.80.0 33.44.4 29.28.6 31.36.5 43.15.3 50.412.6 29.515.6 45.10.0 41.73.4 36.28.9 37.57.6 54.29.1 62.117.0 49.014.2 63.20.0 67.84.6 59.14.1 62.60.6 79.516.3 9531. 14.75.1 19.80.0 15.84.0 17.62.2 21.82.0 23.84.0 31.411.6 18.16.1 24.20.0 19.94.3 19.84.4 29.14.9 29.14.9 38.514.3 28.14.7 32.80.0 32.90.1 31.41.4 32.50.3 46.513.7 65.732.9 14.67.7 22.30.0 23.00.7 19.03.3 22.60.3 26.64.3 39.717.4 18.98.2 27.10.0 28.71.6 23.23.9 26.60.5 39.812.7 49.021.9 30.413.5 43.90.0 46.62.7 42.91.0 42.71.2 54.710.8 79.735. PDDL 23.24.0 27.20.0 26.21.0 26.01.2 30.43.2 37.510.3 38.911.7 28.95.4 34.30.0 33.01.3 31.03.3 42.38.0 42.38.0 48.013.7 46.07.2 53.20.0 53.70.5 49.73.5 51.61.6 71.618.4 79.826.6 20.47.3 27.70.0 25.32.4 22.65.1 25.02.7 34.36.6 40.212.5 24.54.5 29.00.0 30.81.8 29.30.3 28.90.1 36.27.2 49.620. 40.113.2 53.30.0 50.23.1 46.76.6 51.41.9 62.39.0 80.026.7 26 Under review as conference paper at ICLR 2026 Table 8: Accuracy with varying number of LLMs on different datasets. The number of LLMs is 64 for all datasets. We exhibit the performance advantage with BoN and highlight the best result. Model Method MMLU MATH GPQA Code ALFWorld SciWorld GAIA Llama-7B Llama-14B Llama-70B Qwen-7B Qwen-14B Qwen-72B Single BoN Debate DyLAN GPTSwarm MacNet M2CL (ours) Single BoN Debate DyLAN MacNet MacNet M2CL (ours) Single BoN Debate DyLAN GPTSwarm MacNet M2CL (ours) Single BoN Debate DyLAN GPTSwarm MacNet M2CL (ours) Single BoN Debate DyLAN GPTSwarm MacNet M2CL (ours) Single BoN Debate DyLAN GPTSwarm MacNet M2CL (ours) 45.35.2 50.50.0 53.42.9 55.14.6 53.53.0 66.415.9 81.531.0 54.816.2 71.00.0 79.68.6 85.014.0 90.819.8 90.819.8 96.825.8 68.914.1 83.00.0 90.07.0 95.412.4 95.712.7 95.312.3 96.313.3 61.213.0 74.20.0 78.13.9 79.85.6 80.96.7 85.110.9 97.523.3 67.212.5 79.70.0 86.66.9 90.210.5 90.210.5 93.714.0 99.119.4 72.511.7 84.20.0 91.27.0 95.110.9 94.310.1 98.013.8 99.715. 2.94.0 6.90.0 9.22.3 10.43.5 92.1 15.38.4 35.428.5 3.94.9 8.80.0 11.12.3 13.04.2 18.19.3 18.19.3 40.031.2 31.63.2 34.80.0 36.61.8 37.62.8 37.93.1 44.29.4 58.023.2 12.912.0 24.90.0 28.94.0 30.75.8 29.95.0 34.79.8 50.725.8 21.66.2 27.80.0 31.13.3 33.65.8 33.05.2 37.79.9 61.934.1 31.619.4 51.00.0 62.311.3 66.815.8 68.817.8 73.922.9 79.728. 16.810.7 27.50.0 33.56.0 36.99.4 346.5 54.426.9 82.555.0 25.012.7 37.70.0 45.07.3 48.811.1 67.529.8 67.529.8 93.155.4 27.632.0 59.60.0 69.09.4 75.415.8 78.919.3 84.224.6 96.937.3 20.216.2 36.40.0 41.75.3 43.57.1 43.16.7 50.414.0 80.644.2 21.211.6 32.80.0 39.97.1 42.810.0 43.210.4 46.713.9 80.447.6 34.911.0 45.90.0 52.46.5 55.79.8 55.29.3 63.217.3 86.040. 15.86.0 21.80.0 253.2 275.2 25.33.5 40.018.2 60.638.8 23.78.3 32.00.0 36.54.5 39.37.3 55.223.2 55.223.2 82.550.5 35.512.5 48.00.0 54.06.0 58.910.9 58.110.1 57.99.9 92.044.0 51.211.3 62.50.0 66.64.1 67.55.0 68.05.5 79.817.3 90.227.7 56.712.7 69.40.0 75.86.4 80.110.7 80.411.0 91.221.8 98.729.3 59.113.1 72.20.0 78.96.7 83.911.7 84.011.8 86.714.5 97.525. 22.05.2 27.20.0 26.90.3 24.62.6 27.00.2 38.811.6 44.217.0 26.95.7 32.60.0 32.60.0 31.90.7 39.16.5 39.16.5 54.822.2 43.011.8 54.80.0 53.61.2 50.44.4 52.42.4 67.913.1 90.836.0 23.011.5 34.50.0 32.32.2 30.73.8 31.82.7 41.67.1 46.812.3 28.99.5 38.40.0 38.80.4 36.22.2 35.92.5 47.59.1 56.418.0 46.713.4 60.10.0 62.52.4 56.93.2 59.60.5 74.414.3 92.932. 18.87.9 26.70.0 26.30.4 23.33.4 28.82.1 39.612.9 42.916.2 23.612.1 35.70.0 32.03.7 31.04.7 44.48.7 44.48.7 52.817.1 37.313.7 51.00.0 53.52.5 47.63.4 49.31.7 81.630.6 88.937.9 24.414.2 38.60.0 34.14.5 30.08.6 32.75.9 46.78.1 53.314.7 29.915.7 45.60.0 42.72.9 37.08.6 36.09.6 56.811.2 65.820.2 48.317.7 66.00.0 69.13.1 59.96.1 63.22.8 85.119.1 95.929. 15.04.8 19.80.0 16.53.3 17.91.9 22.12.3 27.07.2 32.913.1 17.86.7 24.50.0 19.94.6 19.94.6 31.16.6 31.16.6 40.315.8 28.25.7 33.90.0 32.71.2 30.83.1 32.31.6 49.015.1 68.234.3 14.88.2 23.00.0 23.50.5 19.83.2 23.20.2 28.45.4 42.019.0 17.89.8 27.60.0 29.01.4 24.43.2 26.51.1 42.715.1 51.824.2 29.815.1 44.90.0 47.62.7 43.71.2 43.01.9 59.514.6 84.239. PDDL 22.54.4 26.90.0 26.90.0 25.01.9 31.04.1 38.411.5 40.914.0 28.65.6 34.20.0 32.61.6 30.53.7 44.09.8 44.09.8 49.815.6 45.96.8 52.70.0 53.20.5 50.42.3 51.31.4 75.422.7 82.229.5 20.07.8 27.80.0 26.01.8 23.74.1 25.32.5 37.19.3 42.214.4 24.74.8 29.50.0 31.72.2 28.70.8 29.00.5 38.48.9 50.921. 40.414.0 54.40.0 50.53.9 47.76.7 52.51.9 66.612.2 83.929.5 27 Under review as conference paper at ICLR 2026 Figure 5: Performance of llama-7b as the base model with varying number of LLMs. Uncertainty intervals depict standard deviation over three seeds. M2CL exhibits higher performance and increasing tendency with more LLMs, demonstrating its great collaboration efficiency compared to existing methods. Of note, academic and agentic tasks reasoning are challenging because they require more diverse thinking perspectives and more rigorous analysis. The outperformance of M2CL reveals its capability of enabling LLMs to collaborate in changing discussion state. Figure 6: Performance of llama-13b as the base model with varying number of LLMs. Uncertainty intervals depict standard deviation over three seeds. M2CL exhibits higher performance and increasing tendency with more LLMs, demonstrating its great collaboration efficiency compared to existing methods. Under review as conference paper at ICLR 2026 Figure 7: Performance as llama-70b as the base model with varying number of LLMs. Uncertainty intervals depict standard deviation over three seeds. M2CL exhibits higher performance and increasing tendency with more LLMs, demonstrating its great collaboration efficiency compared to existing methods. Figure 8: Performance of Qwen-7b as the base model with varying number of LLMs. Uncertainty intervals depict standard deviation over three seeds. M2CL exhibits higher performance and increasing tendency with more LLMs, demonstrating its great collaboration efficiency compared to existing methods. 29 Under review as conference paper at ICLR 2026 Figure 9: Performance of Qwen-14b as the base model with varying number of LLMs. Uncertainty intervals depict standard deviation over three seeds. M2CL exhibits higher performance and increasing tendency with more LLMs, demonstrating its great collaboration efficiency compared to existing methods. Figure 10: Performance of Qwen-72b as the base model with varying number of LLMs. Uncertainty intervals depict standard deviation over three seeds. M2CL exhibits higher performance and increasing tendency with more LLMs, demonstrating its great collaboration efficiency compared to existing methods. 30 Under review as conference paper at ICLR 2026 G.3 GUI AGENT We evaluate M2CLs performance on more challenging AndroidWorld, which requires GUI identification, long-horizon planning, and accurate action execution capability. Comparative results are shown in Table 9 and Fig. 11. We observe that M2CL consistently outperforms existing baselines across different model scales up to 50%, with performance gains becoming more pronounced as the number of participating LLMs increases. The superior scalability of M2CL in this setting highlights its ability to exploit diverse responses and maintain consistent under complex, real-world style interactions. N=4 N=8 N=16 N=32 N= Model Single BoN Debate DyLAN GPTSwarm MacNet M2CL 3B 7B 3B 7B 3B 7B 3B 7B 3B 7B 10.96.1 27.011.0 10.96.1 27.011.0 10.96.1 27.011.0 10.96.1 27.011.0 10.96.1 27.011.0 17.00.0 38.00. 17.00.0 38.00.0 17.00.0 38.00.0 17.00.0 38.00.0 17.00.0 38.00.0 12.54.5 30.57.5 15.71.3 33.05. 16.90.1 37.30.7 18.01.0 41.53.5 20.13.1 41.73.7 13.04.0 31.07.0 15.21.8 34.53.5 18.41.4 37.80. 21.54.5 40.02.0 23.66.6 42.24.2 15.02.0 37.01.0 15.61.4 38.00.0 18.11.1 42.54.5 20.63.6 45.07. 24.27.2 49.311.3 16.80.2 39.61.6 19.72.7 44.06.0 22.65.6 47.99.9 25.28.2 52.814.8 28.611.6 55.217. 25.58.5 44.96.9 32.015.0 50.012.0 38.021.0 55.017.0 42.025.0 59.021.0 45.028.0 62.024.0 Table 9: Accuracy with varying number of LLMs from 4 to 64 on AndroidWorld. We exhibit the performance advantage with BoN and highlight the best result. Figure 11: Performance of Qwen2.5-VL (3B and 7B) as the base model with varying number of LLMs. M2CL exhibits higher performance and increasing tendency with more LLMs, demonstrating its great collaboration efficiency compared to existing methods. Uncertainty intervals depict standard deviation over three seeds. 31 Under review as conference paper at ICLR 2026 G.4 CONTEXT CONSTRAINT To assess the effect of context constraint, we carry out experiments by varying the context constraint β from 0 to 10. As illustrated in Figs. 12 to 16, larger value of β results in high degree of consistency among LLMs, leading them to produce similar answers. Conversely, smaller value of β is associated with reduced collaboration among LLMs. Therefore, it is important to adjust β to control the degree of consistency among LLMs for better collaboration. Figure 12: Performance with varying context constraint when 4 LLMs participate. All the curves display the same trend of rising first and then falling, which is consistent with our theory. Figure 13: Performance with varying context constraint when 8 LLMs participate. 32 Under review as conference paper at ICLR 2026 Figure 14: Performance with varying context constraint when 16 LLMs participate. Figure 15: Performance with varying context constraint when 32 LLMs participate. Under review as conference paper at ICLR 2026 Figure 16: Performance with varying context constraint when 64 LLMs participate. 34 Under review as conference paper at ICLR 2026 G.5 DISCREPANCY INTENSITY BY ROUNDS To validate that M2CL can collaborate LLMs to reach an agreement by rounds, we visualize the discrepancy intensity. As illustrated in Figs. 17 to 21, the initial discrepancy intensity of M2CL is higher as its context initialization can make the discussion more creative. The discrepancy intensity of M2CL increases faster because the dynamic adjustment of the context provides LLMs with better ability to effectively receive information from other LLMs, resulting in reduced disagreement and faster collaboration among LLMs to reach consensus. Figure 17: Comparative results on discrepancy intensity with varying model size (from top to bottom correspond to 7B, 14B, and 70B). The number of agents is set as 4. Lower values represent lower degree of disagreement. M2CL can improve consistency with fewer rounds. Of note, M2CL displays both lower initial value and faster decreasing speed, indicating its capability of assigning appropriate contexts based on the given question and current discussion situation. 35 Under review as conference paper at ICLR 2026 Figure 18: Comparative results on discrepancy intensity with varying model size (from top to bottom correspond to 7B, 14B, and 70B). The number of agents is set as 8. Lower values represent lower degree of disagreement. M2CL can improve consistency with fewer rounds. Figure 19: Comparative results on discrepancy intensity with varying model size (from top to bottom correspond to 7B, 14B, and 70B). The number of agents is set as 16. Lower values represent lower degree of disagreement. M2CL can improve consistency with fewer rounds. Under review as conference paper at ICLR 2026 Figure 20: Comparative results on discrepancy intensity with varying model size (from top to bottom correspond to 7B, 14B, and 70B) The number of agents is set as 32. Lower values represent lower degree of disagreement. M2CL can improve consistency with fewer rounds. Figure 21: Comparative results on discrepancy intensity with varying model size (from top to bottom correspond to 7B, 14B, and 70B). The number of agents is set as 64. Lower values represent lower degree of disagreement. M2CL can improve consistency with fewer rounds. 37 Under review as conference paper at ICLR 2026 G.6 TRANSFERABILITY OF CONTEXTS To further study the generalization of the generated contexts, we implement the multi-agent system using GPT-4 as the base model with the context generator trained on llama-7B and compare its performance with using initial contexts. As illustrated in Table 10, the generated contexts outperform initial contexts, indicating that the trained context generator can be expanded to more models for improving overall performance through LLMs collaboration. Dataset Method MMLU MATH GPQA Code ALFWorld SciWorld GAIA = 4 = 8 = = 32 = 64 Initial Generated Initial Generated Initial Generated Initial Generated Initial Generated 84.30.0 95.010.7 86.40.0 98.211.8 87.90.0 98.510.6 89.10.0 98.89.7 88.50.0 98.910. 41.10.0 58.016.9 43.90.0 68.524.6 44.80.0 70.325.5 45.40.0 72.026.6 45.40.0 72.827.4 33.30.0 52.118. 38.80.0 62.523.7 40.10.0 64.324.2 41.50.0 66.024.5 41.80.0 66.925.1 62.70.0 81.018.3 67.00.0 89.122. 68.50.0 90.321.8 69.70.0 91.321.6 70.50.0 91.721.2 70.20.0 82.011.8 72.80.0 82.59.7 74.30.0 84.510. 75.00.0 86.011.0 75.20.0 86.511.3 65.40.0 76.511.1 68.00.0 77.89.8 70.50.0 78.78.2 71.00.0 79.58. 71.50.0 80.28.7 58.70.0 65.46.7 61.00.0 64.33.3 63.00.0 66.13.1 63.50.0 66.83.3 64.00.0 67.23. PDDL 73.50.0 85.211.7 76.00.0 88.212.2 78.20.0 90.011.8 78.50.0 91.212.7 79.20.0 91.512. Table 10: Performance of transferring generated contexts trained on llama-7B to GPT-4 with varying number of LLMs. G.7 ABLATION STUDIES AND COMPLEMENTARY EXPERIMENTS G.7.1 TRAINING CONVERGENCE OF CONTEXT GENERATOR We verify the convergence of solving Problem (16) by displaying the cumulative utility. As shown in Fig. 22, it works well in all dataset and often converges in 60 training steps. Figure 22: The value of L(θ) when training llama-7B with 8 LLMs participating. G.7.2 ABLATION In this section, we assess the effect of key components by ablating them under the same setting. Without context initialization, LLMs fail to develop specialized expertise, resulting in homogeneous policies that inefficiently duplicate effort, demonstrating poor adaptability when faced with novel questions. Tables 11 to 13 underscore the importance of context initialization before discussion to enhance foundational multi-agent capabilities. Without tuning α during discussion rounds, LLMs tend to reach an agreement in the first round, leading to responses that lack creativity and diversity, which ultimately reduces problem-solving ability. Tables 11 to 13 underscore the importance of tuning α during discussion rounds for training. 38 Under review as conference paper at ICLR 2026 Without context evolution, LLMs receive no guidance consider inter-LLM dependencies, making it difficult for them to fully utilize the information contributed by other LLMs. This context misalignment leads to discussion inconsistency and poor collaboration efficiency. Tables 11 to 13 highlight the necessity of context evolution for enabling effective inter-LLM collaboration and achieving consistent improvements across discussion rounds. Dataset Method MMLU MATH GPQA Code ALFWorld SciWorld GAIA = 4 = 8 = 16 = = 64 M2CL w/o init. w/o α w/o evolve M2CL w/o init. w/o α w/o evolve M2CL w/o init. w/o α w/o evolve M2CL w/o init. w/o α w/o evolve M2CL w/o init. w/o α w/o evolve 59.110.6 56.47.9 53.95.4 48.50.0 63.812.9 63.812.9 57.46.5 50.90.0 71.518.5 70.917.9 60.57.5 53.00.0 95.841.3 77.723.2 64.49.9 54.50.0 81.525.3 81.225.0 63.37.1 56.20.0 17.49.9 13.35.8 9.41.9 7.50. 24.916.8 17.29.1 12.94.8 8.10.0 23.915.7 22.614.4 14.26.0 8.20.0 41.431.6 27.818.0 18.18.3 9.80.0 35.423.6 30.518.7 19.27.4 11.80.0 44.716.7 44.516.5 37.89.8 28.00.0 59.331.1 54.926.7 43.014.8 28.20. 69.939.4 69.438.9 46.415.9 30.50.0 94.759.7 83.548.5 50.015.0 35.00.0 82.544.1 80.642.2 49.711.3 38.40.0 32.911.8 31.310.2 26.04.9 21.10.0 38.817.6 37.216.0 29.88.6 21.20.0 48.324.3 45.321.3 32.88.8 24.00. 75.648.8 53.226.4 34.88.0 26.80.0 60.632.0 57.228.6 37.08.4 28.60.0 35.18.8 31.85.5 22.53.8 26.30.0 37.210.2 34.77.7 22.74.3 27.00.0 39.812.7 36.39.2 26.60.5 27.10.0 42.514.5 40.412.4 32.54.5 28.00. 44.215.6 40.712.1 31.73.1 28.60.0 33.08.3 30.15.4 21.13.6 24.70.0 36.110.3 33.17.3 26.81.0 25.80.0 38.411.8 35.38.7 23.72.9 26.60.0 40.713.6 36.79.6 28.61.5 27.10.0 42.914.9 39.511.5 32.94.9 28.00. 25.58.1 23.25.8 20.32.9 17.40.0 27.110.1 24.57.5 20.93.9 17.00.0 28.911.1 26.28.4 22.24.4 17.80.0 31.414.0 28.811.4 24.67.2 17.40.0 32.914.8 30.412.3 24.66.5 18.10.0 PDDL 33.16.2 30.53.6 23.43.5 26.90.0 34.77.0 31.53.8 24.82.9 27.70.0 37.09.5 33.96.4 24.92.6 27.50.0 38.911.4 35.37.8 28.40.9 27.50.0 40.912.6 37.49.1 27.80.5 28.30.0 Table 11: Ablation study on context initialization, tuning α, and context evolution when using llama-7B with varying number of LLMs. Dataset Method MMLU MATH GPQA Code ALFWorld SciWorld GAIA = 4 = 8 = 16 = = 64 M2CL (ours) w/o init. w/o α w/o evolve M2CL (ours) w/o init. w/o α w/o evolve M2CL (ours) w/o init. w/o α w/o evolve M2CL (ours) w/o init. w/o α w/o evolve M2CL (ours) w/o init. w/o α w/o evolve 86.920.2 86.720.0 81.514.8 66.70.0 92.723.2 92.523.0 85.415.9 69.50.0 94.922.9 94.722.7 87.215.2 72.00.0 95.521.8 95.321.6 87.613.9 73.70.0 95.821.6 95.621.4 87.813.6 74.20.0 23.413.5 16.97.0 13.23.3 9.90. 23.013.5 21.912.4 16.97.4 9.50.0 27.316.2 25.013.9 18.77.6 11.10.0 27.915.3 25.713.1 19.16.5 12.60.0 28.114.9 25.912.7 19.46.2 13.20.0 58.920.3 57.919.3 50.912.3 38.60.0 70.732.6 70.332.2 53.915.8 38.10. 73.233.8 72.332.9 57.718.3 39.40.0 74.032.3 73.031.3 58.516.8 41.70.0 74.831.3 73.530.0 59.115.6 43.50.0 47.517.6 45.215.3 37.37.4 29.90.0 55.422.1 53.320.0 43.310.0 33.30.0 61.525.3 60.424.2 47.611.4 36.20. 63.124.6 62.023.5 48.19.6 38.50.0 63.523.5 62.222.2 48.68.6 40.00.0 42.910.9 40.78.7 32.10.1 32.00.0 46.113.0 45.312.2 34.71.6 33.10.0 49.715.3 48.514.1 36.31.9 34.40.0 50.815.5 49.414.1 36.71.4 35.30. 51.215.2 49.613.6 37.01.0 36.00.0 39.99.5 47.817.4 31.91.5 30.40.0 44.713.0 51.820.1 37.65.9 31.70.0 48.915.6 54.521.2 39.96.6 33.30.0 49.914.7 55.220.0 40.55.3 35.20.0 50.313.9 55.619.2 40.84.4 36.40. 30.59.6 33.712.8 24.23.3 20.90.0 33.612.2 38.417.0 26.24.8 21.40.0 36.012.9 39.116.0 27.03.9 23.10.0 36.612.3 39.815.5 27.43.1 24.30.0 36.911.9 40.115.1 27.72.7 25.00.0 PDDL 40.98.1 37.04.2 24.38.5 32.80.0 43.810.6 39.96.7 29.04.2 33.20.0 45.210.2 41.86.8 31.04.0 35.00.0 46.710.5 43.06.8 31.74.5 36.20.0 47.110.3 43.36.5 32.04.8 36.80.0 Table 12: Ablation study on context initialization, tuning α, and context evolution when using llama-13B with varying number of LLMs. 39 Under review as conference paper at ICLR 2026 Dataset Method MMLU MATH GPQA Code ALFWorld SciWorld GAIA = 4 = = 16 = 32 = 64 M2CL (ours) w/o init. w/o α w/o evolve M2CL (ours) w/o init. w/o α w/o evolve M2CL (ours) w/o init. w/o α w/o evolve M2CL (ours) w/o init. w/o α w/o evolve M2CL (ours) w/o init. w/o α w/o evolve 95.618.7 95.418.5 86.79.8 76.90.0 95.411.9 95.111.6 94.010.5 83.50.0 93.99.9 93.29.2 93.29.2 84.00.0 97.010.2 96.810.0 95.28.4 86.80. 96.35.7 96.15.5 95.75.1 90.60.0 40.39.4 39.99.0 36.96.0 30.90.0 43.58.3 43.07.8 39.34.1 35.20.0 51.515.2 47.311.0 40.54.2 36.30.0 54.517.3 51.514.3 44.06.8 37.20.0 58.019.3 53.715.0 43.34.6 38.70. 78.725.6 88.535.4 72.319.2 53.10.0 87.626.6 86.325.3 78.017.0 61.00.0 91.329.5 90.929.1 78.616.8 61.80.0 95.128.5 94.827.9 83.917.3 66.60.0 96.925.1 96.925.1 82.811.0 71.80.0 70.624.7 67.821.9 53.98.0 45.90. 81.030.5 80.029.5 63.813.3 50.50.0 97.245.4 96.945.1 72.220.4 51.80.0 93.738.2 93.437.9 68.613.1 55.50.0 92.034.2 91.733.9 69.611.8 57.80.0 69.517.2 64.812.5 43.39.0 52.30.0 78.924.5 75.821.4 49.35.1 54.40. 84.929.4 81.325.8 64.2-8.7 55.50.0 88.532.3 85.829.6 57.3-1.1 56.20.0 90.833.5 88.731.4 66.9-9.6 57.30.0 65.515.9 62.913.3 47.12.5 49.60.0 76.223.5 71.718.5 49.03.7 52.70.0 81.727.4 79.323.0 64.810.5 54.30. 86.030.5 81.826.3 57.92.4 55.50.0 88.931.9 85.528.5 70.313.3 57.00.0 49.615.5 47.012.9 38.0-0.1 34.10.0 58.723.8 56.121.2 42.67.7 34.90.0 61.525.6 56.620.7 47.011.1 35.90.0 65.729.5 61.224.3 50.914.7 36.20. 68.232.0 65.028.8 41.85.6 36.20.0 PDDL 66.512.2 62.68.3 40.314.0 54.30.0 73.618.3 71.316.0 47.87.5 55.30.0 78.122.8 73.518.2 59.34.0 55.30.0 79.823.5 77.120.8 61.35.0 56.30. 82.226.1 77.521.4 55.6-0.5 56.10.0 Table 13: Ablation study on context initialization, tuning α, and context evolution when using llama-70B with varying number of LLMs. 40 Under review as conference paper at ICLR 2026 G.7.3 RUN-TIME To demonstrate the efficiency of our context initialization, we verify its runtime with varying the number of LLMs. Then, we evaluate the runtime of M2CL compared with baseline algorithms Figure 23: Runtime of initialization. Uncertainty intervals depict standard deviation over three seeds. for average testing time, utilizing the same model size on 8 NVIDIA H800 GPUs. As illustrated by Fig. 24, the runtime of M2CL is slightly longer than other multi-LLM discussion methods as the runtime of context generators is negligible compared with the inference time of LLM, which substantiates the low computational cost of M2CL. Figure 24: Runtime when varying the size of the LLama series models. The number of LLMs is 8. Uncertainty intervals depict standard deviation over three seeds. 41 Under review as conference paper at ICLR"
        },
        {
            "title": "H CASE STUDY",
            "content": "We used problem from the MATH Hendrycks et al. (2021). The number of LLMs is set as 8. For each LLM, we present their instructions, responses, and final answers for 4 discussion rounds. H.1 CASE STUDY OF M2CL (OURS) We provide the case of M2CL solving the problem. As illustrated in Figs. 25 to 28, we observe that the generated contexts evolve progressively to enforce stricter collaboration among LLMs. Initially, the instructions merely encourage LLMs to pay attention to others responses. In subsequent rounds, they guide LLMs to cross-check each others answers and eventually require reaching full agreement. Correspondingly, the LLMs outputs transition from diverse, potentially conflicting answers to single, consistent solution. This behavior demonstrates that our method effectively modulates context evolution to guide toward consensus without sacrificing initial creativity. 42 Under review as conference paper at ICLR 2026 Figure 25: Visualization of M2CL at the first round. 43 Under review as conference paper at ICLR 2026 Figure 26: Visualization of M2CL at the second round. We highlight the guidance on how to cooperate with other LLMs. At the beginning, instructions encourage diverse perspectives and consideration of others responses, but the requirements for discussion consistency are not yet strict. 44 Under review as conference paper at ICLR 2026 Figure 27: Visualization of M2CL at the third round. We highlight the guidance on how to cooperate with other LLMs. As the discussion progresses, the instructions increasingly enforce stricter requirements for cross-checking and aligning answers, helping the models converge toward consistent solution. 45 Under review as conference paper at ICLR 2026 Figure 28: Visualization of M2CL at the last round. We highlight the guidance on how to cooperate with other LLMs. Although the initial round produced divergent answers, the collaborative instructions enable LLMs to exchange and integrate information, ultimately reaching correct consensus. 46 Under review as conference paper at ICLR 2026 H.2 CASE STUDY OF DE E We also provide the case of Debate solving the problem. As illustrated in Figs. 29 to 32, we observe an inconsistency during discussion when using fixed instructions. Figure 29: Visualization of Debate at the first round. 47 Under review as conference paper at ICLR 2026 Figure 30: Visualization of Debate at the second round. Under review as conference paper at ICLR 2026 Figure 31: Visualization of Debate at the third round. 49 Under review as conference paper at ICLR 2026 Figure 32: Visualization of Debate at the last round."
        }
    ],
    "affiliations": [
        "College of Computer Science, Northwest University",
        "Department of Computer Science and Technology, Tsinghua University",
        "School of Cyber Science and Technology, Sun Yat-sen University",
        "Zhongguancun Laboratory"
    ]
}