{
    "paper_title": "TLDR: Token-Level Detective Reward Model for Large Vision Language Models",
    "authors": [
        "Deqing Fu",
        "Tong Xiao",
        "Rui Wang",
        "Wang Zhu",
        "Pengchuan Zhang",
        "Guan Pang",
        "Robin Jia",
        "Lawrence Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Although reward models have been successful in improving multimodal large language models, the reward models themselves remain brutal and contain minimal information. Notably, existing reward models only mimic human annotations by assigning only one binary feedback to any text, no matter how long the text is. In the realm of multimodal language models, where models are required to process both images and texts, a naive reward model may learn implicit biases toward texts and become less grounded in images. In this paper, we propose a $\\textbf{T}$oken-$\\textbf{L}$evel $\\textbf{D}$etective $\\textbf{R}$eward Model ($\\textbf{TLDR}$) to provide fine-grained annotations to each text token. We first introduce a perturbation-based method to generate synthetic hard negatives and their token-level labels to train TLDR models. Then we show the rich usefulness of TLDR models both in assisting off-the-shelf models to self-correct their generations, and in serving as a hallucination evaluation tool. Finally, we show that TLDR models can significantly speed up human annotation by 3 times to acquire a broader range of high-quality vision language data."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 ] . [ 1 4 3 7 4 0 . 0 1 4 2 : r TLDR: Token-Level Detective Reward Model for Large Vision Language Models Deqing Fu1,2,, Tong Xiao1, Rui Wang1, Wang Zhu1,2,, Pengchuan Zhang1, Guan Pang1, Robin Jia2, Lawrence Chen1 1Meta, 2University of Southern California Work done at Meta Although reward models have been successful in improving multimodal large language models, the reward models themselves remain brutal and contain minimal information. Notably, existing reward models only mimic human annotations by assigning only one binary feedback to any text, no matter how long the text is. In the realm of multimodal language models, where models are required to process both images and texts, naive reward model may learn implicit biases toward texts and become less grounded in images. In this paper, we propose Token-Level Detective Reward Model (TLDR) to provide fine-grained annotations to each text token. We first introduce perturbation-based method to generate synthetic hard negatives and their token-level labels to train TLDR models. Then we show the rich usefulness of TLDR models both in assisting off-the-shelf models to self-correct their generations, and in serving as hallucination evaluation tool. Finally, we show that TLDR models can significantly speed up human annotation by 3 times to acquire broader range of high-quality vision language data. Date: October 6, 2024 Correspondence: Deqing Fu at deqingfu@usc.edu; Lawrence Chen at lawrencechen@meta.com"
        },
        {
            "title": "1 Introduction",
            "content": "Large vision language models (VLMs) are becoming increasingly powerful in generating human-like text, exemplified by models like GPT-4 family (OpenAI, 2024), Gemini and PaliGemma (Google, 2023; Beyer et al., 2024), LLaVA model family (Liu et al., 2024), and Llama 3 Vision models (Meta, 2024a). However, they are far from perfect and still suffer from generating hallucinated texts that are not grounded to the reference image. The need for accurate and interpretable reward models (RMs) to highlight the mistakes becomes increasingly critical. Traditional RMs, which are often binary classification models to provide one single score to evaluate entire outputs, have limitations in terms of interpretability and granularity. These models obscure the decision-making process of the model, making it challenging for humans to diagnose and improve performance at fine-grained level. To facilitate better interpretability and granularity, we propose Token-Level Detective Reward (TLDR) model to offer more interpretable alternative. By evaluating and assigning rewards at each token, rather than across entire sequences, TLDR enables greater transparency. This fine-grained approach allows for clearer identification of where model excels in its output generation. Such interpretability is crucial not only for aligning model behavior with human expectations but also for improving human-AI interaction human annotator can swiftly fix the highlighted errors given by TLDR to make them correct because token-level evaluations allow for quicker identification of errors and more targeted improvements. Additionally, naive binary reward model could be biased towards text modalities the longer the text, the higher the score, despite any internal hallucinations, making them less effective in multimodal contexts where visual information is essential. Our work aims to address this by constructing reward model that is more visually grounded, incorporating multimodal cues to better evaluate model performance. The interpretability afforded by token-level granularity helps facilitate this grounding, ensuring that visual and textual signals are both considered in reward calculations. Ablation studies in training TLDR models shown in Tables 2 and 9 verifies the claim by showing the sharp improvement of the RMs performance by further finetuning the linear 1 Figure 1 Token-Level Detective Reward (TLDR) Model. It can be used as hallucination detection, and to prompt models to self-correct with these detection. TLDR can also speed up human annotation speed to fix slightly mistaken image captions, to create high-quality vision language data. projection of the VLM projecting visual features given by the vision encoder to the textual embedding space. Moreover, token-level reward models have the potential to enhance existing methods for model improvement. By providing detailed feedback on token-by-token basis, these models enable more effective self-correction and refinement in generated outputs. more granular understanding of errors can improve the performance of fine-tuning techniques such as DPO (Rafailov et al., 2024) and PPO (Schulman et al., 2017), where strong and interpretable reward signals are essential for optimizing model behavior. In Section 5.4, we show the TLDR model is automatically likelihood training objective, that simultaneously improve the base vision language model behind the RM. In summary, TLDR model aims to develop reward model that not only reflects human preferences more accurately but also enhances the usability and interpretability. By improving the transparency of the reward mechanism at the token level, we provide tool that facilitates faster feedback, better self-correction, simple likelihood finetuning, and trustworthy hallucination evaluation metric."
        },
        {
            "title": "2 Related Work",
            "content": "Reinforcement Learning from Human Feedback and Reward Model. Using Reinforcement Learning to align language models with human feedback or preferences (RLHF, Christiano et al., 2017; Ziegler et al., 2020) has 2 led to phenomenal improved large language models such as ChatGPT (Ouyang et al., 2022) and LLaMA 3 (Meta, 2024a). Similar RLHF techniques are used to align large vision language models Liu et al. (2023) and text-to-image models (Lee et al., 2023; Sun et al., 2023a) as well. In general, RLHF involves training reward model on user preference data collected from human annotators (Wang et al., 2024). Given reward model, policy can be learned using Reinforcement learnings algorithms like Proximal Policy Optimization (PPO, Schulman et al., 2017). Alternatively, recent works have developed Direct Policy optimization (DPO, Rafailov et al., 2024) wherein reward models are mainly used for finding chosen and rejected pairs. Reward models themselves are also evolving including process reward models (Luo et al., 2023), step-wise reward models (Havrilla et al., 2024), etc. Recent works also attempt span-level or token-level detection but they are limited to the language domain (Yoon et al., 2024; Yang et al., 2024), and they are mostly sentence-level (Wu et al., 2023; Niu et al., 2024; Mishra et al., 2024) or need factual augmentations (Sun et al., 2023b). Synthetic Data and Hard Negative Mining. Several NLP datasets have gathered instances of the negative class for their task. Many relied on human annotation, for example, unsupported claims in fact verification (Aly et al., 2021; Wadden et al., 2020), non-entailed hypotheses in NLI (Bowman et al., 2015), unanswerable questions (Rajpurkar et al., 2018). Some have used heuristics and external knowledge sources to automatically mine negative examples (Lee et al., 2021; Wright et al., 2022). Finally, there are hybrid approaches where candidate negative examples are first automatically generated and then manually verified (Wadden et al., 2022), or candidate negative examples are synthesized by model perturbation and verified by the same model (Fu et al., 2023). Large Vision Language Models and Evaluation. There has been plethora of recent developments in VLMs; they can be broadly categorized by their methods for representing visual modalities. representative approach involves tokenizing visual inputs to be jointly trained with language inputs (Yu et al., 2023; Google, 2023; Chameleon, 2024). Another line of work processes continuous visual features by directly projecting them to the language embedding space via learnable function (Liu et al., 2024; Bavishi et al., 2023). At the core of these design choices is the hardness in representing visual features, which has been reported by several early studies (McKinzie et al., 2024) to be the key bottleneck towards better vision-language foundation models. Various benchmark datasets beyond MMMU (Yue et al., 2024) were proposed targeting these bottlenecks, such as BLINK (Fu et al., 2024b) and Vibe-Eval (Padlewski et al., 2024) for visual reasoning, IsoBench (Fu et al., 2024a) and MathVista (Lu et al., 2024) for algorithmic visual problem solving. At the essence of common mistakes made by VLMs, hallucination is significant portion, and thus TLDR is designed as an evaluation tool to measure models hallucination rate (see Table 3)."
        },
        {
            "title": "3 Problem Setup",
            "content": "A multimodal query-response instance = (m, p, d) is usually equipped with three elements, an image m, user text prompt p, and text response d. Training reward model involves training classifier ρ(m, p, d) {0, 1} to predict human preference on the target response given the image and prompt p. In contrast, to have better granularity for the reward model, token-level version is needed. Instead of training point-wise scalar classifier, which only assigns singular value to the instance x, it assigns values for every token of the target response = {e1, , eN } with = tokens in total, where denotes the number of tokens of any text sequence. It involves training the TLDR model Pγ to match fine-grained rewards. TLDR models prediction can be written as γ(m, p, d) = (cid:16) γ(e1 m, p, d), , γ(eN m, p, d) (cid:17) [0, 1]N (1) where for any token in text response d, γ(e m, p, d) = (cid:40) if Pγ(e mk, pk, dk) > θ 1, 0, otherwise , with some threshold θ usually set to 0.5 if not otherwise mentioned. Given the image, prompt, and response tuples (mk, pk, dk) in an evaluation set S, we design three accuracy metrics on the TLDR models performance. Token-Level Accuracy. For each instance xk = (mk, pk, dk), we are given true token-level labels γ(mk, pk, dk) = 3 (cid:16) (cid:17) γ(e1), , γ(eN ) . Then we define token-level accuracy as AT(γ, S) = 1 (cid:88) (mk,pk,dk)S 1 dk (cid:88) edk 1 {γ(e) = γ(e mk, pk, dk)} (2) Sentence-Level Accuracy. Similar to the token-level accuracy but with each response broken into sentences = {s1, , sc(d)} with sj = {enj1+1, , enj } where nj is the token position of the j-th period. We also let n0 = 0 as the starting position. We also c() as the function of counting number of sentences in any text response d. Then we define sentence-level accuracy as AS(γ, S) = 1 (cid:88) (mk,pk,dk)S 1 c(dk) (cid:88) j1, ,c(dk) 1 (cid:89) esj γ(e) = (cid:89) esj γ(e mk, pk, dk) visual illustration on grouping tokens into sentences is shown in Figure 3. Response-Level Accuracy. We define response level prediction as ργ(m, p, d) = (cid:89) ed γ(e m, p, d) and ρ(m, p, d) = γ(e) (cid:89) ed Then we compare with the ground truth labels ρ to define response-level accuracy as AR(ργ, S) = 1 (cid:88) (mk,pk,dk)S 1 {ργ(mk, pk, dk) = ρ(mk, pk, dk)} (3) (4) (5) Notably, the response-level accuracy also applies to response-level naive reward models ρ under the same definition. We will further compare the response-level accuracy of TLDR model AR(ργ, S) and that of the naive model AR(ρ, S) in Section 5. Besides accuracy metrics, which could be biased when labels are imbalanced, especially in the token-level cases where majority of the tokens are neutral tokens words that wont affect response quality, we report mean Average Precision (mAP). Together with normal mAP metrics, for which we call mAP(pos) we also design flipped version mAP(neg), where both the predicted labels and the ground-truth labels are flipped so that we pay more attention to tokens with negative labels. Because the token-level annotations are highly imbalanced, with more than 95% of them are positive tokens (see the example in Figure 1), the mAP(neg) is more meaningful average precision metric. We report both mAP scores in the tuple format mAP(negpos)."
        },
        {
            "title": "4 Synthetic Data Generation",
            "content": "Although aligning models toward user preference has become standard post-training procedure, open-sourced user preference data, especially multimodal ones, are increasingly difficult to source. What is even worse, existing user preference data are mostly coarsely annotated as each instance is only given one label: thumb down or thumb up. To gather large amounts of fine-grained token-level preference data, we adopt the procedure by perturbing gold labels, inspired by Fu et al. (2023). In this work, we mainly focus on two types of tasks: dense captioning and visual question answering (VQA). For VQA data, we synthesize hard negatives from Visual Genome (VG100K) dataset (Krishna et al., 2016), which contains 108,077 images with over 1.7 million question-answer pairs. For any VQA instance with image m, question and answer d, we prompt pretrained large language model ϕ, which takes the original question and answer d, and is instructed to generate perturbed answer = ϕ(p, d) so that its the wrong answer to the question given the image m, i.e., P(d m, p) = 0. Notably, the model used for perturbation ϕ is text-only, without seeing the image to mitigate any visual biases. In this work, we use Llama-3.1-70B (Meta, 2024a) as the perturbation model ϕ. Admittedly, as dense captions are relatively longer than VQA samples, the amount of data available is more limited, and the hard negative synthesis process could be more versatile and more complicated. We 4 synthesize hard negatives from DOCCI dataset (Onoe et al., 2024), which contains over 15,000 images and their corresponding dense captions. However, the amount of captions here is much fewer than the amount of VQA instances. To compensate the imbalance, we use Llama-3.1-70B to aggregate VQA instances to dense captions. For each image in the VG100K dataset, its equipped with on average 16 question-answer pairs {(p1, d1), , (pk, dk)}. The text-only LLM is prompted to combine them into dense caption for image m. Now, combining DOCCI and VG100Ks synthetic caption, we have over 120,000 image-caption pairs. As prior work identifies (Lin et al., 2024), vision language models usually suffer in the following eight taxonomies. We enumerate each of them with an illustrated example pair. I. Spatial Relationship: is left to is right to B. II. Visual Attribute: is yellow. is blue. III. Attribute Binding: is blue and is yellow. is yellow and is blue. IV. Object Identification: dog chasing ball. cat chasing ball. V. Counting: One duck is swimming. Four ducks are swimming. VI. Small Object: Cirrostratus cloud in the sky. Clear sky. VII. Text OCR: shirt writes heavy fog. shirt writes happy frog. VIII. Counterfactual: soldier. soldier has no sword in hand. For each taxonomy t, and for each instance with image and caption d, we use prompt-engineered text-only LLM ϕt to generate perturbed caption = ϕt(d) so that is minimal-edit from d. Furthermore, we prompt-engineer another LLM ϕc(d, d, t) to check they are not paraphrases and their difference lies in the desired taxonomy t. If they fail ϕc(d, d, t), we discard the perturbation. For instance, not every image has text written in it, so there is no way to generate perturbations focused on text OCR. d}, we compute k) {0, 1} depending appear in the neighborhood of ek in or not. Since the original text is human written, all of For either VQA or dense caption tasks, once we obtain successful perturbation = {e the differences to the original text = {e1, , ed} to obtain the token-level label γ(e on whether its tokens have positive label γ(ek) = 1, ek d. We include prompts for perturbation in Appendices A.1 to A.3 and the statistics of our synthetic data in Table 7 at Appendix A.4. 1, , e"
        },
        {
            "title": "5.1 Training TLDR Models\nModel Architecture. As shown in Figure 2, we use PaliGemma-3B-Mix-448 (Beyer et al., 2024) as our\nbackbone pretrained large Vision Language Model f . Instead of using the pretrained language modeling head ℓ\nwhich maps the last hidden states to the vocabulary logits, we train a new reward model head h : RDhidden → R\nto map the last hidden states with dimension Dhidden to a scalar logit for each token from the response d. For\nany instance equipped with image m, prompt p and response d, we denote | · | as the number of tokens after\ntokenization for either image or text modality. The backbone language model gives the last hidden states\nH = f (m, p, d) ∈ R(|m|+|p|+|d|)×Dhidden and the probability of being positive for k-th token ek in the response\nd is given by",
            "content": "Pγ(ek m, p, d) = σ (cid:0)h (cid:0)H,(m+p+k) (cid:1)(cid:1) , where σ is the Sigmoid function. (6) In our setup, the reward model head is simply linear layer with (Dhidden + 1) parameters. We provide more detailed training setups and hyperparameters in Appendix B. Training. As the PaliGemma report (Beyer et al., 2024) suggests, PaliGemma used full attention between input images and input texts, and only has autoregressive attention at generation. Similar to LLaVA model family (Liu et al., 2024), PaliGemma model has four major components: 400M SigLIP (Zhai et al., 2023) Figure 2 TLDR Model Architecture. For any instance with image m, prompt p, and response d, they are passed altogether into the large VLM backbone (in our case, PaliGemma-3B) without the language model head ℓ. Then shared reward model head is applied to every token ek of the response to have binary predictions γ(ek) to determine if ek is good token or bad token. Reward Model TLDR (ours) Naive Token-Level Accuracy AT 98.6 Sentence-Level Accuracy AS 86.5 Response-Level Accuracy AR 83.1 81.1 mAP (negpos) (41.399.8) Table 1 Performance of the TLDR model. As reference, we include scores for the response-level naive reward model trained on the same dataset. We find that the TLDR model has slightly higher response-level accuracy when compared to the naive binary RM. break down of response-level accuracy by taxonomy is shown in Table 9. vision encoder fenc, linear projection module fproj to align the vision features to the proper text embedding spacing, Gemma-2B (Google, 2024) Transformer decoder fdec, and language model head ℓ. We train our randomly initialized reward model head h, together with fproj and fdec. For efficient training, we use LoRA (Hu et al., 2021) technique to update weights Θproj of fproj and Θdec of fdec, so that Θ = Θ + αtrainAB. We choose αtrain = 128 and := rank(A) = rank(B) = 512 for all submodules Θ. Models are trained with respect to the cross-entropy objective on every token of the response d, compared to the token-level label generated following Section 4. In contrast, we compare with naive reward model trained on the same training data and with the same hyperparameters. Although sharing the same architecture and parameter count, the naive reward model differs from the TLDR model as its cross-entroy loss is only computed at the last token of each response d, instead of on all tokens. Evaluation. We evaluate TLDR models performance on the synthetic data generated from the test split of the DOCCI dataset (Onoe et al., 2024). We measure the performance based on the metrics discussed in Section 3. As shown in Table 1, the TLDR model has slightly higher response-level accuracy than the naive binary RM. TLDR model has 41.3 mAP(neg) and signals further room for improvements. break-down of response-level taxonomy in Table 9 at appendix shows that, TLDR model performs the worst one spatial relationship taxonomy, and this resonances prior work that image grounding to spatial relationship is one of the hardest task for both image-to-text VLMs and text-to-image generations (Lin et al., 2024). We conduct further human evaluation on token-level predictions on 100 samples from WinoGround (Thrush et al., 2022) images with captions generated by MiniCPM (Yao et al., 2024). With special focus on false negative (FN) type of errors and averaged among three human annotators, we find the TLDR model has sentence-level FN rate of 8.7%. 6 Figure 3 Level of Granularity in Hallucination Rate. Using the example from Figure 1, we can easily compute token-level hallucination rates following Eq. 7. Then tokens are grouped into sentences which are separated by period marks. An entire sentence with at least one bad token is highlighted as bad sentence. Then the sentence-level hallucination rate of one response is calculated by counting the proportion of bad sentences. Similarly, if there is at least one bad token in the response, the entire response is bad one. Hallucination rates are averaged over an entire evaluation set to determine the overall hallucination rates of model. Linear Projection fproj Gemma Decoder fdec Token-Level Accuracy AT Sentence-Level Accuracy AS Response-Level Accuracy AR 98.6 97.4 98.3 86.5 80.0 84.8 83.1 52.5 79. mAP (negpos) (41.399.8) (18.299.2) (38.299.7) Table 2 Ablation Study. It shows that unfreezing gradients on both linear projection layer fproj and the transformer decoder layers fdec are meaningful. Without tuning fdec, the model barely works, and without fproj, as shown in Table 9, the model is less visually grounded, especially on spatial relationship and counting. Ablation Study. As discussed earlier, we finetune randomly initialized reward model head h, together with LoRA efficiently finetuning the linear projection layer fproj and the Transformer decoder fdec. In the section, we ablate the necessity of LoRA finetuning fproj and fdec. As shown in Table 2, both linear projection module and the decoder module are worth training and they work together to facilitate the performance of our TLDR model. One interesting observation is training fproj on top of fdec barely improves token-level accuracy or mAP(pos). But it helps with both sentence-level and response-level accuracy, and increases mAP(neg) significantly. This could be explained by that finetuning fproj reduces the models false negative rates on tokens because its more visually grounded by tuning the projection from visual space to textual space."
        },
        {
            "title": "5.2 Hallucination Evaluation with TLDR Models",
            "content": "Since our TLDR model provides token-level predictions, we can use it to compute models hallucination rates without requiring ground truth labels. Given image m, prompt p, and model for evaluation ξ. We first obtain models response ˆd = ξ(m, p) with tokens {ˆe1, , ˆe ˆd}. Then our TLDR model gives its prediction γ(m, p, ˆd) = . Then the token-level hallucination rate for this instance is ˆek ˆd γ(ˆek m, p, ˆd). Similar to sentence-level and response-level accuracy defined in Section 3, we can 1 ˆd have sentence-level and response-level hallucination rates as well. Their definitions are shown in Figure 3. γ(ˆe1 m, p, ˆd), , γ(ˆe ˆd m, p, ˆd) (cid:80) (cid:16) (cid:17) Now we can have the token-level hallucination rates for the model ξ given any dataset as follows HT (ξ, S) = 1 (cid:88) (m,p)S 1 ξ(m, p) (cid:124) (cid:123)(cid:122) (cid:125) ˆd (cid:88) ˆek ˆd γ(ˆek m, p, ˆd) (7)"
        },
        {
            "title": "Models",
            "content": "GPT-4o Llama-3.2-90B-Vision GPT-4o-mini GPT-4-Turbo-Vision MiniCPM-Llama-3-V2.5 Llama-3.2-11B-Vision Phi-Vision-3.5-Instruct PaliGemma-3B-Mix-448 Token-Level Hallucination Rate (%) Sentence-Level Response-Level 0.016 0.017 0.030 0.033 0.067 0.073 0.261 4.444 0. 0.19 0.38 0.62 0.81 0.85 2.65 5.96 1.62 1.23 2.12 3.12 3.62 1.88 9.25 17.50 MMMU 69.1 60.3 59.4 56.8 45.8 50.7 43.0 27.3 Table 3 Hallucination Evaluation. We prompt each model with image captioning instructions on 800 images from WinoGround (Thrush et al., 2022), and use TLDR to compute the hallucination rates (the lower the better), with respect to various levels of granularity. Model performances are sorted by token-level rate. We observe that GPT-4o is overall the best model with the least token-level hallucinations but Llama-3.2-90B-Vision has better sentence-level and response-level hallucinations. We also include self-reported MMMU (Yue et al., 2024) results to demonstrate their significant correlation with hallucination rates: the Pearson correlation between log HT and MMMU score is 0.892 with p-value of 0."
        },
        {
            "title": "Guidance Given By",
            "content": "# Samples # Samples Flagged by RM # Self-Corrected Win Tie Loss TLDR (ours) Naive 800 21 15 12 2 7 11 2 2 Table 4 Self-Correction with the Guidance of TLDR Model. GPT-4V is used to generate captions for 800 images from WinoGround, 25 of the captions are flagged by the reward model as containing bad tokens. When prompted to self-correct, extra guidance from TLDR helps GPT-4V correct more of its own hallucinations with larger win rates. We evaluate Llama-3.2-Vision (Meta, 2024b) with 11B and 90B versions, GPT-4o, 4o-min and GPT-4 turbo vision (OpenAI, 2024), MiniCPM (Yao et al., 2024), PaliGemma (Beyer et al., 2024) with an image resolution of 448x448, and Phi 3.5 Vision (Abdin et al., 2024) with our TLDR Model. As shown in Table 3, GPT-4o is overall the best model with the least amount of hallucinations among all granularity. We also observe strong correlation between models hallucinate rates and its visual understanding and reasoning performance evaluated by MMMU. We conjecture for any VLM ξ, Performance(ξ) log HT (ξ) (8)"
        },
        {
            "title": "5.3 Self-Correction with TLDR Models",
            "content": "Hallucination detection and evaluation is small leap forward. The most exciting usage of TLDR model is to guide models with self-correction and to be more grounded to the images by taking another guided look. Token-level annotations from TLDR model can enhance vision-language models ability to self-correct by providing detailed, granular feedback on specific parts of its output. These annotations allow the model to break down its response into smaller, interpretable units, aligning each token with visual and textual cues. By analyzing where errors occur at the token levelwhether in object recognition, attribute descriptions, or language syntaxthe model can more precisely identify the source of the mistake. Additionally, token-level feedback can guide the model to better align its language generation with the visual context, improving coherence and factual accuracy in its self-correction process. In this section, we evaluate on WinoGround (Thrush et al., 2022) dataset to show whether given extra token-level annotation cues, the vision language model is able to self-correct its own hallucinations. Out of 800 captions generated by GPT-4V for images in WinGround, TLDR model flags 25 of them as including 8 Figure 4 TLDR model can guidance existing VLMs to Self-Correct their hallucination when generating captions for images from WinoGround (Thrush et al., 2022). hallucinated tokens. As shown in Table 4, when prompted with TLDR guidance, GPT-4V attempts to self-correct 21 out of 25, and when evaluated by human annotators, 12 of them are improved, 7 of them are tied, and 2 of them are worsened. On the contrary, when prompted to self-correct without TLDRs guidance, GPT-4V attempts to self-correct 15 out of 25 with only 2 wins, 11 ties, and 2 losses. Examples of GPT-4Vs self-correction results are shown in Figure 4. We include both prompt templates for self-correction, with and without TLDRs guidance in Appendix C."
        },
        {
            "title": "5.4 TLDR Automatically Trains Token-Level Likelihood Optimization",
            "content": "The purpose of building reward models is to improve the backbone large vision language model. We find that, free by-product of training the TLDR model is that the backbone models weights are simultaneously updated together with the reward model head. As discussed in Section 5, the linear projection fproj and the transformer decoder Gemma-2B fdec are both updated with LoRA weights during training TLDR. Now we attach back the original pretrained language model head ℓ to the backbone of the updated PaliGemma model with TLDR (by discarding the reward model head h), we obtain an updated vision language model. Now we evaluate whether this new model has improved from the orginal model, by evaluating on both in-distribution tasks from BLINK (Fu et al., 2024b) and out-of-distribution tasks from IsoBench (Fu et al., 2024a). At inference time, we adopt different LoRA alpha to merge the weights, for weight Θ for any updated module, Θ = Θ + αinferAB, where A, are trained LoRA weights. We find the proportion between αinfer and αtrain could affect model performance significantly. We denote this proportion τ := αinfer/αtrain [0, 1]. With τ = 0, we are evaluating the original model before training TLDR, and with τ = 1, we are evaluating the model trained to provide token-level rewards. As shown in Table 5, τ = 0.5 gives the best performance. Such automatic improvement is in fact that TLDR simultaneously trains the backbone VLM with likelihood optimization. The binary cross entropy objective on P(e m, p, d) for any token simply promotes the model to generate if γ(e) = 1, i.e., is good token; and suppresses the model to generate it if γ(e) = 0, i.e., is bad token. As the linear projection layer fproj is also finetuned, the model is promoted to be more visually grounded, with an improvement for spatial relationship and chess winner identification, both of which requires complex spatial reasoning on images."
        },
        {
            "title": "Models",
            "content": "PaliGemma-3B + TLDR (τ = 0.25) + TLDR (τ = 0.5) + TLDR (τ = 1)"
        },
        {
            "title": "Chess Winner\nIdentification",
            "content": "69.2 71.7 71.7 12.5 78.3 80.4 81.1 2.1 41.4 45.1 44.3 34. 45.1 45.1 47.5 44.8 Table 5 Training TLDR model automatically gives better vision language model. We evaluate 3 versions of TLDR backbone model with different scales of LoRA α. They are distinguished by τ = αinfer/αtrain, the proportion of α at inference and training time. We find that when τ = 0.5, i.e., scale the α down by half at inference time, could improve the backbone models performance by at most 3.7 points. Annotator ID Average Annotation Speed (seconds) with Guidance of Binary Reward Model TLDR Model (ours) Annotator Annotator Annotator Average 101.7 109.1 121.3 110.7 31.2 32.9 34. 32.8 Table 6 TLDR Speeds up Human Annotation by 3 Times to Fix Synthetic Image Caption in PixelProse (Singla et al., 2024)"
        },
        {
            "title": "5.5 Speeding Up Human Annotation with TLDR Models",
            "content": "Human annotations, especially on dense captions, are costly and model generated captions are not trustworthy. Recent work such as PixelProse (Singla et al., 2024) has started releasing model generated dense captions with an ambition to use these captions to train better models. However, on random sampling of 3,000 images from PixelProse, TLDR model detects 22.39% of the captions have hallucinated tokens, with token-level hallucination rate of 0.83% and sentence-level hallucination rate of 5.23%. Nonetheless, its always easier and cheaper for human annotators to correct an existing caption than writing long captions from scratch. Instead of using models self-correction as designed in Section 5.3, human correction could be more rigorous to provide better captions. Human annotators are given the similar instruction as model self-correction prompts. Instead of comparing their caption correction quality corrected captions are later cross checked by annotators to ensure quality annotators are asked to time their annotation speed. Each annotator is assigned two set of distinct samples, one with TLDR guidance and the other without, and is asked to fix the caption and time themselves. As shown in Table 6, all three annotators share similar annotation speed when if given TLDR guidance or no extra guidance. Most importantly, all three annotators have 3 times speed up, which could in the future help with creating large bulk of vision language data with lower annotation costs."
        },
        {
            "title": "6 Discussion and Conclusion",
            "content": "In this paper, we introduced Token-Level Detective Reward Model (TLDR) to provide fine-grained annotations for large vision language models. It is more interpretable than traditional naive binary reward models as TLDR can inform the user not only the response could be wrong but also where the response is wrong. Such feature enables many meaning usages, such as models self-correction with TLDR guidance, and hallucination evaluation with TLDR annotations. We also presented naive baseline where TLDR model is automatically likelihood optimization method for its backbone vision language model, and the TLDR-tuned VLM is able 10 to improve in several benchmarks. We believe strong RM is crucial basis for token-level DPO and PPO post-training. Finally, we show that, beyond guiding models to self-correct, TLDR can also assist human annotators to fix synthetically generated image captions by improving the annotation speed by 3 times. Avenues ahead, we aim to design better human annotation interfaces under the human-computer interaction realm, to further reduce annotation overhead, to acquire large amount of high quality image captioning data with both positive human-corrected caption and negative model-generated caption. It will further facilitate us designing token-level policy optimization methods. Meanwhile, we would like to scale up the TLDR model size from currently 3B to much larger size to see if larger backbone VLM gives better TLDR model. We believe our approach has the potential to advance the field of reward modeling and automatic evaluation. We hope that TLDR will make data annotation easier, and guide multimodal LLMs to hallucinate less. We hope our work can inspire further research to rethink the roles of reward models, and how their transparency, interpret ability, and granularity can help advance the field of building better multimodal foundation models."
        },
        {
            "title": "Acknowledgement",
            "content": "DF would like to thank Yuanzhe Richard Pang for initial discussion on token-level predictions and automated model critiques for large language models. The detective and robot figures used in Figures 1 and 4 are from flaticon.com. Getty Acknowledgement. Images in the paper that originated from the WinoGround dataset (Thrush et al., 2022) are compilation of assets, including Getty Images/Natasha Breen, Maki Nakamura, Jessica Peterson, Kundanlall Sharma, lacaosa, Alberto Bogo, Vu Le, Toson Rueangsuksut, Nisian Hughes, Tanja Walter, Douglas Sacha, PBNJ Productions, Glow Images, 10000 Hours, zoranm, Marlene Ford, Westend61."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Qin Cai, Vishrav Chaudhary, Dong Chen, Dongdong Chen, Weizhu Chen, Yen-Chun Chen, Yi-Ling Chen, Hao Cheng, Parul Chopra, Xiyang Dai, Matthew Dixon, Ronen Eldan, Victor Fragoso, Jianfeng Gao, Mei Gao, Min Gao, Amit Garg, Allie Del Giorno, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Wenxiang Hu, Jamie Huynh, Dan Iter, Sam Ade Jacobs, Mojan Javaheripi, Xin Jin, Nikos Karampatziakis, Piero Kauffmann, Mahoud Khademi, Dongwoo Kim, Young Jin Kim, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Liden, Xihui Lin, Zeqi Lin, Ce Liu, Liyuan Liu, Mengchen Liu, Weishung Liu, Xiaodong Liu, Chong Luo, Piyush Madan, Ali Mahmoudzadeh, David Majercak, Matt Mazzola, Caio César Teodoro Mendes, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Liliang Ren, Gustavo de Rosa, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Yelong Shen, Swadheen Shukla, Xia Song, Masahiro Tanaka, Andrea Tupini, Praneetha Vaddamanu, Chunyu Wang, Guanhua Wang, Lijuan Wang, Shuohang Wang, Xin Wang, Yu Wang, Rachel Ward, Wen Wen, Philipp Witte, Haiping Wu, Xiaoxia Wu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Jilong Xue, Sonali Yadav, Fan Yang, Jianwei Yang, Yifan Yang, Ziyi Yang, Donghan Yu, Lu Yuan, Chenruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 technical report: highly capable language model locally on your phone, 2024. https://arxiv.org/abs/2404.14219. Rami Aly, Christos Christodoulopoulos, Oana Cocarascu, Zhijiang Guo, Arpit Mittal, Michael Schlichtkrull, James Thorne, and Andreas Vlachos, editors. Proceedings of the Fourth Workshop on Fact Extraction and VERification (FEVER), Dominican Republic, November 2021. Association for Computational Linguistics. https://aclanthology. org/2021.fever-1.0. Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sağnak Taşırlar. Introducing our multimodal models, 2023. https://www.adept.ai/blog/fuyu-8b. Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, Thomas Unterthiner, Daniel Keysers, Skanda Koppula, Fangyu Liu, Adam Grycner, Alexey Gritsenko, Neil Houlsby, Manoj Kumar, Keran Rong, Julian Eisenschlos, Rishabh Kabra, Matthias Bauer, Matko Bošnjak, Xi Chen, Matthias Minderer, Paul Voigtlaender, Ioana Bica, Ivana Balazevic, Joan Puigcerver, Pinelopi Papalampidi, Olivier Henaff, Xi Xiong, Radu Soricut, Jeremiah Harmsen, and Xiaohua Zhai. Paligemma: versatile 3b vlm for transfer, 2024. https://arxiv.org/abs/2407.07726. Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. large annotated corpus for learning natural language inference. In Lluís Màrquez, Chris Callison-Burch, and Jian Su, editors, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632642, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1075. https://aclanthology.org/D15-1075. Chameleon. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Paul Francis Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. ArXiv, abs/1706.03741, 2017. https://api.semanticscholar.org/CorpusID:4787508. Deqing Fu, Ameya Godbole, and Robin Jia. SCENE: Self-labeled counterfactuals for extrapolating to negative examples. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 78327848, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.485. https://aclanthology.org/2023.emnlp-main.485. Deqing Fu, Ruohao Guo, Ghazal Khalighinejad, Ollie Liu, Bhuwan Dhingra, Dani Yogatama, Robin Jia, and Willie Neiswanger. IsoBench: Benchmarking multimodal foundation models on isomorphic representations. In First Conference on Language Modeling (COLM), 2024a. First four authors contributed equally. Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. arXiv preprint arXiv:2404.12390, 2024b. Google. Gemini: family of highly capable multimodal models, 2023. Google. Gemma: Open models based on gemini research and technology, 2024. https://arxiv.org/abs/2403.08295. Alex Havrilla, Sharath Raparthy, Christoforus Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, 12 and Roberta Raileanu. Glore: When, where, and how to improve llm reasoning via global and local refinements, 2024. https://arxiv.org/abs/2402.10963. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. https://arxiv.org/abs/2106.09685. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International Journal of Computer Vision, 123:32 73, 2016. https://homes.cs.washington.edu/ranjay/visualgenome/index.html. Jinhyuk Lee, Mujeen Sung, Jaewoo Kang, and Danqi Chen. Learning dense representations of phrases at scale. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 66346647, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.518. https://aclanthology.org/2021.acl-long.518. Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human feedback, 2023. https: //arxiv.org/abs/2302.12192. Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan. Evaluating text-to-visual generation with image-to-text generation. ArXiv, abs/2404.01291, 2024. https://api. semanticscholar.org/CorpusID:268857167. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. https://arxiv.org/abs/ 2304.08485. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. https://llava-vl.github.io/blog/2024-01-30-llava-next/. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts, 2024. https://arxiv.org/abs/2310.02255. Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct, 2023. https://arxiv.org/abs/2308.09583. Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Methods, analysis & insights from multimodal llm pre-training. arXiv preprint arXiv:2403.09611, 2024. Meta. The llama 3 herd of models, 2024a. https://arxiv.org/abs/2407.21783. Llama Team @ Meta. Llama 3.2: Revolutionizing edge ai and vision with open, customizable models, 2024b. https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/. Abhika Mishra, Akari Asai, Vidhisha Balachandran, Yizhong Wang, Graham Neubig, Yulia Tsvetkov, and Hannaneh Hajishirzi. Fine-grained hallucination detection and editing for language models, 2024. https://arxiv.org/abs/2401. 06855. Cheng Niu, Yuanhao Wu, Juno Zhu, Siliang Xu, Kashun Shum, Randy Zhong, Juntong Song, and Tong Zhang. Ragtruth: hallucination corpus for developing trustworthy retrieval-augmented language models, 2024. https: //arxiv.org/abs/2401.00396. Yasumasa Onoe, Sunayana Rane, Zachary Berger, Yonatan Bitton, Jaemin Cho, Roopal Garg, Alexander Ku, Zarana Parekh, Jordi Pont-Tuset, Garrett Tanzer, Su Wang, and Jason Baldridge. Docci: Descriptions of connected and contrasting images, 2024. https://arxiv.org/abs/2404.19753. OpenAI. Gpt-4o, 2024. https://openai.com/index/hello-gpt-4o/. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. https://arxiv.org/abs/2203.02155. 13 Piotr Padlewski, Max Bain, Matthew Henderson, Zhongkai Zhu, Nishant Relan, Hai Pham, Donovan Ong, Kaloyan Aleksiev, Aitor Ormazabal, Samuel Phua, Ethan Yeo, Eugenie Lamprecht, Qi Liu, Yuqi Wang, Eric Chen, Deyu Fu, Lei Li, Che Zheng, Cyprien de Masson dAutume, Dani Yogatama, Mikel Artetxe, and Yi Tay. Vibe-eval: hard evaluation suite for measuring progress of multimodal language models, 2024. https://arxiv.org/abs/2405.02287. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model, 2024. https://arxiv.org/abs/2305.18290. Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you dont know: Unanswerable questions for SQuAD. In Iryna Gurevych and Yusuke Miyao, editors, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784789, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-2124. https://aclanthology.org/P18-2124. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. https://arxiv.org/abs/1707.06347. Vasu Singla, Kaiyu Yue, Sukriti Paul, Reza Shirkavand, Mayuka Jayawardhana, Alireza Ganjdanesh, Heng Huang, Abhinav Bhatele, Gowthami Somepalli, and Tom Goldstein. From pixels to prose: large dataset of dense image captions, 2024. https://arxiv.org/abs/2406.10328. Jiao Sun, Deqing Fu, Yushi Hu, Su Wang, Royi Rassin, Da-Cheng Juan, Dana Alon, Charles Herrmann, Sjoerd van Steenkiste, Ranjay Krishna, and Cyrus Rashtchian. Dreamsync: Aligning text-to-image generation with image understanding feedback, 2023a. https://arxiv.org/abs/2311.17946. Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, and Trevor Darrell. Aligning large multimodal models with factually augmented rlhf. 2023b. Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. Winoground: Probing vision and language models for visio-linguistic compositionality. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 52285238, 2022. David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. Fact or fiction: Verifying scientific claims. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7534 7550, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.609. https://aclanthology.org/2020.emnlp-main.609. David Wadden, Kyle Lo, Bailey Kuehl, Arman Cohan, Iz Beltagy, Lucy Lu Wang, and Hannaneh Hajishirzi. Scifact-open: Towards open-domain scientific claim verification, 2022. https://arxiv.org/abs/2210.13777. Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang, Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi, Songyang Gao, Nuo Xu, Yuhao Zhou, Xiaoran Fan, Zhiheng Xi, Jun Zhao, Xiao Wang, Tao Ji, Hang Yan, Lixing Shen, Zhan Chen, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang, Zuxuan Wu, and Yu-Gang Jiang. Secrets of rlhf in large language models part ii: Reward modeling, 2024. https://arxiv.org/abs/2401.06080. Dustin Wright, David Wadden, Kyle Lo, Bailey Kuehl, Arman Cohan, Isabelle Augenstein, and Lucy Lu Wang. Generating scientific claims for zero-shot scientific fact checking. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 24482460, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.175. https://aclanthology.org/2022.acl-long.175. Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A. Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better rewards for language model training, 2023. https://arxiv.org/abs/2306.01693. Kailai Yang, Zhiwei Liu, Qianqian Xie, Jimin Huang, Erxue Min, and Sophia Ananiadou. Selective preference optimization via token-level reward function estimation, 2024. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, Qianyu Chen, Huarong Zhou, Zhensheng Zou, Haoye Zhang, Shengding Hu, Zhi Zheng, Jie Zhou, Jie Cai, Xu Han, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint 2408.01800, 2024. Eunseop Yoon, Hee Suk Yoon, SooHwan Eom, Gunsoo Han, Daniel Nam, Daejin Jo, Kyoung-Woon On, Mark Hasegawa-Johnson, Sungwoong Kim, and Chang Yoo. TLCR: Token-level continuous reward for fine-grained 14 reinforcement learning from human feedback. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics ACL 2024, pages 1496914981, Bangkok, Thailand and virtual meeting, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.889. https://aclanthology.org/2024.findings-acl.889. Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian Karrer, Shelly Sheynin, et al. Scaling autoregressive multi-modal models: Pretraining and instruction tuning. arXiv preprint arXiv:2309.02591, 2023. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi, 2024. https://arxiv.org/abs/2311.16502. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training, 2023. https://arxiv.org/abs/2303.15343. Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences, 2020. https://arxiv.org/abs/1909.08593."
        },
        {
            "title": "A Synthetic Data Generation",
            "content": "A.1 Prompt for Synthesizing Image Caption from VQA . . . . . . . . . . . . . . . . . . . . . . . . A.2 Prompts for Synthesizing VQA Negatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Prompts for Synthesizing Image Caption Negatives . . . . . . . . . . . . . . . . . . . . . . . . A.4 Statistics of Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "B Training and Model Performance",
            "content": "B.1 Model Training Setup and Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Performance Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "A Synthetic Data Generation",
            "content": "A.1 Prompt for Synthesizing Image Caption from VQA Synthesize Caption from Visual Question Answering 16 16 17 22 23 23 23 24 Your task is to convert list of question-answer pairs to descriptive paragraph. Keep in mind these rules: Do not start with greetings or salutations. Simply return the new caption. Do not write anything else at the end of your response. Your crafted descriptive caption should be very faithful to the given question-answer pairs. Do not add any additional information that is not in the question-answer pairs. The parapgraph should not start with the photo, the image, the picture etc. Instead of saying, for example, the photo shows cute koala bear sleeping on the tree, you should just say cute koala bear is sleeping on the tree. For example, you are given the following question-answer pairs: Question: What is the color of the mug? Answer: red. Question: Where is the mug at? Answer: on the table. Question: Is there anything written on the mug? Answer: Yes. Question: What is written on the mug? Answer: Hello World. Question: What is the color of the texts? Answer: yellow. Your response should be descriptive paragraph that aggregate the information from the questionanswer pairs: Paragraph: red mug sitting on the table has yellow texts written on it: Hello World. Now you are given the following question-answer pairs and are asked to generate paragraph: Question: Question Answer: Answer Question: Question Answer: Answer ... Paragraph: 16 A.2 Prompts for Synthesizing VQA Negatives Prompt Template for Generating VQA Negatives You are provided with visual question-answer pair. Your task is to generate wrong answer that is subtly different from the original answer. Keep in mind the following rules: Keep formatting the same, such as sentence structure, paragraph blocks, or newlines. The changes should be subtle but concrete, replacing words and phrases with opposite meanings or alternative options. Do not start with greetings or salutations. Simply return the new answer. Do not write anything else at the end of your response. Do not fix any typos or grammar errors. If there are any, please ignore them. The new answer should be nearly identical, other than 1 or 2 very small changes. The changes should be very visually different. The new answer should be realistic. More importantly, keep the wrong answer within the same taxonomy as the original answer. Here are some examples. Question: What color is the apple? Answer: red. Wrong answer: green. Question: What is the spatial relationship between the man and the chair? Answer: The man is sitting on brown chair. Wrong answer: The man is sitting next to brown chair. Question: How many apples are there? Answer: There are 5 apples. Wrong answer: There are 6 apples. Now write your new answer: Question: Question Answer: Answer Wrong answer: Check if VQA Negative is Valid You are provided with paragraph, and question-answer pair. Your task is to determine if the answer is valid answer to the question given the paragraph. The answer should be simple yes or no. Do not write anything else at the end of your response. Here are some examples. Paragraph: The man is sitting on brown chair. Question: What is the spatial relationship between the man and the chair? Answer: The man is sitting on brown chair. Valid answer: yes. Paragraph: There are six apples on the table and they are all red. One of the apples is rotten and is at the left side of the table. Question: How many apples are there? Answer: There are 5 apples. Valid answer: no. Paragraph: On sunny day, man sailing boat on the ocean sees fish jumping out of the water. Question: What is the man doing? Answer: The man is fishing. Valid answer: no. Now check this paragraph and the question answer pair: Paragraph: Paragraph Question: Question Answer: Answer Valid answer: Note: we discard the synthetic negatives that are marked as valid because the perturbation is unsuccessful. A.3 Prompts for Synthesizing Image Caption Negatives We first present the general prompt for perturbation and then break these down into taxonomy-specific rules and in-context examples on all 8 taxonomies described in Section 4. General Prompt You are provided with caption to an image. Your task is to generate new caption that is subtly different from the original caption. Keep in mind the following rules: Keep formatting the same, such as sentence structure, paragraph blocks, or newlines. The changes should be subtle but concrete, replacing words and phrases with opposite meanings or alternative options. Do not start with greetings or salutations. Simply return the new caption. Do not write anything else at the end of your response. Do not fix any typos or grammar errors. If there are any, please ignore them. The new caption should be nearly identical, other than 1 or 2 very small changes. The changes should be very visually different. The new caption should be realistic. Most importantly, make the change with the following taxonomy: Taxonomy Here are the additional rules for the taxonomy Taxonomy-Specific Rules and In-Context Examples Here is the original caption: Caption Now write your new caption: Spatial Relationship Rules and In-Context Examples Change the spatial relationship of two objects in the given caption. The new spatial relationship should be different from the original spatial relationship. The new spatial relationship should be realistic, and is visually different from the original spatial relationship. You should fix the consistency of the caption. If you change the spatial relationship, you should also change the noun, verb, etc. so that there is no grammar error. Do not change anything not related to spatial relationship, even there are spatial relationship present. Do not change other attributes of objects, such as color, texture, material, etc. If there are no spatial relationship present in the caption, you should simply copy the original caption. For example, Original caption: man is sitting on the left to coffee table. New caption: man is sitting on the right to coffee table. Original caption: duck is swimming in pool and fish is swimming underneath. New caption: duck is swimming in pool and fish is swimming on top of it. Original caption: man is sitting in front of car. New caption: man is sitting in car. Original caption: An apple is placed on an open book. New caption: An apple is placed under an open book. Original caption: There is black cat. New caption: There is black cat. Visual Attribute Rules and In-Context Examples Change the visual attributes of objects in the given caption. The new visual attributes should be different from the original visual attributes. The new visual attributes should be realistic, and is visually different from the original visual attributes. 18 You should fix the consistency of the caption. also change the noun, verb, etc. so that there is no grammar error. Do not change anything not related to visual attributes, even there are visual attributes present. If you change the visual attributes, you should For example, Original caption: man is sitting on marble bench. New caption: man is sitting on wooden bench. Original caption: red apple is placed on table. New caption: green apple is placed on table. Original caption: corgi dog is sitting on chair. New caption: corgi dog is sitting on couch. Attribute Binding Rules and In-Context Examples Change the attribute bindings of many objects in the given caption. Definition of attribute binding is that the attribute of an object is bound to the object. Changing the attribute binding means swap the attributes of many different objects. The new attribute bindings should be different from the original attribute bindings. The new attribute bindings should be realistic, and is visually different from the original attribute bindings. You should fix the consistency of the caption. If you change the attribute bindings, you should also change the noun, verb, etc. so that there is no grammar error. Do not change anything not related to attribute bindings, even there are attribute bindings present. Do not add or deletes any objects and attributes other than changing the attribute bindings. For example, Original caption: man is sitting on bench and woman is sitting on chair. New caption: man is sitting on chair and woman is sitting on bench. Original caption: red apple and stack of blue books are on table. New caption: blue apple and stack of red books are on table. Original caption: An apple made of aluminum and chair made of wood are on display at the art museum. New caption: An apple made of wood and chair made of aluminum are on display at the art museum. Original caption: Two yellow cats are chasing one flurry blue ball of yarn. New caption: Two blue cats are chasing one flurry red ball of yarn. Object Identification Rules and In-Context Examples Change the object identifications in the given caption. Definition for object identification is that the entity of an object, e.g., book, man, dog, table, etc. The new object should be different from the original object. The new object should be realistic, and is visually different from the original object. If possible, you can make the new object subtly different from the original object. For example, change corgi dog to dachshund dog. 19 You should fix the consistency of the caption. If you change the object, you should also change the noun, verb, etc. so that there is no grammar error. Do not change anything not related to object identification, even there are object identifications present. Do not add or deletes any objects and attributes other than changing the object identification. Here are some valid examples Original caption: man is sitting on bench. New caption: man is sitting on chair. Original caption: red apple is placed on table. New caption: red apple is placed on bench. Original caption: corgi dog is sitting on chair. New caption: dachshund dog is sitting on couch. Counting Rules and In-Context Examples Change the counting of one object in the given caption to different number. The new number should be different from the original number. The new number should be realistic, and is not so different from the original number. You should fix the consistency of the caption. If you change the number, you should also change the noun, verb, etc. so that there is no grammar error. Only change the counting of one object in the caption. Do not change the number of other objects. Do not change anything not related to counting, even there are numbers present. Do not change things related to written texts in quotation marks. For example, the original caption has man wears shirt with text cute cat written on it. DO NOT change the caption to man wears shirt with text cute cats written on it. Do not change things ralated to time in the caption. For example, the original caption has The clock reads 13:00. DO NOT change the caption to The clock reads 14:00. Do not change things related to proportions. For example, the orginal caption has The book covers 2/3 of the table. Do NOT change the caption to The book covers 3/4 of the table. You can change the caption to Two books cover 2/3 of the table instead. If there are no counting in the caption, you should simply copy the original caption. Here are some valid examples Original caption: There are five cats on the table and they are black. New caption: There are seven cats on the table and they are black. Original caption: There are two dogs standing on the chairs, one is white and one is black. New caption: There are three dogs standing on the chairs, one is white and the other two are brown. Original caption: side view of Rouen duck that is brown and tan and in some water. It is facing to the right. New caption: side view of two Rouen ducks that are brown and tan and in some water. They are facing to the right. Original caption: pair of stop sign poles on street. New caption: Three stop sign poles on street. Original caption: The sky is blue. New caption: The sky is blue. Small Objects Rules and In-Context Examples Change the small and background objects in the given caption. The new small and background objects should be subtly different. You can change their counts, size, shape, color, etc. You should ONLY change very small, neglible, background objects, that are explictly described as so in the caption. For example, you can pay attention to words like tiny, small, mini, micro, nano, pico, femto, nano, micro, milli, etc. If there are no small objects in the caption, you should simply copy the original caption. For example, Original caption: man is sitting on bench, in library with white background board. On the bookshelf, there is tiny crystal superman figure standing on stack of books. New caption: man is sitting on bench, in library with black background board. On the bookshelf, there is tiny plastic batman figure standing in front of stack of books. Original caption: man is sitting on bench, in library with white background board. New caption: man is sitting on bench, in library with black background board. Text OCR Rules and In-Context Examples Change the text OCR in the image caption to different text. The new text should be different from the original text. The new text should be realistic to the context, and is visually different from the original text. You should fix the consistency of the caption. If you change the text, you should also change the noun, verb, etc. so that there is no grammar error. Do not change anything not related to text, even there are texts present. If there are no texts OCR present in the caption, you should find place to put some reasonable text OCR. If you cant find place to put some reasonable text OCR, you should simply copy the original caption. For example, Original caption: man is wearing shirt with texts SUPERMAN. New caption: man is wearing shirt with texts BATMAN. Original caption: The digital clock reads 12:00 AM. New caption: The digital clock reads 12:08 PM. Original caption: The road sign says STOP. New caption: The road sign says YIELD. Original caption: The board says Best College in the US. New caption: The board says Best College in the World. Original caption: man wearing yellow shirt is sitting on the bench. New caption: man wearing yellow shirt with words Hello World is sitting on the bench. 21 Counterfactual Rules and In-Context Examples Change the caption with counterfactuals. The new caption should be different from the original caption. The new caption should be realistic, and is visually different from the original caption. You should fix the consistency of the caption. If you change the caption, you should also change the noun, verb, etc. so that there is no grammar error. Do not change anything not related to counterfactuals, even there are counterfactuals present. Do not add or deletes any objects and attributes other than changing the counterfactuals. If its hard to put in counterfactual, you should simply copy the original caption. For example, Original caption: man is sitting on bench. New caption: man is not sitting on bench. Original caption: red apple is placed on table. New caption: red apple is not placed on table. Original caption: soldier. New caption: soldier has no sword in hand. A.4 Statistics of Data Task VQA Data Source Taxonomy # Positive # Negative Train Set Proportion (%) VG100K 1,179,007 1,179,007 Image Caption Synthetic Caption from VG100K Image Caption DOCCI Spatial Relation Visual Attribute Attribute Binding Object Identification Counting Small Object Text OCR Counterfactual Spatial Relation Visual Attribute Attribute Binding Object Identification Counting Small Object Text OCR Counterfactual 94, 14,639 45,225 86,366 59,219 75,328 75,156 80,455 84,164 57,153 8,867 13,811 13,561 10,618 10,491 11,680 13,366 12,844 80% 65% Table 7 Statistics of Data. Overall, we have over 1M VQA data with both positive and negative answers, and over 100K caption datapoints with 650K negative captions. We oberserve that we have the least amount of spatial relationship data, because spatial relationship negatives are the hardest to synthesize and not every caption has spatial relationship descriptions."
        },
        {
            "title": "B Training and Model Performance",
            "content": "B.1 Model Training Setup and Hyperparameters In the section, we present all the (hyper-)paramters we used to training TLDR model. Hyperparameters for training TLDR Model Base Model Image Resolution Number of Image Tokens Hidden Dimension Size LoRA Rank LoRA α LoRA dropout GPU Batch Size Gradient Accumulation Steps Warmup Steps Learning Rate Learning Rate Scheduler PaliGemma-3B-Mix-448 448 448 1024 2048 512 128 0.1 8 NVIDIA H100 8 8 200 0.001 Cosine Table 8 Hyperparameters for training TLDR Model. B.2 Performance Evaluation In this section, we present TLDR models performance by taxonomy, and its comparison to the naive binary RM and TLDRs ablations discussed in Section 5. Taxonomy Naive RM TLDR (ours) Spatial Relationship Visual Attribute Attribute Binding Object Identification Counting Small Object Text OCR Counterfactual Overall 74.1 89.8 88.1 73.2 71.0 79.0 82.6 87.3 81.1 60.2 89.8 90.6 90.6 73.9 75.0 86.5 90. 83.1 Ablation Only fproj 50.0 (-10.2) 54.7 (-35.1) 55.0 (-35.6) 51.4 (-39.2) 50.7 (-23.2) 51.6 (-23.4) 52.2 (-34.3) 53.3 (-36.7) 52.5 (-30.6) Only fdec 51.8 (-8.4) 89.8 (0.0) 85.0 (-5.6) 86.9 (-3.7) 68.1 (-5.8) 71.8 (-3.2) 83.7 (-2.8) 89.3 (-0.7) 79.4 (-3.7) Table 9 Response-Level Accuracy by Taxonomy. We find that overall TLDR models have higher response-level accuracy except for spatial relationship and small objects. We suspect the cause is from brutal conversion of prediction from token-level to response level: by taking the product ργ(m, p, d) = (cid:81) ed γ(e m, p, d), any single token has the power to veto the entire response d. In delicate and subtle instances such as spatial relationship and small objects, such veto power by any single token is too brutal, and more elegant conversion from token-level probabilities Pγ(e m, p, d) could be interesting for future work."
        },
        {
            "title": "C Self Correction",
            "content": "In the section, we present two exemplar prompts used for self-correction one with TLDRs guidance and the other with only naive binary RMs guidance saying the original generation is wrong. The displayed image is from WinoGround. Note: For better visualization, we highlight the TLDR Models annotations in red when presenting the TLDR prompts. Self Correction with TLDRs Guidance Image: You task to make the provided Response to the Text Query better aligned with the given image. The Response has several sentences with issues that you need to take closer look at. Focus on the image more when looking at the highlighted words or phrases in these sentences. Here are some rules to keep in mind: If you think you cant do minimal edits, you can rewrite the whole sentences with errors. 1. Your edited response should be as close as possible to the original response (minimal edits) but without the errors. 2. You should try to fix the errors by only changing the highlighted words. If you think these words are hallucinated, after looking closely at the image again, you can delete them. 3. However, do not rewrite sentences without any annotated errors. 4. For sentences not enumerated, you can simply copy them if your changes wont affect them. If you changes to other marked sentences will also change the meaning of the unmarked sentences, you can change them as well. Please make the whole paragraph coherent. 5. The highlighted words that require extra attention are not necessarily always incorrect. If you think they align well with the image, you can keep them as is. After you look closer to the image, use your own judgement to decide if they need to be changed, deleted, or kept. You may change or delete the highlighted tokens if you think they are not aligned with the image. Otherwise, you can keep them as is. Text Query: Describe the image in details. Response: The image shows creative arrangement of colorful Lego bricks forming shape that resembles cat. The bricks are in various colors such as yellow, pink, and green. There are two white paper boats placed on top of the Lego cat, one on the left and one on the right side. The background is plain, light color, providing neutral backdrop that highlights the colorful Lego cat. Please take closer look at these words or phrases: Lego, Lego bricks, Lego cat, bricks, cat. Now we break them into their corresponding sentences to provide you with more context. 24 Please fix the sentence \"The image shows creative arrangement of colorful Lego bricks forming shape that resembles cat\" with more attention to the following words: Lego bricks, cat. Please fix the sentence \"The bricks are in various colors such as yellow, pink, and green\" with more attention to the following words: bricks. Please fix the sentence \"There are two white paper boats placed on top of the Lego cat, one on the left and one on the right side\" with more attention to the following words: Lego. Please fix the sentence \"The background is plain, light color, providing neutral backdrop that highlights the colorful Lego cat\" with more attention to the following words: Lego cat. Please correct all the errors. Do not start or end the edited response with anything extra. Corrected Response: Self Correction with Naive RMs Guidance (no TLDR) Image: You task to make the provided Response to the Text Query better aligned with the given image. Here are some rules to keep in mind: 1. Your edited response should be as close as possible to the original response (minimal edits) but without the errors. 2. However, do not rewrite sentences without any annotated errors. If you think you cant do minimal edits, you can rewrite the whole sentences with errors. Text Query: Describe the image in details. Response: The image shows creative arrangement of colorful Lego bricks forming shape that resembles cat. The bricks are in various colors such as yellow, pink, and green. There are two white paper boats placed on top of the Lego cat, one on the left and one on the right side. The background is plain, light color, providing neutral backdrop that highlights the colorful Lego cat. Please correct all the errors. Do not start or end the edited response with anything extra. Corrected Response:"
        }
    ],
    "affiliations": [
        "Meta",
        "University of Southern California"
    ]
}