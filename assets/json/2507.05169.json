{
    "paper_title": "Critiques of World Models",
    "authors": [
        "Eric Xing",
        "Mingkai Deng",
        "Jinyu Hou",
        "Zhiting Hu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "World Model, the supposed algorithmic surrogate of the real-world environment which biological agents experience with and act upon, has been an emerging topic in recent years because of the rising needs to develop virtual agents with artificial (general) intelligence. There has been much debate on what a world model really is, how to build it, how to use it, and how to evaluate it. In this essay, starting from the imagination in the famed Sci-Fi classic Dune, and drawing inspiration from the concept of \"hypothetical thinking\" in psychology literature, we offer critiques of several schools of thoughts on world modeling, and argue the primary goal of a world model to be simulating all actionable possibilities of the real world for purposeful reasoning and acting. Building on the critiques, we propose a new architecture for a general-purpose world model, based on hierarchical, multi-level, and mixed continuous/discrete representations, and a generative and self-supervision learning framework, with an outlook of a Physical, Agentic, and Nested (PAN) AGI system enabled by such a model."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 9 6 1 5 0 . 7 0 5 2 : r a"
        },
        {
            "title": "Critiques of World Models",
            "content": "Eric Xing,, Mingkai Deng,, Jinyu Hou,, Zhiting Hu, School of Computer Science, Carnegie Mellon University Institute of Foundation Models, Mohamed bin Zayed University of Artificial Intelligence Halıcıoglu Data Science Institute, UC San Diego {epxing, mingkaid, jinyuhou}@cs.cmu.edu, zhh019@ucsd.edu July 2025 Abstract World Model, the supposed algorithmic surrogate of the real-world environment which biological agents experience with and act upon, has been an emerging topic in recent years because of the rising needs to develop virtual agents with artificial (general) intelligence. There has been much debate on what world model really is, how to build it, how to use it, and how to evaluate it. In this essay, starting from the imagination in the famed Sci-Fi classic Dune, and drawing inspiration from the concept of hypothetical thinking in psychology literature, we offer critiques of several schools of thoughts on world modeling, and argue the primary goal of world model to be simulating all actionable possibilities of the real world for purposeful reasoning and acting. Building on the critiques, we propose new architecture for general-purpose world model, based on hierarchical, multi-level, and mixed continuous/discrete representations, and generative and self-supervised learning framework, with an outlook of Physical, Agentic, and Nested (PAN) AGI system enabled by such model."
        },
        {
            "title": "Introduction",
            "content": "A Large Language Model (LLM) simulates the next word in human languages, which has led to systems like ChatGPT that allow people to perform wide range of tasks facilitated through language, such as common conversation, standardized tests, professional writing, and advanced math reasoning, at level rivaling human intelligence. What would you do if you could perfectly simulate the next world every possible future in the environment that we reside? Dune, classic of science fiction that inspired the likes of George Lucas Star Wars and Miyazakis Valley of the Wind, boldly imagines such possibility. The series is centered around the Kwisatz Haderach, prophesized human being who inherits their ancestors memories and simulates the outcomes of all possible plans in order to chart the best path to achieve their goals [25]. Such superhuman ability allows them to command armies to win galactic wars, or to oversee global-scale projects that turn desert planet into green paradise. Is it possible to build towards computer systems with similar functionalities using similar approach? Unlike chat-bot, human consists of hierarchy of abilities that go from immediate, concrete ones (e.g., body control/movement/action, reading/listening, and speaking/drawing) to far-reaching Co-first author 1 Figure 1: Familiar example of reasoning by simulation an individual (possibly self-serving) decides to offer help to crying person by mentally simulating multiple possible outcomes, with the best expected reward in mind. and abstract ones (e.g., planning, collaboration, and strategizing). Furthermore, the same human, while not necessarily perfectly, can perform broad range of tasks (e.g., do household chores, complete risky expeditions, conduct research investigations, navigate social situations, and manage complex enterprises) all with the same cognitive architecture of the human brain. Can single artificial intelligence (AI) system perform all these tasks? Each of these problems can be seen as goal-oriented agent acting in multimodal environment, requiring purposeful reasoning of massive temporal-spacial, social-physical, and emotional-cognitive complexity and depth, to the point that traditional approaches built on logical inference (e.g., induction, deduction, abduction) are often easily overwhelmed. It emerges that the key to generalized decision-making toward such complexity lies in reasoning by seeing the future as seen in Dune an ability formally known as Hypothetical Thinking [4] in the psychology literature, or thought experiments in common practice the ability to simulate the next worlds using mental model of the world. We call such mental model World Model. Specifically, World Model (WM) is generative model that simulates the possibilities in diverse scenarios (e.g., physical world, mental world, social world, and evolutionary world). Operationally, WM takes previous world state and action a, and predicts or simulates the next world state through transformation function, such as conditional probability distribution: p(ss, a) (1) With WM, machines can perform thought experiments by simulating actions and plans in complex scenarios, including counterfactual ones, and extracting the best ones among them. This is consistent with the hypothesis that humans reason not just linearly with formal rules toward goals (e.g., imagine self-serving individual immediately offering help to another person upon seeing her cry in the hopes of stopping her from crying, Figure 1) via deterministic optimization algorithm, but also based on simulations using an internal mental model (e.g., imagine the same individual decides what to do by mentally simulating multiple possible outcomes including self-exhaustion, the other person stops crying, she continues to cry but is grateful, etc., with the best expected reward in mind) [21, 40]. Such WM also enables transfer of knowledge to solving novel tasks, thanks to the fact that real world dynamics, even in different scenarios, share many mechanistic commonalities. For instance, scuba diver experiences how their body moves in response to low gravity, which is likely helpful to walking on the moon. mountaineer would be good at predicting how terrain conceals individual movements, which is useful when its their turn to lead mountain ambush. On the other hand, 2 Figure 2: possible definition of an optimal agent skilled gamer has deep knowledge of how digital characters respond to control signals, which will come in handy when they become drone operators. Therefore, like humans often employ their mental model to help extrapolate from past experiences to act novelly in new environments; machines can leverage WM similarly to better achieve zero-shot capabilities in unfamiliar environments. How should we create such general WM? The key desiderata for building and training WM include the following 5 aspects: identifying and preparing training data with the desired world information; adopting general representation space for the latent world state with possibly richer meaning than the observation data in plain sight; designing an architecture that allows effective reasoning over the representations; choosing an objective that properly guides the model training; determining how to use the world model in decision-making system. Recent years have seen surge in efforts toward world model. In this paper, we provide both empirical and technical critiques of several such efforts, including some very vocal schools of thoughts on WM where systematic proposals on the five aforementioned aspects of WM were offered. We conclude our critiques with brief preview of an alternative architecture PAN Physical, Agentic, and Nested world model, which we argue offers the potential for truly general-purpose and actionable world model, based on the following design principles: 1) data from all modalities of experience; 2) mixed continuous and discrete representation; 3) hierarchical generative modeling with an enhanced LLM backbone, alone with and generative latent predictive architecture; 4) generative loss grounded in observation data; and 5) using world model to simulate experience for training agents with reinforcement learning (RL). Full details of the PAN world models and results will be provided in separate dedicated manuscript [19]."
        },
        {
            "title": "2 World Model and Agent Decision-Making",
            "content": "World model arises in the context of agent decision-making. An agent is an autonomous system that acts in an environment the universe, with both physical and social worlds to achieve goal (e.g., climbing mountain, winning military campaign). We consider an environment with discrete timesteps indexed by (continuous timesteps can be approximated by infinitesimally small discrete time-steps). Formally, the agent takes the current world state st and outputs the next action at based on distribution pπ(atst), known as the policy in reinforcement learning literature. The optimal agent is thus one that best achieves its goals across all environments. The concept of World Model arises as surrogate of the environment in general agentic reasoning."
        },
        {
            "title": "2.1 Agent-Environment Model and Optimal Agent",
            "content": "Consider sequential interaction between an agent and an environment (Figure 2). At time step t, the agent outputs action at, and the universe µ takes the current state st and the action at, and 3 outputs the next state st+1 based on distribution pµ(st+1st, at). The distribution of the interaction trajectory until timestep , or (at, st+1, . . . , aT 1, sT ), given the current state st is thus denoted by: pπ µ(at, st+1, . . . , sT st) = 1 (cid:89) k=t pπ(ak sk) (cid:123)(cid:122) (cid:125) (cid:124) agent pµ(sk+1 sk, ak) (cid:123)(cid:122) (cid:125) (cid:124) universe (2) In each state st, the agent also receives reward r(g, st) based on its goal g. We evaluate the agent by its discounted cumulative reward, denoted as (cid:80) k=t γkr(g, sk) (with the discount parameter γt decaying to zero with time, i.e., limt γt = 0). Note that this reward function can be dense (e.g., gaming scores), but perhaps frequently sparse (e.g., curing disease). The agents long-term success can thus be measured by its expected future discounted reward, also known as value function [38]: π,µ(st) := Eπ,µ (cid:34) (cid:88) γkr(g, sk) (cid:35) st (cid:12) (cid:12) (cid:12) (cid:12) = lim k=t (cid:88) (at,st+1,...,sT ) (cid:80)T (cid:124) k=t γkr(g, sk) (cid:123)(cid:122) (cid:125) goal pπ µ(at, st+1, . . . , sT st) (cid:123)(cid:122) (cid:125) (cid:124) trajectory (3) Based on Equations 2 and 3, we can define the optimal agent in this universe µ as one that maximizes the value function, written formally as below: π µ := arg max π π,µ (4) Some simple derivation will show that the optimal agent in state st will select actions based on the following decision rule π µ(st) when planning for actions at:T 1: π µ(st) = arg max at:T 1 (cid:124) (cid:123)(cid:122) (cid:125) possible actions (cid:32)T 1 (cid:88) (cid:88) st+1:T k=t (cid:124) γkr(g, sk) + γT π,µ(sT ) (cid:123)(cid:122) goal progress (cid:125) (cid:33) 1 (cid:89) i=t pµ(si+1si, ai) (cid:125) (cid:123)(cid:122) (cid:124) universe response (5)"
        },
        {
            "title": "2.2 World Model and Simulative Reasoning",
            "content": "Note that optimal decision-making defined in Equation 5 requires the agent to have access to the ground-truth world state from the universe µ to experience and optimize. However, these are often not the case aside from simple scenarios like the Go and Chess games [33, 34] imagine building an agent to land on Mars, or even real-world robot relying on noisy sensors in daily environments. World Model thus arises as crucial component for predicting the universes response to general agent. Specifically, as illustrated in Figure 3, WM operates on an internal (continuous or discrete) representation of the world state, denoted as belief state ˆst, which is derived from sensory inputs ot via an Encoder (unlike the optimal agent described in 2.1 which has direct access to the true world state st). Given proposed action (as opposed to the true action at used by the optimal agent), the WM predicts the next belief state ˆst+1 according to the distribution pf (ˆst+1ˆst, t). This predicted belief state then allows the agent to propose the next action, continuing the cycle of prediction and action up to desired time horizon . The agent can simulate multiple such sequences of proposed actions and belief states, and select the actual action at (upon observing ot) based on some external function such as Critic that evaluates the outcome against given Goal. Thus, WM essentially functions as generative model of possible future world states, which enables simulative reasoning, or thought experiments. 4 Figure 3: An agent in real world where groundtruth world state and universe are unavailable to experience or experiment, so world model is crucial for simulation. Formally, for the optimal agent π based decision rule in Equation 6 as follows: equipped with WM in belief state ˆst, we define the simulationπ (ˆst) = arg max t:T 1 (cid:124) (cid:123)(cid:122) (cid:125) possible actions (cid:32)T 1 (cid:88) (cid:88) ˆst+1:T k=t (cid:124) γkr(g, ˆsk) + γT g π,f (ˆsT ) (cid:33) 1 (cid:89) (cid:123)(cid:122) goal progress i=t (cid:125) pf (ˆsi+1ˆsi, i) (cid:124) (cid:125) (cid:123)(cid:122) simulation with world model (6) general-purpose WM enables simulation of diverse possibilities across wide range of domains, enabling agents to reason about outcomes without direct interaction with the environment. This includes, but is not limited to the following examples: Physical dynamics: Mechanics of the real world, such as how water pours, how an object moves when thrown, or how machine operates under varying conditions. Embodied experiences: Internal bodily states (e.g., balance, posture), sensations (e.g., heat, pain, dizziness), and complex motor activities like getting dressed or tying shoes. Emotional states: Affective responses such as happiness, sadness, or fear, which can facilitate planning in emotionally charged contexts (e.g., therapy or social interactions). Social situations: The actions and internal states of other individuals, including their embodied or emotional experiences, needs, intentions, and expectations. Mental world: Abstract thought processes such as logistics, tactics, and strategies, potentially in multi-agent or adversarial settings. Counterfactual world: Alternative realities or what if scenarios to guide better decisionmaking under uncertainty or incomplete information. Evolutionary world: Generational dynamics such as genetic inheritance, adaptation, and survival of organisms. As stated earlier, major function of the WM is to enable simulative reasoning where an agent performs series of thought experiments by simulating the outcomes of plans with the WM, based on which it can choose the best plan. This reasoning approach contrasts with alternative approaches also explored in AI systems, such as logical reasoning inspired by type of human mental activity that aims to arrive at conclusion through rigorous inferences or arguments starting from set of premises and then using stepwise relational formal reasoning (e.g., Lambda calculus) to reach conclusion supported by these premises; or model predictive control based reasoning used in the process industries in chemical plants and oil refineries to infer optimal action sequence through mathematical programming (e.g., convex optimization) to satisfy set of constraints. hallmark of simulative reasoning enabled by WM is its flexibility, generalizability, and scalability with respect to changing computing resource, memory, environment, and problem complexity, thanks to the WMs intrinsic ability to simulate all possibilities across domains. In this regard, WM bears important empirical (e.g., end-to-end experience) similarity to modern LLM such as ChatGPT that can operate in subject-agonistic way in the lingual intelligence space. Indeed, as we discuss later, general-purpose WM can make use of LLMs as its key building blocks. Combined with an encoder that estimates beliefs of the world states from arbitrary sensory observations, WM supports machines to perform thought experiments computationally with controlled depth (i.e., number of steps) and width (number of trajectories). AlphaGo [33], for example, can be seen as special case of simulative reasoning with Monte-Carlo Tree Search (MCTS) using known (trivial) WM. In the physical world, simulative reasoning enables autonomous vehicles to drive safely by forecasting future street scenarios [e.g., 41, 37] or military commanders to develop battle-winning tactics by anticipating the outcomes of troop movements [36]. WM also enables simulations on different time scales, allowing one to answer questions about billions of years on Earths evolutionary path, or just few moments on hypothetical Martian civilization."
        },
        {
            "title": "3 The World Model Landscape",
            "content": "Recent work on world modeling have led to variety of systems, many of which optimized for specific domain or type of simulation. Interestingly, commonality nevertheless can be found in these diverse systems in that they all put significant emphasis on video/image generation, and on visual quality of the generated contents: Gaming World Models Systems such as Genie 2 [28] (Google DeepMind), Muse [22] (Microsoft), and Oasis [11] (Decart and Etched) simulate video game environments using generative models. These models can render plausible trajectories from visual and action input, producing up to 1-2 minutes of continuous gameplay content. Despite their advances, these systems remain domainspecific for instance, Genie 2 and Muse rely on restrictive console-style inputs, while Oasis is limited to Minecraft-like settings. Furthermore, their temporal coherence remains shallow, as their current generation horizons (1-2 minutes) fall short of representing full gameplay sessions, which often span several hours. As such, current gaming world models lack the flexibility, generality, and long-term reasoning capabilities required for more open-ended, agent-driven tasks. 3D Scene World Models World Labs [47] and related efforts focus on stylized 3D scene generation and egocentric navigation. While visually appealing, technical details remain scarce. From available demonstrations, these models appear to simulate static environments without dynamic agents, physics, or rich interactivity. This results in incomplete simulations that are insufficient for tasks involving physical causality, multi-agent behavior, or goal-driven planning. Although these systems push the boundary of spatial realism, they do not support full-fledged world modeling for decision-making or agent learning. Physical World Models Generative models such as Wayve GAIA-2 [32] and NVIDIA Cosmos [1] are trained specifically on physical control tasks, including autonomous driving, robotic manipulation, and embodied navigation. These systems demonstrate impressive fidelity in modeling low-level physics and sensory-motor control under diverse conditions (e.g., varying weather, lighting, and geography). However, they are tightly coupled to their respective domains, often relying on taskspecific sensors, data, or control architectures. These models excel in their respective constrained 6 settings, but still yet to address the broader challenge of simulating complex, multi-agent, or socially grounded worlds. Video Generation Models Another popular class of models focus on general-purpose video generation, with recent examples including OpenAIs Sora [7] and Google DeepMinds Veo [15]. These models aim to generate high-quality video sequences from textual prompts and/or prior frames. While visually stunning, these models only generate fixed trajectories and do not support interactions based on alternative actions. Specifically, they lack explicit notions of state, action, and possibly even object-level or conceptual representation within the video frames. They also provide no simulation control that would allow for reasoning about counterfactual outcomes or evaluate different decisions. Consequently, these systems fall outside the definition of world models for reasoning and planning, and are better understood as strict video-generation tools (focused on pixel-level synthesis) rather than parts of decision-making systems. Joint Embedding Predictive Models Last but not least, the joint embedding predictive architecture (JEPA) family, including the V-JEPA series [5, 3], DINO-WM [50], and PLDM [35] from Meta FAIR, has attracted significant attention for its conceptually elegant approach to world modeling. These models forego pixel-level generation and instead predict future latent embeddings, often using an encoder-encoder architecture and supervising the outputs in the latent space with energybased losses. While this design promises to improve tractability, the evidence for practical usability remains scarce as these models have mainly been demonstrated in toy environments with simple heuristics and action spaces. The very recent V-JEPA 2 [3] marks step forward by applying joint embedding prediction to robotic arm manipulation tasks, but it remains unclear whether such models can generalize across more diverse tasks (e.g., making breakfast) or scale to higher-complexity environments with long-term dependencies (e.g., mountaineering). In summary, although the systems surveyed above have demonstrated significant progress in modeling aspects of the world, most fall short of enabling purposeful reasoning and planning in real world applications due to limitations in scope, abstraction, controllability, interactability, and generalizability. In particular, except for JEPA, contemporary WM systems almost universally emphasize video generation as core function, yet this emphasis remains largely unexamined and lacks compelling justification. This focus may reflect the underlying conceptual ambiguities, or even misunderstandings, regarding fundamental question what is world model? We argue that world model is not fundamentally about generating videos, but about serving as sandbox for reasoning and thought-experiment. Our discussion below is organized around this definition to examine the plausibility and feasibility of current technical approaches to world modeling."
        },
        {
            "title": "4 Critiques of World Modeling",
            "content": "A vocal school of thought [23] has made the following claims along the five dimensions in building world model: data, representation, architecture, objective, and usage, which we found to warrant closer examination. 1. Sensory inputs are superior to texts, because of the larger data volume from physical world (i.e., even 4-year-old would have processed 1.114 bytes of visual data, whereas all textual data used to train modern LLMs amount to merely 0.914 bytes). 2. World state should be represented by continuous embeddings instead of discrete tokens to enable gradient-based optimization. 3. Autoregressive, generative models (e.g., LLMs) are doomed, as they are guaranteed to make mistakes eventually and cant model outcome uncertainties. 7 Figure 4: Framework for world model proposed by vocal school of thought. 4. Probabilistic, data-reconstruction objectives (e.g., an encoder-decoder scheme) do not work, as they are intractable and force the model to predict irrelevant details. 5. World models should be used in model-predictive control (MPC), instead of reinforcement learning (RL) framework, as the latter requires too many trials. This school of thought also proposes the following alternative framework for world models, as illustrated by Figure 4, centered around an idea that can be summarized as next representation prediction rather than next data prediction: Text-Free Pretraining: The proposed framework completely eschews text data in favor of continuous sensory data like video, audio, smell, etc. Fixed-Size, Continuous State Embeddings: Given sensory input o, an encoder estimates the world state ˆs = h(o) as an abstract continuous embedding with fixed dimensions (e.g., ˆs Rd). Encoder-Encoder Architecture: Based on action input a, the world model predicts the next state embedding ˆs = (ˆs, a) in deterministic manner. In particular, the architecture does not use decoder to reconstruct the next observation o, but instead applies the encoder again to bootstrap ˆs = h(o) as ground-truth next state for supervision. Reconstruction Loss in Latent Space: Instead of supervising the world model by the difference between the reconstructed next sensory input ˆo and the actual data o, the proposed framework bases learning on the deviation between the predicted next state ˆs and the bootstrapped groundtruth ˆs (e.g., L2 loss ˆs ˆs). Action Selection via MPC: Given current observation ot, the framework favors proposing an initial action sequence (at, at+1, . . . , aT 1), using the world model to simulate the next states (st+1, st+2, ..., sT ), and optimizing the actions based on goal progress Vg(sT ). While these thoughts raise valid questions on some current practices in world modeling and promises appealing solutions, we argue that each of their underlying assumptions introduces critical limitations when applied to general, scalable, and robust world modeling for the purpose of agentic reasoning and decision making. In the following discussion, we offer analytical critiques of the claims and proposals presented above along the dimensions of data, representation, architecture, objective, and usage of building world model."
        },
        {
            "title": "4.1 Data: Information Density, Not Just Volume",
            "content": "World model needs to be trained from sensory inputs due to larger data volume: LLM with 0.914 bytes textual data vs. 4-year-old with 1.114 bytes vision data. Although sensory data streams such as video appear massive in raw volume, much of that data is low in semantic content and highly redundant [14, 44]. In contrast, natural language is an evolved compression of human experiences, optimized over generations of abstract communication and conceptual reasoning [24, 30]. Text captures not only physical realities but also mental, social, and counterfactual phenomena, which are otherwise difficult or impossible to observe directly [10]. For example, concepts such as justice, motivation, or regret are richly encoded in language but have no direct sensory equivalent. Moreover, language provides an interface to collective human memory (e.g., documented observations, scientific discoveries, engineering failures), which are difficult, if not impossible, to derive from raw perceptual input alone [42]. Indeed, models trained on text can write software [43] or solve Olympiad-level math problems [8], whereas those trained on raw visual and motion data alone have been more suited for physical navigation [45] or manipulation tasks [2]. Thus, the path towards general-purpose world modeling must leverage all modalities of experience, whether it be text, images, videos, touch, audio, or more. Crucially, these modalities are not interchangeable, but reflect different layers of experience (e.g., video captures spatiotemporal dynamics in the embodied and physical world, while language encodes abstract concepts and social norms). Overemphasizing certain modality over others, such as video over text, reflects limitation or bias over the very fundamental perception of what world model is and what it can do. successful world model must therefore learn from this stratefied structure of experience to generalize across diverse tasks. Ignoring any level, be it low-level perception or high-level abstraction, risks omitting crucial information needed for intelligent behavior."
        },
        {
            "title": "4.2 Representation: Continuous? Discrete? Or Both?",
            "content": "Do not use discrete tokens. The world state should be represented by continuous embeddings to enable gradient-based optimization. Do humans perform gradient optimization (e.g., SGD) over continuous neural signals or pattern search (e.g., kNN) over discrete concepts during reasoning? We dont know for sure. What we do know is that reasoning can be cognitive or physiological, or both, and it might be unlikely to have one algorithm fit for all. While continuous representations allow for smoother gradient flow, the argument overlooks the inherent noise and high variability associated with continuous sensory inputs, which makes them brittle for reasoning. Human cognition has evolved to counter this variability by categorizing raw perception into discrete concepts [6], which are what we typically encode in language, symbols, and structured thoughts (Figure 5, left). Vocabulary-based tokens are thus not liability, but an asset: they offer stable, composable medium for representing concepts at various levels of abstraction. They form the foundation for designing and building language-based AI systems of today, such as the LLMs, which ground reasoning on sequence of discrete words which are human tokens that correspond to diverse perceptions from the universe (e.g., physical, mental, or social worlds), and allow form of long-term memory to be employed by implementing (ideally, dynamically controllable) context length. Although not entirely accurate, it is reasonable to consider the space of language as man-made (through evolution and learning) latent space of the perceived and describable universe that humans live substantial subspace of the whole universe. Benefitting from massive text-based pretraining, an LLM can learn Indeed, recent work that to simulate contents in this latent space formed by natural language. represents world states in natural language has seen success for reasoning and planning in wide 9 Figure 5: Vocabulary-based tokens is an effective way to categorize perceptual inputs into discrete concepts for reasoning (left). We may scale up or scale out discrete code to deal with increasing data complexity (right). Thm.1 shows either is effective, but scaling out is more efficient. range of practical tasks [18, 12]. Complementing natural language tokens, modern techniques like VQ-VAE [39] allow us to further convert sensory data (e.g., images or audio) into discrete tokens while preserving structure and semantics. While such discrete representations are expected to offer stability and symbolic structure like natural language tokens, natural concern is whether they can faithfully capture the richness of highdimensional, continuous sensory data due to the risk of information loss through distillation. This concern grows with the complexity of the world: Will discrete tokens be sufficient for distinguishing between subtly different world states? Indeed, the world often contains deeper layers of meaning than what is directly observable through sensory input (e.g., puppets movements may reflect the hidden intentions of the puppet master). Capturing such latent structure requires representations that can scale in expressive capacity. In an attempt to understand more deeply the potential and limitation of using discrete tokens, we present theoretical result showing that discrete representations can, in principle, preserve arbitrarily fine distinctions between real-valued inputs, provided we scale them appropriately. Specifically, we consider two intuitive strategies for increasing representational capacity: Learning larger modality tokenizer (scale up): Keep the number of tokens fixed, increase the vocabulary size to allows each token to encode finer-grained chunk of information. Finding longer language expression (scale out): Keep the vocabulary fixed, increase the sequence length and combine more tokens to express more complex inputs. As we show in Theorem 1 below, it is more efficient to scale out by increasing the length of the encoding. Theorem 1 (Completeness of Language Representation). Assume real inputs = [x1, . . . , xT ], where xt RD and xt < K. For any ϵ > 0, there exists language Lϵ = (V, N, fϵ) with vocabulary V, maximal sentence length < , and mapping function fϵ : RT such that for all x, RT D, x > ϵ fϵ(x) = fϵ(x). Explanation. If you have sequence of continuous sensor readings or data points, no matter how small difference you want to be able to distinguish between two sequences, you can always create language (a system of words or symbols) that can represent these sequences uniquely. Proof Sketch. We will prove the contrapositive that fϵ(x) = fϵ(x) x ϵ. Specifically, we propose two ways to scale the discrete code: Case 1 (Learning Larger Modality Tokenizer). Keep the code length constant at , increase the vocabulary size to Mϵ = Kϵ1D (i.e., scaling up). 10 Case 2 (Finding Longer Language Expression). Keep the vocabulary size constant at , increase the maximum sentence length to Nϵ = DlogM Kϵ1 (i.e., scaling out). The detailed proof can be found in Appendix A. As the proof shows, complete representations are achievable with vocabulary-based discrete tokens. However, the way to scale the representation matters (Figure 5, right). In Case 1, the vocabulary size must grow in O((T D)D), i.e., exponentially with the input size wed like to capture, which is likely not sustainable. In Case 2, on the other hand, the sequence length need only increase in the order of O(T log D), which is much more manageable. So in theory, scaling out the token sequences, which can be implemented with an enhanced LLM (with visual tokenizers and vocabulary mergers or switchers), offers more flexible and efficient pathway to capturing complex structure in data generally. In practice, the model may also represent complex inputs efficiently by dynamically resizing the dictionary, and describe novel inputs by growing the vocabulary with more new observations. In summary, given that discrete and continuous latent representations offer complementary level of abstraction, representation power, and operationalizability, we advocate for the approach of mixed representations, which leverages discrete tokens for more robust, interpretable, and symbolic forms of reasoning, while continuous embeddings still play role in capturing fine-grained sensory nuance. While this form of representation is still in its early stages, recent work has demonstrated its promise for generalization in world modeling [48] and other forms of reasoning [e.g., 26]."
        },
        {
            "title": "4.3 Architecture: Autoregressive Generation is Not the Enemy",
            "content": "Abandon autoregressive, generative models; adopt joint embedding predictive architecture (JEPA) to avoid exponential compounding of token generation error, and absorb signal variability. Proponents of JEPA advocate for non-autoregressive, non-generative, encoder-encoder framework that predicts the next latent state directly, sidestepping the need to reconstruct raw observations. However, as we discuss in this section, the architecture of this framework remains fundamentally autoregressive and generative. Formally, JEPA defines two core functions (Figure 6, left): ˆs = h(o), ˆs = (ˆs, a), where is an encoder from observations to latent states, and is the world model which predicts the next latent state given the current state and an action. Recursive application of these two operators defines latent transition model that is effectively autoregressive and generative, even though it symbolically lacks an explicit probabilistic decoder to generate what can be compared against real next observation data. (It does not mean such comparisons are avoided, as the second encoder at the output end, in fact, indirectly still makes that comparison, but with poorer mathematical controllability, as we discuss in the next section.) More precisely, JEPA can be viewed as specifying degenerate conditional distribution, denoted informally as below: pf (ˆsˆs, a) = δ(ˆs (ˆs, a)), where δ() is the Dirac delta function centered at the deterministic prediction. Thus, JEPA is not generative in the probabilistic sense (i.e., it does not model uncertainty or samples from distribution over outcomes), but it is generative in the functional sense of recursively simulating the evolution of the latent states over time, and is therefore subject to the same issues of autoregressive models. This is not to say, however, that autoregressive models are inherently flawed due to error accumulation. Many real-world systems (e.g., three-body problem, fluid dynamics, or financial 11 Figure 6: Comparison of the critiqued JEPA world model architecture (left) and our proposed Generative Latent Prediction (GLP) architecture (right), which includes diffusion-based nextembedding predictor for modeling continuous perceptual dynamics and an enhanced LLM backbone for discrete long-horizon structures. markets) are fundamentally chaotic, with small deviations growing exponentially over time [29]. In such settings, exact prediction is impossible regardless of model class. However, well-structured autoregressive models (e.g., Kalman filters for continuous cases and HMMs for discrete cases) can still learn useful, abstract properties of the system (e.g., whether water will spill, the direction of price movement) that are often surprisingly stable and predictable an insight grounded in ergodic theory and statistical mechanics [27]. common concern with encoder-decoder architectures (which defines an additional function ˆo = g(ˆs) with as decoder from latent states back to observations) is that they may compel the model to reconstruct aspects of the environment that are either inherently unpredictable or irrelevant to task performance. Examples often cited include fine-grained visual details, inconsequential events, or out-of-scene content, which may mislead the model into learning unstable or spurious correlations. Proponents of encoder-only architectures thus suggest that by avoiding this reconstruction step, the resulting WM can focus more selectively on predictable and task-relevant elements. While this motivation is understandable, it remains unclear whether removing the decoder is the effective remedy. In such an architecture as JEPA, supervision occurs solely in the latent space rather than observation space, trading off challenges of pixel-level variability for the risk of indefinability: the predicted latents are not directly grounded in observable data, which makes it difficult to diagnose whether the model is learning meaningful dynamics or collapsing to trivial solutions, an issue we discuss formally in 4.4. It is possible that the degradation of next-state prediction performance in face of data signal variability stems less from the presence of generative decoders than from the use of continuous embeddings itself, which squeezes massive amounts of information into limited subspace with fixed dimension (4.2). Additional instability may arise from the energy-based loss functions typically used for next-latent prediction, which often require heuristics-based regularizers whose behaviors are difficult to understand and control. Furthermore, the challenge of signal variability may be particularly acute in visual-related domains, while many downstream reasoning tasks (including, for example, autonomous driving) may not demand pixel-perfect simulation of the visual world. Therefore, rather than abandoning generative modeling to avoid signal variability, an alternative and well-established strategy is to adopt hierarchical abstraction through what we call Generative Latent Prediction (GLP) architecture (Figure 6, right). Instead of modeling the full world at single level of detail, GLP decomposes the problem across multiple layers of latent prediction, each specialized for different representational granularities, whether it be continuous perceptual features or discrete conceptual tokens. This allows each layer to operate at an appropriate level of abstraction while remaining generative and predictive. For instance: 12 At the lowest level, next-embedding predictors (e.g., latent diffusion models) can handle stochasticity and fine-grained variation in raw, continuous perceptual data (e.g., pixels, audio, proprioception). These models incorporate generative mechanisms (e.g., encoder-decoder architecture) that directly ground predictions in observable data, which leads to stronger supervision as we show in 4.4. At the intermediate level, next-token predictor (e.g., autoregressive Transformer decoder) can reason over discrete modality tokens derived via VQ-VAE-style encoders, capturing the symbolic and compositional structure. At the highest level, an large language model (LLM) operating in thought space composed of language tokens can support long-horizon planning, mental simulation, and counterfactual reasoning. Together with the intermediate level, these two levels of discrete reasoning can be jointly implemented through an enhanced LLM architecture performing next-token prediction. The GLP paradigm not only supports structured, abstract reasoning through next-latent prediction, but also preserves the capacity for detailed reconstruction of the input world, enabling generative supervision and external use. This not only mitigates the compounding of prediction errors by isolating low-level variability within the bottom encoder-decoder layer, but also enables more expressive reasoning and generalization at higher layers of abstraction. Importantly, it allows the model to flexibly mix continuous embeddings for perceptual nuance with discrete tokens for abstract structure, which aligns with our discussion of representation in 4.2. As we further elaborate in 4.4, this encoder-world-model-decoder design leads to stronger supervision and more stable training dynamics than encoder-only approaches like JEPA."
        },
        {
            "title": "4.4 Objective: Learning in Data Space, or Latent Space?",
            "content": "Abandon probabilistic, data-reconstruction objectives; adopt energy-based, latent-reconstruction objectives for tractability. key claim underlying the JEPA framework is that reconstructing raw observations (e.g., pixels in video) is unnecessary, and that learning in latent space is more effective. This has given rise to preference for latent reconstruction objectives, which bypass the decoder and directly supervise transitions between encoded states. Formally, given encoder and world model , the latent reconstruction loss is defined as: Llatent(h, ) = E(o,a,o)D [f (h(o), a) h(o)] , (7) where the model predicts the next latent state ˆs and compares it to the encoded form of the next observation, without reconstruction itself. Despite its apparent simplicity, this objective is prone to collapse, as we show in Proposition 1: the model can trivially minimize the loss by mapping all observations to constant vector and learning an invariant transition. To counteract this tendency, JEPA-style systems often require complex regularizers (e.g., maximizing the information I(ˆs) of latent states). These regularizers, however, are often hard to tune and difficult to understand, which can make training brittle and limits scalability. By contrast, the generative reconstruction loss grounds the learning objective in observable data by introducing decoder g, and supervising the predicted next observation directly as below: Lgen(h, f, g) = E(o,a,o)D [g (h(o), a) o] . (8) Indeed, the generative loss Lgen anchors the learned representation to the structure of the sensory world, and thus avoids the collapse suffered by the latent loss Llatent, as we show in Proposition 2. 13 Figure 7: Comparison of the critiqued world model objective based on reconstruction in latent space (left) and our proposed alternative of generative data reconstruction objectives (right). Proposition 1 (Collapse of Latent Reconstruction Loss). Given O, S, Rd and functions : S, : S, and latent reconstruction loss: Llatent(h, ) = E(o,a,o)D [f (h(o), a) h(o)] , There exists (h, ) and S, such that h(o) = for all and (c, a) = for all A, such that: Llatent(h, ) = min h,f Llatent(h, ) Explanation. If you have an encoder and world model, and you train it using the latent reconstruction loss, there is cheat configuration for the model to minimize the loss while learning nothing about the true dynamics. Proof Sketch. If we construct such degenerate solution (h, ), this solution satisfies Llatent(h, ) = 0, which is global minimum as Llatent(h, ) 0 for all (h, ). Proposition 2 (Non-Collapse of Generative Loss). Given functions : S, : S, : O, and generative loss: Lgen(h, f, g) = E(o,a,o)D [g (h(o), a) o] , Assuming (o1, a1, o2), (o3, a3, o4) such that o2 = o4, then given (h, ), fixed and such that h(o) = and (c, a) = A, there exists (h, ) such that: Lgen(h, , g) < Lgen(h, , g) Explanation. If you add decoder to the model and train it with the generative loss, assuming the data contains different next-observation targets, there will always be another set of encoder and world model that gets lower loss than the previous cheat configuration. Proof Sketch. Given degenerate solution (h, ) and fixed g, construct (h, ) to be equal to (h, ) at every point except (o1, a1, o2) and (o3, a3, o4) where o2 = o4 and the constant-valued (h, ) will get non-zero loss. Instead, set (h, ) to perfectly fit these two targets, so they will get zero loss. Thus we have: Lgen(h, , g) < Lgen(h, , g) 14 Details of the proofs of both propositions are available in Appendices and C, respectively. Beyond the issue of collapsing, more fundamental structural limitation of the latent reconstruction objective is that it essentially acts as loose surrogate for observation-level consistency, as we show in Theorem 2. This means that minimizing Llatent does not, in general, guarantee consistency with what the agent would observe in the world, which can lead to misaligned or brittle representations. In general-purpose settings, we argue that anchoring to the next observation via generative loss provides more stable and mechanistically interpretable training signal. Theorem 2 (Latent reconstruction is an upper-bounded surrogate for generative reconstruction). Given sufficiently powerful encoder : and decoder : O, such that for all latent states ˆs S, the roundtrip reconstruction error satisfies g(ˆs) ˆs ϵ for some small ϵ > 0. For world model : and transition data (o, a, o) D, define the latent-space loss Llatent and the generative loss Lgen as below: Llatent = (h(o), a) h(o) , Lgen = (h(o), a) . Assume that encoder h, decoder g, and world model induce the following conditional distributions: ˆs (h(o), I), ˆo ˆs (g(ˆs), I), and ˆs ˆs, (f (ˆs, a), I). Then, the following inequality holds: Llatent Lgen + ϵ, with Llatent = Lgen when g(ˆs) = ˆs for all ˆs and supp(D) Im(g). Explanation. If your encoder and decoder approximately undo each other, then the JEPA latent reconstruction loss is upper-bounded by the generative loss plus small reconstruction error. These losses are only equal when the encoder-decoder pair perfectly invert each other, which is unrealistic in practice. As such, minimizing the latent loss does not guarantee consistency with observed data, which is required for minimizing the generative loss (Figure 8). Proof Sketch. Observe that the two losses Llatent and Lgen are scaled KL divergences of the Gaussian prediction distributions in the latent and observation spaces, respectively. Apply the encoder to both the observation prediction and true observation distributions in Lgen, and the data processing inequality states that the augmented KL divergence is upper-bounded by the original Lgen. After that, apply the triangle inequality to show that Llatent is upper-bounded by the sum of this augmented KL (upper-bounded by Lgen) and the roundtrip reconstruction error (upper-bounded by ϵ), thus completing the proof. (We include the detailed proof in Appendix D.) In practice, ϵ is small (as is typical for strong modern autoencoders), so Llatent Lgen usually holds, meaning that the former can miss semantically important mistakes that the latter will penalize. Additionally, the use of less-understood regularizers in conjunction with Llatent as the objective makes its outcome even more difficult to assess without necessary boundary conditions imposed by observation data. In conclusion, our argument is not that world models must operate in pixel space, but that they should learn from it. Framing the distinction as next-representation prediction versus nextobservation prediction creates false dichotomy that can lead to theoretical ambiguities and practical instability. The purpose of predicting the next observation is to ensure that the predicted latent representations are meaningfully grounded in the real world, whether conceptually or physically. Conversely, reliable prediction in latent space depends on continual validation through observable data. Mathematically, any latent representation of real-world signal intrinsically suffers from issues of identifiability and stability. As such, alignment and calibration with real data are essential to ensure the representations remain meaningful and robust. Generative reconstruction objectives Figure 8: As Thm. 2 shows, the JEPA latent reconstruction loss (Llatent) is upper bounded by the generative data reconstruction loss (Lgen) plus small encoder-decoder reconstruction error (ϵ). ϵ is small in practice, meaning Llatent Lgen usually holds. Minimizing Llatent, therefore, does not guarantee consistency with observed data, which is required for minimizing Lgen. tether the learned representations to the observable world, providing richer and more stable learning signal that supports meaningful distinctions, general usability, and human interpretability. These properties are critical for downstream usage, whether it be planning trajectories or training agents through reinforcement learning, which we discuss more in 4.5 below."
        },
        {
            "title": "4.5 Usage: MPC or RL?",
            "content": "Abandon reinforcement learning (RL); adopt model-predictive control (MPC) for fewer trials required during training. Beyond training world models, there has also been debate over whether model-predictive control (MPC) is favored over reinforcement learning (RL) in using the WM for reasoning, due to sample efficiency and safety advantages [13]. Here we describe typical MPC setup (Figure 9, left) which is often adopted by recent work [35, 3]: at timestep during inference, the agent infers its current latent state ˆst = h(ot), proposes an initial sequence of actions (at, . . . , aT 1) until some decision horizon , and uses the world model to predict the corresponding next-state sequence (ˆst+1, . . . , ˆsT ). These simulated states can then be evaluated using cost function C(g, ˆs) for goal (e.g., L2 distance between ˆs and encoded goal ˆsg = h(g)), based on which the agent may propose the next action with lower cost. Decision-making thus amounts to finding the action sequence that minimizes the cost function, formalized as below: (a , . . . , 1) = arg min at,...,aT 1 1 (cid:88) k=t C(g, (ˆsk, ak)). In practice, the (continuous) action optimization is often performed using traditional numerical algorithms (e.g., MPPI [46] and CEM [31]) involving decision horizons of 1-20 steps and 100s upon 1000s of action samples, and the agent executes the first action in the final action sequence before replanning in the next step + 1. The appeal of MPC lies in learning from offline trajectories (o1, a1, . . . , oT ) without potentially unsafe exploration in the real world, as well as potential for higher-quality decision-making from world-model-based simulation. However, MPC can suffer from practical limitations. Simulation of latent trajectories using the world model, for instance, must be performed repeatedly at every timestep during inference, leading to high computational overhead and making it difficult to respond effectively in fast-changing environments. Beyond computational efficiency, MPC typically plans only few steps ahead (e.g., up to 10-20 steps) in terms of searching horizon. This limits the extent of its foresight, as long planning horizons (e.g., 100s of steps) can be difficult due to the exploding number of trajectories and worldmodel errors. As the horizon increases, MPC also becomes more difficult to implement and optimize, since the proposal distribution must sample entire action sequences at once over the full planning 16 Figure 9: Comparison of the critiqued approach to using world model (left) vs. our proposed alternative (right). horizon. This is why MPC often relies on relatively simple proposal distributions, such as uniform random sampling or multivariate Gaussians. Indeed, MPC so far has shown promise primarily in simplified settings (e.g., Go) where environment dynamics are simple and slower decision-making is rewarded, but struggles to extend to real-world tasks (e.g., customer service), which typically involve complex dynamics and require mixture of shortand long-term decision-making. On the other hand, RL is general, flexible, and scalable approach to training agents without restrictions on the decision-making method or search horizon. In particular, one can replace the true universe with world model for exploration and learning (as discussed in 2.2). Below we describe an RL setup (Figure 9, right) where the agent interacts with the world model instead in each time step with world state representation ˆst (which may be of the environment [17]: encoded from some observation data ot or completely imagined from scratch), the agent π takes action at pπ(at ˆst) and the world model simulates the next state ˆst+1 pf (ˆst+1 ˆst, at). This may repeat until some rollout horizon or in never-ending manner. Computing the reward at each step r(g, ˆst) based on goal g, the optimal agent π may thus learn by maximizing the expected discounted cumulative reward (with well-formed discount schedule {γk} k=t to ensure numerical stability) as defined below: π (ˆst) = arg max π Eπ,f (cid:34) (cid:88) k=t γkr(g, ˆsk) (cid:35) ˆst (cid:12) (cid:12) (cid:12) (cid:12) (9) = arg max π lim (cid:88) (cid:88) γkr(g, ˆsk) (at,ˆst+1,...,ˆsT ) k=t (cid:89) i=t pπ(ai ˆsi) (cid:124) (cid:125) (cid:123)(cid:122) select action pf (ˆsi+1 ˆsi, ai). (cid:123)(cid:122) (cid:125) (cid:124) simulation with world model Operationally, as we show above, both MPC and RL can use world models, the former only for decision-making while the latter also for learning. We recognize the latter as part of broader paradigm: learning from experience [20]. In this framework, the agent model continually interacts with and learns from an infinite space of imagined worlds, simulated by world model. The countless hypothetical trajectories can then be used to train the agent via RL, imitation learning, or other learning signals that make full use of all experiences. These updates can occur entirely offline, using batches of rollouts from the world model rather than interacting with the real environment. Compared to MPC which is computationally expensive at decision-making time, RL with world model (as in Equation 9) shifts part of the computational cost to the training phase. Instead of planning from scratch at each step, the world model is used offline to train policy network that can later be reused for fast action selection at every state. Crucially, Both RL and MPC along with the world model may be included as components inside the agent model that must carry both deliberate planning and reactive actioning, while another fast policy can still learn to react 17 Figure 10: Illustration of the architecture of PAN-World, our proposed framework for generalpurpose world modeling. At each time point, PAN-World estimates the world state ˆs based on the current sensory observation o, predicts the future world states ˆs based on proposed actions for agentic reasoning, and reconstructs the future observations ˆo for generative supervision and external usage. swiftly when needed. Whereas recent work like o1, o3, and R1 [16] can be seen as special cases in math and coding where model-free policy-based methods enables fast-reacting behaviors, our view is to generalize this pattern: Agents should both reason with and learn from the worlds that they simulate, allowing flexible decision-making, continual improvement, and emergence of intelligence with experience. In summary, as demonstrated above, unlike MPC, RL can learn policy function that reflects long-term cumulative rewards, enabling more strategic reasoning over extended time horizons. This makes it applicable in practical settings like goal-conditioned robotic manipulation, multi-turn dialog systems, or autonomous driving."
        },
        {
            "title": "5 The PAN World Model",
            "content": "Drawing from the critiques made on existing world model frameworks, we arrive at the following conclusion regarding design principles for general-purpose WM: 1) use data from all modalities of experience; 2) employ mixed continuous and discrete representation; 3) adopt hierarchical generative model paradigm with an extended-LLM backbone (for discrete concept-based reasoning), as well as generative embedding predictive module (for continuous gradient-based reasoning), as the reasoning engines; 4) train over generative loss grounded in observation data; and 5) apply world model to simulate experiences for training agents using reinforcement learning. We conclude our critiques of WM with brief preview of an new architecture, PAN Physical, Agentic, and Nested world model, based on the aforementioned designed principles. Details and preliminary results of PAN will be presented in [19]."
        },
        {
            "title": "5.1 A Motivating Usecase",
            "content": "A truly versatile and generalizable WM must be grounded in tasks that reflect the full complexity of real-world reasoning demands. These may include variations in data modality (e.g., verbal, visual, sensory), spatio-temporal scope (from one second in room to days in whole country), action granularity (e.g., fine motor control, bodily movement, expressive gestures), and decision scale (from 18 immediate actions, tactics, to long-term strategies). While many existing WMs are demonstrated on simplified, toy tasks (e.g., manipulating kitchen tools) and simple scenarios (seconds to minutes of video in 3D worlds), such settings fall short of capturing the richness of real-world agentic experience. WM designed around these tasks, therefore, is unlikely to scale to complexities required for real-world applications. For instance, WM that only enables tool manipulation in kitchen is inadequate for planning and executing an end-to-end dinner service in restaurant. In contrast, PAN is motivated by more complex and realistic use case: mountaineering expedition. In this setting, the WM must internalize multimodal sensory inputs and simulate future world states in service of demanding, structured task. This task naturally decomposes into multiple interconnected sub-tasks at different levels: high-level decisions like gear selection, route and segment planning, navigation, weather assessment, pacing, etc., low-level actions like climbing, roping, and precise motor control in response to terrain and surface conditions, and social coordination with teammates through verbal and non-verbal communication, among others. The mountaineers sensory experience includes not only sights and sounds snowfields, cliffs, partner calling out from ahead but also tactile and motion signals such as wind, cold, and muscular strain. The actionable world states that drive purposeful reasoning, such as terrain affordances, team dynamics, or latent risks, exist at multiple levels of abstraction beneath them. PAN thus begins by ingesting this continuous stream of multimodal signals: inputs from vision, sound, temperature, motion, potentially even pain, which may each be relevant for different tasks but together constitute holistic reality."
        },
        {
            "title": "5.2 The PAN Architecture",
            "content": "Following mixed representation and multi-scale reasoning principle, PAN processes multimodal sensory inputs using its Sensory Encoder (h), which maps inputs via both discrete and continuous pathways to capture complementary aspects of the world. On one hand (Figure 10), tokenizer hierarchically maps raw signals into discrete tokens grounded in PANs vocabulary, which spans multiple levels of abstraction. These tokens may consist of abstract tokens learned via VQ-VAEstyle approach [39] as well as concrete words drawn from natural language. The representation may include flexible number of such tokens to compactly reflect deep layers of world information: Where am I? Who is with me? What tools do have? What is my emotional state? As discussed in 4.2, this form of representation can be sufficient to capture relevant information, even for continuous data like video. On the other hand (Figure 6, right), PAN may also encode low-level details into continuous latent embeddings to capture the full nuanced perceptual experience where necessary. Together, these tokens and embeddings form layered estimate of the world state ˆs = {ˆsi}N i=1 over which PAN performs simulation and purposeful reasoning. Given proposed action (e.g., buckle the carabiner to my harness), PAN predicts the next world state ˆs (e.g., conceptual state like am safely anchored, or physical state like that rope is tightening) using World Model Backbone (f ) built on an enhanced LLM and diffusionbased next-latent-embedding predictor (Figure 10). This design is concrete instantiation of the GLP architecture introduced earlier in 4.3 (Figure 6, right). The LLM-based backbone reasons over both natural language tokens and learned conceptual vocabulary some explicit (e.g., particular shape of icepicks), others implicit or emergent (e.g., feelings that arise when sharing hardearned knowledge). This supports broad generalization across domains [12]. During both training and inference, the model can also dynamically extend its vocabulary by introducing new tokens or merging existing ones to maximize the prediction quality. On the other hand, the diffusion-based embedding predictor is responsible for fast, low-level, and often subconscious reasoning that are critical for embodied responses yet difficult to express in language. This module simulates detailed perceptual experiences, such as whether foothold is secured, or how the body might shift its weight during climb [48]. Learned Switch allows PAN to predict the next world state hierarchically ({ˆs i=1) by adaptively combining the LLM-based i}N 19 backbone, multiple vocabularies, and the diffusion-based embedding predictor, depending on task demands. These mechanisms enable PAN-WM to scale across spatio-temporal scopes and action granularities as is required for general usability from concrete physical scenarios like mountain climbing and social interaction, to abstract, far-reaching strategic consequences like nationwide policy changes. To supervise its predictions, and to allow the trained WM to interface with external agents (or human) who may use its outputs, PAN reconstructs the next observation ˆo using Multimodal Decoder (g) and compares it to the actual observation o. Crucially, the decoders outputs are not limited to videos, but includes full sensory experience, which may include sound, temperature, motion, pain, other embodied signals, and/or even text. As discussed in 4.3 and 4.4, this generative supervision grounds the predicted world state ˆs in sensory reality, ensuring that the representation retains all possible information while allowing residual variability to be absorbed by the decoder g. This approach contrasts sharply with models trained on next-representation prediction (e.g., V-JEPA 2 [3]), which supervise the world model purely in latent space. The latter objectives are, at best, loose surrogates of generative objectives and prone to representation collapse or unidentifiability, as they lack grounding in real sensory input. Formally, PAN models the conditional distribution of the next observation given the current observation and proposed action as below: pPAN(o o, a) (cid:88) ph(ˆs o) (cid:124) (cid:123)(cid:122) (cid:125) encoder ˆs,ˆs (cid:88) (cid:89) pf (ˆs ˆs, a) (cid:124) (cid:125) (cid:123)(cid:122) world model (cid:89) pg(o ˆs) (cid:123)(cid:122) (cid:125) (cid:124) decoder pf (ˆs ˆs <j, ˆs, a) pg(o ˆs) ph(ˆsi ˆs<i, o) = = ˆs,ˆs i=1 (cid:124) (cid:123)(cid:122) hierarchical world state inference (cid:125) j=1 (cid:124) (cid:125) (cid:123)(cid:122) switch-based next-state prediction Overall, with its hierarchical, multi-level, and mixed representation architecture, and an encoderdecoder pipeline that threads perception o, action o, belief ˆsi, simulated belief ˆs i, and simulated worlds o, PAN represents general-purpose generative model for simulating actionable real-world possibilities for an agent to perform purposeful reasoning, as we will briefly elude to in 5.4. PAN does not sidestep the variability in raw perceptual input, but instead modularizes and organizes it. This enables richer internal simulation of every layer of experience for more powerful agent reasoning and planning."
        },
        {
            "title": "5.3 Training the PAN World Model",
            "content": "It should be obvious from the mountaineering example that simply watching videos is not enough to learn all the reasoning capability needed to accomplish the final goal, which can take days of time and hundreds upon thousands of actions and steps from the onset, and is built on rich background knowledge about geography, climate, equipments, sports, and even history. The training of PANWM shall use divide-and-conquer approach that begins with pretraining each of its modules independently through self-supervision (e.g., LLM for text data, and diffusion model for video data). These modalityand level-specific modules are then aligned or integrated during the post-training phase using multimodal data, cascaded embedding, and gradient propagation. Modules operating over continuous embeddings can be trained using standard gradient-based optimization techniques. In contrast, components using discrete tokens may benefit from gradient-free methods similar to reinforcement learning [16]. As proven in 4.4, generative, data-reconstruction-based objectives are grounded in observed data and provide stable and reliable learning signal for the entire system. key strength of the PAN architecture is its data efficiency, because of its use of multi-scale and stratified view of the world. In the mountaineering task, when reasoning about nevigation and 20 path-finding, the world states do not need to include snow or rock surface details at pixel level, whereas when deciding where to lay hands or feet during claiming, the world states can ignore geographical contexts. Therefore it is not necessary for WMs simulating highly complex possibilities to be contingent on data that capture all such complexity all at once (e.g., videos that visually cover mountaineering at all levels), but to take advantage of data of different kinds offering information at different levels (e.g., travel book for trail guide and map reading, indoor video for rock climbing and gear usage). After all, it is unrealistic to expect large corpus of videos that comprehensively cover all aspects of alpine climbing. Many general capabilities (e.g., social reasoning, travel planning, cold weather survival) can be learned from abundant language data. Only directly embodied skills (e.g., foot placement, rock climbing technique) require physical data like videos or proprioception, which can be obtained in controlled or simulated environments. Indeed, PANs pretrain-then-align/integrate strategy enables sensory information (e.g., from video diffusion model) to be grounded within higher-level, richer contexts through LLMs, thereby facilitating cross-modal generalization. At the same time, abstract knowledge embedded in LLMs can be anchored to concrete, embodied experiences, increasing the precision and realism of the systems reasoning [49]. The result is WM that, like humans, derives commonsense understanding from diverse set of experiences. Consequently, it does not require exhaustive training data for each specific task (e.g., mountaineering or autonomous driving), but can instead draw on conceptual knowledge acquired from many domains. We believe this kind of general-purpose WM is well-suited to simulating experience for agent decision-making and/or training, as elaborated below."
        },
        {
            "title": "5.4 Towards Agentic Reasoning with PAN",
            "content": "Recall in 2 we outlined an agent architecture for simulative reasoning using the world model. PAN fits naturally into this paradigm, functioning not merely as video generator, but as rich internal sandbox for simulation, experimentation, and foresight. As illustrated in Figure 11, PAN-Agent, prompted by goal and in reception of continuous stream of perception from the real world, is expected to come up with actions, plans (sequence of actions), or strategies (plans in light of counterfactual situations), which would involve using the PAN-WM to precompute and cache diverse set of possible world states, plausible actions within those states, and their simulated outcomes [9]. At decision time, rather than only relying on performing expensive real-time simulations, the agent may consult this cache and select actions based on current beliefs and expected rewards. This decoupling of simulation from action selection allows the agent to reason more deliberately, adaptively, and selectively, avoiding the rigidity of purely reactive policy in end-to-end RL and the computational burden of constant forward rollout in MPC. The result is an agent that more closely mirrors human-like cognition planning ahead, navigating uncertainty, and acting with both flexibility and foresight. We believe this represents promising step towards agents with richer forms of agency one capable not only of simulative reasoning, but also of choosing among imagined futures with intent. Such agents may ultimately approach the adaptability, resilience, and autonomy characteristic of human intelligence."
        },
        {
            "title": "6 Conclusion",
            "content": "We have examined the foundations, debates, and practical challenges in the pursuit of generalpurpose world modeling. Our intent in writing this critique is to inspire further discussions and deeper reflections on the following fundamental questions: what is indeed world model?, what is world model for, and how to build world model of practical and general utility? We argue that WM is not about video or virtual reality generation, but is about simulating all possibilities in real world; and such outcome is not for visual pleasure, but for purposeful reasoning; and current paradigms and efforts towards this end remain primitive. 21 Figure 11: Illustration of our proposed simulative reasoning agent powered by the PAN World Model. Unlike traditional RL agents that rely on reactive policies, or model-precitive control (MPC) agents that expensively simulates futures at decision time, this agent leverages cache of precomputed simulations generated by the PAN WM. During decision-making, the agent selects actions based on its current beliefs and expected outcomes, enabling more efficient, flexible, and intentional form of planning, which, as we argue, is closer to the flexibility of human reasoning. It is our wish that, by offering critical, but analytical and constructive dissections of some of the most popular thoughts on how to build World Models; and by presenting our alternative proposal the PAN architecture, we can spark further advancements in both theory and implementation of stronger world models. The PAN world model we previewed is framework designed for simulating all possible worlds and enabling agent reasoning and planning. By combining multimodal data, hierarchical representations, multi-level generative modeling, and observation-grounded objectives, PAN supports long-horizon simulation and flexible decision-making across tasks in the physical and cyber domains. Looking ahead, the PAN framework opens several promising directions: scaling from single-agent to multi-agent simulations (e.g., collective behaviors of business, society, consequences to public health), extending across time scales (e.g., from milliseconds to millennia), improving simulation fidelity across modalities, and enabling agent learning directly through imagined experience. As world models increasingly serve as the substrate for reasoning, imagination, and action, we believe that frameworks like PAN, with its experience grounding, multi-layer abstraction, and empirical scalability, offers compelling foundation for the development of robust, general-purpose AI."
        },
        {
            "title": "References",
            "content": "[1] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. [2] Figure AI. Helix: vision-language-action model for generalist humanoid control, February 2025. Accessed: 2025-05-01. [3] Mido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Mojtaba, Komeili, Matthew Muckley, Ammar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, Sergio Arnaud, Abha Gejji, Ada Martin, Francois Robert Hogan, Daniel Dugas, Piotr Bojanowski, Vasil Khalidov, Patrick Labatut, Francisco Massa, Marc Szafraniec, Kapil Krishnakumar, Yong Li, Xiaodong Ma, Sarath Chandar, Franziska Meier, Yann LeCun, Michael Rabbat, and Nicolas Ballas. V-jepa 2: Self-supervised video models enable understanding, prediction and planning, 2025. [4] Linden J. Ball. Hypothetical Thinking, page 514528. Cambridge Handbooks in Psychology. Cambridge University Press, 2020. [5] Adrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen, Michael Rabbat, Yann LeCun, Mahmoud Assran, and Nicolas Ballas. Revisiting feature prediction for learning visual representations from video. arXiv preprint arXiv:2404.08471, 2024. [6] Lisa Feldman Barrett. How emotions are made: The secret life of the brain. Pan Macmillan, 2017. [7] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators. 2024. URL https://openai. com/research/video-generation-models-as-world-simulators, 3:1, 2024. [8] Yuri Chervonyi, Trieu Trinh, Miroslav Olˇsak, Xiaomeng Yang, Hoang Nguyen, Marcelo Menegali, Junehyuk Jung, Vikas Verma, Quoc Le, and Thang Luong. Gold-medalist performance in solving olympiad geometry with alphageometry2. arXiv preprint arXiv:2502.03544, 2025. [9] Brandon Chiou, Mason Choey, Mingkai Deng, Jinyu Hou, Jackie Wang, Ariel Wu, Frank Xu, Zhiting Hu, Hongxia Jin, Li Erran Li, Graham Neubig, Yilin Shen, and Eric P. Xing. Reasoneragent: fully open source, ready-to-run agent that does research in web browser and answers your queries, February 2025. [10] Terrence Deacon. The symbolic species: The co-evolution of language and the brain. WW Norton & Company, 1998. [11] Decart, Julian Quevedo, Quinn McIntyre, Spruce Campbell, Xinlei Chen, and Robert Wachen. Oasis: universe in transformer. 2024. [12] Mingkai Deng, Jinyu Hou, Zhiting Hu, Graham Neubig, Hongxia Jin, Yilin Shen, and Eric P. Xing. Simura: Towards general goal-oriented agent via simulative reasoning architecture with llm-based world model, 2 2025. [13] Chelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion, 2017. [14] Rafael Gonzales and Paul Wintz. Digital image processing. Addison-Wesley Longman Publishing Co., Inc., 1987. [15] Google DeepMind. Veo. https://deepmind.google/models/veo/, 2025. Latest release version: Veo 3 (May 20, 2025), accessed: 2025-06-10. 23 [16] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [17] David Ha and Jurgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018. [18] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023. [19] Zhiting Hu and Eric Xing. Pan: physical, agentic, and nested world model for general and super intelligence. Manuscript in preparation, 2025. [20] Zhiting Hu and Eric Xing. Toward standard model of machine learning. arXiv preprint arXiv:2108.07783, 2021. [21] Philip Johnson-Laird. Mental models and human reasoning. Proceedings of the National Academy of Sciences, 107(43):1824318250, 2010. [22] Anssi Kanervisto, Dave Bignell, Linda Yilin Wen, Martin Grayson, Raluca Georgescu, Sergio Valcarcel Macua, Shan Zheng Tan, Tabish Rashid, Tim Pearce, Yuhan Cao, et al. World and human action models towards gameplay ideation. Nature, 638(8051):656663, 2025. [23] Yann LeCun. path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62(1):162, 2022. [24] David JC MacKay. Information theory, inference and learning algorithms. Cambridge university press, 2003. [25] Willis E. McNelly, editor. The Dune Encyclopedia. Berkley Publishing Group, New York, 1984. Published by arrangement with the author. Designed by Jeremiah B. Lighter. [26] Chancharik Mitra, Brandon Huang, Trevor Darrell, and Roei Herzig. Compositional chain-ofthought prompting for large multimodal models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1442014431, 2024. [27] Donald Ornstein and Benjamin Weiss. Statistical properties of chaotic systems. Bulletin of the American Mathematical Society, 24(1):11116, 1991. [28] Jack Parker-Holder, Philip Ball, Jake Bruce, Vibhavari Dasagi, Kristian Holsheimer, Christos Kaplanis, Alexandre Moufarek, Guy Scully, Jeremy Shar, Jimmy Shi, Stephen Spencer, Jessica Yung, Michael Dennis, Sultan Kenjeyev, Shangbang Long, Vlad Mnih, Harris Chan, Maxime Gazeau, Bonnie Li, Fabio Pardo, Luyu Wang, Lei Zhang, Frederic Besse, Tim Harley, Anna Mitenkova, Jane Wang, Jeff Clune, Demis Hassabis, Raia Hadsell, Adrian Bolton, Satinder Singh, and Tim Rocktaschel. Genie 2: large-scale foundation world model. 2024. [29] Lawrence Perko. Differential equations and dynamical systems, volume 7. Springer Science & Business Media, 2013. [30] Steven Pinker. The language instinct (1994/2007). 2007. [31] Reuven Y. Rubinstein. Optimization of computer simulation models with rare events. European Journal of Operational Research, 99(1):89112, 1997. [32] Lloyd Russell, Anthony Hu, Lorenzo Bertoni, George Fedoseev, Jamie Shotton, Elahe Arani, and Gianluca Corrado. Gaia-2: controllable multi-view generative world model for autonomous driving. arXiv preprint arXiv:2503.20523, 2025. 24 [33] David Silver, Aja Huang, Chris Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484489, 2016. [34] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi by self-play with general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815, 2017. [35] Vlad Sobal, Wancong Zhang, Kynghyun Cho, Randall Balestriero, Tim GJ Rudner, and Yann LeCun. Learning from reward-free offline data: case for planning with latent dynamics models. arXiv preprint arXiv:2502.14819, 2025. [36] Ralph Strauch. Battle simulation for command and control training. 1982. [37] Qiao Sun, Xin Huang, Junru Gu, Brian Williams, and Hang Zhao. M2i: From factored marginal trajectory prediction to interactive prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 65436552, 2022. [38] Richard Sutton, Andrew Barto, et al. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998. [39] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. [40] Nicole Van Hoeck, Patrick Watson, and Aron Barbey. Cognitive neuroscience of human counterfactual reasoning. Frontiers in human neuroscience, 9:420, 2015. [41] Balakrishnan Varadarajan, Ahmed Hefny, Avikalp Srivastava, Khaled Refaat, Nigamaa Nayakanti, Andre Cornman, Kan Chen, Bertrand Douillard, Chi Pang Lam, Dragomir Anguelov, et al. Multipath++: Efficient information fusion and trajectory aggregation for behavior prediction. In 2022 International Conference on Robotics and Automation (ICRA), pages 78147821. IEEE, 2022. [42] Lev Vygotsky. Thought and language, volume 29. MIT press, 2012. [43] Xingyao Wang, Boxuan Li, Yufan Song, Frank Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et al. Openhands: An open platform for ai software developers as generalist agents. In The Thirteenth International Conference on Learning Representations, 2024. [44] Yao Wang, Jorn Ostermann, and Ya-Qin Zhang. Video processing and communications, volume 1. Prentice hall Upper Saddle River, 2002. [45] Waymo. Waymo Driver: Self-Driving Car Technology for Reliable Ride, 2025. Accessed: 2025-05-01. [46] Grady Williams, Andrew Aldrich, and Evangelos Theodorou. Model predictive path integral control using covariance variable importance sampling. arXiv preprint arXiv:1509.01149, 2015. [47] World Labs Technical Staff. Generating worlds. https://www.worldlabs.ai/blog, 2025. Accessed: 2025-06-10. [48] Jiannan Xiang, Guangyi Liu, Yi Gu, Qiyue Gao, Yuting Ning, Yuheng Zha, Zeyu Feng, Tianhua Tao, Shibo Hao, Yemin Shi, et al. Pandora: Towards general world model with natural language actions and video states. arXiv preprint arXiv:2406.09455, 2024. 25 [49] Jiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, and Zhiting Hu. Language models meet world models: Embodied experiences enhance language models. Advances in neural information processing systems, 36:7539275412, 2023. [50] Gaoyue Zhou, Hengkai Pan, Yann LeCun, and Lerrel Pinto. Dino-wm: World models on pretrained visual features enable zero-shot planning. arXiv preprint arXiv:2411.04983, 2024. Proof for Theorem 1 Proof. We will prove the countrapositive that fϵ(x) = fϵ(x) x ϵ. t=1,d=1(xtd Based on the conditions, we have it that x = (cid:80)T,D . satisfied when xtd td ϵ td)2 ϵ = ϵ2, which is Because xt := K2 that x ϵ, we need to divide the interval ϵ 4 , we have that xtd 2 [1, D]. Therefore, to satisfy the condition (cid:104) into subintervals with length at most Kϵ1 of them. Further, because there are such intervals to divide, the support of our code should be at least as large as the number of required subintervals as below: , of which there are 2 , D (cid:105) 2 VM Kϵ1T In the two cases below, we will attempt to construct this support by scaling the vocabulary size and the code length individually. It is technically also possible to scale both at the same time, but we focus on the existence for this proof, and leave more efficient solutions to future work. Case 1 (Modality Tokenizer): Take = , and construct vocabulary with size Mϵ = Kϵ1D. We then define the encoding function for as fϵ(x) = (c1, . . . , cT ) where ct V. We define the index of ct as D-digit number of base-Mϵ (d1(ct), d2(ct), . . . , dD(ct)), where dn(ct) = xtn+ K/2 (cid:104) ϵ/ (cid:105) Kϵ1 , and thus enables the representation of 0, digits. This way, given fϵ(x) = fϵ(x), since ct = which implies that xtn D. Thus, we have tn ϵ/ is the n-th digit. Note that this is well defined because dn(ct) Kϵ1D unique values for ct using t) [1, D], [1, ], we have dn(ct) = dn(c x = T,D (cid:88) (xtd td)2 T,D (cid:88) t=1,d=1 t=1,d= ϵ2 = ϵ (10) Case 2 (Finding Language Expression): Assume that the vocabulary has fixed size = (which applies to wide range of natural and machine languages). Take sequence length Nϵ = DlogM Kϵ1 and define fϵ(x) = (c1, . . . , cNϵ ), ct V, We map each xtd to number utd [0, 1] represented by digits of base-N , where the n-th digit dn(utd) = cs+n. In this manner, xtd will correspond to subsequence (cs+1, cs+2, . . . , cs+L), where = (td 1)logM Kϵ1. Thus, we have that Kϵ1 and = logN (cid:32) (cid:88) xtd = n=1 dn(utd) 1 2 (cid:33) K. 26 (11) Now, given x, where fϵ(x) = fϵ(x), we have fϵ(x)td = fϵ(x)td and dn(utd) = dn(utd), so utd and td must lie in the same interval formed by numbers for which all digits dn are identical as below: [u : dn(u) = vn, = 1, . . . , L] = (cid:34) (cid:88) n= vn , (cid:88) n=1 (cid:33) vn +"
        },
        {
            "title": "1\nM L",
            "content": "(12)"
        },
        {
            "title": "Thus we have",
            "content": "xtd td = (cid:33) (cid:32) (cid:88) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)M K (cid:12) n=1 1 2 dn(utd) (cid:12) (cid:12) (cid:12)M logM (cid:12) (cid:12) (cid:12) = (cid:32) (cid:88) n=1 Kϵ1 (cid:33) 1 2 dn(u td) (cid:12) (cid:12) (cid:12)M logM (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:32) (cid:88) = (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) n=1 Kϵ1 dn(utd) (cid:12) ϵ (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) td) dn(u (cid:33) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:88) n=1 (cid:12) (cid:12) (cid:12) (cid:12) And by extension x ϵ Proof for Proposition 1 Proof. We will prove the proposition by constructing such (h, ) pair such that Llatent(h, ) = minh,f Llatent(h, ). ((h, ) is the global minimum of Llatent.) This could be simply done by finding arbitrary Rn and letting h(o) = for all and (c, a) = for all A. Then, we have that Llatent(h, ) = 0. Since Llatent is distance measurement, we have that Llatent 0, which means that (h, ) is the global minimum. Proof for Proposition Proof. We prove the proposition by showing that, with the generative loss Lgen, there is no solution that can be both degenerate and optimal at the same time. More specifically, for any degenerate solution (h, ), and fixed g, there must be way to construct another solution (h, ) which yields lower loss value. More formally, without loss of generality, assume that we have: Some arbitrary constant Rn; : such that h(o) = for all O; : such that (c, a) = for all A; fixed decoder : O; dataset containing at least two distinct triplets: (o1, a1, o2), (o3, a3, o4) with o2 = o4. Assuming all the function families are sufficiently expressive. Then, under the degenerate solution (h, ), for any (o, a, o) D, we have: (h(o), a) = g(f (c, a)) = g(c). Therefore, the generative loss becomes: Lgen(h, , g) = E(o,a,o)D [g(c) o] . Note that this loss is non-zero if there exists at least one (o, a, o) such that g(c) = o, which must be the case here because o2 = o4 while g(c) is fixed across all inputs. We now construct an alternative pair (h, ) such that: 27 For all (o, a, o) {(o1, a1, o2), (o3, a3, o4)}, define h(o) = and (c, a) = c, i.e., h, behave identically to the degenerate solution. For the two special cases, define s1, s2 such that g(s1) = o2 and g(s2) = o4. (This is possible since is fixed and assumed expressive enough to approximate o2 and o4.) Then, set h(o1) = ˆs1, (ˆs1, a1) = s1, and h(o3) = ˆs2, (ˆs2, a3) = s2, where ˆs1, ˆs2 are any distinct intermediate representations to encode o1 and o3. Now, consider the loss under this construction: At (o1, a1, o2): h(o1) = ˆs1, (ˆs1, a1) = s1, g(s1) = o2, so loss is o2 o2 = 0. At (o3, a3, o4): similarly, loss is zero for and . While for and , there must be one of (o1, a1, o2) and (o3, a3, o4) at which the loss is strictly positive. In other words, g(c) o2 + g(c) o4 > At all other datapoints, the loss is unchanged (equal to the degenerate solutions loss) because we did not modify the outputs for those inputs. Hence, the total loss under (h, ) is strictly less than under (h, ): Lgen(h, , g) < Lgen(h, , g) This indicates that the degenerate solution must not be optimal, thus proving that it cannot be global minimizer when is trained or expressive and when the dataset includes different outputs for different inputs. Proof of Theorem Proof. Because the conditional distributions in the statement are all isotropic Gaussians, we can use them to construct the following transition distributions (assuming ˆq is the empirical data distribution): Latent Prediction phf : ˆs o, (f (h(o), a), I) Latent Target ˆqh : ˆs (h(o), I) Observation Prediction phf : ˆo o, (g (h(o), a), I) It emerges that the two loss functions described above can be expressed as scaled KL divergences of those distributions, as shown below: Llatent = (h(o), a) h(o) = (cid:113) 2DKL(ˆqh(ˆs o) phf (ˆs o, a)), Lgen = (h(o), a) = (cid:113) 2DKL(ˆq(o) phf g(o o, a)). Consider applying the encoder transition to ˆq and phf g. In the former case, we will recover the latent target ˆqh, and in the latter case we will have performed roundtrip from latent to observation, and back to latent, resulting in the following conditional distribution: Roundtrip Latent Prediction phf gh : ˆs o, (h (h(o), a), I). Applying the data processing inequality gives the following relation: DKL(ˆqh(ˆs o) phf gh(ˆs o, a)) DKL(ˆq(o) phf g(o o, a)), 28 And because the left-hand side is 1 plug them into the inequality above and get: 2 (h(o), a) h(o)2 and the right-hand side is 1 2 gen, we (h(o), a) h(o) Lgen. (13) Next, because the encoder and decoder satisfy the roundtrip consistency of g(ˆs) ˆs ϵ, plugging in ˆs = (h(o), a) gives: (h(o), a) (h(o), a) ϵ. (14) Finally, we apply the triangle inequality to upper-bound the latent loss as below: Llatent = (h(o), a) h(o) (h(o), a) (h(o), a) (cid:125) (cid:123)(cid:122) ϵ by Inequality 14 (cid:124) + (h(o), a) h(o) (cid:123)(cid:122) (cid:125) Lgen by Inequality (cid:124) Lgen + ϵ When g(ˆs) = ˆs for all ˆs S, we have that : Im(g) is an injective function which maps each point in to unique point in its image Im(g). On the other hand, is the left inverse of on Im(g), which maps each ˆo Im(g) to ˆs s.t. g(ˆs) = ˆo. Thus is bijective in Im(g) by mapping each element Im(g) to exactly one ˆs S. This means defines parameter transformation Th where Th(o) = h(o) for all Im(g). Because ˆq(o) and phf g(o o, a) are supported entirely inside Im(g) and KL divergence is invariant under parameter transformation, we have: 1 2 gen = DKL(ˆq(o) phf g(o o, a)) = DKL(ˆqh(ˆs o) phf gh(ˆs o, a)) = = = 1 2 1 2 1 (h(o), a) h(o)2 (h(o), a) h(o)2 (h g(ˆs) = ˆs) L2 latent. Thus we have Llatent = Lgen."
        }
    ],
    "affiliations": [
        "Halıcıoglu Data Science Institute, UC San Diego",
        "Institute of Foundation Models, Mohamed bin Zayed University of Artificial Intelligence",
        "School of Computer Science, Carnegie Mellon University"
    ]
}