{
    "paper_title": "Does DINOv3 Set a New Medical Vision Standard?",
    "authors": [
        "Che Liu",
        "Yinda Chen",
        "Haoyuan Shi",
        "Jinpeng Lu",
        "Bailiang Jian",
        "Jiazhen Pan",
        "Linghan Cai",
        "Jiayi Wang",
        "Yundi Zhang",
        "Jun Li",
        "Cosmin I. Bercea",
        "Cheng Ouyang",
        "Chen Chen",
        "Zhiwei Xiong",
        "Benedikt Wiestler",
        "Christian Wachinger",
        "Daniel Rueckert",
        "Wenjia Bai",
        "Rossella Arcucci"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The advent of large-scale vision foundation models, pre-trained on diverse natural images, has marked a paradigm shift in computer vision. However, how the frontier vision foundation models' efficacies transfer to specialized domains remains such as medical imaging remains an open question. This report investigates whether DINOv3, a state-of-the-art self-supervised vision transformer (ViT) that features strong capability in dense prediction tasks, can directly serve as a powerful, unified encoder for medical vision tasks without domain-specific pre-training. To answer this, we benchmark DINOv3 across common medical vision tasks, including 2D/3D classification and segmentation on a wide range of medical imaging modalities. We systematically analyze its scalability by varying model sizes and input image resolutions. Our findings reveal that DINOv3 shows impressive performance and establishes a formidable new baseline. Remarkably, it can even outperform medical-specific foundation models like BiomedCLIP and CT-Net on several tasks, despite being trained solely on natural images. However, we identify clear limitations: The model's features degrade in scenarios requiring deep domain specialization, such as in Whole-Slide Pathological Images (WSIs), Electron Microscopy (EM), and Positron Emission Tomography (PET). Furthermore, we observe that DINOv3 does not consistently obey scaling law in the medical domain; performance does not reliably increase with larger models or finer feature resolutions, showing diverse scaling behaviors across tasks. Ultimately, our work establishes DINOv3 as a strong baseline, whose powerful visual features can serve as a robust prior for multiple complex medical tasks. This opens promising future directions, such as leveraging its features to enforce multiview consistency in 3D reconstruction."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 7 6 4 6 0 . 9 0 5 2 : r Does DINOv3 Set New Medical Vision Standard? Comprehensive Benchmark on 2D/3D Classification and Segmentation Che Liu1, Yinda Chen2, Haoyuan Shi2, Jinpeng Lu2, Bailiang Jian7, Jiazhen Pan5,7, Linghan Cai3, Jiayi Wang4, Yundi Zhang5,7, Jun Li7,8, Cosmin I. Bercea7,9, Cheng Ouyang5, Chen Chen6, Zhiwei Xiong2, Benedikt Wiestler7,8, Christian Wachinger7,8, Daniel Rueckert1,7,8, Wenjia Bai1, Rossella Arcucci1 1Imperial College London 2University of Science and Technology of China 3Dresden University of Technology 4University of Erlangen-Nuremberg 5University of Oxford 6University of Sheffield 7Technical University of Munich (TUM) 8Munich Center for Machine Learning 9Helmholtz AI and Helmholtz Munich Equal contribution, Corresponding author The advent of large-scale vision foundation models, pre-trained on diverse natural images, has marked paradigm shift in computer vision. However, how the frontier vision foundation models efficacies transfer to specialized domains remains such as medical imaging remains an open question. This report investigates whether DINOv3, state-of-the-art self-supervised vision transformer (ViT) that features strong capability in dense prediction tasks, can directly serve as powerful, unified encoder for medical vision tasks without domain-specific pre-training. To answer this, we benchmark DINOv3 across common medical vision tasks, including 2D/3D classification and segmentation on wide range of medical imaging modalities. We systematically analyze its scalability by varying model sizes and input image resolutions. Our findings reveal that DINOv3 shows impressive performance and establishes formidable new baseline. Remarkably, it can even outperform medical-specific foundation models like BiomedCLIP and CT-Net on several tasks, despite being trained solely on natural images. However, we identify clear limitations: The models features degrade in scenarios requiring deep domain specialization, such as in Whole-Slide Pathological Images (WSIs), Electron Microscopy (EM), and Positron Emission Tomography (PET). Furthermore, we observe that DINOv3 does not consistently obey scaling law in the medical domain; performance does not reliably increase with larger models or finer feature resolutions, showing diverse scaling behaviors across tasks. Ultimately, our work establishes DINOv3 as strong baseline, whose powerful visual features can serve as robust prior for multiple complex medical tasks. This opens promising future directions, such as leveraging its features to enforce multiview consistency in 3D reconstruction. Date: September 9, 2025 Correspondence: che.liu21@imperial.ac.uk"
        },
        {
            "title": "1 Motivation",
            "content": "The paradigm of large-scale modeling, exemplified by Large Language Models (LLMs) [1], has demonstrated that immense knowledge can be learned from vast, unannotated corpora through self-supervised objectives, leading to impressive scaling laws [2]. While this principle has been extended to and often assumed in computer vision, definitive answer on scaling laws for visual pre-training has been more elusive [3, 4, 5]. Recent works have questioned traditional scaling limits, but their evaluation was often focused on narrower tasks [6, 7, 8], leaving their general-purpose capabilities less explored. The DINO series [9, 10, 11], in contrast, has been instrumental in showing that self-supervised learning (SSL) can produce emergent visual representations of remarkable quality. Most recently, DINOv3 [11] has pushed this frontier by scaling the visual encoder up to 7B parameter scale on 1.7B images, demonstrating unprecedented generalization and strong performance across wide range of visual tasks. This progress in the natural image domain is highly relevant to medical image analysis, field that strongly relies on the quality of visual representations to capture subtle anomalies. Indeed, very recent work [12, 13] has shown promising performance using DINOv3 features on specific medical tasks, although the results often depend on careful hyperparameter tuning, leaving the broader impact less clear. The medical domain is characterized by vast diversity of imaging modalities, from 2D grayscale X-rays [14] to multi-channel RGB histopathology [15] and 3D volumetric scans [16], each demanding distinct visual understanding capabilities. This is further complicated with long-tailed distributions over conditions and the prohibitive cost and regulatory concerns associated with data collection. This heterogeneity and data scarcity highlight the imperative need for strong off-the-shelf vision representation extractors. However, the development of large-scale medical visual foundation model has been hampered by the relative scarcity of curated data due to cost, privacy, and regulatory concerns. Existing efforts, such as BiomedCLIP [17], have attempted to bridge this gap by training visual encoders on web-crawled medical images from research articles with text supervision. While valuable, this approach is limited by the quality and scalability of its data source and still relies on language supervision. This dichotomy leads us to series of fundamental questions: Q1: Can DINOv3s [11] natural-image representations excel on medical vision tasks? Q2: Does scaling visual pre-training on natural images improve performance in the medical domain? Q3: Are the benefits of scaling model size and dataset size transferable across diverse medical tasks and modalities?"
        },
        {
            "title": "2 Benchmark Setup",
            "content": "To evaluate the capabilities of DINOv3 [11] as generic off-the-shelf vision encoder for medical imaging, we designed multi-faceted benchmark assessing its performance across the most common tasks and data dimensionalities. key feature claimed in DINOv3 is the fine granularity of its features. Therefore, we are particularly interested in evaluating how this transfers to fine-grained medical imaging tasks such as image segmentation. Our benchmark is structured to cover wide range of modalities and tasks, including 2D classification, 3D classification, and 3D segmentation. The evaluation spans diverse modalities such as X-ray, Whole Slide Imaging (WSI), Electron Microscopy (EM), and volumetric data from Computed Tomography (CT), Magnetic Resonance Imaging (MRI), and Positron Emission Tomography (PET). We systematically analyse scalability by evaluating three different model scales (DINOv3-S, DINOv3-B, and DINOv3-L) across multiple input resolutions."
        },
        {
            "title": "2.1 Classification on 2D Medical Images",
            "content": "Image classification is foundational task in medical imaging, often used for diagnostic purposes on planar images. For these tasks, we process 2D images directly as input for the DINOv3 encoder. To accommodate DINOv3s 3-channel input requirement, single-channel grayscale images are replicated three times to create 3-channel tensor. For native RGB images, such as those from Whole Slide Imaging (WSI), we use the original data without modification. We benchmark the 2D classification performance on the following publicly available datasets: NIH-14 [14] This dataset is large collection of chest X-ray images for multi-label classification of 14 common thoracic pathologies, comprising 112,120 images from 30,805 unique patients. For our experiments, we adhere strictly to the official patient-wise data splits provided by the dataset creators to ensure reproducibility. RSNA-Pneumonia [18] This dataset from the RSNA Pneumonia Detection Challenge consists of 29,700 chest X-ray images for pneumonia classification. To ensure standardized comparison, we follow the data splitting methodology proposed in the MGCA [19], which provides well-defined protocol for training and testing. Camelyon16 [20] This dataset comprises 399 H&E-stained lymph node WSIs for breast cancer metastasis detection (tumor vs. normal). We adopt multi-fold cross-validation protocol on Camelyon16 [20] test set and additionally report results under the official split for comparability. Under the official split (270 train / 129 test slides), we train on the Camelyon16 [20] training set and report performance on its official test set. To assess cross-cohort generalization, we train models with five different random seeds on Camelyon16 [20] and evaluate them on the Camelyon17 [21] Unseen subset. Camelyon17 [21] This dataset is multi-center cohort for pathological N-staging. The official training set contains 100 patients with 5 labeled slides per patient. Each slide is annotated as negative, micro-metastasis, macro-metastasis, or isolated tumor cells (ITC). In our evaluation protocol, we use Camelyon17 [21] solely as an out-of-distribution testbed for models trained on Camelyon16 [20]. Since the official test annotations are unavailable, we evaluate on the official training set. Following prior practice [22], we remove the ITC slides and split Camelyon17 [21] into Seen (140 slides) and Unseen (324 slides) subsets based on center overlap with Camelyon16; we report generalization on the Unseen subset in the WSI tumor detection benchmark. BCNB [23] This dataset is an Early Breast Cancer Core-Needle Biopsy WSI dataset. It contains 1058 patients with molecular status labels: ER (831 positive / 227 negative), PR (790 positive/ 268 negative), HER2 (277 positive/ 781 negative), and Ki67 (156 positive / 902 negative). WSIs are annotated with tumor type, molecular status, number of lymph node metastases, and axillary lymph node (ALN) metastatic status, among others. Using CLAM [24], we remove background and crop each slide into 224224 patches at the native resolution, yielding on average 968 patches per slide. For the BCNB benchmark, we perform 5-fold cross-validation with 7:1:2 split ratio (train:val:test) within each fold, and evaluate five tasks: ALN metastatic status (N0 vs. N+), and the molecular status prediction of ER, PR, HER2, and Ki67. Unless otherwise specified, preprocessing and tiling are identical across tasks."
        },
        {
            "title": "2.2 Classification on 3D Medical Images",
            "content": "To perform 3D classification with 2D-native encoder like DINOv3, we adopt slice-wise feature extraction strategy. We process each 2D slice of 3D volume independently through the DINOv3 backbone to obtain feature embedding for that slice. The resulting set of slice embeddings is then aggregated into single feature vector representing the entire volume, typically via mean pooling [25]. As with the 2D tasks, grayscale slices are replicated across three channels before being fed into the model. For this task, the models performance is assessed using the following publicly available dataset: CT-RATE [16] This dataset is large-scale collection of 3D medical imaging, pairing 47k non-contrast CT volumes(20k patients) with their corresponding radiology reports. The dataset is annotated for 18 clinically significant abnormalities. For all of our experiments, we utilize the official data splits provided by the organizers for training and evaluation procedures, extracted features from every slice of these over 40,000 volumes and employed two methods for the downstream classification task: zero-shot k-nearest neighbors (k-NN) and linear probing. In the CT-RATE original work [16], the associated dataset is annotated with multi-label binary labels. This annotation scheme specifies for each clinical category whether case has particular condition or does not have that condition. Consequently, this task can be viewed as multi-label binary classification problem, where normal/abnormal binary classifications are performed across multiple categories."
        },
        {
            "title": "2.3 Segmentation on 3D Medical Images",
            "content": "Segmentation on 3D medical images is the task of producing dense, voxel-wise prediction to delineate anatomical structures or pathologies within volumetric scan. To achieve this with 2D encoder, we process the volume on slice-by-slice basis. The 2D feature map extracted from each slice by the DINOv3 encoder is preserved. These 2D feature maps are then stacked along the slice axis to construct pseudo-3D feature volume, which serves as the input to lightweight segmentation head that produces the final voxel-wise predictions. Our evaluation for this task is based on 14 widely-used public datasets: Medical Segmentation Decathlon (MSD) [26] The MSD challenge provides 10 distinct 3D medical image segmentation tasks across various modalities and body parts, including tasks for Brain Tumors, Heart, Liver, Hippocampus, Prostate, Lung, Pancreas, Hepatic Vessel, Spleen, and Colon segmentation. Since the official online evaluation platform is no longer available, we adopt 5-fold cross-validation approach on the public training set. Following the standard protocol established in previous medical SSL works [27], we normalize all volumes and apply standard geometric augmentations, including random rotations and flips. For each fold, we use random 80%/20% split for training and validation, reporting the average performance across all folds. EM Neuron Segmentation in CREMI [28] The CREMI dataset originates from the 2016 CREMI challenge, designed to advance neuron segmentation in electron microscopy volumes. The data are from an adult Drosophila brain imaged at resolution of 4 4 40 nm with 1250 1250 pixels per slice. It includes three subsets, CREMI-A, CREMI-B, and CREMI-C, each providing 125 annotated slices that represent different neuron types. The difficulty increases from to C, with later subsets exhibiting more intricate neuronal morphology. In our setup, we train on the first 100 sections from each subset and evaluate on the remaining 25 sections. EM Neuron Segmentation in AC3/4 [29] Both AC3 and AC4 are densely annotated EM volumes from the Kasthuri15 dataset [29], acquired at 3 3 29 nm resolution with 1024 1024 pixels per slice. AC3 comprises 256 consecutive sections and exhibits greater structural heterogeneity, leading to higher topological complexity. AC4 contains 100 sections with relatively uniform contrast, providing stable target for optimization. In our experiments, we train on the first 80 sections of AC4 and evaluate on the first 100 sections of AC3. Automated Lesion Segmentation in Whole-Body FDG-PET/CT Challenge (AutoPET-II) [30] The autoPET-II challenge provides comprehensive dataset of 1014 whole-body FDG-PET/CT scans for automated tumor lesion segmentation in oncology. The dataset focuses on malignant melanoma, lymphoma, and lung cancer lesions across diverse patient populations. Following established evaluation protocols, we utilize the official train/validation split provided by the organizers. All volumes are preprocessed with intensity normalization, and we apply standard data augmentation techniques including random rotations and flips to enhance model robustness. Head and Neck Tumor segmentation and outcome prediction in PET/CT images (HECKTOR 2022) [31] The HECKTOR 2022 dataset comprises 882 head and neck FDG-PET/CT scans with annotations for primary gross tumor volume (GTVp) and lymph node gross tumor volume (GTVn). This dataset presents unique challenges due to the complex anatomy of the head and neck region and the heterogeneous appearance of head and neck cancers. We follow the challenges standard preprocessing pipeline, which includes image registration between PET and CT modalities and intensity normalization. The evaluation follows the official challenge protocol to ensure fair comparison with published benchmarks."
        },
        {
            "title": "3 Task Adaptation",
            "content": "To assess the quality of the visual features produced by DINOv3 [11], we apply straightforward, standardized adaptation techniques that introduce minimal task-specific parameters. This design ensures the benchmark primarily reflects the strength of the frozen representations."
        },
        {
            "title": "3.1 Classification",
            "content": "Our primary evaluation protocol is linear probing: the DINOv3 [11] backbone remains frozen, and only single linear layer is trained on top of the extracted features using Binary Cross-Entropy (BCE) loss with learning rate of 0.005, batch size of 1024, and for 50 epochs. For the CT-RATE [16] dataset, we additionally perform k-Nearest Neighbors (k-NN) evaluation. We extract feature embeddings for all scans, and for each of the 18 disease categories (treated as independent binary tasks), k-NN predicts the presence or absence of the disease based on feature similarity. For whole-slide pathological classification tasks, we use the multiple instance learning (MIL) paradigm. Each . Per-patch features are WSI is tiled into non-overlapping 224224 patches and treated as bag = {xi}N extracted with frozen DINOv3 encoder (with global average pooling) to obtain ei RD01. We then apply learnable linear projection: i=1 hi = Wprojei + bproj, Wproj RDD0, bproj RD1, hi RD1, < D0. (1) Instance embeddings are aggregated using attention-based deep multiple instance learning (ABMIL) [32]: ai = exp(cid:8)w(cid:0) tanh(Vhi) σ(Uhi)(cid:1)(cid:9) (cid:88) j=1 exp(cid:8)w(cid:0) tanh(Vhj) σ(Uhj)(cid:1)(cid:9) , = (cid:88) i=1 aihi, (2) where U, RHD, RH1, σ() denotes the sigmoid function, ai and (cid:80) ai = 1, and RD1 is the slide-level representation. task-specific head g() maps to ˆY ; training uses bag-level cross-entropy. Unless specified, the DINOv3 encoder is frozen and only the projection, attention, and head are trained."
        },
        {
            "title": "3.2 Segmentation",
            "content": "For 3D medical image segmentation, we leverage DINOv3s 2D feature extraction capabilities in slice-wise manner. Each axial slice of the 3D volume is processed independently through the frozen DINOv3 encoder to extract dense feature maps. These 2D feature maps are then stacked along the slice dimension to construct pseudo-3D feature volume. The segmentation architecture consists of three main components: (1) the frozen DINOv3 encoder for feature extraction, (2) lightweight 3D decoder that processes the pseudo-3D features, and (3) segmentation head that produces voxel-wise predictions. The decoder employs 3D convolutional layers with skip connections to progressively upsample features to the original volume resolution. For the MSD benchmark, we adopt the established 5-fold cross-validation protocol to ensure robust evaluation. Each fold uses an 80%/20% split for training and validation, with careful attention to maintaining patientlevel separation to avoid data leakage. All models are trained using the Dice loss function combined with cross-entropy loss, optimized with AdamW optimizer using learning rate of 1e-4 and cosine annealing schedule. For the AutoPET-II and HECKTOR 2022 benchmarks, we followed the official challenge protocol, using an 80%/20% split for training and validation to maintain consistency with the published benchmarks. Models were trained using combination of the Dice and CE losses and optimized with the AdamW optimizer, learning rate of 1e-4, and linear warmup and cosine annealing schedule. For the EM neuron segmentation benchmarks, CREMI and AC3/4, we follow the experimental protocols established in previous studies to ensure direct comparability. Models are trained using weighted mean squared error objective and optimized with the Adam optimizer, learning rate of 1e-3. During inference, instance segmentations are obtained using the Waterz [33] post-processing method."
        },
        {
            "title": "3.3 Evaluation Metrics\nClassification: We report Area Under the Curve (AUC) of Receiver Operating Characteristic curve, Accuracy,\nPrecision, Recall, and F1-Score. For multi-label tasks such as NIH-14 [14] and CT-RATE [16], these metrics\nare averaged across classes.",
            "content": "Segmentation: For 3D segmentation tasks, we report the mean Dice Score for the MSD [26] datasets. For the PET datasets, we report the Dice Score, HD95, Precision, and Recall, while for the EM datasets, we use the Variation of Information (VOI) [34] and Adapted Rand Error (ARAND) [35]."
        },
        {
            "title": "3.4 Baseline Comparison",
            "content": "To benchmark the performance of DINOv3 [11], we provide comparative overview of contemporary foundation models relevant to medical imaging in Table 1. This comparison highlights key differences in model architecture, pre-training data, and learning paradigms. Methods BiomedCLIP [17] CT-CLIP [36] VoCo [37] UNI [38] CONCH [15] DINOv3-S [11] DINOv3-B [11] DINOv3-L [11] Parameters 86M 86M 72M 304M 86M Pre-trained Data 15M image-text pairs 50K volumes + reports 160K volumes 100M patches 1.17M 2D patch-text pairs Data Type 2D Image-Text 3D Volume-Text 3D Volume 2D Patches Patch-Text Data Modality Diverse Biomedical Images Chest CT Learning Paradigm Text supervision Text supervision Head/Abdomen/Chest CT Visual Self-Supervised Learning Visual Self-Supervised Learning Text supervision Histopathology Histopathology 22M 86M 304M 1.7B images (LVD-1.7B) 2D Image Natural Images Visual Self-Supervised Learning Table 1 Comparison of different foundation models for medical imaging. DINOv3 models are pre-trained on large-scale dataset of natural images, in contrast to the other models which are trained on medical-specific data."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 2D Classification Results Classification on Chest X-ray images: On the NIH-14 and RSNA-Pneumonia chest X-ray datasets, DINOv3 models demonstrate strong, competitive performance. As shown in Table 2, DINOv3-L achieves the highest AUC on NIH-14, outperforming the medical-specific BiomedCLIP model. While BiomedCLIP performs best on the RSNA-Pneumonia task, DINOv3 models are close contenders. However, the results also highlight an inconsistent scaling behavior, as seen in Figure 1. Performance does not reliably improve with larger model sizes or higher input resolutions; for instance, AUC for all models on NIH-14 peaks at 512x512 resolution before declining. This suggests that simply increasing model scale does not guarantee better performance in this domain. Methods NIH-14 RSNA-Pneumonia BiomedCLIP [17] DINOv3-S [11] DINOv3-B [11] DINOv3-L [11] AUC 0.7771 0.7788 0. 0.7865 ACC 0.4820 0.4838 0.4823 0.4674 Precision 0.5454 0.5419 0.5446 0.5355 Recall 0. 0.5791 0.5753 0.5779 AUC ACC Precision 0.8831 0.8374 0.8221 0.8667 0.8274 0.8666 0.8209 0.8708 0.6368 0.6048 0.6227 0. Recall 0.8026 0.8156 0.7679 0.7744 Table 2 2D classification linear probing results comparing baseline and DINOv3 series on the NIH-14 and RSNAPneumonia datasets. All models use an input resolution of 256x256. For each metric, the highest performing method is marked in bold, and the second highest is underlined. (a) NIH-14 dataset. (b) RSNA-Pneumonia dataset. Figure 1 Scaling behavior of DINOv3 models across datasets. The results reveal non-trivial relationship between performance, model size, and input resolution, where larger models or higher resolutions do not consistently yield better outcomes. In the domain of WSIs, DINOv3s performance is significantly weaker Classification on Pathology images: than specialized models. For both the Camelyon16 [20] and Camelyon17 [21] datasets, as shown in Tables 3 and 4 and Figure 2, DINOv3 models are substantially outperformed by pathology-specific foundation models like UNI [38] and CONCH [15]. Their performance is only comparable to generic ResNet50 [39] baseline, indicating that DINOv3s natural image features do not effectively transfer to the fine-grained, textural analysis required for histopathology. This limitation is further confirmed by the radar charts for the BCNB dataset in Figure 3, where DINOv3 again lags behind the domain-specialized models across multiple molecular subtyping tasks. Patch Encoder ResNet50 (ImageNet) [39] UNI [38] CONCH [15] DINOv3-S [11] DINOv3-B [11] Camelyon16 Camelyon16 AUC 0. ACC 0.713 Precision Recall 0.776 0.594 0.965 0.951 0.944 0.961 0.840 0.805 0.847 0. 0.959 0.956 0.898 0.834 0.938 0.928 0.682 0.629 Table 3 In-domain tumour detection on Camelyon16. Patch features are aggregated with ABMIL. Models are trained on the Camelyon16 training set and evaluated on its test set. The highest results are in bold and the second highest are underlined. Patch Encoder ResNet50 (ImageNet) [39] UNI [38] CONCH [15] DINOv3-S [11] DINOv3-B [11] Camelyon16 Camelyon17 (Unseen) AUC 0.852 ACC 0.723 0.937 0.932 0.932 0.939 0.761 0.854 0.710 0. Precision 0.607 0.933 0.934 0.589 0.529 Recall 0.808 0.928 0.913 0.894 0.820 Table 4 Out-of-domain tumour detection on Camelyon17. Models are trained on Camelyon16 and evaluated on Camelyon17 (Unseen). The highest results are in bold and the second highest are underlined. Figure 2 Cross-domain generalization on Camelyon16 [20] and Camelyon17 [21]: In-domain vs. Out-of-domain AUC and ACC comparisons. 4.2 3D Classification Results Classification on 3D CT images: For 3D classification on the CT-RATE [16] dataset, DINOv3 establishes powerful new baseline, significantly outperforming prior models. As detailed in Table 5, all DINOv3 variants, using either k-NN or linear probing, achieve substantially higher scores across all metrics compared to the CT-Net and CT-CLIP [36] baselines. While this comparison is favourable, it is worth noting that CT-CLIP was pre-trained on only 50k samples, unlike other large-scale models such as BiomedCLIP [17] which used 15M samples, making direct comparison of foundation model pre-training scale complex. Notably, DINOv3-B with linear probing achieves an AUC of 0.798, considerable improvement over CT-CLIPs 0.731. This strong performance demonstrates that DINOv3s 2D features, when aggregated slice-wise, are highly effective for volumetric CT classification tasks without requiring any medical-specific pre-training. Figure 3 Performance comparison across ALN metastasis and receptor status tasks on the BCNB [23] dataset. The default feature aggregator for the whole-slide images is the attention-based multiple instance learning method [32]."
        },
        {
            "title": "Precision Recall",
            "content": "Methods CLIP [36] CT-Net [40] 0.263 0.657 0.629 k-NN DINOv3-S [11] 0.716 DINOv3-B [11] 0.737 0.709 DINOv3-L [11] 0.791 0.729 0.350 0.374 0.797 0.423 0.275 0.541 0.250 0. 0.731 0.707 0.323 0.663 CT-CLIP Linear Probing 0. 0.722 0.778 0.798 0.741 0.722 0.791 0.390 0.374 0.690 0.688 0. Table 5 3D classification results on the CT-RATE [16] dataset, evaluated across 18 clinical categories (e.g., Medical material, Arterial wall calcification, Cardiomegaly). The top block shows baseline performance from CT-Net and CT-CLIP. The bottom block evaluates DINOv3 backbones using two methods: k-NN classifier on frozen features (left) and trained linear probing (right). For each method, the best result per metric is in bold and the second-best is underlined. 4.3 3D Segmentation Results Segmentation on MSD benchmarks: On the diverse MSD benchmark, DINOv3 shows mixed and generally modest performance compared to state-of-the-art segmentation-specific models like nnU-Net [41], as shown in Table 6 and 7. While DINOv3-L achieves the best Dice scores on few tasks (e.g., Lung, and Spleen), its overall average performance lags behind top transformer-based and classic methods. This suggests that although its features provide reasonable starting point, the simple frozen-backbone, slice-by-slice approach is insufficient to compete with fully optimized 3D segmentation architectures. More advanced adapters may therefore be required to effectively translate strong 2D visual features into 3D dense prediction tasks. Neuron Segmentation on EM images: DINOv3s features fail catastrophically on EM neuron segmentation. As shown in Tables 8 and 9 , for both CREMI [28] and AC3/4 [29] datasets, the error rates (VOI and ARAND, where lower is better) for all DINOv3 models are an order of magnitude worse than classic segmentation methods. The visualizations in Figure 4 suggest that the features learned from natural images are too coarse and lack the high-frequency textural detail necessary to delineate the intricate and complex boundaries of neurons in EM volumes. This represents clear limitation where the domain shift from natural images to EM is too significant for the features to be useful. Tumor segmentation on FDG-PET/CT images: Similar to its performance on EM images, DINOv3 performs very poorly on tumor segmentation in PET/CT scans across both the AutoPET-II [30] and HECKTOR 2022 [31] datasets. As shown in Table 10, its segmentation performance is drastically lower than established models. This failure likely highlights DINOv3s inability to interpret PET data, as its self-supervised visual features are primarily attuned to anatomical structure. This hypothesis is supported by the visualizations in Figure 5, which suggest that while DINOv3 features capture anatomical shapes in CT images, they fail to isolate the metabolically active tumor regions in PET images, still focusing on underlying structural patterns. Ultimately, the functional information in PET imaging represents fundamental departure from the structural patterns in natural images, creating domain shift that DINOv3s pre-trained features cannot overcome. Methods Task01 (Brain) Task03 (Liver) Task02 (Heart) Supervised Learning Methods 81.3 83.7 72.4 71.8 91.2 90.8 76.8 78. Task04 (Hippo.) Task05 (Prostate) 3D U-Net [42] V-Net [43] nnU-Net [41] TransUNet [44] SwinUNETR [45] UNETR [46] 78.9 74.2 76.5 75.1 96.2 93.4 94.7 93.9 84.1 89.4 79.6 85.1 81.2 87.3 80.4 86.2 Self-Supervised Methods (Linear Fine-tuning) 68.4 73.8 71.2 76.2 64.7 70.1 67.9 73.2 66.1 71.8 67.3 72.5 72.6 77. 62.1 64.5 58.9 61.3 60.2 60.8 65.2 84.1 79.8 81.6 80.4 80.9 83.8 82.3 66.8 65.9 78.2 77.6 84.1 83. 75.3 73.8 MAE-ViT-B/16 [47] MAE-ViT-L/16 [47] SimCLR [48] MoCo-v3 [49] SwAV [50] BYOL [51] DINOv3-S [11] DINOv3-B [11] DINOv3-L [11] 82.1 84.3 91.3 86.7 88.9 87.5 75.2 78.1 72.5 74.8 73.6 74.1 78.9 79.8 80. Table 6 3D segmentation Dice scores (%) across Medical Segmentation Decathlon (MSD) benchmark tasks (Part 1: Tasks 01-05). For each method, the best result per metric is in bold and the second-best is underlined. Methods Task06 (Lung) Task07 (Pancreas) Task08 (Hepatic) Task09 (Spleen) Task10 Average (Colon)"
        },
        {
            "title": "Supervised Learning Methods",
            "content": "3D U-Net [52] V-Net [53] nnU-Net [41] TransUNet [44] SwinUNETR [45] UNETR [46] 67.9 66.4 75.8 70.3 72.6 71.8 71.5 73.2 82.7 76.8 78.9 77.4 55.3 57. 67.9 59.4 62.1 60.7 87.6 89.2 94.8 91.2 92.8 91.9 Self-Supervised Methods (Linear Fine-tuning) MAE-ViT-B/16 [47] MAE-ViT-L/16 [47] SimCLR [48] MoCo-v3 [49] SwAV [50] BYOL [51] DINOv3-S [11] DINOv3-B [11] DINOv3-L [11] 61.4 64.1 57.8 60.9 59.1 60.2 65. 73.1 72.4 66.8 69.3 63.2 66.1 64.7 65.4 70.2 78.9 78.2 48.9 52.1 45.1 48.2 46.8 47.5 53.4 64.8 63.7 81.2 84.8 77.4 80.6 78.9 79.7 82.9 86. 91.2 42.1 41.8 52.6 45.7 47.3 46.2 35.4 38.7 31.9 35.1 33.2 34.6 40.1 49.1 47.8 72.8 73. 81.4 76.2 78.2 77.1 65.6 68.3 62.1 64.8 63.5 64.2 69.0 71.3 71.0 Table 7 3D segmentation Dice scores (%) across Medical Segmentation Decathlon (MSD) benchmark tasks (Part 2: Tasks 06-10). For each method, the best result per metric is in bold and the second-best is underlined."
        },
        {
            "title": "5 Findings",
            "content": "F1: DINOv3s natural-image features excel on some medical tasks but fail on modalities with large domain shift. DINOv3 [11], pretrained solely on natural images, establishes strong new baseline in the medical domain"
        },
        {
            "title": "Method",
            "content": "CREMI-A CREMI-B CREMI-C"
        },
        {
            "title": "Classic Segmentation Methods",
            "content": "Superhuman [54] 0.399 0.241 0.640 0.398 0.236 0.634 MALA [33] 0.329 0.298 0.626 PEA [55] APViT [56] 0.445 0.260 0.704 LSD [57] 0.393 0.217 0.610 CAD [58] 0.313 0.252 0.565 DINOv3-S [11] DINOv3-B [11] DINOv3-L [11] 2.147 1.795 3.942 1.849 1.693 3.542 0.793 0.991 1.784 0.089 0.085 0.091 0.117 0.070 0.079 0.554 0.222 0.776 0.589 0.261 0.850 0.411 0.374 0.785 0.579 0.201 0.781 0.538 0.267 0.805 0.379 0.305 0.684 0.048 0.041 0.041 0.032 0.122 0.030 DINOv3 Foundation Models (Linear Probing) 0.543 0.506 0. 3.048 3.660 6.708 2.535 3.256 5.791 1.852 1.417 3.269 0.642 0.611 0.448 0.820 0.338 1.158 0.842 0.332 1.174 0.745 0.446 1.191 0.884 0.234 1.118 0.836 0.230 1.065 0.738 0.322 1.060 3.890 5.257 9.147 3.457 4.089 7.546 2.557 1.836 4.393 0.179 0.162 0.169 0.110 0.150 0.149 0.917 0.795 0. Table 8 Quantitative comparison of different methods on the CREMI datasets. For each metric, the best result is in bold and the second-best is underlined. Note that all reported metrics are lower-is-better."
        },
        {
            "title": "Method",
            "content": "AC3/4 Wafer"
        },
        {
            "title": "Classic Segmentation Methods",
            "content": "Superhuman [54] MALA [33] PEA [55] APViT [56] LSD [57] CAD [58] 0.597 0.677 0.552 0.767 0.633 0.533 0.433 0.457 0.498 0.204 0.280 0.351 1.031 1.134 1.050 0.976 0.913 0.884 0.179 0.166 0.209 0.078 0.093 0.081 0.452 0.455 0.421 0.581 0.445 0. 0.166 0.158 0.172 0.123 0.115 0.144 DINOv3 Foundation Models (Linear Probing) DINOv3-S [11] DINOv3-B [11] DINOv3-L [11] 3.813 3.070 1.821 5.252 2.009 0.950 8.965 5.079 2. 0.825 0.274 0.268 4.298 3.564 2.061 2.705 1.722 0.568 0.618 0.613 0.593 0.704 0.560 0.559 7.003 5.286 2.629 0.041 0.036 0.034 0.036 0.026 0. 0.331 0.189 0.115 Table 9 Quantitative comparison of different methods on AC3/4 and Wafer4 datasets. We compare classic segmentation methods and DINOv3 foundation models (linear probing). Best results are in bold, and second best are underlined. Note: Reported metrics are all lower-is-better. Figure 4 Visualization of slice from the AC3/4 [29] dataset and feature embeddings. (a) Raw EM image. (bd) Feature embeddings extracted from DINOv3-S/16 (b), DINOv3-B/16 (c), and DINOv3-L/16 (d) models, visualized by projecting the first three principal components into RGB space. (e) Corresponding affinity map derived from the raw image. without any medical specific pre training. It demonstrates impressive performance, even outperforming domain specific models like BiomedCLIP [17] and CT-CLIP [36] in certain scenarios. Specifically, it achieves competitive results on 2D chest ray classification (NIH 14 [14] and RSNA Pneumonia [18] datasets) and sets strong new baseline for 3D CT classification (CT-RATE [16] dataset). However, DINOv3 performs poorly on WSI classification, EM, and PET segmentation. This performance disparity can be hypothesized to stem from the object centric nature of DINOv3 [11] Methods Modality AutoPET-II HECKTOR 2022 Dice HD95 Prec. Rec. Dice HD95 Prec. Rec."
        },
        {
            "title": "Classic Segmentation Methods",
            "content": "CT+PET 59.41 UNet [52] CT+PET 53.21 VNet [53] UNETR [46] CT+PET 51.49 Swin UNETR [59] CT+PET 62.24 CT+PET 62.46 VSmTrans [60] CT+PET 36.50 UNETR++ [61] CT+PET 60.67 U-KAN [62] 241.31 242.78 257.30 242.07 223.88 178.57 70.91 62.32 70.74 50.25 53.21 60.85 55.61 51.49 61.03 48.10 62.91 73.30 44.56 65.19 70.92 52.91 36.50 60.16 29.95 62.03 72.94 55.89 65.03 41.46 73.27 103.02 78.03 27.74 23. 72.13 41.50 78.21 46.01 70.71 39.11 62.43 37.55 61.91 50.97 61.84 21.75 77.72 46."
        },
        {
            "title": "Multimodal Segmentation Methods",
            "content": "Nestedformer [63] CT+PET 61.38 CT+PET 60.86 A2FSeg [64] H-DenseFormer [65] CT+PET 61.50 265.51 131.48 252.98 61.38 64.29 40.17 60.86 76.10 40.90 61.41 75.76 46.79 72.95 32.95 34.84 63.22 32.59 77.02 30.57 78.33 35.31 DINOv3 Foundation Models (Linear Probing) DINOv3-S/16 [11] DINOv3-B/16 [11] DINOv3-L/16 [11] DINOv3-S/16 [11] DINOv3-B/16 [11] DINOv3-L/16 [11]"
        },
        {
            "title": "PET\nPET\nPET",
            "content": "0.00 25475.80 0.00 0.00 21394.57 0.00 0.00 11637.64 0.39 0.00 0.00 0.00 0.00 0.00 0.00 NaN 7541.56 NaN 0.00 0.03 0.00 0.00 0.00 0. 7.10 13940.53 4.37 48.14 17.37 8.74 14114.07 5.43 54.38 21.41 7919.81 37.03 20.57 25.93 10.87 13611.39 6.85 64.96 6.44 10641.95 5.92 9.43 10329.33 9.40 DINOv3-S/16 [11] CT+PET 9.06 13456.42 5.32 65.87 40.13 4294.74 52.82 40.67 DINOv3-B/16 [11] CT+PET 14.53 13188.93 9.50 49.06 39.50 5032.80 45.37 45.18 DINOv3-L/16 [11] CT+PET 12.17 13418.89 7.50 71.16 30.86 8808.90 34.99 39.98 Table 10 Performance comparison of different methods on AutoPET-II and HECKTOR 2022 datasets across CT and PET modalities. Best results across all methods are in bold and second best are underlined. Notably, HD95 is NaN, which means the prediction is all background. pretraining. Since the model learned from vast corpus of natural images from Instagram, its visual features are highly attuned to capturing structures and shapes. This explains its success in modalities like X-ray and CT, where many diagnostic patterns are linked to macroscopic structural abnormalities. In contrast, its performance degrades significantly on image modalities where the visual characteristics differ greatly. For WSI, analysis relies on fine grained textural and cellular patterns, which are less represented in DINOv3 object focused feature space. For EM, the model features lack the high frequency textural detail required to delineate intricate neuronal boundaries. The shift is even more pronounced for PET, as these scans visualize functional metabolic activity, fundamental departure from the structural patterns in natural images that DINOv3 is primed to recognize. F2: Scaling laws from natural images do not consistently transfer to the medical domain. The report finds that DINOv3 does not consistently follow the expected scaling laws in the medical domain. Contrary to trends in natural image tasks, increasing the model size (e.g., from DINOv3 to DINOv3 L) or using higher input resolutions does not reliably lead to better performance. For instance, on the NIH 14 chest ray dataset, performance fluctuates unpredictably across different model scales and input resolutions. This inconsistent scaling behavior is observed across different tasks and datasets, indicating that larger models are not consistently able to achieve the best performance. This suggests that simply using larger model or finer features is not guaranteed strategy for improvement in medical imaging. F3: The benefits of scaling are not uniformly transferable across diverse medical tasks and modalities. The advantages gained from scaling are not uniformly transferable, with different tasks exhibiting markedly different behaviors. This is particularly evident in 2D classification; for both chest X-ray and WSI analysis, Figure 5 Visualization of the first three principal components derived from PCA on image patches. (a) CT images and (b) PET images are shown with their respective PCA visualizations, where each of the first three components is mapped to color channel. (c) The resulting tumor region can be isolated by thresholding the first principal component to remove the background. larger models can paradoxically underperform smaller ones. In contrast, for 3D CT classification, increasing model scale is generally beneficial, though the improvement is not always monotonic. third distinct pattern appears in 3D segmentation, where larger DINOv3 models typically outperform their smaller counterparts. Remarkably, on certain Medical Segmentation Decathlon tasks like Pancreas and Spleen segmentation, the aggregated 2D features from DINOv3 can outperform even the strong nnU-Net baseline. This surprising result underscores the immense potential of leveraging powerful 2D visual priors for complex 3D tasks, indicating that these features are not universal and vary significantly depending on the specific medical task and modality."
        },
        {
            "title": "5.1 Limitations of this Report",
            "content": "While this report presents comprehensive benchmark across diverse tasks and modalities, it has several limitations. First, our analysis focuses exclusively on the DINOv3 model family and does not include comparative evaluation against other foundation models [66]. Second, our experiments are restricted to linear probing protocol with frozen backbone; we do not explore the potential benefits of full fine-tuning or parameter-efficient adaptation methods [67, 68]. Finally, although the selected datasets are diverse, they are not exhaustive. Our benchmark does not cover all medical imaging modalities, such as 4D cardiac MRI [69, 70], or all relevant tasks, such as registration and 3D reconstruction [71, 72]."
        },
        {
            "title": "6.1 Summary of Findings",
            "content": "This report establishes DINOv3 as strong off the shelf encoder for range of medical imaging tasks, particularly those with visual characteristics similar to natural images such as CT and ray analysis. Despite being trained exclusively on non medical data, it sets strong baseline and can outperform domain specific models in certain scenarios. However, our findings highlight critical limitations: DINOv3 performance deteriorates significantly in domains like WSI, EM, and PET, where there may be even greater shifts between training and target distributions. Furthermore, we observe that the scaling laws that govern performance on natural images do not consistently apply in the medical domain; larger models and higher resolutions do not reliably yield better results, revealing complex and task dependent scaling behaviors."
        },
        {
            "title": "6.2 Future Directions",
            "content": "Based on our findings, several promising research avenues emerge. First, to bridge the performance gap in specialized domains, future work should move beyond linear probing and investigate parameter efficient fine tuning methods to adapt DINOv3 features for new domains. Second, for volumetric tasks, there is clear need to develop more sophisticated 2D to 3D adapters that can more effectively translate the powerful slice wise features for dense 3D prediction tasks like segmentation. Finally, the high quality of DINOv3 features in modalities like CT could be leveraged for other complex tasks, such as enforcing multi view consistency in 3D reconstruction from 2D slices or improving medical image registration."
        },
        {
            "title": "References",
            "content": "[1] OpenAI, Chatgpt, 2022. [Online]. Available: https://openai.com/blog/chatgpt [2] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, Scaling laws for neural language models, arXiv preprint arXiv:2001.08361, 2020. [3] I. M. Alabdulmohsin, B. Neyshabur, and X. Zhai, Revisiting neural scaling laws in language and vision, Advances in Neural Information Processing Systems, vol. 35, pp. 22 30022 312, 2022. [4] Z. Xie, Z. Zhang, Y. Cao, Y. Lin, Y. Wei, Q. Dai, and H. Hu, On data scaling in masked image modeling, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 10 36510 374. [5] A. El-Nouby, M. Klein, S. Zhai, M. A. Bautista, A. Toshev, V. Shankar, J. M. Susskind, and A. Joulin, Scalable pre-training of large autoregressive image models, arXiv preprint arXiv:2401.08541, 2024. [6] J. Pan, B. Jian, P. Hager, Y. Zhang, C. Liu, F. Jungmann, H. B. Li, C. You, J. Wu, J. Zhu et al., Beyond benchmarks: Dynamic, automatic and systematic red-teaming agents for trustworthy medical language models, arXiv preprint arXiv:2508.00923, 2025. [7] J. Pan, C. Liu, J. Wu, F. Liu, J. Zhu, H. B. Li, C. Chen, C. Ouyang, and D. Rueckert, Medvlm-r1: Incentivizing medical reasoning capability of vision-language models (vlms) via reinforcement learning, arXiv preprint arXiv:2502.19634, 2025. [8] D. Fan, S. Tong, J. Zhu, K. Sinha, Z. Liu, X. Chen, M. Rabbat, N. Ballas, Y. LeCun, A. Bar et al., Scaling language-free visual representation learning, arXiv preprint arXiv:2504.01017, 2025. [9] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby et al., Dinov2: Learning robust visual features without supervision, arXiv preprint arXiv:2304.07193, 2023. [10] M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, and A. Joulin, Emerging properties in self-supervised vision transformers, in Proceedings of ICCV, 2021, pp. 96509660. [11] O. Siméoni, H. V. Vo, M. Seitzer, F. Baldassarre, M. Oquab, C. Jose, V. Khalidov, M. Szafraniec, S. Yi, M. Ramamonjisoa et al., Dinov3, arXiv preprint arXiv:2508.10104, 2025. [12] S. Yang, H. Wang, Z. Xing, S. Chen, and L. Zhu, Segdino: An efficient design for medical and natural image segmentation with dino-v3, arXiv preprint arXiv:2509.00833, 2025. [13] Y. Li, Y. Wu, Y. Lai, M. Hu, and X. Yang, Meddinov3: How to adapt vision foundation models for medical image segmentation? arXiv preprint arXiv:2509.02379, 2025. [14] X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, and R. M. Summers, Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases, in Proceedings of CVPR, 2017, pp. 34623471. [15] M. Y. Lu, B. Chen, D. F. Williamson, R. J. Chen, I. Liang, T. Ding, G. Jaume, I. Odintsov, L. P. Le, G. Gerber et al., visual-language foundation model for computational pathology, Nature medicine, vol. 30, no. 3, pp. 863874, 2024. [16] I. E. Hamamci, S. Er, C. Wang, F. Almas, A. G. Simsek, S. N. Esirgun, I. Doga, O. F. Durugol, W. Dai, M. Xu et al., Developing generalist foundation models from multimodal dataset for 3d computed tomography, arXiv preprint arXiv:2403.17834, 2024. [17] S. Zhang, Y. Xu, N. Usuyama, J. Bagga, R. Tinn, S. Preston, R. Rao, M. Wei, N. Valluri, C. Wong et al., Largescale domain-specific pretraining for biomedical vision-language processing, arXiv preprint arXiv:2303.00915, 2023. [18] A. Stein, C. Wu, C. Carr, G. Shih, J. Dulkowski, kalpathy, L. Chen, L. Prevedello, M. Kohli, M. McDonald, Peter, P. Culliton, S. Halabi, and T. Xia, RSNA pneumonia detection challenge, https://www.kaggle.com/competitions/rsna-pneumonia-detection-challenge, 2018. [Online]. Available: https: //www.kaggle.com/competitions/rsna-pneumonia-detection-challenge [19] F. Wang, Y. Zhou, S. Wang, V. Vardhanabhuti, and L. Yu, Multi-granularity cross-modal alignment for generalized medical visual representation learning, Advances in neural information processing systems, vol. 35, pp. 33 53633 549, 2022. [20] B. E. Bejnordi, M. Veta, P. J. Van Diest, B. Van Ginneken, N. Karssemeijer, G. Litjens, J. A. Van Der Laak, M. Hermsen, Q. F. Manson, M. Balkenhol et al., Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer, Jama, vol. 318, no. 22, pp. 21992210, 2017. [21] P. Bandi, O. Geessink, Q. Manson, M. Van Dijk, M. Balkenhol, M. Hermsen, B. E. Bejnordi, B. Lee, K. Paeng, A. Zhong et al., From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge, IEEE transactions on medical imaging, vol. 38, no. 2, pp. 550560, 2018. [22] L. Cai, S. Huang, Y. Zhang, J. Lu, and Y. Zhang, Attrimil: Revisiting attention-based multiple instance learning for whole-slide pathological image classification from perspective of instance attributes, Medical Image Analysis, p. 103631, 2025. [23] F. Xu, C. Zhu, W. Tang, Y. Wang, Y. Zhang, J. Li, H. Jiang, Z. Shi, J. Liu, and M. Jin, Predicting axillary lymph node metastasis in early breast cancer using deep learning on primary tumor biopsy slides, Frontiers in oncology, vol. 11, p. 759007, 2021. [24] M. Y. Lu, D. F. Williamson, T. Y. Chen, R. J. Chen, M. Barbieri, and F. Mahmood, Data-efficient and weakly supervised computational pathology on whole-slide images, Nature biomedical engineering, vol. 5, no. 6, pp. 555570, 2021. [25] G. Müller-Franzes, F. Khader, R. Siepmann, T. Han, J. N. Kather, S. Nebelung, and D. Truhn, Medical slice transformer for improved diagnosis and explainability on 3d medical images with dinov2, Scientific Reports, vol. 15, no. 1, p. 23979, 2025. [26] M. Antonelli, A. Reinke, S. Bakas, K. Farahani, A. Kopp-Schneider, B. A. Landman, G. Litjens, B. Menze, O. Ronneberger, R. M. Summers et al., The medical segmentation decathlon, Nature communications, vol. 13, no. 1, p. 4128, 2022. [27] L. Wu, J. Zhuang, and H. Chen, Voco: simple-yet-effective volume contrastive learning framework for 3d medical image analysis, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024, pp. 22 87322 882. [28] CREMI, Miccai challenge on circuit reconstruction from electron microscopy images, https://cremi.org/, 2016. [29] N. Kasthuri, K. J. Hayworth, D. R. Berger, R. L. Schalek, J. A. Conchello, S. Knowles-Barley, D. Lee, A. VázquezReina, V. Kaynig, T. R. Jones et al., Saturated reconstruction of volume of neocortex, Cell, vol. 162, no. 3, pp. 648661, 2015. [30] K. T. Gatidis S, whole-body fdg-pet/ct dataset with manually annotated tumor lesions (fdg-pet-ct-lesions), The Cancer Imaging Archive, vol. 226, 2022. [31] V. Oreiller, V. Andrearczyk, M. Jreige, S. Boughdad, H. Elhalawani, J. Castelli, M. Vallieres, S. Zhu, J. Xie, Y. Peng et al., Head and neck tumor segmentation in pet/ct: the hecktor challenge, Medical image analysis, vol. 77, p. 102336, 2022. [32] M. Ilse, J. Tomczak, and M. Welling, Attention-based deep multiple instance learning, in International conference on machine learning. PMLR, 2018, pp. 21272136. [33] J. Funke, F. Tschopp, W. Grisaitis, A. Sheridan, C. Singh, S. Saalfeld, and S. C. Turaga, Large scale image segmentation with structured loss based deep learning for connectome reconstruction, IEEE transactions on pattern analysis and machine intelligence, vol. 41, no. 7, pp. 16691680, 2018. [34] J. Nunez-Iglesias, R. Kennedy, T. Parag, J. Shi, and D. B. Chklovskii, Machine learning of hierarchical clustering to segment 2d and 3d images, PloS one, vol. 8, no. 8, p. e71715, 2013. [35] I. Arganda-Carreras, S. C. Turaga, D. R. Berger, D. Cireşan, A. Giusti, L. M. Gambardella, J. Schmidhuber, D. Laptev, S. Dwivedi, J. M. Buhmann et al., Crowdsourcing the creation of image segmentation algorithms for connectomics, Frontiers in neuroanatomy, vol. 9, p. 152591, 2015. [36] I. E. Hamamci, S. Er, C. Wang, F. Almas, A. G. Simsek, S. N. Esirgun, I. Doga, O. F. Durugol, W. Dai, M. Xu et al., Developing generalist foundation models from multimodal dataset for 3d computed tomography, arXiv preprint arXiv:2403.17834, 2024. [37] L. Wu, J. Zhuang, and H. Chen, Voco: simple-yet-effective volume contrastive learning framework for 3d medical image analysis, in Proceedings of CVPR, 2024, pp. 22 87322 882. [38] R. J. Chen, T. Ding, M. Y. Lu, D. F. Williamson, G. Jaume, A. H. Song, B. Chen, A. Zhang, D. Shao, M. Shaban et al., Towards general-purpose foundation model for computational pathology, Nature medicine, vol. 30, no. 3, pp. 850862, 2024. [39] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770778. [40] R. L. Draelos, D. Dov, M. A. Mazurowski, J. Y. Lo, R. Henao, G. D. Rubin, and L. Carin, Machine-learning-based multiple abnormality prediction with large-scale chest computed tomography volumes, Medical Image Analysis, vol. 67, p. 101857, 2021. [41] F. Isensee, P. F. Jaeger, S. A. Kohl, J. Petersen, and K. H. Maier-Hein, nnu-net: self-configuring method for deep learning-based biomedical image segmentation, Nature methods, vol. 18, no. 2, pp. 203211, 2021. [42] Ö. Çiçek, A. Abdulkadir, S. S. Lienkamp, T. Brox, and O. Ronneberger, 3d u-net: learning dense volumetric segmentation from sparse annotation, in International conference on medical image computing and computerassisted intervention. Springer, 2016, pp. 424432. [43] F. Milletari, N. Navab, and S.-A. Ahmadi, V-net: Fully convolutional neural networks for volumetric medical image segmentation, in 2016 fourth international conference on 3D vision (3DV). IEEE, 2016, pp. 565571. [44] J. Chen, Y. Lu, Q. Yu, X. Luo, E. Adeli, Y. Wang, L. Lu, A. L. Yuille, and Y. Zhou, Transunet: Transformers make strong encoders for medical image segmentation, arXiv preprint arXiv:2102.04306, 2021. [45] A. Hatamizadeh, V. Nath, Y. Tang, D. Yang, H. R. Roth, and D. Xu, Swin unetr: Swin transformers for semantic segmentation of brain tumors in mri images, in International MICCAI brainlesion workshop. Springer, 2022, pp. 272284. [46] A. Hatamizadeh, Y. Tang, V. Nath, D. Yang, A. Myronenko, B. Landman, H. R. Roth, and D. Xu, Unetr: transformers for 3d medical image segmentation, in Proceedings of the IEEE/CVF winter conference on applications of computer vision, 2022, pp. 17481758. [47] K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick, Masked autoencoders are scalable vision learners, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 16 00016 009. [48] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, simple framework for contrastive learning of visual representations, in International conference on machine learning. PMLR, 2020, pp. 15971607. [49] X. Chen, S. Xie, and K. He, An empirical study of training self-supervised vision transformers, arXiv preprint arXiv:2104.02057, 2021. [50] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin, Unsupervised learning of visual features by contrasting cluster assignments, vol. 33, pp. 99129924, 2020. [51] J.-B. Grill, F. Strub, F. Altché, C. Tallec, P. Richemond, E. Buchatskaya, C. Doersch, B. A. Pires, Z. Guo, M. G. Azar et al., Bootstrap your own latent: new approach to self-supervised learning, vol. 33, pp. 21 27121 284, 2020. [52] Ö. Çiçek, A. Abdulkadir, S. S. Lienkamp, T. Brox, and O. Ronneberger, 3d u-net: learning dense volumetric segmentation from sparse annotation, in Medical Image Computing and Computer-Assisted InterventionMICCAI 2016: 19th International Conference, Athens, Greece, October 17-21, 2016, Proceedings, Part II 19. Springer, 2016, pp. 424432. [53] F. Milletari, N. Navab, and S.-A. Ahmadi, V-net: Fully convolutional neural networks for volumetric medical image segmentation, in 2016 fourth international conference on 3D vision (3DV). Ieee, 2016, pp. 565571. [54] K. Lee, J. Zung, P. Li, V. Jain, and H. S. Seung, Superhuman accuracy on the snemi3d connectomics challenge, arXiv preprint arXiv:1706.00120, 2017. [55] W. Huang, S. Deng, C. Chen, X. Fu, and Z. Xiong, Learning to model pixel-embedded affinity for homogeneous instance segmentation, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, no. 1, 2022, pp. 10071015. [56] R. Sun, N. Luo, Y. Pan, H. Mai, T. Zhang, Z. Xiong, and F. Wu, Appearance prompt vision transformer for connectome reconstruction. in IJCAI, 2023, pp. 14231431. [57] A. Sheridan, T. M. Nguyen, D. Deb, W.-C. A. Lee, S. Saalfeld, S. C. Turaga, U. Manor, and J. Funke, Local shape descriptors for neuron segmentation, Nature methods, vol. 20, no. 2, pp. 295303, 2023. [58] X. Liu, M. Cai, Y. Chen, Y. Zhang, T. Shi, R. Zhang, X. Chen, and Z. Xiong, Cross-dimension affinity distillation for 3d em neuron segmentation, in 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Computer Society, 2024, pp. 11 10411 113. [59] Y. Tang, D. Yang, W. Li, H. R. Roth, B. A. Landman, D. Xu, V. Nath, and A. Hatamizadeh, Self-supervised pre-training of swin transformers for 3d medical image analysis, Proceedings of CVPR, pp. 20 69820 708, 2021. [60] T. Liu, Q. Bai, D. A. Torigian, Y. Tong, and J. K. Udupa, Vsmtrans: hybrid paradigm integrating self-attention and convolution for 3d medical image segmentation, Medical image analysis, vol. 98, p. 103295, 2024. [61] A. M. Shaker, M. Maaz, H. Rasheed, S. Khan, M.-H. Yang, and F. S. Khan, Unetr++: delving into efficient and accurate 3d medical image segmentation, IEEE Transactions on Medical Imaging, 2024. [62] C. Li et al., U-kan makes strong backbone for medical image segmentation and generation, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 39, no. 5, 2025, pp. 46524660. [63] Z. Xing, L. Yu, L. Wan, T. Han, and L. Zhu, Nestedformer: Nested modality-aware transformer for brain tumor segmentation, in International conference on medical image computing and computer-assisted intervention. Springer, 2022, pp. 140150. [64] Z. Wang and Y. Hong, A2fseg: Adaptive multi-modal fusion network for medical image segmentation, in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2023, pp. 673681. [65] J. Shi et al., H-denseformer: An efficient hybrid densely connected transformer for multimodal tumor segmentation, in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2023, pp. 692702. [66] D. Bolya, P.-Y. Huang, P. Sun, J. H. Cho, A. Madotto, C. Wei, T. Ma, J. Zhi, J. Rajasegaran, H. Rasheed et al., Perception encoder: The best visual embeddings are not at the output of the network, arXiv preprint arXiv:2504.13181, 2025. [67] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen et al., Lora: Low-rank adaptation of large language models. ICLR, vol. 1, no. 2, p. 3, 2022. [68] J. Ma, Y. He, F. Li, L. Han, C. You, and B. Wang, Segment anything in medical images, Nature Communications, vol. 15, no. 1, p. 654, 2024. [69] Y. Zhang, P. Hager, C. Liu, S. Shit, C. Chen, D. Rueckert, and J. Pan, Towards cardiac mri foundation models: Comprehensive visual-tabular representations for whole-heart assessment and beyond, arXiv preprint arXiv:2504.13037, 2025. [70] Y. Zhang, C. Chen, S. Shit, S. Starck, D. Rueckert, and J. Pan, Whole heart 3d+ representation learning through sparse 2d cardiac mr images, in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2024, pp. 359369. [71] B. Jian, J. Pan, Y. Li, F. Bongratz, R. Li, D. Rueckert, B. Wiestler, and C. Wachinger, Timeflow: Longitudinal brain image registration and aging progression analysis, arXiv preprint arXiv:2501.08667, 2025. [72] N. Bubeck, S. Shit, C. Chen, C. Zhao, P. Guo, D. Yang, G. Zitzlsberger, D. Xu, B. Kainz, D. Rueckert et al., Latent interpolation learning using diffusion models for cardiac volume reconstruction, arXiv preprint arXiv:2508.13826, 2025."
        },
        {
            "title": "Acknowledgement",
            "content": "The LaTeX template is built upon Metas original template."
        }
    ],
    "affiliations": [
        "Dresden University of Technology",
        "Helmholtz AI and Helmholtz Munich",
        "Imperial College London",
        "Munich Center for Machine Learning",
        "Technical University of Munich (TUM)",
        "University of Erlangen-Nuremberg",
        "University of Oxford",
        "University of Science and Technology of China",
        "University of Sheffield"
    ]
}