{
    "paper_title": "Measuring General Intelligence with Generated Games",
    "authors": [
        "Vivek Verma",
        "David Huang",
        "William Chen",
        "Dan Klein",
        "Nicholas Tomlin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present gg-bench, a collection of game environments designed to evaluate general reasoning capabilities in language models. Unlike most static benchmarks, gg-bench is a data generating process where new evaluation instances can be generated at will. In particular, gg-bench is synthetically generated by (1) using a large language model (LLM) to generate natural language descriptions of novel games, (2) using the LLM to implement each game in code as a Gym environment, and (3) training reinforcement learning (RL) agents via self-play on the generated games. We evaluate language models by their winrate against these RL agents by prompting models with the game description, current board state, and a list of valid moves, after which models output the moves they wish to take. gg-bench is challenging: state-of-the-art LLMs such as GPT-4o and Claude 3.7 Sonnet achieve winrates of 7-9% on gg-bench using in-context learning, while reasoning models such as o1, o3-mini and DeepSeek-R1 achieve average winrates of 31-36%. We release the generated games, data generation process, and evaluation code in order to support future modeling work and expansion of our benchmark."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 5 1 2 7 0 . 5 0 5 2 : r a"
        },
        {
            "title": "Measuring General Intelligence\nwith Generated Games",
            "content": "Vivek Verma David Huang William Chen Dan Klein Nicholas Tomlin Computer Science Divison University of California, Berkeley vivekverma@berkeley.edu"
        },
        {
            "title": "Abstract",
            "content": "We present gg-bench, collection of game environments designed to evaluate general reasoning capabilities in language models. Unlike most static benchmarks, gg-bench is data generating process where new evaluation instances can be generated at will. In particular, gg-bench is synthetically generated by (1) using large language model (LLM) to generate natural language descriptions of novel games, (2) using the LLM to implement each game in code as Gym environment, and (3) training reinforcement learning (RL) agents via self-play on the generated games. We evaluate language models by their winrate against these RL agents by prompting models with the game description, current board state, and list of valid moves, after which models output the moves they wish to take. gg-bench is challenging: state-of-the-art LLMs such as GPT-4o and Claude 3.7 Sonnet achieve winrates of 7-9% on gg-bench using in-context learning, while reasoning models such as o1, o3-mini and DeepSeek-R1 achieve average winrates of 31-36%. We release the generated games, data generation process, and evaluation code in order to support future modeling work and expansion of our benchmark."
        },
        {
            "title": "Introduction",
            "content": "Early researchers in artificial intelligence had broad ambitions of building systems capable of performing at or above human levels across arbitrary tasks. Often credited with the creation of the field of artificial intelligence, John McCarthy conjectured in his 1955 Dartmouth Conference proposal that every aspect of learning or any other feature of intelligence can in principle be so precisely described that machine can be made to simulate it [24]. However, in the subsequent decades, AI research narrowed significantly, focusing on more specific problem domains like chess, rule-based expert systems like DENDRAL [7], and knowledge engineering efforts like Cyc [22, 31]. Concerned that the field had strayed too far from its initial ambitions, Goertzel and Pennachin [16] coined the term artificial general intelligence in the early 2000s and urged researchers to move beyond narrow AI. While the definition and usage of this term have been hotly debated in both AI and psychology [39, 21, 15], in this work we follow Chollet [10] and use general intelligence to refer to the ability of system to generalize and act in unseen contexts and environments. In recent years, large language models (LLMs) have emerged as potential stepping stone toward artificial general intelligence, and their performance on wide variety of popular benchmarks has drastically increased in recent years [6]. However, growing concern is that these gains might not reflect true advancements in their ability to generalize to new domains, but might instead simply be the result of training on larger and more relevant datasets [10]. In other words, many tasks that were previously viewed as tests of out-of-domain generalization have now been moved into the training Preprint. Under review. Figure 1: Overview of our benchmark creation process. We start by generating descriptions of twoplayer strategy games, after which we generate implementations of these games as Gym environments. Lastly, we employ self-play reinforcement learning to train agents on these games distributions of our models. As result, it is an open question whether todays models can adapt and generalize to novel tasks in way that would satisfy our definition of generally intelligent system. In this paper, we propose scalable approach for evaluating whether models can generalize to new domains, leveraging key observation: LLMs are capable of generating complex tasks that they themselves are incapable of solving. Under this view, benchmarks are not static lists of questions but data generating processes, such that individual task instances can be regenerated at will. This approach allows us to generate new tasks in the result of data contamination, and also provides the possibility of generating more difficult tasks as stronger language models are developed and released. We present gg-bench, an evaluation benchmark consisting of games generated entirely by LLMs. The benchmark is created by first using LLMs to generate descriptions of two-player, turn-based strategy games designed to be played in console environment. Then, using the same model, we generate Python implementations of each game in the form of Gym environments [4]. After this, we use self-play reinforcement learning to train agents on each of these games via proximal policy optimization [35]. Finally, in order to evaluate whether target model can generalize to act in these generated games, we evaluate its winrate against the trained RL agents. gg-bench is challenging: state-of-the-art LLMs such as GPT-4o and Claude 3.7 Sonnet achieve winrates between 7.5% and 9% on gg-bench using in-context learning, while reasoning models such as DeepSeek-R1 and o1 achieve average winrates between 31% and 36%. We analyze the diversity of generated games and identify common failure patterns of language models, revealing that their primary shortcomings are an inability to effectively strategize over multiple turns and to correctly generalize from game descriptions to new gameplay scenarios. Lastly, we release the dataset, code for generating the dataset and our experiments at https://github.com/vivek3141/gg-bench. 2 gg-bench The current iteration of gg-bench is benchmark consisting of 126 datapoints, each of which is two-player game. Each datapoint consists of the following components: 1. Game description: natural language description of the game, describing its rules, objectives, and mechanics. 2. Implementation: Gym environment implementation of the game. The gym environment consists of an action space, step function, render function, and valid_moves function. An action space is list of all possible actions that can be taken at any state, while the step function is used to apply an action to the current state of the game. The render function is used to convert the current state of the game to string. The valid_moves function returns list of valid moves given the current state of the game. 2 # Number Duel ## Objective Be the first player to **capture all of your opponent's numbers**. Utilize strategic selection and timing to outmaneuver your opponent. Victory is achieved when your opponent has no numbers remaining in their set. ## Setup class CustomEnv(gym.Env): def __init__(self, N=10): ... self.action_space = spaces.Discrete(N) self.observation_space = spaces.Box( low=0, high=1, shape=(2 * self.N + 1,), dtype=np.float32 ) self.reset() def reset(self, seed=None): 1. **Number Range Selection**: ... - Determine the value of **N**, the maximum number in each player's set. recommended starting value is **N = 10**. 2. **Initial Number Sets**: - Each player receives set of unique numbers ranging from **1 to N** inclusive. ... ### Example Game Setup - **N = 5** - **Player 1's Numbers**: `{1, 2, 3, 4, 5}` - **Player 2's Numbers**: `{1, 2, 3, 4, 5}` - **First Attacker**: Player 1 ### Round 1 - **Player 1** (Attacker) selects **3**. - **Player 2** (Defender) selects **2**. - **Reveal**: - Player 1: **3** - Player 2: **2** - **Outcome**: - 3 (Attacker) > 2 (Defender): Attack successful. - **Player 2's number 2 is captured**. - Player 1's number **3 remains** in their set. ... def step(self, action): ... def render(self): output = [] output.append( f\"Current role: {'Attacker' if self.current_role == 0 else 'Defender'}\" ) ... return \"n\".join(output) def valid_moves(self): ... (b) Code for the Gym environment generated for the description provided. Implementation details are omitted and replaced with ... markers. In the given gym environment for the Number Duel game, the action space indices range from 0 to N-1, corresponding directly to the available numbers player can use for their turn. Each index represents potential move, with index mapping to the number i+1 from player's remaining set. For example, choosing an action with index 0 corresponds to selecting the number 1, index 1 to selecting the number 2, and so forth, up to index N-1 for the number N. This mapping allows players to choose any available number for their attack or defense from their remaining numbers. (a) An example game description from gg-bench. Some parts of the description are elided with ... markers. (c) Action description generated given the description and environment implementation. Figure 2: An environment in gg-bench consists of three components: (a) game description, (b) Gym implementation, and (c) an action space description. Both the game description and action space description are available to the language model when prompted to select move. 3. Action space description: natural language description of each action in the action space. This is used to prompt the language model during evaluation. The dataset is generated synthetically, with OpenAI o1 [27]. An example of generated game, code implementation, and action description can be found in Figure 2. Language models are evaluated based on their winrates against RL-based agents. In order to obtain high-qualtity and diverse games, we employ multi-step generation and filtering process, outlined below: 3 2.1 Game Generation We start by prompting model to generate 1000 unique two-player game rulebooks, each independently sampled. To ensure that language models can interact with the games, the prompt specifies that they must be playable in console environment. We then generate implementations for each generated game in the form of Gym environment [4], along with valid_moves function. Additionally, we generate descriptions mapping each action-space index to its corresponding in-game move. The cost for generating all games with o1 was $1162. The prompts used and implementation details can be found in Appendix B. 2.2 Self-Play Reinforcement Learning We evaluate language models in terms of their winrates against RL-based agents. To obtain these agents, we employ proximal policy optimization (PPO) [35]. PPO works by optimizing clipped surrogate objective, which constrains policy updates to prevent large changes, helping with stability. We train agents using self-play reinforcement learning [18], where the PPO agent acts as both players in the generated environment. We train agents for 106 timesteps and checkpoint every 2.5 105 timesteps. During training, at the start of each episode, we randomly sample previously checkpointed agent to play against, except for the first 2.5 105 timesteps, where we play against random agent. In addition, at each turn, we sample random action with probability ϵ, encouraging exploration. ϵ linearly decays from 1.0 to 0.1 over the training process. The agents are trained to maximize the reward signal, which is 1 for win, 1 for loss, and 0 for draw. During inference, we employ Monte Carlo tree search (MCTS) to select actions. We sample 100 self-play trajectories starting at the current state using the trained RL agent, and log which trajectories result in win for the current player. We then select the action at the root node leading to the child with the highest visit count, i.e., the action associated with the greatest number of simulated wins. 2.3 Filtering Throughout the generation process, we employ multiple filtering steps to ensure the quality of the generated games. These methods are outlined below: Keyword filtering. Some generated games require large amounts of memory or computation, making it infeasible to train RL agents. For example, in word games, the action space is exponential in the number of letters. To prevent this, we apply regex and filter out games with ** in the action space. Execution filtering. Some games have bugs in their implementations. We filter games by execution, checking whether the environment can be instantiated, returns the correct observation dimensions, and has working render function. Game implementations are also generated with function that returns list of valid moves given the current state; for each environment, we play random agents against each other and filter games that throw exceptions even after taking moves from this list. Timeout filtering. In initial experiments, we observed that win-condition checking and move application were often implemented incorrectly, resulting in never-ending games. To address this problem, we implement timeout-based filtering by running an initial evaluation with GPT-4o-mini, where any games that take longer than 100 moves or over 1.5 hours to complete are filtered out. During this stage, we also filter out any games with an exception rate greater than 20%. 2.4 Establishing an Upper Bound We explicitly aim to demonstrate that the benchmark is beatable; that is, for each game included in gg-bench, there should exist some policy that is capable of consistently defeating the RL-based agent that we use to evaluate language models. To empirically verify this, we consider RL agents checkpointed at four intervals throughout training. For each game, we evaluate every pairwise comparison of checkpointed agents across six matches. We then identify the pair of agents with the highest winrate disparity, ensuring one agent consistently outperforms the other. For gg-bench, we select the losing agent from this pair as the opponent that the language model must beat. Games lacking any agent pair with winrate exceeding 80% are removed from consideration. Following this procedure, 126 distinct games remain. Among the 4 Before Filtering After Filtering Mean Std Min Max Mean Std Min Max Description length (tokens) Code length (lines) Action length (tokens) Action space size 1864.3 126.6 124.2 78.6 449.4 41.7 45.6 584.6 810 54 34 2 4505 408 327 13750 1857.2 125.5 122.3 70.0 389.2 39.7 42.4 268. 929 61 34 2 3158 255 253 2500 Table 1: Basic data statistics for the 1000 games before filtering and the 126 games after filtering in gg-bench. Action length is the length of the natural language description of the action space. remaining games, the winning RL agents achieve an average winrate of 91.02% against the chosen benchmark opponents, providing an existence proof that the games are practically beatable."
        },
        {
            "title": "3 Analysis of Generated Games",
            "content": "3.1 Data Statistics We use o1 to generate natural language descriptions and code implementations for 1000 games; of these, 126 games passed all stages of filtering. We report basic statistics for these games in Table 1. 3.2 Diversity of Games 3.2.1 Evaluating Code Similarity To measure the diversity and originality of the generated games, we employ DOLOS [23], an open-source alternative to MOSS [34] for detecting code plagiarism. DOLOS assigns similarity score in the range [0, 1], where 0 indicates no detectable similarity and 1 an identical match. Across all game implementations, we observe median maximum similarity score of 0.41. For context, the example and Java plagiarism datasets provided on the DOLOS website exhibit median similarity score on the plagiarised documents of 0.72. Additionally, we note that much of the similarity between game implementations is caused by boilerplate Gym code, e.g., having similar imports. The distribution of scores is shown in Figure 5 and additional statistics are presented in Table 5. 3.2.2 What type of games are in gg-bench? To categorize the games in gg-bench by underlying strategy and core gameplay mechanics, we employed the goal-driven clustering method introduced by Wang et al. [42]. We use OpenAI o1 [27] to generate distinct categories for games such as number-based puzzles, grid-based movement games, and combinatorial strategy games. Then, we employ OpenAI o3-mini [28] to assign each game to one of the proposed categories. Lastly, we group each of the categories into into five broader ones, described in Table 2. We provide the prompts used for categorization and the implementation details in Appendix D. We also provide more examples of games in Table 3. Examining the distribution, we observe that number games, where the core mechanic involves choosing and manipulating numbers, often through arithmetic or number-theoretic reasoning, are the most common. We hypothesize this is due to number games being the easiest to implement and passing our filtering more than other games. Indeed, as shown in Figure 6, number games only make up 20.3% of the total game distribution prior to filtering as opposed to 36.7% post-filtering. We likewise see consistent inclination toward random-chance mechanics and board games with clear action spaces, while combat-oriented games drop sharplyfrom 31.1% to 9.4% after filtering, likely because their win/lose state conditions are much more challenging to describe and implement. 3.3 Faithfulness of Code Implementations As shown in previous work [25], achieving self-consistencyaccurately mapping language descriptions to code implementations and vice versacan be challenging even for large language models, particularly when handling non-trivial tasks. To guard against the possibility of single-step 5 Category Share Example Core mechanics / objective Number 36.7% Prime Claim Board 27.6% Isolation Card 14.6% HighLow Battle Chance 11.7% Digit Dilemma Combat 9.4% Elemental Clash Players alternately claim the integers 125. Primes add their own value; composites add their value and gift the factor-sum to the rival. Higher total after all picks wins; last pick breaks ties. Players alternately claim unoccupied squares on 13-square line that are not adjacent to any claimed square. The first to leave the opponent without valid move wins. Players simultaneously reveal chosen cards 19 over five rounds, earning 1 pt for higher card or 2 pts via the lowerprevious-card tie-breaker. Highest total score wins. From random 20-digit line, players alternately take one digit from either end and append it to their number; when the line is empty, the higher number wins (ties go to the second mover). Two players start with 10 HP and four one-use spells. Elements interact rock-paper-scissors style; the winner deals damage, while ties hurt both. First to 0 HPor with no spells leftloses. Table 2: Types of games present in gg-bench and illustrative examples from each category. self-consistency failures, where o1 might generate inaccurate game implementations, we manually evaluated randomly selected subset of 10 out of the 126 filtered o1 games. Concretely, we annotated the descriptions, inspected the corresponding code, and then played through these generated environments ourselves. This verification step allowed us to directly assess whether each environments implementation had faithfully matched the game mechanics described in the corresponding text. Of the 10 games we examined, all provided correct implementations. However, the implementation of number games sometimes provided hard-coded details. For instance, in Divide and Conquer (index 154), where players take turns dividing shared number by some prime factor, we noticed that prime factors that can be used are hard-coded as list, with all numbers 50. While the game is still playable with this detail, it could error if the shared number is exceptionally high. However, we note that the language model is told (via the action description) that the list of primes is hard-coded."
        },
        {
            "title": "4 Experiments",
            "content": "Models. We evaluate various state-of-the-art LLMs: OpenAI ChatGPT (GPT-4o, GPT-4o-mini), Anthropic (Claude 3.7 Sonnet), Meta LLaMA (LLaMA-3.3-70B-Instruct). We also test reasoning models such as OpenAI o1, o3-mini and DeepSeek-R1. Small models (7/13B) are not tested due to the difficulty of the benchmark. Input format. In order to get an action from model, we prompt it with the game description, the current board state, list of valid moves, and description of what each move means. The model is then required to output move from this list. If the model outputs an move not present in the list, we re-prompt the model and try again. The prompts used can be found in Appendix B.4. Methods. Each language model plays 30 games against an RL agent for every game in the benchmark. We calculate the winrate as the percentage of games the language model wins. The final score for each language model is the average winrate across all 126 games. 4.1 Results Model performance. As shown in Figure 3a, non-reasoning language models achieve relatively low winrates between 7% and 9%, while reasoning models achieve winrates between 31% and 36%. We observe that GPT-4o and Claude 3.7 Sonnet perform better than GPT-4o-mini and LLama-3.3-70B, indicating that larger models may have an advantage in handling the complexity of gg-bench. We also observe that reasoning models such as DeepSeek-R1 or OpenAI o3-mini achieve much stronger (a) gg-bench winrates (mean 95% CI) (b) Failure reasons for GPT-4o Model LLaMA-3.3-70B GPT-4o-mini GPT-4o Claude 3.7 Sonnet o3-mini DeepSeek-R1 o1 gg-bench 7.42 ( 2.78) 7.64 ( 2.26) 8.94 ( 2.77) 9.53 ( 3.05) 31.08 ( 5.73) 32.50 ( 5.14) 36.28 ( 5.95) 81.98% 8.97% Losses (2971) Faults (184) Draws (325) 5.29% Figure 3: (a) Average winrates of various LLMs on gg-bench (30 games per matchup; 95% CIs in parentheses). (b) Breakdown of GPT-4o failures: Faults are invalid-move errors. performance than non-reasoning models, suggesting that explicit reasoning capabilities are critical for success on gg-bench. This highlights the benchmarks emphasis on structured decision-making and long-horizon planning, which appear to benefit from models trained on reasoning tasks. Cost analysis. Since each game in gg-bench requires interaction with an RL agent, the evaluation process can be expensive. For GPT-4o-mini, GPT-4o, o3-mini and o1 the API costs were $6, $101, $258 and $2547 respectively, while for Claude 3.7 Sonnet, the cost was $118. DeepSeek-R1 was run on the together.ai API, which cost $461. Llama-3.3-70B was run locally on 4xNVIDIA A6000 GPUs. On average, for non-reasoning models, input tokens make up 99.95% of the cost, as the output tokens consist of single number, i.e., the move the model makes. For reasoning models, however, the split skewed towards output tokens, with just 19.07% of the cost going to input tokens. 4.2 Failure Analysis Failure reason breakdown. In Figure 3b, we show the distribution of failure reasons in gg-bench. The majority of losses are due to the RL agent winning, with small percentage of draws and language model faults. The high percentage of RL agent wins suggests that current language models struggle with the strategic reasoning and adaptability required to succeed in these games. The low percentage of draws indicates that the games are well-designed and do not often result in stalemates. Example failed trajectory. Cross Over (index 526) is two-player strategy game where each side attempts to either invade the opponents territory or eliminate all opposing pieces by moving along linear track. On each turn, players can move each of their pieces either one or two steps along the track. In Figure 4, we show an example game where o1 (labeled LLM) loses to the RL agent. The early game is balanced until move 5, where the LLM moves piece P1-C to position 6, which the RL agent captures. After this, the LLM trades back and captures piece P2-B, but in doing so, leaves its own backline undefended; notably, piece P1-A remains idle at position 0 for the entire game. This allows the RL agent to advance P2-C forward, and win the game. This trajectory illustrates the LLMs inability to evaluate long-term consequences of trades and territory exposure. 4.3 Scalability We anticipate that more advanced language models will be capable of generating harder games. To substantiate this claim, we conducted small-scale experiment comparing the quality of games generated by GPT-4o and OpenAI o1. We re-ran the generation pipeline of gg-bench using GPT-4o to create descriptions, implementations and action descriptions. After applying the syntactic and semantic filters described in Section 2.3 followed by the RL-agent upper-bound check in Section 2.4, 126 of the 1000 o1 games remained, whereas only 10 of the 1000 GPT-4o games survived. Manual inspection reveals qualitative gap as well. 8 out of 10 of GPT-4o-generated games are near-identical variants of Tic-Tac-Toe (cf. Appendix E), whereas the o1 set contains diverse collection of novel win conditions and action spaces. These findings provide preliminary evidence that model scale is proportional to the difficulty and quality of the games present in gg-bench. Consequently, this result suggests that gg-bench may be future-proof ; any saturation of the benchmark can potentially be mitigated by re-running the pipeline with better model. 7 Move 5: LLM moved P1-C to vulnerable position 6 0 1 2 3 4 5 Move 6: Agent captures P1-C 0 1 2 3 5 6 7 7 8 10 8 9 10 0 2 3 4 5 6 8 9 10 Move 7: LLM captures P2-B Move 10: Agent wins P2-C crosses into P1 territory 0 1 2 3 4 5 7 8 9 10 x v N m A r 3 e Legend: LLM (P1) piece Agent (P2) piece P1 territory (02) P2 territory (810) Neutral (37) Figure 4: Example trajectory of Cross Over where o1 (labeled LLM) loses to the RL agent. Moves 0-4 are hidden as the game appears balanced until then, with both the LLM and the RL agent advancing their pieces forward. At Move 5, the LLM moves P1-C to position 6, highlighted by the blue arrow."
        },
        {
            "title": "5 Related Work",
            "content": "Benchmarking LLMs with games. Games have long served as testbeds for measuring AI capabilities, leading to breakthroughs like Deep Blue for chess [8], AlphaZero for Go [37], and Libratus for poker [5]. Schaul et al. [33] argue that games offer scalable proxy for artificial general intelligence because they can be procedurally generated to span broad spectrum of difficulties and skills. Recent work has begun to evaluate LLMs with games. Text-adventure suites such as Jericho [17] are designed to test agents abilities to parse narrative state and issue actions. GameBench [11] focuses on hand-picked environments (e.g. Battleship, Connect Four) chosen to stress distinct planning skills while avoiding games likely present in pre-training corpora. Topsakal et al. [41] provide leaderboard for grid-based game competitions. ZeroSumEval [1] conducts arena-style evaluations on LLMs in classic strategy games like chess and poker, as well as knowledge tests and persuasion games. VGBench [46] challenges vision-language agents to complete suite of 20 commercially released Game Boy and MS-DOS titles, ranging from Doom II to Pokémon Red, using only raw pixels as input. Releases of both Claude 3.7 Sonnet [3] and Gemini 2.5 Pro [12] emphasized the models abilities to play Pokémon Red, citing it as strong out-of-distribution test of strategic reasoning. In contrast to all these works, though, we focus on games which are also generated by language models. Scalable benchmarking. Fixed test sets quickly saturate as models improve, prompting shift toward scalable or partially synthetic benchmarks that continuously generate new tasks. BIG-bench [38] introduced community-contributed suite of over 200 tasks covering logic, math, and common-sense reasoning, many of which are procedurally created to avoid memorization, with BIG-bench Hard [40] isolating the most challenging subsets. Dynabench [20] uses dynamic adversarial approach: humans interact with state-of-the-art models in the loop, crafting inputs that fool them; those failures are immediately added to the training and evaluation pool, preventing saturation and exposing model weaknesses in real time. SWE-bench [19] automatically generates test instances by extracting coding 8 tasks from real-world GitHub issues. τ -bench [45] follows hybrid synthetic approach, combining manually designed schemas, LLM-generated dialogues, and human refinement to evaluate agent interactions with tools and users in realistic domains. In contemporary work, Absolute Zero [47] uses LLMs to generate synthetic tasks which are used for training reasoning models. gg-bench inherits this spirit of scalability: new games, code implementations, and RL agents can be regenerated on demand, reducing the potential risks of dataset contamination and benchmark saturation. Reasoning with language models. Many recent advancements in language modeling have been driven by reasoning, or the use of additional inference-time compute in order to obtain higher-quality generations. Early work in this direction showed that prompting models to generate explicit stepby-step answers, i.e., chain of thought, improved their arithmetic and logical consistency [26, 43]. Training models to generate longer chains of thought via reinforcement learning has supposedly resulted in models such as OpenAIs o-series models [27, 28, 29], Googles Gemini 2.5 Pro [12], Claude 3.7 Sonnet with \"extended thinking\" mode [2] and DeepSeeks R1 [13], which have massively outperformed traditional LLMs on wide range of benchmarks. Meanwhile, program-aided reasoning systems like PAL have models emit code that is executed to obtain verifiable answers, pushing performance beyond pure text-only reasoning [14]. Tool-use agents (e.g. ReAct, Reflexion) further integrate search, calculators, or external APIs into the reasoning loop, enabling models to plan, act, and reflect iteratively [44, 36]. Despite these advances, LLMs remain fragile in long-horizon and stateful settings, as evidenced by their performance in gg-bench."
        },
        {
            "title": "6 Discussion & Future Work",
            "content": "In contrast to traditional static benchmarks, the synthetic nature of gg-bench offers additional flexibility for future researchers looking to expand this dataset. We outline some key benefits below: gg-bench is scalable. Because gg-bench is data generating process, new games can be continuously generated using the existing pipeline, allowing the benchmark to expand as needed and mitigating potential risks of data contamination. More importantly, as model capabilities improve and the current iteration of the benchmark becomes saturated, we anticipate that stronger models will also be able to generate increasingly difficult games. RL agents will also likely scale alongside new algorithms and techniques; however, in the future, if training RL agents becomes bottleneck, language models could also be evaluated in arena-style competitions against each other [9, 1]. We predict that this scalability will result in gg-bench having greater longevity than most benchmarks. Controllable evaluation. The data generating process of gg-bench is interpretable by design and therefore easily modifiable. For example, if future researchers wish to focus on games with specific design elements, or to modify aspects of existing games, they can easily do so by modifying our prompts or intermediate game descriptions. Additionally, the difficulty of the benchmark can also be tuned by selecting weaker or stronger RL agent checkpoints to evaluate language models against. Diverse evaluation. Many existing benchmarks evaluate language models using known tasks or games, such as chess. However, because these tasks are often well-represented online (e.g., the web contains millions of games of chess), language models can obtain good performance by simply memorizing task-specific behavior rather than learning to adapt and reason in general settings. In contrast, gg-bench uses language models to design new games which are intended to differ from existing games that are over-represented in training corpora. Future work could further analyze the originality of our games and measure model performance as function of game novelty. Of course, the framework presented in this paper cannot possibly capture all aspects of general intelligence. For instance, the social intelligence of language models [32] cannot be evaluated in the context of two player, zero-sum games. Furthermore, the definition and even the utility of the concept of intelligence have been hotly debated [39, 21]. However, we hope that gg-benchs ability to measure model performance beyond human-curated tasks will provide useful signal to researchers looking to better understand and quantify the domain-general capabilities of language models."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "We thank Alane Suhr, Allen Nie, Jiayi Pan, Lucy Li, Ruiqi Zhong, and Wenting Zhao for feedback and discussions related to this work. Nicholas Tomlin is funded by grant from FAR.AI."
        },
        {
            "title": "References",
            "content": "[1] Hisham Alyahya, Haidar Khan, Yazeed Alnumay, Saiful Bari, and Bülent Yener. ZeroSumEval: An extensible framework for scaling llm evaluation with inter-model competition. arXiv preprint arXiv:2503.10673, 2025. [2] Anthropic. Claude 3.7 Sonnet, 2025. URL https://www.anthropic.com/news/ claude-3-7-sonnet. [3] Anthropic. Claudes extended thinking. 2025. URL https://www.anthropic.com/ research/visible-extended-thinking. [4] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. OpenAI Gym, 2016. URL https://arxiv.org/abs/1606.01540. [5] Noam Brown and Tuomas Sandholm. Superhuman AI for heads-up no-limit poker: Libratus beats top professionals. Science, 359(6374):418424, 2018. doi: 10.1126/science.aao1733. URL https://www.science.org/doi/abs/10.1126/science.aao1733. [6] Sébastien Bubeck, Varun Chadrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with GPT-4, 2023. [7] Bruce Buchanan, Georgia Sutherland, and Edward Feigenbaum. Heuristic DENDRAL: program for generating explanatory hypotheses. Organic Chemistry, 30, 1969. [8] Murray Campbell, A.Joseph Hoane, and Feng hsiung Hsu. Deep Blue. Artificial Intelligence, 134(1):5783, 2002. ISSN 0004-3702. doi: https://doi.org/10.1016/S0004-3702(01)00129-1. URL https://www.sciencedirect.com/science/article/pii/S0004370201001291. [9] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. Chatbot arena: An open platform for evaluating LLMs by human preference. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 83598388. PMLR, 2127 Jul 2024. URL https://proceedings.mlr.press/v235/chiang24b.html. [10] François Chollet. On the measure of intelligence, 2019. URL https://arxiv.org/abs/ 1911.01547. [11] Anthony Costarelli, Mat Allen, Roman Hauksson, Grace Sodunke, Suhas Hariharan, Carlson Cheng, Wenjie Li, Joshua Clymer, and Arjun Yadav. GameBench: Evaluating strategic reasoning abilities of LLM agents, 2024. URL https://arxiv.org/abs/2406.06613. [12] Google DeepMind. Gemini 2.5 Pro. 2025. URL https://deepmind.google/ technologies/gemini/pro/. [13] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang 10 Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. [14] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. PAL: Program-aided language models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 1076410799. PMLR, 2329 Jul 2023. URL https: //proceedings.mlr.press/v202/gao23f.html. [15] Howard Gardner. Frames of mind: The theory of multiple intelligences. Basic books, 2011. [16] Ben Goertzel and Cassio Pennachin. Artificial general intelligence, volume 2. Springer, 2007. [17] Matthew Hausknecht, Prithviraj Ammanabrolu, Côté Marc-Alexandre, and Yuan Xingdi. Interactive fiction games: colossal adventure. CoRR, abs/1909.05398, 2019. URL http://arxiv.org/abs/1909.05398. [18] Johannes Heinrich and David Silver. Deep reinforcement learning from self-play in imperfectinformation games, 2016. URL https://arxiv.org/abs/1603.01121. [19] Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. SWE-bench: Can language models resolve real-world GitHub issues?, 2024. URL https://arxiv.org/abs/2310.06770. [20] Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel, Zeerak Waseem, Pontus Stenetorp, Robin Jia, Mohit Bansal, Christopher Potts, and Adina Williams. Dynabench: Rethinking benchmarking in NLP. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 41104124, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.324. URL https://aclanthology.org/ 2021.naacl-main.324/. [21] Shane Legg, Marcus Hutter, et al. collection of definitions of intelligence. Frontiers in Artificial Intelligence and applications, 157:17, 2007. [22] Douglas Lenat, Ramanathan V. Guha, Karen Pittman, Dexter Pratt, and Mary Shepherd. Cyc: toward programs with common sense. Communications of the ACM, 33(8):3049, 1990. [23] Rien Maertens, Maarten Van Neyghem, Maxiem Geldhof, Charlotte Van Petegem, Niko Strijbol, Peter Dawyndt, and Bart Mesuere. Discovering and exploring cases of educational source code plagiarism with Dolos. SoftwareX, 26:101755, 2024. ISSN 2352-7110. doi: https: //doi.org/10.1016/j.softx.2024.101755. URL https://www.sciencedirect.com/science/ article/pii/S2352711024001262. [24] John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon. proposal for the Dartmouth summer research project on artificial intelligence. AI magazine, 27(4):1212, 1955. [25] Marcus J. Min, Yangruibo Ding, Luca Buratti, Saurabh Pujar, Gail Kaiser, Suman Jana, and Baishakhi Ray. Beyond accuracy: Evaluating self-consistency of code large language models with IdentityChain. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=caW7LdAALh. [26] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show 11 your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021. [27] OpenAI. Introducing OpenAI o1, 2024. URL https://openai.com/o1/. [28] OpenAI. OpenAI o3-mini, 2025. URL https://openai.com/index/openai-o3-mini/. [29] OpenAI. Introducing o3 and o4-mini. 2025. URL https://openai.com/index/ introducing-o3-and-o4-mini/. [30] Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stable-Baselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Research, 22(268):18, 2021. URL http://jmlr.org/papers/v22/ 20-1364.html. [31] Stuart Russell and Peter Norvig. Artificial intelligence: modern approach. Pearson, 2016. [32] Maarten Sap, Ronan Le Bras, Daniel Fried, and Yejin Choi. Neural theory-of-mind? on the limits of social intelligence in large LMs. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 37623780, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.248. URL https://aclanthology.org/2022.emnlp-main.248/. [33] Tom Schaul, Julian Togelius, and Jürgen Schmidhuber. Measuring intelligence through games, 2011. URL https://arxiv.org/abs/1109.1314. [34] Saul Schleimer, Daniel Wilkerson, and Alex Aiken. Winnowing: local algorithms for document fingerprinting. In Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data, pages 7685, 2003. [35] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347. [36] Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning, 2023. URL https://arxiv.org/abs/2303.11366. [37] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, and Demis Hassabis. Mastering chess and shogi by self-play with general reinforcement learning algorithm, 2017. URL https://arxiv.org/abs/1712.01815. [38] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmüller, Andrew Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakas, B. Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bartłomiej Bojanowski, Batuhan Özyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Bryan Orinion, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, César Ferri Ramírez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher D. Manning, Christopher Potts, Cindy Ramirez, Clara E. Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Moseguí González, Danielle Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, Dar Gilboa, David Dohan, David Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Dylan Schrader, Ekaterina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth Donoway, Ellie Pavlick, Emanuele Rodola, Emma Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A. Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim, Eunice Engefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar, Fernando Martínez-Plumed, Francesca Happé, Francois Chollet, Frieda Rong, Gaurav Mishra, Genta Indra Winata, Gerard de Melo, Germán Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Wang, Gonzalo Jaimovitch-López, Gregor Betz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin, Hinrich Schütze, Hiromu Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, Jackson Kernion, Jacob Hilton, Jaehoon Lee, Jaime Fernández Fisac, James B. Simon, James Koppel, James Zheng, James Zou, Jan Kocon, Jana Thompson, Janelle Wingfield, Jared Kaplan, Jarema Radom, Jascha Sohl-Dickstein, Jason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang, Joan Waweru, John Burden, John Miller, John U. Balis, Jonathan Batchelder, Jonathan Berant, Jörg Frohberg, Jos Rozen, Jose Hernandez-Orallo, Joseph Boudeman, Joseph Guerr, Joseph Jones, Joshua B. Tenenbaum, Joshua S. Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert, Kaustubh D. Dhole, Kevin Gimpel, Kevin Omondi, Kory Mathewson, Kristen Chiafullo, Ksenia Shkaruta, Kumar Shridhar, Kyle McDonell, Kyle Richardson, Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan, Lianhui Qin, Lidia ContrerasOchando, Louis-Philippe Morency, Luca Moschella, Lucas Lam, Lucy Noble, Ludwig Schmidt, Luheng He, Luis Oliveros Colón, Luke Metz, Lütfi Kerem Senel, Maarten Bosma, Maarten Sap, Maartje ter Hoeve, Maheen Farooqi, Manaal Faruqui, Mantas Mazeika, Marco Baturan, Marco Marelli, Marco Maru, Maria Jose Ramírez Quintana, Marie Tolkiehn, Mario Giulianelli, Martha Lewis, Martin Potthast, Matthew L. Leavitt, Matthias Hagen, Mátyás Schubert, Medina Orduna Baitemirova, Melody Arnaud, Melvin McElrath, Michael A. Yee, Michael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt, Michael Strube, Michał Swedrowski, Michele Bevilacqua, Michihiro Yasunaga, Mihir Kale, Mike Cain, Mimee Xu, Mirac Suzgun, Mitch Walker, Mo Tiwari, Mohit Bansal, Moin Aminnaseri, Mor Geva, Mozhdeh Gheini, Mukund Varma T, Nanyun Peng, Nathan A. Chi, Nayeon Lee, Neta Gur-Ari Krakover, Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nicole Martinez, Nikita Nangia, Niklas Deckers, Niklas Muennighoff, Nitish Shirish Keskar, Niveditha S. Iyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain Evans, Pablo Antonio Moreno Casares, Parth Doshi, Pascale Fung, Paul Pu Liang, Paul Vicol, Pegah Alipoormolabashi, Peiyuan Liao, Percy Liang, Peter Chang, Peter Eckersley, Phu Mon Htut, Pinyu Hwang, Piotr Miłkowski, Piyush Patil, Pouya Pezeshkpour, Priti Oli, Qiaozhu Mei, Qing Lyu, Qinlang Chen, Rabin Banjade, Rachel Etta Rudolph, Raefer Gabriel, Rahel Habacker, Ramon Risco, Raphaël Millière, Rhythm Garg, Richard Barnes, Rif A. Saurous, Riku Arakawa, Robbe Raymaekers, Robert Frank, Rohan Sikand, Roman Novak, Roman Sitelew, Ronan LeBras, Rosanne Liu, Rowan Jacobs, Rui Zhang, Ruslan Salakhutdinov, Ryan Chi, Ryan Lee, Ryan Stovall, Ryan Teehan, Rylan Yang, Sahib Singh, Saif M. Mohammad, Sajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruetter, Samuel R. Bowman, Samuel S. Schoenholz, Sanghyun Han, Sanjeev Kwatra, Sarah A. Rous, Sarik Ghazarian, Sayan Ghosh, Sean Casey, Sebastian Bischoff, Sebastian Gehrmann, Sebastian Schuster, Sepideh Sadeghi, Shadi Hamdan, Sharon Zhou, Shashank Srivastava, Sherry Shi, Shikhar Singh, Shima Asaadi, Shixiang Shane Gu, Shubh Pachchigar, Shubham Toshniwal, Shyam Upadhyay, Shyamolima, Debnath, Siamak Shakeri, Simon Thormeyer, Simone Melzi, Siva Reddy, Sneha Priscilla Makini, Soo-Hwan Lee, Spencer Torene, Sriharsha Hatwar, Stanislas Dehaene, Stefan Divic, Stefano Ermon, Stella Biderman, Stephanie Lin, Stephen Prasad, Steven T. Piantadosi, Stuart M. Shieber, Summer Misherghi, Svetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali, Tatsu Hashimoto, Te-Lin Wu, Théo Desbordes, Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick, Timofei Kornev, Titus Tunduny, Tobias Gerstenberg, Trenton Chang, Trishala Neeraj, Tushar Khot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria Nyamai, Vikas Raunak, Vinay Ramasesh, Vinay Uday Prabhu, Vishakh Padmakumar, Vivek Srikumar, William Fedus, William Saunders, William Zhang, Wout Vossen, Xiang Ren, Xiaoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh, Yair Lakretz, Yangqiu Song, Yasaman Bahri, Yejin Choi, Yichi Yang, Yiding Hao, Yifu Chen, Yonatan Belinkov, Yu Hou, Yufang Hou, Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zijian Wang, Zijie J. Wang, Zirui Wang, and Ziyi Wu. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models, 2023. URL https://arxiv.org/abs/2206.04615. 13 [39] Robert Sternberg and Douglas Detterman. What is intelligence?: Contemporary viewpoints on its nature and definition. Praeger, 1986. [40] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Challenging BIG-Bench tasks and whether chain-of-thought can solve them, 2022. URL https://arxiv.org/abs/2210.09261. [41] Oguzhan Topsakal, Colby Jacob Edell, and Jackson Bailey Harper. Evaluating large language models with grid-based game competitions: An extensible LLM benchmark and leaderboard, 2024. URL https://arxiv.org/abs/2407.07796. [42] Zihan Wang, Jingbo Shang, and Ruiqi Zhong. Goal-driven explainable clustering via language descriptions. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1062610649, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. emnlp-main.657. URL https://aclanthology.org/2023.emnlp-main.657/. [43] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. URL https://arxiv.org/abs/2201.11903. [44] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. ReAct: Synergizing reasoning and acting in language models, 2023. URL https: //arxiv.org/abs/2210.03629. [45] Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. τ -bench: benchmark for tool-agent-user interaction in real-world domains, 2024. URL https://arxiv.org/abs/ 2406.12045. [46] Alex Zhang and Ofir Press. VideoGameBench: Research preview. 2025. URL https: //www.vgbench.com/. [47] Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Matthieu Lin, Shenzhi Wang, Qingyun Wu, Zilong Zheng, and Gao Huang. Absolute Zero: Reinforced self-play reasoning with zero data. arXiv preprint arXiv:2505.03335, 2025."
        },
        {
            "title": "A Game Descriptions",
            "content": "In Table 3, we provide more examples of games present in gg-bench. These ten games illustrate the diversity of gameplay mechanics, ranging from arithmetic-based challenges (Divide and Conquer) to spatial reasoning (Light Out Duel), hidden information (Line Duel), and combinatorial strategy (Order Challenge). Each game is two-player and turned based."
        },
        {
            "title": "B Implementation Details",
            "content": "In this section, we provide implemetation details, such as prompts used for generation and evaluation or hyperparameters used during RL training. B.1 Game Description Generation We used the following prompt for game description generation: You are tasked with creating rule book for new two player turn-based game designed to be played in command-line interface. The game should be easy and simple to code, with no draw mechanism and should end quickly. Furthermore, the game should be designed such that skilled player should be able to consistently beat an unskilled player. Make sure that the game is unique, and is NOT similar to existing games such as Go, Nim, Tic-Tac-Toe or Chess. The rule book should cover the following aspects: Objective: Clearly define the primary goal of the game. Explain how players can achieve victory and what constitutes win or loss. Game Core mechanics / objective Palindrome Duel Players add or to either end of sequence, avoiding formation of palindromes (length 3). Forming palindrome loses; reaching 11 symbols without palindromes wins. Divide and Conquer Players take turns dividing shared integer by chosen prime factor, aiming to be the one to reduce it exactly to 1. Power Match Line Duel Clash of Powers Reach 27 Number Clash Order Challenge Light Out Duel Command Clash Each round, players choose base (19) and an exponent (19); the higher resulting power wins (ties favor Player 2). Players secretly play power cards (15) on number line from 5 to +5. The difference on each turn pushes marker; reaching the opponents endpoint wins. Players each hold the powers 1,2,4,8,16 and play one per round. Higher number wins unless it is exactly double the opponents, in which case the smaller wins. First to 3 round-wins takes the game. Players alternately add number from 1 to 9 to running total, racing to be the one who hits exactly 27. Exceeding 27 on your turn results in an immediate loss. Both players start at 10 HP and simultaneously play cards 19. Damage dealt equals the difference between cards (ties deal 1 HP to both). First to reduce the opponent to 0 HP wins. Players build strictly increasing sequences by picking unique numbers 19. On each turn, player must pick number larger than their previous pick; failure to move loses. From row of seven lights, players alternately switch off either one light or two adjacent lights. The player who flips off the last remaining light wins. Players start with 5 Command Points and secretly choose each turn among Charge, Attack, Special Attack, or Shield. The goal is to reduce the opponents CP to zero. Table 3: Examples of two-player, turn-based strategy games present in gg-bench. Each row summarizes the core mechanics and objectives of distinct game. Setup: Describe the initial setup of the game, including the arrangement of game elements, player positions, and any starting conditions. Game Components: List and explain all components involved in the game, such as pieces, tokens, boards, or cards. Provide details on their appearance, functionality, and any unique attributes. Turns: Outline the structure of turn, including the order of actions, what players can do during their turn, and how turns progress. Rules and Mechanics: Detail the core rules and mechanics of the game. This should include movement or action rules, special abilities, interactions between game components, and any unique game mechanics. Scoring: Explain how points or other forms of scoring are tracked and how they contribute to winning the game. Examples: Provide example scenarios and command-line interactions or sample turns to illustrate how the rules are applied in practice. Ensure that the rule book is clear, organized, and comprehensive, providing all necessary information to players while allowing for strategic depth and complexity. B.2 Environment Generation In order to generate gym environment from game description, we used the prompt below, providing an example Tic-Tac-Toe environment. We replaced <GameDescription> with the game generated using Appendix B.1. <GameDescription> Given this description, write gym environment that implements this game. Use gymnasium's API to define the environment. The action_space of the environment should be Discrete space, use spaces.Discrete to define the action_space. The observation_space should be Box space, use spaces. The reward should be 1 if the current player wins, and -10 if the current player has played valid move. The environment should internally manage automatically switching between each player, it should be designed for self-play reinforcement learning. The environment should have the following methods: - `reset()`: Reset the environment to its initial state. Returns observation, info (dict). - `step(action)`: Take step in the environment. Returns observation, reward, done, info (dict). - `render()`: Return visual representation of the environment state as string. - `valid_moves()`: Return list of integers of valid moves as indices of the action_space. Here is an example of how to define the environment: ```python import numpy as np import gymnasium as gym from gymnasium import spaces class TicTacToeEnv(gym.Env): def __init__(self): super(TicTacToeEnv, self).__init__() # Define action and observation space self.action_space = spaces.Discrete(9) self.observation_space = spaces.Box( low=-1, high=1, shape=(9,), dtype=np.float32 ) # Initialize the board self.reset() def reset(self, seed=None, options=None): super().reset(seed=seed) self.board = np.zeros(9, dtype=np.float32) self.current_player = 1 self.done = False return self.board, {} # Return observation and info def step(self, action): if self.board[action] != 0 or self.done: return ( self.board, -10, True, False, {}, ) # Observation, reward, terminated, truncated, info self.board[action] = self.current_player 16 # Check for win win_combinations = [ [0, 1, 2], [3, 4, 5], [6, 7, 8], # Rows [0, 3, 6], [1, 4, 7], [2, 5, 8], # Columns [0, 4, 8], [2, 4, 6], # Diagonals ] for combo in win_combinations: if all(self.board[i] == self.current_player for in combo): self.done = True return self.board, 1, True, False, {} # Check for draw if np.all(self.board != 0): self.done = True return self.board, 0, True, False, {} self.current_player *= -1 return self.board, 0, False, False, {} def render(self): board_str = \"-------------n\" for in range(3): board_str += \"\" for in range(3): if self.board[i * 3 + j] == 1: board_str += \" \" elif self.board[i * 3 + j] == -1: board_str += \" \" else: board_str += \" \" board_str += \"n-------------n\" return board_str def valid_moves(self): return [i for in range(9) if self.board[i] == 0] ``` Call the environment `CustomEnv`. Do not include any code that creates the gym environment or tests it. Make sure the environment is fully functional, requires no modifications and adheres to the requirements specified in the prompt. Do not include any placeholder functions or TODOs in the code. B.3 Generation Action Descriptions For generating descriptions as to what each index in the action space corresponds to, we used the following prompt, formatting <GameDescription> with the generated game description, <PythonCode> with the implementation of the game. Here is description for two-player game: <GameDescription> Now, here is some python code that defines gym environment for this game: ```python <PythonCode> ``` 17 Your task is to write brief explanation for the mapping between the action space indices and moves in the game. Be concise with your answer and avoid redundancy. Respond immediately with the explanation. Do not include any other text in your response. B.4 Language Model Evaluation For having the language model play against our RL agents, we used the following system prompt, formatting <GameDescription> with the generated game description and <MoveDescription> with the generation action space description. Here is description for two-player game: <GameDescription> You will be prompted with board state and list of legal moves for the current play. Your task is to pick the best move from this list. Here is description for what each move represents: <MoveDescription> Then, for each turn, we inserted the following prompt, replacing <BoardState> with the rendered board and <LegalMoves> with the list of legal moves the language model is allowed to take. <BoardState> Legal moves: <LegalMoves> Pick the best move from the list of legal moves. Respond with the number you wish to play. Do not include any other text in your response. B.5 Self-Play Reinforcement Learning Reinforcement learning agents are trained using proximal policy optimization (PPO) [35], using the implementation present in Stable Baselines3 [30]. PPO optimizes clipped surrogate objective: LCLIP(θ) = Et (cid:104) min (cid:16) rt(θ) ˆAt, clip(rt(θ), 1 ϵ, 1 + ϵ) ˆAt (cid:17)(cid:105) πθold (atst) is the probability ratio, and ˆAt is the estimated advantage. The clipping where rt(θ) = πθ(atst) prevents large, destabilizing updates by keeping rt(θ) close to 1. Advantage estimation We use generalized advantage estimation (GAE) to compute ˆAt: ˆAt = (cid:88) l= (γλ)lδt+l, δt = rt + γV (st+1) (st) where γ is the discount factor and λ is the GAE decay parameter. Training setup Agents are trained via self-play for 106 timesteps, with checkpoints saved every 2.5 105 steps. Initially, agents play against random policy. After the first checkpoint, opponents are sampled uniformly from past checkpoints. Exploration is encouraged using ϵ-greedy action selection, with ϵ decaying linearly from 1.0 to 0.1. In addition, during training, we apply timeout wrapper to the environment. If the environment crosses 100 moves from either players, the game terminates with an error and is filtered out. This is done to account for any games that unintentionally crept through the filtering present in Section 2.3. We provide the hyperparameters used during training in Table 4. 18 Hyperparameter Value Learning rate Discount factor (γ) GAE lambda (λ) Clip range (ϵ) Batch size Rollout length 3e-4 0.99 0.95 0.2 64 2048 Table 4: Key PPO hyperparameters used during training. Inference via MCTS At inference time, we apply Monte Carlo tree search (MCTS) to pick the move taken by RL agents. At the current state, we start by simulating 100 self-play rollouts using the trained policy. These are done by sampling random action continuously from the probability distribution outputted by the RL policy, applied to both players. Each self-play rollout terminates when an ending state is hit. For each node, we keep track of the number of visits. Let (s, a) be the number of visits to child at root state s. We select the action: = arg max (s, a) i.e., the move leading to the most simulated wins."
        },
        {
            "title": "C Plagiarism Analysis",
            "content": "For each game file in gg-bench, we computed its highest pairwise similarity to all other files using DOLOS [23]. Figure 5 shows the distribution of these maxima, and Table 5 summarizes the key statistics. Figure 5: Distribution of the highest similarity score for every one of the 126 games in gg-bench. Mean Std Min 25% 50% 75% Max Highest similarity 0.436 0.118 0.222 0.351 0. 0.536 0.836 Table 5: Summary statistics of the highest similarity score observed for each game file (n = 126). 19 The median maximum-overlap score is 0.408, and three-quarters of files fall below 0.54, indicating only modest shared code beyond boiler-plate utilities. Only few files exceed 0.70 (the peak is 0.836), and manual inspection attributes these cases to common helper functions rather than direct copying of gameplay logic. Overall, the analysis suggests that plagiarism within the corpus is limited and localised, supporting the benchmarks integrity. Goal-Driven Clustering of Game Descriptions To analyze the diversity of environments in our benchmark, we applied goal-driven clustering algorithm (PAS Propose-Assign-Select) framework introduced by Wang et al. [42] that provides interpretable, language-based explanations for each cluster. We defined our clustering goal as: want to cluster these game descriptions by game type, reflecting on their core themes and the primary strategy of the game. We ran the algorithm on set of 126 game descriptions generated by our LLM pipeline. We used powerful model (o1) to propose candidate cluster explanations and smaller model (o3-mini) to assign texts to those explanations. The result of the assignment step is binary matrix {0, 1}N , where = 126 is the number of descriptions and is the number of candidate explanations. Entry Ai,j = 1 if description was judged to belong to cluster j, and 0 otherwise. These assignments are then fed into an integer linear program (ILP) to select compact set of clusters that covers each description at most once. Concretely: We introduce binary variables sj for each candidate cluster j, where sj = 1 if cluster is selected. We introduce integer variables mi for each description i, enforcing mi = (cid:88) j=1 Ai,j sj, 0 mi 1, to ensure each description is covered at most once (forcing mi = 1 if coverage is required). j=1 sj = K. Otherwise, we allow the If fixed number of clusters is desired, we add (cid:80)M solver to choose K. The objective minimizes the total number of uncovered descriptions. min (cid:88) i=1 (1 mi) + α (cid:88) j=1 sj (α = 0.5 by default), We solve this ILP using PuLPs CBC solver. The chosen clusters with sj = 1 each form one final cluster, and descriptions with Ai,j = 1 are assigned accordingly. The result are coherent groupingse.g. number-based puzzles, grid-movement games, and combinatorial strategy gameswhile ensuring every description is placed exactly once. D.1 Prompting Details Our implementation is carried out entirely via three successively used prompts. Propose. We first split the 126 descriptions into chunks. For every chunk, we query o1 the descriptions in-context as follows: Below are few examples of game descriptions: {game_descriptions} Goal: want to cluster these game descriptions by game type, reflecting on their core themes and the primary strategy of the game. Please brainstorm list of {num_candidates} candidate explanations for clustering these texts. envision the following examples as valid themes: Card Game, Board Game, Word Game, Abstract Strategy Game. Return the list as only numbered items. 20 (a) Prefiltered distribution (b) Post-filtered distribution 33.2% 20.3% 31.1% 36.7% 27.6% 9.4% 11.7% 14.6% Number Board Combat Chance Card Puzzles Figure 6: Genre-cluster distributions of o1-generated games (a) before and (b) after filtering. Puzzles is shorthand for pattern puzzles. The model returns simple numbered list and parsing those lines gives an initial pool of candidate clusters. Handling Duplicates. The raw pool is concatenated and fed back to o1 with meta-prompt Here is list of proposed cluster explanations: {joined_explanations} Please remove any duplicates or near-duplicates, and remove any explanation that is essentially subset or redundant given another. Then return the final list of unique, distinct cluster explanations as numbered list. Do not add extra commentary. This produces the final set of candidate explanations {e1, . . . , eM }. Assign. For every pair of (description di, explanation ej we query the assigner model (o3-mini) with Cluster Explanation: {Example: Card Game: The game primarily involves drawing, playing, or managing cards...} Text: {Example: Game Title: Target Twenty-Three. Objective: Be the player who reaches exactly 23...} Question: Does the text belong to the cluster described above? Answer with only either the 'Yes' or 'No' string and nothing else. An answer of Yes sets Ai,j = 1; No sets Ai,j = 0. The resulting binary matrix is exactly the input to the ILP described above. The pipeline helps keeps clusters concise, enforce disjoint cluster membership during the assignment phase, and preserves interpretability guarantees. We find that using reasoning models to do the task yields the highest quality explanation-based clusters. D.2 Comparing distribution of games in gg-bench pre-filtering and post-filtering Clustering analysis As shown in Figure 6, we outline the game genre distributions for both the 1000 generated games, and the 126 that survive filtering. We notice three key changes when comparing the pre-filtering and post-filtering distributions: Increase in card and number games: Before filtering, Combat was the second-largest category at 31.3%, trailing only Board (33.2%). After filtering, Number games surge from 20.3% to 36.7%, overtaking Board and \"Combat\" as the largest category. Also noteworthy is the preference for card-based game mechanics, increasing from 3.5% to 13.3% after filtering. 21 Disappearance and shrinkage of niche clusters: Make-Sequence\" or \"Pattern Puzzle\" gameswhere players must form exact patterns, such as in Color Bridge (which challenges two opponents to color exactly three adjacent nodes), or by arranging digits, symbols, and the likeare all but eliminated after filtering. Relative stability of chance-based game mechanics: After filtering, the Chance cluster climbs from 6.9% to 11.7%, about one in ten games, indicating that random-element mechanics remain appealing when backed by concrete descriptions and clear win conditions."
        },
        {
            "title": "E Scalability Details",
            "content": "In Table 6, we provide summaries of the 10 GPT-4o games that survived filtering. We observe that 8 out of 10 games here are variants of or identical to Tic-Tac-Toe, where as the other two, Numeral Clash and Sequence Duel are both \"running sum\" games. Game Quantum Duel Dominion Duel Quantum Collapse Cosmic Match Glyph Quest Core mechanics / objective Players alternately place X/O on 3 3 grid; first to form three in row wins, otherwise the filled board resets the round. Classic tic-tac-toe race on 3 3 grid with no-draw rulefirst three-in-a-row claims instant victory. Players drop X/O energy fields on 33 matrix; aligning three triggers collapse and wins the game. Turn-based placement of X/O; first horizontal, vertical, or diagonal triple wins; no draws. Place glyphs plus one-time Block, Swap, or Clear power; first to make three-in-a-row (or V) wins. Quantum Clash Contest nodes on 3 3 circuit using coin-flip challenges and energy tokens; win by line of three activated nodes or total grid control within five rounds. Sequence Duel Elemental Duel Quantum Flip Numeral Clash Players add 13 to shared running total; exact hit of target sum wins, overshoot loses. Place/move tokens to claim Water (row), Fire (column), Earth (diagonal); first to hold all three patterns simultaneously wins. Standard 3 3 alignment plus one-use flip that converts an opponents mark; forced resolution after five rounds; align three to win. Draw numbers 1 5; keep or assign to opponent; first to hit exactly 21 wins, overshooting loses. Table 6: Summaries of the 10 GPT-4o games that survived filtering. Each row summarizes the core mechanics and objectives of distinct game."
        }
    ],
    "affiliations": [
        "Computer Science Division, University of California, Berkeley"
    ]
}