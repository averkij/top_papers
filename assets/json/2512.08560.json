{
    "paper_title": "BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain",
    "authors": [
        "Navve Wasserman",
        "Matias Cosarinsky",
        "Yuval Golbari",
        "Aude Oliva",
        "Antonio Torralba",
        "Tamar Rott Shaham",
        "Michal Irani"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Understanding how the human brain represents visual concepts, and in which brain regions these representations are encoded, remains a long-standing challenge. Decades of work have advanced our understanding of visual representations, yet brain signals remain large and complex, and the space of possible visual concepts is vast. As a result, most studies remain small-scale, rely on manual inspection, focus on specific regions and properties, and rarely include systematic validation. We present a large-scale, automated framework for discovering and explaining visual representations across the human cortex. Our method comprises two main stages. First, we discover candidate interpretable patterns in fMRI activity through unsupervised, data-driven decomposition methods. Next, we explain each pattern by identifying the set of natural images that most strongly elicit it and generating a natural-language description of their shared visual meaning. To scale this process, we introduce an automated pipeline that tests multiple candidate explanations, assigns quantitative reliability scores, and selects the most consistent description for each voxel pattern. Our framework reveals thousands of interpretable patterns spanning many distinct visual concepts, including fine-grained representations previously unreported."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 0 6 5 8 0 . 2 1 5 2 : r BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain Navve Wasserman1, Matias Cosarinsky1, Yuval Golbari1 Aude Oliva2 Antonio Torralba2 Tamar Rott Shaham2 Michal Irani1 1Weizmann Institute of Science 2Massachusetts Institute of Technology Equal contribution"
        },
        {
            "title": "Abstract",
            "content": "Understanding how the human brain represents visual concepts, and in which brain regions these representations are encoded, remains long-standing challenge. Decades of work have advanced our understanding of visual representations, yet brain signals remain large and complex, and the space of possible visual concepts is vast. As result, most studies remain small-scale, rely on manual inspection, focus on specific regions and properties, and rarely include systematic validation. We present large-scale, automated framework for discovering and explaining visual representations across the human cortex. Our method comprises two main stages. First, we discover candidate interpretable patterns in fMRI activity through unsupervised, data-driven decomposition methods. Next, we explain each pattern by identifying the set of natural images that most strongly elicit it and generating natural-language description of their shared visual meaning. To scale this process, we introduce an automated pipeline that tests multiple candidate explanations, assigns quantitative reliability scores, and selects the most consistent description for each voxel pattern. Our framework reveals thousands of interpretable patterns spanning many distinct visual concepts, including fine-grained representations previously unreported. For demo, models and labeled data, see our project page. 1. Introduction Understanding how the human brain represents visual information is long-standing challenge. The visual cortex encodes rich hierarchy of features that support object recognition, scene understanding, and visual reasoning. Yet, the structure of these representations and their underlying organization in the brain remain largely unknown. Functional Magnetic Resonance Imaging (fMRI) has become dominant tool for studying how the human brain processes visual information, providing non-invasive window into cortical activity while participants view natural images [2, 26]. It measures brain activity across the brain, parceled into tiny volume elements (voxels). Over the past decades, fMRI researchers have sought to interpret these complex voxel-level patterns to uncover the structure of visual representations in the brain. Early work targeted retinotopic maps and low-level features [15, 18, 31, 47, 50], while later studies examined higher-level semantics using category-selective analyses [1, 16, 30, 32, 55], data-driven fMRI decompositions [17, 28, 34, 52, 58] and imagebrain models [11, 24, 29, 38]. However, fMRI signals are high-dimensional, comprising tens of thousands of brain voxels per subject that reflect vast range of possible visual concepts, while available image-to-fMRI datasets remain relatively small. As result, current studies often focus on specific properties (e.g., faces, places, food), or on particular brain regions, such as face-selective (FFA) or place-selective (PPA). Moreover, analyses typically rely on manual inspection, making it difficult to scale interpretations across many fMRI patterns and visual concepts. These limitations highlight the need for scalable, automated tools capable of discovering and explaining visual representations across the entire cortex. Recent advances in interpretability of artificial neural networks, such as Language and Vision models, have shown that high-level concepts can be automatically discovered from internal activations [3, 20, 25]. These results suggest that similar data-driven approaches might offer new insight into the representations that emerge in the brain. Inspired by this, we introduce BrainExplore, an unsupervised, datadriven framework for large-scale discovery and evaluation of visual concept representations across the visual cortex. BrainExplore automatically discovers interpretable patterns of brain activity and explains them in natural language. Our pipeline comprises two main steps: (i) deriving candidate interpretable fMRI patterns via perbrain-region decomposition (Fig. 1a); and (ii) discovering interpretable patterns and explaining them (Fig. 1b). The first step performs per brain region decomposition of fMRI activity to extract components that may correspond to meaningful visual representations. Conceptually, each decomposition method 1 Figure 1. BrainExplore: Discovering Interpretable Visual Representations in the Human Brain. (a) Perregion fMRI decomposition learns component patterns such that any response is approximated as linear combination of these patterns. (b) For each pattern, we retrieve its top-N activating fMRI responses and corresponding images, then automatically select the best matching concept and assign an alignment score. We then use these scores to identify the most interpretable patterns and their explanations. (c) Examples of discovered interpretable patterns across regions, showing pattern activations projected onto cortex, top activating images, and their textual explanations. learns set of fMRI patterns (components) such that any fMRI response in given region of interest (ROI) can be approximated as linear combination of these patterns. For each component, we retrieve the top-N fMRI responses with the highest coefficient for that component and collect their corresponding images. This yields set of images per component that visualize the visual concept that most strongly activates it. We then discover specific, interpretable patterns by automatically identifying, for each component, the concept that best aligns with it and assigning an alignment score. This allows us to surface the most interpretable components together with their explanations. Using BrainExplore, we discover thousands of interpretable patterns throughout the brain. Since the method automatically evaluates the quality of each explanation, it enables integration of findings across different decomposition methods rather than relying on single one. We exploit established techniques (PCA [27], NMF [36], ICA [10]) as well as Sparse Autoencoders [13] (SAEs), method widely used to interpret artificial neural networks via projection to large dimension and sparsity constraints. SAE yields large number of interpretable patterns and reveals visual representations that are complementary to other methods, including representations not captured elsewhere. Moreover, to overcome limited data, we leverage an image-tofMRI prediction model [5] that synthesizes brain responses for unseen images. This allows us to expand the dataset from approximately 10k images with measured fMRI to over 120k images in total with measured or predicted responses. This augmentation improves decomposition quality and increases the diversity of discovered representations. Altogether, BrainExplore reveals large set of nuanced representations across the brain. These include interpretable patterns selective for object identity; for people, body parts, and pose (e.g., open mouths, hands holding objects, bent knees, identical objects); as well as distinct indoor and outdoor scenes (e.g., nature, streets, oceans, rooms, toilets; see Fig. 1). All findings are quantitatively evaluated on measured fMRI not used during interpretation. Our contributions are as follows: We propose BrainExplore, large-scale, automated that discovers thousands of interpretable framework, fMRI patterns including previously unreported ones. We enrich fMRI decomposition with predicted fMRI signal, which improves interpretability by large margin. We propose an SAE-based fMRI decomposition that finds interpretable brain patterns beyond standard methods. We will release the brain-inspired concept dictionary, the large-scale dataset of imagefMRI-explanations ranking, and the code, providing benchmark for future work. 2 2. Related Works Contrasting predefined stimulus categories. large body of work has studied category-selective activations by contrasting responses to hand-picked stimulus classes. For example, researchers have mapped the extrastriate body area (EBA) and related regions using body-selective contrasts [16, 55], and characterized scene-selective regions such as the parahippocampal place area (PPA) [19, 44]. Other studies have identified face-selective regions including the fusiform face area (FFA) [32], examined overlapping but separable responses to faces and food in fusiform cortex [1, 30], or investigated object-related properties [35]. These approaches have been highly informative, but they are limited by the need to predefine small set of categories, to use relatively clean images dominated by single concept, and to test each hypothesis separately. Moreover, the same voxels and regions can participate in many concepts, making it difficult to discover nuanced or overlapping representations using only category contrasts. Decomposing fMRI signals. Unsupervised decomposition methods provide an alternative route to fMRI interpretability. Commonly used techniques include Principal Component Analysis (PCA) [27], Independent Component Analysis (ICA) [10], and Non-negative Matrix Factorization (NMF) [36]. These have been widely applied to fMRI research such as resting-state analysis and connectivity [4, 8, 40, 49, 53, 59]. Stimulus-driven decompositions for interpretability have also been explored, in domains such as speech and audio [14, 42, 57] and visual stimuli [7, 17, 28]. For visual concept discovery, early work used PCA to capture large-scale semantic maps [7, 17, 28], while later studies leveraged NMF to obtain more interpretable components for categories such as bodies and food [34, 52, 58, 58]. ICA has seen more limited use, mainly for demonstrating retinotopic organization and low-level visual mapping [51]. Most of these studies focus on specific regions (e.g., early visual cortex, EBA, PPA) or on small set of concepts (e.g., food, social vs. non-social images), and typically examine single decomposition method and only the first few components via manual inspection. We systematically compare multiple decomposition methods for brain interpretability, and scale analysis to tens of thousands of components across many regions and methods. We also leverage image-to-fMRI models to greatly expand the effective training set and image pool used for decomposition, and we introduce SAE [13] for fMRI decomposition. ImagefMRI models. Recent years have brought substantial progress in models that link images and brain activity. This includes encoding models that predict fMRI transformations beresponses from images [5, 33, 41], tween subjects or brains [54, 56], and image reconstruction from fMRI [6, 46]. Beyond pure prediction, several works have used such models to explore cortical representations. Early studies related voxel activations to predefined semantic concepts, for example in faceand place-selective regions [11, 12]. More recent work has learned functional voxel clusters and visualized the kinds of images each cluster corresponds to [5]. Another line of work uses image-tofMRI encoders together with generative image models to synthesize images that strongly activate particular regions or voxels [24, 29, 38, 39]. However, these approaches still operate at the level of individual voxels or entire regions: regions are too coarse to capture fine-grained mixtures of concepts, while voxels are too local and can participate in many concepts, making nuanced representations difficult to disentangle. Our approach is complementary. We use image-tofMRI models primarily as tool for data augmentation and interpretability at the pattern level: predicted responses for large image sets greatly expand the pool of images used to learn decompositions and to retrieve maximally activating examples for each component, enabling more robust and fine-grained hypothesis testing. Automated interpretability. Interpretability of neural networks advanced rapidly in recent years [3, 20, 21, 25]. Recognizing that manual inspection does not scale to modern models, several works have proposed automated interpretability pipelines [22, 43, 45, 48]. The human brain it is large, comshares many of the same challenges: plex system with many units (voxels) and vast repertoire of concepts. However, as far as we know, no large-scale, automatic interpretability pipeline has been proposed for the brain. Our work bridges these directions. We combine unsupervised fMRI decomposition with an automatic interpretability pipeline. Together with data augmentation from image-to-fMRI models and the use of SAEs, this yields scalable framework that discovers many interpretable brain patterns and provides benchmark for evaluating and improving future decomposition methods. 3. Methods We first describe the experimental setting used in our framework (Sec. 3.1). We then present BrainExplore, our brain interpretability framework with four parts (see Fig. 2): (i) Decompose: per-region fMRI decomposition into pattern components (Sec. 3.2); (ii) Visualize & Explain: retrieve the top activating images for each pattern and provide an interpretation of its semantics (Sec. 3.3); (iii) Upscale: scale to an unlimited number of brain patterns without interpreting each stimulus separately (Sec. 3.4); (iv) Discover: score each patternhypothesis alignment for systematic discovery of interpretable patterns (Sec. 3.5). 3.1. Experimental Setting We use the Natural Scenes Dataset (NSD) [2], large publicly available 7-Tesla fMRI dataset that records responses from 8 subjects viewing diverse natural images drawn from 3 Figure 2. BrainExplore Framework. (a) Decompose: per-region fMRI decomposition to discover interpretable patterns (Sec. 3.2). (b) Visualize & Explain: retrieve the top activating images for each pattern and interpret its semantics (Sec. 3.3). (c) Upscale: scale to an unlimited number of patterns by building brain-inspired dictionary and labeling each image with respect to each hypothesis (Sec. 3.4). (d) Discover: score each patternhypothesis alignment, enabling systematic discovery of the most interpretable patterns and the pattern that best explains any given hypothesis (Sec. 3.5). COCO [37]. The dataset contains 73k imagefMRI pairs in total, with around 10k images per subject (some images are shared across subjects). We adopt the post-processed version of NSD released by Gifford et al. [23], which includes 40k voxels per subject together with predefined division into mainly vision-related ROIs. These ROIs are used for our per-ROI decomposition and subsequent analyses. To enrich the training data, we additionally use predicted fMRI responses for images not viewed by any subject. Specifically, we take 120K additional natural images from the unlabeled portion of COCO and employ the image-to-fMRI encoder of Beliy et al. [5] to predict fMRI responses for each subject. This augmentation produces substantially larger set of imagefMRI pairs, which we use both for training decompositions and for retrieving maximally activating images. Importantly, all interpretations are evaluated and verified on measured fMRI. 3.2. fMRI Decomposing The first step in our pipeline performs per-ROI decomposition of fMRI activity to extract component patterns that may correspond to meaningful visual representations. Conceptually, each decomposition method learns set of fMRI patterns (components) such that any fMRI response in given ROI can be approximated as linear combination of these components (Fig. 2a). Each trained decomposition yields two outputs: (i) component matrix, where each column is learned fMRI pattern (for PCA, NMF, and ICA this is the standard loading matrix; for SAEs it corresponds to the decoder weights); and (ii) coefficient matrix, which contains the linear coefficients reconstructing each fMRI response from these patterns. Importantly, all decompositions are learned purely from fMRI responses; no image features or labels are used, ensuring that all inferred visual representations arise from brain activity rather than externally imposing image semantics. We train three standard decomposition methods (PCA, NMF, and ICA) as well as SAEs. For each method, we train two variants: (i) using only measured fMRI responses (10k responses per subject), and (ii) using the combined dataset of measured fMRI and 120k predicted responses. For each method, we adopt default parameter settings and vary few hyperparameters (e.g., variance thresholds for 4 Figure 3. Discovered Interpretable Patterns (EBA and V4). We show patterns of subject 1 with their top activating images and selected explanations. EBA is known to process bodies and actions; V4 is known to encode mid-level features (e.g., color, shape). PCA/NMF/ICA and different seeds; sparsity settings and expansion factors for SAE). Each decomposition produces large pool of candidate patterns per ROI, which we later use in the Discover step. As baseline, we also treat individual voxel activations as single-component decomposition, effectively serving as one-hot voxel basis. Training details are provided in Supp. B. 3.3. From fMRI Patterns to Visual Concepts To interpret each decomposed brain pattern, we first connect it to the set of stimuli that drive it (Fig. 2b). Each fMRI response is directly associated with stimulus the natural image viewed by the subject. After decomposition, every fMRI response is represented by vector of coefficients, one per pattern component, indicating how strongly each pattern contributes to reconstructing that response. We assume that the responses with the highest coefficients for given pattern are the most informative about its underlying visual semantics. Thus, for every pattern, we select its top-N activating responses and collect their corresponding images, producing set of images that visualize what most strongly activates that pattern. We perform this separately for measured-fMRI responses and predicted responses. To explain each pattern, we take the six most activating images from the measured-fMRI pool and the ten most activating images from the predicted-fMRI pool (16 images total). We generate captions for these images using VisionLanguage Model (VLM) and then prompt an LLM to synthesize 510 hypotheses capturing what is shared across them (e.g., object identity, pose, texture, scene category). These hypotheses serve as candidate semantic explanations for the pattern. 3.4. Scaling to an Unlimited Number of Patterns Explaining every component in every ROI and decomposition method is costly and inefficient, especially with high-dimensional SAEs, multiple hyperparameters, and many methods. To scale interpretability, we introduce two steps (Fig. 2c): (i) Hypothesis dictionary generation, which constructs brain-activity-inspired set of candidate hypotheses; and (ii) Hypothesisimage labeling, which assigns binary labels for each hypothesis and image. Together, these two steps create large-scale associations between visual stimuli and candidate descriptions of brain activity, independent of any specific pattern. Once the dictionary and image labels are in place, any new brain pattern can be evaluated by measuring how consistently its top-activating images express concepts from the dictionary. Hypothesis dictionary generation. We first identify patterns that are most likely to be interpretable. To do so, we compute CLIP-based consistency score for every pattern: we embed its top activating images, compute pairwise simFigure 4. Discovered Interpretable Patterns (OPA and PPA). We show Subject 1 patterns with top activating images and selected explanations. OPA is known to process scene layout and navigability; PPA encodes scenes and places (e.g., indoor/outdoor, landmarks). ilarities, and use the mean similarity as proxy to semantic coherence. We then select the top 40 patterns per ROI and per decomposition method and run the Visualize & Explain step on this subset (over 10k patterns), yielding more than 1k unique candidate explanations. These are aggregated into brain-inspired dictionary of visual concepts. To remove duplicates and near-duplicates, we embed each hypothesis with BGE text encoder [9] and merge concepts with high embedding similarity. This produces dictionary of 1,300 concepts. With this dictionary, we can label all stimuli offline with respect to every candidate concept, creating large-scale associations between images and candidate brain activity descriptions, independent of any specific pattern. Further details and prompts appear in Supp. B. Hypothesisimage labeling. Our goal is to obtain, for every image in the dataset (both for the images with measured and predicted fMRI), binary vector indicating which concepts from the dictionary apply to that image. Directly querying VLM with all 1,300 hypotheses per image is infeasible and often leads to inaccurate results, therefore we use two-stage procedure. First, we use CLIP to shortlist small set of candidate hypotheses for each image: We embed each hypothesis under multiple prompt templates, compute its similarity to the CLIP image embedding, and retain the top 300 hypotheses per image. Second, we perform VLM-based verification. For each shortlisted candidate hypothesis, we prompt VLM to output binary decision (0/1) indicating whether the hypothesis holds for given image. To improve reliability, we run second verification pass with different prompt for all hypotheses initially marked as positive, and keep only those confirmed twice. This produces sparse but high-quality binary imagehypothesis label matrix, used later for large-scale patternhypothesis scoring. Additional details and labeling examples are provided in Supp. B. 3.5. Discover Interpretable Brain Patterns The goal of the Discover step (Fig. 2d) is to determine which visual concept each fMRI pattern represents. It uses the outputs of Upscale (the hypothesis dictionary and imagehypothesis labels) to evaluate any decomposition component pattern by checking whether the images that activate it express consistent visual concepts. Hypothesispattern scoring. For each pattern, we quantify how strongly it aligns with each candidate hypothesis. Every fMRI response has coefficient indicating how much given pattern contributes to reconstructing that response. We rank all images by this coefficient and select the top 0.2% most activating images for that pattern. This is done separately for the measured-fMRI pool and the predicted-fMRI pool, and excluding responses that do not meaningfully activate the pattern (SAE coefficients < 0.01; non-SAE coefficients 0). Since each image already 6 Figure 5. Concepts best explained by each ROI (EBA, PPA). Each concept is assigned to single ROIthe one with the highest alignment score. Only concepts with alignment > 0.5 are shown; word size reflects the alignment score within the assigned ROI. has binary labels for all hypotheses, we can directly measure how well hypothesis matches the pattern. For given pattern and hypothesis h, we compute its score as #{top-activating images labeled with h} , where is the total numN ber of selected top activating images for the pattern. This measures how consistently concept appears among the images that drive the pattern. Some hypotheses are globally rare, so even small number of positives may indicate strong alignment. To avoid penalizing such cases, we apply normalization factor based on the global frequency of the hypothesis (the proportion of images labeled with 1 for that hypothesis across the full dataset), capping the factor at 2. The resulting normalized score reflects how unexpectedly frequent the hypothesis is within the patterns top images. Scores are computed independently for the measured and predicted fMRI pools, and the final hypothesis-pattern score is defined as their average. Pattern search. Given the patternhypothesis scores, we support two complementary searches: (i) pattern search: find the most interpretable pattern(s); and (ii) hypothesis search: for given hypothesis (e.g., open mouth), find the best-explained pattern. Both searches can be run per ROI (best pattern within an ROI) or across ROIs (best pattern over all ROIs), enabling region-specific (if/where hypothesis is represented) and method-level (which decompositions capture it) analyses. 4. Experiments 4.1. Evaluation Protocol To ensure fair evaluation, we split each pool into disjoint ranking and evaluation halves: the ranking set is used to assign best pattern-hypothesis pairs; the evaluation set is used to report final metrics at Sec. 4. In the main paper, we present results for Subject 1 from the NSD dataset. Quantitative and qualitative results for the other subjects are provided in Supp. & D, demonstrating the effectiveness of our pipeline across subjects. Quantitative evaluation scores. We report two metrics: (i) Percentage of Interpretable Hypotheses: the fraction of hypotheses that achieve an alignment score above specified threshold (0.5 or 0.8) with at least one pattern. This can be computed either per ROI or across all ROIs. (ii) Number of Interpretable Patterns: the number of patterns whose best hypothesis alignment exceeds the same threshold (0.5 or 0.8). To avoid counting near-duplicate patterns, we remove any component whose voxel-wise correlation with higher-scoring pattern exceeds 0.5. Integration across decompositions. Since every pattern receives score, searches can be conducted not only within single decomposition, but also across methods and hyperparameters, allowing integrated comparison and selection. 4.2. Visual Representations in the Human Brain BrainExplore reveals fine-grained patterns across the brain. BrainExplore identifies interpretable patterns effectively. For any hypothesis of interest, we can retrieve the best aligned pattern within specific ROI or across ROIs, and visualize its representation via the top activating images. Figs. 1 and 3 show examples from the top 16 images per pattern (8 from measured fMRI, 8 from predicted fMRI; full grids appear in Supp. D). Although the predicted fMRI pool is much larger and often yields clearer visualizations, we validate each explanation on measured fMRI as well. Overall, BrainExplore uncovers many fine-grained patterns, including ones not previously identified or localized. For example, in EBA, which is classically linked to body perception and action, we find patterns selective for specific sports (surfing, soccer, tennis, frisbee), actions (jumping, tooth brushing), and body oriented concepts (hands, neckwear). In PPA, which is known to respond to scenes, we observe more nuanced division than prior indoor versus outdoor contrasts, including distinct outdoor concepts for landscapes, commercial buildings, and stone architecture. Additional concepts and ROIs are shown in Supp. D. 7 Table 1. Percentage of Interpretable Hypotheses. The fraction of hypotheses that achieve an alignment score above threshold (0.5 or 0.8) with at least one pattern. Measured fMRI + Predicted fMRI Method > 0.5 > 0.8 > 0. > 0.8 Voxels PCA (Single) NMF (Single) ICA (Single) ICA (Multiple) SAE (Single) SAE (Multiple) SAE+ICA 3.2% 3.4% 1.3% 0.8% 0.8% 5.6% 6.0% 6.1% 0.3% 1.0% 0.3% 0.2% 0.2% 1.1% 1.2% 1.2% 6.7% 7.4% 5.5% 18.1% 18.3% 15.7% 17.4% 21.5% 1.4% 1.0% 0.8% 4.2% 5.4% 3.3% 3.8% 5.9% Different ROIs represent different semantics. Different ROIs are known to support distinct functions. Many of our discovered patterns align strongly with these known roles, which both validates our approach and lends credibility to novel findings. Localizing established functions remains valuableespecially when we reveal finer semantics. To examine this systematically, we first find, for each hypothesis, the best matching pattern across ROIs; then, for each ROI, we list the hypotheses it best explains. In Fig. 5, we show examples where word size reflects the hypothesispattern score in that ROI. As shown, our method recovers many concepts, with different ROIs best explaining different ones, as expected. Interpretable patterns are more localized with SAE. The brain exhibits meaningful spatial organization at both regional and subregional scales. While low-level maps are well studied, higher-level semantic organization remains less clear. We find that SAEs yield interpretable patterns that are notably more spatially localized, despite receiving only 1D voxel vectors and no spatial information or spatial constraint. In Fig. 3, we show patterns from both ICA and SAE. In EBA, for example, soccer, frisbee, and jumping are derived using ICA, whereas the remaining patterns come from SAE. Additional side-by-side comparisons of pattern localization appear in Supp. D. SAE patterns are visibly more compact and clustered rather than scattered. Greater localization is both biologically plausible and of strong interest to the neuroscience community. 4.3. Quantitative Evaluation Table T2 reports the percentage of interpretable hypotheses, the fraction of hypotheses that achieve an alignment score above threshold, grouped by decomposition method. Results for Number of Interpretable Patterns appear in Supp. C. This table shows that SAE yields far more interpretable patterns than other methods: approximately 9k patterns across the brain at 0.5 threshold, compared to 6k for the Voxels baseline and 226 for ICA. Predicted fMRI substantially improves interpretability. We evaluate different decomposition methods, comparing models trained only on measured fMRI with models augmented using large pool of predicted fMRI (used both for learning the decomposition and for retrieving top-activating images). Across all methods, augmenting with predicted fMRI yields large gains. Most notably, ICA improves from 0.8% to 18.3% at the 0.5 threshold. To our knowledge, prior stimulus-driven studies have not demonstrated strong interpretability for ICA, likely because learning stable independent components requires substantially more data. SAE also improves markedly, from 6.0% to 17.4%. Combining decompositions yields the best performance. We first report single-method results (Single); ICA and SAE explain the largest fraction of hypotheses. Leveraging our scoring pipeline, we then integrate multiple runs of the same method with different seeds/hyperparameters (Multiple), which further improves both ICA and SAE. Finally, integrating across methods (ICA+SAE) yields the strongest performance. This is enabled by our automatic scoring, which evaluates any number of patternhypothesis pairs and supports seamless aggregation across decompositions. 4.4. Ablations & Analysis In Supp. we analyze several aspects of our pipeline, including: (i) SAE ablations - the importance of sparsity and of expanding dimensionality; (ii) Predicted fMRI pool size - results with varying imagefMRI pool sizes for retrieving top-activating images and their effect on interpretability; (iii) Per-ROI quantitative results - detailed metrics reported per ROI; and (iv) Complementarity across decompositions - the gains from combining different methods and an analysis of concepts uniquely captured by each. 5. Conclusion We introduced BrainExplore, scalable framework for discovering visual concept representations across the visual cortex. By integrating multiple decomposition methods and enriching both training and retrieval with large pool of predicted fMRI signals, BrainExplore reveals thousands of interpretable patterns that capture fine-grained visual concepts across regions. Both visual inspection and quantitative scores show that our pipeline consistently surfaces highly interpretable patterns. VLM-based labels, though extensive, can be noisy, and the hypothesis dictionary may miss some concepts; both can be expanded, refined, or replaced as better models emerge. More broadly, we believe that current decomposition methods remain far from optimal. BrainExplore provides practical and systematic way to compare, evaluate, and improve these methods (e.g., via SAEs). We hope this framework supports deeper, largescale studies and accelerates discovery well beyond the results presented in this study."
        },
        {
            "title": "References",
            "content": "[1] Kayleigh Adamson and Vanessa Troiani. Distinct and overlapping fusiform activation to faces and food. NeuroImage, 174:393406, 2018. 1, 3 [2] Emily Allen, Ghislain St-Yves, Yihan Wu, Jesse Breedlove, Jacob Prince, Logan Dowdle, Matthias Nau, Brad Caron, Franco Pestilli, Ian Charest, et al. massive 7t fmri dataset to bridge cognitive neuroscience and artificial intelligence. Nature neuroscience, 25(1):116126, 2022. 1, 3 [3] David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection: Quantifying interpretability of deep visual representations. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 65416549, 2017. 1, 3 [4] Christian F. Beckmann and Stephen M. Smith. Probabilistic independent component analysis for functional magnetic resonance imaging. IEEE Trans. Med. Imaging, 23(2):137 152, 2004. 3 [5] Roman Beliy, Navve Wasserman, Amit Zalcher, and Michal Irani. The wisdom of crowd of brains: universal brain encoder. arXiv preprint arXiv:2406.12179, 2024. 2, 3, 4 [6] Roman Beliy, Amit Zalcher, Jonathan Kogman, Navve Wasserman, and Michal Irani. Brain-it: Image reconstrucarXiv tion from fmri via brain-interaction transformer. preprint arXiv:2510.25976, 2025. 3 [7] Gijs Joost Brouwer and David Heeger. Decoding and reconstructing color from responses in human visual cortex. Journal of Neuroscience, 29(44):1399214003, 2009. 3 [8] V. D. Calhoun, Tulay Adali, G. D. Pearlson, and J. J. Pekar. method for making group inferences from functional MRI data using independent component analysis. Hum. Brain Mapp., 14(3):140151, 2001. [9] Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge m3-embedding: Multilingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. arXiv preprint arXiv:2402.03216, 2024. 6 [10] Pierre Comon. Independent component analysis, new concept? Signal processing, 36(3):287314, 1994. 2, 3 [11] Tolga ukur, Alexander Huth, Shinji Nishimoto, and Jack Gallant. Functional subdomains within human ffa. Journal of Neuroscience, 33(42):1674816766, 2013. 1, 3 [12] Tolga ukur, Alexander Huth, Shinji Nishimoto, and Functional subdomains within sceneJack Gallant. selective cortex: parahippocampal place area, retrosplenial complex, and occipital place area. Journal of Neuroscience, 36(40):1025710273, 2016. 3 [13] Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse autoencoders find highly interpretable features in language models, 2023. 2, 3 [14] Fatma Deniz, Anwar Nunez-Elizalde, Alexander Huth, and Jack Gallant. The representation of semantic information across human cerebral cortex during listening versus reading is invariant to stimulus modality. Journal of Neuroscience, 39(39):77227736, 2019. [15] E. A. DeYoe, G. J. Carman, P. Bandettini, S. Glickman, J. Wieser, R. Cox, D. Miller, and J. Neitz. Mapping striate and extrastriate visual areas in human cerebral cortex. Proceedings of the National Academy of Sciences, 93(6):23822386, 1996. 1 [16] Paul Downing, Yuhong Jiang, Miles Shuman, and Nancy Kanwisher. cortical area selective for visual processing of the human body. Science, 293(5539):24702473, 2001. 1, 3 [17] Christine Ecker, Emanuelle Reynaud, Steven Williams, and Michael Brammer. Detecting functional nodes in largescale cortical networks with functional magnetic resonance imaging: principal component analysis of the human visual system. Human brain mapping, 28(9):817834, 2007. 1, 3 [18] Stephen A. Engel, Gary H. Glover, and Brian A. Wandell. Retinotopic organization in human visual cortex and the spatial precision of functional mri. Cerebral Cortex, 7(2):181 192, 1997. 1 [19] Russell Epstein and Chris Baker. Scene perception in the human brain. Annual review of vision science, 5(1):373397, 2019. 3 [20] Yossi Gandelsman, Alexei Efros, and Jacob Steinhardt. Interpreting clips image representation via text-based decomposition. arXiv preprint arXiv:2310.05916, 2023. 1, 3 [21] Yossi Gandelsman, Alexei Efros, and Jacob Steinhardt. Interpreting the second-order effects of neurons in clip. arXiv preprint arXiv:2406.04341, 2024. 3 [22] Amirata Ghorbani, James Wexler, James Zou, and Been Kim. Towards automatic concept-based explanations. Advances in neural information processing systems, 32, 2019. [23] Alessandro Gifford, Benjamin Lahner, Sari Saba-Sadiya, Martina Vilas, Alex Lascelles, Aude Oliva, Kendrick Kay, Gemma Roig, and Radoslaw Cichy. The algonauts project 2023 challenge: How the human brain makes sense of natural scenes. arXiv preprint arXiv:2301.03198, 2023. 4 [24] Zijin Gu, Keith Wakefield Jamison, Meenakshi Khosla, Emily J. Allen, Yihan Wu, Ghislain St-Yves, Thomas Naselaris, Kendrick Kay, Mert R. Sabuncu, and Amy Kuceyeski. Neurogen: Activation optimized image synthesis for discovery neuroscience. NeuroImage, 247:118812, 2022. 1, 3 [25] Evan Hernandez, Sarah Schwettmann, David Bau, Teona Bagashvili, Antonio Torralba, and Jacob Andreas. Natural In Internalanguage descriptions of deep visual features. tional Conference on Learning Representations, 2021. 1, 3 [26] Tomoyasu Horikawa and Yukiyasu Kamitani. Generic decoding of seen and imagined objects using hierarchical visual features. Nature Communications, 8:15037, 2017. 1 [27] Harold Hotelling. Analysis of complex of statistical variables into principal components. Journal of educational psychology, 24(6):417, 1933. 2, 3 [28] Alexander G. Huth, Shinji Nishimoto, An T. Vu, and Jack L. Gallant. continuous semantic space describes the representation of thousands of object and action categories across the human brain. Neuron, 76(6):12101224, 2012. 1, 3 [29] Ethan Hwang, Hossein Adeli, Wenxuan Guo, Andrew Luo, and Nikolaus Kriegeskorte. In silico mapping of visual cat9 egorical selectivity across the whole brain. arXiv preprint arXiv:2510.21142, 2025. 1, 3 [30] Nidhi Jain, Aria Wang, Margaret M. Henderson, Ruogu Lin, Jacob S. Prince, Michael J. Tarr, and Leila Wehbe. Selectivity for food in human ventral visual cortex. Communications Biology, 6(1):175, 2023. 1, [31] Yukiyasu Kamitani and Frank Tong. Decoding the visual and subjective contents of the human brain. Nature Neuroscience, 8(5):679685, 2005. 1 [32] Nancy Kanwisher and Galit Yovel. The fusiform face area: cortical region specialized for the perception of faces. Philosophical Transactions of the Royal Society B: Biological Sciences, 361(1476):21092128, 2006. 1, 3 [33] Kendrick N. Kay, Thomas Naselaris, Ryan J. Prenger, and Identifying natural images from human Jack L. Gallant. brain activity. Nature, 452(7185):352355, 2008. 3 [34] Meenakshi Khosla, Apurva Ratan Murty, and Nancy Kanwisher. highly selective response to food in human visual cortex revealed by hypothesis-free voxel decomposition. Current Biology, 32(19):41594171, 2022. 1, 3 [35] Talia Konkle and Aude Oliva. Examining how the real-world size of objects is represented in ventral visual cortex. Journal of Vision, 10(7):982982, 2010. [36] Daniel Lee and Sebastian Seung. Learning the parts of objects by non-negative matrix factorization. nature, 401 (6755):788791, 1999. 2, 3 [37] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740755. Springer, 2014. 4 [38] Andrew Luo, Maggie Henderson, Leila Wehbe, and Michael Tarr. Brain diffusion for visual exploration: Cortical disIn Advances covery using large scale generative models. in Neural Information Processing Systems, pages 75740 75781. Curran Associates, Inc., 2023. 1, 3 [39] Andrew Luo, Jacob Yeung, Rushikesh Zawar, Shaurya Dewan, Margaret Henderson, Leila Wehbe, and Michael Tarr. Brain mapping with dense features: Grounding cortical semantic selectivity in natural images with vision transformers. arXiv preprint arXiv:2410.05266, 2024. 3 [40] Martin McKeown and Terrence Sejnowski. Independent component analysis of fmri data: examining the assumptions. Human brain mapping, 6(5-6):368372, 1998. 3 [41] Thomas Naselaris, Kendrick N. Kay, Shinji Nishimoto, and Jack L. Gallant. Encoding and decoding in fmri. NeuroImage, 56(2):400410, 2011. Multivariate Decoding and Brain Reading. 3 [42] Sam Norman-Haignere, Nancy Kanwisher, and Josh McDermott. Distinct cortical pathways for music and speech revealed by hypothesis-free voxel decomposition. neuron, 88 (6):12811296, 2015. [43] Tuomas Oikarinen and Tsui-Wei Weng. Clip-dissect: Automatic description of neuron representations in deep vision networks. arXiv preprint arXiv:2204.10965, 2022. 3 [44] Soojin Park, Timothy Brady, Michelle Greene, and Aude Oliva. Disentangling scene content from spatial boundary: complementary roles for the parahippocampal place area and lateral occipital complex in representing real-world scenes. Journal of Neuroscience, 31(4):13331340, 2011. 3 [45] Sarah Schwettmann, Tamar Shaham, Joanna Materzynska, Neil Chowdhury, Shuang Li, Jacob Andreas, David Bau, and Antonio Torralba. Find: function description benchmark for evaluating interpretability methods. Advances in Neural Information Processing Systems, 36:7568875715, 2023. 3 [46] Paul Scotti, Mihir Tripathy, Cesar Kadir Torrico Villanueva, Reese Kneeland, Tong Chen, Ashutosh Narang, Charan Santhirasegaran, Jonathan Xu, Thomas Naselaris, Kenneth Norman, et al. Mindeye2: Shared-subject models enable fmri-to-image with 1 hour of data. arXiv preprint arXiv:2403.11207, 2024. 3 [47] Martin I. Sereno, Anders M. Dale, John B. Reppas, Kai K. Kwong, John W. Belliveau, Thomas J. Brady, Bruce R. Rosen, and Roger B. H. Tootell. Borders of multiple visual areas in humans revealed by functional magnetic resonance imaging. Science, 268(5212):889893, 1995. 1 [48] Tamar Rott Shaham, Sarah Schwettmann, Franklin Wang, Achyuta Rajaram, Evan Hernandez, Jacob Andreas, and Antonio Torralba. multimodal automated interpretability In Forty-first International Conference on Machine agent. Learning, 2024. 3 [49] Stephen M. Smith, Peter T. Fox, Karla L. Miller, David C. Glahn, P. Mickle Fox, Clare E. Mackay, Nicola Filippini, Kate E. Watkins, Roberto Toro, Angela R. Laird, and Christian F. Beckmann. Correspondence of the brains functional architecture during activation and rest. Proceedings of the National Academy of Sciences, 106(31):1304013045, 2009. [50] Roger B. H. Tootell, John B. Reppas, Anders M. Dale, Rodney B. Look, Martin I. Sereno, Rafael Malach, Thomas J. Brady, and Bruce R. Rosen. Visual motion aftereffect in human cortical area mt revealed by functional magnetic resonance imaging. Nature, 375(6527):139141, 1995. 1 [51] VG van de Ven, Jans, Been, Goebel, and de Weerd. Intrinsic functional organization of retinotopic visual fields in human occipital cortex: 3t fmri study. NeuroImage, 47: S63, 2009. 3 [52] Leonard van Dyck, Martin Hebart, and Katharina Dobs. Core neural dimensions of functionally selective areas in the human visual cortex. In European Conference on Visual Perception (ECVP), 2024. 1, 3 [53] Roberto Viviani, Georg Gron, and Manfred Spitzer. Functional principal component analysis of fmri data. Human brain mapping, 24(2):109129, 2005. 3 [54] Navve Wasserman, Roman Beliy, Roy Urbach, and Michal Functional brain-to-brain transformation with no Irani. shared data. arXiv preprint arXiv:2404.11143, 2024. 3 [55] Kevin Weiner and Kalanit Grill-Spector. Not one extrastriate body area: using anatomical landmarks, hmt+, and visual field maps to parcellate limb-selective activations in human lateral occipitotemporal cortex. Neuroimage, 56(4):2183 2199, 2011. 1, 3 [56] Kentaro Yamada, Yoichi Miyawaki, and Yukiyasu Kamitani. Inter-subject neural code converter for visual image representation. NeuroImage, 113:289297, 2015. 10 [57] Alicia Zeng and Jack Gallant. Disentangling superpositions: Interpretable brain encoding model with sparse concept atoms. In The Thirty-ninth Annual Conference on Neural Information Processing Systems. 3 [58] Yuanfang Zhao, Emalie McMahon, and Leyla Isik. Separate neural representations for physical and communicative social interactions: Evidence from data-driven voxel decomposition. 2025. 1, 3 [59] Yuan Zhong, Huinan Wang, Guangming Lu, Zhiqiang Zhang, Qing Jiao, and Yijun Liu. Detecting functional connectivity in fmri using pca and regression analysis. Brain topography, 22(2):134144, 2009. 3 11 Supplementary Materials BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain A. Ablation & Analysis SAE ablations. The two main hyperparameters in our SAEs are (i) the latent-space dimensionality, controlled by the expansion factor, and (ii) the sparsity coefficient, which weights the sparsity regularization on the latent codes. Sparsity encourages each fMRI sample to be reconstructed using only small number of patterns (i.e., sparse linear combination of decoder columns). We evaluate four expansion factors: 0.5, 1, 2, and 4. The settings 0.5 and 1 correspond to regular autoencoders that reduce or roughly preserve the dimensionality, while 2 and 4 yield overcomplete representations that expand it. We also evaluate four sparsity coefficients: 0, 1, 2, and 4, where 0 corresponds to no sparsity penalty (a standard autoencoder). In Tab. T1 we report, for each configuration, the rank score used for model selection (the evaluation results reported in the main paper are computed on separate heldout set), together with the percentage of interpretable hypotheses above two thresholds (0.5 and 0.8). Sparsity has clear and consistent effect: higher sparsity generally produces more interpretable patterns. This is expected, since enforcing that each fMRI sample is reconstructed from only few patterns encourages those patterns to capture meaningful structure rather than overfitting. The expansion factor shows similar trend: larger expansion factors tend to perform better, with 0.5 performing noticeably worse in most cases. Overall, both hyperparameters are important for SAE interpretability. Expansion factors larger than 4 did not yield consistent additional gains and were computationally expensive. Similarly, increasing the sparsity coefficient beyond 4 did not lead to clear improvements. Table T1. SAE Ablation by Expansion Factor and Sparsity. Percentage of Interpretable Hypotheses. The fraction of hypotheses that achieve an alignment score above threshold (0.5 or 0.8) with at least one pattern (rank score). Threshold Exp. Factor 1 2 4 Sparsity > 0.5 > 0. 0.5 1 2 4 0.5 1 2 4 12.1% 19.2% 18.2% 22.0% 14.4% 18.6% 19.2% 22.5% 16.8% 19.8% 20.9% 22.5% 18.2% 21.1% 23.3% 25.0% 1.1% 3.0% 2.6% 3.6% 1.4% 2.6% 2.9% 3.6% 1.7% 3.8% 3.5% 3.7% 2.2% 2.9% 3.5% 3.8% Predicted fMRI pool size. We aim to disentangle the influence of the retrieval pool size (the pool from which the top-N most activating images are retrieved) on interpretability performance. In Tab. T2 we report results for retrieval pools enriched with different amounts of predicted fMRI. All decomposition methods are trained on the same combined data (measured + predicted fMRI). Unlike the main paper, where we also compare models trained only on measured fMRI, here we fix the training data and vary only the retrieval pool. Importantly, increasing the pool size also increases the number of images used to compute the hypothesis scores, since the number of top activating images is defined as fraction of the pool (0.2%). Therefore, larger pools not only tend to yield higher interpretability scores, but also make the scores more robust. Even when scores are similar, we therefore prefer larger pools. As shown in the table, enriching the pool with predicted fMRI leads to clear improvement compared to using measured fMRI alone, and for the best-performing model the largest pool yields the highest and most stable interpretability scores. Table T2. Percentage of Interpretable Hypotheses (different retrieval pools). All decomposition methods are trained on the same combined data (measured + predicted fMRI). Unlike the main paper, where we also compare models trained only on measured fMRI, here we fix the training data and vary only the retrieval pool. We report results for pools enriched with different amounts of predicted fMRI (threshold > 0.5). Method Voxels PCA (Single) NMF (Single) ICA (Single) ICA (Multiple) SAE (Single) SAE (Multiple) SAE+ICA Measured fMRI +Pred 30k +Pred 60k +Pred 90k +Pred 120k 3.8% 1.8% 2.4% 10.4% 10.0% 5.6% 8.2% 9.8% 7.1% 6.8% 7.1% 6.7% 7.1% 7.7% 7.7% 7.4% 5.6% 5.8% 5.9% 5.5% 18.0% 18.5% 17.7% 18.1% 17.4% 17.6% 17.9% 18.3% 16.0% 14.6% 15.5% 15.7% 17.8% 16.8% 17.4% 17.4% 20.7% 20.6% 20.9% 21.5% Per-ROI quantitative results. We report, for each ROI, the percentage of explained hypotheses above 0.5 threshold for the averaged results shown in the paper. V1V3 are averaged across ventral and dorsal partitions, and FBA, FFA, and VWFA are averaged across their sub-areas. Higher-level visual regions show greater interpretability than early visual areaslikely because they encode higherlevel semantics that are both easier to decompose and to describe using natural language within our pipeline. 12 Table T3. Per-ROI quantitative results. Per-ROI percentage of explained hypotheses (> 0.5) on the pool enriched with all predicted fMRI (+120k predicted fMRI). V1V3 are averaged across ventral/dorsal; FBA, FFA, and VWFA are averages of their sub-areas. Method V1 V2 V3 hV4 EBA OFA OPA OWFA PPA FFA RSC FBA VWFA Voxels PCA (Single) NMF (Single) ICA (Single) SAE (Single) SAE+ICA 3.0% 2.1% 2.7% 1.0% 2.8% 0.2% 0.5% 0.6% 1.2% 3.2% 2.9% 2.0% 1.8% 2.6% 0.3% 0.5% 0.5% 0.5% 1.1% 0.2% 0.1% 0.4% 0.4% 1.7% 1.6% 2.2% 2.4% 0.2% 1.9% 2.7% 5.7% 11.4% 15.3% 7.1% 13.9% 14.5% 12.8% 8.2% 6.2% 9.7% 0.6% 0.9% 1.3% 4.8% 14.5% 1.9% 9.2% 7.6% 6.8% 5.7% 2.9% 7.6% 1.8% 2.8% 6.8% 11.0% 18.8% 7.5% 16.8% 17.2% 13.6% 8.6% 5.2% 10.7% 11.8% 5.8% 1.1% 2.8% 4.4% 0.7% 2.3% 2.0% 0.8% 1.1% 3.1% 2.7% 1.2% 10.1% Complementarity across decompositions. We aim to understand which decomposition methods are most beneficial to combine, i.e., which ones provide complementary patterns that explain hypotheses the others do not. To this end, we measure, for every pair of decompositions, the change in the percentage of explained hypotheses (threshold 0.5) when combining their patterns. Combining pair means pooling the patterns from both methods and recomputing the fraction of explained hypotheses; for reference, combining method with itself corresponds to using multiple runs (different seeds) of the same decomposition. The reported value is the difference between the combined score and the maximum score of the two methods when used separately. As shown in Tab. T4, SAE and ICA are the most complementary pair, each substantially improving the other. Table T4. Pairwise complementarity between decompositions. Values show the gain in explained hypotheses (threshold 0.5) when combining each pair, relative to the better single method."
        },
        {
            "title": "Voxels\nPCA\nNMF\nICA\nSAE",
            "content": "0 0.5 1.4 0.1 -0.2 0.5 0 0 0.4 0 1.4 0 0 0.5 0.1 0.1 0.4 0.6 0.2 2.1 -0.2 0 0.1 2.1 0.7 B. Additional Details B.1. SAE Training Details The Sparse Autoencoder (SAE) was trained to learn compact and interpretable latent representation of fMRI activations, enabling decomposition into meaningful spatial patterns. Architecture. We used simple, fully linear autoencoder, where both encoder and decoder are implemented as matrix multiplications. ReLU activation is applied after the encoder to enforce non-negativity in the latent activations. Sparsity is encouraged via an 1 loss applied to the latent space, added to the reconstruction mean-squared error (MSE) loss. The expansion factor controls the latent dimensionality (the ratio between latent and input size), while the sparsity coefficient balances the sparsity loss relative to the reconstruction loss. Combining with predicted fMRI. We combine the real measured fMRI data (10k samples) with predicted fMRI activations from the image-to-fMRI encoder (120k samples). Real fMRI measurements are inherently noisy, and many voxels exhibit low signal-to-noise ratio (SNR). Consequently, the predicted encoder often outputs values close to zero for those voxels, leading to mismatch in distribution between measured and predicted fMRI. Training single SAE on this mixed data causes an imbalance, resulting in higher sparsity for predicted samples and reduced sparsity for measured ones. To address this, we use separate encoder networks for the measured and predicted data, while sharing common decoder. This allows both data types to be decomposed into the same set of components (dictionary patterns) while enabling independent control over their activations. During training, each batch contained an equal number of measured and predicted samples to ensure balanced gradients from both data sources. Hyperparameters. As reported in the ablation studies, we experimented with multiple combinations of sparsity coefficients and expansion factors. For the main hyperparameter search, we considered values {1, 2, 4} for both. The top four models selected according to the rank score threshold of 0.8 on Subject 1, were used in the multiple model setup: sparsity coefficient 4 with expansion factors {1, 2, 4} and sparsity coefficient 1 with expansion factor 2. The single model configuration used sparsity coefficient 4 and expansion factor 4. The same hyperparameters were applied across all subjects. B.2. Baseline Decomposition Training Details All baseline decomposition methods were trained in two (i) using only the measured fMRI data configurations: (10k samples), and (ii) using the combined pool of measured and predicted fMRI activations (120k samples). 13 This setup enables consistent comparison between the baselines and the Sparse Autoencoder (SAE), both when relying solely on measured data and when leveraging the large scale predicted data. Voxels. The voxel baseline uses the raw voxel activations without any decomposition or learned transformation. Each voxel is treated as an independent component. For visualization and interpretability, every voxels activation pattern was duplicated into two components (the original and its negation), allowing retrieval of top activating images for both positive and negative responses. This ensures interpretability is consistent across all decomposition methods, including those that produce only nonnegative activations. PCA. Principal Component Analysis (PCA) serves as deterministic, orthogonal decomposition baseline. It was trained on both measured only and combined fMRI datasets, with the number of components determined by the cumulative explained variance. To ensure consistency for interpretability visualization, each PCA component was duplicated into two versions, positive and negative, allowing activation maps to always correspond to positively responsive image sets. ICA. Independent Component Analysis (ICA) was applied using numbers of components corresponding to the dimensionalities that achieve 90%, 95%, and 98% explained variance in PCA. Higher explained variance targets (>98%) were found to be unstable or unnecessary due to overfitting. ICA was run multiple times with different random seeds to capture variability arising from its nondeterministic optimization process. Similar to PCA, each independent component was duplicated into its positive and negative versions for interpretability visualization. NMF. Nonnegative Matrix Factorization (NMF) was trained with the same component counts as ICA (matching the 0.9, 0.95, and 0.98 explained variance configurations). Since NMF produces strictly nonnegative components, no positive or negative duplication was needed. NMF was also trained on both measured and combined fMRI data. Different hyperparameter settings (for example, initialization and regularization) showed minimal impact on the final interpretability results, consistent with its stable convergence properties under nonnegativity constraints. B.3. Visualize & Explain As described in the paper, in the Visualize step we select, for every pattern, its top-N activating responses and collect their corresponding images, producing set of images that visualize what most strongly activates that pattern. This is done separately for measured fMRI responses and for predicted responses. We take the six most activating images from the measured fMRI pool and the ten most activating images from the predicted fMRI pool (16 images in total). For each image, we first generate detailed caption using Qwen2.5-VLM-32B1. The VLM is instructed (see prompt in Fig. S15) to produce rich descriptions including objects, scene or room type, dominant colors, ongoing activities, and body postures. Given the set of captions, we then ask language model Qwen2.5-32B2 to generate between 3 and 12 hypotheses that may explain what is shared among at least half of those images (Fig. S16). Requesting multiple hypotheses provides diversity, since both the captions and the LLM can be biased toward dominant concept, whereas smaller but meaningful shared attributes might otherwise be missed. We intentionally separate the VLM and LLM stages. We found that current visionlanguage models often struggle to infer what is shared across multiple images, frequently producing incorrect or overly general hypotheses. By contrast, separating the process allows each VLM to focus on single image, producing high-quality detailed captions, while the LLM operates only on textual descriptionsan easier and more reliable input for identifying cross-image commonalities. B.4. Scaling to an Unlimited Number of Patterns Explaining every component in every ROI and for every decomposition method is costly and inefficient. To scale interpretability, we introduce two steps: (i) Hypothesis dictionary generation, and (ii) Hypothesisimage labeling. Once the dictionary and the image labels are in place, any new brain pattern can be evaluated by measuring how consistently its top-activating images express concepts from the dictionary. The prompts used for hypothesisimage labeling are shown in Fig. S17, and examples of images with positively labeled hypotheses appear in Fig. S18. B.5. Region of Interest (ROI) We analyze activity within predefined Regions of Interest (ROIs), which are standard cortical parcels used to summarize responses within broader anatomical and functional subdivisions of the visual system. In the Algonauts/NSD data, these ROIs are supplied by the dataset releases based on independent localizers and anatomical parcellations. The full set used here is: V1v, V1d, V2v, V2d, V3v, V3d, hV4, VWFA-1, VWFA-2, EBA, FBA-1, FBA-2, FFA-1, FFA2, OFA, OPA, OWFA, PPA, and RSC. Early visual areas (V1V3, separated into ventral and dorsal subregions) correspond to the low-level visual cortex and are primarily involved in processing basic visual attributes such as orientation, color, and spatial frequency. Area hV4 is associated with color and shape processing. Higher visual regions 1https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct 2https://huggingface.co/Qwen/Qwen2.5-32B-Instruct 14 show more specialized selectivity: EBA, FBA, and OFA are bodyand face-selective regions; OPA, PPA, and RSC respond to scenes, places, and navigation-related information; VWFA and OWFA are selective to word and object forms, respectively. Together, these ROIs span the visual hierarchy from low-level visual representations to high-level semantic and categorical processing. C. Additional Quantitative Evaluation Table T5. Number of Interpretable Patterns. The number of patterns whose best hypothesis alignment exceeds the same threshold (0.5 or 0.8). To avoid counting near-duplicate patterns, we remove any component whose voxel-wise correlation with higherscoring pattern exceeds 0.5 Measured fMRI + Predicted fMRI Method Voxels PCA (Single) NMF (Single) ICA (Single) ICA (Multiple) SAE (Single) SAE (Multiple) SAE+ICA > 0.5 5234 1045 27 580 424 17242 30005 30583 > 0.8 355 6 2 4 41 586 1074 > 0.5 5905 76 19 226 305 8858 15748 16051 > 0.8 291 6 2 44 61 286 617 679 D. Additional Visualizations Spatial localization of interpretable patterns (SAE vs. ICA). Extending the main-text observation that SAE patterns are more spatially localized, Fig. S1 presents side-byside brain activation maps for multiple ROIs, comparing patterns derived from ICA and from SAE. Despite being trained on one-dimensional voxel vectors with no spatial priors, SAE produces compact, clustered patterns, whereas ICA frequently yields more diffuse or scattered maps. All maps were generated using identical preprocessing, color scaling, and selection criteria (top-activating images per pattern), ensuring that visual differences reflect the decomposition method rather than visualization settings. Additional concepts across ROIs. We present additional interpretable patterns from Subject 1 together with representative top-activating images. We show further results for EBA and RSC in Fig. S2, for OPA in Fig. S3, and for FBA-1 and FBA-2 in Fig. S4. Additional subjects visualization. We present discovered interpretable patterns and concepts for two additional subjects with larger number of examples (Subjects 2 and 5) in Figs. S5 to S8. All subjects trained decompo15 Figure S1. Brain activation map (SAE vs ICA). We show different activation patterns derived from Sparse Autoencoder (SAE) and Independent Component Analysis (ICA) for different ROIs. SAE patterns are notably more compact and clustered and spatially localized, despite both receiving only 1D voxel vectors and no spatial information. sitions and discovered patterns will be publicly available, enabling further research and discoveries. Full top 16 images per pattern. We present full visual grids showing the top 16 activating images for the concepts from EBA discussed in the main paper (Figs. S9 and S10). For each concept, the top row corresponds to predicted fMRI responses and the bottom row to measured fMRI responses. Images are ordered by activation strength, from left to right. We also show all 16 top-activating images for the patterns and concepts presented in the main paper for PPA (Figs. S11 and S12). Concepts best explained by each ROI. We summarize which semantic concepts are most strongly represented across different Regions of Interest (ROIs). Fig. S13 shows, for each ROI, the concepts achieving an alignment score above 0.5, allowing concept to appear in multiple ROIs In contrast, Fig. S14 if it is represented across regions. presents an exclusive version in which each concept is assigned only to the ROI where it achieves its highest alignment score. Together, these visualizations reveal both the distributed and the region-specific organization of conceptual representations across the visual cortex. Prompts. Figures S15 to S17 depict the prompts used in our pipeline: first, detailed per-image captions are elicited (Fig. S15); second, language model infers shared hypotheses across the image set (Fig. S16); and, distinct from the two-stage explanation, the imagehypothesis labeling prompt is used in the upscale stage to decide whether each hypothesis is supported by each image (Fig. S17). Labeled examples. Figure S18 presents representative images together with the hypotheses labeled as positive for each, demonstrating the output produced by the hypothesisimage labeling stage. Figure S2. Discovered Interpretable Patterns (EBA and RSC). We show additional patterns for Subject 1 with top activating images and selected explanations. EBA is known to encode bodies and actions, whereas RSC processes scene information. 16 Figure S3. Discovered Interpretable Patterns (OPA). We show additional patterns for Subject 1 with top activating images and selected explanations. OPA is known to process scene layout and navigability. 17 Figure S4. Discovered Interpretable Patterns (FBA-1 and FBA-2). We show additional patterns for Subject 1 with top activating images and selected explanations. FBA is body-selective area, encompassing two sub-areas shown here, FBA-1 and FBA-2. 18 Figure S5. Discovered Interpretable Patterns of Subject 2 (OPA and VWFA-1). We show patterns for Subject 2 with top activating images and selected explanations. OPA is known to process scene layout and navigability, and VWFA is involved in processing word shapes and visual text. 19 Figure S6. Discovered Interpretable Patterns of Subject 2 (EBA). We show patterns for subject 2 with top activating images and selected explanations. EBA is known to encode bodies and actions. 20 Figure S7. Discovered Interpretable Patterns of Subject 5 (OPA and FFA-1). We show patterns for subject 5 with top activating images and selected explanations. OPA is known to process scene layout and navigability, and FFA is primarily known for face processing. 21 Figure S8. Discovered Interpretable Patterns of Subject 5 (EBA and hV4). We show patterns for subject 5 with top activating images and selected explanations. EBA is known to encode bodies and actions and V4 is known to encode mid-level features (e.g., color, shape). 22 Figure S9. Image grid EBA (Part 1). Full visual grid with top 16 activating images for concepts Tied neckwear, Brushing teeth, Hands, and Jumping found in the EBA region. For each concept, the top 8 images corresponding to Measured and Predicted fMRI responses are shown. 23 Figure S10. Image grid EBA (Part 2). Full visual grid with top 16 activating images for concepts Frisbee, Soccer, Surfing, and Tennis found in the EBA region. For each concept, the top 8 images corresponding to Measured and Predicted fMRI responses are shown. 24 Figure S11. Image grid PPA (Part 1). Full visual grid with top 16 activating images for concepts Stone building, Commercial buildings, Kitchen, and Indoor found in the PPA region. For each concept, the top 8 images corresponding to Measured and Predicted fMRI responses are shown. 25 Figure S12. Image grid PPA (Part 2). Full visual grid with top 16 activating images for concepts Collage, Screen, Healthy food, and Landscape found in the PPA region. For each concept, the top 8 images corresponding to Measured and Predicted fMRI responses are shown. 26 Figure S13. Concepts best explained by every ROI (Non-Exlusive). Concepts represented in each Region of Interest (ROI) that achieve an alignment score > 0.5. concept may appear in multiple ROI word clouds, reflecting its representation across different brain regions. Word size reflects the alignment score of the concept within the assigned ROI. 27 Figure S14. Concepts best explained by every ROI (Exlusive). Each concept is assigned to single ROIthe one with the highest alignment score. Only concepts with alignment > 0.5 are shown. Word size reflects the alignment score within the assigned ROI. 28 Figure S15. Image Captioning Prompt. 29 Figure S16. Hypothesis Generation Prompt. Given set of 10 images and their detailed captions, an LLM is instructed to identify what is common across the images and to generate hypotheses that may explain what is shared among them. 30 Figure S17. ImageHypothesis Labeling Prompt. For each imagehypothesis pair, we label whether the hypothesis is supported by the image or not. 31 Figure S18. Images with per-hypothesis labeling. We show six example images and list all hypotheses labeled as positive for each. The hypotheses are taken from our brain-inspired hypothesis dictionary, and the images are drawn from the set used with predicted fMRI."
        }
    ],
    "affiliations": [
        "Massachusetts Institute of Technology",
        "Weizmann Institute of Science"
    ]
}