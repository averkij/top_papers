{
    "paper_title": "Cartridges: Lightweight and general-purpose long context representations via self-study",
    "authors": [
        "Sabri Eyuboglu",
        "Ryan Ehrlich",
        "Simran Arora",
        "Neel Guha",
        "Dylan Zinsley",
        "Emily Liu",
        "Will Tennien",
        "Atri Rudra",
        "James Zou",
        "Azalia Mirhoseini",
        "Christopher Re"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models are often used to answer queries grounded in large text corpora (e.g. codebases, legal documents, or chat histories) by placing the entire corpus in the context window and leveraging in-context learning (ICL). Although current models support contexts of 100K-1M tokens, this setup is costly to serve because the memory consumption of the KV cache scales with input length. We explore an alternative: training a smaller KV cache offline on each corpus. At inference time, we load this trained KV cache, which we call a Cartridge, and decode a response. Critically, the cost of training a Cartridge can be amortized across all the queries referencing the same corpus. However, we find that the naive approach of training the Cartridge with next-token prediction on the corpus is not competitive with ICL. Instead, we propose self-study, a training recipe in which we generate synthetic conversations about the corpus and train the Cartridge with a context-distillation objective. We find that Cartridges trained with self-study replicate the functionality of ICL, while being significantly cheaper to serve. On challenging long-context benchmarks, Cartridges trained with self-study match ICL performance while using 38.6x less memory and enabling 26.4x higher throughput. Self-study also extends the model's effective context length (e.g. from 128k to 484k tokens on MTOB) and surprisingly, leads to Cartridges that can be composed at inference time without retraining."
        },
        {
            "title": "Start",
            "content": "Cartridges: Lightweight and general-purpose long context representations via self-study Sabri Eyuboglu 1 Ryan Ehrlich 1 Will Tennien 1 Atri Rudra 3 Simran Arora 1,2 Neel Guha 1 Dylan Zinsley 3 Emily Liu 1 James Zou 1 Azalia Mirhoseini 1 Christopher RÃ© 1 1Stanford University # eyuboglu@stanford.edu, rehrlich@stanford.edu, simarora@stanford.edu ' HazyResearch/cartridges 3University at Buffalo 2 Caltech * Equal contribution 5 2 0 2 ] . [ 2 6 6 2 6 0 . 6 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large language models are often used to answer queries grounded in large text corpora (e.g. codebases, legal documents, or chat histories) by placing the entire corpus in the context window and leveraging in-context learning (ICL). Although current models support contexts of 100K1M tokens, this setup is costly to serve because the memory consumption of the KV cache scales with input length. We explore an alternative: training smaller KV cache offline on each corpus. At inference time, we load this trained KV-cache, which we call CARTRIDGE, and decode response. Critically, the cost of training CARTRIDGE can be amortized across all the queries referencing the same corpus. However, we find that the naive approach of training the CARTRIDGE with next-token prediction on the corpus is not competitive with ICL. Instead, we propose SELF-STUDY, training recipe in which we generate synthetic conversations about the corpus and train the CARTRIDGE with context-distillation objective. We find that CARTRIDGES trained with SELF-STUDY replicate the functionality of ICL, while being significantly cheaper to serve. On challenging long-context benchmarks, CARTRIDGES trained with SELF-STUDY match ICL performance while using 38.6 less memory and enabling 26.4 higher throughput. SELF-STUDY also extends the models effective context length (e.g. from 128k to 484k tokens on MTOB) and surprisingly, leads to CARTRIDGES that can be composed at inference time without retraining."
        },
        {
            "title": "Introduction",
            "content": "Large language model (LLM) users often place large text corpora into the context window. For instance, user or organization may use LLMs to understand codebases [56], financial documents [33], legal texts [28, 102], textbooks [61], or personal files [7]. LLMs excel here due to in-context learning (ICL), enabling accurate responses to diverse queries (e.g., factual Q&A, summarization, code generation) [20]. Despite its flexibility, this usage paradigm is costly to serve. ICL requires maintaining KV cache that grows linearly with the input length. For example, LLaMA 70B needs 84 GB of memory (at 16-bit precision) to answer single question over 128k-token context [21]. This severely limits user throughput: on single H100 GPU, LLaMA 8Bs peak throughput (tokens/s) drops by 77 when increasing the context from 1k to 120k tokens (Figure 3). Prior work has thus explored ways to reduce KV cache memory usage. For instance, prompt compression methods reduce the number of tokens stored in the cache using summarization, or self-information filtering [16, 37, 48], while KV cache compression techniques directly compress the stored key-value pairs [23, 60, 75, 98]. Unfortunately, there are memory-quality tradeoffs associated with these methods: in experiments on challenging long-context tasks, we find that performance degrades rapidly when applying these methods with compression ratios greater than 2 (see Figure 4). Motivated by the observation that the cost of preparing KV cache can be amortized across many queries that reference the same corpus, we explore complementary approach based on offline training. Given specific text corpus (e.g. patients medical record) we freeze the LLM and train smaller KV cache offline Figure 1: Producing CARTRIDGES via self-study. For given document corpus, we train CARTRIDGE by distilling the corpus into parameterized KV cache through process we call SELF-STUDY. At inference time, this CARTRIDGE can be loaded into an LLM, which can then be used to answer diverse queries about the corpus, simulating in-context analysis of the corpus while requiring substantially less memory. by backpropagating loss into the key and value vectors in process closely resembling prefix tuning [45, 47]. We call the trained KV cache representing the corpus CARTRIDGE. At inference time, we load the trained CARTRIDGE, append the users messages, and decode. Because users repeatedly reference the same corpora (e.g. SEC filings, codebase, personal files), each CARTRIDGE can be trained once offline and reused. This approach also integrates cleanly with existing inference servers, which are already designed to manage per-user KV caches [44, 101]. Achieving ICL-equivalent functionality requires CARTRIDGES to satisfy two non-trivial desiderata. First, CARTRIDGES should replicate the generality of ICL, and provide accurate responses across diverse user prompts [20]. Second, CARTRIDGES should replicate ICLs structural awarenessits ability to reason over document structure, and understand how distant parts of corpus relate or depend on each other (an ability that degrades when using lossy KV-cache compression methods). It is unclear if there is procedure that satisfies these desiderata, while providing memory efficiency. The natural baseline approach is to train CARTRIDGE with next-token prediction objective on the raw corpus. Excitingly, this yields CARTRIDGES that memorize the corpus perfectly using 107 less memory than the KV-cache. However, the resulting CARTRIDGES are not general - they degrade the LMs ability to respond to diverse questions beyond regurgitating the corpus (Figure 3). To address these challenges and produce general, structurally aware CARTRIDGES for any text corpus, we propose an automated method called SELF-STUDY. SELF-STUDY has two steps: 1. Synthetic data generation (Section 4.1): We generate synthetic training data by prompting the model to quiz itself about the corpus content, resulting in synthetic conversation trace. Training on these lets us avoid training on the same exact text multiple times and improves generality (see Figure 3). To support corpora that exceed the effective context length of the model, we chunk the corpus when generating synthetic conversations. We also curate set of seed prompts that bias the synthetic conversations towards global reasoning and improve structural awareness (see Figure 6 right). 2. Context distillation (Section 4.2): We train on the synthetic conversations using context-distillation objective [10, 72], which aligns the CARTRIDGE-augmented models next-token distributions with the distributions of the model with the corpus in context. We find that the context distillation substantially improves the quality of the CARTRIDGES compared to next-token-prediction (see Figure 6 center). In summary, given large corpus of text, our goal is to train small virtual KV cache, termed CARTRIDGE, that when used by the model, mimics the conversational behavior of the model with the entire corpus in context. To do this, we generate synthetic conversations and train the CARTRIDGE on them with context distillation objective recipe we call SELF-STUDY. Evaluations. We evaluate CARTRIDGES trained with SELF-STUDY on set of challenging benchmarks that pair single large text corpus (100k-484k tokens) with diverse set of queries [2, 33, 76]. We make three claims. First, CARTRIDGES extends the quality-memory frontieraveraged across the benchmarks, CARTRIDGES produced with SELF-STUDY match ICL quality while consuming 38.6 less memory, enabling 26.4 increase in peak throughput (tokens per second) when serving many users with different corpora. These 2 Method Consumes limited memory Retains corpus information Supports diverse prompts In-context learning Prompt / KV cache compression CARTRIDGE + Next-token-prediction CARTRIDGE + SELF-STUDY Figure 2: Comparing KV caching strategies. CARTRIDGE improves memory efficiency, while retaining the quality of in-context learning across broad set of prompts. indicates strength and indicates limitation. memory reductions and speedups represent an order of magnitude improvement over state-of-the-art cache compression baselines (e.g. DuoAttention [84]). Second, CARTRIDGES enables context length extrapolation. On the MTOB benchmark [76], where models must translate from Kalamang, low-resource language, into English, we use SELF-STUDY with LLAMA-8B to construct small CARTRIDGE from 484k token textbook. This CARTRIDGE outperforms ICL over the first 130, 000 tokens of the textbook by 11.0 chrF points and matches the ICL performance over curated subset of the textbook. Third, SELF-STUDY also yields CARTRIDGES that are composable without joint optimization: multiple CARTRIDGES can be concatenated and queried together, emulating ICLs ability to flexibly answer queries over multiple documents concatenated in context (see Figure 7). Additionally, we carefully ablate the design decisions in SELF-STUDY and CARTRIDGES (Section 5.3 and Appendix A). Notably, we compare CARTRIDGES parameterized as KV cache [47] with CARTRIDGES parameterized as LoRA [31] and find that KV cache parameterization performs better on both in-domain and out-of-domain tasks. In this work, we demonstrate how offline KV cache training can dramatically reduce the cost of serving language models in settings where users repeatedly include the same text corpora in context. We hope that these cost reductions could enable new applications that are currently intractable, like coding agents with full-repository context or long-term memory in chatbots."
        },
        {
            "title": "2 Preliminaries",
            "content": "We begin by discussing related work (Section 2.1), formalizing our problem (Section 2.2), and providing background on language models and KV caches (Section 2.3). 2.1 Related work See Appendix for detailed discussion of prior work. Parameter Efficient Fine-Tuning Prior work has explored range of strategies for adapting pretrained langague models: prompt distillation [43, 73], self-instruction [58], and domain-specific training [17]. Variants of this approach train corpus into smaller modules (adapters) which can be added to the model, which have parameter efficiency benefits [31, 45, 47, 54, 74]. number of works have explored the idea of composing multiple different parameter-efficient adapters through various aggregation operations [26, 32, 46, 82, 83, 86, 99, 100]. Most similar to our work are recent knowledge injection methods, which aim to internalize into model weights, allowing models to answer queries from parameter knowledge as opposed to ICL [43] [53] [74]. Recent work like LIFT [53] uses synthetic data to train per-document adapter for long-context documents, but does not study the throughput improvements stemming from the lack of large KV cache. Our approach improves quality-memory (and thus quality-throughput) tradeoff frontier, matching ICL performance and supporting composability while keeping the memory footprint small. Architectures Research has also examined more memory efficient alternatives to traditional attention [79], which leverage sparsity [9, 15, 77, 93] or alter the structure of attention [3, 71], among other strategies [6, 50, 3 Figure 3: CARTRIDGES trained with SELF-STUDY balance the generality and memory consumption tradeoff. We compare four methods on the GENCONVO dataset: CARTRIDGES trained with next-token prediction over C, CARTRIDGES trained with SELF-STUDY, full ICL, and truncated ICL, prompt compression method in which we truncate the to the first tokens. (Left) We evaluate on different slices from the GENCONVO dataset. CARTRIDGES trained with next-token prediction performs well on memorization queries, which resemble its training distribution, but cannot generalize to other queries like the other methods. (Center) The x-axis measures the size of the KV cache in GB for the different methods. The y-axis shows log-perplexity on the GENCONVO dataset averaged over the query types. (Right) Peak throughput (tokens/s) measured for different cache sizes for LLAMA-3B and LLAMA-8B with SGLang [101] on an 1xH100 (See Appendix A). 96]. Certain variants (i.e., grouped-query attention) appear in popular models like Llama-3, which we study in our experiments. Prompt and KV-cache compression As the size of the KV cache drives the model memory footprint, research has examined different strategies for reducing the cache size. One set of approaches focus on making the prompt smallerexplicit methods alter the prompt text through summarization and filtering [16, 37, 48, 63, 94], while implicit methods compress prompt representations into set of soft tokens [14, 24, 45, 55, 65, 91]. Another set of approaches exploits observations about the mathematical structure of the KV cache [11, 41, 92], often finding that because small number of keys dominate the attention scores of subsequent queries, non-impactful key-value pairs (or tokens) can be dropped [23, 49, 60, 75, 98] or merged [80, 81, 97]. Synthetic data generation large body of work has focused on generating synthetic training data [1, 22, 58, 67]. For example, Bonito is model that is fine-tuned to generate synthetic data [58], and MetaSynth is method proposed by Riaz et al. that uses language model to orchestrate several expert LLMs for domainspecific synthetic data generation [67]. The training process for Phi-4, 14 billion parameter language model, also incorporates significant amounts of synthetically generated data [1]. 2.2 Problem setup We assume setting in which users issue stream of diverse queries about common corpus of text. We denote the corpus as and the query set as = {q1, q2, . . . , qm}. Illustrative examples of include legal filings, financial documents, code repositories, chat histories, and medical records. Example: Financial Analysis may correspond to the 2022 Form 10-K filing [78] for AMD, which is almost 100k tokens. The queries an analyst might ask an LLM to answer with respect to this form are diverse, including: (1) recalling factual information, (2) performing mathematical reasoning over values, or (3) even generating creative responses (e.g., poem) grounded in the 10-Ks information. 4 Let = {r1, r2, . . . , rm} denote the responses the LLM produces for the queries. We have two objectives. First, we wish to maximize the quality of responses under some quality metric (e.g. accuracy). Second, we wish to minimize the LLMs memory footprint while it is answering questions with respect to the document. This is because larger memory footprints decrease throughput and necessitate more hardware to serve the same number of users (Figure 3, Right). 2.3 Language models and KV caches Recall that an LLM accepts as input sequence of tokens drawn from discrete vocabulary of tokens, each represented by unique integer. The output, which we denote (x), corresponds to categorical distribution over vocab conditioned on the prefix n. Inside the language model, each token x[i] in is embedded into d-dimensional space, yielding matrix Rnd. The matrix is passed through stack of model layers, which each mix the matrix along the and dimensions, with layer â outputting yl Rnd. The final yL is mapped to the logits over with linear projection. Most modern language models use the Transformer architecture based on self-attention [79]. Given an input Rnd for sequence length and embedding dimension d, it computes the output yl Rnd via the softmax over projections q, k, = uWq, uWk, uWv: y[i] = j=1 exp(q[i]k[j]/ t=1 exp(q[i]k[t]/ d)v[j] d) (1) where weight matrices Wq, Wk and Wv for each layer are learned during training. When generating from , we generate one token at time by sampling from ( x) and appending the sampled token to x. Critically, the attention operator is causal: every output y[i] is conditioned on prior tokens. This allows us to avoid recomputing the keys and values for the prior tokens by storing them in KV cache {k[j], v[j]}i j=1, which grows in i. Thus, generation proceeds in two phases: (1) prefill, where we compute the KV cache for the initial prompt and (2) decode, where we generate the response token by token and append to the KV cache. After prefill, if consists primarily of the corpus C, the KV cache effectively serves as representation of the corpus C. This is why including long corpus in the context produces large memory footprints, as the size of the KV cache scales linearly in the length of x."
        },
        {
            "title": "3 The CARTRIDGE paradigm",
            "content": "In this section, we describe the CARTRIDGE paradigm, in which we generate representations of the corpus offline with training, instead of the standard approach of constructing them on-the-fly with prefill. 3.1 Formalizing CARTRIDGES Our goal is to train CARTRIDGE for given corpus C. CARTRIDGE is small set of parameters (i.e. an adapter [31, 47]) that augments an LLM and causes it to behave as if it had in its context window. Formally, let FZ(q) denote the distribution of augmented with given query q. For all Q, we want to ensure that samples rZ FZ(q) are as good or better than the ICL sample rq (C q), according to some query-specific scoring function. In order for FZ(q) to match or exceed the behavior of (C q), three important criteria should be met. Displays generality: Because might span diverse range of question types (e.g., mathematical reasoning, factual recall comprehension, summarization, and more), it is essential that FZ can generalize across different Q. This is non-trivial because is unknown when is being learned offline. If FZ does not generalize, then practitioners may need to learn different for different distributions of queries, which increases the cost of the CARTRIDGE. Ideally, should only need to be learned once, yet work for multiple types of queries. 5 Captures long range dependencies: should also capture long range dependencies contained within C. In many settings, correctly answering different requires reasoning about the order of information presented in C. It is not clear how to capture these dependencies in Z. Capable of composition: Ideally, the representation of and mechanism by which utilizes it could allow for composition, without any particular joint training of CARTRIDGES. Given Z1 and Z2 corresponding to C1 and C2, ideally F[Z1,Z2](q) is similar to (C1 C2 q]) 3.2 Parameterizing CARTRIDGES We parameterize using simplified version of prefix-tuning [47]. Specifically, we allocate KV cache composed of trainable key and value vectors zk, zv Rpd. The size of the full RLpd2 is controlled by the hyperparameter p. The memory footprint of is equivalent to KV cache for prompt with tokens. In ICL, the KV cache for FC (q) (where is of length nC and is of length nQ) would contain nC + nQ key-value pairs, with the first nC corresponding to and the last nQ corresponding to Q: ICL KV Cache CARTRIDGE KV Cache (k[1], v[1]), . . . , (k[nC ], v[nC ]) (cid:123)(cid:122) (cid:125) (cid:124) KV pairs for , (k[nC + 1], v[nC + 1]) . . . (cid:123)(cid:122) (cid:125) (cid:124) KV pairs for (zk[1], zv[1]), . . . , (zk[p], zv[p]) (cid:125) (cid:123)(cid:122) (cid:124) Trainable KV pairs in , (k[np + 1], v[np + 1]) . . . (cid:123)(cid:122) (cid:125) (cid:124) KV pairs for To train CARTRIDGE, we substitute the key-value pairs corresponding to with Z, and directly optimize them by back-propagating the loss into the key and value vectors. Critically, we freeze all parameters of the model, only training the key and value vectors in Z. We discuss the choice of loss in Section 4.2. Initialization Prior work finds that optimizing randomly initialized cache is unstable and leads to degraded performance [47]. Instead, these works initialize the trainable cache with smaller dimensionality and then re-project it to the original dimension with an MLP. In contrast, we find that proper initialization of allows us to directly optimize the full cache without reparametrization. Specifically, we initialize to the KV cache corresponding to the first tokens of the corpus C. Alternatively, we could use summary of the corpus or filter tokens using off-the-shelf prompt compression strategies [84]. In Section 5.3, we show that our initializations lead to stable training and faster convergence than the random initialization. Why this parameterization? We note that the parameter-efficient fine-tuning literature provides other ways to augment an LLM with set of additional parameters, in particular low-rank adaptation (LoRA) [31, 45, 47]. In Section 5.3, we perform comprehensive comparison of CARTRIDGES parameterized with prefix-tuning and LoRA. 3.3 Serving CARTRIDGES CARTRIDGE can be served efficiently with minimal changes to existing LLM inference servers [40, 44, 101]. Because CARTRIDGE is KV cache, it can be loaded directly into the KV cache slots using existing mechanisms for handling cached prefixes. LLM inference servers are heavily optimized for managing distinct KV-caches for multiple users [90], meaning CARTRIDGES can be served at high throughput using existing inference servers. Decoding tokens with CARTRIDGE is identical to serving request with prefix of length (the hyperparameter denoting the number of trainable tokens in the CARTRIDGE). This contrasts with other methods like LoRA, which require custom infrastructure to serve efficiently to multiple users [13]. See Figure 3 for the relationship between prefix length and throughput."
        },
        {
            "title": "4 SELF-STUDY: A self-supervised method for training CARTRIDGES",
            "content": "In this section, we describe SELF-STUDY, simple approach for training CARTRIDGE on any corpus of text. The design of SELF-STUDY is motivated by experiments showing how CARTRIDGES trained with simpler recipe fail to generalize to diverse user queries. 6 Figure 4: CARTRIDGES matches ICL quality with lower memory costs. We measure LLAMA-3B response quality (y-axis) against KV cache memory (x-axis) for different methods, at different KV cache sizes. The dashed line marks the quality of standard ICL. Motivating observations The naive method for constructing CARTRIDGE would be to fine-tune the parameters of with the next token prediction objective on the corpus text directly. We show results experimenting with this approach in Figure 3, where we evaluate on dataset derived from FinanceBench [33], which we refer to as GENCONVO (see Appendix for details). GENCONVO contains multiple types of questions (e.g. synthesis, reasoning). We find that the naÃ¯ve next-token prediction approach can memorize with near perfect perplexity (Figure 3 left), while consuming 107 less memory than ICL (Figure 3 center). However, generalization to other slices is poor, as shown in Figure 3. We seek training objective that allows the responses from model that uses the CARTRIDGE to generalize to diverse set of user queries, resembling ICL. Motivated by these observations, we describe synthetic data generation recipe in Section 4.1 and contextdistillation objective in Section 4.2. As we show in Figure 3, CARTRIDGES trained with this approach can generate responses to many types of queries that match the quality of queries generated with ICL. See Figure 1 for visualization of the CARTRIDGE approach. 4.1 Self-supervised synthetic data to avoid overfitting Towards training general CARTRIDGES, we propose using LLM generated synthetic data to generate our training dataset Dtrain. Overall synthetic data pipeline Our overall pipeline puts information from the corpus in context and prompts the model to have conversation with itself about the corpus to generate the synthetic queryresponse pairs as shown in Algorithm 1. We represent the concatenation of two vectors with y. Algorithm 1 SELF-STUDY: Data Generation Input: : Corpus, : Model Output: {a1, b1, . . . , ak, bk} : Convo 1: chunk(C) 2: get_seed_prompt() 3: for = 1 to do 4: 5: 6: end for 7: return {a1, b1, . . . , ak, bk} ai ( a1 bi1) bi ( a1 bi1 ai) (1) Get subcorpus of that fits in the context window (2) Get prompt to seed the first message from (3) Sample conversation with back and forths (3.1) Sample As message with and in context (3.2) Sample Bs message with in context The conversation is generated by iteratively sampling generations from two LLM participants and (which are the same model). We maintain two different conversation histories: As starts with user message 7 containing seed prompt (e.g. Please start conversation by asking question about the document above.\") followed by alternating assistant and user messages from and B, respectively. Bs conversation history does not include the seed prompt and contains the same messages as As but with the roles of and swapped. Both have the subcorpus in the system prompt. To build training dataset, we sample mtrain independent conversations and concatenate the messages from and into single sequence of tokens: Dtrain = {x(j) = (j) 1 (j) 1 (j) 2 (j) 2 (j) (j) }mtrain j=1 (2) where each x(j) is concatentation of the messages. Note that all of the datasets on which we evaluate in the main paper involve single-turn. So, we set = 1, generating synthetic conversation with one user message and one assistant message. Note that the chunk and get_seed_prompt functions expose two different ways to control the data distribution of the synthetic data. We find that these two design decisions are critical for training high quality CARTRIDGES with SELF-STUDY. Chunking We use short subcorpora (between 512 and 4096) tokens to let the LLM focus on different parts of the corpus when generating data. This is motivated by observations in prior work [52, 57]. Furthermore, chunking also allows us to train CARTRIDGES on corpora longer than the models context window. Seed prompts Instead of using just one seed prompt, we curate list of five different seed prompt types: structuring, summarization, question, use cases, and creative. The full list of seed prompts used in our experiments is provided in Appendix C. Critically, in all our experiments the seed prompts are generic: they do not mention anything related to the specifics of the corpora we evaluated (e.g. no mention of translation for MTOB or medical terms for LongHealth). We use the same set of seed prompts in all of our main results. In Section 5.3, we ablate the use of diverse seed prompts and find that it improves performance over single generic seed prompt by up to 4.8 accuracy points (43.6 48.4 on LONGHEALTH). 4.2 SELF-STUDY context-distillation objective Given fine-tuning dataset Dtrain, we adapt standard techniques from the model distillation literature [42, 43, 72]. We let (x) denote the next token distribution given some input text x. Our teacher is the model with the subcorpus, c, in context (c) and our student is the same model adapted with trainable cache FZ(). We use classic distillation objective [30] that minimizes the KL-divergence between the teacher and student next-token distributions over sequence of tokens and the corresponding subcorpus used to generate them c. arg min (x,c)Dtrain i= (cid:18) DKL (c x[: i]) FZ(x[: i]) (3) (cid:19) In Appendix A, ablate the use of the context-distillation objective and show that improves accuracy when controlling for the amount of synthetic data (e.g. 3.7 accuracy points on LONGHEALTH)."
        },
        {
            "title": "5 Results",
            "content": "We describe experiments evaluating the effectiveness of CARTRIDGES trained with SELF-STUDY in various long-context scenarios. Our results support the following claims. First, CARTRIDGES trained with SELFSTUDY can match or outperform ICL while maintaining generality and reducing serving costs (Section 5.1). Second, SELF-STUDY is effective on corpora longer than the context window of the LLM (Section 5.2). Third, when we concatenate two different CARTRIDGES without any joint training, the model can respond to queries requiring information from both CARTRIDGES (Section 5.4). Finally, we include ablations to assess the relative benefits of different aspects of SELF-STUDY and CARTRIDGES (Section 5.3). Datasets We study datasets consisting of diverse (q, r) pairs about single long document. Across datasets, ranges between 100k and 484k tokens. Our datasets are drawn from popular long-context benchmarks, with some used as-released and others modified to meet this structure. These include: LONGHEALTH [2], MTOB [76], and QASPER [19]. We evaluate LLM response quality using accuracy for LONGHEALTH, 8 Figure 5: Scaling SELF-STUDY compute. These plots show how quality improves as we scale the training compute with SELF-STUDY. In all plots, the x-axis shows the total number of global training steps with batch size 64 and maximum sequence length 1024. No synthetically generated data is reused (i.e. training proceeds for one epoch). Curves are provided for CARTRIDGES of varying sizes (p {128, 512, 2048, 8192}). (Left) The y-axis shows accuracy on LONGHEALTH [2] with LLAMA-8B. (Middle) The y-axis shows the chrF on MTOB [76] with LLAMA-3B. (Right) The y-axis shows log-perplexity (lower is better) on QASPER [19] with LLAMA-3B. log perplexity for QASPER, and character n-gram f-score (chrF) for MTOB [64, 76]. Because each dataset effectively consists of single document, we train single CARTRIDGE per dataset and evaluate it on the queries response pairs (q, r). Appendix provides further details. 5.1 Pushing the quality/cost tradeoff frontier We assess how CARTRIDGES produced with SELF-STUDY fare in quality and memory consumption against baselines for LONGHEALTH and QASPER on LLAMA-3B. For both datasets, fits within the model context window (128k tokens). We compare to traditional ICL, two prompt compression baselines (prompt truncation and prompt summarization using GPT-4o [59]), and state-of-the-art KV cache compression baseline (Duo Attention [38, 84]). We evaluate memory use in terms of KV cache size: the size of the KV cache for the ICL model and prompt compression methods, the size of the CARTRIDGE, and the size of the compressed KV cache for KV cache compression methods like DuoAttention. Figure 4 presents our main results. On both LONGHEALTH and QASPER, we find cache sizes at which CARTRIDGES outperforms ICL. Compared against ICL, CARTRIDGES offers substantial memory savings at comparable performance: up to 10 for LONGHEALTH, and up to 100 for QASPER. In contrast, compression baseline methods see performance degradations at compression factors as low as 2. Crucially, the small memory footprint of CARTRIDGES allows for much higher peak throughput (tokens/s). As Figure 3 (right) shows, cache sizes which match performance of ICL allow for almost 26 higher throughput. We also observe that CARTRIDGE performance scales as we increase the amount of compute used in selfstudy: the longer an CARTRIDGE is trained, the greater task performance. Figure 5 plots the performance for differentially sized CARTRIDGES as function of the number of training steps. Across all sizes, we observe steady positive correlation between performance and compute. 5.2 Extending the effective context window We evaluate whether SELF-STUDY allows us to accurately process corpora that exceed the context window length. To study this, we consider the MTOB dataset, and LLAMA-8B, which has context window of 128k tokens. MTOB provides two different long documents: full 484k token latex textbook and shorter 60k token version, which was manually-curated by the dataset authors to exclude content not relevant to the translation task. Even though the 484k textbook is 356k tokens longer than LLAMA-8Bs context window length, we can produce CARTRIDGE for the full textbook using the chunking strategy of SELF-STUDY. Figure 4 (middle plot) shows the performance of CARTRIDGES of various sizes trained with SELF-STUDY. 9 Figure 6: Ablating CARTRIDGE and SELF-STUDY design choices. Ablations were performed on the MTOB dataset (see Appendix for full ablation experiments). (Left) We train CARTRIDGES using two different parameterizations: simplified prefix-tuning (as described in Section 3.2) and low-rank adaptation (LoRA) [31]. The x-axis shows accuracy on MMLU and the y-axis shows accuracy on the target dataset. Each point represents different CARTRIDGE size. Center We train CARTRIDGES with SELF-STUDY using two loss functions: next token prediction loss (green) and distillation loss (blue). The axis is the number of training steps, and the axis is accuracy. Each hue represents different CARTRIDGE size. (Right) We generate synthetic data according to Algorithm 1 and ablate the choice of seed prompts sampled on Line 2. We consider two approaches: using single, broad seed prompt (Green) or randomly sampling one of five different types of seed prompts (Blue). The axis is the number of training steps, and the axis is accuracy. As point of comparison, we provide the results for KV cache baseline methods on the smaller 60k token textbook, and also include ICL on truncated version of the long textbook. Like above, we observe that CARTRIDGE can match the performance of ICL on the hand-curated 60k token version, while requiring substantially less memory and only having access to the 484k token version, which exceeds the context window of LLAMA-8B. CARTRIDGES also outperform competitive baselines at every KV cache size, by up to 11.0 chrF points. 5.3 Ablating SELF-STUDY design choices We perform ablations to study different aspects of SELF-STUDY and CARTRIDGE parameterization. We provide full results in Appendix and highlight key findings here and in Figure 6. CARTRIDGE Parameterization In Section 3.2, we discuss how we parameterize the CARTRIDGE with trainable KV cache, which is equivalent to simplified version of prefix tuning [47]. There are number of other ways we could parameterize the CARTRIDGE, notably low-rank adaptation (LoRA), an extremely popular parameter effcient fine-tuning method [31]. We compare the prefix-tuning parameterization with LoRA (see Appendix A.1 for full results). First, we find that the prefix-tuning parameterization is more effective than memory-matched LoRA parameterization on queries related to the corpus. For example, with CARTRIDGES of size 0.6 GB on MTOB, prefix-tuning outperforms LoRA by 4.5 ChRF points. (See Figure 8 for results on LONGHEALTH and QASPER.) Even more interesting is the gap between these parameterizations on queries unrelated to the document like MMLU [29]. When using LoRA parameterization, we find that MMLU accuracy drops precipitously (from 54.7 to 45.3) as we increase the CARTRIDGE size (from 0.15 GB to 1.06 GB). In contrast, with prefix-tuning, the accuracy drops much less rapidly (from 54.7 to 54.3) as we increase the size (from 0.15 GB to 0.96 GB). See Figure 8 for plots illustrating these findings on LONGHEALTH, QASPER, and MTOB. We also show that freezing the attention sink (the first token in the key and value vectors) improves training stability (Figure 10). 10 Figure 7: CARTRIDGE Composition. (Left) Illustration of CARTRIDGE composition, where two independently trained CARTRIDGES (one for Pepsi 10-K and one for an AMD 10-K) are concatenated without any additional training. (Middle) We evaluate composition on dataset of multi-document questions requiring information in two different 100k token documents with LLAMA-3B (see Appendix D). The x-axis shows log-perplexity (lower is better) on gold-standard answers. We compare CARTRIDGE composition with an (a) ICL baseline where we truncate the document to fit in the 128k token context length and (b) an CARTRIDGE baseline where we only include the CARTRIDGE for one of the documents. (Right) Examples of responses to multi-document questions using composed cartridges. CARTRIDGE Initialization We compare three different strategies for initializing the KV cache when using the prefix-tuning parameterization: (1) random vectors (from component-wise standard normal distribution), (2) key and value vectors of random tokens, and (3) key and value vectors of the first tokens of the corpus. We find that initializing with key and value vectors of actual tokens (as opposed to random vectors) is critical for achieving ICL-level performance. On LONGHEALTH, random vectors achieve an accuracy of 29.9% while key and value vectors of random tokens achieve an accuracy of 51.3%. Initializing with the first tokens provides an additional improvement of 4 percentage points to 55.3%. In the original prefix-tuning paper, the authors show that initializing from tokens improves performance when performing supervised fine-tuning on very small datasets [47]. Our results extend this finding to SELF-STUDY, where we train on large synthetic datasets. SELF-STUDY Seed Prompts Next, we ablate the choice of seed prompts (see Line 2 of Algorithm 1). We compare two approaches: (1) always using the same seed prompt (Please generate single chat message to begin conversation about the information in the corpus. Ask question about the corpus or make request.\") and (2) randomly sampling one of five different types of seed prompts (e.g. structuring, summarization; see full list in Appendix C). Note even with the latter approach, the seed prompts are generic: the same set of seed prompts are used for all corpora. On MTOB, we find that using this small set of seed prompts improves over the single seed prompt by 7.9 ChRF points (24.1 32.0; see Figure 6 Left). On LONGHEALTH, the improvement is 4.8 accuracy points (43.6 48.4 on LONGHEALTH; see Figure 11). Interestingly, on QASPER we do not see any significant benefit from using the diverse seed prompts. This is perhaps because, compared to LONGHEALTH and MTOB, the queries in QASPER are less reasoning intensive. SELF-STUDY Objective Finally, we evaluate the importance of the context distillation objective (defined in Section 4.2). Using the same SELF-STUDY synthetic data for both objectives, we compare the contextdistillation objective with simpler next-token prediction objective. On MTOB, we find that using context distillation objective on the synthetic conversation data improves ChRF by 8.6 points (24.9 33.5; see Figure 12 Center). We also see improvements on LONGHEALTH and QASPER (see Figure 12). 5.4 Composing CARTRIDGES We evaluate if independently trained CARTRIDGES can be composed in order to serve queries about two different corpora (see Figure 7, Left). We train CARTRIDGES across sizes {512, 1024, 2048, 4096} and long 10-K documents from AMD, Pepsi, AMEX, and Boeing [33]. For each pair of CARTRIDGES pairwise (6 pairs per cache size), we evaluate using dataset of multi-document questions, i.e., requiring information from both 10-Ks. Surprisingly, we find composition not only leads to coherent LLM generations off-the-shelf without any re-training (Figure 7, Right), but also substantially outperforms the use of single CARTRIDGE (i.e. for 11 only AMD) or ICL (which struggles due to context length limits) (Figure 7, Center) on the multi-document questions."
        },
        {
            "title": "6 Discussion and conclusion",
            "content": "We propose CARTRIDGES as an alternative to ICL for settings where many different user messages reference the same large corpus of text. We demonstrate across diverse set of language model workloads that, when trained via SELF-STUDY, they match ICLs response quality while substantially reducing memory consumption (38.6 memory reduction across our evaluations) and increasing peak throughput (26.4 higher tokens per second). CARTRIDGES are simple to train, composable, and compatible with existing LLM serving infrastructure. However, compared with ICL, SELF-STUDY is not without limitations. Using SELF-STUDY to produce KVcache is much more costly than simply running standard ICL pre-fill. With our unoptimized implementation, training an ICL-quality CARTRIDGE takes 30 minutes on single 8H100 node (for LLAMA-8B) So our work does not provide drop-in replacement for ICL, but rather demonstrates one way to tradeoff increased compute for reduced memory when constructing KV-cache. This tradeoff is extremely advantageous in many settings: users often issue many queries over the same corpus and SELF-STUDY can be trained offline on idle or underutilized compute (e.g. at night when user load is low [25, 34]). Furthermore, there is ample room for optimizations (e.g. improved shared-prefix attention kernels [18, 39, 90]) that would make SELF-STUDY training procedure more efficient. Looking forward, we envision CARTRIDGES enabling broad class of context-aware AI applications that are intractable with ICL today, from medical assistants that know patients full medical history to LLM-powered IDEs that understand entire codebases. Acknowledgments We thank Jordan Juravsky, Dan Biderman, Bradley Brown, Mayee Chen, Avanika Narayan, Avner May, Bill Mark, Benjamin Spector, Roberto Garcia, Quinn Mcintyre, Yasa Baig, Geoff Angus, Kelly Buchanan, Mert Yuksekgonul, Eric Nguyen, Eric Wu, Kevin Wu, Owen Dugan, Jon Saad-Falcon, Simon Guo and the entire Zou, Hazy, and Scaling Intelligence research labs for helpful discussions and feedback. We gratefully acknowledge Modal, Prime Intellect, Voltage Park, and Together AI for providing the GPUs to support for this work. We gratefully acknowledge the support of NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF2247015 (Hardware-Aware), CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); US DEVCOM ARL under Nos. W911NF-23-2-0184 (Longcontext) and W911NF-21-2-0251 (Interactive Human-AI Teaming); ONR under Nos. N000142312633 (Deep Signal Processing); Stanford HAI under No. 247183; NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, Google Cloud, Salesforce, Total, the HAI-GCP Cloud Credits for Research program, the Stanford Data Science Initiative (SDSI), members of the Stanford SEAMS project: IBM and Felicis, as well as members of the Stanford DAWN project: Meta, Google, and VMWare. SE is supported by the NSF Graduate Research Fellowship Program. ARs research is supported by NSF grant CCF#2247014. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of NIH, ONR, or the U.S. Government. Contributions SE and RE conceived of CARTRIDGES and SELF-STUDY. SE, RE, and SA designed the method, implemented the experiments, wrote the manuscript, and contributed equally to the project. NG made substantial contributions to the structure of the project and the final manuscript. EL and DZ implemented and ran experiments and made meaningful contributions to the manuscript. WT implemented the LoRA baselines. DZ and AR led the theoretical analysis. AR, JZ, AM, and CR supervised the project."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Jyoti Aneja, Harkirat Behl, SÃ©bastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell Hewett, Mojan Javaheripi, Piero Kauffmann, et al. Phi-4 technical report. arXiv preprint arXiv:2412.08905, 2024. [2] Lisa Adams, Felix Busch, Tianyu Han, Jean-Baptiste Excoffier, Matthieu Ortala, Alexander LÃ¶ser, Hugo JWL Aerts, Jakob Nikolas Kather, Daniel Truhn, and Keno Bressem. Longhealth: question answering benchmark with long clinical documents. arXiv preprint arXiv:2401.14490, 2024. [3] Joshua Ainslie, James Lee-Thorp, Michiel De Jong, Yury Zemlyanskiy, Federico LebrÃ³n, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. [4] Anthropic. The Claude 3 Model Family: Opus, Sonnet, Haiku. arXiv preprint, 2024. [5] Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri Rudra, and Christopher RÃ©. Zoology: Measuring and improving recall in efficient language models, 2023. [6] Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri Rudra, and Christopher RÃ©. Simple linear attention language models balance the recallthroughput tradeoff. arXiv preprint arXiv:2402.18668, 2024. [7] Simran Arora and Christopher RÃ©. Can foundation models help us achieve perfect secrecy? arXiv preprint arXiv:2205.13722, 2022. [8] Maximilian Beck, Korbinian PÃ¶ppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, GÃ¼nter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. xlstm: Extended long short-term memory. arXiv preprint arXiv:2405.04517, 2024. [9] Iz Beltagy, Matthew Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. [10] Aman Bhargava, Cameron Witkowski, Alexander Detkov, and Matt Thomson. Prompt baking. arXiv preprint arXiv:2409.13697, 2024. [11] Chi-Chih Chang, Wei-Cheng Lin, Chien-Yu Lin, Chong-Yan Chen, Yu-Fang Hu, Pei-Shuo Wang, NingChi Huang, Luis Ceze, Mohamed Abdelfattah, and Kai-Chiang Wu. Palu: Compressing kv-cache with low-rank projection. arXiv preprint arXiv:2407.21118, 2024. [12] Vivek Chari, Guanghui Qin, and Benjamin Van Durme. Kv-distill: Nearly lossless learnable context compression for llms. arXiv preprint arXiv:2503.10337, 2025. [13] Lequn Chen, Zihao Ye, Yongji Wu, Danyang Zhuo, Luis Ceze, and Arvind Krishnamurthy. Punica: Multi-tenant lora serving. Proceedings of Machine Learning and Systems, 6:113, 2024. [14] Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts. arXiv preprint arXiv:2305.14788, 2023. [15] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. [16] Yu-Neng Chuang, Tianwei Xing, Chia-Yuan Chang, Zirui Liu, Xun Chen, and Xia Hu. Learning to compress prompt in natural language formats. arXiv preprint arXiv:2402.18700, 2024. [17] Pierre Colombo, Telmo Pessoa Pires, Malik Boudiaf, Dominic Culver, Rui Melo, Caio Corro, Andre FT Martins, Fabrizio Esposito, Vera LÃºcia Raposo, Sofia Morgado, et al. Saullm-7b: pioneering large language model for law. arXiv preprint arXiv:2403.03883, 2024. 13 [18] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher RÃ©. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in neural information processing systems, 35:16344 16359, 2022. [19] Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah Smith, and Matt Gardner. dataset of information-seeking questions and answers anchored in research papers. arXiv preprint arXiv:2105.03011, 2021. [20] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Tianyu Liu, et al. survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022. [21] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The Llama 3 Herd of Models. arXiv preprint arXiv:2407.21783, 2024. [22] Saumya Gandhi, Ritu Gala, Vijay Viswanathan, Tongshuang Wu, and Graham Neubig. Better synthetic data by retrieving and transforming existing datasets. arXiv preprint arXiv:2404.14361, 2024. [23] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [24] Tao Ge, Jing Hu, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context compression in large language model. arXiv preprint arXiv:2307.06945, 2023. [25] Kanishk Goel, Jayashree Mohan, Nipun Kwatra, Ravi Shreyas Anupindi, and Ramachandran Ramjee. Niyama: Breaking the silos of llm inference serving. arXiv preprint arXiv:2503.22562, 2025. [26] Yunhao Gou, Zhili Liu, Kai Chen, Lanqing Hong, Hang Xu, Aoxue Li, Dit-Yan Yeung, James Kwok, and Yu Zhang. Mixture of cluster-conditional lora experts for vision-language instruction tuning. arXiv preprint arXiv:2312.12379, 2023. [27] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [28] Neel Guha, Julian Nyarko, Daniel Ho, Christopher RÃ©, Adam Chilton, Alex Chohlas-Wood, Austin Peters, Brandon Waldon, Daniel Rockmore, Diego Zambrano, et al. Legalbench: collaboratively built benchmark for measuring legal reasoning in large language models. Advances in Neural Information Processing Systems, 36:4412344279, 2023. [29] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. [30] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. [31] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [32] Chengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, and Min Lin. Lorahub: Efficient cross-task generalization via dynamic lora composition. arXiv preprint arXiv:2307.13269, 2023. [33] Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, and Bertie Vidgen. Financebench: new benchmark for financial question answering. arXiv preprint arXiv:2311.11944, 2023. [34] Shashwat Jaiswal, Kunal Jain, Yogesh Simmhan, Anjaly Parayil, Ankur Mallick, Rujia Wang, Renee St Amant, Chetan Bansal, Victor RÃ¼hle, Anoop Kulkarni, et al. Serving models, fast and slow: optimizing heterogeneous llm inferencing workloads at scale. arXiv preprint arXiv:2502.14617, 2025. 14 [35] Dulhan Jayalath, James Bradley Wendt, Nicholas Monath, Sandeep Tata, and Beliz Gunel. Longrange tasks using short-context llms: Incremental reasoning with structured memories. arXiv preprint arXiv:2412.18914, 2024. [36] Fengqing Jiang. Identifying and mitigating vulnerabilities in llm-integrated applications. Masters thesis, University of Washington, 2024. [37] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Llmlingua: Compressing prompts for accelerated inference of large language models. arXiv preprint arXiv:2310.05736, 2023. [38] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. LLMLingua: Compressing prompts for accelerated inference of large language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1335813376, Singapore, December 2023. Association for Computational Linguistics. [39] Jordan Juravsky, Bradley Brown, Ryan Ehrlich, Daniel Y. Fu, Christopher RÃ©, and Azalia Mirhoseini. Hydragen: High-throughput llm inference with shared prefixes, 2024. [40] Jordan Juravsky, Ayush Chakravarthy, Ryan Ehrlich, Sabri Eyuboglu, Bradley Brown, Joseph Shetaye, Christopher RÃ©, and Azalia Mirhoseini. Tokasaurus: An llm inference engine for high-throughput workloads. https://scalingintelligence.stanford.edu/blogs/tokasaurus/, 2025. [41] Junhyuck Kim, Jongho Park, Jaewoong Cho, and Dimitris Papailiopoulos. Lexico: Extreme kv cache compression via sparse coding over universal dictionaries. arXiv preprint arXiv:2412.08890, 2024. [42] Yoon Kim and Alexander Rush. Sequence-level knowledge distillation. In Proceedings of the 2016 conference on empirical methods in natural language processing, pages 13171327, 2016. [43] Kalle KujanpÃ¤Ã¤, Harri Valpola, and Alexander Ilin. Knowledge injection via prompt distillation. arXiv preprint arXiv:2412.14964, 2024. [44] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611626, 2023. [45] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021. [46] Dengchun Li, Yingzi Ma, Naizheng Wang, Zhengmao Ye, Zhiyuan Cheng, Yinghao Tang, Yan Zhang, Lei Duan, Jie Zuo, Cal Yang, et al. Mixlora: Enhancing large language models fine-tuning with lora-based mixture of experts. arXiv preprint arXiv:2404.15159, 2024. [47] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 45824597, Online, August 2021. Association for Computational Linguistics. [48] Yucheng Li. Unlocking context constraints of llms: Enhancing context efficiency of llms with selfinformation-based content filtering. arXiv preprint arXiv:2304.12102, 2023. [49] Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. Snapkv: Llm knows what you are looking for before generation. Advances in Neural Information Processing Systems, 37:2294722970, 2024. [50] Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434, 2024. 15 [51] Akide Liu, Jing Liu, Zizheng Pan, Yefei He, Gholamreza Haffari, and Bohan Zhuang. Minicache: Kv cache compression in depth dimension for large language models. Advances in Neural Information Processing Systems, 37, 2024. [52] Nelson Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173, 2024. [53] Yansheng Mao, Yufei Xu, Jiaqi Li, Fanxu Meng, Haotong Yang, Zilong Zheng, Xiyuan Wang, and Muhan Zhang. Lift: Improving long context understanding of large language models through long input fine-tuning. arXiv preprint arXiv:2502.14644, 2025. [54] Fanxu Meng, Zhaohui Wang, and Muhan Zhang. Pissa: Principal singular values and singular vectors adaptation of large language models. Advances in Neural Information Processing Systems, 37:121038 121072, 2024. [55] Jesse Mu, Xiang Li, and Noah Goodman. Learning to compress prompts with gist tokens. Advances in Neural Information Processing Systems, 36:1932719352, 2023. [56] Daye Nam, Andrew Macvean, Vincent Hellendoorn, Bogdan Vasilescu, and Brad Myers. Using an llm to help with code understanding. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering, pages 113, 2024. [57] Avanika Narayan, Dan Biderman, Sabri Eyuboglu, Avner May, Scott Linderman, James Zou, and Christopher Re. Minions: Cost-efficient collaboration between on-device and cloud language models. arXiv preprint arXiv:2502.15964, 2025. [58] Nihal Nayak, Yiyang Nan, Avi Trost, and Stephen Bach. Learning to generate instruction tuning datasets for zero-shot task adaptation. arXiv preprint arXiv:2402.18334, 2024. [59] OpenAI. Gpt-4o system card, 2024. [60] Matanel Oren, Michael Hassid, Nir Yarden, Yossi Adi, and Roy Schwartz. Transformers are multi-state rnns. arXiv preprint arXiv:2401.06104, 2024. [61] Lisa Larrimore Ouellette, Amy Motomura, Jason Reinecke, and Jonathan Masur. Can ai hold office hours? Available at SSRN 5166938, 2025. [62] Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, Shishir Patil, Ion Stoica, and Joseph Gonzalez. Memgpt: Towards llms as operating systems. arXiv preprint arXiv:2310.08560, 2023. [63] Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor RÃ¼hle, Yuqing Yang, Chin-Yew Lin, et al. Llmlingua-2: Data distillation for efficient and faithful task-agnostic prompt compression. arXiv preprint arXiv:2403.12968, 2024. [64] Maja Popovic. chrf: character n-gram f-score for automatic mt evaluation. In Proceedings of the tenth workshop on statistical machine translation, pages 392395, 2015. [65] Guanghui Qin, Corby Rosset, Ethan Chau, Nikhil Rao, and Benjamin Van Durme. Dodo: Dynamic contextual compression for decoder-only lms. arXiv preprint arXiv:2310.02409, 2023. [66] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016. [67] Haris Riaz, Sourav Bhabesh, Vinayak Arannil, Miguel Ballesteros, and Graham Horwood. Metasynth: Meta-prompting-driven agentic scaffolds for diverse synthetic data generation. arXiv preprint arXiv:2504.12563, 2025. [68] Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo Luschi, and Douglas Orr. Sparq attention: Bandwidth-efficient llm inference. arXiv preprint arXiv:2312.04985, 2023. 16 [69] Melisa Russak, Umar Jamil, Christopher Bryant, Kiran Kamble, Axel Magnuson, Mateusz Russak, and Waseem AlShikh. Writing in the margins: Better inference pattern for long context retrieval. arXiv preprint arXiv:2408.14906, 2024. [70] Utkarsh Saxena, Gobinda Saha, Sakshi Choudhary, and Kaushik Roy. Eigen attention: Attention in low-rank space for kv cache compression. arXiv preprint arXiv:2408.05646, 2024. [71] Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150, 2019. [72] Charlie Snell, Dan Klein, and Ruiqi Zhong. Learning by distilling context. arXiv preprint arXiv:2209.15189, 2022. [73] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. [74] Weihang Su, Yichen Tang, Qingyao Ai, Junxi Yan, Changyue Wang, Hongning Wang, Ziyi Ye, Yujia Zhou, and Yiqun Liu. Parametric retrieval augmented generation. arXiv preprint arXiv:2501.15915, 2025. [75] Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, and Song Han. Quest: Queryaware sparsity for efficient long-context llm inference. arXiv preprint arXiv:2406.10774, 2024. [76] Garrett Tanzer, Mirac Suzgun, Eline Visser, Dan Jurafsky, and Luke Melas-Kyriazi. benchmark for learning to translate new language from one grammar book. arXiv preprint arXiv:2309.16575, 2023. [77] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, LÃ©onard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre RamÃ©, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. [78] U.S. Securities and Exchange Commission. How to read 10-k, 2011. Accessed: 2025-05-14. [79] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [80] Zhongwei Wan, Xinjian Wu, Yu Zhang, Yi Xin, Chaofan Tao, Zhihong Zhu, Xin Wang, Siqi Luo, Jing Xiong, and Mi Zhang. D2o: Dynamic discriminative operations for efficient generative inference of large language models. arXiv preprint arXiv:2406.13035, 2024. [81] Zheng Wang, Boxiao Jin, Zhongzhi Yu, and Minjia Zhang. Model tells you where to merge: Adaptive kv cache merging for llms on long-context tasks. arXiv preprint arXiv:2407.08454, 2024. [82] Xun Wu, Shaohan Huang, and Furu Wei. Mixture of lora experts. arXiv preprint arXiv:2404.13628, 2024. [83] Chaojun Xiao, Zhengyan Zhang, Chenyang Song, Dazhi Jiang, Feng Yao, Xu Han, Xiaozhi Wang, Shuo Wang, Yufei Huang, Guanyu Lin, et al. Configurable foundation models: Building llms from modular perspective. arXiv preprint arXiv:2409.02877, 2024. [84] Guangxuan Xiao, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Haotian Tang, Yao Fu, and Song Han. Duoattention: Efficient long-context llm inference with retrieval and streaming heads. arXiv preprint arXiv:2410.10819, 2024. [85] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks, 2024. [86] Prateek Yadav, Colin Raffel, Mohammed Muqeeth, Lucas Caccia, Haokun Liu, Tianlong Chen, Mohit Bansal, Leshem Choshen, and Alessandro Sordoni. survey on model moerging: Recycling and routing among specialized experts for collaborative learning. arXiv preprint arXiv:2408.07057, 2024. 17 [87] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [88] Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear transformers with the delta rule over sequence length. arXiv preprint arXiv:2406.06484, 2024. [89] Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear transformers with the delta rule over sequence length, 2025. [90] Zihao Ye, Lequn Chen, Ruihang Lai, Wuwei Lin, Yineng Zhang, Stephanie Wang, Tianqi Chen, Baris Kasikci, Vinod Grover, Arvind Krishnamurthy, and Luis Ceze. Flashinfer: Efficient and customizable attention engine for llm inference serving. arXiv preprint arXiv:2501.01005, 2025. [91] Howard Yen. Long-context language modeling with parallel context encoding. Masters thesis, Princeton University, 2024. [92] Hao Yu, Zelan Yang, Shen Li, Yong Li, and Jianxin Wu. Effectively compress kv heads for llm. arXiv preprint arXiv:2406.07056, 2024. [93] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:1728317297, 2020. [94] Qianchi Zhang, Hainan Zhang, Liang Pang, Hongwei Zheng, and Zhiming Zheng. Adacomp: Extractive context compression with adaptive predictor for retrieval-augmented large language models. arXiv preprint arXiv:2409.01579, 2024. [95] Rongzhi Zhang, Kuang Wang, Liyuan Liu, Shuohang Wang, Hao Cheng, Chao Zhang, and Yelong Shen. Lorc: Low-rank compression for llms kv cache with progressive compression strategy. arXiv preprint arXiv:2410.03111, 2024. [96] Yifan Zhang, Yifeng Liu, Huizhuo Yuan, Zhen Qin, Yang Yuan, Quanquan Gu, and Andrew Chi-Chih Yao. Tensor product attention is all you need. arXiv preprint arXiv:2501.06425, 2025. [97] Yuxin Zhang, Yuxuan Du, Gen Luo, Yunshan Zhong, Zhenyu Zhang, Shiwei Liu, and Rongrong Ji. Cam: Cache merging for memory-efficient llms inference. In Forty-first International Conference on Machine Learning, 2024. [98] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher RÃ©, Clark Barrett, et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36:3466134710, 2023. [99] Ziyu Zhao, Leilei Gan, Guoyin Wang, Wangchunshu Zhou, Hongxia Yang, Kun Kuang, and Fei Wu. Loraretriever: Input-aware lora retrieval and composition for mixed tasks in the wild. arXiv preprint arXiv:2402.09997, 2024. [100] Ziyu Zhao, Tao Shen, Didi Zhu, Zexi Li, Jing Su, Xuwu Wang, Kun Kuang, and Fei Wu. Merging loras like playing lego: Pushing the modularity of lora to extremes through rank-wise clustering. arXiv preprint arXiv:2409.16167, 2024. [101] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Livia Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph Gonzalez, et al. Sglang: Efficient execution of structured language model programs. Advances in Neural Information Processing Systems, 37:6255762583, 2024. [102] Lucia Zheng, Neel Guha, Javokhir Arifov, Sarah Zhang, Michal Skreta, Christopher Manning, Peter Henderson, and Daniel Ho. reasoning-focused legal retrieval benchmark. In Proceedings of the 2025 Symposium on Computer Science and Law, pages 169193, 2025. 18 [103] Yuhao Zhou, Sirui Song, Boyang Liu, Zhiheng Xi, Senjie Jin, Xiaoran Fan, Zhihao Zhang, Wei Li, and Xuanjing Huang. Elitekv: Scalable kv cache compression via rope frequency selection and joint low-rank projection. arXiv preprint arXiv:2503.01586, 2025. Figure 8: Comparing CARTRIDGE parameterizations. We train CARTRIDGES using SELF-STUDY on the corpora from LONGHEALTH (Top), QASPER (Middle), and MTOB (Bottom) using two different parameterizations: simplified prefix-tuning (as described in Section 3.2) and low-rank adaptation (LoRA) [31]. We experiment with different CARTRIDGE sizes and choose LoRA rank and prefix-tuning cache size to align on memory consumption. We evaluate the performance of the CARTRIDGES on questions from the target dataset (LONGHEALTH or QASPER) using the same protocol as in Figure 4 and also on questions from MMLU [29] that are unrelated to the corpora. (Left) The x-axis shows accuracy on MMLU and the y-axis shows accuracy on the target dataset. Each point represents different CARTRIDGE size. (Center) The x-axis shows CARTRIDGE size in GB, and the y-axis shows accuracy on MMLU. (Right) The x-axis shows self-study duration in training steps, and the y-axis shows accuracy on MMLU. The shade of the points represents the size of the CARTRIDGE."
        },
        {
            "title": "A Extended Results",
            "content": "In this section, we ablate the main design choices of CARTRIDGES and SELF-STUDY. 20 A.1 CARTRIDGE design choices: parameterization and initialization In our experiments, we parameterize the CARTRIDGE with simplified version of prefix-tuning and initialize with truncated KV-cache (see Section 3.2). In this section, we describe ablation experiments motivating these design choices. First, we compare two different CARTRIDGE parameterizations (Figure 8): simplified prefix-tuning [47] and low-rank adaptation (LoRA) [31]. Then, we demonstrate the importance of proper CARTRIDGE initialization (Figure 9). Parameterization We evaluate CARTRIDGES trained on corpora from LONGHEALTH or QASPER on both in-domain (i.e. questions from LONGHEALTH or QASPER) and out-of-domain (i.e. questions from an unrelated benchmark, MMLU [29]) queries. We find that the prefix-tuning parameterization is more effective than memory-matched LoRA parameterization on both in-domain and out-of-domain queries. This is illustrated in Figure 8 (Left), where we see that prefix-tuning occupies the top-right corner of the plot (high accuracy on both MMLU and the target dataset). Notably, we find that as we increase the CARTRIDGE size with LoRA tuning, performance on out-of-domain queries (MMLU) drops significantly. At 1.06 GB (LoRA rank 1632), MMLU accuracy drops from 60.0% to 45.3%. This drop in performance is highly correlated with the size of the CARTRIDGE, suggesting that LoRA is not well-suited to large Cartridges, which we show in Figure 4 are important for recovering ICL performance. In contrast, with prefix-tuning the accuracy only drops to 54.3% at 1.06 GB. This degradation is mostly invariant to the size of the CARTRIDGE (54.7% at 0.15 GB), demonstrating that out-of-domain performance is robust across CARTRIDGE sizes. On in-domain queries, prefix-tuning also outperforms LoRA, but the gap is smaller. Across all CARTRIDGE sizes, the best LONGHEALTH accuracy prefix-tuning achieves is 55.6% at 0.96 GB, while the best LoRA accuracy is 47.25% at 0.26 GB. Interestingly, LoRA accuracy at the largest CARTRIDGE sizes is lower; 41.3% at 0.96. It is possible that this is due to the out-of-domain degradation of LoRA we discussed above. Since queries in LONGHEALTH test set are quite different from the synthetic queries generated by SELF-STUDY (e.g. they are multiple choice and require some complicated reasoning traces), out-of-domain robustness may be also important for in-domain performance. It isnt clear why prefix-tuning is so much more robust than LoRA to out-of-domain performance degradation. It is surprising given the similarity between KV-cache and an MLP both are linear transformations separated by non-linearity. It is possible that this is due to the difference in the activation function (SiLU vs. Softmax). We leave more detailed investigation into the root cause of this difference for future work. Initialization The standard way of initializing token CARTRIDGE in our main paper is using the KV cache from the first tokens of the source document. In Figure 9, we ablate different initialization source. We try two additional initalizations: random vectors and random tokens. For random vectors, we simply initialize the parameters of the CARTRIDGE from component-wise standard normal distribution. For random tokens, we initialize the CARTRIDGE as the KV cache of the first tokens of arbitrary text (specifically, the Wikipedia page for gradient). The important difference between the these two strategies is that for random tokens the initial CARTRIDGE is \"valid\" KV cache produced by the model, while for random vectors it is not. Freezing the attention sink small yet important detail of training CARTRIDGE is that we do not let the first tokens key and value vectors to be trainable. As studied in [85], the first key vector, which corresponds to the beginning of sequence token and is thus the same for every sequence, acts as an \"attention sink\". We observed that when training CARTRIDGE, allowing those key and value vectors to be trainable led to training instability (see Figure 10). For example, on some runs the MMLU accuracy would dip to below 30%. 21 Figure 9: Ablating CARTRIDGE initalization. We train CARTRIDGES using SELF-STUDY on the corpora from LONGHEALTH with 3 different initialization strategies. The axis is the number of training steps and the axis is the accuracy on LONGHEALTH. The blue lines are the results when initializing the CARTRIDGE using the KV cache from the first tokens of the document. The purple lines are initializing the CARTRIDGE from the KV cache of unrelated text. The green lines is initializing the CARTRIDGE with random vectors. Initializing from the first tokens leads to slightly stronger results than initializing from the KV cache of random text. This difference may be more prominent on other corpora where the first tokens are more relevant to solving the downstream task. Figure 10: Freezing the attention sink. In both plots, the y-axis is accuracy and the x-axis is training step. The green line which corresponds to run where we allow trainable first token. (Left) The y-axis MMLU accuracy. This plot exemplifies the training instability we observed when the key and value vectors were trainable. The MMLU score dips to below 30% before recovering. (Left) The y-axis is accuracy on questions from LONGHEALTH. A.2 SELF-STUDY design choices: data-generation and objective In SELF-STUDY training we use seeded data-generation process and context-distillation training objective (see Section 4). In this section, we ablate these design choices, comparing against the performance of SELF-STUDY with simpler data-generation and objectives. Data Generation In Section 4.1, we describe how we use five different seed prompt types when generating data with Algorithm 1. These prompt types, structuring, summarization, question, use cases, and creative, are described in more detail in Appendix C.1. 22 In this section, we compare the performance of SELF-STUDY with these five prompt types against SELFSTUDY with single prompt: Please generate single chat message to begin conversation about the information in the corpus. Ask question about the corpus or make request.\" Across three datasets, we find that using the five different prompt types during SELF-STUDY leads to higher quality CARTRIDGES (see Figure 12). On MTOB with CARTRIDGES of size 1024 tokens, we see 7.9 point ChRF improvement (24.1 32.0). On LONGHEALTH, the improvement is 5.5 accuracy points (45.8 51.3). Interestingly, on QASPER, we see no benefit from using the five different prompt types. It is possible this is because the queries in the QASPER dataset are mostly factual questions that do not require complex reasoning like LONGHEALTH and MTOB do. Figure 11: Diverse seed prompts improve quality. We generate synthetic data according to Algorithm 1 and ablate the choice of seed prompts sampled on Line 2. We consider two approaches: using single, broad seed prompt (Green) or randomly sampling one of five different types of seed prompts (Blue). We train CARTRIDGES using self-study with these two strategies on LONGHEALTH, MTOB and QASPER corpora. In all plots, the axis is the number of training steps, and the axis is either accuracy (for LONGHEALTH and MTOB) or perplexity on ground truth answer (for QASPER). We use an CARTRIDGE size of 1024 tokens. Training Objective In Section 4, we describe the context-distillation objective we use [10, 42, 72]. This approach requires that we collect top output probabilities from the in-context models output distribution during data generation. simpler alternative would be to just use next-token prediction objective with cross-entropy loss. In our comparison, we find that this simpler objective underperforms the context-distillation objective (see Figure 12). Most notably, on MTOB with 2048 token CARTRIDGES, context-distillation outperforms next-token prediction by 8.3 ChRF points (24.9 33.2). On LongHealth, the gap is 3.7 accuracy points (47.6 51.3). As shown in Figure 12, quality seems to be consistently improving with more SELF-STUDY compute. It is possible, therefore, that by spending more during SELF-STUDY with the next-token prediction objective, we could close the gap. However, for fixed amount of SELF-STUDY compute, context-distillation is considerably more effective. These results demonstrate how context-distillation plays an important role in efficiently recovering ICL performance with SELF-STUDY. A.3 Throughput measurement details We provide details for the throughput measurements in Figure 3. We use the state-of-the-art SGLang inference system, with default parameters [101]. We measure throughput on single H100 GPU. We first determine the largest batch size that fits in GPU memory, given cache of size tokens. We then randomly initialize CARTRIDGES of size and pre-load the CARTRIDGES into GPU memory. We finally 23 Figure 12: Context-distillation objective improves training efficiency. We train CARTRIDGES using SELFSTUDY on the corpora from LONGHEALTH (Left), MTOB (Center) and QASPER (Right) using two loss functions: next token prediction loss (green) and distillation loss (blue). We evaluate the performance of the CARTRIDGES on questions from the target dataset (LONGHEALTH, MTOB or QASPER) using the same protocol as in Figure 5. In all plots, the axis is the number of training steps, and the axis is either accuracy (for LONGHEALTH and MTOB) or perplexity on ground truth answer (for QASPER). The shade of the points represents the size of the CARTRIDGE. Using distillation loss achieves higher accuracy (or lower perplexity for QASPER) across datasets and CARTRIDGE sizes. measure the time taken to decode 128 tokens per sequence. The CARTRIDGES and decoded tokens are appended to KV-cache during generation. We report the average of 5 iterations after using 3 warm-up iterations."
        },
        {
            "title": "B Extended Related Work",
            "content": "In this section, we provide more in-depth discussion of the place our work occupies in the broader literature. The structure below mirrors the structure of our paper: first we discuss work related to the parameterization and initialization of CARTRIDGES (Appendix B.1), then we cover work that inspired the design of SELFSTUDY (Appendix B.2), and finally we describe other approaches aimed at reducing the size of the KV-cache, many of which we compare against in our experiments (Appendix B.3). B.1 Prior work related to the parameterization of CARTRIDGES Below we discuss prior work from the parameter-efficient fine-tuning literature that inform the way we parameterize CARTRIDGES in our work. B.1.1 Parameter-efficient Fine-tuning (PEFT) In order to adapt large language models (LLMs) to particular domains or tasks in more compute and memory-efficient manner, several parameter-efficient fine-tuning (PEFT) methods have been developed. Some of the most widely used PEFT methods include Low-Rank Adaptation (LoRA) [31], prefix-tuning [47], and prompt-tuning [45]. Leveraging prior observations that fine-tuned language models exhibit an intrinsic low rank structure, Hu et al. propose LoRA, which freezes model parameters and injects trainable rank decomposition matrices between each transformer layer. LoRA exhibits on-par or better fine-tuning quality while reducing the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times [31]. Li et al. and Lester et al. both take different approach to lightweight fine-tuning, proposing tunable \"prefixes\" and \"soft prompts\" respectively to prepend to queries in order to steer the model to desired outputs. Li et al. proposes prefix-tuning, which learns continuous representation for the activation of the 24 prefix at each transformer layer. These learned activations are then prepended to activations obtained by passing the input prompt through the frozen transformer. In contrast, Lester et al. proposes prompt-tuning, which optimizes at the discrete token level and prepends series of learnable tokens to the input prompt. Both methods show strong performance while greatly reducing the number of learnable parameters and improving compute and memory efficiency for language model adaptation. Principal Singular values and Singular vectors Adaptation (PiSSA) [54] is another more recent PEFT method that attempts to ameliorate the slow convergence problems of LoRA. PiSSA initializes the LoRA rank decomposition matrices with the principal components of the original matrix, and exhibits faster convergence and enhanced performance compared to LoRA on several tasks, including GSM8K and MATH. Several of these methods, especially LoRA, have been adapted specifically for distilling knowledge provided in context into the parameters of language model. Some of those methods are described in the sections below, and this work is an extension of prefix-tuning for long-context tasks. B.1.2 Parameter-efficient Adapter Composition and Merging number of works have explored the idea of composing multiple different parameter-efficient adapters (e.g. LoRAs) by summing them together, concatenating them, or using dynamic mixture of experts [26, 32, 46, 82, 83, 86, 99, 100]. For example, Huang et al. propose LoraHub, framework for dynamically weighting and composing multiple language model adapters [32]. Given set of LoRA modules for different upstream tasks and new unseen task with in-context examples, LoraHub dynamically weights the LoRAs and composes new LoRA module for the task. Similarly, Zhao et al. propose method for dynamically retrieving the most relevant language model LoRAs for given task [99]. B.1.3 Parametric Knowledge Injection Several recent works have explored methods for integrating external knowledge directly into model parameters, known as parametric knowledge injection [43, 53, 74]. To the best of our knowledge, these studies are the closest in scope to ours. Like ours, these works address the problem of parametric knowledge injection: how to store large text corpora within parameters of language model. Some use simple synthetic data generation pipelines or context-distillation objectives. Unlike our work, these studies do not highlight the memory reduction and throughput advantages of parametric knowledge injection techniques. We highlight other differences below. One parametric knowledge injection method, recently proposed by Kujanpaa et al., is prompt distillation, in which teacher model with access to privileged knowledge generates question-answer pairs. These pairs are then used to train LoRA adapter for student model (identical to the teacher model, but without access to privileged information) using distillation objective (i.e. mimicking the teachers full token distribution) [43]. This closely resembles our context-distillation objective, which we also found works better than next-token prediction. However, unlike our work, Kujanpaa et al. only train LoRA adapters of single size (rank 1024) and dont assess memory reductions with respect to full in-context learning. Indeed, they do not evaluate against long-context ICL baselines at all, focusing instead on comparison with RAG. Furthermore, they evaluate on relatively simple long-context setting concatenation of SQUAD passages [66] which does not exhibit long range dependencies or require reasoning the way MTOB and LONGHEALTH do. Similarly, Mao et al. propose Long Input Fine-tuning (LIFT), which fine-tunes language model using typical next-token prediction objective on overlapping segments of the corpus, as well as instruction tuning on question answer pairs generated from the corpus. Unlike our work, Mao et al. find that synthetic Q/A pairs offer minimal benefit and can even degrade performance due to overfitting\" [53]. The difference in our findings is perhaps due to the fact that they only generate ten synthetic examples, whereas we generate tens of thousands. Furthermore, they use weaker ICL baseline (Llama 3 8B) that only has 8k tokens of context. Any contexts longer than 8k tokens are truncated before being fed to the ICL baseline. Finally, Su et al. proposes Parametric Retrieval Augmented Generation (Parametric RAG), in which each document has corresponding LoRA adapter, trained on an augmented dataset consisting of the document, 25 rewritten versions of the document, and question-answer pairs generated from the document. At inference time, retriever is used to determine relevants documents, and the corresponding LoRA adapters are merged [74]. This method demonstrates significant gains over RAG on variety of tasks, including WikiMultihopQA. B.2 Prior work related to SELF-STUDY B.2.1 Self Distillation and Context Distillation Self-distillation is another method used to internalize the performance gains provided by information in context (e.g. scratchpads, informative instructions) into the model parameters. In \"Learning by Distilling Context\", the authors distill model with instructions and scratchpads in context into parameters by conditioning the model on [instructions] + [task-input] to predict [scratch-pad] + [final answer]; then fine-tuning the same model to predict its own [final answer] conditioned on the [task-input], without seeing the [instructions] or using the [scratch-pad] [73]. B.2.2 Synthetic Data Generation Due to the ubiquitous need for high quality data for fine-tuning (e.g. for use with the methods described above), large body of work has focused on generating high quality synthetic data [58] [1] [22] [67]. For example, Bonito is model that is fine-tuned to generate synthetic data [58], and MetaSynth is method proposed by Riaz et al. that uses language model to orchestrate several expert LLMs for domain-specific synthetic data generation [67]. The training process for Phi-4, 14 billion parameter language model, also incorporates significant amounts of synthetically generated data [1]. Incorporating synthetic data, in conjunction with new post-training techniques, allows Phi-4 to surpass its teacher model on STEM QA tasks, as well as perform well for its size on reasoning benchmarks. These works demonstrate the potential for synthetic data generation methods to augment the capabilities of language models. B.3 Reducing the size of the KV cache In this section, we discuss existing approaches for reducing the size of the KV cache. First, in Appendix B.3.3, we describe works that propose architectural changes to the multi-head attention operation, which reduce the memory footprint of the KV cache. Next, in Appendix B.3.1, we discuss prompt compression methods, which reduce the size of the KV cache by converting long sequence of input embeddings into shorter one. They can be split into hard-token methods, which output discrete tokens from the vocabulary, and soft-token methods, which output new token embeddings not from the vocabulary. Finally, in Appendix B.3.2, we describe KV cache compression methods. These methods directly modify the key and value matrices in the KV cache. Compared with prompt compression methods, these are more expressive because they can produce KV cache that no sequence of input embeddings could have produced. The methodology proposed in our work relies on cache-tuning, which could be viewed as form of KV cache compression. B.3.1 Prompt compression Hard-token prompt compression Some works aim to reduce the size of KV cache by converting longer text into shorter text [16, 37, 48, 63, 94]. These methods are typically referred to as hard-token prompt compression methods because the resulting KV cache comes from discrete tokens from the vocabulary. Compared with soft-token prompt methods, these methods work well with black-box API models. These methods can be broadly classified into two categories: filtering and summarization based methods. Filtering methods cut text from the original prompt using heuristics such as self-information. For example, LLMLingua and Selective-Context use smaller LLM to filter long prompt (e.g. dropping redundant tokens) before passing it to the main model [37, 48]. Summarization methods paraphrase long prompt into smaller number of tokens [16]. 26 Soft-token prompt compression with adapted LLMs In one line of work, researchers train model (typically an adapted LLM) to compress long prompt into smaller number of soft tokens [14, 24, 55, 65, 91]. For example, Autocompressors and In-context Autoencoders (ICAE) are LLMs that are fine-tuned to output embeddings which can be used in soft-token prompts [14, 24]. Autocompressors are trained with fullparameter fine-tuning and leverage recursive strategy to generate the soft prompts, whereas ICAEs are trained with LoRA and use single forward pass to generate the soft prompts. number of other works also propose using an auxiliary model to produce soft-tokens from long prompt [24, 65]. Gisting is another method that differs from those above in that it uses the same LLM to compress the prompt into soft tokens as it uses to generate the response [55]. Soft-token prompt compression via gradient-descent Soft tokens can also be produced by optimizing input token embeddings with gradient descent. This idea, called prompt tuning, was first proposed for the purpose of conditioning frozen langauge model to perform specific tasks [45]. As such, it is an important part of the parameter-efficient fine-tuning literature and is discussed in more detail in Appendix B.1.1. Since then, Li et al. has extended prefix tuning techniques to long-context settings, proposing new method called prefix propagation, which conditions prefixes on previous hidden states to achieve superior performance on long-document tasks compared to prefix tuning [46]. B.3.2 KV cache compression Hard-token KV cache compression Motivated by the observation that, in some settings, small number of keys dominate the attention scores of subsequent queries, several works have proposed KV cache eviction policies wherein keys and values are dynamically dropped during generation [23, 60, 75, 98]. For example, H20 drops keys and values from generated tokens based on running sum of historical attention scores [98]. Similarly, SnapKV drops keys and values from prompt tokens based on window of queries from the end of the prompt [49]. major limitation of eviction methods is that once key is evicted, it cannot be recovered. Instead of evicting keys permanently, another line of work focuses on selectively loading keys from KV cache to SMs. While these works do not reduce memory consumption of the KV cache, they can speed up inference by making better use of GPU memory bandwidth [68, 75]. For example, the Quest method estimates critical tokens at each decoding step and selectively loads them to SMs [75]. Compared with the hard-token prompt compression methods, KV-cache compression methods allow finegrained control at the level of an attention head. This means that token can be dropped from one attention head but not another. Soft-token KV cache compression with merging In another line of work, instead of evicting tokens from the KV cache, researchers propose merging similar tokens [51, 80, 81, 97]. For example, Cache Merge (CaM) takes keys marked for eviction and merges them instead, using weighting scheme based on attention weights [97]. Wang et al. builds on this work by clustering key states into \"merge sets\" based on cosine similarity, and merging states within \"merge set\" with Gaussian kernel weighting scheme, which upweights states more similar to pivotal state chosen as the token with the largest total attention score [81]. Wan et al. expands on both these works with Dynamic Discriminative Operations (D2O), which performs optimizations at both the layer and token levels. D2O adjusts the KV cache budget for each layer based on its attention density and uses an exponential moving average mechanism to dynamically determine when previously discarded token is similar enough to retained tokens to be merged back in [80]. All of these works demonstrate promising results, offering similar or better performance on several tasks compared to full cache with 50% or more reduction in cache size. However, there is still room for further improvement, as these methods still fail to match full cache performance in several tasks, and even 50% reduction in cache size may still be prohibitively expensive for very large models or very long contexts. Additionally, these works do not evaluate the effectiveness of these methods in long-context settings. Soft-token KV cache compression with low-rank projection number of works leverage the observation that the KV cache exhibits low-rank structure to develop compression methods [11, 70, 92, 95, 103]. Similar to compression methods based on merging, compression methods based on low-rank adaptation achieve performances similar to or exceeding full caches on several tasks at 50% compression, while experiencing performance degradation upon further compression. Soft-token KV cache compression with adapted LLMs Above we discussed how some works adapt an LLM to output shorter sequence of soft tokens given long context. Similarly, one could adapt an LLM to output smaller KV cache given long context. While less explored than the analagous prompt compression approach, there is at least one published method that falls into this category. In KV-distill, the authors add LoRA adapters to an LLMs query projections and train them to to produce queries which aggregate information from prior tokens [12]. The adapter is applied selectively to some tokens and only these tokens are kept in the KV cache. The idea is that these selected tokens can act as sinks to collect information from prior tokens. The adapter is trained with distillation objective between compressed and uncompressed KV cache. However, unlike our work, KV-distill does not use any training at test time. Soft-token KV cache compression with gradient-descent The idea of treating the keys and value matrices in KV cache as weights and training them with gradient descent was first discussed in the prefix-tuning paper [47]. In this work, the method was not applied to long-contexts, but rather as parameter-efficient fine-tuning method that can be applied to training datasets with input-output pairs, so we discuss it in more detail in B.1.1. Since then, we are not aware of works that have applied this technique to handle long-contexts. B.3.3 Architectural changes number of works have proposed architectural changes to the original multi-head attention (MHA) operation [79] that reduce the memory footprint of the KV cache. Because they fundamentally alter the architecture, these methods are not immediately compatible with pre-trained models using the standard MHA operation. The earliest works in this direction developed fixed sparsity patterns in the attention map [9, 15, 93]. For example, many works use sliding window sparsity pattern wherein each token attends to fixed window of tokens around it. These approaches reduce the size of the KV cache because they require only keeping around fixed number of tokens in the KV cache. More recently, some large language models have adopted sliding window sparsity in subset of layers/heads [77]. While the methods above reduce the size of the cache by introducing sparsity at the token-level, another class of methods changes the structure of the attention heads. Multi-query attention (MQA), the earliest of such modifications, uses multiple query heads but only single key and value head [71]. While MQA dramatically reduces the size of the KV cache, it can lead to significant drop in the expressive power of the model. Grouped-query attention (GQA) is middle ground between MQA and MHA that allows group of query heads to attend to single key and value head [3]. Many frontier models use GQA, including the Llama 3 architecture, which we use in our experiments [21, 36, 87]. More recently, number of other architectural modifications have been proposed including including Multi-head Latent Attention [50] and Tensor Product Attention [96]. In another line of work, researchers observe that without the softmax operation in the attention mechanism (i.e. linearizing the attention operator), the KV cache can be faithfully represented by the fixed size matrix KV [6]. This allows us to represent the KV cache with single matrix whose size is independent of the context length. Indeed, large body of work has focused on developing architectures with fixed-size memory consumption (i.e. models that do away with the KV cache). Notable examples include state-space models [27], RNNs [8], and other linear attention variants [6, 88]. 28 Prior work shows that there are tradeoffs between the memory consumption of an architecture and the ability of model to perform recall-intensive tasks, when controlling for compute (i.e. FLOPs) [6]. In this context, our work shows that by increasing compute (i.e. FLOPs), we can reduce the memory consumption of model without sacrificing performance. In Appendix E, we provide prelinary theoretical analysis relating SELF-STUDY with recurrent architectures. However, future work should explore the relationship between CARTRIDGES and recurrent models in more depth. B.3.4 Orchestration for long-context In this section, we describe strategies for managing long-contexts by orchestrating calls to LLMs. For instance, the approach by [69] involves summarizing chunks of the context and then combining the summaries. Similarly, PRISM [35] treats the context as sequence of chunks, capturing key information in structured data format. MemGPT [62] introduces virtual memory paging system, drawing inspiration from operating systems. As context length reaches the limit of available memory, the system strategically determines which information to retain."
        },
        {
            "title": "C Extended method description",
            "content": "In this section, we detail the seed prompts and chunking strategy we used to train CARTRIDGES with SELF-STUDY. C.1 SELF-STUDY seed prompts As discussed in Algorithm 1, we seed the synthetic conversation generation with prompt that elicits conversations about different aspects of the document. For each conversation, we randomly sample one of the following functions and create seed prompt by calling it: Structuring Seed Prompt Generator DATA_FORMATS = [ \" JSON \" , \" YAML \" , \" TOML \" , \" INI \" , \" XML \" , \" plain text \" , 1 def structuring_seed_prompt (** kwargs ): 2 3 4 5 6 7 8 9 10 11 12 13 14 EXAMPLES = [ ( ] specific }} \" data_format = random . choice ( DATA_FORMATS ) \" Can you structure the information in {{ subsection }} of {{ document }} related to {{ something f\" in the following format : { data_format }? \" \" Be sure to include precise information like any dates , times , names , and numerical values . '\" ... ] example = random . choice ( EXAMPLES ) return ( f\" Please generate single chat message instructing an LLM to structure the information in { data_format }. \" \" Output only the chat message itself and absolutely nothing else . \" \" Make sure it is clear what section and document you are asking about . \" f\" The message can follow the following template , filling in details from the corpus : nn '{ example }'\" ) 16 17 18 19 20 21 22 23 24 26 27 28 29 30 31 29 Summarization Seed Prompt Generator prompts = [ 1 def summarization_seed_prompt (** kwargs ): 2 3 ( \" \" Please generate single chat message instructing an LLM to summarize part of the corpus . \" Make sure the instruction is very explicit about the section of the corpus that you want to summarize . \" \" Include details ( ids , names , titles , dates , etc .) that make it clear what you are asking about . \" ) , ( \" Please generate single chat message instructing an LLM to summarize section . \" \" Make sure the instruction is explicit about the section that should be summarized and the document it is from .\" ) , ] prompt = random . choice ( prompts ) return prompt 6 7 8 9 10 11 12 13 14 15 16 Question Seed Prompt Generator prompts = [ 1 def question_seed_prompt (** kwargs ): 2 3 ( corpus above . \" \" Generate question for an LLM that will test its knowledge of the information in the \" In your question be sure to include details ( ids , names , titles , dates , etc .) that make it clear what you are asking about . \" \" Output only single question . Do NOT include any other text or explanation other than the question .\" ) , ( \" Generate message for an LLM that will test its knowledge of the information in the corpus above .\" \" Be sure to include details ( ids , names , titles , dates , etc .) in the question so that it can be answered without access to the corpus (i.e. closed - book setting ). \" \" Output only single question . Do NOT include any other text or explanation other than the question .\" ) , ( \" You are helping to quiz user about the information in the corpus . \" \" Please generate question about the subsection of the corpus above . \" \" Be sure to include details ( ids , names , titles , dates , etc .) in the question to make it clear what you are asking about . \" \" Answer only with the question , do not include any other text .\" ) , ] prompt = random . choice ( prompts ) return prompt 5 6 7 8 9 11 12 13 14 15 16 17 18 19 20 21 22 Use Case Seed Prompt Generator prompt = ( 1 def use_case_seed_prompt (** kwargs ): 2 3 \" You are working to train language model on the information in the following corpus . \" \" Your primary goal is to think about practical , real - world tasks or applications that someone could achieve using the knowledge contained within this corpus . \" \" Consider how user might want to apply this information , not just recall it . \" \" After considering potential use cases , your task will be to generate sample question that reflects one of these downstream applications . \" \" This question / instruction / task should be something user , who has access to this corpus , might ask when trying to accomplish their specific goal . \" \" Output only single question . Do NOT include any other text or explanation other than the question .\" ) return prompt 5 6 7 8 9 10 11 30 Creative Seed Prompt Generator prompt = [ ( 1 def creative_seed_prompt (** kwargs ): 2 3 4 5 6 7 8 9 ] return random . choice ( prompt ) ) , \" You are having creative conversation inspired by the information in the corpus . \" \" Please generate question for your conversation partner to start off the discussion . \" \" Answer only with the question , do not include any other text .\" C.2 SELF-STUDY chunking For the SELF-STUDY data generation process, we extract uniformly random token-level chunks from the input corpus C. corresponding textual description is generally prepended to each chunk to contextualize it when generating the seed prompt. This approach helps the model focus on different parts of the corpus and generate diverse synthetic examples. The specific chunking parameters and descriptions are tailored to each dataset: LONGHEALTH: Chunks are sampled with minimum size of 512 tokens and maximum size of 4096 tokens. The accompanying description is: Below is section of patients medical record. It is part of larger corpus of medical records for Npatients different patients. AMD/FinanceBench: Fixed-size chunks of 8192 tokens are utilized. No specific descriptive text is prepended to these chunks. MTOB: Chunks are sampled with minimum size of 512 tokens and maximum size of 4096 tokens. The description used is: The following is an excerpt from grammar book about the Kalamang language. QASPER: Following our general methodology, chunks are sampled with minimum size of 512 tokens and maximum size of 4096 tokens. generic description is used to contextualize the chunk as an excerpt from research paper, in line with the nature of the Qasper dataset."
        },
        {
            "title": "D Datasets",
            "content": "D.1 GENCONVO To evaluate the ability of our approach to handle diverse queries over long documents, we generated the GENCONVO dataset. We created GENCONVO using the AMD 2022 10-K filing, document from the FinanceBench corpus [33]. The primary purpose of GENCONVO is to simulate wide range of tasks user might ask model to perform given long document, thereby testing the models comprehension, reasoning, and ability to extract varied types of information. The generation process relies on Claude Sonnet 3.7 [4] and is structured as follows: 1. Document Input: The entire source document (e.g., the AMD 2022 10-K, which is less than 200,000 tokens and fits within the models context window) is provided to Claude Sonnet 3.7. 2. Question Generation: series of distinct prompt templates (detailed below), designed to elicit different reasoning traces (e.g., factual recall, synthesis, multi-hop reasoning), are used to generate questions. For the given document and each prompt template, we ask the model to generate 16 unique questions. This involves providing the model with the full document content alongside the specific question-generation prompt. 3. Answer Generation: Subsequently, for each generated question, Claude Sonnet 3.7 is prompted again with the original full document and the generated question to produce an answer. This process ensures that the answers are grounded in the provided document. 31 We hope GENCONVO provides challenging benchmark that moves beyond simple fact retrieval, assessing models capacity for deeper understanding and more complex information processing over long contexts. The following prompt templates were utilized for the question generation phase: Factual Prompt Template Please generate question to test someones ability to remember factual details from the document. The answer should be few tokens long and be factual detail from the statement, such as number, entity, date, title, or name. This question should not be common knowledge: instead, it should be something that is only answerable via information in the document. Knowledge Prompt Template Please generate question that requires combining information mentioned both inside and outside the document. This question should require using fact from the document and also fact that you are confident about, but is not mentioned in the document. For instance: - What are the founding dates of the companies that got acquired this year? This is good question because the names of the acquired companies are mentioned in the document and the founding dates are not mentioned. - What is the name of the CEOs spouse? This is good question because the name of the CEO is mentioned in the document and the spouses name is not mentioned. The answer should be fact that is few tokens long such as number, entity, date, title, or name. Disjoint Prompt Template Please generate multi-hop question that tests someones ability to use factual information mentioned in at least two very different sub-sections of the document. This question shouldnt be standard question about this kind of document. Instead, it should ask about two particularly disconnected ideas, like comparing information about the amount of owned space for the company headquarters with the amount of dollars of estimated liability or comparing the revenue number with the number of employees. This question should also test ones ability to do retrieval: do not give away part of the answer in the question. Ensure that for one to get the correct answer to the question, they need to understand the document. The answer should be short: for example, number, entity, date, title, or name. Synthesize Prompt Template Please generate question that requires synthesizing and aggregating information in the document. For instance, you could ask someone to summarize page of the document, list all the key competitors mentioned in the document, or summarize the companys business model. Structure Prompt Template Please generate question that requires understanding the structure of the document. This question should be more about the structure of the document, rather than the precise statement details. For instance, you could ask someone to list the titles of all the sections in the document, describe the document structure, report the total number of pages, ask which section amongst two sections comes first, or report the section with the largest number of tables. Creative Prompt Template Please generate question about the document to test someones ability to comprehend the content of the document. This question specifically should be focused on their ability to generalize the information about the document to strange question of sorts. This question shouldnt be standard question about this kind of document, it should ask to do something abnormal and creative, like writing poem about financial document. 32 Counting Prompt Template Please generate question that requires counting how frequently different events occur in the document. This question should be about statistical properties of the document, rather than the statement details. For instance, you could ask someone to count the number of times the word \"million\" is mentioned or count the length of the shortest section title. The answer should be number. Reasoning Prompt Template Please generate question that requires mathematical reasoning over the values in the document. This question should require going beyond the facts directly mentioned in the statement, such as asking to compute the percentage increase in revenue between two years, find the largest expense category, or calculate difference in profit between two years. The answer should be number. D.2 LONGHEALTH LONGHEALTH is benchmark for evaluating large language models ability to analyze and interpret long clinical texts [2]. The benchmark consists of 20 fictional clinical case reports (each containing between 5,090 and 6,754 word) and 400 multiple-choice questions based on them. In our experiments, the context consists of the reports for panel of patients. We use = 10 patients, with full panel of approximately 100k tokens, which fits in the context length of the LLAMA 3 models. The questions are categorized into information extraction, negation, and sorting. sorting question is included below: Please answer the question below about the following patient: ID patient_03, Name: Mr. John Williams, Birthday: 1956-08-08 00:00:00, Diagnosis: Multiple Myeloma <question> Mr. Williams received multiple radiologic examinations. In which order did she receive them? </question> <options> CT Whole Body > MR Spine Scan > CT Spine Scan > PSMA-PET-CT Scan > CT Chest > CT Whole Body > Whole Body CT scan Whole Body CT scan > CT Spine Scan > CT Whole Body > MR Spine Scan > CT Chest > PSMA-PET-CT Scan > CT Whole Body. CT Whole Body > CT Whole Body > CT Chest > CT Chest > PSMA-PET-CT Scan > MR Spine Scan > CT Spine Scan > Whole Body CT scan > Chest X-ray CT Chest > CT Spine Scan > CT Whole Body > Whole Body CT scan > PSMA-PET-CT Scan > MR Spine Scan > CT Whole Body Whole Body CT scan > CT Spine Scan > CT Whole Body > MR Spine Scan > CT Chest > CT Whole Body > PSMA-PET-CT Scan </options> You should first think step by step. Then give your final answer exactly as it appears in the options. Your output should be in the following format: <thinking> {{YOUR_THOUGHT_PROCESS}} </thinking> <answer> {YOUR_ANSWER} </answer> An example of negation question is included below: Please answer the question below about the following patient: ID patient_01, Name: Anna Sample, Birthday: 1970-01-01 00:00:00, Diagnosis: DLBCL 33 <question> Which of these examinations were never performed in Mrs. Sample? </question> <options> Bone marrow aspiration CSF aspiration MRI of the head Pulmonary function testing Cardiac stress testing </options> You should first think step by step. Then give your final answer exactly as it appears in the options. Your output should be in the following format: <thinking> {{YOUR_THOUGHT_PROCESS}} </thinking> <answer> {YOUR_ANSWER} </answer> D.3 MTOB The Machine Translation from One Book (MTOB) benchmark tests large language models ability to learn to translate between English and Kalamang, low-resource language with virtually no web presence [76]. The core task is to perform translation (Kalamang to English, and English to Kalamang) by primarily relying on single comprehensive grammar book and small set of accompanying linguistic resources. In our work, we focus on translating from Kalamang to English. The source documents provided by the MTOB benchmark are: grammar of Kalamang: comprehensive grammar textbook, with the original source provided in LATEX format. This book details the phonology, morphology, and syntax of Kalamang. Bilingual Word List (W): list of Kalamang words with their part-of-speech tags and English descriptions. Parallel Kalamang-English Corpus (S): collection of 375 paired Kalamang-English sentences. The MTOB authors preprocessed the grammar textbook from its original LATEX source into several plaintext splits for their baseline experiments. These include: Gm (Medium-length chunk): plaintext segment of approximately 50k tokens consisting of an overview chapter, morpheme table from the grammar book, and the complete bilingual word list (W). Gl (Long-length chunk): larger plaintext segment of approximately 100k tokens, containing chapters from the grammar book that the MTOB authors deemed most important for the translation task. Full Plaintext Textbook (G): The entire grammar book converted to plaintext. The combination of the long-length chunk (Gl), the parallel sentences (S), and the word list (W) exceeds the context window of Llama 3 models. We use the medium-length chunk Gm and the parallel sentence list as input for our ICL baseline. D.4 QASPER QASPER is benchmark for evaluating the ability of large language models to answer questions about scientific papers [19]. To create challenging multi-query long-context setting resembling the setup described in Section 2.2, we concatenate 16 papers all related to QA NLP models to form out corpus C. In total, there are 78 questions about these 16 papers in the dataset, which we use as the queries Q. Because the dataset only includes short answers and ground-truth spans containing evidence for each answer, we rewrite the answers in longer, more conversational format using GPT-4.1 and use these as the targets when evaluating. Theoretical analysis: Relationship between attention, linear attention, and"
        },
        {
            "title": "CARTRIDGES",
            "content": "When we generate text with an autoregressive Transformer, we have to maintain KV-cache that grows linearly with the length of the input and text. In Appendix B.3.3, we discussed number of architectural modifications that either reduce the size of the KV-cache or do away with it altogether. In particular, when generating text with linear attention (e.g. [6]), we only need to maintain constant-sized object the KV-state matrix during generation. Like the KV-state matrix in linear attention, CARTRIDGES consume constant amount of memory (i.e. their size is hyperparameter, which can be set independently of the input length). However, they differ from the KV-state in how they are updated. In this work, CARTRIDGES are updated using SELF-STUDY gradient descent on synthetically generated data. On the other hand, KV-states are updated using linear attention update rule. In this section, we will study the update rules for attention, linear attention, and gradient descent when applied to the multi-query associative recall (MQAR) problem [5], popular synthetic benchmark task used for studying the capabilities of long-context architectures. In particular, we consider variant of the standard MQAR problem where key-value pairs are repeated. First, we highlight some equivalences between the update rules of these approaches in the case where input keys are orthonormal. Then, in the more challenging case where input keys are in Johnson-Lindenstrauss embedding, we provide separation result showing that the gradient descent update rule is able to exactly solve an MQAR problem that linear attention cannot. These theoretical results provide intuition for why constant-sized CARTRIDGES are able to match the performance of full KV-caches in long-context settings when linear-attention architectures have struggled to do so. E.1 Notation All vectors are assumed to be row vectors. Parenthesized superscripts (e.g. k(1)) denote some temporal quality of an element. Subscripts denote different elements in set, as is standard. concise explanation for each variable: : model (and token) dimension. : number of unique key-value pairs. : number of queries. : number of key-value pairs in stream. E.2 MQAR We define the Multiple Query Associative Recall (MQAR) problem. Definition 1. There is universe of keys: and values: R1d, R1d. Definition 2. [5] In the MQAR problem, the input is: (k(1), v(1)), . . . , (k(N), v(N)) where (k(t), v(t)) for 1 N, followed by set of queries Then for each [n], output: E.3 repetitive MQAR q1, . . . qn where qi for 1 n. (cid:40) vi where = max{i [1, N]ki = qj} 0d if no such exists. Definition 3. repetitive MQAR is special case where each (K(t), V(t)) S, where: = {(k1, v1), . . . , (km, vm)}. Additionally, ki is unique. Definition 4. To capture this, (t) E.3.1 Orthonormal Embedding is defined as the number of occurrences of (ki, vi) in the stream at timestep t. First, we will look at the MQAR problem in restricted case, when all keys are orthonormal. Definition 5. We call the set to be orthonormal if for all k, K: k, = (cid:40) 0 1 if = otherwise. E.3.2 Johnson-Lindenstrauss Embedding Next, we will look at the MQAR problem in restricted case, when all keys are in JL embedding. Definition 6. Let Ïµ > 0, we call the set to be ÏµJL if for all k, K: k, = (cid:40) [Ïµ, Ïµ] 1 if = otherwise. . E.4 Model Definitions Below, we will describe three different model architectures. While they each exhibit different performance and capabilities they can be describe with common framework for the MQAR problem. 1. State: is how the model store Key-Value pairs. 2. Update rule: how the model incorporates new Key-Value pairs into its state. 3. Query rule: how the model uses its state to answer look up value or query. E.4.1 Transformer 1. The state is: where, (t) = (K(t), (t)), K(t) Rtd, (t) Rtd. Note that this consumes more memory as the context gets longer. 2. The update rule is: K(t+1) = K(t) k(t+1), (t+1) = (t) v(t+1) 36 3. On query K, return: (cid:16) K(t)(cid:17) (t). These rules define the transformer setting for MQAR. E.4.2 Linear Attention 1. The state: (t) Rdd. 2. The update rule is defined as: (t+1) = (t) + (k(t+1))(v(t+1)). With the initial matrix being initialized to zeros. I.e. (0) = 0dd. 3. On query q, return: qW (t). [89] Linear attention rule emerges if we were to update using the loss function k(t)W (t)vt. Lemma 1. It is important to mention here that we are not using any kernels for linear attention. These rules define the linear attention setting for MQAR. (cid:16) (cid:16) v(t) is the update rule that emerges when we use [89] (t+1) = (t) Lemma 2. the gradient descent loss function: 1 Definition 7. k(t)(cid:17) k(t)W (t) + 2 k(t)W (t) v(t)2 2. k(t)(cid:17) = 1 2 k(t)W (t) v(t)2 2 Proof. In general, gradient descent has the update rule: (t+1) = (t) Î·W (t) . (4) Taking the gradient of the loss function gives us: 1 2 k(t)W (t) v(t)2 2 = = (cid:16) (cid:16) k(t)(cid:17) k(t)(cid:17) (k(t)W (t) v(t)) k(t)W (t) (cid:16) k(t)(cid:17) v(t). Using the above and choosing Î· = 1, we get for Equation (4) (t+1) = (t) 1 (cid:18)(cid:16) k(t)(cid:17) k(t)W (t) (cid:16) k(t)(cid:17) v(t) (cid:19) = (t) (cid:16) k(t)(cid:17) k(t)W (t) + (cid:16) k(t)(cid:17) v(t). E.4.3 Gradient Descent Gradient descent training on the cache. We look at the capability of this trained state on certain input. 1. The state at time is defined as: (t) Rdd. 2. The update rule which follows from Lemma 2: (cid:16) (t+1) = (t) k(t)(cid:17) k(t)W (t) + (cid:16) k(t)(cid:17) v(t). With the initial matrix being initialized to zeros. I.e. (0) = 0dd. 3. On query q, return: qW (t). 37 E.4.4 Orthonormal Case We now see how the three models perform on the repetitive MQAR when is orthonormal. Transformer Lemma 3. On every input to MQAR (even those for 1-rep-MQAR) the state of Transformer needs â¦(Nd) parameters. Intuitively, at each timestep, you will append parameters to the state. At timestep the model will have td parameters. Linear attention Theorem 1. Linear attention can solve repetitive MQAR for any 1 and orthonormal K, up to scaling (producing (t) vi when (t) is queried with ki) and all keys being distinct with O(d2) parameters. Proof. We first prove that for any 0: (t) = i=1 (t) k vi . (5) Base Case: Initially, (0) = 0dd. From this, we indeed have: since for all [m]: (0) = i=1 (0) k vi , (0) = 0. Inductive hypothesis: Assume that the state matrix at some arbitrary integer timestep is as claimed. I.e.: (t) = i=1 (t) k vi . Inductive step: If (k(j), v(j)) appears at timestep + 1 the update rule will be: (t+1) = (t) + (k(t+1))v(t) = (t) + (kj)vj By the inductive hypothesis, we have that: = (t+1) = (t) + kj(vj) i=1 i=1 (t) k (t+1) k vi . = r vi + kj(vj) The final step follows from the fact that all = j. The proof of Equation (5) is complete by induction. = (t+1) (t) + 1 when (k(t+1), v(t+1)) = (kj, vj) and (t+1) = (t) for 38 Finally, it is the case that on query ki: kiW (t) = ki i=1 (t) k vi = i=1 = i=i = i=i (t) = (t) kik vi (t) kik vi + (t) kik vi (t) 0 vi + (t) 1 vi vi, as desired. In the above, the second last inequality follows from from Definition 5 and the fact that all ki are distinct. O(d2) parameters are needed as the matrix must have dimension Gradient Descent Theorem 2. Gradient descent is able to exactly solve the repetitive MQAR (produce vi when (t) is queries with ki) with O(d2) parameters. Proof. Here we can handle repetitions because our update rule includes \"peel\" term. This means it removes the current value stored under key before updating it with new value. We will show by induction that for all 0: (t) = 1 i= (t) >0 i vi . Base Case: Initially, the cache matrix is set to all zeros. From this, naturally follows that: since for all W (0) = i=1 0 i vi , (0) = 0. Inductive hypothesis: Assume that at some arbitrary timestep t, we have: (t) = 1 (t) > i vi Inductive step: If (kâ, vâ) appears at timestep + 1 the update will be: 1 i=1 (t+1) i>0 vi = (cid:32) i=1 1 (t) > i vi (cid:33) (cid:32) i=1 1 (t) >0 (cid:33) â kâk i vi + â vâ the second term reduces to just peeling the term relating to kâ, if it exists, as all other inner products are 0, = = (cid:32) i=1 (cid:32) i=â 1 (t) >0 i vi (cid:33) (cid:33) (cid:18) â vâ (t) â>0 (cid:19) + â vâ 1 (t) >0 i vi + â vâ 39 This replaces the value associated with kâ with the new value, while keeping everything else the same. This is the form that we want, as the only time we want to add key if it is an new key. Finally, it is the case that on query ki: ki (t) = ki (cid:32) i=1 1 (t) >0 i vi (cid:33) (cid:33) 1 (t) >0 ki i vi = (cid:32) i=1 = 1 = 1 (t) i> (t) i>0 1 vi vi Again here matrix of dimension can store orthogonal vectors. Thus this requires, O(d2) parameters. E.4.5 JL Embedding We now see how the 3 models perform on the repetitive MQAR when is ÏµJL. Transformer Lemma 4. On every input to MQAR (even those for 1-rep-MQAR) the state of Transformer needs â¦(Nd) parameters. We note that when is ÏµJL it is no longer possible to get the exact answer from query rule kiW (t). Thus, we need to add decoding step. Definition 8. The output decoding step is vi where: Definition 9. For all i, [m], define: = arg max i[m] vi , kiW (t). Ïµi,j = ki, kj. Linear Attention Theorem 3. Linear attention (+ decoding as in Definition 8) is unable to solve even the 2 repetitive MQAR and each vi being 1-hot encoding unless is Ï JL. (cid:17) (cid:16) 1 Proof. Due to the agreeance between different keys, when querying for key i, there is noise from other keys returned along with the correct answer. While we can tolerate some error, this error scales with the number of times the model has seen single key. Making it unfit for longer contexts, or contexts with many repeats. First, note that the base case Equation (5) from Theorem 1 still holds. In general, this holds for all K. Specifically, on query k1 we have: k1W (t) = (t) 1 k1, k1v1 + (t) 2 k1, k2v2 = (t) 1 v1 + (t) 2 Ïµ1,2v2. Now, consider an input to 2 repetitive MQAR such that (t) 1 < (t) 2 Ïµ1,2. Note that in this case: (t) 1 = v1, k1W (t) < v2, k1W (t) = (t) 2 Ïµ1,2 and hence we output v2 instead of v1. If the embedding was Ï( 1 the number of repeats could not overcome the Ïµ value. Gradient Descent Theorem 4. Gradient descent (+ decoding as in Definition 8) is able to exactly solve repetitive MQAR with m2(m1) and Î± < m1 O(d2) parameters for ÏµJL K, as long as Ïµ m+1 . 1 Proof. We define: (t) i,j to be the coefficient associated with vj in (t). Specifically, let We will prove by induction that: where, (t) = i= j=1 (t) i,j i vj (t) i,j = 1 (ki,vj) has occurred + (t) i,j (cid:12) (cid:12) (cid:12) (t) i,j (cid:12) (cid:12) (cid:12) a=1 ((m 1)Ïµ)a. (6) (7) (8) Base Case: Initially, the state is set to all zeros. From this, naturally follows that all of the Equation (7): (t) i,j are zero. I.e. i,j = 0. Inductive hypothesis: Assume that all for some timestep and 1 i, m: (t) i,j = 1 (ki,vj) has occurred + (t) i,j , where (t) i,j satisfies Equation (8). Inductive Step: If at timestep + 1 we are given (kâ, vâ), from Equation (6) the update looks like: (t+1) = i=1 j=1 (t+1) i,j vj = = i=1 j=1 i= j=1 (t) i,j i vj (t) i,j i vj i=1 j=1 i= j=1 (t) i,j â kâk vj + â vâ Ïµâ,i (t) i,j â vj + â vâ = change the associativity of the summations, (cid:33) j=1 i=1 (t) i,j m j=1 here we separate the first term where = â and = â, vj â vj Ïµâ,i (t) i,j (cid:32) i=1 + â vâ = i=â j=1 (t) i,j C vj + j=1 (t) â,j â vj j=1 (cid:32) i=1 (cid:33) + â vâ â vj Ïµâ,i (t) i,j here we separate the first term where = â and = â, = i=â j= (t) i,j i vj + remove Ïµj,j, = i=â j= (t) i,j i vj + cancel terms, j=1 j=1 (t) â,j C â vj j=1 Ïµâ,âC (t) â,j â vj (cid:32) j= Ïµâ,i (t) i,j i=â (cid:33) + â vâ â vj (t) â,j â vj j=1 (t) â,j â vj (cid:32) j=1 i=â (cid:33) + â vâ â vj Ïµâ,i (t) i,j = i=â j=1 (t) i,j i vj (cid:32) j=1 i=â (cid:33) + â vâ. â vj Ïµâ,i (t) i,j Note with this we can see that: Thus, if = â, we have: (t+1) i,j = (t) i,j i=â Ïµâ,i (t) i,j + 1 j=â if â = if â = . (t+1) i,j = (t) i,j , for = â. The inductive statement holds for these pairs. Now lets consider (t+1) â,j . If â = then: (t+1) â,â = 1 + (t+1) â,â = i=â Ïµâ,i (t) i,j + 1 42 and note that by the triangle inequality and Definition 6: (cid:12) (cid:12) (cid:12) (t+1) â,â (cid:12) (cid:12) Ïµ (cid:12) i=â (cid:12) (cid:12) (cid:12)C (t) i,â (cid:12) (cid:12) (cid:12) by the inductive hypothesis, Ïµ i=â (1 + a=1 ((m 1)Ïµ)a) = ((m 1)Ïµ)(1 + a= ((m 1)Ïµ)a) = ( t+1 a=1 ((m 1)Ïµ)a), as desired. Then for = â, we have: (cid:12) (cid:12) (cid:12) (t+1) j,â (cid:12) (cid:12) (cid:12) = = (cid:12) (cid:12) (cid:12) (cid:12) (t+1) (cid:12) (cid:12)C i,j (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) i=â Ïµâ,i (t) i,j (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) The bounding of (t) â,j is similar to the â = case. With this we have completed the inductive proof on error terms. If the we set: we get the following bound: Before the next steps, we must bound: Ïµ < 1 m2(m 1) , (t) i,j a=1 ((m 1)Ïµ)a < (m 1)Ïµ 1 (m 1)Ïµ 1 m2 1 (cid:12)vi, vj(cid:12) (cid:12) (cid:12) Î± For query with ki, assuming we have seen ki before, we get: ki (t) = vi + j=i (t) i,j vj Now for the decoding step where for an arbitrary vj we get: vj, ki (t) = vj, vi + vj, j=i i,j vj For the case where = it is the case that: vi, ki (t) = 1 + vi, j=i i,j vj 1 1 + 1 Î±. 43 (9) (10) (11) (12) This follows from Equation (11) and Equation (12). For the case where = it is the case that: vj, ki (t) = vi, vj + vj, j=i i,j vj Î± + 1 + Î± This follows from Equation (11) and Equation (12). As result, we will always pick the correct value when Î± < m1 m+1 ."
        }
    ],
    "affiliations": [
        "Caltech",
        "Stanford University",
        "University at Buffalo"
    ]
}