{
    "paper_title": "Dale meets Langevin: A Multiplicative Denoising Diffusion Model",
    "authors": [
        "Nishanth Shetty",
        "Madhava Prasath",
        "Chandra Sekhar Seelamantula"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Gradient descent has proven to be a powerful and effective technique for optimization in numerous machine learning applications. Recent advances in computational neuroscience have shown that learning in standard gradient descent optimization formulation is not consistent with learning in biological systems. This has opened up interesting avenues for building biologically inspired learning techniques. One such approach is inspired by Dale's law, which states that inhibitory and excitatory synapses do not swap roles during the course of learning. The resulting exponential gradient descent optimization scheme leads to log-normally distributed synaptic weights. Interestingly, the density that satisfies the Fokker-Planck equation corresponding to the stochastic differential equation (SDE) with geometric Brownian motion (GBM) is the log-normal density. Leveraging this connection, we start with the SDE governing geometric Brownian motion, and show that discretizing the corresponding reverse-time SDE yields a multiplicative update rule, which surprisingly, coincides with the sampling equivalent of the exponential gradient descent update founded on Dale's law. Furthermore, we propose a new formalism for multiplicative denoising score-matching, subsuming the loss function proposed by Hyvaerinen for non-negative data. Indeed, log-normally distributed data is positive and the proposed score-matching formalism turns out to be a natural fit. This allows for training of score-based models for image data and results in a novel multiplicative update scheme for sample generation starting from a log-normal density. Experimental results on MNIST, Fashion MNIST, and Kuzushiji datasets demonstrate generative capability of the new scheme. To the best of our knowledge, this is the first instance of a biologically inspired generative model employing multiplicative updates, founded on geometric Brownian motion."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 0 3 7 2 0 . 0 1 5 2 : r Dale meets Langevin: Multiplicative Denoising Diffusion Model Nishanth Shetty Department of Electrical Engineering Indian Institute of Science Bengaluru 560012 nishanths@iisc.ac.in Madhava Prasath Department of Electrical Engineering Indian Institute of Science Bengaluru 560012 madhavprasath088@gmail.com Chandra Sekhar Seelamantula Department of Electrical Engineering Indian Institute of Science Bengaluru 560012 css@iisc.ac.in"
        },
        {
            "title": "Abstract",
            "content": "Gradient descent has proven to be powerful and effective technique for optimization in numerous machine learning applications. Recent advances in computational neuroscience have shown that learning in standard gradient descent optimization formulation is not consistent with learning in biological systems. This has opened up interesting avenues for building biologically inspired learning techniques. One such approach is inspired by Dales law, which states that inhibitory and excitatory synapses do not swap roles during the course of learning. The resulting exponential gradient descent optimization scheme leads to log-normally distributed synaptic weights. Interestingly, the density that satisfies the Fokker-Planck equation corresponding to the stochastic differential equation (SDE) with geometric Brownian motion (GBM) is the log-normal density. Leveraging this connection, we start with the SDE governing geometric Brownian motion, and show that discretizing the corresponding reverse-time SDE yields multiplicative update rule, which surprisingly, coincides with the sampling equivalent of the exponential gradient descent update founded on Dales law. Proceeding further, we propose new formalism for multiplicative denoising score-matching, which subsumes the loss function proposed by Hyvärinen for non-negative data. Indeed, log-normally distributed data is positive and the proposed score-matching formalism turns out to be natural fit. This allows for training of score-based models for image data and results in novel multiplicative update scheme for sample generation starting from log-normal density. Experimental results on MNIST, Fashion MNIST, and Kuzushiji datasets demonstrate generative capability of the new scheme. To the best of our knowledge, this is the first instance of biologically inspired generative model employing multiplicative updates, founded on geometric Brownian motion."
        },
        {
            "title": "Introduction",
            "content": "An interesting problem in computational neuroscience is training artificial neural networks (ANNs) in fashion that is consistent with learning and optimization seen in biological systems. Several studies [Song et al., 2005, Loewenstein et al., 2011b, Buzsáki and Mizuseki, 2014, Melander et al., 2021, Pogodin et al., 2024] have confirmed that synaptic weight distributions in biological systems are log-normally distributed and that the neurons obey Dales law [Eccles et al., 1954], which states Preprint. Under review. that excitatory (inhibitory) neurons stay excitatory (inhibitory) throughout the course of learning without synaptic flips. Artificial neural networks trained with gradient descent seldom obey Dales law. Recently, Cornford et al. [2024] proposed the use of exponentiated gradient descent (EGD) to train neural networks and have observed that the training is consistent with Dales law and leads to log-normally distributed synaptic weights, in alignment with experimental findings. Exponentiated gradient descent is derived using mirror descent for particular variant of Bregman divergence. In this paper, we establish concrete link between exponentiated gradient descent optimization to sampling from stochastic differential equations (SDEs) inspired by geometric Brownian motion (GBM). Whereas most diffusion modeling and sampling schemes rely on standard Brownian motion, to the best of our knowledge, this is the first instance where GBM is used. We show that the proposed framework captures the multiplicative nature of updates seen in EGD. The ability of geometric Brownian motion to model processes with proportional changes makes it an ideal candidate for developing biologically inspired generative models. For the purpose of generation, we need the underlying score function used in the reverse-time SDE, for which we develop novel multiplicative score-matching loss. While large body of contemporary generative modeling literature is based on SDEs with additive Gaussian noise, our novel formalism relies on an SDE that governs the forward noising process dynamics with multiplicative log-normal noise. We develop the corresponding reverse-time SDE and show that it results in multiplicative update rule that is structurally equivalent to the exponential gradient-descent scheme Cornford et al. [2024]. The multiplicative update rule obtained as consequence of the discretization of the SDE can be used to sample from the desired distribution whose score function is learnt using neural network. We support the theoretical developments with experimental results on MNIST [LeCun et al., 1998], Fashion MNIST [Xiao et al., 2017] and Kuzushiji image datasets [Clanuwat et al., 2018]. 1.1 Related Works Recent developments in generative modelling employing generative adversarial networks Goodfellow et al. [2014], diffusion models Ho et al. [2020], score-based models Song and Ermon [2019, 2020], Song et al. [2021b], flow-based models Papamakarios et al. [2021] have produced stunning examples across variety of modalities spanning images, video, audio, etc.. In the context of diffusion models, seminal contribution has been the early work by Sohl-Dickstein et al. [2015]. Inspired by non-equilibrium thermodynamics, they introduced the diffusion probabilistic model as tractable and flexible model for sampling and inference. They demonstrated generative capability on toy datasets in two dimensions and image datasets like binarized MNIST and CIFAR-10. Ho et al. [2020] demonstrated that denoising diffusion probabilistic models (DDPMs) could be used for high quality image synthesis. They vastly improved the results from Sohl-Dickstein et al. [2015] and showed performance comparable to state-of-the-art generative models [Karras et al., 2018, 2020] of that time. Progress in score-matching by Song et al. [2019], Song and Ermon [2019, 2020] demonstrated the potential of score-based generative models to be competitive with diffusion models. In the seminal work of Song et al. [2021c], it was shown that an SDE framework unifies both approaches. These SDEs were based on standard Brownian motion. Several alternative formulations that obviate the need for Brownian motion were also proposed. In particular, Bansal et al. [2023] propose generative models that are based on more generic degradation operations and their corresponding restoration operations. They consider blurring and masking among others as degradation operators and show that such generalized degradations could also be used to formulate generative models. Rissanen et al. [2023] proposed that generation could be viewed as the time-reversal of heat equation. Additionally, they showed that their approach allows for certain image properties like shape and colour to be disentangled and they also discuss spectral properties that reveal inductive biases in generative models. Santos et al. [2023] developed discrete state-space diffusion model that relies on pure-death random process and demonstrate competitive generative ability on binarized MNIST, CIFAR-10, and CelebA-64 datasets. recent preprint on image denoising by Vuong and Nguyen [2024] is perhaps the closest to the multiplicative noise model considered in this paper. They consider forward process where images are corrupted by multiplicative log-normal or gamma distributed noise. However, instead of proceeding with the multiplicative noise model, they convert it to an additive one by applying logarithmic transformation. While the log-transformation simplifies the calculations, it reduces the problem to the additive noise setting, losing out completely on the richness of the original multiplicative noise framework. Vuong and Nguyen [2024] remark explicitly that the reverse-time SDE in the 2 multiplicative noise setting comes with lot of complications, which are overcome by converting it to an additive noise model. They also restrict the scope of their work to denoising and do not propose generative framework. 1.2 Organization of the paper Section 2 gives an account of Dales law and progress in computational neuroscience in deploying exponentiated gradient descent to enforce Dales law all of these form the inspiration for this work. In Section 3, we present the essential mathematics behind SDEs and generative modeling required for understanding the contributions of this paper. Section 4 introduces Geometric Brownian Motion (GBM) and its corresponding reverse-time SDE based sampler for image generation. This necessitates new score-matching framework for multiplicative noise which we define in Section 5. Finally, Section 6 presents experiments on MNIST, Fashion-MNIST, and Kuzushiji MNIST datasets, demonstrating the effectiveness and potential of the proposed model."
        },
        {
            "title": "2 Dale’s Law and Exponentiated Gradients",
            "content": "In computational neuroscience, Dales law [Eccles et al., 1954] has been empirically observed to hold in many biological systems barring certain exceptions. Dales law states that presynaptic neurons can only exclusively affect their corresponding postsynaptic counterparts in an excitatory or inhibitory manner. The implication of the law is that the synapses continue to be inhibitory or excitatory during the course of learning without flipping. On the contrary, artificial neural networks have synaptic weights that can flip from excitatory to inhibitory or vice versa during training. Previous attempts [Bartunov et al., 2018, Whittington and Bogacz, 2019, Lillicrap et al., 2020] to incorporate biologically inspired learning rules to train neural networks have had limited success on standard benchmark tasks. Recently, Cornford et al. [2021] demonstrated that ANNs that obey Dales law, which they name Dales ANNs (DANNs), can be constructed without loss in performance compared to weight updates done using standard gradient descent. They show that the ColumnEI models proposed by Song et al. [2016] are suboptimal and can potentially impair the ability to learn by limiting the solution space of weights. DANNs outperform ColumnEI models on tasks across MNIST [LeCun et al., 1998], Fashion-MNIST [Xiao et al., 2017] and Kuzushiji MNIST datasets [Clanuwat et al., 2018]. Cornford et al. [2021] posit that the emergence and prevalence of Dales law in biological systems is possible evolutionary local minima and that the presence of inhibitory units in learning could help avoid catastrophic forgetting [Barron et al., 2017]. Li et al. [2023] demonstrated that methods such as ColumnEI proposed by Song et al. [2016] to incorporate Dales law into the training of recurrent neural networks (RNNs) lead to suboptimal performance on sequence learning tasks, which is primarily attributed to poor spectral properties of the weight matrices, in particular, the multimodal, dispersed nature of the singular value spectrum of the weight matrix. Li et al. [2023] extended the architecture developed by Cornford et al. [2021] to handle sequences using RNNs and showed that these networks are on par with RNNs that are trained without incorporating Dales law. The spectral properties of DANN RNNs are also better than the ColumnEI networks and the singular value spectrum is unimodal and clustered leading to superior performance on tasks such as the adding problem [Hochreiter and Schmidhuber, 1997], sequential MNIST task [Le et al., 2015] and language modelling using the Penn Tree Bank [Marcus et al., 1993]. Cornford et al. [2024] demonstrated that gradient descent is suboptimal phenomenological fit to learning experiments in biologically relevant settings. While stochastic gradient descent for training ANNs is an exceptionally successful and robust model in general, it violates Dales law [Eccles et al., 1954] by allowing for synaptic flips. This leads to the distribution of weights not being log-normal, which contradicts experimentally observed data. Cornford et al. [2024] showed that exponentiated gradient descent (EGD) introduced by Kivinen and Warmuth [1997] respects Dales law and consequently produces log-normally distributed weights. In experiments performed on the Mod-Cog framework [Khona et al., 2023] using RNNs, EGD outperforms gradient descent and is superior to GD for synaptic pruning. The learning task is formulated utilizing the mirror descent framework [Nemirovsky and Yudin, 1985, Bubeck, 2015] as changes to synaptic weights in neural network such that combination of task error and synaptic change penalty must be minimized. 3 This leads to the update rule: Xk+1 = arg min (cid:20) ℓ(X) + 1 η (cid:21) Dϕ(X, Xk) , (1) ℓ(X) where ℓ(X) = ℓ(Xk) + Xk) is the linearization of the task error ℓ(X) X=Xk (X about the point Xk and Dϕ(X, Xk) is the synaptic change penalty. The penalty Dϕ : Rd Rd is chosen as the Bregman divergence corresponding to strictly convex function ϕ : Rd R. 2 Depending on the choice of ϕ, we get different update rules. For instance, when ϕ(X) = 2, 2 the corresponding synaptic change penalty is Dϕ(X, Xk) = 2, and Eq. (1) results in the X=Xk . This update rule for the weights familiar gradient-descent update Xk+1 = Xk η does not guarantee that the entries of Xk+1 and Xk have the same sign, which allows for synaptic flips during training, as also confirmed by Cornford et al. [2024]. Xk ℓ(X) Cornford et al. [2024] chose ϕ(X) = d(cid:80) i=1 (i) (i) log , where (i) denotes the ith entry of X, which results in Dϕ being the unnormalised relative entropy, Dϕ(X, Xk) = (cid:88) (i) log i=1 (i) (i) (i) + (i) . For this choice of Dϕ, the update rule in Eq. (1) takes the form Xk+1 = Xk exp ( η ℓ(X) X=Xk sign(Xk)) , (2) where denotes element-wise multiplication. The update in Eq. 2 is different from standard gradientdescent update in many ways: the update is multiplicative as opposed to additive, involves exponentiation, and preserves the sign of the entries of Xk as iterations proceed. Effectively, the entries in Xk for any have the same sign as those in X0. The update rule in Eq. (2) is referred to as exponentiated gradient descent (EGD) [Kivinen and Warmuth, 1997]. By design, EGD doesnt allow synaptic flips and automatically respects Dales law during the course of training. The update rule also leads to the weights being distributed log-normally as demonstrated by Pogodin et al. [2024]. Exponentiated gradient-descent has been shown to perform on par with gradient descent for models trained on Mod-Cog tasks, although the final weight distributions are different. The networks for both updates are initialized with log-normal weights to adhere to experimental data that shows that the synaptic strengths of neurons in the brain are log-normally distributed [Dorkenwald et al., 2022, Loewenstein et al., 2011a]. The network trained with gradient descent had final weight distribution that was different from log-normal whereas the network trained with exponentiated gradient was log-normally distributed. Additionally, Cornford et al. [2024] have shown that learning with EGD is more robust to synaptic weight pruning and EGD outperforms gradient descent when relevant inputs are sparse and in particular, for continuous control tasks. Pogodin et al. [2024] showed that the distribution of converged weights depends on the geometry induced by the choice of the update algorithm. Gradient-descent updates implicitly assume Euclidean geometry, which is inconsistent with the log-normal weight distribution that is experimentally observed and is ill-suited to data arising in neuroscience. quick glance at Eq. (2) prompts the question: Does there exist sampling equivalent for the exponentiated gradient-descent update rule? This is inspired by the link between gradient-descent and Langevin dynamics as enunciated by Wibisono [2018]. In pursuit of an answer to this question, we realised the connection between the log-normally distributed weights observed at the end of exponentiated gradient descent and the sampling equation lies in geometric Brownian motion. The equilibrium distribution of GBM is the log-normal density and its time-reversal would give us the sampling formula we seek (discussed in Sec. 4)."
        },
        {
            "title": "3 Stochastic Differential Equations and Generative Modelling",
            "content": "Recent generative models such as diffusion models [Ho et al., 2020, Song et al., 2021a] and scorebased models rely heavily on the SDE framework. These models have been immensely successful in generating realistic samples across different data modalities such as images [Song et al., 2021c] 4 and audio [Richter et al., 2025]. The key idea is to construct stochastic process such that one starts with samples from the true, unknown density and progressively transforms them to samples from noisy, easy-to-sample-from density such as the isotropic Gaussian. The task of generation requires inverting the forward process which goes beyond mere time reversal due to the stochastic nature of the dynamics. Theoretical results [Anderson, 1982, Castanon, 1982, Song et al., 2021c] show that there exists corresponding reverse-time SDE for the forward process. The forward process is represented as dXt = h(Xt, t) dt + g(Xt, t) dWt, (3) , t) : Rd is the diffusion function, and Wt where h( denotes the standard Wiener process. We follow the Itô interpretation of SDEs throughout this paper. The corresponding reverse-time SDE for Eq. (3) is given by Rd is the drift function, g( , t) : Rd Rd dXt = (cid:0)h(Xt, t) [g(Xt, t)g(Xt, t)] g(Xt, t)g(Xt, t) log fX (Xt, t)(cid:1) dt +g(Xt, t)d Wt, where Wt is the standard Brownian motion and 2(x), is the row-wise divergence of the matrix-valued function (x) := (f 1(x), 2(x), Rd to the score function sθ : Rd 2021c] d. The issue with generating new samples from Eq. (4) is that we usually do not have access log fX (Xt, t) and this quantity is approximated using neural network Rd, which is trained by optimizing the denoising score-matching loss [Song et al., , d(x)) (x) := ( d(x)) 1(x), [0, 1] (4) , (cid:34) (θ) = U [0,1] pX0 pXtX0 Xt (cid:104) λ(t) (cid:13) (cid:13)sθ(Xt, t) log pXt X0 (Xt X0) (cid:35) , (cid:105) (cid:13) 2 (cid:13) 2 (5) where X0 (Xt and λ(t) is designed to stabilise training. log pXt X0) is determined by the forward SDE (Eq. (3) [Särkkä and Solin, 2019])"
        },
        {
            "title": "4 Geometric Brownian Motion",
            "content": "Brownian motion, originally introduced to model random particle motion [Feynman et al., 1965], is widely used in physics, biology, and signal processing to describe processes with independent and identically distributed (i.i.d.) increments. The resulting distribution is Gaussian following the Central Limit Theorem. For example, the Ornstein-Uhlenbeck SDE (OU-SDE) [Doob, 1942] models the position Yt of Brownian particle as dYt = µ dt + σ dWt, where Wt is Wiener process, yielding Yt = Y0 + µt + σWt, Gaussian process with mean µ and variance σ2. Alternatively, when the relative increments (or ratios) follow the Brownian motion, the resulting stochastic process is called the Geometric Brownian Motion (GBM). Black and Scholes [1973] pioneered the use of GBM for modeling the evolution of stock prices and financial assets in mathematical finance. Just as the normal distribution plays crucial role in Brownian motion, the log-normal distribution plays vital role in the analysis of GBM. Formally, random process Xt is said to follow Geometric Brownian Motion if it satisfies the SDE: (6) dXt = µXt dt + σXt dWt, where Wt is the Wiener process, and µ and σ are known as the percentage drift representing general trend and volatility coefficients representing the inherent stochasticity, respectively. The solution of Eq. (6) Xt evolves to follow log-normal distribution with parameters µ and σ2, i.e., Xt = X0 exp (cid:18)(cid:18) (cid:19) 1 σ2 µ (cid:19) + σWt . There exist several multivariate extensions of GBM [Hu, 2000]. We consider the element-wise extension of Eq. (6) for image data with the forward SDE for time [0, 1]: dXt = µ Xt dt + σXt dWt, (7) where Wiener process. This can be written equivalently, using Itˆos lemma, as denotes element-wise multiplication, µ Rd, σ > 0 and Wt denotes the multivariate 5 Figure 1: The forward and reverse-time SDEs for Geometric Brownian Motion (GBM). The forward SDE describes the evolution of clean image sample to noisy one that eventually becomes lognormally distributed, while the reverse-time SDE captures the dynamics of the process and generates new samples from the unknown density starting from log-normal noise. This is enabled by the knowledge of the unknown density manifesting through the score function. (cid:18) log Xt = µ (cid:19) 1 σ2 2 dt + σdWt, (8) where log is applied element-wise. The distribution of Xt, as it evolves according to Eq. (8), has i.i.d. entries that are log-normally distributed with parameters µ and σ2I, being the identity matrix. Starting from sample X0 from the unknown density pX0 , the solution to Eq. (8) is (cid:18)(cid:18) (cid:19) (cid:19) Xt = X0 exp µ 1 + σWt . σ2 This closed-form expression allows us to easily generate samples from the forward process at arbitrary time instants [0, 1]. The samples at the end of the forward process are log-normally distributed. We now seek to derive the corresponding reverse-time SDE that would enable us to generate samples from the unknown density pX0 starting from samples from the log-normal density. While one could use Eq. (4) to derive the corresponding reverse-time SDE, we propose simpler approach by defining an auxiliary stochastic process Yt = log Xt and leveraging score change-of-variables formula [Robbins, 2024]. This allows us to rewrite Eq. (8) as (cid:18) dYt = µ (cid:19) 1 σ2 2 dt + σdWt. (9) The reverse-time SDE corresponding to the forward SDE in Eq. (9) can be obtained by invoking Eq. (4) and is given by (cid:18) dYt = µ σ2 1 σ2 (cid:19) log pYt(Yt, t) dt + σdWt, (10) log pY (Yt, t) is the score function corresponding to Yt and 1 is vector of all ones. where We invoke the score change-of-variables formula [Robbins, 2024] that allows us to represent log pXt(Xt, t). log pYt(Yt, t) in terms of log pXt(Xt, t) as Thus, we rewrite Eq. (10) in terms of Xt and simplify it to obtain log pYt(Yt, t) = 1 + Xt (cid:19) log pXt(Xt, t) dt + σdWt. (11) dlog Xt = (cid:18) µ 3σ2 2 1 σ2Xt 6 To simulate the reverse-time SDE on computer, it must be discretized in time. We chose the time range [0, 1] with steps, which results in step-size of δ = 1 and for brevity, denote Xkδ as Xk, for = 0, . . . , 1. In particular, we choose the Euler-Maruyama discretization scheme [Higham, 2001] for Eq. (11) to get log Xk 1 = log Xk δ µ (cid:18) 3σ2 2 1 σ2 (Xt log pXt(Xt, t))(cid:12) (cid:12)t=kδ (cid:19) + δσZk, (12) (0, I) (the standard normal distribution), and since the log operates element-wise, where Zk exponentiating both sides gives (cid:18) (cid:18) Xk 1 = Xk exp δ µ (cid:19) 3σ2 1 + δσ2Xk log pXk (Xk, k) + δσZk (cid:19) . (13) The update rule in Eq. (13) is similar to the EGD update rule in Eq. (2). Consider the optimization problem with modification of the task error as Xt+1 = arg min (cid:20) ℓ(ξ(X)) + (cid:21) Dϕ(X, Xt) , 1 η (14) with the choice of ξ : Rd following multiplicative update rule Rd as ξ(i)(X) = 0.5 (cid:0)X (i)(cid:1)2 for = 1, 2, , d. This leads to the Xk+1 = Xk if we assume that ℓ(Xk)), with η = δσ2 and µ = 3σ2 exp ( ℓ(X) ηXk (15) the density pXk (Xk, k) is of the form pXk (Xk, k) = 2 , then the corresponding sampling step in Eq. (13) is X=Xk ) . Interestingly, 1 exp ( of the form Xk (16) (0, I). Therefore, the proposed sampler is structurally equivalent to the modified X=Xk +ηZk) , 1 = Xk ηXk ℓ(X) exp ( where Zk exponential gradient descent step in Eq. (15)."
        },
        {
            "title": "5 Multiplicative Score Matching",
            "content": "Following the definitions of explicit score-matching (ESM) loss and denoising score-matching (DSM) loss for the additive noise case [Vincent, 2011], we propose the multiplicative counterparts M-ESM(θ) and M-DSM(θ) as follows: M-ESM(θ) = Xt pXt M-DSM(θ) = pX0 pXtX0 Xt (cid:20) 1 2 (cid:20) 1 2 Xt log pXt(Xt) Xt sθ(Xt, t) (cid:21) , 2 2 and (17) Xt log pXt X0 (Xt X0) Xt sθ(Xt, t) (cid:21) . 2 2 (18) The two types of score-matching loss functions are related as follows. Theorem 5.1 (Multiplicative Denoising Score-Matching). Under standard assumptions on the density and the score function [Hyvärinen, 2005, Song et al., 2019] over the positive orthant Rd +, the multiplicative explicit score-matching (M-ESM) loss given in Eq. (17) and multiplicative denoising score-matching (M-DSM) loss given in Eq. (18) are equivalent up to constant, i.e., M-DSM(θ) = M-ESM(θ) + C, where is independent of θ. The proof is provided in the supplementary material. The usefulness of this result is explained next. We need the marginal score function log pXt(Xt) in the reverse-time SDE Eq. (13) but optimizing Eq. (17) is intractable since we do not have access to the true marginal score. The theorem provides us with means to optimize for sθ in terms of the conditional score X0), which can be derived from the forward SDE. The challenge in leveraging Eq. (13) to generate new samples arises from our lack of knowledge of log pXt(Xt). This function must be estimated by some form of score-matching. To this end, we propose the following score-matching loss X0 (Xt log pXt M-DSM(θ) = E pX0 pXtX0 X0 Xt (cid:20) 1 2 Xt log pXt X0(Xt X0) Xt sθ(Xt, t) (cid:21) . 2 2 (19) Algorithm 1 Multiplicative updates for generation using Geometric Brownian Motion (GBM). Require: σ, δ, µ, trained score network sθ (0, I), XN 1: 2: for 3: Zk 4: Xk 5: end for (0, I) 1 = Xk sθ(Xk, k) + σδZk + δσ2Xk 1 = exp (Z) 1 to 0 do σ2 2 1 exp µ (cid:17) (cid:17) (cid:16) (cid:16) δ In practice, this choice of the loss function allows us to train the score network sθ using samples from the forward SDE in Eq. (7) and the corresponding conditional score X0) evaluated at discrete instants of time = kδ can be computed using the forward SDE and the expression for the target in the loss function is given by X0(Xt log pXt Xt log pXt X0 (Xt X0) = (cid:18) 1 + (cid:18) 1 σ2tδ log Xk log X0 (cid:18) tδ µ σ2 2 (cid:19)(cid:19)(cid:19) 1 . (20) The proposed loss function in Eq. (19) is the multiplicative noise counterpart of the denoising scorematching loss proposed by Song et al. [2021c] for additive noise. To the best of our knowledge, this formulation of the score-matching loss and its manifestation in the multiplicative noise setting is new. It would be appropriate to remark here that the score term in Eq. (13) also arises in the score-matching loss proposed by Hyvärinen [2007] for non-negative real data given by NN(θ) = 1 2 X0 (cid:2) pX0 X0 log pX0 (X0) X0 sθ(X0) (cid:3) , 2 2 (21) where log pX0(X0) is the true score. Hyvärinen [2007]s formulation is static in the sense that it does not leverage the SDE, whereas we do. Hyvärinen [2007]s score-matching loss can also be seen as an instance of the multiplicative explicit score-matching loss (M-ESM) for = 0. Hyvärinen [2007]s motivation for introducing this loss function is to avoid the singularity at the origin for non-negative data. Our framework encapsulates this variant of the score-matching loss as special case. This is primarily due to the structure of GBM that assumes the log-normal distribution which implicitly restricts the samples to be positive. Thus, our framework generalizes the score-matching loss proposed by Hyvärinen [2007] to the case of multiplicative noise. 5.1 Image Generation using Multiplicative Score Matching The goal in diffusion-based image generative modeling is to construct two stochastic processes, as illustrated in Fig. 1 the forward process to generate noisy version of clean image and the reverse process to enable us to sample from the unknown density pX0. For the forward model, starting from an image X0 coming from the unknown density, the forward SDE in Eq. (8) can be used to generate noisy versions of X0 as follows (cid:19) (cid:18) (cid:18) (cid:19) , δ µ exp 2, and XN + δσZk Xk+1 = Xk σ2 2 (0, I). For the reverse for = 0, . . . , 1 is log-normally distributed and Zk process, i.e., generation, we can generate samples from the reverse-time SDE in Eq. (11) using the discretized version of the reverse-time SDE in Eq. (13) and the score model sθ( ) trained with the loss ). The new generation/sampling defined in Eq. (19) in place of the true score function procedure is summarized in Algorithm 1. The algorithm takes as input the parameters σ, δ, µ and the trained score network sθ and generates samples from the unknown density pX0 by iterating over steps. The algorithm starts with sample XN 1 from the log-normal distribution and iteratively updates the sample using the reverse-time SDE in Eq. (13). The final output should be sample X0 log pXt( pX0 . (22)"
        },
        {
            "title": "6 Experiments",
            "content": "We evaluate the generative performance of the proposed model1 by training the score model on standard datasets such as MNIST, Fashion-MNIST and Kuzushiji MNIST dataset used by Cornford 1Code for this paper is available at https://anonymous.4open.science/r/gbm_dale-CC20 8 Figure 2: Uncurated sample images generated from MNIST, Fashion-MNIST and Kuzushiji MNIST datasets, corresponding to the score model with minimum score-matching loss during training. et al. [2021]. The datasets are split as 60, 000 images for training and 10, 000 images for testing. All images are rescaled to have pixel values in the range [1, 2]. Note that the proposed framework requires non-negative dynamic range of pixel values. We choose = 1000 discretization levels for the forward SDE (7) and leads to the step size δ = 1/N . During sampling, we observed that the same step-size did not always work and we had to work with smaller step-sizes for each of the three datasets. The model is trained using the M-DSM loss defined in Eq. (19). The hyperparameters µ = σ2 2 1, σ and δ are set to 0.8 and 0.001, respectively. The model is trained for 200000 iterations and the checkpoints are saved every 5000 iterations as mentioned in [Song and Ermon, 2020] on two NVIDIA RTX 4090 and two A6000 GPUs. We perform exponential moving average for the saved checkpoints every 50000 iterations. The generated samples are shown in Figure 2, from where we observe that the visual quality of the generated images matches is on par with that of the ground truth. For quantitative assessment, we use Fréchet Inception Distance (FID) [Heusel et al., 2017] and Kernel Inception Distance (KID) [Binkowski et al., 2018] measured between 10, 000 images from the test dataset and the same number of generated images. Lower FID and KID values indicate superior generative performance. While both these metrics are not commonly used to quantify the generative performance for grayscale images, we follow Xu et al. [2023] and report these numbers for transparency and reproducibility (cf. Supplementary Material)."
        },
        {
            "title": "7 Conclusions",
            "content": "We proposed novel generative model based on Geometric Brownian Motion (GBM) and new technique for score-matching. We showed that the GBM framework is natural setting for modeling non-negative data and that the new multiplicative score-matching loss can be used effectively to train the model. The model is capable of generating new samples from image datasets like MNIST, Fashion MNIST and Kuzushiji MNIST. The results are promising from generative modeling perspective. The multiplicative score matching framework can also be suitably adapted for image denoising and restoration tasks where the forward model has multiplicative noise as opposed to the widely assumed additive noise. While this work focuses on log-normal noise, other distributions such as the gamma distribution, could also be considered with associated SDEs. This would broaden the applicability of the model to datasets and domains where various types of multiplicative noise are prevalent such as optical coherence tomography [Li et al., 2025] and synthetic aperture radar [Fracastoro et al., 2021], enabling more robust and versatile generative and restoration capabilities. Starting off with the results shown in the paper, one could also extend applicability of the proposed model to high-resolution images. Application to non-image data, such as financial time-series, is another potential direction for further research."
        },
        {
            "title": "Limitations",
            "content": "The proposed generative model requires large amount of training data and computational resources to achieve good performance, which can be constraint in some applications. In the true spirit of data-driven generation, some of the generated images do not have the same semantic meaning as 9 samples from the source dataset. Incorporating semantics into generative modeling is research direction by itself. Instead of cherry-picking the results, we reported them as obtained to highlight both the strengths and limitations of the proposed approach. The choice of hyperparameters, such as the noise schedule and learning rate, which are carefully tuned, can affect the performance of the model. However, this limitation is true of all deep generative models and not unique to ours."
        },
        {
            "title": "Broader Impact",
            "content": "The proposed approach of leveraging the GBM and multiplicative score-matching is novel and has the potential to advance the field of generative modeling along new lines. The model may find natural applicability in financial time-series modeling, forecasting, and generation. Ethical concerns pertaining to the use of generative models and the potential for misuse by generating biased, fake, or misleading content are all pervasive and the proposed framework is no exception."
        },
        {
            "title": "References",
            "content": "B. D. O. Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12(3):313326, 1982. ISSN 0304-4149. doi: https://doi.org/10.1016/0304-4149(82)90051-5. URL https://www.sciencedirect.com/science/article/pii/0304414982900515. A. Bansal, E. Borgnia, H.-M. Chu, J. Li, H. Kazemi, F. Huang, M. Goldblum, J. Geiping, and T. Goldstein. Inverting arbitrary image transforms without In Advances in Neural Information Processing Systems, volume 36, pages 41259 noise. 41282, 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ 80fe51a7d8d0c73ff7439c2a2554ed53-Paper-Conference.pdf. Cold Diffusion: H. C. Barron, T. P. Vogels, T. E. Behrens, and M. Ramaswami. Inhibitory engrams in perception and memory. Proceedings of the National Academy of Sciences, 114(26):66666674, 2017. doi: 10.1073/pnas.1701812114. URL https://www.pnas.org/doi/abs/10.1073/ pnas.1701812114. S. Bartunov, A. Santoro, B. A. Richards, L. Marris, G. E. Hinton, and T. P. Lillicrap. Assessing the scalability of biologically-motivated deep learning algorithms and architectures. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS18, page 93909400, Red Hook, NY, USA, 2018. M. Binkowski, D. J. Sutherland, M. Arbel, and A. Gretton. Demystifying MMD GANs. In International Conference on Learning Representations, 2018. URL https://openreview.net/ forum?id=r1lUOzWCW. F. Black and M. Scholes. The pricing of options and corporate liabilities. Journal of Political Economy, 81(3):637654, 1973. ISSN 00223808, 1537534X. URL http://www.jstor.org/ stable/1831029. S. Bubeck. Convex optimization: Algorithms and complexity. Found. Trends Mach. Learn., 8(34): 231357, Nov. 2015. ISSN 1935-8237. doi: 10.1561/2200000050. URL https://doi.org/10. 1561/2200000050. G. Buzsáki and K. Mizuseki. The log-dynamic brain: how skewed distributions affect network operations. Nature Reviews Neuroscience, 15(4):264278, 2014. doi: 10.1038/nrn3687. URL https://doi.org/10.1038/nrn3687. D. Castanon. Reverse-time diffusion processes (corresp.). IEEE Transactions on Information Theory, 28(6):953956, 1982. doi: 10.1109/TIT.1982.1056571. T. Clanuwat, M. Bober-Irizar, A. Kitamoto, A. Lamb, K. Yamamoto, and D. Ha. Deep learning for classical japanese literature, 2018. URL https://nips2018creativity.github.io/doc/deep_ learning_for_classical_japanese_literature.pdf. Presented at the Machine Learning for Creativity and Design Workshop, NeurIPS 2018, Montreal, Canada. 10 J. Cornford, D. Kalajdzievski, M. Leite, A. Lamarquette, D. M. Kullmann, and B. A. Richards. Learning to live with dales principle: {ANN}s with separate excitatory and inhibitory units. In International Conference on Learning Representations, 2021. URL https://openreview.net/ forum?id=eU776ZYxEpz. J. Cornford, R. Pogodin, A. Ghosh, K. Sheng, B. A. Bicknell, O. Codol, B. A. Clark, G. Lajoie, and B. A. Richards. Brain-like learning with exponentiated gradients. bioRxiv, 2024. doi: 10.1101/2024.10.25.620272. URL https://www.biorxiv.org/content/early/2024/10/ 26/2024.10.25.620272. J. L. Doob. The brownian movement and stochastic equations. Annals of Mathematics, 43(2): 351369, 1942. ISSN 0003486X, 19398980. URL http://www.jstor.org/stable/1968873. S. Dorkenwald, N. L. Turner, T. Macrina, K. Lee, R. Lu, J. Wu, A. L. Bodor, A. A. Bleckert, D. Brittain, N. Kemnitz, W. M. Silversmith, D. Ih, J. Zung, A. Zlateski, I. Tartavull, S.-C. Yu, S. Popovych, W. Wong, M. Castro, C. S. Jordan, A. M. Wilson, E. Froudarakis, J. Buchanan, M. M. Takeno, R. Torres, G. Mahalingam, F. Collman, C. M. Schneider-Mizell, D. J. Bumbarger, Y. Li, L. Becker, S. Suckow, J. Reimer, A. S. Tolias, N. Macarico da Costa, R. C. Reid, and H. S. Seung. Binary and analog variation of synapses between cortical pyramidal neurons. eLife, 11:e76120, nov 2022. ISSN 2050-084X. doi: 10.7554/eLife.76120. URL https://doi.org/10.7554/ eLife.76120. J. C. Eccles, P. Fatt, and K. Koketsu. Cholinergic and inhibitory synapses in pathway from motor-axon collaterals to motoneurones. The Journal of Physiology, 126(3):524562, 1954. doi: https://doi.org/10.1113/jphysiol.1954.sp005226. URL https://physoc.onlinelibrary. wiley.com/doi/abs/10.1113/jphysiol.1954.sp005226. R. Feynman, R. Leighton, M. Sands, and E. Hafner. The Feynman Lectures on Physics; Vol. I, volume 33. AAPT, 1965. G. Fracastoro, E. Magli, G. Poggi, G. Scarpa, D. Valsesia, and L. Verdoliva. Deep learning methods for synthetic aperture radar image despeckling: An overview of trends and perspectives. IEEE Geoscience and Remote Sensing Magazine, 9(2):2951, 2021. doi: 10.1109/MGRS.2021.3070956. I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, volume 27, 2014. URL https://proceedings.neurips.cc/paper_files/paper/2014/ file/f033ed80deb0234979a61f95710dbe25-Paper.pdf. M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. GANs trained by two time-scale update rule converge to local Nash equilibrium. In Advances in Neural Information Processing Systems, volume 30, 2017. URL https://proceedings.neurips.cc/paper_ files/paper/2017/file/8a1d694707eb0fefe65871369074926d-Paper.pdf. D. J. Higham. An algorithmic introduction to numerical simulation of stochastic differential equations. SIAM Review, 43(3):525546, 2001. doi: 10.1137/S0036144500378302. URL https://doi. org/10.1137/S0036144500378302. J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, NeurIPS, 2020. S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):17351780, 1997. doi: 10.1162/neco.1997.9.8.1735. Y. Hu. Multi-dimensional geometric brownian motions, onsager-machlup functions, and applications to mathematical finance. Acta Mathematica Scientia, 20(3):341358, 2000. ISSN 0252-9602. doi: https://doi.org/10.1016/S0252-9602(17)30641-0. URL https://www.sciencedirect.com/ science/article/pii/S0252960217306410. A. Hyvärinen. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6(24), 2005. URL http://jmlr.org/papers/v6/hyvarinen05a.html. 11 A. Hyvärinen. Some extensions of score matching. Computational Statistics & Data Analysis, 51 (5):24992512, 2007. ISSN 0167-9473. doi: https://doi.org/10.1016/j.csda.2006.09.003. URL https://www.sciencedirect.com/science/article/pii/S0167947306003264. T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=Hk99zCeAb. T. Karras, M. Aittala, J. Hellsten, S. Laine, J. Lehtinen, and T. Aila. Training generative adversarial networks with limited data. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1210412114. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/ 2020/file/8d30aa96e72440759f74bd2306c1fa3d-Paper.pdf. M. Khona, S. Chandra, J. J. Ma, and I. R. Fiete. Winning the lottery with neural connectivity constraints: Faster learning across cognitive tasks with spatially constrained sparse rnns. Neural Computation, 35(11):18501869, 2023. doi: 10.1162/neco_a_01613. J. Kivinen and M. K. Warmuth. Exponentiated gradient versus gradient descent for linear predictors. Information and Computation, 132(1):163, 1997. ISSN 0890-5401. doi: https://doi.org/ 10.1006/inco.1996.2612. URL https://www.sciencedirect.com/science/article/pii/ S0890540196926127. Q. V. Le, N. Jaitly, and G. E. Hinton. simple way to initialize recurrent networks of rectified linear units. CoRR, abs/1504.00941, 2015. URL http://arxiv.org/abs/1504.00941. Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):22782324, 1998. doi: 10.1109/5.726791. P. Li, J. Cornford, A. Ghosh, and B. Richards. Learning better with dales law: spectral perspective. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 944956. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/ file/02dd0db10c40092de3d9ec2508d12f60-Paper-Conference.pdf. S. Li, R. Higashita, H. Fu, B. Yang, and J. Liu. Score prior guided iterative solver for speckles removal in optical coherent tomography images. IEEE Journal of Biomedical and Health Informatics, 29 (1):248258, 2025. doi: 10.1109/JBHI.2024.3480928. T. P. Lillicrap, A. Santoro, L. Marris, C. J. Akerman, and G. Hinton. Backpropagation and the brain. Nature Reviews Neuroscience, 21(6):335346, 2020. doi: 10.1038/s41583-020-0277-3. URL https://doi.org/10.1038/s41583-020-0277-3. Y. Loewenstein, A. Kuras, and S. Rumpel. Multiplicative dynamics underlie the emergence of the log-normal distribution of spine sizes in the neocortex in vivo. Journal of Neuroscience, 31(26):94819488, 2011a. ISSN 0270-6474. doi: 10.1523/JNEUROSCI.6130-10.2011. URL https://www.jneurosci.org/content/31/26/9481. Y. Loewenstein, A. Kuras, and S. Rumpel. Multiplicative dynamics underlie the emergence of the log-normal distribution of spine sizes in the neocortex in vivo. The Journal of Neuroscience, 31 (26):94819488, June 2011b. doi: 10.1523/JNEUROSCI.6130-10.2011. URL https://www. jneurosci.org/content/31/26/9481. I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In 9th International Conference on Learning Representations, ICLR, 2019. URL https://openreview.net/forum?id= Bkg6RiCqY7. S. MacNamara and G. Strang. Operator Splitting, pages 95114. Springer International Publishing, Cham, 2016. ISBN 978-3-319-41589-5. doi: 10.1007/978-3-319-41589-5_3. URL https: //doi.org/10.1007/978-3-319-41589-5_3. M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. Building large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313330, 1993. URL https: //aclanthology.org/J93-2004/. 12 J. B. Melander, A. Nayebi, B. C. Jongbloets, D. A. Fortin, M. Qin, S. Ganguli, T. Mao, and H. Zhong. Distinct in vivo dynamics of excitatory synapses onto cortical pyramidal neurons and parvalbumin-positive interneurons. Cell Reports, 37(6):109972, 2021. ISSN 2211-1247. doi: https://doi.org/10.1016/j.celrep.2021.109972. URL https://www.sciencedirect.com/ science/article/pii/S2211124721014510. A. S. Nemirovsky and D. B. Yudin. Problem complexity and method efficiency in optimization. SIAM Review, 27(2):264265, 1985. doi: 10.1137/1027074. URL https://doi.org/10.1137/ 1027074. G. Papamakarios, E. Nalisnick, D. J. Rezende, S. Mohamed, and B. Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. Journal of Machine Learning Research, 22(57): 164, 2021. URL http://jmlr.org/papers/v22/19-1028.html. R. Pogodin, J. Cornford, A. Ghosh, G. Gidel, G. Lajoie, and B. A. Richards. Synaptic weight distributions depend on the geometry of plasticity. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=x5txICnnjC. J. Richter, D. De Oliveira, and T. Gerkmann. Investigating training objectives for generative speech enhancement. In ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15, 2025. doi: 10.1109/ICASSP49660.2025.10887784. S. Rissanen, M. Heinonen, and A. Solin. Generative modelling with inverse heat dissipation. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=4PJUBT9f2Ol. S. Robbins. Score change of variables, 2024. URL https://arxiv.org/abs/2412.07904. J. E. Santos, Z. R. Fox, N. Lubbers, and Y. T. Lin. Blackout diffusion: generative diffusion models in discrete-state spaces. In Proceedings of the 40th International Conference on Machine Learning, ICML23. JMLR.org, 2023. S. Särkkä and A. Solin. Applied stochastic differential equations, volume 10. Cambridge University Press, 2019. J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Proceedings of the 32nd International Conference on Machine Learning, volume 37 of PMLR, Lille, France, 0709 Jul 2015. PMLR. URL https: //proceedings.mlr.press/v37/sohl-dickstein15.html. H. F. Song, G. R. Yang, and X.-J. Wang. Training excitatory-inhibitory recurrent neural networks for cognitive tasks: simple and flexible framework. PLoS Comput. Biol., 12(2):e1004792, Feb. 2016. J. Song, C. Meng, and S. Ermon. Denoising diffusion implicit models. In 9th International Conference on Learning Representations, ICLR, 2021a. URL https://openreview.net/forum?id= St1giarCHLP. S. Song, P. J. Sjöström, M. Reigl, S. Nelson, and D. B. Chklovskii. Highly nonrandom features of synaptic connectivity in local cortical circuits. PLoS Biology, 3(3):e68, 2005. doi: 10.1371/ journal.pbio.0030068. URL https://journals.plos.org/plosbiology/article?id=10. 1371/journal.pbio.0030068. Y. Song and S. Ermon. the data Information Processing Systems, volume 32, URL https://proceedings.neurips.cc/paper_files/paper/2019/file/ Generative modeling by estimating gradients of distribution. 2019. 3001ef257407d5a371a96dcd947c7d93-Paper.pdf. In Advances in Neural Y. Song and S. Ermon. Improved techniques for training score-based generative modIn Advances in Neural Information Processing Systems, volume 33, pages 12438 els. 12448, 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/ 92c3b916311a5517d9290576e3ea37ad-Paper.pdf. 13 Y. Song, S. Garg, J. Shi, and S. Ermon. Sliced score matching: scalable approach to density and score estimation. In Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI, 2019. URL http://auai.org/uai2019/proceedings/papers/204.pdf. Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based generative modeling through stochastic differential equations. In 9th International Conference on Learning Representations, ICLR, 2021b. URL https://openreview.net/forum?id=PxTIG12RRHS. Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021c. URL https://openreview.net/forum?id=PxTIG12RRHS. C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Los Alamitos, CA, USA, jun 2015. IEEE Computer Society. doi: 10.1109/CVPR.2015.7298594. URL https://doi.ieeecomputersociety.org/ 10.1109/CVPR.2015.7298594. P. Vincent. connection between score matching and denoising autoencoders. Neural Computation, 23(7), 2011. doi: 10.1162/NECO_a_00142. A. Vuong and T. Nguyen. Perception-based multiplicative noise removal using SDEs, 2024. URL https://arxiv.org/abs/2408.10283. J. C. R. Whittington and R. Bogacz. Theories of error Back-Propagation in the brain. Trends in Cognitive Sciences, 23(3):235250, Jan. 2019. A. Wibisono. Sampling as optimization in the space of measures: The Langevin dynamics as composite optimization problem. In Proceedings of the 31st Conference On Learning Theory, 2018. URL https://proceedings.mlr.press/v75/wibisono18a.html. H. Xiao, K. Rasul, and R. Vollgraf. Fashion-MNIST: novel image dataset for benchmarking machine learning algorithms, 2017. URL https://arxiv.org/abs/1708.07747. C. Xu, X. Cheng, and Y. Xie. Normalizing flow neural networks by JKO scheme. In Advances in Neural Information Processing Systems, volume 36, pages 47379 47405, 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ 93fce71def4e3cf418918805455d436f-Paper-Conference.pdf."
        },
        {
            "title": "A Notation",
            "content": "Random variables are denoted in uppercase, and random vectors are denoted by boldface uppercase. Their realizations are denoted using corresponding lowercase letters. The probability density function (p.d.f.) of random variable is denoted by pX (x) and for the random vector X, it is denoted by pX (x). The Stein score of the random vector evaluated at is denoted by log pX (x). Log-normal Distribution (µ, σ2), positive random variable is said to follow the log-normal distribution if log that is, log follows Gaussian distribution with mean µ and variance σ2. We denote this as LN (µ, σ2). The log-normal density is given by (cid:18) exp fW (w) = (log 2σ2 1 wσ2π 0, (cid:19) µ) , > 0, 0. (23) Note that µ and σ2 are not the mean and variance of the log-normal random variable. The mean and variance of the log-normal random variable are E[W ] = exp and Var(W ) = exp (cid:0)σ 1(cid:1) exp (cid:0)2µ + σ2(cid:1), respectively. µ + σ2 2 (cid:17) (cid:16) (0, I) The multivariate log-normal random vector is defined as = exp (µ + σZ) where and the exponentiation is applied element-wise. Effectively, the entries of are independent and (µ, σ2I). identically distributed according to Eq. (23). The corresponding density is denoted as LN Equivalence Between Multiplicative Denoising Score-Matching and Multiplicative Explicit Score-Matching Recall from Sec. 5 of the main document that the multiplicative explicit score-matching loss is given by (cid:20) 1 (cid:13) (cid:13) (cid:13)Xt Xt M-ESM(θ) = Xt and that the multiplicative denoising score-matching loss is given by (cid:20) 1 2 (cid:13) (cid:13) (cid:13)Xt log pXt(Xt) X0 (Xt M-DSM(θ) = log pXt X0) pXt X0 pX0 pXtX Xt (cid:21) sθ(Xt, t) (cid:13) 2 (cid:13) (cid:13) 2 , (24) sθ(Xt, t) (cid:13) 2 (cid:13) (cid:13) 2 (cid:21) . Xt (25) In the following result, we establish the equivalence between multiplicative explicit score-matching and multiplicative denoising score-matching loss. Theorem C.1 (Multiplicative Denoising Score-Matching). Under standard assumptions on the density and the score function [Hyvärinen, 2005, Song et al., 2019] over the positive orthant Rd +, the multiplicative explicit score-matching (M-ESM) loss given in Eq. (24) and multiplicative denoising score-matching (M-DSM) loss given in Eq. (25) are equivalent up to constant, i.e., M-DSM(θ) = M-ESM(θ) + C, where is independent of θ. Proof. We assume that the densities pXt and pXt supported over Rd xt 0, M-ESM(θ) to get X0 (defined in Sec. 4 of the main document) are +, and zero elsewhere. Further, we assume that pXt(xt) > 0, pXt X0 (xt x0) > [0, 1]. The expectations are evaluated over the support Rd +. We expand + for Rd M-ESM(θ) = Xt pXt (cid:20) 1 2 (cid:13) (cid:13) (cid:13)Xt log pXt(Xt) 2(cid:21) (cid:13) (cid:13) (cid:13) (cid:20) 1 + Xt (cid:13) (cid:13) (cid:13)Xt log pXt (Xt))(Xt pXt 2(cid:21) (cid:13) (cid:13) (cid:13) sθ(Xt, t) sθ(Xt, t))(cid:3) . (26) sθ(Xt, t))(cid:3) and express it as Xt pXt (cid:2)(Xt log pXt(Xt))(Xt Now, consider the cross-term Xt pXt an integral over Rd following integrals. The cross-term is given by (cid:2)(Xt +. For brevity of notation, we dont explicitly indicate the support Rd + in the pXt Xt (cid:2)(Xt log pXt(Xt))(Xt (cid:90) sθ(Xt, t))(cid:3) = = (cid:90) (xt log pXt(xt))(xt sθ(xt, t))pXt (xt) dxt (xt pXt(xt))(xt sθ(xt, t)) dxt. (27) We know that the marginal density pXt(xt) can be expressed in terms of the conditional density as (cid:90) pXt(xt) = pXt X0(xt x0)pX0 (x0) dx0. Computing the gradient with respect to xt on both sides yields (cid:90) pXt(xt) = pXt X0(xt x0)pX0 (x0) dx0. (28) (cid:90) (cid:18) (cid:90) = = = (cid:90) (cid:90) X0 Xt (xt pX0 pXtX0 Substituting Eq. (28) in Eq. (27), multiplying and dividing by pXt pXt Xt (cid:2)(Xt log pXt(Xt))(Xt sθ(Xt, t))(cid:3) X0(xt x0), we get xt pXt X0 (xt x0)pX0(x0) dx0 (cid:19) (xt sθ(xt, t)) dxt log pXt (cid:2)(Xt X0(xt log pXt x0))(xt X0 (Xt X0))(Xt X0(xt sθ(Xt, t))(cid:3) . sθ(xt, t)) pXt x0)pX0 (x0)dx0 dxt, (29) Substituting Eq. (29) in Eq. (26) gives the following equivalent expression for the multiplicative explicit score-matching loss: M-ESM(θ) = (cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:58) C1 log pXt(Xt) (cid:20) 1 2 (cid:13) (cid:13) (cid:13) 2(cid:21) Xt log pXt X0 (Xt + Xt (cid:20) 1 2 pXt X0))(Xt sθ(Xt, t) (cid:13) (cid:13) (cid:13)Xt sθ(Xt, t))(cid:3) 2(cid:21) (cid:13) (cid:13) (cid:13) X0 (cid:13) (cid:13) (cid:13)Xt (cid:2)(Xt pXt pX0 pXtX0 (cid:20) 1 (cid:13) (cid:13) (cid:13)Xt 2 (cid:2)(Xt pXt pX0 pXtX0 X0 Xt = Xt Xt sθ(Xt, t) 2(cid:21) (cid:13) (cid:13) (cid:13) log pXt X0 (Xt X0))(Xt sθ(Xt, t))(cid:3) + C1, (30) where C1 is constant that is not dependent on θ. We carry out similar simplification for the multiplicative denoising score-matching loss: M-DSM(θ) = Xt pX0 pXtX0 (cid:20) 1 2 (cid:13) (cid:13) (cid:13)Xt log pXt X0 (Xt X0) Xt sθ(Xt, t) (cid:13) 2 (cid:13) (cid:13) 2 (cid:21) , = (cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:58) C2 X0 (Xt log pXt (cid:20) 1 2 (cid:13) 2 (cid:13) (cid:13) 2 X0) X0 + Xt (cid:21) pX0 pXtX0 Xt pX0 pXtX0 pX0 pXtX0 Xt (cid:13) (cid:13) (cid:13)Xt (cid:2)(Xt log pXt X0(Xt X0))(sθ(Xt, t) Xt)(cid:3) , (cid:20) 1 2 (cid:13) (cid:13) (cid:13)Xt sθ(Xt, t) (cid:13) 2 (cid:13) (cid:13) (cid:21) , or equivalently, M-DSM(θ) = pXt Xt (cid:21) sθ(Xt, t) (cid:13) 2 (cid:13) (cid:13) 2 (cid:13) (cid:13) (cid:13)Xt (cid:20) 1 2 pX0 pXtX0 X0 Xt +C2, (cid:2)(Xt log pXt X0(Xt X0))(sθ(Xt, t) Xt)(cid:3) where C2 is constant that is not dependent on θ. On comparing Eq. (30) and Eq. (31), we get This concludes the proof. M-DSM(θ) = M-ESM(θ) + C2 C1. (31) (32) The implication of the result is as follows: multiplicative explicit score-matching loss is intractable since we do not have access to the true marginal scores, and, this equivalence allows us to optimize the score network parameters by minimizing the multiplicative denoising score-matching loss since the conditional scores can be tractably computed from the forward SDE (cf. Sec. 4)."
        },
        {
            "title": "D Additional Experimental Results",
            "content": "D.1 Architecture of the score network The base architecture is the conditional RefineNet architecture [Song and Ermon, 2019] with dilated convolutions, specifically designed for image generation tasks. The network follows an encoderdecoder structure with skip connections and conditioning is done through class labels using conditional normalization layers. We modify it to work for time-steps because we discretize the SDEs over steps. The key components are the encoder and the decoder. The encoder starts with convolutional layer (begin_conv), has multiple residual blocks organized in stages (res1-res5), performs progressive downsampling through the network, and uses conditional residual blocks that incorporate class information. On the other hand, the decoder uses conditional refine blocks (refine1-refine5), incorporates skip connections from encoder layers and performs progressive upsampling and refines features. D.2 Image datasets for evaluation As mentioned in the main document, we evaluate the proposed model on the following datasets: MNIST, Fashion-MNIST and Kuzushiji-MNIST. The MNIST dataset consists of 70,000 images of handwritten digits, each of size 28 28. The Fashion-MNIST dataset contains 70,000 images of 28. Kuzushiji MNIST is dataset of 70,000 images of handwritten clothing items, also of size 28 Kuzushiji (cursive Japanese) characters, each of size 28 28. The datasets are split into training and test sets, comprising 60,000 and 10,000 images, respectively. D.3 Training details We implemented the proposed model using PyTorch. For MNIST, the model is trained for 300k iterations, and for Fashion MNIST and Kuzushiji MNIST, the model is trained for 200k iterations. The chosen optimizer is AdamW optimizer [Loshchilov and Hutter, 2019]. The checkpoints are saved every 5k iterations as mentioned in [Song and Ermon, 2020]. The models are trained on two NVIDIA RTX 4090 and two NVIDIA A6000 GPUs. The model is trained using the Monte Carlo version of the score-matching loss defined in Eq. (25). ˆ M-DSM(θ) = 1 (cid:88) 1 (cid:88) i=1 k=0 (cid:20) 1 2 (cid:13) (cid:13)x(i) (cid:13) log pXk X0 (cid:16) x(i) (cid:12) (cid:12) x(i) (cid:17) x(i) sθ(x(i) , k) (cid:21) , (cid:13) 2 (cid:13) (cid:13) 2 (33) where = 0, . . . , 1 denotes the discretized time-step, and = 1, . . . , denotes the index of the ith sample. Effectively, we have samples from the training dataset used in the score estimation over time-steps. D.4 Sampling algorithm We observed that the sampler proposed in Algorithm 1 of the main document obtained by EulerMaruyama discretization sometimes generates images of suboptimal quality. To mitigate this effect, we propose slightly modified sampler with step-size that is annealed by factor χ < 1 to progressively reduce the effect of noise during sampling, and repeated sampling steps for each noise level. The modified sampler with the annealed step-size is listed in Algorithm 2. The modification improved the quality of the generated samples. Additionally, the step-size annealing can be viewed as special case of operator splitting methods used in the discretization of SDEs [MacNamara and Strang, 2016]. For the initialization, we must draw sample XN 1 from the log-normal density, whose parameters ˆµ, ˆσ are obtained by fitting log-normal density to the histogram of pixel intensities of the samples at the end of the forward process. In order to simplify the update, we choose µ = σ2 χ = 0.995 and = 3, δ = 2 2 1. We found out empirically that σ = 0.8, 4 gave the best results. 10 17 Algorithm 2 Annealed multiplicative updates for generation using Geometric Brownian Motion. Require: σ, δ, µ, L, κ, χ, ˆµ, ˆσ, trained score network sθ κ = 1 2: XN for ( ˆµ, ˆσ2I) 1 to 1 do 1 LN 1 to do for (0, I) Zk,j Xk 1 = Xk end for κ χ κ end for 4: 6: 8: exp (cid:16) µ (cid:16) δ (cid:17) 3σ2 2 1 + δσ2Xk sθ(Xk, k) + κσδZk,j (cid:17)"
        },
        {
            "title": "E Generated Samples",
            "content": "We present samples generated by the proposed model on MNIST, Fashion MNIST and Kuzushiji MNIST datasets in Figs. 3 to 5. The samples are generated using the trained model and the sampling algorithm described in Algorithm 2. We observe that the generated samples are diverse and resemble the training data. They are also noise-free, which goes to show that the annealed multiplicative sampling update is quite robust. There are some samples that are entirely novel and are not identical to the training data. This effect is more pronounced in MNIST and Kuzushiji MNIST datasets. Samples from the Fashion MNIST dataset are less diverse and seem to have latched on to certain modes of the training data. This is by no means evidence of mode collapse but certain classes are underrepresented in the generation. This is probably because the Fashion MNIST dataset is more complex and has more variability in the images compared to MNIST and Kuzushiji MNIST. Understanding the reason behind this phenomenon requires further investigation. 18 E.1 MNIST Figure 3: The samples have high diversity and the model even generates samples that are not present in the training data but have semantic similarity to the training data. 19 E.2 Kuzushiji MNIST Figure 4: Generated Kuzushiji samples. The generated samples are sufficiently diverse and sharp and distinct from the training data. 20 E.3 Fashion MNIST Figure 5: Generated Fashion MNIST samples. We observe less diversity of the generated samples here compared to MNIST and Kuzushiji MNIST possibly due to the complexity of the training data."
        },
        {
            "title": "F Evaluation Metrics for the Generated Images",
            "content": "We use the following metrics to evaluate the quality of the generated images: Fréchet Inception Distance (FID) [Heusel et al., 2017], which measures the distance between the distribution of generated images and real images in the feature space of pre-trained InceptionV3 network [Szegedy et al., 2015]. Lower values indicate better quality. Kernel Inception Distance (KID) [Binkowski et al., 2018], which is similar to FID, but uses kernel to measure the distance between distributions. It is less sensitive to outliers and is more robust for small sample sizes. Nearest neighbours from training data, which is qualitative measure of how closely the generated samples resemble the training data and to rule out the possibility of memorization of the training samples. The nearest neighbours are identified by measuring the Euclidean 21 distance between generated samples and images from the training data with distances measured both in the pixel space and InceptionV3 feature space. F.1 FID and KID We compute the FID and KID scores using the torcheval library and torchmetrics library for 50k generated samples and 50k real samples from the test set. This is done for grayscale images by repeating the image across the three colour channels and resizing it to 229 229 to match the input dimension expected by the InceptionV3 network. We report the best FID and KID scores obtained in Table 1. We observe that the FID and KID scores are lower for MNIST compared to Kuzushiji MNIST and Fashion MNIST. This is because MNIST is relatively simpler dataset with less variability compared to Kuzushiji MNIST and Fashion MNIST. The FID and KID scores are higher for Fashion MNIST compared to MNIST, indicating that the generated samples are of lower quality and less diversity as evidenced by the samples in Fig. 5. Table 1: FID and KID scores for the samples generated using the proposed model. The scores are computed using 50k generated samples and 10k real samples from the test set. Dataset MNIST Fashion MNIST Kuzushiji MNIST FID 28.9616 116.1499 50. KID 0.0287 0.4374 0.0546 0.0015 0.0044 0.0021 On an absolute scale, the FID and KID scores obtained are below par that of the state-of-the-art diffusion models, which have evolved significantly over the past decade. However, considering that this is the first-ever model founded on geometric Brownian motion, Dales law, and multiplicative updates, the FID and KID scores obtained are definitely encouraging and have lot of scope for improvement in subsequent work. We have also addressed possible future directions in the main document with respect to applying the proposed model on high-resolution image data. F.2 Nearest neighbours We identify the 10 nearest neighbours from the training data using the Euclidean distance between the generated samples and the training samples. The results are displayed in Figs. 6 to 11 of this document. We observe that the generated samples are semantically similar to the training samples, but not identical. This indicates that the model has the capability to generate diverse samples following the underlying distribution and that it does not memorize the training data. The nearest neighbours corresponding to both the pixel space and InceptionV3 feature space are shown in the figures. 22 F.2.1 Nearest neighbours MNIST Figure 6: 10 nearest neighbours (calculated using Euclidean distance on raw images) from MNIST training data for samples generated using the proposed model. The last four rows show different instances of the digit 8, which are quite diverse. Similarly, the two instances of the digit 4 generated are visually quite different. These results show that there is enough diversity in the generated samples and no mode collapse whatsoever. This stands testimony to the robustness of the proposed multiplicative denoising score-matching framework. 23 Figure 7: 10 nearest neighbours (calculated using Euclidean distance on InceptionV3 features) from the training data for samples generated. As mentioned in the caption of Fig. 6, there is sufficient diversity in the generated images. The nearest neighbours identified in the InceptionV3 space are not always semantically similar to the generated digit. For example, instances of digits 0 and 6 show up in the ten nearest neighbours of digit 4. 24 F.3 Nearest neighbours Kuzushiji MNIST Figure 8: 10 nearest neighbours (calculated using Euclidean distance on raw images) from the training data for samples generated. Here, again, we observe sufficient diversity of the generated characters and semantic similarity with the top 10 nearest neighbours. 25 Figure 9: 10 nearest neighbours (calculated using Euclidean distance on InceptionV3 features) from the training data for samples generated. F.4 Nearest neighbours Fashion MNIST Figure 10: 10 nearest neighbours (calculated using Euclidean distance on raw images) from the training data for samples generated. Compared to MNIST and Kuzushiji MNIST, these samples have less diversity and seem to focus on specific modes (although not collapsing on the mode) in the underlying data distribution. 27 Figure 11: 10 nearest neighbours (calculated using Euclidean distance on InceptionV3 features) from the training data for samples generated."
        }
    ],
    "affiliations": [
        "Department of Electrical Engineering Indian Institute of Science Bengaluru 560012"
    ]
}