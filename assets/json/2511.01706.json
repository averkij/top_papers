{
    "paper_title": "Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement",
    "authors": [
        "Sekh Mainul Islam",
        "Pepa Atanasova",
        "Isabelle Augenstein"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Natural Language Explanations (NLEs) describe how Large Language Models (LLMs) make decisions, drawing on both external Context Knowledge (CK) and Parametric Knowledge (PK) stored in model weights. Understanding their interaction is key to assessing the grounding of NLEs, yet it remains underexplored. Prior work has largely examined only single-step generation, typically the final answer, and has modelled PK and CK interaction only as a binary choice in a rank-1 subspace. This overlooks richer forms of interaction, such as complementary or supportive knowledge. We propose a novel rank-2 projection subspace that disentangles PK and CK contributions more accurately and use it for the first multi-step analysis of knowledge interactions across longer NLE sequences. Experiments on four QA datasets and three open-weight instruction-tuned LLMs show that diverse knowledge interactions are poorly represented in a rank-1 subspace but are effectively captured in our rank-2 formulation. Our multi-step analysis reveals that hallucinated NLEs align strongly with the PK direction, context-faithful ones balance PK and CK, and Chain-of-Thought prompting for NLEs shifts generated NLEs toward CK by reducing PK reliance. This work provides the first framework for systematic studies of multi-step knowledge interactions in LLMs through a richer rank-2 subspace disentanglement. Code and data: https://github.com/copenlu/pk-ck-knowledge-disentanglement."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 6 0 7 1 0 . 1 1 5 2 : r Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement"
        },
        {
            "title": "Isabelle Augenstein",
            "content": "University of Copenhagen {seis, pepa, augenstein}@di.ku.dk"
        },
        {
            "title": "Abstract",
            "content": "Natural Language Explanations (NLEs) describe how Large Language Models (LLMs) make decisions, drawing on both external Context Knowledge (CK) and Parametric Knowledge (PK) stored in model weights. Understanding their interaction is key to assessing the grounding of NLEs, yet it remains underexplored. Prior work has largely examined only single-step generation typically the final answer, and has modelled PKCK interaction only as binary choice in rank-1 subspace. This overlooks richer forms of interaction, such as complementary or supportive knowledge. We propose novel rank-2 projection subspace that disentangles PK and CK contributions more accurately and use it for the first multi-step analysis of knowledge interactions across longer NLE sequences. Experiments on four QA datasets and three open-weight instruction-tuned LLMs show that diverse knowledge interactions are poorly represented in rank-1 subspace but are effectively captured in our rank-2 formulation. Our multi-step analysis reveals that hallucinated NLEs align strongly with the PK direction, context-faithful ones balance PK and CK, and Chain-of-Thought prompting for NLEs shifts generated NLEs toward CK by reducing PK reliance. This work provides the first framework for systematic studies of multi-step knowledge interactions in LLMs through richer rank-2 subspace disentanglement."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) are employed to generate Natural Language Explanations (NLEs) in human-readable format, illustrating the underlying decision-making process for predictions 1Code and data: https://github.com/copenlu/ pk-ck-knowledge-disentanglement in complex reasoning tasks such as Claim Verification (CV) and Question Answering (QA). These NLEs are valuable because they can reveal the utilization of external context and the knowledge stored in model parameters. Consider the Fig. 1 example from QA task (Cheng et al., 2024), where Llama-3.1-8B-Instruct (Meta-Team, 2024) generates the NLE by utilizing both Context Knowledge (CK) and Parametric Knowledge (PK), to explain the underlying decision-making process for the final answer prediction. For some tasks, such as CV, we assume the decision-making process and, in turn, the NLE, will rely more on evidence (CK) (Wang and Shu, 2023; Tan et al., 2025), and in QA tasks with misleading external context, we assume they will rely on the PK. On the other hand, Chain-of-Thought (CoT) prompting (Wei et al., 2022), widely adopted reasoning methodology, explicitly elicits NLEs of the intermediate reasoning steps and is hypothesized to influence how LLMs integrate PK and CK (Cheng et al., 2024; Su et al., 2024; Tao et al., 2025), potentially improving contextual grounding. However, it remains unclear what the learned PK-CK interaction dynamics in generating the NLEs are, necessitating multi-step analysis of PK-CK interaction across longer NLE sequences. Prior work (Longpre et al., 2021; Xu et al., 2024; Minder et al., 2025) has primarily focused on uncovering the single-step generation mechanism typically the final answer, and modelled only conflicting PKCK interaction as binary choice in rank-1 subspace, thereby overlooking richer forms such as complementary or supportive knowledge (Cheng et al., 2024). We thus hypothesize that this rank-1 subspace is not sufficient to disentangle the individual contributions of PK and CK in all types of knowledge interaction scenarios. Moreover, they did not characterize the step-by-step PK-CK interaction dynamics over longer sequences, such as NLE generation for Figure 1: Llama-3.1-8B-Instruct model combines parametric (green) and contextual (red) knowledge to generate NLEs. Projection onto learned low-rank subspace disentangles their contributions rank-1 discards richer interactions, while rank-2 separates complementary and conflicting components. different knowledge interactions. To accurately understand the knowledge interaction dynamics during NLE generation, we investigate the following research questions: RQ1. Is rank-1 projection subspace enough for disentangling PK and CK contributions in all types of knowledge interaction scenarios? RQ2. How do individual PK and CK contributions change over the NLE generation steps for different knowledge interactions? RQ3. Can we find reasons for hallucinations PK dominance for the sequences with hallucinated spans (RQ3, 4.4). Finally, for the RQ4 (4.5), we observe that the CoT mechanism is also encoded in the LLM as low-rank space, and the CoT encoding subspace closely aligns with the context direction of the rank-2 knowledge interaction subspace. Overall, this work advances understanding of how LLMs integrate internal and external knowledge by introducing the first systematic framework for multi-step analysis of knowledge interactions via rank-2 subspace disentanglement. based on PK-CK interactions?"
        },
        {
            "title": "2 Related Work",
            "content": "RQ4. How is the CoT mechanism aligned with the knowledge interaction subspace? We perform experiments on four publicly three open-weight available QA datasets for instruction-tuned LMs. In RQ1 (4.2), we find that the rank-1 subspace (Minder et al., 2025) fails to disentangle individual knowledge contributions for different knowledge interactions. For RQ2 (4.3), using more accurate rank-2 subspace disentanglement, we find that, during NLE generation, the model utilizes both knowledge sources with slight prioritization of PK; towards the final answer generation, the model aligns closely with the CK direction for conflicting examples; for supportive examples, it aligns more with the PK direction. This learned rank-2 subspace also illustrates Parametric vs. contextual knowledge in LMs. Prior work has extensively examined what LMs store as parametric knowledge (PK) and how they integrate context knowledge (CK) during inference (Petroni et al., 2019; Jiang et al., 2020; Roberts et al., 2020; Hagström et al., 2025; Marjanovic et al., 2024; Yu et al., 2024). Early studies showed that LMs act as factual knowledge bases, with knowledge retrievable via prompts (Petroni et al., 2019), while probing analyses disentangled what is encoded in the parameters versus what is acquired from context (Brown et al., 2020; Tenney et al., 2019; Bi et al., 2025). Recent research has explored the interplay and conflict between PK and CK. Cheng et al. (2024) revealed PK suppression under various knowltypes (contextmemory, edge interactions, and Xu et al. (2024) cateintergorized conflict context, intra-memory) to study resulting behavioral shifts. Enhancing PKCK balance has been pursued through improved pretraining or finetuning (Zhang et al., 2024), context-aware representation interventions (Yuan et al., 2025b), contrastive decoding (Zhao et al., 2024), and lightweight steering methods that increase context sensitivity without weight updates (Wang et al., 2025). Our work extends this literature by providing geometric framework that accurately models PKCK interactions in rank-2 subspace and enables multi-step analysis of how both knowledge sources jointly shape Natural Language Explanations (NLEs). NLEs and explanation-guided reasoning. Natural Language Explanations (NLEs) (Camburu et al., 2018; Atanasova et al., 2020) have been used to expose, supervise, and steer model reasoning in complex tasks. Human-written NLEs improve commonsense QA via explanation-based fine-tuning (Rajani et al., 2019), while Chainof-Thought (CoT) prompting elicits stepwise reasoning that boosts performance on multi-hop and arithmetic tasks (Wei et al., 2022). Explanations have also been used as few-shot exemplars to enhance in-context learning (Lampinen et al., 2022). Despite their utility, studies reveal that NLEs often fail to reflect true model reasoning: counterfactual and reconstruction tests expose limited faithfulness (Atanasova et al., 2023; Siegel et al., 2024; Yuan et al., 2025a; Wang and Atanasova, 2025), and CoT explanations can rationalize biased outputs rather than genuine reasoning (Turpin et al., 2023). Our work complements these findings by providing geometric, token-level analysis of NLE generation, quantifying how parametric and contextual knowledge jointly shape explanation faithfulness and hallucination. Probing, subspaces, and identifiability. Probing methods uncover low-dimensional structures in language model representations that encode linguistic or factual properties. Hewitt and Manning (2019) showed that syntactic relations are linearly recoverable from word representations, with parse depth and dependency distance captured via geometric transformations. Clark et al. (2019) identified specific BERT attention heads that encode syntactic and semantic cues through Elhage et al. (2022) analyzed linear probes. neuronfeature mappings and proposed that LMs compress information via low-dimensional orthogonal projections, representing dense and sparse features through feature superposition. Building on this view, Minder et al. (2025) modeled PKCK conflict as single controllable direction rank-1 projection subspace. However, if PK and CK are superposed within the representation space, single direction cannot uniquely disentangle their contributions. Motivated by this, we generalize to rank-2 projection subspace that establishes identifiability of tokenlevel PK and CK components. We futher use this more accurate representation of CK-PK interactions to provide the first multi-step analysis of their interaction dynamics during NLE generation."
        },
        {
            "title": "3 Method",
            "content": "3.1 Task Formulation Let and denote two disjoint sets of queries and contexts, respectively, and consider QA dataset C. For questioncontext pair (q, c) D, with and C, language model generates both an answer and an NLE with tokens as = {ei}n i=1. We distinguish between three forms of answer generation: (i) the parametric answer a(q, ε), produced by recalling PK (independent of c, ε denotes no context), (ii) the contextual answer a(q, c), obtained by leveraging the provided context while disregarding parametric recall, and (iii) the final predicted answer combining both the provided context and the PK recall guided by the PK-CK knowledge interaction. During the sequential generation of E, each token ei is influenced by the interaction between a(q, ε) and a(q, c). This evolving interaction guides the generation of NLE E, illustrating the decision-making process of the final predicted answer a. To analyze this process, we quantify the contribution of parametric knowledge (αp ) and contextual knowledge (αc ) at each generation step i, and we define their difference as = αi αi c, [1, n] (1) By tracking across all NLE generation steps, we aim to characterize the interaction and the shifting balance between parametric and contextual sources throughout NLE generation. 3.2 Identify Different PK-CK Interactions To characterize different types of PKCK interactions guided by individual knowledge contribution, we draw from (Minder et al., 2025), who analyze intent-driven answer control, i.e., controlling the model towards specific answer generation aligned with the intent of following either PK or CK. Let wc (instruction to follow the CK only) and wp (instruction to follow the PK only) denote intents toward predicting a(q, c) and a(q, ε), respectively, for given (q, c). The joint intent wb = {wc, wp} shapes how the model balances contextual and parametric influences. Formally, this intent is encoded within the parameters of p, creating causal relationship between and the observed dynamics. The prompt template for the three intent-driven answer control is described in Tab. 1 in A.1.2. Minder et al. (2025) restrict intent to conflicting cases, where a(q, c) = a(q, ε) and the final answer is determined by either wc or wp. We generalize this, following Cheng et al. (2024), to encompass broader set of interactions: Supportive: a(q, c) = a(q, ε); PK and CK reinforce the same outcome. Complementary: = a(q, ε) but a(q, c) = a(q, c) a(q, ε); PK and CK provide non-overlapping but mutually useful contributions. Conflicting: a(q, c) = a(q, ε) and reflects either wc or wp exclusively. Irrelevant: = a(q, ε) while a(q, ε) c; PK dominates, ignoring contextual input. This taxonomy enables systematic investigation of how PK and CK interact throughout NLE generation, moving beyond binary conflict to capture spectrum of dynamics. We reformulate this intent-driven answer control and wb denote the joint intent of wc and wp that illustrates the intrinsic model behaviour in generating the final answer a, considering both the PK and CK governed by the underlying interaction scenario. Unlike in Minder et al. (2025), where wc and wp are represented using one orthonormal direction in learned rank-1 projection subspace, we represent wc and wp using two orthonormal directions, and wb wc and wb wp indicate the individual contributions of CK and PK, respectively, in generating a. Then the joint intent wb = (wc, wp) indicates different knowledge interaction scenarios decided by individual contributions from wc and wp. We learn this knowledge interaction function using rank-2 projection subspace. 3.3 Localize Intent-Guided PK-CK Interactions In this section, we describe where the intentguided knowledge interaction emerges in the model space, followed by how the LM encodes it. Identifying important layers for rank-1 projection subspace. We follow the activation patching-based mechanistic interpretability approach, Patchscope (Minder et al., 2025). They construct two minimally different prompts and as source and target, with the same and c, only differing by the intent, resulting in two different intended answers. To identify the layers capturing the intent present in s, during the forward pass of the target p(.t), the activation from the hidden state of particular layer is replaced by the activations from the same layer during the forward pass with the source p(.s), resulting in p(.t) p(.s). Since, they only consider the conflicting behaviours in understanding the knowledge intearction in LMs, they construct two patching datasets capturing opposite directions: D(cp) = {((q, c, wc), a(q, c)), ((q, c, wp), a(q, ε))} for en- = coding the CK direction, {((q, c, wp), a(q, ε)), ((q, c, wc), a(q, c))} for encoding the PK direction. Layers capturing CK and PK directions, respectively, are selected as: and D(pc) Lcp = { [1, L] p(a(q, c) q, c, wp) τc }, Lpc = { [1, L] p(a(q, c) q, c, wp) τp }, (2) where τc, τp are hyperparameters (Tab 12 in A.1.3). For more details, please refer to Minder et al. (2025). Identifying important layers for rank-2 projection subspace (showing the individual knowledge contribution towards generating a, i.e, wb wc and wb wp). We again follow Patchscope and prepare two patching datasets D(bp) = {((q, c, wb), a), ((q, c, wp), a(q, ε))} for encoding the contribution of PK interacting with CK in generating a, and D(bc) = {((q, c, wb), a), ((q, c, wc), a(q, c))} for encoding the contribution of CK interacting with PK in genImportant layers Lbc and Lbp are erating a. selected using Eq. 2. Encoding the intent via rank-1 subspace projection. Once we identify the important layers capturing the behaviour of targeted knowledge interaction, we aim to identify how the LM encodes it. Minder et al. (2025) hypothesize that LMs encode knowledge interaction within their parameter space using low-rank projection subspace. They model this projection subspace Rdd by rank-1 unit norm direction Rd indicating the unidirectional PK-CK conflicting interaction as = uuT . At any sequence step i, the hidden representation hi Rd for the token ei in the NLE can be linearly decomposed as: hi = (I P) hi + hi = (I P) hi + uT , hi (3) (4) (I P) component of hi captures other properties in the embedding space, and the uT , hi captures the PK-CK knowledge contribution in the PK-CK conflicting direction u. We hypothesize that the rank-1 projection subspace can not disentangle the individual knowledge contributions for different knowledge interaction scenarios. We theoretically argue that fails to satisfy bijective properties of mapping from knowledge direction to individual knowledge contribution for all types of knowledge interaction. Theorem 1 (Non-identifiability under rank-1). Let the hidden representation hi for the input xi at the sequence step is decomposed as hi = ciuCK + piuP + ξi, where uCK, uP are orthonormal directions corresponding to context and parametric knowledge, ci, pi are their contributions, and ξi is noise orthogonal to their span. rank-1 probe with vector observes αi = hi = civ, uCK + piv, uP K. Then (ci, pi) are not uniquely identifiable from αi whenever both coefficients are nonzero. Proof. Let = v, uCK and = v, uP K. For any (c, p), choose (c, p) = (c + δ, + δ) with δ = 0. Then ac + bp = ac + bp, so infinitely many (c, p) yield the same observation. Thus the mapping (c, p) (cid:55) α is non-injective, and the individual contributions cannot be disentangled. Encoding the intent via rank-2 subspace projection. Once we identify important layers for wb wc and wb wp, we learn the joint intent function encoding how individual knowledge contributions wb wc and wb wp are mixed to generate the final answer using the common layers from Lbc and Lbp and rank-2 projection subspace spanned by two orthogonal directions Rd2 as = u(uuT )1uT . Since we aim to identify individual contributions from the individual directions, following (Minder et al., 2025), we consider those two basis vectors as orthonormal and hence the rank-2 projection subspace is reduced to = uuT , since uuT = I. Considering = [ uc; up], at any sequence step i, the hidden representation hi Rd for the token ei in the NLE can be linearly decomposed as: hi = (I P) hi + hi = (I P) hi + uT , hi = (I P) hi + uc uc , hi + up up (5) (6) , hi (7) Let ci = uc , hi and pi = up , hi, then the normalized contribution from CK and PK can = be computed as αi [0, 1] satisfying the pi/(ci + pi), and αi identifiability of individual knowledge contribution (Theorem 1) under rank-2 subspace. The methodology of assigning PK-CK direction to the basis vector is described in A.1.1. = ci/(ci + pi) and αi c, αi"
        },
        {
            "title": "4 Results",
            "content": "In this section, we empirically investigate the four research questions proposed in 1. 4.1 Experimental Setting Dataset and Model: We conduct experiments on three open-weight decoder-only instruct-based LMs: Llama-3.1-8B (Meta-Team, 2024), Gemma2 9B (Gemma-Team, 2024) and Mistral-v0.3 7B (Jiang et al., 2023) using four publicly available QA datasets: BaseFakepedia, MultihopFakepedia (Minder et al., 2025), StrategyQA (Geva et al., 2021), and OpenBookQA (Mihaylov et al., 2018; Cheng et al., 2024). BaseFakepedia is knowledge conflict dataset containing queries with 23 relations from Wikipedia. MultihopFakepedia is an extension of BaseFakepedia containing queries that require extra-hop reasoning in generating answers. StrategyQA is multi-hop reasoning QA dataset with implicit reasoning steps present in the queries. OpenBookQA is commonsense reasoning-based QA dataset. Evaluation Metric: To empirically verify the insufficiency of the rank-1 projection subspace in disentangling individual knowledge contributions, we utilize two specific metrics: Subspace component: For each knowledge interaction type, we compute the subspace component uT , ha of the hidden representation of the answer token ha, which is equivalent to the scalar component of the basis vector for the rank-1 projection subspace (Minder et al., 2025). If adequately captures PKCK interactions in the dataset, then the distribution of these subspace components should have mean significantly different from 0. Such pattern would indicate minimal contribution in the orthogonal complement (I P) and reflect distinct, disentangled interaction behavior across different knowledge types. Explained Variance (Cumulative): Consider the matrix = [{ haj}N j=1], Rdd as the concatenation of hidden representation ha Rd of the answer token over examples. The singular values σ1 >> σ2, ... >> σd from the diagonal matrix Σ Rdd after the Singular Value Decomposition (SVD) of HH = AΣB indicates the strength of HH in orthonormal directions. For top-r singular values σ1 >> σ2, ... >> σr, the cumulative explained variance EVr: Figure 2: Kernel Density Estimate (KDE) of the PK-CK subspace component uT , hi across different knowledge interaction types for four QA datasets using Mistral-7B-Instruct-v0.3. The split noise denotes cases where answers from individual knowledge sources agree with each other but differ from the final answer, i.e., a(q, c) = a(a, ε) and a(a, c) = a. EVr = (cid:80)r j=1 σ2 j/(cid:80)d j=1 σ2 (8) indicates the sufficiency of rank-r projection subspace in encoding the variance in knowledge interactions present in the dataset (Wall et al., 2003; Lazzaretto et al., 2025). If the rank-1 projection subspace adequately captures PK-CK interactions in the dataset, then the cumulative explained variance at rank-1 EV1 should approximate 1.0. 4.2 RQ1: Is Rank-1 Projection Subspace Enough for Disentangling PK and CK Contribution in All Types of Knowledge Interaction Scenarios? Fig. 2 illustrates the Kernel Density Estimate plot of subspace components from the Mistral-7BInstruct-v0.3 model, indicating the histogram distribution of different types of knowledge interactions smoothed using Gaussian kernel parameterised by the kernel width as hyperparameter. Across all datasets and knowledge interaction types (3.2), the mean of the subspace component distribution converges to 0, indicating significant contribution from the orthogonal complement Figure 3: Cumulative explained variance (EVr) at rank(r) from the three models using the four QA datasets. At rank-2, it reaches 1.0 value, indicating sufficiency in capturing different knowledge interaction variants. (I P), showing insufficiency of the rank-1 projection subspace in encoding the PK-CK interaction. This observed behaviour for the complementary and supportive interactions supports our hypothesis that, for examples where both PK and CK contribute equally, the rank-1 fails to distinctly encode individual contributions. However, surprisingly, we observe similar behaviour for the conflicting interaction type as well, illustrating that fails to also differentiate where the conflicts arise from. We observe similar results for the other two models (see App. A.2). These results indicate that different knowledge interactions are poorly captured by the rank-1 projection subspace, with most of the interaction signal residing in the orthogonal complement (I P), thereby suggesting the necessity of higher-rank subspace to effectively disentangle PKCK contributions. To understand the minimum rank required for the learnt projection subspace where the orthogonal complement contribution is minimum and it encodes different knowledge interaction types, we plot the cumulative explained variance EVr for different ranks across all datasets and models in Fig. 3. EVr reaches 1.0 at rank-2 and converges thereafter for all datasets and models. 4.3 RQ2: How Do Individual PK and CK Contributions Change Over the NLE Generation for Different Knowledge Interactions? (a) D(bp) (b) D(bc) Figure 4: Patchscope on OpenBookQA from MetaLlama-3.1-8B-Instruct. a) Activation patching on D(bp) . b) Activation patching on D(bc) . In this section, we investigate the dynamics of and αi individual PK and CK contribution αi at every sequence step over the NLE generation. Among the four datasets, OpenBookQA shows the largest jump in cumulative explained variance from rank-1 to rank-2 across all models  (Fig. 3)  . We therefore use OpenBookQA as our development set to (i) locate intent-sensitive layers via activation patching and (ii) learn the rank-2 projection subspace. We then freeze these choices and evaluate the resulting rank-2 subspace on all datasets. We begin by identifying important layers for encoding individual knowledge contributions towards the final answer generation in the rank2 projection subspace. Fig. 4 shows that patching from BOTHPRI (L1318) yields larger probability gain than BOTHCTX (L1517), indicating that Llama-3.1-8B-Instruct relies more on PK than on CK for OpenBookQA. We observe similar results for gemma-2-9b-it and Mistral-7BFigure 5: Individual PK-CK contribution in generating the answer token for all the datsets from Meta-Llama3.1-8B-Instruct model. Figure 6: Distribution of different knowledge interaction types present in the dataset. Instruct-v0.3 (see Fig. 15, 16, App). This PK dominance in context-rich task suggests that the model tends to recall commonsense information from memory rather than grounding answers in the provided context, highlighting potential for future work on context-sensitive knowledge control. and αi To identify the individual knowledge contribution in generating the final answer token its hidden representation ha to a, we project the rank-2 projection subspace to compute the as the PK-CK contributions. Fig. αi 5 indicates an overall higher CK contribution for the BaseFakepedia and MultihopFakepedia, and higher PK contribution for the StrategyQA and OpenBookQA for the Meta-Llama-3.1-8BInstruct model. This is consistent with dataset designs: Fakepedia variants are evidence-centric and often adversarial/conflicting, pushing the model to prefer the provided context; StrategyQA/OpenBookQA rely more on commonsense priors and sparse cues, which encourages parametric recall. To understand the causal reason behind this knowledge interaction behaviour for (a) Overall (b) Supportive (c) Complementary (d) Conflicting Figure 7: PK-CK interaction dynamics over the sequence steps for different interaction scenarios for Meta-Llama3.1-8B-Instruct. The dotted red and blue lines indicate the mean and mode of NLE lengths. these datasets, we investigate distribution of different knowledge interactions in Fig. 6. We find that both BaseFakepedia and MultihopFakepedia contain more conflicting examples than other knowledge interaction types (as defined in 3.2). Prior works (Cheng et al., 2024; Tao et al., 2024) suggest that for conflicting examples, models tend to suppress PK when sufficient and relevant information is present in CK and for supporting examples, models rely more on PK, with CK acting as regularizer. Also, Tao et al. (2024) suggests that parametric recall is the default unless explicitly overridden by context. Overall, we conclude that for conflicting examples, the model aligns more with the CK direction and for supportive examples, the model aligns more with the PK direction in the rank-2 projection subspace, supporting observations from prior works. and αi To understand the knowledge interaction dynamics during NLE generation (prompt template is described in Tab. 11 in A.1.2), we analyze over all sequence steps variation in αi of NLE generation for different knowledge interaction scenarios. Fig. 7 shows that for all datasets, during most of the NLE generations, the model starts with higher CK, then considers both PK and CK with slight prioritisation of PK. However, for longer NLEs, CK and PK compete with each other with higher fluctuation. Longer NLEs indicate difficult examples with higher depth in multihop reasoning and higher token uncertainty (from Fig. 8), which force the model to iteratively reconcile PK with CK, resulting in this fluctuating behaviour. 9 illustrates the PK-CK knowledge interaction dynamics. The gap between PK and CK is much higher for the examples with hallucinated spans than for the examples with no hallucinated spans across the sequence steps. This result also aligns with similar observations of positive correlation of PK and hallucination in Sun et al. (2025). 4.5 RQ4: How is the CoT mechanism aligned with the knowledge interaction subspace? To verify whether reasoning-based prompting CoT helps the model to stay aligned with the CK and reduces reliance on PK, we compare the PKCK contribution in generating the final answer between standard prompting and CoT prompting (prompt template is described in Tab. 11 in A.1.2). Fig. 10 indicates that CoT maintains similar CK alignment compared to standard prompting for all the datasets, and also reduces PK alignment except for the OpenBookQA dataset."
        },
        {
            "title": "5 Discussion",
            "content": "PK-CK interaction is multidimensional, not binary. Our results provide new insight into how Large Language Models (LLMs) integrate Parametric Knowledge (PK) and Context Knowledge (CK) when generating Natural Language Explanations (NLEs). Prior work typically treats PKCK interaction as one-dimensional phenomenon (Longpre et al., 2021; Minder et al., 2025; Xu et al., 2024), assuming that models choose between relying on either internal parameters or external context. In contrast, our findings demonstrate that this interaction is inherently multidimensional. Our proposed rank-2 projection subspace captures not only conflicts but also complementary and supportive PK-CK relations, revealing that NLE generation also involves dynamic coordination rather than competition between the two knowledge sources. Rank-2 subspace enables identifiable PK and CK contributions. The inadequacy of the rank-1 representation highlights that prior linear or scalar formulations collapse distinct interaction types, leading to inaccurate interpretations of knowledge interactions. By separating PK and CK directions, our framework enables identifiable tracking of their complementary individual contributions across sequence steps. This provides geometric perspective on how models negotiate between internal recall and contextual grounding throughFigure 8: Entropy of NLE generation across all datasets from Meta-Llama-3.1-8B-Instruct for different NLE lengths grouped in four quartiles. Figure 9: PK-CK interaction dynamics over the sequence step from Meta-Llama-3.1-8B-Instruct for the two RAG hallucination datasets. 4.4 RQ3: Can We Find Reasons for Hallucinations Based on PK-CK Interactions? To characterize the knowledge interaction dynamics during the NLE generation in terms of context faithfulness, we investigate the knowledge alignments of hallucinated vs non-hallucinated responses in the rank-2 projection subspace. We utilize two RAG hallucination datasets: RAGTruth and Dollo (AC) (Sun et al., 2025), of sizes 18240 and 297 examples respectively, containing examples from QA, Summarisation and Information Extraction. Each dataset provides human-annotated spans indicating hallucinated content across responses from multiple models. Importantly, whether span is labeled as hallucinated is model-dependent: the same RAG input may yield hallucinated text for one model but not for another. Due to the limitations in the number of models covered in the two datasets, we only consider the data split corresponding to the Meta-Llama-3.1-8B-Instruct model. Fig. Figure 10: Comparison in individual PK-CK contribution in generating the answer token for all the datasets from Meta-Llama3.1-8B-Instruct model between CoT and standard prompting. out reasoning, establishing mechanistic basis for assessing context-faithfulness of NLE. Causal Alignment of Hallucination with PK and CoT with CK in the Rank-2 Subspace. Empirically, the strong alignment of sequences with hallucinated spans with the PK direction extends causal findings from Sun et al. (2025), suggesting that hallucination reflects systematic bias toward parametric recall rather than random generation noise. Conversely, faithful and contextually grounded NLEs balance contributions from both knowledge axes, indicating that equilibrium in the learned subspace corresponds to factual reliability. Similarly, our analysis of CoT prompting shows that CoT operates as distinct low-rank subspace aligned more with CK, clarifying why it enhances contextual grounding without fully suppressing PK influence (Tao et al., 2025). Future Directions and Broader Implications. These observations open several future directions. Extending subspace-based probing to other generative tasks such as summarization, dialogue, and retrieval-augmented reasoning, could reveal whether similar interaction dynamics generalize beyond NLEs. Integrating controllable subspace steering into model training or inference may enable fine-grained modulation of PKCK balance, enhancing both interpretability and factual consistency. Finally, combining this approach with causal interventions such as neuron-level patching could illuminate how specific layers or modules mediate knowledge integration inside LLMs."
        },
        {
            "title": "6 Conclusion",
            "content": "This work establishes that Parametric Knowledge (PK) and Context Knowledge (CK) interaction in LLMs is fundamentally multidimensional, not binary choice between knowledge sources. Our rank-2 projection framework reveals that explanation (NLE) generation involves dynamic coordination between PK and CK, with their geometric balance serving as direct indicator of factual reliability hallucinated NLEs align strongly with the PK axis, while faithful NLEs balance both sources. This provides mechanistic, internal-based signal for detecting hallucination in generated sequences. Beyond NLE generation, our framework can be used in the future to illuminate how models balance between internal knowledge and external grounding across diverse generative tasks. By enabling fine-grained multithis step tracking of knowledge contributions, approach enables controllable steering of PK-CK dynamics, with implications for improving both interpretability and factual consistency in LLMs."
        },
        {
            "title": "Acknowledgements",
            "content": "This co-funded by research was the European Union (ERC, ExplainYourself, 101077481) and by the VILLUM FONDEN (grant number 40543). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council. Neither the European Union nor the granting authority can be held responsible for them."
        },
        {
            "title": "References",
            "content": "Pepa Atanasova, Oana-Maria Camburu, Christina Lioma, Thomas Lukasiewicz, Jakob Grue Simonsen, and Isabelle Augenstein. 2023. Faithfulness Tests for Natural Language Explanations. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 283 294, Toronto, Canada. Association for Computational Linguistics. Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, and Isabelle Augenstein. 2020. GenIn Proerating Fact Checking Explanations. ceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 73527364, Online. Association for Computational Linguistics. Baolong Bi, Shenghua Liu, Yiwei Wang, Yilong Xu, Junfeng Fang, Lingrui Mei, and Xueqi Cheng. 2025. Parameters vs. Context: FineGrained Control of Knowledge Reliance in Language Models. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot In Advances in Neural Information Learners. Processing Systems, volume 33, pages 1877 1901. Curran Associates, Inc. Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-snli: Natural language inference with natural language explanations. Advances in Neural Information Processing Systems, 31. Sitao Cheng, Liangming Pan, Xunjian Yin, Xinyi Wang, and William Yang Wang. 2024. Understanding the Interplay between Parametric and Contextual Knowledge for Large Language Models. Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. 2019. What Does BERT Look at? An Analysis of BERTs AttenIn Proceedings of the 2019 ACL Worktion. shop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 276286, Florence, Italy. Association for Computational Linguistics. Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg, and Christopher Olah. 2022. Toy Models of Superposition. Transformer Circuits Thread. Gemma-Team. 2024. Gemma 2: Improving Open Language Models at Practical Size. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did Aristotle Use Laptop? Question Answering Benchmark with Implicit Reasoning Strategies. Transactions of the Association for Computational Linguistics, 9:346361. Lovisa Hagström, Sara Vera Marjanovic, Haeun Yu, Arnav Arora, Christina Lioma, Maria Maistro, Pepa Atanasova, and Isabelle Augenstein. 2025. Reality Check on Context Utilisation for Retrieval-Augmented Generation. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1969119730, Vienna, Austria. Association for Computational Linguistics. John Hewitt and Christopher D. Manning. 2019. Structural Probe for Finding Syntax in Word In Proceedings of the 2019 Representations. Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41294138, Minneapolis, Minnesota. Association for Computational Linguistics. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7B. Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. 2020. How Can We Know What Transactions of Language Models Know? the Association for Computational Linguistics, 8:423438. Andrew Lampinen, Ishita Dasgupta, Stephanie Chan, Kory Mathewson, Mh Tessler, Antonia Creswell, James McClelland, Jane Wang, and Felix Hill. 2022. Can language models learn In Findings from explanations in context? of the Association for Computational Linguistics: EMNLP 2022, pages 537563, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Margherita Lazzaretto, Jonas Peters, and Niklas Pfister. 2025. Invariant Subspace Decomposition. Journal of Machine Learning Research, 26(95):156. Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. 2021. Entity-Based Knowledge ConIn Proceedings flicts in Question Answering. of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7052 7063, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Sara Vera Marjanovic, Haeun Yu, Pepa Atanasova, Maria Maistro, Christina Lioma, and Isabelle Augenstein. 2024. DYNAMICQA: Tracing Internal Knowledge Conflicts in Language ModIn Findings of the Association for Comels. putational Linguistics: EMNLP 2024, pages 1434614360, Miami, Florida, USA. Association for Computational Linguistics. Meta-Team. 2024. The Llama 3 Herd of Models. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can Suit of Armor Conduct Electricity? New Dataset for Open In Proceedings Book Question Answering. of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2381 2391, Brussels, Belgium. Association for Computational Linguistics. Julian Minder, Kevin Du, Niklas Stoehr, Giovanni Monea, Chris Wendler, Robert West, and Ryan Cotterell. 2025. Controllable Context Sensitivity and the Knob Behind It. In The Thirteenth International Conference on Learning Representations. Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language Models as Knowledge Bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 24632473, Hong Kong, China. Association for Computational Linguistics. Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Explain Yourself! Leveraging Language Models for In Proceedings of Commonsense Reasoning. the 57th Annual Meeting of the Association for Computational Linguistics, pages 49324942, Florence, Italy. Association for Computational Linguistics. Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How Much Knowledge Can You Pack Into the Parameters of Language Model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 54185426, Online. Association for Computational Linguistics. Noah Siegel, Oana-Maria Camburu, Nicolas Heess, and Maria Perez-Ortiz. 2024. \"The Probabilities Also Matter: More Faithful Metric for Faithfulness of Free-Text Explanations in Large Language Models\". In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 530546, Bangkok, Thailand. Association for Computational Linguistics. Xin Su, Tiep Le, Steven Bethard, and Phillip Semi-Structured Chain-ofHoward. 2024. Thought: Integrating Multiple Sources of Knowledge for Improved Language Model Reasoning. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 85978613, Mexico City, Mexico. Association for Computational Linguistics. ZhongXiang Sun, Xiaoxue Zang, Kai Zheng, Jun Xu, Xiao Zhang, Weijie Yu, Yang Song, and Han Li. 2025. ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation via Mechanistic Interpretability. In The Thirteenth International Conference on Learning Representations. Xin Tan, Bowei Zou, and Ai Ti Aw. 2025. Improving Explainable Fact-Checking with ClaimIn Proceedings of Evidence Correlations. the 31st International Conference on Computational Linguistics, pages 16001612, Abu Dhabi, UAE. Association for Computational Linguistics. Yufei Tao, Adam Hiatt, Erik Haake, Antonie J. Jetter, and Ameeta Agrawal. 2024. When Context Leads but Parametric Memory Follows in Large Language Models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 40344058, Miami, Florida, USA. Association for Computational Linguistics. Yufei Tao, Adam Hiatt, Rahul Seetharaman, and Ameeta Agrawal. 2025. Lost-in-theLater\": Framework for Quantifying Contextual Grounding in Large Language Models. Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT Rediscovers the Classical NLP Pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 45934601, Florence, Italy. Association for Computational Linguistics. Miles Turpin, Julian Michael, Ethan Perez, and Samuel R. Bowman. 2023. Language Models Dont Always Say What They Think: Unfaithful Explanations in Chain-of-Thought PromptIn Thirty-seventh Conference on Neural ing. Information Processing Systems. Michael E. Wall, Andreas Rechtsteiner, and Luis M. Rocha. 2003. Singular Value Decomposition and Principal Component Analysis. Haoran Wang and Kai Shu. 2023. Explainable Claim Verification via Knowledge-Grounded Reasoning with Large Language Models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 62886304, Singapore. Association for Computational Linguistics. Yilin Wang, Heng Wang, Yuyang Bai, and Minnan Luo. 2025. Continuously Steering LLMs Sensitivity to Contextual Knowledge with Proxy Models. Yingming Wang and Pepa Atanasova. 2025. SelfCritique and Refinement for Faithful Natural Language Explanations. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP), Suzhou, China. Association for Computational Linguistics. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in In Proceedings of the large language models. 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA. Curran Associates Inc. Rongwu Xu, Zehan Qi, Zhijiang Guo, Cunxiang Wang, Hongru Wang, Yue Zhang, and Wei Xu. 2024. Knowledge Conflicts for LLMs: SurIn Proceedings of the 2024 Conference vey. on Empirical Methods in Natural Language Processing, pages 85418565, Miami, Florida, USA. Association for Computational Linguistics. Haeun Yu, Pepa Atanasova, and Isabelle Augenstein. 2024. Revealing the Parametric Knowledge of Language Models: Unified Framework for Attribution Methods. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 81738186, Bangkok, Thailand. Association for Computational Linguistics. Shuzhou Yuan, Jingyi Sun, Ran Zhang, Michael Färber, Steffen Eger, Pepa Atanasova, and Isabelle Augenstein. 2025a. Graph-Guided Textual Explanation Generation Framework. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP), Suzhou, China. Association for Computational Linguistics. Xiaowei Yuan, Zhao Yang, Ziyang Huang, Yequan Wang, Siqi Fan, Yiming Ju, Jun Zhao, and Kang Liu. 2025b. Exploiting Contextual Knowledge in LLMs through V-usable Information based Layer Enhancement. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3172631741, Vienna, Austria. Association for Computational Linguistics. Hao Zhang, Yuyang Zhang, Xiaoguang Li, Wenxuan Shi, Haonan Xu, Huanshuo Liu, Yasheng Wang, Lifeng Shang, Qun Liu, Yong Liu, and Ruiming Tang. 2024. Evaluating the External and Parametric Knowledge Fusion of Large Language Models. Zheng Zhao, Emilio Monti, Jens Lehmann, and Haytham Assem. 2024. Enhancing Contextual Understanding in Large Language Models through Contrastive Decoding. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 4225 4237, Mexico City, Mexico. Association for Computational Linguistics."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Replication Details A.1.1 Determining PK and CK Directions in the Rank-2 Projection Subspace. Once we obtain the rank-2 projection subspace spanned by the orthonormal basis vectors Rd2, we identify the PK and CK directions as follows: up = arg max uc = arg max (cid:16) uHwp (cid:16) uHwc (cid:17) (cid:17) , , (9) (10) where Hwp Rnd, and Hwc Rnd are matrices of hidden representation of answer tokens a(q, ε) and a(q, c) guided by intents wp and wc respectively over examples. A.1.2 Prompt Template Prompt template to generate controlled answers a(q, ε), a(q, c) and driven by intents wp, wc, and wb are described in Tab. 1. Prompt template for generating NLEs and CoT-based prompting over the datasets and models is described in Tab. 1. A.1.3 Hyperparameters We identify important layers Lbc and Lbp to capture individual PK and CK contributions, respectivel,y from the final answer via Patchscope using the hyperparameters described in Tab. 12. A.2 Additional Results Intent Prompt wp <begin_of_text><start_header_id>system<end_header_id> Answer the following query considering the provided context. Answer with only one word.<eot_id><start_header_id>user<end_header_id> Context: Daily Grind Cafe is popular place to eat lunch out. At Potluck Party, everyone brings their own lunch to share with friends. Instruction: Ignore the context in answering the query. Query: Which action can help person save money? A. stop eating at Daily Grind Cafe B. attend Potluck Party C. Unknown<eot_id><start_header_id>assistant<end_header_id> wc wb <begin_of_text><start_header_id>system<end_header_id> Answer the following query considering the provided context. Answer with only one word.<eot_id><start_header_id>user<end_header_id> Context: Daily Grind Cafe is popular place to eat lunch out. At Potluck Party, everyone brings their own lunch to share with friends. Instruction: Only consider the context in answering the query. Query: Which action can help person save money? A. stop eating at Daily Grind Cafe B. attend Potluck Party C. Unknown<eot_id><start_header_id>assistant<end_header_id> <begin_of_text><start_header_id>system<end_header_id> Answer the following query considering the provided context. Answer with only one word.<eot_id><start_header_id>user<end_header_id> Context: Daily Grind Cafe is popular place to eat lunch out. At Potluck Party, everyone brings their own lunch to share with friends. Instruction: Consider the context in answering the query. Query: Which action can help person save money? A. stop eating at Daily Grind Cafe B. attend Potluck Party C. Unknown<eot_id><start_header_id>assistant<end_header_id> Table 1: Prompt template for intent-driven answer control. Intent Prompt NLE CoT <begin_of_text><start_header_id>system<end_header_id> Answer the following query considering the provided context. Generate your final answer with only one word. If you are unable to answer the query, generate your final answer as \"Unknown\". Also, generate an explanation to determine your final answer. Return your output in JSON format: {\"explanation\": \"your explanation here\", \"answer\": \"your final response here\"}. Only include the JSON object in your response. <eot_id><start_header_id>user<end_header_id> Context: Daily Grind Cafe is popular place to eat lunch out. At Potluck Party, everyone brings their own lunch to share with friends. Query: Which action can help person save money? A. stop eating at Daily Grind Cafe B. attend Potluck Party C. Unknown <eot_id><start_header_id>assistant<end_header_id> <begin_of_text><start_header_id>system<end_header_id> Answer the following query considering the provided context. Generate your final answer with only one word. If you are unable to answer the query, generate your final answer as \"Unknown\". Also, generate an explanation to determine your final answer. Return your output in JSON format: {\"explanation\": \"your explanation here\", \"answer\": \"your final response here\"}. Only include the JSON object in your response. <eot_id><start_header_id>user<end_header_id> Context: Daily Grind Cafe is popular place to eat lunch out. At Potluck Party, everyone brings their own lunch to share with friends. Query: Which action can help person save money? A. stop eating at Daily Grind Cafe B. attend Potluck Party C. Unknown <eot_id><start_header_id>assistant<end_header_id> Figure 11: Prompt templates for NLE generation and CoT-based prompting. Hyperparameter number of samples batch size τp τc margin eps Llama Gemma Mistral 200 10 0.75 0.85 0.3 0. 500 24 0.65 0.60 0.3 0.05 500 10 0.75 0.65 0.3 0.05 Figure 12: Patching hyperparameters for identifying important layers for rank-2 projection subspaces from Llama-3.1-8B-Instruct, Gemma-2-9B-it, and Mistral-7B-Instruct-v0.3. (a) D(bp) (b) D(bc) Figure 13: Kernel Density Estimate (KDE) of the PK-CK subspace component uT , hi across different knowledge interaction types for four questionanswer datasets using the Llama-3.1-8B-Instruct model. The split noise denotes cases where answers from individual knowledge sources agree with each other but differ from the final answer, i.e., a(q, c) = a(a, ε) and a(a, c) = a. Figure 15: Patchscope on OpenBookQA dataset from gemma-2-9b-it. a) Activation patching on D(bp) results in higher contribution of PK in generating the final answer, as the probability gap between the source and target is higher. b) Activation patching on D(bc) results in lower contribution of CK in generating the final answer, as the probability gap between the source and target is lower. We consider the common layers from both activation patching to learn the rank-2 projection subspace. (a) D(bp) (b) D(bc) Figure 14: Kernel Density Estimate (KDE) of the PK-CK subspace component uT , hi across different knowledge interaction types for four questionanswer datasets using the gemma-2-9b-it model. The split none denotes cases where answers from individual knowledge sources agree with each other but differ i.e., a(q, c) = a(a, ε) and from the final answer, a(a, c) = a. Figure 16: Patchscope on OpenBookQA dataset from a) Activation patching on Mistral-7B-Instruct-v0.3. D(bp) results in higher contribution of PK in genw erating the final answer, as the probability gap between the source and target is higher. b) Activation patching on D(bc) results in lower contribution of CK in genw erating the final answer, as the probability gap between the source and target is lower. We consider the common layers from both activation patching to learn the rank-2 projection subspace."
        }
    ],
    "affiliations": [
        "University of Copenhagen"
    ]
}