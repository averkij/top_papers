{
    "paper_title": "Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust MedVQA in Gastrointestinal Endoscopy",
    "authors": [
        "Sushant Gautam",
        "Michael A. Riegler",
        "PÃ¥l Halvorsen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Medical Visual Question Answering (MedVQA) is a promising field for developing clinical decision support systems, yet progress is often limited by the available datasets, which can lack clinical complexity and visual diversity. To address these gaps, we introduce Kvasir-VQA-x1, a new, large-scale dataset for gastrointestinal (GI) endoscopy. Our work significantly expands upon the original Kvasir-VQA by incorporating 159,549 new question-answer pairs that are designed to test deeper clinical reasoning. We developed a systematic method using large language models to generate these questions, which are stratified by complexity to better assess a model's inference capabilities. To ensure our dataset prepares models for real-world clinical scenarios, we have also introduced a variety of visual augmentations that mimic common imaging artifacts. The dataset is structured to support two main evaluation tracks: one for standard VQA performance and another to test model robustness against these visual perturbations. By providing a more challenging and clinically relevant benchmark, Kvasir-VQA-x1 aims to accelerate the development of more reliable and effective multimodal AI systems for use in clinical settings. The dataset is fully accessible and adheres to FAIR data principles, making it a valuable resource for the wider research community. Code and data: https://github.com/Simula/Kvasir-VQA-x1 and https://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 8 5 9 9 0 . 6 0 5 2 : r Kvasir-VQA-x1: Multimodal Dataset for Medical Reasoning and Robust MedVQA in Gastrointestinal Endoscopy Sushant Gautam 1,2, Michael A. Riegler 3, and Pal Halvorsen 1,2 1Simula Metropolitan Center for Digital Engineering (SimulaMet), Norway 2Oslo Metropolitan University (OsloMet), Norway 3Simula Research Laboratory, Norway"
        },
        {
            "title": "Abstract",
            "content": "Medical Visual Question Answering (MedVQA) is promising field for developing clinical decision support systems, yet progress is often limited by the available datasets, which can lack clinical complexity and visual diversity. To address these gaps, we introduce Kvasir-VQA-x1, new, large-scale dataset for gastrointestinal (GI) endoscopy. Our work significantly expands upon the original Kvasir-VQA by incorporating 159,549 new question-answer pairs that are designed to test deeper clinical reasoning. We developed systematic method using large language models to generate these questions, which are stratified by complexity to better assess models inference capabilities. To ensure our dataset prepares models for real-world clinical scenarios, we have also introduced variety of visual augmentations that mimic common imaging artifacts. The dataset is structured to support two main evaluation tracks: one for standard VQA performance and another to test model robustness against these visual perturbations. By providing more challenging and clinically relevant benchmark, Kvasir-VQA-x1 aims to accelerate the development of more reliable and effective multimodal AI systems for use in clinical settings. The dataset is fully accessible and adheres to FAIR data principles, making it valuable resource for the wider research community. Code and data: github.com/Simula/Kvasir-VQA-x1 and huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1 Keywords: medical VQA, gastrointestinal endoscopy, multimodal AI, dataset benchmark, visual perturbations"
        },
        {
            "title": "1 Background & Summary",
            "content": "Medical AI is witnessing transformative shiftfrom pattern recognition to context-aware reasoningdriven by advances in multimodal learning. Within this landscape, Medical Visual Question Answering (MedVQA) has emerged as compelling benchmark for evaluating real-world capabilities of vision-language models."
        },
        {
            "title": "1.1 The Promise and Challenge of Medical Visual Question Answering",
            "content": "Medical Visual Question Answering is an emergent interdisciplinary field at the intersection of artificial intelligence, computer vision, and medicine [30, 24, 31, 16]. It aims to develop systems that interpret medical images and respond to clinically pertinent questions posed in natural language [16]. This capability holds transformative potential for clinical decision support, offering 1 avenues to enhance diagnostic accuracy, reduce clinician workload, improve patient comprehension, and enable more equitable access to medical expertise through automated assistance in telemedicine and low-resource settings [5, 45, 30, 16]. Unlike general-domain visual question answering (VQA), MedVQA poses unique challenges due to the complexity of medical images and the depth of domain-specific knowledge required to answer clinical questions [28]. Visual cues in medical imaging are often subtle and entangled with artifacts, requiring expert-level reasoning [43]. Moreover, clinical questions may demand multi-step inference or integration of prior medical knowledge, i.e., tasks that go beyond simple pattern recognition or factual recall [9, 14, 21]. As such, MedVQA is increasingly viewed as frontier task for evaluating both vision-language reasoning and real-world applicability of multimodal AI systems in high-stakes environments [16]."
        },
        {
            "title": "1.2 The Paradigm Shift Towards Generative Models",
            "content": "Early MedVQA systems were predominantly discriminative, selecting answers from predefined candidate sets [51, 50]. While effective for constrained tasks, such systems fall short when confronted with the open-ended, nuanced questions encountered in clinical practice [15, 30]. This limitation has driven paradigm shift toward generative models, enabled by recent breakthroughs in Large Language Models (LLMs) and VisionLanguage Models (VLMs) [10, 4, 2, 32, 36]. State-of-the-art systems such as Med-Flamingo, LLaVA-Med, MedGemma, and Qwen2.5-VL exemplify this trend, combining advanced image encoders with powerful, instruction-tuned decoders capable of producing rich, context-sensitive responses [35, 26, 4, 34, 13]. These models signal movement towards AI systems that can engage in more natural and human-like diagnostic reasoning [35, 26]. Yet, the promise of generative MedVQA systems is hindered by lack of suitably complex, domain-aligned datasets for training and evaluation [53]. Most existing benchmarks, including VQA-RAD [24], SLAKE [24], and PMC-VQA [53], suffer from small scale, limited question diversity, or over-representation of specific modalities (e.g., radiology). Many question-answer (QA) pairs are simple, fact-based, or automatically generated, which risk introducing noise and fail to promote advanced clinical reasoning [53]. Moreover, traditional evaluation metrics (e.g., BLEU and ROUGE) often fall short in capturing the correctness or clinical utility of generative outputs, highlighting need for new benchmarks and assessment methods rooted in real-world use cases and clinician feedback [1, 37, 42, 52]."
        },
        {
            "title": "1.3 GI Endoscopy: A Unique Frontier for VQA",
            "content": "Gastrointestinal (GI) endoscopy is critical diagnostic and interventional tool in medicine, generating large volumes of high-resolution images that are rich in clinical content but visually complex [6]. These images frequently contain artifacts such as specular reflections, motion blur, and variable lighting, making them challenging modality for automated interpretation [3, 23]. Despite this, the GI domain has received relatively limited attention in VQA research compared to radiology or pathology [12]. Table 1 provides brief comparison of existing GI-focused VQA datasets. Notable GI-specific resources such as HyperKvasir [6], Kvasir-Instrument [23], and KvasirVQA [12] have laid important groundwork, but they often feature QA pairs centered on simple tasks, such as identifying the presence of polyp or recognizing tool, and thus do not fully capture the reasoning depth required for advanced clinical understanding. Similarly, while MedVQA-GI (2023) introduces diverse QA types for colonoscopy, limitations in expert validation and linguistic diversity constrain its utility for training robust generative models [19]. 2 Table 1: Comparison of Existing Medical VQA Datasets with Relevance to Gastrointestinal (GI) Applications Dataset Year Primary Focus Modality GI Endoscopy Size (Images / QA Pairs) / 6,500 / 58,849 Key GI-Relevant Question Types choice, Yes/No, color, location, count Kvasir-VQA [12] 2024 MedVQA-GI [18] 2023 PMC-VQA [53] 2023 SLAKE [31] 2021 VQA-RAD [24] 2018 GI Endoscopy (Colonoscopy) General Medical (80% Radiology) Radiology (CT, MRI, X-ray) Radiology 4,000 / Multiple per image 149k / 227k 642 / 14k 315 / 3,500 MedFrameQA [49] 2024 Multi-image 9,237 frames / 2, Medical Reasoning Notable Limitations for Advanced GI VQA limited for deep scope for more Potential for QA complexity reasoning; conditions Incomplete expert validation; answers; some limited diversity planned for expansion Limited GI-specific QA generation process risks subjective question focus; Small scale; not GI-specific Very small scale; simple QA; not GI-specific Not specifically GI-focused; tests MLLM reasoning general ID, organ, position, Polyp count/location, instrument color, artifacts Modality, abnormality (general) Organ, abnormality (general) Abnormality, modality, (general) Multi-image comparative reasoning organ"
        },
        {
            "title": "1.4 Towards Robust, Reasoning-Centric Evaluation",
            "content": "To drive progress in this domain, we introduce Kvasir-VQA-x1, significantly expanded and meticulously curated dataset designed to benchmark reasoning-intensive visual question answering in GI endoscopy. Building upon the rich visual foundations of HyperKvasir [6], KvasirInstrument [23] and Kvasir-VQA [12], Kvasir-VQA-x1 features substantially augmented corpus of questionanswer pairs that target higher-order reasoning, multi-faceted clinical knowledge, and linguistic variability [44, 17]. Augmented questions were created using structured, LLM-assisted pipeline followed by expert validation to ensure medical realism, linguistic fluency, and answerability from the associated image content. Kvasir-VQA-x1 further incorporates image-level perturbations, including occlusions, contrast shifts, and blur, to support robustness testing under varied clinical imaging conditions [7, 8]. Each QA pair is additionally annotated with quantitative complexity scores, capturing both visual and linguistic difficulty, thereby enabling nuanced stratification of model performance. This complexityaware structuring aligns with current calls in the AI community for richer evaluation benchmarks that reflect real-world diagnostic challenges rather than artificial constraints. Moreover, the dataset is structured into dual evaluation tracks: Track 1: Evaluates core MedVQA performance on standard images and QA pairs. Track 2: Assesses generalization using perturbed images. This dual-track framework facilitates transparent comparison across models while surfacing failure modes that traditional benchmarks may obscure."
        },
        {
            "title": "1.5 Contribution and Outlook",
            "content": "Kvasir-VQA-x1 represents new benchmark tailored for the next generation of MedVQA systems. It addresses core limitations of prior datasets through its combination of: 3 Clinical depth: capturing nuanced reasoning in GI endoscopy, Linguistic diversity: enabling evaluation of generative language fluency, Visual robustness: stress-testing perception under real-world conditions, Complexity scoring: supporting layered evaluation and insight into model weaknesses. We anticipate that this resource will serve both as robust benchmark for state-of-the-art VLMs like MedGemma and Qwen2.5-VL, and as catalyst for methodological innovation in medical multimodal AI. Furthermore, the dataset is prepared and released in accordance with the FAIR Data Principles, ensuring that it is Findable, Accessible, Interoperable, and Reusable. This makes Kvasir-VQA-x1 valuable community asset for advancing clinically relevant and trustworthy AI in gastroenterology and beyond."
        },
        {
            "title": "2.1 Data Acquisition",
            "content": "The Kvasir-VQA-x1 dataset builds upon the original Kvasir-VQA dataset [12], which is itself derived from two public medical image resources: HyperKvasir [6] and Kvasir-Instrument [23]. These datasets contain high-resolution images from gastrointestinal (GI) endoscopy procedures and are widely used in medical image analysis research. Kvasir-VQA was constructed by pairing 6,500 images from these sources with 58,849 annotated question-answer (QA) pairs. Medical professionals contributed to the annotation process, ensuring clinical relevance and quality. The original QA pairs span six distinct categories: Yes/No, singlechoice, multiple-choice, color-related, location-related, and numerical count questions [12]. To expand on this foundation and support more advanced research, we created Kvasir-VQA-x1 by applying structured augmentation strategies to both the language and visual components of the dataset. This involved generating new complex QA pairs and augmenting original images to increase diversity and robustness. 2."
        },
        {
            "title": "Input Data",
            "content": "The base dataset, Kvasir-VQA [12], consists of 6,500 GI endoscopic images sourced from the HyperKvasir [6] and Kvasir-Instrument [23] datasets with 58,849 QA pairs, labeled with medical expert input. Each image is associated with multiple QA pairs that address GI findings, abnormalities, anatomical landmarks, and the presence of medical instruments. These QA pairs tend to be concise and fact-based. The Kvasir-VQA-x1 dataset enhances the original content by introducing newly generated complex question-answer pairs through linguistic rephrasing and merging. It also includes augmented visual variants of the original 6,500 images using weak transformation pipelines such as affine transformations, cropping, and rotation. Each entry in the dataset stores the original or augmented image, the newly formulated question, naturalized answer, complexity score, and the original question-answer pair(s) from which it was derived."
        },
        {
            "title": "2.3 Processing",
            "content": "To generate Kvasir-VQA-x1, we introduced two major enhancements: 4 Generation of Complex Question-Answer Pairs To promote reasoning beyond simple recall, we employed structured pipeline: QA Grouping: All QA pairs for given image were grouped. predefined list of trivial questions (e.g., Is this finding easy to detect?, none) was excluded to preserve quality. Combinatorial Sampling: We sampled sets of 1, 2, or 3 distinct QA pairs per image, balancing linguistic complexity and sample count. Prompt Engineering: Answer Naturalization: Short answers were transformed into fluent, medically appropriate natural language. Question Merging: When multiple QA pairs were sampled, questions were merged into single coherent prompt requiring multi-step reasoning. Strict Formatting: All outputs followed strict formatting rules, including JSON-encodable structure, and avoided copying raw answers. For question merging and answer naturalization, we used locally hosted inference server for Qwen3-30B-A3B [44] language model. This model was chosen for its high performance, low inference cost via mixture-of-experts (MoE), strong reasoning ability, and efficient local deployment. Each new QA pair consists of an image (either original or augmented), complex question, naturalized answer, the JSON-encoded original QA(s), and an integer complexity score ranging from 1 to 3, which reflects the number of original questions that have been combined. This approach enhances linguistic diversity, promotes the generation of more realistic medical language, and encourages information synthesis. The inclusion of complexity score further enables stratified or curriculum-based training and evaluation, supporting more nuanced model development and assessment. Table 2 illustrates an example image from the Kvasir-VQA dataset along with newly generated questionanswer pairs of varying complexity levels, where three representative samples from each category are shown. Weak Image Augmentation for Enhanced Robustness To account for minor variations in imaging (e.g., due to camera angle or lighting), we generated 10 weakly augmented versions for each original image using: RandomResizedCrop (scale: 0.91.0) RandomRotation (10 degrees) RandomAffine (translation up to 10%) ColorJitter (brightness and contrast: 0.81.2) All augmentations used bicubic interpolation and appropriate padding. During dataset construction, each QA pair was paired with either the original image (23% of cases) or an augmented version (77%) to simulate realistic variability. The released dataset only includes the original images, associated QA pairs, and metadata. Augmented images are not included in the dataset but can be generated using our provided augmentation script. We provide scripts to reproduce the exact augmented images used during generation, along with corresponding train/test splits. These controlled augmentations introduce visual variance while preserving semantic meaning. The probabilistic mix ensures models are exposed to both clean and slightly perturbed data, enhancing generalization to real-world clinical settings. 5 Dataset Statistics and Splits The final Kvasir-VQA-x1 dataset comprises 159,549 questionanswer (QA) pairs. Each entry references an original image and includes complex question, naturalized answer, original QA metadata, and complexity score. We release the dataset with only the original images (no augmentations) and provide scripts to generate weakly augmented versions of each image. We define two evaluation settings: 1. Original Setting QA pairs referencing only the original, unaltered images. 2. Transformed Setting QA pairs referencing weakly augmented images, generated using the provided scripts. These settings enable flexible experimentation, including training on clean data and testing robustness under visual perturbations. Each image-question pair is assigned to either the training or test set, ensuring that no identical QA instance appears in both. This setup supports meaningful generalization testing and is well-suited for training deep learning models and conducting reliable benchmark evaluations in the medical VQA domain."
        },
        {
            "title": "2.4 Model Fine-Tuning and Evaluation Strategy",
            "content": "This section outlines the strategic approach for fine-tuning vision-language models and the comprehensive evaluation framework employed. The objective is to enhance model performance on GI endoscopy image analysis and question answering, while also rigorously assessing their robustness and generalization capabilities. Model Fine-Tuning We fine-tune two prominent vision-language models, MedGemma [13] and Qwen2.5-VL [4], to adapt them to our specific dataset of GI endoscopy images. Fine-tuning aims to leverage the pre-trained knowledge of these models and specialize them for the nuances of medical image understanding and clinical question answering. The suffix -ft is appended to denote fine-tuned variants (e.g., medgemma-ft). The fine-tuning process utilizes LoRA (Low-Rank Adaptation). LoRA is chosen because it allows for efficient adaptation of large pre-trained models by injecting trainable low-rank matrices into the transformer layers, primarily targeting the language model components [20, 33, 41]. This approach significantly reduces the number of trainable parameters, making fine-tuning computationally less demanding while maintaining strong performance [27]. For Qwen2.5-VL, we explore two distinct fine-tuning variants to investigate the impact of data augmentation: Qwen2.5-VL-ft-XXXX: This variant is fine-tuned using the original image set. This provides baseline understanding of how the model performs when exposed only to the raw data. Qwen2.5-VL-ft-Trans-XXXX: This variant is fine-tuned using transformed (augmented) image set. The inclusion of augmented images is designed to improve the models robustness and generalization by exposing it to wider variety of visual perturbations, thereby making it less susceptible to minor variations in input. Evaluation Strategy robust evaluation strategy is critical to comprehensively assess the performance of the finetuned models [41]. This involves benchmarking against established baselines, evaluating across different datasets, and employing diverse set of metrics [22]. 6 Table 2: An example image and its associated questionanswer pairs, stratified by complexity. Three samples are shown from each complexity category."
        },
        {
            "title": "Question Class",
            "content": "1 1 1 2 2 3 3 3 Is there any remaining polyp tissue present? Are there any abnormalities present in the gastrointestinal image? Which anatomical landmark is visible in the image? Are there any green or black box artifacts present, and what are the colors of the abnormality? What type of polyp is present and how many findings are visible? What procedure is depicted in the image and what colors are associated with the abnormality? Are there any abnormalities, anatomical landmarks, or polyps visible in the image? Are there any anatomical landmarks visible, what procedure was performed, and is there an instrument present in the image? Are there any anatomical landmarks visible, what type of polyps are present, and what colors are the observed abnormalities?"
        },
        {
            "title": "No polyp remnants identified",
            "content": "polyp removal status evidence of ulcerative colitis abnormality presence No identifiable anatomical landmark present No green or black box artifacts observed, with abnormalities appearing pink and red No polypoidal lesions identified, with single abnormal finding noted. Evidence of colonoscopy findings with pink and red mucosal lesions landmark location box artifact presence, abnormality color polyp type, finding count procedure type, abnormality color No significant abnormalities or polyps were identified, and no anatomical landmarks were noted. No anatomical landmarks are visible, the image is from colonoscopy, and no instrument is present. abnormality presence, landmark presence, polyp size landmark presence, procedure type, instrument location No anatomical landmarks identified, no polyps observed, and multiple abnormalities with pink and red coloration. landmark presence, polyp type, abnormality color 7 Benchmarking Baselines Fine-tuned models are rigorously benchmarked against their base versions: gemma3, medgemma, and Qwen2.5-VL (non-fine-tuned). These baselines serve as crucial reference points, demonstrating the performance improvements attributable to the fine-tuning process. Where applicable, these baselines represent publicly available checkpoints, facilitating reproducibility and comparative analysis with external research. Evaluation Datasets Performance is evaluated using two distinct datasets to assess both primary performance and robustness: Normal: This dataset comprises the original GI endoscopy images. It is used to gauge the models primary performance on the core task of understanding and answering questions based on untransformed clinical images. Transformed: This dataset consists of weakly augmented versions of the GI endoscopy images. Evaluating on this set assesses the models robustness under visual perturbations, indicating their ability to generalize and maintain performance when faced with minor visual variations, which are common in real-world clinical settings. Performance Metrics Models are assessed using comprehensive suite of standard VQA and natural language processing (NLP) metrics [11], chosen to capture various facets of response quality: ROUGE-1, ROUGE-2, ROUGE-L: These metrics measure n-gram overlap and sequence similarity, providing insights into the content overlap between the models answer and the ground truth [29]. METEOR: This metric goes beyond simple n-gram overlap by capturing synonymy and stem similarity, offering more nuanced assessment of semantic equivalence [25]. CHRF++: character-level F-score metric, particularly useful for evaluating text in morphologically rich contexts, ensuring that character-level matches are also considered [39]. BLEU: widely used n-gram precision-based translation metric, traditionally employed in machine translation, but also valuable for evaluating the fluency and accuracy of generated text [38]. BLEURT: learned evaluation metric based on BERT modeling of human quality judgments. This metric aims to align more closely with human perceptions of answer quality, offering more holistic assessment [42]. BERT-F1: An embedding-based similarity metric with F1 aggregation using BERT. This metric assesses the semantic similarity between the generated answer and the ground truth by leveraging contextual embeddings from BERT [52]. Evaluation Granularity To provide detailed understanding of model capabilities, evaluation is performed at multiple granularities: Intermediate Checkpoint Evaluation: Performance is evaluated across intermediate checkpoints (e.g., medgemma-2000, medgemma-3000, medgemma-3952). This allows for the observation of learning trends throughout the fine-tuning process and the identification of optimal performance snapshots before potential overfitting. 8 Overall Performance: Aggregate scores are computed across all complexity levels and question categories to summarize general model performance and derive comparative insights into the overall effectiveness of fine-tuning. Categorical Evaluation: Model performance is broken down across 18 specific clinical question categories (e.g., abnormality color, finding count, polyp type, instrument presence). This fine-grained analysis helps identify strengths and weaknesses in specific clinical reasoning areas. Visualizations like Rank-Normalized Heatmaps (for relative ranking) and Radar Charts (for absolute category-wise scores) are used to illustrate these insights. Complexity-Based Evaluation: Questions are categorized into three levels of reasoning complexity: Level 1: Direct prompts derived from single atomic QA pair, requiring straightforward factual recall. Level 2: Prompts created by merging two atomic QA pairs, demanding moderate reasoning and synthesis across related clinical cues. Level 3: Prompts combining three distinct QA pairs into single question, requiring higher-order reasoning, abstraction, and cross-referencing multiple clinical aspects. This tiered evaluation quantifies the models robustness in handling increasing reasoning demands and linguistic diversity, providing insights into their ability to perform complex clinical inference."
        },
        {
            "title": "2.5 Automated Evaluation using an LLM-based Adjudicator",
            "content": "To address the limitations of traditional n-gram-based metrics (e.g., BLEU, ROUGE) in capturing clinical accuracy and semantic correctness, we implemented sophisticated, automated evaluation protocol using powerful Large Language Model (LLM) as structured adjudicator. This methodology provides fine-grained, categorical assessment of model performance, directly aligning with the clinical reasoning aspects defined in our dataset. The core of this evaluation is programmatic pipeline that leverages the Qwen/Qwen3-30B-A3B model as an impartial medical examiner. For each prediction made by model being tested, detailed, structured prompt is generated. This prompt provides the adjudicator LLM with comprehensive context, including: 1. The Endoscopic Image Question: The input question posed to the model. 2. The Models Generated Response: The answer produced by the fine-tuned model (e.g., MedGemma-ft, Qwen2.5-VL-ft). 3. The Ground-Truth Answer: The correct answer from the Kvasir-VQA-x1 dataset. 4. Evaluation Aspects: list of specific clinical categories (question class) that the question pertains to (e.g., polyp type, instrument presence, abnormality location). 5. Ancillary Context: The questions complexity level (13) and the original, atomic QA pairs from which the complex question was derived, sourced from the Kvasir-VQA dataset. 9 The adjudicator LLM is instructed to act as medical examiner grading an exam. Its task is to systematically compare the models response against the ground-truth answer, focusing only on the specified evaluation aspects. For each aspect, it must assign binary score: 1 if the models response correctly and completely addresses that specific aspect, and 0 if it is incorrect, incomplete, or fails to address it. brief textual justification for the score is also required. The entire process is automated using an asynchronous Python script that sends batched requests to hosted endpoint for the Qwen/Qwen3-30B-A3B [44] model. The adjudicators output is captured in structured JSON format: { \"eval_json\": { \"polyp_type\": { \"score\": 1, \"reason\": \"The model correctly identified the polyp as sessile.\" }, \"instrument_presence\": { \"score\": 0, \"reason\": \"The model failed to mention the presence of biopsy forceps, which are visible.\" } } } This automated, LLM-driven adjudication process yields rich, multi-faceted evaluation. By aggregating the binary scores on per-category basis, we can compute the categorical accuracy metrics presented in Section 4. This approach allows us to move beyond surface-level text similarity and perform scalable, reproducible, and semantically nuanced assessment of each models clinical reasoning capabilities across different domains of GI endoscopy."
        },
        {
            "title": "2.6 Training Configuration",
            "content": "We fine-tuned visionlanguage models on the Kvasir-VQA-x1 dataset using parameter-efficient tuning with Low-Rank Adaptation (LoRA). All experiments were conducted with standardized hyperparameters, clinical instruction prompts, and multi-GPU infrastructure. Below, we outline the training details necessary for reproducibility. The training setup, including hardware specifications and core hyperparameters, are summarized in Table 3 and 5. Model Setup. Each model was initialized from publicly available checkpoint and adapted using LoRA, with frozen vision backbones and trainable language layers. The instruction prompt used during fine-tuning was: You are medical vision-language assistant; given an endoscopic image and clinical question that may ask about one or more findings, provide concise, clinically accurate response addressing all parts of the question in natural-sounding medical language as if spoken by doctor in single sentence. We employed the Hugging Face transformers [47], PEFT [48], and swift [54] toolchains with DeepSpeed ZeRO Stage 2 [40] optimization."
        },
        {
            "title": "GPUs\nPrecision\nFramework\nOptimizer\nScheduler\nEffective Batch Size\nMax Sequence Length\nWarmup Ratio",
            "content": "48 A100 (80GB) bfloat16 (DeepSpeed) Hugging Face, Swift, PEFT Fused AdamW (DeepSpeed) Linear / Cosine (model-specific) 36 (MedGemma), 32 (Qwen2.5) 1000 tokens 0.03 Table 3: Training environment and hyperparameters. Implementation Notes. All models used LoRA with frozen vision encoders. For all variants, LoRA targeted all projection layers (q proj, proj, proj, etc.). Reproducibility. Fixed random seeds, and released configuration files should ensure reproducibility. Adapter weights and training logs will be made publicly available."
        },
        {
            "title": "3 Data Records",
            "content": "The Kvasir-VQA-x1 dataset is hosted on the Hugging Face Datasets Hub at: https://huggingface. co/datasets/SimulaMet/Kvasir-VQA-x1. It adheres to the FAIR principlesFindable, Accessible, Interoperable, and Reusableto support reproducible research in medical multimodal AI [46]."
        },
        {
            "title": "Dataset Access and Exploration",
            "content": "Users can interact with the dataset through: Web Interface: Browse, filter, and search the dataset directly on the Hugging Face platform. Python API: Load the dataset using the datasets library: from datasets import load_dataset dataset = load_dataset(\"SimulaMet/Kvasir-VQA-x1\") Command-Line Interface (CLI): Utilize the Hugging Face CLI for dataset operations."
        },
        {
            "title": "Dataset Structure",
            "content": "Each entry in the dataset comprises the following fields: img id: Unique identifier linking to the corresponding image from the Kvasir VQA dataset. complexity: Integer score (1, 2, or 3) indicating the reasoning complexity of the question. question: Natural language question derived from one or more atomic QA pairs. answer: Clinically validated answer corresponding to the question. original: JSON-encoded list of the original atomic QA pairs used to generate the complex question. 11 question class: List of clinical categories associated with the question (e.g., polyp type, instrument presence, finding count). See Table 2 in the Kvasir VQA paper [12] for different questions in the original dataset."
        },
        {
            "title": "Data Splits",
            "content": "The dataset is divided into two predefined splits: Train: Contains samples associated with the training subset of the original images. Test: Contains samples associated with the testing subset, reserved for final model evaluation."
        },
        {
            "title": "Dataset Statistics",
            "content": "The final Kvasir-VQA-x1 dataset includes 159,549 questionanswer pairs linked to 6,500 original GI endoscopy images. Each QA pair is annotated with reasoning complexity score (Level 13) and associated clinical question classes. Below, we summarize key statistics. Table 4: Dataset distribution by complexity level and question class Category / Description Level 1 Level 2 Level"
        },
        {
            "title": "Total",
            "content": "Complexity levels (overall) Level 1 Simple recall from single QA pair Level 2 Merged reasoning across two QA pairs Level 3 Multi-hop reasoning across three QA pairs"
        },
        {
            "title": "Question classes\nabnormality color\nabnormality location\nabnormality presence\nbox artifact presence\nfinding count\nfinding presence\ninstrument count\ninstrument location\ninstrument presence\nlandmark color\nlandmark location\nlandmark presence\npolyp count\npolyp removal status\npolyp size\npolyp type\nprocedure type\ntext presence",
            "content": "54 856 52 349 52 344 54 856 (34.4%) 52 349 (32.8%) 52 344 (32.8%) 3 125 3 112 3 099 3 940 3 429 2 500 3 555 3 154 3 148 88 2 158 2 000 3 134 3 945 3 258 3 331 3 939 3 941 6 127 6 233 6 193 7 792 6 856 0 7 019 6 228 6 327 186 4 416 3 965 6 247 7 993 6 590 6 717 7 923 7 9 355 9 464 9 243 11 744 10 286 0 10 698 9 533 9 402 260 6 560 6 066 9 428 11 661 9 794 9 960 11 805 11 773 18 607 18 809 18 535 23 476 20 571 2 500 21 272 18 915 18 877 534 13 134 12 031 18 809 23 599 19 642 20 008 23 667 23 600 We note that the uneven per-class counts in Table 4 largely reflect the underlying distribution and mergeability of the original Kvasir-VQA annotations: rare classes (e.g., landmark color landmark color) simply had few atomic QA pairs to begin with, and binary presence checks 12 (finding presence) cannot be meaningfully composed into multi-step questions. Moreover, multihop QA generation requires co-occurring annotations on the same image, and our question-merging step further prevents ambiguous or clinically irrelevant merges. Together, these factors naturally constrain Level 2 and 3 sample sizes for certain categories while preserving the integrity and clinical validity of the dataset."
        },
        {
            "title": "4 Technical Validation",
            "content": "Before diving into the empirical analysis, we now shift our focus from dataset construction to how well state-of-the-art vision-language models perform on the Kvasir-VQA-x1 benchmark. Table 5 summarizes the fine-tuning results for both models, including training duration, accuracy, and evaluation loss."
        },
        {
            "title": "Model",
            "content": "Params Epochs LR LoRA (r/Î±) Time Eval Acc. Eval Loss MedGemma-Transf. Qwen2.5-VL-Transf. Qwen2.5-VL 4.3B 8.3B 8.3B 4 4 3 2e-5 2e-5 2e-5 16 / 64 16 / 64 16 / 27 30.9 23 84.97% 85.91% 85.78% 0.4111 0.3883 0.3906 Table 5: Fine-tuning summary table for both models. The fine-tuning evaluation loss for all models was computed on randomly selected 1% subset of the training data, which was held out and not used during training."
        },
        {
            "title": "4.1 Model Fine-Tuning and Evaluation Strategy",
            "content": "This section presents the empirical findings from the fine-tuning and evaluation of the visionlanguage models, highlighting their performance across various metrics, clinical categories, and reasoning complexities. Table 6: Evaluation metrics for various models on Normal and Transformed validation sets. Model ROUGE-1 ROUGE-2 ROUGE-L METEOR CHRF++ BLEU BLEURT BERT-F1 gemma3 medgemma-4b medgemma-2000 medgemma-3000 medgemma-3952 Qwen2.5-VL Q-VL-ft-3000 Q-VL-ft-3333 Q-VL-ft-Trans-3000 Q-VL-ft-Trans-4444 gemma3 medgemma-4b medgemma-2000 medgemma-3000 medgemma-3952 Qwen2.5-VL Q-VL-ft-3000 Q-VL-ft-3333 Q-VL-ft-Trans-3000 Q-VL-ft-Trans0.159 0.227 0.665 0.673 0.677 0.230 0.715 0.716 0.710 0.712 0.158 0.227 0.664 0.673 0.676 0.229 0.707 0.708 0.709 0.711 0.036 0.064 0.457 0.468 0.476 0.072 0.531 0.534 0.526 0.529 0.035 0.063 0.456 0.469 0.472 0.071 0.522 0.524 0.524 0.527 Normal Validation Set 0.131 0.192 0.634 0.643 0.648 0.193 0.689 0.690 0.683 0. 0.214 0.251 0.634 0.643 0.648 0.286 0.688 0.689 0.681 0.686 Transformed Validation Set 0.212 0.250 0.633 0.644 0.647 0.284 0.681 0.682 0.681 0.685 0.130 0.192 0.633 0.644 0.646 0.192 0.681 0.682 0.683 0.685 13 29.167 31.698 62.489 63.591 63.931 33.910 67.792 67.910 67.005 67. 29.038 31.645 62.617 63.659 63.928 33.835 67.132 67.150 66.891 67.413 0.019 0.037 0.404 0.419 0.428 0.033 0.476 0.478 0.463 0.472 0.018 0.037 0.404 0.420 0.426 0.032 0.467 0.467 0.461 0.470 -0.723 -0.557 0.305 0.325 0.333 -0.531 0.402 0.404 0.396 0.398 -0.734 -0.555 0.308 0.326 0.329 -0.529 0.388 0.390 0.393 0.396 0.861 0.878 0.946 0.948 0.948 0.875 0.954 0.954 0.954 0. 0.861 0.878 0.946 0.948 0.948 0.875 0.953 0.953 0.953 0.954 The overall performance, as measured by standard VQA and NLP metrics, is summarized in Table 6. These aggregate scores provide comprehensive overview of the models general capabilities."
        },
        {
            "title": "Categorical Performance",
            "content": "The evaluation broke down model performance across 18 distinct clinical question categories. Figure 1: Rank-normalized heatmap illustrating comparative performance rankings (1 = best, 5 = worst) of the models across Kvasir-VQA categories. Qwen2.5-VL-7B-FT consistently ranks first across most categories. Radar Charts (Figure 2) illustrated the absolute performance scores of the five models (Gemma3-4B, MedGemma, MedGemma-FT, Qwen2.5-VL-7B, and Qwen2.5-VL-7B-FT) across these categories. Higher values consistently indicated better performance. MedGemma-FT and Qwen2.5-VL-7B-FT demonstrated notable improvements over their base models in several clinical domains. Rank-Normalized Heatmaps (Figure 1) further clarified the comparative performance. This visualization assigned ranks (1 = best, 5 = worst) to models within each category. Qwen2.5VL-7B-FT consistently ranked first across most categories, demonstrating its superior ability to answer questions across wide range of clinical scenarios after fine-tuning. MedGemma-FT also showed strong relative performance compared to its base version. Complexity-Based Performance"
        },
        {
            "title": "The analysis of performance across different reasoning complexity levels revealed how models",
            "content": "handle increasing inferential demands (Figure 3). 14 Figure 2: Radar plot showing absolute performance scores of five models (Gemma3-4B, MedGemma, MedGemma-FT, Qwen2.5-VL-7B, and Qwen2.5-VL-7B-FT) across various question categories. Higher values indicate better performance. (a) Complexity Level 1 (b) Complexity Level 2 (c) Complexity Level Figure 3: Model performance across different complexity levels. Accuracy scores are plotted for each model across different question categories, grouped by reasoning complexity. 15 For Complexity Level 1 (Figure 3(a)), which involved direct factual recall, most fine-tuned models exhibited strong performance, indicating their proficiency in extracting straightforward information from images. At Complexity Level 2 (Figure 3(b)), requiring moderate reasoning and synthesis, the finetuned models, particularly Qwen2.5-VL-7B-FT, maintained significant advantage, showcasing their ability to integrate information from multiple clinical cues. For Complexity Level 3 (Figure 3(c)), which demanded higher-order reasoning and abstraction across multiple clinical aspects, the fine-tuned models, especially Qwen2.5-VL-7B-FT, consistently outperformed their base counterparts. This indicated that fine-tuning significantly enhanced their capacity for complex clinical inference and cross-referencing. Across all complexity levels, the fine-tuned models demonstrated increased accuracy, validating the effectiveness of the fine-tuning process in improving their ability to handle diverse linguistic and reasoning demands. Table 7 details model accuracy across all question categories and complexity levels. Robustness to Visual Perturbations through Augmentation-Based Fine-Tuning To assess the robustness of fine-tuned models under visual perturbations, we evaluated variants trained using augmented (transformed) images across both the original (normal) and transformed validation sets. Notably, the performance of these modelsspecifically Q-VL-ft-Trans-3000 and QVL-ft-Trans-4444remained highly consistent across the two sets. The absolute differences across key metrics such as ROUGE-L, METEOR, BLEURT, and BERT-F1 were marginal, often within range of 0.0010.002. This stability indicates strong generalization capacity, even when evaluated on previously unseen perturbations. In contrast, models trained exclusively on normal (unaugmented) images exhibited modest decline in performance when evaluated on the transformed validation set. While the degradation was not severe, it was systematicmost notably reflected in slight drop in ROUGE-L and BERTF1 scores. Conversely, models trained on transformed data retained or slightly improved their performance when evaluated on the normal set, confirming that augmentation during training does not compromise performance on clean inputs but rather enhances generalizability."
        },
        {
            "title": "4.2 Discussion",
            "content": "Our results offer comprehensive view of how modern Vision-Language Models (VLMs) perform on complex clinical reasoning tasks. The findings reveal clear narrative about the roles of fine-tuning, model scale, and architectural design, while also exposing the current frontiers of compositional reasoning."
        },
        {
            "title": "4.2.1 The Unifying Power of Fine-Tuning",
            "content": "The most significant finding is the transformative impact of domain-specific fine-tuning. Across the board, the fine-tuned variants demonstrate dramatic leap in performance, with MedGemma-ft and Qwen2.5-VL-ft achieving mean accuracies of 87% and 90%, respectively, compared to their base checkpoints scores hovering around 30-45%. This underscores critical point: while large-scale pre-training provides essential foundational knowledge, it is insufficient for the nuanced demands of medical VQA. Fine-tuning on high-quality, in-domain dataset like Kvasir-VQA-x1 is the dominant factor in unlocking clinical competency. Interestingly, this intensive fine-tuning acts as great equalizer. purpose-built 4B parameter MedGemma, after tuning, performs nearly on par with much larger 7B generalist Qwen model. 16 Table 7: Aspect-wise accuracy (%) of different models across clinical question categories and reasoning complexity levels, computed using LLM-based adjudication. Each score reflects the proportion of correct responses per aspect (question class), where correctness is determined by structured large language model evaluator assigning binary score per aspect. Question Class gemma3 medgemma medgemma-ft Qwen2.5-VL-7B Qwen2.5-VL-7B-ft Complexity Level 1 abnormality color abnormality location abnormality presence box artifact presence finding count finding presence instrument count instrument location instrument presence landmark color landmark location landmark presence polyp count polyp removal status polyp size polyp type procedure type text presence abnormality color abnormality location abnormality presence box artifact presence finding count instrument count instrument location instrument presence landmark color landmark location landmark presence polyp count polyp removal status polyp size polyp type procedure type text presence abnormality color abnormality location abnormality presence box artifact presence finding count instrument count instrument location instrument presence landmark color landmark location landmark presence polyp count polyp removal status polyp size polyp type procedure type text presence abnormality color abnormality location abnormality presence box artifact presence finding count finding presence instrument count instrument location instrument presence landmark color landmark location landmark presence polyp count polyp removal status polyp size polyp type procedure type text presence 11.46 0.00 21.60 37.86 34.02 0.42 41.78 8.61 4.39 33.33 0.00 0.45 30.21 42.82 10.90 0.60 49.74 83. 16.18 1.89 32.21 22.41 33.95 26.44 6.47 23.84 5.56 3.43 6.21 23.61 24.56 11.44 5.93 40.71 73.13 14.74 2.13 43.71 26.28 27.38 17.83 9.24 25.35 16.00 4.11 9.24 28.53 22.62 14.29 9.08 44.66 67.07 14.66 1.68 36.04 27.05 30.63 0.42 24.69 8.19 21.47 16.36 3.14 6.69 27.19 26.38 12.71 6.61 44.20 71.75 3.50 0.00 26.85 44.90 47.51 31.65 71.87 44.81 43.92 16.67 0.00 0.45 41.39 42.54 11.21 0.60 38.66 23.95 15.03 0.95 29.01 34.46 37.38 33.90 14.95 43.59 11.11 1.32 9.20 31.06 18.73 12.88 8.11 42.28 42.01 10.90 0.85 37.49 38.04 32.99 24.77 16.11 42.56 4.00 3.64 12.54 28.86 15.67 17.24 10.60 38.70 36. 11.01 0.74 32.84 38.07 36.78 31.65 35.58 20.76 43.13 9.09 2.25 9.32 31.77 20.84 14.68 8.09 39.88 36.32 45.54 46.32 87.04 86.41 89.15 100.00 97.49 73.29 80.07 25.00 60.94 74.44 95.17 96.06 29.28 60.78 86.34 94.57 17.52 0.92 30.56 70.39 57.77 64.56 85.79 61.42 61.49 25.00 4.72 13.00 83.99 73.80 31.78 0.60 16.75 69.63 Complexity Level 2 60.95 51.10 75.89 89.51 81.74 96.07 77.35 91.52 33.33 66.23 87.13 94.61 74.68 80.11 64.43 93.19 89.82 17.81 3.00 31.37 69.95 46.22 73.56 35.34 68.80 27.78 8.18 21.15 46.75 40.51 22.89 11.08 40.05 69. Complexity Level 3 Overall 53.42 38.62 75.19 88.99 74.84 93.93 76.61 91.53 56.00 62.50 87.30 91.91 72.86 76.58 66.94 97.57 86.15 54.56 44.11 77.52 88.70 79.43 100.00 95.25 76.28 89.69 41.82 63.34 85.02 93.38 77.04 70.07 65.07 94.22 88.76 17 16.88 2.45 36.13 68.93 43.47 71.32 37.96 67.68 20.00 10.62 23.81 54.42 31.42 22.69 15.02 51.22 64. 17.29 2.37 33.59 69.53 46.69 64.56 74.45 41.20 67.07 23.64 8.77 21.04 57.02 41.03 24.26 11.29 41.70 66.88 56.69 55.83 91.05 91.02 90.03 100.00 98.33 78.34 82.09 58.33 67.81 77.58 97.58 95.49 33.02 66.47 88.14 93.58 67.16 53.00 76.39 91.19 82.31 94.90 80.12 91.04 66.67 72.03 86.44 93.82 77.97 81.55 65.68 92.54 91.29 56.62 40.91 80.37 90.31 77.05 94.30 77.44 92.51 48.00 67.14 88.75 94.26 72.69 77.98 70.06 98.09 88.50 60.10 47.50 80.98 90.73 80.89 100.00 95.16 78.51 90.34 56.36 68.76 86.04 94.69 77.99 71.86 68.02 94.57 90.26 This shows that the datasets signal is incredibly strong, effectively aligning models of different scales and pre-training backgrounds to the specific task."
        },
        {
            "title": "4.2.2 Scale and Architecture: The Deciding Factors",
            "content": "Despite the leveling effect of fine-tuning, the slight but consistent performance advantage of Qwen2.5-VL-7B-ft suggests that architectural superiority and scale become the deciding factors at the performance ceiling. We attribute Qwens edge to two primary aspects: 1. Flexible Image Resolution: Qwens Vision Transformer (ViT) can process images at their native aspect ratio and dynamic resolutions. In contrast, MedGemmas SigLIP encoder uses fixed size input. On heterogeneous dataset like Kvasir, which aggregates images from various endoscopic systems, Qwens flexibility likely preserves more contextual and fine-grained visual information, aiding in different tasks. 2. Hierarchical Vision Features: Qwens use of hierarchical vision backbone (FPN-like features) might provide richer spatial cues, contributing to its stronger performance on localization-dependent tasks."
        },
        {
            "title": "4.2.3 The Nuances of Reasoning Complexity: A Synthesis Sweet Spot",
            "content": "A counter-intuitive yet critical finding is that for several categories, even the fine-tuned models achieve higher scores on Level 2 complexity questions than on the seemingly simpler Level 1 questions (e.g., for Qwen2.5-VL-ft in abnormality color, L2 scores 67.16% vs. L1s 56.69%). This contradicts simple monotonic difficulty scale and points to Level 2 as synthesis sweet spot, an optimal nexus of complexity and context that aligns perfectly with the models fine-tuned capabilities. We attribute this to several factors grounded in our methodology: Contextual Richness over Atomic Recall: The process of Question Merging and Answer Naturalization for Level 2 questions creates coherent prompts that are rich in context. For model fine-tuned on this dataset, synthesizing information from two related clinical cues may be more robust task than recalling single, isolated, and potentially ambiguous atomic fact from Level 1 query. The combined context in L2 questions provides more clues, reducing single-point failures. While Level 2 performance exceeds Level 1 in several categories, this may also be due to reduced ambiguity from merged prompts. For example, What is the color of the abnormality? (L1) lacks specificity compared to What is the color of the polyp and where is it located? (L2). Thus, improved scores may reflect clearer referents rather than higher reasoning alone. Optimized for Synthesis: The explicit goal of the Kvasir-VQA-x1 dataset is to promote reasoning beyond simple recall. The fine-tuning process therefore optimizes the models for exactly this kind of synthesis. Level 2, demanding moderate reasoning and synthesis, represents the core challenge of the dataset, and the models proficiency here reflects their successful adaptation to this core task. The High Cost of Higher-Order Reasoning (Level 3): Conversely, the performance drop at Level 3 is pronounced and expected. Combining three distinct clinical facts exponentially increases the cognitive load. The primary driver of failure is likely error accumulation; single error in perceiving one of the three components, or in their synthesis, results in score of zero due to the strict correctly and completely criterion of the LLM adjudicator. This, combined with potential data sparsity for L3 examples and the challenge of true abstraction, firmly establishes Level 3 as benchmark for future advances in multi-hop VQA."
        },
        {
            "title": "4.2.4 Effectiveness of Augmentation Strategies for Generalization",
            "content": "The results highlight the effectiveness of incorporating visual augmentations during fine-tuning to improve model robustness. Models trained on transformed images demonstrated strong invariance to input perturbations, maintaining stable performance across both validation domains. This outcome reinforces the utility of data augmentation in clinical vision-language applications, where minor variations in endoscopic imagery are common due to differences in equipment, lighting, or procedural context. By contrast, models trained solely on clean images showed limited resilience to such variations. Although their performance remained relatively high, the observed degradation on the transformed validation set suggests susceptibility to distributional shifts. Importantly, training with augmented data did not impair performance on the original images, suggesting no trade-off in fidelity. These findings support the adoption of augmentation-informed training as principled approach to enhance generalization. In clinical deployments where robustness and reliability are paramount, such strategies can be instrumental in reducing performance variance and ensuring consistent outputs across heterogeneous input conditions."
        },
        {
            "title": "4.2.5 Limitations and Future Directions",
            "content": "While our study provides valuable insights, it has several limitations that open avenues for future work."
        },
        {
            "title": "Limitations",
            "content": "Dataset Specificity: Our analysis is confined to single, albeit complex, sub-specialty of gastroenterology. The models performance may not generalize to other medical domains like radiology or pathology without further fine-tuning. Evaluation Protocol: The LLM-based adjudicator, while powerful, is not infallible. The strict, binary scoring for complex questions may harshly penalize partially correct answers, particularly for Level 3, potentially underestimating models partial reasoning capabilities. Persistent Error Modes: Even the best models struggle with tasks requiring precise metric and spatial understanding (e.g., polyp size, abnormality location) and calibrated color perception (abnormality color ), indicating that current vision encoders or multi-modal feature projection techniques are not fully optimized for these fine-grained clinical assessments. Homogeneity bias in LLM-as-a-Judge: key limitation of our evaluation protocol is the use of Qwen-based LLM as the adjudicator, which introduces potential homogeneity bias. Since several evaluated models (e.g., Qwen2.5-VL-7B) share architectural lineage with the adjudicator, this overlap may result in self-enhancement bias, where congruent tokenization and latent representations lead to systematically favorable judgments."
        },
        {
            "title": "Future Directions",
            "content": "Advanced Training Strategies: Employ curriculum learning that leverages the synthesis sweet spot, perhaps by starting with Level 2 questions to build strong reasoning foundation before introducing simpler Level 1 recall tasks and more complex Level 3 abstraction challenges. 19 Explicit Spatial and Metric Supervision: Enhance model training by incorporating auxiliary tasks, such as predicting bounding boxes for abnormality location or adding segmentation masks to improve polyp size estimation. Data Augmentation: Implement targeted augmentations, such as simulating variable lighting and white balance, to improve performance on color-dependent tasks. Refined Evaluation: Develop more nuanced evaluation protocols, such as ensemble adjudication or credit-based scoring for complex questions, to better handle cases of right answer, wrong wording and to provide credit for partially correct reasoning. To address the homogeneity bias in LLM-as-a-judge evaluation framework, future studies should incorporate adjudication using structurally distinct LLMs (e.g., Claude or Gemini) to ensure impartiality. Ideally, an ensemble of heterogeneous adjudicators should be employed to cross-validate scores and reduce the influence of architectural dependencies in automated evaluation."
        },
        {
            "title": "4.3 Conclusion",
            "content": "In this paper, we have introduced Kvasir-VQA-x1, comprehensive Visual Question Answering dataset designed to advance the development of multimodal AI systems in the field of gastrointestinal endoscopy. Our primary contribution is the creation of large-scale resource that addresses key limitations of existing MedVQA datasets. By generating 159,549 contextually-rich as well as complex question-answer pairs, we have significantly increased the linguistic and reasoning diversity available for model training and evaluation. key innovation of our work is the structured approach to data creation. Through pipeline assisted by large language models and validated by clinical experts, we have produced questions that require deeper level of clinical understanding, moving beyond simple image recognition. The stratification of these questions by complexity, from single-fact retrieval to multi-step reasoning, provides clear framework for assessing the inferential capabilities of AI models. Furthermore, the inclusion of visually augmented images allows for thorough evaluation of model robustness, critical factor for reliable deployment in real-world clinical environments where image quality can vary. Our evaluation of leading vision-language models, MedGemma and Qwen2.5-VL, on KvasirVQA-x1 demonstrates the challenges posed by our dataset and highlights the performance gains achievable through fine-tuning. The detailed, category-based analysis reveals the specific strengths and weaknesses of current models in different areas of clinical reasoning. For future work, the Kvasir-VQA-x1 dataset can serve as foundational benchmark for developing the next generation of MedVQA systems. We anticipate that this resource will encourage research into more sophisticated models capable of nuanced, multi-step reasoning and greater resilience to visual perturbations. By making our dataset and evaluation scripts publicly available, we hope to foster collaborative effort towards building more trustworthy and clinically-impactful AI in gastroenterology and other medical specialties."
        },
        {
            "title": "5 Usage Notes",
            "content": "Kvasir-VQA-x1 is designed for flexible use in multimodal AI research, particularly in medical image understanding and clinical question answering. Researchers may use the dataset for: Training and evaluation of vision-language models (VLMs), including instruction-tuned or generative systems. Robustness analysis through controlled perturbation-based testing using the provided augmentation scripts. Curriculum learning or stratified benchmarking by leveraging the provided complexity scores (Levels 13) for progressive evaluation of model reasoning. Clinical interpretability research, including hallucination detection, failure mode analysis, or human-in-the-loop evaluations. Each QA instance can be associated with metadata in DataFrame/JSON format, enabling structured preprocessing, parsing, and filtering. Users are advised to refer to the accompanying documentation and sample code to ensure compatibility with model inputs and evaluation pipelines. Recommended Tools: We encourage using the provided preprocessing scripts and evaluation toolkit, which support loading the dataset, augmenting images, and benchmarking model outputs. Licensing: The dataset is released under Creative Commons Attribution Non Commercial 4.0 to foster widespread academic research and innovation in medical AI, subject to ethical use and citation."
        },
        {
            "title": "6 Code Availability",
            "content": "The full codebase for dataset generation, image augmentation, training configurations, and evaluation scripts is publicly available at: https://github.com/simula/Kvasir-VQA-x1 This repository includes: Scripts for generating augmented images used in the transformed track. Preprocessing and JSON validation utilities. Sample training and evaluation workflows for fine-tuning VLMs. Baseline implementations for metrics and plotting tools (heatmaps, radar charts)."
        },
        {
            "title": "7 Acknowledgements",
            "content": "This work has benefited from the Experimental Infrastructure for Exploration of Exascale Computing (eX3), which is financially supported by the Research Council of Norway under contract 270053. We thank the medical experts who contributed to annotation and validation, as well as the SimulaMet and OsloMet infrastructure teams for supporting computational needs during large-scale fine-tuning."
        },
        {
            "title": "Use of AI Disclosure",
            "content": "Various AI/LLM tools were used to draft the structure and improve language clarity. All content has been carefully reviewed, verified, and finalized by the authors."
        },
        {
            "title": "References",
            "content": "[1] Mahyar Abbasian, Elahe Khatibi, Iman Azimi, David Oniani, Zahra Shakeri Hossein Abad, Alexander Thieme, Ram Sriram, Zhongqi Yang, Yanshan Wang, Bryant Lin, et al. Foundation metrics for evaluating effectiveness of healthcare conversations powered by generative AI. npj Digital Med., 7(82):114, March 2024. ISSN 2398-6352. doi:10.1038/s41746024-01074-z. [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: Visual Language Model for Few-Shot Learning. arXiv, April 2022. doi:10.48550/arXiv.2204.14198. [3] Sharib Ali, Felix Zhou, Adam Bailey, Barbara Braden, James East, Xin Lu, and Jens Rittscher. deep learning framework for quality assessment and restoration in video endoscopy. arXiv, April 2019. doi:10.1016/j.media.2020.101900. [4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [5] Yakoub Bazi, Mohamad Mahmoud Al Rahhal, Laila Bashmal, and Mansour Zuair. Bioengineering, 10(3):380, March 2023. VisionLanguage Model ISSN 2306-5354. for Visual Question Answering in Medical Imagery. doi:10.3390/bioengineering10030380. [6] Hanna Borgli, Vajira Thambawita, Pia H. Smedsrud, Steven Hicks, Debesh Jha, Sigrun L. Eskeland, Kristin Ranheim Randel, Konstantin Pogorelov, Mathias Lux, Duc Tien Dang Nguyen, Dag Johansen, Carsten Griwodz, Hakon K. Stensland, Enrique Garcia-Ceja, Peter T. Schmidt, Hugo L. Hammer, Michael A. Riegler, Pal Halvorsen, and Thomas de Lange. HyperKvasir, comprehensive multi-class image and video dataset for gastrointestinal endoscopy. Sci. Data, 7(283):114, August 2020. ISSN 2052-4463. doi:10.1038/s41597-020-00622-y. [7] Alexander Buslaev, Alex Parinov, Eugene Khvedchenya, Vladimir I. Iglovikov, and Alexandr A. Kalinin. Albumentations: fast and flexible image augmentations. arXiv, September 2018. doi:10.3390/info11020125. [8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. Simple Framework for Contrastive Learning of Visual Representations. arXiv, February 2020. doi:10.48550/arXiv.2002.05709. [9] Xupeng Chen, Zhixin Lai, Kangrui Ruan, Shichu Chen, Jiaxiang Liu, and Zuozhu Liu. R-LLaVA: Improving Med-VQA Understanding through Visual Region of Interest. arXiv, October 2024. doi:10.48550/arXiv.2410.20327. [10] Wenjie Dong, Shuhao Shen, Yuqiang Han, Tao Tan, Jian Wu, and Hongxia Xu. Generative Models in Medical Visual Question Answering: Survey. Appl. Sci., 15(6):2983, March 2025. ISSN 2076-3417. doi:10.3390/app15062983. [11] Mingqi Gao, Xinyu Hu, Xunjian Yin, Jie Ruan, Xiao Pu, and Xiaojun Wan. LLM-based NLG Evaluation: Current Status and Challenges. Computational Linguistics, pages 127, 2025. doi:10.1162/coli 00561. [12] Sushant Gautam, Andrea Storas, Cise Midoglu, Steven A. Hicks, Vajira Thambawita, Pal Halvorsen, and Michael A. Riegler. Kvasir-vqa: text-image pair gi tract dataset. In Proceedings of the First International Workshop on VisionLanguage Models for Biomedical Applications (VLM4Bio 24), page 10 pages. ACM, 2024. doi:10.1145/3689096.3689458. URL https://huggingface.co/collections/google/ [13] Google. face, May Medgemma hugging 2025. medgemma-release-680aade845f90bec6a3f60c4. [Online; accessed 29. May 2025]. [14] Tiancheng Gu, Kaicheng Yang, Dongnan Liu, and Weidong Cai. LaPA: Latent Prompt Assist Model For Medical Visual Question Answering. arXiv, April 2024. doi:10.48550/arXiv.2404.13039. [15] Erjian Guo, Zhen Zhao, Zicheng Wang, Tong Chen, Yunyi Liu, and Luping Zhou. DiN: Diffusion Model for Robust Medical VQA with Semantic Noisy Labels. arXiv, March 2025. doi:10.48550/arXiv.2503.18536. [16] Iryna Hartsock and Ghulam Rasool. Vision-language models for medical report generation and visual question answering: review. Front. Artif. Intell., 7:1430984, November 2024. ISSN 2624-8212. doi:10.3389/frai.2024.1430984. [17] Pengcheng He, Jianfeng Gao, and Weizhu Chen. DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing. arXiv, November 2021. doi:10.48550/arXiv.2111.09543. [18] Steven Hicks, Andrea Storas, Pal Halvorsen, Thomas de Lange, Michael Riegler, and Vajira Thambawita. Overview of imageclefmedical 2023-medical visual question answering for gastrointestinal tract. In CLEF (Working Notes), pages 13161327, 2023. [19] Steven Hicks, Andrea Storas, Pal Halvorsen, Thomas de Lange, Michael Riegler, and Vajira Thambawita. Overview of imageclefmedical 2023-medical visual question answering for gastrointestinal tract. In CLEF (Working Notes), pages 13161327, 2023. [20] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models. arXiv, June 2021. doi:10.48550/arXiv.2106.09685. [21] Xinyue Hu, Lin Gu, Qiyuan An, Mengliang Zhang, Liangchen Liu, Kazuma Kobayashi, Tatsuya Harada, Ronald M. Summers, and Yingying Zhu. Expert Knowledge-Aware Image Difference Graph Representation Learning for DifferenceAware Medical Visual Question Answering. arXiv, July 2023. doi:10.1145/3580305.3599819. [22] Tauhidul Islam, Md. Sadman Hafiz, Jamin Rahman Jim, Md. Mohsin Kabir, and M. F. Mridha. systematic review of deep learning data augmentation in medical imaging: Recent advances and future research directions. Healthcare Analytics, 5:100340, June 2024. ISSN 2772-4425. doi:10.1016/j.health.2024.100340. [23] Debesh Jha, Sharib Ali, Krister Emanuelsen, Steven A. Hicks, Vajira Thambawita, Enrique Garcia-Ceja, Michael A. Riegler, Thomas de Lange, Peter T. Schmidt, Havard D. Johansen, Dag Johansen, and Pal Halvorsen. Kvasir-Instrument: Diagnostic and Therapeutic Tool Segmentation Dataset in Gastrointestinal Endoscopy. In MultiMedia Modeling, pages 218229. Springer, Cham, Switzerland, January 2021. ISBN 978-3-030-67835-7. doi:10.1007/978-3-030-67835-7 19. [24] Jason J. Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman. dataset of clinically generated viISSN 2052-4463. sual questions and answers about radiology images. Sci. Data, 5(180251):110, November 2018. doi:10.1038/sdata.2018.251. [25] Alon Lavie and Abhaya Agarwal. Meteor: an automatic metric for MT evaluation with high levels of correlation with In DL Hosted proceedings, pages 228231. Association for Computational Linguistics, June 2007. human judgments. doi:10.5555/1626355.1626389. [26] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. LLaVA-Med: Training Large Language-and-Vision Assistant for Biomedicine in One Day. arXiv, June 2023. doi:10.48550/arXiv.2306.00890. [27] Chenyu Lian, Hong-Yu Zhou, Yizhou Yu, and Liansheng Wang. Less Could Be Better: Parameter-efficient Fine-tuning Advances Medical Vision Foundation Models. arXiv, January 2024. doi:10.48550/arXiv.2401.12215. [28] Xiao Liang, Di Wang, Haodi Zhong, Quan Wang, Ronghan Li, Rui Jia, and Bo Wan. Candidate-Heuristic In-Context Information Processing & Learning: new framework for enhancing medical visual question answering with LLMs. Management, 61(5):103805, September 2024. ISSN 0306-4573. doi:10.1016/j.ipm.2024.103805. [29] Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pages 7481, 2004. [30] Zhihong Lin, Donghao Zhang, Qingyi Tao, Danli Shi, Gholamreza Haffari, Qi Wu, Mingguang He, and Zongyuan ISSN 0933-3657. Ge. Medical visual question answering: survey. Artif. Intell. Med., 143:102611, September 2023. doi:10.1016/j.artmed.2023.102611. [31] Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu. Slake: Semantically-Labeled Knowledge-Enhanced Dataset For Medical Visual Question Answering. In IEEE 18th International Symposium on Biomedical Imaging (ISBI), pages 1316. IEEE, 2021. doi:10.1109/ISBI48211.2021.9434010. [32] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual Instruction Tuning. arXiv, April 2023. doi:10.48550/arXiv.2304.08485. [33] Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan. Peft: State-ofthe-art parameter-efficient fine-tuning methods. In Peft: State-of-the-art parameter-efficient fine-tuning methods. 2022. [34] Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Cyril Zakka, Yash Dalmia, Eduardo Pontes Reis, Pranav Rajpurkar, and Jure Leskovec. Med-Flamingo: Multimodal Medical Few-shot Learner. arXiv, July 2023. doi:10.48550/arXiv.2307.15189. [35] Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Cyril Zakka, Yash Dalmia, Eduardo Pontes Reis, Pranav Rajpurkar, and Jure Leskovec. Med-Flamingo: Multimodal Medical Few-shot Learner. arXiv, July 2023. doi:10.48550/arXiv.2307.15189. [36] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Diogo Almeida, Janko Altenschmidt, Sam Altman, et al. doi:10.48550/arXiv.2303.08774. GPT-4 Technical Report. Ilge Akkaya, Florencia Leoni Aleman, arXiv, March 2023. [37] Sophie Ostmeier, Justin Xu, Zhihong Chen, Maya Varma, Louis Blankemeier, Christian Bluethgen, Arne Edward Michalson, Michael Moseley, Curtis Langlotz, Akshay S. Chaudhari, et al. GREEN: Generative Radiology Report Evaluation and Error Notation. arXiv, May 2024. doi:10.18653/v1/2024.findings-emnlp.21. [38] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: method for automatic evaluation of maIn DL Hosted proceedings, pages 311318. Association for Computational Linguistics, July 2002. chine translation. doi:10.3115/1073083.1073135. [39] Maja Popovic. chrF: character n-gram F-score for automatic MT evaluation. ACL Anthology, pages 392395, September 2015. doi:10.18653/v1/W15-3049. [40] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. ZeRO: Memory Optimizations Toward Training Trillion Parameter Models. arXiv, October 2019. doi:10.48550/arXiv.1910.02054. [41] Seyed Amir Ahmad Safavi-Naini, Shuhaib Ali, Omer Shahab, Zahra Shahhoseini, Thomas Savage, Sara Rafiee, Jamil S. Samaan, Reem Al Shabeeb, Farah Ladak, Jamie O. Yang, et al. Vision-Language and Large Language Model Performance in Gastroenterology: GPT, Claude, Llama, Phi, Mistral, Gemma, and Quantized Models. arXiv, August 2024. doi:10.48550/arXiv.2409.00084. [42] Thibault Sellam, Dipanjan Das, and Ankur P. Parikh. BLEURT: Learning Robust Metrics for Text Generation. arXiv, April 2020. doi:10.48550/arXiv.2004.04696. [43] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Mohamed Amin, Le Hou, Kevin Clark, Stephen R. Pfohl, Heather Cole-Lewis, et al. Toward expert-level medical question answering with large language models. Nat. Med., 31(3):943950, March 2025. ISSN 1546-170X. doi:10.1038/s41591-024-03423-7. [44] Qwen Team. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. [45] Xiaofei Wang, Hayley M. Sanders, Yuchen Liu, Kennarey Seang, Bach Xuan Tran, Atanas G. Atanasov, Yue Qiu, Shenglan Tang, Josip Car, Ya Xing Wang, et al. ChatGPT: promise and challenges for deployment in lowand middle-income countries. Lancet Regional Health Western Pacific, 41, December 2023. ISSN 2666-6065. doi:10.1016/j.lanwpc.2023.100905. [46] Mark D. Wilkinson, Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, Jan-Willem Boiten, Luiz Bonino da Silva Santos, Philip E. Bourne, et al. The FAIR Guiding Principles for scientific data management and stewardship. Sci. Data, 3(160018):19, March 2016. ISSN 2052-4463. doi:10.1038/sdata.2016.18. [47] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien 23 Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 3845, Online, October 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2020.emnlp-demos.6. [48] Lingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui Tao, and Fu Lee Wang. Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: Critical Review and Assessment. arXiv, December 2023. doi:10.48550/arXiv.2312.12148. [49] Suhao Yu, Haojin Wang, Juncheng Wu, Cihang Xie, and Yuyin Zhou. MedFrameQA: Multi-Image Medical VQA Benchmark for Clinical Reasoning. arXiv, May 2025. doi:10.48550/arXiv.2505.16964. [50] Ting Yu, Zixuan Tong, Jun Yu, and Ke Zhang. Fine-grained Adaptive Visual Prompt for Generative Medical Visual Question Answering. AAAI, 39(9):96629670, April 2025. ISSN 2374-3468. doi:10.1609/aaai.v39i9.33047. [51] Li-Ming Zhan, Bo Liu, Lu Fan, Jiaxin Chen, and Xiao-Ming Wu. Medical Visual Question Answering via Conditional Reasoning. In ACM Conferences, pages 23452354. Association for Computing Machinery, New York, NY, USA, October 2020. doi:10.1145/3394171.3413761. [52] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. BERTScore: Evaluating Text Generation with BERT. arXiv, April 2019. doi:10.48550/arXiv.1904.09675. [53] Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi Xie. PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering. arXiv, May 2023. doi:10.48550/arXiv.2305.10415. [54] Yuze Zhao, Jintao Huang, Jinghan Hu, Xingjun Wang, Yunlin Mao, Daoze Zhang, Zeyinzi Jiang, Zhikai Wu, Baole Ai, Ang Wang, Wenmeng Zhou, and Yingda Chen. Swift:a scalable lightweight infrastructure for fine-tuning, 2024. URL https://arxiv.org/abs/2408.05517."
        }
    ],
    "affiliations": [
        "Oslo Metropolitan University (OsloMet), Norway",
        "Simula Metropolitan Center for Digital Engineering (SimulaMet), Norway",
        "Simula Research Laboratory, Norway"
    ]
}