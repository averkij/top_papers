{
    "paper_title": "SemanticGen: Video Generation in Semantic Space",
    "authors": [
        "Jianhong Bai",
        "Xiaoshi Wu",
        "Xintao Wang",
        "Fu Xiao",
        "Yuanxing Zhang",
        "Qinghe Wang",
        "Xiaoyu Shi",
        "Menghan Xia",
        "Zuozhu Liu",
        "Haoji Hu",
        "Pengfei Wan",
        "Kun Gai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 9 1 6 0 2 . 2 1 5 2 : r SemanticGen: Video Generation in Semantic Space Jianhong Bai1, Xiaoshi Wu2, Xintao Wang2, Xiao Fu3, Yuanxing Zhang2, Qinghe Wang4, Xiaoyu Shi2, Menghan Xia5, Zuozhu Liu2, Haoji Hu1, Pengfei Wan2, Kun Gai2 1Zhejiang University, 2Kling Team, Kuaishou Technology, 3CUHK, 4DLUT, 5HUST Project webpage: https://jianhongbai.github.io/SemanticGen/ Figure 1. Examples synthesized by SemanticGen. SemanticGen generates high-quality videos from text prompts in the semantic representation space and scales to long-form generation of up to one-minute videos. Video results are on our project page."
        },
        {
            "title": "Abstract",
            "content": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in compact, high-level semantic space for global planning, followed by the addition of high-frequency Work done during an internship at Kling Team, Kuaishou Tech. Corresponding authors. details, rather than directly modeling vast set of low-level video tokens using bi-directional attention. SemanticGen adopts two-stage generation process. In the first stage, diffusion model generates compact semantic video features, In the secwhich define the global layout of the video. ond stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces highquality videos and outperforms state-of-the-art approaches and strong baselines. 1. Introduction Video generative models [17, 55, 59] have made rapid advancements in recent years. Mainstream diffusion-based [26, 42, 54] methods first train variational autoencoder (VAE) [35] with reconstruction objective to project videos from pixel space into latent space, and then train diffusion model to fit the distribution of VAE latents. While effective, this paradigm has two key limitations. First, it suffers from slow convergence speed. To attain highquality videos, existing methods often rely on extremely large computational budgets on the order of hundreds of thousands of GPU-hours [51] highlighting the need for more compute-efficient training paradigms. Second, scaling to long videos remains challenging. Since modern VAEs typically have modest compression ratios, 60s, 480p, 24fps video clip expands to over 0.5M tokens, making bidirectional full-attention diffusion modeling impractical. Although previous works attempt to reduce complexity via sparse attention [68, 69] or to adopt an autoregressive [7, 37] or diffusionautoregressive hybrid [9, 30] video generation framework, they often suffer from temporal drift or noticeable degradation in visual quality. To address these limitations, we propose SemanticGen, framework that generates videos in high-level semantic space before refining details in the VAE latent space, as illustrated in Fig. 2. Our key insight is that, given the substantial redundancy inherent in videos, generation should first occur in compact semantic space for global planning, and add high-frequency details afterwards rather than directly modeling vast collections of low-level video tokens. Technically, SemanticGen follows two-stage paradigm. We first train video diffusion model to denoise VAE latents conditioned on semantic representations, then train semantic representation generator for high-level semantic modeling. We leverage off-the-shelf video-understanding tokenizers as the semantic encoder. However, we observe that directly sampling in high-dimensional semantic space could result in slower convergence speed and inferior performance. To this end, we propose semantic space compression via lightweight MLP for effective training and sampling. Finally, we inject the compressed semantic embeddings into the video generator. Beyond improving training efficiency, this design extends naturally to long video generation, mitigating drift without sacrificing fidelity. In addition to the proposed SemanticGen, recent studies extensively explore integrating semantic representations into video generation. series of works [12, 13, 72] incorporate semantic-level objectives into VAEs to learn semantic-rich and easily generable representations. These approaches are orthogonal to SemanticGen, as our framework is not tied to specific VAE tokenizer. Another line of research optimizes latent generative models using semantic features. For example, REPA [74] aligns generaFigure 2. Illustration of the proposed SemanticGen. tive model hidden states with semantic representations to accelerate convergence, while RCG [40] employs twostage process for unconditional image generation by first modeling semantic features and then mapping them to VAE latents. TokensGen [47] is the most related method, as it also adopts two-stage paradigm for video generation but further compresses VAE latents instead of using semantic features. However, we find that generating in the semantic space is fundamentally different from modeling in the compressed VAE space. In particular, the semantic space exhibits substantially faster convergence, as shown in Fig. 9. Experimental results demonstrate that our method outperforms state-of-the-art approaches and strong baselines in generating both short and long videos. Ablation studies further validate the effectiveness of our key design choices. The main contributions of this work are as follows: We propose SemanticGen, novel video generation framework that initially models in compact semantic space before mapping to low-level latent space. We identify key requirements for semantic encoders in video generation and develop semantic representation compression to effectively integrate semantic representations into the generation process. We conduct comprehensive experiments and ablation studies to demonstrate that SemanticGen provides significant advantages in terms of convergence speed and outperforms baselines in long video generation. 2. Related Works 2.1. Video Generative Models Recent advancements in video generative models can be roughly divided into diffusion-based approaches, autoregressive approaches, and their hybrid variants. Diffusionbased methods [26, 42, 54] model all frames with bidirectional attention and generate all frames simultaneously [11, 24, 27, 28, 61]. Early attempts [46, 22, 65] extended text-to-image models [50] with temporal modeling layers. Subsequent works [38, 56, 59, 71] benefit from the scalability of Diffusion Transformers [48], achieving high-fidelity short video generation. However, due to the quadratic complexity of full sequence attention, its effective scalability to long video generation scenarios is limited. Autoregressive techniques [7, 18, 37, 66, 73] generate each frame or patch of the video sequentially, facilitating applications like realtime video generation [14, 41], long-video generation [21, 64], etc. Meanwhile, line of works [9, 15, 29, 30, 33, 77] adopts diffusion-autoregressive hybrid paradigm, aiming to combine the advantages of both paradigms. Representative works, such as diffusion-forcing [9], use time-varying noise scheduler to achieve both causal modeling and full sequence attention. Self-forcing [30] builds upon this and addresses the training-inference gap, reducing error accumulation issues. Nevertheless, these two types of methods generally exhibit inferior performance compared to diffusionbased methods. In this paper, we propose novel diffusionbased video generation paradigm that achieves faster convergence and can effectively generalize to long video generation [16, 23, 34, 45, 62, 70, 75]. 2.2. Semantic Representation for Generation Recent studies demonstrate that incorporating semantic representations can significantly enhance the performance of generative models. One line of research [13, 44, 52, 72, 76] focuses on introducing semantic representations to optimize the tokenizer of generative models. VA-VAE [72] aligns VAE [35] latents with pre-trained semantic representations [25, 46], while DC-AE [13] and MAETok [12] integrate semantic objectives [25] into VAE training. More recent work, RAE [76], directly uses semantic encoder in generative tasks and trains the corresponding decoder with reconstruction objective. SVG [52], building on selfsupervised representations, additionally trains residual encoder for improved reconstruction. These approaches consistently yield faster convergence and stronger image generation performance compared to using raw VAE latents. Another line of methods directly optimizes latent generators [39, 40, 63, 67, 74]. RCG [40] proposes first modeling self-supervised representations and then mapping them to the image distribution. REPA [74] aligns noisy input states in diffusion models with representations from pretrained semantic visual encoders. DDT [63] uses decoupled diffusion transformers to separately learn semantic representations and high-frequency details. X-Omni [20] generates discrete semantic tokens with unified autoregressive model and decodes with diffusion generator. Our method fine-tunes diffusion models to learn compressed semantic representations that are subsequently mapped to the VAE latent space. This design leads to notably faster convergence than generating directly in the VAE latent space and effectively scales to long video generation. 3. Method In this section, we present the design of SemanticGen. To generate the video clip from the text prompt, we leverage pre-trained video diffusion models (Sec. 3.1) and semantic encoders. We start by training video diffusion model to generate video VAE latents conditioned on their compressed semantic representations (Sec. 3.2). Next, we learn the distribution of the compressed semantic representation from the text input (Sec. 3.3). During inference, we first generate the semantic representation, then map it to the VAE latent space. We also demonstrate that our model can effectively generalize to long video generation (Sec. 3.4). The overview of the model is depicted in Fig. 3. 3.1. Preliminary: Text-to-Video Base Model Our study is conducted over an internal pre-trained text-tovideo foundation model. It is latent video diffusion model, consisting of 3D Variational Autoencoder (VAE) [35] and Transformer-based diffusion model (DiT) [48]. Typically, each Transformer block is instantiated as sequence of spatial attention, 3D (spatial-temporal) attention, and crossattention modules. The generative model adopts the Rectified Flow framework [19] for the noise schedule and denoising process. The forward process is defined as straight path between the data distribution and standard normal distribution, i.e. zt = (1 t)z0 + tϵ, (1) where ϵ (0, I) and denotes the iterative timestep. To solve the denoising processing, we define mapping between samples z1 from noise distribution p1 to samples z0 from data distribution p0 in terms of an ordinary differential equation (ODE), namely: dzt = vΘ(zt, t)dt, (2) where the velocity is parameterized by the weights Θ of neural network. For training, we regress vector field ut that generates probability path between p0 and p1 via Conditional Flow Matching [42]: LLCM = Et,pt(z,ϵ),p(ϵ)vΘ(zt, t) ut(z0ϵ)2 2, (3) t(ψ1 where ut(z, ϵ) := ψ (zϵ)ϵ) with ψ(ϵ) denotes the function of Eq. 1. For inference, we employ Euler discretization for Eq. 2 and perform discretization over the timestep interval at [0, 1], starting at = 1. We then processed with iterative sampling with: zt = zt1 + vΘ(zt1, t) t. (4) 3.2. Video Generation with Semantic Embeddings SemanticGen aims to generate videos by leveraging their compact semantic representations. Hence, we first fine-tune pre-trained video diffusion model to learn denoising VAE latents conditioned on semantic representations. Figure 3. Overview of SemanticGen. (a) We train semantic generator to fit the compressed semantic representation distribution of off-the-shelf semantic encoders. (b) We optimized latent diffusion model for denoising video VAE latents conditioned on their semantic representations. (c) During inference, we integrate the semantic generator and VAE latent generator to achieve high-quality T2V generation. What Kinds of Semantic Encoders Are Needed for Video Generation? To select suitable off-the-shelf semantic encoders for video generation tasks, we identify three key requirements. First, the semantic tokenizer must be pre-trained on large-scale video dataset, as this enables the model to capture temporal semantics, such as object motion and camera movement. State-of-the-art image tokenizers, such as SigLip2 [58] and DINOv3 [53], are trained solely on image datasets, and therefore are unable to effectively model temporal information. Second, the output representations should be compact in both spatial and temporal dimensions. The key insight is that, due to the inherent high redundancy in videos, generation should first occur in high-level compact semantic space for global planning, followed by the addition of visual details. Lastly, the semantic tokenizer should be trained on variety of video lengths, resolutions, thereby supporting the generation of diverse video content with variable length, aspect ratios, etc. As result, we utilize the vision tower of Qwen-2.5-VL [3] as our semantic encoder, which is trained with visionlanguage alignment objective [49] on image and video datasets. For video input, it first samples video frames at lower fps (default 2.0), compresses 14x14 image patches into single token, and then further compresses along each dimension by factor of 2. This process ultimately transforms video R3F HW , into semantic represensem RdFs/2H/28W/28, where is the embedtation ding dimension, and Fs is the number of video frames sampled as input to the Qwen2.5-VL vision tower. Note that the framework proposed in this paper does not rely on using specific semantic tokenizer. Other video semantic tokenizers, such as V-JEPA 2 [1], VideoMAE 2 [60], and 4DS [8], are also compatible with SemanticGen. In this paper, we use Qwen-2.5-VL to validate the effectiveness of the proposed method, and we leave the systematic exploration of other semantic encoders as future work. Figure 4. Video generation conditioned on semantic features extracted from reference video. Row 1: The reference video. Rows 24: Reconstructions based on semantic representations (Sem. Rep.) with dimensions of 2048, 64, and 8, respectively. Row 5: T2V generation results without semantic representations. Semantic Representation Compression for Effective Training. In our experiment, we empirically find that directly optimizing pre-trained video diffusion model to fit high-dimensional semantic representations can result in slower convergence and inferior performance with fixed training steps (please refer to Fig. 8). We hypothesize there are two-fold reasons. First, the high dimensionality of the semantic features leads to rich information, which may require longer convergence time during training. Second, the original semantic space is not conducive to sampling by diffusion model. Therefore, we use learnable MLP to compress the semantic space for effective training. The it reduces the dimensionality MLP serves two purposes: of the semantic representation and models the compressed feature space as Gaussian distribution. The MLP outputs the mean and variance of this distribution, and we add the KL divergence objective as the regularizer, encouraging the learned compressed semantic space to resemble Gaussian distribution. The sampled embedding zsem is then input into the diffusion model. This approach alleviates the fitting complexity for the semantic representation generation model, which will be introduced in Sec. 3.3. In-Context Conditioning. The pipeline of the VAE latent generation stage is illustrated in Fig. 3(b). During training, we first feed the input video to the semantic encoder and the learnable MLP to get its compact semantic representation zsem, which is then injected into the diffusion model via in-context conditioning [2]. Specifically, we concatenated noised VAE latents zt and compressed semantic representations zsem as the models input, i.e., zinput := [zt, zsem]. To verify that the compressed semantic representation captures the videos high-level semantics and effectively guides generation, we extract semantic features from reference video and inject them into the VAE latent generator. The generated video, shown in Fig. 4, preserves the spatial layout and motion patterns of the reference video while differing in fine details. This demonstrates that the compressed semantic representations encode high-level informationsuch as structure and dynamics, while discarding low-level attributes like texture and color. At the inference stage, zsem is generated by the semantic representation generation model in Sec. 3.3. Similar to RAE [76], we add noise to zsem to reduce the training-inference gap. 3.3. Semantic Representation Generation After training VAE latent generator to synthesize VAE latents with the compressed semantic representations in Sec. 3.2, we further learn the semantic representation distribution by another video diffusion model. In this stage, we freeze the visual encoder and the MLP, and fine-tune only the latent diffusion model. We observe significant improvement in convergence speed after regularizing the semantic space with the learnable MLP. The results are summarized in Fig. 8 and Tab. 3. Additionally, we ablate the design of using semantic encoders rather than VAE encoders in Fig. 9, and we observe significantly faster convergence speed compared to modeling the compressed VAE latents. 3.4. Extension to Long Video Generation Previous diffusion-based video generation approaches [36, 51, 59] often struggle to effectively scale to long video generation scenarios. This is because the computational cost of bi-directional attention increases quadratically with video length. Directly training the entire video (e.g., 1 minute) Figure 5. Implementation of Swin-Attention. When generating long videos, we apply full attention to model the semantic representations and use shifted-window attention [43] to map them into the VAE space. The blue squares indicate VAE latents, while the yellow squares denote semantic representations. in the VAE latent space introduces unacceptable computational complexity. We propose to tackle this problem with SemanticGen. Our core insight is that, when generating long videos, we perform full-attention modeling only in the highly compressed semantic space to maintain consistency across scenes and characters in the video. When mapping to the VAE latent space, we use shifted window attention [43] to ensure that the computational cost does not grow quadratically with the number of frames. Since the semantic space naturally has high compression ratio in our implementation, the number of tokens is only 1/16 of the VAE tokens the process of semantic representation generation introduces only few additional computational costs. Meanwhile, the implementation of the shifted window attention in the VAE latent generation stage significantly reduces the models computational cost compared to previous methods. We illustrate the implementation of Swin attention in Fig. 5. To be specific, we interleave the VAE latent and their semantic representations, placing both types of tokens from video of length Tw into an attention window. The window then shifted by half window size Tw/2 at odd layers. 4. Experimental Results 4.1. Experiment Settings Implementation Details. For short video generation, we train SemanticGen on an internal text-video pair dataset. For long video generation, we construct long video training data by splitting movie and TV show clips into 60-second segments and using an internal captioner to generate the corresponding text prompts. During training, we sample frames from the video at fps=24 as input to the VAE, and at fps=1.6 as input to the semantic encoder. We use the vision tower of Qwen2.5-VL-72B-Instruct [3] as the semantic encoder in our main experiments. Figure 6. Comparison with state-of-the-art methods on short video generation. It shows that SemanticGen generates high-quality videos that adhere to the text prompts and are comparable to strong baselines. Figure 7. Comparison with state-of-the-art methods on long video generation. It demonstrates that SemanticGen generates videos with long-term consistency and significantly alleviates the drifting issues. Evaluation Set and Metrics. We evaluate shortand long-form video generation using the standard VBench [31] and VBench-Long [32] benchmarks. We extend their official prompt sets and apply them to SemanticGen and all baselines. Video quality is assessed using the standard VBench metrics. For long video generation, we additionally measure quality drift using drift, proposed in FramePack [75], defined as the absolute value of the difference in metric values between the initial and final segments of video. 4.2. Comparison with State-of-the-Art Methods Baselines. We compared the proposed SemanticGen with state-of-the-art T2V methods in Tab. 1 and Tab. 2. For short video generation, we use Wan2.1-T2V-14B [59], and HunyuanVideo [38] as baselines. For long video generation, we use open-source models SkyReels-V2 [10], Self-Forcing [30], and LongLive [70] as baselines. It is important to note that existing video generation studies typically use different base models, training data, and training steps, making fair comparisons challenging. To provide reliable assessment of our proposed paradigm, we include additional baselines that continue training the base model using the standard diffusion loss without semantic modeling, while keeping the data and the number of training steps identical. These comparisons are included as important baselines in Tab. 1 and Tab. 2, denoted as Base-CT and Base-Swin-CT. Table 1. Quantitative comparison with state-of-the-art methods on short video generation. Method Subject Consistency Background Consistency Temporal Flickering Motion Smoothness Hunyuan-Video [38] Wan2.1-T2V-14B [59] Base-CT SemanticGen 91.11% 97.23% 96.17% 97.79% 95.32% 98.28% 97.27% 97.68% 97.49% 98.35% 98.07% 98.47% 99.07% 99.08% 99.07% 99.17% Imaging Quality 64.23% 66.63% 65.77% 65.23% Aesthetic Quality 62.60% 65.61% 63.97% 64.60% Table 2. Quantitative comparison with state-of-the-art methods on long video generation. Method Subject Consistency Background Consistency Temporal Flickering Motion Smoothness Imaging Quality Aesthetic Quality SkyReels-V2 [10] Self-Forcing [30] LongLive [70] Base-CT-Swin SemanticGen 93.13% 90.41% 94.77% 94.01% 95.07% 95.11% 93.42% 95.90% 94.84% 96.70% 98.41% 98.51% 98.48% 98.64% 98.31% 99.24% 99.17% 99.21% 99.32% 99.55% 62.17% 66.00% 62.73% 70.23% 64.73% 70.17% 68.15% 61.66% 70.47% 64.09% drif 9.00% 12.39% 4.08% 5.20% 3.58% Qualitative Results. We present synthesized examples of SemanticGen in Fig. 1. Please refer to the project page for video results. SemanticGen demonstrates the ability to: 1) generate high-quality videos adhering to the text prompts; 2) generate long videos with long-term consistency, and significantly alleviate drifting issues. We compare SemanticGen with state-of-the-art methods on short video generation and long video generation in Fig. 6 and Fig. 7, respectively. For short video generation, SemanticGen surpasses baseline methods in text-following accuracy. For example, the baselines fail to generate the man turning his head to the left or the melting process of the snowflake. Compared with continuing to train the base model using the diffusion-based framework (denoted as Base-CT in Fig. 6), our method achieves comparable performance. For long video generation, SemanticGen achieves better long-term consistency and significantly alleviates drifting issues. We observe that baselines may exhibit severe color shifts or inconsistencies across frames. Similar phenomena appear when continuing to train the base text-to-video generation model with Swin attention without global semantic modeling (denoted as Base-CT-Swin in Fig. 7), where we observe inconsistent backgrounds across generated frames and more artifacts, highlighting the importance of performing global planning in the high-level semantic space. Quantitative Results. We quantitatively evaluate SemanticGen against baselines using automatic metrics, with results summarized in Tables 1 and 2. We adopt the VBench metrics to assess visual quality. For short-video generation, SemanticGen achieves performance comparable to state-ofthe-art T2V models and the continued-training baseline of our base model. For long-video generation, SemanticGen substantially outperforms all baselines in terms of video consistency and temporal stability, benefiting from using full attention to model high-level semantic features that enhance long-term coherence. We also employ the driftingmeasurement metric drif introduced in [75] to quantify quality degradation over time. drif is defined as the difference in metric values between the first and last 15% segments of video. SemanticGen also surpasses baselines on this metric. Meanwhile, SemanticGen learns in datadriven manner to generate coherent multi-shot videos. 4.3. More Analysis and Ablation Studies The Effectiveness of Semantic Space Compression. In Sec. 3.2, we propose to compress the semantic representation space using lightweight MLP for efficient training. The effectiveness of this approach is validated through qualitative and quantitative results in Fig. 8 and Tab. 3, respectively. Specifically, we use the vision tower of Qwen2.5VL3B-Instruct [3] as the semantic encoder, where the vanilla semantic representation has dimension of 2048. We first train three VAE latent generators (illustrated in Fig. 3b) using: (1) no MLP, (2) an MLP with 64 output channels, and (3) an MLP with 8 output channels, each for 10K steps. Based on these models, we further train three corresponding semantic generation models (illustrated in Fig. 3a) for 50K steps. During inference, we first use the semantic generator to produce the video semantic representation, which is then used as condition input to the VAE latent generation model to map it into the VAE space. As shown in Fig. 8, we observe that the visual quality of the generated videos improves as the dimensionality decreases, exhibiting fewer broken frames and artifacts. We further quantitatively evaluate the video quality on VBench metrics with 47 text prompts in Tab. 3, which also confirms this trend. This indicates that compressing the pre-trained semantic representation space to lower dimension accelerates the convergence of the semantic generator. Figure 8. Qualitative ablation on semantic space compression. Row 1: SemanticGen without compression; Row 2: Compress the semantic space using an MLP with 64 output channels; Row 3: Compress the semantic space using an MLP with 8 output channels. Table 3. Quantitative ablation of semantic space compression with respect to different representation dimensionalities. Method Subject Consistency Background Consistency Temporal Flickering Motion Smoothness Imaging Quality Aesthetic Quality w.o. compression (dim=2048) w. compression (dim=64) w. compression (dim=8) 96.29% 97.36% 97.49% 96.54% 96.85% 97.34% 96.39% 98.23% 98.27% 99.31% 98.34% 99.38% 58.88% 67.42% 68.16% 60.62% 68.43% 60.95% pressed VAE latents rather than semantic features. Both the semantic generator and the VAE latent generator are trained from scratch for 10K steps, and the results are shown in Fig. 9. We observe that modeling in the VAE space leads to significantly slower convergence, as the generated results only contain coarse color patches. In contrast, the model trained in the semantic space is already able to produce reasonable videos under the same number of training steps. This demonstrates that the proposed SemanticGen framework effectively accelerates the convergence of diffusionbased video generation models. 5. Conclusion and Limitations In this paper, we propose SemanticGen, video generation framework that synthesizes videos in compact semantic space. The key idea is to first generate high-level semantic representations for global planning and then refine them with high-frequency details. SemanticGen follows twostage pipeline: it first produces semantic video features that define the global layout, and then generates VAE latents conditioned on these features to produce the final video. We observe faster convergence in the semantic space than in the VAE latent space, and the approach scales efficiently to long video generation. Despite these advantages, several limitations remain. Long video generation struggles to maintain consistency in textures, as semantic features cannot fully preserve fine-grained details. In addition, SemanticGen inherits constraints from its semantic encoders. For instance, sampling at low fps leads to the loss of highfrequency temporal information, as shown in the appendix. Figure 9. Ablation on the representation space. We visualize the generation results of learning on the semantic space and the compressed VAE latent space with the same training steps. SemanticGen Achieve Faster Convergence Speed. In this paper, we propose to first learn compact semantic representations and then map them into the VAE latent space. natural question arises: Does leveraging semantic representations truly benefit video generation? In other words, what happens if we adopt the same two-stage pipeline but learn compact VAE latents instead of semantic representations [47]? To investigate this, we keep the SemanticGen framework unchanged except for replacing the semantic encoder with VAE encoder, training generator to model com-"
        },
        {
            "title": "References",
            "content": "[1] Mido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Matthew Muckley, Ammar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, et al. V-jepa 2: Selfsupervised video models enable understanding, prediction and planning. arXiv preprint arXiv:2506.09985, 2025. 4, 1 [2] Jianhong Bai, Menghan Xia, Xiao Fu, Xintao Wang, Lianrui Mu, Jinwen Cao, Zuozhu Liu, Haoji Hu, Xiang Bai, Recammaster: Camera-controlled Pengfei Wan, et al. generative rendering from single video. arXiv preprint arXiv:2503.11647, 2025. 5 [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 4, 5, 7, 1 [4] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, et al. Lumiere: space-time diffusion model for video generation. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. 2 [5] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [6] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2256322575, 2023. [7] Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. 2, 3 [8] Joao Carreira, Dilara Gokay, Michael King, Chuhan Zhang, Ignacio Rocco, Aravindh Mahendran, Thomas Albert Keck, Joseph Heyward, Skanda Koppula, Etienne Pot, et al. Scaling 4d representations. arXiv preprint arXiv:2412.15212, 2024. 4, 1 [9] Boyuan Chen, Diego Martı Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. NeurIPS, 37:2408124125, 2024. 2, 3 [10] Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Junchen Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Skyreels-v2: Zheng Chen, Chengcheng Ma, et al. arXiv preprint Infinite-length film generative model. arXiv:2504.13074, 2025. 6, 7, 1 [11] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7310 7320, 2024. 2 [12] Hao Chen, Yujin Han, Fangyi Chen, Xiang Li, Yidong Wang, Jindong Wang, Ze Wang, Zicheng Liu, Difan Zou, and Bhiksha Raj. Masked autoencoders are effective tokenizers for diffusion models. In Forty-second International Conference on Machine Learning, 2025. 2, 3 [13] Junyu Chen, Dongyun Zou, Wenkun He, Junsong Chen, Enze Xie, Song Han, and Han Cai. Dc-ae 1.5: Accelerating diffusion model convergence with structured latent space. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1962819637, 2025. 2, [14] Ming Chen, Liyuan Cui, Wenyuan Zhang, Haoxian Zhang, Yan Zhou, Xiaohan Li, Songlin Tang, Jiwen Liu, Borui Liao, Hejia Chen, et al. Midas: Multimodal interactive digitalhuman synthesis via real-time autoregressive video generation. arXiv preprint arXiv:2508.19320, 2025. 3 [15] Justin Cui, Jie Wu, Ming Li, Tao Yang, Xiaojie Li, Rui Wang, Andrew Bai, Yuanhao Ban, and Cho-Jui Hsieh. Selfforcing++: Towards minute-scale high-quality video generation. arXiv preprint arXiv:2510.02283, 2025. 3 [16] Karan Dalal, Daniel Koceja, Jiarui Xu, Yue Zhao, Shihao Han, Ka Chun Cheung, Jan Kautz, Yejin Choi, Yu Sun, and Xiaolong Wang. One-minute video generation with test-time training. In CVPR, pages 1770217711, 2025. 3 [17] Google DeepMind. https://https://deepmind.google/models/veo/, Veo 3. 2025.5. [18] Haoge Deng, Ting Pan, Haiwen Diao, Zhengxiong Luo, Yufeng Cui, Huchuan Lu, Shiguang Shan, Yonggang Qi, and Xinlong Wang. Autoregressive video generation without vector quantization. arXiv preprint arXiv:2412.14169, 2024. 3 [19] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 3 [20] Zigang Geng, Yibing Wang, Yeyao Ma, Chen Li, Yongming Rao, Shuyang Gu, Zhao Zhong, Qinglin Lu, Han Hu, Xiaosong Zhang, et al. X-omni: Reinforcement learning makes discrete autoregressive image generative models great again. arXiv preprint arXiv:2507.22058, 2025. 3 [21] Yuchao Gu, Weijia Mao, and Mike Zheng Shou. Longcontext autoregressive video modeling with next-frame prediction. arXiv preprint arXiv:2503.19325, 2025. [22] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 2 [23] Yuwei Guo, Ceyuan Yang, Ziyan Yang, Zhibei Ma, Zhijie Lin, Zhenheng Yang, Dahua Lin, and Lu Jiang. Long arXiv preprint context arXiv:2503.10589, 2025. 3 tuning for video generation. [24] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, et al. Ltx-video: Realtime video latent diffusion. 2024. 2 arXiv preprint arXiv:2501.00103, [25] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000 16009, 2022. 3 [26] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 33:68406851, 2020. 2 [27] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 2 [28] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. NeurIPS, 35:86338646, 2022. 2 [29] Jinyi Hu, Shengding Hu, Yuxuan Song, Yufei Huang, Mingxuan Wang, Hao Zhou, Zhiyuan Liu, Wei-Ying Ma, and Maosong Sun. Acdit: Interpolating autoregressive conditional modeling and diffusion transformer. arXiv preprint arXiv:2412.07720, 2024. 3 [30] Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self forcing: Bridging the traintest gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025. 2, 3, 6, 7, [31] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchIn CVPR, pages mark suite for video generative models. 2180721818, 2024. 6 [32] Ziqi Huang, Fan Zhang, Xiaojie Xu, Yinan He, Jiashuo Yu, Ziyue Dong, Qianli Ma, Nattapol Chanpaisit, Chenyang Si, Yuming Jiang, et al. Vbench++: Comprehensive and versatile benchmark suite for video generative models. arXiv preprint arXiv:2411.13503, 2024. 6 [33] Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video arXiv preprint arXiv:2410.05954, generative modeling. 2024. 3 [34] Jihwan Kim, Junoh Kang, Jinyoung Choi, and Bohyung Han. Fifo-diffusion: Generating infinite videos from text without training. NeurIPS, 37:8983489868, 2024. 3 [35] Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 2, 3 [36] Kling. Kling. Kling. Accessed Sept.30, 2024 [Online] https://kling.kuaishou.com/en, 2024. 5 [37] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, et al. Videopoet: large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023. 2, 3 [38] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 2, 6, [39] Theodoros Kouzelis, Efstathios Karypidis, Ioannis Kakogeorgiou, Spyros Gidaris, and Nikos Komodakis. Boosting generative image modeling via joint image-feature synthesis. arXiv preprint arXiv:2504.16064, 2025. 3 [40] Tianhong Li, Dina Katabi, and Kaiming He. Return of unconditional generation: self-supervised representation generation method. NeurIPS, 37:125441125468, 2024. 2, 3 [41] Shanchuan Lin, Ceyuan Yang, Hao He, Jianwen Jiang, Yuxi Ren, Xin Xia, Yang Zhao, Xuefeng Xiao, and Lu Jiang. Autoregressive adversarial post-training for real-time interactive video generation. arXiv preprint arXiv:2506.09350, 2025. 3 [42] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 2, 3 [43] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1001210022, 2021. 5 [44] Chuofan Ma, Yi Jiang, Junfeng Wu, Jihan Yang, Xin Yu, Zehuan Yuan, Bingyue Peng, and Xiaojuan Qi. Unitok: unified tokenizer for visual generation and understanding. arXiv preprint arXiv:2502.20321, 2025. [45] Yihao Meng, Hao Ouyang, Yue Yu, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Hanlin Wang, Yixuan Li, Cheng Chen, Yanhong Zeng, et al. Holocine: Holistic generation of cinematic multi-shot long video narratives. arXiv preprint arXiv:2510.20822, 2025. 3, 1 [46] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 3 [47] Wenqi Ouyang, Zeqi Xiao, Danni Yang, Yifan Zhou, Shuai Yang, Lei Yang, Jianlou Si, and Xingang Pan. Tokensgen: Harnessing condensed tokens for long video generation, 2025. 2, 8 [48] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 3, 1 [49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 4 [50] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [51] Team Seawead, Ceyuan Yang, Zhijie Lin, Yang Zhao, Shanchuan Lin, Zhibei Ma, Haoyuan Guo, Hao Chen, Lu Qi, Sen Wang, et al. Seaweed-7b: Cost-effective training of video generation foundation model. arXiv preprint arXiv:2504.08685, 2025. 2, 5 [52] Minglei Shi, Haolin Wang, Wenzhao Zheng, Ziyang Yuan, Xiaoshi Wu, Xintao Wang, Pengfei Wan, Jie Zhou, and Jiwen Lu. Latent diffusion model without variational autoencoder. arXiv preprint arXiv:2510.15301, 2025. 3 [53] Oriane Simeoni, Huy Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michael Ramamonjisoa, et al. Dinov3. arXiv preprint arXiv:2508.10104, 2025. 4 [54] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. and Stefano Ermon. arXiv preprint [55] Sora. Sora. Sora. Accessed Sept.30, 2024 [Online] https: / / openai . com / index / video - generation - models-as-world-simulators/, 2024. 2 [56] Genmo Team. Mochi 1. https://github.com/ genmoai/models, 2024. 2 [57] Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tianning Zhang, WQ Zhang, Weifeng Luo, et al. Magi-1: Autoregressive video generation at scale. arXiv preprint arXiv:2505.13211, 2025. 1 [58] Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. 4 [59] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 2, 5, 6, [60] Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, and Yu Qiao. Videomae v2: Scaling video masked autoencoders with dual masking. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1454914560, 2023. 4, 1 [61] Qinghe Wang, Yawen Luo, Xiaoyu Shi, Xu Jia, Huchuan Lu, Tianfan Xue, Xintao Wang, Pengfei Wan, Di Zhang, and Kun Gai. Cinemaster: 3d-aware and controllable framework for cinematic text-to-video generation. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 110, 2025. 2 [62] Qinghe Wang, Xiaoyu Shi, Baolu Li, Weikang Bian, Quande Liu, Huchuan Lu, Xintao Wang, Pengfei Wan, Kun Gai, and Xu Jia. Multishotmaster: controllable multi-shot video generation framework. arXiv preprint arXiv:2512.03041, 2025. 3 [63] Shuai Wang, Zhi Tian, Weilin Huang, and Limin Wang. arXiv preprint Ddt: Decoupled diffusion transformer. arXiv:2504.05741, 2025. 3 [64] Yuqing Wang, Tianwei Xiong, Daquan Zhou, Zhijie Lin, Yang Zhao, Bingyi Kang, Jiashi Feng, and Xihui Liu. Loong: Generating minute-level long videos with autoregressive language models. arXiv preprint arXiv:2410.02757, 2024. 3 [65] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. International Journal of Computer Vision, 133(5):30593078, 2025. 2 [66] Dirk Weissenborn, Oscar Tackstrom, and Jakob Uszkoreit. Scaling autoregressive video models. arXiv preprint arXiv:1906.02634, 2019. 3 [67] Ge Wu, Shen Zhang, Ruijing Shi, Shanghua Gao, Zhenyuan Chen, Lei Wang, Zhaowei Chen, Hongcheng Gao, Yao Tang, Jian Yang, et al. Representation entanglement for generation: Training diffusion transformers is much easier than you think. arXiv preprint arXiv:2507.01467, 2025. 3 [68] Haocheng Xi, Shuo Yang, Yilong Zhao, Chenfeng Xu, Muyang Li, Xiuyu Li, Yujun Lin, Han Cai, Jintao Zhang, Dacheng Li, et al. Sparse videogen: Accelerating video diffusion transformers with spatial-temporal sparsity. arXiv preprint arXiv:2502.01776, 2025. 2 [69] Yifei Xia, Suhan Ling, Fangcheng Fu, Yujie Wang, Huixia Li, Xuefeng Xiao, and Bin Cui. Training-free and adaptive sparse attention for efficient long video generation. arXiv preprint arXiv:2502.21079, 2025. [70] Shuai Yang, Wei Huang, Ruihang Chu, Yicheng Xiao, Yuyang Zhao, Xianbang Wang, Muyang Li, Enze Xie, Yingcong Chen, Yao Lu, et al. Longlive: Real-time interactive long video generation. arXiv preprint arXiv:2509.22622, 2025. 3, 6, 7, 1 [71] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2 [72] Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. In CVPR, pages 1570315712, 2025. 2, 3 [73] Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion models. In CVPR, pages 2296322974, 2025. 3 [74] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024. 2, 3 [75] Lvmin Zhang and Maneesh Agrawala. Packing input frame context in next-frame prediction models for video generation. arXiv preprint arXiv:2504.12626, 2(3):5, 2025. 3, 6, 7 [76] Boyang Zheng, Nanye Ma, Shengbang Tong, and Saining Xie. Diffusion transformers with representation autoencoders. arXiv preprint arXiv:2510.11690, 2025. 3, 5 [77] Deyu Zhou, Quan Sun, Yuang Peng, Kun Yan, Runpei Dong, Duomin Wang, Zheng Ge, Nan Duan, and Xiangyu Zhang. Taming teacher forcing for masked autoregressive video generation. In CVPR, pages 73747384, 2025. SemanticGen: Video Generation in Semantic Space"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Introduction of the Base Text-to-Video GenB.3. Failure Cases Visualization eration Model We use transformer-based latent diffusion model [48] as the base T2V generation model, as illustrated in Fig. 10. We employ 3D-VAE to transform videos from the pixel space to latent space, upon which we construct transformerbased video diffusion model. We use 3D self-attention, enabling the model to effectively perceive and process spatiotemporal tokens, thereby achieving high-quality and coherent video generation model. Specifically, before each attention or feed-forward network (FFN) module, we map the timestep to scale, thereby applying RMSNorm to the spatiotemporal tokens. B. More Results B.1. More Results of SemanticGen More synthesized results of SemanticGen are presented in Fig. 11 and Fig. 12. SemanticGen demonstrates the ability to: 1) generate high-quality videos adhering to the text prompts; 2) generate long videos with long-term consistency, and significantly alleviate drifting issues. Please refer to the project page for video results. B.2. Comparison with Additional Baselines We include qualitative comparisons on long video generation with additional baselines in Fig. 13. We use open-source models MAGI-1 [57], SkyReels-V2 [10], SelfForcing [30], LongLive [70], and HoloCine [45] as baselines. To provide reliable assessment of our proposed paradigm, we include additional baselines that continue training the base model using the standard diffusion loss without semantic modeling, while keeping the data and the number of training steps identical. These comparisons are included as important baselines in Fig. 13, denoted as BaseCT and Base-Swin-CT. SemanticGen achieves better longterm consistency and significantly alleviates drifting issues. We observe that baselines may exhibit severe color shifts or inconsistencies across frames. Similar phenomena appear when continuing to train the base text-to-video generation model with Swin attention without global semantic modeling (denoted as Base-CT-Swin in Fig. 13), where we observe inconsistent backgrounds across generated frames and more artifacts, highlighting the importance of performing global planning in the high-level semantic space. We present the failure cases in Fig. 14. Since our model utilizes pre-trained video understanding tokenizers as semantic encoders, SemanticGen inherits constraints from its understanding tokenizers. For instance, sampling at low fps as the input to the understanding tokenizers leads to the loss of high-frequency temporal information. To illustrate this phenomenon, we input reference video into the semantic encoder to extract semantic representations, and then feed these representations into the VAE latent generator to produce video. As shown in the first two rows of Fig. 14, when we sample at fps=1.6 as the input to the semantic encoder, it fails to capture the temporal variation within 1/24 of second (e.g., the sky changing from bright to dark and back due to lightning), resulting in the generated video lacking flicker. We anticipate the development of video understanding tokenizers that simultaneously achieve high temporal compression and high sampling rates, which could further enhance SemanticGens performance. Additionally, we observed that for long video generation, fine-grained details (such as textures or small objects) may not be consistently preserved, as semantic features cannot fully capture these details, as shown in the last row of Fig. 14. C. Future Work Systematic Analysis of Different Semantic Encoders In this paper, we propose first modeling compact semantic representations and then mapping them into the VAE latent space. We leverage the vision tower of Qwen-2.5-VL [3] as the semantic encoder to demonstrate the effectiveness of SemanticGen. systematic analysis of using different semantic encoders [1, 8, 60] is valuable. Specifically, it is important to explore whether the generated performance varies when using semantic encoders trained with different paradigms (e.g., visual-text alignment, self-supervised learning, etc.). Towards More Informative Video Semantic Encoders Pre-trained video semantic encoders play vital role in SemanticGen, as generation first occurs in the semantic space. Therefore, more powerful semantic encoder could lead to better generation performance. For example, we need tokenizer that not only achieves high temporal compression but also samples the original video at high frame rate, which would better facilitate modeling high-frequency temporal information. Figure 10. Overview of the base text-to-video generation model. Figure 11. More synthesized results of SemanticGen. Figure 12. More synthesized results of SemanticGen. Figure 13. Comparison with additional baselines. Figure 14. Visualization of failure cases."
        }
    ],
    "affiliations": [
        "CUHK",
        "DLUT",
        "HUST",
        "Kling Team, Kuaishou Technology",
        "Zhejiang University"
    ]
}