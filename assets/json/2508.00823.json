{
    "paper_title": "IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation",
    "authors": [
        "Wenxuan Guo",
        "Xiuwei Xu",
        "Hang Yin",
        "Ziwei Wang",
        "Jianjiang Feng",
        "Jie Zhou",
        "Jiwen Lu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual navigation with an image as goal is a fundamental and challenging problem. Conventional methods either rely on end-to-end RL learning or modular-based policy with topological graph or BEV map as memory, which cannot fully model the geometric relationship between the explored 3D environment and the goal image. In order to efficiently and accurately localize the goal image in 3D space, we build our navigation system upon the renderable 3D gaussian (3DGS) representation. However, due to the computational intensity of 3DGS optimization and the large search space of 6-DoF camera pose, directly leveraging 3DGS for image localization during agent exploration process is prohibitively inefficient. To this end, we propose IGL-Nav, an Incremental 3D Gaussian Localization framework for efficient and 3D-aware image-goal navigation. Specifically, we incrementally update the scene representation as new images arrive with feed-forward monocular prediction. Then we coarsely localize the goal by leveraging the geometric information for discrete space matching, which can be equivalent to efficient 3D convolution. When the agent is close to the goal, we finally solve the fine target pose with optimization via differentiable rendering. The proposed IGL-Nav outperforms existing state-of-the-art methods by a large margin across diverse experimental configurations. It can also handle the more challenging free-view image-goal setting and be deployed on real-world robotic platform using a cellphone to capture goal image at arbitrary pose. Project page: https://gwxuan.github.io/IGL-Nav/."
        },
        {
            "title": "Start",
            "content": "IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation Wenxuan Guo1 Xiuwei Xu1 Hang Yin1 Ziwei Wang2 Jianjiang Feng1 Jie Zhou1 Jiwen Lu 1Tsinghua University 2Nanyang Technological University {gwx22, xxw21, yinh23}@mails.tsinghua.edu.cn ziwei.wang@ntu.edu.sg {jfeng, jzhou, lujiwen}@tsinghua.edu.cn 5 2 0 A 1 ] . [ 1 3 2 8 0 0 . 8 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Visual navigation with an image as goal is fundamental and challenging problem. Conventional methods either rely on end-to-end RL learning or modular-based policy with topological graph or BEV map as memory, which cannot fully model the geometric relationship between the explored 3D environment and the goal image. In order to efficiently and accurately localize the goal image in 3D space, we build our navigation system upon the renderable 3D gaussian (3DGS) representation. However, due to the computational intensity of 3DGS optimization and the large search space of 6-DoF camera pose, directly leveraging 3DGS for image localization during agent exploration process is prohibitively inefficient. To this end, we propose IGL-Nav, an Incremental 3D Gaussian Localization framework for efficient and 3D-aware image-goal navigation. Specifically, we incrementally update the scene representation as new images arrive with feed-forward monocular prediction. Then we coarsely localize the goal by leveraging the geometric information for discrete space matching, which can be equivalent to efficient 3D convolution. When the agent is close to the goal, we finally solve the fine target pose with optimization via differentiable rendering. The proposed IGL-Nav outperforms existing state-of-the-art methods by large margin across diverse experimental configurations. It can also handle the more challenging free-view imagegoal setting and be deployed on real-world robotic platform using cellphone to capture goal image at arbitrary pose. Project page: https://gwxuan.github.io/IGL-Nav/. 1. Introduction Image-goal navigation, which requires an agent initialized in unknown environment to navigate to the location and orientation specified by an image [39], is fundamental problem in wide range of robotic tasks. This task requires the *Equal contribution. Corresponding author. Figure 1. IGL-Nav effectively guides the agent to reach free-view image goal via incremental 3D gaussian localization. agent to precisely understand spatial information, as well as to reason how to explore the scene with past observations, which is hard to learn with end-to-end RL [32] due to low sample efficiency and catastrophic forgetting. Recent advances in visual navigation have witnessed significant progress in modular-based approaches [2, 3, 7, 14, 23, 34, 35], which establish an explicit memory to cache observed environmental information and derive navigation policies based on the memory representations. While these approaches demonstrate enhanced capabilities for longhorizon reasoning and temporal dependency modeling in object-goal navigation tasks, their extension to image-goal navigation remains challenging. Unlike object-goal scenarios that primarily rely on high-level semantic understanding, image-goal navigation necessitates the preservation and processing of low-level visual features, including fine-grained texture patterns and color distributions. Consequently, conventional representation paradigms of memory such as topological graphs prove insufficient for effectively encoding the requisite environmental information in image-goal settings. To address these limitations, RNRMap [14] introduces renderable neural radiance map representation. Drawing inspiration from NeRF [20], this representation enables photorealistic image rendering from arbitrary camera viewpoints. The renderable nature ensures the preservation of crucial low-level visual features, which has demonstrated superior performance in image-goal navigation. However, since NeRF is an implicit field with high computational cost, RNR-Map has to maintain the renderable representation in 2D BEV map for efficient and explicit memory management. This 2D projection inherently loses critical 3D structural information, forcing RNRMap to impose strict constraints on goal image acquisition, specifically requiring horizontal camera angles to ensure alignment with its BEV map. This significantly reduces its applicability in real-world scenarios. Therefore, an efficient 3D-aware memory representation is still desirable for image-goal navigation. In this paper, we propose to leverage 3D Gaussian Splatting (3DGS) [10] as the scene representation for imagegoal navigation. The 3DGS representation demonstrates exceptional suitability for the task: (1) as an explicit representation, 3DGS can be easily initialized with the observed RGB-D image and be incrementally accumulated in 3D space; (2) it supports efficient differentiable rendering, which can be used to localize the camera pose of goal image with iterative optimization. Despite these compelling properties, adapting 3DGS representations for image-goal navigation presents significant challenges. While 3DGS achieves rendering speeds orders of magnitude faster than NeRF, their optimization process remains computationally prohibitive for real-time online inference required in navigation tasks. Furthermore, goal image localization within scene-level 3DGS maps becomes intractable due to the exponential search space complexity inherent in 6-DoF camera pose estimation. To this end, we propose IGL-Nav, an Incremental 3D Gaussian Localization framework that (1) progressively constructs 3DGS through feed-forward prediction, eliminating offline optimization; and (2) enables efficient hierarchical goal search by harnessing both geometric and photometric attributes of 3DGS through our novel coarse-to-fine localization strategy. Extensive experiments on various datasets in Habitat simulator show our IGL-Nav significantly outperforms previous state-of-the-art imagegoal navigation methods. Moreover, benefit from our explicit 3D representation, IGL-Nav is also able to handle the more practical free-view image-goal setting, where there is no assumption on both camera intrinsics and extrinsics of the goal image. We further deploy our method on real-world robot, where casually taken photo from cellphone can be used as goal to guide the agent navigating to specified location in complicated and large-scale environments. sents scene as dense set of points with gaussian embedding and leverages efficient rasterization techniques for high-fidelity, real-time rendering. Recently, feed-forward 3DGS models [46] have been proposed, primarily to address the issue of sparse-view scene reconstruction. Unlike traditional methods that iteratively optimize 3DGS parameters, feed-forward 3DGS predicts the gaussian distribution through network, significantly improving modeling efficiency. In embodied AI, 3DGS has been applied to manipulation tasks, with dynamic 3DGS frameworks and gaussian world models used to model and predict robotic actions [19]. And systems like Gaussian-Grasper [38] leverage RGB-D inputs for language-guided grasping. 3DGS also helps bridge the gap between simulated and real-world environments for generalizing learned behaviors. Techniques such as Robo-GS [18] and SplatSim [22] improve Sim-to-Real transfer by leveraging efficient representation of 3DGS. The incremental 3DGS scene representation used in our IGL-Nav also follows the paradigm of feed-forward 3DGS, making it suitable for scene modeling based on online inputs in navigation tasks. Image-goal Navigation. Image-goal navigation involves the agent navigating to the location where the goal image is captured [39], requiring precise alignment in both position and orientation. To address this challenge, researchers have employed various strategies. Some focus on optimizing reinforcement learning (RL) policies [1, 32, 33] that directly map observations to actions. Others concentrate on constructing detailed maps [3, 11, 14, 24], or on developing carefully crafted matching algorithms [29]. However, image-goal navigation requires that the query image be captured by the agents camera, which limits the cameras intrinsic parameters, height, and the fact that it can only rotate around the Z-axis. Considering these constraints in practical applications, we propose free-view image-goal navigation, where the target image can be captured by any camera at free 3D position and orientation. In addition, instance image-goal navigation [12] is similar task, where the target image focuses on specific categories of objects within the scene. In instance image-goal task, GaussNav [16] also uses 3DGS-based scene representation. However, GaussNav requires first completing the exploration of the entire building to optimize the 3DGS representation, and then render images at multi poses for comparison with the target image. This approach limits efficiency in practical applications. In contrast, our IGL-Nav simultaneously performs exploration, incremental modeling, and target localization, and it incorporates coarse-to-fine localization strategy, making full use of the 3DGS representation. 2. Related Work 3. Approach 3D Gaussian Splatting. 3DGS [10] has emerged as powerful technique for 3D scene representation. It repreIn this section, we first describe our task definition. Next, we explain several core modules of IGL-Nav, including incremental scene representation with 3DGS, and coarse-tofine target localization. Finally, we detail the overall navigation pipeline. 3.1. Problem Statement We study the problem of free-view image-goal navigation, which is more challenging and practical setting. In this task, mobile agent is instructed with navigating to specified location depicted by an image g, taken by camera with pose Tg. The agent is equipped with camera B. It receives posed RGB-D video stream {I t, Dt, Tt}T t=1 and is required to execute an action at each time it receiving new RGB-D observation. t, Dt, Tt refer to RGB image, depth image and camera pose at time instant t. is the set of actions, which consists of move forward, turn left, turn right and stop. The task is considered successfully completed if the agent terminates within horizontal neighborhood of the target pose, satisfying P(Tf inal) P(Tg)2 < ϵ within maximum of navigation steps. Here refers to 3D-to-BEV projection. Comparison with Relevant Tasks. In the free-view image-goal navigation, there is no assumption on the correlation between camera and B. For example, in real application scenarios, can be cellphone, and is RGBD camera with totally different camera intrinsics. Previous image-goal setting [7, 14] can be regarded as special case of our task where and Tg is restricted to lie within camera Bs achievable pose space. Instance-imagegoal navigation [13] also aims to decouple camera and B. However, this setting requires that there must be an instance located at image center, and only 6 categories of instances are supported. These limitations fundamentally constrain the systems operational flexibility and real-world deployment potential. In this paper, we conduct experiments on both conventional and free-view image-goal settings for comprehensive evaluation of different approaches. 3.2. Incremental Scene Representation We adopt 3DGS as our scene representation due to its explicit nature and efficient rendering capability. However, the original 3DGS are obtained through offline optimization on image set and thus hard to be applied in real-time tasks. Recent feed-forward methods [46] abandon optimization and directly predict pixel-aligned 3DGS parameters, but they still rely on multi-view images to reconstruct geometric information of the scene. In visual navigation, the agent needs to incrementally build scene representation along with its exploration, so the 3DGS should be generated in real-time and update as new images arrive. To accommodate streaming video input while effectively leveraging camera pose and depth priors, we present the first feedforward 3DGS reconstruction model for monocular RGB-D sequences, which supports real-time 3DGS reconstruction and incremental accumulation. Gaussian Parameters Prediction. At time step t, the agent receives new RGB-D observations RHW 3 and Dt RHW 1. Our incremental reconstruction model is essentially mapping fθ from observations to 3DGS parameters, including position µk, opacity αk, covariance Σk and spherical harmonics ck: fθ : (I t, Dt) (cid:55) {(µk, αk, Σk, ck)}HW The 3DGS parameters are predicted in pixel-aligned manner, thus an observation input of size corresponds to an output of gaussians. (1) k=1 The feed-forward model fθ is shown in Figure 2. We first concatenate the normalized RGB and depth images, and then extract dense monocular scene embedding with UNet-based encoder E. Then 3DGS parameters are regressed through gaussian head H, composed of few CNN and linear layers. This process can be expressed as: C2D, D, α, Σ, = H(E), = E(I, D) (2) where C2D and are residuals of image coordinates and depth. We omit subscript for simplicity. Using the camera intrinsic matrix M, pose Tt and inverse projection Proj1, we can compute the 3DGS positions as: µ = Proj1(C2D + C2D, + M, Tt) (3) We also lift from 2D to the corresponding 3D positions. Finally, the 3DGS scene representation and the corresponding 3D embedding can be updated as: Gt = Gt1 (µt, αt, Σt, ct) and Et = Et1 t. When the number of 3DGS in the scene is large, we prune Gt and Et based on opacity and 3DGS density to reduce memory footprint. Additionally, we can use to extract the 3D embedding Eg of the target image g. If depth and camera intrinsics are unavailable for g, we simply use monocular depth estimator [21] to predict them. Training and Loss. Our feed-forward model can be trained using passive offline RGB-D video streams. We randomly sample training episodes from navigation training set. In each episode, frames are randomly selected to predict 3DGS parameters, and images from other viewpoints are rendered for loss computation. The training loss is linear combination of L-2 and LPIPS [37] losses. 3.3. Coarse-to-fine Localization Since the target image is captured by an arbitrary camera at any pose (6-DoF), the search space of the target is extremely large. To perform efficient and accurate visual navigation, we design coarse-to-fine target localization strategy. Coarse localization leverages the incremental scene embedding Et to predict the approximate target location in real-time during exploration. Once the agent is close to the target, fine localization is employed to accurately determine the accurate target position and guide the agent to reach it. Figure 2. Illustration of IGL-Nav. (a) We maintain an incremental 3DGS scene representation with feed-forward prediction. (b) The coarse target localization is modeled as 5-dimension matching problem, which is efficiently implemented by leveraging the target embedding as 3D convolutional kernel. (c) Fine target localization via differentiable 3DGS rendering and matching-constrained optimization. , zi = , yi = space. For (x, y, z), the 3D space is voxelized into grids {(xi, yi, zi) xi = }, where is voxel size. For (θ, ϕ), we discretize the spherical surface into vertices of hierarchical mesh via γ-level subdivision of regular icosahedron, as shown in Figure 2. In this way, we can rotate Eg according to the discretized sphere to obtain 3D embeddings {E1 }. By translating these embeddings to the discretized voxel grids and computing the extent of alignment between the translated embedding and Et, the coarse target pose can be determined by: g, ..., EN maximize i,k A(Et, (Ek , (xi, yi, zi))) (4) where computes the extent of alignment between two sets of 3D features. stands for translation operation. and are used to query the corresponding (x, y, z, θ, ϕ). However, the above operation is still hard to achieve realtime inference. We need to traverse all voxel grids and compare the translated 3D embedding with Et. Assume there are grids at all, then times comparisons should be performed. Moreover, during each comparison, we should compute the geometric similarity between two 3D pointclouds as well as their feature similarity, which is especially time-consuming and hard to be accelerated on GPUs. To solve this problem, we propose to further discretize the 3D embeddings Et and Eg. For Et, we can simply quantize the pointclouds into voxels, where voxel features are obtained by taking average of the pointcloud features inside each voxel. For {E1 }, we uniformly quantize them into voxels. Note that although {E1 } are different pointclouds, they will share the g, ..., EN g, ..., EN (a) Line LR is Figure 3. Modeling of the camera pose space. almost always parallel to the ground. (b) Line AO is parallel to Plane XOY . Plane AOB is perpendicular to Plane XOY . 3.3.1. Coarse Target Localization Although camera can capture the target image at an arbitrary pose, we observe that the top frame of the camera is almost always parallel to the ground when taking photo, as shown in Figure 3. Therefore, we can represent the actual camera rotation with (θ, ϕ), which denotes rotation around the X-axis by θ degrees, followed by rotation towards the Z-axis by ϕ degrees. Based on this observation, we define sphere-based space : {(x, y, z, θ, ϕ)} to represent camera pose. Here (x, y, z) represents the position of camera and (θ, ϕ) refers to As rotation. We can thus represent the target pose Tg as (xg, yg, zg, θg, ϕg). The 3D embedding of the target Eg is initialized at the origin of the sphere-based space and should be aligned with the scene embedding Et under translation (xg, yg, zg) and rotation (θg, ϕg), which are unknown. To efficiently search the target camera pose in the fivedimensional space, we discretize to reduce the search (xg, xt) between the target image and the rendered image. Here (xg, xt) is the coordinate set of matched pixels. If the number of matching pairs exceeds threshold τ , it is considered that appears in agents field of view. Matching-constrained Optimization. Via differentiable rendering, we can optimize the current camera pose with photometric loss between the rendered image and g, and between rendered depth and Dg (Dg/Mg are the depth / intrinsics of g, which are estimated by [21] if not available), as done in [9, 28]. Although this is an intuitive way to solve Tg, we empirically find it leads to unsatisfactory performance in our case where the quality of Gt may degrade due to incremental accumulation without optimization. Fortunately, we observe the pixels that are successfully matched are of high quality. In order to overcome the imperfect details in the rendering results, we propose to only focus on the matching pairs in 3D space for accurate camera pose optimization. The problem can be formulated as: ˆT = argmin TSE(3) L(T g, Dg, Mg, Gt) (6) We iteratively optimize the pose to minimize the geometric discrepancy between rendering results and target image. At each iteration of optimization, we leverage current for rendering and obtain the matched points in Euclidean space: (xg, x), (dg, d) = M(I g, Dg, R(Gt Mg, T)) (7) (X g, X) = Proj1((xg, x), (dg, d) Mg) where is differentiable rendering of color and depth. The matching and querying operation first adopts LoFTR to get matching pair (xg, x) between and the rendered RGB image, and then queries the corresponding depth value (dg, d) from Dg and the rendered depth respectively. Then we formulate the optimization loss as: (8) ="
        },
        {
            "title": "1\nQ",
            "content": "Q1 (cid:88) (X i2) i=0 (9) where is the number of matching pairs. Note that LoFTR predicts (xg, x) in differentiable way, so gradient can be backpropagated through both the rendered color and depth images. In this way, we effectively align and Tg by focusing on the most confident rendering results. 3.4. Navigation We divide the navigation process into two stages: exploration based on coarse localization and target reaching based on fine localization. Figure 4 illustrates the workflow of IGL-Nav. We will describe each stage in this section. Exploration for Target Discovery. When the agent is initialized in new environment, its observations of the scene are insufficient. Therefore, we combine coarse target localization with frontier-based exploration to explore Figure 4. Navigation pipeline of IGL-Nav. same shape after voxelization, which forms 3D convolutional kernel RLLLCinCout. Here Cin refers to the output channel of E, Cout equals to the number of kernels . Therefore, Eq (4) can be rewritten as: argmax x,y,z,k C(f1(V(Et)), f2(K))[x][y][z][k] (5) where means 3D convolution operation, quantizes scene embedding Et into voxels. We use two MLP f1 / f2 with input channel Cin and output channel to project scene embedding and convolutional kernel to learnable feature space before convolution, which further aligns the embedding space of Et and Eg. The activation map after 3D convolution is of shape , from which we query index of the maximum value and thus obtain coarse localization of the target pose. To further improve computational efficiency, we use pillar-based voxelization [15, 36]. Training and Loss. Similar to the scene representation training, we train the coarse localization module using offline passive video streams. In each training segment, we randomly select position and capture target images with arbitrary intrinsic parameters and orientations. We use focal loss [17] to supervise the activation map after 3D convolution. Additionally, we apply cross-entropy loss to supervise the outputs nearby target pose in the activation map. 3.3.2. Fine Target Localization Our fine localization method aims to accurately determine the targets 6-DoF pose once the agent is close to the target region. It leverages the differentiable rendering ability of 3DGS to reach target pose via iterative optimization. Rendering-based Stopper. First, we use renderingbased stopper to determine if the agent is close to the target. Since the intrinsics of camera and may differ significantly, directly comparing the current observation with the target image with feature matching is difficult. Thanks to the real-time rendering capability of 3DGS Gt, we can render an image at camera Bs current viewpoint with the same intrinsic parameters as camera A. We use local feature matching method, LoFTR [27], to predict matching pairs Table 1. Image-goal Navigation Results. SR: Success Rate, SPL: Success weighted by Path Length. The best result in each column is bold, and the second best is underlined. Straight Curved Method Easy DDPPO [30] NRNS [7] ZSEL [1] OVRL [33] NRNS + SLING [29] OVRL + SLING [29] RNR-Map [14] FeudalNav [8] IGL-Nav (Ours) SR 43.2 64.1 - 53.6 85.3 71.2 76.4 82.6 87.9 SPL 38.5 55.4 - 34.7 74.4 54.1 55.3 75.0 82.5 Medium SR SPL 36.4 47.9 - 48.6 66.8 60.3 73.6 71.0 80.8 34.8 39.5 - 33.3 49.3 44.4 46.1 57.4 69.0 Hard Overall Easy SR 7.4 25.2 - 32.5 41.1 43.0 54.6 49.0 61.7 SPL 7.2 18.1 - 21.9 28.8 29.1 30.2 34.2 40.9 SR 29.0 45.7 - 44.9 64.4 58.2 68.2 67.5 76. SPL 26.8 37.7 - 30.0 50.8 42.5 43.9 55.5 64.1 SR 22.2 27.3 41.0 53.6 58.6 68.4 75.3 72.5 82.8 SPL 16.5 10.6 28.2 31.8 16.1 47.0 52.5 51.3 77. Medium SR SPL 20.7 23.1 27.3 47.6 47.6 57.7 70.9 64.4 80.7 18.5 10.4 13.9 30.2 16.8 39.8 42.3 40.7 70.0 Hard SR 4.2 10.5 18.6 35.6 24.9 40.2 51.0 43.7 57.0 SPL 3.7 5.6 9.3 22.0 10.1 25.5 27.4 25.3 39.6 Overall SR SPL 15.7 20.3 25.9 45.6 43.7 55.4 65.7 60.2 73. 12.9 8.8 17.6 28.0 14.3 37.4 40.8 39.1 62.4 the scene and discover potential targets. Based on the posed RGB-D inputs, we maintain an online occupancy map to indicate explored, unexplored and occupied area in BEV, where the frontiers of explored area can be computed. At each time step, we select the nearest frontier to the agent and generate binary scores Sf on the BEV map, where points on the selected frontier are set to 1, others are set to 0. We then project the activation map obtained in our coarse target localization module to BEV to get Sa. We first filter the activation map Sa by threshold σa, setting scores below the threshold to zero. The agent then prioritizes exploring the location with the highest value in Sa. If all values in Sa are zero, the agent selects the nearest location where the frontier score map Sf equals 1. We adopt Fast Marching Method [26] (FMM) for path planning and action generation given the to-be-explored location. Reaching Target. During exploration, the agent gradually approaches the target. We use the rendering-based stopper to determine if the target appears in agents field of view. Once the target is detected, we switch to fine localization to compute the precise target pose. The XY coordinates of the computed pose is set to be destination, for which we apply FMM again for navigation. 4. Experiment In this section, we first describe our experimental setting. Then we compare IGL-Nav with state-of-the-art image-goal navigation methods. Finally we conduct in-depth modulebased analysis on our framework and further provide realworld deployment results. 4.1. Experimental Setup We conduct experiments on image-goal navigation and the more challenging free-view image-goal navigation tasks. Datasets and Benchmarks. For image-goal navigation, we follow the public Gibson [31] image-goal navigation dataset within the Habitat simulator [25] introduced by NRNS [7]. The Gibson dataset includes 72 houses for training and 14 for validation. The NRNS dataset contains two path types (straight and curved), each with three difficulty levels (easy, medium, hard). For free-view image-goal navigation as introduced in Sec. 3.1, we collect large amount of data with Gibson for validation. Given the significant impact of the cameras field of view (FOV) on scene matching, we categorize our dataset into two FOV-based groups (50 75 and 75 100), which can be intuitively understood as portrait and landscape orientations. Each category further includes three difficulty levels based on distance. Additionally, compared to the NRNS dataset, our free-view image-goal navigation dataset features target images captured from arbitrary angles and heights. Each of the six subsets contains 500 randomly sampled episodes. Compared Methods. We compare IGL-Nav with existing state-of-the-art image-goal navigation methods [1, 7, 8, 14, 29, 30, 33]. For image-goal setting, we report results from the respective papers. For the proposed free-view image-goal setting, we evaluate open-sourced methods on this benchmark and compare with them. Since some methods [7, 29, 30, 33] only release test code, we perform zeroshot transfer to apply them to the new setting without retraining. We also report the zero-shot performance of IGLNav for fair comparison. For methods [7, 30] that provide training scripts, we train them on the free-view image-goal navigation data for comparison. 4.2. Comparison with State-of-the-art We compare with state-of-the-art image-goal navigation methods on the two benchmarks described above. Table 1 demonstrates the results on image-goal navigation task. IGL-Nav establishes new state-of-the-art performance and outperforms previous methods by large margin on all metrics, which validates the effectiveness of 3D gaussian representation and the proposed coarse-to-fine target localization strategy for image-goal navigation. The results on free-view image-goal navigation task is shown in Table 2. As this task is much more challenging than conventional image-goal setting, we observe significant performance drop on each metric. When directly Table 2. Free-view Image-goal Navigation Results. SR: Success Rate, SPL: Success weighted by Path Length. Narrow FOV (50 75) Wide FOV (75 100) Method Easy SR SPL Medium SR SPL Hard Overall Easy SR SPL SR SPL SR SPL Medium SR SPL Hard SR SPL Overall SR SPL Zero-shot Transfer (Training on Image-goal Navigation Data) DDPPO [30] NRNS [7] OVRL [33] NRNS + SLING [29] OVRL + SLING [29] IGL-Nav (Ours) 15.8 19.8 23.8 32.8 28.2 53.2 10.5 10.6 16.6 15.3 20.1 45. 9.6 15.8 19.2 23.6 23.2 47.8 7.2 9.0 10.5 13.2 18.7 40.5 5.4 7.8 8.2 9.8 11.8 28.2 3.1 4.0 6.9 5.6 7.1 22.0 Supervised (Training on Free-view Image-goal Navigation Data) BC + GRU BC + Metric Map DDPPO [30] NRNS [7] NRNS + SLING [29] IGL-Nav (Ours) 13.2 22.8 19.4 30.8 40.2 70.4 6.8 15.9 11.3 24.4 31.8 64.2 10.4 20.6 16.4 27.8 37.2 60.6 8.8 15.6 10.4 24.5 23.9 51.4 6.6 7.4 9.6 11.2 19.8 40.0 4.9 5.2 6.0 8.9 9.8 28. 10.3 14.5 17.1 22.1 21.1 43.1 10.1 16.9 15.1 23.3 32.4 57.0 6.9 7.9 11.3 11.4 15.3 35.9 6.8 12.2 9.2 19.3 21.8 48.2 20.2 28.4 27.6 38.6 36.4 56.2 22.0 25.4 26.8 39.6 49.8 77. 16.5 16.6 19.2 19.1 25.9 48.3 14.4 19.5 17.8 35.0 35.0 73.1 16.6 21.2 22.8 32.6 31.6 55.2 16.0 22.8 19.0 35.8 40.8 69.8 12.5 14.5 12.6 18.5 18.5 46.1 11.4 18.5 12.4 30.1 30.4 60. 9.8 10.6 14.8 17.2 15.2 30.8 9.2 4.8 15.6 13.8 21.6 42.8 5.7 5.9 8.6 8.3 7.6 23.9 6.9 3.5 9.8 8.3 12.7 31.9 15.5 20.1 21.7 29.5 27.7 47.4 15.7 17.7 20.5 29.7 37.4 63. 11.6 12.3 13.5 15.3 17.3 39.4 10.9 13.8 13.3 24.5 26.0 55.0 Table 3. Performance of IGL-Nav when depth and camera intrinsics are unavailable. Table 4. Effects of different subdivision levels in coarse target localization to the final performance. Method Predicted Depth Measured Depth Narrow FOV (50 75) Wide FOV (75 100) SR 53.8 57.0 SPL 44.7 48.2 SR 61.0 63.3 SPL 51.7 55.0 Figure 5. Rendering results of our incremental 3DGS. transferred from image-goal to free-view image-goal setting, IGL-Nav still maintains huge performance lead compared with other state-of-the-art methods. The performance of IGL-Nav can be further boosted with training data on the free-view image-goal task. Note that the zero-shot transferring performance of IGL-Nav is even better than other methods under supervised setting, which demonstrates the great generalization ability of our approach. 4.3. Analysis of IGL-Nav We further conduct in-depth module-by-module analysis on our IGL-Nav framework with sufficient visualization results and ablation studies, which is divided into three parts according to our module design. All ablation studies are conducted on the free-view image-goal setting. Incremental 3DGS Prediction. Following the setting of RNR-Map [14], we assume depth information and camLevel (γ) 1 2 3 Narrow FOV (50 75) Wide FOV (75 100) SR 19.7 41.3 57.0 SPL 12.0 34.4 48.2 SR 24.9 48.9 63.3 SPL 16.8 42.1 55.0 Table 5. Effects of different stoppers in fine target localization to the final performance. Stopper IGL-Nav w/out Stopper IGL-Nav w/ SLING [29] IGL-Nav Narrow FOV (50 75) Wide FOV (75 100) SR 45.7 49.0 57.0 SPL 32.9 40.7 48.2 SR 46.2 52.4 63.3 SPL 37.6 45.0 55.0 era intrinsics are known in our experiments. When these information is unavailable, we can simply adopt depth estimator [21] to predict them. As shown in Table 3, with predicted depth and camera intrinsics, the performance of IGLNav is still robust. We further visualize rendering results of our 3DGS representation in Figure 5. Although maintained in an incremental and feed-forward manner, our 3DGS still demonstrates photorealistic novel view synthesis capability. Coarse Target Localization. In our coarse localization module, the sphere space is discretized with regular icosahedron and its γ-level fractal. larger value of γ leads to finer discretization of the spherical surface and results in greater number of convolution kernels . We study the the effects of different levels to the final performance in Table 4. It is shown that using 3-level subdivision achieves best performance, because finer discretization will reduce quantization error and improve the accuracy of coarse localization. However, larger γ results in high computational cost, making training inefficient. Fine Target Localization. We compare different stoppers in Table 5. The first row refers to only using coarse Figure 6. Visualization of navigation process in Habitat simulator. Figure 7. Visualization of navigation process in the real world. The agent is successfully guided to free-view goal image captured by cellphone in complex indoor environments. IGL-Nav exhibits strong generalization ability and sim-to-real transfer performance. target localization, and the second row refers to using the widely adopted SLING [29] as the stopper and fine localization module. It is shown that our 3DGS-based stopper and matching-constrained optimization is more suitable for the overall navigation system of IGL-Nav. We also visualize the navigation process in Figure 6. The agent is guided with frontier location, activation map obtained with 3D convolution and iterative pose optimization during the exploration. It is shown that our IGL-Nav can effectively localize the target image even with partial observation, and accurately guide the agent to final location with fine-grained rendering-based optimization. 4.4. Real-world Deployment We further deploy IGL-Nav on real-world robotic platform to test its generalization ability. The model is directly taken from the free-view image-goal setting (supervised) without any finetuning on real-world data. As shown in Figure 1 and 7, we use cellphone to capture the target image in viewpoint that is unreachable by the robotic agents camera. Benefit from the flexible rendering capability of 3DGS representation, the agent effectively reaches this free-view goal with the coarse-to-fine localization method. 5. Conclusion In this paper , we have proposed IGL-Nav for efficient and 3D-aware image-goal navigation. We incrementally maintain 3DGS scene representation in feed-forward manner, which is then utilized for coarse-to-fine target localization. We analyze the pose space of the goal image and discretize both the pose space and scene embedding to apply efficient 3D convolution-based coarse matching. When the agent is close to the goal, we switch to fine localization by optimizing the camera pose via differentiable rendering on the confident matching pairs. The proposed IGL-Nav significantly outperforms existing state-of-the-art methods on image-goal and free-view image-goal settings. Real-world experiments further demonstrate our generalization ability. limitation of IGL-Nav is that it requires depth and camera intrinsics of goal image. However, as we show in experiments, using existing monocular depth estimation [21] to predict them can satisfactorily solve this problem."
        },
        {
            "title": "References",
            "content": "[1] Ziad Al-Halah, Santhosh Kumar Ramakrishnan, and Kristen Grauman. Zero experience required: Plug & play modular transfer learning for semantic visual navigation. In CVPR, pages 1703117041, 2022. 2, 6 [2] Devendra Singh Chaplot, Dhiraj Prakashchand Gandhi, Abhinav Gupta, and Russ Salakhutdinov. Object goal navigation using goal-oriented semantic exploration. In NeurIPS, pages 42474258, 2020. 1 [3] Devendra Singh Chaplot, Ruslan Salakhutdinov, Abhinav Gupta, and Saurabh Gupta. Neural topological slam for visual navigation. In CVPR, pages 1287512884, 2020. 1, 2 [4] David Charatan, Sizhe Lester Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction. In CVPR, pages 1945719467, 2024. 2, 3 [5] Anpei Chen, Haofei Xu, Stefano Esposito, Siyu Tang, and Andreas Geiger. Lara: Efficient large-baseline radiance fields. In ECCV, pages 338355. Springer, 2024. [6] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting from sparse In ECCV, pages 370386. Springer, multi-view images. 2024. 2, 3 [7] Meera Hahn, Devendra Singh Chaplot, Shubham Tulsiani, Mustafa Mukadam, James Rehg, and Abhinav Gupta. No rl, no simulation: Learning to navigate without navigating. NeurIPS, 34:2666126673, 2021. 1, 3, 6, [8] Faith Johnson, Bryan Bo Cao, Ashwin Ashok, Shubham Jain, and Kristin Dana. Feudal networks for visual navigation. arXiv preprint arXiv:2402.12498, 2024. 6 [9] Nikhil Keetha, Jay Karhade, Krishna Murthy Jatavallabhula, Gengshan Yang, Sebastian Scherer, Deva Ramanan, and Jonathon Luiten. Splatam: Splat track & map 3d gaussians for dense rgb-d slam. In CVPR, pages 2135721366, 2024. 5 [10] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. TOG, 42(4):1391, 2023. 2 [11] Nuri Kim, Obin Kwon, Hwiyeon Yoo, Yunho Choi, Jeongho Park, and Songhwai Oh. Topological semantic graph memIn CoRL, pages 393402. ory for image-goal navigation. PMLR, 2023. 2 [12] Jacob Krantz, Stefan Lee, Jitendra Malik, Dhruv Batra, and Devendra Singh Chaplot. Instance-specific image goal navigation: Training embodied agents to find object instances. arXiv preprint arXiv:2211.15876, 2022. 2 [13] Jacob Krantz, Theophile Gervet, Karmesh Yadav, Austin Wang, Chris Paxton, Roozbeh Mottaghi, Dhruv Batra, Jitendra Malik, Stefan Lee, and Devendra Singh Chaplot. NavIn ICCV, pages igating to objects specified by images. 1091610925, 2023. 3 [14] Obin Kwon, Jeongho Park, and Songhwai Oh. Renderable neural radiance map for visual navigation. In CVPR, pages 90999108, 2023. 1, 2, 3, 6, [15] Alex Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders for object detection from point clouds. 1269712705, 2019. 5 In CVPR, pages [16] Xiaohan Lei, Min Wang, Wengang Zhou, and Houqiang Li. Gaussnav: Gaussian splatting for visual navigation. T-PAMI, 2025. 2 [17] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object detection. In ICCV, pages 29802988, 2017. 5 [18] Haozhe Lou, Yurong Liu, Yike Pan, Yiran Geng, Jianteng Chen, Wenlong Ma, Chenglong Li, Lin Wang, Hengzhen Feng, Lu Shi, et al. Robo-gs: physics consistent spatialtemporal model for robotic arm with hybrid representation. arXiv preprint arXiv:2408.14873, 2024. [19] Guanxing Lu, Shiyi Zhang, Ziwei Wang, Changliu Liu, Jiwen Lu, and Yansong Tang. Manigaussian: Dynamic gaussian splatting for multi-task robotic manipulation. In ECCV, pages 349366. Springer, 2024. 2 [20] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, pages 405421. Springer, 2020. 1 [21] Luigi Piccinelli, Yung-Hsu Yang, Christos Sakaridis, Mattia Segu, Siyuan Li, Luc Van Gool, and Fisher Yu. Unidepth: In CVPR, Universal monocular metric depth estimation. pages 1010610116, 2024. 3, 5, 7, 8 [22] Mohammad Nomaan Qureshi, Sparsh Garg, Francisco Yandun, David Held, George Kantor, and Abhisesh Silwal. transfer of rgb manipulaSplatsim: Zero-shot sim2real arXiv preprint tion policies using gaussian splatting. arXiv:2409.10161, 2024. 2 [23] Santhosh Kumar Ramakrishnan, Devendra Singh Chaplot, Ziad Al-Halah, Jitendra Malik, and Kristen Grauman. Poni: Potential functions for objectgoal navigation with In CVPR, pages 1889018900, interaction-free learning. 2022. 1 [24] Nikolay Savinov, Alexey Dosovitskiy, and Vladlen Koltun. Semi-parametric topological memory for navigation. arXiv preprint arXiv:1803.00653, 2018. [25] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: platform for embodied ai research. In ICCV, pages 93399347, 2019. 6 [26] James Sethian. Fast marching methods. SIAM review, 41 (2):199235, 1999. 6 [27] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR, pages 89228931, 2021. 5 [28] Yuan Sun, Xuan Wang, Yunfan Zhang, Jie Zhang, Caigui Jiang, Yu Guo, and Fei Wang. icomma: Inverting 3d gaussians splatting for camera pose estimation via comparing and matching. arXiv preprint arXiv:2312.09031, 2023. 5 [29] Justin Wasserman, Karmesh Yadav, Girish Chowdhary, Abhinav Gupta, and Unnat Jain. Last-mile embodied visual In CoRL, pages 666678. PMLR, 2023. 2, 6, navigation. 7, 8 [30] Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva, and Dhruv Batra. Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames. arXiv preprint arXiv:1911.00357, 2019. 6, 7 [31] Fei Xia, Amir Zamir, Zhiyang He, Alexander Sax, Jitendra Malik, and Silvio Savarese. Gibson env: Real-world perception for embodied agents. In CVPR, pages 90689079, 2018. 6 [32] Karmesh Yadav, Arjun Majumdar, Ram Ramrakhya, Naoki Yokoyama, Alexei Baevski, Zsolt Kira, Oleksandr Maksymets, and Dhruv Batra. Ovrl-v2: simple stateof-art baseline for imagenav and objectnav. arXiv preprint arXiv:2303.07798, 2023. 1, [33] Karmesh Yadav, Ram Ramrakhya, Arjun Majumdar, Vincent-Pierre Berges, Sachit Kuhar, Dhruv Batra, Alexei Baevski, and Oleksandr Maksymets. Offline visual representation learning for embodied navigation. In ICLRW, 2023. 2, 6, 7 [34] Hang Yin, Xiuwei Xu, Zhenyu Wu, Jie Zhou, and Jiwen Lu. Sg-nav: Online 3d scene graph prompting for llm-based zero-shot object navigation. In NeurIPS, 2024. 1 [35] Hang Yin, Xiuwei Xu, Linqing Zhao, Ziwei Wang, Jie Zhou, and Jiwen Lu. Unigoal: Towards universal zero-shot goalIn CVPR, pages 1905719066, 2025. oriented navigation. 1 [36] Tianwei Yin, Xingyi Zhou, and Philipp Krahenbuhl. CenterIn CVPR, pages based 3d object detection and tracking. 1178411793, 2021. 5 [37] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep In CVPR, pages 586595, features as perceptual metric. 2018. [38] Yuhang Zheng, Xiangyu Chen, Yupeng Zheng, Songen Gu, Runyi Yang, Bu Jin, Pengfei Li, Chengliang Zhong, Zengmao Wang, Lina Liu, et al. Gaussiangrasper: 3d language gaussian splatting for open-vocabulary robotic grasping. RAL, 2024. 2 [39] Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph Lim, Abhinav Gupta, Li Fei-Fei, and Ali Farhadi. Target-driven visual navigation in indoor scenes using deep reinforcement learning. In ICRA, pages 33573364. IEEE, 2017. 1,"
        }
    ],
    "affiliations": [
        "Nanyang Technological University",
        "Tsinghua University"
    ]
}