{
    "paper_title": "CriticLean: Critic-Guided Reinforcement Learning for Mathematical Formalization",
    "authors": [
        "Zhongyuan Peng",
        "Yifan Yao",
        "Kaijing Ma",
        "Shuyue Guo",
        "Yizhe Li",
        "Yichi Zhang",
        "Chenchen Zhang",
        "Yifan Zhang",
        "Zhouliang Yu",
        "Luming Li",
        "Minghao Liu",
        "Yihang Xia",
        "Jiawei Shen",
        "Yuchen Wu",
        "Yixin Cao",
        "Zhaoxiang Zhang",
        "Wenhao Huang",
        "Jiaheng Liu",
        "Ge Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Translating natural language mathematical statements into formal, executable code is a fundamental challenge in automated theorem proving. While prior work has focused on generation and compilation success, little attention has been paid to the critic phase-the evaluation of whether generated formalizations truly capture the semantic intent of the original problem. In this paper, we introduce CriticLean, a novel critic-guided reinforcement learning framework that elevates the role of the critic from a passive validator to an active learning component. Specifically, first, we propose the CriticLeanGPT, trained via supervised fine-tuning and reinforcement learning, to rigorously assess the semantic fidelity of Lean 4 formalizations. Then, we introduce CriticLeanBench, a benchmark designed to measure models' ability to distinguish semantically correct from incorrect formalizations, and demonstrate that our trained CriticLeanGPT models can significantly outperform strong open- and closed-source baselines. Building on the CriticLean framework, we construct FineLeanCorpus, a dataset comprising over 285K problems that exhibits rich domain diversity, broad difficulty coverage, and high correctness based on human evaluation. Overall, our findings highlight that optimizing the critic phase is essential for producing reliable formalizations, and we hope our CriticLean will provide valuable insights for future advances in formal mathematical reasoning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 1 8 1 6 0 . 7 0 5 2 : r CriticLean: Critic-Guided Reinforcement Learning for Mathematical Formalization"
        },
        {
            "title": "Nanjing University",
            "content": "Full author list in Contributions"
        },
        {
            "title": "Abstract",
            "content": "Translating natural language mathematical statements into formal, executable code is fundamental challenge in automated theorem proving. While prior work has focused on generation and compilation success, little attention has been paid to the critic phasethe evaluation of whether generated formalizations truly capture the semantic intent of the original problem. In this paper, we introduce CriticLean, novel critic-guided reinforcement learning framework that elevates the role of the critic from passive validator to an active learning component. Specifically, first, we propose the CriticLeanGPT, trained via supervised fine-tuning and reinforcement learning, to rigorously assess the semantic fidelity of Lean 4 formalizations. Then, we introduce CriticLeanBench, benchmark designed to measure models ability to distinguish semantically correct from incorrect formalizations, and demonstrate that our trained CriticLeanGPT models can significantly outperform strong openand closed-source baselines. Building on the CriticLean framework, we construct FineLeanCorpus, dataset comprising over 285K problems that exhibits rich domain diversity, broad difficulty coverage, and high correctness based on human evaluation. Overall, our findings highlight that optimizing the critic phase is essential for producing reliable formalizations and we hope our CriticLean will provide valuable insights for future advances in formal mathematical reasoning. Date: July 8, 2025 Correspondence: liujiaheng@nju.edu.cn, zhangge.eli@bytedance.com Project Page: https://github.com/multimodal-art-projection/CriticLean"
        },
        {
            "title": "Introduction",
            "content": "The formalization of mathematical statements [64] is critical task in modern mathematical computation, particularly in the context of theorem provers like Lean 4 [22]. The translation of natural language mathematical problems into formal, executable code remains significant challenge, as it requires not only syntactical accuracy but also deep understanding of the problems semantics [44, 49]. Existing approaches have shown progress, but they often face limitations in accuracy, especially in the context of complex, high-level problems that involve sophisticated mathematical reasoning [3, 23, 58, 59, 75]. In contrast to existing works, we argue that the critic phasethe step where the semantic correctness of generated formalizations is evaluatedis not only underexplored but also fundamentally essential to the success of mathematical autoformalization. Therefore, in this paper, we systematically investigate and optimize the critic component and introduce CriticLean, comprehensive framework that places the critic model at the center of the formalization pipeline. Unlike prior work that primarily focuses on generation quality or compiler validity, CriticLean introduces the reinforcement learning-based CriticLeanGPT models explicitly trained to evaluate whether the Lean 4 output truly reflects the intent of the original mathematical statement., and we present full methodology for 1 Figure 1 Autoformalization. Illustration of CriticLean framework based on Critic-Guided Reinforcement Learning for Mathematics training, and evaluating CriticLeanGPT model. Specifically, as shown in Figure 1, for each natural language statement, we apply the autoformalization iteratively based on the feedback from the Lean compiler and the CriticLeanGPT models, which is trained to critically assess whether generated formalization accurately represents the semantics of the original mathematical statement. In each iteration, the feedback provides valuable signals that drive an iterative refinement process, further improving the quality of the final Lean code output. Additionally, we present CriticLeanBench, benchmark designed to evaluate the performance of CriticLeanGPT models, which contain 500 natural language and Lean 4 language statement pairs (i.e., 250 correct and 250 incorrect pairs). Through extensive experiments, we demonstrate that our trained CriticLeanGPT models outperform the SOTA open-source models [4, 10, 11] and many closed-source API models [9, 12, 42] greatly. Furthermore, building upon our critic-centric CriticLean pipeline, we propose the high-quality open-source Lean 4 statement dataset FineLeanCorpus, comprising 285,957 fully verified entries. When compared to the previous related datasets (e.g., LeanWorkBook [65]), FineLeanCorpus is distinguished by its diversity in mathematical domains, difficulty distribution, and strict semantic validation via critical feedback loops. Notably, its difficulty distribution and targeted domain enrichment create more structurally balanced training environment, mitigating overfitting and transforming sparse topics into well-supported sub-domains. Furthermore, to foster research into the upper echelons of mathematical reasoning, we have curated the specialized subset called FineLeanCorpus-Diamond, comprising over 36,000 high-difficulty problems. ."
        },
        {
            "title": "2.1 Autoformalization",
            "content": "Autoformalization [48, 60, 68] refers to the process by which AI systems parse natural language (NL) contents and translate them into machine-verifiable formal representations, such as those in theorem provers like Lean4 [38] or Isabelle [40]. Recent advances leverage large language models (LLMs) [72] to tackle this problem through two primary paradigms: (1) In-context learning [57], where models utilize annotated examples [28, 33, 60] to generate formalizations without explicit fine-tuning, (2) Supervised fine-tuning (e.g., [26, 61, 67]), which adapts general-purpose LLMs into domain-specific autoformalization experts. To assess correctness, prior works [26, 61, 67] employ LLM-as-judge [76] to verify semantic alignment between formal and informal statements. We advance this by training the first open-sourced, domain-specific light LLM on top of Qwen [52] family for critiquing Lean4 statement alignment via reinforcement learning [46], enhancing 2 both critique capability and formalization robustness."
        },
        {
            "title": "2.2 RL for LLM Reasoning",
            "content": "The exploration of complex reasoning capabilities in Large Language Models (LLMs) has achieved significant advancements [14, 17, 2931, 55], with Reinforcement Learning (RL) establishing itself as critical paradigm for transcending the constraints inherent to Supervised Fine-Tuning (SFT) [29, 32, 56, 66, 70]. Notable methodologies, including GRPO [11, 46], DAPO [66] have demonstrated substantial gains in mathematical reasoning and intricate problem-solving domains. However, the underlying mechanisms by which RL enhances reasoning capabilities remain insufficiently characterized. Empirical analyses increasingly indicate that RL primarily functions to activate, refine, or optimize the sampling of latent reasoning competencies rather than generating entirely novel cognitive frameworks de novo. For example, Yue et al. [69] interrogate whether current reinforcement learning frameworks incorporating verifiable rewards (RLVR) genuinely expand the frontier of reasoning performance or merely enhance the efficiency of retrieving pre-existing solutions."
        },
        {
            "title": "3.1 CriticLeanBench",
            "content": "3.1.1 Overview of CriticLeanBench CriticLeanBench aims to evaluate the critical reasoning of LLMs in key aspects such as translating natural language mathematical statements into formally verified theorem declarations in Lean 4, including critique and correction. By integrating these core dimensions, CriticLeanBench can comprehensively measure the performance of models in Formalization tasks. In this section, we will elaborate on the construction principles and processes of CriticLeanBench; the construction process is shown in Figure 2. CriticLeanBench is constructed following the following principles: (1) It covers various types of errors, aiming to thoroughly evaluate the comprehensive capabilities of LLMs in capturing the semantic intent of formalized statements, using Template for systematic assessment; (2) It incorporates diverse data sources to enhance the diversity and representativeness of evaluation data; and (3) It ensures the reliability and validity of the evaluation benchmark through combination of expert review and automated verification. 3.1.2 Automatically verified This section outlines the methodology employed to apply the Automatic Validation Filter , guided by predefined criteria for compiler and model-based verification. Data Collection We selected Math Statements from data sources [1, 16, 24, 36, 41, 43, 66], such as Omni-MATH [8], AIME [75], U-MATH [5], DEMI-MathAnalysis [6], HARDMath [7], OlympiadBench [13], and BlueMO [73], along with their corresponding Lean 4 Statements from public dataset [67]. Compiler Verified We submitted the Lean 4 statements from the above dataset to the Lean 4 compiler. If the compilation succeeded, the results were passed to the DeepSeek R1 [11] model for further processing. Compile false: For statements that failed to compile, we randomly sampled 50 entries and retained the compiler feedback messages, which were included as part of our CriticLeanBench benchmark. LLM Verified For the data that has passed compilation, we utilize template to process each samples Math Statement and its corresponding Lean 4 Code Statement through the DeepSeek R1 large language model. The model is tasked with determining whether the Lean 4 Code Statement is consistent with the Math Statement, producing for each sample tag indicating consistency or inconsistency along with reasoning statement. To extract structured outputs from the models responses, we employ regular expression-based pattern matching. The results are then submitted to human reviewers for further validation. 3 Figure 2 An overview for the CriticLeanBench construction. 3.1.3 Human Validation Filter This section outlines the methodology employed to apply the Human Validation Filter, guided by predefined criteria(detailed in Appendix C). We categorize the data into two groups based on whether the Lean-compiled autoformalization output semantically aligns with the refined statement. Therefore, Lean-compiled autoformalization falls into the following: Human Check right: If the autoformalized statement both compiles and accurately captures the mathematical meaning, logic, and intended behavior of the original problem, it satisfies the semantic consistency criteria. Human Check false: If the autoformalized statement compiles but fails to accurately represent the original mathematical problems semantics. It violates one or more of the semantic consistency criteria, despite being syntactically valid Lean code. This often happens when theres mismatch between the codes logic and the intended mathematical meaning. Within the two categories, we select representative samples that mirror the characteristics of the original autoformalization statements while maximizing diversity within each subset and capturing the full spectrum of observed error types in the negative samples. This is achieved by balancing several factors: Stratified Representation: The selected samples should maintain similar distribution across different strata of the original dataset. These strata are defined by: Problem Complexity: This encompasses factors like the number of variables, quantifiers, logical connectives, and the depth of nested mathematical structures. Mathematical Branches: The samples should represent the various mathematical domains present in the original data. Statement Well-Formedness: This refers to the degree to which the original mathematical statements 4 adhere to standard mathematical notation and conventions. This stratification ensures that the subsets reflect the variability in the quality of the original problem statements. Comprehensive Error Coverage: The negative samples are specifically chosen to exemplify the full range of typical errors observed in the autoformalization process. This range includes fundamental semantic and logical issues, such as Premise Translation Errors (e.g., incorrect domains or missing conditions), Mathematical Representation Errors (e.g., faulty expressions or definitions), and Goal Translation Errors. The set also covers issues such as Incorrect Assumptions, logical flaws like Operator & Parenthesis Errors (e.g., misplaced quantifiers), and high-level structural problems like Incomplete Formalization, where crucial context is omitted(detailed in figure9)."
        },
        {
            "title": "3.1.4 Data Statistics",
            "content": "Statistics #Problems Correct Pairs - human check Incorrect Pairs - human/compiler check Number 500 250 250 250 200/50 Question Tokens Length - max/min/avg 1, 583/495/700.94 Benchmark CriticBench CodeCriticBench LLaVA-Critic CriticLeanBench (ours) Critic Lean Test Table 1 Dataset statistics and comparison of various code benchmark datasets. Table 1 (left) presents the dataset statistics for CriticLeanBench. The dataset contains total of 500 problems, comprising 250 correct and 250 incorrect Q/GT (Question and GT label) pairs distributed across the collection. The questions exhibit notable variation in length, with maximum token count of 1,583 and minimum of 495 tokens, calculated using the Qwen2.5 [4] tokenizer. On average, each question contains approximately 700.94 tokens. This characteristic distinguishes CriticLeanBench as complex evaluation benchmark that requires models to process relatively lengthy inputs compared to those in standard datasets. 3.1.5 Comparison to Other Benchmarks In Table 1 (right), CriticLeanBench has the following features: (1) We focus on modest data size of 500 samples, acknowledging the expensive manual annotation costs (ranging from tens to hundreds of dollars per instance) and prioritizing efficiency without compromising evaluation rigor; (2) We integrate both Critic and Lean functionalities, distinguishing ourselves from benchmarks like CriticBench [27] and CodeCriticBench [71] that lack Lean capabilities, and LLaVA-Critic [63] that omits both Lean capabilities and test evaluation components; (3) We incorporate test component focused on translating mathematical statements into formally verified theorem declarations in Lean 4, offering fine-grained evaluation framework to assess the correctness of model transformations. This aspect of evaluative completeness remains unmatched by existing datasets."
        },
        {
            "title": "4 CriticLeanInstruct",
            "content": "To enhance the CriticLeanGPT models efficacy in critically evaluating the successful transformation of mathematical statements into Lean code, we constructs comprehensive training dataset comprising 48,000 samples. This dataset consists of three high-quality components, designed to comprehensively improve the models critical thinking and reasoning capabilities. To significantly broaden the CriticLeanGPT models knowledge coverage, we also integrates three times additional code and math [18] datasets, enabling it to more comprehensively understand mathematical concepts and Lean code structures. 5 Specifically, the Seed Data, with 1:3 mix of math and code, is termed CriticLeanInstruct(16K). When combined with data augmentation while retaining the same ratio, it forms the full CriticLeanInstruct dataset."
        },
        {
            "title": "4.1 Seed Data",
            "content": "The seed data comprises 4,000 samples evenly split into 2,000 correct and 2,000 incorrect instances. For both correct and incorrect samples, human experts provided critical feedback F. Additionally, the incorrect samples include compiler error messages generated by the Lean 4 Compiler as supplementary feedback. Then, we adopt the Gemini2.5-Pro [9] to extend the critical feedback to detailed Chain-of-Thought (CoT) explanations. Figure 3 Distribution of Different Error Types. To understand the primary failure modes, we analyzed the error distribution across the 2,000 incorrect samples in our seed data as illustrated in Figure 3. The analysis reveals two primary challenges:(1) Syntactic Barriers: The most frequent obstacles are syntactic, such as Type Error and Syntax Errors. These issues typically prevent the code from compiling, indicating fundamental difficulty in mastering the formal language.(2) Semantic Gaps: Beyond syntax, these errors stem from failure to interpret the natural language scenario and accurately model its key information. This is evident in high rates of errors when translating problems core logical componentsincluding its premises, goal, and mathematical representation. This semantic challenge is particularly acute in application-style \"word problems,\" where difficulty comprehending complex contexts leads to fundamentally flawed formalization. Drawing from the error taxonomy (detailed in Appendix D) and building upon our initial annotation standards (detailed in Appendix C), we established fine-grained checklist, which is provided in full in Appendix E. This checklist formalizes the observed error paradigms, providing the foundational framework for methodically constructing the negative samples used throughout our study."
        },
        {
            "title": "4.2 Data Augmentation",
            "content": "4.2.1 Correct Samples We selected 5560 correct mathematical statements and Lean code pairs from the FormalMATH [67] dataset and used the Gemini-2.5-Pro model to generate Critical Chain-of-Thought for initial assessment. To ensure data quality, we kept these samples where the models judgment is correct."
        },
        {
            "title": "4.2.2 Incorrect Samples",
            "content": "We provide two strategies to obtain the incorrect samples as follows: Based on OmniMath, we adopt the Kimina-Autoformalizer-7B [54] to generate the Lean 4 code statements, and we kept 2,000 Lean code snippets that failed to compile due to syntactic or logical issues during automated formalization processes. For each detected error, detailed Chain-of-Thought (CoT) explanation was generated to elucidate the errors cause, enabling the model to recognize common compilation error patterns and thereby enhance its understanding of Lean 4 code syntax. Based on correct mathematical statements and Lean 4 code from the FormalMATH [67] dataset, we implemented three-step collaborative process to generate negative samples, aiming to enhance the CriticLeanGPT models ability to identify subtle errors and logical flaws. First, checklist of various potential issues, refined by human experts, was established as shown in Appendix E. Second, the Gemini 2.5 Pro model was invoked to randomly select error types from this checklist and modify correct Lean code accordingly, generating incorrect samples(detailed in Appendix H). Then, we adopt the Gemini model to generate the critical Chain-of-Thought explaination."
        },
        {
            "title": "4.3 Training Paradigm\nSupervised fine-tuning (SFT). These models are instruction-tuned versions based on their respective pre-\ntrained Qwen2.5 checkpoints, with a focus on improving their ability to interpret and formalize complex\nmathematical statements expressed in natural language. The CriticLeanInstruct 4 dataset includes Critic\ndata consisting of mathematical statements converted into formally verified theorem declarations in Lean 4,\nalong with three times as much code and mathematics data for SFT (Supervised Fine-Tuning). We used the\nLLaMA-Factory [77] framework to facilitate the fine-tuning process and optimize model performance.",
            "content": "Reinforcement Learning Optimization (RL). The recent success of R1-style methods has demonstrated the effectiveness of online RL using discrete, rule-based rewards [45]. In our pipeline, Qwen2.5 series and Qwen3 [52] series are further refined using reinforcement learning signals derived from both format validation of the generated critic data and consistency checking between model predictions and expert-labeled ground truth (GT) labels . Specifically, the RL training data consists of 4,000 Seed Data 4.1, where each example transforms mathematical statement into corresponding formal proof in Lean 4. Based on this dataset, we apply rule-based RL approach to optimize the models capability in judgment reasoning. More specifically, we mainly utilize the GRPO [46] algorithm within the VeRL [47] reinforcement learning framework, whose optimization objective is: Jonline(πθ; D) = xD,{yi}G i=1πθold (cid:34) 1 (yx) (cid:88) min (cid:18) πθ(yix) πθold (yix) Ai, i=1 (cid:19) , 1 ϵ, 1 + ϵ Ai βDKL(πθπref) (cid:19) (cid:21) clip (cid:18) πθ(yix) πθold (yix) where is group size, and Ai is advantage. The reward function is designed as follows1: raccuracy = rformat = (cid:40) (cid:40) 1, 0, 1, 0, if judgement = label if judgement = label if format is right if format is wrong rfinal = min(raccuracy, rformat) 1We do not include length penalty in rewards to encourage longer thinking. 7 (1) (2) (3) (4)"
        },
        {
            "title": "5.1 Experimental Setup\nBaseline Models. For the closed-sourced API models, we select the following models: Claude35_Sonnet2\n[2], Doubao-1.5-pro-32k [12], Gemini 2.5 Pro [50], GPT-4o-2024-11-20 [42]. We select a group of the most\nadvanced open-source LLMs to serve as critic models for evaluation, which includes various reasoning models\n(DeepSeek-R1 [11], QwQ-32B [53], Qwen3-8B [52], Qwen3-14B [52], Qwen3-32B [52]), DeepSeek-Prover\nmodels [62] (DeepSeek-Prover-V1.5-RL, DeepSeek-Prover-V1.5-SFT), Llama-3.3-70B-Instruct [10] and several\nQwen models (Qwen2.5-Coder-7B-Instruct [19], Qwen2.5-Coder-32B-Instruct [19], Qwen2.5-7B-Instruct [51],\nQwen2.5-14B-Instruct [51] and Qwen2.5-32B-Instruct [51]).",
            "content": "CriticLeanGPT Models. We further evaluate three variants from the Qwen2.5 series and Qwen3 series. These are instruction-based models fine-tuned specifically on either the CriticLeanInstruct 4 dataset or the RL-based clean critic Seed Data 4.1. All open-source models are inferred using the vLLM [21] framework with default inference parameters."
        },
        {
            "title": "5.2 Main Results",
            "content": "5.2.1 Evaluation of Critic Capability As indicated in Table 2, the experimental results clearly demonstrate the effectiveness of the CriticLeanGPT models trained on our CriticLeanInstruct, in converting natural language mathematical statements into Lean 4 formal theorem declarations. Within the CriticLeanBench benchmark, our CriticLeanGPT models trained via supervised fine-tuning (SFT) and reinforcement learning (RL), along with their enhanced variantsoutperform range of closed-source API models, open-source models, and baseline models, highlighting distinct advantages. These outcomes yield several key insights: (1) Reasoning models excel in critical tasks, with Gemini 2.5 Pro, QwQ-32B, Qwen3-32B, and DeepSeek-R1 all attaining scores above 80. When compared to baseline models, our Qwen3-32B-RL model, optimized through RL, achieves strong accuracy level, underscoring the efficacy of both our training methodology and dataset. (3) Our innovative mixed SFT strategy substantially boosts the performance of the Qwen2.5 family, with notable improvements observed across the 7B, 14B, and 32B models. (4) Additionally, SFT and RL significantly strengthen the models capacity to identify erroneous samples, as evidenced by higher true negative rates (TNR) and lower false negative rates (FNR)a critical enhancement for accurate detection of incorrect formalizations, which is indispensable for effective critical tasks."
        },
        {
            "title": "5.3 Ablation Study",
            "content": "5.3.1 Effect of Reasoning Data We conduct an ablation study on CriticLeanBench to evaluate the impact of different training strategies. As shown in Table 3, incorporating code and math reasoning data significantly improves performance across all model sizes compared to using Seed Data 4.1. Specifically, using the CriticLeanInstruct 4, which samples Critic and non-Critic data at ratio of 1:3, leads to substantial gains, demonstrating that integrating diverse reasoning tasks enhances the critical reasoning capabilities of the model. This suggests that multi-task learning with math and code data improves the critique abilities of mathematical formalization. 5.3.2 Effect of SFT Dataset Size Figure 6 highlights notable parameter-dependent relationship between the size of the SFT dataset and key reasoning performance. Across all model scales, performance improvements are observed through SFT, albeit with varying degrees of fluctuation depending on the training set size. Smaller models (e.g., 7B) exhibit more pronounced gains as the dataset expands, whereas larger models (e.g., 32B) demonstrate less consistent trend, with marginal improvements at lower data volumes but substantial gains at higher data volumes. These findings align with prior studies [39, 78], underscoring the interplay between model capacity, data scale, and performance optimization in SFT scenarios. The results emphasize the need for tailored strategies to balance data efficiency and model generalization, particularly for large-scale architectures. 8 Model ACC TPR FPR TNR FNR Reasoning Open-source Gemini-2.5-Pro QwQ-32B Qwen3-32B Qwen3-235B-A22B DeepSeek-R1 Qwen3-14B Qwen3-8B Doubao-1.5-pro-32k Claude35-Sonnet Qwen2.5-32B-Instruct Qwen2.5-Coder-32B-Instruct Llama-3.3-70B-Instruct GPT-4o-2024-11-20 Qwen2.5-14B-Instruct Qwen2.5-Coder-7B-Instruct Qwen2.5-7B-Instruct DeepSeek-Prover-V1.5-SFT DeepSeek-Prover-V1.5-RL Qwen3-8B-RL Qwen3-14B-RL Qwen3-32B-RL Qwen2.5-7B-Instruct-RL Qwen2.5-14B-Instruct-RL Qwen2.5-32B-Instruct-RL Qwen2.5-7B-Instruct-SFT Qwen2.5-14B-Instruct-SFT Qwen2.5-32B-Instruct-SFT Qwen2.5-7B-Instruct-SFT-RL Qwen2.5-14B-Instruct-SFT-RL Qwen2.5-32B-Instruct-SFT-RL SOTA LLMs 95.6 93.6 96.0 90.4 90.8 92.4 94.4 95.2 97.2 91.6 91.6 95.2 95.6 80.4 88.4 89.6 78.8 76.4 4.4 6.4 4.0 9.6 9.2 7.6 5.6 4.8 2.8 8.4 8.4 4.8 4.4 19.6 11.6 10.4 21.2 23. 82.8 79.2 75.2 79.2 77.2 74.8 65.2 61.6 51.2 54.4 51.6 41.2 40.0 52.8 42.4 32.0 26.0 23.6 CriticLeanGPT (Ours) 90.0 91.6 88.4 85.6 85.2 60.4 94.4 83.6 85.2 90.4 81.6 88.0 10.0 8.4 11.6 14.4 14.8 39.6 5.6 16.4 14.8 9.6 18.4 12.0 72.0 78.0 85.6 51.6 53. 83.6 45.2 57.6 67.2 46.0 67.6 69.2 89.2 86.4 85.6 84.8 84.0 83.6 79.8 78.4 74.2 73.0 71.6 68.2 67.8 66.6 65.4 60.8 52.4 50.0 79.8 84.8 87.0 68.6 69.4 72.0 69.8 70.6 76.2 68.2 74.6 78.6 17.2 20.8 24.8 20.8 22.8 25.2 34.8 38.4 48.8 45.6 48.4 58.8 60.0 47.2 57.6 68.0 74.0 76.4 28.0 22. 14.4 48.4 46.4 16.4 54.8 42.4 32.8 54.0 32.4 30.8 Table 2 Performance on CriticLeanBench. The best, the second-best and the third-best scores for each indicator are shown in box , bold and underlined, respectively. 9 Model ACC TPR FPR TNR FNR Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct-SFT(Critic Only) Qwen2.5-7B-Instruct-SFT 7B Size Models 60.8 64.0 69.8 Qwen2.5-14B-Instruct Qwen2.5-14B-Instruct-SFT(Critic Only) Qwen2.5-14B-Instruct-SFT 14B Size Models 66.6 67.4 70.6 32B Size Models Qwen2.5-32B-Instruct Qwen2.5-32B-Instruct-SFT(Critic Only) Qwen2.5-32B-Instruct-SFT 73.0 71.0 76. 89.6 70.8 94.4 80.4 80.8 83.6 91.6 72.0 85.2 10.4 29. 5.6 19.6 19.2 16.4 8.4 28.0 14.8 32.0 57.2 45. 52.8 54.0 57.6 54.4 70.0 67.2 68.0 42.8 54. 47.2 46.0 42.4 45.6 30.0 32.8 Table 3 Comparison of model performance under different training strategies: base model, SFT on Critic data only, and SFT on combined Critic, code, and math data. The best score for each indicator is shown in box ."
        },
        {
            "title": "5.4 Analysis",
            "content": "5.4.1 Scaling Analysis We evaluate the performance of Qwen series models on CriticLeanBench across different model scales, including Qwen2.5-Coder, Qwen2.5-Instruct, Qwen2.5-Instruct-SFT, Qwen2.5-Instruct-SFT-RL, Qwen3, and Qwen3RL. The results in Figure 4 show that the performance improves consistently as the model size increases, demonstrating clear scaling law of LLMs on CriticLeanBench. 5.4.2 Effect of Pass@k The Pass@k metric serves as means to identify the highest-quality answers from multi-modal large language models (MLLMs) for final responses, highlighting the models potential for enhancement through post-training techniques like Reinforcement Learning from Human Feedback (RLHF [25]) and Gradient Penalty Policy Optimization (GRPO [46]). In this study, Pass@k evaluations were conducted on Qwen2.5-7B-Instruct, Qwen2.57B-Instruct-SFT-RL, Qwen3-8B, and Qwen3-8BRL using CriticLeanBench, with set to 8 and 32, and evaluation metrics including Accuracy (ACC), True Positive Rate (TPR), and True Negative Rate (TNR). Figure 6 Comparison of model performance under different amounts of SFT data: base model, CriticLeanInstruct(16K), and CriticLeanInstruct. As shown in Figure 5, we observe that models subjected to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) achieve better overall performance. Considering Pass@32, the Accuracy of Qwen2.5-7B-Instruct is outperformed by that of Qwen2.5-7B-Instruct-SFT-RL, which has undergone SFT and RL processing, leading to notable improvement in the overall correctness rate. Regarding the True Negative Rate, models without SFT and RL processing are more prone to misclassifying incorrect samples as correct, whereas processed models effectively mitigate such errors; this is evident in the higher TNR of Qwen2.5-7B-Instruct-SFT-RL compared to Qwen2.5-7B-Instruct at Pass@32, reducing the likelihood of 10 Figure 4 Scaling Analysis of LLMs on CriticLeanBench. ˆ denoted closed-source LLMs. such misclassifications. similar trend is observed in Qwen3-8B. Additionally, all models exhibit improved performance as increases, suggesting that SFTand RL-optimized models can more effectively select high-quality answers when provided with more candidate responses."
        },
        {
            "title": "6 FineLeanCorpus",
            "content": "To construct the FineLeanCorpus, we began by aggregating vast and diverse collection of natural language mathematical problems. Sourcing from wide array of materials, including high school olympiad datasets (e.g., AoPS, BlueMO), standard high school curricula (e.g., TAL-SCQ5), and undergraduatelevel challenges (e.g., Omni-MATH), we ensured an extensive initial distribution in both the mathematical domain and difficulty. The first step in our process was to standardize this heterogeneous collection into uniform, proof-based format, making each problem compatible with the Lean 4 theorem prover and ready for the subsequent formalization pipeline. Statistics #Problems Length Statement Number 285957 maximum length minimum length avg length 2980 tokens 9 tokens 78.4 tokens Lean Result(success) maximum length minimum length avg length 768 tokens 14 tokens 87.8 tokens These standardized problems were then subjected to rigorous, gated auto-formalization process powered by our CriticLean framework (Figure 1).The Kimina-Autoformalizer-7B model first generates candidate formal statement. This statement must pass syntactic check via the Lean 4 compiler; failure leads to regeneration. successful compilation is followed by semantic correctness check from our CriticLeanGPT model, with rejection also triggering new attempt. This regenerative approach is critical, maximizing the yield from our source corpus by iteratively seeking valid formalization. Finally, to further enhance precision, we apply final filtering stage using another, Table 5 Dataset statistics of FineLeanCorpus. Figure 5 Performance on CriticLeanBench using Pass@k metrics, where = 8 (left) and = 32 (right). Dataset AOPs [35] DeepMath-103k [15] NuminaMath-TIR [24] DeepTheorem [74] DeepScaleR [34] DAPO-Math-17k [66] Omni-MATH [8] IneqMath [20] BlueMO [73] TAL-SCQ5K [37] OnlineMathContest Multi-Source Math Competition High School Olympiad Difficulty Level High School Olympiad Diverse High School High School Olympiad High School Olympiad High School Undergraduate High School Olympiad High School Olympiad High School High School Size 199982 28538 22847 15131 11544 4078 1176 963 616 393 70 619 Table 4 Overview of different sources for FineLeanCorpus. higher-performance CriticLeanGPT model. Manual validation indicates that this step is expected to eliminate 74.7% of the remaining incorrect formalizations. The resulting corpus is characterized by its expressive range: natural language statements vary from concise 9 tokens to complex 2,980 tokens (avg. 78.4), while their corresponding Lean formalizations span from 14 to 768 tokens (avg. 87.8), reflecting the deep spectrum of complexity successfully captured by our pipeline. To further analyze the differences between our FineLeanCorpus dataset and the Lean-Workbook, we employed the templates from Appendix and Appendix to assess the difficulty levels and classify the mathematical domains of the datasets using Doubao-1.5-pro [12]. comparative analysis of our proposed FineLeanCorpus against Lean-Workbook (Figure 7, Figure 8, Table 4 and Table 5) reveals two fundamental advancements: (1) Scale and Coverage: Our corpus provides significant quantitative expansion in both problem difficulty and domain coverage. This expansion is evident not only across nearly the entire difficulty spectrumoffering much richer data pool for training foundational skillsbut also in the substantial augmentation of high-volume domains like Intermediate Algebra and Elementary Number Theory. (2) FineLeanCorpus exhibits more diverse and structurally balanced profile, achieved by collecting natural language statements from numerous sources. From difficulty perspective, it features multimodal distribution with substantial problem counts at several distinct complexity points, in stark contrast to the unimodal distribution of Lean-Workbook. This characteristic is crucial for mitigating model overfitting to narrow complexity band. From domain perspective, it substantially reinforces previously underrepresented areas. For instance, categories such as Analytic Geometry and Integral Calculus are significantly expanded, while niche topics like Algorithms and Graph Theory are also robustly augmented. This targeted enrichment transforms sparsely sampled topics into well-supported, learnable sub-domains, yielding more comprehensive dataset designed to foster 12 holistic reasoning capabilities. More details regarding our FineLeanCorpus dataset, including its fine-grained mathematical domain distribution (see Appendix A), are provided for further analysis. Furthermore, to push the boundaries of current models and foster research into the upper echelons of mathematical reasoning, we have curated specialized training subset from FineLeanCorpus, which we designate as Diamond. This subset comprises 36,033 problems with difficulty rating exceeding 5. The purpose of this high-difficulty training set is to create demanding training environment that fosters the development of the sophisticated, multi-step reasoning required to tackle the most formidable mathematical problems.A detailed breakdown of the mathematical domain distribution within this subset is provided in table 11. Dataset Source Theorems Level Detailed Critic Process Lean-Workbook [65] Synthetic 140K Undergraduate Opaque FineLeanCorpus (ours) Synthetic 285K Diverse Transparent Difficulty Profile Avg: 3.70 Top-tier (6): 7.81% Avg: 3.24 Top-tier (6): 11.10% Topic Diversity Highly Skewed Balanced & Diverse Table 6 Comparison of dataset statistics. FineLeanCorpus offers transparent critic process, higher proportion of top-tier problems, and more balanced and diverse topic distribution compared to the highly skewed Lean-Workbook. Figure 7 Comparison of dataset statistics. FineLeanCorpus offers transparent critic process, higher proportion of top-tier problems, and more balanced and diverse topic distribution compared to the highly skewed Lean-Workbook."
        },
        {
            "title": "6.1 Analysis on CriticLean Pipeline",
            "content": "As shown in Table 7, our autoformalization pipeline significantly improves accuracy. We selected 50 problems from the Omni-MATH and applied the following three formalization strategies, with the correctness of all outputs confirmed by manual human inspection. Model Kimina-Autoformalizer-7B Kimina-Autoformalizer-7B (Compiler) Kimina-Autoformalizer-7B (CriticLean) AVG 38.0 54.0 84. Our baseline model, Kimina-Autoformalizer-7B, is used in each strategy: (1) Single Pass baseline (38.0% accuracy); (2) Compiler Feedback loop, where the model regenerates formalizations until they successfully compile (54.0% accuracy); (3) our CriticLean Pipeline, which extends this process, regenerating formalizations until they both compile successfully and are validated by our integrated CriticLeanGPT Model (84.0% accuracy). While compiler feedback resolves syntactical errors, it fails to detect logical flaws. Our pipeline addresses this gap. The integration of critic model, which performs deeper semantic and logical validation, is directly Table 7 Human evaluation accuracy results for autoformalization performance: The best score is highlighted in box . 13 Figure 8 Math Domain Distributions: FineLeanCorpus vs. Lean-Workbook. # Attempt Count / Ratio 5 10 50 100 200 63 / 12.6% 137 / 27.4% 170 / 34.0% 229 / 45.8% 245 / 49.0% 264 / 52.8% Table 8 Effectiveness of the Multi-Attempt Strategy on Formalization Yield. The table shows the cumulative number of successfully formalized problems retained by the critic model as the attempt limit increases. Statistics are from 500-problem sample. Dataset NuminaMath-TIR [24] DeepMath-103k [16] AOPs [35] DeepTheorem [74] Accuracy Dataset Accuracy Dataset 78% 84% 73% 100% IneqMath [20] DAPO-Math-17k [66] TAL-SCQ5K [37] DeepScaleR [34] 96% 69% 75% 100% BlueMO [73] OnlineMathContest Omni-MATH [8] Accuracy 86% 88% 84% Table 9 Human evaluation results of different sources. responsible for the accuracy increase from 54.0% to 84.0%. By filtering out plausible but incorrect formalizations, our method provides more robust path toward reliable autoformalization. These manually-verified results present strong empirical evidence for the efficacy of our approach. Table 8 shows our pipeline achieved 52.8% success rate across 500 problems, where success required passing both syntactic validation and our critic models semantic check. The value of our multi-attempt strategy is evident: while only 12.6% of samples passed on the first try, the pipeline successfully recovered an additional 40.2% that would be discarded by single-pass systems. Conversely, the 47.2% failure rate within the 200attempt limit highlights fundamental bottleneck: the pipelines performance is ultimately constrained by the base auto-formalization models ability to produce candidate our critic can approve. Moreover, as shown in Table 9, we also provide the human evaluation results of different sources. We observe that the accuracy varies across different sources, discrepancy we attribute primarily to the differing domain and difficulty distributions of the respective datasets. 2 2Our human evaluation standard is particularly stringent. To calibrate our criteria, we inspected random sample of"
        },
        {
            "title": "7 Conclusion",
            "content": "This paper presents CriticLean, comprehensive framework that positions the critic as central component in the autoformalization of mathematical statements. Through the development of CriticLeanGPT and the construction of CriticLeanBench, we demonstrate that explicitly modeling and training the critic yields significant improvements in formalization quality. Our pipeline not only refines the translation process through semantic validation, but also enables the construction of FineLeanCorpus, which is validated by both compiler and critic. entries from the Lean-Workbook, which yielded an accuracy of 84%."
        },
        {
            "title": "8 Contributions and Acknowledgments",
            "content": "Co-First Authors Zhongyuan Peng, Yifan Yao, Kaijing Ma Co-Authors Shuyue Guo, Yizhe Li, Yichi Zhang, Chenchen Zhang, Yifan Zhang, Zhouliang Yu, Luming Li, Minghao Liu, Yihang Xia, Jiawei Shen, Yuchen Wu, Yixin Cao, Zhaoxiang Zhang, Wenhao Huang"
        },
        {
            "title": "Corresponding Authors",
            "content": "Jiaheng Liu, Ge Zhang"
        },
        {
            "title": "References",
            "content": "[1] AI Research Group of TAL Education Group. K-12 handwritten mathematical expressions dataset (hme100k). https://ai.100tal.com/dataset. Accessed: 2025-04-05. [2] Anthropic. Claude 3.5 sonnet model card addendum, 2024. URL https://www.paperswithcode.com/ paper/claude-3-5-sonnet-model-card-addendum. Accessed: 2024-09-21. [3] Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward Ayers, Dragomir Radev, and Jeremy Avigad. Proofnet: Autoformalizing and formally proving undergraduate-level mathematics. arXiv preprint arXiv:2302.12433, 2023. [4] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [5] Konstantin Chernyshev, Vitaliy Polshkov, Ekaterina Artemova, Alex Myasnikov, Vlad Stepanov, Alexei Miasnikov, and Sergei Tilga. U-math: university-level benchmark for evaluating mathematical skills in llms. arXiv preprint arXiv:2412.03205, 2024. [6] B.P. Demidovich. Problems in Mathematical Analysis. Edited by B. Demidovich. Translated From the Russian by G. Yankovsky. Russian Monographs and Texts on Advanced Mathematics and Physics. Mir Publishers, 1964. URL https://books.google.com/books?id=XdmpwgEACAAJ. [7] Fan, Martinson, EY Wang, Hausknecht, Brenner, Liu, Peng, Wang, and MP Brenner. Hardmath: benchmark dataset for challenging problems in applied mathematics. arxiv 2024. arXiv preprint arXiv:2410.09988. [8] Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, et al. Omni-math: universal olympiad level mathematic benchmark for large language models. arXiv preprint arXiv:2410.07985, 2024. [9] Google. Gemini: family of highly capable multimodal models, 2023. [10] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [11] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [12] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. [13] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems, 2024. [14] Yancheng He, Shilong Li, Jiaheng Liu, Weixun Wang, Xingyuan Bu, Ge Zhang, Zhongyuan Peng, Zhaoxiang Zhang, Zhicheng Zheng, Wenbo Su, and Bo Zheng. Can large language models detect errors in long chain-of-thought reasoning?, 2025. URL https://arxiv.org/abs/2502.19361. [15] Zhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian Yu, Zhenwen Liang, Wenxuan Wang, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Deepmath-103k: large-scale, challenging, decontaminated, and verifiable mathematical dataset for advancing reasoning. 2025. URL https://arxiv.org/abs/2504.11456. [16] Zhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian Yu, Zhenwen Liang, Wenxuan Wang, et al. Deepmath-103k: large-scale, challenging, decontaminated, and verifiable mathematical dataset for advancing reasoning. arXiv preprint arXiv:2504.11456, 2025. 17 [17] Hui Huang, Yancheng He, Hongli Zhou, Rui Zhang, Wei Liu, Weixun Wang, Wenbo Su, Bo Zheng, and Jiaheng Liu. Think-j: Learning to think for generative llm-as-a-judge. ArXiv, abs/2505.14268, 2025. URL https://api.semanticscholar.org/CorpusID:278769843. [18] Hugging Face. Open r1: fully open reproduction of deepseek-r1, January 2025. URL https://github.com/ huggingface/open-r1. [19] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. [20] Sheng Jiayi, Lyu Luna, Jin Jikai, Xia Tony, Gu Alex, Zou James, and Lu Pan. Solving inequality proofs with large language models. arXiv preprint arXiv:2506.07927, 2025. [21] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [22] Leanprover Community. read-eval-print-loop for Lean 4, 2023. https://github.com/ leanprover-community/repl. [23] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy GurAri, and Vedant Misra. Solving quantitative reasoning problems with language models, 2022. URL https: //arxiv.org/abs/2206.14858. [24] Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. Numinamath tir. [https://huggingface.co/AI-MO/NuminaMath-TIR](https://github. com/project-numina/aimo-progress-prize/blob/main/report/numina_dataset.pdf), 2024. [25] Zihao Li, Zhuoran Yang, and Mengdi Wang. Reinforcement learning with human feedback: Learning dynamic choices via pessimism. arXiv preprint arXiv:2305.18438, 2023. [26] Yong Lin, Shange Tang, Bohan Lyu, Jiayun Wu, Hongzhou Lin, Kaiyu Yang, Jia Li, Mengzhou Xia, Danqi Chen, Sanjeev Arora, and Chi Jin. Goedel-prover: frontier model for open-source automated theorem proving, 2025. URL https://arxiv.org/abs/2502.07640. [27] Zicheng Lin, Zhibin Gou, Tian Liang, Ruilin Luo, Haowei Liu, and Yujiu Yang. Criticbench: Benchmarking llms for critique-correct reasoning. arXiv preprint arXiv:2402.14809, 2024. [28] Chengwu Liu, Jianhao Shen, Huajian Xin, Zhengying Liu, Ye Yuan, Haiming Wang, Wei Ju, Chuanyang Zheng, Yichun Yin, Lin Li, et al. Fimo: challenge formal dataset for automated theorem proving. arXiv preprint arXiv:2309.04295, 2023. [29] Jiaheng Liu, Ken Deng, Congnan Liu, Jian Yang, Shukai Liu, He Zhu, Peng Zhao, Linzheng Chai, Yanan Wu, Ke Jin, et al. M2rc-eval: Massively multilingual repository-level code completion evaluation. arXiv preprint arXiv:2410.21157, 2024. [30] Jiaheng Liu, Chenchen Zhang, Jinyang Guo, Yuanxing Zhang, Haoran Que, Ken Deng, ZhiqiBai, Jie Liu, Ge Zhang, JiakaiWang, Yanan Wu, Congnan Liu, Jiamang Wang, Lin Qu, Wenbo Su, and Bo Zheng. DDK: Distilling domain knowledge for efficient large language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=xgiurUq0ss. [31] Jiaheng Liu, Dawei Zhu, Zhiqi Bai, Yancheng He, Huanxuan Liao, Haoran Que, Zekun Wang, Chenchen Zhang, Ge Zhang, Jiebin Zhang, et al. comprehensive survey on long context language modeling. arXiv preprint arXiv:2503.17407, 2025. [32] Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, and Yi Dong. ProRL: Prolonged reinforcement learning expands reasoning boundaries in large language models. arXiv preprint arXiv:2505.24864, 2025. [33] Jianqiao Lu, Yingjia Wan, Zhengying Liu, Yinya Huang, Jing Xiong, Chengwu Liu, Jianhao Shen, Hui Jin, Jipeng Zhang, Haiming Wang, Zhicheng Yang, Jing Tang, and Zhijiang Guo. Process-driven autoformalization in lean 4, 2024. URL https://arxiv.org/abs/2406.01940. 18 [34] Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Tang, Manan Roongta, Colin Cai, Surpasshttps://pretty-radio-b75.notion.site/ Jeffrey Luo, Tianjun Zhang, Erran Li, Raluca Ada Popa, and Ion Stoica. ing o1-preview with 1.5b model by scaling rl. DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2, 2025. Notion Blog. Deepscaler: [35] Sadegh Mahdavi, Muchen Li, Kaiwen Liu, Christos Thrampoulidis, Leonid Sigal, and Renjie Liao. Leveraging online olympiad-level math problems for llms training and contamination-resistant evaluation, 2025. URL https://arxiv.org/abs/2501.14275. [36] Sadegh Mahdavi, Muchen Li, Kaiwen Liu, Christos Thrampoulidis, Leonid Sigal, and Renjie Liao. Leveraging online olympiad-level math problems for llms training and contamination-resistant evaluation. arXiv preprint arXiv:2501.14275, 2025. [37] Math-eval. TAL-SCQ5K. https://github.com/math-eval/TAL-SCQ5K, 2023. [38] Leonardo de Moura and Sebastian Ullrich. The lean 4 theorem prover and programming language. In Automated Deduction CADE 28: 28th International Conference on Automated Deduction, Virtual Event, July 1215, 2021, Proceedings, page 625635, Berlin, Heidelberg, 2021. Springer-Verlag. ISBN 978-3-030-79875-8. doi: 10.1007/978-3-030-79876-5_37. URL https://doi.org/10.1007/978-3-030-79876-5_37. [39] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, s1: Simple test-time scaling. arXiv preprint Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. arXiv:2501.19393, 2025. [40] Tobias Nipkow, Markus Wenzel, and Lawrence C. Paulson. Isabelle/HOL: proof assistant for higher-order logic. Springer-Verlag, Berlin, Heidelberg, 2002. ISBN 3540433767. [41] Online Math Contest. Online math contest. https://onlinemathcontest.com/. Accessed: 2025-04-05. [42] OpenAI. Gpt-4 technical report. arxiv 2303.08774. View in Article, 2(5), 2023. [43] Miguel Angel Peñaloza Perez, Bruno Lopez Orozco, Jesus Tadeo Cruz Soto, Michelle Bruno Hernandez, Miguel Angel Alvarado Gonzalez, and Sandra Malagon. Ai4math: native spanish benchmark for university-level mathematical reasoning in large language models. arXiv preprint arXiv:2505.18978, 2025. [44] Peter Scholze. Liquid tensor experiment. Experimental Mathematics, 31(2):349354, 2022. [45] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300. [46] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [47] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. [48] Christian Szegedy. promising path towards autoformalization and general artificial In Intelligent Computer Mathematics: 13th International Conference, CICM 2020, Bertinoro, Italy, July 2631, 2020, Proceedings 13, pages 320. Springer, 2020. intelligence. [49] Terence Tao. The polynomial freiman-ruzsa conjecture, 2023. https://github.com/teorth/pf. [50] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [51] Qwen Team. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm.github.io/ blog/qwen2.5/. [52] Qwen Team. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. [53] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL https://qwenlm. github.io/blog/qwq-32b/. 19 [54] Haiming Wang, Mert Unsal, Xiaohan Lin, Mantas Baksys, Junqi Liu, Marco Dos Santos, Flood Sung, Marina Vinyes, Zhenzhe Ying, Zekai Zhu, Jianqiao Lu, Hugues de Saxcé, Bolton Bailey, Chendong Song, Chenjun Xiao, Dehao Zhang, Ebony Zhang, Frederick Pu, Han Zhu, Jiawei Liu, Jonas Bayer, Julien Michel, Longhui Yu, Léo Dreyfus-Schmidt, Lewis Tunstall, Luigi Pagani, Moreira Machado, Pauline Bourigault, Ran Wang, Stanislas Polu, Thibaut Barroyer, Wen-Ding Li, Yazhe Niu, Yann Fleureau, Yangyang Hu, Zhouliang Yu, Zihan Wang, Zhilin Yang, Zhengying Liu, and Jia Li. Kimina-prover preview: Towards large formal reasoning models with reinforcement learning. 2025. URL http://arxiv.org/abs/2504.11354. [55] Weixun Wang, Shaopan Xiong, Gengru Chen, Wei Gao, Sheng Guo, Yancheng He, Ju Huang, Jiaheng Liu, Zhendong Li, Xiaoyang Li, Zichen Liu, Haizhou Zhao, Dakai An, Lunxi Cao, Qi Cao, Wanxi Deng, Feilei Du, Yiliang Gu, Jiahe Li, Xiang Li, Mingjie Liu, Yijia Luo, Zihe Liu, Yadao Wang, Pei Wang, Tianyuan Wu, Yanan Wu, Yuheng Zhao, Shu-Man Zhao, Jin Yang, Si-Yue Yang, Ying Tan, Huimin Yi, Yuchi Xu, Yu-Ming Yuan, Xingyao Zhang, Lin Qu, Wenbo Su, Wei Wang, Jiamang Wang, and Boyuan Zheng. Reinforcement learning optimization for large-scale learning: An efficient and user-friendly scaling library. 2025. [56] Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Liyuan Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, Weizhu Chen, Shuohang Wang, Simon Shaolei Du, and Yelong Shen. Reinforcement learning for reasoning in large language models with one training example, 2025. URL https://arxiv.org/abs/2504. 20571. [57] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [58] Sean Welleck, Jiacheng Liu, Ronan Le Bras, Hannaneh Hajishirzi, Yejin Choi, and Kyunghyun Cho. Naturalproofs: Mathematical theorem proving in natural language, 2021. URL https://arxiv.org/abs/2104.01112. [59] Sean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, and Yejin Choi. Naturalprover: Grounded mathematical proof generation with language models, 2022. URL https://arxiv.org/abs/2205.12910. [60] Yuhuai Wu, Albert Q. Jiang, Wenda Li, Markus N. Rabe, Charles Staats, Mateja Jamnik, and Christian Szegedy. Autoformalization with large language models, 2022. [61] Huajian Xin, Daya Guo, Zhihong Shao, Zhizhou Ren, Qihao Zhu, Bo Liu, Chong Ruan, Wenda Li, and Xiaodan Liang. Deepseek-prover: Advancing theorem proving in llms through large-scale synthetic data. arXiv preprint arXiv:2405.14333, 2024. [62] Huajian Xin, Z. Z. Ren, Junxiao Song, Zhihong Shao, Wanjia Zhao, Haocheng Wang, Bo Liu, Liyue Zhang, Xuan Lu, Qiushi Du, Wenjun Gao, Qihao Zhu, Dejian Yang, Zhibin Gou, Z. F. Wu, Fuli Luo, and Chong Ruan. Deepseek-prover-v1.5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search. 2024. URL https://arxiv.org/abs/2408.08152. [63] Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. Llava-critic: Learning to evaluate multimodal models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1361813628, 2025. [64] Kaiyu Yang, Gabriel Poesia, Jingxuan He, Wenda Li, Kristin Lauter, Swarat Chaudhuri, and Dawn Song. Formal mathematical reasoning: new frontier in ai, 2024. URL https://arxiv.org/abs/2412.16075. [65] Huaiyuan Ying, Zijian Wu, Yihan Geng, Zheng Yuan, Dahua Lin, and Kai Chen. Lean workbook: large-scale lean problem set formalized from natural language math problems, 2025. URL https://arxiv.org/abs/ 2406.03847. [66] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. DAPO: An open-source LLM reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [67] Zhouliang Yu, Ruotian Peng, Keyi Ding, Yizhe Li, Zhongyuan Peng, Minghao Liu, Yifan Zhang, Zheng Yuan, Huajian Xin, Wenhao Huang, Yandong Wen, Ge Zhang, and Weiyang Liu. Formalmath: Benchmarking formal mathematical reasoning of large language models, 2025. URL https://arxiv.org/abs/2505.02735. [68] Zhouliang Yu, Yuhuan Yuan, Tim Xiao, Fuxiang Frank Xia, Jie Fu, Ge Zhang, Ge Lin, and Weiyang Liu. Generating symbolic world models via test-time scaling of large language models. arXiv preprint arXiv:2502.04728, 2025. [69] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in LLMs beyond the base model? arXiv preprint arXiv:2504.13837, 2025. [70] Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, et al. VAPO: Efficient and reliable reinforcement learning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025. [71] Alexander Zhang, Marcus Dong, Jiaheng Liu, Wei Zhang, Yejie Wang, Jian Yang, Ge Zhang, Tianyu Liu, Zhongyuan Peng, Yingshui Tan, et al. Codecriticbench: holistic code critique benchmark for large language models. arXiv preprint arXiv:2502.16614, 2025. [72] Ge Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou Leuang Yu, Danny Pan, Esther Cheng, Jie Liu, Qunshu Lin, et al. Map-neo: Highly capable and transparent bilingual large language model series. arXiv preprint arXiv:2405.19327, 2024. [73] Yifan Zhang, Yifan Luo, and Yizhou Chen. Bluemo: comprehensive collection of challenging mathematical olympiad problems from the little blue book series., 2024. [74] Ziyin Zhang, Jiahao Xu, Zhiwei He, Tian Liang, Qiuzhi Liu, Yansi Li, Linfeng Song, Zhenwen Liang, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Deeptheorem: Advancing llm reasoning for theorem proving through natural language and reinforcement learning, 2025. URL https://arxiv.org/abs/2505. 23754. [75] Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. Minif2f: cross-system benchmark for formal olympiad-level mathematics. arXiv preprint arXiv:2109.00110, 2021. [76] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. [77] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. URL http://arxiv.org/abs/2403.13372. [78] Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36: 5500655021, 2023."
        },
        {
            "title": "Appendix",
            "content": "A Fine-Grained Domain Distribution of FineLeanCorpus The following table illustrates the fine-grained domain distribution of the FineLeanCorpus by presenting its 50 most frequent mathematical topics for illustrative purposes. To enhance readability and highlight the hierarchical structure, entries are sorted alphabetically by Main Category, then Sub-Category, and Topic. Table 10 Fine-Grained Domain Distribution of FineLeanCorpus Main Category Algebra Sub-Category Intermediate Algebra Linear Algebra Topic Diophantine Equations Functional Equations Inequalities Other Polynomials Matrices Other Vector Spaces Applied Mathematics Algorithms Greedy Algorithms Optimization Other Probability Calculus Differential Calculus Integral Calculus Other Linear Programming Other Other Conditional Probability Expected Value Other Applications of Derivatives Derivatives Limits Other Definite Integrals Other Limits Limits of Multivariable Functions Limits of Sequences Other Combinatorics Constructive torics Enumerative torics CombinaInvariants Other CombinaCombinations Extremal Combinatorics Other Permutations Other Pigeonhole Principle Graph Theory Other 22 Count 23 30178 75339 65171 32424 4361 54 2179 46 258 273 363 792 2417 57 14948 787 5391 15165 18761 1086 35 106 8592 54 5579 10460 869 375 1377 42 (Continued) Distribution of Problems by Mathematical Topic Main Category Discrete Mathematics Sub-Category Logic Other Set Theory Geometry Analytic Geometry Euclidean Geometry Number Theory Elementary Number Theory Topic Propositional Logic Other Cardinality Other Conic Sections Other Circles Coordinate Geometry Other Transformations Triangles Diophantine Equations Divisibility Inequalities Modular Arithmetic Other Prime Numbers Count 444 1262 1585 1498 254 1880 3584 4172 404 7243 17208 21160 42 9284 18636 12071 Fine-Grained Domain Distribution of FineLeanCorpus-Diamond The following table illustrates the fine-grained domain distribution of the Diamond dataset, our high-difficulty subset, by presenting its 50 most frequent mathematical topics for illustrative purposes. To enhance readability and highlight the hierarchical structure, entries are sorted alphabetically by Main Category, then Sub-Category, and Topic. Table 11 Fine-Grained Domain Distribution of FineLeanCorpus-Diamond Main Category Algebra Sub-Category Intermediate Algebra Linear Algebra Analysis Applied Mathematics Real Analysis Optimization Other Probability Calculus Differential Calculus Topic Functional Equations Inequalities Other Polynomials Matrices Other Vector Spaces Density of Sets Other Other Conditional Probability Expected Value Other Derivatives Count 6732 12533 3621 2798 955 7 206 3 108 9 23 63 913 23 (Continued) Distribution of Problems by Mathematical Topic Main Category Sub-Category Integral Calculus Combinatorics Other Constructive torics Enumerative torics Topic Differential Equations Limits Other Definite Integrals Other Limits Other CombinaInvariants Other CombinaCombinations Extremal Combinatorics Graph Theory Other Logic Other Set Theory Discrete Mathematics Geometry Analytic Geometry Euclidean Geometry Other Permutations Other Pigeonhole Principle Other Trees Other Propositional Logic Other Cardinality Other Conic Sections Other Circles Coordinate Geometry Other Transformations Triangles Other Other Number Theory Elementary Number Theory Diophantine Equations Divisibility Inequalities Modular Arithmetic Other Prime Numbers 24 Count 3 31 375 2280 2666 44 814 33 620 1165 70 201 442 18 6 21 1260 463 379 29 21 132 45 306 15 1200 2106 3877 18 1593 3523 3699 Figure 9 Distribution of Different Error Types of CriticLeanBench."
        },
        {
            "title": "C Formalization Quality Assessment Criteria",
            "content": "Formalization Quality Assessment Criteria I. Integrity & Accuracy of Mathematical Content Conditions & Hypotheses: Are all explicit premises, variable domains (e.g., N, R, Fin k), index ranges (e.g., a0 vs. a1), properties of specific objects (e.g., geometric shapes, algebraic structures), and implicit context (e.g., non-zero divisors) accurately translated? Are mathematical meanings preserved (e.g., = 0 vs. > 0)? Goals & Conclusions: Are all goals/conclusions translated (including multiple parts/cases)? Is the goal type accurate (e.g., specific value, extremum, existence/uniqueness)? For extrema, is attainability addressed? Is the mathematical meaning accurately translated? II. Clarity & Correctness of Logical Structure Propositional Structure: Are logical connectives (, , , , ) and quantifiers (, , !) used correctly? Are quantifier order, scope, and nesting accurate (e.g., dependencies like ϵ > 0, δ > 0, . . . )? Relation of Conditions to Conclusions: How do multiple premises combine (e.g., (A B) vs. (B C))? Are constraints within the correct scope? Reasoning Path: Does the formalization reflect the original logic and key steps without introducing flaws or altering proof difficulty? 25 III. Lean Conventions & Technical Accuracy Syntax & Declarations: Is the syntax (e.g., parentheses, keywords, type declarations) correct? Are theorem, example, lemma used appropriately? Type System: Do operations, parameters, and return values satisfy type constraints? Are numerals used in the correct type? Are mathematical concepts correctly mapped to Lean counterparts? Definitions & Library Usage: Are custom definitions clear? Are imports correct and non-redundant? Are standard symbols and operations used correctly (e.g., Complex.abs, Nat.Prime)? Code Style & Readability: Are names clear and consistent? Are there sufficient comments for complex parts? Is there any redundancy? IV. Problem Comprehension & Overall Consistency Grasping the Core: Does the formalization capture the core mathematical idea? Internal Self-Consistency: Are there any logical contradictions between the translated parts? Suitability for Formalization: Is the problem suitable for formalization? Are assumptions/interpretations documented? V. Formalization Strategy & Choices Abstraction Level: Is the abstraction level appropriate, avoiding unnecessary generalization or over-specification? Alternative Evaluation: Were alternatives considered? Was decomposition/modularization used for complex problems? VI. Provability & Proof Assistance Proof Complexity: Does the formalization maintain similar proof complexity? Were lemmas added to simplify the proof? Automation Potential: Is the structure amenable to automation tools?"
        },
        {
            "title": "D Error Taxonomy",
            "content": "Error Taxonomy 1. Semantic and Logical Errors 1.1 Premise Translation Error Description: This error occurs when formalizing the given conditions, constraints, or assumptions from the original problem, resulting in discrepancy between the logical premises in the Lean code and the problems description. Examples: Failing to constrain variables to be positive, integers, or coprime as required; not ensuring that denominators in mathematical expressions are non-zero; omitting geometric constraints such as \"A, B, form triangle\" in geometry problems; not explicitly specifying the exact range of variable. For instance, relationships between angles and sides must be precisely defined in Lean, otherwise the resulting correspondence may not be unique. Generated code 1.2 Mathematical Representation Error Description: This error involves an inaccurate representation of the form and meaning of mathematical entities such as variables and expressions from the original problem. This leads to the formalized mathematical proposition in the code being inconsistent with the original problem, thereby undermining formal semantic correspondence. Examples: Formalizing cubic polynomial as quadratic one; mistranslating \"all eigenvalues are 1\" as \"the determinant is 1\"; simplifying complex algebraic relationship into an incorrect equation; using the conclusion as premise; mismatch of mathematical entities. An incorrect expression structure can change the intrinsic structure and semantics of the original mathematical expression, even if the computed result might happen to be the same. For example, incorrectly formalizing finite nested expression (e.g., 2002 + 21(2001 + . . . )) as the sum of an infinite series. 1.3 Goal Translation Error Description: The final goal or conclusion achieved by the code does not match what the problem asks for, failing to complete the specified task. Examples: The problem asks for specific numerical value, but the code only proves its existence; the problem asks to calculate the radius of convergence, but the code incorrectly solves for the sum of the series; the final answer has numerical calculation error or formal writing error (e.g., writing fraction n/m as m/n). The final goal might also be translated incompletely, with omissions. 1.4 Variable Usage Error Description: Improper use of variables type, scope, name, or index. Examples: Using natural numbers (N) for variable that requires real numbers (R); off-byone errors in summation or sequence indices; confusing or redefining variable names. 1.5 Misuse of Mathematical Concepts Description: Incorrectly using mathematical formalism in Lean to represent different mathematical concept. Examples: Translating \"calculate the residue\" as \"find the limit\"; formalizing \"locally uniform convergence\" as \"pointwise convergence\"; treating problem of counting unordered combinations (e.g., non-congruent triangles) as one of counting ordered tuples. 1.6 Incorrect Assumption Description: Adding conditions that are not present in the original problem, which oversimplifies the problem or leads to an incorrect conclusion. Example: Introducing an unfounded assumption, such as specific numerical value. 2. Lean Syntax and Technical Errors Description: These are technical issues at the code level that prevent the code from compiling or cause unexpected runtime behavior. 2.1 Syntax Error Description: The code does not conform to the basic syntax rules of Lean 4. Examples: theorem statement is missing its name; the by sorry block to skip proof is absent; incorrect keywords or symbols are used. 27 Generated code 2.2 Type Error Description: Performing incompatible operations on variables of different data types, including type mismatches and type casting errors. Examples: Performing division on natural number (Nat) and expecting fractional result, but the outcome is floored to 0; failing to cast integers or natural numbers to real numbers before performing real-valued operations. 2.3 Operator & Parenthesis Error Description: The calculation order of an expression does not match the intended logic due to misunderstandings of operator precedence, improper placement of quantifiers, or incorrect use of parentheses. Example: tan2( π 9 ) is incorrectly parsed as ( tan π 9 )2. 2.4 Library Usage Error Description: Improper use of functions or definitions from mathlib. Examples: Using .ncard on an incorrect type of set; using deprecated function name like Complex.abs. 3. Translation Completeness and Other Meta-Errors Description: These errors reflect that the formalization fails to cover all requirements of the problem, or that the problem itself is difficult to formalize. 3.1 Unformalizable Problem Description: The original problem description is vague, ambiguous, relies on diagrams, or involves real-world scenarios that are difficult to express in formal logic. Examples: The problem depends on geometric figure that is not explicitly defined; physical context or narrative scenario cannot be modeled precisely; the problem statement itself contains mathematical errors. Generated code 3.2 Incomplete Formalization Description: The code only formalizes part of the problem, omitting other requirements. Examples: Ignoring \"prove or disprove\" requirement and assuming the statement is true by default; omitting multi-step derivation requirements from the problem. 3.3 Improvable Code Style Description: The code may be logically correct but can be improved in terms of clarity, robustness, or adherence to conventions. Examples: Adding parentheses could enhance logical clarity; variable names are not intuitive; better use could be made of Leans syntactic features. Complete Checklist for Lean4 Mathematical Formalization Complete Checklist for Lean4 Mathematical Formalization Conditions & Hypotheses: 1. Completeness of Preconditions: Are all explicitly stated preconditions in the problem translated without omission? 28 2. Accuracy of Variable Domains: Are the domains of variables (e.g., N, N+, R, Fin , Set.Icc ) accurately translated? 3. Accuracy of Indexing: Do the starting points and ranges of sequence/function indices (e.g., a0 vs a1, Finset.range vs Finset.Icc 1 ) align with the original intent? 4. Clarity of Object Properties: Are the properties of specific objects (e.g., geometric figures like trapezoids, incircles; algebraic structures like groups, rings) clearly expressed? 5. Inclusion of Implicit Conditions: Are common implicit contextual conditions in mathematics (e.g., non-zero divisors, non-negative radicands, non-degenerate geometric objects, definedness of functions/sequences at application points, default to real numbers if unspecified) appropriately added? 6. Accuracy of Conditional Semantics: Is the mathematical meaning of conditions (e.g., \"not equal to 0\" (= 0) vs \"greater than 0\" (> 0), direction of inequality signs (> vs )) accurately translated? Goals & Conclusions: 1. Completeness of Goals/Conclusions: Are all goals/conclusions that need to be proven or solved translated? (Pay special attention to multi-part conclusions and multiple solution scenarios). 2. Precision of Goal Type: Is the type of goal to be solved precise (e.g., specific value, extremum, existence/uniqueness, universal property, equivalence relation, implication)? 3. Attainability in Extremum Problems: For extremum problems, is \"attainability\" explicitly stated (i.e., demonstrating not just an inequality, but also that equality can be achieved)? 4. Accuracy of Goal Semantics: Is the mathematical meaning of the goals accurately translated? Combination of Preconditions Logical Structure: 1. Accuracy of Logical Connectives: Does the use of logical connectives ( (iff), (if...then...), (and), (or), (not)) accurately reflect the logical relationships of the original proposition? 2. Appropriateness of Quantifiers: Is the use of quantifiers ( (for all), (exists), ! (exists uniquely)) appropriate? 3. Correctness of Quantifier Scope and Nesting: Do the order, scope, and nesting of quantifiers correctly express the dependencies between variables (e.g., in ϵ > 0, δ > 0, . . . , δ depends on ϵ)? 4. Combination of Preconditions: How do multiple preconditions combine to affect the conclusion (e.g., differentiate (A B) from (B C))? 5. Fidelity to Original Logic: Does the formalization faithfully represent the inherent logic and key steps of the original mathematical problem? Lean Technical Accuracy: 1. Correctness of Basic Syntax: Is the basic Lean syntax (parenthesis matching, keywords like theorem , def , variable , let , by ) entirely correct? 2. Adherence to Type Constraints: Do all operations, function parameters, and return values satisfy Leans type constraints? 3. Correct Mapping of Mathematical Concepts: Are mathematical concepts correctly mapped to their Lean counterparts? 4. Clarity of Custom Definitions: Are all custom functions, predicates, and notations used clearly defined? 5. Correctness of Imports: Are necessary definitions and lemmas correctly imported from Mathlib? 29 Overall Consistency: 1. Capturing Core Mathematical Ideas: Does the formalization truly capture the core mathematical ideas and goals of the original problem? 2. Absence of Logical Contradictions: Are there any logical contradictions between the translated conditions, definitions, and goals? 3. Appropriateness for Formalization: Is the problem itself suitable for precise, unambiguous mathematical formalization? 4. Documentation of Assumptions: Are any assumptions or interpretative choices made during the formalization process documented? Prompt:Critical Feedback to CoT Prompt:Critical Feedback to CoT Instruction:You will be provided with mathematical text and its Lean4 code representation. Your task is to evaluate whether the Lean4 code accurately and semantically represents the mathematical text. You will be given boolean indicating conversion success and potentially failure information. Based on this, use step-by-step Chain of Thought (COT) to generate detailed explanation for why the conversion is considered successful or failed, focusing on the semantic equivalence and formal correctness of the Lean4 code relative to the mathematical meaning. The Lean4 code must preserve the intended meaning in the mathematical text and use correct Lean4 syntax and structure. Input: You will receive the following four values: 1. Mathematical Text: string containing mathematical content. 2. Lean4Code: string representing the code equivalent of the mathematical text. 3. Conversion Success: boolean value (True or False) indicating whether the mathematical text was successfully converted to the Lean4 code representation. 4. Reason: string representing the code equivalent of the mathematical text. 5. If Conversion Success is True, this field will typically be empty. You must generate the detailed justification for the success. 6. If Conversion Success is False, this field will contain specific information pinpointing why the conversion failed. You must elaborate on this identified failure, incorporating the analysis modules described below. Your Role: You are an AI language assistant. Your role is to analyze the provided information and, using step-by-step Chain of Thought (COT) approach, generate the explanation for the conversions success (if indicated as successful) or elaborate on the identified failure information (if indicated as failed). Guidelines: 1. Understand the Content: Carefully read the mathematical text, the code representation, the Conversion Success value, and the Reason input (if Conversion Success is False). 2. Generating the Explanation for Success (if Conversion Success is True): If the conversion is successful, provide detailed, step-by-step explanation using COT to justify why it is successful. Your justification should implicitly cover: (a) Mathematical Text Analysis: Briefly identify the core mathematical components (definitions, variables, operations, relations, constraints, statements) present in the text. 30 (b) Lean4 Code Analysis: Briefly describe the structure and components of the provided Lean4 code, outlining how it represents the mathematical elements. (c) Comparative Analysis: Systematically compare each mathematical component identified in the text analysis with its corresponding part described in the Lean4 code analysis. Explain step-by-step how the Lean4 syntax and constructs accurately translate the mathematical concept and semantics. (d) Confirmation of Correctness: Explicitly confirm the absence of errors by verifying that: Definitions: Types (variables, functions, sets, etc.) are correctly defined in Lean4, match the mathematical context, accurately reflect the intended meaning, and have valid interpretation within Lean4. (Absence of Errors in Definition) Constraints: All constraints from the mathematical text are present (no omissions), accurately represented (no inconsistencies or logical flaws), and without unnecessary additions (no redundancy). (Absence of Constraint Errors) Syntax: The Lean4 code uses correct syntax for logical expressions (quantifiers, connectives), mathematical expressions ((cid:80), (cid:82) , {}, operations), and overall Lean4 language structure (theorem declarations, keywords like by). (Absence of Syntax Errors) Proof Targets/Statements: If applicable, the proof target or statement in Lean4 is complete, unambiguous, and consistent with the mathematical claim. (Absence of Proof Target Errors) Confirm that the Lean4 code fully and accurately captures the mathematical meaning without loss or distortion, preserving the intended meaning. Highlight the key aspects demonstrating correct and complete translation. 3. Elaborating on Failure (if Conversion Success is False): If the conversion is unsuccessful, use step-by-step COT approach structured around the following analysis modules to elaborate on the specific failure identified in the input Reason field: (a) Mathematical Text Analysis: Briefly identify the core mathematical components (definitions, variables, operations, relations, constraints, statements) present in the text. This establishes the intended meaning. (b) Lean4 Code Analysis: Briefly describe the structure and components of the provided Lean4 code, outlining how it *attempts* to represent the mathematical elements identified in the text analysis step. This describes the code *as presented*, before focusing on the error. (c) Comparative Analysis: Compare the intended meaning and structure derived from the mathematical text analysis with the actual Lean4 code structure described in the previous step. Crucially, use the input Reason to pinpoint and focus the comparison on the specific segment(s) where the Lean4 code fails to accurately represent the mathematical text, clearly demonstrating the mismatch or divergence indicated by the failure reason. (d) Identification of Omission/Error: Based on the discrepancy identified in the comparative analysis (which was guided by the input Reason), clearly articulate the specific error. Categorize this error by referencing the relevant error type and explain why the issue constitutes this type of error. Use the following categories and examples as guide: Errors in Definition: e.g., Incorrect or mismatched type definitions (Type Mismatch), failure to reflect the precise mathematical context (Context Mismatch), definitions being mathematically meaningless or ill-formed in Lean4 (Ill-formed Definition). 31 Constraint Errors: e.g., Omission of necessary constraints (Constraint Omission), Redundancy of constraints (Constraint Redundancy), Inconsistency with mathematical text (Constraint Inconsistency), Logical Errors within the translated constraints (Logical Flaw in Constraint). Syntax Errors: e.g., Errors in Logical Expression Syntax (quantifiers , ; connectives , , , ), Mathematical Expression Syntax (notation for sums (cid:80), integrals (cid:82) , sets {}, specific operations), or Lean4 Language Structure Syntax (keywords like theorem, def, variable, assume, show, by; overall declaration structure). (Invalid Lean Syntax, Logical Syntax Error, Mathematical Notation Error) Proof Target Errors: e.g., Complete Omission of the target statement (Target Omission), Partial Omission or ambiguity in the target (Target Incomplete/Ambiguous), Inconsistency between the Lean4 goal and the mathematical claim (Target Mismatch). (e) Explanation of Impact: Detail the consequences of this specific error. Explain how it alters the logical meaning compared to the original mathematical text, introduces ambiguity, makes the statement fundamentally different, or renders the Lean4 code syntactically invalid or semantically incorrect relative to the intended mathematical statement. 4. Crucially: Your final explanation should present this analysis directly. Do not explicitly state \"The provided reason indicates...\" or similar phrases referring back to the input Reason field in your output. Simply explain the error based on the structure above. Focus on Explanation: Your primary goal is to generate clear and comprehensive explanation using the COT method, ensuring each step in your reasoning is explicit. No Evaluation of Failure Information: If the conversion failed, do not question or evaluate the validity of the failure information given in the Reason input. Your task is solely to elaborate on it based on that information, following the specified structure and error categorization. Output Format: The output must be in English and presented strictly as follows: Reason: [Your step-by-step Chain of Thought explanation here, either justifying success or elaborating on the identified failure following the specified modules and error types for failures] This section will contain single paragraph with your detailed Chain of Thought explanation. Notes: Clarity and Conciseness: Ensure your explanation is clear, logical, and easy to understand. While using COT, ensure each step is articulated concisely. Professional Tone: Maintain an objective and professional tone throughout your response. No Additional Information: Do not introduce any external information or perform the conversion yourself. Focus solely on generating the explanation based on the provided inputs, using step-by-step approach guided by the structure and error categories provided. 32 Prompt:Formal Verification Expert Prompt:Formal Verification Expert Role: Lean & Formal Verification Expert Input: - Mathematical_Text: math problem and its answer (no proof). - Lean4Code: Lean 4 theorem statement formalizing the problem. Proof is intentionally omitted (e.g., sorry). Goal: Determine if the Lean theorem statement is an exact and faithful formalization of the mathematical problem. **Do not evaluate or consider the answer or the proof. Your sole task is to verify the correctness of the formalization.** Evaluation Stages (All required): 1. Math Assertion Analysis Identify all structurally and semantically relevant components of the mathematical problem, including variables, types, quantifiers, constraints, logic structure, conclusion, and so on. The analysis should be based on the actual content of the text. 2. Lean Statement Analysis (ignore proof part) Extract all structurally and semantically relevant components from the Lean statement, including variables, types, conditions, quantifiers, constraints, the final claim, and so on. The analysis should reflect the actual content present in the Lean code. 3. Comparative Verification Check for exact correspondence between the math and Lean statements; you may refer to aspects like: - Semantic alignment, logic structure, and quantifier correctness. - Preservation of constraints and boundary assumptions. - Accurate typing and use of variables. - Syntactic validity and proper Lean usage (free from errors). - Use of symbols and constructs without semantic drift. - No missing elements, no unjustified additions, and no automatic corrections or completions. 4. Final Judgement Based solely on the above analysis, is correct and exact formalization of the mathematical problem. judge whether the Lean statement 5. Accuracy Confirmation If correct: clearly confirm why all elements match. If incorrect: list all mismatches and explain how each one affects correctness. 33 Note: While the analysis may be broad and open to interpreting all relevant features, the final judgment must be based only on what is explicitly and formally expressed in the Lean statement. **Do not consider or assess any part of the proof. Your judgment should be entirely about the accuracy of the statement formalization.** Output Format: Return exactly one JSON object: json { \"reasons\": \"Your detailed CoT analysis: 1. Math Assertion Analysis: [...] 2. Lean Statement Analysis (Proof Ignored): [...] 3. Comparative Verification: [...] 4. Conclusion: [...] 5. Accuracy Confirmation: [match confirmation or list of discrepancies...]\", \"is_assistant_correct\": \"[Correct/Incorrect]\" } Input Data: Start of Mathematical_Text {mathematical_statement} End of Mathematical_Text Start of Lean4Code {autoformalization_placeholder} End of Lean4Code Prompt:Lean Flaw Injection Prompt:Lean Flaw Injection You are an exceptional Lean 4 code formalization engineer. Your current task is to meticulously introduce errors into correct Lean 4 code, following specific checklist of error types. will provide you with: 1. problem pair consisting of: (a) mathematical definition or statement. (b) Its corresponding correct Lean 4 code formalization. 2. An error checklist and Lean 4 theorem statement: list of potential error types or modification strategies, along with Lean 4 theorem statement that formalizes the problem. Note that the proof is intentionally omitted (e.g., using sorry). Your process should be as follows: 1. From the provided checklist, select exactly 2 error types or modification strategies. Each chosen item must be directly and plausibly applicable to the structure, logic, or types within the provided mathematical statement and its Lean 4 code. 2. Based on your selection(s), intentionally modify the Lean 4 code to make it incorrect or subtly deviate from the original mathematical intent. The modification should be contextually relevant to the provided mathematical statement and its original Lean 4 formalization. Aim for errors that someone might plausibly make when formalizing *this specific* concept, rather than completely arbitrary changes. 3. When applying multiple selected error types, aim to incorporate all of them naturally into the code modification. If this proves too complex or makes the resulting error contrived, you may focus on primary subset of the selected types, but clearly explain your rationale and how the chosen modifications relate to your selections. 4. Important: Do not add any comments directly within the mathematical description or the Lean 4 code itself to explain your changes. All explanations should be in the \"Detailed Explanation of Modifications\" section. You must then return your response as JSON object with the following structure and content: json Output Format You must then return your response as JSON object with the following structure and content: { } \"modified_lean_code\": \"[Your intentionally flawed Lean 4 code here. Please ensure that you do not add any comments directly within the mathematical description or the Lean 4 code itself to explain your changes.]\", \"explanation\": \"Here, you will:n1. Clearly state which 2 error types or modification strategies you selected from the checklist. n2. Explain precisely what changes you made to the original correct code to introduce these errors/deviations.n3. Describe how these changes reflect the selected strateg(ies). Crucially, point out the specific \"error points\" you introduced by comparing the modified code to the (unstated) original correct version, and explain *why* this type of error is plausible or relevant given the specific mathematical content and structure of the original code.\" checklist: {selected_checklist_items_placeholder} \"Mathematical statement\": \"{refined_statement_placeholder}\" \"Lean4 code\": \"{autoformalization_placeholder}\" Prompt: Mathematical Problem Classification Task Prompt: Mathematical Problem Classification Task am working with natural language statements of advanced mathematics problems, which are intended to be later formalized in Lean. Before formalization, need to categorize each problem into its appropriate mathematical domain. #OBJECTIVE# 1. Summarize the math problem in one or two sentences, highlighting the key mathematical concepts or structures involved. 2. Classify the problem into one or more mathematical domains, using hierarchical classification chain. For example: Algebra -> Intermediate Algebra -> Inequalities. The classification should be based on the mathematical content of the natural language problem, even if no formal notation is used yet. #DOMAIN TAXONOMY# The domain classification should follow standard taxonomy: <math domains> Algebra -> Intermediate Algebra -> Inequalities Algebra -> Intermediate Algebra -> Polynomials Algebra -> Intermediate Algebra -> Functional Equations Algebra -> Linear Algebra -> Vector Spaces Algebra -> Linear Algebra -> Matrices Geometry -> Euclidean Geometry -> Triangles Geometry -> Euclidean Geometry -> Circles Geometry -> Euclidean Geometry -> Coordinate Geometry Geometry -> Euclidean Geometry -> Transformations Geometry -> Analytic Geometry -> Conic Sections Number Theory -> Elementary Number Theory -> Divisibility Number Theory -> Elementary Number Theory -> Modular Arithmetic Number Theory -> Elementary Number Theory -> Diophantine Equations Number Theory -> Elementary Number Theory -> Prime Numbers Combinatorics -> Enumerative Combinatorics -> Permutations Combinatorics -> Enumerative Combinatorics -> Combinations Combinatorics -> Extremal Combinatorics -> Pigeonhole Principle Combinatorics -> Constructive Combinatorics -> Invariants Combinatorics -> Graph Theory -> Trees Calculus -> Differential Calculus -> Derivatives Calculus -> Integral Calculus -> Definite Integrals Discrete Mathematics -> Logic -> Propositional Logic Discrete Mathematics -> Set Theory -> Cardinality Applied Mathematics -> Probability -> Expected Value Applied Mathematics -> Probability -> Conditional Probability Applied Mathematics -> Optimization -> Linear Programming Applied Mathematics -> Algorithms -> Greedy Algorithms Algebra -> Intermediate Algebra -> Other Geometry -> Euclidean Geometry -> Other Number Theory -> Elementary Number Theory -> Other Combinatorics -> Enumerative Combinatorics -> Other Calculus -> Integral Calculus -> Other Discrete Mathematics -> Other -> Other </math domains> #RESPONSE FORMAT# ##Summarization [A brief summary of the problem, describing the mathematical concepts it involves.] ##Math domains [A hierarchical classification of the mathematical domains involved in the problem.] #INSTRUCTIONS# You may include up to three domain classification chains, separated by semicolons. The format must be: Major Domain -> Subdomain -> Specific Topic. If concept doesnt fit exactly, use Other as the last node only. For example: Algebra -> Intermediate Algebra -> Other. Avoid using vague or overlapping categories. End each report with the line === report over ===. #INPUT# Below is the original natural language math problem statement: <statement>{statement}</statement> #OUTPUT FORMAT# Please return your response in JSON format. For example: { \"Summary\": \"This problem involves minimizing symmetric function over real variables under absolute value constraints.\", \"Domains\": [ \"Algebra -> Intermediate Algebra -> Inequalities\", \"Calculus -> Differential Calculus -> Applications of Derivatives\" ] } Prompt:Difficulty Level Assessment Prompt:Difficulty Level Assessment You are an exceptional Lean 4 code formalization engineer. Your current task is to meticulously introduce errors into correct Lean 4 code, following specific checklist of error types. am working with natural language statements of advanced mathematics problems, which are intended to be later formalized in Lean. Before that, we aim to assess the **intrinsic difficulty** of the problem in its current informal (natural language) form. #OBJECTIVE Assign **difficulty score** to the problem, on scale from 0 to 10. Your rating should reflect the mathematical reasoning required to solve the problem including the level of abstraction, creativity, number of steps, and familiarity with advanced techniques. 37 #DIFFICULTY REFERENCE ##Examples for difficulty levels For reference, here are problems from each of the difficulty levels 1-10: 1: How many integer values of satisfy < 3π? (2021 Spring AMC 10B, Problem 1) 1.5: number is called flippy if its digits alternate between two distinct digits. For example, 2020 and 37373 are flippy, but 3883 and 123123 are not. How many five-digit flippy numbers are divisible by 15? (2020 AMC 8, Problem 19) 2: fair 6-sided die is repeatedly rolled until an odd number appears. What is the probability that every even number appears at least once before the first occurrence of an odd number? (2021 Spring AMC 10B, Problem 18) 2.5: A, B, are three piles of rocks. The mean weight of the rocks in is 40 pounds, the mean weight of the rocks in is 50 pounds, the mean weight of the rocks in the combined piles and is 43 pounds, and the mean weight of the rocks in the combined piles and is 44 pounds. What is the greatest possible integer value for the mean in pounds of the rocks in the combined piles and C? (2013 AMC 12A, Problem 16) 3: Triangle ABC with AB = 50 and AC = 10 has area 120. Let be the midpoint of AB, and let be the midpoint of AC. The angle bisector of BAC intersects DE and BC at and G, respectively. What is the area of quadrilateral FDBG? (2018 AMC 10A, Problem 24) 3.5: Find the number of integer values of in the closed interval [500, 500] for which the equation log(kx) = 2 log(x + 2) has exactly one real solution. (2017 AIME II, Problem 7) 4: Define sequence recursively by x0 = 5 and for all nonnegative integers n. Let be the least positive integer such that xn+1 = x2 + 5xn + 4 xn + 6 xm 4 + 1 220 . In which of the following intervals does lie? (A) [9, 26] (B) [27, 80] (C) [81, 242] (D) [243, 728] (E) [729, ) (2019 AMC 10B, Problem 24 and 2019 AMC 12B, Problem 22) 4.5: Find, with proof, all positive integers for which 2n + 12n + 2011n is perfect square. (USAJMO 2011/1) 5: Find all triples (a, b, c) of real numbers such that the following system holds: + + = 1 + 1 + 1 , a2 + b2 + c2 = 1 a2 + 1 b2 + 1 c2 . (JBMO 2020/1) 5.5: Triangle ABC has BAC = 60, CBA 90, BC = 1, and AC AB. Let H, I, and be the orthocenter, incenter, and circumcenter of ABC, respectively. Assume that the area of pentagon BCOIH is the maximum possible. What is CBA? (2011 AMC 12A, Problem 25) 6: Let ABC be an acute triangle with circumcircle ω, and let be the intersection of the altitudes of ABC. Suppose the tangent to the circumcircle of HBC at intersects ω at points and 38 with HA = 3, HX = 2, and HY = 6. The area of ABC can be written in the form n, where and are positive integers, and is not divisible by the square of any prime. Find + n. (2020 AIME I, Problem 15) 6.5: Rectangles BCC1B2, CAA1C2, and ABB1A2 are erected outside an acute triangle ABC. Suppose that BC1C + CA1A + AB1B = 180. Prove that lines B1C2, C1A2, and A1B2 are concurrent. (USAMO 2021/1, USAJMO 2021/2) 7: We say that finite set in the plane is balanced if, for any two different points A, in S, there is point in such that AC = BC. We say that is centre-free if for any three points A, B, in S, there is no point in such that = = C. Show that for all integers 3, there exists balanced set consisting of points. Determine all integers 3 for which there exists balanced centre-free set consisting of points. (IMO 2015/1) 7.5: Let be the set of integers. Find all functions : such that xf (2f (y) x) + y2f (2x (y)) = (x)2 + (yf (y)) for all x, with = 0. (USAMO 2014/2) 8: For each positive integer n, the Bank of Cape Town issues coins of denomination 1/n. Given finite collection of such coins (of not necessarily different denominations) with total value at most 99 + 1/2, prove that it is possible to split this collection into 100 or fewer groups, such that each group has total value at most 1. (IMO 2014/5) 8.5: Let be the incentre of acute triangle ABC with AB = AC. The incircle ω of ABC is tangent to sides BC, CA, and AB at D, E, and F, respectively. The line through perpendicular to EF meets ω at R. Line AR meets ω again at P. The circumcircles of triangle PCE and PBF meet again at Q. Prove that lines DI and PQ meet on the line through perpendicular to AI. (IMO 2019/6) 9: Let be positive integer and let be finite set of odd prime numbers. Prove that there is at most one way (up to rotation and reflection) to place the elements of around the circle such that the product of any two neighbors is of the form x2 + + for some positive integer x. (IMO 2022/3) 9.5: An anti-Pascal triangle is an equilateral triangular array of numbers such that, except for the numbers in the bottom row, each number is the absolute value of the difference of the two numbers immediately below it. For example, the following is an anti-Pascal triangle with four rows which contains every integer from 1 to 10. 4 7 1 2 5 8 3 9 Does there exist an anti-Pascal triangle with 2018 rows which contains every integer from 1 to 1 + 2 + 3 + + 2018? (IMO 2018/3) 10: Prove that there exists positive constant such that the following statement is true: Consider an integer > 1, and set of points in the plane such that the distance between any two different points in is at least 1. It follows that there is line ℓ separating such that the distance from any point of to ℓ is at least cn1/3. #OBJECTIVE 1. Summarize the math problem in brief sentence, describing the concepts involved in the math 39 problem. 2. Based on the source of the given problem, as well as the difficulty of the problems referenced in these materials and the solution to the current problem, please provide an overall difficulty score for the current problem. The score should be number between 1 and 10, with increments of 0.5, and should align perfectly with the materials. #STYLE# Data report. #TONE# Professional, scientific. #AUDIENCE# Students. Enable them to better understand the difficulty of the math problems. #RESPONSE: MARKDOWN REPORT# ##Summarization [Summarize the math problem in brief paragraph.] ##Difficulty [Rate the difficulty of the math problem and give the reason.] #ATTENTION# Add \"=== report over ===\" at the end of the report. #INPUT# Below is the original natural language math problem statement: <statement>{statement}</statement> #OUTPUT FORMAT# You must respond with JSON object: { } \"Difficulty\": float (between 0 and 10), \"Rationale\": \"Explain your score in 1-3 sentences. Mention structural elements or comparison to benchmark problems.\""
        }
    ],
    "affiliations": []
}