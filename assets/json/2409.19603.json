{
    "paper_title": "One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos",
    "authors": [
        "Zechen Bai",
        "Tong He",
        "Haiyang Mei",
        "Pichao Wang",
        "Ziteng Gao",
        "Joya Chen",
        "Lei Liu",
        "Zheng Zhang",
        "Mike Zheng Shou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce VideoLISA, a video-based multimodal large language model designed to tackle the problem of language-instructed reasoning segmentation in videos. Leveraging the reasoning capabilities and world knowledge of large language models, and augmented by the Segment Anything Model, VideoLISA generates temporally consistent segmentation masks in videos based on language instructions. Existing image-based methods, such as LISA, struggle with video tasks due to the additional temporal dimension, which requires temporal dynamic understanding and consistent segmentation across frames. VideoLISA addresses these challenges by integrating a Sparse Dense Sampling strategy into the video-LLM, which balances temporal context and spatial detail within computational constraints. Additionally, we propose a One-Token-Seg-All approach using a specially designed <TRK> token, enabling the model to segment and track objects across multiple frames. Extensive evaluations on diverse benchmarks, including our newly introduced ReasonVOS benchmark, demonstrate VideoLISA's superior performance in video object segmentation tasks involving complex reasoning, temporal understanding, and object tracking. While optimized for videos, VideoLISA also shows promising generalization to image segmentation, revealing its potential as a unified foundation model for language-instructed object segmentation. Code and model will be available at: https://github.com/showlab/VideoLISA."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 9 2 ] . [ 1 3 0 6 9 1 . 9 0 4 2 : r One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos Zechen Bai1 Tong He2 Haiyang Mei1 Pichao Wang2 Ziteng Gao1 Joya Chen1 Lei Liu2 Zheng Zhang2 Mike Zheng Shou1 1Show Lab, National University of Singapore 2Amazon"
        },
        {
            "title": "Abstract",
            "content": "We introduce VideoLISA, video-based multimodal large language model designed to tackle the problem of language-instructed reasoning segmentation in videos. Leveraging the reasoning capabilities and world knowledge of large language models, and augmented by the Segment Anything Model, VideoLISA generates temporally consistent segmentation masks in videos based on language instructions. Existing image-based methods, such as LISA, struggle with video tasks due to the additional temporal dimension, which requires temporal dynamic understanding and consistent segmentation across frames. VideoLISA addresses these challenges by integrating Sparse Dense Sampling strategy into the videoLLM, which balances temporal context and spatial detail within computational constraints. Additionally, we propose One-Token-Seg-All approach using specially designed <TRK> token, enabling the model to segment and track objects across multiple frames. Extensive evaluations on diverse benchmarks, including our newly introduced ReasonVOS benchmark, demonstrate VideoLISAs superior performance in video object segmentation tasks involving complex reasoning, temporal understanding, and object tracking. While optimized for videos, VideoLISA also shows promising generalization to image segmentation, revealing its potential as unified foundation model for language-instructed object segmentation. Code and model will be available at: https://github.com/showlab/VideoLISA."
        },
        {
            "title": "Introduction",
            "content": "We live in dynamic world. Localizing objects of interest in videos according to human intent is crucial task for intelligent models and systems. Language, as natural interface, serves as the primary reference for identifying target objects. However, language expressions vary widely across different scenarios, presenting varying levels of difficulty. While category names are straightforward references, detailed text descriptions from tasks like referring segmentation [27, 46, 56] introduce greater complexity. In real-world applications, these expressions can be more complex, involving intent understanding, reasoning, and world knowledge, making them more user-friendly yet significantly more challenging for models to understand and act upon. Recent advancements in the image domain have shown progress in language-instructed reasoning for detection and segmentation tasks. Models leveraging multimodal large language models (MLLMs), such as those in DetGPT [51] and LISA [30], have demonstrated the ability to localize target objects by harnessing the implicit reasoning capabilities and world knowledge embedded in large language models (LLMs). However, these advancements have not seamlessly translated to video tasks, particularly video object segmentation (VOS). The primary challenge in VOS stems from the additional temporal dimension, which introduces complexities absent in static images. VOS requires Corresponding Author Preprint. To appear in NeurIPS, 2024. models to 1) on the input side, capture and comprehend the temporal dynamics present in the video; and 2) on the output side, predict temporally consistent segmentation masks across frames. These challenges render existing image-based methods inadequate for handling video tasks. In this work, we introduce VideoLISA, video-based MLLM designed to address language-instructed reasoning segmentation in videos. Our goal is to segment target objects throughout the entire video based on diverse language queries that necessitate scene understanding, temporal comprehension, and implicit reasoning. Drawing inspiration from previous works [51, 30], we employ an LLM to inherit its complex reasoning capabilities and adopt the Segment Anything Model (SAM) [29] to produce segmentation masks. To overcome the unique challenges presented by video data, we propose two key innovations: Sparse Dense Sampling strategy and One-Token-Seg-All approach. To equip the model with video temporal understanding ability, it is necessary to involve multiple frames. Processing visual features from all sampled frames in full feature resolution is computationally prohibitive due to the large number of tokens. In pursuit of efficiency, reducing the frame number would limit the perception of temporal dynamics while down-sampling frame features would lose visual details that are essential for dense prediction tasks exemplified by segmentation. Our intuition is that adjacent frames in videos usually share similar visual contents and features. Therefore, we leverage this inherent temporal redundancy in videos and propose the Sparse Dense Sampling strategy. It uniformly samples set of dense frames, preserving full-resolution features (dense tokens), and down-samples the remaining interleaved frames to lower resolution (sparse tokens). Dense tokens provide detailed visual information needed for accurate segmentation, while sparse tokens capture the temporal context, ensuring that the model remains aware of motion and changes over time. This balance allows the model to construct coherent spatiotemporal narrative without excessive computational demands. For achieving temporal consistency in segmentation, instead of handling separate representations for each frame, we propose One-Token-Seg-All approach. Prior arts [21, 7] reveal that one compact representation can potentially associate the same object across video frames. In this work, we design special <TRK> token to segment and track target objects across multiple frames. Specifically, we incorporate the <TRK> token into the models vocabulary and utilize its last hidden embedding in the LLM to prompt the mask decoder to produce segmentation masks. We improve the temporal consistency from two aspects. First, when generating the <TRK> token, the model sees the video content through the temporal module, which serves as the information foundation for cross-frame association. In addition, during training, the <TRK> token is intentionally trained to segment multiple frames simultaneously, preventing the model from learning shortcuts that focus only on spatial information of certain frame. During inference, single <TRK> token can segment and track objects across an entire video. The <TRK> token acts as unified spatiotemporal representation, encapsulating object information across multiple frames and reducing the complexity of handling multiple prompts. We evaluate our model on comprehensive range of public benchmarks, including standard video/image referring segmentation, motion-guided video segmentation, and image reasoning segmentation. To further assess the models capabilities in complex reasoning, temporal understanding, and object tracking, we introduce the ReasonVOS benchmark. Extensive experiments and ablation studies demonstrate the effectiveness of our approach. Although our model is particularly designed for videos, experiments show that it generalizes well on images, making it potential foundation model for unified language instructed object segmentation. Our contributions: Sparse Dense Sampling Strategy: We devise sampling strategy for video-LLM training that achieves balance between temporal context length and spatial visual detail under computational constraints. This strategyis shown to be effective for spatiotemporal dense prediction tasks, exemplified by video object segmentation. One-Token-Seg-All Approach: We design an effective approach for temporal consistent object segmentation in videos by utilizing special <TRK> token. This strategy demonstrates robust performance in video object segmentation, leveraging the video-LLM learning module and specially designed training objective. VideoLISA Model: We propose VideoLISA, video-LLM that democratizes reasoning segmentation to videos. Additionally, we introduce the ReasonVOS benchmark, focusing on complex reasoning, temporal understanding, and object movements. This benchmark, along with range of public benchmarks, comprehensively validates our models performance."
        },
        {
            "title": "2.1 Video Object Segmentation",
            "content": "In computer vision, video object segmentation is well-studied task [65]. Specifically, referring video object segmentation (RVOS) aims to segment the target object mentioned in natural language expression in video [56, 8, 60, 66, 34, 61, 47]. Compared with image segmentation, RVOS is more challenging since both the action and appearance of the referred object must be segmented in video. Gavrilyuk et al. (2018) were the first to propose the RVOS task and the A2D-Sentences benchmark [23]. This field continues to evolve with new benchmarks emerge such as Ref-DAVIS17 [28], Ref-YouTube-VOS [56], and MeViS [14]. Many previous studies have primarily adapted referring image segmentation approaches for frame-by-frame object segmentation. For example, URVOS [56] and RefVOS [6] utilize cross-modal attention for per-frame segmentation. Some recent works, such as ReferFormer [61] and MTTR [8], employ DETR-like structure, which simplifies the referring pipeline and achieves impressive performance. R2VOS [34] enhances multi-modal alignment through text reconstruction. OnlineRefer [60] proposes an online model with explicit query propagation. SgMg [47] proposes segment-and-optimize paradigm to solve the feature drift issue. Despite the impressive results achieved by these methods, several challenges remain. First, most existing methods are deficient in comprehending the motion information in videos and languages, as revealed by the recent MeViS [14] benchmark. Second, there are few studies on complex reasoning-based segmentation in the video domain, both methodologically and benchmark-wise."
        },
        {
            "title": "2.2 Multimodal Large Language Model",
            "content": "The remarkable advancements of large language models (LLMs) motivate the research community to extend the foundational capabilities of LLMs to the visual domain, leading to multimodal large language models (MLLMs) [67, 4]. The pioneering works of MLLMs, such as LLaVA [39], MiniGPT4 [71], and InstructBLIP [13], exhibit impressive visual understanding capabilities, including image captioning [57, 3] and visual question answering. When extending into the video domain, prominent issue is handling the temporal dimension. One straightforward approach is to concatenate the tokens from multiple frames [37], though the temporal length might be limited by computational resources. To address this, one line of work [45, 26, 25] explores pooling (merging) strategies to reduce the number of tokens, such as pooling along the spatial and temporal dimensions separately [45], token merging based on similarity [26], and pooling with different strengths at slow-fast pace [25]. Another line of work [33, 68] utilizes the Q-former [32] architecture to extract abstracted features, which greatly reduces the number of tokens. More recently, some studies have further integrated region-level image understanding and grounding abilities into MLLMs. Kosmos-2 [50] and Shikra [10] directly quantize bounding boxes into discrete location tokens or numeric representations of positions. GPT4RoI [69] uses simple pooling operation to extract features within boxes or masks as the region representations. Another line of work leverages the reasoning ability of MLLMs and resorts to off-the-shelf models for localization. For example, DetGPT [51] utilizes pre-trained LLM and an open-vocabulary object detector to detect the target object based on human intent described in natural language. LISA [30] connects an MLLM and the Segment Anything (SAM) [29] model using special token to produce fine-grained segmentation masks. Although these works have achieved impressive performance on image tasks, they are still incapable of processing videos. For object segmentation in videos, very few studies have leveraged the reasoning ability of LLMs to overcome current limitations. PG-Video-LLaVA [49] utilizes off-the-shelf object detector and tracker to obtain the target objects first and then match it with the entities mentioned in the generated text. TrackGPT [72] makes straightforward extension of LISA by iteratively updating the special token with video progresses. However, the absence of video learning module significantly limits its perception and reasoning of temporal dynamics."
        },
        {
            "title": "3 Method",
            "content": "The task of language-instructed reasoning segmentation in videos can be formally defined as follows. Given video Xvid and language expression Xtxt, the model takes both as input and outputs the pixel-level segmentation masks for all frames. Xtxt is free-form text that particularly emphasizes implicit intent reasoning, world knowledge, and video temporal dynamics. 3 Figure 1: Framework of our approach."
        },
        {
            "title": "3.1 Architecture",
            "content": "Fig. 1 illustrates the model architecture. It consists of visual tokenizer, an LLM, vision encoder, and promptable mask decoder. We omit the text tokenizer in the LLM for simplicity. The visual tokenizer and LLM are initialized from LLaVA [39, 53]. The vision encoder and mask decoder are initialized from SAM [29]. Given video, we first uniformly sample Tsparse frames and encode them into visual tokens via the visual tokenizer, resulting in Tsparse tokens in total. Ideally, larger Tsparse would be better for capturing temporal dynamics. However, it is prohibitive to let the LLM process such large number of tokens. Thus, we develop the Sparse Dense Sampling strategy to reduce the number of tokens, which will be elaborated in Sec. 3.2. After that, the visual tokens are concatenated with text tokens and fed into the LLM. To equip the LLM with segmentation capabilities, following previous work [30], we extend the vocabulary of the LLM with special token <TRK>. During generation, this special token carries rich semantic information from the text prompt and video content, providing signals for decoding pixel-level segmentation masks. Specifically, we extract the last layer embedding corresponding to the <TRK> token and transform it into prompt embedding with multi-layer perceptron (MLP). At the same time, the vision encoder extracts per-frame features from the video. Finally, the prompt embedding and the visual features are processed by the mask decoder to produce the segmentation masks. Note that for one video, there is only one prompt embedding that is in charge of all the frames. The One-Token-Seg-All approach will be introduced in Sec. 3.3."
        },
        {
            "title": "3.2 Sparse Dense Sampling",
            "content": "Given the Tsparse tokens, we aim to reduce the number of tokens while preserving enough spatial details and temporal dynamics. Therefore, we further sample Tdense frames out of Tsparse frames. The visual tokens of the Tdense frames are all preserved in full resolution, i.e., dense tokens. Then, we apply global average pooling on the Tsparse frames to reduce them to low resolution, i.e., sparse tokens. In our implementation, each frame is represented by only one token. Finally, the total number of tokens is reduced to Tsparse + Tdense L, which is significantly smaller than Tsparse L. The rationale behind this strategy is the inherent temporal redundancy in video data. By exploiting this, we reduce the computational burden without losing critical information. The dense tokens provide visual details for their adjacent sparse frames, while the sparse tokens capture the temporal dynamics for the dense frames. In Sec. 2.2, we have discussed several popular temporal learning strategies in video-LLM. Although they exhibit remarkable performance in general video understanding tasks, our empirical studies (see Tab. 5) demonstrate that these popular strategies are not seamlessly transferable to video object segmentation. This is likely because they either lose spatial details or temporal information, both of which are essential in dense prediction tasks in videos."
        },
        {
            "title": "3.3 One Token Seg All",
            "content": "As shown in Fig. 1, throughout the video, we use single special <TRK> token for segmenting all the frames. We provide an in-depth analysis of the rationale behind this approach. In our model, the promptable segmentation model is initialized from SAM, in which the decoder takes the prompt 4 Figure 2: Exploration of One-Token-Seg-All approach. embedding and visual features as inputs and outputs masks. Our intuition is that segmenting one object in multiple frames can be regarded as segmenting multiple regions (instances) in one image grid. From this perspective, SAM [29] itself already has the potential to segment objects across multiple frames, if the prompt is properly given. Previous works [21, 7, 20] suggest that one compact representation has the potential to associate the same entity across video frames. For example, from the perspective of object tracking [7], the prompt embedding can be regarded as semantic kernel while the visual features are the context to be contrasted. This motivates us to explore whether one prompt embedding is capable of tracking under the promptable decoding paradigm of SAM. To answer this question, one key problem is whether the prompt embedding contains enough semantic information to serve as the kernel. In SAM, its own prompt encoder mainly accepts visual prompts, such as points, boxes, and masks. In videos, the object moves dynamically. Our pilot study in Fig. 2 shows that visual prompts quickly fail in the presence of object motion. This is expected since these visual prompts heavily rely on the objects spatial location. We then explore the prompt embedding produced by an image reasoning segmentation model, LISA [30], which employs LLM and is trained with segmentation data. It can be expected that its prompt embedding should contain more semantic information, at least significantly more than that of the visually instructed prompt. The second row of Fig. 2 validates this hypothesis by applying one prompt embedding to multiple frames. Compared to box prompts, the prompt embedding from LISA shows improved resilience to object movement, as demonstrated in the first three frames. However, when the objects motion becomes larger and distractor object appears, the segmentation fails again, drifting to another object nearby. We identify two primary factors that account for the failure. Firstly, the input of LISA model only has one frame, which contains very limited temporal information. Therefore, the generated prompt embedding lacks the information required for cross-frame association. Secondly, during the training of LISA, the prompt embedding is trained to segment only one frame. This potentially allows it to learn shortcut that merely encompasses positional information, rather than learning the semantic information that generalizes across frames. In our work, the approach of using one token to segment multiple frames has been developed by addressing these issues accordingly. Firstly, the Sparse Dense Sampling-based temporal learning module provides spatiotemporal information of the video. The model sees the video content, which is the foundation of mask association. Furthermore, during training, we intentionally train the <TRK> token to segment multiple frames. This objective would enforce the token to learn more semantic information that can be used as the semantic kernel and segment the target object across frames. The last row of Fig. 2 presents the segmentation and tracking produced by the <TRK> token in our VideoLISA."
        },
        {
            "title": "3.4 Training and Inference",
            "content": "Training Data. The training data for our model mainly consists of two parts: 1) image segmentation and 2) video segmentation. For the image part, we follow the setting of LISA [30]. For the video data, we employ video object segmentation (VOS) and referring video segmentation data (RVOS). During pre-processing, we fill the original category name or referring expression in the dataset into template. For example: USER: <VIDEO> Can you segment {description} in this ASSISTANT: Sure, it is <TRK>., where {description} is the placeholder to fill. scene? For VOS data that contain videos with multi-class labels, we randomly choose one class and merge all the masks belonging to this class into one binary mask. Training Objective. The model is trained end-to-end using the text generation loss Ltxt and segmentation loss Lseg. The segmentation loss 5 consists of per-pixel binary cross-entropy (BCE) loss and DICE loss. The final loss is computed as the weighted sum of the three losses. For video training, we compute the segmentation loss on the sampled Tdense frames in parallel and average them. Inference. During inference, given video, Tsparse and Tdense frames are sampled similarly to training, except that the Tdense frames are uniformly sampled from Tsparse rather than randomly. After obtaining the <TRK> token from the LLM, we feed all the frames of the video into the mask decoder one by one, using the same <TRK> token to segment each frame, yielding list of masks. Post optimization. Among these frames, the Tdense frames are seen in full resolution by the model, making their segmentation masks more reliable and accurate. For the remaining frames, although the One-Token-Seg-All strategy exhibits impressive cross-frame segmentation performance, our empirical observations indicate it inevitably suffers from low mask quality, likely limited by the inherent capability of the SAM model. Thus, we employ post-optimization as an optional step to further enhance mask quality. Specifically, we take XMem++ [5] as the post-optimization approach. Compared to XMem [12], which propagates one mask through the video, XMem++ distinguishes itself by taking multiple reliable masks as reference and inferring the masks of the remaining frames. This paradigm is naturally suitable for our method since the Tdense frames span uniformly across the video, providing long-range yet diverse masks as references."
        },
        {
            "title": "5.1 Experimental Setting",
            "content": "Datasets Our model is trained on variety of segmentation datasets. The image-based datasets include 1) semantic segmentation: ADE20K [70], COCO-Stuff [9], PACO-LVIS [52], and PASCALPart [11]; 2) referring segmentation: refCLEF, refCOCO, refCOCO+ [27], and refCOCOg [46]; 3) reason segmentation: 239 ReasonSeg samples from LISA [30]. The video-based datasets we use include: 1) semantic VOS: YouTube-VOS [63]; 2) referring VOS: Refer-YouTube-VOS [56] and MeViS [14]. The evaluation benchmarks will be elaborated in the corresponding experiment sections. Implementation Details We implement our model with LLaVA-Phi-3-V [53], multimodal LLM based on Phi-3 [1] with 3.8B parameters. We adopt the vision encoder and mask decoder from SAM [29]. We conduct joint training using both image and video datasets. For video data, we set Tsparse = 32 and Tdense = 4 according to our GPU memory. For image data, we duplicate the images as pseudo video data. We train our model using 64 NVIDIA 24G A10 GPUs with distributed training script based on DeepSpeed [54]. We use the AdamW [42] optimizer with the learning rate and weight decay set to 0.0003 and 0, respectively. We also adopt WarmupDecayLR as the learning rate scheduler, with the warmup iterations set to 100. The weights of the text generation loss (λtxt) and the mask loss (λseg) are both set to 1.0. The weights of the BCE loss (λbce) and the DICE loss (λdice) are set to 2.0 and 0.5, respectively. The per-device batch size is set to 2. For ablation studies, the total number of iterations is 3, 000 and each experiment takes around 10 hours. For the final model used for comparison, we scale up the training to 6, 000 iterations, which takes 20 hours. 6 Table 1: The quantitative evaluation results on Refer-Youtube-VOS and Refer-DAVIS-17. In the table, bold denotes the best scores; underline denotes the second place. Method Year Refer-Youtube-VOS &F Refer-DAVIS-17 &F Traditional methods without reasoning ability URVOS [56] CMPC-V [40] YOFO [31] LBDT [18] MLSA [59] PMINet + CFBI [19] MTTR [8] CITD [35] ReferFormer [61] R2-VOS [34] SgMg [47] OnlineRefer [60] LLM-based methods with reasoning ability LISA-7B [30] LISA-13B [30] TrackGPT-7B [72] TrackGPT-13B [72] VideoLISA-3.8B (One-Token-Seg-All) VideoLISA-3.8B (Post-optimization) 2020 2021 2022 2022 2022 2021 2022 2021 2022 2023 2023 2023 2023 2023 2023 2023 2024 2024 47.2 47.5 48.6 49.4 49.7 54.2 55.3 61.4 62.9 61.3 65.7 63.5 50.2 52.6 56.4 59.5 61.7 63.7 45.2 45.6 47.5 48.2 48.4 53.0 54.0 60.0 61.3 59.6 63.9 61.6 49.7 52.1 55.3 58.1 60.2 61. 49.1 49.3 49.7 50.6 50.9 55.5 56.6 62.7 64.6 63.1 67.4 65.5 50.6 53.0 57.4 60.8 63.3 65.7 51.6 - 53.3 54.3 57.9 - - - 61.1 - 63.3 64.8 58.4 60.7 63.2 66.5 67.7 68.8 47.2 - 48.8 - 53.8 - - - 58.1 - 60.6 61.6 54.9 56.8 59.4 62.7 63.8 64. 55.9 - 57.8 - 62.0 - - - 64.1 - 66.0 67.7 61.9 64.6 67.0 70.4 71.5 72.7 Table 2: Results on MeViS benchmark. Table 3: Results on ReasonVOS benchmark. Methods URVOS [56] LBDT [18] MTTR [8] ReferFormer [61] VLT+TC [16] LMPM [14] Year &F 2020 27.8 25.7 29.9 2022 29.3 27.8 30.8 2022 30.0 28.8 31.2 2022 31.0 29.8 32.2 2021 35.5 33.6 37.3 2023 37.2 34.2 40. Methods MTTR [8] ReferFormer [61] SOC [44] OnlineRefer [60] SgMg [47] LISA [30] Year &F 2022 31.1 29.1 33.1 2022 32.9 30.2 35.6 2023 35.9 33.3 38.5 2023 38.7 34.6 42.9 2023 36.2 33.7 38.7 2023 31.1 29.1 33.1 VideoLISA-3.8B (One-Token-Seg-All) 2024 42.3 39.4 45.2 VideoLISA-3.8B (Post-optimization) 2024 44.4 41.3 47.6 VideoLISA-3.8B (One-Token-Seg-All) 2024 45.1 43.1 47.1 VideoLISA-3.8B (Post-optimization) 2024 47.5 45.1 49.9 Evaluation Metrics For image-based evaluation, we adopt two metrics commonly used in previous works [27, 30]: gIoU and cIoU. gIoU is defined by the average of all per-image Intersection-overUnions (IoUs), while cIoU is defined by the cumulative intersection over the cumulative union. For video-based evaluation, we follow previous practices [61, 60] and use region similarity (J), contour accuracy (F), and their average value (J&F)."
        },
        {
            "title": "5.2.1 Referring Video Object Segmentation",
            "content": "We adopt two benchmarks of standard referring video object segmentation. Ref-Youtube-VOS is evaluated on the official challenge server 2. Ref-DAVIS-17 is evaluated by the official evaluation code 3. The evaluation results are shown in Tab. 1. Our method demonstrates competitive performance on both benchmarks, achieving comparable or superior results to existing methods. For Refer-DAVIS17, our method achieves state-of-the-art performance, outperforming all the other methods by considerable margin. In Refer-YouTube-VOS, our method performs well compared to traditional RVOS methods, achieving high rank. State-of-the-art methods, such as SgMg [47], achieve remarkable performance, thanks to its dedicated video backbones, such as Video-Swin [41]. However, among LLM-based methods with reasoning ability, our model, despite having only 3.8B parameters, outperforms other methods with much larger LLMs, such as LISA-13B and TrackGPT-13B."
        },
        {
            "title": "5.2.2 Motion-guided Video Object Segmentation",
            "content": "We further evaluate our model on motion-guided VOS using the MeViS [14] benchmark. Consistent with previous studies [14, 24], we evaluate our models performance on the validation set of the MeViS benchmark. The results in Tab. 2 demonstrate that our method achieves state-of-the-art performance in this benchmark, outperforming previous methods by large margin. We attribute this performance gap to our models adeptness in capturing temporal dynamics and cross-modal interaction, facilitated by the Sparse Dense Sampling-based temporal module and the One-Token-Seg-All training paradigm. 2https://codalab.lisn.upsaclay.fr/competitions/3282 3https://github.com/davisvideochallenge/davis2017-evaluation 7 Table 4: Reasoning segmentation results among ours and previous related works. ft denotes using 239 reasoning segmentation image-instruction pairs to finetune the model. Method OVSeg [36] GRES [38] X-Decoder [73] SEEM [74] Grounded-SAM [55] LISA-7B [30] LISA-7B (ft) [30] LISA-13B [30] LISA-13B (ft) [30] VideoLISA-3.8B (Ours) val overall short query test long query overall gIoU cIoU gIoU cIoU gIoU cIoU gIoU cIoU 20.8 28.5 22.0 22.4 16.3 22.6 18.7 25.5 16.4 26.0 34.1 44.4 48.4 52.9 45.8 48.9 51.1 56.2 54.4 61.4 28.7 22.6 22.2 25.6 22.4 36.6 49.4 46.4 54.0 56. 22.5 23.8 17.5 20.8 18.6 34.7 51.0 46.5 54.3 57.7 26.1 21.3 21.7 24.3 21.3 36.8 47.3 44.8 51.7 53.8 18.6 19.9 17.9 21.2 14.5 46.0 54.0 46.9 62.9 67.1 15.5 15.0 11.6 11.5 10.8 34.4 40.6 43.3 42.0 42.7 18.0 17.6 20.4 20.1 17.8 37.6 40.6 39.9 44.3 43.8 Table 5: Ablation study on the temporal modeling architecture. Table 6: Ablation study on the mask association i.e., tracking architecture. Method ReasonSeg (val) giou ciou MeViS (valid_u) &F Method MeViS (valid_u) &F LISA-7B (Baseline) LISA-7B (Vid. FT) VideoLISA-3.8B (n-frame) VideoLISA-3.8B (ST Pooling [45]) VideoLISA-3.8B (Slow-Fast Pooling [25]) VideoLISA-3.8B (Sparse Dense Sampling) 51.7 48.6 55.6 56.0 54.0 58.9 56.7 56.2 60.8 59.9 54.4 60.0 43.2 44.8 49.9 50.8 50.2 51.7 39.9 41.1 46.7 47.8 47.2 48.4 46.5 48.6 53.0 53.8 53.1 54.9 LISA-7B (Baseline) LISA-7B + XMem[12] VideoLISA-3.8B (One-Token-Seg-One) VideoLISA-3.8B (One-Token-Seg-All) VideoLISA-3.8B (Post optimization) 43.2 45.6 46.1 51.7 54.5 39.9 41.9 42.4 48.4 50.9 46.5 49.3 49.8 54.9 58."
        },
        {
            "title": "5.2.3 Reasoning Video Object Segmentation",
            "content": "In Tab. 3, we compare various methods on the newly organized ReasonVOS benchmark. For traditional VOS methods, the metrics are evaluated using their released checkpoints pre-trained on the Ref-YouTube-VOS dataset. This benchmark focuses on complex reasoning, temporal understanding, and segmentation temporal consistency, which present significant challenges for existing VOS methods and image-based reasoning segmentation methods. It can be observed that most previous methods exhibit unsatisfactory performance on this benchmark. Traditional RVOS methods, such as ReferFormer [61], excel at tracking moving objects but struggle with comprehending complex language expressions, particularly those requiring multi-step reasoning with world knowledge. On the other hand, LLM-based models, like LISA [30], have better language understanding and reasoning capabilities. The main reasons for the poor performance are: 1) incapability to capture temporal dynamics in the video, and 2) difficulty in segmenting temporally consistent masks. In contrast, our VideoLISA model demonstrates remarkable performance, thanks to the advanced model design that considers all these crucial aspects."
        },
        {
            "title": "5.3 Evaluation on Image Tasks",
            "content": "We use the image reasoning segmentation benchmark [30] to assess the reasoning capability of our model. During testing, we duplicate an image into multiple frames as pseudo video. The results are shown in Tab. 4. We observe that our VideoLISA achieves state-of-the-art performance on both validation set and test set. Remarkably, despite our model employing an LLM with significantly fewer parameters, it outperforms larger models, such as LISA-7B and LISA-13B, demonstrating its exceptional reasoning capability. We attribute the impressive performance to the following aspects. From data perspective, VideoLISA benefits from joint training on both image and video datasets, allowing it to learn from more abundant and diverse supervision signals. On the model aspect, the temporal learning module and the One-Token-Seg-All training encourage the model to leverage multiple frames of video simultaneously to conduct reasoning, rather than focusing on one image. Even when generalizing to image tasks, where the video is simulated by an image, the models reasoning capability remains effective. We provide more experiment results on image referring segmentation in the appendix. These experiments demonstrate that our model is capable of imagebased tasks, suggesting the potential for unifying image/video referring/reasoning segmentation tasks into language-instructed object segmentation task solvable by single VideoLISA model."
        },
        {
            "title": "5.4 Ablation Studies",
            "content": "We conduct ablation studies on various design choices of our model. The detailed experiment results are provided in the appendix. Here, we summarize the main takeaways for each study. 8 Ablation of temporal learning module. In this study of Tab. 5, we compare our Sparse Dense Sampling strategy with various design choices, including LISA [30] finetuned on videos, the most straightforward solution that directly concatenate visual tokens from multiple frames (n-frame), the strategy that pools along spatial and temporal dimension separately (ST Pooling), the strategy that pools each frame with different strengths in slow fast pace. The comparison of the experiment results shows that our Sparse Dense Sampling strategy outperforms other video-LLM training (sampling) strategies. In addition to demonstrating the effectiveness of our method, this study also reveals the unique properties of the VOS task. On the one hand, it requires detailed visual information for accurate segmentation, which makes the pooling-based strategies yield inferior results. On the other hand, temporal information is also necessary for the model to comprehend motions and behaviors, as validated by the comparison between n-frame and ours. Ablation of temporal association module. The main takeaway of this part, as shown in Table 5, lies in the comparison between our method and extensions of image-based LISA. Specifically, we upgrade LISA to fit the VOS task by 1) (baseline) using one <SEG> token from the first frame to segment subsequent frames, 2) marrying LISA with an off-the-shelf tracking model. With the help of the tracker, LISA performs clearly better than the baseline, while still performs worse than our method. The main issue comes from that without perception of the video, the model is incapable of processing queries that are concerned with the full video content and temporal dynamic. We further quantify the effect of the One-Token-Seg-All approach by contrasting it with strawman setting, OneToken-Seg-One. The comparison clearly validates the effect and necessity of the One-Token-Seg-All approach."
        },
        {
            "title": "6 Limitation and Future Work",
            "content": "Despite the remarkable performance shown on various benchmarks, our model still has limitations. We discuss them in this section to inspire future work. First, our model exhibits deficiencies in computational efficiency. Although we have already reduced the size of LLM to 3.8B, which is much smaller than previous models (7B, 13B), it still incurs relatively high computational cost compared to previous work on video object segmentation. In other words, introducing MLLM brings remarkable understanding and reasoning ability to the model, while also inducing computational costs. Exploring methods to achieve trade-off between these aspects presents an interesting avenue for future research. Second, we observe that state-of-the-art approaches to video object segmentation often employ dedicated video backbones to enhance performance. Intuitively, using vision encoder pre-trained on videos would be beneficial for temporal-related tasks, such as object tracking. However, integrating video backbone while ensuring compatibility with LLM and SAM decoder is non-trivial. In this work, we focus on empowering video segmentation tasks with reasoning capabilities based on LLM. Exploring the integration of video backbone represents potential avenue for future research."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we propose VideoLISA, video-based LLM designed for language instructed reasoning segmentation in videos. It leverages the reasoning capabilities of LLM and employs SAM to produce segmentation masks. To address the unique challenges in marrying LLM with video object segmentation, we propose two key innovations. Firstly, Sparse Dense Sampling strategy is designed to enable LLM to capture and understand temporal dynamics in videos. By leveraging the inherent temporal redundancy property of videos, this strategy achieves delicate balance between preserving visual details and temporal context, making it favorable for video object segmentation tasks. Secondly, we propose One-Token-Seg-All approach to achieve temporally consistent segmentation masks in the promptable mask decoding paradigm. Based on dedicated investigation of the potential and challenges associated with using single unified prompt to segment video frames, we enhance this capability from both input information foundation and training objective perspectives. Extensive ablation studies have investigated the function and rationale of the design choices of two modules. Equipped with the two designs above, our VideoLISA model shows impressive capabilities in video object segmentation, particularly emphasizing complex reasoning, temporal understanding, and object tracking, as validated by our newly organized ReasonVOS benchmark. Furthermore, it demonstrates notable performance on image segmentation tasks, positioning it as potential unified model for language-instructed object segmentation."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. [2] Ali Athar, Jonathon Luiten, Paul Voigtlaender, Tarasha Khurana, Achal Dave, Bastian Leibe, and Deva Ramanan. Burst: benchmark for unifying object recognition, segmentation and tracking in video. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 16741683, 2023. [3] Zechen Bai, Yuta Nakashima, and Noa Garcia. Explain me the painting: Multi-topic knowledgeable art description generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 54225432, 2021. [4] Zechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, Zheng Zhang, and Mike Zheng Shou. Hallucination of multimodal large language models: survey. arXiv preprint arXiv:2404.18930, 2024. [5] Maksym Bekuzarov, Ariana Bermudez, Joon-Young Lee, and Hao Li. Xmem++: Production-level video segmentation from few annotated frames. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 635644, 2023. [6] Miriam Bellver, Carles Ventura, Carina Silberer, Ioannis Kazakos, Jordi Torres, and Xavier Giro-i Nieto. closer look at referring expressions for video object segmentation. Multimedia Tools and Applications, 82(3):44194438, 2023. [7] Luca Bertinetto, Jack Valmadre, Joao Henriques, Andrea Vedaldi, and Philip HS Torr. Fully-convolutional siamese networks for object tracking. In Computer VisionECCV 2016 Workshops: Amsterdam, The Netherlands, October 8-10 and 15-16, 2016, Proceedings, Part II 14, pages 850865. Springer, 2016. [8] Adam Botach, Evgenii Zheltonozhskii, and Chaim Baskin. End-to-end referring video object segmentation with multimodal transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 49854995, 2022. [9] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 12091218, 2018. [10] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llms referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023. [11] Xianjie Chen, Roozbeh Mottaghi, Xiaobai Liu, Sanja Fidler, Raquel Urtasun, and Alan Yuille. Detect what you can: Detecting and representing objects using holistic models and body parts. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 19711978, 2014. [12] Ho Kei Cheng and Alexander Schwing. Xmem: Long-term video object segmentation with an atkinsonshiffrin memory model. In European Conference on Computer Vision, pages 640658. Springer, 2022. [13] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advances in Neural Information Processing Systems, 36, 2024. [14] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, and Chen Change Loy. Mevis: large-scale benchmark for video segmentation with motion expressions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 26942703, 2023. [15] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, Philip HS Torr, and Song Bai. Mose: new dataset for video object segmentation in complex scenes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2022420234, 2023. [16] Henghui Ding, Chang Liu, Suchen Wang, and Xudong Jiang. Vision-language transformer and query generation for referring segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1632116330, 2021. [17] Henghui Ding, Chang Liu, Suchen Wang, and Xudong Jiang. Vision-language transformer and query generation for referring segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1632116330, 2021. [18] Zihan Ding, Tianrui Hui, Junshi Huang, Xiaoming Wei, Jizhong Han, and Si Liu. Language-bridged spatial-temporal interaction for referring video object segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 49644973, 2022. [19] Zihan Ding, Tianrui Hui, Shaofei Huang, Si Liu, Xuan Luo, Junshi Huang, and Xiaoming Wei. Progressive multimodal interaction network for referring video object segmentation. The 3rd Large-scale Video Object Segmentation Challenge, 8:6, 2021. 10 [20] Ke Fan, Zechen Bai, Tianjun Xiao, Tong He, Max Horn, Yanwei Fu, Francesco Locatello, and Zheng Zhang. Adaptive slot attention: Object discovery with dynamic slot number. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2306223071, 2024. [21] Ke Fan, Zechen Bai, Tianjun Xiao, Dominik Zietlow, Max Horn, Zixu Zhao, Carl-Johann Simon-Gabriel, Mike Zheng Shou, Francesco Locatello, Bernt Schiele, et al. Unsupervised open-vocabulary object localization in videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1374713755, 2023. [22] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6202 6211, 2019. [23] Kirill Gavrilyuk, Amir Ghodrati, Zhenyang Li, and Cees GM Snoek. Actor and action video segmentation from sentence. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 59585966, 2018. [24] Shuting He and Henghui Ding. Decoupling static and hierarchical motion perception for referring video segmentation. arXiv preprint arXiv:2404.03645, 2024. [25] De-An Huang, Shijia Liao, Subhashree Radhakrishnan, Hongxu Yin, Pavlo Molchanov, Zhiding Yu, and Jan Kautz. Lita: Language instructed temporal-localization assistant. arXiv preprint arXiv:2403.19046, 2024. [26] Peng Jin, Ryuichi Takanobu, Caiwan Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. arXiv preprint arXiv:2311.08046, 2023. [27] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 787798, 2014. [28] Anna Khoreva, Anna Rohrbach, and Bernt Schiele. Video object segmentation with language referring expressions. In Computer VisionACCV 2018: 14th Asian Conference on Computer Vision, Perth, Australia, December 26, 2018, Revised Selected Papers, Part IV 14, pages 123141. Springer, 2019. [29] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 40154026, 2023. [30] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. arXiv preprint arXiv:2308.00692, 2023. [31] Dezhuang Li, Ruoqi Li, Lijun Wang, Yifan Wang, Jinqing Qi, Lu Zhang, Ting Liu, Qingquan Xu, and Huchuan Lu. You only infer once: Cross-modal meta-transfer for referring video object segmentation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 12971305, 2022. [32] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023. [33] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. [34] Xiang Li, Jinglu Wang, Xiaohao Xu, Xiao Li, Bhiksha Raj, and Yan Lu. Robust referring video object segmentation with cyclic structural consensus. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2223622245, 2023. [35] Chen Liang, Yu Wu, Tianfei Zhou, Wenguan Wang, Zongxin Yang, Yunchao Wei, and Yi Yang. Rethinking cross-modal interaction from top-down perspective for referring video object segmentation. arXiv preprint arXiv:2106.01061, 2021. [36] Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan Zhao, Hang Zhang, Peizhao Zhang, Peter Vajda, and Diana Marculescu. Open-vocabulary semantic segmentation with mask-adapted clip. In CVPR, 2023. [37] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. [38] Chang Liu, Henghui Ding, and Xudong Jiang. Gres: Generalized referring expression segmentation. In CVPR, 2023. [39] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. [40] Si Liu, Tianrui Hui, Shaofei Huang, Yunchao Wei, Bo Li, and Guanbin Li. Cross-modal progressive comprehension for referring segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(9):47614775, 2021. 11 [41] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 32023211, 2022. [42] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [43] Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Liujuan Cao, Chenglin Wu, Cheng Deng, and Rongrong Ji. Multi-task collaborative network for joint referring expression comprehension and segmentation. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 1003410043, 2020. [44] Zhuoyan Luo, Yicheng Xiao, Yong Liu, Shuyan Li, Yitong Wang, Yansong Tang, Xiu Li, and Yujiu Yang. Soc: Semantic-assisted object cluster for referring video object segmentation. Advances in Neural Information Processing Systems, 36, 2024. [45] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. [46] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1120, 2016. [47] Bo Miao, Mohammed Bennamoun, Yongsheng Gao, and Ajmal Mian. Spectrum-guided multi-granularity In Proceedings of the IEEE/CVF International Conference on referring video object segmentation. Computer Vision, pages 920930, 2023. [48] Jiaxu Miao, Xiaohan Wang, Yu Wu, Wei Li, Xu Zhang, Yunchao Wei, and Yi Yang. Large-scale video In Proceedings of the IEEE/CVF Conference on panoptic segmentation in the wild: benchmark. Computer Vision and Pattern Recognition, pages 2103321043, 2022. [49] Shehan Munasinghe, Rusiru Thushara, Muhammad Maaz, Hanoona Abdul Rasheed, Salman Khan, Mubarak Shah, and Fahad Khan. Pg-video-llava: Pixel grounding large video-language models. arXiv preprint arXiv:2311.13435, 2023. [50] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. [51] Renjie Pi, Jiahui Gao, Shizhe Diao, Rui Pan, Hanze Dong, Jipeng Zhang, Lewei Yao, Jianhua Han, Hang Xu, and Lingpeng Kong Tong Zhang. Detgpt: Detect what you need via reasoning. arXiv preprint arXiv:2305.14167, 2023. [52] Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Marquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts and attributes of common objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 71417151, 2023. [53] Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad S. Khan. Llava++: Extending visual capabilities with llama-3 and phi-3, 2024. [54] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 35053506, 2020. [55] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang. Grounded sam: Assembling open-world models for diverse visual tasks, 2024. [56] Seonguk Seo, Joon-Young Lee, and Bohyung Han. Urvos: Unified referring video object segmentation network with large-scale benchmark. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XV 16, pages 208223. Springer, 2020. [57] Li Wang, Zechen Bai, Yonghua Zhang, and Hongtao Lu. Show, recall, and tell: Image captioning with recall mechanism. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 1217612183, 2020. [58] Zhaoqing Wang, Yu Lu, Qiang Li, Xunqiang Tao, Yandong Guo, Mingming Gong, and Tongliang Liu. Cris: Clip-driven referring image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1168611695, 2022. [59] Dongming Wu, Xingping Dong, Ling Shao, and Jianbing Shen. Multi-level representation learning with semantic alignment for referring video object segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 49965005, 2022. [60] Dongming Wu, Tiancai Wang, Yuang Zhang, Xiangyu Zhang, and Jianbing Shen. Onlinerefer: simple online baseline for referring video object segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 27612770, 2023. 12 [61] Jiannan Wu, Yi Jiang, Peize Sun, Zehuan Yuan, and Ping Luo. Language as queries for referring video In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern object segmentation. Recognition, pages 49744984, 2022. [62] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. Pllava: Parameter-free llava extension from images to videos for video dense captioning. arXiv preprint arXiv:2404.16994, 2024. [63] Linjie Yang, Yuchen Fan, and Ning Xu. The 2nd large-scale video object segmentation challenge - video object segmentation track, October 2019. [64] Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, Hengshuang Zhao, and Philip HS Torr. Lavt: Languageaware vision transformer for referring image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1815518165, 2022. [65] Rui Yao, Guosheng Lin, Shixiong Xia, Jiaqi Zhao, and Yong Zhou. Video object segmentation and tracking: survey. ACM Transactions on Intelligent Systems and Technology (TIST), 11(4):147, 2020. [66] Linwei Ye, Mrigank Rochan, Zhi Liu, and Yang Wang. Cross-modal self-attention network for referring In Proceedings of the IEEE/CVF conference on computer vision and pattern image segmentation. recognition, pages 1050210511, 2019. [67] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models. arXiv preprint arXiv:2306.13549, 2023. [68] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. [69] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, and Ping Luo. Gpt4roi: Instruction tuning large language model on region-of-interest. arXiv preprint arXiv:2307.03601, 2023. [70] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 633641, 2017. [71] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. [72] Jiawen Zhu, Zhi-Qi Cheng, Jun-Yan He, Chenyang Li, Bin Luo, Huchuan Lu, Yifeng Geng, and Xuansong Xie. Tracking with human-intent reasoning. arXiv preprint arXiv:2312.17448, 2023. [73] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang, Lu Yuan, et al. Generalized decoding for pixel, image, and language. In CVPR, 2023. [74] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Gao, and Yong Jae Lee. Segment everything everywhere all at once. arXiv:2304.06718, 2023."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Evaluation on Image Segmentation In this section, we evaluate our VideoLISA model on the referring image segmentation task with three widely adopted benchmarks. The results are presented in Tab. 7. On the refCOCO and refCOCO+ benchmarks, our VideoLISA achieves comparable performance with the image-based LISA model. On the refCOCOg benchmark, VideoLISA outperforms previous methods, achieving state-of-the-art performance. In general, the results of this experiment, along with the image reasoning segmentation results shown in the main paper, effectively demonstrate that our VideoLISA model is strong competitor in image segmentation tasks. Table 7: Referring segmentation results (cIoU) among ours and existing methods. Method MCN [43] VLT [17] CRIS [58] LAVT [64] ReLA [38] X-Decoder [73] SEEM [74] LISA-7B [30] VideoLISA-3.8B (Ours) refCOCO testA 64.2 70.5 73.2 75.8 76.5 - - 76.5 76. testB 59.7 65.2 66.1 68.8 70.2 - - 71.1 68.8 val 62.4 67.5 70.5 72.7 73.8 - - 74.1 73.8 refCOCO+ testA 55.0 61.0 68.1 68.4 71.0 - - 67.4 68.8 testB 44.7 50.1 53.7 55.1 57.7 - - 56.5 56.2 val 50.6 56.3 62.3 62.1 66.0 - - 62.4 63.4 refCOCOg val(U) 49.2 55.0 59.9 61.2 65.0 64.6 65.7 66.4 68.3 test(U) 49.4 57.7 60.4 62.1 66.0 - - 68.5 68.8 A.2 Ablation Studies In this section, we present ablation studies on the temporal learning module (the Sparse Dense Sampling strategy), the temporal mask association module (the One-Token-Seg-All approach), and the training data recipe. For fair comparisons, unless specified, all VideoLISA variants are uniformly trained with the same training setting: 1) 3k iterations in total, 2) the same training data recipe, 3) the same learning rate scheduler, and 4) the same training objective. Three benchmarks are used for analysis: 1) ReasonSeg [30] evaluates the reasoning ability of the model; 2) MeViS [14] reflects the models performance on temporal learning; and 3) Ref-DAVIS-17 [28] measures the general RVOS capability of the model. For evaluation on video benchmarks, the performance metrics of VideoLISA are computed using the simple One-Token-Seg-All approach without post-optimization, revealing the models essential capabilities. A.2.1 Temporal Learning Module In Tab 8, we compare various strategies for temporal learning. The first row shows the vanilla LISA-7B model, which only focuses on image-based reasoning segmentation. To infer LISA-7B on video data, we employ similar One-Token-Seg-All strategy, where the <TRK> token (called [SEG] in the original LISA) comes from the first frame. This performance serves as baseline for comparison. In the second row, we construct naive solution to adapt LISA to the video domain. Specifically, we finetune LISA-7B on the aforementioned video segmentation datasets. The results show that simply finetuning on video data does not significantly improve video performance and even hurts the performance on image reasoning segmentation. Although training on video datasets may enhance the models ability to understand temporally related text queries, it still lacks temporal modeling ability from video data, resulting in undesirable performance. Next, we compare various temporal learning strategies within the VideoLISA framework using the One-Token-Seg-All training objective. We first experiment with straightforward video training strategy, called n-frame, which directly concatenates the visual features from sampled frames as input to the large language model. In our implementation, the value of is set to the same as Tdense for comparison. As shown in the third row, we observe that with this simple strategy, the model achieves surprisingly good performance across the benchmarks, significantly outperforming LISA-based methods. Exposure to multiple frames enables the model to perceive temporal dynamics, 14 Table 8: Ablation study on the temporal modeling architecture. *LISA-7B is reproduced using the released codebase. Method ReasonSeg (val) giou ciou MeViS (valid_u) &F LISA-7B* (Baseline) LISA-7B* (Vid. FT) VideoLISA-3.8B (n-frame) VideoLISA-3.8B (Spatial & Temporal Pooling [45]) VideoLISA-3.8B (Slow-Fast Pooling [25]) VideoLISA-3.8B (Sparse Dense Sampling) 51.7 48.6 55.6 56.0 54.0 58.9 56.7 56.2 60.8 59.9 54.4 60. 43.2 44.8 49.9 50.8 50.2 51.7 39.9 41.1 46.7 47.8 47.2 48.4 46.5 48.6 53.0 53.8 53.1 54.9 Ref-DAVIS-17 &F 58.8 58.5 65.5 62.2 65.7 67.8 55.1 54.6 62.2 58.4 62.1 64.3 62.5 62.5 68.9 66.3 69.4 71.3 Table 9: Ablation study on the mask association i.e., tracking architecture. *LISA-7B is reproduced using the released codebase."
        },
        {
            "title": "Method",
            "content": "MeViS (valid_u) &F LISA-7B* (Baseline) LISA-7B* + XMem[12] VideoLISA-3.8B (One-Token-Seg-One) VideoLISA-3.8B (One-Token-Seg-All) VideoLISA-3.8B (Post optimization) 43.2 45.6 46.1 51.7 54.5 39.9 41.9 42.4 48.4 50. 46.5 49.3 49.8 54.9 58.1 Ref-DAVIS-17 &F 58.8 62.7 60.2 67.8 68.7 55.1 60.0 56.5 64.3 65. 62.5 65.5 63.8 71.3 72.0 while the One-Token-Seg-All training objective supervises the model in learning mask association over the temporal dimension, thereby improving multimodal reasoning and temporal consistency in segmentation. However, due to computational limits, it is prohibitive to include too many frames as it would result in large number of tokens. To enable long temporal context perception, we experiment with several pooling strategies, including pooling along the spatial and temporal dimensions separately [45], pooling with different strengths in slow-fast pace [22, 25], and our Sparse Dense Sampling strategy. The comparison in Tab. 8 reveals that our Sparse Dense Sampling strategy is more favorable setting among the experiment designs. The first spatial-temporal pooling strategy eliminates valuable visual details of the video, resulting in inferior performance. The second slow-fast paced pooling strategy is similar to ours in implementation. The key difference is that it applies pooling to all frames, albeit with different strengths, while ours preserves the full visual details of the dense frames. This difference leads to the observed performance gap. We argue that this difference is significant due to the unique nature of the video object segmentation task. On one hand, it requires detailed visual information for accurate segmentation, causing pooling-based strategies to yield inferior results. On the other hand, the temporal dimension is also necessary for the model to comprehend motions and behaviors, as validated by the comparison between the n-frame approach and ours. Although recent studies [62] show that applying pooling to visual tokens does not affect the performance of VQA tasks, our experiments validate that preserving the full resolution of visual tokens is necessary for dense prediction tasks, and applying pooling leads to sub-optimal results. A.2.2 Temporal Association Module In Tab. 9, we compare the design choices for the temporal association module, i.e., tracking. As in previous comparisons, the One-Token-Seg-All strategy in LISA-7B serves as the baseline in the first row. One straightforward solution based on LISA is to plug an off-the-shelf tracker into the model. During inference, LISA outputs the segmentation mask of the first frame based on language instruction. The tracker then tracks the segmented object through the video, yielding segmentation masks for the subsequent frames. Specifically, we adopt the popular XMem [12] model as the tracker, as shown in the second row of the table. Compared to VideoLISA (both One-Token-Seg-All and post-optimization), LISA+XMem achieves worse performance on these benchmarks. This validates that simply plugging an existing tracker into an image-based reasoning segmentation model does not address the problem of video reasoning segmentation. The vital issue is that the LLM in charge of perception and reasoning does not capture the entire video content, making its predictions nonsensical. In contrast, VideoLISAs temporal learning module and dedicated training objective enrich the <TRK> token with semantic information, enabling it to find the target object across all frames. Image Seg. Video Seg. Training Data Table 10: Ablation study on the training data recipe. MeViS (valid_u) ReasonSeg (val) giou Image QA Video QA &F ciou 57.2 41.4 58.9 56.0 60.6 60.0 46.5 60.0 65.6 67.4 46.0 49.3 51.7 49.8 52.0 43.3 45.8 48.4 46.8 49. 48.6 52.8 54.9 52.9 54.8 Ref-DAVIS-17 &F 62.6 66.0 67.8 66.8 66.9 58.9 62.7 64.3 63.4 63. 66.3 69.3 71.3 70.3 70.3 To quantify the effect of the One-Token-Seg-All training objective, we build strawman setting named One-Token-Seg-One. In this setting, the video content is captured with the temporal learning module, but the training only supervises the segmentation of one frame. The comparison is shown in the third and fourth rows of Tab. 9. We observe that the slight difference in supervision leads to significant performance gap in the benchmarks. This indicates that the One-Token-Seg-All training objective is essential for achieving temporally consistent masks. In the last row, we present post-optimization, which leverages both the reasoning and segmentation abilities of VideoLISA and mature tracking model. Specifically, we first use VideoLISA to produce the <TRK> token and then use it to segment the sampled dense frames. Then, the post-optimization model, implemented as XMem++ [5], takes dense frames and their segmentation masks as references in its permanent memory and infers the masks for the remaining frames. The reasons for choosing the dense frames as the mask reference include: 1) the dense frames are seen by VideoLISA, thus their masks should be more accurate than those of other unseen frames, and 2) the dense frames are intentionally sampled from the video in uniform manner, naturally providing long-range yet diverse reference signal. By leveraging the association ability from the post-optimization step, VideoLISA achieves the best performance. A.2.3 Ablation on Training Data Our model undergoes joint training on both image and video datasets. An investigation of the training data is presented in Tab. 10. We first observe that with image-only segmentation datasets, the model achieves decent performance in reasoning segmentation. However, the performance on video benchmarks is unsatisfactory, possibly due to insufficient temporal information in the training data. When using video-only segmentation settings, compared to image-only, the performance on video benchmarks increases significantly. Simultaneously, the model experiences dramatic drop in performance in reasoning segmentation. This comparison demonstrates that video training is helpful for the VOS task, while image data is also necessary to exploit the reasoning ability of the model. When combining the image and video segmentation datasets, the model yields remarkable performance across various benchmarks. Next, we additionally explore the effect of using visual question answering (VQA) data. We first observe that after adding Image-QA data into training, the model experiences slight performance drop in all benchmarks. Then, with the involvement of Video-QA data, the model achieves much better performance on the reasoning segmentation benchmark. Among the two video benchmarks, compared to the model trained with segmentation-only data, this model shows slightly better performance on the MeViS offline validation set yet worse performance on Ref-DAVIS-17. Intuitively, VQA data has the potential to enhance the models reasoning ability. However, it may also make the multi-task training more challenging, as revealed by the performance fluctuation among different benchmarks. Maintaining the compatibility of different types of training data and tasks is left for future work. A.3 Qualitative Results In Fig. 3, we use representative video to showcase the versatile language-instructed reasoning capabilities of our model. VideoLISA can do segmentation in videos via language referring, world knowledge reasoning, and video temporal reasoning. Additionally, the model can discern subtle differences in language instructions and is not biased to salient or moving objects. In Fig. 6 and Fig. 7, we provide more abundant qualitative examples of VideoLISA. The red text is only for illustration purposes. No special prompting techniques were employed. Its important to note that these examples were generated using the One-Token-Seg-All inference approach without post-optimization. 16 Figure 3: VideoLISA is capable model on video object segmentation with versatile languageinstructed reasoning abilities. Beyond basic language referring, it enables complex reasoning by leveraging world knowledge and videos temporal dynamics. Figure 4: Failure cases of VideoLISA. 17 Figure 5: ReasonVOS benchmark. The left part shows the statistics of data samples. The right part shows the source of the videos. A.4 Failure Cases To understand the limitations and capability boundaries of our method, we analyze several failure cases as shown in Fig. 4. In the first example, the video shows car crashing into grocery store. We prompt the model to find the unusual object that interrupts the peace of the scene. Although we try to rephrase the prompt in various ways, the model consistently outputs the object in the bottom left corner. We hypothesize that the issue stems from the inherent hallucination of the MLLM, which recognizes the object as stove, telephone pole, or something else. In the second example, we ask the model to find the kid who loses the game We humans have the background knowledge to determine the match result. However, it seems like this game is beyond the knowledge scope of the MLLM, causing it to segment the wrong person. Consequently, we provide some background information about the game rules in the text prompt and then ask the same question. As shown in the third example of Fig. 4, with this cue, the model is able to segment the correct person. These examples demonstrate that the reasoning capabilities of VideoLISA are bounded by the multimodal large language model behind it, yet this can be alleviated by prompt engineering techniques. The third example also exhibits low-quality segmentation masks in certain frames, leaving room for future improvements."
        },
        {
            "title": "B Benchmark",
            "content": "We show the data statistics of our ReasonVOS benchmark in Fig. 5. We select videos and mask annotations from various sources and annotate additional text descriptions. In total, ReasonVOS consists of 91 videos. We manually annotate 105 video-instruction-mask samples as seed data and use Claude 3 API to augment the data into 458 samples. We further categorize the text descriptions into short query and long query. Short queries are descriptions of specific objects, usually in the format of attributive clauses. Long queries are instructions that require reasoning, usually in the format of full sentence."
        },
        {
            "title": "C Broader Impact",
            "content": "The development of our reasoning-based video segmentation model holds significant potential for transforming variety of fields by enhancing the ability to analyze and interpret video content. In the realm of surveillance, this technology can improve security measures by accurately identifying and tracking suspicious behavior, thereby preventing potential threats. In educational settings, the model can assist teachers in identifying and addressing student engagement patterns, fostering more responsive learning environment. For healthcare, our model can be applied to monitor patient activities, supporting early intervention and personalized care strategies. Additionally, in everyday scenarios, such as pet care or home organization, this technology can assist individuals in making informed decisions quickly and efficiently. By leveraging advanced reasoning capabilities, our model not only advances the field of computer vision but also provides practical solutions that enhance safety, learning, health, and daily life. However, it is crucial to consider ethical implications, such as privacy concerns and the potential for misuse, ensuring that these technologies are implemented responsibly and equitably. 18 Figure 6: More qualitative examples of VideoLISA. 19 Figure 7: More qualitative examples of VideoLISA."
        }
    ],
    "affiliations": [
        "Amazon",
        "Show Lab, National University of Singapore"
    ]
}