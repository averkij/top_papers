{
    "paper_title": "Beyond Decoder-only: Large Language Models Can be Good Encoders for Machine Translation",
    "authors": [
        "Yingfeng Luo",
        "Tong Zheng",
        "Yongyu Mu",
        "Bei Li",
        "Qinghong Zhang",
        "Yongqi Gao",
        "Ziqiang Xu",
        "Peinan Feng",
        "Xiaoqian Liu",
        "Tong Xiao",
        "Jingbo Zhu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The field of neural machine translation (NMT) has changed with the advent of large language models (LLMs). Much of the recent emphasis in natural language processing (NLP) has been on modeling machine translation and many other problems using a single pre-trained Transformer decoder, while encoder-decoder architectures, which were the standard in earlier NMT models, have received relatively less attention. In this paper, we explore translation models that are universal, efficient, and easy to optimize, by marrying the world of LLMs with the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder unchanged. We also develop methods for adapting LLMs to work better with the NMT decoder. Furthermore, we construct a new dataset involving multiple tasks to assess how well the machine translation system generalizes across various tasks. Evaluations on the WMT and our datasets show that results using our method match or surpass a range of baselines in terms of translation quality, but achieve $2.4 \\sim 6.5 \\times$ inference speedups and a $75\\%$ reduction in the memory footprint of the KV cache. It also demonstrates strong generalization across a variety of translation-related tasks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 4 9 5 6 0 . 3 0 5 2 : r Preprint. Under review. Beyond Decoder-only: Large Language Models Can be Good Encoders for Machine Translation Yingfeng Luo, Tong Zheng, Yongyu Mu, Bei Li, Qinghong Zhang, Yongqi Gao Ziqiang Xu, Peinan Feng, Xiaoqian Liu, Tong Xiao, Jingbo Zhu NLP Lab, Northeastern University, Shenyang, China NiuTrans Research, Shenyang, China luoyingfeng_neu@outlook.com {xiaotong,zhujingbo}@mail.neu.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "The field of neural machine translation (NMT) has changed with the advent of large language models (LLMs). Much of the recent emphasis in natural language processing (NLP) has been on modeling machine translation and many other problems using single pre-trained Transformer decoder, while encoder-decoder architectures, which were the standard in earlier NMT models, have received relatively less attention. In this paper, we explore translation models that are universal, efficient, and easy to optimize, by marrying the world of LLMs with the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder unchanged. We also develop methods for adapting LLMs to work better with the NMT decoder. Furthermore, we construct new dataset involving multiple tasks to assess how well the machine translation system generalizes across various tasks. Evaluations on the WMT and our datasets show that results using our method match or surpass range of baselines in terms of translation quality, but achieve inference speedups and 75% reduction in the memory footprint 2.4 of the KV cache. It also demonstrates strong generalization across variety of translation-related tasks. 6.5 NiuTrans/LaMaTE"
        },
        {
            "title": "Introduction",
            "content": "The last decade has yielded remarkable breakthroughs in machine translation (MT) through the use of deep neural networks, scaled dramatically in both model parameters and training data. During this period, early methods, known as neural machine translation (NMT), were largely based on the encoder-decoder architecture (Sutskever et al., 2014; Bahdanau et al., 2015). In NMT, the machine translation problem is commonly treated as sequence-to-sequence task, where the input sequence is first encoded into an intermediate representation, and the output sequence is then generated based on this representation, as illustrated in Figure 1 (a). Models of this kind are typically trained on bilingual text in supervised manner, and their inference on modern GPUs is efficient. However, like their predecessors in past decades (e.g., statistical machine translation models), NMT models are generally developed for specific tasks, such as translation in specific genre or domain. Everything changed in NLP, with the success of large language models (LLMs) through large-scale self-supervised learning (Brown et al., 2020). In the LLM paradigm, the translation problem is framed as the token prediction problem in language modeling, as illustrated in Figure 1 (b). Such an approach greatly simplifies the modeling. We can now pre-train the LLM on large amounts of text via self-supervision so as to produce single model that can be fine-tuned and prompted for various translation-related tasks, such as constrained translation and post-editing. Nevertheless, as consequence of using large-scale neural networks, LLMs are computationally expensive and pose challenges for applications requiring low latency and small memory footprint. Corresponding author 1 Preprint. Under review. Figure 1: Architectures of machine translation models. In standard NMT models, an encoder is used to encode the source-language sequence x, and decoder is used to generate the target-language sequence from left to right. In LLMs, the decoder-only architecture is adopted. Both and y, along with the prompt c, are represented as single sequence, which is processed by large decoding network. In the LaMaTE model, an LLM serves as the encoder. The output of the LLM is transformed into the input to the NMT decoder through an adaptor. The NMT decoder then generates the target-language sequence as usual. The fundamental change brought by LLMs has led most of the field to focus on replacing previous NLP systems with LLMs. However, machine translation researchers might still expect that system could be efficient and easy to optimize, and, at the same time, could generalize across various sorts of tasks. In this paper, we examine how the world of LLMs can be married with the world of NMT, thereby benefiting from both paradigms. One simple approach is to use an LLM as the encoder in NMT (call it LaMaTE Large Language Models as Machine Translation Encoders). As both the encoder and decoder are essentially based on language models, we can view the whole system as single language model. The decoupling of encoding and decoding from language modeling confers great modeling flexibility. direct result of this is that we can employ deep, complex encoder and lightweight decoder, as illustrated in Figure 1 (c). This heterogeneous architecture is particularly well-suited for machine translation, where we can use powerful encoder to understand the input text, while generating high-quality translations at lower decoding cost. In order to examine to what extent the model can generalize, we develop new benchmark, called the Comprehensive Machine Translation benchmark (ComMT). It consists of several diverse tasks that evaluate different aspects of translation model. While there have been datasets developed for fine-tuning LLMs for multiple translation tasks (Alves et al., 2024), our focus is on broader range of application scenarios. We hope that such benchmark can be adopted to provide systematic evaluation of machine translation systems, and that this, in turn, will encourage practitioners to pay more attention to the issue of generalization when developing these systems. We conduct extensive experiments and evaluate various models including NMT, LLMs, and our LaMaTE method. Our results show that the LaMaTE model achieves comparable or better performance than range of baseline systems on several tasks, but runs 2.4 6.5 times faster and reduces the memory footprint of the KV cache by 75%. Evaluations on the ComMT dataset also demonstrate the strong generalization capabilities of the LaMaTE model, showing significant improvements over baseline systems. These results are not surprising, but intriguing, as findings in NMT remain applicable in the era of LLMs, for instance, scaling up the encoding network is still beneficial for machine translation tasks. This suggests an interesting direction for future work, where we could develop powerful yet efficient system by using strong model for language understanding and lightweight model for language generation. Preprint. Under review."
        },
        {
            "title": "2 Related Work",
            "content": "Designing widely applicable models for MT has been an active area of NLP research for decades. Although MT models have evolved significantly over time, most of them still operate on an analyzethen-generate paradigm (Brown et al., 1993; Koehn et al., 2003; Bahdanau et al., 2015). For example, in statistical MT, the source-language sentence is parsed into either syntactic or non-syntactic forms, and the translation is generated by mapping these parsed forms to target-language constructions (Chiang, 2005). LLMs can broadly be categorized as following similar design: they first compute the key-value cache for each input sequence, and then produce output tokens in left-to-right manner based on this cache. From modeling perspective, therefore, decoupling the encoding and decoding processes is natural design choice for both traditional MT and LLMs. In NLP, large body of work has focused on pre-training and applying text encoders, such as the BERT series of models (Devlin et al., 2019), which are primarily designed to address language understanding problems. More recently, there have been attempts to use LLMs as text encoders, though these models are more commonly used for generating text. For example, BehnamGhader et al. (2024) and Muennighoff et al. (2024) fine-tuned LLMs for text encoding and showed that LLMs can produce high-quality representations of text in various embedding tasks. However, it is rare to see studies on incorporating LLMs in the encoding for NMT or other language generation tasks. In fact, the focus of research in MT has shifted. Recent studies in this field are now more concerned with the adaptation of LLMs, rather than the improvement of model architectures (Li et al., 2024; Zheng et al., 2024). One strand of research aims to enable LLMs to translate via prompting techniques, including designing better prompts (Zhang et al., 2023a; Zhu et al., 2024), introducing translation demonstrations (Mu et al., 2023; Chitale et al., 2024), and using chain-of-thought reasoning (Lu et al., 2024). Another strand focuses on developing MT-specific LLMs via fine-tuning (Xu et al., 2023; Yang et al., 2023b; Alves et al., 2024; Guo et al., 2024; Zheng et al., 2025); we have followed this approach, but with focus on improving the model architecture, as well as developing new benchmark for evaluating universal MT models. While LLMs have significantly enhanced MT with their superior versatility and generalization capabilities, much of the current research continues to focus on single-task contexts, which do not fully utilize the potential of LLMs to address diverse translation challenges. The research highlighted in Alves et al. (2024) seeks to develop more universal model by integrating mix of general and translation-related tasks. Yet, despite these advancements, there remains notable gap in the availability of comprehensive and high-quality data resources specifically tailored to the development of universal translation models. Addressing this deficiency would maximize LLM capabilities and broaden their applicability across variety of translation-related tasks. This work is also related to efficient methods for LLMs. For example, one can compress the model using quantization and pruning techniques (Xiao et al., 2023; Ma et al., 2023), and speed-up inference using speculative decoding algorithms (Leviathan et al., 2023; Kim et al., 2023; Hu et al., 2025). But these methods and ours are in no way contradictory. Since our model follows standard encoding (or prefilling) and decoding frameworks, it can be easily combined with various efficient methods to further improve efficiency."
        },
        {
            "title": "3 LaMaTE",
            "content": "In this section, we introduce LaMaTE and its training method. 3.1 Model Architecture We begin by outlining the basic concepts and notation needed for our description. There are three networks that we consider here, as illustrated in Figure 2. NMT Encoder Enc(x). It is standard Transformer encoder, consisting of an embedding layer and number of stacked Transformer layers. The output of the NMT encoder is bidirectional representation of the input token sequence x, denoted by H1. 1H is sequence of vectors, each corresponding to the representation at each position of x. Preprint. Under review. Figure 2: Architecture of the NMT Encoder, NMT Decoder, and LLM Decoder. We omit the layer normalization and residual connections for simplicity. NMT Decoder Dec(H, y). It shares the same architecture as the NMT encoder, with an additional cross-attention sublayer added in each Transformer layer. The NMT decoder accepts two inputs one from the encoder for cross-attention, and another as the regular sequential input. In the self-attention mechanism, each position can only attend to the preceding positions. So its output representation is unidirectional. Typically, Softmax layer is added on top of the decoder to generate distributions of tokens. LLM Decoder Dec( , [c, x, y]). It operates on token sequences only, without the need for input from the encoder. Thus, the cross-attention sublayers are removed, and the first parameter of the above function is left blank. Here [c, x, y] denotes the concatenation of the prompt c, the input sequence x, and the translation y. For NMT models, we can simply connect the NMT encoder and decoder together. The probability of token prediction can be expressed as Pr(yi x, y<i) = Softmax(WS)i = Dec(H, y<i) = Enc(x) (1) (2) (3) where yi denotes the target-language token at position i, y<i denotes the target-language tokens that precede position i, and Softmax(WS)i denotes the Softmax function that computes the distribution of tokens at position i. WS maps the decoder output to representation space of the vocabulary size using the linear mapping matrix W. The LaMaTE model follows this encode-decode architecture, but with an LLM decoder replacing the NMT encoder, given by = Decϕ(H, y<i) , [c, x]) = Decθ( (4) (5) Here we use the subscripts ϕ and θ to denote the parameters for the NMT decoder and the LLM decoder, respectively. The function Decθ( , [c, x]) serves as unidirectional encoder. Although it is generally thought that bidirectional encoders can make better use of contextual information, our experiments in Section 6.4 demonstrate that using such unidirectional encoders based on LLMs is also very effective. Given that both the encoder and decoder are unidirectional, we can roughly think of the whole model as hybrid language model: large, powerful model is used for prefilling, and small, efficient model is used for decoding. The use of pre-trained LLMs for encoding allows the model to have stronger understanding of the input sequence, which has been found to be very beneficial in NMT systems (Dou et al., 2018; Wang et al., 2019). As another bonus, one can easily adapt the model via fine-tuning and prompting, thereby deploying single such system for many different tasks. For example, by giving an appropriate prompt c, we can guide the system to produce outputs following specific needs or contexts. This gives LaMaTE big advantage over traditional NMT models. Preprint. Under review. Figure 3: The architecture of LaMaTE, where the Adaptor consists of three components: Fusion combines the representations of layer groups gk, MLP reduces the representationss dimensionality, and EncStack learns bidirectional representations. The training process consists of two stages: the first stage trains the Adaptor and Decoder, and the second stage trains all model parameters. One problem with the model described in Eqs. (4-5) is that the LLM decoder may have larger hidden size than that of the NMT decoder. In this case, we add an adaptor to the output of the LLM decoder to reduce its dimensionality, so that it matches the NMT decoder. We can then redefine as (6) = Fω(Decθ( , [c, x])) where Fω( ) is the adaptor with the parameters ω. 3.2 Adaptor One simple form of adaptors is linear mapping of the output of the LLM decoder, that is, we linearly transform representations from Decθ( ). But we find that this method falls short of our expectations because LLMs are typically too big that simply transforming their output representations into lower-dimensional ones makes it difficult for untrained smaller models to use them directly. ) to lower-dimensional representations required by Decϕ( Instead, we design an adaptor that makes better use of the LLM outputs for the NMT decoder. It involves three components. The complete model architecture is illustrated in Figure 3. Instead of considering only the output of the final layer of the LLM, we fuse the outputs of the intermediate layers to form finer-grained output of the LLM decoder. Such methods have been found to be very beneficial to deep-to-shallow architectures in NMT (Wang et al., 2019; Yang et al., 2023a). To do this, we divide the LLM layers into groups, and fuse their outputs in the following form Hfuse = LayerNorm 1 k=1 (cid:32) wkgk (cid:33) (7) where LayerNorm( ) is the layer normalization function, gk is the output of the k-th layer group, and wk is the learnable weight for gk. We use the last layers hidden states of each group as that groups hidden states. We place 2-layer MLP after layer fusion to reduce the hidden size of Hfuse (denoted by d1) to the hidden size of the NMT decoder (denoted by d2). This network is given by where GELU( linear mapping matrices. ) is the activation function, and W1 d2 and W2 Hmlp = GELU(HfuseW1)W2 Rd1 (8) Rd2 d2 are the It is also possible to learn bidirectional representations from unidirectional representations. common method is to incorporate some Transformer encoder layers. Thus the self-attention models can help generate bidirectional representations. The final output of the adaptor is then given by = EncStack(Hmlp) (9) 5 Preprint. Under review. ) is stack of Transformer encoder layers. Note that EncStack( where EncStack( ) consists of only few layers, and thus adds very small computational overhead. This step of bidirectional representation learning is optional, and one can choose whether to adopt it in practice."
        },
        {
            "title": "3.3 Model Training",
            "content": "The LaMaTE model has three sets of parameters: θ for the LLM decoder, ϕ for the NMT decoder, and ω for the adaptor. Here, θ is initialized with the pre-trained parameters of the LLM, while ϕ and ω need to be trained from scratch. Optimizing these parameters on bitext seems straightforward, but poses practical challenges. Since fine-tuning pre-trained LLMs is costly, using less labeled data is generally favorable. However, we found that this approach can lead to inadequate learning of the new parameters ϕ and ω. On the other hand, extensive fine-tuning may cause the LLM to forget its original knowledge encoded in θ. Similar findings have been reported in the literature (Xu et al., 2023). Here, we present two-stage training method that performs more efficient learning for different types of parameters. In the first stage, we freeze θ and pre-train both ϕ and ω. The task of pre-training is standard translation task. We train the model to translate source-language sequences to corresponding target-language sequences. Therefore, the adaptor and the NMT decoder can learn to map from source-language representations to translations. Since the parameters of the LLM decoder are frozen, this process requires only the forward pass of this large network, without the need for the backward pass. It is thus less computationally expensive than LLM fine-tuning, making it possible to scale up the pre-training to large datasets. The second stage is fine-tuning stage, where θ, ϕ, and ω are fine-tuned together on various tasks. For each task, as with instruction fine-tuning for LLMs, we provide LaMaTE with task-specific instructions, inputs and outputs, and optimize all the parameters end-to-end. In this way, the model is adapted to follow these instructions and can be deployed as single, universal model to handle variety of translation-related problems. We use our ComMT dataset for fine-tuning in this work."
        },
        {
            "title": "4 ComMT",
            "content": "4.1 Tasks Many translation-related tasks, such as document-level translation, rely on both understanding complex inputs and generating coherent, contextually appropriate outputs, which are not captured by commonly used sentence-level translation tasks. To generalize LaMaTE to diverse tasks and evaluate it on these tasks, we developed new evaluation benchmark called ComMT. It comprises five tasks, with examples for each provided in Table 16: General Translation. This is standard sentence in, sentence out task in general domains, serving as the foundation for more specialized tasks. Document-level Translation. This task extends translation from sentence-level to documentlevel, focusing on maintaining coherence and context in extended texts, rather than merely achieving sentence-level accuracy. Our document-level translation tasks mainly involve general long texts, such as news articles. Domain Translation. This task focuses on domain-specific translation, ensuring the accurate use of domain-specific terminology and expressions. Based on the application scenario and existing research, we identified five initial domains for study: Medical, Law, Information Technology(IT), Colloquial, and Literature. Terminology-constrained Translation. In this task, the system is required to produce translations that follow the given terminology translation requirements. Automatic Post-editing. This task focuses on enhancing the quality of preliminary machine translation outputs by automatically correcting errors in grammar, spelling, and style in the initial translations. 6 Preprint. Under review. Figure 4: Our comprehensive translation dataset, ComMT, includes diverse translation-related tasks. The table presents the training set statistics for ComMT. De Ru Zh Cs Task General Translation Doc-level translation Domain Translation Subtask WMT23 WMT22 FLORES-200 - Medical Law IT Colloquial Literature Terminology-constrained Translation Automatic Post-edition - - De2En En2De Ru2En En2Ru Zh2En En2Zh Cs2En En2Cs 2074 2037 1012 519 - 959 519 - - 2898 - 557 2037 1012 547 744 969 - 595 - 775 1000 2074 2037 1012 500 1772 - 650 507 - 500 1023 1976 1875 - 415 580 - - 520 401 1638 - 549 1984 - 500 748 - - 517 506 2948 1980 2074 2037 1012 500 2094 - - 652 550 798 1723 2016 - - 513 - - 564 506 - 526 - 1448 - 500 999 - - - - - - Table 1: Statistics for the ComMT Test Set. We ensure that the source language of the test set is original, so not all language directions and tasks have corresponding data due to the difficulty in collecting certain datasets. We collected data mainly from public resources, driven by two goals: (i) to find as many data sources as possible in order to increase data diversity, and (ii) to collect as much high-quality, manually annotated translation data as possible. ComMT is multilingual and supports four languages: German, Czech, Russian, and Chinese. The reader can refer to Appendix Section for more process details of ComMT. Ultimately, we created training set of 239k samples, as shown in Figure 4. Additionally, we also created test set, detailed in Table 1. For the General Translation task, we used established test sets, while for other tasks, we carefully curated test data by reorganizing existing datasets. We ensured that the source language in each case was original to avoid translationese issues (Graham et al., 2020; Läubli et al., 2020). However, some languages currently lack test sets because obtaining the task data is challenging. In the future, we plan to continue expanding this dataset to support more languages and tasks. Preprint. Under review. Figure 5: Comparison of performance across three datasetsWMT17-20, TowerBlock, and ComMT fine-tuned on Llama3-8B and evaluated on the WMT23 test set. 4.2 Quality Verification To verify the quality of our dataset, we fine-tuned the Llama3-8B model (Dubey et al., 2024) and compared it against two other datasets commonly used in LLM-based MT research: (i) merged news test set from WMT17 to WMT20 with 61k samples, widely adopted data setting in previous studies (Xu et al., 2023; Jiao et al., 2023; Guo et al., 2024); (ii) the TowerBlock dataset (Alves et al., 2024), which comprises 638k samples across translation tasks, named entity recognition (NER), general dialogue, and other tasks. We evaluated the resulting models on the WMT23 test set, with the results shown in Figure 5. The results clearly demonstrate that ComMT surpasses WMT17WMT20 in both the En and En directions. However, while it excels in the En direction, it underperforms in the En direction compared to TowerBlock. This discrepancy may be attributed to TowerBlocks larger dataset size and its inclusion of additional general task data, predominantly in English, which may specifically enhance performance when translating into English. We intend to investigate this further in future research. Overall, ComMT exhibits significant diversity and broad applicability, providing well-curated data resource for developing and evaluating universal translation models."
        },
        {
            "title": "5 Experimental Setup",
            "content": "5.1 Data and Evaluation Metrics We trained our model on multilingual translations between English (En) and four languages: German (De), Czech (Cs), Russian (Ru), and Chinese (Zh), resulting in total of eight translation directions. The parallel corpus for training stage 1 was sourced from WMT2023, as detailed in Table 6. We sampled 10M bilingual sentence pairs for each language, yielding total of 40M pairs for training stage 1. The training stage 2 utilizes data from the ComMT training set with 239k samples. We conducted tests on ComMT and evaluated the models translation performance across all tasks using COMET (wmt22-comet-da) (Rei et al., 2020) and SacreBLEU (Post, 2018). Additionally, we used Terminology Success Rate (TSR) for the terminology-constrained translation task and Human Translation Edit Rate (HTER) for the automatic post-editing task. 5.2 Training Our multilingual translation experiments necessitate language model with multilingual capabilities for better encoding. To assess the fundamental translation capabilities of state-of-the-art multilingual LLMs, we conducted comprehensive evaluation across several widely used models, including XGLM-7.5B (Lin et al., 2022), Bloom-7B (Scao et al., 2022), Falcon-7B (Almazrouei et al., 2023), Preprint. Under review. Figure 6: Evaluation of LLM translation capabilities in 0-shot and 3-shot settings using the WMT23 test set. Qwen2-7B (Yang et al., 2024a), Llama3-8B (Dubey et al., 2024). These models were evaluated in both zero-shot and three-shot settings across English German, Czech, Russian, and Chinese translation tasks. The averaged performance in in Figure 6 show that Llama3-8B consistently outperformed the other models in both En En translation directions. Its superior performance across different language pairs suggests that it possesses stronger multilingual capabilities. Based on these results, we selected Llama3-8B as our backbone model for subsequent experiments. and Llama3-8B has model dimension of 4096 and consists of 32 layers. For the decoder, we set the dimension to 1024 with 8 layers, which also applies to the EncStack of the adaptor, with the adaptor and decoder together introducing fewer than 500M parameters. Hyperparameters for training are detailed in Table 7. 5.3 Models for Comparison We create two categories of models for comparison: Decoder-only Models. This includes: BigTranslate (Yang et al., 2023b), Bayling-13B (Zhang et al., 2023b), Aya-23-8B (Üstün et al., 2024), ALMA-7B (Xu et al., 2023) and TowerInstruct-7B (Alves et al., 2024). Also included are variants of Llama3-8B: Llama3-8B-Base (3-shot in-context learning), Llama3-8B-Inst (general instruction-tuned), and Llama3-8B-SFT (fine-tuned on ComMT). Encoder-Decoder Models. This includes: (i) NLLB-3.3B (Costa-jussà et al., 2022): comprehensive multilingual model. (ii) Traditional encoder-decoder models: NMT-40-8 (40 encoder layers, 8 decoder layers) and NMT-8-8 (8 encoder layers, 8 decoder layers), both with model dimension of 1024, trained from scratch on 161M bilingual sentences from combined parallel data of four languages. (iii) Fine-tuned encoder-decoder models: mT5-Large (1.2B) (Xue et al., 2021), aligned with our methods data settings."
        },
        {
            "title": "6 Results and Analyses",
            "content": "6.1 Main Results The averaged performance across all directions is shown in Table 2, with more detailed results provided in Table 14. Comparison with Encoder-Decoder Models. In comparison to encoder-decoder models, LaMaTE demonstrates stronger performance, surpassing NLLB-3.3B, NMT, and mT5-Large across all evaluated tasks. This result hightlights the potential of LLMs as high-capacity encoders, capable of producing more expressive and informative representations. These enhanced representations provide solid foundation for the decoder, ultimately improving translation quality. Moreover, LaMaTEs consistent performance across multiple tasks underscores its strong generalization capability. 9 Preprint. Under review. Model General Trans Domain Trans Doc-level Trans Terminology-con Trans Automatic Post-edition COMET BLEU COMET BLEU COMET BLEU COMET BLEU TSR Decoder-only Models COMET BLEU HTER ALMA-7B* 83.23 77.84 BigTranslate* Aya-23-8B 82.72 TowerInstruct-7B 83.12 74.81 Bayling-13B 79.03 Llama3-8B-Base 73.56 Llama3-8B-Inst 82.41 Llama3-8B-SFT NLLB-3.3B* NMT-8-8* NMT-40-8* mT5-Large LaMaTE (Ours) 81.30 79.70 80.89 81.26 82.32 31.49 22.04 32.11 32.75 21.34 26.89 23.20 32.42 31.63 30.08 32.05 29.34 33.85 83.93 77.53 84.07 83.65 76.50 81.21 73.04 84.60 81.38 80.63 81.50 82.06 84.69 82.03 72.36 84.98 81.61 74.10 80.44 81.79 83. 18.59 9.18 32.19 28.22 19.46 26.77 27.22 31.75 31.94 21.99 33.67 33.25 24.07 28.29 22.69 36.92 Encoder-Decoder Models 32.66 32.76 34.06 30.26 37.49 73.60 73.98 74.74 77.69 84.34 11.72 10.11 12.39 14.43 31.69 81.84 77.29 81.47 82.57 77.01 81.81 71.36 84.01 80.21 77.03 77.19 81.80 83. 24.89 43.20 18.56 33.10 31.14 74.15 31.84 76.98 25.08 71.94 29.84 79.20 24.41 86.46 36.77 87.52 26.32 41.25 25.54 40.56 25.80 41.15 30.34 76.93 34.76 75.64 85.25 78.84 87.68 88.91 73.90 86.26 68.05 88.57 84.97 84.50 85.10 85.88 88.60 54.42 40.91 69.04 29.82 36.23 67.92 77.41 23.49 29.67 114.80 61.43 38.93 25.70 101.85 70.13 31.57 46.42 47.52 47.57 62.77 69. 52.42 50.21 50.26 45.10 33.42 Table 2: Performance on ComMT test set. Bold numbers represent the highest scores in each category, while underlined numbers indicate the second highest scores. Models marked with \"*\" cannot handle additional inputs for the terminology-constrained translation and automatic post-editing tasks, we only use the source sequence as input. Comparison with Fine-Tuned LLMs. When compared with fine-tuned LLMs, LaMaTE achieves comparable overall performance to Llama3-8B-SFT. While Llama3-8B-SFT holds slight advantage in the terminology-constrained translation task with higher TSR scores, there is no significant difference in other tasks. Additionally, LaMaTE outperforms earlier fine-tuned LLMs like Bayling13B and remains competitive with more advanced models such as TowerInstruct-7B. This further demonstrates that LLMs can serve as effective MT encoders, offering an effective and computationally efficient alternative to the approach of fine-tuning LLMs as direct generators. Misalignment Evaluation. To further investigate translation quality, we analyze misalignment issues commonly observed in LLM-based generation (Zhang et al., 2024; Zeng et al., 2024), where models may generate output that diverges from the source text. We compare decoder-only models (Llama3-8B and TowerInstruct-7B) and encoder-decoder (mT5-large and LaMaTE) across three direction. metrics: off-target rate, unaligned source words, and unaligned target words in En Refer to the calculation details in Section D.1. As shown in Figure 7, encoder-decoder models consistently exhibit lower scores across all three dimensions, indicating they reduce misalignment compared to decoder-only models. LaMaTE follows the encoder-decoder paradigm, which also helps mitigate misalignment issues as an added bonus. We speculate that this improvement can be attributed to the cross-attention mechanism in the NMT decoder, which allows direct token-wise Figure 7: Comparison of decoder-only (Llama3-8B and TowerInstruct-7B) and encoder-decoder (mT5-large and LaMaTE) models on off-target rate (OTR), unaligned source words (USW), and unaligned target words (UTW). 10 Preprint. Under review."
        },
        {
            "title": "Models",
            "content": "HRL(12) MRL(23) LRL(22) XRL(42) NMT-40-8 mT5-large LaMaTE (Ours) 30.13 29.12 32.11 30.28 30.14 32.13 21.02 27.46 26.20 16.59 24.64 21. Table 3: BLEU performance evaluated on the FLORES-200 devtest set, with models trained on OPUS-100 data (99 languages). HRL, MRL, LRL, and XRL represent high, medium, low, and very low-resource languages, respectively. The bracketed numbers indicate the count of languages. interaction between the target and the source sequence, thereby enhancing source-target alignment. In contrast, LLMs rely on single attention mechanism to jointly process the concatenated source and target text, which could potentially cause loss of attention focus (Zhang et al., 2024). Large-Scale Multilingual Translation. To further validate whether our method can benefit broader range of languages, we conducted extensive multilingual translation tasks (99 languages). Specifically, we utilized OPUS-100 (Zhang et al., 2020) as our training set, which includes bilingual data for 99 languages paired with English, totaling 55M sentences. Table 13 shows the classification of resource levels for these languages. We trained the models on the En direction and evaluated with Flores-200 dev-test set (Costa-jussà et al., 2022). For our method, we froze the LLM parameters during training and only trained the parameters of the adaptor and decoder. Due to the high cost of fine-tuning LLMs on such large-scale dataset, we did not compare the LLM-SFT model. As shown in Table 3, LaMaTE significantly outperforms the NMT-40-8 model across all resource levels and surpasses the mT5-large model in high and medium language resources. This highlights the effectiveness of leveraging LLMs as encoders for multilingual translation, as their rich representations can boost performance. However, our model underperforms compared to mT5-large in low and very low-resource languages, likely due to limited pre-training data for these languages in the Llama3-base model. Using model with stronger multilingual capabilities as the encoder could further improve performance. 6.2 Efficiency Analysis Figure 8 illustrates the efficiency comparisons between LaMaTE and other models, with their configuration details are provided in Table 10. The results highlight two key improvements: decoding speedup and memory efficiency. The left chart in Figure 8 demonstrates that as the source sequence length and batch size increase, LaMaTE achieves 2.4 to 6.5 speedup over Llama3-8B (see Table 11 for details). Furthermore, as shown in the right chart, LaMaTE significantly reduces KV cache memory by 75% compared to Llama3-8B. This reduction in memory usage enhances scalability, enabling larger batch sizes and longer source sequences. For instance, at an 80G memory budget, Llama3-8B encounters Figure 8: Comparison of efficiency: The left chart displays the decoding speedup ratio of LaMaTE versus Llama3-8B under varying source sequence lengths and batch sizes, and the right chart shows the theoretical KV cache size factor for each model. 11 Preprint. Under review. out-of-memory issues at batch size of 24 for source lengths of 300-400, whereas LaMaTE still works normally. This improvement is particularly beneficial for large-scale inference, where optimizing throughput is crucial for real-world deployment. Overall, LaMaTE significantly enhances computational efficiency without compromising translation quality, demonstrating its effectiveness in balancing performance and compute resources."
        },
        {
            "title": "6.3 Depth vs Performance",
            "content": "Figure 9: The impact of EncStack and decoder depth on model performance and efficiency. To analyze how the depth of the EncStack and decoder affects performance and efficiency, we first fix the decoder depth and vary the EncStack depth, then fix the EncStack depth and adjust the decoder depth. We evaluate translation performance in both En En directions and measure decoding speed, as shown in Figure 9. and The left panel presents the effect of increasing the number of EncStack layers. The results show that adding more EncStack layers leads to modest improvements in both translation directions, but the gains are relatively small compared to variations in the decoder. Additionally, since the EncStack operates only on the encoder side, its impact on decoding efficiency is minimal. The right panel illustrates the effect of varying decoder depth. Unlike the encoder, increasing decoder layers significantly influences both translation performance and efficiency. While deeper decoders consistently improve translation quality, particularly in the En direction, they also introduce notable trade-off: decoding speed drops sharply as depth increases, with 39% reduction when moving from 8 to 16 layers. This suggests that scaling the encoder offers more effective trade-off between performance gains and computational efficiency. Alternatively, the decoder can be scaled using the Mixture of Experts (MoE) technique, which enables capacity expansion without an increase in inference cost. We leave further exploration of these strategies for future work. 6.4 Ablation Study Aspect - Training Method Adaptor Design Decoder Variant Models COMET BLEU LaMaTE (Ours) 82.32 W/o S2 80.07( W/ S2 & Frozen LLM 82.16( W/o Layer Fusion W/o EncStack Concat Decoder Prefix Decoder 82.08( 82.02( 81.52( 81.66( 2.25) 0.16) 0.24) 0.30) 0.80) 0.66) 33.85 29.67( 33.73( 33.14( 33.32( 31.51( 32.38( 4.18) 0.12) 0.71) 0.53) 2.34) 1.47) Table 4: Ablation studies on training methods, adaptor design, and decoder variants. The numbers in the bottom right represent the performance gap relative to the complete model (LaMaTE). Preprint. Under review. We present ablation studies in Table 4. First, training the model with only stage 1 (W/o S2) results in substantial performance drop. Incorporating stage 2 while freezing LLM parameters yields noticeable improvements, but still falls short of LaMaTEs performance. This underscores the importance of both high-quality data and LLM parameter tuning in maximizing model performance. For the adaptor, removing the layer fusion leads to performance drop, indicating that fusing intermediate LLM layers provides more informative representation than relying solely on the final layer. Similarly, removing EncStack results in slight degradation. These findings confirm that the adaptor enhances the native representations of LLMs, rendering them more suitable for the NMT decoder. Besides, we explore two decoder variantsConcat Decoder and Prefix Decoderboth of which remove the cross-attention layer in the standard decoder. For detailed description of these variants, see Appendix D.4. As shown in the results, the standard decoder proves superior to alternative designs. Together with the observations from Figure 7, we argue that maintaining cross-attention in the decoder is particularly beneficial for translation tasks that demand strong alignment."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we explore the connection between the two worlds of LLMs and NMT, presenting LaMaTEa method that leverages LLMs as MT encoders and pairs them with lightweight decoders. We design an adaptor better to align LLMs representations for the decoder and propose two-stage training strategy to develop universal translation model. Additionally, we introduce ComMT, new dataset suite encompassing diverse translation-related tasks, facilitating the development and evaluation of universal translation models. Experiments on ComMT demonstrate LaMaTEs impressive performance and generalization ability, while significantly improving computational efficiencyachieving 2.4 to 6.5 faster decoding speeds and reducing KV cache memory usage by 75%. We hope this study will provide valuable insights and inspire further exploration into optimizing LLMs and expanding their role in NLP tasks."
        },
        {
            "title": "Limitations",
            "content": "Our approach requires pre-training the decoders parameters using large-scale bilingual data in the first stage, as the decoder is randomly initialized. This process may not be the most efficient. Future work could explore initializing these parameters from the encoder or directly leveraging small pre-trained language model to reduce the state gap between the encoder and decoder. Additionally, since our decoder is lightweight, it may become bottleneck when generating translations into many target languages, so expanding its capacity is essential for better performance. more effective strategy might involve scaling the decoders capacity with Mixture-of-Experts (MoE) instead of adding more layers, thereby boosting performance without compromising efficiency."
        },
        {
            "title": "References",
            "content": "David Ifeoluwa Adelani, Md Mahfuz Ibn Alam, Antonios Anastasopoulos, Akshita Bhagia, Marta R. Costa-jussà, Jesse Dodge, Fahim Faisal, Christian Federmann, Natalia Fedorova, Francisco Guzmán, Sergey Koshelev, Jean Maillard, Vukosi Marivate, Jonathan Mbuya, Alexandre Mourachko, Safiyyah Saleem, Holger Schwenk, and Guillaume Wenzek. Findings of the wmt22 shared task on large-scale machine translation evaluation for african languages. In WMT, pp. 773800. Association for Computational Linguistics, 2022. Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. GQA: training generalized multi-query transformer models from multi-head checkpoints. In EMNLP, pp. 48954901. Association for Computational Linguistics, 2023. Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. The falcon series of open language models. CoRR, abs/2311.16867, 2023. 13 Preprint. Under review. Duarte M. Alves, José Pombal, Nuno Miguel Guerreiro, Pedro Henrique Martins, João Alves, M. Amin Farajian, Ben Peters, Ricardo Rei, Patrick Fernandes, Sweta Agrawal, Pierre Colombo, José G. C. de Souza, and André F. T. Martins. Tower: An open multilingual large language model for translation-related tasks. CoRR, abs/2402.17733, 2024. Antonios Anastasopoulos, Alessandro Cattelan, Zi-Yi Dou, Marcello Federico, Christian Federmann, Dmitriy Genzel, Francisco Guzmán, Junjie Hu, Macduff Hughes, Philipp Koehn, Rosie Lazar, William Lewis, Graham Neubig, Mengmeng Niu, Alp Öktem, Eric Paquin, Grace Tang, and Sylwia Tur. TICO-19: the translation initiative for covid-19. In Karin Verspoor, Kevin Bretonnel Cohen, Michael Conway, Berry de Bruijn, Mark Dredze, Rada Mihalcea, and Byron C. Wallace (eds.), Proceedings of the 1st Workshop on NLP for COVID-19@ EMNLP 2020, Online, December 2020. Association for Computational Linguistics, 2020. doi: 10.18653/V1/2020.NLPCOVID19-2.5. URL https://doi.org/10.18653/v1/2020.nlpcovid19-2.5. Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. On the cross-lingual transferability of monolingual representations. CoRR, abs/1910.11856, 2019. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In ICLR, 2015. Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. Llm2vec: Large language models are secretly powerful text encoders. CoRR, abs/2404.05961, 2024. Peter Brown, Stephen Della Pietra, Vincent Della Pietra, and Robert Mercer. The mathematics of statistical machine translation: Parameter estimation. Computational linguistics, 19(2):263311, 1993. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In NeurIPS, 2020. David Chiang. hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd annual meeting of the association for computational linguistics (acl05), pp. 263270, 2005. Pranjal A. Chitale, Jay P. Gala, and Raj Dabre. An empirical study of in-context learning in llms for machine translation. In ACL (Findings), pp. 73847406. Association for Computational Linguistics, 2024. Shamil Chollampatt, Raymond Hendy Susanto, Liling Tan, and Ewa Szymanska. Can automatic post-editing improve nmt? In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pp. 27362746. Association for Computational Linguistics, 2020. doi: 10.18653/V1/2020.EMNLP-MAIN.217. URL https://doi.org/10 .18653/v1/2020.emnlp-main.217. Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel R. Bowman, Holger Schwenk, and Veselin Stoyanov. Xnli: Evaluating cross-lingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2018. Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Y. Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loïc Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, 14 Preprint. Under review. Safiyyah Saleem, Holger Schwenk, and Jeff Wang. No language left behind: Scaling humancentered machine translation. CoRR, abs/2207.04672, 2022. doi: 10.48550/ARXIV.2207.04672. URL https://doi.org/10.48550/arXiv.2207.04672. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep In NAACL-HLT (1), pp. 41714186. bidirectional transformers for language understanding. Association for Computational Linguistics, 2019. Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. Unified language model pre-training for natural language understanding and generation. In NeurIPS, pp. 1304213054, 2019. Zi-Yi Dou and Graham Neubig. Word alignment by fine-tuning embeddings on parallel corpora. In Paola Merlo, Jörg Tiedemann, and Reut Tsarfaty (eds.), Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021, pp. 21122128. Association for Computational Linguistics, 2021. doi: 10.18653/V1/2021.EACL-MAIN.181. URL https://doi.org/10.18653/v1/2021.e acl-main.181. Zi-Yi Dou, Zhaopeng Tu, Xing Wang, Shuming Shi, and Tong Zhang. Exploiting deep representations In EMNLP, pp. 42534262. Association for Computational for neural machine translation. Linguistics, 2018. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurélien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozière, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Grégoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. Desmond Elliott, Stella Frank, Khalil Simaan, and Lucia Specia. Multi30k: Multilingual englishgerman image descriptions. In Proceedings of the 5th Workshop on Vision and Language, pp. 7074. Association for Computational Linguistics, 2016. doi: 10.18653/v1/W16-3210. URL http://www.aclweb.org/anthology/W16-3210. Christian Federmann and William D. Lewis. The microsoft speech language translation (MSLT) corpus for chinese and japanese: Conversational test data for machine translation and speech recognition. In Sadao Kurohashi and Pascale Fung (eds.), Proceedings of Machine Translation Summit XVI, Volume 1: Research Track, MTSummit 2017, September 18-22, 2017, Nagoya, Aichi, Japan, pp. 7285, 2017. URL https://aclanthology.org/2017.mtsummit-paper s.6. Christian Federmann, Tom Kocmi, and Ying Xin. NTREX-128 news test references for MT evaluation of 128 languages. In Proceedings of the First Workshop on Scaling Up Multilingual Evaluation, pp. 2124, Online, nov 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.sumeval-1.4. Marina Fomicheva, Shuo Sun, Erick Fonseca, Frédéric Blain, Vishrav Chaudhary, Francisco Guzmán, Nina Lopatina, Lucia Specia, and André F. T. Martins. MLQE-PE: multilingual quality estimation and post-editing dataset. arXiv preprint arXiv:2010.04480, 2020. 15 Preprint. Under review. Yvette Graham, Barry Haddow, and Philipp Koehn. Statistical power and translationese in machine translation evaluation. In EMNLP (1), pp. 7281. Association for Computational Linguistics, 2020. Nuno M. Guerreiro, Elena Voita, and André Martins. Looking for needle in haystack: comprehensive study of hallucinations in neural machine translation. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. URL https://aclantholo gy.org/2023.eacl-main.75. Jiaxin Guo, Hao Yang, Zongyao Li, Daimeng Wei, Hengchao Shang, and Xiaoyu Chen. novel paradigm boosting translation capabilities of large language models. CoRR, abs/2403.11430, 2024. Kazuma Hashimoto, Raffaella Buschiazzo, James Bradbury, Teresa Marshall, Richard Socher, and Caiming Xiong. high-quality multilingual dataset for structured documentation translation. In Ondrej Bojar, Rajen Chatterjee, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno-Yepes, Philipp Koehn, André Martins, Christof Monz, Matteo Negri, Aurélie Névéol, Mariana L. Neves, Matt Post, Marco Turchi, and Karin Verspoor (eds.), Proceedings of the Fourth Conference on Machine Translation, WMT 2019, Florence, Italy, August 1-2, 2019 - Volume 1: Research Papers, pp. 116127. Association for Computational Linguistics, 2019. doi: 10.18653/V1/W19-5212. URL https://doi.org/10.18653/v1/w19-5212. Jie He, Tao Wang, Deyi Xiong, and Qun Liu. The box is in the pen: Evaluating commonsense reasoning in neural machine translation. In Trevor Cohn, Yulan He, and Yang Liu (eds.), Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020, volume EMNLP 2020 of Findings of ACL, pp. 36623672. Association for Computational Linguistics, 2020. doi: 10.18653/V1/2020.FINDINGS-EMNLP.327. URL https://doi.or g/10.18653/v1/2020.findings-emnlp.327. Julian Hitschler, Shigehiko Schamoni, and Stefan Riezler. Multimodal pivots for image capIn Proceedings of the 54th Annual Meeting of the Association for Compution translation. tational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer Linguistics, 2016. doi: 10.18653/V1/P16-1227. URL https://doi.org/10.18653/v1/p16-1227. Zhengmian Hu, Tong Zheng, Vignesh Viswanathan, Ziyi Chen, Ryan Rossi, Yihan Wu, Dinesh Manocha, and Heng Huang. Towards optimal multi-draft speculative decoding. In The Thirteenth International Conference on Learning Representations, 2025. Yuchen Eleanor Jiang, Tianyu Liu, Shuming Ma, Dongdong Zhang, Ryan Cotterell, and Mrinmaya Sachan. Discourse centric evaluation of machine translation with densely annotated parallel corpus. In Proceedings of the 2023 Conference of the Association for Computational Linguistics: Human Language Technologies, pp. 15501565, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.main.111. URL https://aclanthology .org/2023.acl-main.111. Wenxiang Jiao, Jen-tse Huang, Wenxuan Wang, Zhiwei He, Tian Liang, Xing Wang, Shuming Shi, and Zhaopeng Tu. Parrot: Translating during chat using large language models tuned with human translation and feedback. In EMNLP (Findings), pp. 1500915020. Association for Computational Linguistics, 2023. Linghao Jin, Jacqueline He, Jonathan May, and Xuezhe Ma. Challenges in context-aware neural machine translation. In EMNLP, pp. 1524615263. Association for Computational Linguistics, 2023a. Linghao Jin, Jacqueline He, Jonathan May, and Xuezhe Ma. Challenges in context-aware neural machine translation. In Empirical Methods in Natural Language Processing (EMNLP), 2023b. Yimin Jing, Deyi Xiong, and Yan Zhen. Bipar: bilingual parallel dataset for multilingual and cross-lingual reading comprehension on novels. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pp. 24522462. Association for Preprint. Under review. Computational Linguistics, 2019. doi: 10.18653/V1/D19-1249. URL https://doi.org/10 .18653/v1/D19-1249. Marzena Karpinska and Mohit Iyyer. Large language models effectively leverage document-level context for literary translation, but critical errors persist. In Philipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz (eds.), Proceedings of the Eighth Conference on Machine Translation, pp. 419451, Singapore, December 2023. Association for Computational Linguistics. doi: 10.186 53/v1/2023.wmt-1.41. URL https://aclanthology.org/2023.wmt-1.41. Marzena Karpinska, Nishant Raj, Katherine Thai, Yixiao Song, Ankita Gupta, and Mohit Iyyer. Demetr: Diagnosing evaluation metrics for translation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, 2022a. Marzena Karpinska, Katherine Thai, Kalpesh Krishna, John Wieting, Moira Inghilleri, and Mohit Iyyer. Par3, 5 2022b. URL https://github.com/ngram-lab/par3. Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik, Michael W. Mahoney, Amir Gholami, and Kurt Keutzer. Speculative decoding with big little decoder. In NeurIPS, 2023. Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondrej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, Barry Haddow, Philipp Koehn, Benjamin Marie, Christof Monz, Makoto Morishita, Kenton Murray, Makoto Nagata, Toshiaki Nakazawa, Martin Popel, Maja Popovic, and Mariya Shmatova. Findings of the 2023 conference on machine translation (WMT23): llms are here but not quite there yet. In WMT, pp. 142. Association for Computational Linguistics, 2023. Philipp Koehn, Franz Josef Och, and Daniel Marcu. Statistical phrase-based translation. In 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Langauge Technology (HLT-NAACL 2003), pp. 4854. Association for Computational Linguistics, 2003. Abdullatif Köksal and Arzucan Özgür. The RELX dataset and matching the multilingual blanks for cross-lingual relation classification. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 340350, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.32. URL https://www.aclweb.org/anthology /2020.findings-emnlp.32. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In SOSP, pp. 611626. ACM, 2023. Samuel Läubli, Sheila Castilho, Graham Neubig, Rico Sennrich, Qinlan Shen, and Antonio Toral. set of recommendations for assessing human-machine parity in language translation. J. Artif. Intell. Res., 67:653672, 2020. Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In ICML, volume 202 of Proceedings of Machine Learning Research, pp. 1927419286. PMLR, 2023. Bei Li, Tong Zheng, Rui Wang, Jiahao Liu, Qingyan Guo, Junliang Guo, Xu Tan, Tong Xiao, Jingbo Zhu, Jingang Wang, and Xunliang Cai. Predictor-corrector enhanced transformers with exponential moving average coefficient learning. In NeurIPS, 2024. Yunlong Liang, Fandong Meng, Yufeng Chen, Jinan Xu, and Jie Zhou. Modeling bilingual conversational characteristics for neural chat translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 57115724, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.444. URL https://aclanthology.org/2021.acl-long.444. Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian OHoro, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona T. 17 Preprint. Under review. Diab, Veselin Stoyanov, and Xian Li. Few-shot learning with multilingual language models. CoRR, abs/2112.10668, 2021. URL https://arxiv.org/abs/2112.10668. Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian OHoro, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona T. Diab, Veselin Stoyanov, and Xian Li. Few-shot learning with multilingual generative language models. In EMNLP, pp. 90199052. Association for Computational Linguistics, 2022. Boxiang Liu and Liang Huang. Paramed: parallel corpus for english-chinese translation in the biomedical domain. BMC Medical Informatics Decis. Mak., 21(1):258, 2021. doi: 10.1186/S129 11-021-01621-8. URL https://doi.org/10.1186/s12911-021-01621-8. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. Hongyuan Lu, Haoran Yang, Haoyang Huang, Dongdong Zhang, Wai Lam, and Furu Wei. Chainof-dictionary prompting elicits translation in large language models. In EMNLP, pp. 958976. Association for Computational Linguistics, 2024. Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On the structural pruning of large language models. In NeurIPS, 2023. Sameen Maruf, Fahimeh Saleh, and Gholamreza Haffari. survey on document-level neural machine translation: Methods and evaluation. ACM Comput. Surv., 54(2):45:145:36, 2022. Yongyu Mu, Abudurexiti Reheman, Zhiquan Cao, Yuchun Fan, Bei Li, Yinqiao Li, Tong Xiao, Chunliang Zhang, and Jingbo Zhu. Augmenting large language model translators via translation memories. In ACL (Findings), pp. 1028710299. Association for Computational Linguistics, 2023. Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela. Generative representational instruction tuning. CoRR, abs/2402.09906, 2024. Mathias Müller, Annette Rios, Elena Voita, and Rico Sennrich. Large-Scale Test Set for the Evaluation of Context-Aware Pronoun Translation in Neural Machine Translation. In WMT 2018, Brussels, Belgium, 2018. Association for Computational Linguistics. Matteo Negri, Marco Turchi, Rajen Chatterjee, and Nicola Bertoldi. ESCAPE: large-scale synthetic corpus for automatic post-editing. In LREC. European Language Resources Association (ELRA), 2018. Maria Nadejde, Anna Currey, Benjamin Hsu, Xing Niu, Marcello Federico, and Georgiana Dinu. CoCoA-MT: dataset and benchmark for Contrastive Controlled MT with application to formality. In Findings of the Association for Computational Linguistics: NAACL 2022, Seattle, USA, July 2022. Association for Computational Linguistics. Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka Shah, Íñigo Goiri, Saeed Maleki, and Ricardo Bianchini. Splitwise: Efficient generative LLM inference using phase splitting. In ISCA, pp. 118132. IEEE, 2024. Aleksandr Perevalov, Dennis Diefenbach, Ricardo Usbeck, and Andreas Both. Qald-9-plus: multilingual dataset for question answering over dbpedia and wikidata translated by native speakers. In 2022 IEEE 16th International Conference on Semantic Computing (ICSC), pp. 229234, 2022. doi: 10.1109/ICSC52841.2022.00045. Edoardo Maria Ponti, Goran Glavas, Olga Majewska, Qianchu Liu, Ivan Vulic, and Anna Korhonen. XCOPA: multilingual dataset for causal commonsense reasoning. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pp. 23622376. Association for Computational Linguistics, 2020. doi: 10.18653/V1/2020.EMNLP-MAIN.185. URL https://doi.org/10.18653/v1/2020.emnlp-main.185. Matt Post. call for clarity in reporting BLEU scores. In WMT, pp. 186191. Association for Computational Linguistics, 2018. 18 Preprint. Under review. Alec Radford. Improving language understanding by generative pre-training. 2018. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1140:67, 2020. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: memory optimizations toward training trillion parameter models. In SC, pp. 20. IEEE/ACM, 2020. Ricardo Rei, Craig Stewart, Ana C. Farinha, and Alon Lavie. COMET: neural framework for MT evaluation. In EMNLP (1), pp. 26852702. Association for Computational Linguistics, 2020. Nils Reimers and Iryna Gurevych. Making monolingual sentence embeddings multilingual using knowledge distillation. In EMNLP (1), pp. 45124525. Association for Computational Linguistics, 2020. Parker Riley, Timothy Dozat, Jan A. Botha, Xavier Garcia, Dan Garrette, Jason Riesa, Orhan Firat, and Noah Constant. FRMT: benchmark for few-shot region-aware machine translation, 2022. URL https://arxiv.org/abs/2210.00193. Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, and et al. BLOOM: 176bparameter open-access multilingual language model. CoRR, abs/2211.05100, 2022. Priyanka Sen, Alham Fikri Aji, and Amir Saffari. Mintaka: complex, natural, and multilingual dataset for end-to-end question answering. In Proceedings of the 29th International Conference on Computational Linguistics, pp. 16041619, Gyeongju, Republic of Korea, October 2022. International Committee on Computational Linguistics. URL https://aclanthology.org /2022.coling-1.138. Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. Language models are multilingual chain-of-thought reasoners. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum?id=fR3wGCk-IXp. David Stap, Eva Hasler, Bill Byrne, Christof Monz, and Ke Tran. The fine-tuning paradox: Boosting translation quality without sacrificing llm abilities, 2024. URL https://arxiv.org/abs/ 2405.20089. Ralf Steinberger, Andreas Eisele, Szymon Klocek, Spyridon Pilos, and Patrick Schlüter. DGTTM: freely available translation memory in 22 languages. CoRR, abs/1309.5226, 2013. URL http://arxiv.org/abs/1309.5226. Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. In NIPS, pp. 31043112, 2014. Kenan Tang. PETCI: parallel english translation dataset of chinese idioms. CoRR, abs/2202.09509, 2022. URL https://arxiv.org/abs/2202.09509. Liang Tian, Derek F. Wong, Lidia S. Chao, Paulo Quaresma, Francisco Oliveira, and Lu Yi. Umcorpus: large english-chinese parallel corpus for statistical machine translation. In Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Hrafn Loftsson, Bente Maegaard, Joseph Mariani, Asunción Moreno, Jan Odijk, and Stelios Piperidis (eds.), Proceedings of the Ninth International Conference on Language Resources and Evaluation, LREC 2014, Reykjavik, Iceland, May 2631, 2014, pp. 18371842. European Language Resources Association (ELRA), 2014. URL http://www.lrec-conf.org/proceedings/lrec2014/summaries/774.html. 19 Preprint. Under review. Ahmet Üstün, Viraat Aryabumi, Zheng Xin Yong, Wei-Yin Ko, Daniel Dsouza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh Fadaee, Julia Kreutzer, and Sara Hooker. Aya model: An instruction finetuned open-access multilingual language model. In ACL (1), pp. 1589415939. Association for Computational Linguistics, 2024. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, pp. 59986008, 2017. Elena Voita, Rico Sennrich, and Ivan Titov. When good translation is wrong in context: Contextaware machine translation improves on deixis, ellipsis, and lexical cohesion. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 11981212, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1116. URL https://www.aclweb.org/anthology/P19-1116. Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao. Learning deep transformer models for machine translation. In ACL (1), pp. 18101822. Association for Computational Linguistics, 2019. Rachel Wicks and Matt Post. Identifying context-dependent translations for evaluation set production. In Philipp Koehn, Barry Haddon, Tom Kocmi, and Christof Monz (eds.), Proceedings of the Eighth Conference on Machine Translation, WMT 2023, Singapore, December 6-7, 2023, pp. 452467. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.WMT-1.42. URL https://doi.org/10.18653/v1/2023.wmt-1.42. Guangxuan Xiao, Ji Lin, Mickaël Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In ICML, volume 202 of Proceedings of Machine Learning Research, pp. 3808738099. PMLR, 2023. Tong Xiao and Jingbo Zhu. Foundations of large language models. arXiv preprint arXiv:2501.09223, 2025. Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla. paradigm shift in machine translation: Boosting translation performance of large language models. CoRR, abs/2309.11674, 2023. Mingzhou Xu, Longyue Wang, Derek F. Wong, Hongye Liu, Linfeng Song, Lidia S. Chao, Shuming Shi, and Zhaopeng Tu. Guofeng: benchmark for zero pronoun recovery and translation. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pp. 1126611278. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022.EMNLP-MAIN.774. URL https://doi.org/10.18653/v 1/2022.emnlp-main.774. Weijia Xu and Marine Carpuat. Rule-based morphological inflection improves neural terminology translation. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pp. 5902 5914. Association for Computational Linguistics, 2021. doi: 10.18653/V1/2021.EMNLP-MAIN. 477. URL https://doi.org/10.18653/v1/2021.emnlp-main.477. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mt5: massively multilingual pre-trained text-to-text transformer. In NAACL-HLT, pp. 483498. Association for Computational Linguistics, 2021. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, 20 Preprint. Under review. Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report. CoRR, abs/2407.10671, 2024a. Jian Yang, Yuwei Yin, Liqun Yang, Shuming Ma, Haoyang Huang, Dongdong Zhang, Furu Wei, and Zhoujun Li. Gtrans: Grouping and fusing transformer layers for neural machine translation. IEEE ACM Trans. Audio Speech Lang. Process., 31:14891498, 2023a. Wen Yang, Chong Li, Jiajun Zhang, and Chengqing Zong. Bigtrans: Augmenting large language models with multilingual translation capability over 100 languages. CoRR, abs/2305.18098, 2023b. Yinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. PAWS-X: Cross-lingual Adversarial Dataset for Paraphrase Identification. In Proc. of EMNLP, 2019. Yuming Yang, Wantong Zhao, Caishuang Huang, Junjie Ye, Xiao Wang, Huiyuan Zheng, Yang Nan, Yuran Wang, Xueying Xu, Kaixin Huang, Yunke Zhang, Tao Gui, Qi Zhang, and Xuanjing Huang. Beyond boundaries: Learning universal entity taxonomy across datasets and languages for open named entity recognition. CoRR, abs/2406.11192, 2024b. Jiali Zeng, Fandong Meng, Yongjing Yin, and Jie Zhou. Improving machine translation with large language models: preliminary study with cooperative decoding. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pp. 1327513288. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.FINDINGS-ACL.786. URL https://doi.org/10.18653/v1/2024.findings-acl.786. Biao Zhang, Philip Williams, Ivan Titov, and Rico Sennrich. Improving massively multilingual neural machine translation and zero-shot translation. In ACL, pp. 16281639. Association for Computational Linguistics, 2020. Biao Zhang, Barry Haddow, and Alexandra Birch. Prompting large language model for machine translation: case study. In ICML, volume 202 of Proceedings of Machine Learning Research, pp. 4109241110. PMLR, 2023a. Hongbin Zhang, Kehai Chen, Xuefeng Bai, Yang Xiang, and Min Zhang. Paying more attention to source context: Mitigating unfaithful translations from large language model. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pp. 13816 13836. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.FINDINGS-A CL.821. URL https://doi.org/10.18653/v1/2024.findings-acl.821. Shaolei Zhang, Qingkai Fang, Zhuocheng Zhang, Zhengrui Ma, Yan Zhou, Langlin Huang, Mengyu Bu, Shangtong Gui, Yunji Chen, Xilin Chen, and Yang Feng. Bayling: Bridging cross-lingual alignment and instruction following through interactive translation for large language models. CoRR, abs/2306.10968, 2023b. Shaohui Zheng, Zhixu Li, Jiaan Wang, Jianfeng Qu, An Liu, Lei Zhao, and Zhigang Chen. Longdocument cross-lingual summarization. In Tat-Seng Chua, Hady W. Lauw, Luo Si, Evimaria Terzi, and Panayiotis Tsaparas (eds.), Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining, WSDM 2023, Singapore, 27 February 2023 - 3 March 2023, pp. 10841092. ACM, 2023. doi: 10.1145/3539597.3570479. URL https://doi.org/10.114 5/3539597.3570479. Tong Zheng, Bei Li, Huiwen Bao, Tong Xiao, and JingBo Zhu. EIT: enhanced interactive transformer. In ACL (1), pp. 77347751. Association for Computational Linguistics, 2024. Tong Zheng, Yan Wen, Huiwen Bao, Junfeng Guo, and Heng Huang. Asymmetric conflict and synergy in post-training for llm-based multilingual machine translation. arXiv preprint arXiv:2502.11223, 2025. Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. Distserve: Disaggregating prefill and decoding for goodput-optimized large language model serving. arXiv preprint arXiv:2401.09670, 2024. 21 Preprint. Under review. Wenhao Zhu, Shujian Huang, Tong Pu, Pingxuan Huang, Xu Zhang, Jian Yu, Wei Chen, Yanfeng Wang, and Jiajun Chen. Fdmt: benchmark dataset for fine-grained domain adaptation in machine translation. arXiv preprint arXiv:2012.15717, 2021. Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, and Lei Li. Multilingual machine translation with large language models: Empirical results and analysis. In NAACL-HLT (Findings), pp. 27652781. Association for Computational Linguistics, 2024. Preprint. Under review. Encoder-Decoder vs Decoder-Only Figure 10: The Encoder-Decoder and Decoder-only architecture. Model Connection Type Fuse Type TopOnly Layer-Wise Cross Attention Concat Attention Encoder-Decoder Causal LM Prefix LM Src Mask Fully-visible Causal Fully-visible Enc-Dec Parameter Sharing Table 5: Comparision of different architecture by analyzing their structural elements. Recent advancements in NLP have been profoundly shaped by Transformer models (Vaswani et al., 2017; Radford, 2018; Devlin et al., 2019), which have revolutionized both model design and task handling. The original Transformer (Vaswani et al., 2017) was designed for sequence-to-sequence (seq2seq) tasks, utilizing an encoder-decoder architecture where the encoder encodes the input sequence and the decoder generates the output. Subsequent models like GPT (Radford, 2018) leverage only the Transformers decoder, omitting the cross-attention layer, for language modeling tasks. Both architectures are illustrated in Figure 10. Although originally designed for different purposes, many NLP tasks can be framed as either seq2seq or language modeling problems (Xiao & Zhu, 2025). From macroscopic perspective, the encoder-decoder architecture employs an explicit encoder module to encode the input, after which the decoder generates output based on both the source representation from the encoders top layer and the target representation from preceding layer. Given the source sequence and target sequence Y, this process can be represented as follows 2: Xl = FFN(SAtt(Xl Yl = FFN(CAtt(SAtt(Yl 1, )) 1, Mc), XL, )) (10) (11) where denotes the layer index, Xl and Yl represent the representations of the source and target at l-th layer, respectively, with X0 and Y0 indicating the embeddings. Each layer of the encoder first performs self-attention (SAtt) with fully-visible mask pattern , indicating that all tokens in the source can attend to each other, followed by feed-forward neural network (FFN). The encoder generates its final representation XL in parallel by stacking such layers. The decoder is similar to the encoder but differs in that: (i) due to its autoregressive nature, its self-attention uses causal mask pattern Mc, allowing attention only to historical tokens; (ii) it incorporates an additional cross-attention (CAtt) layer to integrate representations from the source, where the mask is fully-visible mask pattern, enabling the target to attend to the entire source. 2For simplicity, we omit layer normalization and residual connections in the following formulas and descriptions. 23 Preprint. Under review. Contrastingly, the decoder-only architecture processes both the source and target sequences within single module: [Xl, Yl] = FFN(SAtt([Xl 1, Yl 1], Mc)) (12) In practice, due to the nature of causal attention, the representation of is independent of Y. This allows us to encode independently to obtain representations at each layer, as shown in Equation (10). Subsequently, the representations of Xl at each layer are concatenated with the corresponding target representations, enabling self-attention computations of the target across both source and target sequences, as shown in Figure 10(b). It is important to note that: (i) when encoding X, we can modify its original causal attention mask Mc to be fully-visible mask , similar to used in the encoderdecoder architecture, thus creating variant of the CausalLM model known as the prefixLM model (Dong et al., 2019; Raffel et al., 2020); (ii) the fusion of source and target information is achieved by computing attention on concatenated representations of both, distinct from cross-attention, which we refer to in this paper as concat attention; (iii) the interaction between and is Layer-Wise, rather than using only the top-layer representation of as in the encoder-decoder architecture (TopOnly). In practice, some modern The overall comparison of these architectures is shown in Table 5. deployment frameworks of LLMs (decoder-only models) explicitly separate encoding (prefilling) and decoding processes across distinct computational resources, making the architecture structurally resemble encoder-decoder models (Zhong et al., 2024; Patel et al., 2024). In this light, the so-called decoder-only model can be considered variant of the encoder-decoder model, wherein the encoding function is implicitly integrated through shared parameters with the decoder. Conversely, one may view the encoder-decoder model as an extension of PrefixLM, with more explicit division between encoding and decoding stages."
        },
        {
            "title": "B ComMT",
            "content": "B.1 Data Collection, Categorization and Processing Our data collection and curation efforts are guided by two key principles: ensuring high quality and maintaining diversity across translation tasks. To this end, we extensively gathered publicly available datasets from the research community, focusing primarily on well-established repositories such as OPUS 3, WMT 4, and Papers with Code 5. These sources have been widely used in MT research. To uphold data quality, we retained only those datas that had undergone manual annotation, culminating in collection of over 50 high-quality datasets covering diverse range of domains, and translation scenarios. For specialized tasks like document-level translation with limited annotated data, we sampled from resources such as news-commentary. To further refine our dataset and remove potentially low-quality samples, we employed the COMETkiwi (Rei et al., 2020) model for filtering. We set the filtering threshold at 0.55 for literature and doc-level translation tasks and 0.75 for all other tasks. After filtering, we carefully organized and categorized each dataset based on its characteristics according to our classification protocol. All collected datasets are presented in Table 15. The principles for processing each task are as follows: General Translation. Datasets lacking distinctive features are classified as general sentence-level tasks. We included small amount of domain-specific data to enhance generalizability, making this task the largest part of ComMT. Document-level Translation. This task requires the model to consider broader context and capture nuances across sentences (Maruf et al., 2022). Jin et al. (2023a) indicates that 3 preceding sentences are usually sufficient to disambiguate most discourse phenomena. Based on this finding and practical considerations, we restricted our document-level translation tasks to texts under 500 words and organized the data accordingly. 3https://opus.nlpl.eu/ 4https://www2.statmt.org/wmt23/ 5https://paperswithcode.com/datasets 24 Preprint. Under review. Domain Translation. For the colloquial domain, we focus on informal, non-written text data, mainly from social media platforms, dialogues, and subtitles. For the literature domain, considering the complex discourse phenomena in literary texts, sentence-level translation is unsuitable; thus, we construct this task as multi-sentences form. We categorize sentences into varying lengths of [100, 200, 300, 400] words, distributed in proportions of [0.2, 0.3, 0.3, 0.2], to enhance generalizability. Terminology-constrained Translation. Due to the limited availability of terminology translation data, we sampled data and used the B2NERD (Yang et al., 2024b) model to extract term pairs. Note that each sample may contain one or more terminology pairs in our data. Automatic Post-editing. We focused on collecting authentic data for this task rather than using synthetic data due to its potential lack of accuracy in representing real post-editing scenarios (Negri et al., 2018). In-context Learning Translation. We designed this task mainly to leverage the inherent In-context learning capabilities of LLMs, which potentially enable models to adapt on-the-fly. We extracted 5% of the data from each task category to create few-shot datasets. We structured these into 1-shot, 2-shot, and 3-shot data sets in proportions of [0.3, 0.3, 0.4]. B.2 Construction of Training and Test Sets For train sets, to manage the diverse data sources and prevent excessive data accumulation, we have set cap of 5,000 samples per dataset. For test sets, we ensure that the source language data is \"source-original\" to closely simulate real-world scenarios and prevent \"translationese\" effects that could negatively impact evaluation accuracy (Graham et al., 2020; Läubli et al., 2020). For general translation tasks, we keep data from WMT22 (Adelani et al., 2022), WMT23 (Kocmi et al., 2023), and Flores-200 (Costa-jussà et al., 2022), and use wmt23 as the default test set. For most other tasks, we combine samples from multiple sources and strive to maintain minimum of 500 samples in each test set to ensure reliable evaluation results. However, some languages currently lack test sets because obtaining the task data is challenging. In the future, we plan to continue expanding this dataset to support more languages and tasks."
        },
        {
            "title": "C Detailed Experimental Setups",
            "content": "C.1 Datasets and Hyperparameters Language Pair De-En Cs-En Ru-En Zh-En Before Clean After Clean 50.1M 56.3M 39.2M 40.8M 46.4M 50.4M 30.8M 33.6M Table 6: Statistics on the use of parallel data from WMT2023. Note that due to the extensive bilingual data in the En-De CommonCrawl corpus, we only sampled portion and merged it with other data to create dataset of 50M. For En-Cs, we excluded the CzEng 2.0 dataset due to licensing issues. C.2 Beam Search vs Sampling While traditional MT research predominantly employs beam search for decoding, LLM-based generation often utilizes sampling strategies for faster and more diverse outputs. This raises the question of whether LLMs, when used for translation, should follow the conventional beam search approach or adopt sampling-based decoding. To investigate this, we conducted comparative analysis of beam search (beam size = 5) and sampling (temperature = 0.7, top-k = 50, top-p = 0.8) in terms of translation quality and decoding speed (batch size = 4). As shown in Table 8, while beam search is significantly slower, it consistently yields higher translation quality. Based on these observations, we adopt beam search as the primary decoding method in this paper. 25 Preprint. Under review. Hyperparameter Learning Rate Adam β LR Scheduler Number of Epochs Global Batch Size Train Steps Warmup Ratio Weight Decay Decoding Method Beam Size Stage1 5e-4 (0.9, 0.999) inverse_sqrt 1 2,560 30,000 0.01 0.01 Stage2 2e-5 (0.9, 0.999) cosine 1 384 1,200 0.01 0.01 beam search Table 7: Hyperparameter configuration during two-stage training and decoding. In the first training stage, we use pure data parallelism because the LLM parameters are frozen. In the second stage, we employed DeepSpeed ZeRO-2 (Rajbhandari et al., 2020) for full parameters training. Models Decoding Method De2En Ru2En Zh2En En2De En2Cs En2Ru En2Zh Avg. Speed (tokens/s) TowerInstruct-7B Llama3-8B-SFT LaMaTE Beam Search Sampling Beam Search Sampling Beam Search Sampling 85.15 84.98 83.76 83. 83.90 82.82 83.18 82.49 81.66 80.58 81.46 80.53 80.36 79.79 76.97 76. 79.13 77.97 83.10 82.03 82.66 80.01 80.73 80.73 78.88 68.97 85.80 81. 86.81 86.81 85.39 84.14 82.42 82.21 82.71 82.71 85.93 84.39 80.80 82. 84.41 84.41 83.16 81.15 84.03 80.87 82.59 80.57 114 167 123 296 379 Table 8: Comparison of the effectiveness and efficiency of beam search versus sampling."
        },
        {
            "title": "D Additional Results",
            "content": "D.1 Detailed Performance on ComMT We present detailed performance of each translation task and language pair in the ComMT benchmark in Table 14. D.2 Misalignment Evaluation To assess the translation misalignment issue, we used three metrics: off-target rate, unaligned source words, and unaligned target words. Off-target refers to instances where machine-generated translations contain segments from incorrect languages or exhibit code-switching. We utilize langdetect 6 to determine the language of each translation. The off-target rate for translation is calculated by subtracting the probability of the predicted target language from 1. We then average this rate across all sentences. Unaligned source words (USW) refer to words in the source sentence that do not have corresponding translation in the target sentence. Conversely, unaligned target words (UTW) capture instances where words appear in the translation without clear support from the source sentence, indicating potential insertions or hallucinations. We employ awesome-align (Dou & Neubig, 2021) to obtain word alignments. Table 9 displays the results of these three indicators. Models Off-target Rate De Cs Llama3-8B-SFT 5.63 6.07 Tower-7B 6.52 mT5-large 4.97 LaMaTE 9.45 10.41 9.14 7.93 Ru 2.92 3.49 2.53 2. Zh 3.76 2.46 3.50 3.17 USW Rate Ru De Cs Zh De Cs USW Rate Ru 12.03 10.78 10.00 10.74 17.59 17.65 15.03 16.11 17.13 15.79 15.31 16.34 19.36 18.72 16.88 17.57 14.16 12.39 11.60 11.82 14.80 15.51 11.45 11. 14.10 13.28 12.51 12.61 Zh 22.59 23.93 18.27 17.51 Table 9: Results of the Decoder-only and Encoder-Decoder models on three indicators of misalignment. 6https://github.com/Mimino666/langdetect Preprint. Under review. D.3 Comparison of Decoding Speed Models Decoder-only Encoder-Decoder Llama2-7B Llama3-8B Llama2-13B NMT-40-8 mT5-large NLLB-3.3B LaMaTE (Ours) Dim Encoder layer Decoder layer Vocab size Params KV Cache (Kb) 4096 - 32 32k 6.73B 524b(s+t) 4096 - 32 128k 8.01B 131b(s+t) 5120 - 40 32k 13.01B 819b(s+t) 1024 40 8 128k 0.77B 32b(s+t) 1024 24 24 256k 1.23B 98b(s+t) 2048 24 24 256k 3.34B 196b(s+t) 4096-1024 32-8 8 128k 8.5B 32b(s+t) Table 10: Details on parameter and theoretical KV cache size of the compared models in our work. In the KV Cache, b, s, and denote the batch size, source sequence length, and target sequence length, respectively. Note that Llama3-8B uses GQA (Ainslie et al., 2023), resulting in smaller KV cache usage compared to Llama2-7B. We first summarize the key architectural details of the evaluated models in Table 10. Next, we evaluate the decoding efficiency of these models in Table 11 by comparing their decoding speed across different sequence lengths and batch sizes. VLLM (Kwon et al., 2023) accelerates LLM decoding but has limited beam search support and compatibility issues with some models. To ensure fair and consistent evaluation, we use the original Transformers framework across all models. The results demonstrate that LaMaTE consistently outperforms Llama3-8B in decoding speed, particularly as batch sizes and input lengths increase, achieving speedup ranging from 2.4 to 6.5. Batch Size Length Decoder-only Encoder-Decoder Llama2-7B Llama3-8B Llama2-13B NMT-40-8 mT5-large NLLB-3.3B LaMaTE (Ours) Speedup 1 4 6 8 16 24 0-100 100-200 200-300 300-400 4000-100 100-200 200-300 300-400 400-500 0-100 100-200 200-300 300-400 400-500 0-100 100-200 200-300 300-400 400-500 0-100 100-200 200-300 300-400 400-500 0-100 100-200 200-300 300-400 400-500 0-100 100-200 200-300 300-400 40035 33 31 29 29 67 53 45 38 38 114 57 47 38 38 144 59 49 39 39 162 59 OOM OOM OOM 167 OOM OOM OOM OOM 130 OOM OOM OOM OOM 32 33 34 32 33 63 62 58 52 51 123 87 77 66 65 172 100 87 73 72 210 109 93 78 278 118 99 82 81 308 121 102 OOM OOM 28 23 21 19 18 51 34 29 24 24 76 35 OOM OOM OOM 90 OOM OOM OOM OOM 100 OOM OOM OOM OOM 95 OOM OOM OOM OOM OOM OOM OOM OOM OOM 170 167 162 160 160 310 294 289 281 283 515 472 453 421 668 565 549 495 500 789 643 617 553 556 956 728 690 609 602 976 739 708 624 619 43 44 43 42 43 84 83 81 80 164 141 133 123 123 230 183 171 155 154 289 216 198 177 175 446 269 244 207 206 550 263 250 OOM OOM 48 42 38 36 77 63 55 52 51 181 112 94 82 79 217 116 95 84 83 232 119 99 87 82 281 OOM OOM OOM OOM 307 OOM OOM OOM OOM 77 83 83 82 81 152 156 154 157 155 296 276 272 263 259 416 363 355 335 334 532 446 432 404 397 816 631 589 535 1050 728 667 594 593 2.41 2.52 2.44 2.56 2.45 2.41 2.52 2.66 3.02 3.04 2.41 3.17 3.53 3.98 3.98 2.42 3.63 4.08 4.59 4.64 2.53 4.09 4.65 5.18 5. 2.94 5.35 5.95 6.52 6.58 3.41 6.02 6.54 - - Table 11: Evaluate model decoding speed (tokens/s) across various batch sizes and source sequence lengths. Speedup indicates the decoding speedup ratio of LaMaTE versus Llama3-8B. 27 Preprint. Under review. D.4 Comparison of Decoder Variants The decoder of the original transformer utilizes cross attention to integrate the encoders representation, as shown in Equation (11). We refer to this standard decoder Cross Decoder. We propose two variants of the decoder that omit the cross-attention layer. The first variant, referred to as the Concat Decoder, handles the encoders representation HE by incorporating it directly into the self-attention layers of the decoder. Specifically, in the self-attention computation, the keys and values are computed from [HE, Yl 1], while the queries are derived solely from Yl 1. Thus, this allows target tokens to integrate source and target information within single attention computation: Yl = FFN(SAtt([HE, Yl 1])) (13) The second variant, taking inspiration from recent research in multimodal language model (Liu et al., 2023), referred to as Prefix Decoder, where the encoder representations are concatenated directly with the decoders embeddings Y0 before being fed to the upper decoder layers: Yl = FFN(SAtt(Yl 1)), Y0 = [HE, Y0] (14) To preserve the bidirectional nature of source representations within the decoder, we adopt masking strategy similar to PrefixLM, ensuring that source-side tokens retain their bidirectional feature. comparative overview of the three variants is presented in Figure 11. Table 12 displays their performances. As shown, the Cross Decoder, i.e., the standard decoder, achieves the best overall performance. Figure 11: Three variants of decoders: Cross Decoder is the standard decoder, while Concat Decoder and Prefix Decoder remove the cross-attention sublayer, integrating source information through self-attention and early fusion methods, respectively. Decodery Cross Decoder Concat Decoder Prefix Decoder Training 1 Stage say 1 say 2 say 1 say 2 say 1 say 2 Cs De Zh Ru COMET BLEU COMET BLEU COMET BLEU COMET BLEU COMET BLEU 29.67 33.85 29.33 31.51 29.25 32.38 80.07 82.32 80.01 81.52 80.11 81.66 33.22 35.34 32.89 34.04 32.73 32.06 79.93 82.22 79.45 79.89 79.12 81.17 78.64 81.46 78.99 81.14 79.24 81.07 79.38 81.71 79.45 81.63 79.71 81. 31.68 39.50 30.50 32.38 30.04 37.56 26.67 28.61 26.40 28.47 26.73 27.54 85.31 86.54 85.02 85.97 85.37 85.54 29.92 34.00 30.15 33.49 30.21 33.10 Avg. Table 12: comparison of decoder variants. 28 Preprint. Under review."
        },
        {
            "title": "Resource Languages",
            "content": "High (>1%) Medium (>0.1%) Arabic (ar), German (de), Spanish (es), French (fr), Italian (it), Japanese (ja), Dutch (nl), Polish (pl), Portuguese (pt), Russian (ru), Turkish (tr), Chinese (zh) Bulgarian (bg), Bengali (bn), Catalan (ca), Czech (cs), Danish (da), Modern Greek (el), Estonian (et), Persian (fa), Finnish (fi), Hindi (hi), Hungarian (hu), Indonesian (id), Korean (ko), Lithuanian (lt), Latvian (lv), Norwegian (no), Romanian (ro), Slovak (sk), Slovene (sl), Swedish (sv), Thai (th), Ukrainian (uk), Vietnamese (vi) Afrikaans (af), Azerbaijani (az), Belarusian (be), Basque (eu), Galician (gl), Gujarati (gu), Modern Hebrew (he), Armenian (hy), Icelandic (is), Georgian (ka), Kazakh (kk), Kannada (kn), Macedonian (mk), Malayalam (ml), Mongolian (mn), Low (>0.01%) Marathi (mr), Nepali (ne), Albanian (sq), Serbian (sr), Tamil (ta), Telugu (te), Urdu (ur) Amharic (am), Aragonese (an), Assamese (as), Breton (br), Bosnian (bs), Welsh (cy), Dzongkha (dz), Esperanto (eo), Western Frisian (fy), Irish (ga), Gaelic (gd), Hausa (ha), Croatian (hr), Igbo (ig), Central Khmer (km), Kurdish (ku), Kirghiz (ky), Limburgish (li), Malagasy (mg), Malay (ms), Very-Low Maltese (mt), Burmese (my), Norwegian Bokmål (nb), Norwegian Nynorsk (nn), (<0.01%) Occitan (oc), Oriya (or), Panjabi (pa), Pashto (ps), Kinyarwanda (rw), Northern Sami (se), Serbo-Croatian (sh), Sinhala (si), Tajik (tg), Turkmen (tk), Tatar (tt), Uighur (ug), Uzbek (uz), Walloon (wa), Xhosa (xh), Yiddish (yi), Yoruba (yo), Zulu (zu) Table 13: We categorized 99 languages in OPUS-100 (Zhang et al., 2020) into four resource levels high, medium, low, and very lowbased on the proportion of data available on the internet. Preprint. Under review. Model NLLB-3.3B BigTranslate Aya-23-8B TowerInstruct-7B Bayling-13B NMT-8-8 NMT-40-8 mT5-Large Llama3-8B-Base, 3-shot Llama3-8B-Inst, 0-shot Llama3-8B-SFT LaMaTE-s1 LaMaTE-s2 Model NLLB-3.3B ALMA-7B BigTranslate Aya-23-8B TowerInstruct-7B Bayling-13B NMT-8-8 NMT-40-8 mT5-Large Llama3-8B-Base, 3-shot Llama3-8B-Inst, 0-shot Llama3-8B-SFT LaMaTE-s1 LaMaTE-s2 Model NLLB-3.3B ALMA-7B BigTranslate Aya-23-8B TowerInstruct-7B Bayling-13B mT5-Large NMT-8-8 NMT-40-8 Llama3-8B-Base, 3-shot Llama3-8B-Inst, 0-shot Llama3-8B-SFT LaMaTE Model General Translation En2De En2Cs En2Ru En2Zh De2En COMET BLEU COMET BLEU COMET BLEU COMET BLEU COMET BLEU 79.62 70.27 82.27 83.10 70.03 77.18 78.42 78.83 73.82 78.81 82.66 77.73 80.68 33.51 14.59 34.13 35.00 15.97 29.16 31.81 30.74 18.61 29.01 37.76 29.96 37.59 88.06 83.45 87.25 78.88 68.15 85.54 86.91 84.22 81.63 77.46 85.80 85.31 86.54 36.79 23.50 32.02 17.41 14.34 35.94 38.04 30.08 24.34 22.29 29.64 33.22 35.34 83.74 79.46 83.92 85.39 64.22 79.48 80.65 81.62 80.17 80.21 83.94 79.19 82.45 29.03 19.04 26.41 29.81 12.38 25.43 26.07 24.10 23.20 22.67 27.03 24.16 26. 79.66 80.60 84.03 85.93 81.88 80.29 82.05 83.59 81.89 67.46 83.72 80.88 84.05 34.84 30.99 41.39 42.66 37.89 38.78 40.73 36.89 39.51 18.51 40.66 39.61 45.98 81.35 79.36 84.29 85.15 81.39 81.33 82.85 82.93 80.45 74.40 83.76 82.12 83.75 35.42 26.43 41.24 44.33 27.85 33.33 36.99 37.12 32.84 33.20 40.62 33.39 41.41 Ru2En Zh2En Avg. En Avg. En COMET BLEU COMET BLEU COMET BLEU COMET BLEU 80.70 82.46 76.84 81.38 83.18 77.34 78.84 79.83 80.42 80.17 70.55 81.66 79.56 80.97 31.59 31.52 23.43 29.92 34.54 21.12 29.13 30.28 27.85 30.14 21.49 31.88 29.18 30.29 77.44 79.45 75.48 77.54 80.36 76.89 76.15 76.63 77.97 75.38 68.44 76.97 76.40 78.87 22.14 22.77 16.29 21.03 23.99 18.59 21.04 22.56 19.73 19.08 15.12 20.70 20.22 22. 82.77 84.28 78.45 84.37 83.33 71.07 80.62 82.01 82.07 79.38 75.99 84.03 80.78 83.43 33.54 31.33 22.03 33.49 31.22 20.15 32.33 34.16 30.45 26.42 23.12 33.77 31.74 36.46 79.83 82.18 77.23 81.07 82.90 78.54 78.77 79.77 80.44 78.67 71.13 80.80 79.36 81.20 29.72 31.66 22.05 30.73 34.29 22.52 27.83 29.94 28.23 27.35 23.27 31.07 27.60 31.24 Domain Translation(COMET) Medical Law IT De2En En2De Ru2En En2Ru Zh2En En2Zh Cs2En En2De En2Cs En2Ru En2Cs 87.32 87.06 84.79 87.15 87.51 85.62 86.37 87.12 87.50 85.14 73.47 86.65 87.10 86.65 86.10 82.63 85.88 86.89 80.11 85.01 86.69 87.03 83.32 84.79 85.86 86.39 84.49 84.56 81.04 84.08 85.44 82.47 83.24 83.85 84.74 83.11 74.20 83.95 84.36 88.71 88.02 80.14 88.04 87.77 64.29 86.63 86.93 87.79 84.61 85.72 88.94 88.95 84.29 84.83 81.70 84.82 85.40 83.00 82.90 83.61 84.42 82.46 74.58 84.85 84. 85.29 85.66 84.17 86.49 87.08 85.03 85.86 85.46 86.09 83.96 75.20 87.69 88.73 88.20 87.57 85.61 87.71 87.15 84.12 86.56 87.76 88.21 86.57 66.78 88.07 87.68 86.89 85.45 83.26 86.31 86.27 75.39 84.16 87.98 88.28 79.46 83.59 85.53 87.70 89.88 88.94 88.03 89.24 82.14 70.65 86.85 91.14 91.78 82.13 81.95 88.18 91.36 86.81 86.60 78.57 87.19 85.27 66.26 84.93 83.17 83.03 84.43 84.20 88.88 89.48 92.09 92.35 84.08 92.97 84.86 75.52 91.49 92.73 93.25 88.48 86.25 92.84 93. Colloquial Literature Avg. COMET Avg. COMET De2En En2De Ru2En En2Ru Zh2En En2Zh De2En Ru2En Zh2En En2Zh 87.09 NLLB-3.3B 88.65 ALMA-7B 82.68 BigTranslate 89.18 Aya-23-8B 88.48 TowerInstruct-7B 85.94 Bayling-13B 87.08 mT5-Large 87.79 NMT-8-8 NMT-40-8 88.98 Llama3-8B-Base, 3-shot 86.74 61.97 Llama3-8B-Inst, 0-shot 89.20 Llama3-8B-SFT 88.76 LaMaTE 81.32 82.55 69.91 83.50 81.73 71.40 78.05 78.19 79.76 75.46 73.28 85.02 83. 82.75 85.86 74.72 86.61 86.66 81.97 83.92 83.20 84.45 85.39 62.26 86.75 85.58 78.94 84.26 66.70 84.90 82.12 59.95 80.69 77.54 79.28 80.81 81.34 85.46 82.84 80.69 83.13 79.57 83.18 82.45 80.22 80.73 79.55 80.53 78.91 57.89 81.00 82.75 63.65 74.75 67.78 74.77 75.44 73.92 71.22 62.26 63.82 75.56 70.70 77.33 75.17 65.98 72.17 57.47 72.45 73.29 69.36 69.09 64.20 66.56 72.82 69.35 75.62 72.15 59.98 72.27 62.22 68.68 75.38 71.30 68.30 51.11 48.39 70.58 72.04 72.26 74. 65.94 76.54 72.72 77.01 79.37 73.87 75.36 71.86 75.10 76.32 60.14 79.75 78.51 85.04 86.86 82.05 87.56 86.79 82.73 86.85 84.52 86.09 79.51 58.92 84.73 87.53 30 En 84.32 85.76 79.30 86.28 84.57 73.20 84.17 84.20 85.23 81.68 77.76 86.63 87. En 78.44 82.09 75.76 81.86 82.72 79.79 79.94 77.05 77.76 80.73 68.32 82.57 82.25 Preprint. Under review. Model NLLB-3.3B ALMA-7B BigTranslate Aya-23-8B TowerInstruct-7B Bayling-13B mT5-Large NMT-8-8 NMT-40-8 Llama3-8B-Base, 3-shot Llama3-8B-Inst, 0-shot Llama3-8B-SFT LaMaTE Model Domain Translation(BLEU) Medical Law IT De2En En2De Ru2En En2Ru Zh2En En2Zh Cs2En En2De En2Cs En2Ru En2Cs 42.09 41.25 32.69 41.82 43.83 35.04 38.44 43.71 44.14 37.89 27.21 41.70 41.98 33.26 30.20 22.00 30.05 33.71 23.28 27.88 34.21 35.43 25.20 28.33 30.53 32. 44.36 41.06 29.48 41.01 46.31 32.82 36.80 42.18 43.40 37.57 28.12 42.70 42.04 33.54 28.97 18.81 28.83 32.37 13.71 27.38 31.68 32.48 24.07 26.47 36.18 37.64 31.29 33.32 23.56 32.63 36.13 27.43 28.16 33.36 35.28 29.33 22.31 36.24 32.76 36.81 36.75 31.56 39.09 42.04 36.40 34.86 38.55 39.82 30.84 18.24 45.76 48.97 46.54 42.46 37.05 43.92 43.18 31.26 38.61 43.68 46.12 39.71 22.53 49.07 45.16 48.74 36.61 30.83 42.46 41.83 25.50 39.24 49.85 51.68 30.64 35.31 41.08 47. 50.43 35.95 37.70 42.64 19.25 18.74 39.42 52.75 54.17 26.62 29.54 39.90 50.83 35.83 33.21 20.34 35.59 33.04 19.31 34.08 34.06 33.26 31.98 32.75 40.96 42.23 33.94 31.69 20.5 34.20 19.03 15.14 33.81 38.86 40.33 23.74 25.40 36.77 45.64 Colloquial Literature Avg. BLEU Avg. BLEU De2En En2De Ru2En En2Ru Zh2En En2Zh De2En Ru2En Zh2En En2Zh 48.09 NLLB-3.3B 49.19 ALMA-7B 32.92 BigTranslate 50.86 Aya-23-8B 53.77 TowerInstruct-7B 43.73 Bayling-13B 49.23 mT5-Large 50.55 NMT-8-8 NMT-40-8 54.16 Llama3-8B-Base, 3-shot 46.09 22.55 Llama3-8B-Inst, 0-shot 55.53 Llama3-8B-SFT 53.41 LaMaTE 42.90 38.16 20.54 42.54 42.61 28.55 36.53 41.63 43.43 28.92 32.94 49.83 48.91 32.44 37.63 19.81 40.04 39.68 27.45 32.27 32.31 34.32 36.14 16.55 41.79 36.94 20.69 26.41 8.45 28.84 26.95 12.25 24.05 23.74 24.53 23.83 25.04 31.01 29.80 20.94 22.20 15.15 21.36 22.65 20.46 21.39 21.32 21.83 18.16 9.10 22.28 24. 33.72 32.15 24.08 35.65 34.11 30.57 31.40 33.79 33.75 25.50 11.21 28.71 38.41 14.51 22.77 14.91 23.65 24.35 20.23 19.02 11.61 13.06 24.24 21.02 30.50 25.60 Doc-level Translation 20.69 24.04 7.68 25.38 27.79 18.10 20.99 16.73 18.45 25.13 24.38 36.33 26.40 5.24 12.13 3.74 10.57 15.06 9.61 9.37 4.74 4.60 11.30 11.75 16.36 16.49 11.76 13.88 10.05 16.48 18.49 13.41 13.18 11.34 13.52 14.75 7.75 21.76 21. En 34.69 31.27 22.27 34.22 31.22 21.53 31.08 35.50 36.58 26.01 24.82 36.59 40.45 En 30.62 32.61 21.70 33.12 35.28 26.61 29.43 30.02 31.54 30.56 20.55 37.25 34. Model NLLB-3.3B ALMA-7B BigTranslate Aya-23-8B TowerInstruct-7B Bayling-13B mT5-Large NMT-8-8 NMT-40-8 Llama3-8B-Base, 3-shot Llama3-8B-Inst, 0-shot Llama3-8B-SFT LaMaTE Model NLLB-3.3B ALMA-7B BigTranslate Aya-23-8B TowerInstruct-7B Bayling-13B mT5-Large NMT-8-8 NMT-40-8 Llama3-8B-Base, 3-shot Llama3-8B-Inst, 0-shot Llama3-8B-SFT LaMaTE De2En En2De En2Ru Zh2En En2Zh d-COMET d-BLEU d-COMET d-BLEU d-COMET d-BLEU d-COMET d-BLEU d-COMET d-BLEU 74.01 83.73 74.77 86.10 85.86 84.71 79.29 74.80 76.69 34.42 83.01 84.87 83.52 11.75 25.58 9.04 39.87 37.74 30.56 17.04 10.94 14.04 81.76 33.97 35.07 32. 73.19 81.10 68.47 85.19 83.41 72.55 71.22 71.76 71.40 19.31 82.89 81.62 82.17 14.56 20.14 9.80 28.92 25.61 17.50 10.69 9.78 11.78 75.99 24.09 26.35 27.45 78.10 85.93 71.34 89.15 80.93 45.83 78.99 74.49 74.33 15.26 84.27 85.25 87.02 7.37 14.29 5.61 25.79 18.57 4.82 7.28 3.58 4.72 81.27 19.32 21.78 22.56 68.34 76.53 68.01 76.61 82.53 78.62 74.71 69.76 69.89 29.39 81.91 80.62 82.68 11.14 20.97 8.48 28.05 38.88 23.69 18.11 16.00 17.26 76.52 36.35 40.85 39. 70.17 80.56 80.24 82.31 81.47 79.92 80.97 74.33 75.75 31.71 70.12 86.25 83.50 18.58 16.26 22.46 33.04 22.22 20.90 22.73 11.18 13.37 83.67 15.48 38.08 37.56 En2Cs Cs2En Avg. En Avg. En d-COMET d-BLEU d-COMET d-BLEU d-COMET d-BLEU d-COMET d-BLEU 78.43 84.74 68.86 88.44 66.40 62.70 77.80 77.24 78.54 13.67 83.53 83.21 86.57 10.41 12.38 3.98 24.36 8.22 7.80 7.24 7.73 10.54 77.69 18.99 19.49 22.92 74.34 82.69 74.65 88.33 87.13 85.48 80.39 75.92 76.81 36.83 85.23 84.98 85.36 9.24 17.71 6.17 41.14 36.69 24.23 15.45 9.48 12.75 85.34 34.60 35.28 34.95 74.97 83.08 72.23 86.27 78.05 65.25 77.25 74.46 75.01 19.99 80.20 84.08 84.82 12.73 15.77 10.46 28.03 18.66 12.76 11.99 8.07 10.10 79.66 19.47 26.43 27. 72.23 80.98 72.48 83.68 85.17 82.94 78.13 73.49 74.46 33.55 83.38 83.49 83.85 10.71 21.42 7.90 36.35 37.77 26.16 16.87 12.14 14.68 81.21 34.97 37.07 35.76 31 Preprint. Under review. Model NLLB-3.3B ALMA-7B BigTranslate Aya-23-8B TowerInstruct-7B Bayling-13B mT5-large NMT-8-8 NMT-40-8 Llama3-8B-Base, 3-shot Llama3-8B-Inst, 0-shot Llama3-8B-SFT LaMaTE Model NLLB-3.3B ALMA-7B BigTranslate Aya-23-8B TowerInstruct-7B Bayling-13B mT5-Large NMT-8-8 NMT-40-8 Llama3-8B-Base, 3-shot Llama3-8B-Inst, 0-shot Llama3-8B-SFT LaMaTE Model NLLB-3.3B ALMA-7B BigTranslate Aya-23-8B TowerInstruct-7B Bayling-13B mT5-large NMT-8-8 NMT-40-8 Llama3-8B-Base, 3-shot Llama3-8B-Inst, 0-shot Llama3-8B-SFT LaMaTE Model NLLB-3.3B ALMA-7B BigTranslate Aya-23-8B TowerInstruct-7B Bayling-13B mT5-Large NMT-8-8 NMT-40-8 Llama3-8B-Base, 3-shot Llama3-8B-Inst, 0-shot Llama3-8B-SFT LaMaTE Terminology-constrained Translation De2En En2De En2Ru Zh2En COMET BLEU TSR COMET BLEU TSR COMET BLEU TSR COMET BLEU TSR 79.50 79.49 77.88 80.95 81.35 81.33 81.74 78.80 79.35 81.73 64.85 82.53 81.83 21.18 21.41 18.06 31.71 31.09 29.76 33.70 21.26 21.75 34.34 20.68 37.28 33.03 36.03 36.38 32.25 67.25 66.27 70.28 76.34 35.62 36.56 77.08 84.67 87.15 71.05 89.59 88.49 83.85 83.35 88.85 81.58 87.50 89.37 89.45 85.18 85.99 89.85 90.17 48.37 40.64 29.16 39.86 46.57 33.65 42.76 47.79 47.99 38.04 40.56 49.62 52.12 77.52 69.95 58.41 85.78 87.54 84.10 88.30 77.14 76.83 88.30 95.03 94.27 90. 89.77 88.98 81.29 86.17 89.39 66.44 87.79 87.34 88.64 86.72 84.68 89.84 89.53 30.02 24.98 17.44 27.29 30.85 12.46 25.71 27.85 27.92 22.60 24.76 31.70 32.59 57.83 55.01 40.16 70.55 73.50 51.24 66.55 55.24 55.24 71.85 80.92 84.69 72.79 65.76 73.29 66.59 71.99 73.28 70.96 71.20 55.58 54.37 73.40 57.72 74.56 72.71 8.74 12.33 7.16 13.40 15.43 12.03 12.44 6.16 5.97 13.87 7.96 17.06 13.91 8.91 24.08 12.55 71.18 78.84 74.29 76.15 8.53 8.80 76.49 83.35 82.18 75. En2Zh En2Cs Avg. En Avg. En COMET BLEU TSR COMET BLEU TSR COMET BLEU TSR COMET BLEU TSR 85.89 85.95 83.19 87.37 87.42 85.46 86.40 84.48 85.10 85.62 72.46 89.44 89.72 40.52 38.38 32.19 46.84 48.68 42.96 40.66 38.79 39.34 40.65 32.78 56.76 58.44 64.84 62.56 50.24 88.37 90.45 92.87 85.81 60.07 61.52 90.59 95.36 96.06 87. 85.89 85.77 80.99 88.95 85.67 77.95 86.79 86.25 86.83 86.68 82.60 88.69 88.29 31.77 27.63 19.24 44.92 35.56 27.96 41.25 35.04 35.70 40.98 39.90 47.34 41.01 39.87 37.15 26.39 71.59 74.10 58.11 69.73 43.65 44.88 75.64 84.29 86.45 61.54 87.79 87.30 82.33 86.46 87.83 77.86 87.12 86.86 87.51 86.05 81.43 89.46 89.43 Automatic Post-edition 37.67 32.91 24.51 39.73 40.42 29.26 37.60 37.37 37.74 35.57 34.50 46.36 46. 60.02 56.17 43.80 79.07 81.40 71.58 77.60 59.03 59.62 81.60 88.90 90.37 78.23 72.63 76.39 72.24 76.47 77.32 76.15 76.47 67.19 66.86 77.57 61.29 78.55 77.27 14.96 16.87 12.61 22.56 23.26 20.90 23.07 13.71 13.86 24.11 14.32 27.17 23.47 22.47 30.23 22.40 69.22 72.56 72.29 76.25 22.08 22.68 76.79 84.01 84.67 73.05 De2En En2De Ru2En En2Ru COMET BLEU HTER COMET BLEU HTER COMET BLEU HTER COMET BLEU HTER 89.18 88.12 85.70 89.08 88.98 84.89 88.34 89.46 89.60 88.15 61.70 90.08 90.50 52.90 45.56 42.44 75.51 80.62 39.59 73.97 52.67 53.55 61.72 23.09 78.05 76.86 38.94 44.08 51.04 22.83 17.06 63.14 21.57 36.74 36.43 33.26 94.49 19.53 21. En2Zh 85.99 84.24 75.70 84.71 85.64 74.17 85.15 84.50 84.88 84.06 78.79 86.56 86.07 51.55 42.71 24.55 63.41 71.84 29.85 70.13 50.25 50.39 59.78 36.02 66.46 64.63 38.70 46.84 68.72 29.53 20.87 76.56 22.17 39.38 39.55 32.26 59.94 26.00 25.80 85.53 86.74 78.43 90.93 91.64 78.77 89.25 85.05 86.10 86.97 55.35 90.34 91.04 48.71 47.01 30.62 75.46 81.49 28.56 70.37 51.81 53.23 66.00 17.51 75.15 75. 42.47 42.97 59.87 19.22 14.41 92.13 26.23 37.90 36.46 29.09 112.49 20.76 19.88 85.92 85.93 77.21 88.49 91.13 51.54 81.60 84.70 85.12 88.84 79.42 91.36 90.89 40.49 35.14 21.55 54.35 75.32 7.89 35.31 43.46 39.98 62.54 20.86 69.48 68.24 53.89 58.49 75.29 40.16 19.14 218.99 86.20 49.92 52.69 32.92 95.40 25.77 26.72 COMET BLEU HTER COMET Avg. En BLEU HTER COMET En Avg. BLEU 75.87 79.05 73.91 82.84 85.73 72.20 82.15 75.99 77.08 81.94 74.51 82.91 82.35 34.04 28.71 23.21 63.29 74.12 38.00 54.63 34.69 34.85 54.66 36.41 55.08 53. 99.76 90.65 103.89 84.56 53.72 160.32 90.52 99.98 100.00 74.88 145.27 77.20 86.66 82.59 83.07 75.61 85.35 87.50 65.97 82.97 81.73 82.36 84.95 77.57 86.94 86.43 42.03 35.52 23.10 60.35 73.76 25.25 53.36 42.80 41.74 58.99 31.10 63.67 62.21 64.12 65.33 82.63 51.42 31.24 151.96 66.30 63.09 64.08 46.68 100.20 42.99 46.39 87.36 87.43 82.07 90.01 90.31 81.83 88.80 87.26 87.85 87.56 58.53 90.21 90.77 50.81 46.29 36.53 75.49 81.06 34.08 72.17 52.24 53.39 63.86 20.30 76.60 76. Table 14: Performance on ComMT benchmark. 32 HTER 40.71 43.52 55.45 21.03 15.74 77.64 23.90 37.32 36.44 31.17 103.49 20.14 20.45 Preprint. Under review. Data Set WMT 14-23-news WMT 16-21-ape8 WMT 20,22-chat"
        },
        {
            "title": "Language",
            "content": "Cs, De, Ru, Zh De, Ru, Zh"
        },
        {
            "title": "Task",
            "content": "GT"
        },
        {
            "title": "APE",
            "content": "De DT(colloquial) WMT 14, 18-22-medical10 Cs, De, Ru, Zh DT(medical) WMT 16-it WMT 20-robustness12 WMT 21, 23-terminology13 WMT 23-literary IWSLT 14-1714 IWSLT 1615 IWSLT23_OPUS_OpenSubtitles news-commentary-v18 17 GlobalVoices 18 DiscoMT19 frmt (Riley et al., 2022) PAWS-X (Yang et al., 2019) Cs, De De Cs, De, Ru, Zh Zh Cs, De, Ru, Zh De De Cs, De, Ru, Zh Cs, De, Ru De Zh De, Zh XNLI-15way (Conneau et al., 2018) De, Ru, Zh NTREX128 (Federmann et al., 2022) Cs, De, Ru, Zh DT(it) DT(colloquial)"
        },
        {
            "title": "DLT",
            "content": "APE GT, DT(colloquial) DCL GT DCL GT GT GT GT CommonMT (He et al., 2020) BMELD (Liang et al., 2021) Zh Zh GT, APE DT(colloquial) par3 (Karpinska et al., 2022b) Cs, De, Ru, Zh DLT, DT(literature) BWB (Jiang et al., 2023) UM-corpus (Tian et al., 2014) mZPRT (Xu et al., 2022) Zh Zh Zh DLT, DT(literature) GT, DT(law, literature) DLT, DT(colloquial, literature) tico19 (Anastasopoulos et al., 2020) Ru, Zh GT 7https://www.statmt.org/wmt21/translation-task.html. 8https://www.statmt.org/wmt21/ape-task.html. 9https://www.statmt.org/wmt20/chat-task.html. 10https://www.statmt.org/wmt21/biomedical-translation-task.html. 11https://statmt.org/wmt16/it-translation-task.html. 12https://statmt.org/wmt20/robustness.html. 13https://statmt.org/wmt21/terminology-task.html. 14https://wit3.fbk.eu/2014-01. 15https://wit3.fbk.eu/2016-02. 16https://iwslt.org/2024/subtitling. 17https://data.statmt.org/news-commentary/v18.1/training/. 18https://opus.nlpl.eu/GlobalVoices/corpus/version/GlobalVoices. 19https://www.idiap.ch/webarchives/sites/www.idiap.ch/workshop/DiscoMT/. 33 Preprint. Under review."
        },
        {
            "title": "Data Set",
            "content": "FGraDA (Zhu et al., 2021) NLLB (Costa-jussà et al., 2022)"
        },
        {
            "title": "Language",
            "content": "Zh Ru MULTI30k (Elliott et al., 2016) Cs, De FLORES-200 (Costa-jussà et al., 2022) Cs, De, Ru, Zh localization-xml-mt (Hashimoto et al., 2019) DEMETR (Karpinska et al., 2022a) mlqe-pe (Fomicheva et al., 2020) XQUAD (Artetxe et al., 2019) p2p-data (Jin et al., 2023b) NEJM (Liu & Huang, 2021) De, Ru, Zh De, Ru, Zh De, Ru, Zh De, Ru, Zh Zh Zh DGT-TM (Steinberger et al., 2013) Cs, De health_term (Xu & Carpuat, 2021) XCOPA (Ponti et al., 2020) MINTAKA (Sen et al., 2022) De Zh De MGSM (Shi et al., 2023) De, Ru, Zh MSLT (Federmann & Lewis, 2017) Perseus (Zheng et al., 2023) BiPaR (Jing et al., 2019) Zh Zh Zh XStoryCloze (Lin et al., 2021) Ru, Zh RELX (Köksal & Özgür, 2020) PETCI (Tang, 2022) De Zh QALD-9-Plus (Perevalov et al., 2022) De, Ru De De Ru De, Ru De, Zh De SubEdits (Chollampatt et al., 2020) hallucinations-in-nmt (Guerreiro et al., 2023) good-translation-wrong-in-context (Voita et al., 2019) CoCoA-MT (Nadejde et al., 2022) unfaithful(Zhang et al., 2024) ContraPro (Müller et al., 2018) LiteraryTranslation (Karpinska & Iyyer, 2023) ctxpro (Wicks & Post, 2023) DeCOCO (Hitschler et al., 2016) IdiomsInCtx-MT (Stap et al., 2024)"
        },
        {
            "title": "Task",
            "content": "GT GT, DT(colloquial) GT GT DT(it)"
        },
        {
            "title": "APE",
            "content": "GT DLT, DT(literature) DT(medical) DT(law) TCT GT GT GT DT(colloquial) GT, DLT, DT(it, medical) DLT, DT(literature) GT, DT(literature) GT GT, ICL GT APE APE GT, ICL, DT(colloquial) GT, DT(colloquial) GT GT, APE, DT(colloquial) Cs, De, Ru, Zh DLT, DT(literature) GT, ICL, DT(colloquial) GT GT, ICL De, Ru De De, Ru Preprint. Under review."
        },
        {
            "title": "Data Set",
            "content": "Books 20 EUbookshop"
        },
        {
            "title": "Language",
            "content": "De, Ru Cs, De, Ru TED2020 (Reimers & Gurevych, 2020) Cs, De, Ru"
        },
        {
            "title": "Task",
            "content": "DLT, DT(literature)"
        },
        {
            "title": "DLT",
            "content": "Table 15: Data composition of our ComMT. We collected as many translation-related datasets as possible and manually categorized them based on their characteristics according to our predefined category. Task abbreviations: GT - general translation, DLT - doc-level translation, DT - domain translation, TCT - terminology-constrained translation, APE - automatic post-editing, ICL - in-context learning. General translation Prompt: Translate the following text from English into Chinese. English: know that advertising is how they make their money, but all that garbage seems counterproductive if it drives people away. Chinese: 我知道广告是为他们创收的一种方式而如果大量的广告让观众反感离开似 乎会适得其反 Doc-level translation Prompt: Translate the following text from English into Chinese. English: The outliers tend to be those who die young, so that typical (median) life expectancy is higher than average life expectancy. This means that raising the average HLE can be achieved by raising the HLE of those at the bottom of the health distribution to that of the typical (median) person. This not only makes targeting inequality more attractive, but does not require path-breaking medical innovations to achieve longer lifespans just the achievement of typical outcomes for more people. With this in mind, it is urgent to close the sizeable rich-poor life-expectancy gap around 15 years in the United States. As metric for economic and social progress, targeting HLE implicitly acknowledges that aging is malleable (if it wasnt, it wouldnt be viable target). It turns out that range of behaviors and policies, as well as the environment we inhabit, influence how we age and how long we live. It is estimated that our genetics account for only one-quarter of the factors contributing to how we age. Given this malleability, it is crucial that governments focus on HLE for the maximum number of people. Such focus would also help governments confront one of the biggest challenges of the future: societal aging. Given that every country in the world is expected to experience societal aging, focusing on how well we age becomes paramount. This age malleability requires drawing distinction between chronological and biological measures of age and focusing on the latter. Chinese: 反常之处便是那些年纪轻轻便死去的人他们让典型中位寿命预期长于平 均寿命预期这意味着提高平均健康寿命预期可以通过将位于健康分布底层的人变成典 型中位健康寿命预期的人来实现这不仅让针对不平等性问题变得更有吸引力 也不必一定需要突破性的医学创新才能实现生命周期的延长而只需要让更多人实现 典型结果基于此现在迫切需要缩小美国庞大的贫富寿命预期差距大约在15年左 右作为一个经济和社会进步的指标健康寿命预期间接承认衰老具有可塑性若非如 此的话这就不是一个可行的目标一系列行为和政策以及我们所居住的环境都 影响着我们如何变老和如何延长我们的寿命据估计我们的器官大约占我们衰老的四 分之一因素考虑到这一可塑性政府必须关注最大数量人口的见刊寿命预期关注这 一点也有助于政府面对未来最巨大的挑战之一社会老龄化世界上每一个国家都会经 历社会老龄化关注我们以多么优秀的方式衰老变得至关重要这一年龄可塑性需要区 分年龄的时间和生物指标专注于后者 20https://opus.nlpl.eu/Books/corpus/version/Books 21https://opus.nlpl.eu/EUbookshop/corpus/version/EUbookshop 35 Preprint. Under review. Domain translation Domain: Medical Prompt: Translate the following text from English into Chinese. English: The median age of the 30 patients was 56.5 (28-80) years old, among them, 25 patients were primary plasma cell leukemia, and 5 patients were secondary plasma cell leukemia. Chinese: 30例PCL患者中位年龄56.528-80岁25例为原发性浆细胞白血病5例为继 发性浆细胞白血病 Domain: Law Prompt: Translate the following text from English into Chinese. English: Article 8 Small and medium-sized enterprises shall observe the laws and regulations of the State on labor safety, occupational health, social security, protection of resources and environment, quality, taxation and finance, etc. and shall operate and manage according to law, and may not infringe upon the legitimate rights and interests of the employees or damage the public interests. Chinese: 第八条 中小企业必须遵守国家劳动安全职业卫生社会保障资源环保质 量财政税收金融等方面的法律法规依法经营管理不得侵害职工合法权益不 得损害社会公共利益 Domain: IT Prompt: Translate the following text from English into Chinese. English: If you are using Customer Portal and want to allow self-registration, follow these steps: Chinese: 如果您正使用客户入口网站并希望允许自助注册,请按以下步骤操作: Domain: Colloquial Prompt: Translate the following text from English into Chinese. English: Believe me, Im gonna take care of you and hes gonna be OK. Chinese: 相信我,我会照顾好你们,他会没事的. Domain: Literature Prompt: Translate the following text from English into Chinese. English: The President required the name of that citizen. The accused explained that the citizen was his first witness. He also referred with confidence to the citizens letter, which had been taken from him at the Barrier, but which he did not doubt would be found among the papers then before the President. Chinese: 庭长问那公民是谁被告说那公民便是他的第一个证人他还很有把握地提起 那人的信那是在城门口从他身上取走的他相信可以在庭长的卷宗中找到 Terminology-constrained translation Prompt: Translate the following text from English into Chinese using the provided terminology pairs, ensuring the specified terms are accurately translated as indicated. Terminology pairs: \"National Football League\" = \"国家橄榄球联盟\" English: Tims younger brother, Tod Leiweke, is currently the chief operating officer of the National Football League since 2015. Chinese: 蒂姆的弟弟托德莱维克自 2015 年以来担任国家橄榄球联盟的首席运营官 Automatic post-edition Prompt: Improve the following machine-generated translation from English to Chinese. Correct errors and generate more accurate translation. English: unfazed, Matteo hires an assassin to bomb the resort to create chaos and mayhem. Chinese: 马特奥不慌不忙地雇用了一名刺客来轰炸制造混乱和混乱的手段. Improved translation: 马特奥不慌不忙地雇用了一名刺客来轰炸度假村从而引起混乱 Table 16: Examples from ComMT."
        }
    ],
    "affiliations": [
        "NLP Lab, Northeastern University, Shenyang, China",
        "NiuTrans Research, Shenyang, China"
    ]
}