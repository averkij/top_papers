{
    "paper_title": "Thinker: Learning to Think Fast and Slow",
    "authors": [
        "Stephen Chung",
        "Wenyu Du",
        "Jie Fu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent studies show that the reasoning capabilities of Large Language Models (LLMs) can be improved by applying Reinforcement Learning (RL) to question-answering (QA) tasks in areas such as math and coding. With a long context length, LLMs may learn to perform search, as indicated by the self-correction behavior observed in DeepSeek R1. However, this search behavior is often imprecise and lacks confidence, resulting in long, redundant responses and highlighting deficiencies in intuition and verification. Inspired by the Dual Process Theory in psychology, we introduce a simple modification to the QA task that includes four stages: Fast Thinking, where the LLM must answer within a strict token budget; Verification, where the model evaluates its initial response; Slow Thinking, where it refines the initial response with more deliberation; and Summarization, where it distills the refinement from the previous stage into precise steps. Our proposed task improves average accuracy from 24.9% to 27.9% for Qwen2.5-1.5B, and from 45.9% to 49.8% for DeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone achieves 26.8% accuracy using fewer than 1000 tokens, demonstrating substantial inference efficiency gains. These findings suggest that intuition and deliberative reasoning are distinct, complementary systems benefiting from targeted training."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 7 9 0 1 2 . 5 0 5 2 : r Thinker: Learning to Think Fast and Slow Stephen Chung DualityRL Wenyu Du DualityRL Jie Fu Shanghai AI Lab"
        },
        {
            "title": "Abstract",
            "content": "Recent studies show that the reasoning capabilities of Large Language Models (LLMs) can be improved by applying Reinforcement Learning (RL) to questionanswering (QA) tasks in areas such as math and coding. With long context length, LLMs may learn to perform search, as indicated by the self-correction behavior observed in DeepSeek R1. However, this search behavior is often imprecise and lacks confidence, resulting in long, redundant responses and highlighting deficiencies in intuition and verification. Inspired by the Dual Process Theory in psychology, we introduce simple modification to the QA task that includes four stages: Fast Thinking, where the LLM must answer within strict token budget; Verification, where the model evaluates its initial response; Slow Thinking, where it refines the initial response with more deliberation; and Summarization, where it distills the refinement from the previous stage into precise steps. Our proposed task improves average accuracy from 24.9% to 27.9% for Qwen2.5-1.5B, and from 45.9% to 49.8% for DeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone achieves 26.8% accuracy using fewer than 1000 tokens, demonstrating substantial inference efficiency gains. These findings suggest that intuition and deliberative reasoning are distinct, complementary systems benefiting from targeted training."
        },
        {
            "title": "Introduction",
            "content": "Multiple studies have shown that the reasoning capabilities of Large Language Models (LLMs) can be enhanced by applying Reinforcement Learning (RL) to question-answering (QA) tasks [1, 2, 3], demonstrating impressive mathematical and coding performance across benchmarks. With long context lengths, an interesting emergent behavior is self-correction within the chain-of-thought (CoT), where the LLM learns to perform search, such as verifying its steps, backtracking, and trying alternative paths. However, it has been observed that this emergent search tends to be inefficientthe CoT is often long and redundant [4, 5]. For example, Deepseek R1s reasoning typically involves excessive backtracking and verification [1]. likely cause is inefficient temporal credit assignment: for instance, in the GRPO algorithm used to train Deepseek R1, the entire generation sequence receives the same scalar advantage. That is, if the final answer is correct, the probability of the whole sequence is increasedregardless of which parts were actually useful. As result, futile search paths and uncertain verifications are also rewarded, as long as the correct solution is eventually produced. Consequently, intuitionthe ability to identify promising search paths rapidlyand verificationthe ability to evaluate search path confidentlyare not explicitly trained and may therefore be underdeveloped. typical RL solution to this issue is to use more precise temporal credit assignment, such as incorporating critic to compute more accurate advantage for each token, as in PPO [6]. However, studies show that PPO performs similarly to GRPO [7, 2]indicating that the critic may not be Equal contribution. Correspondence to: stephen.chung@dualityrl.com. Preprint. Under review. accurate enough for token-level credit assignment. Another approach is to use lower discount rate or shorter context length to encourage more efficient search; however, this may hinder the emergence of search behavior, as studies show that long context length is necessary for strong performance [8, 9]. To address this dilemma, we draw inspiration from how human decision-making is modeled under Dual Process Theory [10]. According to this theory, humans possess two distinct but complementary cognitive systems: System 1, which operates quickly and intuitively based on heuristics but is prone to biases, and System 2, which is slower, more deliberate, and capable of reflective reasoning. Within this framework, typical decision-making process unfolds as follows: 1. System 1 rapidly generates candidate option based on intuition. 2. System 2 evaluates this option through mental simulation. 3. If the option passes verification, it is implemented; otherwise, System 2 attempts to refine it. Figure 1: Conceptual model of the interaction between Fast Thinking and Slow Thinking modes in the Thinker task, based on Dual Process Theory. 4. If refinement fails, the process returns to System 1 for another option. Inspired by this decision-making process, we propose the Thinker task as an alternative to the standard QA task. In typical QA task, the model receives question and generates final answer in single pass. binary reward is given based solely on the correctness of the final answer. In contrast, the Thinker task decomposes the response into four-step process: 1. Fast Thinking: The agent generates an initial answer using small token budget. 2. Verification: The agent evaluates the correctness of the initial answer using large token budget. If verified, it is accepted as the final answer. 3. Slow Thinking: If the initial answer fails verification, the agent can produce another final answer, using large token budget. 4. Summarization: The agent summarizes the reasoning from the slow thinking step into concise summary that leads to the same final answer. We design distinct reward signals for each step, aiming to enhance different capabilities of the agent: intuition from Fast Thinking, evaluation from Verification, refinement from Slow Thinking, and integration from Summarization. Crucially, the reward signal for each task is restricted to that task alone. This separation allows for more precise temporal credit assignment by isolating learning signals for each task. For example, in the Fast Thinking task, the agent receives binary reward based on the correctness of the initial answer, encouraging it to identify promising search paths under strict token budgetsthereby strengthening intuition. Meanwhile, the Slow Thinking task preserves the opportunity for the agent to learn more general search strategy to refine previously incorrect answers. The design facilitates virtuous loop between intuition and reasoning. Fast Thinking helps Slow Thinking by providing promising initial search path, while Slow Thinking helps Fast Thinking by refining flawed intuition. This bidirectional refinement mirrors how expert human decision-making evolves through repeated interactions between intuition in System 1 and reasoning in System 2 [11]. Experimental results validate our approach: relative to the QA task, the Thinker task yields consistent gains across diverse math benchmarks, with average relative performance gains of 11.9% for Qwen2.5-1.5B models and 8.50% for DeepSeek-R1-Distill-Qwen-1.5B models. Furthermore, our analysis reveals notable reduction in reflection patterns, suggesting more direct reasoning. In summary, the proposed Thinker task offers the following key strengths: Specialized Training: Dedicated sub-tasks and rewards are designed to explicitly train distinct agent capabilities, providing richer and more targeted learning signals. General Applicability: The Thinker task can replace standard QA tasks without imposing constraints on the choice of RL algorithm or model architecture. Inference Efficiency: The Fast Thinking mode, requiring minimal token generation, can be deployed standalone for simpler tasks, offering flexible trade-off between performance and computational cost during inference. Strong Empirical Performance: Our experiments demonstrate that agents trained with the Thinker task consistently outperform those trained on standard QA tasks across various benchmarks."
        },
        {
            "title": "2 Background",
            "content": "In single-turn QA task, question is sampled from dataset, and the LLM generates response to the question. Concretely, let the dataset be denoted as = {(x(i), i=1, where x(i) denotes the i-th question, (i) is its corresponding ground-truth answer, and is the size of the dataset. Let πθ( x) denote the models policy, parameterized by θ. response πθ( x) is sampled for question x. The objective is to maximize: (i))}N J(θ) = Ex,yD[R(a, y)], (1) where πθ( x), and is the reward function, such as binary function that returns 1 if the extracted answer from matches the ground-truth answer y, and 0 otherwise. In more general multi-turn task, we allow the dialogue to continue after the first response. Concretely, we denote xt and at as the prompt and model response at turn t. The initial prompt x0 is randomly sampled from the dataset D. To generate the subsequent prompt xt, we define the transition function xt = g(x0:t1, at1) (with xa:b endpoint-inclusive), which determines the next prompt based on previous prompts and responses, or whether to terminate the episode. Thus, the objective in the multi-turn task becomes: J(θ) = Ex0,yD (cid:34) (cid:88) (cid:35) Rt(a0:t, y) , (2) where at πθ( x0:t, a0:t1), that is, the response is conditioned on all previous prompts and responses, and is the terminal step. t="
        },
        {
            "title": "3 Method",
            "content": "In the proposed Thinker task, we decompose the QA task into four steps. The whole task occurs within single dialogue, meaning that the agent receives all prompts and responses from previous steps in addition to the current prompt. An illustration of the Thinker task is shown in Figure 2. 3.1 Task Description Step 1 - Fast Thinking. In the first step, the agent is prompted to answer the given question concisely within strict token budget. The response in this step is restricted to relatively short maximum generation length (e.g., 1000 tokens), meaning that the response will be truncated if it exceeds this length. The reward Rfast in this step is defined as binary function based on the correctness of the extracted answer. Specifically, let yfast denote the extracted answer; then Rfast = 1{yfast = y}. The agent always proceeds to the next step after the response. Motivation. The motivation of this step is to explicitly train the agents intuition. As the agent must generate response under strict token budget, it cannot search extensively. It is usually restricted to few search paths, which are directly reinforced if one leads to the correct answer. Step 2 - Verification. In the second step, the agent is prompted to verify whether the fast answer yfast is correct, and must output either Yes or No. The response in this step is restricted to relatively long maximum generation length (e.g., 6000 tokens). The reward Rverify in this step is defined as weighted binary function based on the correctness of the verification. Specifically, let yverify denote the extracted answer; then: Rverify = (cid:26)(1 pfast-acc) 1{yverify = Yes} pfast-acc 1{yverify = No} if yfast = y, otherwise, (3) 3 Figure 2: The four-step Thinker task. Each stage involves user prompt, model response, and specific rewards and transition conditions designed to train distinct agent capabilities (intuition, evaluation, refinement, and integration). Reward function details are in the main text. where pfast-acc denotes the trailing accuracy of the fast thinking step (averaged over the batch). This weighting is used to balance the two classes, so as to discourage the agent from always outputting Yes or No when the accuracy of the fast thinking step is too high or too low. The transition function to the next step depends on whether the agent is in training or inference mode. In inference mode, if the agent answers Yes, then the fast answer is chosen as the final answer, and the episode terminates; otherwise, the agent proceeds to the next step. In training mode, if the fast answer is correct, then it is chosen as the final answer; otherwise, the agent proceeds to the next step. The distinction between training and inference mode aims to ensure that, during training, the Slow Thinking step primarily focuses on instances where the fast answer was incorrect. This prevents the agent from needing to re-verify an already correct fast answer in the Slow Thinking stage. However, during inference, we do not have access to the ground-truth answer, so we must rely on the agents verification. Motivation. The motivation of the second step is to explicitly train the agents evaluation capability. The agent receives clear binary reward based on whether its verification result is correct. Verifying an answer is often easier than generating one. If the fast answer is already verified to be correct, there is no need to proceed further, thus saving computational cost. Step 3 - Slow Thinking. In the third step, the agent is prompted that the fast answer has been verified to be incorrect and is asked to try an alternative answer. The response in this step is restricted to relatively long maximum generation length (e.g., 6000 tokens). The reward Rslow in this step is defined as binary function based on the correctness of the extracted answer. Specifically, let yslow denote the extracted answer; then Rslow = 1{yslow = y}. In both inference and training modes, yslow is always chosen as the final answer if the Slow Thinking step is executed. In inference mode, the episode ends here. In training mode, the agent proceeds to the next step if the slow answer yslow is correct; otherwise, the episode ends. This distinction exists because the purpose of the next stepsummarizationis to distill the long and correct response in this step to improve intuition, which is not applicable in inference mode. 4 Motivation. The motivation of this step is to encourage the agent to learn to refine incorrect fast answers for difficult questions. It should learn to use the reasoning from the verification step and revise errors to arrive at the correct answer. If such refinement is not possible, it should learn to try an alternative approach, leveraging the generous token budget for generation. Step 4 - Summarization. In the fourth step, the agent is prompted that the previous slow answer is correct and is asked to concisely summarize the steps leading to it. Crucially, the prompt for the Summarization step includes the original question again, mirroring its presentation in the Fast Thinking prompt. The response in this step is restricted to relatively short maximum generation length (e.g., 1000 tokens). The reward Rsummary in this step is designed based on two criteria: 1. Correctness: The extracted answer from the summary, ysummary, should be the same as the previous slow answer, meaning it should not produce summary that leads to an incorrect answer. 2. Consistency: The response should be consistent with what the model is expected to produce in the Fast Thinking modethat is, its probability conditioned on the Fast Thinking prompt should not be unduly low. For example, directly outputting the final answer without intermediate steps is considered inconsistent, as the likelihood of producing the correct answer directly under Fast Thinking mode is typically very low. Combined, the reward function in this step is defined as: Rsummary = 1{ysummary = yslow} + log (asummary xfast), (4) where xfast is the prompt in Fast Thinking step (i.e., the initial prompt), log (asummary xfast) is the log probability of the summary response under the Fast Thinking prompt, and is scalar hyperparameter. In experiments, we found that the agent sometimes still degenerates to give very short answer despite the log probability term. To mitigate this, we gate the reward to 0 if the length of the generated response is less than low threshold (e.g., 300 tokens). Motivation. The motivation of the final step is to reinforce concise reasoning patterns by rewarding correct and consistent summaries. key design element is that the original question is re-presented in the Summarization prompt, mirroring its appearance in the Fast Thinking step. The agent is trained to produce concise reasoning trace that leads to the correct answer for this input. This encourages the model to form strong association between the original question and correct, concise solution path. We hypothesize that this targeted reinforcement distills the successful but lengthy reasoning from Slow Thinking into compact form suited to the Fast Thinking modethereby improving the agents intuition. In addition to intuition, this step also trains the agents integration ability, as it must extract and condense key reasoning steps from the longer trace generated in the previous step. 3.2 Training with the Thinker Task Training LLMs with the Thinker task requires particular considerations regarding reward propagation. Since the reward at each step is specific to that step alone, it should not be propagated backward to earlier steps. This implies that the discount factor between steps should be set to 0, while that within each step should be high (e.g., 1) to enable effective credit assignment over tokens. The Thinker task defines only the RL environment and imposes no restrictions on the choice of algorithm or model architecture, allowing compatibility with any standard RL method (e.g., PPO) and LLM."
        },
        {
            "title": "4 Related Work",
            "content": "Environment Augmentation. Our Thinker task is form of environment augmentation for QA, inspired by Dual Process Theory and related to concepts like the Thinker MDP [12, 13]. While Thinker MDP provides agents with simulated world model for interaction before action, our task structures QA into stages where self-generated intermediate outcomes guide subsequent reasoning. This contrasts with multi-attempt tasks [14] that allow iterative revision but require ground-truth access during inference, constraint our method avoids. RL and Reasoning in LLMs. large number of studies have demonstrated the effectiveness of applying RL to enhance the reasoning capabilities of LLMs [1, 2, 3, 9]. Our work builds on these efforts by decomposing the QA task into the four-step Thinker task. It has been observed that LLMs 5 trained with RL can produce inefficient CoT [4], leading to overthinking [15]. Our work relates to strategies for controlling generation length (often termed token budgets). Examples include dynamic token budgets that scale with problem complexity [5], or user-defined budgets [16, 17]. For instance, Muennighoff et al. [18] utilize token budget controls during the generation of CoT data for supervised fine-tuning (SFT). While these approaches primarily focus on token budget control within single response generation or for SFT data preparation, our method introduces structured, multi-step RL task explicitly designed to train distinct agent abilities. In addition, our work is related to methods for encouraging self-correction in LLMs [19]. For instance, methods like Self-Refine [20] and Reflexion [21] primarily use prompt engineering and few-shot examples to enable agents to incorporate internal or external feedback for refining subsequent responses. However, unlike our work, these methods typically do not involve RL fine-tuning of the agent for these self-correction behaviors; instead, the correction capability is elicited at inference time through prompting."
        },
        {
            "title": "5 Experiments",
            "content": "This section details the experiments conducted to assess whether the Thinker task can more effectively enhance LLM reasoning capabilities compared to standard QA task. We focus on the mathematical reasoning domain here. 5.1 Experimental Setup To evaluate the Thinker task, we fine-tune two publicly available models: Qwen2.5-1.5B (Q1.5B) [22] and DeepSeek-R1-Distill-Qwen-1.5B (R1.5B) [1]. While sharing base architecture, R1.5B has undergone additional distillation using reasoning data from DeepSeek-R1, endowing it with stronger initial reasoning and search behaviors compared to the base Q1.5B. Training both models allows us to investigate the Thinker tasks impact on model with foundational capabilities (Q1.5B) and its ability to further enhance model already specialized for reasoning (R1.5B). We fine-tune these models using RL on both the Thinker task and standard QA task (serving as our baseline). For all experiments, we employ PPO [6]. Key hyperparameters include discount rate γ = 1, GAE lambda λ = 1, and sampling temperature of 1. No KL penalty against reference policy was applied. The Fast Thinking and Summarization stages use 1000-token budget, while Verification and Slow Thinking use 6000-token budget. Most other hyperparameters and training details mirror those in Open-Reasoner-Zero, with details provided in Appendix A. For the training data, we utilized the 129K math question-answering dataset provided by OpenReasoner-Zero. Each training run for both the Thinker task and the baseline required approximately 7 days on two compute nodes, each equipped with 8 A100 GPUs. Our implementation, adapted from the Open-Reasoner-Zero codebase [3], is publicly available at https://github.com/DualityRL/ thinker-task. 5.2 Training Dynamics and Evaluation Results The training performance is shown in Figure 3, measured by accuracy on questions in the training data. For the Thinker task, we plot the fast accuracy (the accuracy of the agent during the Fast Thinking step) and the final accuracy (the accuracy of the final answer). Note that this final accuracy is not directly comparable to that of the baseline, as the Thinker-task agent effectively has two attempts during training due to the transition function from Verification to Slow Thinking. Nonetheless, we observe that both its fast and final accuracy improve steadily, while the accuracy of the baseline plateaus rather quickly. This suggests that the Thinker-task agent is developing stronger underlying intuition for the task, potentially more so than just acquiring generic search algorithm, which may contribute to its sustained improvement. We also observe that the fast accuracy exceeds the baselines accuracy in the Q1.5B model, indicating the effectiveness of the Fast Thinking mode. To evaluate the models, we consider the following common benchmarks: MATH500, AIME2024, AIME2025, GPQA Diamond, Olympiadbench, AMC23, and Minerva Math. We evaluate agent Pass@1 accuracy on these benchmarks every 50 training steps. The average performance across the benchmarks during training is shown in Figure 4. The trend is similar to the training performance, with the agent trained on the Thinker task surpassing the baseline. Detailed breakdowns for each benchmark are provided in Appendix B. Notably, for the R1.5B model, moderate gap exists between 6 (a) Qwen2.5-1.5B (b) DeepSeek-R1-Distill-Qwen-1.5B Figure 3: Accuracy on training set. (a) Qwen2.5-1.5B (b) DeepSeek-R1-Distill-Qwen-1.5B Figure 4: Evaluation performance across seven common benchmarks. its final accuracy and fast accuracy. This disparity is likely attributable to R1.5Bs strong inherent reasoning capabilities, enabling its Slow Thinking mode to effectively utilize the larger token budget for refining initial answers. The performance of the final models, evaluated on the aforementioned benchmarks and the additional CollegeMath benchmark, is detailed in Table 1. Due to the large number of questions in CollegeMath, we did not evaluate training checkpoints on this benchmark, only the final models. In addition to the Fast Accuracy and Final Accuracy of the Thinker-task agent and the accuracy of the fine-tuned baseline agent, Table 1 also includes results for: (1) the pretrained model (the model before any fine-tuning); (2) reported figures from Open-Reasoner-Zero [3] (labelled ORZ); and (3) reported figures from SimpleRL [2]. Both ORZ and SimpleRL utilize PPO to train agents on standard QA tasks. From Table 1, we observe that the Thinker-task agent consistently performs better than the other methods across almost all benchmarks. For the Q1.5B model, the Thinker-Fast agents performance already surpasses that of the baseline, while the full Thinker-task agent achieves 11.9% relative performance gain on average compared to this baseline. For the R1.5B model, the Thinker-Fast agent performs slightly worse than the baseline but still significantly outperforms the pretrained model. This result for Thinker-Fast is notable, suggesting substantial efficiency gain, as it only requires 1000 token budget compared to the 8000 token budget of the pretrained model. The full Thinker-task model with R1.5B surpasses the baseline by an average relative increase of 8.50%. These results collectively suggest the benefits of decomposing the single-turn QA task into the proposed four-step Thinker task. 5.3 Analysis and Case Study Reflection and Response Length. Beyond overall performance, we investigated whether the Thinker task encourages more direct reasoning with less overt self-reflection or self-doubt. We compiled vocabulary list of terms commonly associated with self-reflection in Deepseek R1s outputs (e.g., \"wait,\" \"however,\" \"alternatively\") and measured the average occurrence of these reflection patterns 7 Table 1: Performance comparison across various mathematical reasoning benchmarks. Average (Avg.) scores are presented. All scores are Pass@1 accuracy (%) averaged over 16 samples. Top score in each benchmark column is bolded. Standard errors are provided in Appendix B. Method MATH AIME AIME GPQA Olympiad AMC Minerva College Math Math 2025 Diamond bench 2024 500 23 Qwen2.5-1.5B (Q1.5B) Pretrained 9.05 Baseline 57.98 64.25 Thinker Thinker-Fast 61.60 58.00 ORZ 59.00 SimpleRL 0.00 3.33 6.25 6.25 3.50 4.20 0.00 3.33 2.50 2.50 1.00 - 4.55 21.46 23.74 26.39 16.80 - DeepSeek-R1-Distill-Qwen-1.5B (R1.5B)"
        },
        {
            "title": "76.21\nPretrained\n86.24\nBaseline\n87.02\nThinker\nThinker-Fast 77.21",
            "content": "17.50 35.42 35.62 11.46 17.92 23.75 27.71 11.46 13.76 25.69 36.08 30.08 3.09 24.54 28.11 24.78 - 21.00 37.46 49.22 54.21 40.39 4.06 34.38 40.62 35.94 - 35. 2.30 17.78 19.03 18.66 - 20.20 55.94 72.81 81.72 59.22 24.82 32.08 33.23 29.23 7.40 36.21 38.33 37.85 - - 38.85 42.02 42.77 41.37 Avg. 3.81 24.88 27.85 26.75 - - 35.31 45.90 49.80 37.55 during training, with results shown in Figure 5b. We observed that the Thinker-task agent tends to use fewer such reflection patterns in its reasoning trace, suggesting it may be learning to reason more directly. Paradoxically, despite reduced reflection patterns, the Thinker agents responses were generally longer than the baselines (Figure 5b). This increased length primarily stems from repeated self-verifications within the Verification step (e.g., \"Let me check... Yes. Let me check again... Yes.\"), explaining the growing verification length in Figure 5b. This behavior is likely incentivized by our answer extraction focusing only on the last boxed output, encouraging reiteration and refinement if tokens remain. Future work could mitigate this by, for instance, permitting only single boxed answer in the Verification step. (a) Reflection Pattern (b) Response Length Figure 5: Average reflection pattern count and response length for DeepSeek-R1-Distill-Qwen-1.5B (R1.5B) during training. Role of Summarization. Finally, we investigated the importance of the Summarization step through an ablation analysis. We trained Q1.5B model using modified Thinker task where the Summarization step was removed. The average reflection pattern count and response length for the Thinker task, both with and without the Summarization step, are presented in Figure 6. Without the Summarization step, the model exhibited more frequent increase in reflection patterns, accompanied by highly fluctuating response lengths during training. This suggests that the Summarization step may contribute to stabilizing the learning process, potentially by discouraging degeneration into excessive reflection. Removing the Summarization step negatively impacted Fast Thinking: average Fast Accuracy on benchmarks dropped from 26.75% (with summarization) to 24.84% (without). This suggests summarization, by potentially distilling concise reasoning, enhances intuition for Fast Thinking. 8 Interestingly, final accuracy remained comparable (27.85% with vs. 27.84% without summarization), indicating that the Slow Thinking step could often compensate for the reduced fast accuracy. Detailed ablation results can be found in Appendix B. (a) Reflection Pattern (b) Response Length Figure 6: Average reflection pattern count and response length for Qwen2.5-1.5B (Q1.5B) during training, with and without Summarization stage. Case Study. We conducted case study to examine how the agents reasoning adapts across the Thinker task stages. Observations from representative outputs (see Appendix for full examples) highlight the importance of the Fast Thinking mode. When an initial Fast Thinking output is incorrect, the agent first engages in Verification stage to assess its own answer with clear reference to the steps given in the Fast Thinking mode. If errors are identified, the subsequent Slow Thinking stage involves detailed refinement. During Slow Thinking, the agent explicitly scrutinizes the flawed reasoning from the Fast Thinking attempt and insights from its own Verification, endeavoring to generate new, correct solution. Furthermore, after successful correction, the agent learns to distill the long reasoning into concise explanations during the last Summarization stage."
        },
        {
            "title": "6 Limitation",
            "content": "This study has several limitations. Firstly, our experiments were confined to two 1.5B parameter models, and we did not investigate the scalability of the Thinker task to larger models due to computational constraints. Secondly, hyperparameters, such as token limits for each task stage, were set heuristically without systematic tuning, suggesting our reported results may not be optimal. Lastly, agents trained with the Thinker task generally produce slightly longer responses than standard QA agents, potentially impacting computational efficiency at inference if all stages are utilized."
        },
        {
            "title": "7 Future Work and Conclusion",
            "content": "In this paper, we introduced the Thinker task, novel approach that decomposes the standard singleturn question-answering process into structured four-stage task. This decomposition aims to provide more explicit reward signals for training distinct agent capabilities: intuition via Fast Thinking, evaluation via Verification, refinement via Slow Thinking, and integration via Summarization. Our experiments demonstrate significant performance improvements across majority of mathematical reasoning benchmarks. Furthermore, our analysis indicates that agents trained with the Thinker task exhibit reduction in overt reflection patterns during their reasoning process, suggesting more direct problem-solving. Beyond the specific application to question-answering, this work underscores the potential of environment augmentation in RL. In RL, while significant attention is often devoted to algorithm development, the environment itself is typically treated as given problem specification. However, as demonstrated by this paper and prior research such as the Thinker MDP, there is considerable untapped potential in designing environments that offer richer inputs, more structured interactions, or more nuanced reward signals. Future research could explore alternative methods for enriching RL environments, perhaps by developing more dynamic tasks that adapt to an agents learning state or that explicitly target the development of wider array of cognitive capabilities. Such advancements in environment design could unlock new levels of performance and generalization in RL agents."
        },
        {
            "title": "Acknowledgement",
            "content": "Stephen Chung completed this work at the University of Cambridge, while Wenyu Du completed this work at the University of Hong Kong."
        },
        {
            "title": "References",
            "content": "[1] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [2] Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025. [3] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025. [4] Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187, 2024. [5] Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyu Zhao, Shiqing Ma, and Zhenyu Chen. Token-budget-aware llm reasoning. arXiv preprint arXiv:2412.18547, 2024. [6] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [7] Jian Hu. Reinforce++: simple and efficient approach for aligning large language models. arXiv preprint arXiv:2501.03262, 2025. [8] Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chain-of-thought reasoning in llms. arXiv preprint arXiv:2502.03373, 2025. [9] Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1preview with 1.5b model by scaling rl. https://github.com/agentica-project/rllm, 2025. Notion Blog. [10] Daniel Kahneman. Thinking, fast and slow. macmillan, 2011. [11] Gary Klein. Sources of power: How people make decisions. MIT press, 2017. [12] Stephen Chung, Ivan Anokhin, and David Krueger. Thinker: Learning to plan and act. Advances in Neural Information Processing Systems, 36:2289622933, 2023. [13] Kevin Wang, Jerry Xia, Stephen Chung, and Amy Greenwald. Dynamic thinker: Optimizing decision-time planning with costly compute. In The Seventeenth Workshop on Adaptive and Learning Agents. [14] Stephen Chung, Wenyu Du, and Jie Fu. Learning from failures in multi-attempt reinforcement learning. arXiv preprint arXiv:2503.04808, 2025. [15] Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, et al. Stop overthinking: survey on efficient reasoning for large language models. arXiv preprint arXiv:2503.16419, 2025. [16] Pranjal Aggarwal and Sean Welleck. L1: Controlling how long reasoning model thinks with reinforcement learning. arXiv preprint arXiv:2503.04697, 2025. [17] Yuhui Xu, Hanze Dong, Lei Wang, Doyen Sahoo, Junnan Li, and Caiming Xiong. Scalable chain of thoughts via elastic reasoning. arXiv preprint arXiv:2505.05315, 2025. 10 [18] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. [19] Ryo Kamoi, Yusen Zhang, Nan Zhang, Jiawei Han, and Rui Zhang. When can llms actually correct their own mistakes? critical survey of self-correction of llms. Transactions of the Association for Computational Linguistics, 12:14171440, 2024. [20] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:4653446594, 2023. [21] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652, 2023. [22] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [23] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pages 35053506, 2020. [24] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [25] Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael Jordan, et al. Ray: distributed framework for emerging {AI} applications. In 13th USENIX symposium on operating systems design and implementation (OSDI 18), pages 561577, 2018."
        },
        {
            "title": "A Experimental Details",
            "content": "This section describes the details of the experiment in Section 5. Our implementation, adapted from the Open-Reasoner-Zero codebase [3], is publicly available at https://github.com/DualityRL/ thinker-task. A.1 Hyperparameters We use the same hyperparameters as those provided in Open-Reasoner-Zero [3], except that we reduce the number of samples per prompt from 64 to 32 to save computational resources. One training step proceeds as follows: we first randomly sample 128 prompts (rollout batch size) from the training dataset and generate 32 samples per prompt, totaling 128 32 = 4,096 samples. We then divide the generated samples into 1 (12) training batch for the actor (critic), where each training batch is used for one optimizer update. We tune the coefficient in Rsummary by searching over {1e-4, 1e-3, 1e-2}. Other Thinker-taskspecific hyperparameters are selected using heuristics. We use lower sampling temperature during summarization, as we observe that higher temperatures tend to produce less concise and consistent summaries. For the baseline model, we use similar hyperparameters, except with generation length of 8,000 tokens. We found that 8,000 tokens yield optimal baseline performance on R1.5B. Table 2: Hyperparameters used in experiments. Parameter PPO Rollout Batch Size Number of Samples Per Prompt Number of Epochs Actor Learning Rate Number of Actor Update Steps Critic Learning Rate Number of Critic Update Steps Discount Rate γ GAE Lambda λ Clip Ratio ϵ KL Loss Sampling Temperature Sampling Temperature in Summarization Generation Length Fast Thinking Verification Slow Thinking Summarization Reward-specific Coefficient in Rsummary Minimum Length for Summarization Value 128 32 1 1e-6 1 5e-6 12 1 1 0.2 None 1 0. 1,000 6,000 6,000 1,000 1e-3 300 A.2 Prompt Templates The prompt templates used in the four stages of the Thinker task are illustrated in Box A.1. Note that not all prompts are necessarily used. For example, in training mode, if the agents fast answer is correct, the Slow Thinking and Summarization prompt will be skipped. Please refer to the main text for the termination conditions. 12 Box A.1: Prompt Templates for Thinker Task 1. Fast Thinking User: Answer the below question with concise steps and output the final answer within boxed{}. Limit your response below 1000 words. This is the problem: {question} Assistant: <Agent Response> 2. Verification User: Is your answer above correct? Please verify each step and the answer carefully. Output boxed{Yes} if your answer is correct, or boxed{No} if your answer is incorrect. Assistant: <Agent Response> 3. Slow Thinking User: Your initial answer is incorrect. Now, think about the possible errors and consider alternative solutions. The reasoning process should be enclosed within <think>...</think>. This is the problem: {question} Lets think step by step and output the final answer within boxed{}. Assistant: <think> <Agent Response> 4. Summarization User: Your final answer is correct. Now summarize the steps leading to your final answer concisely and precisely, excluding internal reasoning. Limit your response between 300 and 1000 words. This is the problem: {question} Assistant: <Agent Response> A.3 Computational Resources Each training run for both the Thinker task and the baseline required approximately 7 days on two compute nodes, each equipped with 8 A100 GPUs. We use the Deepspeed [23], vLLM [24], and Ray [25] library for distributed training."
        },
        {
            "title": "B Result Details",
            "content": "This section describes additional experimental results that were omitted from the main text due to length constraints. B.1 Evaluation Results Figure 7 and Figure 8 show the breakdown of Figure 4 from the main text, corresponding to the evaluation results of fine-tuning Q1.5B and R1.5B on the QA task or the Thinker task during training. The detailed evaluation results of the ablated run in which the Summarization step is removed can be found in Table 3, labeled as SkipSum. Table 4 presents the standard error corresponding to the results reported in Table 1. (a) MATH 500 (b) AIME (c) AIME 25 (d) GPQA Diamond (e) Olympiad Bench (f) AMC 23 Figure 7: Detailed evaluation results of Q1.5B fine-tuned using QA task or Thinker task on individual mathematical reasoning benchmarks. (g) Minerva Math 14 (a) MATH 500 (b) AIME 24 (c) AIME 25 (d) GPQA Diamond (e) Olympiad Bench (f) AMC 23 (g) Minerva Math Figure 8: Detailed evaluation results of R1.5B fine-tuned using QA task or Thinker task on individual mathematical reasoning benchmarks. Table 3: Mathematical reasoning performance of the Thinker agent trained without the Summarization stage. Average (Avg.) scores are presented. All scores are Pass@1 accuracy (%) averaged over 16 samples. Standard errors are provided in Table 4. Method MATH AIME AIME GPQA Olympiad AMC Minerva College Math Math 2025 Diamond bench 2024 500 23 Avg. Qwen2.5-1.5B (Q1.5B) SkipSum 60.30 SkipSum-Fast 64.30 5.00 9.17 1.25 4.17 20.27 18.62 24.17 29. 30.00 37.50 19.85 20.82 38.24 39.42 24.88 27.89 B.2 Ablation Study on Fast Thinking Mode To understand the importance of the Fast Thinking mode in the overall Thinker task, we experiment by using less-trained agent to generate the Fast Thinking response, while still using the fully trained agent to generate responses for the remaining stages. This allows us to measure the impact of Fast Thinking quality on overall performance. Specifically, we use four earlier R1.5B Thinker-agent checkpoints (Step 0, which is the pretrained model; Step 200; Step 400; and Step 600) to generate the Fast Thinking response, and use the fully trained R1.5B Thinker-agent for the remaining stages. We evaluate final accuracy across the eight benchmarks, as in the main evaluation. The results are shown in Figure 9. We observe general positive correlation between the Fast Thinking accuracy of checkpoint and the final accuracy, suggesting that the Fast Thinking response has substantial influence on subsequent stages. For instance, when we use the pretrained model (Step 0) to generate the Fast Thinking response, final accuracy drops significantly from 49.8% to 36.3%. However, we also observe that 15 Table 4: Standard error of performance on mathematical benchmarks in Table 1. All scores are in %. Method MATH AIME AIME GPQA Olympiad AMC Minerva College Math Math 2025 Diamond bench 2024 500 23 Qwen2.5-1.5B (Q1.5B) Pretrained 0.36 Baseline 0.33 Thinker 0.19 Thinker-Fast 0.19 0.23 SkipSum SkipSum-Fast 0.39 0.00 0.61 0.67 0.52 0.61 0.57 0.00 0.86 0.57 0.57 0.42 0.71 DeepSeek-R1-Distill-Qwen-1.5B (R1.5B) Pretrained Baseline Thinker Thinker-Fast 0.33 0.24 0.25 0. 1.34 1.05 1.35 1.10 0.96 0.52 1.09 0.91 0.40 0.47 0.39 0.46 0.91 0.51 0.58 0.56 0.63 0.45 0.15 0.34 0.09 0.27 0.21 0.22 0.28 0.34 0.31 0. 0.91 1.20 1.32 0.85 1.25 1.37 1.20 0.99 0.96 1.46 0.19 0.50 0.13 0.43 0.48 0.22 0.42 0.30 0.38 0.24 0.09 0.11 0.08 0.11 0.05 0.09 0.08 0.08 0.09 0. Avg. 0.14 0.24 0.17 0.10 0.20 0.12 0.24 0.22 0.16 0.24 this sensitivity diminishes as Fast Thinking performance improves. For example, using the Step 200 model, which has moderate Fast Thinking accuracy of 28.9%, leads to final performance of 49.18%a minor drop from 49.8%. We conjecture that this is due to the robustness of the Slow Thinking mode: since it is trained specifically to handle incorrect Fast Thinking answers, it can often recover from slightly flawed initial intuition. However, if the Fast Thinking intuition is very poor (as in the pretrained model), the subsequent stages may fail to recover due to the lack of meaningful starting point. qualitative analysis of how the Fast Thinking stage interacts with subsequent stages can be found in the case study in Appendix C, which shows that the trained agent is able to correct flawed heuristics from the Fast Thinking mode during the Verification and Slow Thinking mode. Figure 9: Final accuracy on the evaluation benchmarks of the Thinker agent (R1.5B) when its Fast Thinking stage is generated by model checkpoints at previous training steps. The original Fast Thinking accuracy of these respective checkpoints is also shown. All scores are Pass@1 accuracy (%) averaged over 16 samples. Error bars represent standard error, which are typically minor in this data."
        },
        {
            "title": "C Case Study",
            "content": "In this section, we present sampled responses from the fine-tuned R1.5B agent on the Thinker task, aiming to understand the behavior learned by the agent. Only responses with an incorrect fast answer are selected, so that the interaction between Fast Thinking and Slow Thinking can be observed. C.1 Case Study I: Identifying Flaws in Fast Thinking (Box C.1, Box C.2) This example demonstrates the Thinker tasks ability to guide the agent from an uncertain, flawed initial attempt to correct, verified solution by structuring its reasoning process. Fast Thinking. The agent adopts quick, System-1-like heuristic despite expressed uncertainty (the side length of the larger hexagon is + 2 2 = + 4? Not sure.) and proceeds with this flawed assumption (S = + 4). This leads to an incorrect perimeter, 6 3 12, which is physically implausible as it results in negative value. Verification. The agent directly confronts this flawed assumption. It explicitly questions the initial logic (The side length of this larger hexagon might be + 4? Or is it + 2? Wait, reconsider. <...> the relationship isnt straightforward.). This critical re-evaluation leads to the correct geometric 3 insight, yielding the relationship = + 4 . Based on this corrected understanding, the agent identifies the error in its initial reasoning and conclusion, stating, The previously given answer was 3 12. But that would not match. <...> Thus our initial approach is wrong. Slow Thinking. The agent then leverages the insight from Verification. It explicitly focuses on the 3 difference in the apothems to re-derive = + 4 . This demonstrates clear adoption of the successful reasoning trace from Verification. The agent then systematically solves for the side length and calculates the correct perimeter, 18 4 3. Notably, it independently performs numerical check, showcasing deeper level of deliberation and confidence in its refined answer. Summarization. The provided summary effectively distills the core mathematical steps for solving the problem into clear, concise, and logically consistent sequence. It accurately establishes the relationship between the side lengths of the inner and outer hexagons, correctly formulates the equation for the paths area, and finds the pools perimeter efficiently. Interestingly, it also employs thinking block that reflects certain self-correction patterns observed in earlier steps. This case highlights the agents capacity for targeted error identification and conceptual correction. The progression shows clear refinement of reasoning, moving from the System-1-like heuristic in Fast Thinking to more rigorous, System-2-like approach in Verification and Slow Thinking. The explicit references between stagesVerification critiquing Fast Thinkings initial approach and Slow Thinking building directly on Verifications apothem insightunderscore the efficacy of the structured task in fostering coherent, self-correcting thought processes. C.2 Case Study II: Propagation of Error from Verification to Slow Thinking (Box C.3) This example provides counterpoint to the previous successful error-correction cases. It demonstrates scenario in which the agent arrives at an incorrect final answer due to error propagation and insufficient depth in later-stage reasoning. Fast Thinking. The agent, faced with complex product of fractional parts, makes guess. After calculating the first few terms and noting the initial product starts with 2 1 2 = 1, it states: Since initial term is 2 and product involves fractions potentially leading to 1/2. This leads to the incorrect answer ( 1 2 ), driven more by the available options and superficial pattern than rigorous derivation. Verification. This stage correctly identifies Fast Thinkings answer as likely wrong. However, it introduces its own flawed heuristic: Since each term ak is less than 1, and multiple such terms ak1 multiplied, so perhaps the total product tends to zero. While it correctly prompts re-evaluation by concluding the original answer was incorrect, its reasoning toward an alternative answer (0) is based on wrong heuristics without considering the specific structure of this sequence (which actually involves telescoping product that simplifies to a1 a98 Slow Thinking. Instead of uncovering the correct algebraic simplification, Slow Thinking explicitly adopts and reinforces the flawed heuristic from Verification. After refining some calculations, it , leading to final answer of 1). 17 reasons: Alternatively, since the product seems to be approaching zero due to the product of many terms each less than 1. But given the options, perhaps its 0. This echoes Verifications logic and leads to the incorrect final answer (0). This case demonstrates how flawed reasoning can persist within the Thinker task. While Verification correctly identified Fast Thinkings initial guess as wrong, it unfortunately introduced its own plausible but incorrect heuristic. Subsequently, Slow Thinking, anchored on this flawed premise inherited from Verification, performing more calculations around it rather than challenging its validity. This highlights the possibility of an anchoring bias on an incorrect heuristic, and ultimately, superficial deliberation in the later stages that fails to correct the core flawed assumption. Box C.1: Example Response from Thinker Agent: Hexagon Path Problem Problem: path 2 wide and with an area of 36 m2 has been built around swimming pool in the shape of regular hexagon. What is the perimeter of the pool? User: <Fast Thinking Prompt: Answer concisely...> Assistant: <Initial Reasoning Trace showing uncertainty about side length> 3. <...> the side length of the larger hexagon is + 2 2 = + 4? Not sure. <Calculations based on the assumption = + 4> 13. So 8a = 8 3 2. 14. Perimeter of the hexagon is 6a = 6( 3 16 = 3 2) = 6 3 12. 6 3 User: <Verification Prompt: Is your answer above correct?> Assistant: <Reasoning Trace in Verification, questioning the = + 4 assumption (a relabeled as s)> The side length of this larger hexagon might be + 4? Or is it + 2? Wait, reconsider. <...> the relationship isnt straightforward. <Derivation of the correct relationship = + 4 3 <Calculations using the corrected S, leading to Perimeter = 18 4 The previously given answer was 6 But that would not match. So our approach seems incorrect. <...> Thus our initial approach is wrong. Therefore, the original answer is incorrect. <...> based on apothems> 3 12. 3> User: <Slow Thinking Prompt: Your initial answer is incorrect. Now, think about errors...> Assistant: <think> <revisiting the core geometric error from Fast Thinking> When you add path around the hexagon, the increase in side length can be found by considering the distance from the center. <...> The path is 2m wide, which think refers to the distance from the pool to the path, that would mean the difference in the apothems is 2m. So: 2 3 2 = 2 3 , consistent with Verifications insight> <Re-derivation of = + 4 3 = + 4 3 <Systematic algebraic solution using the corrected S, leading to = 3 2 3, followed by numerical verification> <Calculation of perimeter = 18 4 Hence, the perimeter is 6s 18 3 meters. <...> Yes, that seems consistent. 3 3 > Final Answer. The perimeter of the pool is 18 4 </think> <Narrative solution in Slow Thinking, presenting the final corrected steps> 3 meters. Box C.2: Example Summary from Thinker Agent: Hexagon Path Problem Problem: path 2 wide and with an area of 36 m2 has been built around swimming pool in the shape of regular hexagon. What is the perimeter of the pool? 3) = + 4/ User: <Summarization Prompt: Summarize concisely...> Assistant: <think> 1. Let be the side length of the original hexagonal pool. The path surrounds the pool and forms larger hexagon. The width of the path is 2m, so the side length of the larger hexagon is + 2 (2/ 2. The area of the path is the difference between the area of the larger hexagon and the smaller one. The area of regular hexagon is (3 3. Let be the side length of the pool, then the side length of the larger hexagon is 3) = + 4/ 3. Wait, no, the correct relationship is that the side length increases + 2 (2/ by 2 (2/ 3. 3) = 4/ 4. The area of the path is 36 = (3 3/2) [(s + 4/ 5. Simplifying the equation, we find that = 3( 3)2 s2]. 3)/3. The perimeter is 6s = 184 3. Wait, actually, the relationship is more complex. 3/2) (side length)2. 3. Thus, the perimeter of the pool is 18 4 </think> 3 meters. Let be the side length of the original hexagonal pool. The path width is 2m. The side length of the larger hexagon (pool + path) is = + 4 . The area of the path is 36 m2, which is 3 the difference between the areas of the larger and smaller hexagons. The area of regular hexagon is 3 2 (side length)2. Therefore: (cid:32)(cid:18) (cid:19)2 (cid:33) 3 3 2 + 4 3 = 36 Simplifying the term inside the brackets: (cid:18) + (cid:19)2 4 s2 = s2 + 8s 3 + 16 3 s2 = 8s + 16 3 Substituting this back into the area equation: (cid:18) 8s 3 3 3 Multiplying through by 3 3 2 : 3 3 8s 3 + Solving for s: 12s + 8 (cid:19) + 16 3 = 36 3 3 2 16 3 = 36 3 = 36 12s = 36 8 36 8 12 3 = 3 3 3 The perimeter of the pool is 6s: = (cid:32) 6 3 2 3 (cid:33) 3 = 18 3 Thus, the perimeter of the pool is 18 4 3 meters. 19 Box C.3: Example Response from Thinker Agent: Sequence Product Problem Problem: Define the sequence {an} : a1 = 1, a2 = 2, an+2 = an + an+1, N+, then find (cid:105) . Options: A. 2, B. 0, C. 1, D. 1 2 . . . (cid:110) a99 a98 (cid:104) a98 a2 (cid:110) a3 (cid:104) a2 a1 (cid:111) (cid:111) (cid:105) User: <Fast Thinking Prompt: Answer concisely...> Assistant: <Initial computation of sequence terms and first few product terms: [a2/a1] = 2, {a3/a2} = 1/2> 7. Notice that the product of fractional parts potentially telescopes or simplifies. However, its complex. 8. Given the options to D, and considering the pattern, likely answer is B.0 or C.1, but need to think carefully. Since initial term is 2 and product involves fractions potentially leading to 1/2. User: <Verification Prompt: Is your answer above correct?> Assistant: <Re-evaluation of sequence and product structure, calculating [a2/a1] = 2 and [a98/a2] 17> Wait, count the number of fractional parts: <...> 96 terms. <...> Since each term {ak/ak1} is less than 1, and multiple such terms multiplied, so perhaps the total product tends to zero. Thus, the product is very small, so the overall expression would be close to zero. <...> Therefore, the original answer was incorrect, so the correct answer should be , as the initial answer was incorrect. <...> now think the final answer might be B.0. User: <Slow Thinking Prompt: Your initial answer is incorrect. Now, think about errors...> Assistant: <think> <Recalculates sequence terms, [a2/a1] = 2. Identifies an = Fn+1 and that F99 is even, refining [a98/a2] to F99/2> Given that each ratio ak/ak1 approaches the golden ratio ϕ 1.618. Thus, each {ak/ak1} approaches ϕ 1 = ψ 0.618. Thus, if each term {ak/ak1} is about ψ 0.618. Thus, product of 96 terms [error in count, should be 97]: ψ96. Therefore, product=2 (ψ)96 floor(F99/2). <...> Alternatively, since the product seems to be approaching zero due to the product of many terms each less than 1. But given the options, perhaps its 0. Therefore, the answer is B.0. </think> <Narrative solution, reinforcing the (incorrect) conclusion that the product is 0>"
        },
        {
            "title": "D Preliminary Experiments on Larger Models",
            "content": "To understand the scalability of the Thinker task, we fine-tune DeepSeek-R1-Distill-Qwen-7B (R7B) [1], larger version of R1.5B that also undergoes distillation using reasoning data from DeepSeek-R1. We use the same set of hyperparameters and training data as in the main experiments. The baseline corresponds to fine-tuning R7B on the standard QA task. The average evaluation accuracy across the seven benchmarks (excluding CollegeMath due to the large number of questions) during training is shown in Figure 10. Due to limited computational resources, we have not yet trained the Thinker agent to convergence, but we already observe its performance surpassing that of the baseline, which has plateaued. The detailed performance of the best checkpoints from both runs can be found in Table 5. Similar to the results observed in smaller models, we found that agents trained under the Thinker task consistently perform better than those trained on the baseline QA task across all benchmarks. The overall performance improved from 54.41% to 58.91%, representing relative improvement of 8.27%. This suggests that larger models also benefit from the Thinker task. Additionally, we note that Thinker-Fast performance improves from 37.55% in R1.5B to 45.45% in R7B, demonstrating that the Fast Thinking mode scales well with model size. Figure 10: Evaluation performance of R7B averaged across seven common benchmarks. Table 5: Performance comparison across various mathematical reasoning benchmarks. Average (Avg.) scores are presented. All scores are Pass@1 accuracy (%) averaged over 16 samples. Top score in each benchmark column is bolded. Standard errors are provided in Table 6. Method MATH AIME AIME GPQA Olympiad AMC Minerva College Math Math 2025 Diamond bench 2024 500 Avg. DeepSeek-R1-Distill-Qwen-7B (R7B) 84.05 Pretrained Baseline 91.03 92.80 Thinker Thinker-Fast 84.60 37.50 47.50 55.83 22.92 28.54 34.58 37.50 19.17 17.58 34.63 45.90 38. 37.92 56.76 60.83 48.93 36.41 87.81 91.25 65.62 34.49 40.23 43.77 41.75 40.72 42.71 43.39 42.32 39.65 54.41 58.91 45.45 Table 6: Standard error of performance on mathematical benchmarks in Table 5. All scores are in %. Method MATH AIME AIME GPQA Olympiad AMC Minerva College Math Math 2025 Diamond bench 2024 23 DeepSeek-R1-Distill-Qwen-7B (R7B) 0.43 Pretrained 0.19 Baseline Thinker 0.13 Thinker-Fast 0.25 1.08 1.62 1.54 0.52 0.86 1.29 1.27 0.57 0.70 1.00 0.53 0. 0.45 0.31 0.15 0.19 4.37 0.79 0.85 1.24 0.80 0.27 0.31 0.36 0.12 0.07 0.06 0.06 21 Avg. 0.48 0.39 0.32 0."
        }
    ],
    "affiliations": [
        "DualityRL",
        "Shanghai AI Lab"
    ]
}