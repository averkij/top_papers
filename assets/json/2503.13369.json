{
    "paper_title": "Sightation Counts: Leveraging Sighted User Feedback in Building a BLV-aligned Dataset of Diagram Descriptions",
    "authors": [
        "Wan Ju Kang",
        "Eunki Kim",
        "Na Min An",
        "Sangryul Kim",
        "Haemin Choi",
        "Ki Hoon Kwak",
        "James Thorne"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Often, the needs and visual abilities differ between the annotator group and the end user group. Generating detailed diagram descriptions for blind and low-vision (BLV) users is one such challenging domain. Sighted annotators could describe visuals with ease, but existing studies have shown that direct generations by them are costly, bias-prone, and somewhat lacking by BLV standards. In this study, we ask sighted individuals to assess -- rather than produce -- diagram descriptions generated by vision-language models (VLM) that have been guided with latent supervision via a multi-pass inference. The sighted assessments prove effective and useful to professional educators who are themselves BLV and teach visually impaired learners. We release Sightation, a collection of diagram description datasets spanning 5k diagrams and 137k samples for completion, preference, retrieval, question answering, and reasoning training purposes and demonstrate their fine-tuning potential in various downstream tasks."
        },
        {
            "title": "Start",
            "content": "Sightation Counts: Leveraging Sighted User Feedback in Building BLV-aligned Dataset of Diagram Descriptions Wan Ju Kangα Eunki Kimα Na Min Anα Haemin Choiβ,δ Ki Hoon Kwakγ,δ Sangryul Kimα James Thorneα 5 2 0 2 M 7 1 ] . [ 1 9 6 3 3 1 . 3 0 5 2 : r KAIST AIα Sungkyunkwan Universityβ Yonsei Universityγ Work done as KAIST AI research internδ α{soarhigh, eunkikim, naminan, sangryul, thorne}@kaist.ac.kr β chm1009@g.skku.edu γkihoon090@yonsei.ac.kr https://hf.co/Sightation"
        },
        {
            "title": "Abstract",
            "content": "Often, the needs and visual abilities differ between the annotator group and the end user group. Generating detailed diagram descriptions for blind and low-vision (BLV) users is one such challenging domain. Sighted annotators could describe visuals with ease, but existing studies have shown that direct generations by them are costly, bias-prone, and somewhat lacking by BLV standards. In this study, we ask sighted individuals to assessrather than producediagram descriptions generated by vision-language models (VLM) that have been guided with latent supervision via multipass inference. The sighted assessments prove effective and useful to professional educators who are themselves BLV and teach visually impaired learners. We release SIGHTATION, collection of diagram description datasets spanning 5k diagrams and 137k samples for completion, preference, retrieval, question answering, and reasoning training purposes and demonstrate their fine-tuning potential in various downstream tasks1."
        },
        {
            "title": "Introduction",
            "content": "Recent research has seen rapid development in vision-language models (VLM). Seeing the world and the data within has significantly advanced machine intelligence in variety of tasks (Liu et al., 2024; Zhu et al., 2023; Yang et al., 2024; Qwen et al., 2025; Xu et al., 2024a; Li et al., 2024b), reaching fast-growing user pool with quicker and easier access. However, the same cannot be said of blind and low-vision (BLV) individuals. Widely adopted evaluation metrics have been shown to be biased against their preferences (Kapur and Kreiss, 2024) and benchmark studies tend to pursue larger audience first (Li et al., 2024a,d). Publicly available 1Wherever possible, we use color blind safe palettes in figures and tables. 1 reward models for generic VLMs are scarce (Zang et al., 2025) let alone for the visually impaired. Vision-language dataset research appears divided between breadth (Tang et al., 2023; Lu et al., 2023), specificity (Masry et al., 2024b,a), and volume (Zhang et al., 2025; Lee et al., 2022). Perhaps the classroom setting best exemplifies the circumstances BLV individuals face: textual information is combined with images (such as diagrams, graphs, and figures) to help learners fully grasp complex information (Vekiri, 2002; Cheng and Gilbert, 2009; Tippett, 2016; Gates, 2018). VLMs at the command of BLV users must therefore provide select, curated information rather than an indiscriminate narration of data. Instilling this behavior in VLMs, however, remains challenging primarily due to dataset concerns. The unavailability of large-scale BLValigned datasets has prompted previous studies to crowdsource few expert sighted annotators to generate descriptions. The limitation of this approach is twofold: i) it does not account for the preference misalignment between the BLV evaluator and the sighted generator (Lundgard and Satyanarayan, 2022); ii) it is prone to modeling the generations after the annotator rather than the task, introducing annotator bias into the dataset (Geva et al., 2019). While Kreiss et al. (2022) has illustrated the potential of sighted users as BLV preference estimators for few specific qualities of generations, whether their findings will generalize to dataset-scale volume of generations or with other aspects of perceived quality remains unknown. We construct, what is to the best of our knowledge, the first dataset that addresses the union of aforementioned challenges. We prompt VLM to generate guide, which will be input to second inference pass to latently supervise the second-pass behavior in favor of BLV users. Then, we further invoke the VLM to generate diagram descriptions, saving on crowdsourcing cost and reducing"
        },
        {
            "title": "Average\nText\nLength",
            "content": "Validated by BLV?"
        },
        {
            "title": "Dimensions\nAssessed",
            "content": "SIGHTATION (Ours) -COMPLETIONS -PREFERENCE -RETRIEVAL -VQA -REASONING VisText (Tang et al., 2023) MathVista (Lu et al., 2023) (Masry et ChartGemma 2024b) DiagramQG (Zhang et al., 2024b) VizWiz-VQA (Gurari et al., 2018) VizWiz-LF (Huh et al., 2024) al., 188.3 (words) 74.6 58.0 37.5 9.5 8.6 73. Completion Preference alignment Retrieval Reward modeling Question answering Completion VQA, Reasoning Completion"
        },
        {
            "title": "DQA\nVQA\nVQA",
            "content": "Factuality Informativeness Succinctness Diversity Usefulness, in 4 finer aspects Interpretiveness Preferred Description Best Sentence Accuracy, Descriptiveness Correctness Informativeness, Factual Correctness, Structure Diversity, Object Density Diversity, Answerability Relevance, Helpfulness, Plausibility, Fluency, Correctness Table 1: The SIGHTATION collection has been validated by teaching professionals who are visually impaired and are experienced instructors at schools for the blind. As the most text-dense diagram description dataset to date, it can be used to drive variety of training objectives towards BLV accessibility needs. We discuss few prime examples in Section 4. This table includes only the few most closely related works; we deliver an extended comparison in Table 5. annotator fatigue. We distribute to sighted annotators set of assessment tasks, substantially less demanding than generation task, implying easier recruiting of sufficiently large annotator population, potentially mitigating annotator bias. Finally, we design the assessment tasks such that they are finer-grained than any prior work we are aware of. The compilation we named SIGHTATION is the first large-scale BLV-aligned dataset that is validated by BLV professionals and can be used to train on broad range of objectives. few statistics to highlight our dataset performance include: preference-tuning 2B model on our dataset to achieve an average 1.67σ increase in the usefulness rated by the BLV group; instruction-tuning 2B model on our dataset to outperform 3B model fine-tuned on chart comprehension (Masry et al., 2024b) in 8 out of 11 automatic metrics; contrastive tuning BLIP-2(Li et al., 2023) for retrieval purposes to outperform COCO-tuned BLIP-2 by 65%p on Precision@1."
        },
        {
            "title": "2 Related Work",
            "content": "Accessibility Studies. Lundgard and Satyanarayan (2022) found that BLV and sighted reader groups differ significantly on which semantic content they consider as most useful, suggesting that access to meaningful information is strongly reader-specific. VizWiz-VQA (Gurari et al., 2018) contains images and visual QA pairs produced by blind people encouraging the development of more generalized algorithms that can assist the blind. As an extended work, VizWiz-LF (Huh et al., 2024) includes longform answers from BLV people. VisText (Tang et al., 2023) contains charts and captions that convey different levels of semantic content. As shown in Table 1, VizWiz-VQA and VizWiz-LF were validated by BLV users but only focus on Visual QA (VQA) applications. VisText examines the role of the level of semantic content but was not validated by BLV for dataset purposes. As diagram description dataset validated by BLV users, SIGHTATION explores diverse use cases, with assessments on various aspects. Image Description Tasks and Models. Wang et al. (2024) presented the QWEN2-VL collection, which includes three open-weights models: 2B, 7B, and 72B. QWEN2-VL matches the performance of GPT-4O and CLAUDE3.5-SONNET (Anthropic, 2024) in multimodal scenarios, surpassing other open-weights VLMs at the time. GPT-4O (Hurst et al., 2024) accepts multimodal input and generates high-quality outputs including text and codes, showing powerful multimodal understanding capability. Using these VLMs, the image description task aims to generate descriptive textual context for images of different types (e.g., photographs, illustrations, schematics, 2 Figure 1: The key benefit of utilizing sighted user feedback lies in their assessments, which are based on solid visual grounding. The compiled assessments prove an effective training substance for steering VLMs towards more accessible descriptions. Dataset use and the subsequent validation are described in Sec. 4. complete list of use cases is provided in Appendix A. and diagrams). Flickr8K and PASCAL-50S comprise natural images, captions, and human judgments(Hodosh et al., 2013; Vedantam et al., 2015), and Polaris (Wada et al., 2024) incorporated synthetic captions from image captioning models. ChartGemma (Masry et al., 2024b) contains chart images collected from specialized websites and instruction-tuning data generated from the charts. MathVista (Lu et al., 2023) encompasses diverse visual contexts from natural images to diagrams or plots that require mathematical reasoning. However, Table 1 shows that these datasets have an average text length much shorter than ours, even though charts and mathematical images could be highly information-dense. Complementing the limitation, SIGHTATION provides contexts that top in average text length to date with variants for downstream tasks. Human Annotation Efforts. Human judgment annotations are essential in evaluating image captions, complementary to automatic metrics. Common approaches involve employing annotators to assess captions based on rating scales for specific dimensions of text quality (Gehrmann et al., 2023). HowFigure 2: The qualities assessed by their respective groups. 3 ever, it comes with challenges, including subjectivity and consistency issues. Amidei et al. (2019) argues that the evaluation of generated text is intrinsically subjective and relies on different factors including annotator experience, motivation, knowledge, or education. related line of research (Glockner et al., 2024; Nie et al., 2020) directly addressing this limitation advocates that generations from few-annotator pools fall short in terms of coverage of the distribution of opinions."
        },
        {
            "title": "3 The SIGHTATION Dataset",
            "content": "SIGHTATION is BLV-specific vision-language dataset for the educational domain. It is built upon the AI2D dataset (Kembhavi et al., 2016): we chose this for two reasons: it contains diagrams from grade school material, requiring no specialized expertise or domain knowledge in our annotator recruiting process; diagrams pose unique challenge to VLMs in that they often require an understanding of the rendered schematics and the natural objects. AI2D contains 5k science diagrams, with 150k annotations, spanning OCR texts and bounding box locations, as well as 15k multiple choice questions. Of these features, we take only the diagrams, to simplify SIGHTATION-like dataset construction in the future. All notation and labeling methods used in this section are summarized in separate Table 6 to aid comprehension."
        },
        {
            "title": "3.1 Overview",
            "content": "Different annotator roles can be found in Figure 2. There are total of 9 aspects to be assessed, and these were inspired by various related studies. In Kreiss et al. (2023), relevance and irrelevance aspects are studied to measure the image information carried in text and the inclusion of extraneous information in the text, respectively. As such, we chose to examine Informativeness and Factuality dimensions. These both require reliable visual grounding so were assigned to the sighted accordingly. We also opted for some measures to be assessed by all groups. Since brevity (Lundgard and Satyanarayan, 2022) and diverse opinion coverage (Glockner et al., 2024; Nie et al., 2020) have been pointed out as contributors to perceived quality, we chose to incorporate them as the Succinctness and the Diversity aspects, both of which are assessable with text comprehension alone. Following Tang et al. (2023), we split the use cases for the usefulness measure along typical vision-language comprehension tasks common in the classroom: Useful-Sum (summarization), Useful-MCQ (multiple-choice questions), and Useful-OEQ (open-ended questions). These were assigned to the BLV educators, adept at teaching and knowledgeable in accessibility needs. general usefulness measure UsefulGen was assigned to the sighted educators to probe their estimate of BLV needs. Finally, categorical variable, Nature, was assigned to the BLV educators to ask for their opinion on how interpretive the text appears. These different subsets were assigned to pursue synergistic interplay between varying visual abilities, teaching experience, and accessibility requirements. The sighted general group, shown on the left in Figure 1 ensures that the diagram content is well-conveyed in the description. Sighted educators, shown on the top right of Figure 1 validate the general groups assessment whilst also rating the general usefulness of the description to BLV users. Finally, the text-based assessment by BLV educators, shown on the bottom right in the same figure, gauges the alignment of SIGHTATIONtuned descriptions with BLV preferences. more detailed description of the annotation tasks is in Section 3.3 for the sighted general group and in Section 4.2.1 for the sighted and BLV."
        },
        {
            "title": "Supervision",
            "content": "Previous work (Lundgard and Satyanarayan, 2022) has shown that crowdsourced data visualization descriptions written by sighted crowdworkers were not equally useful to the BLV groups as they were to the sighted, in terms of describing low-level numerical elements or high-level insights such as subjective commentary. Building on this, we hypothesized that the key to generating description that is useful to BLV individuals lies not only in what is seen but also in how the perceived information is articulated. We hypothesized that introducing auxiliary data such as plausible question-answer pairs, would have good effect as they assist the description generator with understanding which parts are critical and which are less so. In implementing this idea, we incorporated two-pass guided generation process. The first inference pass is to create the guide, which is VLMgenerated set of question-answer pairs in response to an input diagram. We carefully examine the quality of the question and answer pairs we have generated and, in the Appendix A.1, provide more indepth analysis of how these pairs differ from those originally included in the AI2D dataset. Then, the second pass generates the diagram description in response to the input diagram and the guided generation prompt, as shown on the leftmost part of Figure 1. We applied this generation process with two models: GPT-4O MINI and QWEN2-VL 72B model, producing four descriptions for each of the 5k diagrams in the AI2D dataset. The working dataset thus contains 20k descriptions."
        },
        {
            "title": "3.3 Annotation Tasks",
            "content": "1k images were randomly sampled from the working dataset. They were then paired with their respective descriptions generated by GPT-4O MINI (Descg and Descg ++) and descriptions generated by QWEN2-VL (Descq and Descq ++) were distributed to the 30 sighted annotators, to complete three tasks: i) preference choice, ii) quality rating, and iii) best sentence choice. The 1k tuples were partitioned into 10, so that 3 participants perform the annotation on shared total of 100 tuples. First, annotators were asked to select pairwise preferred descriptions: one from the GPT pair and the other from the Qwen pair. Second, for all four diagram descriptions, they were asked to rate the description quality across the 4 aspects assigned to them, as in Figure 2, on 5-point Likert scale. Lastly, they were asked to pick the bestcontributing sentence from each of the four diagram descriptions. Sample screenshots of the annotation interface, along with the annotation guidelines, are provided in Appendix I. The total number of annotations is 11,804, spanning 998 diagrams and 3,992 descriptions. Further statistics and post-processing steps are found in Appendix C."
        },
        {
            "title": "3.4 Dataset Construction",
            "content": "In this section, we describe how the annotated tuples are processed for various downstream tasks."
        },
        {
            "title": "3.4.1 Chat Completion",
            "content": "SIGHTATIONCOMPLETIONS contains instructionresponse pairs from two sets: i) all the 4k humanannotated descriptions over 1k images, with the base instruction in Appendix and ii) the top 25% highly rated descriptions for each of the 4 aspects annotated. For the latter subset, we augment the base instruction to pair responses that were of high quality in some aspect. We append an aspectspecific suffix outlining the desired quality according to our annotation guidelines in Appendix I. For instance, the aspect suffix for the factuality dimension is: When generating the diagram description, pay close attention to making it factual. highly factual description delivers only the facts that are grounded in the diagram. With the former set consisting of 4k (diagram, base prompt, description) samples and the latter set consisting of 1k (diagram, augmented prompt, description) samples per aspect, our completions dataset totals 8k samples."
        },
        {
            "title": "3.4.2 Preference Alignment",
            "content": "SIGHTATIONPREFERENCE also proceeds from the 4k diagram-description pairs, consisting of 4 descriptions for every image. From these 4, we take the 6 possible pairwise combinations and label chosen and rejected to each contender in the pairwise comparisons as follows. In-model Contenders Within each of the 2 samemodel comparisons, (e.g., Descg versus Descg ++) we directly take the Preferencemodel annotation to assign chosen and rejected. This assignment results in 2 1k = 2k chosen-rejected preference pairs. Cross-model Contenders Within each of the 4 cross-model comparisons, (e.g., Descg ++ versus Descq), we averaged the rating scores per contender and assigned2 chosen to the ratings winner. This assignment results in 4 1k = 4k preference pairs. 2Ties are technically possible, but the collected annotations did not contain any. Synthetic Contenders Additionally, we synthesized an inferior (rejected) variant of description by removing its best sentence. To account for the reduced length, we remove random non-best sentence from the original description and label this variant chosen. This assignment results in 4 1k = 4k preference pairs per annotator. maximum of three annotators evaluated the same sample, so the preference pairs total 12k. After deduplicating (e.g., annotators selecting the same sentence as the best sentence), we have 10k preference pairs. Putting together the in-model (2k), cross-model (4k), and synthetic (10k) contenders and their respective labels, SIGHTATIONPREFERENCE spans 16k pairs."
        },
        {
            "title": "3.4.3 Retrieval",
            "content": "Each row in SIGHTATIONRETRIEVAL contains an image as retrieval query, accompanied by the top 1, top 5, and top 10 descriptions as the positives, as well as 10 hard negatives. This set contains 1k rows, with potential well beyond that number. For instance, more than 63 million unique combinations can be derived utilizing 5 random samples from the 10 positives and 5 random samples from the 10 negatives. Further details can be found in Appendix D."
        },
        {
            "title": "4 Performance Analysis",
            "content": "We designed series of experiments to measure the performance of SIGHTATION as dataset. First, we fine-tuned various models on our dataset. Then, we asked sighted and BLV teachers at schools for the blind to evaluate the generated texts. Additionally, we employ VLM judges and number of well-known classic metrics to evaluate the descriptions. We report the main findings on the extent and breadth of performance enhancement our dataset can cultivate."
        },
        {
            "title": "4.1 Fine Tuning",
            "content": "We chose to experiment with the QWEN2-VL series (Wang et al., 2024) considering its size variety, state-of-the-art performance at the time of writing, as well as whether the largest variant (72B) could fit on our compute cluster in its default precision, bf16, unquantized. We fine-tuned the 2B and 7B models and performed comparative analyses. Finer details on the tuning configuration are found in Appendix H."
        },
        {
            "title": "4.1.1 On SIGHTATIONCOMPLETIONS",
            "content": "We conducted supervised fine tuning (SFT) on our completions dataset. The 2B model underwent full fine tuning, whereas the 7B model underwent parameter-efficient fine tuning (PEFT)."
        },
        {
            "title": "4.1.2 On SIGHTATIONPREFERENCE",
            "content": "For preference alignment tuning, we chose to perform Direct Preference Optimization (DPO, (Rafailov et al., 2024)). Since reward models trained on generic data may not accurately represent BLV preferences, we opted for DPO, widely used algorithm free of reward models. Before the actual DPO training, as is common in practice, we first subjected the 2B and 7B models to SFT. However, we recognized that sharing the same set of diagrams across the SFT and DPO stages could pose higher overfitting risks. With that in mind, instead of using SIGHTATIONCOMPLETIONS for SFT, we randomly sampled 1k diagrams along with their 4 descriptions from the remaining pool of generated descriptions (i.e., the ones not in SIGHTATIONCOMPLETIONS) and used these to compile 4k completion samples. Afterwards, DPO was run on SIGHTATIONPREFERENCE. At both the SFT and DPO stages, the 2B model was fully fine-tuned, and the 7B model was trained with PEFT."
        },
        {
            "title": "4.1.3 On SIGHTATIONRETRIEVAL",
            "content": "We performed contrastive training to fine-tune BLIP-2 (Li et al., 2023) for its appeal in imagetext matching. To save compute, we trained only parts of the model and with just the top 1 positive and randomly chosen negative. The training was carried out with InfoNCE loss (Oord et al., 2018), widely used choice for contrastive objectives."
        },
        {
            "title": "4.2.1 By Teaching Professionals",
            "content": "We recruited 17 specialized educators who teach BLV learners at schools for the visually impaired. 8 of them are themselves blind or have low vision; remaining 9 are sighted. We refer to these groups as the BLV educator group and the sighted educator group, respectively. Their demographics are reported in Tables 17 and 18 BLV Educators Each BLV educator was given 40 diagrams, each with two competing descriptions. They were asked to rate text-based qualities. They were asked to perform quantitative assessment on the aspect set pictured in Figure 2. Following Tang et al. (2023); Lundgard and Satyanarayan (2022), we chose to investigate the usefulness of the diagram descriptions, but in three finer manifestations. Specifically, we asked the BLV educators to assess how useful the description is as textual aid providing i) summary of the diagram content, ii) clues that would be helpful when solving short-answer multiple-choice questions about the diagram, and iii) clues that would be helpful when answering long-answer open-ended questions about the diagram. Sighted Educators Each sighted educator was given 40 diagrams, each with two competing descriptions with randomized order of presentation. They were then asked to evaluate the descriptions according to the guidelines for the sighted educator group, found in Appendix I. Their aspect set, also shown in Fig. 2, includes usefulness estimate to BLV users."
        },
        {
            "title": "4.2.2 By Automatic Metrics",
            "content": "We perform VLM-as-a-Judge (Dubois et al. (2023),Zheng et al. (2023)) evaluation with QVQ72B-PREVIEW, where we instruct the VLM to take the Image, Descmodel, and Descmodel triplet as input and produce JSON-formatted evaluation with the same aspects as with the human annotation. ++ As for classic metrics, we collect widely recognized reference-free metrics since the AI2D dataset does not contain references: CLIP score (Hessel et al., 2021), SigLIP score (Zhai et al., 2023), BLIP-2 Retrieval score (Li et al., 2023), SelfBLEU (based on BLEU (Papineni et al., 2002)), PAC score (Sarto et al., 2023), and LongCLIP-B/L (Zhang et al., 2024a). For the retrieval task, we chose to measure recall@K and precision@K for = 1, 5, 10, as do numerous retrieval studies."
        },
        {
            "title": "5 Results",
            "content": "We report the evaluation results by the BLV educator group, the sighted educator group, VLM judges, and classic metrics. For each group, we discuss the effectiveness of the combined recipe, then with the guided generation ablated, and with the tuning step ablated. Here, we focus on the evaluation by BLV; sighted educator and VLM-as-a-Judge evaluation, as well as classic metric results are found in Appendix E. 6 Figure 3: Tuning VLMs on SIGHTATION enhanced various qualities of the diagram descriptions, evaluated by BLV educators, and shown here as normalized ratings averaged in each aspect. The capability of the dataset is most strongly pronounced with the 2B variant, shown above. Full results across 4 models and 22 metrics are reported in Tables 9, 10, 11, and 12."
        },
        {
            "title": "5.1 Evaluation by BLV Educators",
            "content": "Here, we conduct an analysis of effect size, an intuitive choice for aggregate analysis on different sample sets rated by different evaluators. Figure 3 shows the effect size computed from BLV educators assessment. The radial axis corresponds to the mean ratings on each of the two sets of samples under comparison, normalized by their pooled standard deviation (σ). Naturally, the radial axis is in units of the pooled standard deviation. The first radar chart in Figure 3 shows the result of comparing Descq2bbase and Descq2bdpo . The latter was rated more than 1σ higher in interpretiveness (Nature); 0.8σ better in diversity and usefulness for open-ended questions; 0.4σ units more useful as summary. ++ ++ ++ In the middle of the same figure is shown the ablated result of fine tuning, with the guided generation turned on for both sets: comparison beand Descq2bdpo tween Descq2bbase . All 6 aspects were judged in favor of the latter, with as large as 1.2σ difference in interpretiveness and diversity and 0.8σ in usefulness for open-ended questions. On the right is shown the effect of the guided generation on SIGHTATIONPREFERENCE-tuned 2B model: comparison between Descq2bdpo and Descq2bdpo . Guided generation yields significant ++ enhancement for the DPO-tuned case, with 1σ higher in usefulness for multiple choice questions, followed by approximately 0.8σ improvement in usefulness for open-ended questions, an overall improvement in every aspect down to succinctness, with 0.2σ. However, as will be discussed with Table 4, this effect by the guided generation is achieved only after the model is fine-tuned on our dataset, implying that good alignment is prerequisite for attempting to benefit from test-time prompting."
        },
        {
            "title": "6 Discussion",
            "content": "Tables 2, 3, and 4 show Cohens d, which is the size of the effect of the treatment in the respective table. Ratings on Nature are not included in the average computation since it is categorical variable; i.e., low Nature rating simply means the description was perceived to be more straight facts-oriented than commentary-oriented, and not necessarily of lower quality. Combined Effect Size Table 2 shows the effect size of fine tuning on SIGHTATION and applying the guided generation prompt at test time. With the combined recipe applied, the 2B model achieves an average of 0.36σ units of improvement, while the 7B model, 0.58σ units. Intriguing observations can be made on succinctness. The 2B model exhibited the smallest effect size in this aspect, whereas the 7B model achieved the highest enhancement. This suggests that the combined recipe applied on the smaller model had negligible effect in making its descriptions more succinct. In fact, the combined recipe enhanced Nature by large effect (1.08σ), implying that, with smaller models, the prime importance of the combined recipe lies in shaping the descriptions to be far more interpretive. The opposite can be said of the 7B model: the combined recipe greatly (1.69σ) enhances its succinctness, whilst shaping its descriptions far less interpretive (2.38σ) and straight facts-oriented instead. This"
        },
        {
            "title": "Aspect",
            "content": "Succinct Diverse Useful-Sum Useful-MCQ Useful-OEQ"
        },
        {
            "title": "Average\nNature",
            "content": "2B -0.09 0.90 0.39 -0.18 0.76 0.36 1.08 7B 1.69 0.46 0.53 0.20 0.00 0.58 -2."
        },
        {
            "title": "Aspect",
            "content": "2B 2B+GG 7B 7B+GG"
        },
        {
            "title": "GPT",
            "content": "2B Base 2B DPO Succinct Diverse Useful-Sum Useful-MCQ Useful-OEQ"
        },
        {
            "title": "Average\nNature",
            "content": "0.06 0.87 0.20 0.29 1.01 0.49 1.49 0.08 1.08 0.55 0.00 0.90 0.52 1.06 0.37 -0.06 0.14 -0.54 -0.74 -0.17 -3. -0.11 0.00 0.36 0.00 -0.19 0.01 -0.31 Succinct Diverse Useful-Sum Useful-MCQ Useful-OEQ"
        },
        {
            "title": "Average\nNature",
            "content": "0.18 -0.13 0.48 0.13 0.76 0.28 0.33 -0.17 -0.13 -0.17 -0.20 -0.07 -0.15 0.08 0.17 0.47 0.57 0.92 0.77 0.58 3. Table 2: Combined recipe effect size on each aspect, measured with BLV assessment. Table 3: Fine tuning effect size on each aspect, measured with BLV assessment. Table 4: Guided generation effect size on each aspect, measured with BLV assessment. is in line with 3 separate comments by our BLV educators (B1, B2, and B5) who have, unknowingly of each others interview responses, stressed the importance of succinctness: The description must deliver all visual items in an accurate and consistent manner, with not too long text and including the key elements. Tuning Effect Size Table 3 shows the effect size of fine tuning on SIGHTATION. For instance, with guided generation absent, the 2B model still reaps 0.87σ units of improvement in the diversity aspect of its descriptions. The improvement margin is even amplified further by applying guided generation on the tuned model, except for usefulness in solving questions. The table shares the observation made on the succinctness-nature relationship conveyed in Table 2, albeit to lesser extent on the 7B model with guided generation. This set, whose ratings are on the rightmost column of Table 3, showed meaningful effect size only in usefulness as summary and nature. This implies that larger models are already somewhat capable of capitalizing on the guided generation prompt at test time and carry less reliance on the fine tuning process. Guided Generation Effect Size Table 4 shows that the guided generation yields benefits even to GPT, possibly indicative of the underrepresentation of BLV accessibility needs and preferences in the pre-training data. It is important to note that, for the 2B model, the best effect of guided generation is achieved only after the model is tuned on our dataset, again highlighting the BLV alignment capabilities of our dataset, that cannot be mimicked by test-time prompt engineering alone."
        },
        {
            "title": "7 Conclusion",
            "content": "stead of crowdworkers, who pose annotator bias concerns and are bottlenecked by cost and fatigue, ii) validated by specialized teaching professionals at schools for the blind, and iii) demonstrated across wide range of use cases, making the most of the invaluable feedback from BLV and sighted groups and inviting continued active endeavor towards accessible language and education."
        },
        {
            "title": "Limitations",
            "content": "Challenges in Supervision and Capturing Details in Diagram One challenge of our current approach is that the supervision signal predominantly relies on the QA format, leaving the exploration of alternative supervision substances relatively underdeveloped. In addition, our pipeline does not fully leverage advanced segmentation techniques, which could be crucial for accurately capturing and interpreting complex diagrammatic details. These constraints may affect the systems performance with diagrams that feature intricate or non-standard layouts. This aspect will be revisited in future research, as it holds the potential to achieve further advancements beyond the performance improvements demonstrated with our current dataset version."
        },
        {
            "title": "Ethics Statement",
            "content": "Potential Risks in Dataset Generation We acknowledge that during the process of creating our dataset, we utilized various LLMs, and there is potential ethical risk that unintended biases or unexpected outcomes may have been inadvertently included. However, once the human labels are applied, the post-processed information minimizes this risk. We release SIGHTATION, suite of the datasets showcasing these key characteristics: i) produced with BLV-oriented guided generation of VLMs inAI Assistant Also, we hereby acknowledge that we have received assistance with grammar and word choice from LLMs such as chatGPT-4o in 8 preparing this paper. However, all text is ultimately composed in the authors own words and was originally formulated by them. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2021. Clipscore: referencefree evaluation metric for image captioning. arXiv preprint arXiv:2104.08718."
        },
        {
            "title": "References",
            "content": "Jacopo Amidei, Paul Piwek, and Alistair Willis. 2019. Agreement is overrated: plea for correlation to assess human evaluation reliability. In INLG 2019, Tokyo, Japan. ACL. AI Anthropic. 2024. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card, 1. Shreyanshu Bhushan and Minho Lee. 2022. Block diagram-to-text: Understanding block diagram images by generating natural language descriptors. In Findings of AACL 2022, Online only. ACL. Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et al. 2023. Alpagasus: Training better alpaca with fewer data. arXiv preprint arXiv:2307.08701. Maurice Cheng and John Gilbert. 2009. Towards better utilization of diagrams in research into the use of representative levels in chemical education. In Multiple representations in chemical education, pages 5573. Springer. Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. 2023. Alpacafarm: simulation framework for methods that learn from human feedback. Advances in Neural Information Processing Systems, 36:3003930069. Peter Gates. 2018. The importance of diagrams, graphics and other visual representations in stem teaching. STEM education in the junior secondary: The state of play, pages 169196. Sebastian Gehrmann, Elizabeth Clark, and Thibault Sellam. 2023. Repairing the cracked foundation: survey of obstacles in evaluation practices for generated text. Journal of Artificial Intelligence Research, 77:103166. Mor Geva, Yoav Goldberg, and Jonathan Berant. 2019. Are we modeling the task or the annotator? an investigation of annotator bias in natural language understanding datasets. arXiv preprint arXiv:1908.07898. Max Glockner, Ieva Staliunaite, James Thorne, Gisela Vallejo, Andreas Vlachos, and Iryna Gurevych. 2024. AmbiFC: Fact-checking ambiguous claims with evidence. Transactions of the Association for Computational Linguistics, 12. Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey Bigham. 2018. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 36083617. Micah Hodosh, Peter Young, and Julia Hockenmaier. 2013. Framing image description as ranking task: Data, models and evaluation metrics. Journal of Artificial Intelligence Research, 47:853899. Mina Huh, Fangyuan Xu, Yi-Hao Peng, Chongyan Chen, Hansika Murugu, Danna Gurari, Eunsol Choi, and Amy Pavel. 2024. Long-form answers to visual questions from blind and low vision people. arXiv preprint arXiv:2408.06303. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Rhea Kapur and Elisa Kreiss. 2024. Reference-based metrics are biased against blind and low-vision users image description preferences. In NLP4PI 2024, Miami, Florida, USA. ACL. Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. 2016. diagram is worth dozen images. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11 14, 2016, Proceedings, Part IV 14, pages 235251. Springer. Elisa Kreiss, Cynthia Bennett, Shayan Hooshmand, Eric Zelikman, Meredith Ringel Morris, and Christopher Potts. 2022. Context matters for image descriptions for accessibility: Challenges for referenceless evaluation metrics. arXiv preprint arXiv:2205.10646. Elisa Kreiss, Eric Zelikman, Christopher Potts, and Nick Haber. 2023. Contextref: Evaluating referenceless metrics for image description generation. arXiv preprint arXiv:2309.11710. Dong Won Lee, Chaitanya Ahuja, Paul Pu Liang, Sanika Natu, and Louis-Philippe Morency. 2022. Multimodal lecture presentations dataset: Understanding multimodality in educational slides. arXiv preprint arXiv:2208.08080. Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. 2024a. Seedbench: Benchmarking multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1329913308. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR. 9 Lei Li, Yuanxin Liu, Linli Yao, Peiyuan Zhang, Chenxin An, Lean Wang, Xu Sun, Lingpeng Kong, and Qi Liu. 2024b. Temporal reasoning transfer from text to video. arXiv preprint arXiv:2410.06166. Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and Qi Liu. 2024c. Multimodal arxiv: dataset for improving scientific comprehension of large vision-language models. arXiv preprint arXiv:2403.00231. Lei Li, Yuancheng Wei, Zhihui Xie, Xuqing Yang, Yifan Song, Peiyi Wang, Chenxin An, Tianyu Liu, Sujian Li, Bill Yuchen Lin, et al. 2024d. Vlrewardbench: challenging benchmark for visionlanguage generative reward models. arXiv preprint arXiv:2411.17451. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2629626306. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. 2023. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255. Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, KaiWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS). Alan Lundgard and Arvind Satyanarayan. 2022. Accessible visualization via natural language descriptions: four-level model of semantic content. IEEE Transactions on Visualization and Computer Graphics, 28(1):10731083. Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. 2022. ChartQA: benchmark for question answering about charts with visual and logical reasoning. In Findings of ACL 2022, Dublin, Ireland. ACL. Ahmed Masry, Mehrad Shahmohammadi, Md Rizwan Parvez, Enamul Hoque, and Shafiq Joty. 2024a. ChartInstruct: Instruction tuning for chart comprehension and reasoning. In Findings of ACL 2024, Bangkok, Thailand. ACL. Ahmed Masry, Megh Thakkar, Aayush Bajaj, Aaryaman Kartha, Enamul Hoque, and Shafiq Joty. 2024b. Chartgemma: Visual instruction-tuning for chart reasoning in the wild. arXiv preprint arXiv:2407.04172. Yixin Nie, Xiang Zhou, and Mohit Bansal. 2020. What can we learn from collective human opinions on arXiv preprint natural language inference data? arXiv:2010.03532. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36. Sara Sarto, Manuele Barraco, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara. 2023. Positiveaugmented contrastive learning for image and video the captioning evaluation. IEEE/CVF conference on computer vision and pattern recognition, pages 69146924."
        },
        {
            "title": "In Proceedings of",
            "content": "Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu. 2020. Mpnet: Masked and permuted pretraining for language understanding. Advances in neural information processing systems, 33:16857 16867. Benny Tang, Angie Boggust, and Arvind Satyanarayan. 2023. Vistext: benchmark for semantically rich chart captioning. arXiv preprint arXiv:2307.05356. Christine Tippett. 2016. What recent research on diagrams suggests about learning with rather than learning from visual representations in science. International Journal of Science Education, 38(5):725 746. Ramakrishna Vedantam, Lawrence Zitnick, and Devi Parikh. 2015. Cider: Consensus-based image deIn Proceedings of the IEEE scription evaluation. conference on computer vision and pattern recognition, pages 45664575. Ioanna Vekiri. 2002. What is the value of graphical displays in learning? Educational psychology review, 14:261312. Yuiga Wada, Kanta Kaneda, Daichi Saito, and Komei Sugiura. 2024. Polos: Multimodal metric learning from human feedback for image captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1355913568. 10 Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. 2024. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191. Xinyu Zhang, Lingling Zhang, Yanrui Wu, Muye Huang, Wenjun Wu, Bo Li, Shaowei Wang, and Jun Liu. 2024b. Diagramqg: dataset for generating concept-focused questions from diagrams. arXiv preprint arXiv:2411.17771. Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao, and Ping Luo. 2024a. Lvlm-ehub: comprehensive evaluation benchmark for large visionIEEE Transactions on Pattern language models. Analysis and Machine Intelligence. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592. Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen Lin. 2024b. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. arXiv preprint arXiv:2406.08464. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2 technical report. arXiv preprint arXiv:2407.10671. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2024. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9556 9567. Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Ziyu Liu, Shengyuan Ding, Shenxi Wu, Yubo Ma, Haodong Duan, Wenwei Zhang, et al. 2025. Internlm-xcomposer2. 5-reward: simple yet effective multi-modal reward model. arXiv preprint arXiv:2501.12368. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. 2023. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1197511986. Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang, and Jiaqi Wang. 2024a. Long-clip: Unlocking the long-text capability of clip. In European Conference on Computer Vision, pages 310325. Springer. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675. Wenqi Zhang, Hang Zhang, Xin Li, Jiashuo Sun, Yongliang Shen, Weiming Lu, Deli Zhao, Yueting Zhuang, and Lidong Bing. 2025. 2.5 years in class: multimodal textbook for vision-language pretraining. arXiv preprint arXiv:2501.00958."
        },
        {
            "title": "A Our Complete Dataset Collection",
            "content": "We describe the rest of the dataset collection. A.1 SIGHTATIONVQA In constructing Desc++ for comparison with Desc, we discovered that the quality of the QuestionAnswer pairs directly determines the quality of the resulting context. To clarify why we invested significant effort in carefully designing these question answer pairs, we employed an LLM as judge to evaluate and classify them according to different quality levels. To measure the quality of the Question Answer pairs, we used the VLM-as-a-Judge prompt using GPT-4o model. The prompt itself is found in Appendix G. Figure 4: Percentage distribution of the quality of question-answer pairs in AI2D and SIGHTATIONVQA Following Chen et al. (2023) and Xu et al. (2024b), we compared two sets of QA pairs with GPT-4O. Our generated QA sets are with up to six QA pairs for each of 4,903 diagrams, producing total of 29,438 QA pairs (sometimes exceeding six pairs per diagram). As can be seen Figure 4, we found that 92.66% of these our generated QA pairs were rated Excellent, while 4.47% were deemed Good, underscoring their high quality. By contrast, the QA pairs sourced from the AI2D dataset, though numerous, included large portion of masked or minimally informative queries. After filtering out these masked questions, we were left with 9,708 self-contained questions spanning 3,099 diagrams, where 73.86% received an Excellent rating and 13.65% were deemed Good. This comparison reveals that our generated QA pairs provide more robust and contextually relevant foundation, reinforcing the value of our meticulous QA design in constructing effective Desc++. A.2 SIGHTATIONREASONING Employing Desc and Desc++, we constructed SIGHTATIONREASONING, reasoning dataset that consists of reasoning path and reasoning QA pairs. The prompts used for the construction of reasoning datasets are found in Appendix G. To verify the quality of contents as reasoning dataset, 10% of the samples were randomly selected to be manually inspected. Reasoning Path The reasoning path explains the logical flow or deployment of the contents in diagram such as cause-effect relationships, step-by-step processes, explanations of phenomena, comparions of contrasts, or dependencies between components. Employing 1k diagram images and descriptions in SIGHTATION, the reasoning path was identified and generated by QVQ-72B-Preview. The reasoning path extracted from Desc and Desc++ is denoted as RPath and RPath++ respectively. Consequently, one diagram possesses two reasoning paths, resulting in 2k paths in total. Reasoning QA The reasoning QA encompasses five types of QA pairs that require logical understanding of diagram contents and reasoning capabilities: Causal, Process, Conditional, Explanatory, and Reverse. Similarly to the reasoning path data, RQA and RQA++ were generated by QVQ-72B-PREVIEW using 1k diagram images and descriptions. As result, one diagram contains 10 reasoning QA pairs in which RQA and RQA++ respectively include 5 pairs. While SIGHTATIONVQA covers the visual"
        },
        {
            "title": "Dataset",
            "content": "SIGHTATION (Ours) -COMPLETIONS -PREFERENCE -RETRIEVAL -VQA -REASONING VisText (Tang et al., 2023) MathVista (Lu et al., 2023) ChartGemma (Masry et al., 2024b) CBD (Bhushan and Lee, 2022) VizWiz-VQA (Gurari et al., 2018) VizWiz-LF (Huh et al., 2024) DiagramQG (Zhang et al., 2024b) ScienceQA (Lu et al., 2022) ChartQA (Masry et al., 2022) Flickr8K (Hodosh et al., 2013) PASCAL-50S (Vedantam et al., 2015) Polaris (Wada et al., 2024) Multimodal Arxiv (Li et al., 2024c)"
        },
        {
            "title": "Average\nText\nLength",
            "content": "Validated by BLV? 188.3 (words) 74.6 58.0 37.5 114.5 8.6 73.2 9.5 119.7 13.0 11.8 8.8 11. 49."
        },
        {
            "title": "Applications",
            "content": "Completion Preference alignment Retrieval Reward modeling Completion VQA, Reasoning Completion"
        },
        {
            "title": "Summarization\nVQA\nVQA",
            "content": "DQA VQA, Reasoning VQA Description Description"
        },
        {
            "title": "Description",
            "content": "Description, VQA, Reasoning"
        },
        {
            "title": "Dimensions\nAssessed",
            "content": "Factuality Informativeness Succinctness Diversity Usefulness, in 4 finer aspects Interpretiveness Accuracy, Descriptiveness Correctness Informativeness, Factual Correctness, Structure Adequacy, Fluency, Coherence Diversity, Answerability Relevance, Helpfulness, Plausibility, Fluency, Correctness Diversity, Object Density Correctness Syntactic Diversity Diversity Factuality, Literality, Generality Fluency, Relevance, Descriptiveness Factual Alignment, Visual Clarity, Unambiguous Textual Information, Question and Option Relevance, Comprehensive Integration, Equitable Content VQA, Reasoning Difficulty, Knowledge, Reasoning MMMU (Yue et al., 2024) 53.2 Table 5: Extended related work. structure and details of diagram, the reasoning QA in SIGHTATIONREASONING consists of more knowledge-intensive questions that require logical thinking, paving the way for the reasoning applications of SIGHTATION. Evaluation The reasoning path of SIGHTATIONREASONING can be used as an overall representation of \"logical flow\" or \"relationships between instances\" in diagram when understanding it, which was emphasized in the BLV educator questionnaire. To make model employ this information when responding to reasoning questions and evaluate the reasoning paths, we fed QWEN2-VL-7B-INSTRUCT with RPath and RPath++ separately and asked it to solve 10 questions in RQA and RQA++. The similarity score between the gold answers and generated answers was calculated using BERTSCore (Zhang et al., 2019), and the scores for the two cases both resulted in 0.975, verifying the equal usefulness of RPath and RPath++."
        },
        {
            "title": "B Further Related Work",
            "content": "In Table 5, we extend Table 1 for more comprehensive view of neighboring datasets. To the best of our knowledge, there exists no dataset to date surpassing our contribution in terms of the breadth of use cases and granularity of validation with BLV individuals."
        },
        {
            "title": "C Details on the Annotations",
            "content": "C.1 Logistics All experimentation was reviewed and approved by the Institutional Review Board. Recruiting the sighted general group was done via an online forum. Each sighted general group annotator was paid 13 Figure 5: Less can be more for BLV users. Our approach streamlines details to highlight the core information while emphasizing key details to increase information density and maximize information efficiency per unit length."
        },
        {
            "title": "Description",
            "content": "()model ()anchor"
        },
        {
            "title": "Bestmodel\nanchor",
            "content": "The description Desc generated by (or an annotation on generation from) model {g, q}, for GPT-4O MINI and QWEN2-VL, respectively. Later overloaded with narrower descriptors, such as base, sft, and sft+dpo to refer to the baseline/tuned models. The conditioning input at the description generation stage. anchor {None, ++}, for the one-pass image-only conditioning and the two-pass image+QA conditioning, respectively. Preference annotation between two Descmodels on different conditioning inputs. Value takes either of the anchor set {None, ++} Rating annotation in terms of Aspect {Factuality, Informativeness, Succinctness, Diversity, Usefulness-Gen, Usefulness-Sum, UsefulnessMCQ, Usefulness-OEQ, Nature}, for description generated by model conditioned on anchor. Value is an integer ranging from 1 to 5, on the 5-point Likert scale. Best sentence annotation. Value is substring of Descmodel anchor. Table 6: Notations an approximate equivalent of USD80 for completing the assigned task. Recruiting the educators was done by directly corresponding with the schools for the blind. sighted educator was compensated an approximate equivalent of USD80. BLV educator was compensated an approximate equivalent of USD80 to USD160, depending on the number of samples completed. C.2 Annotations Statistics Preliminaries Of the 1,000 diagrams distributed to the annotators, 956 have been annotated by three annotators; 41 by two; 1 by single annotator; and 2 by none. We collected annotations on 3,992 diagram-description pairs, each with at most 3 annotations. 14 Internal Consistency statistic is widely interpreted as the reliability of set of survey items. In Table 7, we report the Cronbachs alpha value for each assessment group. The"
        },
        {
            "title": "Group",
            "content": "Cronbachs α"
        },
        {
            "title": "Sighted General\nSighted Educators\nBLV Educators",
            "content": "0.70 0.94 0.80 Table 7: Our survey items are considered of acceptable ( 0.7) to excellent ( 0.9) reliability. Point-Biserial Correlation We examine the relationship between the binary variable, Preference, and the 5-point scale ratings per aspect."
        },
        {
            "title": "Factuality",
            "content": "Informativeness Succinctness Diversity Usefulness-Gen"
        },
        {
            "title": "Sighted General\nSighted Educators",
            "content": "0.36 0.25 0.37 0.30 0.31 0.30 0.34 0.34 0.43 Table 8: Correlation values between preference choice and aspect ratings were found to be moderately positive and statistically significant. (***: < 0.001) Cohens Cohens is widely used statistic to measure the size of the effect of treatment. It is the difference in the means of the treatment and control groups, normalized by the pooled standard deviation. By guidelines set forth by Cohen himself, values over 0.2 are typically considered small effect size; 0.5, medium; and 0.8, large. Figure 6: Win rates by model. C.3 Annotations Post-processing Preference Choice We aggregate the multiple annotations on the basis of majority. That is, for the three-annotation samples, 3:0 or 2:1 is considered victory and the victor Desc wins that sample. For two-annotation samples with differing preferences, tie is recorded. The overall win-loss statistics normalized against the number of diagrams (998) is shown in Figure 6."
        },
        {
            "title": "Rating Assessment",
            "content": "Best Sentence Choice The best sentence for each context was manually selected by BLV annotators after listening to the context. We analyzed peoples preferences by examining the position and length of the best sentence within each context. Position The normalized position of the best sentence is shown in Figures 7-8. To calculate the relative position, both the context and the best sentence were tokenized at the word level, and the position of the overlapping best sentence within the context was identified. This position was then normalized to 15 value between 0 and 1 by dividing it by the total length of the context. Furthermore, since some BLV annotators could not select best sentence within the context, filtering step was applied by setting an overlap threshold of 0.9 to account for such cases. Figures 7-8 illustrate that the best sentences in each context are predominantly positioned at the beginning and end. This pattern can be attributed to cognitive biases, specifically primacy bias and recency bias. Primacy bias refers to the tendency to place greater importance on the first pieces of information encountered in sequence, while recency bias reflects the tendency to prioritize the most recently encountered information. Consequently, these biases increase the likelihood that preferred sentences will be selected from the beginning and end of the context. Figure 7: Descriptions generated by GPT-4O MINI Figure 8: Descriptions generated by QWEN2-VL Length The length of the best sentence in each context is presented in Figure 9. The length was determined by counting the total number of words in the best sentence. As shown in Figure 10, the best sentences across different contexts predominantly consist of 20 to 30 words, exhibiting similar distribution pattern."
        },
        {
            "title": "D Retrieval Dataset Construction",
            "content": "The winner among the four human-annotated descriptions was assigned as the top 1 positive in terms of preference and average rating. The top 5 set contains all 4 human-annotated descriptions and 1 synthesized description; the top 10 set is superset of the top 5, joined by 5 more synthetic descriptions. The synthetic descriptions are perturbed versions of the human-annotated descriptions, each missing random, non-best sentence. The 10 hard negatives for an image were selected among the combined pool of top 1 descriptions for other images, sorted by cosine similarity in the embedding space. The embeddings were computed by widely used sentence transformer, ALL-MPNET-BASE-V2(Song et al., 2020)."
        },
        {
            "title": "E Detailed Results",
            "content": "We report the VLM-as-a-Judge evaluation and classic metric results in Tables 9, 10, 11, and 12. 16 Figure 9: boxplot of best sentence length E.1 Evaluation by Automatic Metrics QVQ-72B-PREVIEW On GPT and Qwen 72B generations, the VLM judge did not reveal significant difference between the two anchors, and the little differences present aligned with assessments by the sighted general group, as can be expected from general-purpose VLM. It is important to note that even state-of-the-art VLM fails to capture the BLV perspectives in text evaluation. Classic Metrics To our surprise, almost all instances of classic metric evaluations resulted in win for the ++ anchor. However, the numbers from classic metrics evaluation are more of shortcoming on the part of the classic metrics, rather than an accurate portrayal of the effectiveness of our proposed latent supervision. This is because our gold ground truths from BLV educators show that, while the QA-guided generation does manifest in ways beneficial to BLV individuals, classic automatic metrics poorly represent the assessment space covered by BLV, such as with the Diversity and Usefulness-OEQ aspects."
        },
        {
            "title": "Assessments for",
            "content": "Desc Desc++ Experiment 1a GPT-4O MINI vs. GPT-4O MINI CLIP Score SigLIP Score BLIP-2 Retrieval Score Self-BLEU PAC-Score LongCLIP-B Score LongCLIP-L Score VLM-as-a-Judge Evaluation Average Factuality Informativeness Succinctness Diversity Sighted General Group Average Factuality Informativeness Succinctness Diversity Sighted Educator Group Average Factuality Informativeness Succinctness Diversity Usefulness to BLV BLV Educator Group Average Succinctness Diversity Usefulness, Summary Usefulness, Multiple-chioce Questions Usefulness, Open-ended Questions Nature of Context 0.476 0.921 0.495 0.256 0.699 0.507 0. 4.080 4.433 4.200 4.108 3.578 3.983 4.128 4.367 3.556 3.879 3.22 3.35 3.43 2.78 3.18 3.35 2.98 2.43 3.23 2.95 3.20 2.88 2.98 0.524 0.914 0.505 0.268 0.703 0.493 0.469 4.033 4.445 4.166 4.146 3.375 3.962 4.093 4.032 4.040 3.685 3.35 3.30 3.43 3.53 3.08 3.40 3.17 2.55 3.15 3.33 3.28 3.13 3.17 Table 9: The full evaluation on descriptions by GPT. Nature of Context values are not in bold because it is categorical variable."
        },
        {
            "title": "Assessments for",
            "content": "Desc Desc++ CLIP Score SigLIP Score BLIP-2 Retrieval Score Self-BLEU PAC-Score LongCLIP-B LongCLIP-L VLM-as-a-Judge Evaluation Average Factuality Informativeness Succinctness Diversity Sighted General Group Average Factuality Informativeness Succinctness Diversity Sighted Educator Group Average Factuality Informativeness Succinctness Diversity Usefulness to BLV 0.451 0.911 0.494 0.260 0.709 0.443 0.468 4.094 4.483 4.239 4.026 3. 4.002 3.982 4.233 3.889 3.905 4.01 4.05 4.38 3.80 3.80 4.03 0.549 0.932 0.506 0.274 0.716 0.610 0.532 3.916 4.428 3.952 4.072 3.210 3.850 4.060 3.782 4.035 3.523 4.13 4.05 4.13 4.48 3.83 4.15 Experiment 1b QWEN2-VL-72BINSTRUCT vs. QWEN2-VL-72BINSTRUCT Table 10: The full evaluation on descriptions by the 72B model. Due to limited recruiting, BLV annotators were not given this set. 19 + + + e d + c t + + e s D + + e e c 9 4 5 0 . 0 4 9 0 . 9 0 5 . 1 8 2 0 . 8 1 7 0 . 5 5 5 . 1 4 5 0 . 9 1 5 3 . 4 8 7 . 7 7 5 3 . 9 5 6 3 . 4 5 0 . 9 4 4 . 6 6 4 . 0 5 . 0 5 4 . 2 3 4 . 0 5 . 7 1 3 . 0 3 3 . 3 5 . 5 4 3 . 3 7 3 . 0 0 . 0 0 3 . 1 5 4 0 . 4 1 9 . 1 9 4 0 . 7 7 2 0 . 2 1 7 . 5 4 4 0 . 9 5 4 0 . 4 3 3 . 1 3 4 3 . 8 3 4 3 . 7 4 3 . 8 1 1 3 . 4 3 4 . 2 4 . 9 3 4 . 7 3 4 . 8 1 . 4 3 4 . 2 6 2 . 5 1 . 3 0 2 . 8 8 2 . 8 8 . 8 2 2 . 0 5 2 . 4 3 5 . 1 3 9 . 0 7 0 5 . 0 1 9 2 . 0 0 1 7 . 0 8 4 5 . 0 6 4 5 . 3 6 6 . 3 4 7 9 . 3 5 1 7 . 3 4 7 7 . 3 8 8 1 . 3 6 6 4 . 1 1 9 . 0 3 9 4 . 0 5 8 2 . 0 6 0 7 . 0 2 5 4 . 0 4 5 4 . 2 3 7 . 3 6 2 9 . 3 4 5 8 . 3 7 0 7 . 3 2 4 4 . 3 8 5 5 . 0 1 4 9 . 0 9 0 5 . 0 8 7 2 . 0 8 1 7 . 0 1 8 5 . 3 8 5 . 0 9 0 5 . 3 3 8 7 . 3 7 6 5 . 3 2 6 6 . 3 5 2 0 . 5 9 . 3 3 0 . 4 5 0 . 4 0 9 . 3 0 8 . 3 8 9 . 5 2 . 3 3 3 . 3 0 1 . 3 0 4 . 3 0 1 . 3 1 2 . 5 2 . 3 2 4 4 . 0 6 1 9 . 0 1 9 4 . 0 4 7 2 . 0 1 1 7 . 9 1 4 . 0 7 1 4 . 0 7 0 3 . 3 6 2 4 . 3 4 9 3 . 3 6 4 3 . 2 6 0 . 3 1 9 . 3 5 9 . 3 3 0 . 4 8 9 . 3 5 6 . 3 9 . 3 3 3 . 3 5 4 . 3 8 1 . 3 3 5 . 3 5 1 . 5 1 . 3 3 3 . 3 r n a v d - - -"
        },
        {
            "title": "M\nL\nV",
            "content": "s v m n e n u i t y r D r p G t d t S e i r I n i S l c t e D"
        },
        {
            "title": "V\nL\nB\no\nt",
            "content": "s l s o e c c - i M , n e n s d e - O , n e r u , n e x o e a a A r o u L n i S i v r l i R 2 - B c L e S C ) o ( r M"
        },
        {
            "title": "U\nE\nL\nB",
            "content": "- S c - B - C L - C L + 2 + e . b 2 e f e e e r T T - 2 - - 2 Q n - F a u n a v m a t ."
        },
        {
            "title": "T\nF\nS",
            "content": "o i a r d 2 f i l E : 1 1 a i a t t w o g a r o a c r s , c t i i - 5 n u d l o u e C r N . r e t n e i t d u t t r e a S r F . u d h . a v i e a t u b b o a l 20 + + + e d + c t + + e s D + + e e c 3 9 5 0 . 2 5 9 0 . 0 1 5 . 5 7 2 0 . 8 1 7 0 . 3 8 5 . 8 8 5 0 . 2 4 6 3 . 1 6 1 . 2 4 6 3 . 8 5 8 3 . 6 0 9 . 5 9 3 . 5 9 3 . 3 1 . 0 0 4 . 0 7 3 . 5 9 . 1 7 3 . 5 6 4 . 0 9 . 5 3 4 . 5 9 3 . 8 5 . 5 8 1 . 7 0 4 0 . 3 2 9 . 0 9 4 0 . 8 6 2 0 . 1 1 7 . 7 1 4 0 . 2 1 4 0 . 8 4 9 . 9 8 2 4 . 0 0 1 4 . 4 0 9 . 8 9 4 3 . 7 9 3 . 0 0 . 8 0 4 . 8 8 3 . 8 8 . 3 0 4 . 2 8 3 . 8 4 . 3 1 4 . 5 2 4 . 5 1 . 0 7 3 . 3 2 2 . 9 8 5 . 4 4 9 . 0 1 1 5 . 0 2 8 2 . 0 4 1 7 . 0 9 8 5 . 0 4 1 5 . 8 5 7 . 3 1 6 2 . 4 0 7 7 . 3 4 6 9 . 3 6 3 0 . 3 1 1 4 . 8 1 9 . 0 9 8 4 . 0 5 7 2 . 0 6 0 7 . 0 2 5 4 . 0 6 8 4 . 1 2 0 . 4 1 7 3 . 4 1 6 1 . 4 4 7 9 . 3 6 7 5 . 3 7 7 5 . 0 2 5 9 . 0 0 1 5 . 0 4 7 2 . 0 0 2 7 . 0 1 8 5 . 3 8 5 . 0 2 5 6 . 3 7 5 1 . 4 5 4 6 . 3 2 9 8 . 3 3 1 9 . 7 9 . 3 6 5 . 4 7 8 . 3 5 1 . 4 4 6 . 3 7 9 . 2 8 . 3 5 5 . 4 0 2 . 4 5 5 . 4 0 2 . 4 0 8 . 0 6 . 1 3 2 4 . 0 2 2 9 . 0 0 9 4 . 0 8 6 2 . 0 3 1 7 . 9 1 4 . 0 7 1 4 . 0 1 5 9 . 3 1 7 2 . 4 1 0 1 . 4 6 4 9 . 6 8 4 . 3 7 3 . 4 2 8 . 4 7 6 . 4 5 9 . 3 3 2 . 7 3 . 4 7 8 . 3 0 3 . 4 0 2 . 4 5 1 . 4 0 4 . 0 8 . 3 5 3 . 2 r n a v d - - -"
        },
        {
            "title": "M\nL\nV",
            "content": "s v m n e n u i t y r D r p G t d t S e i r I n i S l c t e D"
        },
        {
            "title": "V\nL\nB\no\nt",
            "content": "s l s o e c c - i M , n e n s d e - O , n e r u , n e x o e a a A r o u L n i S i v r l i R 2 - B c L e S C ) o ( r M"
        },
        {
            "title": "U\nE\nL\nB",
            "content": "- S c - B - C L - C L + 7 + e . b 7 e f e e e r T T - 7 - - 2 Q n - F e t p - 5 n u l m e t e i l n h t N . o 7 f i l : 2 1 a T"
        },
        {
            "title": "T\nF\nS",
            "content": ", c 2 h s . u d h i a t t l u i m a o a c r s , c t r s a e C r N . r e t n e m t d b s o e s m F s . a v i e a i u b b"
        },
        {
            "title": "F Annotator Demographics and Interviews",
            "content": "F.1 Demographics F.1.1 BLV Educators Please refer to Table 17. F.1.2 Sighted Educators Please refer to Table 18."
        },
        {
            "title": "Prompts for Context Generation",
            "content": "Generating Desc You are helpful expert who is knowledgeable in various fields of academia. You are skilled in reading, interpreting, and understanding academic papers and figures contained therein. You are tasked with elaborating on the given information, which consists of figure image. Write an informative and explanatory text in one paragraph under 200 words that describes the basic characteristics of the figure and incorporates important information. You may attempt to internally identify implicit points of curiosity for someone who is trying to understand the given figure, and then include explanations for those points in your response. Avoid mere reiteration of the given information as much as possible. You need not specify the origins of various parts of your response. [Optional: Aspect Suffix] Generating Desc++ You are helpful expert who is knowledgeable in various fields of academia. You are skilled in reading, interpreting, and understanding academic papers and figures contained therein. You are tasked with elaborating on the given information, which consists of figure image and several question-answer pairs that have been derived from the figure. Write an informative and explanatory text in one paragraph under 200 words that describes the basic characteristics of the figure and incorporates important information from the question-answer pairs. You may attempt to internally identify implicit points of curiosity for someone who is trying to understand the given figure, and then include explanations for those points in your response. Avoid mere reiteration of the given information as much as possible. You need not specify the origins of various parts of your response. Here is the reference information: [QA_PAIRS: vqas] [Optional: Aspect Suffix]"
        },
        {
            "title": "Aspect Suffixes",
            "content": "Factuality When generating the diagram description, pay close attention to making it factual. highly factual text delivers only the facts that are grounded in the diagram. Informativeness When generating the diagram description, pay close attention to making it informative. highly informative text describes all of the diagram, holistically. Succinctness When generating the diagram description, pay close attention to making it succinct. highly succinct text is concise and to the point. Diversity When generating the diagram description, pay close attention to making it diverse. highly diverse text captures variety of perspectives from the diagram and employs multiple effective ways of getting the diagram message across. Prompt for Question-answer Pair Generation Please generate six question-and-answer pairs based on the provided image to aid in creating comprehensive context. This context should include all essential details, allowing BLV (Blind and Low Vision) users to rely on the generated text instead of viewing the image (e.g., accessing information audibly). The question-and-answer pairs should cover both the main structure and finer details present in the image."
        },
        {
            "title": "Description Generators Metrics",
            "content": "Assessments for Descq72bbase Descq7bdpo ++ Experiment 3a QWEN2-VL-72BINSTRUCT vs. FINE-TUNED QWEN2-VL-7BINSTRUCT CLIP Score SigLIP Score BLIP-2 Retrieval Score Self-BLEU PAC-Score LongCLIP-B Score LongCLIP-L Score VLM-as-a-Judge Evaluation Average Factuality Informativeness Succinctness Diversity Sighted Educators Group Average Factuality Informativeness Succinctness Diversity Usefulness to BLV BLV Educators Group Average Succinctness Diversity Usefulness, Summary Usefulness, Multiple-choice Questions Usefulness, Open-ended Questions Nature of Context 0.390 0.911 0.487 0.260 0.709 0.388 0.445 4.095 4.477 4.262 3.990 3.652 3.21 3.30 3.33 2.95 3.13 3.35 3.69 3.60 3.60 3.95 3.70 3.70 3.60 0.610 0.952 0.513 0.275 0.719 0.612 0.555 3.650 4.238 3.586 3.894 2.880 3.01 3.28 2.95 3.18 2.68 2.98 4.33 4.55 3.90 4.30 4.55 4.45 4. Table 13: The smaller model outperforms larger variant across many metrics. It is also important to note that the VLM judgments align better with sighted educators than with BLV educators. Further analysis is found in Section 5. This tendency is especially strong with the pairwise comparison between 72Band 7B-generated descriptions. Nature of Context values are not in bold because it is categorical variable."
        },
        {
            "title": "Description Generators Metrics",
            "content": "Assessments for Descq7bbase Descq2bdpo ++ Experiment 3b QWEN2-VL-7BINSTRUCT vs. FINE-TUNED QWEN2-VL-2BINSTRUCT CLIP Score SigLIP Score BLIP-2 Retrieval Score Self-BLEU PAC-Score LongCLIP-B Score LongCLIP-L Score VLM-as-a-Judge Evaluation Average Factuality Informativeness Succinctness Diversity Sighted Educators Group Average Factuality Informativeness Succinctness Diversity Usefulness to BLV BLV Educators Group Average Succinctness Diversity Usefulness, Summary Usefulness, Multiple-choice Questions Usefulness, Open-ended Questions Nature of Context 0.486 0.922 0.500 0.268 0.713 0.316 0.559 3.921 4.203 4.046 3.942 3.493 4.75 4.75 4.65 4.88 4.80 4.65 4.13 4.05 4.08 3.85 4.53 4.23 4.08 0.514 0.940 0.500 0.281 0.718 0.684 0.441 3.545 3.935 3.592 3.709 2.945 4.44 4.50 4.38 4.40 4.63 4.28 4.32 4.15 4.15 4.13 4.58 4.35 4. Table 14: The 2B model performs on par with the 7B variant. Again, VLM judgments align better with sighted educators than with BLV educators. Further analysis is found in Section 5. Nature of Context values are not in bold because it is categorical variable."
        },
        {
            "title": "Description Generators Metrics",
            "content": "Assessments for Descchartgemma Descq2bsft Experiment 3c CHARTGEMMA (3B) vs. FINE-TUNED QWEN2-VL-2BINSTRUCT CLIP Score SigLIP Score BLIP-2 Retrieval Score Self-BLEU PAC-Score LongClip-B LongClip-L VLM-as-a-Judge Evaluation Average Factuality Informativeness Succinctness Diversity 0.450 0.872 0.511 0.305 0.705 0.316 0.559 2.951 3.068 2.848 3.253 2. 0.550 0.940 0.490 0.280 0.716 0.684 0.441 3.860 4.119 3.967 3.925 3.428 Table 15: 2B model fine-tuned on SIGHTATIONCOMPLETIONS outperforms 3B model tuned on larger dataset. Note that CHARTGEMMA is not meant for conversational use. Hence, for fair comparison, we did not enter our guided generation prompt and instead input only the brief request Generate caption to both models. Figure 10: Retrieval performance was measured with 2-way cross validation. On our test set (Left), the COCO-tuned BLIP-2 generalizes poorly, whereas on the COCO test set (Right), the SIGHTATIONRETRIEVAL-tuned BLIP-2 performs on par with the COCO-tuned BLIP-2."
        },
        {
            "title": "Train set",
            "content": "N/A (Pre-trained)"
        },
        {
            "title": "COCO",
            "content": "SIGHTATIONRETRIEVAL (Ours) 2-way Cross-validation of BLIP-"
        },
        {
            "title": "COCO Ours COCO",
            "content": "Recall@1 Recall@5 Recall@10 Precision@1 Precision@5 Precision@10 0.171 0.767 0.856 0.767 0.048 0.210 0.340 0.371 0.324 0.263 0.180 0.766 0.185 0.831 0.033 0.134 0.229 0.250 0.204 0.175 0.924 0.831 0.900 0."
        },
        {
            "title": "Ours",
            "content": "0.076 0.348 0.549 0.585 0.535 0.425 Table 16: SIGHTATIONRETRIEVAL shows promising potential as challenging and effective training material for image-to-text retrievers. Two important observations can be made: the model trained on our set generalizes to COCO better than the other direction; our model performs on par with the model that was both trained and tested on COCO. = 10 values are missing for tests with COCO, since its samples contain only 5 positives each. ID"
        },
        {
            "title": "Age",
            "content": "B1 B2 B3 B4 B5 B6 B7 B"
        },
        {
            "title": "M\nF\nM\nM\nM\nM\nM\nM",
            "content": "54 46 47 51 20 46 44 45 Teaching Experience (years) 28 21 5 26 1 19"
        },
        {
            "title": "16\nCongenital\n9\n14\nCongenital\n—\nCongenital\nCongenital",
            "content": "AI Use, Generic AI Use, Accessibility ChatGPT, Gemini ChatGPT ChatGPT, Gemini SeeingAI, ChatGPT, Adot, Perplexity, Adot SeeingAI, ChatGPT Be_My_Eyes, SeeingAI, ChatGPT, Claude Be_My_Eyes, SeeingAI, ChatGPT SenseReader SenseReader SenseReader SenseReader, NVDA, VoiceOver SenseReader, NVDA SenseReader SenseReader, VoiceOver SenseReader, VoiceOver Table 17: BLV Teachers Information. All the BLV teachers in our study were of blindness level 1, the severest. ID S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 Sex Age Teaching Experience (years) AI Use - Generic"
        },
        {
            "title": "M\nM\nM\nF\nF\nF\nM\nF\nM\nF",
            "content": "39 51 48 40 56 49 49 49 44 50 6.5 20 21 13 33 20 20 24 14 20 ChatGPT ChatGPT, wrtn ChatGPT ChatGPT ChatGPT Gemini ChatGPT, Claude ChatGPT Table 18: Sighted Teachers Information."
        },
        {
            "title": "Prompts for Reasoning Path Generation",
            "content": "You will be provided with diagram, along with two descriptions of it. As an expert and experienced educator, you are tasked to examine your descriptions to identify common reasoning paths, such as cause-effect relationships, step-by-step processes, explanations of phenomena, comparisons of contrasts, and dependencies between components. The identified reasoning paths should be under 25 words. Please provide the reasoning paths that you examined in the following JSON format: {\"Context1\": {\"ReasoningPath\": text}, \"Context2\": {\"ReasoningPath\": text}} DO NOT return anything other than the JSON above."
        },
        {
            "title": "Prompts for Reasoning QA Generation",
            "content": "You will be provided with diagram, along with two descriptions of it. As an expert and experienced educator, you are tasked to examine your descriptions to generate reasoning question and answer pairs of five categories such as: Causal Reasoning: \"Why does [event] happen?\" Process Reasoning: \"What happens after [event]?\" Conditional Reasoning: \"What if [condition] changes?\" Explanatory Reasoning: \"Explain the role of [component] in the process.\" Reverse Reasoning: \"Given [outcome], what might have caused it?\" Please provide the reasoning question and answer pairs that you generated in the following JSON format: {\"Context 1\": {\"Causal\": {\"Question\": text, \"Answer\": text}, \"Process\": {\"Question\": text, \"Answer\": text}, \"Conditional\": {\"Question\": text, \"Explanatory\": {\"Question\": text, \"Answer\": text}, \"Answer\": text}, \"Reverse\": {\"Question\": text, \"Answer\": text} }, \"Context 2\": {\"Causal\": {\"Question\": text, \"Answer\": text}, .. \"Reverse\": {\"Question\": text, \"Answer\": text} } Each generated question should be under 15 words and each corresponding answer should be under 25 words. DO NOT return anything other than the JSON above. 27 Prompt for VLM-as-a-Judge Evaluation of Description Pairs You will be provided with diagram, along with two descriptions of it. As an expert and experienced educator, you are tasked to evaluate each description based on the following qualities on 5-point Likert scale. For each statement, give score corresponding to how strongly you agree with the given statement: 1 (Strongly Disagree), 2 (Disagree), 3 (Neutral), 4 (Agree), or 5 (Strongly Agree). Diversity: The description captures variety of perspectives from the diagram and conveys multiple effective ways of getting the diagram message across. Succinctness: The description is concise and to the point, avoiding unnecessary details. Factuality: The description is accurate and reflects solely the information presented in the diagram. Informativeness: The description covers the diagram holistically, and it effectively conveys the main trends and insights of the diagram. Please provide your ratings in the following JSON format: { Context 1: { Diversity: score, Succinctness: score, Factuality: score, Informativeness: score }, Context 2: { Diversity: score, Succinctness: score, Factuality: score, Informativeness: score } } DO NOT return anything other than the JSON above. Prompt for VLM-as-a-Judge Evaluation of Question-Answer Pairs Instruction You need to rate the quality of the given Question and Answer in relation to diagram. Specifically, assess whether the Q&A correctly references and interprets the information presented in the diagram. Consider the Q&As clarity, specificity, and coherence as they pertain to the diagrams content.You must take into account not only the Answer but also whether an appropriate Question has been provided for the given diagram at the same time. The rating scale is as follow: very poor: The Q&A is unclear, vague, or incoherent in relation to the diagram. It lacks essential information or misinterprets the diagrams content. poor: The Q&A is somewhat unclear or omits important details from the diagram. It requires significant clarification or correction to align with the diagram. average: The Q&A is moderately clear and specific. It may need additional details or minor clarifications to fully match the diagrams information. good: The Q&A is clear, specific, and mostly well-formed in referencing the diagram. It provides sufficient context to understand how the diagram supports the question and answer. excellent: The Q&A is very clear, specific, and well-articulated. It precisely references and fully aligns with the diagram, containing all necessary details and context. Output Format Given the users diagram, question, and answer, you must: Provide an assessment that briefly explains the strengths and/or weaknesses of how the Q&A relates to the diagram. Output your rating (one of: very poor, poor, average, good, excellent) by filling in the placeholders below. [ { \"explanation\": \"[...]\", \"input_quality\": \"[very poor/poor/average/good/excellent]\" }, ... ]"
        },
        {
            "title": "Notes",
            "content": "DO NOT return anything else other than the JSON above. Number of item in above list should be same as the number of given QA pairs. Also the order for the explanation and input quality should be same as input QAs order Fine-tuning Configurations"
        },
        {
            "title": "Script Arguments\nDataset Name",
            "content": "Training Configurations Output Directory Evaluation Strategy Train Batch Size Evaluation Batch Size Gradient Accumulation Steps Training Epochs Save Total Limit bfloat16 Enabled Evaluation Steps Label Names Load Best Model at End Metric for Best Model Use Liger Max Sequence Length Remove Unused Columns Dataset Kwargs Gradient Checkpointing Gradient Checkpointing Kwargs Dataset Num Processors Torch Compile DDP Find Unused Parameters"
        },
        {
            "title": "Model Config\nUse PEFT\nModel Path\nTorch Dtype\nAttention Implementation",
            "content": "SFT Config (Qwen2-VL-2B-Instruct) DPO Config (Qwen2-VL-2B-Instruct)"
        },
        {
            "title": "SIGHTATIONPREFERENCE",
            "content": "anonymous steps 1 1 8 1 5 true 10 [\"labels\"] true eval_loss true 1024 false skip_prepare_dataset: true true use_reentrant: false 8 true anonymous steps 1 1 8 1 5 true 10 [\"labels\"] true eval_loss true 1024 true skip_prepare_dataset: false true use_reentrant: false 8 true false Qwen/Qwen2-VL-2B-Instruct bfloat16 flash_attention_2 false Qwen/Qwen2-VL-2B-Instruct bfloat16 flash_attention_2 Table 19: SFT and DPO configurations for Qwen2-VL-2B-Instruct. Tuning was performed on 4 A6000 GPUs."
        },
        {
            "title": "Script Arguments\nDataset Name",
            "content": "Training Configurations Output Directory Evaluation Strategy Train Batch Size Evaluation Batch Size Gradient Accumulation Steps Training Epochs Save Total Limit bfloat16 Enabled Evaluation Steps Label Names Load Best Model at End Metric for Best Model Use Liger Max Sequence Length Remove Unused Columns Dataset Kwargs Gradient Checkpointing Gradient Checkpointing Kwargs Dataset Num Processors DDP Find Unused Parameters Model Config Use PEFT Model Path Torch Dtype Attention Implementation LoRA Rank (r) LoRA Alpha LoRA Dropout LoRA Target Modules SFT Config (Qwen2-VL-7B-Instruct) DPO Config (Qwen2-VL-7B-Instruct)"
        },
        {
            "title": "SIGHTATIONPREFERENCE",
            "content": "anonymous steps 1 1 8 1 5 true 10 [\"labels\"] false eval_loss true 1024 false skip_prepare_dataset: true true use_reentrant: false 8 true anonymous steps 1 1 8 1 5 true 10 [\"labels\"] false eval_loss true 1024 true skip_prepare_dataset: false true use_reentrant: false 8 true true Qwen/Qwen2-VL-7B-Instruct bfloat16 flash_attention_2 16 16 0.1 q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj true Qwen/Qwen2-VL-7B-Instruct bfloat16 flash_attention_2 16 16 0.1 q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj Table 20: SFT and DPO configurations for Qwen2-VL-7B-Instruct. Tuning was performed on 4 A6000 GPUs."
        },
        {
            "title": "Configuration",
            "content": "BLIP-2 (Salesforce/blip2-itm-vit-g) Text model on CUDA:0, Vision model on CUDA:1 SIGHTATIONRETRIEVAL InfoNCE (temperature = 0.07) 1 (with gradient accumulation steps = 4) 5 AdamW (Text LR: 5e-5, Vision LR: 2e-5) Model GPUs Dataset Loss Batch Size Epochs Optimizer Gradient Clipping Max norm = 1.0 Scheduler Frozen Layers Linear warmup (10% of steps) All except: layernorm, projection, encoder layers 10-11 (Vision); layernorm, projection, encoder layers 10-11, crossattention (Text) Best and per-epoch saved to anonymized path"
        },
        {
            "title": "Checkpoints",
            "content": "Table 21: Training configurations for BLIP-2 image-text retrieval."
        },
        {
            "title": "I Guidelines",
            "content": "Annotation Guidelines for the Sighted General Group (1/4) Please carefully read the guidelines below and ensure accurate labeling. Your responses are considered high-quality data and can have critical implications for the experiment. Pay special attention to the Caution section. Annotation Guidelines Thank you for contributing to this project. In the following paragraphs, we will walk you through the project description, your tasks, and annotation examples. Project Our project targets the visually impaired. People who are Blind or have Low Vision (BLV) do not always benefit from the latest AI developments in the same way or extent as sighted users. In this pilot study, we would like to first assess exactly how much state-of-the-art models may assist sighted users, so that we may gain insights into (i) what state-of-the-art models can and cannot do and (ii) what modifications might be necessary to alter their assistive information to cater specifically for BLV users. Task Each task you are about to complete consists of: 1 image 2 image description pairs, each containing two texts Given these, you are tasked with: Selecting the overall winner for each pair Rating the qualities of each text Copying and pasting the best contributing sentence for each text Annotation Guidelines for the Sighted General Group (2/4) Here is detailed instruction on each task."
        },
        {
            "title": "Selecting the overall winner",
            "content": "Have look at both texts. In your opinion, which is the better description of the text? This may be based on general impression or your internal criteria. 33 Annotation Guidelines for the Sighted General Group (3/4)"
        },
        {
            "title": "Rating the qualities of each text",
            "content": "We break down what makes an image description good one into few qualities. Do not feel overly pressured to justify or rationalize your choice of the overall winner. In fact, you may treat the overall winner choice task completely independent of the quality rating task. On scale from 1 to 5, how well does the text exhibit the following qualities? Factuality: * Does the text contain facts about the image content? * You may give low score if the text contains statements that cannot be inferred from the image. These extraneous claims can include knowledge from the world external to the image, even though that knowledge itself may be true. * You may give low score if the text contains wrong statements about the image. Informativeness: * Does the text holistically describe the image content and help you become better informed about it? * You may give low score if some parts of the image seem left out in the text description. Succinctness: * Does the text describe the image content in concise yet helpful way? * Judgments based solely on text length should be avoided. * Instead, think of the density of information contained in the text. * You may give low score if the text contains redundant/repeated information, inefficient sentence structures, and/or overly simple vocabulary that tend to make the text feel sparse. Diversity: * Does the text help you understand the image in various ways? * There may be multiple effective descriptors about one image. There may be multiple perspectives and/or approaches to understanding one image. Do you think the given text addresses these? * You may give low score if the text feels too focused on small parts of the image or views the image in an overly specific (possibly contrived) perspective to lay out the description. 34 Annotation Guidelines for the Sighted General Group (4/4)"
        },
        {
            "title": "Copying and pasting the best sentence for each text",
            "content": "For each text, drag (highlight) the sentence that has best contributed to each text overall. The best sentence does not have to function as one-sentence summary. This should be from the English text, not the Korean translation. Paste the sentence into the text field and press enter to submit. You will see your submitted best sentence on the display. Press the trash can button (Delete) to change your mind and submit different sentence."
        },
        {
            "title": "Caution",
            "content": "Text length should not be criterion for your assessment. Best sentence should be copied from the English paragraph. Evaluation Guidelines for the Sighted Educator Group (1/2) Thank you again for joining our experiments. Attached is spreadsheet containing 40 images, each of which has two descriptions, produced by various AI models in response to our request to generate context for the input diagram. As annotators, you are tasked with evaluating the quality of these texts. Selecting the Preferred Text Have look at the diagram in the Image column, along with the two contexts written in the Context1 and Context2 columns. In the Preferred Text column, enter your choice as 1 or 2. This preference may rely on your own personal criteria. 35 Evaluation Guidelines for the Sighted Educator Group (2/2) Quantitative Assessment Give score ranging from 1 to 5 for each quality listed below. For each statement, enter your assessment on scale from 5 if you Strongly Agree to 1 if you Strongly Disagree. Factuality: The text delivers only facts that are grounded on the diagram. Even if piece of knowledge in the text is factual, you may give low score if that knowledge cannot be inferred from the diagram. Wrong textual descriptions of the diagram content also have low merit. Informativeness: The text describes all of the diagram, holistically. You may give low score to context leaving out parts of the diagram. Succinctness: The text is concise and to the point. Please assess whether the context conveys an appropriate density of information. You may give low score if context seems repetitive. Please avoid scoring based on apparent text length. Diversity: The text captures variety of perspectives from the diagram and employs multiple effective ways of getting the diagram message across. There may be multiple different ways to understand diagram. Please assess whether these ways have been put together in the given context. Usefulness: The text is helpful to BLV. As an experienced educator for learners with visual impairments, please evaluate how useful and helpful the text would be. In the Reason column, please justify your preference choice (i.e., the Qualitative Assessment 1 or 2 selection) with brief explanation. Simple comments, as long as they tell us the textual quality your choice was based on, may still prove helpful for our research. See examples below. contains various descriptions of ants written more logically Context 1 contains more realistic definition of food web. 2 is more concise. Context 1 lacks description of the artery. While both texts faithfully address the rotation and revolution movements of the Earth, Context 1 describes in-depth how they manifest as different natural phenomena on the planet."
        },
        {
            "title": "Evaluation Guidelines for the BLV Educator Group",
            "content": "Thank you again for joining our experiments. Attached is spreadsheet containing 80 images, each with description that has been produced by various AI models in response to our request to generate context for the input diagram. The spreadsheet comes with 81 rows and 8 columns. The table headers are as follows: Image, Context, Succinctness, Diversity, Usefulness (Summary), Usefulness (Multiple-choice Questions), Usefulness (Open-ended Questions), and Nature of Context. Apart from the header row, the remaining 80 rows each contain 1 image and 1 description. As annotators, you are tasked with evaluating these texts, based on the text alone. Quantitative Assessment Give score ranging from 1 to 5 for each quality listed below. For each statement, on scale from 5 if you Strongly Agree to 1 if you Strongly Disagree. Succinctness: The text is concise and to the point. Please assess whether the context conveys an appropriate density of information. You may give low score if context seems repetitive. Please avoid scoring based on apparent text length. Diversity: The text captures variety of perspectives from the diagram and employs multiple effective ways of getting the diagram message across. There may be multiple different ways to understand diagram. Please assess whether these ways have been put together in the given context. Usefulness (Summary): The text serves as good summary. Please assess how well the text helps you formulate an idea of the diagram content. Usefulness (Multiple-choice Questions): The text would be useful in solving short-answer, multiple-choice questions based on the diagram. Suppose you are to solve short-answer multiple-choice questions that have been constructed from the diagram. How well would the context help you answer these questions? Usefulness (Open-ended Questions): The text would be useful in solving descriptive, essay type questions based on the diagram. Suppose you are to answer an open-ended long-answer question that has been constructed from the diagram. How well would the context help you answer such question? Nature of Context: The text is rich in interpretive detail. On scale of 1 to 5, if the text appears to lay out plain and straightforward facts from the diagram, give score of 1. If it rather contains interpretive descriptions, and/or reasoned explanations, give score of 5."
        }
    ],
    "affiliations": [
        "KAIST AI",
        "Sungkyunkwan University",
        "Yonsei University"
    ]
}