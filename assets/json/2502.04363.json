{
    "paper_title": "On-device Sora: Enabling Diffusion-Based Text-to-Video Generation for Mobile Devices",
    "authors": [
        "Bosung Kim",
        "Kyuhwan Lee",
        "Isu Jeong",
        "Jungmin Cheon",
        "Yeojin Lee",
        "Seulki Lee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present On-device Sora, a first pioneering solution for diffusion-based on-device text-to-video generation that operates efficiently on smartphone-grade devices. Building on Open-Sora, On-device Sora applies three novel techniques to address the challenges of diffusion-based text-to-video generation on computation- and memory-limited mobile devices. First, Linear Proportional Leap (LPL) reduces the excessive denoising steps required in video diffusion through an efficient leap-based approach. Second, Temporal Dimension Token Merging (TDTM) minimizes intensive token-processing computation in attention layers by merging consecutive tokens along the temporal dimension. Third, Concurrent Inference with Dynamic Loading (CI-DL) dynamically partitions large models into smaller blocks and loads them into memory for concurrent model inference, effectively addressing the challenges of limited device memory. We implement On-device Sora on the iPhone 15 Pro, and the experimental evaluations demonstrate that it is capable of generating high-quality videos on the device, comparable to those produced by Open-Sora running on high-end GPUs. These results show that On-device Sora enables efficient and high-quality video generation on resource-constrained mobile devices, expanding accessibility, ensuring user privacy, reducing dependence on cloud infrastructure, and lowering associated costs. We envision the proposed On-device Sora as a significant first step toward democratizing state-of-the-art generative technologies, enabling video generation capabilities on commodity mobile and embedded devices. The code implementation is publicly available at an GitHub repository: https://github.com/eai-lab/On-device-Sora."
        },
        {
            "title": "Start",
            "content": "On-device Sora: Enabling Diffusion-Based Text-to-Video Generation for Mobile Devices Kyuhwan Lee Ulsan National Institute of Science and Technology South Korea hanbitchan@unist.ac.kr Bosung Kim Ulsan National Institute of Science and Technology South Korea bosung.k@unist.ac.kr Isu Jeong Ulsan National Institute of Science and Technology South Korea ijeong@unist.ac.kr 5 2 0 2 5 ] . [ 1 3 6 3 4 0 . 2 0 5 2 : r Jungmin Cheon Ulsan National Institute of Science and Technology South Korea jungmin0210@unist.ac.kr Yeojin Lee Ulsan National Institute of Science and Technology South Korea yeojin@unist.ac.kr Seulki Lee Ulsan National Institute of Science and Technology South Korea seulki.lee@unist.ac.kr Abstract We present On-device Sora, first pioneering solution for diffusion-based on-device text-to-video generation that operates efficiently on smartphone-grade devices. Building on Open-Sora, On-device Sora applies three novel techniques to address the challenges of diffusion-based text-to-video generation on computationand memory-limited mobile devices. First, Linear Proportional Leap (LPL) reduces the excessive denoising steps required in video diffusion through an efficient leap-based approach. Second, Temporal Dimension Token Merging (TDTM) minimizes intensive token-processing computation in attention layers by merging consecutive tokens along the temporal dimension. Third, Concurrent Inference with Dynamic Loading (CI-DL) dynamically partitions large models into smaller blocks and loads them into memory for concurrent model inference, effectively addressing the challenges of limited device memory. We implement On-device Sora on the iPhone 15 Pro, and the experimental evaluations demonstrate that it is capable of generating highquality videos on the device, comparable to those produced by Open-Sora running on high-end GPUs. These results show that On-device Sora enables efficient and high-quality video generation on resource-constrained mobile devices, expanding accessibility, ensuring user privacy, reducing dependence on cloud infrastructure, and lowering associated costs. We envision the proposed On-device Sora as significant first step toward democratizing state-of-the-art generative technologies, enabling video generation capabilities on commodity mobile and embedded devices. The code implementation is publicly available at an GitHub repository1. 1https://github.com/eai-lab/On-device-Sora"
        },
        {
            "title": "1 Introduction\nRecent advancements in generative models [62] have signifi-\ncantly expanded capabilities in data generation across vari-\nous modalities, including text [31], image [58], and video [64].\nIn particular, diffusion-based models for image tasks [16, 28,\n50, 51, 54, 56, 60, 77] have emerged as foundational tools for\na wide range of applications, such as image generation [29],\nimage editing [34], and personalized content creation [78].\nFurther extending these technologies, diffusion-based gener-\native models are now driving remarkable progress in video\ngeneration tasks [6, 7, 25, 27, 30, 35, 44, 45, 59, 68, 70], includ-\ning video synthesis [39] and real-time video analysis [48],\nfacilitating the development of advanced generative video\nservices and applications in new and unprecedented ways.\nOne of the most notable video generation models is Sora\n[41], which has demonstrated remarkable potential in creat-\ning high-quality videos from textual descriptions (prompts),\neffectively transforming abstract concepts into visually re-\nalistic content. However, the inherently high complexity of\ndiffusion processes for video generation, combined with the\nsubstantial size of diffusion-based Transformer models [50],\nimposes significant computational and memory demands.\nFor instance, Sora [41] is estimated to take an hour to pro-\nduce five minutes of video using an NVIDIA H100 GPU of 80\nGB RAM. Compared to large language models (LLMs) [79],\nthe costs for diffusion-based video generation models like\nSora are several orders of magnitude higher, indicating their\nresource-intensive nature. As a result, the accessibility of\nvideo generative services remains predominantly confined\nto environments with extensive computational resources,\nlimiting their applicability in resource-constrained devices.\nWith the rapid expansion of mobile, IoT, and embedded\ndevices, there is a growing demand for running generative\napplications directly on the device. In response, numerous",
            "content": "Bosung Kim, Kyuhwan Lee, Isu Jeong, Jungmin Cheon, Yeojin Lee, and Seulki Lee studies have primarily focused on the development of ondevice image generation [10, 13, 14, 38, 66, 80], as image generation is comparatively less resource-intensive than video. However, enabling on-device video generation remains in its nascent stages due to the significantly higher computational and memory resource demands compared to image generation [18]. Moreover, given that video data is typically much larger and more sensitive than image data [15], there is an increasing need for innovative technologies that can enable efficient and effective on-device video generation. We introduce On-device Sora, the first standalone framework for diffusion-based on-device text-to-video generation, which is capable of producing high-quality videos on smartphone-grade devices. On-device Sora leverages OpenSora [82] as its backbone and significantly enhances the efficiency of video generation while maintaining the comparable video quality, enabling on-device text-to-video generation with limited computational power and memory. To the best of our knowledge, On-device Sora proposed in this work is the first solution that enables the efficient generation of Sora-level video directly on the device, which has previously been restricted to large-scale data center environments. On-device Sora addresses three fundamental challenges in enabling diffusion-based on-device text-to-video generation. First, diffusion-based video generation entails substantial number of denoising steps, requiring repeated executions of diffusion models, ranging from dozens to thousands of iterations. To address this, we propose Linear Proportional Leap (LPL), which reduces nearly half of the denoising steps by leaping through large portion of steps using the Eulers method [5] in an estimated direct trajectory to generate videos efficiently. Second, state-of-the-art diffusion models, such as STDiT (Spatial-Temporal Diffusion Transformer) [82], employed by Open-Sora, exhibit high computational complexity due to their substantial amount of token processing [8] in attention modules [67]. To tackle the high computational complexity of STDiT, we propose Temporal Dimension Token Merging (TDTM), which merges consecutive tokens [8, 9] in temporal order at attention layers of STDiT, effectively reducing the token processing up to quarter. Lastly, the substantial size of state-of-the-art models required for text-to-video generation, i.e., T5 [52] and STDiT [82], presents challenges for execution on memoryconstrained mobile devices. To overcome this, we propose Conference Inference and Dynamic Loading (CI-DL), which integrates concurrent model inference with the dynamic loading of models into memory. It dynamically divides the model into blocks and loads them into memory based on the devices available memory capacity at run-time, allowing simultaneous model block loading and inference. With these three proposed methods, high-quality video generation becomes feasible on smartphone-grade devices with limited computing resources, overcoming the requirements for substantial computational power, such as high-end GPUs. Currently, video generative technology remains neither widely accessible nor commonly available to the public [41]. Therefore, enabling video generation on commodity mobile and embedded devices can not only enhance the accessibility of advanced video generation technologies but also provide additional benefits. Users can ensure privacy and mitigate concerns about data transmission and leakage of sensitive personal information. On-device video generation eliminates the need for interaction with the third-party cloud servers, which may pose security risks due to potential data breaches or unauthorized access. Furthermore, cloud-based generative models often suffer from latency issues influenced by network speed and may not function optimally when connectivity is unstable or unavailable. On-device generative models, in contrast, can provide more reliable services, offering stable functionality regardless of network conditions. Another limitation of cloud-based video generation is their shared nature, which limits personalization and the ability to cater to individual users. On-device video generation, however, can be tailored to each user, enabling personalized experiences through customized video creations. Through on-device generation, videos can be fine-tuned and customized for individual users, providing contents that better reflect personal preferences, data, contexts, and environments. From financial perspective, generating videos on-device is far more economical. For example, the cost of an NVIDIA H100 GPU is around $25,000, whereas an iPhone 15 Pro is about only $999, which is 25 times more affordable. Additionally, the annual maintenance costs for cloud-based video generation are estimated to range between $250,000 and $800,000, whereas smartphone incurs negligible maintenance expenses. We implement On-device Sora on the iPhone 15 Pro [2] based on Open-Sora [82], which is an open-source text-tovideo generation framework, enabling standalone text-tovideo generation directly on the device. The full implementation is available as open-source code in publicly accessible anonymous GitHub repository1. The extensive experiments are conducted to evaluate the performance of the proposed On-device Sora using the state-of-the-art video benchmark tool, i.e., VBench [32], compared with Open-Sora running on NVIDIA A6000 GPUs. The experimental results demonstrate that On-device Sora can effectively generate videos of equivalent quality to Open-Sora [82], while accelerating video generation when applying the proposed three methods. While the iPhone 15 Pro has GPU of 143 times less computational power [2] and 16 times smaller memory (RAM) compared to the NVIDIA A6000 GPU, the evaluation results demonstrate that On-device Sora significantly improves the efficiency of video generation by effectively compensating for the limited computing resources of the device. On-device Sora: Enabling Diffusion-Based Text-to-Video Generation for Mobile Devices"
        },
        {
            "title": "2.1 Background: Open-Sora\nFigure 1 illustrates the structure of Open-Sora [82], generat-\ning videos from prompts (texts) through 1) prompt embed-\nding, 2) latent video generation, and 3) video decoding.",
            "content": "Figure 1: Open-Sora [82] generates realistic videos from the user prompt (text) through three stages: 1) prompt embedding, 2) latent video generation, and 3) video decoding. 1) Prompt (Text) Embedding. The first stage of OpenSora [82] is to map user prompt, textual description of the desired video, to an embedding vector, which is used as input for the subsequent video generation stage. To produce prompt embeddings from user texts, Open-Sora employs T5 (Text-to-Text Transformer) [52], language model specifically fine-tuned to support video generation tasks. 2) Latent Video Generation. The next stage is to generate the latent video representation conditioned on the prompt embedding obtained from T5 (Text-to-Text Transformer) [52]. To this end, Open-Sora employs STDiT (Spatial-Temporal Diffusion Transformer) [82], diffusion-based text-to-video model using the Markov chain [76]. Since maintaining temporal consistency across video frames is essential in video generation, STDiT [82] applies the spatial-temporal attention mechanism [72] to the patch representations. It allows effective learning of temporal features across video frames through the temporal attention, enhanced by incorporating rope embeddings [61]. During the forward process of STDiT, the Gaussian noise 𝝐𝑘 is iteratively added to the latent video representation x𝑘 over 𝐾 steps, transforming the intact video representation x0 into the complete Gaussian noise x𝐾 in the (1) latent space with the forward distribution 𝑞(x𝑘 x𝑘 1) as: x𝑘 = 1 𝛽𝑘 x𝑘 1 + 𝛽𝑘 𝝐𝑘 𝑞(x𝑡 x𝑘 1) = (x𝑘 ; 1 𝛽𝑘 x𝑘 1, 𝛽𝑘 I) where 𝛽𝑘 is the parameter determining the extent of noise. To generate (recover) the latent representation x0, the noise 𝝐𝑘 is repeatedly removed (denoised) from the complete Gaussian noise x𝐾 through the reverse process using the estimated noise with the reverse distribution 𝑝𝜽 (x𝑘 1x𝑡 ) modeled by STDiT [82] with the parameter set 𝜽 , as follows: x𝑘 1 = 𝝁𝜽 (x𝑘, 𝑘) + 𝛽𝑘 𝝐 where 𝝐 (0, I) 𝑝𝜽 (x𝑘 1x𝑘 ) = (x𝑘 1; 𝝁𝜽 (x𝑘, 𝑘), 𝚺𝜽 (x𝑘, 𝑘)) (2) where 𝝁𝜽 (x𝑘, 𝑘) and 𝚺𝜽 (x𝑘, 𝑘) is the mean and variance of x𝑘 estimated by the STDiT model training, respectively. This reverse process repeatedly denoises x𝑘 into x𝑘 1 over large number of 1𝑘 𝐾 denoising steps (i.e., from dozens to thousands of steps), eventually generating the de-noised latent video representation x0 close to the intact representation. 3) Video Decoding. Finally, the latent video representation x0 generated from STDiT is decoded and up-scaled into the human-recognizable video through the VAE (Variational Autoencoder) [17]. The VAE employed in Open-Sora utilizes both 2D and 3D structures; the 2D VAE is based on SDXL [51], while the 3D VAE adopts the architecture of Magvit-v2 [74]."
        },
        {
            "title": "2.2 Challenges in On-device Video Generation\nAlthough Open-Sora [82] has enabled the generation of high-\nquality videos from texts, adapting this advanced generative\ncapability to mobile devices presents key challenges.\nC1) Excessive Denoising Steps. Table 1 shows the latency\nof each model component in Open-Sora, where the latent\nvideo generation process (denoising process) performed by\nSTDiT is the most time-consuming. That is because a sub-\nstantial number of denoising steps is required to remove\nthe noise 𝝐𝑘 from x𝐾 to obtain x0 during latent video gen-\neration [60]. Such extensive denoising iterations presents\nconsiderable challenges on mobile devices with constrained\ncomputational capabilities. To reduce the number of denois-\ning steps, Open-Sora employs Rectified Flow [40], which\nredefines the objective of diffusion models, inducing the\nmodel to directly approximate the straight mapping trajec-\ntory between the initial and target distributions. In image\ngeneration, it enables a single-step denoising without a sig-\nnificant loss in image quality. However, in video generation\nmodels [1, 82], the accumulated variance across sequential\nvideo frames prevents the trajectory from being fully repre-\nsented with a single drift. As a result, Open-Sora [82] requires\nat least 𝐾 = 30 or 50 denoising steps to produce high-quality\nvideos. While the numerous denoising steps are manageable\nin server-level environments—where the complete denoising",
            "content": "Bosung Kim, Kyuhwan Lee, Isu Jeong, Jungmin Cheon, Yeojin Lee, and Seulki Lee Figure 2: On-device Sora enables efficient text-to-video generation directly on the device by employing three key approaches: 1) Linear Proportional Leap, 2) Temporal Dimension Token Merging, and 3) Concurrent Inference with Dynamic Loading. Table 1: The number of executions (iterations) of each model component (i.e., T5 [52], STDiT [82], and VAE [17]) in OpenSora [82] and their total latencies on iPhone 15 Pro [2]. Component T5 [52] STDiT [82] VAE [17] Iterations 1 50 1 Inference Time (s) Total Latency (s) 110.505 1768.320 135. 110.505 35.366 135.047 process typically finishes within one minuteon mobile devices, it may take several tens of minutes for full denoising. Accordingly, new approach to reduce denoising steps is essential to enable efficient on-device video generation. C2) Intensive Token Processing. While large number of denoising steps poses significant challenge to video generation on mobile devices, even single denoising step itself is computationally intensive. The primary reason is that the computational complexity of the attention mechanism [47] in STDiT [82] grows quadratically with the token size, which significantly increases the computational load for token processing and, consequently, the models inference latency. To address this challenge, Token merging [9] has been proposed to improve the throughput of vision Transformer models. Token merging progressively merges similar visual tokens within the transformer to accelerate the model inference latency by reducing the size of tokens to be processed. While token merging has been applied to diffusion models, it is only applied to spatial tokens [8, 9] and has not been applied to the temporal tokens in video diffusion models, such as STDiT [82]. Thus, novel token merging method is required to improve the computational efficiency of token processing in video generation, while preserving high video quality. C3) High Memory Requirements. Figure 3 shows the memory requirements of model components executed by Open-Sora [82], i.e., VAE [17], T5 [52], and STDiT [82], where their cumulative memory demand, i.e., 23 GB, can easily surpass the memory capacity of many mobile devices. For Figure 3: The size of Open-Sora models: T5 [52] (18.00 GB), STDiT [82] (4.50 GB), and VAE [17] (0.82 GB), which exceeds the available memory capacity of iPhone 15 Pro [2] (3.3 GB). instance, the iPhone 15 Pro [2], with 8 GB of memory, restricts the available memory for single application to 3.3 GB to ensure the system stability. Furthermore, the individual memory requirements of T5 and STDiT exceed 3.3 GB, creating challenges in loading them into memory. In addition, some memory must be reserved for model execution (inference), complicating the deployment of Open-Sora on mobile devices. Thus, executing large video generative models with limited device memory is another challenge that should be addressed to enable on-device video generation."
        },
        {
            "title": "3 Overview: On-device Sora\nBuilding on Open-Sora [82] outlined above (Sec. 2.1), we\npropose On-device Sora (Figure 2), which enables diffusion-\nbased text-to-video generation on mobile devices by address-\ning the key challenges (Sec. 2.2), as summarized below.\n1) Linear Proportional Leap (Sec. 4). To reduce the exces-\nsive number of denoising steps, we propose Linear Propor-\ntional Leap, which enables the generation of high-quality\nvideos with nearly half of the required full denoising steps.\nThis is achieved by leveraging the linear trajectory proper-\nties of Rectified Flow [40] employed in STDiT [82]. Instead\nof performing the full sequence of denoising steps during\nvideo generation, it makes a direct leap along the linear tra-\njectory toward the target data at the middle of the denoising\nprocess by utilizing the pre-trained flow fields [40]. Thus,",
            "content": "On-device Sora: Enabling Diffusion-Based Text-to-Video Generation for Mobile Devices Linear Proportional Leap effectively reduces the most timeconsuming stage of video generationthe iterative execution of STDiT [82]by minimizing the number of denoising steps. Notably, this reduction in denoising steps is accomplished without necessitating additional model training, architectural modifications, or data calibration, allowing for saving substantial time and computational resources. 2) Temporal Dimension Token Merging (Sec. 5). To lighten the intensive computation required for token processing, we propose Temporal Dimension Token Merging, which merges consecutive video frames in the form of latent representations at each attention layer of STDiT [82]. The proposed token merging reduces the amount of tokens to be processed by half and lowers the computational load of attention modules up to one-quarter. To the best of our knowledge, it is the first to merge tokens in temporal order at attention layers in diffusion-based video generation models. 3) Concurrent Inference with Dynamic Loading (Sec. 6). To execute large video generative models (i.e., T5 [52] and STDiT [82]) with the limited device memory, we propose Concurrent Inference with Dynamic Loading, which partitions the models into smaller blocks that can be loaded into the memory and executed concurrently. By parallelizing model execution and block loading, it effectively accelerates iterative model inference, e.g., multiple denoising steps. Also, it improves memory utilization while minimizing the block loading overhead by retaining specific model blocks in memory dynamically based on the available runtime memory."
        },
        {
            "title": "4.1 Rectified Flow\nThe reverse diffusion process is performed through multiple\ndenoising steps, transforming an initial Gaussian distribution\ninto a desired distribution corresponding to the input prompt.\nSeveral ODE-based methods [43, 81] reformulate this process\nby training neural network models to predict the drift at each\ntime point, building the distribution trajectories from the\ninitial to the target point by using ODE solvers [55, 73].",
            "content": "Rectified Flow [40] simplifies the transition from the initial point to the target point by training the model to predict drift aligned with the direct linear trajectory connecting these two points. Using the Euler method [12], the 𝑘th trajectory 𝒛𝒌 is derived by updating the previous trajectory 𝒛𝒌 1 with the estimated drift 𝒗 (𝑃𝑘, 𝑡𝑘 ) and step size 𝑑𝑡𝑘 defined by two sampled time steps 𝑡𝑘 and 𝑡𝑘+1, as follows: 𝒛𝑘 = 𝒛𝑘 1 + 𝒗 (𝑃𝑘, 𝑡𝑘 )𝑑𝑡𝑘 1 𝑘 𝐾 where 𝑡𝑘 [0, 1] and 𝑑𝑡𝑘 = (cid:40)𝑡𝑘 𝑡𝑘+1, 𝑡𝑘, (3) if 𝑡𝑘 𝑡𝐾 if 𝑡𝑘 = 𝑡𝐾 . Here, the time step 𝑡𝑘 [0, 1] corresponds to the normalized reverse process at the 𝑘th denoising step, with 𝑡𝑘 = 1 representing the time step at which data is fully noisy (start of denoising) and 𝑡𝑘 = 0 corresponding to the time step when data reaches the desired distribution (end of denoising). The drift 𝒗 (𝑃𝑘, 𝑡𝑘 ) is predicted from STDiT [82] given the 𝑘th position on the trajectory, 𝑃𝑘 = 𝑡𝑘𝑃1 + (1 𝑡𝑘 )𝑃0, computed from linear interpolation with sampled time step, 𝑡𝑘 [40]. In Rectified Flow [40], the model is trained to predict the direct direction toward target point at any point on the trajectory. This allows diffusion-based image generation models for achieving denoising process in few steps without significant performance degradation. While the actual trajectory may not be fully captured by single drift, it can be effectively approximated with reduced number of drifts. However, in diffusion-based video generation, achieving single-step generation is challenging due to the additional temporal variance of video data and the increased complexity of the target distribution. With Rectified Flow [40], Open-Sora [82] generates video outputs with 𝐾 = 30 or 50 denoising steps, in contrast to image generation, which produces images in single step."
        },
        {
            "title": "4.2 Linear Proportional Leap\nBuilding upon Rectified Flow [40], Linear Proportional Leap\nreduces denoising steps, as illustrated in Figure 4.",
            "content": "If the 𝑛th data distribution is sufficiently close to the 𝐾th target distribution in the denoising process, the trajectories 𝒛𝑛+1...𝐾 would be approximately straight for the remaining time steps 𝑡𝑛+1...𝐾 , making the estimation of the drift 𝒗 (𝑃𝑘, 𝑡𝑘 )𝑑𝑡𝑘 in Equation (3) unnecessary for 𝑘 > 𝑛. Consequently, 𝒗 (𝑃𝑘, 𝑡𝑘 )𝑑𝑡𝑘 is estimated only for 1 𝑘 𝑛, allowing the denoising process to stop early at the (𝑛 + 1)th step, rather than continuing to the full 𝐾th step. For the remaining time steps 𝑡𝑛+1...𝐾 , the trajectories 𝒛𝑛+1...𝐾 linearly leap towards the target data distribution, with the straight direction of 𝒗 (𝑃𝑛+1, 𝑡𝑛+1) and 𝑑𝑡𝑛+1 scaled proportionally to 𝑡𝑛+1...𝐾 . By assuming 𝑘 = 𝐾, 𝒛𝑘 is derived from Equation (3) as: 𝒛𝑘 = 𝒛𝑘 1 + 𝒗 (𝑃𝑘, 𝑡𝑘 )𝑑𝑡𝑘 𝑘 1 = 𝒛0 + 𝒗 (𝑃𝑖, 𝑡𝑖 )(𝑡𝑖 𝑡𝑖+1) + 𝒗 (𝑃𝑘, 𝑡𝑘 )𝑡𝑘 (4) 𝑖= If the denoising process stops at the step 𝑛, the 𝑛th trajectory can be represented as 𝒛𝑛 = 𝒛0 + (cid:205)𝑛 𝑖=1 𝒗 (𝑃𝑖, 𝑡𝑖 )(𝑡𝑖 𝑡𝑖+1). Bosung Kim, Kyuhwan Lee, Isu Jeong, Jungmin Cheon, Yeojin Lee, and Seulki Lee made proportionally to the remaining steps. Figure 5 visualizes the cosine similarities between consecutive drifts, which stabilizes after certain number of steps, suggesting that the trajectory toward the target data distribution is nearly linear. This enables fewer steps by utilizing the leap with the larger step size to efficiently progress to the desired direction. Figure 4: An abstracted illustration of trajectories and latent visualizations for 𝐾 = 30 and 𝑛 = 15: (a) Rectified Flow [40] with full 𝑘 = 30 denoising steps, generating intact and complete data, (b) Rectified Flow [40] with 𝑛 + 1 = 16 denoising steps without applying Linear Proportional Leap, resulting in low-quality data generation from variance with high step sizes (𝑑𝑡𝑘 ), and (c) Linear Proportional Leap with 𝑛 + 1 = 15 + 1 denoising steps, producing data nearly equivalent to (a). Then, if we apply the identical drift 𝒗 (𝑃𝑛+1, 𝑡𝑛+1)𝑑𝑡𝑛+1 to the remaining 𝑛 + 1 𝑖 𝑘 steps, Equation (4) becomes: 𝒛𝑘 = 𝒛𝑛 + 𝒗 (𝑃𝑛+1, 𝑡𝑛+1) 𝑘 1 𝑖=𝑛+1 (𝑡𝑖 𝑡𝑖+1) + 𝒗 (𝑃𝑛+1, 𝑡𝑛+1)𝑡𝑘 = 𝒛𝑛 + 𝒗 (𝑃𝑛+1, 𝑡𝑛+1)(𝑡𝑛+1 𝑡𝑛+2 + + 𝑡𝑘 1 𝑡𝑘 + 𝑡𝑘 ) = 𝒛𝑛 + 𝒗 (𝑃𝑛+1, 𝑡𝑛+1)𝑡𝑛+1 (5) Thus, the required denoising steps are reduced to 𝑛 + 1 out of the total 𝑘 steps, allowing STDiT to be executed only 𝑛 + 1 times, with the last (𝑛 + 1)th trajectory applied to its time step 𝑡𝑛+1. Replacing 𝑑𝑡𝑘 with 𝑡𝑛+1 in Equation (5), instead of computing difference between the sampled time steps, can invoke an identical effect under assumption that later steps tend to sustain their drift directions. It enables the immediate completion of the denoising process, as 𝑡𝑛+1 is equivalent to the remaining steps required to reach the end of denoising. For instance, when 𝐾 = 30 and 𝑛 = 15, the denoising steps proceed for the initial 𝑛 = 15 steps, and at step 𝑛 = 16, the trajectory transitions by leaping towards the linear direction of the target distribution. Due to the straightened trajectory, whose straightness increases as the denoising process approaches its end, combining the current drift with the product of the remaining step size yields result that closely approximates the outcome achievable through the full denoising process. This enables reduction of the denoising process by half without requiring additional model training. Linear Proportional Leap can be dynamically applied by measuring the cosine similarity between two consecutive drifts 𝒗 at runtime. When the cosine similarity appears that the current trajectory is sufficiently linear, linear leap is Figure 5: An example of cosine similarities between two consecutive drifts estimated from STDiT [82], i.e., 𝒗 (𝑃𝑛, 𝑡𝑛) and 𝒗 (𝑃𝑛1, 𝑡𝑛1) for 30 (red) and 50 steps (blue)."
        },
        {
            "title": "5.1 Token Merging\nSTDiT [82] consists of multiple attention layers, i.e., cross-\nand self-attention, of the linear and quadratic complexity,\nrespectively. In video generation, these attentions extend\nacross two dimensions, i.e., spatial and temporal dimension.\nGeneral model optimization techniques, e.g., pruning [53],\nquantization [24], and distillation [26], may reduce the STDiT’s\ncomputational complexity. However, they often necessitate\nmodel re-training (fine-tuning) or specialized hardware for\nimplementation, and most importantly, the performance of\nvideo generation can hardly be preserved. In contrast, token\nmerging [9] reduces the size of tokens processed in attention\nlayers, decreasing computational complexity without requir-\ning model re-training or hardware-specific adaptations.",
            "content": "In STDiT [82], the attention bias influenced by input tokens in cross-attention layers leads to higher computational demands compared to self-attention layers. As result, developing an effective token merging method for cross-attention is crucial in video generation. However, existing token merging [8, 9, 37] applied to the cross-attention have shown suboptimal performance, and applying them to the self-attention in STDiT has observed video quality drops [8, 9]. On-device Sora: Enabling Diffusion-Based Text-to-Video Generation for Mobile Devices After being processed by each attention, 𝑻 𝑚𝑒𝑟𝑔𝑒𝑑 is unmerged into 𝑻 𝑢𝑛𝑚𝑒𝑟𝑔𝑒𝑑 of the dimension [𝐵, 𝑆𝑇 , 𝐶] as: 𝑻 𝑢𝑛𝑚𝑒𝑟𝑔𝑒𝑑 = 𝑇 𝐷𝑇 𝑀𝑢𝑛𝑚𝑒𝑟𝑔𝑒 (Attention(𝑻 𝑚𝑒𝑟𝑔𝑒𝑑 )) 𝑇 𝐷𝑇 𝑀𝑢𝑛𝑚𝑒𝑟𝑔𝑒 (𝑻 ) [2𝑖] = 𝑻 [:, 𝑖, :] (8) (9) where Attention() is either the selfor cross-attention. Temporal Dimension Token Merging can be selectively applied during the denoising process to minimize potential negative impacts on video quality that may arise from processing merged tokens. Specifically, out of total of 𝐾 denoising steps, tokens can be merged only for the initial 𝑘 steps, while the tokens for the remaining 𝐾𝑘 steps remain unmerged. This is based on the observation that, when tokens are merged along the temporal dimension, the noise values vary slightly across framesa phenomenon not observed in image diffusion [56] that does not involve temporal dimension in the generation process. However, because the noise values in the early denoising steps are less subtle and critical, it is expected that applying token merging exclusively for initial steps does not substantially drop video quality."
        },
        {
            "title": "Loading",
            "content": "We tackle the challenge of limited device memory in text-tovideo generation, restricting the on-device inference of large diffusion-based models, by introducing Concurrent Inference with Dynamic Loading (CI-DL), which partitions models and executes them in concurrent and dynamic manner."
        },
        {
            "title": "6.1 Concurrent Inference\nThe model components of On-device Sora [41], i.e., STDiT [82]\nand T5 [52], easily exceed the available memory of many\nmobile devices, e.g., 3.3 GB RAM of iPhone 15 Pro, as shown\nin Figure 3. Given that the Transformer architecture [69],\nwhich is the backbone for both T5 [52] and STDiT [82], we\npartition these models into smaller blocks (segments) and\nload them into memory accordingly for model inference.",
            "content": "Figure 8: The block loading and inference cycles for (a) sequential loading and inference, and (b) concurrent inference. To execute model inference using the model partitioning, each block must be loaded sequentially before execution, increasing the overall latency of video generation by incurring block loading time. Figure 8-(a) shows the sequential Figure 6: In attention layers of STDiT [82], two consecutive tokens are merged along the temporal dimension and subsequently unmerged after processing, reducing the token size by half and the computational complexity up to quarter."
        },
        {
            "title": "5.2 Temporal Dimension Token Merging\nFigure 6 illustrates Temporal Dimension Token Merging.\nBased on the hypothesis that successive video frames exhibit\nsimilar values, two consecutive frames are merged over the\ntemporal dimension by averaging, creating a single token\nwithout an overhead of calculating frame similarity. This\nreduces the size of tokens by half while preserving the es-\nsential temporal information. Consequently, it decreases the\ncomputation of self-attention by a factor of four, according\nto the self-attention’s quadratic complexity, O (𝑛2). Similarly,\nit reduces the computation of cross-attention by half, based\non the cross-attention’s linear complexity, O (𝑛𝑚). Then, the\noutput token processed through attention modules replicates\nthe dimensions for each frame, restoring them to their origi-\nnal size. Figure 7 depicts the token merging and unmerging.",
            "content": "Figure 7: An illustration of the token merging and unmerging process over the temporal dimension. Given the token 𝑻 𝑖𝑛 as an input for selfor cross-attention layer with the dimension [𝐵, 𝑆𝑇 , 𝐶], where 𝐵 denotes the batch size, 𝑆 is the number of pixel patches, 𝑇 is the number of frames, and 𝐶 is the channel dimension, the input token 𝑻 𝑖𝑛 is merged into 𝑻 𝑚𝑒𝑟𝑔𝑒𝑑 , using the index 𝑖, as: 𝑇 𝐷𝑇 𝑀𝑚𝑒𝑟𝑔𝑒 (𝑻 ) [𝑖] = 𝑻 𝑚𝑒𝑟𝑔𝑒𝑑 = 𝑇 𝐷𝑇 𝑀𝑚𝑒𝑟𝑔𝑒 (𝑻 𝑖𝑛) 1 2 From this, two adjacent tokens are averaged along the temporal dimension, producing the merged token 𝑻 𝑚𝑒𝑟𝑔𝑒𝑑 of [𝐵, 𝑆𝑇 /2, 𝐶], which is processed by the selfor cross-attention. (𝑻 [:, 𝑖, :] + 𝑻 [:, 𝑖 + 1, :]) (7) (6) Bosung Kim, Kyuhwan Lee, Isu Jeong, Jungmin Cheon, Yeojin Lee, and Seulki Lee block load and inference cycle of STDiT [82], where GPU remains idle intermittently, waiting for each block to be loaded into memory, and only begins execution after the loading is complete. This sequential loading and execution process significantly increases the overall latency of model inference. To minimize the increase in model inference latency caused by sequential block loading and execution, we propose Concurrent Inference, which leverages both the CPU and GPU for parallel block loading and execution; CPU loads the (𝑖 + 1)th block, while GPU concurrently executes the 𝑖th block. Initially, the first and second blocks are loaded into memory concurrently, with the first block completing its loading first. Subsequently, the inference of the first block and the loading of the second block occur simultaneously. This process continues such that the inference of the 𝑖th block and the loading of the (𝑖 + 1)th block overlap, ensuring continuous parallelism until the final block. Figure 8-(b) depicts the load and inference cycle of STDiT with Concurrent Inference, which shows that GPU is active without almost no idle time by performing block loading and inference in parallel. Given the number of model blocks 𝑏, block loading latency 𝑙, and inference latency of block 𝑒, the latency reduction 𝑟 achieved through Concurrent Inference is given by: 𝑟 = 𝑏 min(𝑙, 𝑒) 𝛼 (10) where 𝛼 is the overhead caused by the block loading. Given the large number of denoising steps performed by STDiT, which is partitioned into multiple blocks for execution, similar to T5 [52], the number of blocks 𝑏 is expected to be large, leading to significant reduction in latency. It is expected to accelerate the overall model inference effectively regardless of the devices memory capacity. When the available memory is limited, then 𝑏 increases, while with larger memory, both 𝑙 and 𝑒 increase. In either case, it can result in an almost constant latency reduction 𝑟 in Equation (10)."
        },
        {
            "title": "6.2 Dynamic Loading\nTo further enhance the model inference latency, we propose\nDynamic Loading, which is applied in conjunction with Con-\ncurrent Inference. It maintains a subset of model blocks in\nmemory without unloading them, with the selection for the\nsubset of blocks to be retained in memory dynamically deter-\nmined based on the device’s available memory at runtime.\nThe available memory of the device can vary at runtime\nbased on the system status and configurations of applica-\ntions running on the mobile device. By retaining a subset of\nmodel blocks in memory, the overhead of reloading these\nblocks during subsequent steps of Concurrent Inference can\nbe eliminated, enabling reductions in model inference la-\ntency. To achieve this, we measure the device’s run-time\nmemory capacity and the memory required for inferring\na single model block during the initial step of Concurrent",
            "content": "Inference. Next, the memory allocated for retaining certain model blocks is dynamically determined as the difference between the available memory and the memory required for inferring model block. Then, series of model blocks that fit within this allocated memory is loaded in retained state. Figure 10 depicts Dynamic Loading; the first four model blocks are loaded in retrained state. Unlike other blocks, e.g., the 5th block, these blocks are not unloaded to memory after the initial step, reducing block loading overhead. Applying Dynamic Loading, the latency reduction 𝑟 in Equation (10) for Concurrent Inference is updated as: 𝑟 = (𝑏 𝑑) min(𝑙, 𝑒) + 𝑑 𝑙 𝛼 (1 𝑑/𝑏) (11) where 𝑑 is the number of blocks maintained in memory. As the number of model blocks retained in memory increases (i.e., larger 𝑑), the overhead of loading unloading blocks is further minimized, fully accelerating the overall model inference under the devices run-time memory constraints. Dynamically keeping some model blocks in memory with retrained state is particularly advantageous for STDiT [82] that is iteratively executed for denoising steps, which entails loading and reusing the same blocks for each step."
        },
        {
            "title": "8 Experiment\n8.1 Video Generation Performance\nWe evaluate the quality of videos generated on an iPhone 15\nPro [2], in comparison to videos produced by Open-Sora [82]\nrunning on NVIDIA A6000 GPUs. To assess both tempo-\nral and frame-level video quality, we utilize VBench [32],",
            "content": "On-device Sora: Enabling Diffusion-Based Text-to-Video Generation for Mobile Devices Figure 9: Example videos generated by On-device Sora and Open-Sora [82] (68 frames, 256256 resolution). Table 2: The VBench [32] evaluation by category: On-device Sora vs. Open-Sora [82] (68 frames, 256256 resolution)."
        },
        {
            "title": "Vehicles",
            "content": "Open-Sora On-device Sora Open-Sora On-device Sora Open-Sora On-device Sora Open-Sora On-device Sora Open-Sora On-device Sora Open-Sora On-device Sora Open-Sora On-device Sora Open-Sora On-device Sora Subject Consistency 0.97 0.95 0.99 0.98 0.97 0.95 0.96 0.96 0.97 0.96 0.98 0.97 0.98 0.97 0.97 0.94 Temporal Quality Background Consistency 0.98 0.97 0.98 0.98 0.97 0.97 0.97 0.96 0.97 0.97 0.98 0.98 0.98 0.98 0.97 0.96 Temporal Flickering 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.98 Motion Smoothness 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0. Dynamic Degree 0.15 0.28 0.05 0.12 0.26 0.38 0.38 0.43 0.23 0.25 0.15 0.16 0.10 0.17 0.37 0.44 Frame-Wise Quality Imaging Aesthetic Quality Quality 0.56 0.51 0.55 0.48 0.60 0.53 0.56 0.49 0.60 0.52 0.53 0.48 0.57 0.48 0.55 0.48 0.56 0.45 0.53 0.45 0.58 0.50 0.55 0.46 0.50 0.50 0.47 0.48 0.54 0.48 0.49 0.47 Figure 10: The block loading and inference cycle for Dynamic Loading applied with Concurrent Inference. the state-of-the-art benchmark for text-to-video generation, which provides comprehensive evaluation metrics, including subject consistency, background consistency, temporal flickering, motion smoothness, dynamic degree, aesthetic quality, and imaging quality. The evaluation is conducted on 68-frame videos at 256256 resolution, using text prompts provided by VBench [32], consisting of 100 examples each across eight categories: animals, architecture, food, humans, lifestyle, plants, scenery, and vehicles. Table 2 and Figure 11 present the comparison of the generated videos based on those metrics. The results demonstrate that On-device Sora generates videos with quality nearly equivalent to OpenSora in most evaluation metrics, exhibiting only slight drop in frame-level quality, averaging 0.029, while achieving an average improvement of 0.07 in dynamic degree. Figure 9 shows example prompts and videos, compared with Open-Sora [82]. For the prompt \"a stack of dried leaves burning in forest\", both On-device Sora and Open-Sora generate visually plausible videos, both illustrating stack of burning dry leaves and forest in the background. Similarly, for \"close-up of lemur\", both models produce descriptive videos: On-device Sora shows lemur turning its head, while Open-Sora delivers less dynamic yet visually comparable depiction."
        },
        {
            "title": "8.2 Linear Proportional Leap\nTable 3 presents the video generation performance and speed-\nup of On-device Sora when applying Linear Proportional\nLeap (LPL) in Sec. 4. In the table, ‘LPL Setting’ indicates the",
            "content": "Bosung Kim, Kyuhwan Lee, Isu Jeong, Jungmin Cheon, Yeojin Lee, and Seulki Lee Figure 11: visual comparison of videos generated by On-device Sora and Open-Sora [82], evaluated using VBench [32]. Table 3: The video quality and generation speedup under different settings of LPL (Linear Proportional Leap)."
        },
        {
            "title": "LPL Setting",
            "content": "SSIM FVD 16/30 (53%) 21/30 (70%) 23/30 (76%) 25/30 (80%) 30/30 (100%) Dynamic (𝜇:17.73/30) 0.805 0.832 0.840 0.848 - 0.827 527.27 344.40 305.47 276.68 - 370.86 Temporal Quality Subject Consistency 0.97 0.97 0.97 0.97 0.97 0.97 Background Consistency 0.97 0.97 0.97 0.97 0.97 0.97 Temporal Flickering 0.99 0.99 0.99 0.99 0.99 0.99 Motion Smoothness 0.99 0.99 0.99 0.99 0.99 0.99 Dynamic Degree 0.18 0.20 0.21 0.21 0.21 0.20 Frame-Wise Quality Aesthetic Quality 0.50 0.50 0.50 0.50 0.50 0. Imaging Quality 0.55 0.56 0.56 0.56 0.57 0.56 Speedup 1.94 1.49 1.34 1.24 1.00 1.53 Table 4: The video quality and speedup under different merging steps of TDTM (Temporal Dimension Token Merging)."
        },
        {
            "title": "Merging Steps",
            "content": "SSIM FVD 30/30 (100%) 25/30 (83%) 20/30 (66%) 15/30 (50%) 10/30 (33%) 0/30 (0%) 0.595 0.599 0.604 0.612 0.622 - 1225.69 1168.85 1056.47 924.92 784.67 - Subject Consistency 0.97 0.97 0.97 0.97 0.97 0. Temporal Quality Background Consistency 0.97 0.97 0.97 0.97 0.97 0.97 Temporal Flickering 0.99 0.99 0.99 0.99 0.99 0.99 Motion Smoothness 0.99 0.99 0.99 0.99 0.99 0.99 Dynamic Degree 0.06 0.06 0.06 0.12 0.16 0.23 Frame-Wise Quality Imaging Aesthetic Quality Quality 0.56 0.50 0.56 0.50 0.56 0.50 0.57 0.50 0.56 0.50 0.58 0. Speedup 1.27 1.21 1.16 1.13 1.10 1.00 number of denoising steps used for video generation out of total of 30 steps, while the remaining steps are omitted by LPL. We also evaluate dynamic version of LPL, referred to as Dynamic in Table 3, which determines the number of denoising steps at runtime based on the cosine similarities between two adjacent drifts estimated using STDiT [82]. The dynamic LPL stops denoising and makes proportional leap toward the final video when three cosine similarity measurements fail to improve with tolerance of 104, with minimum 15 denoising steps (50%). Overall, LPL enable the generation of videos whose quality, as measured by SSIM [20], FVD [65], and VBench [32], is comparable to that of Open-Sora [82], without noticeable visual degradation (e.g., 0.740 vs. 0.743 in average of VBench [32]), while accelerating video generation up to 1.94. It is shown that LPL can reduce denoising latency linearly without incurring computational overhead. By directly reducing denoising steps, it facilitates efficient video generation while maintaining robust performance. Figure 12 presents example videos generated using LPL, where all LPL settings consistently produce semantically identical target videos, with most video quality remaining stable across various numbers of denoising steps. Figure 12: The snapshots of videos (68 frames, 256256 resolution) applied with various LPL settings  (Table 3)  . On-device Sora: Enabling Diffusion-Based Text-to-Video Generation for Mobile Devices"
        },
        {
            "title": "8.3 Temporal Dimension Token Merging\nTable 4 presents the video quality and video generation\nspeedup achieved when varying numbers of denoising steps\nto which Temporal Dimension Token Merging (TDTM) (Sec. 5)\nis applied, indicated as ‘Merging Steps’ in the table, out of\na total of 30 denoising steps. Each configuration of merg-\ning steps is evaluated on 68-frame videos at a resolution of\n256×256 pixels, using SSIM [20], FVD [65], and VBench [32]\nas evaluation metrics. The results indicate that increasing\nthe number of merging steps consistently accelerates video\ngeneration, ranging from 1.10× to 1.27× speedups, while\nmaintaining stable quality metrics; the average scores for\nSSIM, FVD, and VBench remain at 0.606, 1032.120, and 0.725,\nrespectively. Nonetheless, declines in the dynamic degree\nmetric is observed, revealing a trade-off between maintaining\nvisual dynamics and reducing token processing complexity.\nThis indicates the importance of striking a balance between\nvideo dynamics and speedup. We found that selectively ap-\nplying TDTM to specific denoising steps can effectively re-\nduce visual noises. For instance, limiting TDTM to the first 15\ndenoising steps while not applying it to the rest steps tends\nto result in a less severe quality drop compared to applying\nTDTM to all steps, which can mitigate issues like flickering\nor dynamic degree to provide improved video quality.",
            "content": "Figure 13 shows examples of video frames generated with varying numbers of denoising steps to which TDTM is applied, out of 30 steps. The results demonstrate that even as TDTM is applied to an increasing number of denoising steps, the quality of the video frames seems to remain consistent. Figure 13: The snapshots of videos (68 frames, 256256 resolution) applied with various merging steps of TDTM  (Table 4)  ."
        },
        {
            "title": "Loading",
            "content": "Figure 14-(b) illustrates the model block loading and inference cycles of STDiT [82] when applying the proposed Concurrent Inference (Sec. 6.1), whereas Figure 14-(a) depicts the case without its application. It can be observed that, with Concurrent Inference, the GPU executes each model block for inference without almost no idle time in parallel with the model block loading, indicated by the overlap between the red (loading) and black (inference) boxes in Figure 14- (b), resulting in block inference latency reduction from 29s to 23s. In contrast, without Concurrent Inference, each model block inference is executed only after the corresponding block is fully loaded to memory, indicated by the lack of overlap between the red (loading) and black (inference) boxes in Figure 14-(a). Consequently, the total latency of the full denoising process using multiple executions of STDiT [82] is reduced by approximately 25%, decreasing from 1,000 to 750 seconds for 30 denoising steps. Given that STDiT is executed multiple times to perform numerous denoising steps, it significantly accelerates the STDiTs overall inference. In addition, when applied with Dynamic Loading (Sec. 6.2), it achieves an additional average speed improvement of 17 seconds as reloading is not required for certain model blocks that are retained in memory, provided in Equation (11). Figure 15 shows the case of T5 [52]. Unlike STDiT, which exhibits similar latencies for both block loading and inference execution, T5 exhibits much longer block loading latency relative to inference latency. Consequently, the overlap between the model loading and inference is minimal, as shown by the small region of overlap between the red (loading) and black (inference) boxes. As result, the latency improvement is expected to be less substantial, as in Equation (10). Nevertheless, the inference latency is reduced from 164 to an average of 137 seconds, achieving reduction of 16%. This result implies the effectiveness of concurrent loading and inference, even in cases when there is an imbalance between the latencies of model block loading and inference."
        },
        {
            "title": "8.5 Video Generation Latency\nTable 5 shows video generation latencies of two resolutions\nwhen each of the proposed methodologies—LPL, TDTM, and\nCI-DL—is applied individually, as well as the latency when\nall of them are applied together (‘All’). Latencies are mea-\nsured with LPL activated at the 15th denoising step and\nTDTM applied throughout all steps, reported as the mean\nof three independent experiments. ‘STDiT’ and ‘Total’ spec-\nify whether the latency is measured solely for STDiT [82]\nor for end-to-end video generation, including T5 [52] and\nVAE [17]. The results demonstrate substantial latency reduc-\ntions for each methodology compared to the case without",
            "content": "Bosung Kim, Kyuhwan Lee, Isu Jeong, Jungmin Cheon, Yeojin Lee, and Seulki Lee Neural Engine [2], which delivers peak performance of 35 TOPS. However, the current limitations in Apples software and SDK support for state-of-the-art diffusion-based models [82] make the iPhones NPU challenging to utilize effectively. We look forward to the development of software support for NPUs and leave the exploration of this for future work. Also, we plan to investigate the potential of NPUs on variety of mobile devices, such as Android smartphones. Model Optimization. In On-device Sora, only T5 [52] is quantized to int8, reducing its size from 18 GB to 4.64 GB, while STDiT [82] and VAE [17] are executed with float32 due to their performance susceptibility, which has significant impact on video quality. Additionally, we do not apply pruning [53] or knowledge distillation [23], as these methods also drop visual fidelity. In particular, we observe that naively shrinking STDiT [82] leads to significant visual loss, caused by iterative denoising steps, where errors propagate and accumulate to the final video. Another practical difficulty in achieving lightweight model optimization is the lack of resources required for model optimization; both re-training and fine-tuning state-of-the-art diffusion-based models typically demand several tens of GPUs, and the available datasets are often limited to effectively apply optimization methods. To tackle this challenge, we propose model training-free acceleration techniques for video generation in this work, i.e., Linear Proportional Leap (Sec. 4) and Temporal Dimension Token Merging (Sec. 5). We recognize further model optimization as potential direction for our future work. Image-to-Video Generation. Building on the proposed On-device Sora, it is also feasible to extend its capabilities to image-to-video generation, empowering users to create personalized videos based on their own visual data. Furthermore, On-device Sora could evolve to accept both text and images as multi-modal generative solution, providing an integrated approach to versatile data generation, expected to facilitate the personalization of video generation on mobile devices. We envision that On-device Sora would lay the foundation for future multi-modal visual and textual generation applications on various mobile systems, fostering the ondevice revolution and expansion of generative technologies."
        },
        {
            "title": "10 Related Work\nOn-device Video Generation. Numerous video diffusion\nmodels have advanced the related technologies, including\nMake-A-Video [59], Snap Video [44], Imagen Video [27],\nVideo Diffusion [30], Stable Video Diffusion[6], Dreamix [45],\nMCVD [68], Tune-A-Video [70], Text2Video-Zero [35], among\nothers. Given the substantial resource requirements of video",
            "content": "Figure 14: The block loading and inference cycles of STDiT [82] without (a) and with (b) Concurrent Inference. The red box represents the loading cycle, while the black box indicates the model block inference on the iPhone 15 Pros GPU. Figure 15: The block loading and inference cycles of T5 [52] without (a) and with (b) Concurrent Inference. using the proposed methods, i.e., 293.51 vs. 1768.32 seconds  (Table 1)  for STDiT. For the 192192 resolution video, STDiT (denoising process) takes less than five minutes when all three methodologies are applied. Additionally, it indicates that the methodologies do not interfere with each other, instead work synergistically to enhance latency. Table 5: Ablation study on video generation latency (s). All denotes the combined application of LPL, TDTM, and CI-DL. Resolution Measurement 192 256256 STDiT Total STDiT Total LPL 390.72 514.24 573.88 754.08 TDTM CI-DL 566.81 696.03 691.25 823.03 947.67 965.40 1127.88 1148.63 All 293.51 416.50 454.48 638."
        },
        {
            "title": "9 Discussions and Limitations\nLatency Improvement. Although On-device Sora enables\nefficient video generation, the generation latency remains\nhigher compared to utilizing high-end GPUs; it requires sev-\neral minutes to generate a video, whereas an NVIDIA A6000\nGPU takes one minute. This discrepancy is evident due to\nthe substantial disparity in computational resources between\nthem. For instance, the iPhone 15 Pro’s GPU features up to\n2.15 TFLOPS with 3.3 GB of available memory, compared to\nthe NVIDIA A6000, which offers up to 309 TFLOPS and 48 GB\nof memory. Despite this significant resource gap, On-device\nSora achieves exceptional efficiency in video generation. Cur-\nrently, it utilizes only the iPhone 15 Pro’s GPU. We anticipate\nthat the latency could be significantly enhanced if it can lever-\nage NPU (Neural Processing Unit), e.g., the iPhone 15 Pro’s",
            "content": "On-device Sora: Enabling Diffusion-Based Text-to-Video Generation for Mobile Devices generation, some studies have investigated optimization under various conditions. For example, QVD [63] employs posttraining quantization [42] to enhance computational efficiency and reduce memory usage, while MagicVideo [83] minimizes computations by generating videos in the latent space. CMD [75] decomposes videos into content frames and low-dimensional motion representations to reduce the computational cost of video generation. Similarly, SimDA [71] improves the efficiency of diffusion models by utilizing an effective adapter mechanism. While those studies have sought to enhance the efficiency of video generation, none of them have successfully demonstrated to generate videos on commodity mobile devices, primarily due to high resource demands even after applying their optimization techniques. To the best of our knowledge, the proposed On-device Sora is the first standalone and efficient video generation solution capable of operating on resource-constrained smartphones. On-device Image Generation. There are many works that generate images on the device [4, 10, 33, 38, 66]. However, on-device video generation is struggling due to various challenges. First, video generation requires processing substantially more complex and larger data than image generation [6, 11, 46]. While image generation focuses on single frame, video generation involves handling multiple frames, with the data requirements scaling with the videos frame rate (fps) and duration. Additionally, video generation must maintain temporal consistency across frames [44, 59, 71], i.e., it demands maintaining coherent temporal relationships, including object movement, lighting transitions, and background dynamics between successive frames. Achieving this level of consistency necessitates significantly more computation and memory. Furthermore, these requirements often require complex and large models, exacerbating the challenges when operating on resource-limited mobile devices. Rectified Flow. While Open-Sora [82] reduces the number of denoising steps by leveraging Rectified Flow [40], most related approaches [19, 84] require conditioned model training and/or distillation [84]. In contrast, the proposed Linear Proportional Leap effectively reduces the denoising steps without significant performance drop, as validated using VBench [32], without requiring model re-training or distillation. Notably, it can be easily activated at runtime by just calculating the cosine similarities of drifts between consecutive denoising steps. By developing the current Euler method [12] into more advanced methodologies, it may be possible to further reduce denoising steps while ensuring stability within boundaries. We leave this for future work. Token Merging. Most token merging methods [8, 9, 21] primarily focus on image generation, where tokens are merged based on the spatial similarity rather than temporal similarity. Although some temporal token merging techniques have been proposed, they are applied to models in other domains [22, 37], not in video generation. As such, Temporal Dimension Token Merging is the first to apply token merging based on the successive similarities between frames in video generation. Additionally, while previous works predominantly apply token merging to self-attention due to performance degradation [8, 9, 21, 37], On-device Sora shows that token merging can be applied to cross-attention with minimal performance loss, achieving 50% merging ratio."
        },
        {
            "title": "11 Conclusion\nWe propose On-device Sora, the first pioneering solution for\ngenerating videos on mobile devices using diffusion-based\nmodels, which addresses several key challenges in video gen-\neration to enable efficient on-device operation. The issue\nof extensive denoising steps is addressed through Linear\nProfessional Leap, the challenge of handling large tokens is\nmitigated with Temporal Dimension Token Merging, and\nmemory limitations are overcome through Concurrent In-\nference with Dynamic Loading. These proposed efficient\non-device video generation methodologies are not limited\nto On-device Sora but are broadly applicable to various ap-\nplications, providing significant advancements in on-device\nvideo generation without additional model re-training.",
            "content": "References [1] Hervé Abdi. 2007. Singular value decomposition (SVD) and generalized singular value decomposition. Encyclopedia of measurement and statistics 907, 912 (2007), 44. [2] Apple. 2023. iPhone 15 ProTechnical Specifications. [Online]. Available: https://support.apple.com/en-us/111829. [3] Apple. 2024. Swift. https://developer.apple.com/swift/. [4] Sergei Belousov. 2021. MobileStyleGAN: lightweight convolutional neural network for high-fidelity image synthesis. arXiv preprint arXiv:2104.04767 (2021). [5] BN Biswas, Somnath Chatterjee, SP Mukherjee, and Subhradeep Pal. 2013. discussion on Euler method: review. Electronic Journal of Mathematical Analysis and Applications 1, 2 (2013), 20902792. [6] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Stable video diffusion: Scaling Voleti, Adam Letts, et al. 2023. latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127 (2023). [7] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. 2023. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2256322575. [8] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. 2022. Token merging: Your vit but faster. arXiv preprint arXiv:2210.09461 (2022). [9] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. 2023. Token Merging: Your ViT But Faster. In International Conference on Learning Representations. https://openreview.net/forum?id=JroZRaRw7Eu [10] Thibault Castells, Hyoung-Kyu Song, Tairen Piao, Shinkook Choi, Bo-Kyeong Kim, Hanyoung Yim, Changgwun Lee, Jae Gon Kim, and Bosung Kim, Kyuhwan Lee, Isu Jeong, Jungmin Cheon, Yeojin Lee, and Seulki Lee Tae-Ho Kim. 2024. EdgeFusion: On-Device Text-to-Image Generation. arXiv preprint arXiv:2404.11925 (2024). [11] Baoyang Chen, Wenmin Wang, and Jinzhuo Wang. 2017. Video imagination from single image with transformation generation. In Proceedings of the on Thematic Workshops of ACM Multimedia 2017. 358366. [12] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. 2018. Neural ordinary differential equations. Advances in neural information processing systems 31 (2018). [13] Yu-Hui Chen, Raman Sarokin, Juhyun Lee, Jiuqiang Tang, Chuo-Ling Chang, Andrei Kulik, and Matthias Grundmann. 2023. Speed is all you need: On-device acceleration of large diffusion models via gpu-aware optimizations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 46514655. [14] Jiwoong Choi, Minkyu Kim, Daehyun Ahn, Taesu Kim, Yulhwa Kim, Dongwon Jo, Hyesung Jeon, Jae-Joon Kim, and Hyungjun Kim. 2023. Squeezing large-scale diffusion models for mobile. arXiv preprint arXiv:2307.01193 (2023). [15] Zijun Deng, Xiangteng He, and Yuxin Peng. 2023. Efficiency-optimized video diffusion models. In Proceedings of the 31st ACM International Conference on Multimedia. 72957303. [16] Prafulla Dhariwal and Alexander Nichol. 2021. Diffusion models beat gans on image synthesis. Advances in neural information processing systems 34 (2021), 87808794. [17] Carl Doersch. 2016. Tutorial on variational autoencoders. arXiv preprint arXiv:1606.05908 (2016). [18] Mohamed Elasri, Omar Elharrouss, Somaya Al-Maadeed, and Hamid Tairi. 2022. Image generation: review. Neural Processing Letters 54, 5 (2022), 46094646. [19] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. 2024. Scaling rectified flow transformers for highresolution image synthesis. In Forty-first International Conference on Machine Learning. [20] Qiang Fan, Wang Luo, Yuan Xia, Guozhi Li, and Daojing He. 2019. Metrics and methods of video quality assessment: brief review. Multimedia Tools and Applications 78 (2019), 3101931033. [21] Zhanzhou Feng and Shiliang Zhang. 2023. Efficient vision transformer via token merger. IEEE Transactions on Image Processing (2023). [22] Leon Götz, Marcel Kollovieh, Stephan Günnemann, and Leo Schwinn. 2024. Efficient Time Series Processing for Transformers and StateSpace Models through Token Merging. arXiv preprint arXiv:2405.17951 (2024). [23] Jianping Gou, Baosheng Yu, Stephen Maybank, and Dacheng Tao. 2021. Knowledge distillation: survey. International Journal of Computer Vision 129, 6 (2021), 17891819. [24] Robert M. Gray and David L. Neuhoff. 1998. Quantization. transactions on information theory 44, 6 (1998), 23252383. IEEE [25] William Harvey, Saeid Naderiparizi, Vaden Masrani, Christian Weilbach, and Frank Wood. 2022. Flexible diffusion modeling of long videos. Advances in Neural Information Processing Systems 35 (2022), 2795327965. [26] Geoffrey Hinton. 2015. Distilling the Knowledge in Neural Network. arXiv preprint arXiv:1503.02531 (2015). [27] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. 2022. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303 (2022). [28] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. Advances in neural information processing systems 33 (2020), 68406851. [29] Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim Salimans. 2022. Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research 23, 47 (2022), 133. [30] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. 2022. Video diffusion models. Advances in Neural Information Processing Systems 35 (2022), 8633 8646. [31] Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric Xing. 2017. Toward controlled generation of text. In International conference on machine learning. PMLR, 15871596. [32] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. 2024. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2180721818. [33] Haolin Jia, Qifei Wang, Omer Tov, Yang Zhao, Fei Deng, Lu Wang, Chuo-Ling Chang, Tingbo Hou, and Matthias Grundmann. 2023. BlazeStyleGAN: real-time on-device styleGAN. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 46904694. [34] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. 2023. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 60076017. [35] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. 2023. Text2video-zero: Text-to-image diffusion models are zero-shot video generators. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 1595415964. [36] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, Daniel Haziza, Luca Wehrstedt, Jeremy Reizenstein, and Grigory Sizov. 2022. xFormers: modular and hackable Transformer modelling library. https://github.com/ facebookresearch/xformers. [37] Xirui Li, Chao Ma, Xiaokang Yang, and Ming-Hsuan Yang. 2024. Vidtome: Video token merging for zero-shot video editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 74867495. [38] Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. 2024. Snapfusion: Text-to-image diffusion model on mobile devices within two seconds. Advances in Neural Information Processing Systems 36 (2024). [39] Ming-Yu Liu, Xun Huang, Jiahui Yu, Ting-Chun Wang, and Arun Mallya. 2021. Generative adversarial networks for image and video synthesis: Algorithms and applications. Proc. IEEE 109, 5 (2021), 839 862. [40] Xingchao Liu, Chengyue Gong, and Qiang Liu. 2022. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003 (2022). [41] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, et al. 2024. Sora: review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177 (2024). [42] Zhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei Ma, and Wen Gao. 2021. Post-training quantization for vision transformer. Advances in Neural Information Processing Systems 34 (2021), 2809228103. [43] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. 2022. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095 (2022). [44] Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Ekaterina Deyneka, Tsai-Shien Chen, Anil Kag, Yuwei Fang, Aleksei Stoliar, Elisa Ricci, Jian Ren, et al. 2024. Snap video: Scaled spatiotemporal On-device Sora: Enabling Diffusion-Based Text-to-Video Generation for Mobile Devices transformers for text-to-video synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 70387048. [45] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and Yedid Hoshen. 2023. Dreamix: Video diffusion models are general video editors. arXiv preprint arXiv:2302.01329 (2023). [46] Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai. 2024. Openvid-1m: large-scale high-quality dataset for text-to-video generation. arXiv preprint arXiv:2407.02371 (2024). [47] Zhaoyang Niu, Guoqiang Zhong, and Hui Yu. 2021. review on the attention mechanism of deep learning. Neurocomputing 452 (2021), 4862. [48] Xavier Orriols. 2004. Generative models for video analysis and 3D range data applications. Universitat Autònoma de Barcelona,. [49] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems 32 (2019). [50] William Peebles and Saining Xie. 2023. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 41954205. [51] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. 2023. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952 (2023). [52] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. 2020. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research 21, 140 (2020), 167. [53] Russell Reed. 1993. Pruning algorithms-a survey. IEEE transactions on Neural Networks 4, 5 (1993), 740747. [54] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1068410695. [55] Carl Runge. 1895. Über die numerische Auflösung von Differentialgleichungen. Math. Ann. 46, 2 (1895), 167178. [56] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. 2022. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 conference proceedings. 110. [57] Özgür Sahin and Özgür Sahin. 2021. Introduction to Apple ML tools. Develop Intelligent iOS Apps with Swift: Understand Texts, Classify Sentiments, and Autodetect Answers in Text Using NLP (2021), 1739. [58] Tamar Rott Shaham, Tali Dekel, and Tomer Michaeli. 2019. Singan: Learning generative model from single natural image. In Proceedings of the IEEE/CVF international conference on computer vision. 45704580. [59] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. 2022. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792 (2022). [60] Jiaming Song, Chenlin Meng, and Stefano Ermon. 2020. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502 (2020). [61] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing 568 (2024), 127063. [62] Masahiro Suzuki and Yutaka Matsuo. 2022. survey of multimodal deep generative models. Advanced Robotics 36, 5-6 (2022), 261278. [63] Shilong Tian, Hong Chen, Chengtao Lv, Yu Liu, Jinyang Guo, Xianglong Liu, Shengxi Li, Hao Yang, and Tao Xie. 2024. QVD: Post-training Quantization for Video Diffusion Models. In Proceedings of the 32nd ACM International Conference on Multimedia. 1057210581. [64] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. 2018. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717 (2018). [65] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphaël Marinier, Marcin Michalski, and Sylvain Gelly. 2019. FVD: new metric for video generation. (2019). [66] Pavan Kumar Anasosalu Vasu, James Gabriel, Jeff Zhu, Oncel Tuzel, and Anurag Ranjan. 2023. Mobileone: An improved one millisecond mobile backbone. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 79077917. [67] Vaswani. 2017. Attention is all you need. Advances in Neural Information Processing Systems (2017). [68] Vikram Voleti, Alexia Jolicoeur-Martineau, and Chris Pal. 2022. Mcvdmasked conditional video diffusion for prediction, generation, and interpolation. Advances in neural information processing systems 35 (2022), 2337123385. [69] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations. 3845. [70] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. 2023. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 76237633. [71] Zhen Xing, Qi Dai, Han Hu, Zuxuan Wu, and Yu-Gang Jiang. 2024. Simda: Simple diffusion adapter for efficient video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 78277839. [72] Chenggang Yan, Yunbin Tu, Xingzheng Wang, Yongbing Zhang, Xinhong Hao, Yongdong Zhang, and Qionghai Dai. 2019. STAT: Spatialtemporal attention mechanism for video captioning. IEEE transactions on multimedia 22, 1 (2019), 229241. [73] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. 2023. Diffusion models: comprehensive survey of methods and applications. Comput. Surveys 56, 4 (2023), 139. [74] Lijun Yu, José Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. 2023. Language Model Beats DiffusionTokenizer is Key to Visual Generation. arXiv preprint arXiv:2310.05737 (2023). [75] Sihyun Yu, Weili Nie, De-An Huang, Boyi Li, Jinwoo Shin, and Anima Anandkumar. 2024. Efficient Video Diffusion Models via ContentFrame Motion-Latent Decomposition. arXiv preprint arXiv:2403.14148 (2024). [76] Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang, and In So Kweon. 2023. Text-to-image diffusion models in generative ai: survey. arXiv preprint arXiv:2303.07909 (2023). [77] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 38363847. [78] Xulu Zhang, Xiao-Yong Wei, Wengyu Zhang, Jinlin Wu, Zhaoxiang Zhang, Zhen Lei, and Qing Li. 2024. Survey on Personalized Content Synthesis with Diffusion Models. arXiv preprint arXiv:2405.05538 (2024). Bosung Kim, Kyuhwan Lee, Isu Jeong, Jungmin Cheon, Yeojin Lee, and Seulki Lee [79] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. survey of large language models. arXiv preprint arXiv:2303.18223 (2023). [80] Yang Zhao, Yanwu Xu, Zhisheng Xiao, and Tingbo Hou. 2023. Mobilediffusion: Subsecond text-to-image generation on mobile devices. arXiv preprint arXiv:2311.16567 (2023). [81] Kaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. 2023. Dpmsolver-v3: Improved diffusion ode solver with empirical model statistics. Advances in Neural Information Processing Systems 36 (2023), 5550255542. [82] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. 2024. OpenSora: Democratizing Efficient Video Production for All. https://github. com/hpcaitech/Open-Sora [83] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. 2022. Magicvideo: Efficient video generation with latent diffusion models. arXiv preprint arXiv:2211.11018 (2022). [84] Yuanzhi Zhu, Xingchao Liu, and Qiang Liu. 2025. SlimFlow: Training Smaller One-Step Diffusion Models with Rectified Flow. In European Conference on Computer Vision. Springer, 342359."
        }
    ],
    "affiliations": [
        "Ulsan National Institute of Science and Technology South Korea"
    ]
}