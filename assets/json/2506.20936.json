{
    "paper_title": "PhysRig: Differentiable Physics-Based Skinning and Rigging Framework for Realistic Articulated Object Modeling",
    "authors": [
        "Hao Zhang",
        "Haolan Xu",
        "Chun Feng",
        "Varun Jampani",
        "Narendra Ahuja"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Skinning and rigging are fundamental components in animation, articulated object reconstruction, motion transfer, and 4D generation. Existing approaches predominantly rely on Linear Blend Skinning (LBS), due to its simplicity and differentiability. However, LBS introduces artifacts such as volume loss and unnatural deformations, and it fails to model elastic materials like soft tissues, fur, and flexible appendages (e.g., elephant trunks, ears, and fatty tissues). In this work, we propose PhysRig: a differentiable physics-based skinning and rigging framework that overcomes these limitations by embedding the rigid skeleton into a volumetric representation (e.g., a tetrahedral mesh), which is simulated as a deformable soft-body structure driven by the animated skeleton. Our method leverages continuum mechanics and discretizes the object as particles embedded in an Eulerian background grid to ensure differentiability with respect to both material properties and skeletal motion. Additionally, we introduce material prototypes, significantly reducing the learning space while maintaining high expressiveness. To evaluate our framework, we construct a comprehensive synthetic dataset using meshes from Objaverse, The Amazing Animals Zoo, and MixaMo, covering diverse object categories and motion patterns. Our method consistently outperforms traditional LBS-based approaches, generating more realistic and physically plausible results. Furthermore, we demonstrate the applicability of our framework in the pose transfer task highlighting its versatility for articulated object modeling."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 6 3 9 0 2 . 6 0 5 2 : r PhysRig: Differentiable Physics-Based Skinning and Rigging Framework for Realistic Articulated Object Modeling Hao Zhang1* Haolan Xu1* Chun Feng1 Varun Jampani2 Narendra Ahuja1 1University of Illinois Urbana Champaign jamesdemon923@gmail.com haoz19@illinois.edu 2Stability AI chunf2@illinois.edu varunjampani@gmail.com n-ahuja@illinois.edu Figure 1. PhysRig is differentiable physics-based skinning approach that models objects as soft-body volumes driven by embedded driving points, enabling realistic deformations and capturing complex dynamics across diverse topologies and motionsfrom humanoids to dinosaurs and flying creatures."
        },
        {
            "title": "Abstract",
            "content": "Skinning and rigging are fundamental components in animation, articulated object reconstruction, motion transfer, and 4D generation. Existing approaches predominantly rely on Linear Blend Skinning (LBS), due to its simplicity and differentiability. However, LBS introduces artifacts such as volume loss and unnatural deformations, and it fails to model elastic materials like soft tissues, fur, and flexible appendages (e.g., elephant trunks, ears, and fatty tissues). In this work, we propose PhysRig: differentiable physics-based skinning and rigging framework that overcomes these limitations by embedding the rigid skeleton into volumetric representation (e.g., tetrahedral mesh), which is simulated as deformable soft-body structure driven by the animated skeleton. Our method leverages continuum mechanics and discretizes the object as particles embedded in an Eulerian background grid to ensure differentiability with respect to both material properties and skeletal motion. Additionally, we introduce material prototypes, significantly reducing the learning space while maintaining high expressiveness. To evaluate our framework, we construct comprehensive syn- *Equal contribution thetic dataset using meshes from Objaverse [4], The Amazing Animals Zoo [31], and MixaMo [1], covering diverse object categories and motion patterns. Our method consistently outperforms traditional LBS-based approaches, generating more realistic and physically plausible results. Furthermore, we demonstrate the applicability of our framework in the pose transfer task highlighting its versatility for articulated object modeling. 1. Introduction Skinning and rigging are essential for animating articulated objects and play critical role in numerous applications, including character animation, motion retargeting, 4D reconstruction, and generative modeling. Among existing approaches, Linear Blend Skinning (LBS) remains the dominant method due to its efficiency and differentiability. However, LBS suffers from severe limitations, including unnatural distortions (e.g., collapsing joints, candy-wrapper artifacts, and volume shrinkage) and an inability to capture the behavior of elastic materials. These artifacts become especially problematic when modeling characters with highly deformable regions, such as an elephants trunk, humans soft tissue, or flexible appendages. To address these shortcomings, we introduce differentiable physics-based skinning and rigging framework that models articulated object deformation as volumetric simulation problem. Instead of directly mapping vertices to rigid skeleton transformations, we embed the skeleton into deformable soft-body volume (e.g., bounded by set of Gaussians and tetrahedral meshes), which is driven by skeletal motion while respecting fundamental physical principles. In particular, we leverage continuum mechanics and the material point method to establish fully differentiable deformation process, ensuring that both the material properties and skeletal motion are incorporated in physically consistent manner. Unlike LBS, which applies simple linear blending, our approach captures intricate material behaviors by modeling stress-strain relationships and dynamic responses to skeletal forces, allowing us to achieve more realistic and physics-driven deformations. major challenge encountered with these physics-based methods is large number of material parameters and complex particle interactions, which makes optimization challenging. To overcome this, we introduce material prototypes, vocabulary of primitives that can be combined to represent all material properties, and span common deformation behaviors of articulated objects. This novel approach significantly reduces the learning space while maintaining expressiveness. It provides structured way to interpolate material properties across different object types, enabling more efficient learning while preserving the diversity of real-world material responses. Evaluating physics-based skinning models is challenging due to the lack of suitable benchmark datasets. Existing datasets are primarily built via LBS-based deformations and lack sufficient variation in material properties and deformation types. To address this gap, we construct comprehensive synthetic dataset incorporating meshes from Objaverse [4], The Amazing Animals Zoo [31], and MixaMo [1], covering diverse range of objects, motion patterns, and material properties. Using this dataset, we demonstrate that our method outperforms LBS-based approaches, producing more realistic deformations across variety of articulated objects. Additionally, we showcase the effectiveness of our framework in downstream tasks such as pose transfer and 4D object generation, illustrating its broad applicability. Our key contributions can be summarized as follows: differentiable physics-based skinning/rigging framework, leveraging continuum mechanics to enable realistic and physically plausible deformations while remaining differentiable. novel material prototype formulation, which reduces the learning complexity by introducing structured interpolation approach while maintaining high material expressiveness. novel synthetic dataset for evaluating physics-based skinning models, demonstrating our frameworks superiority over LBS-based approaches. Our approach bridges the gap between physics-based simulation and differentiable learning, providing powerful tool for articulated object modeling in computer vision and graphics. By introducing differentiable physics-driven deformation process, our framework enables new opportunities for more accurate, physically consistent skinning and rigging, with broad implications for animation, motion generation, and 4D modeling. 2. Related Work Skinning in 4D Modeling and Animation. Skinning is fundamental to 3D character animation, modeling surface deformations induced by skeletal motion [23, 25]. Among various techniques, Linear Blend Skinning (LBS) remains the most widely used due to its simplicity and computational efficiency [18]. LBS is integral to many vision tasks, including video-to3D reconstruction and avatar modeling. Parametric models like SCAPE [2] and SMPL [22] rely on predefined skeletons and skinning weights, limiting their adaptability. Neural implicit approaches [7, 17, 26, 3840, 4345] improve generalization but still require precise skeletal information. In avatar modeling, explicit methods [11, 34, 35, 41] optimize SMPL parameters, whereas implicit ones [8, 16, 27, 28, 30, 32, 36] leverage neural representations but face challenges in optimization and topological consistency. LBS has also been applied to pose transfer [19, 29], with MagicPose4D [42] enabling cross-species motion. However, these methods often require recalculating skeletons and skinning weights for novel motions. Since LBS linearly blends external skeletal motion, it fails to capture true internal deformations, prompting research into physically-based skinning [6, 14, 15, 24]. While such methods better model volumetric changes, their non-differentiability limits integration with deep learning. Our approach introduces differentiable physics-based skinning model, enabling efficient joint optimization via gradient descent. Physical 4D Generation. In multiphysics simulation, the Material Point Method (MPM) [9, 12, 13] excels in handling topology changes and frictional interactions across various materials. Recent works [5, 20, 33, 47] integrate MPM for physically plausible motion but rely on manual parameter tuning. Differentiable approaches [10, 21, 46] learn material properties but are restricted to simple motions. To bridge this gap, we propose PhysRig, differentiable physics framework that learns material parameters for articulated objects, ensuring physical consistency across complex motions. Figure 2. Overview of PhysRig. Given 3D object, we first compute coarse skinning weights, which initialize embedded driving points for local deformation control. These points, assigned velocities, are linked to an elastic 3D volume with material parameters governing deformation. The differentiable physics-based skinning module generates natural deformations, optimizing velocities and material properties via backward propagation. Finally, multi-view animations illustrate physically plausible shape deformations over time. 3. Method outer surface. PhysRig can be formulated as: In this paper, we introduce PhysRig, differentiable physicsbased skinning framework for 3D object deformation, applicable to meshes, point clouds, and Gaussian representations. If the input is mesh or Gaussian representation, we first perform filling operation to obtain solid volume in the form of point cloud, and the total number of points is . As shown in Fig. 2, unlike traditional Linear Blend Skinning (LBS), which applies weighted sum of bone transformations, PhysRig employs differentiable physics simulator (Sec.3.1) to model the 3D object as volumetric structure. Instead of directly manipulating vertex positions, it embeds driving points within the volume to induce deformation. PhysRig optimizes two key components to achieve fine-grained control and produce the desired deformation: Material properties, including Youngs modulus RP and Poissons ratio ν RP for material prototypes. The material properties of all points are then computed using function based on the Mahalanobis distance between each point and the material prototypes (Sec. 3.2). These properties govern elasticity and deformation behavior, determining how internal forces propagate through the structure. Driving point velocities, R{lM,3}, representing the motion of the internal skeletal structure parameterized by transformations {t0, ..., tM }, ti SE(3), where is the number of virtual joints. These velocities drive the deformation, with = 8 set by default and the driving points positions are initialized by coarse skinning weights or uniform sampling (detailed in Sec. 3.3). The driving points encode the skeletal motion, propagating movement to the surrounding 3D volume, while the material properties define how internal motion influences the objects = F(X, E, ν, v, t), (1) where RN 3 denotes the initial point positions, and is the time step governing temporal evolution. The function computes the deformed positions via differentiable physics simulation. 3.1. Physics-Based Simulation To model object deformations under external interactions, we simulate motion using the principles of continuum mechanics. Our approach represents objects as continuous volumetric materials governed by conservation laws, enabling differentiable physics-based deformation modeling. 3.1.1. Continuum Mechanics Formulation We describe the motion of deformable object using timedependent mapping function ϕ, which transforms material coordinates in the undeformed space Ω0 to world coordinates in the deformed space Ωt: = ϕ(X, t). The evolution of ϕ is constrained by fundamental physical laws: Conservation of Mass. The total mass within material region remains constant over time: (cid:90) Bt ϵ ρ(x, t) dx = (cid:90) B0 ϵ ρ(ϕ1(x, t), 0) dx, (2) where ρ(x, t) is the density field. Conservation of Momentum. The motion of the object is dictated by the balance of internal and external forces: (cid:90) Bt ϵ ρ(x, t)a(x, t) dx = (cid:90) Bt ϵ σ dx + (cid:90) Bt ϵ ext dx, (3) where a(x, t) = 2ϕ t2 represents acceleration, ext denotes external forces, and σ is the Cauchy stress tensor, which encodes local deformation behavior. Grid-to-Particle (G2P) Update. After computing velocity updates on the grid, the velocities are interpolated back to particles, and positions are updated: 3.1.2. Material Model and Deformation Representation To model elastic responses, we use constitutive model relating the stress tensor σ to the deformation gradient = ϕ . We adopt Fixed Corotated hyperelastic material model, which effectively captures nonlinear deformations while maintaining stability."
        },
        {
            "title": "The Cauchy stress tensor is derived from the strain energy",
            "content": "density function ψ(F ): vt+1 = (cid:88) (xi xp)vi, = xp + tvt+1 xt+ . (8) Deformation Gradient Update. The velocity gradient and deformation gradient are updated as: vt+ = 4 x2 (cid:88) (xi xp)vi(xi xp)T , (9) σ = 1 det(F ) ψ T . (4) t+ = (I + (cid:88) viN (xi xp)T )Fp. Following the Fixed Corotated model, the strain energy function is given by: ψ(F ) = µ (cid:88) i=1 (σi 1)2 + λ 2 (det(F ) 1)2, (5) where σi are the singular values of , and the material parameters µ and λ are related to Youngs modulus and Poissons ratio ν: µ = 2(1 + ν) , λ = Eν (1 + ν)(1 2ν) . (6) 3.1.3. Simulation via the Material Point Method We employ the Material Point Method (MPM) [9] to solve the governing equations efficiently. MPM discretizes the object as particles embedded in an Eulerian background grid, enabling robust handling of large deformations while ensuring differentiability. Particle-to-Grid (P2G) Transfer. At each simulation step, per-particle mass and momentum are transferred to the grid using B-spline interpolation: mivi = (cid:88) (cid:20) mpvp + (xi xp) (cid:16) mpCp 4 x2t Vp (cid:17) ψ (cid:21) (xi xp) + fi. (7) where: mi, vi are the mass and velocity at grid node i, (xi xp) is the interpolation kernel, Cp is the velocity gradient at the particle, fi is the external force, Vp is the volume of the particle, which scales the contribution of the T stress force term. The stress-based force term, Vp , represents the internal elastic forces exerted by the particle. The factor Vp ensures that the contribution is properly scaled according to the physical size of the particle, preventing instabilities when transferring forces to the grid. ψ where: vt+1 is the velocity gradient at particle p, describing how velocity varies locally. t+1 is the updated deformation gradient, tracking material deformation over time. (xi xp) is the spatial gradient of the interpolation function, describing how interpolation weights change with position. is the identity matrix, ensuring that the deformation gradient starts from an undeformed state. is the time step, controlling how much deformation accumulates per iteration. By iterating these updates, MPM efficiently captures complex material deformations while maintaining differentiability. Driving Points Gradient Update. Driving points influence the motion of specific object regions by modifying the velocities of nearby grid nodes within their control region. The velocity update for driving point vd,j is determined by the contributions from the affected grid nodes and is given by: vd,j vd,j + 1 Rc (cid:88) iRc vi, (10) where vi represents the velocity gradient at grid node within the control region Rc. 3.1.4. Optimization Strategy for Inverse Skinning Inverse Skinning is the process of recovering underlying motion parameters, such as material properties and driving point velocities, from observed deformations of 3D object. Unlike traditional skinning methods (LBS), where deformations are computed from transformations and skinning weights, our inverse skinning aims to estimate the driving point velocities and material properties (Youngs modulus E, Poissons ratio ν) that best explain given motion sequence. This requires optimizing physical parameters to minimize discrepancies between simulated and observed motion. Iteritively Optimization. To ensure stability, we adopt an iterative training strategy. First, we initialize the positions of the driving points and estimate their approximate velocities for each frame. We then alternate between the following Method Michelle Ortiz Humanoid Character Mutant Jellyman UR CD UR CD UR CD UR CD Kaya Mammoth UR CD UR CD UR CD UR CD UR CD Leopard Stego Krin Quadruped Animal LBS-1 LBS-2 LBS-3 Ours-init Ours Method LBS-1 LBS-2 LBS-3 Ours-init Ours 2.82 3.06 3.32 3.19 4.7 1.426 0.891 0.372 2.105 0.139 2.18 2.93 3.01 2.91 4.43 2.993 2.011 1.472 12.755 1.375 2.02 3.37 3.47 2.98 4. 2.512 1.438 0.801 5.977 0.527 3.05 3.25 3.66 3.43 4.34 6.532 4.130 3.811 14.161 1.214 2.42 2.97 3.43 3.39 4.8 2.081 1.214 0.408 2.027 0.228 3.03 3.13 3.21 3.43 4. 2.413 1.328 0.493 1.822 0.212 2.54 2.8 3.25 3.13 4.59 1.782 0.891 0.346 4.991 0.127 3.07 3.64 3.83 3.51 4.5 0.682 0.202 0.103 1.664 0.085 3.03 3.38 3.47 3.63 4. 0.561 0.271 0.048 0.844 0.032 Quadruped Animal Cow Other Entities Angelfish Whale UR CD UR CD UR CD UR CD UR CD UR CD UR CD UR CD UR CD Pterosaur Raccoon Cobra Shark T-rex Ave. 2.51 3.0 2.99 3.1 4.24 1.351 1.019 0.781 2.244 0. 2.29 2.58 3.48 3.37 4.63 1.243 0.791 0.342 4.737 0.198 2.88 3.21 3.25 3.17 4.66 5.861 3.763 0.687 6.574 0.588 2.54 2.78 3.05 2.97 4.49 6.722 4.512 1.082 9.522 0. 2.77 2.95 3.17 3.11 4.56 0.292 0.253 0.134 4.561 0.132 3.11 3.57 3.61 3.81 4.32 0.841 0.614 0.209 0.296 0.021 2.64 2.79 3.01 3.77 4.76 2.712 2.133 0.607 4.049 0. 3.27 3.59 3.81 3.93 4.34 0.078 0.046 0.031 0.178 0.016 2.72 3.12 3.35 3.34 4.52 2.43 1.56 0.73 4.67 0.37 Table 1. Comparison of different rigging methods for inverse skinning. UR : User Study Rate, CD : Chamfer Distance. LBS-1, LBS-2, and LBS-3 correspond to using RigNet [37], Pinocchio [3], and ground truth skinning weights, respectively, as initialization for jointly optimizing skinning weights and bone transformation. PhysRig utilizes Pinocchio to obtain coarse skinning weights for initializing driving points, and then iteratively learns material parameters and driving point velocities. Our dataset consists of 17 diverse objects among humans, quadrupeds, and other entities, totaling 120 motion sequences. We report the average performance across all motions for objects with multiple motions. The User Study setup is provided in the appendix Sec. A.4 two optimization steps: (1) Material Parameter Optimization: Fix the driving point velocities and update the material parameters using all frames as single batch. (2) Driving Point Velocity Optimization: Fix the material parameters and sequentially update the velocities of the driving points for each frame. The optimization progresses frame by frame, moving to the next frame once the loss falls below predefined threshold. These two steps are repeated iteratively until either the overall loss falls below set threshold or the total number of iterations reaches the stopping criterion. This strategy is designed to account for the differing requirements of material parameters and velocity optimization. Optimizing material parameters requires information accumulated across multiple frames, as the material properties influence the objects global behavior over time. In contrast, optimizing driving point velocities must be performed sequentially on per-frame basis. Simultaneously optimizing velocities across multiple frames is ineffective, as accurate simulation of later frames is only meaningful if the preceding frames have already been well-optimized. 3.2. Material Prototype To efficiently represent material properties across an objects volume, we introduce material prototypes, each characterized by two learnable parameters: Youngs modulus and Poissons ratio. The material properties at any point within the volume are computed as weighted sum of these prototypes. The weights are determined using function based on the Mahalanobis distance between the query position and the prototypes. Specifically, we define each material prototype as Gaussian ellipsoid, parameterized by its center RP 3, orientation RP 33, and diagonal scale Λ RP 33, where denotes the number of prototypes. The weight assignment follows: Wn,p = softmaxpP (d(xn, Cp, Qp)) (11) where d(xn, Cp, Qp) is the Mahalanobis distance, defined as: d(xn, Cp, Qp) = (xn Cp)T Qp(xn Cp), Qp = VT ΛpVp. Here, xn represents the coordinates of query point n, and the Mahalanobis distance function ensures that weights are assigned based on the spatial relationship between the query position and the material prototypes. This formulation enables an efficient and differentiable material representation that generalizes across diverse volumetric structures. Compared to directly learning per-point material properties or employing triplane-based function that maps spatial coordinates to material parameters, our material prototype representation offers significantly more compact and efficient parameterization. By leveraging small set of prototypes rather than densely modeling every point, we substantially reduce the optimization space while maintaining high expressiveness. Moreover, the prototype-based formulation naturally enforces smooth material transitions, preventing noisy or abrupt variations that are common in per-point learning approaches. This property aligns more closely with the behavior of real-world materials, where material properties exhibit gradual spatial variations rather than sharp discontinuities. 3.3. Driving Point Initialization Driving points are crucial component of PhysRig, as efficiently initializing their positions and velocities significantly Mat Field Per-point w/o Locating w/o Vel Init Prototypes: 25 Prototypes: 100 Prototypes: 200 Michelle Leopard Angelfish UR 3.31 3.15 4.31 4.08 - 4.7 - CD UR 3.47 1.93 3.61 2.31 4.23 0.186 4.03 0.183 - 0.147 4.45 0.139 0.133 - CD UR 3.93 1.58 3.73 1.77 3.97 0.358 4.17 0.301 - 0.229 4.32 0.212 0.207 - CD 0.23 0.25 0.031 0.029 0.023 0.021 0. Converge Iteration - - 8000 5000 2000 2500 2500 Table 2. Ablation study on material prototypes vs. material field vs. per-point for material representation, the impact of the number of material prototypes, and the effect of driving point initialization, including (i) joint localization and (ii) velocity initialization. Although the coarse skinning weights obtained from preexisting models may not be highly accurate, they provide decent starting point. Our method refines these initial estimates (velocity) during optimization, ultimately yielding more accurate motion parameters that adapt to the specific material properties of the object. improves optimization efficiency. To achieve this, we propose coarse-to-fine initialization strategy based on skinning weights. We first obtain coarse skinning weights using existing rigging models such as Pinocchio [3] or RigNet [37], which provide an approximate mapping between the objects surface and skeletal structure. We then place driving points at joint locations, which naturally reside at the boundaries between adjacent parts. 3.3.1. Affinity-Based Seg via Spectral Clustering Given per-vertex skinning weights RN B, where is the number of vertices and is the number of bones, we construct an affinity matrix to measure similarity between vertices: (cid:18) Ai,j = exp Wi Wj2 σ2 (cid:19) , (12) where σ controls the sensitivity of similarity measurement. larger σ results in smoother clustering, while smaller σ captures finer-scale differences. Using A, we compute the graph Laplacian: = A,where Di,i = (cid:80) Ai,j. We obtain low-dimensional embedding by computing the smallest eigenvectors of and apply k-means clustering to segment the object into rigid regions, each assigned cluster label ci. Note that could be different with B. Since the coarse skinning weights may not always meet our expectations, our approach allows for flexible control over the number of parts by adjusting k. 3.3.2. Locating Joint via Skinning Weight Variance To extract joint locations, we analyze the segmentation output to identify transition regions where adjacent rigid components meet. vertex is classified as boundary vertex if: for some (i), where (i) denotes the set ci = cj, of neighboring vertices in the mesh. These boundary vertices form the primary candidates for joint locations. To further refine the detected joints, we analyze variance in skinning weights at boundary vertices. Specifically, we define the joint set as: = , where WN (i),b is the mean skinning weight of neighboring vertices of i, and τ is threshold for detecting significant weight variations. This step ensures that only regions with meaningful changes in skinning influences are selected as joints. (cid:0)Wi,b WN (i),b (cid:110) (cid:80) > τ (cid:1)2 (cid:111) 3.3.3. Driving Points Initialization At each identified joint, we uniformly place driving points to ensure fine-grained control over the deformation of nearby volumetric regions. Each driving points initial velocity is computed as the average velocity of its surrounding volume, ensuring smooth and physically consistent initializa- (cid:80) vi tion: vp = points influencing the driving point. iNp Np , where Np represents the set of nearby Figure 3. PhysRig enables pose transfer for generated objects. 4. Experiments In this section, we compare PhysRig with the traditional neural Linear Blend Skinning (LBS) method on the inverse skinning task, which serves as fundamental component for various applications such as 3D video reconstruction and part decomposition. This comparison highlights PhysRigs strong capability in dynamic modeling and optimization for articulated objects. To facilitate the evaluation, we introduce new dataset, which is constructed from existing datasets (Objaverse, The Amazing Animals Zoo and Mixamo) and includes entities with diverse structural variations. Additionally, we generate large amount of synthetic data using PhysRig, enabling more comprehensive analysis of its optimization performance, particularly in learning material properties and driving point velocities. For more details on the dataset (Sec. A.1) and implementation (Sec. A.2), more experimental (Sec. A.3) results, and video results please refer Figure 4. Animation results from the PhysRig approach. These results are obtained from the inverse skinning problem by optimizing material properties and driving point velocities to minimize the deviation from the ground truth mesh sequence. to the supplementary materials. 4.1. Inverse Skinning Evaluation We evaluate the effectiveness of our inverse skinning method across diverse set of humanoid characters, quadruped animals, and other articulated entities. We compare against traditional Linear Blend Skinning (LBS) baselines, including RigNet [37], Pinocchio [3], and ground truth skinning weight initialization, as well as the results after driving points initialization before optimization (Ours-init). The evaluation metrics include User Study Rate (UR), which quantifies perceptual quality based on user preferences, with scores ranging from 0 to 5 (higher is better), and Chamfer Distance (CD), which evaluates geometric fidelity. Table 1 presents the results. Our method consistently outperforms all baselines, achieving the highest UR scores and the lowest CD across all evaluated categories. Notably, on humanoid characters, our method achieves UR of 4.7 on Michelle and UR of 4.8 on Kaya, surpassing all LBS-based approaches. Similarly, for quadrupeds, our approach demonstrates superior performance, particularly on the Leopard (UR: 4.45, CD: 0.212) and Stego (UR: 4.5, CD: 0.085), Figure 5. Comparison of the learned material properties with ground truth using our method. highlighting its robustness across diverse morphologies. Our approach also generalizes well to other articulated entities, such as the Angelfish (UR: 4.32, CD: 0.021) and Pterosaur (UR: 4.49, CD: 0.653), showcasing its effectiveness beyond conventional character rigging. These results indicate that our inverse skinning formulation not only improves perceptual quality but also significantly reduces geometric error compared to existing baselines. For qualitative comparisons and analysis, please refer to the appendix (Sec. A.3). 4.2. Ablation Study We conduct an ablation study to analyze the impact of different components in our method, particularly focusing on material representation and driving point initialization. The results are summarized in Table 2. Material Representation: We compare our prototypebased material representation against material fields and perpoint assignments. The per-point approach leads to higher geometric error (CD: 2.31 on Michelle, 1.77 on Leopard), indicating that it struggles to find the optimal solution. The material field (triplane) method also underperforms, demonstrating increased Chamfer Distance across all test cases. In contrast, our prototype-based representation significantly reduces CD and achieves high UR scores. Effect of Driving Point Initialization: Removing joint localization (w/o Locating) results in increased CD values (e.g., 0.186 on Michelle), requiring 8000 iterations for convergence. Similarly, excluding velocity initialization (w/o Vel Init) leads to higher CD (0.183 on Michelle) and slower convergence (5000 iterations). These findings suggest that both joint localization and velocity initialization are crucial for improving optimization efficiency and accuracy. Effect of Material Prototype Count: We also investigate the impact of the number of material prototypes. Reducing the prototype count to 25 does not degrade performance and instead accelerates convergence, achieving the fastest convergence at 2000 iterations while maintaining competitive accuracy (CD: 0.147 on Michelle). Increasing the prototype count to 100 strikes good balance between performance and convergence time (UR: 4.7, CD: 0.139 on Michelle, convergence: 2500 iterations). Further increasing the prototypes to 200 yields marginal improvement in CD (0.133 on Michelle) but does not significantly affect UR, suggesting diminishing returns. Overall, these results demonstrate that our material prototype representation, combined with joint localization and velocity initialization, leads to improved inverse skinning accuracy and faster optimization convergence. 4.3. Apply PhysRig for Pose Transfering As shown in Figure 3, PhysRig enables pose transfer by taking mesh sequence as input. Inspired by MagicPose4D [42], we first extract the skeleton from the input mesh and align it with the generated mesh. By transferring the bone angles at each frame, we obtain the skeleton sequence for the generated object. This allows us to compute joint velocities between consecutive frames, which serve as the driving point velocities for deforming the generated mesh (volume). Unlike traditional methods that rely on skinning weight, PhysRig achieves more realistic deformations while significantly improving generalization, as it eliminates the need for explicit skinning weight prediction. 5. Conclusion We introduced PhysRig, differentiable physics-based skinning framework that addresses the limitations of Linear Blend Skinning (LBS) by modeling deformations through volumetric simulation. By embedding skeletons into softbody representation and leveraging continuum mechanics, our approach achieves realistic, physically plausible deformations while remaining fully differentiable. To enhance efficiency, we introduced material prototypes, reducing learning complexity while maintaining expressiveness. Our evaluation of diverse synthetic dataset demonstrated superior performance over traditional LBS-based methods. Additionally, PhysRig enables applications such as pose transfer, motion retargeting, and 4D generation, bridging the gap between physics-based simulation and differentiable learning. Future work includes integrating real-world priors and optimizing for real-time applications, expanding PhysRigs potential in animation and simulation."
        },
        {
            "title": "References",
            "content": "[1] Adobe. Mixamo. 1, 2 [2] Dragomir Anguelov, Praveen Srinivasan, Daphne Koller, Sebastian Thrun, Jim Rodgers, and James Davis. Scape: shape completion and animation of people. ACM Trans. Graph., 24 (3):408416, 2005. 2 [3] Ilya Baran and Jovan Popovic. Automatic rigging and animation of 3d characters. 26(3):72es, 2007. 5, 6, 7 [4] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: universe of 10m+ 3d objects. Advances in Neural Information Processing Systems, 36:3579935813, 2023. 1, 2 [5] Zhoujie Fu, Jiacheng Wei, Wenhao Shen, Chaoyue Song, Xiaofeng Yang, Fayao Liu, Xulei Yang, and Guosheng Lin. Sync4d: Video guided controllable dynamics for physicsbased 4d generation. arXiv preprint arXiv:2405.16849, 2024. 2 [6] Nico Galoppo, Miguel Otaduy, Serhat Tekin, Markus Gross, and Ming Lin. Soft articulated characters with fast contact In Computer Graphics Forum, pages 243253. handling. Wiley Online Library, 2007. [7] Simon Giebenhain, Tobias Kirschstein, Markos Georgopoulos, Martin Runz, Lourdes Agapito, and Matthias Nießner. Learning neural parametric head models. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2023. 2 [8] Shoukang Hu, Tao Hu, and Ziwei Liu. Gauhuman: Articulated gaussian splatting from monocular human videos. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2041820431, 2024. 2 [9] Yuanming Hu, Yu Fang, Ziheng Ge, Ziyin Qu, Yixin Zhu, Andre Pradhana, and Chenfanfu Jiang. moving least squares material point method with displacement discontinuity and two-way rigid body coupling. ACM Transactions on Graphics (TOG), 37(4):114, 2018. 2, 4 [10] Tianyu Huang, Haoze Zhang, Yihan Zeng, Zhilu Zhang, Hui Li, Wangmeng Zuo, and Rynson WH Lau. Dreamphysics: Learning physical properties of dynamic 3d gaussians with video diffusion priors. arXiv preprint arXiv:2406.01476, 2024. 2 [11] Boyi Jiang, Yang Hong, Hujun Bao, and Juyong Zhang. Selfrecon: Self reconstruction your digital avatar from monocular video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 56055615, 2022. 2 [12] Chenfanfu Jiang, Craig Schroeder, Andrew Selle, Joseph Teran, and Alexey Stomakhin. The affine particle-in-cell method. ACM Transactions on Graphics (TOG), 34(4):110, 2015. 2 [13] Chenfanfu Jiang, Craig Schroeder, Joseph Teran, Alexey Stomakhin, and Andrew Selle. The material point method for simulating continuum materials. In Acm siggraph 2016 courses, pages 152. 2016. [15] Theodore Kim and Doug James. Physics-based character skinning using multi-domain subspace deformations. In Proceedings of the 2011 ACM SIGGRAPH/eurographics symposium on computer animation, pages 6372, 2011. 2 [16] Muhammed Kocabas, Jen-Hao Rick Chang, James Gabriel, Oncel Tuzel, and Anurag Ranjan. Hugs: Human gaussian splats. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 505515, 2024. 2 [17] Jiahui Lei, Yufu Wang, Georgios Pavlakos, Lingjie Liu, and Kostas Daniilidis. Gart: Gaussian articulated template models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1987619887, 2024. 2 [18] J. P. Lewis, Matt Cordner, and Nickson Fong. Pose space deformation: unified approach to shape interpolation and In Proceedings of the 27th skeleton-driven deformation. Annual Conference on Computer Graphics and Interactive Techniques, page 165172, USA, 2000. ACM Press/AddisonWesley Publishing Co. 2 [19] Zhouyingcheng Liao, Jimei Yang, Jun Saito, Gerard PonsMoll, and Yang Zhou. Skeleton-free pose transfer for stylized 3d characters. In European Conference on Computer Vision, pages 640656. Springer, 2022. 2 [20] Jiajing Lin, Zhenzhong Wang, Shu Jiang, Yongjie Hou, and Min Jiang. Phys4dgen: physics-driven framework for controllable and efficient 4d content generation from single image. arXiv preprint arXiv:2411.16800, 2024. 2 [21] Fangfu Liu, Hanyang Wang, Shunyu Yao, Shengjun Zhang, Jie Zhou, and Yueqi Duan. Physics3d: Learning physical properties of 3d gaussians via video diffusion. arXiv preprint arXiv:2406.04338, 2024. 2 [22] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. Smpl: skinned multiperson linear model. ACM Trans. Graph., 34(6), 2015. 2 [23] N. Magnenat-Thalmann and D. Thalmann. Complex models for animating synthetic actors. IEEE Computer Graphics and Applications, 11(5):3244, 1991. [24] Aleka McAdams, Yongning Zhu, Andrew Selle, Mark Empey, Rasmus Tamstorf, Joseph Teran, and Eftychios Sifakis. Efficient elasticity for character skinning with contact and collisions. In ACM SIGGRAPH 2011 papers, pages 112. 2011. 2 [25] Alex Mohr and Michael Gleicher. Building efficient, accurate character skins from examples. ACM Trans. Graph., 22(3): 562568, 2003. 2 [26] Pablo Palafox, Aljaˇz Boˇziˇc, Justus Thies, Matthias Nießner, and Angela Dai. Npms: Neural parametric models for 3d deformable shapes. In Proceedings - 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, pages 1267512685. Institute of Electrical and Electronics Engineers Inc., 2021. Publisher Copyright: 2021 IEEE; 18th IEEE/CVF International Conference on Computer Vision, ICCV 2021 ; Conference date: 11-10-2021 Through 17-102021. 2 [14] Junggon Kim and Nancy Pollard. Fast simulation of skeleton-driven deformable body characters. ACM Transactions on Graphics (TOG), 30(5):119, 2011. 2 [27] Panwang Pan, Zhuo Su, Chenguo Lin, Zhen Fan, Yongjie Zhang, Zeming Li, Tingting Shen, Yadong Mu, and Yebin Liu. Humansplat: Generalizable single-image human gaus- [41] Ilya Zakharkin, Kirill Mazur, Artur Grigorev, and Victor Lempitsky. Point-based modeling of human clothing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1471814727, 2021. [42] Hao Zhang, Di Chang, Fang Li, Mohammad Soleymani, and Narendra Ahuja. Magicpose4d: Crafting articulated models with appearance and motion control. arXiv preprint arXiv:2405.14017, 2024. 2, 8 [43] Hao Zhang, Fang Li, and Narendra Ahuja. Open-nerf: Towards open vocabulary nerf decomposition. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 34563465, 2024. 2 [44] Hao Zhang, Fang Li, Samyak Rawlekar, and Narendra Ahuja. Learning implicit representation for reconstructing articulated objects. arXiv preprint arXiv:2401.08809, 2024. [45] Hao Zhang, Fang Li, Samyak Rawlekar, and Narendra Ahuja. S3o: dual-phase approach for reconstructing dynamic shape and skeleton of articulated objects from single monocular video. In International Conference on Machine Learning, pages 5919159209. PMLR, 2024. 2 [46] Tianyuan Zhang, Hong-Xing Yu, Rundi Wu, Brandon Feng, Changxi Zheng, Noah Snavely, Jiajun Wu, and William Freeman. Physdreamer: Physics-based interaction with 3d objects via video generation. In European Conference on Computer Vision, pages 388406. Springer, 2024. 2 [47] Haoyu Zhao, Hao Wang, Xingyue Zhao, Hongqiu Wang, Zhiyu Wu, Chengjiang Long, and Hua Zou. Automated 3d physical simulation of open-world scene with gaussian splatting, 2024. sian splatting with structure priors. Advances in Neural Information Processing Systems, 37:7438374410, 2024. 2 [28] Zhiyin Qian, Shaofei Wang, Marko Mihajlovic, Andreas Geiger, and Siyu Tang. 3dgs-avatar: Animatable avatars via deformable 3d gaussian splatting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 50205030, 2024. 2 [29] Chaoyue Song, Jiacheng Wei, Ruibo Li, Fayao Liu, and Guosheng Lin. 3d pose transfer with correspondence learning and mesh refinement. Advances in Neural Information Processing Systems, 34:31083120, 2021. 2 [30] Shih-Yang Su, Frank Yu, Michael Zollhofer, and Helge Rhodin. A-nerf: Articulated neural radiance fields for learning human shape, appearance, and pose. Advances in neural information processing systems, 34:1227812291, 2021. 2 [31] Truebones Motions Animation Studios. The Amazing Animals Zoo Dataset. 1, 2 [32] Chung-Yi Weng, Brian Curless, Pratul Srinivasan, Jonathan Barron, and Ira Kemelmacher-Shlizerman. Humannerf: Free-viewpoint rendering of moving people from In Proceedings of the IEEE/CVF conmonocular video. ference on computer vision and pattern Recognition, pages 1621016220, 2022. [33] Tianyi Xie, Zeshun Zong, Yuxing Qiu, Xuan Li, Yutao Feng, Yin Yang, and Chenfanfu Jiang. Physgaussian: PhysicsIn Prointegrated 3d gaussians for generative dynamics. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 43894398, 2024. 2 [34] Yuliang Xiu, Jinlong Yang, Xu Cao, Dimitrios Tzionas, and Michael Black. Econ: Explicit clothed humans optimized In Proceedings of the IEEE/CVF via normal integration. conference on computer vision and pattern recognition, pages 512523, 2023. 2 [35] Hongyi Xu, Eduard Gabriel Bazavan, Andrei Zanfir, William Freeman, Rahul Sukthankar, and Cristian Sminchisescu. Ghum & ghuml: Generative 3d human shape and articulated pose models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 61846193, 2020. 2 [36] Hongyi Xu, Thiemo Alldieck, and Cristian Sminchisescu. H-nerf: Neural radiance fields for rendering and temporal reconstruction of humans in motion. Advances in Neural Information Processing Systems, 34:1495514966, 2021. 2 [37] Zhan Xu, Yang Zhou, Evangelos Kalogerakis, Chris Landreth, and Karan Singh. Rignet: Neural rigging for articulated characters. arXiv preprint arXiv:2005.00559, 2020. 5, 6, 7 [38] Gengshan Yang, Deqing Sun, Varun Jampani, Daniel Vlasic, Forrester Cole, Huiwen Chang, Deva Ramanan, William Freeman, and Ce Liu. Lasr: Learning articulated shape reconstruction from monocular video. In CVPR, 2021. 2 [39] Gengshan Yang, Deqing Sun, Varun Jampani, Daniel Vlasic, Forrester Cole, Ce Liu, and Deva Ramanan. Viser: Videospecific surface embeddings for articulated 3d shape reconstruction. In NeurIPS, 2021. [40] Gengshan Yang, Minh Vo, Natalia Neverova, Deva Ramanan, Andrea Vedaldi, and Hanbyul Joo. Banmo: Building animatable 3d neural models from many casual videos. In CVPR, 2022. 2 A. Appendix Section A.2. Implementation Details Due to page limitations in the main text, we have included additional supplementary materials in the appendix. This section primarily provides the following: 1. comprehensive introduction to our newly proposed dataset (Sec. A.1). 2. detailed description of the implementation (Sec. A.2). 3. qualitative comparison and analysis of inverse skinning using neural blend skinning weights (Sec. A.3). 4. Setup for User Study (Sec. A.4). 5. Visualization of Material Prototype Centers  (Fig.7)  . A.1. Dataset To evaluate PhysRig and compare its performance against traditional Linear Blend Skinning (LBS) methods, we construct diverse simulation dataset tailored for the inverse skinning task. This dataset enables comprehensive analysis of PhysRigs ability to recover underlying motion parameters and material properties under various challenging conditions. We curate 17 structurally distinct objects from Objaverse, Mixamo, and The Amazing Animals Zoo, ensuring broad coverage of different articulation types and deformation patterns. These objects are categorized into three groups: (i) Humanoid Characters (5 objects), (ii) Quadruped Animals (6 objects): leopard, mammoth, stego, krin, cow, and raccoon, and (iii) Other Entities (6 objects): t-rex, pterosaur, whale, angelfish, cobra, and shark Each object is associated with 1 to 4 motion sequences, resulting in total of 40 motion sequences, with each sequence containing 20 to 100 frames. This setup ensures diverse range of temporal dynamics, allowing us to evaluate PhysRigs generalization across various topologies and articulation mechanisms. To further assess PhysRigs ability to learn material properties, we provide two different material configurations for each of the 40 motion sequences, resulting in total of 80 cases: (i) Homogeneous-material objects, where the entire structure exhibits uniform material property. (ii) Heterogeneous-material objects, where different regions are assigned distinct material properties, simulating realistic soft-tissue variations and composite structures. In total, our dataset consists of 120 cases, including 40 cases with original motion sequences and 80 cases with different material configurations. By systematically introducing controlled material variations, our dataset enables fine-grained evaluation of PhysRigs capability to recover Youngs modulus, Poissons ratio, and skeletal motion parameters across diverse material configurations. Given that the objects in our dataset primarily consist of animals and humans, which typically exhibit similar Poissons ratios across different tissues, we assume homogeneous Poissons ratio for all objects. This dataset serves as quantitative benchmark, evaluating both motion accuracy and material property estimation. In our experiments, the mesh object consists of approximately 2000 to 50000 vertices. We discretize the simulation field into 1003 grid for simulation. To reduce computational complexity, the number of material prototypes is set to be 25200, which are uniformly distributed within the volume. For accurate motion modeling, we employ 100 substeps between successive frames (25 FPS), corresponding to duration of: = 4 104 seconds per sub-step. We adopt an alternating optimization strategy to separately optimize the material properties and velocity. Specifically, the material parameters are optimized using the AdamW optimizer, while the velocity parameters are optimized using SGD. To achieve efficient convergence, the initial learning rate for material training is set to be 20 times that of velocity training. The initial learning rate for velocity optimization is adjusted based on different scenarios, ranging from 5 103 to 2 102, with commonly used values of 0.008 and 0.01. The material learning rate is set accordingly as 20 times the velocity learning rate. All optimization processes employ linear learning rate decay, with the learning rate reset to its initial value at the beginning of each alternating phase. During training, each scene undergoes three alternating optimization cycles for material and velocity, where the material is trained for 20 iterations per cycle, while the velocity is optimized for 30 iterations per frame. This iterative alternating optimization strategy gradually reduces the coupling between velocity and material, leading to more stable parameter learning. Voxel-Based Adaptive Sampling. To achieve comprehensive volumetric representation, we first load the input triangular mesh and extract its vertices and surface sample points. We then adaptively partition the space into multiple voxels based on the bounding box of the mesh and predefined resolution. For each voxel, we evaluate its center point to determine whether it lies inside the mesh. If the center is within the mesh interior, we randomly generate set number of sampling points inside the voxel. Each of these points is then checked to verify whether it remains inside the mesh. Finally, we aggregate all valid interior sampling points, along with surface points and original vertices, into unified complete point cloud. This ensures dense and well-distributed volumetric representation of the input mesh, which is subsequently outputted for further processing. A.3. Qualitative Comparisons on Inverse Skinning Although the LBS-based method is initialized with ground truth skinning weights, we observe that when jointly optimizing skinning weights and bone transformations, the optimization process can fall into suboptimal local minimum due to Figure 6. Qualitative Comparisons of PhysRig and neural linear blend skinning using ground truth skinning weights as initialization. incorrect bone transformations, ultimately leading to unsatisfactory results. Additionally, despite incorporating the least motion loss, LBS often exhibits noticeable frame-to-frame jitter or abrupt changes in motion. Moreover, LBS can suffer from various artifacts, such as unrealistic folding (e.g., in the leopard case), incorrect changes in volume size (e.g., in the Michelle case), or distorting the wrong body parts in an attempt to minimize loss, leading to severe deformations (e.g., in the T-Rex case). In contrast, PhysRig, being grounded in physics-based simulator, produces significantly more realistic and physically plausible results. A.4. User Study We provide user study for comparison between PhysRig and neural linear blend skinning on inverse skinning. We asked 50 lay participants from online platforms for user studies, to rate the quality of 120 optimized mesh sequences from LBS-1, 2, 3, and PhysRig on scale of 0 (low) to 5 (high). The participants are paid at an hourly rate of 16 USD. The results from different methods are anonymized as to E, and the order is randomized. We provide the video visualization of these comparisons. Figure 7. Visualization of Material Prototype Centers."
        }
    ],
    "affiliations": [
        "Stability AI",
        "University of Illinois Urbana Champaign"
    ]
}