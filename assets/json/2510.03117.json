{
    "paper_title": "Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction",
    "authors": [
        "Kaisi Guan",
        "Xihua Wang",
        "Zhengfeng Lai",
        "Xin Cheng",
        "Peng Zhang",
        "XiaoJiang Liu",
        "Ruihua Song",
        "Meng Cao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This study focuses on a challenging yet promising task, Text-to-Sounding-Video (T2SV) generation, which aims to generate a video with synchronized audio from text conditions, meanwhile ensuring both modalities are aligned with text. Despite progress in joint audio-video training, two critical challenges still remain unaddressed: (1) a single, shared text caption where the text for video is equal to the text for audio often creates modal interference, confusing the pretrained backbones, and (2) the optimal mechanism for cross-modal feature interaction remains unclear. To address these challenges, we first propose the Hierarchical Visual-Grounded Captioning (HVGC) framework that generates pairs of disentangled captions, a video caption, and an audio caption, eliminating interference at the conditioning stage. Based on HVGC, we further introduce BridgeDiT, a novel dual-tower diffusion transformer, which employs a Dual CrossAttention (DCA) mechanism that acts as a robust ``bridge\" to enable a symmetric, bidirectional exchange of information, achieving both semantic and temporal synchronization. Extensive experiments on three benchmark datasets, supported by human evaluations, demonstrate that our method achieves state-of-the-art results on most metrics. Comprehensive ablation studies further validate the effectiveness of our contributions, offering key insights for the future T2SV task. All the codes and checkpoints will be publicly released."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 7 1 1 3 0 . 0 1 5 2 : r TAMING TEXT-TO-SOUNDING VIDEO GENERATION VIA ADVANCED MODALITY CONDITION AND INTERACTION Kaisi Guan,1,2 Xihua Wang*1 Zhengfeng Lai,2 Xin Cheng 1 Ruihua Song1 Meng Cao2 Peng Zhang2 Kieran Liu2 1Renmin University of China 2Apple Figure 1: Examples of sounding videos generated by our BridgeDiT model, showcasing high quality, temporal synchronization, and text alignment. Our method generates high-fidelity video frames and detailed audio spectrograms that remain faithful to the given text prompts. Critically, as highlighted in the dashed boxes, the generated audio and video are precisely synchronized, demonstrating strong temporal coherence between visual events and their corresponding sounds. More cases are shown in the anonymous demo page https://bridgedit-t2sv.github.io."
        },
        {
            "title": "ABSTRACT",
            "content": "This study focuses on challenging yet promising task, Text-to-Sounding-Video (T2SV) generation, which aims to generate video with synchronized audio from text conditions, meanwhile ensuring both modalities are aligned with text. Despite progress in joint audio-video training, two critical challenges still remain unaddressed: (1) single, shared text caption where the text for video is equal to the text for audio (TV = TA) often creates modal interference, confusing the pretrained backbones, and (2) the optimal mechanism for cross-modal feature interaction remains unclear. To address these challenges, we first propose the Hierarchical Visual-Grounded Captioning (HVGC) framework that generates pairs of disentangled captions, video caption (TV ), and an audio caption (TA), eliminating interference at the conditioning stage. Based on HVGC, we further introduce BridgeDiT, novel dual-tower diffusion transformer, which employs Dual CrossAttention (DCA) mechanism that acts as robust bridge to enable Equal contribution. The work was done during an internship at Apple. Project Lead. Corresponding Author. 1 symmetric, bidirectional exchange of information, achieving both semantic and temporal synchronization. Extensive experiments on three benchmark datasets, supported by human evaluations, demonstrate that our method achieves state-ofthe-art results on most metrics. Comprehensive ablation studies further validate the effectiveness of our contributions, offering key insights for the future T2SV task. All the codes and checkpoints will be publicly released."
        },
        {
            "title": "INTRODUCTION",
            "content": "Human perception is inherently multi-sensory, with vision and sound tightly coupled. Generating videos with synchronized audio from text (Text-to-Sounding-Video, T2SV) represents crucial step toward world-modeling. Recent years have witnessed rapid progress in Text-to-Video (T2V) (Blattmann et al., 2023; Brooks et al., 2024; Zheng et al., 2024; Weijie Kong, 2024; Wan et al., 2025) and Text-to-Audio (T2A) (Liu et al., 2023; Huang et al., 2023; Wang et al., 2025a; Evans et al., 2025) generation. With these unimodal capabilities becoming increasingly mature, the community naturally shifts attention to the more challenging task of T2SV (Tang et al., 2023; Liu et al., 2024a; Ishii et al., 2024; Liu et al., 2025; Weng et al., 2025). Prior strategies for T2SV suffer from critical limitations. The simple approach of generating video and audio independently with T2V and T2A models fails to achieve temporal synchronization. Pipelined methods (e.g., TVA or TAV) attempt to address this, but they suffer from error accumulation. This is because the second-stage generative model (Video-to-Audio (Wang et al., 2024b; Cheng et al., 2025b;a; Wang et al., 2025a) or Audio-to-Video (Jeong et al., 2023; Cao et al., 2023; Zhang et al., 2024)), having been trained only on ground-truth data, cannot correct the errors from the first stage and often amplifies them. To overcome these limitations, research has increasingly shifted toward joint video-audio generation, where both modalities are synthesized simultaneously. The single-tower paradigm (Ruan et al., 2023; Tang et al., 2023; Sun et al., 2024; Wang et al., 2024a; Zhao et al., 2025), which learns the audio-video joint distribution from scratch in one shared model, is often data-intensive and complex to optimize, demanding significant computational resources and often struggling with training stability. Thus, the dual-tower architecture (Ishii et al., 2024; Liu et al., 2024a; 2025; Wang et al., 2025b; Weng et al., 2025) has emerged as the dominant approach. This strategy leverages pretrained T2V and T2A backbones and connects them with lightweight and trainable interaction module, enabling the generation of synchronized sounding videos without the large cost of from-scratch training. Despite its promise, this paradigm still faces two fundamental yet under-explored challenges: C1. The Conditioning Problem: Dual-tower framework is typically initialized with unimodal backbones (T2V, T2A), where each backbone (tower) is pretrained with modality-specific caption, however current dual-tower methods (Liu et al., 2024a; 2025; Zhao et al., 2025; Weng et al., 2025) often use shared caption for both towers ( TV = TA), mixing visual and auditory conditions. This mixture leads to modal interference problem: text that is semantically relevant for one modality often appears as irrelevant noise to the other. For example, given the text red car emits sharp honk, the video tower is forced to process the auditory text sharp honk, while the audio tower is forced to interpret the visual attribute red. Such modal interference pushes both towers into out-of-distribution text condition, thereby degrading performance. C2. The Interaction Problem: The interaction module is the architectural component responsible for exchanging information between the video and audio towers. However, its optimal design still remains unsolved. The core challenge is enabling an effective yet efficient exchange of features, which is essential for ensuring that the final output is synchronized both semantically and temporally. In this work, we first address the conditioning problem by introducing Hierarchical Visual-Grounded Captioning (HVGC) framework. HVGC satisfies two critical requirements: (1) provide disentangled, modality-pure text captions for each tower, aligning with their pretraining; and (2) ensure the accuracy of these captions. While direct Audio LLMs (Chu et al., 2024) can provide separate audio captions, they often yield inaccurate or noisy descriptions due to the inherent information sparsity of raw audio, leading to severe hallucinations (Nishimura et al., 2024; Kuan et al., 2024). HVGC rectifies this by employing visual grounding throughout its three-stage pipeline: (i) generating detailed visual description, (ii) extracting auditory-relevant concepts from it, and (iii) producing modality-pure audio caption that remains robustly grounded in the visual context. This design ensures both the separation and accuracy crucial for text conditioning. Building on HVGC, we further propose BridgeDiT, dual-tower architecture with Dual CrossAttention (DCA) fusion mechanism. This design enables symmetric, bidirectional information exchange between the video and audio towers. Extensive experiments together with human evaluation demonstrate that our model achieves state-of-the-art results. Furthermore, we conduct detailed ablation studies that validate the critical role of our HVGC framework and, through comparisons with alternative fusion mechanisms, prove the superiority of our Dual Cross-Attention fusion mechanism. In summary, our main contributions are as followered: (i) novel Hierarchical Visual-Grounded Captioning (HVGC) framework that generates disentangled text caption to eliminate modal interference in T2SV task; (ii) the BridgeDiT architecture, featuring Dual CrossAttention (DCA) mechanism for effective and efficient cross-modal fusion; and (iii) comprehensive experiments and analyses that demonstrate state-of-the-art performance and provide valuable insights into caption pipeline and architecture design choice."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Text-Condition Single Modal Generation Text-condition single modal generation, including Text-to-Video (T2V) (Wang et al., 2023; Zheng et al., 2024; Lin et al., 2024; Weijie Kong, 2024; Kuaishou, 2024; Brooks et al., 2024; Wan et al., 2025) and Text-to-Audio (T2A) (Liu et al., 2023; 2024b; Evans et al., 2025; Huang et al., 2023), has become prominent area of research in recent years. Both domains have followed parallel evolution: architectures have advanced from UNets ( Ozgun icek et al., 2016) to the state-of-the-art Diffusion Transformer (DiT) (Peebles & Xie, 2022), while training paradigms have shifted from DDPM (Ho et al., 2020) to more efficient methods like EDM (Karras et al., 2022) and flow matching (Lipman et al., 2022). While current T2V and T2A models can independently generate high-quality content, they struggle to generate videos with semantically and temporally synchronized sound, which is addressed in this work. Video-Audio Cross-Modal Generation To improve semantic and temporal synchronization, some works explore audio-video cross-modal generation for T2SV task. This includes Video-toAudio (V2A) generation (Wang et al., 2024b; Cheng et al., 2025b; Xing et al., 2024; Cheng et al., 2025a; Wang et al., 2025a), which uses video to condition audio generation, and Audio-to-Video (A2V) generation (Lee et al., 2022; Jeong et al., 2023; Cao et al., 2023), which uses audio to condition video generation. These unidirectional models can be chained into pipelines (e.g., TVA or TAV) to achieve more synchronized audio-visual content than independent generation. However, these pipelined methods suffer from error accumulation (Liu et al., 2024a; 2025) problem. Since these cross-modal models are trained on ground-truth data, any artifacts or inconsistencies from the initial text-conditional stage (Guan et al., 2025) are inevitably propagated, leading to suboptimal final results. To avoid this error accumulation problem, we instead pursue joint generation approach where both modalities are created in single step. Text-Condition Audio-Video Joint Generation To overcome the error accumulation of pipelined methods, recent research has shifted towards audio-video joint generation. Existing methods largely follow two paradigms: single-tower and dual-tower. The single-tower approach learns the joint audio-video distribution from scratch (Ruan et al., 2023; Tang et al., 2023; Sun et al., 2024; Wang et al., 2024a; Zhao et al., 2025); however, this method requires vast, costly paired datasets and is difficult to collect and train, and has shown limited practical success. As result, the dual-tower paradigm has emerged as more practical alternative. It leverages pre-trained T2V and T2A models, focusing the training effort on an interaction module responsible for fusing audio and video features. The design of this module is critical, with current strategies including Full Attention for direct fusion (Wang et al., 2025b), ControlNet-style (Zhang et al., 2023) conditioning that enables bidirectional influence (i.e., video conditioning audio generation (Liu et al., 2024a), and audio conditioning video (Weng et al., 2025)), and specialized components like the Prior Estimator in JavisDiT (Liu et al., 2025). Our work adopts the dual-tower paradigm but explore new ways to achieve more holistic interaction among the text, audio, and video modalities in the T2SV task. 3 Figure 2: Our three-stage Hierarchical Visual-Grounded Captioning (HVGC) framework generates disentangled modality-pure text captions. First, Vision-Language Large Model (VLLM) produces detailed video caption (TV ). Subsequently, Large Language Model (LLM) extracts relevant audio tags from this video caption. Finally, the framework leverages both the visual context in TV and the extracted audio tags to generate pure audio caption (TA)."
        },
        {
            "title": "3.1 PRELIMINARY",
            "content": "Generative Models in Denoised Manner Denoised generative models learn complex data distribution p(x) by reversing process that destroys data to simple Gaussian prior (0, I). Diffusion models (Ho et al., 2020) approach this by training network ϵθ to predict the noise ϵ added to data sample x0 at timestep t: LDDPM(θ) = Et,x0,ϵ (cid:2)ϵ ϵθ( αtx0 + 1 αtϵ, t)2(cid:3) . (1) Flow Matching (FM) (Lipman et al., 2022) models learn velocity field vθ that transports noise sample x0 to data sample x1 by approximating the target field x1 x0. The training objective is: LFM(θ) = Et,x0,x1 (cid:2)vθ(tx0 + (1 t)x1, t) (x1 x0)2(cid:3) . (2) Generation in both cases involves starting with sampled noise and applying the learned network iteratively denoise to obtain clean data sample. More background is in Appendix B. Problem Formulation For the T2SV task, we adopt the dual-tower paradigm. This approach is highly practical as it leverages the capabilities of pre-trained unimodal models, video tower GV θ and an audio tower GA θ . In this setup, the towers independently process their respective text captions, TV and TA, audio timestep tA and video timestep tV , noisy audio latent xA(tA) and nosiy video latent xV (tV ) while trainable interaction module, BAV , facilitates cross-modal communication: θ (ˆa, ˆv) = Gmodel(TA, TV , xA(tA), xV (tV ), tA, tV ), where Gmodel = {GA θ , GV θ , BAV θ }. (3) The final output consists of the predicted audio ˆa and video ˆv noise vector. Training Objective The training objective of T2SV is the sum of the loss from the two towers: = Laudio + Lvideo. (4) The audio tower follows diffusion training setup using v-prediction diffusion (Salimans & Ho, 2022) loss objective. Given the continuous timestep tA [0, 1], the signal and noise scaling factors are α(tA) = cos(tAπ/2) and σ(tA) = sin(tAπ/2). We denote xA as the audio latent vector from the audio Variational AutoEncoder (VAE) (Kingma & Welling, 2022) encoder. It predicts the target α(tA)ϵA σ(tA)xA and for the noisy audio latent xA(tA) = α(tA)xA + σ(tA)ϵA: Laudio = ˆa (α(tA)ϵA σ(tA)xA)2 . (5) The video tower follows flow matching (Lipman et al., 2022) loss objective. The corresponding video timestep tV is defined as tV = 1000 tA. xV is the video latent vector. It predicts the target vector field ϵV xV and for the noisy video latent xV (tV ) = (1 tV /1000)xV + (tV /1000)ϵV : Lvideo = ˆv (ϵV xV )2 . (6) Here, ϵA and ϵV are Gaussian noise vectors sampled from (0, I). The detailed inference process is further shown in Appendix C.3. 4 Figure 3: The BridgeDiT Architecture. (a): The overall dual-tower architecture. Parallel video and audio DiT streams are connected by our proposed BridgeDiT Block at specific layers. Right: Details of fusion strategies within the block, showcasing our proposed Dual Cross-Attention (b) alongside the Full-Attention (c) and Additive Fusion (d) baselines. 3.2 HIERARCHICAL VISUAL-GROUNDED CAPTIONING FRAMEWORK To address the conditioning problem, we introduce the Hierarchical Visual-Grounded Captioning (HVGC) framework. As illustrated in Figure 2, HVGC is three-stage pipeline designed to generate disentangled, modality-pure video caption (TV ) and audio caption (TA) from sounding videos. Since directly generating captions from raw audio, even with advanced Audio Large-Language-Models (LLMs) (Chu et al., 2024), can lead to severe hallucination issues (Sung-Bin et al., 2024; Nishimura et al., 2024; Kuan et al., 2024), for instance, rhythmic drumming might be misinterpreted as high heels clicking on pavement. This is due to the ambiguity of information conveyed by audio. HVGC tackles this by grounding audio caption generation in rich visual context. Initially (Stage 1), powerful Vision-Language Large Language Model (VLLM), such as Qwen2.5VL-72B (Bai et al., 2025), produces comprehensive visual description (TV ) of the video clip. We employ an in-context learning approach with meticulously designed prompt. This prompt guides the VLLM to detail the videos environment, subject actions, cinematography, and overall style. Subsequently (Stage 2), an auxiliary Large Language Model (LLM) abstracts key auditory event tags (e.g., hammer striking metal, hiss of sparks) directly from TV . This process, inspired by Chain-of-Thought (CoT) prompting (Wei et al., 2022; Teng et al., 2025), acts as an intermediate filter, distilling the visual context into relevant sound-producing elements, thereby preventing the final audio caption from including non-existent sounds. Finally (Stage 3), leveraging both the detailed video caption(TV ) and the abstracted auditory tags, we use an LLM (Qwen2.5-72B (Yang et al., 2024)) to generate the final audio caption (TA). This crucial step ensures TA is not only contextually consistent with the video narrative but also articulated exclusively using non-visual, auditory language. This hierarchical, visually-grounded approach delivers pure unimodal captions, effectively eliminating cross-modal interference for optimal performance of our dual-tower T2SV model. Detailed prompts for HVGC are provided in the Appendix C.5. 3.3 THE BRIDGEDIT ARCHITECTURE To address the interaction problem, we introduce BridgeDiT, novel dual-tower diffusion transformer architecture for sounding video generation. As depicted in Figure 3 (a), BridgeDiT consists of two parallel pre-trained DiT backbones for video and audio that remain largely frozen. To thoroughly investigate the optimal strategy for effective cross-modal fusion, we propose Dual CrossAttention (DCA) fusion mechanism within each BridgeDiT Block. We compare DCA against several alternative fusion mechanisms with detailed experiments presented in Section 4.3.2. Furthermore, an ablation study on the optimal placement of BridgeDiT Blocks across different layers is discussed in Appendix D. Dual CrossAttention Fusion As detailed in Figure 3 (b), our fusion mechanism takes the video latent LV and the audio latent LA. Features are then updated through two parallel, symmetric streams within the block. In the Audio-to-Video (A-to-V) stream, video features are refined based 5 on audio context. For this operation, the video latent LV first passes through Layer Normalization (LN) layer and is then projected by Linear-V to form the query (Q). Concurrently, the audio latent LA also undergoes Layer Normalization and is then projected by Linear-A to provide the key (K) and value (V). Here, LN () denotes Layer Normalization: QV = LinearQV (LN(LV )), KA = LinearKA(LN(LA)), VA = LinearVA (LN(LA)) (7) The resulting projections (QV , KA, VA) are fed into cross-attention layer. The output of this attention operation is then integrated back into the video latent via residual connection to produce the updated video latent, : = Attention(QV , KA, VA) + LV (8) This is subsequently passed through Layer Normalization and an MLP block with another residual connection, completing the video feature update. Concurrently, the Video-to-Audio (V-to-A) stream operates in perfectly symmetric manner. In this case, the audio latent LA serves as the query, while the video latent LV provides the key and value. The update process is analogous, yielding the updated audio latent, A. Consistent with the DiT (Peebles & Xie, 2022) paradigm, the BridgeDiT Block is also conditioned on the timestep condition (tV and tA) via the adaptive layer normalization (AdaLN) (Perez et al., 2018) mechanism. Alternative Fusion Mechanism To thoroughly validate the effectiveness of our Dual CrossAttention (DCA) fusion mechanism, we compare it against several alternative fusion strategies adopted from existing works. These baselines, also visualized in Figure 3 alongside our DCA, are implemented under the same settings as the BridgeDiT Block. Full Attention Fusion: As shown in Figure 3 (c), this method performs joint self-attention operation across both modalities. First, the video latent LV and audio latent LA are independently projected into query, key, and value representations after normalization. These modality-specific projections are then concatenated along the sequence dimension to form unified tensors: Qcat = Concat(QV , QA), Kcat = Concat(KV , KA), Vcat = Concat(VV , VA). (9) single self-attention operation is applied to these unified tensors, allowing for all-to-all interaction. The output is then residually connected with the original concatenated latents: cat = Attention(Qcat, Kcat, Vcat) + Concat(LV , LA). (10) Finally, this fused representation is split back into separate video and audio latents, JoinDiT (Wang et al., 2025b) use this fusion for image-conditioned sound video generation. Additive Fusion: As shown in Figure 3 (d), this method uses highly efficient and lightweight alternative that projects and combines video and audio feature with element-wise addition. Due to its small parameter count, this method was adapted by SSVG (Ishii et al., 2024). and A. Unidirectional Cross-Attention: This approach treats one modality as the condition for the other in ControlNet-style (Zhang et al., 2023). In our baseline implementations, the tower that provides the condition processes its own features using standard self-attention block, while the other tower uses the same cross-attention block as our DCA fusion mechanism. We implement both V2A (Liu et al., 2024a) and A2V (Weng et al., 2025) variants for comparison. (The architecture figures are omitted for brevity, as the overall structure is similar to DCA)"
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENT SETUP Implementation Details For the video backbone, we utilize the WAN 2.1 (1.3B) model (Wan et al., 2025), retaining its original configuration with UMT5-XXL (Raffel et al., 2020) text encoder to generate 81 frames at 15fps and 480p resolution. For the audio backbone, we employ Stable Audio Open 1.0 (Evans et al., 2025) with T5-base text encoder (Raffel et al., 2020), generating audio at 44.1kHz sample rate. The total generation length is standardized to 5.4 seconds. Our BridgeDiT architecture consists of 4 BridgeDiT Blocks, which are uniformly inserted between the corresponding layers of the video and audio towers. More details are in the Appendix C. 6 Table 1: Automatic evaluation on the AVSync15 dataset. Best and second-best are highlighted. Process Method Wan + SDA + + Wan + MMAudio Wan + SeeingHearing SDA + TPos SDA + TempoToken + AV JointDiT JavisDiT SSVG MTV CoDi BridgeDiT (ours) VA Quality Text Alignment Synchronization FVD KVD FAD KL CLIPSIM CLAP VA-IB AV-Align 28.12 828.33 28.12 828.33 28.12 828.33 18.64 1975.22 20.56 1516.53 29.94 992.71 28.05 878.70 25.78 1028.78 27.66 982.09 24.96 1387.14 765.74 28.52 26.22 33.50 35.87 25.45 17.63 34.17 20.75 22.45 15.84 10.79 34.59 30.78 34.64 25.52 30.78 30.78 30.34 22.99 23.67 21.22 17.94 35. 22.56 22.56 22.56 92.95 42.30 25.20 23.23 31.44 24.60 39.24 21.33 11.90 7.98 14.21 11.90 11.90 6.51 13.48 14.35 16.46 16.56 5.34 0.205 0.243 0.208 0.176 0.215 0.156 0.158 0.126 0.149 0.081 0.275 3.17 1.40 2.89 3.17 3.17 1.77 3.50 4.35 3.53 5.24 1.30 Table 3: Ablation study on disentangled text condition. We compare shared caption strategies (using the video caption or an Omini model caption) against disentangled caption strategies (using an Audio-LLM or our method) in both full-training and zero-shot settings. FVD FAD CLIPSIM CLAP VA-IB AV-Align If-Training No No Yes Yes Caption Source Shared text condition (TV = TA) Video Caption (TV ) Omini Caption Video Caption (TV ) Omini Caption Disentangled text condition (TV = TA) TA from AudioLLM HVGC TA from AudioLLM HVGC (ours) No No Yes Yes 908.81 1362.83 828.33 1232. 908.81 908.81 776.25 765.74 16.39 13.75 7.75 7.56 16.36 14.90 12.12 5.34 28.34 25.81 28.31 25.76 28.34 28.34 26.35 28.52 9.65 22.45 25.10 31. 20.62 28.37 27.76 35.95 18.92 23.36 29.22 33.76 15.86 25.36 19.44 34.59 0.172 0.135 0.245 0.229 0.135 0.211 0.242 0.275 Dataset We evaluate our model on the T2SV task using three datasets: AVSync15 (Zhang et al., 2024), VGGSound-SS (Chen et al., 2021), and Landscape (Lee et al., 2022). (1) AVSync15 (Zhang et al., 2024) is subset of VGGSound Chen et al. (2020) and contains synchronized audio-video pairs across 15 categories. The dataset is split into 1350 videos for training and 150 for testing. (2) VGGSound-SS (Chen et al., 2021) is sound source localization dataset, also derived from VGGSound (Chen et al., 2020), where the sounding object is always visually present. It includes 5,158 videos from 220 different classes. We randomly sample 150 videos to form our test set. (3) Landscape (Lee et al., 2022): This dataset comprises 928 videos, depicting 9 different scenic categories. Since the official versions of these datasets lack standard captions, we generated them using HVGC. For preprocessing, we first ensure audio-visual correspondence by retaining only pairs with an ImageBind (Girdhar et al., 2023) score above 0.3. Subsequently, all videos are standardized to 5.4-second duration via random cropping or padding. Baseline We compare our method against comprehensive set of baselines, which we categorize into five distinct T2SV generation strategies. (1) A: This baseline generates video and audio independently. To ensure fair comparison, we implement this by disabling the interaction modules and only training two separate towers, as Wan+SDA. (2) A: This pipeline first generates video from text and subsequently generates the audio conditioned on the video. We employ Wan-1.3B for the T2V step, followed by the V2A models MMAudio (Cheng et al., 2025a) and SeeingHearing (Xing et al., 2024). (3) V: This method first generates audio from text using Stable Diffusion Audio Open (SDA) (Evans et al., 2025), then generates video conditioned on this audio using the T-Pos (Jeong et al., 2023) and TempoToken (Cao et al., 2023). (4) AV: This approach uses an intermediate image generated by Qwen-Image (Wu et al., 2025). Then, the JointDiT (Wang et al., 2025b) model is used to jointly generate the video and audio from this image. (5) AV: This strategy includes existing joint-training models such as JavisDiT (Liu et al., 2025), SSVG (Ishii et al., 2024), CoDi (Tang et al., 2023), and MTV (Weng et al., 2025). More details about these baselines are in the Appendix C.2. 7 8.05 5.80 0.177 0. FVD FVD 737.96 700.18 Method 5.39 8. LandScape 737.96 737.96 VGGSound-SS FAD AV-Align FAD AV-Align Wan + SDA Table 2: Performance on VGGSound-SS and Landscape. AV denotes AV-Align metric here. Best and second-best are highlighted. + SDA + TPos SDA + TempoToken + Wan + MMAudio Wan + SeeingHearing Evaluation Metric We evaluate T2SV from three perspectives: different generation text quality, alignment, and audio-video synchronization. (1) Generation Quality. For video quality, we employ the Frechet Video Distance (FVD) (Unterthiner et al., 2018) and Kernel Video (UnDistance terthiner 2018). For audio quality, we use the Frechet Audio Distance (Kilgour et al., 2018) (FAD) and the KullbackLeibler (Wang et al., 2024b) (KL) divergence score. (2) Text Alignment. We evaluate video and audio text alignment separately. We use CLIPSIM (Radford et al., 2021) to evaluate video-text alignment and CLAP (Elizalde et al., 2023) score to measure audio-text alignment. (3) Audio-Video Synchronization. We evaluate both semantic sync using the ImageBind score (IB-VA) (Girdhar et al., 2023) and temporal sync using the AV-Align score (Yariv et al., 2024). VA JavisDiT SSVG CoDi + VA JointDiT (KVD) al., et 637.50 1032.87 1203.68 668.87 1186.39 1220.31 BridgeDiT (ours) 0.185 0.143 0.082 0.179 0.148 0. 9.22 8.97 13.75 1732.47 1942.94 1837.42 2089.05 700.18 700.18 7.78 8.86 8.38 0.218 0. 0.333 0.321 0.143 0.206 0.163 0.242 5.80 5.80 5.31 7.59 8.05 8. 937.09 866.59 628.07 615.28 0.3617 0. 0.075 0.258 6.35 7.19 6.01 4. 4.2 MAIN RESULTS: COMPARISON WITH BASELINES We compare our propose approach with baselines on three datasets and present results in Table 1 and Table 2, demonstrating that our approach surpasses all baselines on most metrics, including video quality (FVD, KVD), audio quality (FAD, KL), audio-text alignment (CLAP), and temporal synchronization (AV-Align). First, our model significantly outperforms the Wan+SDA baseline, which is equivalent to our architecture but with the interaction modules removed. This validates the effectiveness of our BridgeDiT Block, proving that enabling cross-modal interaction is crucial for enhancing the generative quality of both modalities and achieving strong semantic and temporal synchronization. Second, BridgeDiT consistently outperforms pipelined baselines , which suggests that our joint generation approach effectively mitigates the error accumulation inherent in pipeline strategy. We observe two minor exceptions. Our CLIPSIM score (28.52) is slightly lower than that of JointDiT (Wang et al., 2025b) (29.94), gap we attribute to better alignment with T2I backbone Qwen-Image (Wu et al., 2025). Our IB-VA score (34.59) is also surpassed by SeeingHearing (35.87), which is expected as SeeingHearing model uses ImageBind Score (Girdhar et al., 2023) as classifier guidance. Finally, as shown in Table 2, BridgeDiT also achieves state-of-the-art results on most metrics on the VGGSound-SS (Chen et al., 2021) and Landscape (Lee et al., 2022) datasets, confirming its strong generalization capability. 4.3 ABLATION STUDIES 4.3.1 EFFECT OF DISENTANGLED TEXTUAL CONDITIONING We compare HVGC against several caption strategies as shown in Table 3: shared captions (using either video caption for both towers or single caption from Omini Model like Qwen2.5-Omini (Xu et al., 2025)) and disentangled captions (generating TA with an Audio LLM like Qwen2-Audio (Chu et al., 2024)). From the results, we derive three key insights: (1) Our HVGC framework consistently yields the best performance across both zero-shot (without training the interaction module) and fulltraining settings, demonstrating its robust superiority. (2) Within the shared text condition setting, the Omini-model caption improves audio-related metrics (FAD, CLAP) but harms video quality and synchronization. This highlights the inherent limitation of single shared caption to adequately serve both modalities. (3) The alternative disentangled baseline, which uses an Audio LLM for the audio caption (TA), performs poorly. This is due to significant hallucination issues, where the model invents sounds inconsistent with the visual scene, thereby degrading overall performance (see Appendix C.7 for examples). This finding underscores the importance of our HVGC method."
        },
        {
            "title": "4.3.2 ANALYSIS OF FUSION MECHANISMS",
            "content": "To investigate the optimal architecture for cross-modal interaction, we conduct an ablation study on different fusion mechanisms, as illustrated in Figure 4. We compared our proposed Dual CrossAttention (DCA) fusion mechanism against several existing fusion mechanisms: FullAttention Fusion (FullAttn) (Wang et al., 2025b), Additive fusion (Add-Fusion) (Ishii et al., 2024), and two unidirectional cross-attention variants (V2A-CrossAttn (Liu et al., 2024a) and A2V-CrossAttn (Weng et al., 2025)). We measure AV-Align (Yariv et al., 2024) (top) and VAIB Score (Girdhar et al., 2023) (bottom) over the course of training. The results clearly demonstrate that our DCA fusion mechanism consistently outperforms all other fusion mechanisms. It achieves the highest scores in both AV-Align and VA-IB Score throughout the training process, indicating superior temporal and semantic synchronization. The second-best method is FullAttn, which allows for expressive and all-to-all feature interaction. The unidirectional cross-attention methods (V2A-CrossAttn, A2V-CrossAttn) and additive fusion (Add-Fusion) show comparatively weaker performance. All the experiments underscore our key insight: an effective and efficient bidirectional information exchange is critical for achieving state-of-the-art audio-video synchronization. 4.4 CASE STUDIES Figure 4: Comparing different fusion mechanisms. Our DCA fusion mechanism outperforms all other baselines in both AV-Align and VA-IB Score. Figure 1 presents case studies that highlight the capabilities of our BridgeDiT model. Powered by the combination of our HVGC framework and the DCA fusion mechanism, our model generates high-quality sound videos that are semantically synchronized, temporally synchronized, and highly aligned with the text conditions. The first case (the blacksmith) showcases precise temporal synchronization, as the visual impact of the hammer striking the iron aligns perfectly with the sharp clang event in the audio spectrogram. The second case (the saxophone player) demonstrates strong text alignment; the generated video accurately depicts key entities from the visual prompt, including the saxophone and metal drum, while the audio faithfully synthesizes the complex soundscape described in the audio prompt. These examples are representative of our models performance, and we provide more comprehensive collection of generated videos on our anonymous demo page 1. 4.5 USER STUDY Method VQ AQ Table 4: User study on AVSync15 test set. To further validate our model with human preference, we conducted user study on the AVSync15 test set with 150 samples. Five evaluators rate the generated sounding videos on 0-5 scale with 0.5-point increments across five criteria: Video Quality (VQ), Audio Quality (AQ), Text Alignment (TA), Synchronization (Sync), and Overall quality. As shown in Table 4, our BridgeDiT model is rated highest across all five dimensions, significantly outperforming all baselines, while the pipelined Wan + MMAudio method ranked as the second-best performer. Notably, while some baselines may achieve superior results on certain metrics (as in Table 1), our model remains the leading preference in human evaluations. This suggests that automatic metrics may not fully align with human preference. Wan + SDA Wan + Seeing Wan + MMAudio SDA + TmToken JointDiT JavisDiT BridgeDiT (ours) 2.68 2.90 3.16 1.52 2.59 1.94 3.52 2.16 2.65 2.87 1.33 2.39 1.76 3. 2.78 2.79 3.12 1.42 2.56 1.89 3.46 2.90 2.96 3.26 2.90 2.62 2.00 3.60 3.30 3.30 3.30 1.42 2.74 2.01 3.59 Sync Overall TA 1https://bridgedit-t2sv.github.io"
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this work, we address two fundamental challenges in T2SV generation: the condition problem caused by shared text caption and the interaction problem in dual-tower architectures. We introduce the Hierarchical Visual-Grounded Captioning (HVGC) framework to generate disentangled, modality-pure captions and the BridgeDiT architecture with its Dual CrossAttention mechanism for symmetric and efficient fusion. Through comprehensive experiments, our method achieves stateof-the-art performance, result supported by both automatic metrics and human evaluations. Finally, our detailed ablation studies validate the effectiveness of each proposed component and offer valuable insights for the design of future T2SV models. Limitations and future work direction are discussed in Appendix E."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "We acknowledge that Text-to-Sounding-Video generation technology, like other generative technologies, carries potential risks of misuse. The ability to create realistic and synchronized audiovisual content from text could be exploited to generate convincing disinformation and fraudulent materials. The primary motivation for our research, however, is positive. We believe this technology holds significant potential for beneficial applications. We are committed to the responsible advancement of this field and encourage continued research into synthetic content detection and the establishment of clear ethical guidelines for deployment."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "To ensure the reproducibility of our work, detailed experimental information can be found in Appendix (including Compute Resources C.1, Baselines C.2, Inference Process Details C.3, Hyperparameters C.4, HVGC Prompts C.5, Human Annotation Command C.6, and Caption Examples C.7). Furthermore, the complete source code, trained model checkpoints, and datasets necessary to replicate our results will be made publicly available at https://bridgedit-t2sv. github.io/. We are committed to transparency and facilitating future research in this area."
        },
        {
            "title": "REFERENCES",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets, 2023. URL https://arxiv.org/abs/2311.15127. Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators, 2024. URL https://openai.com/research/ video-generation-models-as-world-simulators. Defu Cao, Furong Jia, Sercan Arik, Tomas Pfister, Yixiang Zheng, Wen Ye, and Yan Liu. Tempo: arXiv preprint Prompt-based generative pre-trained transformer for time series forecasting. arXiv:2310.04948, 2023. Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zisserman. Vggsound: large-scale audio-visual dataset. In International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2020. Honglie Chen, Weidi Xie, Triantafyllos Afouras, Arsha Nagrani, Andrea Vedaldi, and Andrew Zisserman. Localizing visual sounds the hard way. In CVPR, 2021. 10 Ho Kei Cheng, Masato Ishii, Akio Hayakawa, Takashi Shibuya, Alexander Schwing, and Yuki Mitsufuji. Mmaudio: Taming multimodal joint training for high-quality video-to-audio synthesis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2890128911, 2025a. Xin Cheng, Xihua Wang, Yihan Wu, Yuyue Wang, and Ruihua Song. Lova: Long-form video-toaudio generation. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 15. IEEE, 2025b. Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen2-audio technical report. arXiv preprint arXiv:2407.10759, 2024. Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang. Clap learning In ICASSP 2023-2023 IEEE International audio concepts from natural language supervision. Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 15. IEEE, 2023. Zach Evans, Julian Parker, CJ Carr, Zack Zukowski, Josiah Taylor, and Jordi Pons. Stable audio In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal open. Processing (ICASSP), pp. 15. IEEE, 2025. Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In CVPR, 2023. Kaisi Guan, Zhengfeng Lai, Yuchong Sun, Peng Zhang, Wei Liu, Kieran Liu, Meng Cao, and Ruihua Song. Etva: Evaluation of text-to-video alignment via fine-grained question generation and answering. arXiv preprint arXiv:2503.16867, 2025. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020. URL https://arxiv.org/abs/2006.11239. Jiawei Huang, Yi Ren, Rongjie Huang, Dongchao Yang, Zhenhui Ye, Chen Zhang, Jinglin Liu, Xiang Yin, Zejun Ma, and Zhou Zhao. Make-an-audio 2: Temporal-enhanced text-to-audio generation, 2023. Masato Ishii, Akio Hayakawa, Takashi Shibuya, and Yuki Mitsufuji. simple but strong baseline for sounding video generation: Effective adaptation of audio and video diffusion models for joint generation. arXiv preprint arXiv:2409.17550, 2024. Yujin Jeong, Wonjeong Ryoo, Seunghyun Lee, Dabin Seo, Wonmin Byeon, Sangpil Kim, and Jinkyu Kim. The power of sound (tpos): Audio reactive video generation with stable diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 78227832, 2023. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models. Advances in neural information processing systems, 35:2656526577, 2022. Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Sharifi. Frechet audio distance: metric for evaluating music enhancement algorithms. arXiv preprint arXiv:1812.08466, 2018. Diederik Kingma and Max Welling. Auto-encoding variational bayes, 2022. URL https: //arxiv.org/abs/1312.6114. Inc Kuaishou. Kling video generation, 2024. URL https://klingai.com/. Chun-Yi Kuan, Wei-Ping Huang, and Hung-yi Lee. Understanding sounds, missing the questions: The challenge of object hallucination in large audio-language models. arXiv preprint arXiv:2406.08402, 2024. 11 Seung Hyun Lee, Gyeongrok Oh, Wonmin Byeon, Chanyoung Kim, Won Jeong Ryoo, Sang Ho Yoon, Hyunjun Cho, Jihyun Bae, Jinkyu Kim, and Sangpil Kim. Sound-guided semantic video generation. In Computer VisionECCV 2022: 17th European Conference, Tel Aviv, Israel, October 2327, 2022, Proceedings, Part XVII, pp. 3450. Springer, 2022. Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark Plumbley. Audioldm: Text-to-audio generation with latent diffusion models. arXiv preprint arXiv:2301.12503, 2023. Haohe Liu, Gael Le Lan, Xinhao Mei, Zhaoheng Ni, Anurag Kumar, Varun Nagaraja, Wenwu Wang, Mark Plumbley, Yangyang Shi, and Vikas Chandra. Syncflow: Toward temporally aligned joint audio-video generation from text. arXiv preprint arXiv:2412.15220, 2024a. Haohe Liu, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Qiao Tian, Yuping Wang, Wenwu Wang, Yuxuan Wang, and Mark Plumbley. Audioldm 2: Learning holistic audio generation with self-supervised pretraining. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 32:28712883, 2024b. Kai Liu, Wei Li, Lai Chen, Shengqiong Wu, Yanhao Zheng, Jiayi Ji, Fan Zhou, Rongxin Jiang, Jiebo Luo, Hao Fei, et al. Javisdit: Joint audio-video diffusion transformer with hierarchical spatio-temporal prior synchronization. arXiv preprint arXiv:2503.23377, 2025. Taichi Nishimura, Shota Nakada, and Masayoshi Kondo. On the audio hallucinations in large audiovideo language models. arXiv preprint arXiv:2401.09774, 2024. William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with general conditioning layer. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-totext transformer. Journal of Machine Learning Research, 21(140):167, 2020. URL http: //jmlr.org/papers/v21/20-074.html. Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, and Baining Guo. Mm-diffusion: Learning multi-modal diffusion models for joint audio and video generation. In CVPR, 2023. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022. Mingzhen Sun, Weining Wang, Yanyuan Qiao, Jiahui Sun, Zihan Qin, Longteng Guo, Xinxin Zhu, and Jing Liu. Mm-ldm: Multi-modal latent diffusion model for sounding video generation. In Proceedings of the 32nd ACM International Conference on Multimedia, pp. 1085310861, 2024. Kim Sung-Bin, Oh Hyun-Bin, JungMok Lee, Arda Senocak, Joon Son Chung, and Tae-Hyun Oh. Avhbench: cross-modal hallucination benchmark for audio-visual large language models. arXiv preprint arXiv:2410.18325, 2024. 12 Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Mohit Bansal. Any-to-any generation via composable diffusion. Advances in Neural Information Processing Systems, 36:1608316099, 2023. Fengwei Teng, Zhaoyang Yu, Quan Shi, Jiayi Zhang, Chenglin Wu, and Yuyu Luo. Atom of thoughts for markov llm test-time scaling. arXiv preprint arXiv:2502.12018, 2025. Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report, 2023. URL https://arxiv.org/abs/2308. 06571. Jun Wang, Xijuan Zeng, Chunyu Qiang, Ruilong Chen, Shiyao Wang, Le Wang, Wangjing Zhou, Pengfei Cai, Jiahui Zhao, Nan Li, et al. Kling-foley: Multimodal diffusion transformer for highquality video-to-audio generation. arXiv preprint arXiv:2506.19774, 2025a. Kai Wang, Shijian Deng, Jing Shi, Dimitrios Hatzinakos, and Yapeng Tian. Av-dit: Efficient audio-visual diffusion transformer for joint audio and video generation. arXiv preprint arXiv:2406.07686, 2024a. Xihua Wang, Yuyue Wang, Yihan Wu, Ruihua Song, Xu Tan, Zehua Chen, Hongteng Xu, and Guodong Sui. Tiva: Time-aligned video-to-audio generation. In Proceedings of the 32nd ACM International Conference on Multimedia, pp. 573582, 2024b. Xihua Wang, Ruihua Song, Chongxuan Li, Xin Cheng, Boyuan Li, Yihan Wu, Yuyue Wang, Hongteng Xu, and Yunfeng Wang. Animate and sound an image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2336923378, June 2025b. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Qi Tian Weijie Kong. Hunyuanvideo: systematic framework for large video generative models, 2024. URL https://arxiv.org/abs/2412.03603. Shuchen Weng, Haojie Zheng, Zheng Chang, Si Li, Boxin Shi, and Xinlong Wang. Audio-sync video generation with multi-stream temporal control. arXiv preprint arXiv:2506.08003, 2025. Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu. Qwen-image technical report, 2025. URL https://arxiv.org/abs/ 2508.02324. Yazhou Xing, Yingqing He, Zeyue Tian, Xintao Wang, and Qifeng Chen. Seeing and hearing: Opendomain visual-audio generation with diffusion latent aligners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 71517161, 2024. Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, Bin Zhang, Xiong Wang, Yunfei Chu, and Junyang Lin. Qwen2.5-omni technical report. arXiv preprint arXiv:2503.20215, 2025. 13 An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. Guy Yariv, Itai Gat, Sagie Benaim, Lior Wolf, Idan Schwartz, and Yossi Adi. Diverse and aligned audio-to-video generation via text-to-video model adaptation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 66396647, 2024. Lin Zhang, Shentong Mo, Yijing Zhang, and Pedro Morgado. Audio-synchronized visual animation. In Proceedings of the European Conference on Computer Vision (ECCV), 2024. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. Lei Zhao, Linfeng Feng, Dongxu Ge, Rujin Chen, Fangqiu Yi, Chi Zhang, Xiao-Lei Zhang, and Xuelong Li. Uniform: unified multi-task diffusion transformer for audio-video generation. arXiv preprint arXiv:2502.03897, 2025. Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. Ozgun icek, Ahmed Abdulkadir, Soeren S. Lienkamp, Thomas Brox, and Olaf Ronneberger. 3d u-net: Learning dense volumetric segmentation from sparse annotation, 2016. URL https: //arxiv.org/abs/1606.06650."
        },
        {
            "title": "A THE USE OF LARGE LANGUAGE MODELS",
            "content": "In this work, Large Language Models (LLMs) are used solely for enhancing writing clarity and English expression. All core contributions, including model design, mathematical formulations, and experimental analyses, are developed independently by the authors. The authors take full responsibility for the final content, ensuring no plagiarism or fabrication occurred."
        },
        {
            "title": "B DIFFUSION AND FLOW MATCHING GENERATION MODELS",
            "content": "Generative models are designed to learn complex data distribution p(x) from simple prior, typically standard Gaussian distribution (0, I). Many state-of-the-art approaches are based on learning to reverse predefined process that maps data to noise. prominent family of such models is diffusion models. In their foundational formulation (DDPM) (Ho et al., 2020), they utilize fixed forward process that progressively adds Gaussian noise to data sample x0 over discrete timesteps. The resulting noisy sample at any time t, denoted xt, can be expressed as xt = 1 αtϵ, where αt is predefined noise schedule and ϵ (0, I). neural network, ϵθ(xt, t), is then trained to predict the noise component ϵ from the corrupted sample: (cid:2)ϵ ϵθ(xt, t)2(cid:3) LDDPM(θ) = Et,x0,ϵ αtx0 + More recent frameworks, such as EDM (Karras et al., 2022), generalize this process by formulating it as solving continuous-time stochastic differential equation (SDE). EDM provides principled design methodology, emphasizing crucial choices like network preconditioning. The denoiser network, Dθ(xt, σt), is scaled to have consistent input and output magnitudes across all noise levels σt. This network is often trained to predict the clean data x0 directly, using weighted loss function that prioritizes different noise levels: LEDM(θ) = Et,x0,ϵ (cid:2)λ(σt)Dθ(xt, σt) x02(cid:3) For the generation part, both approaches start with sample from the prior, xT (0, I), and iteratively apply the learned denoising function to recover clean sample x0. As an alternative to the noise-prediction framework, Flow Matching (FM) (Lipman et al., 2022) models learn to generate data in single continuous-time transformation. These models learn vector field vt that transports samples from prior distribution p0 (noise) to the target data distribution p1 (data) by following an ordinary differential equation (ODE): dxt dt = vt(xt). To make training tractable, FM trains network vθ to approximate simple, predefined vector field. For linear path between noise sample x0 p0 and data sample x1 p1, the target vector field is simply their difference, x1 x0. The corresponding FM loss is: LFM(θ) = Et,x0,x1 (cid:2)vθ(t, (1 t)x0 + tx1) (x1 x0)2(cid:3) To generate sample, one simply solves the learned ODE dxt starting with an initial sample x0 p0. dt = vθ(t, xt) from = 0 to = 1, Classifier-Free Guidance Conditional generation in these models is commonly achieved using Classifier-Free Guidance (CFG) (Ho & Salimans, 2022). This technique steers the generation process towards desired condition (e.g., text prompt) without needing an external classifier. The model, here denoted with the noise predictor ϵθ(xt, t, c), is jointly trained on conditional inputs and null token . During sampling, the guided prediction ˆϵθ is an extrapolation from the unconditional prediction towards the conditional one: ˆϵθ = ϵθ(xt, t, ) + w(ϵθ(xt, t, c) ϵθ(xt, t, )) The guidance scale > 1 is hyperparameter that adjusts the strength of the condition. larger typically improves fidelity to the condition at the cost of reduced sample diversity. This technique is applied analogously to other model predictions like Dθ or vθ."
        },
        {
            "title": "C EXPERIMENTS SETUP",
            "content": "C.1 COMPUTE RESOURCES All experiments in this work were conducted on 4 nodes equipped with NVIDIA H100 80GB GPUs. Each node further utilized 64 Intel(R) Xeon(R) Platinum 8481C CPUs @ 2.70GHz, with 2TB of 15 RAM and 4TB of SSD storage. For generating the high-quality examples presented in our anonymous demo page, we utilized 2 NVIDIA B200 180GB GPU nodes. On these nodes, we replaced our standard video generation backbone with the more advanced Wan14B model to achieve superior visual fidelity, while maintaining identical CPU, RAM, and storage specifications per node. C.2 BASELINES Here we detail the baseline models used in our work. Wan (Wan et al., 2025) is large-scale video generative model (available in 1.3B and 14B versions) renowned for producing high-resolution and temporally coherent videos, representing leading open-source T2V model. Stable-Audio-Open (Evans et al., 2025) is diffusion-based text-to-audio generation model trained on large dataset to create diverse and realistic audio content. MMAudio (Cheng et al., 2025a) is video-to-audio synthesis model designed to generate synchronized sound for silent video clips. Seeing-and-Hearing (Xing et al., 2024) introduces diffusion latent aligners that leverage the ImageBind embedding space to create shared latent space for visual and auditory data, enabling semantic alignment guidance.na TPos (Jeong et al., 2023) focuses on audio-reactive video generation, creating dynamic and visually engaging videos that respond to the rhythm and emotional tone of an input audio track. TempoToken (Cao et al., 2023) proposes TempoTokens, learnable embeddings that guide audio-to-video generation, ensuring both temporal alignment between audio and visual output. JointDiT (Wang et al., 2025b) is dual-tower joint generative model for image-conditioned sound video generation. It employs Full Attention fusion mechanism, though its performance can be limited by its T2V backbone (e.g., Stable Video Diffusion). JavisDiT (Liu et al., 2025) is Joint Audio-Video Diffusion Transformer (JAVG) built on the DiT architecture. It achieves high-quality, synchronized audio-video generation from openended prompts by introducing Hierarchical Spatio-Temporal Synchronized Prior (HiST-Sypo) Estimator for fine-grained alignment. SSVG (Ishii et al., 2024) presents simple yet strong baseline for sounding video generation. It integrates base audio and video diffusion models with novel mechanisms like timestep adjustment and Cross-Modal Conditioning as Positional Encoding (CMC-PE), which is an additivefusion mechanism to enhance audio-video alignment. MTV (Weng et al., 2025) is versatile framework for audio-sync video generation that explicitly separates audio into speech, effects, and music tracks. This enables disentangled control over lip motion, event timing, and visual mood, leading to fine-grained and semantically aligned video generation. It also introduces the DEMIX dataset. CoDi (Tang et al., 2023) (Composable Diffusion) is versatile any-to-any generation model that composes diffusion models trained on different modalities to handle various input and output modalities, including text, images, video, and audio. C. INFERENCE OF OUR BRIDGEDIT MODEL For both video and audio generation, we apply Classifier-Free Guidance independently, leveraging separate guidance scales for each modality to fine-tune their respective generation quality and adherence to the text prompts. The guided noise prediction for each modality is given by: ˆϵv(xv, TV ) = ϵv(xv, ) + wv (ϵv(xv, TV ) ϵv(xv, )) ˆϵa(xa, TA) = ϵa(xa, ) + wa (ϵa(xa, TA) ϵa(xa, )) (11) (12) Here, xv and xa represent the noisy video and audio latents at given timestep, respectively. ϵv(xv, TV ) and ϵa(xa, TA) are the predictions from the BridgeDiT model conditioned on their respective text prompts, while ϵv(xv, ) and ϵa(xa, ) are predictions from unconditioned (null) prompts. wv and wa are the video and audio guidance scales, allowing for independent control over the trade-off between sample quality and text alignment for each modality. C.4 HYPERPARAMETERS Table 5: Key hyperparameters for our BridgeDiT model. Parameter Value Training Configuration Optimizer Learning Rate Weight Decay Adam Betas LR Scheduler LR Warmup Steps Total Training Steps Minimum Learning Rate Unconditional Probability (CFG) Training Precision Architecture Configuration AdamW 5e-5 1e-3 (0.9, 0.95) Cosine decay with linear warmup 1,000 15,000 1e-6 0.1 bfloat16 4 1536 12 Number of BridgeDiT Blocks BridgeDiT Block Channels (Q, K, V) BridgeDiT Block Heads BridgeDiT Timestep Embedding Dim 1536 Video Tower Bridge Points (Layers) Audio Tower Bridge Points (Layers) Trainable Layers (Video) Trainable Layers (Audio) [3, 7, 11, 15] [2, 5, 8, 11] Last 5 blocks Last 5 blocks Sampling Configuration Video Resolution Video Number of Frames Video Frame Rate (fps) Audio Sample Rate Audio Duration Number of Inference Steps Video Guidance Scale (CFG) 834 480 81 15 44100 Hz 5.4 seconds 50 6.0 C.5 PROMPTS OF OUR HIERARCHICAL VISUAL-GROUNDED CAPTIONING (HVGC) FRAMEWORK 17 Figure 5: Prompts for Stage1: Detailed Visual Scene Description 18 Figure 6: Prompts for Stage2: Auditory Concept Abstraction 19 Figure 7: Prompts for Stage3: Visually-Grounded Audio Caption Generation C.6 DETAILED COMMAND FOR HUMAN ANNOTATION Figure 8: Detailed Command for Human Annotation 21 C.7 EXAMPLES RESULTS FOR HVGC FRAMEWORK Table 6: Examples of captions generated by our HVGC Case1. HVGC Video Caption In dimly lit blacksmith workshop illuminated by the dramatic, warm light emanating from the forge, weathered blacksmith with soot-stained hands stands at his anvil, his face etched with concentration. The atmosphere is thick with anticipation, the air filled with the scent of heated metal and the faint glow reflecting off the walls. The blacksmith, clad in rugged leather apron, holds glowing orange piece of iron with tongs, its intense heat casting flickering shadows across the room. He raises heavy hammer above his head, muscles tensing as he prepares to strike. The hammer descends with force, colliding with the glowing iron and sending shower of bright orange sparks flying into the air; the metal visibly deforms under the powerful blow. The blacksmiths arm recoils slightly from the impact, and he quickly adjusts his grip on the tongs, turning the iron to shape it further. Each strike is precise, the hammer connecting with the metal in rhythmic dance of creation. The camera captures this intense moment from close-up angle, focusing on the intricate details of the hammers texture and the vibrant hues of the heated iron. The anvil, worn from years of use, bears the marks of countless creations, adding to the scenes authenticity. The visual counterpart of the hammers resounding impacts is evident in the way the metal bends and reshapes, each deformation testament to the blacksmiths skill. The entire scene is rendered in photorealistic, cinematic style. HVGC Audio Caption The resounding clang of heavy hammer striking metal echoes through the workshop, each powerful blow punctuated by the sizzle of heated iron and the occasional hiss of sparks. Omini Caption blacksmith hammers glowing, orange-hot metal blade on an anvil, shaping it with precise, rhythmic strikes. The intense heat radiates from the blade as sparks fly with each impact, while the sound of heavy hammering echoes in the workshop, capturing the raw power and craftsmanship of traditional metalworking. Audio-LLM Caption There was knocking sound, bit like the sound of stones. 22 Table 7: Examples of captions generated by our HVGC Case2. HVGC Video Caption In bustling subway station at Union Square, the atmosphere is alive with the vibrant energy of urban nightlife, illuminated by the stark white tiles and the soft glow of overhead fluorescent lights. young man, dressed in blue patterned shirt and dark shorts, stands as the focal point, his golden saxophone held close as he plays with fervent passion. His cheeks puff rhythmically, his fingers dance deftly across the instruments keys, and his body sways to the beat, creating visual counterpart to the musics lively tempo. Behind him, another musician, clad in black, sits with large metallic drum, his hands moving in blur as they strike the surface, sending vibrations through the air that ripple like invisible waves. The camera pans smoothly, capturing the dynamic interplay between the musicians and their instruments, emphasizing the physicality of their performance. Passersby weave through the scene, some pausing to listen, their presence adding layers of depth and movement. The walls are adorned with advertisements, one featuring woman in casual attire, her image static yet contributing to the urban tapestry. The floor beneath the musicians feet shows signs of wear, hinting at the constant flow of people and the passage of time. The visual style is crisp and photorealistic, with high contrast and sharp details that bring every element into vivid focus, creating highly detailed, 8K representation of this captivating moment in the heart of the city. HVGC Audio Caption The rich, soulful notes of saxophone fill the bustling subway station, blending with metal drum and the ambient hum of the urban environment. Omini Caption In the bustling underground corridor of Union Square, two street musicians ignite the space with raw energy saxophonist in blue patterned shirt dances while blowing soulful notes from his golden baritone, and drummer behind him pounds rhythmically on snare strapped to his chest. Their music echoes off white-tiled walls as commuters blur past. Audio-LLM Caption fast and cheerful piece of music, very unrestrained and passionate, like an orchestral piece."
        },
        {
            "title": "D ABLATION STUDY ON BRIDGEDIT BLOCK PLACEMENT",
            "content": "Table 8: Ablation study on the placement of BridgeDiT Blocks. Performance is highest when interaction is focused on the early-to-mid layers of the architecture. Placement Strategy Video Layers Audio Layers IB-VA AV-Align Early Layers Middle Layers Late Layers Uniform Uniform (Early Bias) [0, 1, 2, 3] [13, 14, 15, 16] [27, 28, 29, 30] [6, 12, 18, 24] [3, 7, 11, 15] [0, 1, 2, 3] [10, 11, 12, 13] [21, 22, 23, 24] [2, 8, 13, 18] [2, 5, 8, 11] 28.30 31.89 19.32 33.65 34.59 0.2223 0.2481 0.1831 0.2502 0.2746 To understand the impact of the interaction modules placement, we conducted an ablation study by inserting four BridgeDiT Blocks at different stages within the dual-tower architecture. We evaluated five distinct placement strategies: concentrating the blocks in the early, middle, or late layers, as well as two uniform distribution strategies. The results, presented in Table 8, reveal clear trend. The Uniform (Early Bias) strategy, where blocks are inserted uniformly across the first half of the network layers, yields the best performance on both the ImageBind (IB-VA) and AV-Align metrics. Performance is strongest when interaction occurs in the early-to-mid layers, as seen in the Middle Layers and Uniform configurations. Conversely, concentrating the interaction exclusively in the deepest, final layers (Late Layers) results in significant degradation of performance. This suggests that for achieving robust audio-visual synchronization, the most critical feature exchange occurs at the early and intermediate representational stages. We hypothesize that these layers contain the optimal balance of detailed spatial-temporal information (from early layers) and abstract semantic concepts (from middle layers). Relying only on the highly abstract features from the final layers is insufficient for the precise alignment required for the T2SV task."
        },
        {
            "title": "E LIMITATION AND FUTURE WORK",
            "content": "E.1 LIMITATION Despite the promising results, our work has several limitations. The primary challenge, shared by the entire T2SV field, is the scarcity of large-scale, high-quality, and well-annotated audio-video data. Our methods performance is highly dependent on data quality; unstable or low-resolution videos can degrade the capabilities of the pre-trained backbones, while noisy audio or the presence of out-of-frame sounds complicates the learning of precise synchronization. Our data filtering and hierarchical captioning are steps to mitigate this, but the need for better datasets remains. Furthermore, the current version of BridgeDiT is focused exclusively on generating sound effects. It does not yet support speech, which would require dedicated lip-synchronization mechanisms or complex musical scores. Finally, the overall performance of our model is inherently bounded by the capabilities of the chosen foundational T2V and T2A models, suboptimal base models significantly limit the overall generation quality. E.2 FUTURE WORK DIRECTION These limitations pave the way for several exciting future directions. crucial step is the collection of larger, higher-quality audio-visual datasets, coupled with more efficient data processing pipelines for cleaning, filtering, and captioning. Building on our architecture, we plan to extend BridgeDiT to support speech and music. This will involve incorporating specialized modules for lip-synchronization and developing techniques to capture the rhythm and mood of musical inputs. Moreover, we are interested in exploring post-training refinement techniques. For instance, applying Reinforcement Learning with Human Feedback (RLHF), with rewards specifically designed to enhance audio-visual synchronization, could further improve the models temporal and semantic coherence. We believe these future steps will continue to advance the field towards the generation of truly holistic and synchronized multi-sensory experiences."
        }
    ],
    "affiliations": [
        "Apple",
        "Renmin University of China"
    ]
}