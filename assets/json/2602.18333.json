{
    "paper_title": "On the \"Induction Bias\" in Sequence Models",
    "authors": [
        "M. Reza Ebrahimi",
        "Michaël Defferrard",
        "Sunny Panchal",
        "Roland Memisevic"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite the remarkable practical success of transformer-based language models, recent work has raised concerns about their ability to perform state tracking. In particular, a growing body of literature has shown this limitation primarily through failures in out-of-distribution (OOD) generalization, such as length extrapolation. In this work, we shift attention to the in-distribution implications of these limitations. We conduct a large-scale experimental study of the data efficiency of transformers and recurrent neural networks (RNNs) across multiple supervision regimes. We find that the amount of training data required by transformers grows much more rapidly with state-space size and sequence length than for RNNs. Furthermore, we analyze the extent to which learned state-tracking mechanisms are shared across different sequence lengths. We show that transformers exhibit negligible or even detrimental weight sharing across lengths, indicating that they learn length-specific solutions in isolation. In contrast, recurrent models exhibit effective amortized learning by sharing weights across lengths, allowing data from one sequence length to improve performance on others. Together, these results demonstrate that state tracking remains a fundamental challenge for transformers, even when training and evaluation distributions match."
        },
        {
            "title": "Start",
            "content": "On the Induction Bias in Sequence Models M.Reza Ebrahimi 1 Michaël Defferrard 1 Sunny Panchal 1 Roland Memisevic 1 6 2 0 2 0 2 ] . [ 1 3 3 3 8 1 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Despite the remarkable practical success of transformer-based language models, recent work has raised concerns about their ability to perform state tracking. In particular, growing body of literature has shown this limitation primarily through failures in out-of-distribution (OOD) generalization, such as length extrapolation. In this work, we shift attention to the in-distribution implications of these limitations. We conduct largescale experimental study of the data efficiency of transformers and recurrent neural networks (RNNs) across multiple supervision regimes. We find that the amount of training data required by transformers grows much more rapidly with statespace size and sequence length than for RNNs. Furthermore, we analyze the extent to which learned state-tracking mechanisms are shared across different sequence lengths. We show that transformers exhibit negligible or even detrimental weight sharing across lengths, indicating that they learn length-specific solutions in isolation. In contrast, recurrent models exhibit effective amortized learning by sharing weights across lengths, allowing data from one sequence length to improve performance on others. Together, these results demonstrate that state tracking remains fundamental challenge for transformers, even when training and evaluation distributions match. 1. Introduction State tracking is key capability of most intelligent systems and of most models of computation. It is the process of monitoring and updating the status of an entity or process with which the system interacts over period of time. State tracking is particularly important in multi-hop, interactive tasks, such as that of an agent interacting with an interface or of dialogue system interacting with user across multiple turns. State tracking has become popular area of investigation in recent years, especially in the study of LLM capabilities and failure modes. In this context, numerous studies have shown that transformer-based models are fundamentally limited in their ability to perform state tracking (for example, (Anil et al., 2022; Dziri et al., 2024)). This contrasts with recurrent networks, which excel at state tracking (although their wide-spread applicability is unfortunately hampered by their relative training inefficiency). The limitations of transformers have been demonstrated as limitations in outof-distribution (OOD) generalization, specifically lengthgeneralization: after training models on tasks encoded in sequences of given range of lengths, they were evaluated on sequences with lengths that were not seen during training. In these scenarios, the trained models fail to consistently generate correct outputs on the evaluation data, although they are able to solve the tasks for even unseen sequences in the training range of lengths. It could be argued that in any real-world use cases, OOD state tracking failures may not be an issue as long as enough training data with step-by-step sequential supervision is available. If training data covers all sequence lengths that may be encountered at inference time, inference can rely entirely on in-distribution generalization. Unfortunately, although this argument is true in principle, it is hard to quantify enough in this context. It is also hard to quantify how the amount of training data required for any given task may depend on the length of the sequences and the size of the state space. To shed light on these questions, in this work we perform detailed and systematic empirical study of the in-distribution performance of transformer-based models and contrast these with recurrent models. To this end, we train and evaluate range of representative models on range of simple state tracking tasks. Independently varying sequence length and size of the state space in these tasks then allows us to discover regularities in the dependence of generalization error on these parameters. This in turn makes it possible to obtain crude lower bound of the minimal amount of training data likely required to solve such tasks. 1Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc. Correspondence to: Reza Ebrahimi <ebrahimi@qualcomm.com>. key difference between transformer-based and recurrent models is that, at every timestep, the former compute outputs by applying function that depends on all inputs and 1 On the Induction Bias in Sequence Models outputs generated previously (the context window), making it possible, in principle, to re-calculate the required state from the past information globally at each time step. Recurrent networks, on the other hand, compute outputs by applying function that depends on only the current hidden state, making it impossible to perform such re-calculation. This makes it strictly necessary for recurrent network to encode any relevant information from the past within single hidden state vector. This inductive bias encourages recurrent network to incorporate the information from the current timestep into its representation of state at the moment where this information is available. Conversely, it discourages it from saving this information off to determine future state updates globally from the past information. The immediate state updates thus encourage the network to process the input sequence step-by-step, making any state update explicit as soon as this is possible, rather than potentially deferring such updates to later point in time. Such step-by-step state updates are natural inductive bias (Mitchell, 1997) in the context of simple state tracking tasks, as they make it possible to reduce complex multi-step dependencies to sequence of single-step computations. They also allow model to share weights across multiple different sequence lengths, as it breaks state updates into single-step, repeatable computations. By analogy to the induction step in mathematical proof, we shall refer to this kind of inductive bias in this work as induction bias (sic). We show that the degree of knowledge transfer across multiple different sequence lengths in the indistribution setting is highly correlated with the ability of model to length-generalize. 1.1. Related work range of studies has shown that transformer-based sequence models fail to length-generalize in state tracking tasks (Anil et al., 2022; Deletang et al., 2023; Dziri et al., 2024; Abbe et al., 2024; Ebrahimi et al., 2024). Unlike our work, these studies solely discuss OOD scenarios, while we discuss in-distribution data efficiency instead. The inability to length-generalize in state tracking tasks has been shown to hold also for most existing state-space models (SSM) (Sarrof et al., 2024; Merrill et al., 2024; Cirone et al., 2024; Shakerinava et al., 2026). However recent work has shown that making the hidden-to-hidden transition matrix in the SSM input-dependent and non-diagonal can recover the ability to length-generalize. (Fan et al., 2024; Grazzi et al., 2025; Ebrahimi & Memisevic, 2025; Terzic et al., 2025a;b). Liu et al. (2023); Li et al. (2025) show that transformers solve state tracking tasks in-distribution by making use of parallel mechanisms reminiscent of associative scan. While this view can help explain the OOD failures of these models, it also hints at the absence of an induction bias which affects data efficiency as we show in this work. Formally, the presence of the induction bias in model means that the joint distribution over tokens, conditioned on the most recent hidden state, factorizes, such that xt, ht), where xt is the p(xt+1 tth token and ht the hidden state in timestep t, representing minimal sufficient statistic for determining xt+1. x1, . . . , xt, ht) = p(xt+1 We show that the presence (or respectively absence) of an induction bias, or its relative strength, provides simple explanation for wide range of the empirical findings we present. Key take-aways from our study include the following: We show that there is distinct difference between the supervision regimes in which transformers and recurrent networks perform well in-distribution. We show that transformers can relatively efficiently learn state tracking tasks in-distribution on one (fixed) sequence length at time, but generalizing indistribution over multiple sequence lengths requires significantly more training data. We present evidence that, unlike recurrent networks, transformers tend to fail at sharing parameters across sequence lengths and instead learn separate solution mechanisms for different lengths. 2. Methodology Task: We consider the task of modular addition, where model is provided sequence of integers = (x1, x2, . . . , xn) with each xi drawn uniformly at random from Zm = . The objective is to compute 1 the sum of the sequence modulo m: 0, 1, . . . , } { = xi (cid:33) (cid:32) i=1 (cid:88) (mod m), xi Zm. For = 2, the task reduces to computing the parity of binary sequence. From an algebraic perspective, modular addition over Zm (cyclic group) serves as the canonical representative for commutative operations, as every finite abelian group is isomorphic to direct product of such cyclic groups. We also experiment with non-commutative operations by considering the task of permutation composition over the symmetric group S5. This task serves as the canonical noncommutative counterpart for state tracking, as by Cayleys Theorem, every finite group is isomorphic to subgroup of symmetric group (Dummit et al., 2004). For additional details and empirical results on the permutation composition task, we refer the reader to Appendix Section B.2. 2 On the Induction Bias in Sequence Models Figure 1. Example of the three task formats for the addition modulo 5 task applied to the sequence 2 1 0 3 4. Length Distributions: For each generated sample, we first determine the sequence length , where denotes the maximum sequence length. We then sample without replacement to ensure that sequence every sample in the dataset is unique. We use three distinct strategies for length selection: 2, . . . , { Zn } 1. Fixed: The length is held constant at = L. 2. Uniform: Lengths are sampled uniformly at random from the set 2, . . . , { . } 3. Short-to-Long: Sequences are sampled in ascending order of length, exhausting the available sequences for length before proceeding to + 1. Task Formats: We consider three task formats that vary in the density and structure of the supervision signal. Let i=1 xi) (mod m) denote the k-th partial sum of sk = ( the input sequence. The formats, illustrated in Figure 1, are (cid:80) defined as follows: 1. Outcome Supervision: The model is provided the input sequence and is trained to predict only the final sum sn. This format provides no intermediate supervision, requiring the model to discover the latent computational logic of the task on its own during training. 2. Chain-of-Thought (CoT): The model is trained to generate the sequence of intermediate partial sums (s1, s2, . . . , sn) following the input sequence. This decomposes the task into sequence of iterative applications of the operator. 3. Aligned Chain-of-Thought (ACoT): The model is tasked to output, for each input token xi, the corresponding partial sum si. While conceptually similar to the scratchpad, this format provides per-token supervision that is aligned with the input. This format is similarly used in prior work (Li et al., 2025; Zhang et al., 2025) and is also referred to as state-supervision. Unlike outcome supervision, both CoT and ACoT constitute form of process supervision, as they provide explicit training signals for the intermediate solution steps. Sample Efficiency: To quantify the data efficiency of model under specific task configuration, we define the minimal sample size required to learn the task reliably. Let denote training set of cardinality , and let ) denote the validation loss of model trained val(ϕ; using hyperparameter configuration ϕ D"
        },
        {
            "title": "L\non",
            "content": "D Φ. We consider task successfully learned if the minimum validation loss over the hyperparameter grid falls below convergence threshold: min ϕΦ val(ϕ; ) ϵ, (1) where Φ is predefined grid of learning rates and random seeds, and ϵ is convergence threshold. Formally, we define as the smallest training set size satisfying this criterion: = min : min ϕΦ val(ϕ; ) (cid:26) . (2) ϵ (cid:27) In practice, we estimate by performing binary search over the training set size. Note that both training and validation samples are drawn from the same underlying data generation process, and therefore reflects the in-distribution sample efficiency. In addition to low validation loss, we also consider perfect validation accuracy as an alternative success criterion and find no meaningful difference in the results. For the results in Section 4, we use the latter. Models: We compare multi-layer decoder-only transformer architecture (Vaswani et al., 2017), with two recurrent alternatives: Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) and dense state-space models (DenseSSMs) (Fan et al., 2024; Terzic et al., 2025a; Ebrahimi & Memisevic, 2025). In Dense-SSMs, the state transition matrix is dense and fully input-dependent, property shown to support effective state tracking in linear recurrent models (Merrill et al., 2024). We adopt the variant used by Ebrahimi & Memisevic (2025), in which inputstate interactions are purely multiplicative with no additive terms: ht = Axtht1, (3) where the transition matrix Axt is given by linear function of the input xt. This architecture is commonly referred to as bilinear RNN, since ht depends bilinearly on the input and the previous hidden state. 3 On the Induction Bias in Sequence Models Experimental Setup: We perform large-scale systematic evaluation of data efficiency on synthetic state-tracking tasks. To estimate , we use hybrid binarygeometric search procedure (see Algorithm 1) that evaluates candidate sample sizes over at most 20 steps, training models across hyperparameter grid consisting of 3 learning rates and 5 random seeds (15 configurations total for each size ). sample size is considered successful if at least one configuration achieves validation loss below ϵ = 104. Each model is trained for fixed budget of 250k optimization steps independent of the training set size . This amounts to over 190,000 training runs for the results reported in this paper, excluding development runs. The transformer model used is based on the GPT-2 architecture (Radford et al., 2019) with 6 layers and model (embedding/hidden) dimension of 256. Both the LSTM and Dense-SSM models use single-layer recurrent cell followed by linear classification head. We use input and hidden dimension of 768 for LSTM and 256 for the Dense-SSM. Additionally, we experiment with 2-layer transformer and LSTM with hidden dimension of 256, with results provided in the Appendix Section B.3. We ensure that the training and validation sets are strictly disjoint. The validation set contains 2,000 samples (or at most 20% of the available data) and remains identical across different training set sizes, except for variations introduced by the random seed. In addition, we always use at most 20% of the available samples at each sequence length for validation, with the remainder reserved exclusively for training. Also, for all tasks, multi-digit integers are represented as single tokens during tokenization. Additional implementation details are provided in Appendix A. 3. In-distribution Data Efficiency 2M, an order-of-magnitude increase in sample complexity. Figure 2 further illustrates this gap in sample requirements for the case = 2 (parity) across the two formats. It has been hypothesized that by outputting intermediate steps autoregressively, the model can attend to its own previous outputs, effectively simulating larger depth circuit (Li et al., 2024), and the results confirm this hypothesis. In contrast, Aligned Chain-of-Thought forces the model to compress the computation into single forward pass per token without the benefit of re-attending to intermediate results, which appears less aligned with the transformers non-recurrent nature. Observation 3.2 Recurrent models prefer aligned supervision (Aligned Chain-of-Thought). Conversely, recurrent models (LSTMs and Dense-SSMs) demonstrate superior sample efficiency when trained with the Aligned CoT (ACoT) format, which provides supervision aligned with the evolution of the hidden state (see also Figure 2). In contrast, RNNs struggle with CoT, which is likely due to their recall bottleneck (Wen et al., 2025; Phan et al., 2025): model must output the sequence of partial sums (s1, . . . , sn) after processing the entire input sequence. This effectively requires it to unroll the chain of intermediate computations from the beginning. In fact, we note that under the CoT format, recurrent models even fail to generalize to longer sequences despite their sequential inductive bias (see Table 2 for length-generalization results). The task thereby becomes bottlenecked by the models limited memory capacity rather than its state-tracking ability. { { 5, 10, 20, 30 } 2, 3, 5, 10, 15, 20, 50, 75, 100 } We perform the above binary search procedure to identify the minimal dataset size (N ) across all combinations of and modumaximum sequence length lus , for each of the three task formats, length distributions, and models described earlier. The results are summarized in Table 1. For ease of comparison, we also visualize selected slices of this table in figures throughout this section. Comparable results for models of different sizes are reported in Appendix B.3. From the table we can infer the following key observations: Observation 3.1 Transformers prefer non-aligned supervision (Chain of Thought). We observe clear preference of transformers for CoT over the Aligned CoT format. For example, at = 5 and = 20, CoT requires 1.7K samples, while Aligned CoT requires Figure 2. Minimal dataset size for the uniform length distribution with = 2 (parity). RNNs favor ACoT, whereas transformers favor CoT. 4 On the Induction Bias in Sequence Models Furthermore, in the uniform setting, we find that recurrent models trained with ACoT require fewer data points as the maximum sequence length increases, as expected. In contrast, transformers trained with CoT fail to leverage this additional supervision. This trend is also evident in Figure 4, which compares sample complexity on the parity task. Observation 3.3 Recurrent models outperform transformers in the absence of intermediate supervision. In the Outcome Supervision setting, the model must implicitly infer the latent algebraic structure of the task solely from the final solution, without any guidance on the intermediate steps. This requires the model to effectively marginalize over unobserved computational paths with difficulty scaling with both the state space size and sequence length n. We observe that recurrent models significantly outperform transformers in this regime. While transformers fail to converge for all but the most trivial configurations (very small and n), the recurrent architectures successfully learn the task for higher moduli and extended sequence lengths, achieving convergence with orders of magnitude fewer training samples. Figure 3 illustrates this behavior for the parity case (m = 2). Figure 4. Sample complexity (log scale) for transformers trained with CoT and RNNs with ACoT on the parity task. RNNs exhibit the expected improvement in sample efficiency with increasing sequence length, while transformers fail to leverage the additional supervision. Observation 3.5 With outcome supervision, short sequences are more valuable for learning than long sequences in recurrent models. Figure 3. for the outcome supervision format with uniform length distribution and = 2 (parity). In the absence of intermediate supervision, single-layer RNNs significantly outperform the 6-layer transformer. Observation 3.4 With intermediate supervision, longer sequences improve the data efficiency of recurrent models but not transformers. Intuitively, under formats with intermediate supervision (CoT or ACoT), longer sequences should improve sample efficiency. This is because with intermediate solutions, the effective amount of supervised tokens increases linearly with sequence length. We validate this hypothesis in recurrent models trained with Aligned Chain-of-Thought: the fixed length distribution (comprising only longest sequences) yields the highest data efficiency, followed by uniform, and finally short-to-long. Figure 5. Sample complexity (log scale) in the Outcome Supervision format for the uniform and short-to-long length setting, with = 2 (parity). Recurrent models require fewer training samples under the short-to-long setting, indicating that shorter sequences provides stronger learning signal. In the Outcome Supervision setting, we compare the data requirements under the uniform and short-to-long length 5 On the Induction Bias in Sequence Models Table 1. Minimal number of training samples required to learn the modulo addition task. dash () indicates that the task was not learned at the maximum training set size."
        },
        {
            "title": "Uniform",
            "content": "Short-to-Long 5 10 20 30 10 20 30 5"
        },
        {
            "title": "Transformer",
            "content": "Chain-of-Thought Aligned Chain-of-Thought"
        },
        {
            "title": "LSTM",
            "content": "Chain-of-Thought Aligned Chain-of-Thought"
        },
        {
            "title": "Outcome\nSupervision",
            "content": "Dense-SSM Chain-of-Thought Aligned Chain-of-Thought 2 3 5 10 15 20 50 75 100 2 3 5 10 15 20 50 75 100 2 3 5 10 15 20 50 75 2 3 5 10 15 20 50 75 100 2 3 5 10 15 20 50 75 100 2 3 5 10 15 20 50 75 100 2 3 5 10 15 20 50 75 100 2 3 5 10 15 20 50 75 100 2 3 5 10 15 20 50 75 19 119 1.1K 10 16 33 94 166 377 1.1K 1.7K 2.5K 16 91 528 1.3K 1.5K 2.6K 13K 21.1K 23.6K 14 195 2K 6.4K 18.8K 20 1.6K 6.1K 15.8K 21.2K 43K 56.5K 84.2K 4 10 95 278 534 798 3.7K 7.4K 12.8K 13 56 323 2.7K 6.5K 15.7K 110 538 3.4K 12.6K 21.9K 2 4 18 109 219 438 3.2K 8.1K 12.5K 364 16 18 30 66 116 178 678 1.2K 1.9K 121 2K 3.9K 6K 7.9K 21.9K 1M 15.2M 101 620 5.9K 307 2.9K 10.3K 15.7K 21.1K 29.6K 50.4K 58K 9.5M 6 8 70 194 478 672 4.6K 9.2K 16.6K 58 390 3.8K 8.3M 172 1K 5.9K 2 6 9 41 109 233 1.7K 4K 6K 19 21 31 62 78 116 553 1K 1.6K 644 3.7K 9.2K 15.5K 23.3K 85.3K 252 1.7K 4.7K 9.3K 20.4K 29.1K 42K 54.3K 2 8 39 146 409 652 6K 12.2K 21.9K 33 8K 10.9M 2.1K 3 4 6 21 66 124 1.1K 3.1K 4.3K 20 25 34 70 107 135 470 946 1.6K 1.2K 4K 20.8K 131.4K 309 8.6K 14K 44K 2 6 31 130 371 798 7.4K 13.3K 23.7K 70 2.2K 1 4 6 17 37 78 874 2.1K 3K 6 37 2.6K 36 78 148 427 709 1K 2.6K 4.2K 5.8K 27 180 671 1.7K 2.4K 4.5K 22K 102.1K 8M 16 78 387 2.2K 11.8K 45 2K 14.3K 32K 40.7K 206K 4M 4 18 153 447 865 1.3K 4.1K 8.4K 13.3K 6 31 118 573 1.6K 3K 15.6K 216 1.5K 12.5K 21.2K 3 7 24 148 412 633 4.5K 10.8K 16.1K 549 198 465 1.1K 2.3K 2.5K 3.8K 8.6K 11K 13.2K 313 8.3K 14.5K 32.3K 61.5K 190.9K 21 122 707 8.6K 692 7.4K 28.4K 544.4K 67K 409.1K 2.2M 4 12 107 313 631 934 5.3K 11.3K 19.4K 27 69 285 2.2K 511 6.2K 19.5K 2 6 17 74 186 382 3.2K 6.8K 10.2K 744 1.2K 1.7K 3.4K 5K 5.6K 12K 18.1K 21.1K 2K 66.8K 2M 64 317 13.4K 40.3K 68.1K 500K 14M 2 6 80 247 567 870 6.1K 14.2K 24.3K 38 209 1K 4.5K 12.1K 2 4 10 56 101 241 2K 5.1K 6.7K 1K 1.4K 1.7K 4K 5.1K 7.4K 11.2K 16.1K 19.1K 11.7K 54 526 37.1K 5.2M 2.1M 2 8 47 232 500 820 6.9K 16.8K 28.5K 70 265 1.1K 1 4 10 27 70 116 1.6K 3.8K 6K 45 243 1.5K 39 108 647 11K 54.3K 168.6K 6.4M 19 207 909 11.2K 55K 172.6K 6.7M 12 83 381 2.2K 48 2.1K 16.8K 65.6K 215.4K 10.4M 9 27 142 685 1.5K 2.7K 14.7K 41.1K 99.7K 9 24 69 321 970 2K 11K 234 1.5K 14.8K 56.2K 2.5M 6 9 30 102 228 396 2.5K 5.6K 10K 913 824 28.8K 2.4M 856 29.7K 2.4M 9 90 22.7K 843 29K 7 28 148 1.3K 3.9K 9.4K 156.2K 697.4K 10 32 83 452 1.4K 8.4K 875 29.8K 2.4M 4 11 26 101 225 397 2.5K 5.6K 10K 1M 1M 13 90 1.1M 7 34 253 1.8K 5.2K 13.4K 12 41 132 678 3.7K 8.8K 16 100 8 34 235 2K 7.1K 25.9K 12 41 163 1.1K 3.8K 9K 4 10 25 101 225 405 2.5K 5.6K 12.8K 4 10 25 101 225 405 2.5K 5.6K 16.4K On the Induction Bias in Sequence Models distributions. Recall that these two distributions differ only in the order in which samples are presented during training. We observe that recurrent models require fewer samples in the short-to-long setting, suggesting that shorter sequences provide stronger learning signal than longer sequences for these models. This effect is illustrated in Figure 5 for the case = 2. 4. Weight Sharing Across Sequence Length key hypothesis for why recurrent networks dominate transformers with respect to data efficiency, as shown in the previous section, is that their induction bias encourages step-by-step updates to their representations of state. This, in turn, should allow the model to share the same solution mechanisms across the whole sequence length. In this section, we investigate the extent to which the learned mechanisms are shared across different sequence lengths. Specifically, we examine whether the model develops lengthspecific heuristics, effectively specialized circuits for fixed-length sequences, or whether it has internalized the inherent inductive structure of the task. The latter implies the discovery of transition operator that can be applied iteratively. Assessing the degree of this cross-length sharing is critical for understanding models inductive capacity and to generalize to sequence lengths not encountered in the training distribution. We quantify the cross-length mechanism sharing through the lens of sample efficiency. Intuitively, if model utilizes shared mechanism (e.g., transition operator) across varying lengths, the sample cost to learn the task over distribution of lengths should be significantly lower than the sum of costs to learn each length individually. This is due to the amortization of the learning cost: the data required to learn the operation at length simultaneously contributes to the models learning at length + k. Formally, we compare the total number of training examples required for model to simultaneously learn the task for (the joint task) against all sequence lengths 2, . . . , { 1 independent models, the sum of samples required by each optimized for single fixed length. Let joint denote the minimal sample size required for the joint task, and denote the minimal sample size for model trained and evaluated exclusively on sequences of length n. We define the Sharing Factor κ as: } κ = n=2 joint (cid:80) (4) The value of κ provides insight into the extent of acrosslength mechanism sharing: κ > 1 indicates mechanism sharing and amortized learning. This suggests the model has internalized 7 the inductive nature of the task, and data from one sequence length accelerates the acquisition of the task across the entire distribution. κ 1 suggests that the model learns length-specific solutions in isolation, effectively partitioning capacity into independent circuits. κ < 1 represents regime of destructive interference. In this case, the length-specific solutions compete for model capacity, making it more data-efficient to train separate models for each length than to optimize single model for the joint task. Figure 6 illustrates the sample complexity to learn addition modulo 5, for all sequence lengths (N joint), compared against the cumulative samples required by independent n=2 models trained on individual sequence lengths ( n), for . We evaluate these metrics for modular } addition with = 5 across the three previously defined task formats, and draw the following key observations. Comparable results for the permutation composition task (symmetric group S5) are reported in Appendix Section B.2. 2, . . . , 10 { (cid:80) Observation 4.1 Transformers have low sharing factor for all task formats. As demonstrated, we observe low sharing factor in transformers across all task formats, with κ 1 or κ < 1 in all cases. Notably, in the Chain-of-Thought (CoT) setting, despite being transformers most efficient task configuration, we observe an extreme case of length isolation (κ = 0.28). Observation 4. Transformers show destructive interference with CoT. The observed sharing factor of κ 1 for transformer with CoT indicates regime of destructive interference where length-specific solutions compete for model capacity, such that training on diverse length distribution is substantially less data-efficient than training independent models on each length. Observation 4.3 Recurrent networks have high sharing factors in their preferred task formats. In contrast, both recurrent models exhibit clear evidence of mechanism sharing and amortized learning across sequence lengths (κ 1) under the Outcome Supervision and Aligned Chain-of-Thought formats. However, this is no longer the case in the Chain-of-Thought format (κ 1), where the recurrent models also fail to share across the sequence lengths, likely due to the previously discussed On the Induction Bias in Sequence Models recall bottleneck. However, unlike transformers, we do not observe destructive interference in this case. Observation 4.5 OOD generalization implies high sharing factor, and vice versa. Observation 4.4 Longer sequences increase data efficiency for DenseSSMs. As noted in the previous section, the sample requirement for the Dense-SSM under ACoT decreases as the maximum sequence length increases. This indicates that through cross-length mechanism sharing, the model leverages the higher density of supervision signals in longer sequences. Interestingly, we observe consistent correlation between the sharing factor κ and length generalization: cases with 1) correspond to those in which high sharing factor (κ the model learns length-generalizable solution (see Table 2). Conversely, cases with low sharing factor (κ 1) are precisely those in which the learned solution fails to extrapolate beyond the training sequence lengths. This provides additional evidence that in-distribution data efficiency and circuit sharing are fundamental implications of length generalization in state tracking. Figure 6. Sample complexity comparison between training single model jointly across all sequence lengths and the cumulative sample complexity of independently trained models for each sequence length, together with the corresponding sharing factor, for the task of addition modulo 5. The results suggest that transformers learn largely isolated solutions for each sequence length. 8 On the Induction Bias in Sequence Models 5. Conclusions Our study indicates that state tracking poses severe challenges for transformer-based sequence models not only out-of-distribution but also in-distribution: They require extraordinarily large amounts of training data to generalize on simple tasks and require Chain-of-Thought supervision to learn in-distribution on even moderate sequence lengths. This suggests that end-to-end learning in applied agentic scenarios, such as robotics or GUI control, could be even more challenging. The fact that data requirements scale with sequence length may also help explain well-known challenges at large context lengths (context rot). We study performance across limited, albeit representative, number of models and task types. Unfortunately, the large search space over parameters (performed using binary search in our experiments) requires number of many thousand individual training runs. This makes this comparison highly computationally demanding even for the current set of models and tasks. However, as models were chosen to be simple and representative it seems very likely that the findings will persist even under slight model variations, similar to how they did in previous OOD studies."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Abbe, E., Bengio, S., Lotfi, A., Sandon, C., and Saremi, O. How far can transformers reason? the globality barrier and inductive scratchpad. Advances in Neural Information Processing Systems, 37:2785027895, 2024. Anil, C., Wu, Y., Andreassen, A., Lewkowycz, A., Misra, V., Ramasesh, V., Slone, A., Gur-Ari, G., Dyer, E., and Neyshabur, B. Exploring length generalization in large language models. Advances in Neural Information Processing Systems, 35:3854638556, 2022. Cirone, N. M., Orvieto, A., Walker, B., Salvi, C., and Lyons, T. Theoretical foundations of deep selective state-space models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. Deletang, G., Ruoss, A., Grau-Moya, J., Genewein, T., Wenliang, L. K., Catt, E., Cundy, C., Hutter, M., Legg, S., Veness, J., and Ortega, P. A. Neural networks and the chomsky hierarchy. In The Eleventh International Conference on Learning Representations, 2023. Dummit, D. S., Foote, R. M., et al. Abstract algebra, volume 3. Wiley Hoboken, 2004. Dziri, N., Lu, X., Sclar, M., Li, X. L., Jiang, L., Lin, B. Y., Welleck, S., West, P., Bhagavatula, C., Le Bras, R., et al. Faith and fate: Limits of transformers on compositionality. Advances in Neural Information Processing Systems, 36, 2024. Ebrahimi, M. and Memisevic, R. Revisiting bi-linear state transitions in recurrent neural networks. In The Thirtyninth Annual Conference on Neural Information Processing Systems, 2025. Ebrahimi, M., Panchal, S., and Memisevic, R. Your context is not an array: Unveiling random access limitations in transformers. In First Conference on Language Modeling, 2024. Fan, T.-H., Chi, T.-C., and Rudnicky, A. Advancing regular language reasoning in linear recurrent neural networks. In NAACL (Short Papers), pp. 4553, 2024. Grazzi, R., Siems, J., Franke, J. K., Zela, A., Hutter, F., and Pontil, M. Unlocking state-tracking in linear RNNs through negative eigenvalues. In The Thirteenth International Conference on Learning Representations, 2025. Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural computation, 9(8):17351780, 1997. Kingma, D. P. and Ba, J. Adam: method for stochastic optimization. CoRR, abs/1412.6980, 2014. Li, B. Z., Guo, Z. C., and Andreas, J. (how) do language models track state? In Forty-second International Conference on Machine Learning, 2025. Li, Z., Liu, H., Zhou, D., and Ma, T. Chain of thought empowers transformers to solve inherently serial problems. In The Twelfth International Conference on Learning Representations, 2024. Liu, B., Ash, J. T., Goel, S., Krishnamurthy, A., and Zhang, In The C. Transformers learn shortcuts to automata. Eleventh International Conference on Learning Representations, 2023. Merrill, W., Petty, J., and Sabharwal, A. The illusion of state in state-space models. In International Conference on Machine Learning, pp. 3549235506. PMLR, 2024. Mitchell, T. M. Machine learning, volume 1. McGraw-hill New York, 1997. Phan, B., Ebrahimi, R., Haresh, S., and Memisevic, R. Delayed attention training improves length generalization in transformerrnn hybrids. What Can(t) Transformers Do? Workshop at NeurIPS, 2025. 9 On the Induction Bias in Sequence Models Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Sarrof, Y., Veitsman, Y., and Hahn, M. The expressive capacity of state space models: formal language perIn The Thirty-eighth Annual Conference on spective. Neural Information Processing Systems, 2024. Shakerinava, M., Khavari, B., Ravanbakhsh, S., and Chandar, S. The expressive limits of diagonal SSMs for statetracking. In The Fourteenth International Conference on Learning Representations, 2026. Terzic, A., Hersche, M., Camposampiero, G., Hofmann, T., Sebastian, A., and Rahimi, A. On the expressiveness and length generalization of selective state-space models on regular languages. In Proceedings of the AAAI Conference on Artificial Intelligence, 2025a. Terzic, M., Rahimi, A., and et al. Structured sparse transition matrices to enable state tracking in state-space models. In NeurIPS 2025, 2025b. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Wen, K., Dang, X., and Lyu, K. RNNs are not transformers (yet): The key bottleneck on in-context retrieval. In The Thirteenth International Conference on Learning Representations, 2025. Zhang, D. W., Defferrard, M., Rainone, C., and Memisevic, R. Grounding code understanding in step-by-step execution, 2025. 10 On the Induction Bias in Sequence Models A. Implementation Details A.1. Search Procedure for Determining To identify the minimal sample size required for model to successfully learn target task, we use hybrid BinaryGeometric search as described in Algorithm 1. The algorithm conducts search over sample sizes, combining an initial exponential reduction phase with subsequent binary search phase. The search begins at predefined maximum sample size Nmax. For any candidate size , the algorithm trains models using multiple configurations drawn from fixed hyperparameter grid Φ. In our implementation, each evaluation consists of 15 5 random seeds). sample size is considered successful if at least one configuration model instances (3 learning rates attains validation loss below threshold ϵ, in which case we decrease the next sample size, and otherwise, the size is labeled unsuccessful and the next trial size is increased. Algorithm 1 BinaryGeometric Search for Inputs: Max sample size Nmax, Geometric Multiplier , Step limit S, Hyperparameter grid Φ, threshold ϵ Output: Minimal sample size 0 1: 2: 3: 4: step Nmax Nmax 0 5: while step < do # Lower bound (largest failed size tried so far) # Best size so far (smallest successful size tried so far) # Current candidate # Step 1: Evaluate model configuration at size Success for all ϕ Train and evaluate model with ϕ on samples val < ϵ then false Φ do"
        },
        {
            "title": "L\nif",
            "content": "val"
        },
        {
            "title": "L\nSuccess\nbreak",
            "content": "true # Step 2: Update bounds & choose next candidate if Success then 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: N if = 0: else: else // (L + ) // 2 if = Nmax then return (N + ) //"
        },
        {
            "title": "L\nN",
            "content": "step + 1 step 23: 24: return # Update the best value # Phase 1: Exponential decay # Phase 2: Binary search update # Failure: task not learned with max sample size # Update lower bound # Binary search update We use geometric multiplier of = 1000, maximum of = 20 search steps, and success threshold of ϵ = 104. The hyperparameter grid is Φ = LR { { 103, 104, 105 }} { seed { 10, 20, 30, 40, 50 , }} yielding 15 configurations per evaluation. Each model is trained for fixed budget of 250k optimization steps with batch size 64 using the Adam optimizer (Kingma & Ba, 2014), independent of the training set size . This implies maximum feasible sample size of Nmax = 250,000 64 = 16M. The maximum training set size is the minimum between the feasible 16M samples and the total number of sequences available under the specified configuration of maximum sequence length and modulus m: mL for the Fixed distribution, 11 On the Induction Bias in Sequence Models n=1 mn for the uniform and short-to-long distributions (see Section 2). The final training set size is obtained after and deducting the validation set. (cid:80) A.2. Evaluation We ensure that the training and validation sets are strictly disjoint. The validation set contains 2,000 samples (or at most 20% of the available data) and remains identical across different training set sizes, except for variations introduced by the random seed. In addition, we always use at most 20% of the available samples at each sequence length for validation, with the remainder reserved exclusively for training. Also, for all tasks, multi-digit integers are represented as single tokens during tokenization. Finally, for the Chain-of-Thought task format, validation loss is computed using teacher forcing rather than autoregressive sampling. A.3. Models The transformer model is based on the GPT-2 architecture (Radford et al., 2019), with 6 layers and model (embedding/hidden) dimension of 256. Other architectural parameters, including an MLP expansion factor of 4, follow the default GPT-2 (small) settings. Both the LSTM and Dense-SSM use single-layer recurrent cell followed by linear classification head to map the hidden state to token logits. We use an input and hidden dimension of 768 for the LSTM, and 256 for the Dense-SSM. See Ebrahimi & Memisevic (2025) for additional details on the Bilinear architecture used for Dense-SSM. We also experiment with 2-layer transformer and single-layer LSTM with hidden dimensionality of 256; sample-efficiency results for these variants are provided in the Appendix Section B.3. B. Additional Experimental Results B.1. Evaluating Length-generalization Table 2 reports accuracy on sequences of length 2 the maximum length used during training, normalized such that 0 corresponds to random chance. All models are trained using the maximum available training set size for each configuration. 12 Table 2. Accuracy on sequences of length 2 the maximum used during training, normalized such that 0 corresponds to random chance. On the Induction Bias in Sequence Models"
        },
        {
            "title": "Uniform",
            "content": "Short-to-Long 5 10 20 30 10 20 30 5"
        },
        {
            "title": "Chain of Thought",
            "content": "Aligned Chain-of-Thought"
        },
        {
            "title": "Chain of Thought",
            "content": "Aligned Chain-of-Thought"
        },
        {
            "title": "Outcome\nSupervision",
            "content": "Dense-SSM"
        },
        {
            "title": "Chain of Thought",
            "content": "Aligned Chain-of-Thought 2 3 5 10 15 20 50 75 100 2 3 5 10 15 20 50 75 100 2 3 5 10 15 20 50 75 100 2 3 5 10 15 20 50 75 100 2 3 5 10 15 20 50 75 2 3 5 10 15 20 50 75 100 2 3 5 10 15 20 50 75 100 2 3 5 10 15 20 50 75 100 2 3 5 10 15 20 50 75 100 0.00 0.01 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.02 0.01 0.01 0.01 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.01 0.01 0.00 0.00 0.00 0.09 0.05 0.61 0.13 0.92 0.00 0.00 0.00 0.00 0.14 0.03 0.01 0.01 0.00 0.00 0.00 0.00 0.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.00 0.00 0.00 1.00 0.00 0.47 0.00 0.00 0.00 0.00 0.02 0.00 0.01 0.00 0.00 0.00 0.00 0. 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.00 0.00 0.00 0.02 0.01 0.00 0.00 0.00 0.00 0.01 0.02 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.01 0.01 0.00 0.00 0.00 0.00 0.01 1.00 0.00 0.01 0.01 0.00 0.00 0.00 0.00 0.00 0.18 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.00 0.10 0.00 0.00 0.00 0.00 0.00 0.03 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.01 0.01 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.01 0.00 0.01 0.01 0.00 0.00 0.00 0. 0.00 0.00 0.01 0.00 0.01 0.00 0.00 0.00 0.00 0.60 0.12 0.01 0.01 0.01 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.01 0.00 0.00 0.00 0.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.03 0.01 0.01 0.00 0.01 0.00 0.00 0.00 0.01 0.01 0.00 0.00 0.01 0.00 0.00 0.00 0. 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.02 0.00 0.01 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.02 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.01 0.00 0.00 0.00 0.00 0. 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.00 0.00 0.02 0.01 0.01 0.00 0.00 0.00 0.00 0.02 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 13 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.01 0.00 0.01 0.00 0.00 0.00 0.01 0.01 0.00 0.00 0.00 1.00 1.00 0.88 0.86 0.50 0.00 0.00 0.00 0.00 0.11 0.03 0.04 0.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.00 0.04 0.03 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.01 0.00 0.00 0.00 1.00 1.00 0.93 0.59 0.11 0.00 0.00 0.00 0. 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.50 0.83 0.25 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.00 0.00 0.01 0.01 0.01 0.01 0.01 0.00 0. 0.00 0.00 0.01 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.03 0.00 0.00 0.01 0.00 0.00 0.00 0.00 1.00 0.96 0.84 0.99 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.01 0.00 0.00 0.00 0.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 0.18 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.02 0.01 0.00 0.01 0.01 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.01 0.00 0.00 0.00 0.00 0.02 0.99 1.00 0.13 0.01 0.00 0.00 0.00 0. 0.00 0.00 0.01 0.01 0.01 0.00 0.00 0.00 0.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.98 1.00 1.00 1.00 0.10 1.00 0.48 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.01 0.00 0.01 0.00 0.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.00 0.00 0.00 0.01 0.01 0.00 0.00 0.00 0. 0.00 0.04 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.01 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 1.00 1.00 0.98 0.26 0.17 0.02 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.97 0.95 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.00 0. 0.00 0.02 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.00 0.03 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.02 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.02 0.01 0.01 0.01 0.00 0.00 0.00 0.00 0.00 1.00 0.98 0.00 0.12 0.06 0.00 0.00 0.00 0. 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00 1.00 1.00 1.00 1.00 0.98 0.16 0.16 1.00 1.00 1.00 0.13 0.32 0.00 0.00 0.00 0.00 0.02 0.02 0.00 0.00 0.00 0.01 0.00 0.00 0.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0. 0.00 0.02 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.01 0.02 0.00 0.01 0.01 0.00 0.00 0.00 0.00 0.99 0.18 0.00 0.12 0.16 0.00 0.00 0.00 0.00 0.01 0.03 0.00 0.00 0.01 0.00 0.00 0.00 0.00 1.00 1.00 1.00 1.00 1.00 1.00 0.45 0.02 0.05 1.00 1.00 1.00 0.11 0.64 0.02 0.00 0.00 0. 0.02 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.29 0.01 0.12 0.02 0.00 0.00 0.01 0. 0.03 0.00 0.01 0.01 0.00 0.00 0.00 0.00 0.00 1.00 1.00 1.00 1.00 1.00 0.59 0.14 0.02 0.03 1.00 1.00 0.29 0.12 0.00 0.00 0.00 0.00 0.00 0.01 0.01 0.00 0.00 0.01 0.00 0.00 0.00 0.00 1.00 1.00 1.00 1.00 0.93 1.00 1.00 1.00 1.00 B.2. Permutation Composition Task On the Induction Bias in Sequence Models Task: To show our findings generalize beyond commutative operations, we consider the task of permutation composition (simulating the symmetric group Sm). Each element of the group represents permutation of the set , resulting in group of cardinality Sm is bijectively mapped to unique . Given an input sequence of permutations = (π1, π2, . . . , πn), the model is required integer token in } to compute their sequential composition: = m!. In our experimental setup, each permutation π Sm 0, 1, . . . , m! 1, . . . , } { 1 { denotes the permutation composition operator. This task significantly elevates the complexity of state tracking, as where the model can no longer rely on the order-invariance property characteristic of abelian groups. = πn πn1 π1, (5) Algebraic Significance: The symmetric group Sm serves as the canonical non-commutative structure for evaluating state tracking. Its fundamental importance is grounded in Cayleys Theorem, which states that every finite group is isomorphic to subgroup of the symmetric group SG (Dummit et al., 2004). Hence, by analyzing performance on Sm, we effectively probe the models capacity to internalize the transition dynamics of any finite discrete group. As noted in Figure 7, we observe the same patterns described in Section 4 and Figure 6, supporting the generalization of these findings and the subsequent arguments to non-commutative state-tracking tasks. Figure 7. Similar to Figure 6, but for permutation composition task (S5). The results suggest that transformers learn largely isolated solutions for each sequence length. B.3. Data Efficiency Evaluation for Smaller Models On the Induction Bias in Sequence Models Table 3. for LSTMs with 256 and 768 hidden dimensions. We observe similar trends across both model sizes."
        },
        {
            "title": "Uniform",
            "content": "10 20 30 34 122 853 12.2K 26.3K 53.1K 162.1K 351.8K 14.7M 8 17 379 677 1.3K 7.3K 12.2K 23.9K 21 122 707 8.6K 692 7.4K 28.4K 544.4K 67K 409.1K 2.2M 4 12 107 313 631 5.3K 11.3K 19.4K 67 317 686 24.6K 129.2K 68.1K 8 70 317 637 1.2K 8.7K 15.7K 25.3K 64 317 13.4K 40.3K 68.1K 500K 14M 2 6 80 567 870 6.1K 14.2K 24.3K 306.4K 8 8 257 772 1.2K 11.5K 20K 9.1M 54 526 37.1K 5.2M 2.1M 2 8 47 232 500 6.9K 16.8K 28.5K 5 26 487 3.4K 11.1K 21.7K 46.3K 64.6K 413.8K 595.1K 3.8M 4 144 477 846 1.2K 5.8K 10.3K 16K 16 78 387 2.2K 11.8K 45 2K 14.3K 32K 40.7K 206K 4M 4 18 153 447 1.3K 4.1K 8.4K 13.3K Short-to-Long 20 30 13 180 1.4K 29.9K 2.5M 7 32 148 1.2K 4K 10.1K 168.2K 1.2M 90 22.7K 843 29K 28 148 1.3K 3.9K 9.4K 156.2K 697.4K 26 132 16 581.1K 10 225 1.7K 8.4K 8 39 209 1.9K 57.6K 176.3K 194.9K 90 1.1M 34 253 1.8K 5.2K 13.4K 100 34 235 2K 7.1K 25.9K 5 12 81 5.1K 21.6K 73.7K 276.7K 8.4M 12 27 606 1.4K 2.6K 20K 42.6K 78.6K 12 83 381 2.2K 48 2.1K 16.8K 65.6K 215.4K 10.4M 9 27 142 685 1.5K 2.7K 14.7K 41.1K 99.7K"
        },
        {
            "title": "Fixed",
            "content": "5 10"
        },
        {
            "title": "LSTM",
            "content": "Dim ="
        },
        {
            "title": "Chain of Thought",
            "content": "Aligned Chain-of-Thought"
        },
        {
            "title": "LSTM",
            "content": "Dim ="
        },
        {
            "title": "Chain of Thought",
            "content": "Aligned Chain-of-Thought 2 3 5 10 15 20 50 75 100 2 3 5 10 15 20 50 75 100 2 3 5 10 15 20 50 75 100 2 3 5 10 15 20 50 75 100 2 3 5 10 15 20 50 75 2 3 5 10 15 20 50 75 100 14 151 1.3K 6.3K 21.9K 26 10.4K 19.3K 30.9K 33.7K 41.1K 46.4K 8 10 95 324 993 4.6K 9.2K 202 620 6.7K 346 5K 9.6K 14.8K 21.1K 21.6K 36.5K 50.5K 13.4M 8 11 227 502 902 5.3K 9.2K 14.5K 15.6K 14 195 2K 6.4K 18.8K 20 1.6K 6.1K 15.8K 21.2K 43K 56.5K 84.2K 4 10 95 278 798 3.7K 7.4K 101 620 5.9K 307 2.9K 10.3K 15.7K 21.1K 29.6K 50.4K 58K 9.5M 6 8 194 478 672 4.6K 9.2K 12.8K 16.6K 257 413 3.2K 20.5K 32K 15.4M 10 8 52 452 919 7.3K 13.5K 19.8K 1.7K 4.7K 9.3K 20.4K 29.1K 42K 54.3K 8 39 146 409 652 6K 12.2K 21.9K 14.9K 8 8 39 209 1K 4K 15.4K 32.6K 309 8.6K 14K 44K 2 31 130 371 798 7.4K 13.3K 23.7K 15 On the Induction Bias in Sequence Models Table 4. for transformers with 2 and 6 layers. We observe similar trends across both model depths. Short-to-Long 20 30 971 824 28.8K 2.4M 869 29.9K 2.5M 913 824 28.8K 2.4M 856 29.7K 2.4M 1M 1.1M 1M 1M 5 48 42 117 653 11.1K 54.2K 168.8K 6.5M 36 234 1.5K 11.9K 55.3K 175.5K 6.8M 45 243 1.5K 39 108 647 11K 54.3K 168.6K 6.4M 19 207 909 11.2K 55K 172.6K 6.7M"
        },
        {
            "title": "Uniform",
            "content": "5 10 20 30 5"
        },
        {
            "title": "Chain of Thought",
            "content": "Aligned Chain-of-Thought"
        },
        {
            "title": "Chain of Thought",
            "content": "Aligned Chain-of-Thought 2 3 5 10 15 20 50 75 100 2 3 5 10 15 20 50 75 100 2 3 5 10 15 20 50 75 100 2 3 5 10 15 20 50 75 100 2 3 5 10 15 20 50 75 2 3 5 10 15 20 50 75 100 134 1.5K 14 19 32 86 160 229 795 1.5K 2.1K 26 129 1.1K 2.1K 3.1K 4.2K 10.1K 21.6K 25K 19 119 1.1K 10 16 33 94 166 377 1.1K 1.7K 2.5K 16 91 528 1.3K 1.5K 2.6K 13K 21.1K 23.6K 594 26 24 31 61 97 142 542 958 1.5K 197 2.1K 6.5K 12.4K 31.2K 59.5K 364 16 18 30 66 116 178 678 1.2K 1.9K 121 2K 3.9K 6K 7.9K 21.9K 1M 15.2M 21 25 34 52 79 97 334 688 1.1K 3K 7.5K 19 21 31 62 78 116 553 1K 1.6K 644 3.7K 9.2K 15.5K 23.3K 85.3K 48 1.7K 37 93 241 600 805 1.3K 3.2K 5.1K 6.8K 37 239 1.6K 4.6K 8.2K 11.2K 15.8K 33.8K 99.8K 37 2.6K 36 78 148 427 709 1K 2.6K 4.2K 5.8K 27 180 671 1.7K 2.4K 4.5K 22K 102.1K 8M 807 214 456 893 1.8K 2.9K 3K 6.2K 15.1K 20.3K 439 11.1K 40.9K 156.1K 1.6M 549 198 465 1.1K 2.3K 2.5K 3.8K 8.6K 11K 13.2K 313 8.3K 14.5K 32.3K 61.5K 190.9K 724 1.3K 1.8K 3.6K 7.4K 8.5K 70.8K 33.6K 5.2K 744 1.2K 1.7K 3.4K 5K 5.6K 12K 18.1K 21.1K 2K 66.8K 2M 1.1K 1.7K 2K 4.1K 6.8K 10K 16M 209.2K 363.8K 1K 1.4K 1.7K 4K 5.1K 7.4K 11.2K 16.1K 19.1K 11.7K 23 31 39 54 62 78 272 600 1K 2K 20 25 34 70 107 135 470 946 1.6K 1.2K 4K 20.8K 131.4K"
        }
    ],
    "affiliations": [
        "Qualcomm AI Research"
    ]
}