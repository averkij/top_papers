{
    "paper_title": "Mirror Speculative Decoding: Breaking the Serial Barrier in LLM Inference",
    "authors": [
        "Nikhil Bhendawade",
        "Kumari Nishu",
        "Arnav Kundu",
        "Chris Bartels",
        "Minsik Cho",
        "Irina Belousova"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Speculative decoding accelerates LLM inference by using a draft model to look ahead, but gains are capped by the cost of autoregressive draft generation: increasing draft size elevates acceptance rates but introduces additional latency overhead exacerbating the speed-accuracy tradeoff. Prior methods (Medusa, Hydra, EAGLE) partially reduce draft cost but either degrade acceptance or introduce overheads that limit scaling. We present Mirror Speculative Decoding (Mirror-SD), an inference algorithm that breaks the latency-acceptance tradeoff. Mirror-SD launches branch-complete rollouts from early-exit signals in parallel with the target model's suffix and explicitly maps computation across heterogeneous accelerators (GPU and NPU) to exploit cross-device parallelism. The draft speculates forward continuations for the target to verify, while the target simultaneously speculates correction paths for the draft, converting speculation into two complementary execution pipelines. To further cut draft latency without weakening acceptance semantics, we add speculative streaming so the draft emits multiple tokens per step. This dual strategy of parallel heterogeneous execution plus multi-token speculative streaming pushes speculative decoding toward its ideal regime of high acceptance with low overhead. On SpecBench with server-scale models from 14B to 66B parameters, Mirror-SD delivers consistent end-to-end gains, achieving 2.8x-5.8x wall-time speedups across diverse tasks and a 30% average relative improvement over the strongest baseline, EAGLE3."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 1 6 1 3 1 . 0 1 5 2 : r Published as conference paper at ICLR 2026 MIRROR SPECULATIVE DECODING: BREAKING THE SERIAL BARRIER IN LLM INFERENCE Nikhil Bhendawade, Kumari Nishu, Arnav Kundu, Chris Bartels, Minsik Cho, Irina Belousova Apple {nbhendawade, knishu, kundu, cbartels, minsik, ibelousova}@apple.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Speculative decoding accelerates LLM inference with draft lookahead, but its effectiveness is bottlenecked by autoregressive draft generation: larger drafts improve acceptance yet also increase speculation latency overhead, capping speedup. Existing approaches such as Medusa, Hydra, EAGLE partially address draft inefficiency, but ultimately trade acceptance rates for reduced draft latency, or preserve acceptance at the cost of added overheads that limit scaling. Modern SoCs increasingly integrate heterogeneous accelerators, most commonly GPUs and NPUs with complementary throughput and efficiency characteristics, yet existing approaches are accelerator-agnostic and usually place both draft and target on the same type of device, which leaves cross-accelerator parallelism unused. We introduce Mirror Speculative Decoding (Mirror-SD), which breaks the latencyacceptance tradeoff by launching branch-complete rollouts from earlyexit signals in parallel with the targets suffix and by explicitly mapping computation across heterogeneous accelerators. In this design, the draft speculates forward token continuations for target to verify, while the target speculates correction paths for the draft, creating bidirectional speculative process. To further reduce draft speculation latency overhead while preserving acceptance semantics, we pair Mirror-SD with speculative streaming (SS) so the draft emits multiple tokens per step. This dual strategy of combining parallel heterogeneous execution and SS pushes speculative decoding closer to its ideal regime of high acceptance while reducing speculation overhead. On SpecBench with server-scale models from 14B to 66B parameters, Mirror-SD consistently delivers realistic end-to-end gains, achieving 2.85.8 wall-time speedups across diverse tasks representing 30% average relative improvement over the strongest baseline, EAGLE3."
        },
        {
            "title": "INTRODUCTION",
            "content": "Autoregressive (AR) large language models (LLMs) have achieved state-of-the-art performance across wide spectrum of natural language processing (NLP) tasks, yet their decoding latency remains fundamental bottleneck, particularly for real-time applications such as interactive dialogue, code generation, and on-device assistants (Brown et al., 2020; Pope et al., 2023). Speculative decoding (SD) has emerged as promising paradigm to mitigate this limitation by coupling lightweight draft model with larger, high-fidelity target model (Leviathan et al., 2023; Chen et al., 2023). In the canonical two-model SD framework, the draft model generates candidate tokens which are then verified by the target model in serial pipeline. While this approach reduces the number of target model invocations, the sequential dependency between draft and target stages limits achievable speedups. Recent works attempt to relax the serial constraints by equipping the target itself with speculative capacity. Medusa (Cai et al., 2023) equips the target with parallel decoding heads, while EAGLE (Li et al., 2024a) introduces dedicated speculation layer. However, the same tradeoff remains: larger speculative modules improve acceptance at the cost of higher draft construction latency, while smaller ones reduce overhead but lower acceptance and limit speedup. detailed discussion of related approaches is provided in Appendix A. 1 Published as conference paper at ICLR 2026 The central challenge of speculative decoding lies in reconciling these competing factors: (i) enabling parallel execution of draft and target models to eliminate serial dependencies, (ii) scaling the draft capacity to achieve higher acceptance rates without incurring proportional latency overhead, and (iii) designing bandwidth-efficient communication protocols that allow draft and target to exchange token-level feedback with minimal synchronization overhead. Achieving this balance reframes speculative decoding from primarily model-level optimization toward system-level codesign challenge, opening the path to real-time and efficient LLM inference. Modern System on Chip (SoC) architectures increasingly feature heterogeneous compute units that combine general-purpose CPUs with specialized accelerators such as GPUs and dedicated neural processing units (NPUs) (Jouppi et al., 2021; Intel Corporation, 2023; Advanced Micro Devices (AMD), 2023). This design trend enables efficient partitioning of workloads across compute substrates optimized for different performance and power trade-offs. For instance, Apples M-series chips integrate high-throughput GPU and dedicated Apple Neural Engine (ANE) (Apple Inc., 2023a;b). Similarly, Qualcomms Snapdragon 8 Gen 3 features an Adreno GPU alongside Hexagon NPU optimized for mixed-precision inference (Qualcomm Technologies Inc., 2023). This architectural heterogeneity motivates division-of-labor strategy for speculative decoding, wherein the draft model operates on the NPU exploiting its efficiency for approximate inference, while the target model executes on the GPU, which is better suited for high-fidelity, throughput-critical computation. Such partitioning leverages available NPU capacity and reduces contention on the GPU, thereby improving end-to-end latency in multi-accelerator deployments. In this work, we propose novel architecture that operationalizes this vision by partitioning speculative decoding across heterogeneous compute units, mapping draft inference onto compute-dense NPUs and target verification onto high-throughput GPUs. This design leverages underutilized accelerator capacity, overlaps execution between models, and employs token-level feedback mechanisms to maximize acceptance while minimizing draft construction latency overhead."
        },
        {
            "title": "2 SPECULATIVE DECODING: FORMALIZATION AND LIMITS",
            "content": "To ground our discussion, we first formalize standard autoregressive (AR) decoding and speculative decoding (SD), establishing the baseline needed to analyze the limits of SD precisely. Autoregressive (AR) decoding. Let denote finite vocabulary. We write x1:m for the context of length and y1:T for the response of length to be generated. decoder-only AR model with parameters θ defines the conditional distribution pθ(y1:T x1:m) = (cid:89) t=1 pθ (yt x1:m, y<t) , pθ( x1:m, y<t) = Softmax(cid:0)W, ht (cid:1), (1) where ht RH is the next-token representation at position + t, and RVH is the output head mapping hidden states to vocabulary logits (Radford & Narasimhan, 2018; Vaswani et al., 2017). Scaling inference of such models often requires distributing computation across multiple devices via tensor parallelism, which partitions per-layer parameters across devices and aggregates partial results with collectives such as ALLREDUCE (Hansen-Palmus et al., 2024; Li et al., 2024e). The per-token latency is then set by the critical path combining local compute and synchronizations. Speculative decoding (SD). Speculative decoding augments target model ftarget( ) with computationally cheaper draft model fdraf t( ) (Leviathan et al., 2023; Chen et al., 2023). At step t, conditioned on the verified prefix (x, y<t), the draft proposes γ-token window ˆyt+1:t+γ fdraf t( y<t, x) , (2) which the target then verifies left-to-right, producing the largest prefix on which both models agree: At max (cid:110) {0, . . . , γ} : r, ˆyt+j = arg max ftarget( y<t+j1, x) (cid:111) . (3) The agreed-upon tokens are committed as yt+1:t+At = ˆyt+1:t+At. If the draft and target disagree before the end of the window (At < γ), the target emits correction yt+At+1 and decoding resumes from (x, yt+At). The (window-normalized) acceptance rate is ρ(γ; ϕ, θ) = E[At] γ 2 [0, 1], (4) Published as conference paper at ICLR 2026 Figure 1: Mirror-SD verification and reuse (example with γ = 3, κ = 1). At early exit, the target (blue) emits Mt = {m1, . . . , m4} and continues to the final layer. The draft (orange) expands Mt into branch-complete continuations i0:i3 (grid). After verification, the target accepts ˆy0, ˆy1 and issues correction y2 at depth τ = 2. Reuse is possible if there exists precomputed branch whose prefix matches the accepted tokens (ˆy0, ˆy1) and whose node at depth τ equals y2 (green). Otherwise, speculation is recomputed (See Section 3.1 for the formal rule). which quantifies the expected fraction of the drafts proposals that are accepted by the target for window length γ. Let Tdraft(γ; ϕ) and Ttarget(γ; θ) denote the wall-times to produce and to verify the window in Equations (2) and (3) (the latter includes the teacher-forced roll-forward through accepted tokens). Because verification cannot begin before speculation is available, and the next speculation cannot begin before the final acceptance decision at step is known, the happen-before relation is ˆyt+1:t+γ (verification at t) ˆynext t+1:t+γ, yielding serial step latency TSD(γ; ϕ, θ) = Tdraft(γ; ϕ) + Ttarget(γ; θ). Increasing draft capacity (larger γ, deeper/wider fd) typically increases ρ but also increases Tdraft, while tiny drafts reduce Tdraft but suffer low ρ (Leviathan et al., 2023; Chen et al., 2023). Equation (5) exposes the core limitation: improvements in acceptance must compensate for the added draft latency, intrinsically coupling acceptance with latency. (5)"
        },
        {
            "title": "3 MIRROR SPECULATIVE DECODING",
            "content": "We propose Mirror Speculative Decoding (Mirror SD), systemsalgorithm co-design that enables parallel draft-target execution by conditioning the draft on intermediate target-layer distributions and reconciling via bandwidth-light token channel. This section develops the method end-toendformal semantics, latency models, and realizable tensor-parallel implementation. 3.1 EARLY-EXIT PROXIES AND BRANCH-COMPLETE CONCURRENT SPECULATION. Consider target transformer of depth with layers L1, . . . , LN and intermediate representations h(ℓ) at step t. Applying the LM head WLM to an intermediate state yields proxy next-token distribution p(ℓ)( y<t, x) = Softmax(cid:0)WLM h(ℓ) (6) which is typically strongly correlated with the final distribution p(N )( y<t, x) (Pal et al., 2023a). We designate an early-exit layer ℓe {1, . . . , 1} and expose low-bandwidth token channel: ℓ < N, (cid:1), Mt = Top-κ(cid:0)p(ℓe)( y<t, x)(cid:1) = {(vi, log pi)}κ i=1, vi V, (7) 3 Published as conference paper at ICLR 2026 containing only the top-κ candidate tokens and their log-probabilities. While this message is sent, the target continues its verification pass through Lℓe+1, . . . , LN to form the full next-token distribution p(N )( y<t, x). Let γ denote the speculative window length. Given Mt, the draft begins branch-complete rollout in parallel: for each candidate vi and for every prefix length γ, it prepares speculative continuation for the next step of decoding starting from vi, {1, . . . , κ}, {1, . . . , γ} : ˆy(i) t+1:t+r fd( y<t, x, yt+1 = vi) . (8) While the drafts batched branches run, the target finishes verification against the currently selected draft path under the standard speculative rule and determines the first mismatch (the correction). Formally, let At max be the accepted prefix length, where ytarg (greedy/stochastic sampling). If At < γ, the correction occurs at index τ = At +1 with token {0, . . . , γ} : ˆyt+j = ytarg t+j t+j are the targets tokens obtained from p(N )( y<t+j1, x) (cid:111) (cid:110) ct+τ ytarg t+τ p(N )( y<t+τ 1, x). Let Tt be the hypothesis tree built at early exit from the top-κ roots {vi}, whose nodes at depth store the token at position + and its precomputed continuation. Verification vs. reuse criterion. At step t, the target accepts prefix of length At and issues correction at τ = At+1 with token ct+τ . The early-exit message Mt induces hypothesis tree Tt rooted at the top-κ candidates, with Pathsr(Tt) denoting all root-to-depth-r prefixes, which serve as anchors for speculative continuations. The accepted prefix is Πt = (ytarg ), and the corrected prefix extends it with the correction token, Π+ = (Πt, ct+τ ). Reuse occurs whenever this corrected prefix already appears as path in Tt, i.e. t+1 , . . . , ytarg t+At so that only the correction must be checked while the accepted positions 1:At remain fixed. Operational selection of the next window. Π+ Pathsτ (Tt), ˆy t+1:t+γ = branch rooted at ct+1, precomputed continuation at depth τ along Πt, At 1 Π+ fresh rollout from (y1:t+At, ct+τ ), otherwise. At = 0 : vi = ct+1, Pathsτ (Tt), In all cases, the committed output is ytarg t+1:t+At , after which decoding advances to the next step. Effect of sampling width at early exit. Let q() = p(N )( ht) and p() = p(ℓe)( ht). We denote the top-κ mass overlap as: Ωκ = (cid:88) q(y). (9) It follows that P(cid:0)yt+1 Top-κ(p)(cid:1) = Ωκ, which is nondecreasing in κ and satisfies limκV Ωκ = 1. Larger κ therefore reduces fallbacks requiring speculation recomputation and improves throughput, while leaving acceptance semantics intact (See Section B). yTop-κ( p) 3.2 DRAFT EXECUTION WITH SPECULATIVE STREAMING For the draft model fd, we employ Speculative Streaming (SS) (Bhendawade et al., 2024), speculative mechanism that verifies previously proposed tokens while generating new speculative tokens in the same forward pass using multi-stream attention. Applying SS to the target would modify its decoding dynamics and alter the final distribution p(N )( y<t, x) (Bhendawade et al., 2024), breaking the lossless guarantee established in Section B. In contrast, using SS on the draft accelerates speculation generation without changing acceptance semantics, since all commitments still require verification against the unchanged target. This design leverages SS precisely where it yields additional concurrency while preserving correctness (See Section B). Section D.2 illustrates the SS mechanism and compares draft-only speedups between vanilla and SS drafts. Published as conference paper at ICLR 2026 Figure 2: Heterogeneous sharding in Mirror-SD. The target (blue) uses Megatron-style TP with two collectives per MHA/MLP block, while the draft (orange) uses SPD-style sharding across GD NPUs with only two synchronizations per step. This design reduces sync cost, enlarges draft capacity, and improves acceptance without raising critical-path latency. Note: The beige bands labeled AllReduce Draft + Target are visual shorthand: the draft and target perform separate all-reduces within their own device groups, with no cross-collective coupling. Multi-stream attention (MSA) factorization. Let (ℓ) denote the main-stream hidden state at layer ℓ and step t, and S(ℓ) t,j the hidden state of lookahead stream {1, . . . , γ}. Speculative streaming (SS) constructs attention masks so that each St,j attends to the verified prefix and to lower-index lookahead streams {St,1, . . . , St,j}, while the main stream Mt attends only to the verified prefix. At the top layer, shared LM head (d) LM projects these hidden states to token logits: (d) LM (N ) (cid:55) pd( ht) and (d) LM S(N ) t,j (cid:55) pd( ht, j), = 1, . . . , γ. so single forward pass yields both the distribution used to verify the prior draft and the distributions needed to grow the next speculative window across multiple lookahead depths. SS trains these streams with future n-gram prediction objective without introducing additional heads. Work-conserving draft generation within Mirror-SD. Within each Mirror-SD step, the draft must furnish branch-complete speculative window of length γ at the rendezvous ( Section 3.1). Under SS, single draft internal step can emit ηj 1 tokens by verifying the prior proposal and predicting multiple future tokens in one pass (Bhendawade et al., 2024). Consequently, the number of draft steps required to materialize γ tokens satisfies (cid:108) γ η (cid:109) , η = 1 (cid:88) j=1 ηj. 3.3 HETEROGENEOUS SHARDING OF MIRROR-SD We co-schedule depthN target on GT =8 GPUs and depthNd draft on GD=8 NPUs. The target is pre-trained model and thus kept in its standard Megatron-style tensor parallel (TP) form (Shoeybi et al., 2019), ensuring compatibility with existing inference stacks and KV-cache layouts. In contrast, the draft is trained from scratch using the SPD architecture (Kim et al., 2025) and deployed on NPUs. We write for permicrobatch sequence length, for microbatch size, and for vocabulary size. Figure 2 illustrates the heterogeneous sharding setup with an example configuration (target of 8 layers, draft of 4 layers); in practice, both target and draft may use different depths based on the experiment configuration. 5 Published as conference paper at ICLR 2026 Target sharding We use Megatron-style TP on the target: column-parallel Wqkv and Wo in MHA, and column/row-parallel W1, W2 in the MLP. Each transformer block performs the standard two TP collectives (attention and MLP). At early exit ℓe, the target emits Top-κ(cid:0)p(ℓe)(cid:1) over the token channel while continuing the verification phase; acceptance remains decided against p(N ) and is therefore unchanged relative to vanilla SD (See Section B). Draft sharding. The draft is trained with SPD architecture (Kim et al., 2025). We divide the Nd layers into two contiguous segments. Within each segment we instantiate GD parallel tracks; track {1, . . . , GD} is pinned to NPU and advances through all layers of its segment using resident weight shard. There is no inter-NPU traffic inside segment (See Figure 2). At the segment boundary, all tracks perform single global synchronization to re-align tensor partitions, and second synchronization occurs at the end of the forward pass to assemble full-width logits for the main and lookahead streams. Each internal draft step executes two all-reduce collectives on activation shards while weights remain sharded. This replaces per-layer synchronization with fixed two-collective cost, reducing latency and enabling more parameters to be sharded across NPUs. In practice, this expands draft capacity and improves acceptance rates ρ(γ; ϕ, θ) without increasing critical-path latency. Cross-accelerator rendezvous. Mirror-SD performs two token-level exchanges per step: earlyexit (ℓe) and final verification (N ). These exchanges carry O(B κ) small items (IDs and logprobabilities) and are negligible in practice (microseconds) compared to millisecond-scale target/draft compute; they are accounted for by Trv in the latency model. 3.4 LATENCY ANALYSIS Let the target early-exit at layer ℓe in depth-N stack with perlayer times cℓ, and write 1:ℓe target = ℓe(cid:88) ℓ=1 cℓ, ℓe+1:N target = (cid:88) cℓ. ℓ=ℓe+1 Let γ be the speculative window length and let gen draft(γ) denote the time to produce branchcomplete draft window (absorbing any multi-token SS steps). We account for the two rendezvous overheads at early exit and final verification, where the GPUNPU token exchanges carry only O(Bκ) IDs/log-probabilities. (ee) rv , (fv) rv , Trv (ee) rv + (fv) rv , single Mirror-SD step consists of (i) target prefix, (ii) early-exit rendezvous, (iii) parallel region where the target suffix overlaps the draft generation, and (iv) final rendezvous. The step latency is rv + max(cid:8)T ℓe+1:N draft(γ)(cid:9) + (fv) rv . TMirror = 1:ℓe target + (ee) , gen (10) target Let the overlap budget be ℓe+1:N under the target suffix and target . If gen draft(γ) , the entire draft generation is hidden TMirror = Ttarget + Trv. Otherwise the draft dominates the parallel region and target + (ee) TMirror = 1:ℓe rv + gen draft(γ) + (fv) rv . Thus, scaling the draft that only increases overlapped gen draft(γ) is free up to budget , while the token-channel transfers remain small O(Bκ) term. We provide full accounting of sampling/transfer costs, multi-step SS, and synchronization in Section C."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "We evaluate Mirror-SD on broad suite of generation workloads under realistic serving constraints, using server-scale decoder-only LLMs that are routinely deployed in production inference stacks across mid to large capacities, and we compare against strong speculative-decoding baselines. 6 Published as conference paper at ICLR"
        },
        {
            "title": "4.1 EVALUATION PROTOCOL",
            "content": "Datasets and tasks. We integrate our approach with the open-source SpecBench framework (Xia et al., 2024b) to ensure fair, reproducible comparison against prior methods. SpecBench provides standardized prompts and pre/post-processing, sampling settings and released configs and seeds (Xia et al., 2024b). We report results on multi-turn interactive conversation (MT Bench), translation, summarization, mathematical reasoning, machine translation and retrieval-augmented generation (RAG). Context and generation lengths follow the SpecBench protocol (Xia et al., 2024b). Models and baselines. We evaluate Mirror-SD on server-scale targets that are deployable in production inference stacks: Qwen3-14B and Qwen3-32B (Yang et al., 2025), Mistral-24B (Mistral, 2025), and OPT-66B (Zhang et al., 2022). For Qwen targets, we train 0.6B-parameter draft with 2 segments and 8 tracks and deploy it on 8 NPUs as described in Section 3.3. For Mistral we train 0.5B draft, and for OPT we train 200M draft, both sharded as in Section 3.3 to optimize synchronization cost. All draft models are trained with SS objective described in (Bhendawade et al., 2024) on UltraChat (Ding et al., 2023). Across all target models, drafts are launched from the mid-layer early exit ( 1 2 of total depth) with top-κ=8 under batch size 1. Please refer to Section for the effects of early-exit depth and κ. Baselines include vanilla SD, Medusa (Cai et al., 2024), Hydra (Ankner et al., 2024b), EAGLE 2/3 (Li et al., 2024b; 2025a), Recycling (Luo et al., 2024), PLD (Saxena, 2023a), SpS (Joao Gante, 2023), REST (He et al., 2024), and Lookahead (Fu et al., 2023). All baselines have public implementations in SpecBench (Xia et al., 2024b), and we use the corresponding implementations. Metrics. We focus solely on efficiency, without reporting accuracy metrics, since Mirror-SD is lossless and guarantees identical outputs to the target model under the same decoding process (see Section B). Our two key metrics are: (i) end-to-end wall-time speedup over target-only autoregressive decoding, reported as speedup factor; and (ii) acceptance length, the expected number of tokens accepted per speculative window, averaged across steps and prompts. We report greedy decoding with temperature τ = 0 and stochastic decoding with τ = 1. The same decoding hyperparameters are used for all methods. Serving configuration and reproducibility. Target models are distributed across eight M2 Ultra GPUs using Megatron-style tensor parallelism (Section 3.3), while the draft runs on eight NPUs (Apple Inc. (2023a)). All evaluations use fixed batch size of 1 and speculative window length γ=7; please refer to Section D.1 for analysis of batching effects. The token channel transmits only the top-κ token IDs and log-probabilities in bf16. For determinism, interconnects are pinned and frequency scaling is disabled. Timings include compute, collectives, and rendezvous overhead. 4.2 TRI-OBJECTIVE ANALYSIS WITH AN MT-BENCH DIAGNOSTIC draft(γ) with = ℓe+1:N the speculative window γ, the acceptance length Speculative decoding couples three quantities: E[At] = γ ρ(γ; ϕ, θ), and the per-speculation-step latency. In vanilla SD, enlarging γ typically boosts acceptance but also increases draft construction time, yielding an upward-sloping latency curve. For Mirror-SD, the step latency follows the model in Section 3.4 (Equation (10)): as long as gen , increasing γ (and thus E[At]) adds no marginal latency; once gen draft(γ) > , latency grows by the excess beyond . Acceptance semantics remain unchanged ( Section B). We validate these hypotheses on MT-Bench (Bai et al., 2024) by sweeping γ, measuring E[At] and the observed draft construction time, and comparing three methods that share the same target: (i) vanilla SD with autoregressive drafts from 12M to 1.7B parameters, (ii) Mirror-SD with 0.6M draft, and (iii) Mirror-SD with speculative-streaming draft (Section 3.2) of 0.6B. Figure 3a places γ on the x-axis, E[At] on the y-axis, and draft construction latency on the z-axis. target Findings. Vanilla SD traces an ascending surface: larger drafts increase E[At] but raise step latency commensurately. Mirror-SD shifts this surface downward by overlapping draft generation on NPUs with target verification on GPUs, revealing near-zero-slope regime wherever gen draft(γ) . Adding speculative streaming further reduces gen draft(γ) by requiring fewer internal draft steps to cover the same window length γ, which extends the near-zero-slope region and pushes the surface down again. Across γ, Mirror-SD and Mirror-SD+SS dominate the Pareto frontierachieving higher E[At] at given latency, lower latency at given E[At], and wider feasible range before saturating the overlap budget defined in Section 3.4. 7 Published as conference paper at ICLR 2026 (a) Tri-objective diagnostic on MT-Bench. (b) Acceptance length across SpecBench and MT Bench tasks with 0.6B draft and 32B Target. MT Bench tasks are reported individually. Figure 3: (a) Speculative window γ, acceptance length, and latency tradeoffs on MT Bench. (b) Acceptance length E[At] across tasks from SpecBench and MT Bench (mean std). Table 1: SpecBench wall-time speedups. Mirror-SD outperforms prior methods across models, tasks, and decoding temperatures, showing consistent improvements. Model Task EAGLE3 EAGLE2 Hydra Recycling Medusa Vanilla-SD PLD SpS REST Lookahead Mirror-SD Qwen3-14B (T=0) Qwen3-14B (T=1) Qwen3-32B (T=0) Qwen3-32B (T=1) Translation Summarization Question Answering Mathematical Reasoning Retrieval Aug. Generation Multi-turn Conversation Translation Summarization Question Answering Mathematical Reasoning Retrieval Aug. Generation Multi-turn Conversation Translation Summarization Question Answering Mathematical Reasoning Retrieval Aug. Generation Multi-turn Conversation Translation Summarization Question Answering Mathematical Reasoning Retrieval Aug. Generation Multi-turn Conversation 2.53x 2.91x 3.09x 3.36x 2.66x 3.29x 1.92x 2.84x 2.61x 3.25x 2.53x 3.05x 2.52x 2.98x 2.76x 3.77x 2.65x 3.29x 2.36x 2.79x 2.34x 3.45x 2.34x 3.14x 1.98x 2.19x 2.39x 2.75x 2.13x 3.05x 1.81x 2.05x 2.00x 2.54x 1.86x 2.78x 2.10x 2.59x 2.26x 3.49x 2.22x 3.24x 1.79x 2.22x 2.09x 3.13x 1.96x 2.58x 2.03x 2.00x 2.19x 2.53x 2.04x 2.45x 1.81x 1.66x 1.85x 2.42x 1.59x 2.16x 2.14x 1.98x 2.17x 2.52x 1.92x 2.75x 1.90x 1.75x 1.72x 2.35x 1.79x 2.29x 1.86x 2.30x 2.13x 2.58x 2.06x 2.44x 1.78x 1.84x 1.84x 2.29x 1.89x 2.15x 1.57x 1.98x 1.63x 1.95x 1.61x 1.79x 1.40x 1.48x 1.46x 1.80x 1.50x 1.63x 1.65x 1.55x 1.62x 2.12x 1.64x 1.93x 1.54x 1.40x 1.37x 2.01x 1.47x 1.81x 1.56x 1.56x 1.81x 2.23x 1.59x 1.92x 1.42x 1.45x 1.61x 1.66x 1.35x 1.73x 2.34x 1.76x 1.81x 2.80x 2.02x 2.07x 2.19x 1.50x 1.36x 2.53x 1.68x 1.98x 2.74x 2.07x 2.06x 3.33x 2.33x 2.67x 2.43x 1.92x 1.89x 2.88x 2.08x 2.39x 1.18x 1.15x 1.21x 2.12x 1.87x 1.38x 1.14x 1.31x 1.61x 1.67x 1.59x 1.15x 1.67x 1.75x 1.57x 1.63x 1.81x 1.49x 1.07x 1.04x 1.08x 1.86x 1.40x 1.20x 1.04x 1.18x 1.28x 1.49x 1.42x 1.05x 1.56x 1.60x 1.30x 1.42x 1.41x 1.37x 1.09x 1.24x 1.15x 1.82x 1.62x 1.38x 1.17x 1.59x 1.70x 1.68x 1.70x 1.33x 1.42x 1.69x 1.76x 1.53x 1.65x 1.63x 1.03x 1.09x 1.03x 1.59x 1.43x 1.16x 1.04x 1.37x 1.44x 1.36x 1.59x 1.20x 1.28x 1.35x 1.48x 1.34x 1.48x 1.47x 1.09x 1.30x 1.27x 1.70x 1.32x 1.35x 1.03x 1.13x 1.15x 1.39x 1.07x 1.24x 1.12x 1.26x 1.13x 1.49x 1.15x 1.33x 1.05x 1.17x 1.04x 1.28x 1.07x 1.17x 4.13x 3.07x 3.18x 5.32x 3.49x 3.70x 3.89x 2.81x 2.80x 5.02x 2.95x 3.48x 3.72x 3.14x 3.04x 5.84x 3.42x 3.59x 3.15x 2.92x 2.90x 5.08x 3.33x 3.28x 4.3 EFFECTIVENESS Table 1 reports end-to-end wall-time speedups across SpecBench (Xia et al., 2024b) tasks. clear pattern emerges: Mirror-SD shows improvements over baselines across model sizes, temperatures, and workloads. On Qwen3-14B, Mirror-SD averages 3.8 acceleration with greedy sampling, compared to 2.97 for the strongest prior methods; on Qwen3-32B, the average rises to 3.78, eclipsing baselines at roughly 3. The gains are most pronounced on long-horizon workloads (e.g., mathematical reasoning), where Mirror-SD reaches up to 5.84 speedup. The improvement is driven 8 Published as conference paper at ICLR 2026 (a) OPT (b) Mistral Figure 4: Speedup for OPT and Mistral under drafting strategies across tasks and temperatures. primarily by larger acceptance length E[At]: Mirror-SD lets us scale the draft and apply speculative streaming without paying proportional step latency, which increases the number of tokens committed per target step. Since throughput scales roughly with the expected tokens accepted per step, 1 + E[At], these acceptance gains translate directly into wall-time speedups. Retrievalaugmented generation shows similar effect, benefitting from stable intermediate distributions that allow the draft to sustain long accepted prefixes. Even on high-entropy domains such as multi-turn conversation, where acceptance is intrinsically harder, Mirror-SD consistently delivers 3.33.7 acceleration compared to the 1.8-2.4 range of Hydra, Recycling or Medusa. In translation and QA, the margin is steadier but no less striking: Mirror-SD maintains speedup edge across both greedy and stochastic decoding, validating that its improvements are insensitive to decoding regime. For an intuition grounded in the concurrency model and scaling laws behind Figure 3a, see Section C. 4.4 GENERALIZABILITY ACROSS MODEL FAMILIES To test whether the gains of Mirror-SD extend beyond Qwen, we repeat the study on two server-scale decoder-only families: Mistral-24B and OPT-66B. For each target, we hold decoding hyperparameters and draft capacity fixed and compare four variants: (1) standard speculative decoding with an autoregressive draft, (2) standard speculative decoding with speculative-streaming draft, (3) Mirror-SD with an autoregressive draft, and (4) Mirror-SD with speculative-streaming draft. Figure 4 reports end-to-end speedups over target-only decoding for translation, summarization, and multi-turn conversation under τ = 0 and τ = 1 regimes. Across both families and all tasks, the vanilla SD baseline with autoregressive-draft generation yields the smallest gains; adding speculative streaming increases throughput; switching to Mirror-SD produces further jump; combining Mirror-SD with speculative streaming delivers the largest speedups. This progression matches the analysis in Sections 3.2 and 3.4: Mirror-SD shortens the critical path by overlapping draft generation with the target suffix, while speculative streaming reduces the draft generation time gen draft(γ) by emitting multiple tokens per internal draft step. Together, these effects allow larger acceptance lengths E[At] without additional step latency until the overlap budget is reached, and the targets output distribution remains unchanged by construction. These results show that pairing Mirror-SD with speculative-streaming draft generalizes across model families, delivering higher throughput without altering the base architecture or quality."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We introduced Mirror Speculative Decoding (Mirror-SD), systemsalgorithm co-design that overlaps target and draft computation, reduces draft synchronizations, and confines cross-accelerator traffic to lightweight token channel. Deployed on heterogeneous GPUNPU setups, Mirror-SD consistently accelerates decoding by 2.8X to 5.8X while preserving correctness. By reducing serial bottlenecks and leveraging multi-accelerator SoCs, Mirror-SD demonstrates practical low-latency approach for large-scale LLM serving. 9 Published as conference paper at ICLR"
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin et al. Phi-3 technical report: highly capable language model locally on your phone, 2024. Advanced Micro Devices (AMD). Introducing Ryzen AI: AI Engine Powered by XDNA Architecture, 2023. URL https://www.amd.com/en/processors/ryzen-ai.html. Accessed: July 2025. Megha Agarwal, Asfandyar Qureshi, Nikhil Sardana, Linden Li, Julian Quevedo, and Daya Khudia. Llm inference performance engineering: Best practices., 2023a. Rishabh Agarwal, Nino Vieillard, Piotr Stanczyk, Sabela Ramos, Matthieu Geist, and Olivier Bachem. Gkd: Generalized knowledge distillation for auto-regressive sequence models. arXiv preprint arXiv:2306.13649, 2023b. Joshua Ainslie, Santiago Ontanon, Yi Tay, James Lee-Thorp, Michiel de Jong, Yinhan Yang, Dustin Tran, Jason Lee, Huaixiu Steven Chen, and Mandy Guo. Colt5: Faster long-range transformers with conditional computation. Transactions of the Association for Computational Linguistics (TACL), 11:551568, 2023. Zachary Ankner, Rishab Parthasarathy, Aniruddha Nrusimha, Christopher Rinard, Jonathan RaganKelley, and William Brandon. Hydra: Sequentially-dependent draft heads for medusa decoding, 2024a. Zachary Ankner, Rishab Parthasarathy, Aniruddha Nrusimha, Christopher Rinard, Jonathan RaganKelley, and William Brandon. Hydra: Sequentially-dependent draft heads for medusa decoding, 2024b. Anonymous. Designing draft models for speculative decoding. In Submitted to ACL Rolling Review - April 2024, 2024a. URL https://openreview.net/forum?id=mACk3ZVHoU. under review. Anonymous. Faster speculative decoding via effective draft decoder with pruned candidate tree. arXiv preprint under ACL ARR 2024, December 2024b. URL https://openreview.net/ forum?id=acl-676. ACL ARR 2024 December Submission 676. Apple. Use writing tools on your mac, n.d. URL https://support.apple.com/guide/ mac-help/use-writing-tools-mchldcd6c260/mac. Accessed: 2025-02-09. Apple Inc. Apple unveils m2 ultra, the worlds most powerful chip for personal computer. https: //www.apple.com/newsroom/2023/06/apple-introduces-m2-ultra/, 2023a. Accessed: 2025-09-22. Apple Inc. Apple unveils M3, M3 Pro, and M3 Max: The most advanced chips for personal computer, 2023b. URL https://www.apple.com/newsroom/2023/10/ apple-unveils-m3-m3-pro-and-m3-max. Accessed: July 2025. Apple Inc. About the apple thunderbolt pro cables. https://support.apple.com/en-us/ 118204, 2024. Accessed: 2025-09-24. Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, and Wanli Ouyang. MT-bench-101: fine-grained benchmark for evaluating large language models in multi-turn dialogues. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 74217454, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.401. URL https://aclanthology.org/2024.acl-long.401/. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. 10 Published as conference paper at ICLR 2026 Nikhil Bhendawade, Irina Belousova, Qichen Fu, Henry Mason, Mohammad Rastegari, and Mahyar Najibi. Speculative streaming: Fast llm inference without auxiliary models. arXiv preprint arXiv:2402.11131, 2024. Nikhil Bhendawade, Mahyar Najibi, Devang Naik, and Irina Belousova. M2r2: Mixture of multirate residuals for efficient transformer inference. arXiv preprint arXiv:2502.02040, 2025. URL https://doi.org/10.48550/arXiv.2502.02040. Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, and Judy Hoffman. Token merging for efficient vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 12161225, 2023. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Warren Burton. Speculative computation, parallelism, and functional programming. IEEE Transactions on Computers, 100(12):11901193, 1985. Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, and Tri Dao. Medusa: Simple framework for accelerating llm generation with multiple decoding heads. https://github.com/ FasterDecoding/Medusa, 2023. Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, De huai Chen, and Tri Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads. ArXiv, abs/2401.10774, 2024. URL https://api.semanticscholar.org/ CorpusID:267061277. Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023. Yulong Chen, Yang Liu, Liang Chen, and Yue Zhang. DialogSum: real-life scenario dialogue summarization dataset. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 5062 5074, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. findings-acl.449. URL https://aclanthology.org/2021.findings-acl.449. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https: //lmsys.org/blog/2023-03-30-vicuna/. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240): 1113, 2023. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. discourse-aware attention model for abstractive summarization of long documents. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pp. 615621, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-2097. URL https://aclanthology.org/N18-2097. Luciano Del Corro, Allie Del Giorno, Sahaj Agarwal, Bin Yu, Ahmed Awadallah, and Subhabrata Mukherjee. Skipdecode: Autoregressive skip decoding with batching and caching for efficient llm inference. arXiv preprint arXiv:2307.02628v1, 2023. 11 Published as conference paper at ICLR 2026 Jiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and Li Yuan. ChatLaw: Open-source legal large language model with integrated external knowledge bases. arXiv preprint arXiv:2306.16092, 2023. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm.int8(): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022. Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: sparse-quantized representation for near-lossless llm weight compression. arXiv preprint arXiv:2306.03078, 2023. Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233, 2023. Ondˇrej Duˇsek, Jekaterina Novikova, and Verena Rieser. Evaluating the State-of-the-Art of End-toEnd Natural Language Generation: The E2E NLG Challenge. Computer Speech & Language, 59:123156, January 2020. doi: 10.1016/j.csl.2019.06.009. Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot. In International Conference on Machine Learning, pp. 1032310337. PMLR, 2023. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. Breaking the sequential dependency of llm inference using lookahead decoding, November 2023. URL https://lmsys.org/blog/ 2023-11-21-lookahead-decoding/. Xinyang Geng and Hao Liu. Openllama: An open reproduction of llama, May 2023. URL https: //github.com/openlm-research/open_llama. Raghavv Goel, Mukul Gagrani, Wonseok Jeon, Junyoung Park, Mingu Lee, and Christopher Lott. Direct alignment of draft model for speculative decoding with chat-fine-tuned llms. arXiv preprint arXiv:2403.00858, 2024. Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Knowledge distillation of large language models. arXiv preprint arXiv:2306.08543, 2023. Tom Gunter, Zirui Wang, Chong Wang, Ruoming Pang, Andy Narayanan, Aonan Zhang, Bowen Zhang, Chen Chen, Chung-Cheng Chiu, David Qiu, Deepak Gopinath, Dian Ang Yap, Dong Yin, Feng Nan, Floris Weers, Guoli Yin, Haoshuo Huang, Jianyu Wang, Jiarui Lu, John Peebles, Ke Ye, Mark Lee, Nan Du, Qibin Chen, Quentin Keunebroek, Sam Wiseman, Syd Evans, Tao Lei, Vivek Rathod, Xiang Kong, Xianzhi Du, Yanghao Li, Yongqiang Wang, Yuan Gao, Zaid Ahmed, Zhaoyang Xu, Zhiyun Lu, Al Rashid, Albin Madappally Jose, Alec Doane, Alfredo Bencomo, Allison Vanderby, Andrew Hansen, Ankur Jain, Anupama Mann, Areeba Kamal, Bugu Wu, Carolina Brum, Charlie Maalouf, Chinguun Erdenebileg, Chris Dulhanty, Dominik Moritz, Doug Kang, Eduardo Jimenez, Evan Ladd, Fangping Shi, Felix Bai, Frank Chu, Fred Hohman, Hadas Kotek, Hannah Gillis Coleman, Jane Li, Jeffrey Bigham, Jeffery Cao, Jeff Lai, Jessica Cheung, Jiulong Shan, Joe Zhou, John Li, Jun Qin, Karanjeet Singh, Karla Vega, Kelvin Zou, Laura Heckman, Lauren Gardiner, Margit Bowler, Maria Cordell, Meng Cao, Nicole Hay, Nilesh Shahdadpuri, Otto Godwin, Pranay Dighe, Pushyami Rachapudi, Ramsey Tantawi, Roman Frigg, Sam Davarnia, Sanskruti Shah, Saptarshi Guha, Sasha Sirovica, Shen Ma, Shuang Ma, Simon Wang, Sulgi Kim, Suma Jayaram, Vaishaal Shankar, Varsha Paidi, Vivek Kumar, Xin Wang, Xin Zheng, Walker Cheng, Yael Shrager, Yang Ye, Yasu Tanaka, Yihao Guo, Yunsong Meng, Zhao Tang Luo, Zhi Ouyang, Alp Aygar, Alvin Wan, Andrew Walkingshaw, Andy Narayanan, Antonie Lin, Arsalan Farooq, Brent Ramerth, Colorado Reed, Chris Bartels, Chris Chaney, David Riazati, Eric Liang, Erin Feldman, Gabriel Hochstrasser, Guillaume Seguin, Irina Belousova, Joris Pelemans, Karen Yang, Keivan Alizadeh, Liangliang Cao, Mahyar Najibi, Marco Zuliani, Max Horton, Minsik Cho, Nikhil Bhendawade, Patrick Dong, Piotr Maj, Pulkit Agrawal, Qi Shan, Qichen Fu, Regan Poston, Sam Xu, Shuangning Liu, Sushma Rao, Tashweena Heeramun, Thomas Merth, Uday Rayala, Victor Cui, Vivek Rangarajan Sridhar, Wencong Zhang, Wenqi Zhang, Wentao Published as conference paper at ICLR 2026 Wu, Xingyu Zhou, Xinwen Liu, Yang Zhao, Yin Xia, Zhile Ren, and Zhongzheng Ren. Apple intelligence foundation language models. arXiv preprint arXiv:2407.21075, 2024. URL https://doi.org/10.48550/arXiv.2407.21075. Jan Hansen-Palmus, Michael Truong Le, Oliver Hausdorfer, and Alok Verma. Communication compression for tensor parallel llm inference. ArXiv, abs/2411.09510, 2024. URL https: //api.semanticscholar.org/CorpusID:274023002. Zhenyu He, Zexuan Zhong, Tianle Cai, Jason D. Lee, and Di He. Rest: Retrieval-based speculative decoding. ArXiv, abs/2311.08252, 2023. URL https://api.semanticscholar.org/ CorpusID:265157884. Zhenyu He, Zexuan Zhong, Tianle Cai, Jason Lee, and Di He. REST: Retrieval-based speculative In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Proceedings of the 2024 decoding. Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 15821595, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.88. URL https://aclanthology.org/2024.naacl-long.88/. Fenglu Hong, Ravi Raju, Jonathan Lingjie Li, Bo Li, Urmish Thakker, Avinash Ravichandran, Swayambhoo Jain, and Changran Hu. Training domain draft models for speculative decoding: Best practices and insights. arXiv preprint arXiv:2503.07807, 2025. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=nZeVKeeFYf9. Intel Corporation. Intel Core Ultra Processors, 2023. URL https://www.intel.com/ content/www/us/en/products/details/processors/core/ultra.html. Accessed: July 2025. Albert Q. Jiang et al. Mistral 7b, 2023. Joao Gante. Assisted generation: new direction toward low-latency text generation, 2023. URL https://huggingface.co/blog/assisted-generation. Norman Jouppi et al. Ten lessons from three generations shaped googles tpuv4i. In 2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA). IEEE, 2021. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Han-Byul Kim, Duc N. M. Hoang, Arnav Kundu, Mohammad Samragh, and Minsik Cho. Spd: Sync-point drop for efficient tensor parallelism of large language models. ArXiv, abs/2502.20727, 2025. URL https://api.semanticscholar.org/CorpusID:276724757. Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik, Michael Mahoney, Amir In Thirty-seventh Gholami, and Kurt Keutzer. Speculative decoding with big little decoder. Conference on Neural Information Processing Systems, 2023. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452466, 2019. Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pp. 1927419286. PMLR, 2023. Yuanzhi Li, Sebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023. 13 Published as conference paper at ICLR 2026 Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle: Speculative sampling requires rethinking feature uncertainty. arXiv preprint arXiv:2401.15077, 2024a. Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle-2: Faster inference of language models with dynamic draft trees. arXiv preprint arXiv:2406.16858, 2024b. Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. EAGLE: Speculative sampling requires rethinking feature uncertainty. In International Conference on Machine Learning, 2024c. Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. EAGLE-2: Faster inference of language models with dynamic draft trees. In Empirical Methods in Natural Language Processing, 2024d. Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle-3: Scaling up inference acceleration of large language models via training-time test. arXiv preprint arXiv:2503.01840, 2025a. Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. EAGLE-3: Scaling up inference acceleration of large language models via training-time test. In Annual Conference on Neural Information Processing Systems, 2025b. Zonghang Li, Wenjiao Feng, Mohsen Guizani, and Hongfang Yu. Tpi-llm: Serving 70b-scale llms efficiently on low-resource edge devices. ArXiv, abs/2410.00531, 2024e. URL https://api. semanticscholar.org/CorpusID:273023213. Xianzhen Luo, Yixuan Wang, Qingfu Zhu, Zhiming Zhang, Xuanyu Zhang, Qing Yang, and Dongliang Xu. Turning trash into treasure: Accelerating inference of large language models with token recycling. arXiv preprint arXiv:2408.08696, 2024. Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. Specinfer: Accelerating generative llm serving with speculative inference and token tree verification. arXiv preprint arXiv:2305.09781, 2023. Mistral. Mistral Small 3 (2501). 2025. https://mistral.ai/news/mistral-small-3. Giovanni Monea, Armand Joulin, and Edouard Grave. Pass: Parallel speculative sampling. arXiv preprint arXiv:2311.13581, 2023. Xuefei Ning, Zinan Lin, Zixuan Zhou, Huazhong Yang, and Yu Wang. Skeleton-of-thought: Large language models can do parallel decoding. arXiv preprint arXiv:2307.15337, 2023. Nvidia. Fastertransformer, 2024. URL https://github.com/NVIDIA/ FasterTransformer. Koyena Pal, Jiuding Sun, Andrew Yuan, Byron C. Wallace, and David Bau. Future lens: Anticipating subsequent tokens from single hidden state. ArXiv, abs/2311.04897, 2023a. URL https://api.semanticscholar.org/CorpusID:265050744. Koyena Pal, Jiuding Sun, Andrew Yuan, Byron Wallace, and David Bau. Future lens: Anticipating subsequent tokens from single hidden state. arXiv preprint arXiv:2311.04897, 2023b. Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference. Proceedings of Machine Learning and Systems, 5, 2023. Weizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang, and Ming Zhou. Prophetnet: Predicting future n-gram for sequence-to-sequence pre-training. arXiv preprint arXiv:2001.04063, 2020. Qualcomm Technologies 2023. Brief, Inc. uct snapdragon-8-gen-3-mobile-platform. Accessed: July 2025. URL Snapdragon Platform Prod3 Mobile https://www.qualcomm.com/products/ 8 Gen Alec Radford and Karthik Narasimhan. Improving language understanding by generative pretraining. 2018. URL https://api.semanticscholar.org/CorpusID:49313245. Published as conference paper at ICLR 2026 David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter Conway Humphreys, and Adam Santoro. Mixture-of-depths: Dynamically allocating compute in transformer-based language models. arXiv preprint arXiv:2404.02258, 2024. URL https://doi.org/10.48550/ arXiv.2404.02258. Mohammad Samragh, Arnav Kundu, David Harrison, Kumari Nishu, Devang Naik, Minsik Cho, and Mehrdad Farajtabar. Your llm knows the future: Uncovering its multi-token prediction potential. arXiv preprint arXiv:2507.11851, 2025. Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Riccardo Marin, and Emanuele Rodol`a. Accelerating transformer inference for translation via parallel decoding. arXiv preprint arXiv:2305.10427, 2023. Apoorv Saxena. Prompt lookup decoding, November 2023a. URL https://github.com/ apoorvumang/prompt-lookup-decoding/. Apoorv Saxena. Prompt lookup decoding, November 2023b. URL https://github.com/ apoorvumang/prompt-lookup-decoding/. Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Q. Tran, Yi Tay, and Donald Metzler. Confident adaptive language modeling. In Proceedings of the 36th International Conference on Neural Information Processing Systems (NeurIPS), pp. 1745617472, April 2022. Dmitriy Serdyuk, Nan Rosemary Ke, Alessandro Sordoni, Adam Trischler, Chris Pal, and Yoshua arXiv preprint Twin networks: Matching the future for sequence generation. Bengio. arXiv:1708.06742, 2017. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. ArXiv, abs/1909.08053, 2019. URL https://api.semanticscholar.org/ CorpusID:202660670. Benjamin Spector and Chris Re. Accelerating llm inference with staged speculative decoding. arXiv preprint arXiv:2308.04623, 2023. Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Blockwise parallel decoding for deep autoregressive models. Advances in Neural Information Processing Systems, 31, 2018. Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention span in transformers. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 331335, 2019. Mingjie Sun, Zhuang Liu, Anna Bair, and Zico Kolter. simple and effective pruning approach for large language models. arXiv preprint arXiv:2306.11695, 2023a. Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ahmad Beirami, Himanshu Jain, and Felix Yu. Spectr: Fast speculative decoding via optimal transport. arXiv preprint arXiv:2310.15141, 2023b. Yi Tay, Dara Bahri, Donald Metzler, et al. Scale efficiently: Insights from training and scaling large language models. arXiv preprint arXiv:2210.03863, 2022. Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron et al. Llama 2: Open foundation and fine-tuned chat models, 2023b. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 15 Published as conference paper at ICLR 2026 Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, et al. Huggingfaces transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019. Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. BloombergGPT: large language model for finance. arXiv preprint arXiv:2303.17564, 2023. Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and Zhifang Sui. Unlocking efficiency in large language model inference: comprehensive survey of speculative decoding. ArXiv, abs/2401.07851, 2024a. URL https: //api.semanticscholar.org/CorpusID:266999159. Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and Zhifang Sui. Unlocking efficiency in large language model inference: comprehensive survey of speculative decoding. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics ACL 2024, pp. 76557671, Bangkok, Thailand and virtual meeting, August 2024b. Association for Computational Linguistics. doi: 10.18653/ v1/2024.findings-acl.456. URL https://aclanthology.org/2024.findings-acl. 456. Minghao Yan, Saurabh Agarwal, and Shivaram Venkataraman. Decoding speculative decoding. arXiv preprint arXiv:2402.01528, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ Salakhutdinov, and Quoc Le. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems, 32, 2019. Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. Advances in Neural Information Processing Systems, 35:2716827183, 2022. Hanling Yi, Feng Lin, Hongbin Li, Peiyang Ning, Xiaotian Yu, and Rong Xiao. Generation meets verification: Accelerating large language model inference with smart parallel auto-correct decoding. arXiv preprint arXiv:2402.11809, 2024a. doi: 10.48550/arXiv.2402.11809. Hanling Yi, Feng Lin, Hongbin Li, Peiyang Ning, Xiaotian Yu, and Rong Xiao. Generation meets verification: Accelerating large language model inference with smart parallel auto-correct decoding. arXiv preprint arXiv:2402.11809, 2024b. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, et al. Spider: large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. arXiv preprint arXiv:1809.08887, 2018. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. 16 Published as conference paper at ICLR 2026 Victor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from natural language using reinforcement learning. arXiv preprint arXiv:1709.00103, 2017a. Victor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from natural language using reinforcement learning. CoRR, abs/1709.00103, 2017b. Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon, Afshin Rostamizadeh, Sanjiv Kumar, Jean-Francois Kagy, and Rishabh Agarwal. Distillspec: Improving speculative decoding via knowledge distillation. arXiv preprint arXiv:2310.08461, 2023. 17 Published as conference paper at ICLR"
        },
        {
            "title": "APPENDIX CONTENTS",
            "content": "A Related Works Correctness: Acceptance and Distribution Latency and Communication Analysis Extended Ablations & Empirical Analysis D.1 Batching Effects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Draft-side speedups with speculative streaming . . . . . . . . . . . . . . . . . . . Fallback Dynamics: Influence of Top-κ and Early-Exit Depth E.1 Setup and definitions . E.2 Monotonicity in . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3 Monotonicity in early-exit depth . . . . . . . . . . . . . . . . . . . . . . . . . . . E.4 Empirical confirmation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.5 Practical recommendation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Experimental Details F.1 Target and Draft Sharding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.2 Draft Model Configuration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . LLM Usage Statement 19 20 22 22 22 24 24 24 25 25 25 25 26 18 Published as conference paper at ICLR"
        },
        {
            "title": "A RELATED WORKS",
            "content": "Speculative decoding with draft models. The original speculative decoding paradigm accelerates autoregressive generation by pairing small, fast draft model with larger target model, which verifies proposed tokens (Chen et al., 2023; Leviathan et al., 2023). This approach achieves substantial wall-time savings whenever the draft is hardware-efficient and closely aligned with the target. Domain-specialized drafts trained via distillation further improve acceptance in task-specific settings (Hong et al., 2025). Recent variants explore parallelization strategies, such as batch-axis speculation (Sun et al., 2023b) and tree-structured drafts (Miao et al., 2023; Spector & Re, 2023), to raise acceptance rates and amortize draft cost. Single-model approaches. An alternative line of work removes the explicit draft model and equips the target itself with speculative capacity. Medusa predicts multiple tokens in parallel via extra heads (Cai et al., 2023), while Hydra enforces autoregressive coupling across those heads to raise acceptance (Ankner et al., 2024b). EAGLE introduces dedicated speculation layer (Li et al., 2024a), with EAGLE-2 enabling dynamic tree retries (Li et al., 2024b) and EAGLE-3 moving to token-level prediction with multi-layer fusion (Li et al., 2025a). Prompt-lookup decoding (PLD) and Lookahead propose suffixes by retrieval rather than generation (Saxena, 2023a; Fu et al., 2023), which is effective when prefixcontinuation correlations are strong. Recycling reduces wasted work by reusing intermediate activations when speculative branches are invalidated, instead of recomputing full forwards (Luo et al., 2024). Other recent advances include structured or retrieval-based decoding policies (Yi et al., 2024a; He et al., 2024). Across the single-model designs, speculative capacity is integrated into the target stack, so larger or wider modules increase acceptance but still add work on the targets critical path; by contrast, Mirror-SD runs draft and target on heterogeneous devices and overlaps draft within the targets suffix window, converting added draft capacity into acceptance gains without inflating per-step latency proportionally. Dynamic and adaptive decoding. Beyond speculation, range of methods accelerate inference by adapting compute during decoding. CALM (Schuster et al., 2022) and related early-exit methods reduce cost by exiting tokens at shallow layers, while skip decoding (Corro et al., 2023) mitigates key-value cache mismatch via position-dependent layer skipping. Mixture-of-Depths (MoD) (Raposo et al., 2024) routes only subset of tokens through full blocks, yielding non-uniform FLOP allocation. Other strategies include token merging (Bolya et al., 2023) to reduce sequence length dynamically, adaptive span models (Sukhbaatar et al., 2019) that learn context windows per token, and CoLT5 (Ainslie et al., 2023) which routes tokens through heavy or light pathways. More recently, M2R2 (Bhendawade et al., 2025) introduces accelerated residual streams to improve early alignment and efficiency. Together, these approaches trade fixed per-token compute for dynamic allocation, complementing speculative decodings strategy of parallelizing token generation. Positioning. Mirror-SD builds on these advances but takes distinct perspective: it is systemsalgorithm co-design aimed at minimizing the critical path in speculative decoding. By launching drafts from intermediate target layers, overlapping draft and target compute, and confining crossaccelerator communication to lightweight token exchanges, Mirror-SD complements prior algorithmic improvements and makes speculation more effective in heterogeneous GPUNPU deployments. CORRECTNESS: ACCEPTANCE AND DISTRIBUTION Let γ be the speculative window length, the number of transformer layers in the target, and let At {0, . . . , γ} denote the accepted-prefix length at step t. Recall that the targets final next-token distribution is p(N )( h) and that verification commits the longest prefix of the draft that matches the targets tokens. Acceptance operator (rule-level equivalence). For any realized draft proposal ˆyt+1:t+γ and realized target tokens ytarget t+1:t+γ (obtained by rolling the target with teacher forcing along the agreed prefix and stopping at the first mismatch), both vanilla SD and Mirror-SD compute At = max (cid:110) γ : ˆyt+j = ytarget t+j r (cid:111) . (11) 19 Published as conference paper at ICLR 2026 (a) Qwen-14B (b) Qwen-32B. Figure 5: Batching effects on speedup across tasks and scales. Both vanilla SD and Mirror-SD slow down as batch size increases due to growing draft compute and verification cost, but Mirror-SD consistently outperforms vanilla SD by preserving non-zero overlap under batching. Equation (11) is the same acceptance operator in both algorithms: Mirror-SD never commits token that was not verified against p(N ), and any commit is exactly the longest verified prefix. Thus, Mirror-SD changes only the schedule by which draft proposals are produced (overlapping with target compute), not the acceptance rule. Distributional equivalence (when the verified draft path is identically distributed). Fix the models (fdraf t, ftarget) and window γ. Let Ct be the decoding context at step (prompt and previously committed tokens), and let ζdraf t, ζtarget collect all random seeds for draft and target sampling. Define the function S(ˆyt+1:t+γ, ytarget t+1:t+γ) = max{r γ : ˆyt+j = ytarget t+j r}, so that At = S(ˆy, ytarget) in both procedures. Assume the draft sequence actually presented to verification in Mirror-SD, denoted ˆyMir the same conditional distribution as the vanilla draft sequence ˆyVan t+1:t+γ given Ct: t+1:t+γ, has Then, under common coupling of (ζd, ζt), ˆyMir t+1:t+γ d= ˆyVan t+1:t+γ Ct. (12) PMirror(At = r) = P(cid:0)S(ˆyMir, ytarg) = r(cid:1) = P(cid:0)S(ˆyVan, ytarg) = r(cid:1) = PVanilla(At = r) , Hence the acceptance-rate statistic ρ(γ; ϕ, θ) = E[At]/γ coincides between Mirror-SD and vanilla SD. {0, . . . , γ}. (13) Sufficient condition for equation 12. Condition equation 12 holds if the draft path used for verification in Mirror-SD is sampled from fdraf t( ht) exactly as in vanilla SD, or more generally if the branch-selection policy induces the same conditional law for the verified draft sequence as vanilla SD. Under this mild parity condition, Mirror-SD is distributionally identical to vanilla SD with respect to At, while still enjoying the latency benefits of overlapping draft computation with the targets suffix."
        },
        {
            "title": "C LATENCY AND COMMUNICATION ANALYSIS",
            "content": "This appendix consolidates the latency model of Mirror-SD with its tensor-parallel (TP) communication costs. Draft and Target Latencies Within one Mirror-SD step, the draft may take 1 internal steps. With speculative streaming (SS), step emits ηj 1 tokens so that (cid:80)J j=1 ηj γ, with average 20 Published as conference paper at ICLR 2026 η = 1 (cid:80) ηj and gen draft(γ) = (cid:88) (ud + sd ), j=1 (cid:108) γ η (cid:109) . Here ud cℓ = ut is device-local compute and sd ℓ + st ℓ, giving draft synchronization. For the target, each layer ℓ incurs 1:ℓe target = ℓe(cid:88) ℓ=1 cℓ, ℓe+1:N target = (cid:88) cℓ. ℓ=ℓe+1 At early exit and final verification, rendezvous costs decompose as rv = (ee) (ee) samp + (ee) xfer , (fv) rv = (fv) samp + (fv) xfer , Trv = (ee) rv + (fv) rv , where transfers involve only O(Bκ) IDs/log-probs and are negligible compared with compute. Mirror-SD Latency Law The per-step latency is TMirror = 1:ℓe target + (ee) rv + max{T ℓe+1:N target , gen draft(γ)} + (fv) rv . (14) Let = ℓe+1:N draft cost dominates the parallel region: TMirror = 1:ℓe draft(γ) , draft work is fully hidden: TMirror = Ttarget + Trv. Otherwise, draft(γ) + Trv. Compared to vanilla SD, . If gen target target + ℓe+1:N Mirror-SD hides draft work up to , leaving only lightweight rendezvous terms on the critical path. TSD = 1:ℓe draft(γ), target target + gen + gen Comparison to vanilla SD (per step). Vanilla SD executes draft and target serially: TSD = 1:ℓe target + ℓe+1:N target + gen draft(γ) = Ttarget + gen draft(γ), where we write def= ℓe+1:N target for the overlap budget. Using the Mirror-SD law above, TMirror = 1:ℓe target + (ee) rv + max{, gen draft(γ)} + (fv) rv = Ttarget + Trv, if gen draft(γ) , and TMirror = 1:ℓe target + gen draft(γ) + Trv, if gen draft(γ) > , with Trv = (ee) Per-step time saved. The improvement is rv +T (fv) rv . def= TSD TMirror = (cid:0) min{, gen draft(γ)}(cid:1) Trv, i.e., Mirror-SD hides up to the smaller of the overlap budget and the draft time, minus lightweight rendezvous. Thus Mirror-SD is strictly faster whenever Trv < min{, gen draft(γ)}. Per-step speedup. The piecewise speedup = TSD/TMirror is Ttarget + gen , Ttarget + Trv target + + gen 1:ℓe target + gen 1:ℓe draft(γ) draft(γ) + Trv draft(γ) = , if gen draft(γ) , if gen draft(γ) > . In practice Trv is O(Bκ) token/log-prob exchange and sampling, i.e., microsecond-scale, so the conditions above are typically satisfied; speculative streaming (larger η) further reduces and gen draft(γ), making full hiding (T gen draft(γ) ) common. Communication Costs under TP For devices and message size (per rank), AllReduce cost is Tallreduce(M ; G) = α log + βM, 21 Published as conference paper at ICLR 2026 with α per-hop latency and β per-word transfer time. Target: Let HT be the target hidden width, GT its TP degree, and ST the effective tokens per collective. Each of the blocks performs two collectives on shards of size MT = ST HT , giving GT comm target = 2N Tallreduce (cid:0)MT; GT (cid:1). Draft: Let HD be the draft hidden width, GD its TP degree, and SD the effective tokens per draft collective. Each draft internal step performs two collectives on shards of size MD = SD HD , so comm draft (over steps) = 2J Tallreduce GD (cid:0)MD; GD (cid:1), comm draft-step = 2 Tallreduce which is included in gen draft(γ). (cid:0)MD; GD (cid:1), Cross-accelerator: Token-channel exchanges remain O(Bκ) IDs/log-probs and are microsecondscale. EXTENDED ABLATIONS & EMPIRICAL ANALYSIS D.1 BATCHING EFFECTS In deployment, batching is often enabled to improve throughput and amortize GPU compute, but it is not universal: many interactive or privacy-sensitive settings prioritize per-request latency and avoid batching. To ensure completeness, we therefore also evaluate Mirror-SD under batched inference. The key question is whether speculative decoding, and Mirror-SD in particular, retains its gains when batching is enabled, or whether draft overhead grows to the point of erasing speedup. To bound the growth of draft-side computation with increasing batch size and to keep draft execution maximally hidden under the target, we scale the draft hyperparameters with B: as increases, we reduce both Top-κ and the number of SS lookahead streams so that aggregate draft cost and the token-channel payload remain controlled. Concretely, we use κ=8 with two SS streams for {1, 8}; from B=16 onward we use single SS stream and progressively reduce κ: κ=4 for B=16, κ=2 for B=32, and κ=1 for 64. Observed trends. We find that vanilla SD speedup declines steadily as batch size increases (Figure 5b). Larger batches lengthen the target verification phase both because more sequences must be processed in parallel and because batching introduces additional padding and synchronization under tensor-parallel execution. Mirror-SD also shows downward trend with B, but consistently outperforms vanilla SD (Figure 5b, Figure 5a). As grows, the draft must evaluate top-κ candidates across γ positions for each sequence, which increases draft compute and intra-NPU communication and pushes the draft path toward compute-bound regime. Consequently, its ability to overlap with target verification diminishes. This decreased yet positive overlap is sufficient for Mirror-SD to maintain consistent speedup lead over vanilla SD as batching increases. In practice, batching introduces several intertwined effects: (i) the target takes longer, enlarging the potential overlap window; (ii) the draft also takes longer, and its relative overhead grows with the κ γ expansion; (iii) autoregressive baselines slow as increases; (iv) speculative decoding slows even more, as it inherits both ARs slowdown and the drafts added work; and (v) under tensor-parallel sharding, both SD variants lose relative speedup, but Mirror-SD maintains consistent lead by exploiting concurrency across heterogeneous accelerators. Relative draft overhead. We also report normalized relative draft overhead in Figure 5, defined as the fraction of draft speculation time that cannot be hidden under target verification, normalized against the total overhead of vanilla SD. This metric is dimensionless and directly reveals how much of the draft path remains exposed on the critical path. As batch size increases, the verification phase grows longer, but draft compute and intra-NPU communication grow even faster (since each sequence requires top-κ rollouts across γ positions). Consequently, relative draft overhead rises with B, aligning with the decreasing speedups observed in our batching experiments. D.2 DRAFT-SIDE SPEEDUPS WITH SPECULATIVE STREAMING We quantify the internal draft gains from Speculative Streaming (SS) under the same targets and decoding settings as our main experiments. As described in Section 3.2, SS verifies previously 22 Published as conference paper at ICLR 2026 (a) Speculative Streaming (SS): each draft step proposes multiple tokens via lookahead streams; accepted tokens extend the prefix, rejected ones are dropped. (b) Draft-only speedup from SS relative to an autoregressive draft with 3 lookahead streams. Lower for given γ reduces gen draft (γ), enlarging the overlap margin in Mirror-SD and translating into end-to-end speedups. Figure 6: Comparison of Speculative Streaming (SS) draft dynamics (left) and resulting speedups (right). (a) Humanities (b) Math (c) Roleplay (d) STEM (e) Coding (f) Math reasoning Figure 7: Fallback frequency vs. Top-κ and early-exit depth across six tasks (Humanities, Math, Roleplay, STEM, Coding, Math Reasoning). Each panel shows fallback frequency as function of for exits at 1/4, 1/2, and 3/4 of depth; smaller values indicate fewer fallbacks and greater reuse. proposed tokens while producing multiple new lookahead tokens in single forward pass via multistream attention. Empirically, this reduces the number of draft internal steps needed to materialize window of length γ, typically yielding γ and corresponding reduction in draft generation time gen draft(γ). Figure 6b reports the draft-only speedup of SS over plain autoregressive draft across translation, summarization, QA, mathematical reasoning, RAG, and MT-Bench. The effect is consistent across workloads: SS achieves substantially fewer internal steps for the same γ and, consequently, shorter gen draft(γ). When composed with Mirror-SDs overlap (Section 3.4), this pushes the operating point further into the zero-slope region where increases in γ raise acceptance length E[At] = γ ρ(γ; ϕ, θ) without increasing step latency. Because acceptance semantics are unchanged ( Section B), all end-to-end gains are purely systems-level. 23 Published as conference paper at ICLR 2026 FALLBACK DYNAMICS: INFLUENCE OF TOP-κ AND EARLY-EXIT DEPTH E.1 SETUP AND DEFINITIONS At decoding step t, let the targets final next-token distribution be q() = p(N )( y<t, x) and the early-exit proxy be p() = p(ℓe)( y<t, x). The target accepts prefix of length At and, if mismatch occurs, issues correction at index τ = At+1 with token ct+τ . The draft precomputes branch-complete window conditioned on the early-exit Top-κ set Mt = {(vi, log pi)}κ i=1. Reuse succeeds iff the targets correction lies on precomputed path, Π+ Pathsτ (Tt), otherwise we fallback (re-initialize the draft from the corrected context). Let Ft = {Π+ Pathsτ (Tt)} and FF E[Ft]. Define the overlap mass (cid:88) / Ωκ(ℓe) def= q(y), i.e., the probability under that the next token lies in the early-exit Top-κ set. yTop-κ( p) E.2 MONOTONICITY IN Proposition 1 (Top-κ reduces fallback). For fixed early-exit layer ℓe, the fallback frequency FF(ℓe, κ) is nonincreasing in the integer κ and vanishes as κ : κ2 κ1 = FF(ℓe, κ2) FF(ℓe, κ1), lim κV FF(ℓe, κ) = 0. Proof. If At = 0 (mismatch on the first token), reuse succeeds iff yt+1 Top-κ(p(ℓe)), so Pr[Ft = 1 At = 0] = 1 Ωκ(ℓe). If At 1, the root matches yt+1 and reuse at depth τ requires ct+τ to appear on some branch of the hypothesis tree Tt seeded by Top-κ(p(ℓe)). Increasing κ only adds roots/paths and never removes existing ones, so {Π+ Pathsτ (Tt)} is monotone in κ. Taking expectations over yields the claim. The limit follows because Ωκ(ℓe) 1 as κ , at which point the hypothesis tree contains all needed paths. useful corollary is which is tight when most fallbacks occur at τ = 1 (high-entropy regimes). FF(ℓe, κ) 1 Ωκ(ℓe), E.3 MONOTONICITY IN EARLY-EXIT DEPTH Proposition 2 (Deeper exit reduces fallback). Fix κ. As the early-exit layer ℓe moves deeper (toward ), the overlap mass Ωκ(ℓe) = (cid:88) q(y) yTop-κ(p(ℓe )) converges to its maximal value q(S) with = Top-κ(q); consequently FF(ℓe, κ) 1 Ωκ(ℓe) decreases with depth and stabilizes at its minimum for sufficiently deep exits. def= p(ℓ) 0. Proof. As the layer index ℓ increases, the distributions p(ℓ) approach q; write εℓ Let Sℓ = Top-κ(p(ℓ)) and = Top-κ(q). Because Sℓ maximizes p(ℓ)-mass among all size-κ sets, and any such set satisfies q(A) p(ℓ)(A) κ εℓ, we have Ωκ(ℓ) = q(Sℓ) q(S) 2κ εℓ ℓN q(S). If the Top-κ boundary of has margin κ > 0, then whenever εℓ < κ/2 the Top-κ set stabilizes (Sℓ = S) for all deeper layers, so Ωκ(ℓ) = q(S) thereafter. Since reuse probability is monotone in the q-mass captured by the seed set, the bound FF(ℓe, κ) 1Ωκ(ℓe) implies (weakly) decreasing FF with depth and eventual stabilization at its minimum. 24 Published as conference paper at ICLR 2026 E.4 EMPIRICAL CONFIRMATION Figure 7 reports fallback frequency as function of for early exits at 1/4, 1/2, and 3/4 of depth across six tasks. Two consistent trends emerge: Top-κ effect. Increasing monotonically lowers fallback, with diminishing returns once Ωκ saturates. This matches the bound FF 1 Ωκ(ℓe) and reflects higher probability that the drafts precomputed path already contains the targets correction. Early-exit effect. Holding fixed, moving the exit deeper (1/4 1/2 3/4) lowers fallback across tasks. Deeper exits raise Ωκ by improving agreement between the early-exit proxy and the final distribution, so the correction token more often lies on precomputed branch. E.5 PRACTICAL RECOMMENDATION Unless otherwise noted, across all SpecBench experiments reported in Table 1 we set the Topκ width to κ = 8 and fix the early exit to the middle of the network (ℓe = N/2, Exit 1/2). In practice, this mid-depth, k=8 configuration works well across most setups, balancing fallback probability and the overlap budget for draft precomputation. Choosing and ℓe trades small token-channel payload and longer precomputation for fewer fallbacks and, consequently, longer accepted prefixes per step. In Mirror-SD, the channel payload is O(Bκ) and the precomputation runs in parallel under the target suffix; thus, within the overlap budget, increasing or moving ℓe deeper reduces fallback without adding step latency, directly improving end-to-end throughput via larger expected acceptance length. For bandwidth-constrained deployments, κ=8, ℓe=N/2 is robust default; when acceptance is still low, increase κ or move the exit slightly deeper (subject to the overlap budget), and when channel or memory is tight, reduce κ or use slightly shallower exit."
        },
        {
            "title": "F ADDITIONAL EXPERIMENTAL DETAILS",
            "content": "F.1 TARGET AND DRAFT SHARDING For the experiments in Section 4, both target and draft models were distributed across eight Apple M2 Ultra systems (Apple Inc., 2023a), each integrating high-throughput GPU and dedicated Neural Engine (NPU). We allocate the target to GPUs using Megatron-style tensor parallelism and the draft to NPUs using SPD-style sharding (see Section 3.3). Each M2 Ultra consists of dual-die package connected internally by UltraFusion, die-to-die interconnect providing up to 2.5 TB/s of bandwidth while presenting the system as single logical GPU/NPU pair (Apple Inc., 2023a). Across machines, we organize the 8 nodes into groups of 2, linked by Thunderbolt 5 interconnects (up to 120 Gbps peak bandwidth) (Apple Inc., 2024). Groups are further connected through high-speed network fabric, providing sufficient bandwidth for inter-group synchronization with submillisecond latency. In this setup, cross-accelerator token-channel communication consists only of O(Bκ) items (token IDs and few log-probabilities), transferred via GPUCPUNPU copies. These messages remain negligible compared to inter-layer collectives and draft compute, consistent with the latency analysis in Section 3.4. F.2 DRAFT MODEL CONFIGURATION The draft used in our experiments is 0.6B-parameter model trained with the SPD architecture (Kim et al., 2025). It is organized into 16 transformer layers, divided into two contiguous segments of 8 layers each. Within every segment we instantiate GD=8 parallel tracks, where track {1, . . . , GD} is pinned to NPU and advances through its resident shard of the segment. Each track operates with hidden size of 256 per shard. As in Section 3.3, there is no inter-NPU traffic within segment. Synchronization occurs only twice per forward pass: once at the segment boundary to re-align tensor partitions, and once at the output to assemble logits for both main and lookahead streams. Published as conference paper at ICLR"
        },
        {
            "title": "G LLM USAGE STATEMENT",
            "content": "In preparing this manuscript, we used AI-assisted tools to check grammar and to rephrase some sentences for clarity and readability. No content, results, or analysis were generated by AI systems; all scientific contributions and conclusions are our own."
        }
    ],
    "affiliations": [
        "Apple"
    ]
}