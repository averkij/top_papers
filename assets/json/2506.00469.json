{
    "paper_title": "Massively Multilingual Adaptation of Large Language Models Using Bilingual Translation Data",
    "authors": [
        "Shaoxiong Ji",
        "Zihao Li",
        "Jaakko Paavola",
        "Indraneil Paul",
        "Hengyu Luo",
        "Jörg Tiedemann"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper investigates a critical design decision in the practice of massively multilingual continual pre-training -- the inclusion of parallel data. Specifically, we study the impact of bilingual translation data for massively multilingual language adaptation of the Llama3 family of models to 500 languages. To this end, we construct the MaLA bilingual translation corpus, containing data from more than 2,500 language pairs. Subsequently, we develop the EMMA-500 Llama 3 suite of four massively multilingual models -- continually pre-trained from the Llama 3 family of base models extensively on diverse data mixes up to 671B tokens -- and explore the effect of continual pre-training with or without bilingual translation data. Comprehensive evaluation across 7 tasks and 12 benchmarks demonstrates that bilingual data tends to enhance language transfer and performance, particularly for low-resource languages. We open-source the MaLA corpus, EMMA-500 Llama 3 suite artefacts, code, and model generations."
        },
        {
            "title": "Start",
            "content": "EMMA-500 Gen"
        },
        {
            "title": "Shaoxiong Ji",
            "content": "1,2 2 , Zihao Li 2 , Jaakko Paavola 1 , Indraneil Paul , Hengyu Luo 2 , and Jörg Tiedemann 1Technical University of Darmstadt 2University of Helsinki {shaoxiong.ji; indraneil.paul}@tu-darmstadt.de {shaoxiong.ji; zihao.li; jaakko.paavola; hengyu.luo; jorg.tiedemann}@helsinki.fi"
        },
        {
            "title": "Abstract",
            "content": "This paper investigates critical design decision in the practice of massively multilingual continual pre-training the inclusion of parallel data. Specifically, we study the impact of bilingual translation data for massively multilingual language adaptation of the Llama3 family of models to 500 languages. To this end, we construct the MaLA bilingual translation corpus, containing data from more than 2,500 language pairs. Subsequently, we develop the EMMA-500 Llama 3 suite of four massively multilingual models continually pre-trained from the Llama 3 family of base models extensively on diverse data mixes up to 671B tokens and explore the effect of continual pre-training with or without bilingual translation data. Comprehensive evaluation across 7 tasks and 12 benchmarks demonstrates that bilingual data tends to enhance language transfer and performance, particularly for low-resource languages. We open-source the MaLA corpus, EMMA-500 Llama 3 suite artefacts, code, and model generations. Website: mala-lm.github.io/emma-500-gen2 Models: huggingface.co/collections/MaLA-LM EMMA-500 Data: huggingface.co/datasets/MaLA-LM/mala-bilingual-translation-corpus Evaluation: github.com/MaLA-LM/emma-"
        },
        {
            "title": "3.1 Impact of Mono- vs. Bilingual Training . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.2 Low-Resource Language Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.3 Model Adaptability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.4 Per-Language Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.5 Overall Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "5 2 0 2 1 3 ] . [ 1 9 6 4 0 0 . 6 0 5 2 : r a"
        },
        {
            "title": "A Frequently Asked Questions",
            "content": ""
        },
        {
            "title": "Corresponding author",
            "content": "1 2 3 3 4 6 7 7 8 9 9 10"
        },
        {
            "title": "B Details of MaLA Translation Corpus",
            "content": "B.1 Data Sources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Supported Languages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Token Counts"
        },
        {
            "title": "C Details of Data Mixes",
            "content": "C.1 Additional Code Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Detailed Evaluation Setup D.1 Tasks and Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Evaluation Software . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "E Detailed Results",
            "content": "E.1 Text Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Commonsense Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3 Natural Language Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.4 Machine Translation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.5 Text Summarization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.6 Machine Reading Comprehension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.7 Math . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "1 Introduction",
            "content": "24 24 24 25 25 26 27 27 28 29 29 30 31 31 32 34 35 37 37 Large language models (LLMs) pre-trained on massive data have promoted multilingual natural language processing (NLP). However, multilingual models such as BLOOM (Scao et al., 2022) and Llama (Touvron et al., 2023a,b) often struggle with low-resource languages and are still limited in their language coverage (Huang et al., 2023, 2025; Sindhujan et al., 2025). Recent works extend pre-trained LLMs into multiple languages via continual pre-training (CPT). For example, LLaMAX (Lu et al., 2024) and xLLMs-100 (Lai et al., 2024) adopt CPT and instruction fine-tuning to extend existing LLMs into 100 languages, and MaLA-500 (Lin et al., 2024) and EMMA-500 (Ji et al., 2024a) perform continual pre-training (low-rank and full-parameter CPT using Llama 2) to adapt LLMs into 500 languages. Despite these efforts, challenges still remain in adapting LLMs to low-resource languages, especially in massively multilingual scenario with more than 500 languages. This paper studies CPT in massively multilingual setting. Prior work like LlaMAX (Lu et al., 2024) uses both monolingual and parallel texts for CPT in 100 languages, and EMMA-500 Llama 2 (Ji et al., 2024a) uses only monolingual texts in 500 languages. Our novelty is to scale the number of languages up to more than 500 languages, compile new corpus with bilingual translation data (MaLA), and study the comparative effects of continual pre-training with monolingual and bilingual translation data.1 Contributions Our contributions are three-fold: Data: We compile bilingual translation corpus for Massive Language Adaptation in more than 2,500 language pairs and 500 languages, namely the MaLA translation corpus. Models: We train and release 4 models, namely EMMA-500 Llama 3/3.1 Mono/Bi for Enhancing Massively Multilingual Adaptation,2 by continually pre-training of Llama 3 & 3.1 (8B) using both monolingual and bilingual MaLA corpus augmented with diverse data types, up to 671B tokens. 1Monolingual data consists of texts written in single language. Bilingual translation data, also called parallel corpora, comprises pairs of sentences in two different languages that express the same meaning. In this paper, we treat the terms bilingual translation corpora/texts/data, bilingual corpora/texts/data, parallel corpora/texts/data, and bitexts as equivalent. 2We use MaLA and EMMA-500 to name the corpus and models, following the naming convention of EMMA-500 Llama 2 (Ji et al., 2024a) which is not an artifact of this paper. MaLA and EMMA-500 are collection of corpora and models. In this paper, we compile and release the MaLA bilingual translation corpus, i.e., MaLA-LM/mala-bilingual-translation-corpus. Mono and Bi in model names indicate CPT on monolingual (fig. 2b) and bilingual (fig. 2a) mixes, respectively. 2 Evaluation: We conduct comprehensive evaluation across 7 tasks and 12 benchmarks. Our empirical evaluation ablates the impact of two diverse data mixes and analyzes gains in task generalization and multilingual robustness."
        },
        {
            "title": "Evaluation Results and Findings",
            "content": "CPT using data mix with bilingual translation data generally exhibits better multilingual performance than monolingual mix3, particularly in low-resource languages and in machine translation tasks that directly benefit from parallel data. Heavily pre-trained models (e.g., Llama 3 and 3.1) that consume more training tokens are more resistant to further adaptation than English-centric models (e.g., Llama 2) when scaling to include many additional languages. As for overall performance, our EMMA-500 models are the best at machine translation (Flores200) and competitive at text classification (Taxi1500 and SIB-200) and commonsense reasoning (XCOPA and XStoryCloze). EMMA-500 CPT models exhibit lower average accuracy on the BELEBLE comprehension benchmark, but they outperform baselines across greater number of languages. While multilingual models can achieve broad coverage, perfect uniformity across all tasks and languages remains an unattainable goal. However, we show that multilingual performance and language equality can be pushed forward with parallel training data. Outline Section 2 presents the data and model training with newly compiled bilingual translation corpus introduced in Section 2.1, data mixing introduced in Section 2.2, and settings for model training introduced in Section 2.3. Sections and describe the details about the MaLA translation corpus and data mixes. We evaluate the resulting models and discuss the evaluation results in Section 3. Detailed evaluation setup and per-benchmark and per-language results are presented in Sections and respectively. We conclude the paper in Section 5. Sections 4 and introduce related work and ethics consideration."
        },
        {
            "title": "2.1 MaLA Translation Corpus",
            "content": "Bilingual translation corpora are language datasets that contain text data in one language aligned with text data in another language. We extend the MaLA (Massive Language Adaptation) corpus (Ji et al., 2024a) by incorporating parallel data in more than 500 languages and 2,500 language pairs. The resulting parallel dataset is named the MaLA translation corpus (MaLA for short), which is suitable for adapting language models in massively multilingual scenarios. The section describes the process of building the MaLA translation corpus in way similar to the MaLA corpus with monolingual texts, but focuses on bilingual texts. We follow similar data integration pipeline to the process of compiling the MaLA corpus (Ji et al., 2024a), including extraction, harmonization, language code normalization, and writing system recognition. The bilingual data comes from various sources, including OPUS (Tiedemann, 2012), NLLB (NLLB Team et al., 2022), and Tatoeba (Tiedemann, 2020). The datasets from OPUS are made by an existing compilation: Lego-MT (Yuan et al., 2023). Table 8 in Section B.1 shows the data sources for bilingual texts. The main difference in the script recognition, as well as language code conversion, with the bilingual corpora, is in the form of the label; we obtain label in the form of language pair, e.g., eng_Latn-zho_Hani. Language Code Normalization Language code normalization converts various language codes into standardized format to ensure consistency and compatibility across different systems and applications. With the bilingual corpora, we face similar issues as with the monolingual ones when converting language denotations given in OPUS4 to ISO 639-3 language codes. Moreover, with 3A monolingual mix (fig. 2b) contains monolingual data in different languages but not in the aligned format as parallel data. 4https://github.com/Helsinki-NLP/OPUS 3 bilingual corpora, we want to specify dialects based on the ISO 3166-1 alpha-3 standard. The recognition and handling of language codes are based on the following procedure: If the language code of dataset provided by OPUS matches language code in ISO-639-3, then we consider it such. If the language code does not match one in ISO-639-3, we use the langcodes package5 to convert it to ISO-639-3. If the above steps fail, we assign unknown as the language code. Writing System Recognition We implement writing system identification following ISO 15924 standards using the GlotScript library (Kargaran et al., 2023). For script detection, we analyze 100-line samples by default, reverting to first-line analysis when standard detection failsa known limitation affecting both our bilingual and monolingual corpora (Ji et al., 2024a). We do not classify dataset into multiple scripts, even in cases of code-mixing, where multiple scripts are used. Data Cleaning The bilingual corpus compilation faces significant quality variability, such as noisy sentence pairs. The Lego-MT dataset (Yuan et al., 2023) applies some data cleaning, including deduplication, removing missing translations, and length mismatching. For other data sources, we add various procedures, including dataset-specific cleaning and deduplication, after integrating all data sets into our collection. We eliminate lines that contain the exact word or character repeated more than five consecutive times. This problem appears in particular in the Tatoeba parallel training data (Tiedemann, 2020) and in the majority of these cases is erroneous. We use OpusFilter (Aulamo et al., 2020) for deduplication of data points. Key Statistics After pre-processing and cleaning, we obtain the MaLA translation corpus in 2,507 language pairs. Table 1 shows the total number of whitespace-separated tokens and the number of language pairs across different resource categories. Compared with Lego-MT and NLLB, MaLA has similar number of language pairs but more tokens. We categorize language pairs into 5 resource levels based on token counts: high-resource (>1B), medium-high (>500M), medium (>100M), medium-low (>10M), and low (>1M). Different from the monolingual MaLA, we add two categories of very high and very low resources in the resource level classification, i.e., very high-resource pairs (>10B) and very low-resource (<1M). In total, there are more than 426B tokens in the MaLA translation corpus.6 We further sample the MaLA translation corpus for continued training LLMs, considering balanced corpus size and language coverage. Table 1: Key statistics of the MaLA translation corpus and comparison with existing parallel corpora."
        },
        {
            "title": "MaLA",
            "content": "Lego-MT Categories NLLB Pairs Tokens Pairs Tokens Pairs Tokens 2.8E+10 4 8.5E+10 4 very high 1.2E+11 83 2.1E+11 high 51 4.5E+10 67 4.7E+10 medium-high 22 6.1E+10 281 6.4E+10 medium 75 2.0E+10 2.0E+10 508 medium-low 113 350 low 2.5E+09 1.7E+09 655 1.8E+08 1893 1.7E+08 1154 1.3E+08 909 very low 2508 2.2E+11 2509 2.8E+11 2507 4.3E+11 sum 5.1E+10 4 1.4E+11 51 1.5E+10 65 1.8E+10 264 4.1E+09 480 1.4E+"
        },
        {
            "title": "2.2 Data Mixing",
            "content": "We blend the data compiled in MaLA with multilingual non-parallel data obtained from the cleaned and deduplicated MaLA corpus (Ji et al., 2024a) along with texts selectively sourced from factual and high-quality domains with view to retaining the knowledge acquired during the annealing phase of pre-training (Hu et al., 2024). We mirror existing practice and sample books and scientific papers (Soldaini et al., 2024) along with instruction-like (Maini et al., 2024) data. Following Ji et al. (2024a), 5https://pypi.org/project/langcodes/ 6Note that whitespace-based token counting is not accurate for language where words are not typically separated by spaces. In these languages, the absence of whitespace makes it challenging to determine token boundaries, leading to inaccurate token counts when using whitespace as delimiter. We use whitespace as the delimiter because of its efficiency in processing text. 4 we use scientific papers and books from CSL (Li et al., 2022), pes2o (Soldaini and Lo, 2023), and free e-books from the Gutenberg project7 compiled by Faysse (2023). Multilingual instruction data is sourced from the training set of xp3x (Crosslingual Public Pool of Prompts eXtended)8 and the Aya collection9. Finally, we augment our mixes with code and code-adjacent procedural text due to its demonstrable benefits towards reasoning and entity-tracking (Petty et al., 2024; Ruis et al., 2025) (details in Section C.1). We manually mix up different types of data to balance the language coverage across different levels of resources and types while ensuring that lowand medium-resource languages remain overrepresented. Figure 2 shows the composition categorized by language resources according to the number of tokens. Notably, medium-resource language pairs contribute the majority of bilingual data, and medium-high-resource languages are the largest category for monolingual and instruction data. (a) Bilingual translation data (b) Monolingual data (c) Instruction data Figure 1: Composition by language resource levels of different data types. Two Data Mixes We make two data mixes to ablate the effect of incorporating bilingual texts into continual pre-training. The first data mix is bilingual mix (Figure 2b), which incorporates various data types. The second is monolingual mix (Figure 2b), which is derived from the bilingual mix but specifically omits any bilingual data, focusing solely on subset with monolingual texts per document. Detailed data statistics of these two mixes are presented in Table 10. Most translation data are aligned sentences, and there is not sufficient document-level aligned data publicly available to be collected in the MaLA corpus, especially in the massively multilingual scenario. When using translation data, we concatenate the source and target language texts in specific format to form chunk of pairs in the same language pair. For every ten samples, i.e., sentence pairs, to make document for training,10 the format is structured as follows: [{ src_lang_code }]: {src_text} [{ tgt_lang_code }]: {tgt_text} # 8 lines for 8 samples [{ src_lang_code }]: {src_text} [{ tgt_lang_code }]: {tgt_text} In this format, the notation {} denotes the variables for source and target language codes and texts. This method allows us to work with pseudo-document-level data and clearly delineate between the source and target languages, facilitating better processing and understanding of the translation data without switching between multiple languages within the context window. By organizing the data this way, we facilitate the model to learn from the relationships between the two languages effectively. This structured approach ensures clarity and consistency in how bilingual data is presented, making it easier to process and analyze. Figure 2 shows the composition of our two data mixes. In the monolingual mix, monolingual web-crawled text is the largest data type, as its name suggests. The bilingual mix incorporates considerably bigger portion of bilingual texts, 6% more than the monolingual texts. Continual training on these two mixes facilitates the adaptation of LLMs to massively multilingual languages and analyzes the effect of scaling massively multilingual training using bilingual data. 7https://www.gutenberg.org/ 8https://hf.co/datasets/CohereForAI/xP3x 9https://hf.co/datasets/CohereForAI/aya_collection_language_split 10The choice of 10 is inspired by yí mù shí háng, Chinese idiom that literally translates to one glance ten lines, means that someone reads very quickly and efficiently. 5 (a) Data mix 1: bilingual (b) Data mix 2: monolingual Figure 2: Two date mixes and their composition. The bilingual mix includes all types of data. The monolingual mix consists of subset of the bilingual mix that excludes bilingual data. Manual Curation of Data Mixes Our data mixes in Figure 2 are crafted through experience and intuition. We draw on our knowledge and understanding of the domain to create effective data combinations to balance the language resources and represent different text types. This allows for reasonable selection that can capture the complexities of the data landscape and is aligned with the goals for massively multilingual adaptation. From an algorithmic perspective, grid search or other similar methods could be used to explore the space of possible data distributions. However, algorithmic search involves systematically exploring predefined set of hyperparameter values by evaluating all possible combinations through model training on the searched mixes, which can become computationally expensive and time-consuming, especially as the number of parameters increases. This makes it challenging to apply algorithms like grid search effectively in scenarios where the data mix needs to be optimized, which would require thousands of training runs and tens of millions of GPU hours or even more. Searching for an optimal (or near-optimal) data mix is practically infeasible. Our paper focuses on offering useful resource for continual pre-training in massively multilingual scenario. To do this, we select the corpus to maintain diversity and balance among various languages and text types that align with the models intended goal."
        },
        {
            "title": "2.3 Model Training",
            "content": "We continue training the decoder-only Llama 3 models (8B parameters) using the causal language modeling objective, exposing the pre-trained model to new data and languages to develop our EMMA-500 model. To enhance efficiency, we use training strategies that optimize memory usage, precision handling, and distributed training. EMMA-500 Llama 3 series models are trained on the LUMI Supercomputer, powered by 100% renewable and carbon-neutral energy, utilizing 64 compute nodes with 256 AMD MI250x GPUs (512 AMD Graphics Compute Dies) with the GPT-NeoX framework (Andonian et al., 2023). We continue training the base models in full-parameter manner without modifying the tokenizer. The training setup includes global batch size of 2048 and sequence lengths of 8192 tokens. Modeland data-specific settings are presented in Table 2. For training on the monolingual mix, the process spans 25,000 steps, accumulating total of 419 billion Llama 3 tokens. For the bilingual mix that contains more tokens, we train for more steps, up to 40,000, leading to total of 671 billion tokens. We employ the Adam optimizer (Kingma and Ba, 2015) with learning rate of 0.0001, betas set to [0.9, 0.95], and an epsilon of 1e-8. We experiment with different learning rates and evaluate early checkpoints trained up to 5,000 steps (84 billion tokens). We find that training with the original learning rate of 0.0003 used by the Llama 3 leads to instability, such as many fast spikes, resulting in poor performance. Thus, we opt for smaller learning rate with more stable training curve. All experiments consume more than 800k GPU hours on the LUMI Supercomputer. However, we could not perform grid search on the learning rate due to the constraint of computing resources. cosine learning rate scheduler, with warm-up of 1,000 and 2,000 iterations for monolingual and bilingual mixes, respectively, is used to regulate learning dynamics. To optimize memory consumption, activation checkpointing is applied. Additionally, we leverage mixed-precision techniques, utilizing bfloat16 for computations while maintaining FP32 for gradient accumulation to balance efficiency and accuracy. 6 Table 2: Continual pre-trained models and settings. Base Model Data Mix Our Model Name Steps Warmup Tokens Llama 3 Llama 3.1 Monolingual (fig. 2b) EMMA-500 Llama 3 Mono 25,000 1,000 Bilingual (fig. 2a) 40,000 2,000 Monolingual (fig. 2b) EMMA-500 Llama 3.1 Mono 25,000 1,000 40,000 2,000 Bilingual (fig. 2a) EMMA-500 Llama 3.1 Bi EMMA-500 Llama 3 Bi 419B 671B 419B 671B"
        },
        {
            "title": "3 Evaluation and Discussion",
            "content": "This section is structured to systematically evaluate the overall performance of our models on multilingual and bilingual benchmarks, assessing improvements in both text understanding and generation. We analyze the impact of bilingual continual pre-training on multilingual language models and conduct ablation studies to isolate the contributions of bilingual pre-training compared to monolingual training. We also focus on low-resource language performance, demonstrating how continual pre-training enhances representation and generalization for underrepresented languages. No single model can be universally the best among all baselines across the full spectrum of multilingual tasks, benchmarks, and languages. We provide an analysis of language gains and failure cases, identifying which languages benefit the most and highlighting remaining challenges. Tasks and Benchmarks We evaluate all models on 7 tasks and 12 benchmarks that cover from 10 to 1500 languages. Table 11 shows the details of those tasks, benchmarks, evaluation metrics, the number of languages and samples per language, and the domain. We do not use LLMs-as-a-judge (Li et al., 2024a) for evaluation due to its well-known limitations, especially in multilingual scenarios, such as low agreement with human judges (de Wynter et al., 2024). Baselines We consider open-weight decoder-only models with 7-9B parameters as baselines. We primarily compare our CPT models with the original Llama 3/3.1 base models (Dubey et al., 2024) and the LlaMAX models (Lu et al., 2024) continually trained from Llama 3. We also compare with wide set of baselines (Section D.3), including (1) Llama 2 models and their CPT models; (2) multilingual models, including recent advances such as Aya 23 (Aryabumi et al., 2024), Gemma (Team et al., 2024), Qwen (Yang et al., 2024), and Marco-LLM (Ming et al., 2024)."
        },
        {
            "title": "3.1 Impact of Mono- vs. Bilingual Training",
            "content": "To understand the role of continual training with bilingual translation data, we conduct controlled ablation studies by comparing monolingual continual pre-training (with the monolingual corpus only, i.e., data mix 2 in Figure 2b) vs. bilingual continual pre-training (with monolingual and bilingual extension, i.e., data mix 1 in Figure 2a). Figure 3 compares their average performance on each benchmark. For CPT with Llama 3 as shown in Figure 3a, continual training on data mix with bilingual translation data consistently improves commonsense reasoning, natural language inference, reading comprehension, machine translation, and sometimes improves text classification when evaluated on Taxi1500, and retains similar summarization and math performance to the original base models. For CPT with Llama 3.1 as shown in Figure 3b, continual training on data mix with bilingual translation data consistently improves text classification, commonsense reasoning, and machine translation. Except for summarization tasks, our CPT models trained on the monolingual mix are usually better than on the bilingual mix. More remarkably, CPT with both Llama 3 and 3.1 shows large improvement on machine translation with an increase from 9% to 140% in terms of BLEU or chrF++ scores on translation directions from and to English on the Flores200 dataset. Our study provided insights into how bilingual texts contribute to language adaptation, transferability, and performance stability. Overall, continual training with bilingual translation data tends to improve the multilingual performance, especially for machine translation, which benefits directly from the use of parallel texts in training. 7 (a) Llama (b) Llama 3.1 Figure 3: Comparison of monolingual and bilingual CPT. The scores are averaged across all evaluated languages of the corresponding benchmarks. The baseline model LlaMAX does not have CPT variant trained on Llama 3.1. Our models show tendency for bilingual CPT to be better than monolingual CPT in most benchmarks and remarkable advance on the Flores200 translation benchmark."
        },
        {
            "title": "3.2 Low-Resource Language Performance",
            "content": "Low-resource languages often struggle with data scarcity and representation biases in multilingual models. We evaluate how baselines and our models perform on low-resource languages, and analyze whether continual pre-training on our data mixtures, especially the bilingual extension, can mitigate these limitations. We focus on two benchmarks, SIB-200 and Flores200, with more than 200 languages, and one benchmark, Taxi1500, with more than 1500 languages, and examine the low-resource languages of these benchmarks according to the categorization in Table 9. Table 3 shows the average performance on low-resource languages. Our EMMA-500 Llama 3 bilingual model obtained the best translation performance, followed by our Llama 3.1 bilingual one. They surpass the advanced Macro-LLM by large margin on English to other translations and small margin on other to English translations. For text classification, our models experience different levels of drops in the low-resource languages of SIB-200. Notably, our bilingual models perform the best on Taxi1500, showing their outstanding performance on low-resource languages validated on this massively multilingual benchmark. The results on low-resource languages show that CPT on massive parallel data enhances the performance on low-resource languages, especially for massively multilingual classification and translation. Table 3: Performance on low-resource languages. The text classification task uses SIB-200 (SIB) and Taxi1500 (Taxi) datasets. Underline and bold represents the absolute best, underline means the second best, and bold signifies the best within specific group. Our EMMA-500 models trained on bilingual data are the best two models in most cases. Model SIB Taxi Flores200 (Eng-X) Flores200 (X-Eng) ACC ACC chrF++ 16.23 22.74 18.04 Llama 2 7B 18.12 26.02 16.01 Llama 2 7B Chat 15.98 23.84 17.15 CodeLlama 2 7B 7.92 10.78 23.53 LLaMAX Llama 2 7B 30.45 28.66 15.57 LLaMAX Llama 2 7B Alpaca 23.13 24.76 6.64 MaLA-500 Llama 2 10B v1 6.93 19.04 22.72 MaLA-500 Llama 2 10B v2 16.04 25.03 17.98 YaYi Llama 2 7B 17.15 19.53 18.06 TowerBase Llama 2 7B 16.84 20.81 17.92 TowerInstruct Llama 2 7B 35.88 31.97 21.73 EMMA-500 Llama 2 7B 17.23 Occiglot Mistral 7B v0.1 33.27 22.62 16.92 Occiglot Mistral 7B v0.1 Instruct 34.87 19.59 12.32 17.73 14.93 BLOOM 7B 16.83 29.99 17.00 BLOOMZ 7B 14.20 36.40 16.19 YaYi 7B 16.89 42.20 22.52 Aya 23 8B 25.13 58.06 19.08 Aya Expanse 8B 24.75 59.97 16.55 Gemma 7B 28.50 47.19 21.48 Gemma 2 9B 18.87 48.94 8.18 Qwen 1.5 7B 56.06 23.01 18.17 Qwen 2 7B 18.46 54.85 17.82 Qwen 2.5 7B 24.79 65.07 21.97 Marco-LLM GLO 7B 65.53 23.61 25.84 Llama 3 8B 26.46 63.17 22.02 Llama 3.1 8B 4.93 49.70 22.91 LLaMAX Llama 3 8B 28.94 60.91 20.48 LLaMAX Llama 3 8B Alpaca EMMA-500 Llama 3 8B Mono 41.29 62.34 22.16 EMMA-500 Llama 3 8B Bi 45.22 39.65 26.72 EMMA-500 Llama 3.1 8B Mono 40.32 26.17 19.62 EMMA-500 Llama 3.1 8B Bi 63.33 25.42 45. BLEU chrF++ 31.26 32.61 29.47 14.08 44.08 13.89 15.76 32.30 32.44 26.32 48.00 32.13 32.61 28.51 35.63 21.25 33.31 37.85 45.65 41.36 37.01 38.90 40.11 45.89 45.72 46.13 4.87 47.43 53.16 56.69 50.34 55.84 4.93 5.30 4.57 0.86 13.36 0.65 0.55 4.81 5.10 3.54 16.87 4.69 4.31 2.67 7.48 4.23 6.41 6.78 9.56 12.71 5.93 5.57 5.71 9.51 10.51 10.66 0.49 12.44 22.03 25.99 21.06 25.80 BLEU 13.69 13.05 11.52 2.08 23.85 2.55 3.17 13.76 14.50 5.24 27.34 14.00 12.31 9.96 21.06 4.72 14.48 13.61 25.47 25.19 16.42 18.40 19.97 26.38 25.49 25.91 0.52 26.91 32.55 36.72 29.51 35."
        },
        {
            "title": "3.3 Model Adaptability",
            "content": "It is unrealistic to expect single model to perform the best across all tasks and benchmarks, given the inherent trade-offs in multilingual generalization, task specialization, and resource distribution. In practice, we experience some performance drop of our models on certain benchmarks. In this section, we analyze model adaptability by examining how CPT impacts performance when applied to different base models across wide range of languages. To quantify the effect, we compute the performance gain as the difference between the CPT model and its corresponding base model on each benchmark. We compare LLaMA 2, LLaMA 3, and LLaMA 3.1 as base models for continual pre-training, each offering progressively larger and more recent training corpora. LLaMA 2 was trained on 2T tokens, while LLaMA 3 significantly expands coverage with over 15T tokens. LLaMA 3.1 further updates the model with extensive data and long-context fine-tuning. This progression allows us to assess the adaptability of CPT across different model generations and data scales. Figure 4 shows the performance difference and the number of benchmarks where CPT models experience degradation. CPT on Llama 2 observes very small drop of BERTScore on MassiveSumm, where the score of the long subset decreases from 63.89 to 63.80 and the short one from 65.35 to 65.14. However, for Llama 3 and 3.1, both CPT on monolingual and bilingual mixes observe more cases of performance drops. For high-resource languages, Llama 3.1 degrades across more benchmarks than Llama 3. However, this trend diminishes notably when evaluating low-resource languages, where CPT remains effective to some extent. This comparison provides insight into the effectiveness of continual pre-training with monolingual and bilingual data mixes in adapting different LLMs with various training to diverse linguistic settings. Our results align with the findings from Springer et al. (2025) that over-trained language models are harder to fine-tune. At massively multilingual scale, we corroborate that continuing pretraining on well-trained modelsparticularly those already optimized for high-resource languages, e.g., Llama 3 and 3.1poses significant challenges when extending to hundreds of additional languages. Figure 4: Model adaptability measured by the number of benchmarks on which CPT models are worse than the base model. CPT on LLaMA 2 (L2 Mono) shows negligible BERTScore drop on MassiveSumm. More highly optimized models such as LLaMA 3 and 3.1 present greater challenges for effective continual pre-training compared to LLaMA 2, especially for high-resource languages, while the situation slightly eases for low-resource languages."
        },
        {
            "title": "3.4 Per-Language Performance",
            "content": "Despite notable improvements, multilingual adaptation continues to present challenges. To scrutinize per-language performance, we evaluate each model by (1) counting the number of baseline models it outperforms across languages (from total of 32 baselines; see Section D.3), and (2) calculating 9 Table 4: Percentage of top-k rankings across four multilingual benchmarks. The darker color indicates the better. EMMA-500especially with bilingual trainingconsistently outperforms baselines on Taxi1500 and Flores200. While Llama 3 and LlaMAX models yield higher accuracy on BELEBELE, EMMA-500 models cover more languages with improved relative performance. Models"
        },
        {
            "title": "BELEBELE",
            "content": "SIB-200 Taxi1500 Flores200 Top 3 Top 5 Top 10 Top 3 Top 5 Top 10 Top 3 Top 5 Top 10 Top 3 Top 5 Top 10 Llama 3 8B LLaMAX Llama 3 8B LLaMAX Llama 3 8B Alpaca EMMA-500 Llama 3 8B Mono EMMA-500 Llama 3 8B Bi 0.82 0.82 1.64 0.00 1.64 0.00 2.46 9.02 4.10 13. 25.41 43.41 65.37 0.49 11.48 7.32 5.37 23.90 20.49 37.70 30.24 43.41 2.93 10.24 44.26 7.17 13.80 95.61 6.10 19.38 35.12 6.10 3.45 77.07 76.10 14.60 22.50 20.49 32.45 47.38 25.12 0.00 0.00 38.16 0.99 0.00 0.00 58.73 44.83 0.00 0.00 10.62 44.79 96.06 1.97 77.34 78.23 88.18 98.52 100.00 the percentage of languages where the model ranks among the top performers. This provides fine-grained view of cross-lingual competitiveness. Table 4 presents the top-k performance percentage across four multilingual benchmarksBELEBELE, SIB-200, Taxi1500, and FLORES-200. The results demonstrate that EMMA-500 models, particularly those trained with bilingual data, achieve higher top-k percentages on low-resource-heavy benchmarks like Taxi-1500 and FLORES-200. In contrast, base LLaMA 3 and LLaMAX variants exhibit more limited top-tier rankings. Notably, for BELEBELE, machine comprehension benchmark, Llama 3 and 3.1 are not very strong models on this benchmark as evaluated in Section E.6. Existing CPT models, like LlaMAX Llama 3, and our CPT models obtained degraded performance. Despite having decreased average accuracy, our models have better performance for more languages than the base model and the LlaMAX Llama 3 model. We further look at the evaluation results of selected languages on BELEBELE in Table 5 for mix of low-resource (e.g., Khalkha Mongolian, Northern Sotho, Plateau Malagasy) and high-resource (e.g., English, French, Italian) languages. Table 5 reports absolute scores and the delta () between each CPT model and the Llama 3 8B base model. Results show that while CPT variants generally improve scores for low-resource languagesespecially in the EMMA-500 setupsthey tend to suffer from notable performance degradation in high-resource languages. However, this trade-off is arguably acceptable as the main focus of this paper is to adapt LLMs into low-resource languages. These analyses provide deeper understanding of the strengths and limitations of massively multilingual adaptation and guide future improvements in multilingual language model training and evaluation. First, they show averaging scores across languages to compare model performance, while convenient, has several important limitations, e.g., obscuring disparities in performance across languages and resources, and ignoring per-language variance. Second, this comparison of per-language performance highlights the benefits of our CPT models in promoting cross-lingual competitiveness. Table 5: Performance of selected languages in the BELEBELE benchmark. denotes the difference between the CPT models score and that of the base model. All models are based on Llama 3 8B. CPT on Llama 3 exhibits significant drop in some high-resource languages, much bigger than the increase in some low-resource languages. Model Average khk_Cyrl nso_Latn plt_Latn eng_Latn fra_Latn ita_Latn Llama 3 8B LLaMAX 40.73 36.96 35.11 34.67 29.22 30.33 30.44 34.11 74.56 62.33 60.89 48. 60.22 47.33 -3.77 -0.44 1.11 3. -12.23 -12.11 -12.89 LLaMAX Alpaca 39.41 35.56 30. 33.78 65.78 54.67 53.67 -1. 0.45 1.56 3.34 -8.78 -6.22 -6. EMMA-500 Mono 39.73 38.44 37.00 34.78 56.00 51. 49.56 EMMA-500 Bi -1.00 3. 7.78 4.34 -18.56 -9.78 -10.66 39. 39.33 35.22 33.00 56.78 49.67 46. -0.89 4.22 6.00 2.56 -17.78 -11. -14."
        },
        {
            "title": "3.5 Overall Results",
            "content": "We evaluate the model on comprehensive set of multilingual and bilingual benchmarks, assessing both language understanding and generation tasks. Tables 6 and 7 present the results of deterministic and generation tasks. Our EMMA-500 models are the best at machine translation (Flores200) and competitive at text classification (Taxi1500 and SIB-200) and commonsense reasoning (XCOPA and 10 XStoryCloze). While our models are competitive in many cases, we also observe some performance degradation, such asin math tasks Performance inevitably varies across tasks and languages, reflecting trade-offs in model capacity and data representation. Section presents deeper dive into the performance details across all tasks and languages evaluated, providing insights into model behavior across multilingual datasets and benchmarks. We also provide detailed per-language results, available at https://mala-lm.github.io/emma-500-v2 Table 6: Overall results of deterministic tasks including text classification, commonsense reasoning, natural language inference, reading comprehension, and math reasoning. XC: XCOPA; XSC: XStoryCloze; BELE: BELEBELE; ARC: the multilingual AI2 Reasoning Challenge; Dir.: MGSM by direct prompting; CoT: MGSM by CoT prompting. Underline and bold represents the absolute best, underline means the second best, and bold signifies the best within specific group. Our EMMA-500 models are among the top 2 models on text classification and commonsense reasoning."
        },
        {
            "title": "Model",
            "content": "Llama 2 7B Llama 2 7B Chat CodeLlama 2 7B LLaMAX Llama 2 7B LLaMAX Llama 2 7B Alpaca MaLA-500 Llama 2 10B v1 MaLA-500 Llama 2 10B v2 YaYi Llama 2 7B TowerBase Llama 2 7B TowerInstruct Llama 2 7B EMMA-500 Llama 2 7B Occiglot Mistral 7B v0.1 Occiglot Mistral 7B v0.1 Instruct BLOOM 7B BLOOMZ 7B YaYi 7B Aya 23 8B Aya Expanse 8B Gemma 7B Gemma 2 9B Qwen 1.5 7B Qwen 2 7B Qwen 2.5 7B Marco-LLM GLO 7B Llama 3 8B Llama 3.1 8B LLaMAX Llama 3 8B LLaMAX Llama 3 8B Alpaca EMMA-500 Llama 3 8B Mono EMMA-500 Llama 3 8B Bi EMMA-500 Llama 3.1 8B Mono EMMA-500 Llama 3.1 8B Bi"
        },
        {
            "title": "Math",
            "content": "SIB Taxi XC XSC XNLI BELE 22.41 25.58 23.35 10.61 27.89 23.25 19.30 24.57 19.34 20.53 31.27 32.69 34.31 17.82 29.73 35.76 41.50 57.01 58.21 46.25 47.95 54.95 53.89 64.15 63.70 61.42 48.60 58.97 60.62 39.40 26.16 62.07 17.54 15.44 17.03 23.52 15.09 25.27 23.39 17.73 17.73 17.29 19.82 22.26 18.76 14.76 16.96 16.12 22.64 18.73 13.83 18.05 7.29 21.87 17.87 21.99 21.73 20.20 23.01 17.71 22.32 25.13 19.71 24.87 56.67 55.85 54.69 54.38 56.60 53.09 53.09 56.71 56.33 57.05 63.11 56.67 56.55 56.89 54.87 56.64 55.13 56.38 63.64 66.33 59.44 60.31 61.71 62.45 61.71 61.71 63.04 64.36 66.20 66.82 65.38 67.25 57.55 58.41 55.68 60.36 63.83 53.07 53.07 58.42 57.78 59.24 66.38 58.10 59.39 59.30 57.12 60.67 60.93 64.80 65.01 67.67 59.85 61.46 62.06 63.87 63.41 63.58 64.31 68.27 67.36 68.35 67.64 68.47 40.19 38.58 40.19 44.27 45.09 38.11 38.11 41.28 39.84 40.36 45.14 42.35 40.81 41.60 37.13 39.87 43.12 45.56 42.58 46.74 39.47 42.77 43.31 43.99 44.97 45.62 44.13 45.08 44.15 45.15 39.98 44.67 26.27 29.05 27.38 23.09 24.48 22.96 22.96 28.32 26.36 27.93 26.75 30.16 32.05 24.11 39.32 37.97 40.08 46.98 43.37 54.49 41.83 49.31 54.11 53.95 40.73 45.19 36.96 39.41 39.73 39.84 38.86 37. ARC 27.56 28.02 25.23 26.09 31.06 21.16 21.16 28.40 27.94 30.10 29.53 29.77 30.88 23.65 23.95 24.44 31.08 36.56 38.68 44.15 28.93 33.82 35.30 36.34 34.80 34.93 33.54 34.53 33.22 34.84 34.00 34.59 Dir. CoT 6.69 10.22 5.93 3.35 5.05 0.91 0.91 7.09 6.15 7.24 17.02 13.31 22.76 2.87 2.55 2.76 22.29 43.02 38.22 32.95 31.56 48.95 53.78 51.85 27.45 28.36 20.80 14.18 23.53 23.49 24.95 23.85 6.36 10.91 6.64 3.62 6.35 0.73 0.73 7.22 6.16 8.24 18.09 14.07 22.16 2.29 2.15 3.02 24.71 41.45 35.78 44.69 30.36 51.47 55.60 52.02 28.13 27.31 19.96 17.16 25.33 26.29 27.35 25."
        },
        {
            "title": "4 Related Work",
            "content": "Multilingual Continual pre-training Multilingual language models have made significant progress in extending language understanding across diverse linguistic landscapes. Models such as mT5 (Xue et al., 2021) and BLOOM (Scao et al., 2022) have demonstrated strong multilingual capabilities by pre-training on extensive multilingual corpora. However, its performance on low-resource languages remains limited. Recent developments such as Qwen 2 (Yang et al., 2024) and Llama 3 (Dubey et al., 2024) have further improved the efficiency and scalability of LLMs and shown stronger multilingual capabilities. Continual pre-training has emerged as an effective strategy for expanding language models linguistic and domain coverage without requiring full retraining. Prior works such as MaLA500 (Lin et al., 2024), LlaMAX (Lu et al., 2024), and EMMA-500 (Ji et al., 2024a) have demonstrated the feasibility of continual pre-training on diverse multilingual datasets. These approaches highlight the importance of incremental learning to incorporate new languages and improve cross-lingual transfer. Despite these advancements, prior work has primarily focused on monolingual text pre-training, with limited emphasis on bilingual adaptation for improved cross-lingual transfer. Our work extends this line of research by introducing bilingual translation corpus for continual pre-training and 11 Table 7: Overall results of generation tasks including text summarization and machine translation. BERT: BERTScore; R-L: ROUGE-L. Underline and bold represents the absolute best, underline means the second best, and bold signifies the best within specific group. Our EMMA-500 models trained with bilingual mix are the best-performing models on machine translation."
        },
        {
            "title": "Model",
            "content": "Llama 2 7B Llama 2 7B Chat CodeLlama 2 7B LLaMAX Llama 2 7B LLaMAX Llama 2 7B Alpaca MaLA-500 Llama 2 10B v1 MaLA-500 Llama 2 10B v2 YaYi Llama 2 7B TowerBase Llama 2 7B TowerInstruct Llama 2 7B EMMA-500 Llama 2 7B Occiglot Mistral 7B v0.1 Occiglot Mistral 7B v0.1 Instruct BLOOM 7B BLOOMZ 7B YaYi 7B Aya 23 8B Aya Expanse 8B Gemma 7B Gemma 2 9B Qwen 1.5 7B Qwen 2 7B Qwen 2.5 7B Marco-LLM GLO 7B Llama 3 8B Llama 3.1 8B LLaMAX Llama 3 8B LLaMAX Llama 3 8B Alpaca EMMA-500 Llama 3 8B Mono EMMA-500 Llama 3 8B Bi EMMA-500 Llama 3.1 8B Mono EMMA-500 Llama 3.1 8B Bi Flores200 MassiveSumm-L MassiveSumm-S XL-Sum chrF++ 15.13 16.95 14.94 7.42 28.35 6.08 6.38 14.87 16.03 15.64 33.25 16.10 15.80 11.80 16.10 13.50 16.15 23.89 23.05 26.48 17.77 17.17 17.49 23.34 24.08 24.69 4.65 26.86 38.34 42.15 37.41 42.07 BLEU chrF++ 30.32 31.72 28.57 13.66 42.27 13.60 15.44 31.38 31.47 25.43 45.78 31.13 31.65 27.84 34.74 21.36 32.36 36.86 43.68 38.87 35.87 37.61 38.89 44.57 43.72 44.10 4.66 45.45 50.86 54.33 48.12 53. 4.62 4.95 4.27 0.80 12.51 0.60 0.54 4.41 4.83 3.23 15.58 4.32 3.99 2.81 7.44 4.37 6.46 6.88 9.05 12.09 5.87 5.56 5.72 9.27 9.93 10.11 0.45 11.64 20.33 24.02 19.44 23.86 BLEU 12.93 12.28 10.82 1.99 22.29 2.29 2.87 12.98 13.74 4.81 25.37 13.12 11.61 9.57 20.22 4.82 13.87 13.12 23.79 23.15 15.58 17.39 18.95 25.17 23.78 24.19 0.48 25.10 30.38 34.40 27.57 33.64 R-L 4.74 4.73 5.63 4.56 4.61 4.39 4.37 4.98 4.81 4.82 4.79 5.14 5.16 4.88 2.91 4.95 6.33 7.44 6.18 5.86 6.09 6.65 7.44 6.57 5.10 5.41 6.00 7.62 5.38 5.57 5.44 4.78 BERT 63.89 63.52 64.51 62.69 62.76 64.50 64.66 64.17 64.51 64.61 63.80 63.95 63.50 64.36 57.20 64.24 65.94 67.66 62.14 59.70 59.19 56.31 61.62 57.15 55.58 56.09 65.90 67.64 61.06 59.23 61.89 58.47 R-L 7.85 9.76 7.59 5.22 10.71 4.97 5.02 7.80 8.11 10.14 8.32 8.16 7.82 6.79 3.28 8.28 8.43 9.24 8.35 7.86 8.49 8.63 9.04 8.10 6.44 6.77 8.77 12.44 7.18 7.74 7.21 6.67 BERT 65.35 67.01 64.83 63.06 67.92 63.51 63.75 65.24 65.53 67.76 65.14 63.65 63.79 62.30 29.75 65.44 65.85 67.68 62.25 58.11 62.70 56.14 58.91 57.45 50.98 54.96 66.46 68.95 63.46 63.18 63.36 61. R-L 7.11 8.84 7.15 5.29 10.11 5.45 5.44 7.98 7.65 8.89 8.58 7.33 8.31 6.99 11.15 12.06 8.68 10.51 6.70 7.38 9.58 10.18 10.41 11.46 8.47 8.57 8.28 11.39 9.11 9.10 9.66 8.56 BERT 66.52 68.44 65.74 64.59 69.24 63.96 64.28 67.21 67.09 68.46 67.20 66.20 66.96 64.78 69.82 69.74 66.79 68.73 64.52 65.45 69.13 69.35 69.69 70.41 67.08 66.97 66.72 69.99 66.12 66.72 67.21 65.90 adapting Llama 3 and 3.1 (8B) to enhance bilingual and multilingual generalization, particularly for underrepresented languages. Training on Bilingual Translation Data Parallel texts have been used for pre-training LLMs. PolyLM (Wei et al., 2023) trained on 1 billion parallel multilingual data (0.16% of pre-training corpora). Poro (Luukkonen et al., 2024) trained on 8 billion English-Finnish cross-lingual texts (slightly under 1% of pre-training corpora). Li et al. (2024b) trained small language models, i.e., BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019), and BART (Lewis, 2019), in controlled setting with multilingual and bilingual texts using different learning objectives. In continual pretraining or fine-tuning, bilingual texts are also widely used. Ji et al. (2024b) showed continual pre-training of multilingual BART (Tang et al., 2020) with machine translation failed enhance crosslingual representation learning. Xu et al. (2024) fine-tuned LLMs with monolingual data and parallel data for machine translation. Kondo et al. (2024) showed that continual pre-training and supervised fine-tuning on parallel data can enhance the accuracy in English-Japanese translation. Similarly, Fujii et al. (2024) investigated cross-lingual continual pre-training on English and Japanese and showed that the use of parallel corpora enhanced translation performance. Li et al. (2025) studied the data mixing strategies for continual pre-training using monolingual and parallel texts and code data across 30+ languages. While previous studies have explored bilingual and multilingual pretraining with parallel texts, most efforts have been limited to small subset of languages or specific translation tasks. Unlike PolyLM and Poro, which incorporate modest fraction of bilingual data into predominantly monolingual training, our work systematically integrates bilingual texts across 500+ languages."
        },
        {
            "title": "5 Conclusion",
            "content": "This work advances massively multilingual adaptation of LLMs using bilingual translation data across 2,500+ language pairs (500+ languages). The four released EMMA-500 models using Llama 3 and 3.1 trained on both monolingual and bilingual data mixes, including the newly compiled MaLA translation corpus, establish new benchmarks in multilingual coverage while maintaining competitive performance across 7 diverse tasks. Notably, they achieve state-of-the-art results on machine translation while showing robust generalization to text classification and reasoning tasks. Despite advancements, achieving consistently high performance across diverse benchmarks is constrained by linguistic and task variability, which highlights intrinsic tensions between scale and specialization in multilingual NLP."
        },
        {
            "title": "Limitations",
            "content": "Multilingual Benchmark Multilingual language models are designed to accommodate users across various linguistic and cultural backgrounds. However, many widely used multilingual evaluation benchmarks, including some in this research, are developed through human or machine translation. As result, they often emphasize knowledge and subject matter primarily from English-speaking sources, with potential translation-related distortions that can undermine the accuracy of model assessments (Chen et al., 2024). This discrepancy highlights the need for more comprehensive, nativelanguage test sets that better reflect the full range of linguistic diversity. We encourage collaborative efforts to create large-scale benchmarks that offer more reliable evaluation of these models across different languages and cultures. Human Evaluation Although human assessment is valuable tool, it presents challenges such as variability, subjectivity, and high costsespecially when evaluating models across numerous languages. Recruiting expert annotators proficient in less common languages is particularly difficult, making large-scale human evaluation unfeasible. Even assessing limited selection of languages requires significant resources. While this study acknowledges these constraints, we recognize that human evaluations play crucial role in supplementing automated assessment methods. Given these limitations, we rely on automatic evaluation tools to ensure scalability and consistency, despite their imperfections. Model Performance Despite its strengths in multilingual processing, our models face challenges in areas such as mathematical reasoning and machine reading comprehension. Its performance on machine-translated mathematical benchmarks remains limited, likely due to the inherent difficulty of numerical reasoning and translation artifacts that may obscure problem clarity. Similarly, while our model achieves improvements in various NLP tasks, it struggles with reading comprehension, which demands deep contextual understanding and logical inference. Addressing these weaknesses will require further refinements, such as incorporating domain-specific training data or exploring alternative model architectures optimized for these challenges. Besides, averaging scores across languages can mask important disparities, such as strong performance on high-resource languages and poor results on low-resource ones. It also ignores linguistic diversity and per-language difficulty, potentially leading to misleading conclusions about overall model performance. Real-world Usage This research primarily focuses on enhancing continual training with bilingual texts and improving language model performance through continual pre-training. However, the model is not yet suitable for deployment in real-world applications. It has not undergone thorough human alignment processes or adversarial robustness testing (red-teaming) to ensure safety and reliability. While our work contributes to advancements in multilingual NLP, additional refinements such as aligning the model with human preferences and conducting rigorous safety evaluationsare necessary before it can be practically implemented. Data Mix We examine two manually constructed data mixes to assess the impact of CPT with bilingual translation data. While numerous alternative configurations are possible, systematically validating the effectiveness of different data mixes or searching for the optimal data mixes requires lot of computing resources, which are not affordable for small research team like us. Nonetheless, the two data mixes carefully decided by us already demonstrate measurable improvements in the downstream evaluation, which highlight the utility of our design choices. Model Training Model training involves few hyperparameters, and wide range of base models can be selected for continual pre-training. When preparing the bilingual translation data for training, the number of lines is intuitively chosen. However, an exhaustive search over all possible configuration combinationsincluding data volume, model choice, and hyperparameter settingswould incur prohibitive computational costs. Consequently, we focus on limited set of configurations that are feasible under available resources. Community Collaboration This study was conducted without direct community collaboration. Nonetheless, we recognize the value of community collaboration and are open to future partnerships with researchers and practitioners in this area."
        },
        {
            "title": "Acknowledgment",
            "content": "The work has received funding from the European Unions Horizon Europe research and innovation programme under grant agreement No 101070350 and from UK Research and Innovation (UKRI) under the UK governments Horizon Europe funding guarantee [grant number 10052546], and the Digital Europe Programme under grant agreement No 101195233. The authors wish to acknowledge CSC IT Center for Science, Finland, for computational resources."
        },
        {
            "title": "References",
            "content": "Aakanksha, A. Ahmadian, B. Ermis, S. Goldfarb-Tarrant, J. Kreutzer, M. Fadaee, and S. Hooker. The multilingual alignment prism: Aligning global and local preferences to reduce harm, 2024. URL https://arxiv.org/abs/2406.18682. D. I. Adelani, J. O. Alabi, A. Fan, J. Kreutzer, X. Shen, M. Reid, D. Ruiter, D. Klakow, P. Nabende, E. Chang, et al. few thousand translations go long way! leveraging pre-trained models for african news translation. arXiv preprint arXiv:2205.02022, 2022. URL https://aclanthology. org/2022.naacl-main.223/. D. I. Adelani, H. Liu, X. Shen, N. Vassilyev, J. O. Alabi, Y. Mao, H. Gao, and E. A. Lee. SIB-200: simple, inclusive, and big evaluation dataset for topic classification in 200+ languages and dialects. CoRR, abs/2309.07445, 2023. doi: 10.48550/arXiv.2309.07445. URL https://doi.org/ 10.48550/arXiv.2309.07445. D. M. Alves, J. Pombal, N. M. Guerreiro, P. H. Martins, J. Alves, A. Farajian, B. Peters, R. Rei, P. Fernandes, S. Agrawal, et al. Tower: An open multilingual large language model for translationrelated tasks. arXiv preprint arXiv:2402.17733, 2024. URL https://arxiv.org/abs/2402.17733. C. Amrhein, N. Moghe, and L. Guillou. ACES: Translation accuracy challenge sets for evaluating machine translation metrics. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 479513, Abu Dhabi, United Arab Emirates (Hybrid), Dec. 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.wmt-1.44. A. Andonian, Q. Anthony, S. Biderman, S. Black, P. Gali, L. Gao, E. Hallahan, J. Levy-Kramer, C. Leahy, L. Nestler, K. Parker, M. Pieler, J. Phang, S. Purohit, H. Schoelkopf, D. Stander, T. Songz, C. Tigges, B. Thérien, P. Wang, and S. Weinbach. GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch, 9 2023. URL https://www.github.com/eleutherai/gpt-neox. V. Aryabumi, J. Dang, D. Talupuru, S. Dash, D. Cairuz, H. Lin, B. Venkitesh, M. Smith, J. A. Campos, Y. C. Tan, et al. Aya 23: Open weight releases to further multilingual progress. arXiv preprint arXiv:2405.15032, 2024. 14 M. Aulamo, S. Virpioja, and J. Tiedemann. OpusFilter: configurable parallel corpus filtering toolbox. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 150156. Association for Computational Linguistics, July 2020. doi: 10.18653/v1/2020.acl-demos.20. URL https://www.aclweb.org/anthology/2020.acl-demos. 20. J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. L. Bandarkar, D. Liang, B. Muller, M. Artetxe, S. N. Shukla, D. Husa, N. Goyal, A. Krishnan, L. Zettlemoyer, and M. Khabsa. The BELEBELE benchmark: parallel reading comprehension dataset in 122 language variants. arXiv preprint arXiv:2308.16884, 2023. URL https://aclanthology.org/2024.acl-long.44/. P. Chen, S. Yu, Z. Guo, and B. Haddow. Is it good data for multilingual instruction tuning or just bad multilingual evaluation for large language models? arXiv preprint arXiv:2406.12822, 2024. URL https://arxiv.org/abs/2406.12822. L. Chiruzzo, P. Amarilla, A. Ríos, and G. Giménez Lugo. Development of Guarani - Spanish parallel corpus. In N. Calzolari, F. Béchet, P. Blache, K. Choukri, C. Cieri, T. Declerck, S. Goggi, H. Isahara, B. Maegaard, J. Mariani, H. Mazo, A. Moreno, J. Odijk, and S. Piperidis, editors, Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 26292633, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL <https://aclanthology.org/2020.lrec-1.320>. P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv, abs/1803.05457, 2018. K. Cobbe, V. Kosaraju, M. Bavarian, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems, 2021. A. Conneau, R. Rinott, G. Lample, A. Williams, S. R. Bowman, H. Schwenk, and V. Stoyanov. Xnli: Evaluating cross-lingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2018. M. R. Costa-jussà, J. Cross, O. Çelebi, M. Elbayad, K. Heafield, K. Heffernan, E. Kalbassi, J. Lam, D. Licht, J. Maillard, et al. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672, 2022. J. Dang, S. Singh, D. Dsouza, A. Ahmadian, A. Salamanca, M. Smith, A. Peppin, S. Hong, M. Govindassamy, T. Zhao, et al. Aya expanse: Combining research breakthroughs for new multilingual frontier. arXiv preprint arXiv:2412.04261, 2024. A. de Wynter, I. Watts, T. Wongsangaroonsri, M. Zhang, N. Farra, N. E. Altıntoprak, L. Baur, S. Claudet, P. Gajdusek, C. Gören, Q. Gu, A. Kaminska, T. Kaminski, R. Kuo, A. Kyuba, J. Lee, K. Mathur, P. Merok, I. Milovanović, N. Paananen, V.-M. Paananen, A. Pavlenko, B. P. Vidal, L. Strika, Y. Tsao, D. Turcato, O. Vakhno, J. Velcsov, A. Vickers, S. Visser, H. Widarmanto, A. Zaikin, and S.-Q. Chen. RTP-LX: Can LLMs evaluate toxicity in multilingual scenarios? arXiv preprint 2404.14397, 2024. URL https://arxiv.org/abs/2404.14397. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2019. A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. EdTeKLA Research Group. Indigenous languages corpora. IndigenousLanguages_Corpora, 2022. Accessed: 2024-05-30. https://github.com/EdTeKLA/ M. Faysse. Dataset card for \"project gutenberg\", 2023. URL https://huggingface.co/datasets/ manu/project_gutenberg. K. Fujii, T. Nakamura, M. Loem, H. Iida, M. Ohi, K. Hattori, H. Shota, S. Mizuki, R. Yokota, and N. Okazaki. Continual pre-training for cross-lingual llm adaptation: Enhancing japanese language capabilities. arXiv preprint 2404.17790, 2024. URL https://arxiv.org/abs/2404.17790. K. Fujii, Y. Tajima, S. Mizuki, H. Shimada, T. Shiotani, K. Saito, M. Ohi, M. Kawamura, T. Nakamura, T. Okamoto, S. Ishida, K. Hattori, Y. Ma, H. Takamura, R. Yokota, and N. Okazaki. Rewriting pre-training data boosts llm performance in math and code, 2025. URL https://arxiv.org/abs/ 2505.02881. A.-P. Galarreta, A. Melgar, and A. Oncevay. Corpus creation and initial SMT experiments between Spanish and Shipibo-konibo. In R. Mitkov and G. Angelova, editors, Proceedings of the International Conference Recent Advances in Natural Language Processing, RANLP 2017, pages 238244, Varna, Bulgaria, Sept. 2017. INCOMA Ltd. doi: 10.26615/978-954-452-049-6_033. URL https://doi. org/10.26615/978-954-452-049-6_033. L. Gao, J. Tow, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, K. McDonell, N. Muennighoff, et al. framework for few-shot language model evaluation. Zenodo, 2023. Y. Gbedevi. Ewe language corpus, 2019. URL https://www.kaggle.com/datasets/yvicherita/ ewe-language-corpus. Accessed: 2024-08-27. X. Gutierrez-Vasques, G. Sierra, and I. H. Pompa. Axolotl: web accessible parallel corpus for SpanishNahuatl. In N. Calzolari, K. Choukri, T. Declerck, S. Goggi, M. Grobelnik, B. Maegaard, J. Mariani, H. Mazo, A. Moreno, J. Odijk, and S. Piperidis, editors, Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC16), pages 42104214, Portorož, Slovenia, May 2016. European Language Resources Association (ELRA). URL https://aclanthology.org/ L16-1666. T. Hasan, A. Bhattacharjee, M. S. Islam, K. Mubasshir, Y.-F. Li, Y.-B. Kang, M. S. Rahman, and R. Shahriyar. XL-sum: Large-scale multilingual abstractive summarization for 44 languages. In C. Zong, F. Xia, W. Li, and R. Navigli, editors, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 46934703, Online, Aug. 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.413. URL https://aclanthology.org/2021. findings-acl.413. E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9. S. Hu, Y. Tu, X. Han, C. He, G. Cui, X. Long, Z. Zheng, Y. Fang, Y. Huang, W. Zhao, X. Zhang, Z. L. Thai, K. Zhang, C. Wang, Y. Yao, C. Zhao, J. Zhou, J. Cai, Z. Zhai, N. Ding, C. Jia, G. Zeng, D. Li, Z. Liu, and M. Sun. Minicpm: Unveiling the potential of small language models with scalable training strategies. CoRR, abs/2404.06395, 2024. doi: 10.48550/ARXIV.2404.06395. URL https://doi.org/10.48550/arXiv.2404.06395. H. Huang, T. Tang, D. Zhang, X. Zhao, T. Song, Y. Xia, and F. Wei. Not all languages are created equal in LLMs: Improving multilingual capability by cross-lingual-thought prompting. In H. Bouamor, J. Pino, and K. Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1236512394, Singapore, Dec. 2023. Association for Computational Linguistics. doi: 10. 18653/v1/2023.findings-emnlp.826. URL https://aclanthology.org/2023.findings-emnlp. 826/. K. Huang, F. Mo, X. Zhang, H. Li, Y. Li, Y. Zhang, W. Yi, Y. Mao, J. Liu, Y. Xu, J. Xu, J.-Y. Nie, and Y. Liu. survey on large language models with multilingualism: Recent advances and new frontiers, 2025. URL https://arxiv.org/abs/2405.10936. S. Ji, Z. Li, I. Paul, J. Paavola, P. Lin, P. Chen, D. OBrien, H. Luo, H. Schütze, J. Tiedemann, et al. Emma-500: Enhancing massively multilingual adaptation of large language models. arXiv preprint arXiv:2409.17892, 2024a. S. Ji, T. Mickus, V. Segonne, and J. Tiedemann. Can machine translation bridge multilingual pretraining and cross-lingual transfer learning? In Proceedings of LREC-COLING, 2024b. 16 A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed. Mistral 7b, 2023. URL https://arxiv.org/abs/2310.06825. E. Joanis, R. Knowles, R. Kuhn, S. Larkin, P. Littell, C.-k. Lo, D. Stewart, and J. Micher. The Nunavut Hansard InuktitutEnglish parallel corpus 3.0 with preliminary machine translation results. In N. Calzolari, F. Béchet, P. Blache, K. Choukri, C. Cieri, T. Declerck, S. Goggi, H. Isahara, B. Maegaard, J. Mariani, H. Mazo, A. Moreno, J. Odijk, and S. Piperidis, editors, Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 25622572, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL https: //aclanthology.org/2020.lrec-1.312. A. H. Kargaran, A. Imani, F. Yvon, and H. Schütze. GlotLID: Language identification for low-resource languages. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. URL https://openreview.net/forum?id=dl4e3EBz5j. D. P. Kingma and J. Ba. Adam: method for stochastic optimization. In International Conference for Learning Representations, 2015. D. Kocetkov, R. Li, L. B. Allal, J. Li, C. Mou, Y. Jernite, M. Mitchell, C. M. Ferrandis, S. Hughes, T. Wolf, D. Bahdanau, L. von Werra, and H. de Vries. The stack: 3 TB of permissively licensed source code. Trans. Mach. Learn. Res., 2023, 2023. URL https://openreview.net/forum?id=pxpbTdUEpD. M. Kondo, T. Utsuro, and M. Nagata. Enhancing translation accuracy of large language models through continual pre-training on parallel data. arXiv preprint 2407.03145, 2024. URL https: //arxiv.org/abs/2407.03145. W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. V. D. Lai, C. V. Nguyen, N. T. Ngo, T. Nguyen, F. Dernoncourt, R. A. Rossi, and T. H. Nguyen. Okapi: Instruction-tuned large language models in multiple languages with reinforcement learning from human feedback. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 318327, 2023. W. Lai, M. Mesgar, and A. Fraser. LLMs beyond English: Scaling the multilingual capability of LLMs with cross-lingual feedback. In L.-W. Ku, A. Martins, and V. Srikumar, editors, Findings of the Association for Computational Linguistics ACL 2024, pages 81868213, Bangkok, Thailand and virtual meeting, Aug. 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.findings-acl.488. M. Lewis. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. H. Li, Q. Dong, J. Chen, H. Su, Y. Zhou, Q. Ai, Z. Ye, and Y. Liu. Llms-as-judges: comprehensive survey on llm-based evaluation methods, 2024a. URL https://arxiv.org/abs/2412.05579. Y. Li, Y. Zhang, Z. Zhao, L. Shen, W. Liu, W. Mao, and H. Zhang. CSL: large-scale chinese scientific literature dataset. In Proceedings of the 29th International Conference on Computational Linguistics, pages 39173923, 2022. Z. Li, S. Ji, T. Mickus, V. Segonne, and J. Tiedemann. comparison of language modeling and translation as multilingual pretraining objectives. In Proceedings of EMNLP, 2024b. Z. Li, S. Ji, H. Luo, and J. Tiedemann. Rethinking multilingual continual pretraining: Data mixing for adapting llms across languages and resources. arXiv preprint 2504.04152, 2025. URL https: //arxiv.org/abs/2504.04152. C.-Y. Lin. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-1013. P. Lin, S. Ji, J. Tiedemann, A. F. Martins, and H. Schütze. MaLA-500: Massive language adaptation of large language models. arXiv preprint arXiv:2401.13303, 2024. X. V. Lin, T. Mihaylov, M. Artetxe, T. Wang, S. Chen, D. Simig, M. Ott, N. Goyal, S. Bhosale, J. Du, et al. Few-shot learning with multilingual generative language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 90199052, 2022. A. Lozhkov, R. Li, L. B. Allal, F. Cassano, J. Lamy-Poirier, N. Tazi, A. Tang, D. Pykhtar, J. Liu, Y. Wei, et al. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173, 2024. Y. Lu, W. Zhu, L. Li, Y. Qiao, and F. Yuan. LLaMAX: Scaling linguistic horizons of LLM by enhancing translation capabilities beyond 100 languages. arXiv preprint arXiv:2407.05975, 2024. R. Luukkonen, J. Burdge, E. Zosa, A. Talman, V. Komulainen, V. Hatanpää, P. Sarlin, and S. Pyysalo. Poro 34b and the blessing of multilinguality. arXiv preprint 2404.01856, 2024. URL https: //arxiv.org/abs/2404.01856. C. Ma, A. ImaniGooghari, H. Ye, E. Asgari, and H. Schütze. Taxi1500: multilingual dataset for text classification in 1500 languages, 2023. M. Mager, D. Carrillo, and I. Meza. Probabilistic finite-state morphological segmenter for wixarika (huichol) language. Journal of Intelligent & Fuzzy Systems, 34(5):30813087, 2018. P. Maini, S. Seto, R. H. Bai, D. Grangier, Y. Zhang, and N. Jaitly. Rephrasing the web: recipe for compute and data-efficient language modeling. In L. Ku, A. Martins, and V. Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1404414072. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.757. URL https: //doi.org/10.18653/v1/2024.acl-long.757. Masakhane. lacuna_pos_ner: POS and NER for african languages. https://github.com/ masakhane-io/lacuna_pos_ner, 2022. Accessed: 2024-05-30. T. Mayer and M. Cysouw. Creating massively parallel bible corpus. In N. Calzolari, K. Choukri, T. Declerck, H. Loftsson, B. Maegaard, J. Mariani, A. Moreno, J. Odijk, and S. Piperidis, editors, Proceedings of the Ninth International Conference on Language Resources and Evaluation, LREC 2014, Reykjavik, Iceland, May 26-31, 2014, pages 31583163. European Language Resources Association (ELRA), 2014. URL http://www.lrec-conf.org/proceedings/lrec2014/summaries/220.html. L. Ming, B. Zeng, C. Lyu, T. Shi, Y. Zhao, X. Yang, Y. Liu, Y. Wang, L. Xu, Y. Liu, X. Zhao, H. Wang, H. Liu, H. Zhou, H. Yin, Z. Shang, H. Li, L. Wang, W. Luo, and K. Zhang. Marco-llm: Bridging languages via massive multilingual training for cross-lingual enhancement, 2024. URL https: //arxiv.org/abs/2412.04003. N. Muennighoff, T. Wang, L. Sutawika, A. Roberts, S. Biderman, T. L. Scao, M. S. Bari, S. Shen, Z.-X. Yong, H. Schoelkopf, et al. Crosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786, 2022. J. Mukiibi, B. Claire, and N.-N. Joyce. An english-luganda parallel corpus, May 2021. URL https: //doi.org/10.5281/zenodo.4764039. NLLB Team, M. R, J. Cross, O. Çelebi, M. Elbayad, K. Heafield, K. Heffernan, E. Kalbassi, J. Lam, D. Licht, J. Maillard, et al. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672, 2022. J. E. Ortega, R. A. Castro-Mamani, and J. R. Montoya Samame. Overcoming resistance: The normalization of an Amazonian tribal language. In A. Karakanta, A. K. Ojha, C.-H. Liu, J. Abbott, J. Ortega, J. Washington, N. Oco, S. M. Lakew, T. A. Pirinen, V. Malykh, V. Logacheva, and X. Zhao, editors, Proceedings of the 3rd Workshop on Technologies for MT of Low Resource Languages, pages 113, Suzhou, China, Dec. 2020. Association for Computational Linguistics. URL https://aclanthology.org/2020.loresmt-1.1. K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: method for automatic evaluation of machine translation. In P. Isabelle, E. Charniak, and D. Lin, editors, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL https://aclanthology.org/P02-1040. I. Paul, G. Glavas, and I. Gurevych. Ircoder: Intermediate representations make language models robust multilingual code generators. In L. Ku, A. Martins, and V. Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1502315041. Association for Computational Linguistics, 2024. URL https://aclanthology.org/2024.acl-long.802. I. Paul, H. Yang, G. Glavas, K. Kersting, and I. Gurevych. Obscuracoder: Powering efficient code LM pre-training via obfuscation grounding. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https: //openreview.net/forum?id=VYvxrD7aS0. J. Petty, S. van Steenkiste, and T. Linzen. How does code pretraining affect language model task performance? CoRR, abs/2409.04556, 2024. doi: 10.48550/ARXIV.2409.04556. URL https: //doi.org/10.48550/arXiv.2409.04556. E. M. Ponti, G. Glavaš, O. Majewska, Q. Liu, I. Vulić, and A. Korhonen. XCOPA: multilingual dataset for causal commonsense reasoning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020. M. Popović. chrF: character n-gram F-score for automatic MT evaluation. In O. Bojar, R. Chatterjee, C. Federmann, B. Haddow, C. Hokamp, M. Huck, V. Logacheva, and P. Pecina, editors, Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392395, Lisbon, Portugal, Sept. 2015. Association for Computational Linguistics. doi: 10.18653/v1/W15-3049. URL https: //aclanthology.org/W15-3049. M. Post. call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186191, Belgium, Brussels, Oct. 2018. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/W18-6319. R. Puri, D. S. Kung, G. Janssen, W. Zhang, G. Domeniconi, V. Zolotov, J. Dolby, J. Chen, M. R. Choudhury, L. Decker, V. Thost, L. Buratti, S. Pujar, S. Ramji, U. Finkler, S. Malaika, and F. Reiss. Codenet: large-scale AI for code dataset for learning diversity of coding tasks. In J. Vanschoren and S. Yeung, editors, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ a5bfc9e07964f8dddeb95fc584cd965d-Abstract-round2.html. Qwen, :, A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei, H. Lin, J. Yang, J. Tu, J. Zhang, J. Yang, J. Yang, J. Zhou, J. Lin, K. Dang, K. Lu, K. Bao, K. Yang, L. Yu, M. Li, M. Xue, P. Zhang, Q. Zhu, R. Men, R. Lin, T. Li, T. Tang, T. Xia, X. Ren, X. Ren, Y. Fan, Y. Su, Y. Zhang, Y. Wan, Y. Liu, Z. Cui, Z. Zhang, and Z. Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023. L. Ruis, M. Mozes, J. Bae, S. R. Kamalakara, D. Gnaneshwar, A. Locatelli, R. Kirk, T. Rocktäschel, E. Grefenstette, and M. Bartolo. Procedural knowledge in pretraining drives reasoning in large language models. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id= 1hQKHHUsMx. 19 T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilić, D. Hesslow, R. Castagné, A. S. Luccioni, F. Yvon, M. Gallé, et al. BLOOM: 176B-parameter open-access multilingual language model. arXiv preprint, 2022. F. Shi, M. Suzgun, M. Freitag, X. Wang, S. Srivats, S. Vosoughi, H. W. Chung, Y. Tay, S. Ruder, D. Zhou, D. Das, and J. Wei. Language models are multilingual chain-of-thought reasoners, 2022. A. Sindhujan, D. Kanojia, C. Orasan, and S. Qian. When llms struggle: Reference-less translation evaluation for low-resource languages, 2025. URL https://arxiv.org/abs/2501.04473. L. Soldaini and K. Lo. peS2o (Pretraining Efficiently on S2ORC) Dataset. Technical report, Allen Institute for AI, 2023. ODC-By, https://github.com/allenai/pes2o. L. Soldaini, R. Kinney, A. Bhagia, D. Schwenk, D. Atkinson, R. Authur, B. Bogin, K. R. Chandu, J. Dumas, Y. Elazar, V. Hofmann, A. H. Jha, S. Kumar, L. Lucy, X. Lyu, N. Lambert, I. Magnusson, J. Morrison, N. Muennighoff, A. Naik, C. Nam, M. E. Peters, A. Ravichander, K. Richardson, Z. Shen, E. Strubell, N. Subramani, O. Tafjord, P. Walsh, L. Zettlemoyer, N. A. Smith, H. Hajishirzi, I. Beltagy, D. Groeneveld, J. Dodge, and K. Lo. Dolma: an open corpus of three trillion tokens for language model pretraining research. In L. Ku, A. Martins, and V. Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1572515788. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.840. URL https://doi.org/10.18653/ v1/2024.acl-long.840. J. M. Springer, S. Goyal, K. Wen, T. Kumar, X. Yue, S. Malladi, G. Neubig, and A. Raghunathan. Overtrained language models are harder to fine-tune. arXiv preprint arXiv:2503.19206, 2025. D. Su, K. Kong, Y. Lin, J. Jennings, B. Norick, M. Kliegl, M. Patwary, M. Shoeybi, and B. Catanzaro. Nemotron-cc: Transforming common crawl into refined long-horizon pretraining dataset. CoRR, abs/2412.02595, 2024. doi: 10.48550/ARXIV.2412.02595. URL https://doi.org/10.48550/ arXiv.2412.02595. M. Szafraniec, B. Rozière, H. Leather, P. Labatut, F. Charton, and G. Synnaeve. Code translation with compiler representations. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview. net/forum?id=XomEU3eNeSQ. Y. Tang, C. Tran, X. Li, P.-J. Chen, N. Goyal, V. Chaudhary, J. Gu, and A. Fan. Multilingual translation with extensible multilingual pretraining and finetuning. arXiv preprint arXiv:2008.00401, 2020. R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto. Stanford Alpaca: An instruction-following LLaMA model. https://github.com/tatsu-lab/stanford_ alpaca, 2023. G. Team, M. Riviere, S. Pathak, P. G. Sessa, C. Hardin, S. Bhupatiraju, L. Hussenot, T. Mesnard, B. Shahriari, A. Ramé, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. J. Tiedemann. Parallel data, tools and interfaces in OPUS. In N. Calzolari, K. Choukri, T. Declerck, M. U. Doğan, B. Maegaard, J. Mariani, A. Moreno, J. Odijk, and S. Piperidis, editors, Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC12), pages 22142218, Istanbul, Turkey, May 2012. European Language Resources Association (ELRA). URL http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf. J. Tiedemann. The Tatoeba Translation Challenge Realistic data sets for low resource and multilingual MT. In Proceedings of the Fifth Conference on Machine Translation, pages 11741182, Online, Nov. 2020. Association for Computational Linguistics. URL https://www.aclweb.org/ anthology/2020.wmt-1.139. H. Touvron, T. Lavril, G. Izacard, X. Martinet, M. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. LLaMA: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023a. doi: 10.48550/arXiv.2302.13971. URL https: //doi.org/10.48550/arXiv.2302.13971. 20 H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint, 2023b. C. M. University. Haitian creole data, 2010. URL http://www.speech.cs.cmu.edu/haitian/. Accessed: 2024-05-30. D. Varab and N. Schluter. MassiveSumm: very large-scale, very multilingual, news summarisation dataset. In M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1015010161, Online and Punta Cana, Dominican Republic, Nov. 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.797. URL https://aclanthology.org/2021.emnlp-main.797/. J. M. L. C. Velázquez. Spanish-hnähnü corpus, 2021. URL https://tsunkua.elotl.mx/about/. W. Wang, Z. Tu, C. Chen, Y. Yuan, J.-t. Huang, W. Jiao, and M. R. Lyu. All languages matter: On the multilingual safety of large language models. arXiv preprint arXiv:2310.00905, 2023. J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. H. Chi, Q. V. Le, D. Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=_VjQlMeSB_J. X. Wei, H. Wei, H. Lin, T. Li, P. Zhang, X. Ren, M. Li, Y. Wan, Z. Cao, B. Xie, T. Hu, S. Li, B. Hui, B. Yu, D. Liu, B. Yang, F. Huang, and J. Xie. Polylm: An open source polyglot large language model. arXiv preprint 2307.06018, 2023. URL https://arxiv.org/abs/2307.06018. G. I. Winata, A. F. Aji, S. Cahyawijaya, R. Mahendra, F. Koto, A. Romadhony, K. Kurniawan, D. Moeljadi, R. E. Prasojo, P. Fung, T. Baldwin, J. H. Lau, R. Sennrich, and S. Ruder. Nusax: Multilingual parallel sentiment dataset for 10 indonesian local languages. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 815834, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.eacl-main.57. T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, et al. Huggingfaces transformers: State-of-the-art natural language processing. arXiv preprint, 2019. H. Xu, Y. J. Kim, A. Sharaf, and H. H. Awadalla. paradigm shift in machine translation: Boosting translation performance of large language models. arXiv preprint 2309.11674, 2024. URL https: //arxiv.org/abs/2309.11674. L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, and C. Raffel. mT5: massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483498, 2021. A. Yang, B. Yang, B. Hui, B. Zheng, B. Yu, C. Zhou, C. Li, C. Li, D. Liu, F. Huang, et al. Qwen technical report. arXiv preprint arXiv:2407.10671, 2024. F. Yuan, Y. Lu, W. Zhu, L. Kong, L. Li, Y. Qiao, and J. Xu. Lego-MT: Learning detachable models for massively multilingual machine translation. In A. Rogers, J. Boyd-Graber, and N. Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 1151811533, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. findings-acl.731. URL https://aclanthology.org/2023.findings-acl.731. S. Zhang, B. Frey, and M. Bansal. Chren: Cherokee-english machine translation for endangered language revitalization. In EMNLP2020, 2020. T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675, 2019."
        },
        {
            "title": "Goals and Achievements",
            "content": "Q: What is the main goal of this paper? The main goal of this paper, as suggested by its title, is the perform massively multilingual adaptation of large language models using bilingual translation data. Specifically, we compile massive parallel corpus, compose two data mixes, perform continual pre-training of large language models and comprehensive evaluation, and examine interesting research questions such as the effect of using bilingual translation data. Q: What are the main achievements and the scientific novelty? A: Our main achievements are summarized as the contributions as written in the Section 1 Introduction. For continual pre-training, we do not modify the model architecturewe do not claim this novelty. However, our scientific novelty is more on the resource construction side, including the creation of MaLA translation corpus and continually pre-trained EMMA-500 models based on Llama 3 and 3.1 on two diverse data mixes. Besides, our empirical evaluation reveals some valuable findings that can potentially promote future development."
        },
        {
            "title": "Data",
            "content": "Q: How was the data mix tuned? Data mix optimization is distinct research area. Our focus was on creating robust bilingual translation corpus through careful curation for language and content diversity. While we didnt perform specific data mix ablations, we ensured balanced representation across languages and domains and compared two data mixes with or without bilingual translation data to examine the effect of including parallel data for continual pre-training. Searching for the optimal data composition requires lot of computing resources and remains an interesting direction for future work. Q: Does whitespace cleaning only apply to whitespace-tokenized languages, or do other languages require different cleaning methods? A: Whitespace cleaning is mainly relevant for whitespace-tokenized languages (like English) to standardize text for processing. However, non-whitespace-tokenized languages (e.g., Chinese, Japanese, Korean) may not need this step but often require other pre-processing, such as character normalization or segmentation. Q: How were the existing datasets and their languages selected for inclusion? A: The datasets and languages were chosen based on Ji et al. (2024a) adopting the the following criteria: Language Coverage: Emphasis on broad multilingual representation, including both highand low-resource languages. Data Quality: Preference for high-quality, diverse content (books, papers, code) to enhance model generalization. Task Relevance: Alignment with goals for continual pre-training and multilingual adaptation (e.g., translation, classification, reasoning). Availability: Prioritization of publicly accessible datasets to ensure transparency and reproducibility."
        },
        {
            "title": "Evaluation",
            "content": "Q: How do we determine the number of shots for in-context learning? A: We use the same number of shots as Ji et al. (2024a), where the number of shots used in evaluation follows the setup from Llama 2, which demonstrates strong performance on high-resource languages with this configuration. Rather than performing an exhaustive grid search for optimal shot count, we maintain this established approach. We provide the model weights, enabling researchers to experiment with different numbers of shots according to their specific evaluation needs. 22 Q: Why no human evaluation? A: Human evaluation faces scalability challenges in multilingual settings due to subjectivity, cultural biases, and high costsespecially for low-resource languages requiring specialized annotators. While valuable, these limitations make automated metrics more practical for our large-scale study. We acknowledge human evaluations importance, but emphasize its unsustainability for massively multilingual work. Q: How reliable is machine-generated benchmark? A: Machine-generated benchmarks provide scalable, consistent, and objective evaluation across many tasks and languages, enabling efficient model comparison. However, they often lack nuance in assessing creativity, contextual appropriateness, and cultural sensitivity. Potential biases in generation or evaluation metrics may also affect reliability. The study on the reliability of machine-generated benchmarks is out of the scope of this paper. Q: Why no safety-related evaluation? A: Safety evaluation is distinct research area. Our work focuses on CPT, releasing multilingual base model (not instruction-tuned) and corpus. As this isnt an end-user product, safety wasnt primary focus. Existing benchmarks like Wang et al. (2023) and Aakanksha et al. (2024) cover few languages and arent ideal for massively multilingual settings."
        },
        {
            "title": "Comparison with Baselines",
            "content": "Q: Is comparing models on unsupported languages unfair? A: We evaluate all models across diverse languages, including unsupported ones, to assess generalization capabilities. Theres no established standard for fair baselines in such cross-lingual comparisons. Evaluating on unsupported languages stresses the models zero-shot transferability. Q: What is the relation to the EMMA-500 Llama 2 model (Ji et al., 2024a)? A: This work is follow-up of the EMMA-500 Llama 2 model. Our models follow most of the settings of the EMMA-500 Llama 2 model, but use additional bilingual translation data, continue pre-pretraining on Llama 3 and 3.1 models, and study on two big data mixes. Moreover, our analyses reveals valuable findings. Q: Why not comparing with model XXX? A: Our model selection prioritizes representative architectures that have diverse capabilities. Particularly, we focus on open-weight decoder-only models from 7-9B parameters. Our current exclusion reflects either the overlap with existing benchmarks or technical constraints. Also, some newer models would come up. We actively welcome feedback to expand coverage of baselines."
        },
        {
            "title": "Others",
            "content": "Q: Will the corpus, model weights, benchmarks, and scripts be publicly available? A: Yes. We release the MaLA translation corpus, all model weights, and code. Benchmarks and processing scripts are open-sourced by their original authors. Q: How do we categorize languages into resource groups? A: We follow the resource groups used by Ji et al. (2024a), where languages are grouped into five categories (high to low resource) based on token counts in the MaLA monolingual corpus. This data-driven approach reflects actual text availability. We include ISO codes and writing systems in the MaLA parallel corpus for standardization across NLP applications. Q: Why dont we separate metrics for officially supported vs. unseen languages? A: While possible for models declaring supported languages, this cant be done for LLaMA or GEMMA which lack such specifications. We include per-language results in the Appendix and will share all model generations on our project website (https://mala-lm.github.io/emma-500-v2), enabling community analysis with custom metrics while keeping the paper concise."
        },
        {
            "title": "B Details of MaLA Translation Corpus",
            "content": "B.1 Data Sources Table 8 lists the corpora and collections we use as bilingual data sources in this work. data source being bilingual means that it contains corresponding, or parallel, text records in two languages. This usually means one side has been translated, either by machine or human. Pedantically, massively parallel bilingual corpora can be considered special case of multilingual corpora containing sentences in different languages that share the same meaning. In this work, some data sources have parallel text data in more than two languages, but in such cases, we always iteratively determine one as the source and one as the target language, i.e., create bilingual corpora out of them. In some source datasets, the source language is not explicitly stated, and as such we pick one language to regard as the source language without actual knowledge of the direction of translation. In such cases, we tend to pick high-resource language, such as English, as the source language. Metadata In the case of bilingual corpora, we define each record in the JSONL file to consist of the fields src_lang, src_txt, tgt_lang, tgt_txt, url, collection, source, original_src_lang, and original_tgt_lang. The contents of these fields are as follows. The fields src_txt and tgt_txt contain the parallel text data in the source and target language, respectively, and src_lang and tgt_lang are the ISO 639-3 language codes of those languages. Identically with the monolingual corpora, url contains URL indicating the web address from which the text data has been extracted, if available, collection the name of the collection, and source the name of more specific part of the collection which the text data was extracted from. Lastly, original_src_lang and original_tgt_lang contain the language denotations of the source and target language, respectively, as they are given in the source data. Table 8: Source datasets used for compiling the MaLA bilingual translation data. Dataset Languages URL CMU Haitian Creole (University, 2010) English-Luganda Parallel Corpus (Mukiibi et al., 2021) Ewe language corpus (Gbedevi, 2019) Nunavut Hansard InuktitutEnglish Parallel Corpus 3.0 (Joanis et al., 2020) AmericasNLP 2021 (Galarreta et al., 2017; Gutierrez-Vasques et al., 2016; Velázquez, 2021) AmericasNLP 2023 (Chiruzzo et al., 2020; Gutierrez-Vasques et al., 2016; Mager et al., 2018; Ortega et al., 2020; Velázquez, 2021) AmericasNLP 2022 Indigenous Languages Corpora (EdTeKLA Research Group, 2022) ACES (Amrhein et al., 2022) ChrEn (Zhang et al., 2020) NusaX-MT (Winata et al., 2023) Lego-MT (Yuan et al., 2023) Tatoeba (Tiedemann, 2020) NLLB (NLLB Team et al., 2022) Lacuna Project (Masakhane, 2022) lafand-mt (Adelani et al., 2022) B.2 Supported Languages Haitian Creole Luganda-English http://www.speech.cs.cmu.edu/haitian/ https://zenodo.org/records/4764039 EweEnglish InuktitutEnglish 10 11 4 Cree 146 2 12 433 487 202 1 21 https://www.kaggle.com/datasets/yvicherita/ ewe-language-corpus https://nrc-digital-repository.canada.ca/eng/view/object/?id= c7e34fa7-7629-43c2-bd6d-19b32bf64f60 https://turing.iimas.unam.mx/americasnlp/americasnlp_2021. html;https://github.com/AmericasNLP/americasnlp2021 https://turing.iimas.unam.mx/americasnlp/2023_st.html https://github.com/AmericasNLP/americasnlp2022 https://github.com/EdTeKLA/IndigenousLanguages_Corpora https://huggingface.co/datasets/nikitam/ACES https://huggingface.co/datasets/chr_en https://huggingface.co/datasets/indonlp/NusaX-MT https://github.com/CONE-MT/Lego-MT https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/ master/data https://huggingface.co/datasets/allenai/nllb https://github.com/masakhane-io/lacuna_pos_ner https://github.com/masakhane-io/lafand-mt/tree/main Table 9 shows the language codes of the MaLA corpus (Ji et al., 2024a), where unseen means the languages are not used for training EMMA-500. The classification system for token counts categorizes language resources based on their size into five distinct tiers: high for resources exceeding 1 billion tokens, indicating vast amount of data; medium-high for those with more than 100 million tokens, reflecting substantial dataset; medium for resources that contain over 10 million tokens, representing moderate size; medium-low for datasets with over 1 million tokens, indicating smaller yet significant amount of data; and finally, low for resources containing less than 1 million 24 Table 9: Languages by resource groups categorized by counting the number of tokens in the MaLA monolingual corpus (Ji et al., 2024a). Unseen means those languages are not used for continual pre-training in this paper. Category Languages Language Codes high low medium medium-high 27 68 79 medium-low 162 unseen fra_Latn, mon_Cyrl, kat_Geor, tgk_Cyrl, kaz_Cyrl, glg_Latn, hbs_Latn, kan_Knda, mal_Mlym, rus_Cyrl, cat_Latn, hye_Armn, guj_Gujr, slv_Latn, fil_Latn, bel_Cyrl, isl_Latn, nep_Deva, mlt_Latn, pan_Guru, afr_Latn, urd_Arab, mkd_Cyrl, aze_Latn, deu_Latn, eng_Latn, ind_Latn prs_Arab, nqo_Nkoo, emp_Latn, pfl_Latn, teo_Latn, gpe_Latn, izz_Latn, shn_Mymr, hak_Latn, pls_Latn, evn_Cyrl, djk_Latn, toj_Latn, nog_Cyrl, ctu_Latn, tca_Latn, jiv_Latn, ach_Latn, mrj_Latn, ajp_Arab, apc_Arab, tab_Cyrl, hvn_Latn, tls_Latn, bak_Latn, ndc_Latn, trv_Latn, top_Latn, kjh_Cyrl, guh_Latn, mni_Mtei, csy_Latn, noa_Latn, dov_Latn, bho_Deva, kon_Latn, hne_Deva, kcg_Latn, mni_Beng, hus_Latn, pau_Latn, jbo_Latn, dtp_Latn, kmb_Latn, hau_Arab, pdc_Latn, nch_Latn, acf_Latn, bim_Latn, ixl_Latn, dty_Deva, kas_Arab, lrc_Arab, alz_Latn, lez_Cyrl, lld_Latn, tdt_Latn, acm_Arab, bih_Deva, mzh_Latn, guw_Latn, rop_Latn, rwo_Latn, ahk_Latn, qub_Latn, kri_Latn, gub_Latn, laj_Latn, sxn_Latn, luo_Latn, tly_Latn, pwn_Latn, mag_Deva, xav_Latn, bum_Latn, ubu_Latn, roa_Latn, mah_Latn, tsg_Latn, gcr_Latn, arn_Latn, csb_Latn, guc_Latn, bat_Latn, knj_Latn, cre_Latn, bus_Latn, anp_Deva, aln_Latn, nah_Latn, zai_Latn, kpv_Cyrl, enq_Latn, gvl_Latn, wal_Latn, fiu_Latn, swh_Latn, crh_Latn, nia_Latn, bqc_Latn, map_Latn, atj_Latn, npi_Deva, bru_Latn, din_Latn, pis_Latn, gur_Latn, cuk_Latn, zne_Latn, cdo_Latn, lhu_Latn, pcd_Latn, mas_Latn, bis_Latn, ncj_Latn, ibb_Latn, tay_Latn, bts_Latn, tzj_Latn, bzj_Latn, cce_Latn, jvn_Latn, ndo_Latn, rug_Latn, koi_Cyrl, mco_Latn, fat_Latn, olo_Latn, inb_Latn, mkn_Latn, qvi_Latn, mak_Latn, ktu_Latn, nrm_Latn, kua_Latn, san_Latn, nbl_Latn, kik_Latn, dyu_Latn, sgs_Latn, msm_Latn, mnw_Latn, zha_Latn, sja_Latn, xal_Cyrl, rmc_Latn, ami_Latn, sda_Latn, tdx_Latn, yap_Latn, tzh_Latn, sus_Latn, ikk_Latn, bas_Latn, nde_Latn, dsb_Latn, seh_Latn, knv_Latn, amu_Latn, dwr_Latn, iku_Cans, uig_Latn, bxr_Cyrl, tcy_Knda, mau_Latn, aoj_Latn, gor_Latn, cha_Latn, fip_Latn, chr_Cher, mdf_Cyrl, arb_Arab, quw_Latn, shp_Latn, spp_Latn, frp_Latn, ape_Latn, cbk_Latn, mnw_Mymr, mfe_Latn, jam_Latn, lad_Latn, awa_Deva, mad_Latn, ote_Latn, shi_Latn, btx_Latn, maz_Latn, ppk_Latn, smn_Latn, twu_Latn, blk_Mymr, msi_Latn, naq_Latn, tly_Arab, wuu_Hani, mos_Latn, cab_Latn, zlm_Latn, gag_Latn, suz_Deva, ksw_Mymr, gug_Latn, nij_Latn, nov_Latn, srm_Latn, jac_Latn, nyu_Latn, yom_Latn, gui_Latn tha_Thai, kat_Latn, lim_Latn, tgk_Arab, che_Cyrl, lav_Latn, xho_Latn, war_Latn, nan_Latn, grc_Grek, orm_Latn, zsm_Latn, cnh_Latn, yor_Latn, arg_Latn, tgk_Latn, azj_Latn, tel_Latn, slk_Latn, pap_Latn, zho_Hani, sme_Latn, tgl_Latn, uzn_Cyrl, als_Latn, san_Deva, azb_Arab, ory_Orya, lmo_Latn, bre_Latn, mvf_Mong, fao_Latn, oci_Latn, sah_Cyrl, sco_Latn, tuk_Latn, aze_Arab, hin_Deva, haw_Latn, glk_Arab, oss_Cyrl, lug_Latn, tet_Latn, tsn_Latn, hrv_Latn, gsw_Latn, arz_Arab, vec_Latn, mon_Latn, ilo_Latn, ctd_Latn, ben_Beng, roh_Latn, kal_Latn, asm_Beng, srp_Latn, bod_Tibt, hif_Latn, rus_Latn, nds_Latn, lus_Latn, ido_Latn, lao_Laoo, tir_Ethi, chv_Cyrl, wln_Latn, kaa_Latn, pnb_Arab div_Thaa, som_Latn, jpn_Japn, hat_Latn, sna_Latn, heb_Hebr, bak_Cyrl, nld_Latn, tel_Telu, kin_Latn, msa_Latn, gla_Latn, bos_Latn, dan_Latn, smo_Latn, ita_Latn, mar_Deva, pus_Arab, srp_Cyrl, spa_Latn, lat_Latn, hmn_Latn, sin_Sinh, zul_Latn, bul_Cyrl, amh_Ethi, ron_Latn, tam_Taml, khm_Khmr, nno_Latn, cos_Latn, fin_Latn, ori_Orya, uig_Arab, hbs_Cyrl, gle_Latn, cym_Latn, vie_Latn, kor_Hang, lit_Latn, yid_Hebr, ara_Arab, sqi_Latn, pol_Latn, tur_Latn, swa_Latn, hau_Latn, ceb_Latn, eus_Latn, kir_Cyrl, mlg_Latn, jav_Latn, snd_Arab, sot_Latn, por_Latn, uzb_Cyrl, fas_Arab, nor_Latn, est_Latn, hun_Latn, ibo_Latn, ltz_Latn, swe_Latn, tat_Cyrl, ast_Latn, mya_Mymr, uzb_Latn, sun_Latn, ell_Grek, ces_Latn, mri_Latn, ckb_Arab, kur_Latn, kaa_Cyrl, nob_Latn, ukr_Cyrl, fry_Latn, epo_Latn, nya_Latn aym_Latn, rue_Cyrl, rom_Latn, dzo_Tibt, poh_Latn, sat_Olck, ary_Arab, fur_Latn, mbt_Latn, bpy_Beng, iso_Latn, pon_Latn, glv_Latn, new_Deva, gym_Latn, bgp_Latn, kac_Latn, abt_Latn, quc_Latn, otq_Latn, sag_Latn, cak_Latn, avk_Latn, pam_Latn, meo_Latn, tum_Latn, bam_Latn, kha_Latn, syr_Syrc, kom_Cyrl, nhe_Latn, bal_Arab, srd_Latn, krc_Cyrl, lfn_Latn, bar_Latn, rcf_Latn, nav_Latn, nnb_Latn, sdh_Arab, aka_Latn, bew_Cyrl, bbc_Latn, meu_Latn, zza_Latn, ext_Latn, yue_Hani, ekk_Latn, xmf_Geor, nap_Latn, mzn_Arab, pcm_Latn, lij_Latn, myv_Cyrl, scn_Latn, dag_Latn, ban_Latn, twi_Latn, udm_Cyrl, som_Arab, nso_Latn, pck_Latn, crs_Latn, acr_Latn, tat_Latn, afb_Arab, uzs_Arab, hil_Latn, mgh_Latn, tpi_Latn, ady_Cyrl, pag_Latn, kiu_Latn, ber_Latn, iba_Latn, ksh_Latn, plt_Latn, lin_Latn, chk_Latn, tzo_Latn, tlh_Latn, ile_Latn, lub_Latn, hui_Latn, min_Latn, bjn_Latn, szl_Latn, kbp_Latn, inh_Cyrl, que_Latn, ven_Latn, vls_Latn, kbd_Cyrl, run_Latn, wol_Latn, ace_Latn, ada_Latn, kek_Latn, yua_Latn, tbz_Latn, gom_Latn, ful_Latn, mrj_Cyrl, abk_Cyrl, tuc_Latn, stq_Latn, mwl_Latn, tvl_Latn, quh_Latn, gom_Deva, mhr_Cyrl, fij_Latn, grn_Latn, zap_Latn, mam_Latn, mps_Latn, tiv_Latn, ksd_Latn, ton_Latn, bik_Latn, vol_Latn, ava_Cyrl, tso_Latn, szy_Latn, ngu_Latn, hyw_Armn, fon_Latn, skr_Arab, kos_Latn, tyz_Latn, kur_Arab, srn_Latn, tyv_Cyrl, bci_Latn, vep_Latn, crh_Cyrl, kpg_Latn, hsb_Latn, ssw_Latn, zea_Latn, ewe_Latn, ium_Latn, diq_Latn, ltg_Latn, nzi_Latn, guj_Deva, ina_Latn, pms_Latn, bua_Cyrl, lvs_Latn, eml_Latn, hmo_Latn, kum_Cyrl, kab_Latn, chm_Cyrl, cor_Latn, cfm_Latn, alt_Cyrl, bcl_Latn, ang_Latn, frr_Latn, mai_Deva rap_Latn, pmf_Latn, lsi_Latn, dje_Latn, bkx_Latn, ipk_Latn, syw_Deva, ann_Latn, bag_Latn, bat_Cyrl, chu_Cyrl, gwc_Arab, adh_Latn, szy_Hani, shi_Arab, njy_Latn, pdu_Latn, buo_Latn, cuv_Latn, udg_Mlym, bax_Latn, tio_Latn, kjb_Latn, taj_Deva, lez_Latn, olo_Cyrl, rnl_Latn, bri_Latn, inh_Latn, kas_Cyrl, wni_Latn, anp_Latn, tsc_Latn, mgg_Latn, udi_Cyrl, mdf_Latn, agr_Latn, xty_Latn, llg_Latn, nge_Latn, gan_Latn, tuv_Latn, stk_Latn, nut_Latn, thy_Thai, lgr_Latn, hnj_Latn, dar_Cyrl, aia_Latn, lwl_Thai, tnl_Latn, tvs_Latn, jra_Khmr, tay_Hani, gal_Latn, ybi_Deva, snk_Arab, gag_Cyrl, tuk_Cyrl, trv_Hani, ydd_Hebr, kea_Latn, gbm_Deva, kwi_Latn, hro_Latn, rki_Latn, quy_Latn, tdg_Deva, zha_Hani, pcg_Mlym, tom_Latn, nsn_Latn, quf_Latn, jmx_Latn, kqr_Latn, mrn_Latn, bxa_Latn, abc_Latn, mve_Arab, lfa_Latn, qup_Latn, yin_Latn, roo_Latn, mrw_Latn, nxa_Latn, yrk_Cyrl, bem_Latn, kvt_Latn, csw_Cans, bjr_Latn, mgm_Latn, ngn_Latn, pib_Latn, quz_Latn, awb_Latn, myk_Latn, otq_Arab, ino_Latn, tkd_Latn, bef_Latn, bug_Bugi, aeu_Latn, nlv_Latn, dty_Latn, bkc_Latn, mmu_Latn, hak_Hani, sea_Latn, mlk_Latn, cbr_Latn, lmp_Latn, tnn_Latn, qvz_Latn, pbt_Arab, cjs_Cyrl, mlw_Latn, mnf_Latn, bfm_Latn, dig_Latn, thk_Latn, zxx_Latn, lkb_Latn, chr_Latn, pnt_Latn, vif_Latn, fli_Latn, got_Latn, hbb_Latn, tll_Latn, bug_Latn, kxp_Arab, qaa_Latn, krr_Khmr, kjg_Laoo, isu_Latn, kmu_Latn, gof_Latn, sdk_Latn, mne_Latn, baw_Latn, idt_Latn, xkg_Latn, mgo_Latn, dtr_Latn, kms_Latn, ffm_Latn, hna_Latn, nxl_Latn, bfd_Latn, odk_Arab, miq_Latn, mhx_Latn, kam_Latn, yao_Latn, pnt_Grek, kby_Latn, kpv_Latn, kbx_Latn, cim_Latn, qvo_Latn, pih_Latn, nog_Latn, nco_Latn, rmy_Cyrl, clo_Latn, dmg_Latn, aaa_Latn, rel_Latn, ben_Latn, loh_Latn, thl_Deva, chd_Latn, cni_Latn, cjs_Latn, lbe_Latn, ybh_Deva, zxx_Zyyy, awa_Latn, gou_Latn, xmm_Latn, nqo_Latn, rut_Cyrl, kbq_Latn, tkr_Latn, dwr_Ethi, ckt_Cyrl, ady_Latn, yea_Mlym, nhx_Latn, niv_Cyrl, bwt_Latn, xmg_Latn, chy_Latn, mfj_Latn, hre_Latn, bbk_Latn, shn_Latn, lrc_Latn, qvc_Latn, muv_Mlym, mdr_Latn, luy_Latn, lzh_Hani, fuh_Latn, mle_Latn, brx_Deva, pex_Latn, kau_Latn, yrk_Latn, hin_Latn, ekm_Latn, msb_Latn, unr_Orya, cac_Latn, chp_Cans, ckt_Latn, bss_Latn, lts_Latn, bbj_Latn, ttt_Cyrl, kwu_Latn, smn_Cyrl, kpy_Cyrl, tod_Latn, wbm_Latn, tcy_Latn, arc_Syrc, nst_Latn, tuz_Latn, bob_Latn, bfn_Latn, pli_Deva, snl_Latn, kwd_Latn, lgg_Latn, nza_Latn, wbr_Deva, lan_Latn, kmz_Latn, bzi_Thai, hao_Latn, nla_Latn, qxr_Latn, ken_Latn, tbj_Latn, blk_Latn, ybb_Latn, nwe_Latn, gan_Hani, snk_Latn, kak_Latn, tpl_Latn, hla_Latn, tks_Arab, pea_Latn, bya_Latn, enc_Latn, jgo_Latn, tnp_Latn, aph_Deva, bgf_Latn, brv_Laoo, nod_Thai, niq_Latn, nwi_Latn, xmd_Latn, gbj_Orya, syr_Latn, ify_Latn, xal_Latn, bra_Deva, cgc_Latn, bhs_Latn, pwg_Latn, ang_Runr, oki_Latn, qve_Latn, qvm_Latn, bkm_Latn, bkh_Latn, niv_Latn, zuh_Latn, mry_Latn, fiu_Cyrl, ssn_Latn, rki_Mymr, sox_Latn, yav_Latn, nyo_Latn, dag_Arab, qxh_Latn, bze_Latn, myx_Latn, zaw_Latn, ddg_Latn, wnk_Latn, bwx_Latn, mqy_Latn, lad_Hebr, boz_Latn, lue_Latn, ded_Latn, pli_Latn, avk_Cyrl, wms_Latn, sgd_Latn, azn_Latn, ajz_Latn, psp_Latn, jra_Latn, smt_Latn, ags_Latn, csw_Latn, wtk_Latn, emp_Cyrl, koi_Latn, tkr_Cyrl, amp_Latn, ymp_Latn, mfh_Latn, tdb_Deva, omw_Latn, khb_Talu, doi_Deva, gld_Cyrl, ava_Latn, chu_Latn, dnw_Latn, azo_Latn, dug_Latn, bce_Latn, kmr_Latn, kpy_Armn, abq_Cyrl, trp_Latn, ewo_Latn, the_Deva, hig_Latn, pkb_Latn, mxu_Latn, oji_Latn, tnt_Latn, mzm_Latn, mns_Cyrl, lbe_Cyrl, qvh_Latn, kmg_Latn, sps_Latn, brb_Khmr, tah_Latn, sxb_Latn, mkz_Latn, mgq_Latn, got_Goth, lns_Latn, arc_Latn, akb_Latn, skr_Latn, nsk_Cans, sml_Latn, pce_Mymr, eee_Thai, lhm_Deva, yux_Cyrl, bqm_Latn, bcc_Arab, nas_Latn, agq_Latn, xog_Latn, tsb_Latn, fub_Latn, mqj_Latn, nsk_Latn, bxr_Latn, dln_Latn, ozm_Latn, rmy_Latn, cre_Cans, kim_Cyrl, cuh_Latn, ngl_Latn, yas_Latn, bud_Latn, miy_Latn, ame_Latn, pnz_Latn, raj_Deva, enb_Latn, cmo_Khmr, saq_Latn, tpu_Khmr, eve_Cyrl, cdo_Hani tokens, which suggests minimal data presence. This hierarchy helps in understanding the scale and potential utility of the language resources available. B.3 Token Counts Figure 5 shows the numbers of segments and tokens across all language pairs in the MaLA translation corpus."
        },
        {
            "title": "C Details of Data Mixes",
            "content": "Table 10 provides how different data types and resource categories contribute to the overall dataset composition. Our data mixes aim to make balanced distribution over highand low-resources. However, due to the nature of high-resource languages, they still contribute to large portion. For example, English research papers form substantial portion. Medium-high and medium-resource monolingual and bilingual texts contribute lot. Medium and low-resource languages consist of 33% and 19% of bilingual and monolingual mixes, respectively. 25 (a) Number of tokens Figure 5: Numbers of segments and tokens across all language pairs in the MaLA bilingual translation corpus. (b) Number of segments C.1 Additional Code Data Code Data We source code data from de-duplicated version of The Stack (Kocetkov et al., 2023) and oversample files from the algorithmic coding (Puri et al., 2021) and data science domains 11 owing to the benefits of training models on self-contained code (Fujii et al., 2025). We discard all non-data programming languages that occur fewer than 50k times, with the exception of llvm, following prior work detailing its importance in multi-lingual and low-resource code generation (Paul et al., 2024; Szafraniec et al., 2023). We also discard samples that manifest code in rare but valid extensions. Finally, we source from data-heavy formats but follow precedent (Lozhkov et al., 2024) and subsample them more aggressively. The surviving data is filtered as follows: For files forked more than 25 times, we retain them if the average line length is less than 120, the maximum line length is less than 300, and the alphanumeric fraction is more than 30%. For files forked between 15 and 25 times, we retain them if the average line length is less than 90, the maximum line length is less than 150, and the alphanumeric fraction is more than 40%. For files forked less than 15 times, we retain them if the average line length is less than 80, the maximum line length is less than 120, and the alphanumeric fraction is more than 45%. Code-Adjacent Procedural Data We augment our data mix with procedural code-adjacent data ranging from instructive data such as StackOverflow QnAs, library documentation and tutorials 12 to Jupyter notebooks, synthetic code textbooks (Su et al., 2024), GitHub commits, and issue descriptions 11https://huggingface.co/datasets/AlgorithmicResearchGroup/arxiv_research_code 12https://huggingface.co/datasets/BEE-spoke-data/code-tutorials-en 26 (Lozhkov et al., 2024). We further mirror Paul et al. (2025) in sourcing parallel math problem solutions to code data. Table 10: Composition of training data mixtures showing original and final token counts by data type, with sampling rates and distribution percentages across bilingual and monolingual configurations."
        },
        {
            "title": "Percentage of Mixes\nBilingual Monolingual",
            "content": "instruction code book paper monolingual bilingual EN high medium-high medium medium-low low code/reasoning code Gutenberg EN ZH EN high medium-high medium medium-low low very high high medium-high medium medium-low low very low 0.1 0.2 0.5 5.0 20.0 50.0 1.0 1.0 1.0 1.0 1.0 0.1 0.5 1.0 5.0 20.0 50.0 0.1 0.1 0.2 0.5 1.0 2.0 10.0 9,204,199,807 39,403,448,029 30,651,187,534 1,444,764,863 47,691,495 3,064,796 612,208,775 43,478,432,765 5,173,357,710 38,256,934,181 61,787,372 3,002,029,817 40,411,201,964 27,515,227,962 2,747,484,380 481,935,633 97,535,696 85,001,097,362 207,688,940,222 46,777,497,823 64,375,100,302 20,361,578,347 2,503,240,829 175,309,923 920,419,981 7,880,689,606 15,325,593,767 7,223,824,315 953,829,900 153,239,800 612,208,775 43,478,432,765 5,173,357,710 38,256,934,181 61,787,372 300,202,982 20,205,600,982 27,515,227,962 13,737,421,900 9,638,712,660 4,876,784,800 4,250,054,868 20,768,894,022 9,355,499,565 32,187,550,151 20,361,578,347 5,006,481,658 1,753,099,230 0.32% 2.72% 5.28% 2.49% 0.33% 0.05% 0.21% 14.99% 1.78% 13.19% 0.02% 0.10% 6.97% 9.49% 4.74% 3.32% 1.68% 1.47% 7.16% 3.23% 11.10% 7.02% 1.73% 0.60% 0.47% 4.01% 7.81% 3.68% 0.49% 0.08% 0.31% 22.15% 2.64% 19.49% 0.03% 0.15% 10.29% 14.02% 7.00% 4.91% 2.48% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%"
        },
        {
            "title": "D Detailed Evaluation Setup",
            "content": "D.1 Tasks and Benchmarks Table 11 summarizes the tasks and benchmarks used for evaluation. Text Classification SIB-200 Adelani et al. (2023) and Taxi-1500 Ma et al. (2023) are two representative multilingual topic-classification benchmarks. SIB-200 is based on Flores-200, covering 205 languages and dialects. The topics are \"science/technology\", \"travel\", \"politics\", \"sports\", \"health\", \"entertainment\", and \"geography\". Taxi-1500 is sentence classification data set with 6 topics, i.e., recommendation, faith, description, sin, grace, and violence, collected from the Parallel Bible Corpus (Mayer and Cysouw, 2014). It covers 1,502 typologically diverse languages spanning 112 language families. Commonsense Reasoning We evaluate EMMA-500 models commonsense-reasoning skills with two multilingual benchmarks. XCOPA (Ponti et al., 2020) targets causal inference across 11 languages, XStoryCloze (Lin et al., 2022) gauges narrative completion in 11 languages. Natural Language Inference We evaluate on XNLI (Conneau et al., 2018), where sentence pairs in different languages need to be classified as entailment, contradiction, or neutral. Machine Translation FLORES-200 (Costa-jussà et al., 2022) is multilingual benchmark designed to evaluate machine translation performance across 204 language pairs involving English, yielding 408 translation directions with an emphasis on low-resource settings. Text Summarization We use two multilingual news summarization datasets: MassiveSumm (Varab and Schluter, 2021) and XL-Sum (Hasan et al., 2021). We subsample MassiveSumm into two sets, i.e., MassiveSumm long13 designed for longer texts, aiming for median of 5500 tokens within 13https://huggingface.co/datasets/MaLA-LM/MassiveSumm_long 27 Table 11: Evaluation statistics. Sample/Lang: average number of test samples per language; Lang: number of languages covered. Tasks Dataset Metric Samples/Lang Lang Domain Text Classification (Section E.1) Commonsense Reasoning (Section E.2) SIB200 (Adelani et al., 2023) Taxi1500 (Ma et al., 2023) XCOPA (Ponti et al., 2020) XStoryCloze (Lin et al., 2022) Natural Language Inference (Section E.3) XNLI (Conneau et al., 2018) Accuracy Accuracy Accuracy Accuracy Accuracy Machine Translation (Section E.4) FLORES-200 (Costa-jussà et al., 2022) BLEU, chrF++ Summarization (Section E.5) XL-Sum (Hasan et al., 2021) ROUGE-L, BERTScore MassiveSumm Long (Varab and Schluter, 2021) ROUGE-L, BERTScore MassiveSumm Short (Varab and Schluter, 2021) ROUGE-L, BERTScore Machine Comprehension (Section E.6) Math (Section E.7) BELEBELE (Bandarkar et al., 2023) ARC multilingual (Lai et al., 2023) MGSM direct (Shi et al., 2022) MGSM CoT (Shi et al., 2022) Accuracy Accuracy Accuracy Accuracy 204 111 600 1870 1012 2537 3908 5538 900 1170 250 250 205 Misc Bible 11 Misc 11 Misc 15 Misc 204 Misc 44 News 55 News 88 News 122 Misc 31 Misc 10 Misc 10 Misc range of 3500 to 7500 tokens, and MassiveSumm short14 on shorter texts, targeting median of 1200 tokens with maximum of 1500 tokens. For practical reasons, we ensure balanced number of documents across all languages, with minimum of 100 and maximum of 2500 documents per language in both MassiveSumm subsets. Additionally, we utilized the XL-Sum, diverse dataset containing over one million professionally annotated article-summary pairs across 44 languages from the BBC. Machine Reading Comprehension We use two datasets of machine reading comprehension: BELEBELE (Bandarkar et al., 2023) and the multilingual ARC challenge. The BELEBELE dataset (Bandarkar et al., 2023) is parallel multilingual machine reading comprehension benchmark spanning 122 languages across resource levels. Each of its carefully validated multiple-choice questions (with four options per question) derives from FLORES-200 passages, testing nuanced comprehension while maintaining full parallelism for cross-lingual comparison. The AI2 Reasoning Challenge (ARC) dataset (Clark et al., 2018) is rigorously constructed benchmark for advanced question answering and reasoning evaluation. It comprises 7,787 genuine grade-school level science questions. We use the multilingual ARC translated by Lai et al. (2023) using ChatGPT. Math We use the Multilingual Grade School Math Benchmark (MGSM) (Shi et al., 2022) that extends the GSM8K dataset (Cobbe et al., 2021) to evaluate cross-lingual mathematical reasoning in large language models. Derived from GSM8Ks collection of 8,500 linguistically diverse grade-school math word problems requiring multi-step reasoning, MGSM comprises 250 carefully selected problems manually translated into ten typologically diverse languages. The evaluation benchmarks are abbreviated as follows: XC: XCOPA (Cross-lingual Choice of Plausible Alternatives) XSC: XStoryCloze (Cross-lingual Story Cloze Test) BELE: BELEBELE, Multilingual Reading Comprehension Benchmark ARC: Multilingual AI2 Reasoning Challenge For the MGSM benchmark, we distinguish between: Dir.: Direct prompting (question-only) CoT: Chain-of-Thought prompting (with reasoning steps) (Wei et al., 2022) D.2 Evaluation Software We evaluate benchmarks using the lm-evaluation-harness framework (Gao et al., 2023) for supported tasks, and custom or open-source scripts for others. For text classification (e.g., SIB-200, Taxi-1500), we predict the highest-probability category using next-token probabilities via Transformers (Wolf et al., 2019). For translation and open-ended generation, we use vLLM (Kwon et al., 2023) for faster inference. 14https://huggingface.co/datasets/MaLA-LM/MassiveSumm_short 28 D.3 Baselines We compare our models with several groups of baseline approaches, covering both established and emerging LLMs. Llama 2 series including CPT models: Llama 2 base & chat (Touvron et al., 2023b): the second generation of Metas foundational 7BB parameter models release in 2023; CodeLlama 2 (Roziere et al., 2023): continual pre-trained model specialized for code; LlaMAX Llama 2 base and Alpaca (Lu et al., 2024): continual pre-trained and instruction-tuned variants on Alpaca (Taori et al., 2023); MaLA-500 Llama 2 v1 & v2 (Lin et al., 2024): multilingual adaptations with continual pretraining and low-rank adaptation (Hu et al., 2022); YaYi Llama 215: Chinese-optimized version; TowerBase and TowerInstruct Llama 2 (Alves et al., 2024): domain-specific adaptations optimized mainly for machine translation; EMMA-500 Llama 2 (Ji et al., 2024a): massively multilingual variant. Multilingual LLMs including recent advances: Occiglot Mistral v0.1 base and instruct 16: multilingual variants continue-trained and instructtuned on Mistral (Jiang et al., 2023); BLOOM & BLOOMZ (Muennighoff et al., 2022; Scao et al., 2022): early open multilingual and instruction-tuned models released in 2022; YaYi 17: model with Chinese-focused multilingual capabilities; Aya 23 (Aryabumi et al., 2024): multilingual instruction-tuned LLM supporting 23 languages; Aya Expanse (Dang et al., 2024): multilingual variant optimized with various post-training methods such as synthetic data augmentation, iterative preference training, and model merging; Gemma series: Googles multilingual models including generations 118 & 2 (Team et al., 2024); Qwen series (Bai et al., 2023): Alibabas multilingual offerings including generations 1.519, 2 (Yang et al., 2024), & 2.5 (Qwen et al., 2025); Macro-LLM (Ming et al., 2024): continue-pretrained LLM using Qwen2. Llama 3 series including CPT models: Llama 3 (Dubey et al., 2024): Metas latest generation with cutoff or March 2023; Llama 3.1 (Dubey et al., 2024): Refined version with improved capabilities, which extends the data cutoff to December 2023; LLaMAX Llama 3 base and Alpaca (Lu et al., 2024): continue-trained and instruction-tuned variants. Our evaluation encompasses these representative models to ensure comprehensive comparison. The selection covers diverse architectures and linguistic capabilities. We continue to expand our benchmarking coverage and welcome collaboration inquiries regarding additional model evaluations."
        },
        {
            "title": "E Detailed Results",
            "content": "This section presents the detailed results of each evaluated task, including average scores categorized by language resource groups and per-language scores if the benchmark has few languages. For massively multilingual benchmarks with more than dozen of languages, we provide the results on our project website (https://mala-lm.github.io/emma-500-v2). We also provide all the model outputs of different generation tasks on the project website. 15http://https://huggingface.co/wenge-research/yayi-7b-llama2 16http://https://huggingface.co/occiglot 17http://https://huggingface.co/wenge-research/yayi-7b 18http://https://huggingface.co/google/gemma-7b 19http://https://huggingface.co/Qwen/Qwen1.5-7B 29 E.1 Text Classification We evaluate with 3-shot prompting on the SIB-200 and Taxi-1500 text classification datasets. The prompt template for SIB-200 is as follows: Topic Classification : science/technology , travel , politics , sports , health , entertainment , geography. {examples} The topic of the news \"${text }\" is For Taxi-1500, the prompt template is as follows: Topic Classification : Recommendation , Faith , Description , Sin , Grace , Violence. {examples} The topic of the verse \"${text }\" is As for SIB-200 task, Table 12 shows that EMMA-500 Llama 3.1 8B Bi attains the highest accuracy among EMMA-500 model series (62.07%), edging its backbone by +0.65% and crossing the 60% threshold together with EMMA-500 Llama 3 8B Mono (60.62%). Yet the other two variants fall significantly below their bases: EMMA-500 Llama 3 8B Bi drops to 39.40%, while EMMA-500 Llama 3.1 8B Mono slips to 26.16%. The mixed pictureimprovements for some settings, regressions for otherssuggests that our current continual pre-training strategy for EMMA-500 models may not be consistently helpful for SIB-200 task, and calls for deeper analysis in terms of data mix and training strategy. With its religious text domain, short inputs and 1500+ languages, Taxi-1500 is markedly harder. Here, the advantages of EMMA-500 are more pronounced. Where most 8 models struggle to reach 20% accuracy, all four EMMA-500 variants hover around or above that mark. EMMA-500 Llama-3 8B-Bi achieves 25.13%, gain of 3.40% over the base model, and EMMA-500 Llama-3.1 8B-Bi reaches 24.87%. Table 12: 3-shot results on SIB-200 and Taxi-1500 (Accuracy %). Model SIB-200 TaxiAvg High Med-High Medium Med-Low Low Avg High Med-High Medium Med-Low Low 22.41 25.98 Llama 2 7B 25.58 29.15 Llama 2 7B Chat 23.35 25.80 CodeLlama 7B 10.61 12.83 LLaMAX Llama 2 7B 27.89 33.36 LLaMAX Llama 2 7B Alpaca 23.25 23.00 MaLA-500 10B V1 19.30 17.83 MaLA-500 10B V2 24.57 27.99 Yayi Llama 2 7B 19.34 21.30 Tower Base Llama 2 7B V0.1 Tower Instruct Llama 2 7B V0.2 20.53 22.91 31.27 33.02 EMMA-500 Llama 2 7B 32.69 37.77 Occiglot Mistral 7B 34.31 37.79 Occiglot Mistral 7B Instruct 17.82 21.57 BLOOM 7B1 29.73 29.77 BLOOMZ 7B1 35.76 39.51 Yayi 7B 41.50 45.45 Aya 23 8B 57.01 63.82 Aya Expanse 8B 58.21 68.76 Gemma 7B 46.25 51.94 Gemma 2 9B 47.95 55.88 Qwen 1.5 7B 54.95 66.84 Qwen 2 7B 53.89 64.57 Qwen 2.5 7B 64.15 72.73 Macro-LLM GLO 7B 63.70 73.89 Llama 3 8B 61.42 70.83 Llama 3.1 8B 48.60 51.41 LLaMAX Llama 3 8B 58.97 70.50 LLaMAX Llama 3 8B Alpaca EMMA-500 Llama 3 8B Mono 60.62 60.72 EMMA-500 Llama 3 8B Bi 39.40 37.14 EMMA-500 Llama 3.1 8B Mono 26.16 25.31 EMMA-500 Llama 3.1 8B Bi 62.07 59.80 22.17 25.27 23.01 10.96 27.69 23.44 18.57 24.08 18.90 20.22 30.34 32.91 34.01 19.54 29.95 36.50 42.17 59.73 60.00 45.79 49.42 58.04 56.45 65.98 66.11 63.42 48.19 60.57 58.70 39.83 25.74 60.15 22.43 25.43 23.39 10.66 27.88 23.03 18.42 24.64 19.07 20.46 30.86 32.87 34.20 18.61 30.13 36.62 42.43 58.99 60.14 46.78 49.53 57.27 56.08 65.73 66.33 63.67 48.94 60.96 60.26 38.88 25.84 61.60 23.36 22.74 17.54 19.57 26.58 26.02 15.44 18.78 24.27 23.84 17.03 17.47 11.11 10.78 23.52 23.08 29.74 28.66 15.09 18.87 23.30 23.13 25.27 23.53 19.51 19.04 23.39 21.27 25.72 25.03 17.73 18.62 19.86 19.53 17.73 18.74 21.10 20.81 17.29 20.16 31.93 31.97 19.82 24.32 34.07 33.27 22.26 24.74 35.37 34.87 18.76 24.59 18.13 17.73 14.76 15.71 29.89 29.99 16.96 16.89 36.40 36.40 16.12 16.56 43.42 42.20 22.64 22.82 59.84 58.06 18.73 19.56 62.08 59.97 13.83 25.10 47.82 47.19 18.05 29.88 50.34 48.94 7.29 12.42 58.33 56.06 21.87 27.85 56.82 54.85 17.87 18.12 66.96 65.07 21.99 21.31 67.89 65.53 21.73 32.43 65.33 63.17 20.20 28.22 50.16 49.70 23.01 22.32 63.71 60.91 17.71 30.56 62.10 62.34 22.32 23.13 38.39 39.65 25.13 29.53 25.80 26.17 19.71 18.82 63.31 63.33 24.87 25.71 17.78 15.67 17.11 23.54 15.31 25.08 23.35 17.84 17.76 17.69 21.07 22.53 19.41 14.98 16.99 16.17 22.63 19.07 14.78 19.43 7.61 22.42 18.01 21.84 22.64 21.12 22.84 18.56 22.04 26.13 19.41 25. 17.89 15.77 17.12 23.55 15.43 25.00 23.07 17.90 17.87 17.72 21.32 22.61 19.42 14.93 16.98 16.24 22.56 19.08 15.42 20.27 7.75 22.66 17.95 21.92 23.07 21.59 22.85 19.32 22.02 26.45 19.45 25.45 18.17 18.04 16.09 16.01 17.17 17.15 23.52 23.53 15.70 15.57 24.83 24.76 22.94 22.72 18.01 17.98 18.04 18.06 18.05 17.92 21.68 21.73 22.69 22.62 19.68 19.59 14.91 14.93 17.00 17.00 16.26 16.19 22.56 22.52 19.03 19.08 16.92 16.55 21.72 21.48 8.18 23.16 23.01 17.85 17.82 21.93 21.97 23.79 23.61 22.22 22.02 22.82 22.91 20.83 20.48 22.10 22.16 26.71 26.72 19.47 19.62 25.40 25.42 8.39 30 E.2 Commonsense Reasoning In the commonsense reasoning evaluation, all evaluations are zero-shot; accuracy is the evaluation metric. Languages are bucketed into groups according to language availability and possible corpus size, following Ji et al. (2024a). Across XCOPA and XStoryCloze evaluation results  (Table 13)  , the four EMMA-500 variants built on Llama-3/3.1 8 clearly lift commonsense-reasoning accuracy over their backbones. The best model, EMMA-500 Llama 3.1 8B Bi, averages 67.25% on XCOPA and 68.47% on XStoryClozeroughly +5% over vanilla Llama-3.1 and almost ahead of, all other similar size models. The performance gains are consistent across all different languages (Tables 14 and 15). Besides, improvements concentrate in mediumand low-resource languages: EMMA-500 lifts Llama-3/3.1 by roughly +7% in mediumresource languages and flattens the usual low-resource performance drop on XStoryCloze ( 64% vs. 53% for the base), while high-resource accuracy remains competitive at around 70%. One more interesting finding is that EMMA-500 monolingual variants are slightly behind bilingual variants by about 1%, confirming that balanced bilingual continual training adds marginal yet consistent performance gains. Table 13: 0-shot results (Accuracy %) on commonsense reasoning: XCOPA and XStoryCloze."
        },
        {
            "title": "XStoryCloze",
            "content": "Avg High Medium Low Avg High Medium Low 56.67 62.10 Llama 2 7B 55.85 61.25 Llama 2 7B Chat 54.69 58.70 CodeLlama 2 7B 54.38 55.50 LLaMAX Llama 2 7B 56.60 59.80 LLaMAX Llama 2 7B Alpaca 53.09 53.55 MaLA-500 Llama 2 10B v1 53.09 53.55 MaLA-500 Llama 2 10B v2 56.71 62.10 YaYi Llama 2 7B 56.33 62.50 TowerBase Llama 2 7B 57.05 62.90 TowerInstruct Llama 2 7B 63.11 66.60 EMMA-500 Llama 2 7B Occiglot Mistral 7B v0.1 56.67 62.80 Occiglot Mistral 7B v0.1 Instruct 56.55 62.85 56.89 59.95 BLOOM 7B 54.87 56.35 BLOOMZ 7B 56.64 59.55 YaYi 7B 55.13 59.05 Aya 23 8B 56.38 61.50 Aya Expanse 8B 63.64 70.35 Gemma 7B 66.33 73.40 Gemma 2 9B 59.44 66.85 Qwen 1.5 7B 60.31 68.65 Qwen 2 7B 61.71 72.30 Qwen 2.5 7B 62.45 72.20 Marco-LLM GLO 7B 61.71 68.35 Llama 3 8B 61.71 69.30 Llama 3.1 8B 63.04 67.25 LLaMAX Llama 3 8B 64.36 69.55 LLaMAX Llama 3 8B Alpaca EMMA-500 Llama 3 8B Mono 66.20 69.25 EMMA-500 Llama 3 8B Bi 66.82 70.80 EMMA-500 Llama 3.1 8B Mono 65.38 68.85 EMMA-500 Llama 3.1 8B Bi 67.25 71.75 53.90 51.60 57.55 63.38 53.13 50.60 58.41 64.80 52.53 51.60 55.68 60.68 54.13 51.40 60.36 64.34 55.17 52.40 63.83 69.08 53.27 50.20 53.07 58.15 53.27 50.20 53.07 58.15 54.13 50.60 58.42 64.98 52.90 52.20 57.78 64.35 54.00 52.00 59.24 66.83 62.57 52.40 66.38 68.92 53.37 52.00 58.10 65.18 52.97 52.80 59.39 66.94 55.87 50.80 59.30 61.99 54.60 50.60 57.12 61.14 55.50 51.80 60.67 64.90 53.30 50.40 60.93 67.27 53.77 51.60 64.80 73.10 61.43 50.00 65.01 69.46 64.27 50.40 67.67 72.47 55.90 51.00 59.85 66.62 56.40 50.40 61.46 69.45 56.63 49.80 62.06 69.41 57.77 51.60 63.87 71.26 59.03 51.20 63.41 68.50 58.80 48.80 63.58 68.66 62.30 50.60 64.31 67.78 62.97 52.00 68.27 72.92 66.20 54.00 67.36 69.25 66.67 51.80 68.35 70.22 65.13 53.00 67.64 69.64 66.83 51.80 68.47 70.34 54.45 49.21 54.77 49.74 52.33 49.90 58.82 53.47 62.01 54.33 49.22 48.08 49.22 48.08 54.91 49.04 53.67 49.57 54.53 49.70 65.73 61.32 53.28 50.03 54.33 50.63 59.05 53.08 55.82 49.67 59.40 52.65 58.82 49.34 61.93 49.80 64.49 54.93 66.69 57.64 56.04 50.56 56.97 50.50 58.36 51.09 61.23 50.66 62.03 53.44 62.09 53.87 63.48 57.28 67.27 58.64 66.83 63.70 67.79 64.82 67.19 63.57 68.13 64.49 E.3 Natural Language Inference According to the XNLI evaluation results shown in Table 16, the bilingual EMMA-500 8B variant outperform its backbone on average (45.15% vs 44.97%) and lift low-resource accuracy by roughly +4%, closing most of the gap to the stronger Gemma 2 9 while matching Aya Expanse 8 B. The monolingual Llama-3 variant keeps pace (44.15%), but the same Llama backbone models with monolingual continual training leads to performance drop (39.98% in EMMA-500 Llama 3.1 8B Mono), leaving the bilingual Llama-3.1 model (44.67%) as the only Llama-3.1-based EMMA-500 choice that preserves gains in low-resource languages without drop in high-resource languages. Overall, EMMA-500 offers modest NLI benefits concentrated in low-resource languages. 31 Table 14: 0-shot results (ACC %) on XCOPA in all languages Model Avg et-acc stderr ht-acc stderr id-acc stderr it-acc stderr qu-acc stderr sw-acc stderr ta-acc stderr th-acc stderr tr-acc stderr vi-acc stderr zh-acc stderr 56.67 48.60 Llama 2 7B 55.85 47.80 Llama 2 7B Chat 54.69 46.80 CodeLlama 2 7B 54.38 49.20 LLaMAX Llama 2 7B 56.60 51.20 LLaMAX Llama 2 7B Alpaca 53.09 48.60 MaLA-500 Llama 2 10B v1 53.09 48.60 MaLA-500 Llama 2 10B v2 56.71 48.80 YaYi Llama 2 7B 56.33 46.00 TowerBase Llama 2 7B 57.05 48.80 TowerInstruct Llama 2 7B 63.11 61.40 EMMA-500 Llama 2 7B Occiglot Mistral 7B v0.1 56.67 47.20 Occiglot Mistral 7B v0.1 Instruct 56.55 46.80 56.89 48.20 BLOOM 7B 54.87 49.20 BLOOMZ 7B 56.64 50.40 YaYi 7B 55.13 50.20 Aya 23 8B 56.38 50.80 Aya Expanse 8B 63.64 59.20 Gemma 7B 66.33 63.80 Gemma 2 9B 59.44 52.00 Qwen 1.5 7B 60.31 50.80 Qwen 2 7B 61.71 49.40 Qwen 2.5 7B 62.45 49.60 Marco-LLM GLO 7B 61.71 53.40 Llama 3 8B 61.71 53.00 Llama 3.1 8B 63.04 61.80 LLaMAX Llama 3 8B 64.36 63.00 LLaMAX Llama 3 8B Alpaca EMMA-500 Llama 3 8B Mono 66.20 70.80 EMMA-500 Llama 3 8B Bi 66.82 71.40 EMMA-500 Llama 3.1 8B Mono 65.38 65.20 EMMA-500 Llama 3.1 8B Bi 67.25 70.40 2.24 50.60 2.24 50.80 2.23 51.80 2.24 52.60 2.24 54.20 2.24 53.40 2.24 53.40 2.24 50.80 2.23 50.20 2.24 51.60 2.18 58.00 2.23 51.40 2.23 51.00 2.24 50.80 2.24 54.00 2.24 53.00 2.24 52.20 2.24 51.60 2.20 54.80 2.15 52.80 2.24 53.00 2.24 50.80 2.24 52.80 2.24 52.40 2.23 52.80 2.23 53.80 2.18 56.40 2.16 54.00 2.04 61.80 2.02 60.80 2.13 59.00 2.04 61. 2.24 62.40 2.24 62.40 2.24 57.40 2.24 53.80 2.23 57.20 2.23 53.00 2.23 53.00 2.24 62.60 2.24 60.20 2.24 62.00 2.21 74.20 2.24 57.00 2.24 58.40 2.24 69.80 2.23 60.60 2.23 63.40 2.24 57.00 2.24 61.20 2.23 72.00 2.23 77.80 2.23 64.40 2.24 70.60 2.23 72.60 2.24 74.20 2.23 71.40 2.23 71.60 2.22 73.00 2.23 73.00 2.18 75.00 2.19 78.00 2.20 76.00 2.18 77.00 2.17 65.80 2.17 67.20 2.21 63.00 2.23 52.60 2.21 61.00 2.23 59.40 2.23 59.40 2.17 67.00 2.19 70.80 2.17 71.00 1.96 69.40 2.22 74.60 2.21 73.80 2.06 52.80 2.19 51.40 2.16 51.80 2.22 60.40 2.18 63.40 2.01 72.80 1.86 75.80 2.14 65.20 2.04 71.40 2.00 74.60 1.96 73.40 2.02 71.60 2.02 72.60 1.99 68.40 1.99 71.80 1.94 73.40 1.85 73.40 1.91 73.00 1.88 74.60 2.12 51.60 2.10 50.60 2.16 51.60 2.24 51.40 2.18 52.40 2.20 50.20 2.20 50.20 2.10 50.60 2.04 52.20 2.03 52.00 2.06 52.40 1.95 52.00 1.97 52.80 2.23 50.80 2.24 50.60 2.24 51.80 2.19 50.40 2.16 51.60 1.99 50.00 1.92 50.40 2.13 51.00 2.02 50.40 1.95 49.80 1.98 51.60 2.02 51.20 2.00 48.80 2.08 50.60 2.01 52.00 1.98 54.00 1.98 51.80 1.99 53.00 1.95 51.80 2.24 52.20 2.24 52.20 2.24 48.80 2.24 54.00 2.24 55.00 2.24 52.80 2.24 52.80 2.24 53.20 2.24 50.60 2.24 51.00 2.24 66.20 2.24 51.60 2.23 50.00 2.24 51.80 2.24 53.40 2.24 55.40 2.24 52.20 2.24 53.60 2.24 60.60 2.24 63.80 2.24 52.60 2.24 52.40 2.24 52.20 2.24 54.40 2.24 58.00 2.24 55.40 2.24 62.60 2.24 63.40 2.23 67.60 2.24 68.20 2.23 68.40 2.24 69.60 2.24 53.40 2.24 50.60 2.24 55.00 2.23 58.00 2.23 57.00 2.23 57.60 2.23 57.60 2.23 55.20 2.24 54.40 2.24 54.20 2.12 60.00 2.24 57.20 2.24 56.60 2.24 59.20 2.23 57.40 2.23 56.20 2.24 55.60 2.23 55.20 2.19 61.60 2.15 63.60 2.24 55.80 2.24 52.80 2.24 54.60 2.23 57.20 2.21 60.00 2.23 61.20 2.17 60.80 2.16 63.20 2.10 61.80 2.08 63.40 2.08 62.40 2.06 63.20 2.23 56.20 2.24 55.00 2.23 55.40 2.21 57.20 2.22 56.40 2.21 54.20 2.21 54.20 2.23 54.20 2.23 56.00 2.23 56.40 2.19 55.60 2.21 55.80 2.22 55.00 2.20 55.40 2.21 53.00 2.22 54.60 2.22 52.60 2.23 50.20 2.18 60.40 2.15 63.80 2.22 57.60 2.23 61.00 2.23 58.20 2.21 58.80 2.19 58.60 2.18 57.80 2.19 59.20 2.16 61.20 2.18 60.20 2.16 58.20 2.17 59.80 2.16 59. 2.22 54.80 2.23 55.20 2.23 53.80 2.21 53.00 2.22 55.20 2.23 51.60 2.23 51.60 2.23 55.40 2.22 53.80 2.22 54.60 2.22 62.00 2.22 54.40 2.23 56.20 2.23 51.00 2.23 52.20 2.23 52.00 2.24 59.60 2.24 57.80 2.19 65.60 2.15 67.40 2.21 58.80 2.18 57.20 2.21 60.00 2.20 66.40 2.20 62.40 2.21 61.80 2.20 61.40 2.18 64.00 2.19 64.00 2.21 66.60 2.19 62.80 2.19 67.20 2.23 62.80 2.23 61.20 2.23 55.80 2.23 53.00 2.23 55.20 2.24 52.40 2.24 52.40 2.23 63.20 2.23 59.20 2.23 58.60 2.17 70.20 2.23 55.20 2.22 55.80 2.24 70.80 2.24 59.80 2.24 66.40 2.20 58.80 2.21 60.20 2.13 74.20 2.10 76.60 2.20 69.20 2.21 69.40 2.19 75.20 2.11 73.80 2.17 71.60 2.18 72.40 2.18 70.40 2.15 71.80 2.15 73.00 2.11 73.20 2.16 74.60 2.10 74.60 2.16 65.00 2.18 61.40 2.22 62.20 2.23 63.40 2.23 67.80 2.24 50.80 2.24 50.80 2.16 62.80 2.20 66.20 2.20 67.40 2.05 64.80 2.23 67.00 2.22 65.60 2.04 65.20 2.19 62.00 2.11 68.00 2.20 57.40 2.19 64.60 1.96 68.80 1.90 73.80 2.07 74.20 2.06 76.60 1.93 79.40 1.97 75.20 2.02 67.80 2.00 70.40 2.04 68.80 2.01 70.60 1.99 66.60 1.98 70.00 1.95 65.00 1.95 70.60 2.14 2.18 2.17 2.16 2.09 2.24 2.24 2.16 2.12 2.10 2.14 2.10 2.13 2.13 2.17 2.09 2.21 2.14 2.07 1.97 1.96 1.90 1.81 1.93 2.09 2.04 2.07 2.04 2.11 2.05 2.14 2.04 Table 15: 0-shot results (Accuracy %) on XStoryCloze in all languages Model Avg ar-acc stderr en-acc stderr es-acc stderr eu-acc stderr hi-acc stderr id-acc stderr my-acc stderr ru-acc stderr sw-acc stderr te-acc stderr zh-acc stderr 57.55 49.90 Llama 2 7B 58.41 50.50 Llama 2 7B Chat 55.68 50.10 CodeLlama 2 7B 60.36 58.90 LLaMAX Llama 2 7B 63.83 60.36 LLaMAX Llama 2 7B Alpaca 53.07 48.18 MaLA-500 Llama 2 10B v1 53.07 48.18 MaLA-500 Llama 2 10B v2 58.42 49.97 YaYi Llama 2 7B 57.78 49.17 TowerBase Llama 2 7B 59.24 49.31 TowerInstruct Llama 2 7B 66.38 66.25 EMMA-500 Llama 2 7B Occiglot Mistral 7B v0.1 58.10 51.29 Occiglot Mistral 7B v0.1 Instruct 59.39 52.68 59.30 58.57 BLOOM 7B 57.12 56.52 BLOOMZ 7B 60.67 61.81 YaYi 7B 60.93 62.61 Aya 23 8B 64.80 69.42 Aya Expanse 8B 65.01 60.42 Gemma 7B 67.67 65.25 Gemma 2 9B 59.85 55.39 Qwen 1.5 7B 61.46 59.96 Qwen 2 7B 62.06 62.41 Qwen 2.5 7B 63.87 64.73 Marco-LLM GLO 7B 63.41 58.64 Llama 3 8B 63.58 59.10 Llama 3.1 8B 64.31 59.96 LLaMAX Llama 3 8B 68.27 64.73 LLaMAX Llama 3 8B Alpaca EMMA-500 Llama 3 8B Mono 67.36 64.06 EMMA-500 Llama 3 8B Bi 68.35 66.38 EMMA-500 Llama 3.1 8B Mono 67.64 64.20 EMMA-500 Llama 3.1 8B Bi 68.47 66.18 1.29 77.04 1.29 78.69 1.29 71.48 1.27 75.51 1.26 81.47 1.29 73.53 1.29 73.53 1.29 79.09 1.29 77.23 1.29 80.87 1.22 76.44 1.29 77.37 1.28 79.42 1.27 70.55 1.28 73.00 1.25 74.32 1.25 74.59 1.19 80.41 1.26 80.15 1.23 80.15 1.28 78.16 1.26 78.95 1.25 77.56 1.23 79.75 1.27 78.69 1.27 78.16 1.26 76.97 1.23 83.19 1.23 76.17 1.22 75.78 1.23 75.71 1.22 75.12 1.08 67.37 1.05 67.11 1.16 63.40 1.11 65.25 1.00 70.68 1.14 62.41 1.14 62.41 1.05 68.70 1.08 69.82 1.01 71.61 1.09 70.02 1.08 73.40 1.04 74.19 1.17 66.18 1.14 64.59 1.12 69.42 1.12 67.31 1.02 74.26 1.03 70.62 1.03 74.26 1.06 68.30 1.05 69.76 1.07 69.16 1.03 72.14 1.05 70.62 1.06 70.81 1.08 69.09 0.96 73.00 1.10 69.42 1.10 71.28 1.10 70.62 1.11 71.34 1.21 50.36 1.21 50.83 1.24 50.43 1.23 54.47 1.17 54.86 1.25 49.90 1.25 49.90 1.19 50.63 1.18 50.76 1.16 50.69 1.18 64.73 1.14 52.08 1.13 53.01 1.22 57.25 1.23 51.09 1.19 56.06 1.21 50.96 1.13 51.36 1.17 57.58 1.13 60.16 1.20 51.89 1.18 52.02 1.19 51.82 1.15 52.28 1.17 55.79 1.17 55.33 1.19 59.23 1.14 59.96 1.19 66.71 1.16 66.64 1.17 66.18 1.16 66.45 1.29 53.74 1.29 54.07 1.29 49.70 1.28 58.17 1.28 62.14 1.29 47.65 1.29 47.65 1.29 54.27 1.29 52.88 1.29 52.95 1.23 64.92 1.29 51.49 1.28 52.75 1.27 60.42 1.29 57.64 1.28 63.67 1.29 64.26 1.29 68.50 1.27 64.92 1.26 66.91 1.29 55.66 1.29 57.97 1.29 58.70 1.29 61.02 1.28 62.81 1.28 63.27 1.26 62.87 1.26 67.50 1.21 66.05 1.21 66.91 1.22 67.17 1.22 67. 1.28 59.23 1.28 59.63 1.29 55.86 1.27 60.62 1.25 66.45 1.29 47.92 1.29 47.92 1.28 61.42 1.28 58.31 1.28 59.56 1.23 68.63 1.29 58.64 1.28 60.36 1.26 64.53 1.27 55.33 1.24 62.41 1.23 66.64 1.20 72.47 1.23 67.64 1.21 71.34 1.28 62.87 1.27 64.39 1.27 67.04 1.26 68.70 1.24 65.92 1.24 68.03 1.24 66.78 1.21 72.40 1.22 68.63 1.21 69.56 1.21 69.62 1.20 70.68 1.26 1.26 1.28 1.26 1.22 1.29 1.29 1.25 1.27 1.26 1.19 1.27 1.26 1.23 1.28 1.25 1.21 1.15 1.20 1.16 1.24 1.23 1.21 1.19 1.22 1.20 1.21 1.15 1.19 1.18 1.18 1.17 48.05 48.64 49.37 52.48 53.81 46.26 46.26 47.45 48.38 48.71 57.91 47.98 48.25 48.91 48.25 49.24 47.72 48.25 52.28 55.13 49.24 48.97 50.36 49.04 51.09 52.42 55.33 57.31 60.69 63.00 60.95 62.54 1.29 63.00 1.29 65.52 1.29 59.23 1.29 61.22 1.28 67.44 1.28 54.93 1.28 54.93 1.29 64.79 1.29 67.04 1.29 69.36 1.27 68.50 1.29 62.94 1.29 65.06 1.29 52.75 1.29 52.15 1.29 52.15 1.29 68.70 1.29 73.86 1.29 70.55 1.28 73.73 1.29 63.27 1.29 69.56 1.29 69.69 1.29 70.42 1.29 68.76 1.29 68.63 1.28 68.30 1.27 72.80 1.26 69.49 1.24 70.95 1.26 70.62 1.25 71.41 1.24 50.50 1.22 52.02 1.26 50.03 1.25 57.18 1.21 60.16 1.28 48.71 1.28 48.71 1.23 50.03 1.21 50.36 1.19 51.49 1.20 64.73 1.24 49.83 1.23 50.43 1.28 53.94 1.29 52.15 1.29 53.61 1.19 50.36 1.13 51.62 1.17 62.14 1.13 63.73 1.24 51.69 1.18 51.42 1.18 51.82 1.17 53.67 1.19 56.39 1.19 55.92 1.20 60.69 1.15 64.00 1.18 65.59 1.17 67.37 1.17 66.05 1.16 67.50 1.29 54.33 1.29 53.34 1.29 53.74 1.27 59.30 1.26 59.30 1.29 52.61 1.29 52.61 1.29 53.94 1.29 53.14 1.29 54.14 1.23 64.66 1.29 53.14 1.29 53.81 1.28 57.31 1.29 58.17 1.28 57.91 1.29 54.00 1.29 55.13 1.25 63.27 1.24 64.79 1.29 53.94 1.29 54.07 1.29 55.86 1.28 61.55 1.28 63.00 1.28 61.15 1.26 63.60 1.24 65.19 1.22 67.04 1.21 67.31 1.22 65.92 1.21 66. 1.28 59.56 1.28 62.21 1.28 59.17 1.26 60.82 1.26 65.45 1.28 51.69 1.28 51.69 1.28 62.34 1.28 58.50 1.28 63.00 1.23 63.40 1.28 60.89 1.28 63.34 1.27 61.88 1.27 59.43 1.27 66.78 1.28 63.14 1.28 67.57 1.24 65.59 1.23 68.96 1.28 67.97 1.28 69.03 1.28 68.23 1.25 69.29 1.24 65.78 1.25 66.58 1.24 64.59 1.23 70.88 1.21 67.11 1.21 66.71 1.22 67.04 1.21 67.64 1.26 1.25 1.26 1.26 1.22 1.29 1.29 1.25 1.27 1.24 1.24 1.26 1.24 1.25 1.26 1.21 1.24 1.20 1.22 1.19 1.20 1.19 1.20 1.19 1.22 1.21 1.23 1.17 1.21 1.21 1.21 1.20 E.4 Machine Translation We evaluate all models for machine translation under 3-shot prompting setup. Specifically, we use the prompt below. Translate the following sentence from {src_lang} to {tgt_lang} [{ src_lang }]: {src_sent} [{ tgt_lang }]: Translation quality is assessed using BLEU (Papineni et al., 2002) and chrF++ (Popović, 2015), implemented via sacrebleu(Post, 2018). BLEU is computed using the flores200 tokenizer, which allows consistent segmentation even for languages lacking explicit word boundaries; chrF++ is calculated with word n-gram order of 2. For transparency and reproducibility, we report full metric signatures. On translations from all other languages to English  (Table 18)  , EMMA-500 variants based on both Llama-3 8B and Llama-3.1 8B show clear, systematic gains under monolingual and bilingual 20BLEU: nrefs:1case:mixedeff:notok:flores200smooth:exp version:2.4.2; chrF++: nrefs:1case:mixedeff:yesnc:6nw:2 space:noversion:2.4.2 32 Table 16: 0-shot results on XNLI (Accuracy %). Model Avg High Medium Low 40.19 45.26 Llama 2 7B 38.58 42.77 Llama 2 7B Chat 40.19 46.27 CodeLlama 2 7B 44.27 46.53 LLaMAX Llama 2 7B 45.09 48.47 LLaMAX Llama 2 7B Alpaca 38.11 42.10 MaLA-500 Llama 2 10B v1 38.11 42.10 MaLA-500 Llama 2 10B v2 41.28 47.32 YaYi Llama 2 7B 39.84 46.08 TowerBase Llama 2 7B 40.36 47.07 TowerInstruct Llama 2 7B 45.14 46.09 EMMA-500 Llama 2 7B Occiglot Mistral 7B v0.1 42.35 49.90 Occiglot Mistral 7B v0.1 Instruct 40.81 47.58 41.60 45.13 BLOOM 7B 37.13 40.02 BLOOMZ 7B 39.87 43.85 YaYi 7B 43.12 48.51 Aya 23 8B 45.56 50.48 Aya Expanse 8B 42.58 46.44 Gemma 7B 46.74 48.50 Gemma 2 9B 39.47 40.95 Qwen 1.5 7B 42.77 47.31 Qwen 2 7B 43.31 47.80 Qwen 2.5 7B 43.99 48.74 Marco-LLM GLO 7B 44.97 48.82 Llama 3 8B 45.62 49.61 Llama 3.1 8B 44.13 46.30 LLaMAX Llama 3 8B 45.08 48.18 LLaMAX Llama 3 8B Alpaca EMMA-500 Llama 3 8B Mono 44.15 47.15 EMMA-500 Llama 3 8B Bi 45.15 47.20 EMMA-500 Llama 3.1 8B Mono 39.98 42.34 EMMA-500 Llama 3.1 8B Bi 44.67 46.79 37.72 34.97 36.75 33.87 37.29 33.86 42.64 43.03 42.80 42.89 35.85 34.65 35.85 34.65 38.41 34.94 36.33 34.39 36.92 33.79 44.40 44.71 38.39 35.19 37.18 34.52 39.69 38.38 35.56 34.51 38.24 35.15 41.95 34.67 44.24 38.38 41.00 38.01 45.11 46.49 38.80 37.83 41.35 36.53 41.85 37.24 41.44 39.57 43.84 39.56 44.04 40.83 42.83 42.41 42.82 43.40 41.73 42.99 43.64 44.07 39.00 37.20 42.88 44.00 Table 17: 0-shot results (Accuracy %) on XNLI in all languages. Model Avg ar-acc stderr bg-acc stderr de-acc stderr el-acc stderr en-acc stderr es-acc stderr fr-acc stderr hi-acc stderr ru-acc stderr sw-acc stderr th-acc stderr tr-acc stderr ur-acc stderr vi-acc stderr zh-acc stderr 40.19 35.42 Llama 2 7B 38.58 34.42 Llama 2 7B Chat 40.19 33.41 CodeLlama 2 7B 44.27 33.78 LLaMAX Llama 2 7B 45.09 34.42 LLaMAX Llama 2 7B Alpaca 38.11 35.94 MaLA-500 Llama 2 10B v1 38.11 35.94 MaLA-500 Llama 2 10B v2 41.28 34.14 YaYi Llama 2 7B 39.84 33.90 TowerBase Llama 2 7B 40.36 33.65 TowerInstruct Llama 2 7B 45.14 34.78 EMMA-500 Llama 2 7B Occiglot Mistral 7B v0.1 42.35 33.86 Occiglot Mistral 7B v0.1 Instruct 40.81 34.38 41.60 33.85 BLOOM 7B 37.13 32.69 BLOOMZ 7B 39.87 39.80 YaYi 7B 43.12 33.90 Aya 23 8B 45.56 34.10 Aya Expanse 8B 42.58 33.49 Gemma 7B 46.74 34.18 Gemma 2 9B 39.47 34.34 Qwen 1.5 7B 42.77 33.69 Qwen 2 7B 43.31 34.38 Qwen 2.5 7B 43.99 33.45 Marco-LLM GLO 7B 44.97 33.65 Llama 3 8B 45.62 33.86 Llama 3.1 8B 44.13 34.26 LLaMAX Llama 3 8B 45.08 35.42 LLaMAX Llama 3 8B Alpaca EMMA-500 Llama 3 8B Mono 44.15 33.01 EMMA-500 Llama 3 8B Bi 45.15 33.94 EMMA-500 Llama 3.1 8B Mono 39.98 34.66 EMMA-500 Llama 3.1 8B Bi 44.67 34.22 0.96 42.65 0.95 37.07 0.95 37.75 0.95 46.83 0.95 46.39 0.96 41.20 0.96 41.20 0.95 42.61 0.95 41.37 0.95 42.93 0.95 46.27 0.95 41.37 0.95 38.84 0.67 39.92 0.94 34.02 0.98 35.78 0.95 39.36 0.95 41.89 0.95 43.49 0.95 49.52 0.95 40.92 0.95 45.46 0.95 42.85 0.95 45.50 0.95 45.34 0.95 45.58 0.95 43.82 0.96 42.57 0.94 46.06 0.95 46.71 0.95 41.33 0.95 44.58 0.99 47.11 0.97 43.09 0.97 47.23 1.00 48.96 1.00 49.76 0.99 47.51 0.99 47.51 0.99 48.84 0.99 47.87 0.99 48.84 1.00 47.07 0.99 51.77 0.98 50.84 0.69 39.78 0.95 41.69 0.96 42.61 0.98 49.36 0.99 51.04 0.99 48.63 1.00 51.37 0.99 42.77 1.00 48.19 0.99 47.31 1.00 50.40 1.00 50.48 1.00 51.45 0.99 51.29 0.99 51.45 1.00 48.59 1.00 50.08 0.99 42.61 1.00 49.84 1.00 36.67 0.99 38.15 1.00 37.63 1.00 42.57 1.00 43.41 1.00 34.46 1.00 34.46 1.00 37.35 1.00 35.26 1.00 34.98 1.00 45.86 1.00 37.71 1.00 40.00 0.69 35.37 0.99 35.82 0.99 36.47 1.00 41.12 1.00 42.17 1.00 38.11 1.00 43.25 0.99 36.18 1.00 36.83 1.00 41.45 1.00 37.35 1.00 39.28 1.00 38.96 1.00 43.21 1.00 42.61 1.00 37.91 1.00 44.26 0.99 35.62 1.00 45.14 0.97 55.30 0.97 50.24 0.97 54.78 0.99 54.90 0.99 58.11 0.95 56.18 0.95 56.18 0.97 56.47 0.96 56.35 0.96 56.95 1.00 53.78 0.97 55.86 0.98 55.66 0.68 53.97 0.96 46.87 0.96 50.52 0.99 51.57 0.99 53.86 0.97 52.05 0.99 53.45 0.96 49.08 0.97 54.26 0.99 53.65 0.97 53.82 0.98 55.02 0.98 55.22 0.99 54.70 0.99 55.06 0.97 56.39 1.00 53.69 0.96 47.99 1.00 51.85 1.00 40.52 1.00 39.44 1.00 44.38 1.00 47.59 0.99 48.96 0.99 34.10 0.99 34.10 0.99 45.78 0.99 41.69 0.99 46.51 1.00 47.07 1.00 51.65 1.00 48.63 0.70 48.82 1.00 36.02 1.00 47.71 1.00 50.36 1.00 47.35 1.00 44.14 1.00 51.41 1.00 37.87 1.00 47.23 1.00 48.92 1.00 50.52 1.00 49.52 1.00 50.28 1.00 43.65 1.00 46.91 0.99 47.43 1.00 48.51 1.00 42.33 1.00 47. 0.98 50.08 0.98 44.82 1.00 49.20 1.00 47.79 1.00 51.97 0.95 47.59 0.95 47.59 1.00 51.04 0.99 49.44 1.00 46.43 1.00 46.87 1.00 51.93 1.00 51.69 0.71 49.80 0.96 43.05 1.00 48.19 1.00 51.16 1.00 53.65 1.00 49.76 1.00 52.29 0.97 43.13 1.00 51.41 1.00 51.57 1.00 51.57 1.00 50.56 1.00 51.77 0.99 43.78 1.00 50.28 1.00 47.95 1.00 48.88 0.99 42.41 1.00 46.71 1.00 37.71 1.00 35.78 1.00 35.94 1.00 45.50 1.00 45.62 1.00 33.65 1.00 33.65 1.00 39.48 1.00 34.54 1.00 34.74 1.00 47.59 1.00 35.74 1.00 34.30 0.71 46.51 0.99 40.40 1.00 40.04 1.00 46.67 1.00 48.31 1.00 44.34 1.00 47.31 0.99 38.15 1.00 44.98 1.00 42.49 1.00 42.13 1.00 47.55 1.00 49.40 0.99 44.14 1.00 45.10 1.00 45.78 1.00 48.67 0.99 41.16 1.00 47.91 0.97 42.37 0.96 42.09 0.96 46.06 1.00 45.54 1.00 46.27 0.95 33.94 0.95 33.94 0.98 46.27 0.95 45.94 0.95 46.27 1.00 46.55 0.96 47.63 0.95 40.04 0.70 43.03 0.98 37.47 0.98 39.12 1.00 48.67 1.00 51.00 1.00 47.39 1.00 49.56 0.97 38.88 1.00 47.39 0.99 47.71 0.99 46.63 1.00 49.16 1.00 49.16 1.00 46.27 1.00 48.80 1.00 47.79 1.00 48.15 0.99 44.58 1.00 48.96 0.99 34.94 0.99 34.22 1.00 33.29 1.00 43.05 1.00 43.57 0.95 35.22 0.95 35.22 1.00 35.70 1.00 35.02 1.00 33.94 1.00 46.18 1.00 34.70 0.98 33.17 0.70 37.88 0.97 33.61 0.98 34.06 1.00 34.50 1.00 37.03 1.00 40.64 1.00 45.58 0.98 35.42 1.00 37.31 1.00 34.22 1.00 34.70 1.00 38.92 1.00 39.36 1.00 42.45 1.00 43.73 1.00 46.14 1.00 45.10 1.00 38.47 1.00 46.55 0.96 36.35 0.95 33.49 0.94 35.02 0.99 41.85 0.99 40.80 0.96 33.69 0.96 33.69 0.96 35.62 0.96 34.78 0.95 33.90 1.00 41.97 0.95 37.39 0.94 36.35 0.69 35.05 0.95 33.09 0.95 34.34 0.95 35.90 0.97 41.04 0.98 37.95 1.00 49.88 0.96 44.22 0.97 38.27 0.95 43.33 0.95 44.62 0.98 46.27 0.98 48.07 0.99 45.58 0.99 47.23 1.00 41.00 1.00 45.82 0.98 37.19 1.00 41.73 0.96 37.27 0.95 36.95 0.96 38.59 0.99 43.29 0.99 43.86 0.95 33.82 0.95 33.82 0.96 39.36 0.95 35.74 0.95 37.87 0.99 44.86 0.97 43.01 0.96 37.99 0.67 35.09 0.94 33.73 0.95 33.17 0.96 48.39 0.99 48.84 0.97 43.25 1.00 50.72 1.00 38.84 0.97 43.53 0.99 42.57 1.00 44.82 1.00 48.23 1.00 49.32 1.00 46.55 1.00 44.78 0.99 44.50 1.00 44.70 0.97 40.56 0.99 44. 0.97 33.61 0.97 33.90 0.98 33.25 0.99 44.18 0.99 44.30 0.95 35.02 0.95 35.02 0.98 33.49 0.96 33.37 0.97 33.53 1.00 45.98 0.99 33.49 0.97 34.06 0.67 42.20 0.95 36.83 0.94 37.07 1.00 33.61 1.00 37.07 0.99 35.42 1.00 44.02 0.98 33.86 0.99 34.02 0.99 34.18 1.00 39.40 1.00 33.49 1.00 35.06 1.00 39.20 1.00 39.24 1.00 41.81 1.00 41.29 0.98 35.94 1.00 43.73 0.95 36.63 0.95 38.11 0.94 40.40 1.00 43.86 1.00 43.09 0.96 36.02 0.96 36.02 0.95 37.51 0.95 37.19 0.95 37.35 1.00 47.03 0.95 38.63 0.95 37.59 0.70 47.39 0.97 36.67 0.97 44.18 0.95 42.25 0.97 50.12 0.96 43.29 1.00 45.70 0.95 44.38 0.95 43.57 0.95 47.39 0.98 45.38 0.95 49.00 0.96 47.11 0.98 45.02 0.98 46.43 0.99 43.09 0.99 43.57 0.96 40.68 0.99 41.41 0.97 36.18 0.97 36.95 0.98 35.94 0.99 34.38 0.99 35.78 0.96 33.25 0.96 33.25 0.97 35.54 0.97 35.22 0.97 37.47 1.00 35.22 0.98 40.56 0.97 38.63 0.71 35.35 0.97 35.02 1.00 34.94 0.99 39.96 1.00 45.98 0.99 36.67 1.00 32.93 1.00 33.98 0.99 35.38 1.00 37.63 1.00 39.52 1.00 38.15 1.00 39.76 1.00 38.11 1.00 36.59 0.99 34.74 0.99 33.90 0.98 34.10 0.99 35.54 0.96 0.97 0.96 0.95 0.96 0.94 0.94 0.96 0.96 0.97 0.96 0.98 0.98 0.68 0.96 0.96 0.98 1.00 0.97 0.94 0.95 0.96 0.97 0.98 0.97 0.98 0.97 0.97 0.95 0.95 0.95 0.96 continual training. Specifically, our continual training strategy boosts the BLEU/chrF++ scores of Llama-3 8B from 23.78/43.72 to 30.38/50.86 (EMMA-500 Llama 3 8B Mono, +6.6 BLEU) and further to 34.4/54.33 (EMMA-500 Llama 3 8B Bi, +10.6 BLEU); the same schedule lifts Llama-3.1 8B from 24.19/44.1 to 27.57/48.12 (EMMA-500 Llama 3.1 8B Mono, +3.4 BLEU) and 33.6/53.5 ((EMMA-500 Llama 3 8B Bi, +9.5 BLEU). Gains are even larger when English is the source language  (Table 19)  : EMMA-500 Llama 3 Bi more than doubles base-model BLEU (9.93 24.02) and ChrF++ (24.08 42.15), while EMMA-500 Llama 3.1 Bi shows comparable jump (10.11 23.86 BLEU). Improvements are greatest in the medium-low and low-resource languages, confirming that language-balanced bilingual continual training is especially effective for non-English translations in low-resource settings. 33 Table 18: 3-shot results (BLEU/chrF++) on FLORES-200, from all other languages to English (X-Eng)"
        },
        {
            "title": "High",
            "content": "Medium-high Medium Medium-low Low 5.29/25.73 2.26/15.24 2.34/13.32 2.85/15.33 Llama 2 7B Llama 2 7B Chat CodeLlama 7B LLaMAX Llama 2 7B LLaMAX Llama 2 7B Alpaca MaLA-500 10B V1 MaLA-500 10B V2 Yayi Llama 2 7B Tower Base Llama 2 7B V0.1 Tower Instruct Llama 2 7B V0.2 4.81/25.43 EMMA-500 Llama 2 7B Occiglot Mistral 7B Occiglot Mistral 7B Instruct BLOOM 7B1 BLOOMZ 7B1 Yayi 7B Aya 23 8B Aya Expanse 8B Gemma 7B Gemma 2 9B Qwen 1.5 7B Qwen 2 7B Qwen 2.5 7B Macro-LLM GLO 7B Llama 3 8B Llama 3.1 8B LLaMAX Llama 3 8B LLaMAX Llama 3 8B Alpaca EMMA-500 Llama 3 8B Mono EMMA-500 Llama 3 8B Bi EMMA-500 Llama 3.1 8B Mono EMMA-500 Llama 3.1 8B Bi 12.93/30.32 19.98/38.72 12.90/30.87 12.28/31.72 19.12/39.31 11.96/31.89 10.82/28.57 17.77/37.45 10.72/29.09 1.99/13.66 3.63/21.51 22.29/42.27 34.98/56.64 24.68/45.21 5.01/16.22 2.29/13.60 2.87/15.44 5.84/18.73 12.98/31.38 19.77/39.36 12.93/31.97 13.74/31.47 21.68/40.54 13.81/32.18 9.75/33.70 25.37/45.78 34.84/56.32 26.72/47.21 13.12/31.13 19.94/38.88 12.78/30.98 11.61/31.65 16.92/39.16 11.27/31.61 9.57/27.84 15.83/36.50 12.72/32.18 20.22/34.74 31.74/46.29 26.57/41.03 4.82/21.36 5.04/23.09 13.87/32.36 19.49/39.55 15.32/34.13 13.12/36.86 17.35/44.58 14.77/39.30 23.79/43.68 36.37/57.74 26.72/47.06 23.15/38.87 34.42/51.93 23.37/38.63 15.58/35.87 24.44/46.60 16.96/38.19 17.39/37.61 28.37/50.71 19.49/40.81 18.95/38.89 29.32/51.08 21.04/41.75 25.17/44.57 35.96/56.86 28.34/48.18 23.78/43.72 35.67/57.20 26.55/47.03 24.19/44.10 36.13/57.57 26.76/47.27 0.48/4.66 25.10/45.45 37.58/58.67 27.58/48.13 30.38/50.86 39.52/60.25 31.90/52.47 34.40/54.33 43.48/63.45 36.45/56.47 27.57/48.12 37.34/58.82 29.18/49.85 33.64/53.49 42.78/62.87 35.87/55.72 6.25/24.58 0.55/5.11 0.77/6.36 5.85/27.70 2.48/16. 2.85/14.18 3.51/16.08 13.13/31.00 15.02/33.00 12.24/32.08 14.32/34.15 10.93/29.19 12.77/31.13 2.16/14.65 24.04/44.54 26.58/47.33 2.57/13.81 3.25/15.87 13.13/32.03 15.02/33.96 13.97/32.28 16.05/34.31 5.18/25.97 26.93/47.47 28.89/49.76 13.31/31.64 15.17/33.58 11.80/32.21 13.20/33.98 11.77/31.02 10.88/29.80 24.76/39.25 22.39/37.03 5.69/23.82 14.87/33.96 16.27/35.46 14.54/39.15 14.81/39.83 26.34/46.77 27.78/48.45 24.85/40.38 27.22/43.58 16.97/38.06 18.17/39.11 19.53/40.66 20.66/41.61 21.21/41.80 22.11/42.65 28.15/47.90 28.54/48.47 26.51/46.96 28.08/48.72 26.73/47.25 28.40/49.05 0.54/5.08 27.49/48.12 29.79/50.60 32.44/53.10 34.13/54.81 36.98/57.00 38.45/58.44 29.50/50.33 31.20/52.20 36.20/56.16 37.61/57.62 5.20/22.08 0.57/5.19 13.69/31.26 13.05/32.61 11.52/29.47 2.08/14.08 23.85/44.08 2.55/13.89 3.17/15.76 13.76/32.30 14.50/32.44 5.24/26.32 27.34/48.00 14.00/32.13 12.31/32.61 9.96/28.51 21.06/35.63 4.72/21.25 14.48/33.31 13.61/37.85 25.47/45.65 25.19/41.36 16.42/37.01 18.40/38.90 19.97/40.11 26.38/45.89 25.49/45.72 25.91/46.13 0.52/4.87 26.91/47.43 32.55/53.16 36.72/56.69 29.51/50.34 35.91/55.84 Table 19: 3-shot results (BLEU/chrF++) on FLORES-200, from English to all other languages (Eng-X) Model Avg High Medium-high Medium Medium-low Low 1.14/8. 0.69/6.15 0.59/6.45 10.72/26.07 4.99/16.25 4.62/15.13 Llama 2 7B 10.81/26.27 5.19/17.51 4.95/16.95 Llama 2 7B Chat 10.03/25.25 4.61/15.56 4.27/14.94 CodeLlama 7B 0.80/7.42 2.08/13.88 LLaMAX Llama 2 7B 12.51/28.35 26.39/45.94 16.08/32.23 LLaMAX Llama 2 7B Alpaca 1.62/10.25 0.60/6.08 MaLA-500 10B V1 1.37/10.37 0.54/6.38 MaLA-500 10B V2 10.77/25.96 4.89/15.86 4.41/14.87 Yayi Llama 2 7B 11.56/25.70 5.35/16.43 Tower Base Llama 2 7B V0.1 4.83/16.03 Tower Instruct Llama 2 7B V0.2 3.23/15.64 3.72/16.21 7.66/24.72 15.58/33.25 28.66/46.92 18.00/34.66 EMMA-500 Llama 2 7B 10.97/25.72 5.13/16.65 4.32/16.10 Occiglot Mistral 7B 4.69/16.44 9.80/24.99 3.99/15.80 Occiglot Mistral 7B Instruct 4.30/14.82 6.98/20.40 2.81/11.80 BLOOM 7B1 22.08/32.72 12.35/22.06 7.44/16.10 BLOOMZ 7B1 12.07/27.36 6.87/18.72 4.37/13.50 Yayi 7B 10.76/23.27 8.16/18.42 6.46/16.15 Aya 23 8B 10.76/32.48 8.24/26.48 6.88/23.89 Aya Expanse 8B 9.05/23.05 Gemma 7B 18.18/37.69 10.87/26.37 12.09/26.48 25.23/44.22 14.93/30.26 Gemma 2 9B 13.12/29.98 7.14/19.71 5.87/17.77 Qwen 1.5 7B 12.08/28.96 6.53/19.22 5.56/17.17 Qwen 2 7B 12.15/28.73 6.81/19.33 5.72/17.49 Qwen 2.5 7B 17.74/36.96 11.10/26.36 9.27/23.34 Macro-LLM GLO 7B 9.93/24.08 Llama 3 8B 20.79/40.03 11.76/26.96 10.11/24.69 21.29/40.67 12.00/27.60 Llama 3.1 8B 0.45/4.65 LLaMAX Llama 3 8B 11.64/26.86 25.20/45.23 14.71/30.78 LLaMAX Llama 3 8B Alpaca EMMA-500 Llama 3 8B Mono 20.33/38.34 33.89/51.45 22.10/39.37 EMMA-500 Llama 3 8B Bi 24.02/42.15 38.25/55.19 25.73/43.49 EMMA-500 Llama 3.1 8B Mono 19.44/37.41 32.50/50.37 21.25/38.54 EMMA-500 Llama 3.1 8B Bi 23.86/42.07 37.81/54.81 25.76/43.65 0.64/5.45 1.11/7.77 6.03/18.21 6.27/19.62 5.58/17.75 1.04/8.88 0.80/7.13 0.67/7.31 5.90/18.04 6.03/18.47 4.00/17. 4.14/15.68 4.56/17.18 3.92/15.40 0.92/8.14 14.07/30.87 16.00/34.14 0.57/6.26 0.46/6.42 4.07/15.38 4.37/16.06 3.21/16.09 16.56/34.25 18.85/37.66 4.36/16.57 5.51/18.54 4.01/16.39 5.09/18.34 3.27/13.71 3.51/13.81 10.55/20.25 9.21/18.84 5.19/15.91 5.64/16.88 7.75/18.86 6.53/17.12 7.21/25.53 7.79/26.89 10.04/25.66 11.49/27.58 13.67/29.59 15.29/32.05 6.01/19.13 5.77/18.80 5.92/18.94 10.30/25.76 11.19/27.21 10.97/26.59 12.58/28.89 11.10/27.20 12.76/29.55 0.56/5.19 13.25/29.63 14.99/32.76 21.39/39.56 24.19/42.93 25.32/43.74 28.32/47.00 20.42/38.61 23.17/41.97 25.26/43.74 28.15/46.84 7.13/21.03 6.72/20.30 6.92/20.61 0.57/5.28 4.93/16.23 5.30/18.12 4.57/15.98 0.86/7.92 13.36/30.45 0.65/6.64 0.55/6.93 4.81/16.04 5.10/17.15 3.54/16.84 16.87/35.88 4.69/17.23 4.31/16.92 2.67/12.32 7.48/16.83 4.23/14.20 6.41/16.89 6.78/25.13 9.56/24.75 12.71/28.50 5.93/18.87 5.57/18.17 5.71/18.46 9.51/24.79 10.51/25.84 10.66/26.46 0.49/4.93 12.44/28.94 22.03/41.29 25.99/45.22 21.06/40.32 25.80/45.07 E.5 Text Summarization We conduct zero-shot text summarization evaluations on three benchmark datasets: MassiveSumm long, MassiveSumm short, and XL-Sum. The prompt template for the three benchmarks is as follows: 34 Document: {text} Based on the previous text , provide brief single summary: In order to try to get LLM to answer in the corresponding language of the document, we use Google Translate to translate the prompt into the corresponding language. We evaluate the generated summaries using two widely recognized metrics: ROUGE-L (Lin, 2004) and BERTScore (Zhang et al., 2019). ROUGE-L captures lexical overlap by measuring the longest common subsequence (LCS) between the reference and generated summaries. Precision and recall are calculated as the ratio of the LCS to the generated and reference summary lengths, respectively, with an F-score used for the final metric. BERTScore, on the other hand, measures semantic similarity by comparing contextual embeddings from pre-trained language models. Specifically, we employ the bert-base-multilingual-cased21 model, ensuring compatibility across multiple languages. Our EMMA-500 models demonstrate consistent improvements over their Llama 3 and Llama 3.1 base models in zero-shot summarization across the MassiveSumm (long/short) and XL-Sum benchmarks (Tables 20, 21, 22). Notably, enhancements in semantic quality, measured by BERTScore, are frequently observed, especially on the MassiveSumm benchmarks. Nonetheless, Llama 3/3.1 base models do not obtain impressive summarization performance on these two subsets of MassiveSumm. On the XL-Sum benchmark, our EMMA-500 Llama 3/3.1 CPT models achieve better average performance than recent advances such as Aya 23 and Gemma 2. Table 20: Zero-short performance of MassiveSumm long set (ROUGE-L/BERTScore). Our EMMA-500 models demonstrate consistent improvements over their Llama 3 and Llama 3.1 base models. Model Avg High Medium-high Medium Medium-low Low 4.74/63.89 3.88/61.92 3.88/61.92 Llama 2 7B 4.73/63.52 3.88/61.57 3.88/61.57 Llama 2 7B Chat 5.63/64.51 4.45/60.73 4.45/60.73 CodeLlama 7B 4.56/62.69 3.78/61.26 3.78/61.26 LLaMAX Llama 2 7B 4.61/62.76 3.76/61.42 3.76/61.42 LLaMAX Llama 2 7B Alpaca 4.39/64.50 3.59/61.82 3.59/61.82 MaLA-500 10B V1 4.37/64.66 3.62/62.01 3.62/62.01 MaLA-500 10B V2 4.98/64.17 4.05/62.17 4.05/62.17 Yayi Llama 2 7B 4.81/64.51 3.88/62.39 3.88/62.39 Tower Base Llama 2 7B V0.1 Tower Instruct Llama 2 7B V0.2 4.82/64.61 3.84/62.52 3.84/62.52 4.79/63.80 3.81/61.99 3.81/61.99 EMMA-500 Llama 2 7B 5.14/63.95 4.37/61.66 4.37/61.66 Occiglot Mistral 7B 5.16/63.50 4.57/61.79 4.57/61.79 Occiglot Mistral 7B Instruct 4.88/64.36 4.00/62.34 4.00/62.34 BLOOM 7B1 2.91/57.20 2.12/58.71 2.12/58.71 BLOOMZ 7B1 4.95/64.24 4.26/61.87 4.26/61.87 Yayi 7B 6.33/65.94 5.54/62.67 5.54/62.67 Aya 23 8B 7.44/67.66 6.29/64.39 6.29/64.39 Aya Expanse 8B 6.18/62.14 5.25/60.19 5.25/60.19 Gemma 7B 5.86/59.70 4.96/57.51 4.96/57.51 Gemma 2 9B 6.09/59.19 5.42/58.28 5.42/58.28 Qwen 1.5 7B 6.65/56.31 6.47/54.91 6.47/54.91 Qwen 2 7B 7.44/61.62 6.88/58.85 6.88/58.85 Qwen 2.5 7B 6.57/57.15 5.84/53.04 5.84/53.04 Macro-LLM GLO 7B 5.10/55.58 4.15/54.48 4.15/54.48 Llama 3 8B 5.41/56.09 4.36/55.24 4.36/55.24 Llama 3.1 8B 6.00/65.90 4.74/62.84 4.74/62.84 LLaMAX Llama 3 8B 7.62/67.64 6.13/64.77 6.13/64.77 LLaMAX Llama 3 8B Alpaca EMMA-500 Llama 3 8B Mono 5.38/61.06 4.61/57.84 4.61/57.84 EMMA-500 Llama 3 8B Bi 5.57/59.23 4.85/57.46 4.85/57.46 EMMA-500 Llama 3.1 8B Mono 5.44/61.89 4.70/59.04 4.70/59.04 EMMA-500 Llama 3.1 8B Bi 4.78/58.47 3.70/53.79 3.70/53.79 3.40/62.75 4.78/63.95 3.42/62.34 4.76/63.62 4.11/63.72 5.58/64.54 3.29/61.00 4.60/62.73 3.28/61.22 4.65/62.81 3.11/63.73 4.41/64.58 3.11/63.82 4.40/64.73 3.69/63.15 5.02/64.21 3.43/63.49 4.85/64.59 3.40/63.49 4.85/64.67 3.43/62.66 4.82/63.88 3.93/64.35 5.15/64.05 4.19/63.86 5.19/63.53 3.76/64.65 4.92/64.45 1.89/57.54 2.94/57.17 3.81/64.25 4.99/64.32 4.69/65.29 6.34/66.01 5.65/66.65 7.48/67.76 4.80/62.14 6.23/62.19 4.81/59.83 5.89/59.74 5.07/60.73 6.09/59.14 5.43/54.74 6.64/56.16 6.19/59.98 7.50/61.59 5.26/55.92 6.60/57.11 4.04/57.63 5.13/55.65 4.34/57.85 5.45/56.19 4.57/65.35 6.03/66.00 5.92/66.58 7.69/67.77 4.42/60.75 5.41/61.08 4.67/60.13 5.62/59.30 4.49/61.99 5.48/61.98 3.90/58.36 4.81/58. 4.78/63.95 4.76/63.62 5.58/64.54 4.60/62.73 4.65/62.81 4.41/64.58 4.40/64.73 5.02/64.21 4.85/64.59 4.85/64.67 4.82/63.88 5.15/64.05 5.19/63.53 4.92/64.45 2.94/57.17 4.99/64.32 6.34/66.01 7.48/67.76 6.23/62.19 5.89/59.74 6.09/59.14 6.64/56.16 7.50/61.59 6.60/57.11 5.13/55.65 5.45/56.19 6.03/66.00 7.69/67.77 5.41/61.08 5.62/59.30 5.48/61.98 4.81/58.48 E.6 Machine Reading Comprehension We first evaluate on the BELEBELE dataset (Bandarkar et al., 2023), comprehensive multilingual benchmark for machine reading comprehension, spanning 122 languages across both highand lowresource categories. Notably, this benchmark demonstrates significant difficulty, with even the English subset posing substantial challenges to state-of-the-art language models like Llama 3. As shown in Table 23, which presents zero-shot performance across language resource groups, our continual pre-training approach on Llama 3 and 3.1, as well as LLaMAX CPT models, yields some degraded 21https://huggingface.co/google-bert/bert-base-multilingual-cased 35 Table 21: Zero-short performance of MassiveSumm short set (ROUGE-L/BERTScore). Our EMMA-500 models demonstrate consistent improvements over their Llama 3 and Llama 3.1 base models."
        },
        {
            "title": "High",
            "content": "Medium-high Medium Medium-low Low 6.23/62.78 6.64/62.55 7.85/65.35 Llama 2 7B 8.29/64.64 8.88/64.52 9.76/67.01 Llama 2 7B Chat 5.88/62.10 6.19/61.81 7.59/64.83 CodeLlama 7B 4.10/59.92 5.22/63.06 4.47/59.76 LLaMAX Llama 2 7B 8.93/65.09 10.71/67.92 9.52/64.83 LLaMAX Llama 2 7B Alpaca 4.00/61.65 4.34/61.56 4.97/63.51 MaLA-500 10B V1 4.00/61.69 4.34/61.61 5.02/63.75 MaLA-500 10B V2 6.46/63.33 6.91/63.23 7.80/65.24 Yayi Llama 2 7B 6.26/62.78 Tower Base Llama 2 7B V0.1 6.70/62.59 8.11/65.53 8.24/65.12 Tower Instruct Llama 2 7B V0.2 10.14/67.76 8.88/65.06 6.41/62.56 6.85/62.52 EMMA-500 Llama 2 7B 8.32/65.14 6.05/61.74 6.39/61.67 Occiglot Mistral 7B 8.16/63.65 6.37/62.13 6.72/62.31 Occiglot Mistral 7B Instruct 7.82/63.79 5.34/58.80 5.71/58.74 BLOOM 7B1 6.79/62.30 3.41/36.51 3.72/39.04 BLOOMZ 7B1 3.28/29.75 7.25/64.79 7.73/65.10 Yayi 7B 8.28/65.44 6.99/63.73 7.49/63.54 Aya 23 8B 8.43/65.85 7.96/65.95 8.49/65.77 Aya Expanse 8B 9.24/67.68 7.09/60.45 7.56/60.19 Gemma 7B 8.35/62.25 5.86/53.92 6.36/55.20 Gemma 2 9B 7.86/58.11 7.40/61.00 7.88/61.33 Qwen 1.5 7B 8.49/62.70 7.17/53.36 7.65/53.08 Qwen 2 7B 8.63/56.14 8.85/60.34 9.49/60.70 Qwen 2.5 7B 9.04/58.91 7.51/57.15 8.09/56.96 Macro-LLM GLO 7B 8.10/57.45 4.53/48.77 4.78/48.31 Llama 3 8B 6.44/50.98 5.02/53.04 5.31/52.26 Llama 3.1 8B 6.77/54.96 8.77/66.46 LLaMAX Llama 3 8B 6.60/63.77 7.05/63.80 12.44/68.95 10.27/67.22 9.54/67.08 LLaMAX Llama 3 8B Alpaca EMMA-500 Llama 3 8B Mono 5.97/62.06 6.39/61.99 7.18/63.46 EMMA-500 Llama 3 8B Bi 6.01/60.71 6.49/60.71 7.74/63.18 EMMA-500 Llama 3.1 8B Mono 6.15/61.81 6.68/63.09 7.21/63.36 EMMA-500 Llama 3.1 8B Bi 5.21/58.16 5.61/58.37 6.67/61.64 5.92/64.92 7.79/65.43 7.59/66.03 9.83/67.08 5.54/64.49 7.52/65.03 3.46/62.60 5.16/63.15 8.12/66.93 10.73/67.99 3.53/63.32 5.04/63.76 3.55/63.46 5.09/63.97 5.96/64.80 7.84/65.34 5.94/64.96 8.04/65.58 7.44/66.81 10.15/67.86 6.13/64.26 8.30/65.25 5.79/64.17 8.15/64.00 6.05/64.25 7.72/63.74 4.97/61.23 6.73/62.31 2.78/32.31 3.16/29.30 6.62/65.09 8.30/65.66 6.14/65.59 8.47/66.04 7.38/67.20 9.32/67.88 6.39/62.35 8.35/62.28 5.95/57.87 7.85/57.99 6.87/63.25 8.44/62.60 6.33/55.72 8.49/55.57 7.64/60.70 9.03/58.90 6.87/60.07 8.06/57.25 4.68/52.54 6.29/50.70 4.91/55.77 6.67/54.79 6.27/65.97 8.65/66.58 9.42/68.35 12.44/69.09 5.68/64.19 7.19/63.69 5.72/63.07 7.76/63.28 5.67/62.64 7.25/63.45 5.01/61.53 6.67/61.59 7.88/65.33 9.87/66.98 7.60/64.93 5.26/63.10 10.81/67.90 5.13/63.74 5.18/63.95 7.87/65.21 8.12/65.48 10.18/67.71 8.43/65.16 8.26/63.96 7.84/63.71 6.82/62.26 3.20/29.11 8.35/65.42 8.47/65.85 9.39/67.72 8.47/62.29 7.97/58.11 8.50/62.57 8.54/55.69 9.04/58.56 8.10/57.29 6.45/50.91 6.83/54.97 8.76/66.47 12.48/68.94 7.24/63.50 7.82/63.26 7.34/63.46 6.75/61.64 Table 22: Zero-short performance of XL-Sum (ROUGE-L/BERTScore). Our EMMA-500 models demonstrate consistent improvements over their Llama 3 and Llama 3.1 base models. Model Avg High Medium-high Medium Medium-low Low 9.52/65.35 7.40/61.16 7.28/61. 9.66/64.20 6.66/62.29 9.75/64.83 7.11/66.52 Llama 2 7B 12.91/67.19 12.32/67.84 8.84/68.44 Llama 2 7B Chat 9.89/62.99 7.15/65.74 CodeLlama 7B 5.29/64.59 6.86/61.74 LLaMAX Llama 2 7B 10.11/69.24 14.44/67.88 14.31/68.35 LLaMAX Llama 2 7B Alpaca 7.98/60.89 5.45/63.96 MaLA-500 10B V1 7.80/61.21 5.44/64.28 MaLA-500 10B V2 11.64/66.06 10.91/66.54 7.98/67.21 Yayi Llama 2 7B 10.64/65.26 10.45/65.62 Tower Base Llama 2 7B V0.1 7.65/67.09 12.81/67.15 12.00/67.56 Tower Instruct Llama 2 7B V0.2 8.89/68.46 11.61/66.17 11.75/66.93 8.58/67.20 EMMA-500 Llama 2 7B 10.38/63.81 10.17/64.44 7.33/66.20 Occiglot Mistral 7B 12.53/65.19 11.44/65.98 8.31/66.96 Occiglot Mistral 7B Instruct 6.99/64.78 BLOOM 7B1 8.83/61.91 11.15/69.82 20.33/67.38 19.39/68.58 BLOOMZ 7B1 12.06/69.74 19.99/67.84 19.25/68.82 Yayi 7B 12.91/66.50 11.90/66.45 8.68/66.79 Aya 23 8B 10.51/68.73 14.88/67.38 14.09/67.96 Aya Expanse 8B 9.16/62.37 6.70/64.52 Gemma 7B 9.82/61.04 7.38/65.45 Gemma 2 9B 9.58/69.13 Qwen 1.5 7B 14.33/67.56 13.58/68.14 10.18/69.35 15.67/68.37 14.66/68.66 Qwen 2 7B 10.41/69.69 15.68/69.57 14.96/69.69 Qwen 2.5 7B 11.46/70.41 17.22/70.74 16.65/70.83 Macro-LLM GLO 7B 10.96/64.16 11.24/65.16 8.47/67.08 Llama 3 8B 10.97/63.60 11.19/64.55 8.57/66.97 Llama 3.1 8B 8.28/66.72 LLaMAX Llama 3 8B 10.85/64.80 10.97/65.33 11.39/69.99 15.45/69.66 15.82/70.20 LLaMAX Llama 3 8B Alpaca EMMA-500 Llama 3 8B Mono 12.96/64.51 13.25/65.23 9.11/66.12 EMMA-500 Llama 3 8B Bi 13.30/65.28 13.11/65.91 9.10/66.72 EMMA-500 Llama 3.1 8B Mono 12.88/65.35 13.18/66.30 9.66/67.21 EMMA-500 Llama 3.1 8B Bi 12.19/64.10 11.75/64.40 8.56/65.90 9.16/62.76 9.83/62.66 8.62/63.21 7.73/65.39 9.67/67.51 7.83/64.68 5.79/63.17 5.94/62.55 5.91/62.79 8.71/66.32 8.30/65.97 9.65/67.52 9.52/67.68 8.02/64.88 9.11/66.04 7.55/63. 8.34/65.08 7.62/65.58 10.47/67.45 9.54/67.69 7.71/64.74 8.38/63.72 5.99/61.96 5.71/63.24 12.42/68.26 11.12/68.38 11.19/68.19 5.88/62.48 6.38/61.14 5.86/62.71 6.34/61.51 8.59/66.63 9.41/66.17 8.97/65.39 8.21/66.12 10.30/67.06 9.53/67.72 10.29/67.92 9.41/67.80 8.86/64.39 7.93/65.00 10.08/65.16 9.04/66.17 7.64/62.69 7.43/63.72 15.83/68.77 12.52/69.37 12.50/69.21 16.07/68.85 13.42/69.14 13.37/68.97 9.25/67.11 9.92/67.34 12.58/69.15 11.24/69.07 11.32/68.88 7.33/64.93 8.17/64.49 8.91/62.78 8.01/64.14 12.04/68.02 10.42/68.34 10.44/68.16 12.70/68.42 11.12/68.58 11.10/68.39 12.95/70.62 11.38/70.19 11.31/69.93 14.31/71.32 12.65/71.03 12.62/70.76 10.12/65.04 9.29/65.82 10.14/64.81 9.45/65.66 9.71/66.92 9.04/67.25 13.36/70.75 12.44/70.53 12.42/70.34 11.30/66.81 10.09/66.83 9.96/66.50 11.17/67.34 10.10/67.46 9.94/67.12 11.53/67.78 10.63/67.93 10.58/67.66 10.51/66.43 9.51/66.53 9.31/65.79 9.43/65.66 9.07/66.99 7.30/64.67 8.07/64.11 9.47/66.31 9.43/66.91 improvements over the base Llama model. However, when measured by the number of languages with improved performance, our EMMA-500 CPT is better than the base models. Contemporary 36 models such as Aya Expanse, Llama 3.1, and Qwen 2 demonstrate more competent performance, suggesting advances in multilingual understanding and reasoning capacities. On the harder ARCMultilingual set, the bilingual Llama-3 variant essentially ties the base model ( 35%), while the others dip by up to one point. These results suggest that EMMA-style bilingual continual pre-training alone is insufficient for cross-language passage-question reasoning at the 8 scale, the fundamental limitations of Llama models, and the average-based evaluation practice. Table 23: 0-shot results (Accuracy %) on BELEBELE, and 5-shot results (Accuracy %) on the multilingual ARC."
        },
        {
            "title": "Model",
            "content": "Avg High Med-high Medium Med-low Low Avg High Medium Low"
        },
        {
            "title": "ARC Multilingual",
            "content": "26.27 26.76 Llama 2 7B 29.05 31.84 Llama 2 7B Chat 27.38 27.37 CodeLlama 2 7B 23.09 23.23 LLaMAX Llama 2 7B 24.48 25.46 LLaMAX Llama 2 7B Alpaca 22.96 23.02 MaLA-500 Llama 2 10B v1 22.96 23.02 MaLA-500 Llama 2 10B v2 28.32 29.64 YaYi Llama 2 7B 26.36 27.43 TowerBase Llama 2 7B 27.93 29.88 TowerInstruct Llama 2 7B 26.75 28.32 EMMA-500 Llama 2 7B Occiglot Mistral 7B v0.1 30.16 32.25 Occiglot Mistral 7B v0.1 Instruct 32.05 34.14 24.11 24.25 BLOOM 7B 39.32 45.43 BLOOMZ 7B 37.97 44.37 YaYi 7B 40.08 43.85 Aya 23 8B 46.98 52.22 Aya Expanse 8B 43.37 52.63 Gemma 7B 54.49 64.10 Gemma 2 9B 41.83 48.86 Qwen 1.5 7B 49.31 57.62 Qwen 2 7B 54.11 63.47 Qwen 2.5 7B 53.95 63.54 Marco-LLM GLO 7B 40.73 46.07 Llama 3 8B 45.19 52.50 Llama 3.1 8B 36.96 40.06 LLaMAX Llama 3 8B 39.41 44.06 LLaMAX Llama 3 8B Alpaca EMMA-500 Llama 3 8B Mono 39.73 43.65 EMMA-500 Llama 3 8B Bi 39.84 43.51 EMMA-500 Llama 3.1 8B Mono 38.86 41.68 EMMA-500 Llama 3.1 8B Bi 37.00 40.59 26.35 29.97 27.33 23.15 24.82 22.98 22.98 28.67 26.85 28.51 28.18 30.94 32.62 24.52 43.67 42.71 41.71 48.99 47.83 58.90 44.79 52.20 57.91 58.67 42.92 48.01 37.92 41.10 41.40 41.09 39.34 38.36 26.07 28.95 27.30 23.07 24.41 22.97 22.97 28.11 26.29 27.57 27.58 30.02 31.74 24.12 41.51 40.49 39.22 46.57 44.82 55.83 42.18 50.04 55.47 55.50 41.03 45.65 36.78 39.52 39.96 40.05 38.49 37.21 26.41 26.27 27.56 33.12 29.47 29.09 28.02 33.69 27.30 27.38 25.23 28.86 23.10 23.08 26.09 30.00 24.60 24.49 31.06 36.89 22.98 22.97 21.16 21.92 22.98 22.97 21.16 21.92 28.37 28.26 28.40 34.30 26.48 26.34 27.94 35.32 28.19 27.92 30.10 38.88 27.14 26.94 29.53 34.10 30.40 30.15 29.77 38.39 32.40 32.08 30.88 40.29 24.11 24.08 23.65 26.27 40.08 39.51 23.95 26.94 38.72 38.09 24.44 27.96 40.93 39.81 31.08 40.05 48.36 46.93 36.56 47.87 45.43 43.94 38.68 46.46 56.85 55.05 44.15 54.59 43.00 41.78 28.93 35.55 51.16 49.48 33.82 43.88 56.24 54.30 35.30 46.43 56.20 54.31 36.34 46.72 41.87 40.88 34.80 42.43 46.74 45.34 34.93 42.43 37.68 37.09 33.54 39.81 40.53 39.61 34.53 41.34 40.76 40.01 33.22 38.32 40.74 39.99 34.84 40.37 39.62 38.94 34.00 39.38 37.94 37.25 34.59 39.85 27.31 21.02 27.79 21.29 24.64 21.65 25.92 21.48 31.85 22.49 20.48 21.32 20.48 21.32 28.35 21.11 26.82 20.51 28.85 21.16 29.82 23.34 28.51 21.03 29.65 21.13 22.72 21.89 22.74 22.18 23.29 21.91 30.02 21.61 36.15 23.09 40.47 26.06 46.18 27.82 28.14 21.92 32.64 23.17 34.31 23.00 35.88 24.11 35.53 24.06 35.89 24.00 34.84 23.59 35.56 24.34 34.56 24.67 36.45 25.31 35.39 25.01 36.21 25.38 E.7 Math We use the Multilingual Grade School Math Benchmark (MGSM) (Shi et al., 2022) to evaluate the mathematical reasoning in LLMs. As shown in Table 24, we evaluate model performance using 3-shot prompting with flexible answer matching. Our assessment employs two distinct prompting strategies: (1) Direct: direct question prompting, and (2) Chain-of-Thought (CoT): prompting accompanied by step-by-step reasoning examples (Wei et al., 2022). The results show that our CPT models, as well as LLaMAX CPT models, obtain decreased average performance on direct prompting. Our EMMA-500 Llama 3.1 8B Mono model slightly improves the base model. Notably, our EMMA-500 Llama 3/3.1 CPT models significantly improve the performance on low-resource languages for both direct and CoT prompting. Tables 25 and 26 present the per-language performance across all the languages by direct and CoT prompting."
        },
        {
            "title": "F Ethics Considerations",
            "content": "Data Collection and Use This work involves the compilation and use of large-scale monolingual and bilingual textual corpora covering over 500 languages. All datasets used for continual pre-training are obtained from publicly available sources or established linguistic resources that permit academic research use. Given the scale and diversity of the data, we acknowledge the potential presence of 37 Table 24: 3-shot results (Accuracy %) on MGSM obtained by direct and CoT prompting. Our EMMA-500 CPT models significantly improve the performance on low-resource languages."
        },
        {
            "title": "Avg High Medium Low Avg High Medium Low",
            "content": "6.69 5.93 3.35 5.05 0.91 0.91 7.09 6.15 7.24 8.07 Llama 2 7B 10.22 13.73 Llama 2 7B Chat 7.07 CodeLlama 2 7B 4.00 LLaMAX Llama 2 7B 5.20 LLaMAX Llama 2 7B Alpaca 1.33 MaLA-500 Llama 2 10B v1 1.33 MaLA-500 Llama 2 10B v2 9.47 YaYi Llama 2 7B 8.33 TowerBase Llama 2 7B 9.53 TowerInstruct Llama 2 7B 17.02 19.20 EMMA-500 Llama 2 7B Occiglot Mistral 7B v0.1 13.31 16.87 Occiglot Mistral 7B v0.1 Instruct 22.76 29.80 2.60 BLOOM 7B 2.67 BLOOMZ 7B 2.93 YaYi 7B 22.29 30.67 Aya 23 8B 43.02 55.47 Aya Expanse 8B 38.22 36.60 Gemma 7B 32.95 28.00 Gemma 2 9B 31.56 40.00 Qwen 1.5 7B 48.95 54.40 Qwen 2 7B 53.78 65.33 Qwen 2.5 7B 51.85 63.80 Marco-LLM GLO 7B 27.45 27.87 Llama 3 8B 28.36 29.00 Llama 3.1 8B 20.80 22.73 LLaMAX Llama 3 8B 14.18 14.87 LLaMAX Llama 3 8B Alpaca EMMA-500 Llama 3 8B Mono 23.53 22.33 EMMA-500 Llama 3 8B Bi 23.49 23.00 EMMA-500 Llama 3.1 8B Mono 24.95 23.67 EMMA-500 Llama 3.1 8B Bi 23.85 23.67 2.87 2.55 2.76 1.20 7.60 6.36 2.13 0.80 10.91 13.53 2.13 8.73 6.64 1.20 2.93 4.33 3.62 0.80 2.00 8.07 6.35 1.60 4.00 1.27 0.73 0.00 0.27 1.27 0.73 0.00 0.27 8.73 7.22 1.20 1.47 6.16 0.80 8.60 1.73 2.00 8.24 10.47 1.73 2.40 18.09 20.00 11.87 1.60 14.07 18.80 4.53 2.80 22.16 30.40 7.47 2.20 2.29 3.60 2.80 1.67 2.15 1.20 2.40 2.40 2.93 3.02 1.47 0.80 24.71 35.07 3.47 18.93 5.20 41.45 54.67 38.27 27.20 35.78 34.67 35.73 30.80 44.69 36.07 16.00 4.00 30.36 40.60 38.80 14.80 51.47 58.93 36.13 8.00 55.60 68.87 32.80 11.60 52.02 62.93 5.60 28.13 28.53 26.13 4.40 27.31 27.27 26.13 6.80 19.96 20.93 20.40 11.87 6.00 17.16 20.87 25.60 12.00 25.33 24.67 24.67 10.80 26.29 24.87 27.20 11.20 27.35 26.60 25.33 11.20 25.76 26.27 0.80 2.13 1.60 2.80 2.00 2.67 2.40 2.27 0.80 4.13 0.00 0.00 0.00 0.00 1.20 2.27 0.80 2.40 1.20 1.87 2.80 13.20 1.20 3.60 2.80 7.87 2.00 1.47 2.00 3.07 2.00 2.40 2.40 5.47 18.53 5.20 37.07 26.80 52.00 47.20 14.80 2.40 39.07 14.40 36.40 8.80 34.93 11.60 5.20 26.67 8.40 25.47 3.60 22.40 12.27 4.00 27.73 13.60 30.67 11.60 30.27 10.80 27.87 12. Table 25: 3-shot results (Accuracy %) on MGSM in all languages by direct prompting and flexible matching. Model Avg bn bn-stderr de de-stderr en en-stderr es es-stderr fr fr-stderr ja ja-stderr ru ru-stderr sw sw-stderr te te-stderr th th-stderr zh zh-stderr 6.69 2.80 Llama 2 7B 10.22 2.80 Llama 2 7B Chat 5.93 1.60 CodeLlama 2 7B 3.35 2.80 LLaMAX Llama 2 7B 5.05 4.00 LLaMAX Llama 2 7B Alpaca 0.91 0.00 MaLA-500 Llama 2 10B v1 0.91 0.00 MaLA-500 Llama 2 10B v2 7.09 3.20 YaYi Llama 2 7B 6.15 2.40 TowerBase Llama 2 7B 7.24 1.60 TowerInstruct Llama 2 7B 17.02 8.80 EMMA-500 Llama 2 7B Occiglot Mistral 7B v0.1 13.31 3.20 Occiglot Mistral 7B v0.1 Instruct 22.76 4.80 2.87 2.40 BLOOM 7B 2.55 3.20 BLOOMZ 7B 2.76 2.40 YaYi 7B 22.29 3.20 Aya 23 8B 43.02 22.40 Aya Expanse 8B 38.22 34.40 Gemma 7B 32.95 29.60 Gemma 2 9B 31.56 12.40 Qwen 1.5 7B 48.95 44.40 Qwen 2 7B 53.78 26.80 Qwen 2.5 7B 51.85 31.60 Marco-LLM GLO 7B 27.45 17.60 Llama 3 8B 28.36 20.00 Llama 3.1 8B 20.80 13.20 LLaMAX Llama 3 8B 14.18 12.00 LLaMAX Llama 3 8B Alpaca EMMA-500 Llama 3 8B Mono 23.53 20.00 EMMA-500 Llama 3 8B Bi 23.49 21.20 EMMA-500 Llama 3.1 8B Mono 24.95 21.20 EMMA-500 Llama 3.1 8B Bi 23.85 20.80 1.05 8.00 1.05 16.80 0.80 8.80 1.05 3.60 1.24 3.60 0.00 0.00 0.00 0.00 1.12 8.40 0.97 8.40 0.80 10.00 1.80 23.20 1.12 21.20 1.35 34.00 0.97 1.60 1.12 1.60 0.97 2.00 1.12 37.20 2.64 69.20 3.01 44.80 2.89 40.40 2.09 44.00 3.15 65.60 2.81 63.20 2.95 64.40 2.41 39.60 2.53 41.20 2.15 20.40 2.06 17.60 2.53 30.00 2.59 30.80 2.59 34.80 2.57 38.00 1.72 17.60 2.37 22.80 1.80 12.80 1.18 6.00 1.18 10.80 0.00 1.20 0.00 1.20 1.76 15.60 1.76 11.60 1.90 15.20 2.68 34.00 2.59 30.00 3.00 46.40 0.80 4.00 0.80 3.60 0.89 6.00 3.06 50.00 2.93 78.40 3.15 58.80 3.11 56.40 3.15 55.20 3.01 80.80 3.06 83.20 3.03 77.60 3.10 50.80 3.12 55.20 2.55 24.40 2.41 25.20 2.90 36.00 2.93 35.60 3.02 39.60 3.08 33.20 2.41 11.20 2.66 19.60 2.12 8.80 1.51 2.00 1.97 6.00 0.69 2.00 0.69 2.00 2.30 16.00 2.03 9.20 2.28 15.60 3.00 28.00 2.90 27.20 3.16 40.00 1.24 3.60 1.18 2.40 1.51 2.40 3.17 41.20 2.61 72.00 3.12 48.00 3.14 50.80 3.15 48.40 2.50 76.00 2.37 74.00 2.64 71.20 3.17 47.20 3.15 48.40 2.72 26.00 2.75 17.20 3.04 34.80 3.03 36.80 3.10 35.60 2.98 39. 2.00 12.00 2.52 19.20 1.80 10.40 0.89 7.20 1.51 6.40 0.89 2.40 0.89 2.40 2.32 10.40 1.83 8.80 2.30 12.80 2.85 25.60 2.82 21.60 3.10 31.60 1.18 1.20 0.97 3.20 0.97 5.60 3.12 39.20 2.85 63.60 3.17 39.60 3.17 36.00 3.17 45.20 2.71 69.60 2.78 66.40 2.87 63.20 3.16 36.40 3.17 38.40 2.78 22.00 2.39 17.20 3.02 30.00 3.06 34.00 3.03 33.20 3.10 32.40 2.06 2.40 2.50 2.40 1.93 5.60 1.64 3.20 1.55 3.20 0.97 1.20 0.97 1.20 1.93 5.20 1.80 4.80 2.12 1.60 2.77 9.20 2.61 6.40 2.95 18.40 0.69 2.00 1.12 2.80 1.46 1.60 3.09 27.60 3.05 50.80 3.10 16.80 3.04 1.20 3.15 16.80 2.92 1.60 2.99 52.40 3.06 50.40 3.05 3.60 3.08 3.60 2.63 20.80 2.39 11.20 2.90 3.20 3.00 2.80 2.98 1.20 2.97 2.00 0.97 8.00 0.97 14.40 1.46 6.00 1.12 2.40 1.12 4.40 0.69 2.00 0.69 2.00 1.41 5.60 1.35 10.00 0.80 10.00 1.83 22.80 1.55 15.20 2.46 23.60 0.89 3.60 1.05 3.60 0.80 1.20 2.83 34.00 3.17 69.60 2.37 41.20 0.69 35.20 2.37 43.20 0.80 67.20 3.16 72.80 3.17 63.60 1.18 37.60 1.18 40.00 2.57 22.80 2.00 15.60 1.12 33.20 1.05 30.40 0.69 34.80 0.89 28.40 1.72 2.80 2.22 0.80 1.51 2.00 0.97 1.60 1.30 4.80 0.89 0.40 0.89 0.40 1.46 0.80 1.90 1.20 1.90 1.60 2.66 16.80 2.28 2.40 2.69 6.40 1.18 4.00 1.18 2.80 0.69 1.20 3.00 2.80 2.92 12.80 3.12 37.60 3.03 37.60 3.14 7.20 2.98 16.00 2.82 20.00 3.05 18.00 3.07 24.00 3.10 20.80 2.66 21.20 2.30 10.80 2.98 33.20 2.92 30.00 3.02 36.80 2.86 27.20 1.05 1.20 0.56 0.80 0.89 1.20 0.80 0.80 1.35 1.60 0.40 0.00 0.40 0.00 0.56 1.20 0.69 0.80 0.80 2.00 2.37 2.40 0.97 1.60 1.55 2.80 1.24 3.60 1.05 1.20 0.69 2.40 1.05 0.80 2.12 5.20 3.07 27.20 3.07 30.80 1.64 4.00 2.32 14.80 2.53 8.00 2.43 11.60 2.71 5.60 2.57 4.40 2.59 6.80 1.97 6.00 2.98 12.00 2.90 10.80 3.06 11.20 2.82 11.20 0.69 0.80 0.56 2.80 0.69 5.20 0.56 1.60 0.80 3.20 0.00 0.40 0.00 0.40 0.69 0.40 0.56 1.60 0.89 2.00 0.97 10.00 0.80 8.00 1.05 11.20 1.18 2.00 0.69 1.20 0.97 0.80 0.56 4.40 1.41 21.60 2.82 42.80 2.93 40.00 1.24 28.40 2.25 56.00 1.72 61.60 2.03 48.80 1.46 36.80 1.30 37.60 1.60 26.80 1.51 12.80 2.06 23.60 1.97 22.80 2.00 23.60 2.00 28. 0.56 6.80 1.05 10.00 1.41 2.80 0.80 5.60 1.12 7.60 0.40 0.40 0.40 0.40 0.40 11.20 0.80 8.80 0.89 7.20 1.90 6.40 1.72 9.60 2.00 31.20 0.89 3.60 0.69 2.40 0.56 4.80 1.30 4.80 2.61 7.60 3.14 29.20 3.10 4.40 2.86 42.40 3.15 46.40 3.08 63.20 3.17 70.00 3.06 2.80 3.07 2.40 2.81 24.40 2.12 10.40 2.69 2.80 2.66 3.20 2.69 2.40 2.85 1.60 1.60 1.90 1.05 1.46 1.68 0.40 0.40 2.00 1.80 1.64 1.55 1.87 2.94 1.18 0.97 1.35 1.35 1.68 2.88 1.30 3.13 3.16 3.06 2.90 1.05 0.97 2.72 1.93 1.05 1.12 0.97 0.80 sensitive or harmful content. Future work could include filtering mechanisms or annotation efforts to improve the quality and safety of multilingual data. Representation of Low-Resource Languages Our research aims to expand LLM coverage to underrepresented languages. However, the quantity and quality of data for low-resource languages remain uneven. This may inadvertently lead to biased model behaviors or inadequate performance for some linguistic groups. Community Involvement This work does not directly involve community collaboration. However, we recognize the importance of inclusive research practices and welcome future partnerships with Table 26: 3-shot results (Accuracy %) on MGSM in all languages by CoT prompting and flexible matching. Model Avg bn bn-stderr de de-stderr en en-stderr es es-stderr fr fr-stderr ja ja-stderr ru ru-stderr sw sw-stderr te te-stderr th th-stderr zh zh-stderr 6.36 2.00 Llama 2 7B 10.91 2.40 Llama 2 7B Chat 6.64 1.60 CodeLlama 2 7B 3.62 3.60 LLaMAX Llama 2 7B 6.35 3.20 LLaMAX Llama 2 7B Alpaca 0.73 0.00 MaLA-500 Llama 2 10B v1 0.73 0.00 MaLA-500 Llama 2 10B v2 7.22 2.80 YaYi Llama 2 7B 6.16 3.60 TowerBase Llama 2 7B 8.24 1.20 TowerInstruct Llama 2 7B 18.09 8.40 EMMA-500 Llama 2 7B Occiglot Mistral 7B v0.1 14.07 2.40 Occiglot Mistral 7B v0.1 Instruct 22.16 4.00 2.29 2.00 BLOOM 7B 2.15 2.00 BLOOMZ 7B 3.02 4.80 YaYi 7B 24.71 6.00 Aya 23 8B 41.45 21.20 Aya Expanse 8B 35.78 36.40 Gemma 7B 44.69 48.80 Gemma 2 9B 30.36 12.80 Qwen 1.5 7B 51.47 45.20 Qwen 2 7B 55.60 25.60 Qwen 2.5 7B 52.02 32.00 Marco-LLM GLO 7B 28.13 19.20 Llama 3 8B 27.31 19.20 Llama 3.1 8B 19.96 12.40 LLaMAX Llama 3 8B 17.16 14.80 LLaMAX Llama 3 8B Alpaca EMMA-500 Llama 3 8B Mono 25.33 23.20 EMMA-500 Llama 3 8B Bi 26.29 22.00 EMMA-500 Llama 3.1 8B Mono 27.35 22.00 EMMA-500 Llama 3.1 8B Bi 25.76 24.40 0.89 7.60 0.97 17.60 0.80 9.20 1.18 3.60 1.12 4.80 0.00 0.40 0.00 0.40 1.05 8.40 1.18 8.00 0.69 12.80 1.76 22.40 0.97 21.60 1.24 33.20 0.89 1.60 0.89 2.00 1.35 3.20 1.51 40.80 2.59 68.80 3.05 40.80 3.17 53.60 2.12 43.20 3.15 66.00 2.77 69.60 2.96 65.20 2.50 38.40 2.50 42.00 2.09 20.40 2.25 18.00 2.68 34.00 2.63 34.40 2.63 38.00 2.72 38.00 1.68 16.00 2.41 27.20 1.83 13.20 1.18 7.20 1.35 15.20 0.40 0.40 0.40 0.40 1.76 16.80 1.72 11.20 2.12 18.80 2.64 37.60 2.61 33.20 2.98 43.20 0.80 4.80 0.89 2.40 1.12 4.80 3.11 48.80 2.94 78.80 3.11 60.80 3.16 72.00 3.14 58.40 3.00 82.00 2.92 81.60 3.02 78.80 3.08 54.00 3.13 52.80 2.55 24.80 2.43 26.00 3.00 37.60 3.01 40.40 3.08 40.00 3.08 35.20 2.32 12.40 2.82 19.20 2.15 10.80 1.64 3.60 2.28 9.60 0.40 0.40 0.40 0.40 2.37 12.40 2.00 8.40 2.48 15.20 3.07 25.60 2.98 26.80 3.14 42.00 1.35 2.40 0.97 3.20 1.35 3.60 3.17 41.20 2.59 76.80 3.09 45.20 2.85 60.40 3.12 50.40 2.43 75.20 2.46 73.60 2.59 70.00 3.16 46.00 3.16 42.80 2.74 25.60 2.78 22.40 3.07 40.40 3.11 42.80 3.10 41.60 3.03 38. 2.09 9.20 2.50 18.80 1.97 10.80 1.18 5.20 1.87 5.20 0.40 1.60 0.40 1.60 2.09 10.40 1.76 7.60 2.28 12.00 2.77 21.60 2.81 22.40 3.13 31.60 0.97 2.40 1.12 3.20 1.18 4.00 3.12 40.00 2.68 65.20 3.15 42.00 3.10 50.00 3.17 42.00 2.74 71.60 2.79 68.00 2.90 64.00 3.16 36.00 3.14 34.80 2.77 18.40 2.64 24.00 3.11 32.80 3.14 37.20 3.12 36.40 3.09 35.20 1.83 3.60 2.48 4.00 1.97 4.40 1.41 2.80 1.41 4.40 0.80 1.60 0.80 1.60 1.93 4.80 1.68 3.60 2.06 2.40 2.61 8.00 2.64 6.40 2.95 16.80 0.97 0.40 1.12 2.00 1.24 2.00 3.10 28.40 3.02 29.60 3.13 3.60 3.17 2.00 3.13 11.20 2.86 2.80 2.96 60.00 3.04 48.80 3.04 2.00 3.02 2.80 2.46 18.80 2.71 13.60 2.98 1.60 3.06 0.80 3.05 3.20 3.03 1.60 1.18 8.00 1.24 15.20 1.30 7.60 1.05 2.80 1.30 3.60 0.80 2.40 0.80 2.40 1.35 7.20 1.18 8.40 0.97 13.60 1.72 22.80 1.55 17.20 2.37 24.40 0.40 4.40 0.89 1.60 0.89 1.20 2.86 37.20 2.89 67.60 1.18 39.20 0.89 50.00 2.00 36.80 1.05 68.40 3.10 74.80 3.17 64.40 0.89 38.00 1.05 37.20 2.48 20.40 2.17 22.40 0.80 33.60 0.56 31.60 1.12 37.20 0.80 33.20 1.72 2.00 2.28 2.00 1.68 1.20 1.05 0.80 1.18 4.80 0.97 0.40 0.97 0.40 1.64 2.00 1.76 1.20 2.17 1.60 2.66 21.20 2.39 3.20 2.72 5.60 1.30 1.60 0.80 2.40 0.69 1.60 3.06 5.60 2.97 11.20 3.09 38.80 3.17 53.60 3.06 8.80 2.95 17.20 2.75 17.20 3.03 18.00 3.08 25.20 3.06 25.20 2.55 23.20 2.64 12.40 2.99 32.40 2.95 33.60 3.06 42.40 2.98 35.20 0.89 0.80 0.89 0.80 0.69 1.60 0.56 0.80 1.35 0.40 0.40 0.00 0.40 0.00 0.89 1.20 0.69 0.80 0.80 1.20 2.59 2.40 1.12 1.20 1.46 1.60 0.80 2.00 0.97 2.00 0.80 2.80 1.46 2.40 2.00 2.40 3.09 24.80 3.16 44.40 1.80 4.00 2.39 17.60 2.39 9.20 2.43 9.60 2.75 6.40 2.75 5.60 2.68 3.60 2.09 7.20 2.97 12.80 2.99 12.00 3.13 10.40 3.03 8.80 0.56 1.60 0.56 2.80 0.80 6.00 0.56 2.00 0.40 4.80 0.00 0.00 0.00 0.00 0.69 3.20 0.56 2.80 0.69 3.20 0.97 11.60 0.69 6.00 0.80 8.40 0.89 2.00 0.89 1.60 1.05 0.80 0.97 8.40 0.97 20.00 2.74 44.00 3.15 55.20 1.24 25.60 2.41 58.40 1.83 56.80 1.87 56.00 1.55 39.20 1.46 35.60 1.18 30.80 1.64 11.60 2.12 24.00 2.06 29.20 1.93 26.40 1.80 25. 0.80 8.00 1.05 11.60 1.51 4.00 0.89 4.80 1.35 6.80 0.00 0.80 0.00 0.80 1.12 12.40 1.05 9.20 1.12 10.80 2.03 16.40 1.51 11.20 1.76 24.80 0.89 2.40 0.80 1.20 0.56 6.00 1.76 6.80 2.53 2.80 3.15 4.80 3.15 1.60 2.77 26.00 3.12 60.80 3.14 72.80 3.15 63.60 3.09 4.00 3.03 1.60 2.93 19.60 2.03 13.20 2.71 2.80 2.88 1.20 2.79 2.00 2.75 1.20 1.72 2.03 1.24 1.35 1.60 0.56 0.56 2.09 1.83 1.97 2.35 2.00 2.74 0.97 0.69 1.51 1.60 1.05 1.35 0.80 2.78 3.09 2.82 3.05 1.24 0.80 2.52 2.15 1.05 0.69 0.89 0.69 linguists, native speakers, and regional institutions to improve the quality and cultural relevance of multilingual language technologies."
        }
    ],
    "affiliations": [
        "Technical University of Darmstadt",
        "University of Helsinki"
    ]
}