{
    "paper_title": "AgencyBench: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World Contexts",
    "authors": [
        "Keyu Li",
        "Junhao Shi",
        "Yang Xiao",
        "Mohan Jiang",
        "Jie Sun",
        "Yunze Wu",
        "Shijie Xia",
        "Xiaojie Cai",
        "Tianze Xu",
        "Weiye Si",
        "Wenjie Li",
        "Dequan Wang",
        "Pengfei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) based autonomous agents demonstrate multifaceted capabilities to contribute substantially to economic production. However, existing benchmarks remain focused on single agentic capability, failing to capture long-horizon real-world scenarios. Moreover, the reliance on human-in-the-loop feedback for realistic tasks creates a scalability bottleneck, hindering automated rollout collection and evaluation. To bridge this gap, we introduce AgencyBench, a comprehensive benchmark derived from daily AI usage, evaluating 6 core agentic capabilities across 32 real-world scenarios, comprising 138 tasks with specific queries, deliverables, and rubrics. These scenarios require an average of 90 tool calls, 1 million tokens, and hours of execution time to resolve. To enable automated evaluation, we employ a user simulation agent to provide iterative feedback, and a Docker sandbox to conduct visual and functional rubric-based assessment. Experiments reveal that closed-source models significantly outperform open-source models (48.4% vs 32.1%). Further analysis reveals significant disparities across models in resource efficiency, feedback-driven self-correction, and specific tool-use preferences. Finally, we investigate the impact of agentic scaffolds, observing that proprietary models demonstrate superior performance within their native ecosystems (e.g., Claude-4.5-Opus via Claude-Agent-SDK), while open-source models exhibit distinct performance peaks, suggesting potential optimization for specific execution frameworks. AgencyBench serves as a critical testbed for next-generation agents, highlighting the necessity of co-optimizing model architecture with agentic frameworks. We believe this work sheds light on the future direction of autonomous agents, and we release the full benchmark and evaluation toolkit at https://github.com/GAIR-NLP/AgencyBench."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 6 1 ] . [ 1 4 4 0 1 1 . 1 0 6 2 : r AGENCYBENCH: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World Contexts Keyu Li1,3,4 Junhao Shi1,4 Yang Xiao2,4 Mohan Jiang1,3,4 Jie Sun3,4 Yunze Wu1,4 Dayuan Fu3,4 Shijie Xia1,3,4 Xiaojie Cai1,4 Tianze Xu1,4 Weiye Si1,4 Wenjie Li2 Dequan Wang1,3,4 Pengfei Liu1,3,4 1SJTU 2PolyU 3SII 4GAIR"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) based autonomous agents demonstrate multifaceted capabilities to contribute substantially to economic production. However, existing benchmarks remain focused on single agentic capability, failing to capture long-horizon real-world scenarios. Moreover, the reliance on human-inthe-loop feedback for realistic tasks creates scalability bottleneck, hindering automated rollout collection and evaluation. To bridge this gap, we introduce AGENCYBENCH, comprehensive benchmark derived from daily AI usage, evaluating 6 core agentic capabilities across 32 real-world scenarios, comprising 138 tasks with specific queries, deliverables, and rubrics. These scenarios require an average of 90 tool calls, 1 million tokens, and hours of execution time to resolve. To enable automated evaluation, we employ user simulation agent to provide iterative feedback, and Docker sandbox to conduct visual and functional rubric-based assessment. Experiments reveal that closed-source models significantly outperform opensource models (48.4% vs 32.1%). Further analysis reveals significant disparities across models in resource efficiency, feedback-driven self-correction, and specific tool-use preferences. Finally, we investigate the impact of agentic scaffolds, observing that proprietary models demonstrate superior performance within their native ecosystems (e.g., Claude-4.5-Opus via Claude-Agent-SDK), while open-source models exhibit distinct performance peaks, suggesting potential optimization for specific execution frameworks. AGENCYBENCH serves as critical testbed for next-generation agents, highlighting the necessity of co-optimizing model architecture with agentic frameworks. We believe this work sheds light on the future direction of autonomous agents, and to facilitate community adoption, we release the full benchmark and evaluation toolkit at https://github.com/GAIR-NLP/AgencyBench."
        },
        {
            "title": "Benchmark",
            "content": "Browsecomp Terminal-bench SWE-verified MCPUniverse GAIA2 Toolathlon UltraHorizon Avg. Tok. Avg. Turns"
        },
        {
            "title": "Diverse\nAgentic",
            "content": "User Sim."
        },
        {
            "title": "Docker\nSandbox",
            "content": "10k 15k 200k 15 7.5 22.5 26.8 60 AGENCYBENCH 1000k 90 Figure 1: Overview of AGENCYBENCH. Left: Distribution of the 32 scenarios and 138 tasks across 6 distinct agentic capabilities. Right: Comparison with existing benchmarks. AGENCYBENCH focuses on diverse, longhorizon real-world tasks, requiring an average of 1M tokens and 90 multi-turn tool uses. It integrates user simulation agent for iterative feedback and Docker-based sandbox for automated rubric-based assessment. Corresponding author. 1 1. Introduction"
        },
        {
            "title": "Introduction",
            "content": "With the rapid advancement of Large Language Models (LLMs) (OpenAI, 2025b; DeepMind, 2025; Anthropic, 2025a; xAI, 2025), integrating these models with advanced scaffolds to form autonomous agents has become paradigm shift. As these agents increasingly permeate diverse domains, ranging from economic production, scientific research, and software development to everyday use, establishing rigorous benchmark to measure their practical economic value and performance is becoming unprecedentedly urgent (Pan et al., 2025; OpenRouter, 2025; Kwa et al., 2025). However, current agent benchmarks face significant limitations: (1) Existing benchmarks often exhibit scarcity of long-horizon tasks (Li et al., 2025a; Andrews et al., 2025) or focus narrowly on single agentic capability, such as tool use (Terminal-bench Team, 2025; Wu et al., 2025b), software engineering (Jimenez et al., 2023) or research (Wei et al., 2025; Xu et al., 2025), failing to capture the long-horizon nature and diversity of real-world tasks. (2) Furthermore, completing realistic tasks often necessitates continuous human feedback to guide agents through multi-turn interactions. This reliance on human-in-the-loop processes creates bottleneck, restricting the automation of rollout collection and evaluation. To bridge this gap, we introduce AGENCYBENCH, comprehensive benchmark designed to evaluate agent capabilities through highly long-horizon, diverse, and authentic real-world tasks. By requesting AI researchers, active AI practitioners, and software engineer developers, we systematically construct 32 real-world scenarios evaluating 6 core agentic capabilities, comprising total of 138 specific tasks. Each task is defined by queries (descriptions of task requirements), deliverables (descriptions of the expected outputs), and rubrics (evaluation criteria for assessment). These scenarios are notably demanding: on average, resolving single scenario approximately 90 tool calls, consumes 1 million tokens, and requires hours of execution time, rigorously evaluating agents ability to maintain context and execute logic over extended periods to satisfy multi-turn, long-horizon real-world needs. The comparison between AGENCYBENCH and various other benchmarks is shown in Figure 1. To facilitate scalable and automated rollout collection, we develop an agent scaffold equipped with comprehensive tool suite operating within an isolated workspace. In this environment, the agent engages in multi-turn interactions to generate raw deliverables, assisted by user simulation agent that mimics real-world scenarios and provides iterative feedback to bypass human-in-the-loop limitations. Subsequently, these deliverables are synchronized to Docker-based remote sandbox, which emulates human-computer operations (e.g., UI rendering, mouse/keyboard inputs) to produce visual evaluation artifacts. The process concludes in separate eval-space, where these artifacts and raw deliverables undergo automated rubric-based assessment using executable scripts. Extensive experiments on AGENCYBENCH indicate that closed-source models achieve an average score of 48.4%, while open-source models average 32.1%. Closed-source models range from 56.5% (GPT-5.2) to 44.3% (Grok4.1-Fast), whereas open-source models span from 38.6% (GLM-4.6) to 27.0% (Qwen-3-235B-A22B-Thinking). The overall performance reveals that current frontier models still struggle to master the long-horizon, real-world tasks. Further analysis reveals distinct behavioral differences among the models: GPT-5.2 demonstrates stronger capabilities in feedback-driven self-correction, Grok-4.1-fast exhibits higher token utilization efficiency, Claude4.5-Opus shows preference for shell-based tools, and Gemini-3-Pro favors file-related and memory management tools. Additionally, comparative studies on agentic scaffolds highlight home-field advantage, where models achieve peak performance when paired with their native or specifically optimized frameworks. In summary, our contributions are as follows: (1) We introduce AGENCYBENCH, challenging benchmark with 138 authentic tasks across 32 scenarios to evaluate long-horizon agentic capabilities. (2) We develop unified evaluation framework leveraging user simulation agent and Docker-based remote sandboxes to achieve fully automated evaluation. (3) We provide comprehensive evaluation of frontier models, quantifying the gap between proprietary and open-source models and uncovering distinct behavioral patterns."
        },
        {
            "title": "2 Related Work",
            "content": "Large Language Model Agents The continuous and rapid evolution of large language models (LLMs) (Team et al., 2025; Yang et al., 2025; Liu et al., 2025; GLM Team, 2025; xAI, 2025; Anthropic, 2025a; DeepMind, 2025; OpenAI, 2025b) has fundamentally reshaped the landscape of artificial intelligence, propelling the field from simple conversational tasks to complex reasoning (Xiao et al., 2025c,d,b) and multi-turn tool-use tasks (Li et al., 2025b; Xiao et al., 2025a; Wu et al., 2025a). To enable these models to tackle intricate real-world challenges, numerous agentic scaffolds have been developed (Yao et al., 2022; Yang et al., 2024; Wang et al., 2024; Cursor, 2025; OpenAI, 2025a; Anthropic, 2025c), effectively unlocking the potential for long-horizon task completion through iterative feedback and structured planning. Agents are now expected to autonomously navigate diverse scenarios, ranging from full-stack front-end development to complex game engine manipulations. Agent Benchmarks To rigorously evaluate these growing capabilities, various benchmarks have emerged, focusing on specific vertical domains like tool use (Li et al., 2025a; Barres et al., 2025; Terminal-bench Team, 2025; Wu et al., 2025b), software development (Miserendino et al.; DesignArena Team, 2025; Jimenez et al., 2023), 2 3. AGENCYBENCH Figure 2: AGENCYBENCH Rollout Generation and Evaluation Pipeline. Rollout generation takes place within workspace, where the agent receives task queries and deliverables, completing tasks through multi-turn interactions with the environment (e.g., tool execution results and feedback from the user simulation agent). Upon task completion, deliverables are synced to Docker sandbox for operation execution (e.g., UI actions), and resulting artifacts are transferred to eval-space for scoring (0 10) via rule-based or LLM-based judges based on task rubrics. or open-ended research (Wei et al., 2025; Andrews et al., 2025; Patwardhan et al., 2025), with some recent efforts assessing potential economic utility (Miserendino et al.; Patwardhan et al., 2025; Xiao et al., 2025b). However, current benchmarks often lack the necessary complexity to differentiate frontier models, as the limited number of tool calls and shallow interaction depths remain insufficient to test the upper limits of modern agents (Wu et al., 2025b; Andrews et al., 2025; Li et al., 2025a; Luo et al., 2025). AGENCYBENCH addresses this critical gap by curating 138 authentic, high-fidelity tasks across 6 agentic capabilities, where the average rollout exceeds 1M tokens and necessitates over 90 precise tool calls, significantly raising the bar for task difficulty and context retention."
        },
        {
            "title": "3 AGENCYBENCH",
            "content": "In this section, we detail the hierarchical design of AGENCYBENCH (Section 3.1), the data collection process and the formal definition of interaction rollouts (Section 3.2). Finally, we describe our automated evaluation framework (Section 3.3), which leverages Docker-based remote sandbox and rubric-based rigorous, scalable, and reproducible assessment. Rollout generation and evaluation pipeline is illustrated in Figure 2, with specific evaluation example detailed in Figure 3. 3.1 AGENCYBENCH Design 3.1.1 Design Pattern Capabilities, Scenarios, and Tasks To capture the multifaceted and long-horizon nature of real-world tasks, AGENCYBENCH is structured hierarchically. It evaluates 6 core agentic capabilities: game development, frontend development, back-end development, code generation, research, and MCP tool use. These capabilities are distributed across 32 authentic real-world scenarios, such as developing Gomoku game from scratch (developing board game capability), conducting project-level code debugging (agentic debugging capability), performing in-depth corporate research (research capability), etc. Serving as complete testing unit, each scenario is structured as logical hierarchy of 1 to 5 tasks, arranged in ascending order of difficulty and presented sequentially, where the completion results of preceding tasks influence subsequent ones. Through this design, scenarios are expanded into comprehensive collection totaling 138 distinct tasks. This design is intended to simulate real-world tasks that progress from basic to complex tasks, thereby requiring agent to maintain context and execute logic over extended periods to satisfy multi-turn, long-horizon nature of real-world tasks. Workspace, Eval-space and Scaffold To address the limitations of existing benchmarks that rely on unscalable human experts feedback for complex environments, we design robust execution infrastructure. Each task operates within an isolated workspace to ensure reproducibility and prevent state interference. This workspace is equipped with an agent scaffold (a suite of tools including file manipulation, command-line execution, web search, context management, ...) allowing the agent to generate rollouts in realistic setting. For each task, the agent engages in multi-turn interactions with the environment using the scaffold within the workspace to generate deliverables. To facilitate assessment, deliverables are synced to Docker-based remote sandbox, which emulates human computer operations (e.g., mouse clicks, UI rendering) to produce visualizable artifacts. These artifacts are subsequently 3 3.2 Rollout Collection Figure 3: An Illustrative Evaluation Scenario in AGENCYBENCH: Developing Gomoku Game. The scenario consists of five sequential tasks with increasing complexity, requiring the incremental addition of new features. The primary deliverables include HTML, CSS, and JS source code. Evaluation scripts execute these files within remote Docker sandbox, performing interactive operations such as clicking, screen recording, and capturing screenshots (visualized as video frames in the figure). The resulting evaluation artifacts are retrieved to eval-space, where text and vision agents assess the code and visual deliverables, respectively, providing scores and qualitative feedback based on rubrics. Right: The file organization architecture during runtime, showing the isolated workspace and eval-space for each task to ensure environmental consistency and prevent cross-task interference. transferred to local eval-space, where the evaluation process is completed automatically using executable scripts without human intervention. 3.1.2 Data Collection We employ 20 human experts, including AI researchers, active AI practitioners, software engineer developers to collect real-world tasks. Based on this data, these human experts systematically construct the 32 scenarios and 138 tasks. For each task, experts manually construct and verify three key components: (1) the Query, which describes the specific requirements; (2) the Deliverables, which define the expected file outputs or terminal states; and (3) the Rubrics, which establish the objective criteria for assessment. Furthermore, experts develop executable evaluation scripts for each task. separate panel of four experts conducts comprehensive review of the entire dataset to verify descriptive accuracy, difficulty calibration, and environment configurations. To ensure the highest quality standards, strict unanimous consensus policy is enforced: task is only finalized if all experts reach full agreement. If any discrepancy arises, the task is flagged and remanded for revision, requiring subsequent re-evaluation until it meets the 100% approval threshold. 3.2 Rollout Collection Rollout Definition We formalize the interaction process as sequence of states and actions. Assuming scenario consists of five sequential tasks, we denote their individual rollouts as τ1, τ2, ... τ5. These are defined as follows: τ1 = (q1, a, t, . . . , a, u11, . . . , a, u12, ..., a, t, ...) τ2 = (q2, a, t, . . . , a, u21, . . . , a, u22, ..., a, t, ...) ... τ5 = (q5, a, t, . . . , a, u51, . . . , a, u52, ..., a, t, ...) Here, for the i-th task, qi represents the initial query provided by AGENCYBENCH. The pair (a, t) denotes the agents reasoning and specific tool calls (e.g., file writing, shell commands). user simulation agent provides feedback uij (which prompts further agent actions) whenever the deliverables fail to meet the score threshold prescribed by the rubric. The complete rollout τ for the scenario is defined as the ordered concatenation of these trajectories: 4 3.3 Evaluation Model Game Frontend Backend Code Research MCP SAvg Att Proprietary Models GPT-5.2 Claude-4.5-Opus Gemini-3-Pro Claude-4.5-Sonnet Grok-4.1-fast 74.7 52.2 49.325.4 52.10.1 81.0+6.3 60.7+8.5 51.623.1 56.5+4.3 38.813.4 65.79.0 52.1 64. 50.7 61.0 49.611.4 24.026.7 68.8+4.4 64.4+12.3 31.329.7 23.527.2 40.024.4 61.8+9.7 35.325.7 17.333.4 71.4+7.0 62.3+10.2 37.323.7 26.424.3 63.80.6 68.6+16.5 Open-Source Models GLM-4.6 Kimi-K2-Thinking Deepseek-V3.2 Qwen-3-235B-A22B-Thinking 64.3 59.2 40.618.6 54.79.6 36.522.7 49.315.0 40.418.8 57.76. 11.9 32.0 20.0 24.3+4.3 11.80.1 33.6+1.6 31.418.4 20.7+0.7 11.60.3 22.010.0 48.51.3 34.4+2.4 20.928.9 3.316.7 4.67.3 49.8 1.46 56.5 1.54 47.78.8 46.99.6 1.46 46.410.1 1.49 44.312.2 1.55 1.54 38.6 1.79 34.24.4 28.610.0 1.63 27.011.6 1. Table 1: Main Experimental Results. The table compares proprietary and open-source models. The rows for GPT-5.2 and GLM-4.6 are highlighted in blue as baselines. Colored subscripts indicate the performance gap compared to the baseline (red for improvement, blue for degradation). This formulation captures the iterative and long-horizon nature of real-world tasks. τ = (τ1, τ2, τ3, τ4, τ5) User Simulation Agent To simulate realistic human-agent collaboration without manual intervention, we implement user simulation agent, responsible for providing feedback to the task executing agent, enabling targeted improvements based on task execution. Feedback is determined based on the fulfillment of the current tasks rubrics. For instance, if an agent completes only 6 out of 10 rubrics, the user simulation agent will return the 4 failed rubrics along with their specific reasons for failure. We utilize Claude-4-Sonnet with temperature setting of 0.0 for this role (detailed prompts in Section A.2). To ensure the validity of the user simulation agent, we conducted human verification study on 50 randomly sampled interaction rollouts. Four human experts independently drafted justifications for unmet rubrics, which another four experts then independently scored against the agents feedback. Scores were assigned on an integer scale from 0 to 5, representing Highly Inconsistent, Inconsistent, Uncertain, Consistent, and Highly Consistent, respectively. The final average score reached 4.69, demonstrating high degree of alignment and confirming that the agent serves as reliable surrogate for human experts. 3.3 Evaluation Our evaluation framework is entirely rubric-based, ensuring standardized assessment across diverse domains. We employ executable evaluation scripts that map the agents deliverables to score ranging from 0 to 10. Depending on the nature of the task, we utilize either rule-based methods or LLM-as-judge mechanisms. Rule-based Evaluation For tasks with objectively verifiable ground truth, such as correct tool execution, mathematical optimization, or specific file generationwe employ rule-based evaluation. In these cases, the rubrics are directly translated into assertion logic within the evaluation scripts. The final score is calculated by mapping the pass rate of these assertions or the optimization metric to the 0-10 scale. LLM-as-Judge For tasks involving subjective qualities or complex visual outputs (e.g., game aesthetics, front-end layout), we employ an LLM-as-judge. Specifically for game and frontend scenarios, we implement multimodal judging system: (1) Text-based Judge: Evaluates code quality and logic based on the text deliverables and rubrics. We utilize Claude-4-Sonnet with temperature of 0.0. (2) Vision-based Judge: Evaluates dynamic behavior and visual correctness based on screenshots and recordings captured from remote sandbox interactions. We utilize Gemini-2.5-pro with temperature of 0.0. Detailed prompts are provided in Section A.2. For tasks requiring visual deliverables, the final score is calculated as the average of the ratings from the text and vision-based judges; otherwise, the score is determined solely by the text-based judge. To validate the reliability of our LLM judges, we compare their ratings against human annotations on held-out set of 50 tasks. Four human experts independently scored these 50 tasks based on the rubrics. The results showed Kappa score of 0.93 between the human and LLM judge scores, demonstrating the reliability of the evaluation. To comprehensively evaluate agent capabilities and resource use in AGENCYBENCH, we adopt the following metrics for assessment. 5 4. Experiments Figure 4: Efficiency Comparison Across Models. Efficiency is calculated by dividing the average score by the number of attempts and average token consumption, respectively. GPT-5.2 achieves the highest attempt efficiency, while Qwen-3-235B-A22B-Thinking ranks the lowest. For token efficiency, Grok-4.1-Fast performs best, whereas Claude-4.5-Sonnet is the least efficient one. Metric: Average Score (SAvg) Calculated as the percentage derived from rubric-based evaluations (e.g., satisfying 6 out of 10 criteria yields 60%), where higher scores indicate superior capability. Metric: Average Attempts (Att) Measures the average iterations per scenario. task is passed if the task score is at least 60%. If task fails, the user simulation agent provides feedback up to maximum of rounds until the threshold is met. Att denotes the average rounds used (ranging from 1 to K), where lower values imply stronger autonomy. Let denote the total number of tasks in the current scenario, and MAtt represent the total count of attempts used across all tasks. The Average Attempts (Att) is defined as: Att = MAtt (1) Metric: Pass Rate (P ass@k) Given tasks in scenario, let Npass be the number of tasks achieving the 60% score threshold within feedback rounds. The metric is defined as: ass@k = Npass (2) We set the feedback limit to 1 and 2, reporting ass@1 (up to 1 feedback round) and ass@2 (up to 2 feedback rounds), respectively. Metric: Efficiency We denote ok as average tokens used per scenario and compute Attempt Efficiency (Eatt) and Token Efficiency (Etok) to normalize performance against resource costs where higher values indicate better resource optimization: Eatt = Savg Att , Etok = Savg ok (3)"
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we conduct systematic evaluation of various LLMs on AGENCYBENCH. We first outline experimental setup and metrics (Section 4.1), followed by analysis of overall performance across agentic capabilities (Section 4.2). We then investigate the crucial ability of self-correction through feedback (Section 4.3), analyze resource use (Section 4.4) and the economic efficiency of models (Section 4.5), and decode the distinct behavioral patterns in tool usage (Section 4.6). Finally, we examine the influence of different agentic scaffolds on model performance (Section 4.7). 4.1 Experimental Setup Models and Scaffold We evaluate comprehensive suite of models, comprising closed-source LLMs: GPT5.2 (OpenAI, 2025b), Claude-4.5-Opus, Claude-4.5-Sonnet (Anthropic, 2025a), Grok-4.1-Fast (xAI, 2025) and 6 4.2 Main Results Model Pass@1 Pass@2 Rise(%) Proprietary Models GPT-5.2 Claude-4.5-Sonnet Claude-4.5-Opus Gemini-3-Pro Grok-4.1-Fast 28.1 21.9 15.6 28.1 25. Open-Source Models GLM-4.6 Kimi-K2-Thinking DeepSeek-V3.2 Qwen-3-235B-A22B-Thinking 28.1 6.3 9.4 3.1 53.1 40.6 28.1 37.5 31.3 37.5 25.0 9.4 9.4 88.9 85.7 80.0 33.3 25. 33.3 300.0 0.0 199.7 Table 2: Impact of User Simulation Agent Feedback Attempts. While additional interactions significantly boost performance for certain models (e.g., GPT-5.2 and Kimi-K2-Thinking), the gains are less pronounced for others like DeepSeek-V3.2 and Grok-4.1-Fast. open-source LLMs: GLM-4.6 (GLM Team, 2025), DeepSeek-V3.2 (Liu et al., 2025), Qwen-3-235B-A22BThinking (Yang et al., 2025), Kimi-K2-Thinking (Team et al., 2025). All models are accessed via OpenRouter API with the temperature set to 0.7 to balance creativity and determinism. We utilize the agentic scaffold described in Section 3.1.1, which is equipped with robust set of tools (detailed in Table 6). 4.2 Main Results Overall Performance and Efficiency Table 1 highlights capability gap between proprietary and open-source models. GPT-5.2 achieves the highest overall average score (56.5%) among proprietary models, whereas GLM4.6 leads the open-source category with 38.6%. Conversely, Qwen-3-235B-A22B-Thinking records the lowest performance (27.0%), underscoring that substantial room for improvement. Regarding user simulation agent feedback attempts (Att), GPT-5.2 and Gemini-3-Pro demonstrate superior capability, requiring the fewest average attempts (1.46) to complete tasks. In the open-source sector, GLM-4.6 performs comparably to proprietary models (1.54), while Qwen-3-235B-A22B-Thinking and Kimi-K2-Thinking struggle with higher attempt counts (1.79), indicating weaker error-recovery abilities. Model Tok (M) (H) Turns Proprietary Models GPT-5.2 Claude-4.5-Sonnet Gemini-3-Pro Grok-4.1-Fast Claude-4.5-Opus 3.4 4.1 1.8 1.2 1.7 Open-Source Models GLM-4.6 Kimi-K2-Thinking DeepSeek-V3.2 Qwen-3-235B-A22B-Thinking 2.4 2.8 1.5 1.2 0.6 0.9 0.3 0.3 0.8 0.6 1.2 1.0 1.4 89.0 64.0 37.0 37.0 36.0 105.0 65.0 21.0 21.0 Table 3: Resource Usage Comparison. AGENCYBENCH tasks typically require 1 million tokens and time on the scale of hours to complete, highlighting their long horizon characteristic. Agentic Capabilities Performance varies significantly across agentic capabilities, revealing distinct specializations. Gemini-3-Pro dominates game (60.7%) and front-end (81.0%). GPT-5.2 excels in back-end and code, while Claude-4.5-Sonnet achieving the highest in research (71.4%). Among open-source models, GLM-4.6 exhibits https://openrouter.ai/ 7 4.3 Feedback-driven Self-correction Analysis Figure 5: Tool Invocation Patterns Across Models. Claude-4.5-Opus and GPT-5.2 shows preference for shell execution tools, while Gemini-3-Pro and Qwen-3-235B-A22B-Thinking favor file operation and memory management. Grok-4.1-Fast, GLM-4.6, and Deepseek-V3 series exhibit strong preference for web search tools. balanced performance, while Qwen-3-235B-A22B-Thinking demonstrates relative strength in research despite lower overall average. 4.3 Feedback-driven Self-correction Analysis Table 2 quantifies feedback-driven self-correction ability by Pass@1 and Pass@2. Top-tier proprietary models demonstrate sophisticated error handling. GPT-5.2 achieves an 88.9% relative increase, and the Claude series similarly exceeds 80% improvement. In contrast, while Gemini-3-Pro matches the initial Pass@1 of GPT-5.2, its ability to leverage feedback is markedly lower (33.3% Rise), suggesting that while its initial intuition is strong, its self-correction mechanisms are less responsive. In the open-source domain, Kimi-K2-Thinking and Qwen-3235B-A22B-Thinking achieve remarkable improvements after feedback (300% and nearly 200%, respectively). Conversely, DeepSeek-V3.2 achieves (0.0% Rise), persistently adhering to erroneous paths despite external critique. 4.4 Resource Consumption Analysis Table 3 details the trade-off between performance ceilings and costs: Tok represents the token consumption, measured in millions; represents the average scenario execution time, measured in hours; Turns represents the number of tool-calling rounds. GPT-5.2 acts as brute-force reasoner, consuming 3.4 million tokens and 89 turns on average to secure top scores. In contrast, Grok-4.1-Fast represents the pinnacle of speed and frugality (1.2M tokens, 0.3h). GLM-4.6 exhibits unique profile: despite high turns count (105), its resource usage remains moderate. Notably, Kimi-K2-Thinking and Qwen-3-235B-A22B-Thinking incur the highest time costs (1.2h), likely due to the latency of generating internal reasoning traces. 4.5 Attempt and Token Efficiency Analysis To decouple raw performance from expenditure, we analyze efficiency in Figure 4. Attempt Efficiency is dominated by GPT-5.2 (38.7%), indicating that its high resource usage is justified by high success rate per attempt. Token Efficiency favors Grok-4.1-Fast (37.2%), making it the most economically viable choice for resource-constrained environments. Claude-4.5-Sonnet ranks lowest (11.4%), indicating excessive token generation (4.1M) does not yield proportional performance gains. 4.6 Behavioral Patterns in Tool Invocation Figure 5 reveals model architectures imprint distinct personalities on problem-solving strategies: Claude-4.5-Opus and GPT-5.2 prefer system-level manipulation via shell execution (45.5% and 43.5%); Gemini-3-Pro distinctively utilizes explicit memory tools (6.9%), suggesting strategy rooted in managing long-horizon context banks; Qwen-3-235B-A22B-Thinking exhibits an extreme reliance on file operations (77.6%), prioritizing direct content 8 4.7 Impact of Agentic Scaffolds Model Scaffold Our Scaffold Claude-Agent-SDK OpenAI-Agents-SDK GPT-5.2 Claude-4.5-Opus Gemini-3-Pro Kimi-K2-Thinking Minimax-M2 GLM-4.6 Proprietary Models 53.53.9 71.3+20.5 45.90.4 Open-Source Models 44.65.9 54.4+8.6 44.2+10.6 57.4 50.8 46.3 50.5 45.8 33.6 58.7+1.3 47.13.7 46.9+0.6 37.712.8 43.02.8 36.0+2.4 Table 4: Impact of Agentic Scaffolds on Model Performance. We evaluate models across 10 representative scenarios using three distinct frameworks: our custom scaffold, the Claude-Agent SDK, and the OpenAI-AgentsSDK. The results highlight the sensitivity of model performance to the agentic scaffold, with distinct native ecosystem preferences observed for proprietary models. The rows for GPT-5.2 and Kimi-K2-Thinking are highlighted in blue as baselines. Colored subscripts indicate the performance gap compared to the baseline (red for improvement, blue for degradation). verification; Grok-4.1-Fast and GLM-4.6 exhibit high reliance on web search (9.5% and 8.6%), appearing to offload knowledge retrieval to external sources rather than relying on internal parametric memory. 4.7 Impact of Agentic Scaffolds To investigate the influence of agentic scaffold on model performance, we conducted an ablation study on subset of 10 representative scenarios using three distinct frameworks: our native scaffold (used in the main experiments), the Claude-Agent-SDK (Anthropic, 2025b), and the OpenAI-Agents-SDK (OpenAI, 2025c). We report the Average Score (SAvg) in Table 4. Ecosystem Synergy in Proprietary Models Experimental results reveal significant Ecosystem Synergy effect, where proprietary models demonstrate peak performance within their native frameworks. Most notably, Claude-4.5Opus achieves substantial performance boost of 20.5% when operating within the Claude-Code SDK compared to our generalist scaffold. This suggests deep optimization between the models training objective and its proprietary tool definitions/prompt structures. Similarly, GPT-5.2 shows preference for the OpenAI-Agents-SDK (+1.3%), outperforming its results on third-party alternatives. Scaffold Sensitivity in Open-Source Models Among open-source models, the impact of scaffold choice is heterogeneous. GLM-4.6 and Minimax-M2 exhibit strong compatibility with the Claude-Agent-SDK scaffold, improving by 10.6% and 8.6% respectively. This improvement may stem from the SDKs structured prompt engineering, which likely aligns well with the instruction-following capabilities of these models. Furthermore, it suggests the possibility that these models may have been specifically optimized during training to adapt to the interaction patterns characteristic of the Claude-Agent-SDK ecosystem. Conversely, Kimi-K2-Thinking performs best on our custom scaffold, experiencing significant degradation when migrated to external SDKs (dropping by 12.8% on the OpenAI-Agents-SDK). These findings underscore that agentic performance is not solely model-intrinsic property but result of the coupling between the model and its agentic scaffold."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we introduced AGENCYBENCH, comprehensive evaluation framework designed to rigorously assess the frontiers of autonomous agents in long-horizon, real-world contexts. By synthesizing 138 authentic tasks across 32 diverse scenariosrequiring an average of 1 million tokens and 90 tool callswe bridge the gap between existing toy benchmarks and the complexities of actual economic production. To ensure scalability and reproducibility, we developed unified automated evaluation pipeline leveraging user simulation agents and Docker-based remote sandboxes. Our extensive evaluation reveals that while proprietary models currently lead in complex reasoning and self-correction, the gap between closed-source and open-source models remains significant. Even the most advanced models struggle to fully master long-horizon autonomy without substantial resource consumption, highlighting the need for improved efficiency. Furthermore, our analysis of agentic scaffolds demonstrates that performance is highly sensitive to the interaction environment, with models often exhibiting native advantage within their proprietary ecosystems. AGENCYBENCH serves not only as leaderboard but as 9 5. Conclusion diagnostic tool. We hope this benchmark will drive future research towards more resource-efficient, self-correcting, and scaffold-agnostic agents capable of genuine real-world utility."
        },
        {
            "title": "Limitations",
            "content": "Model Selection Coverage While AGENCYBENCH evaluates diverse set of representative proprietary and open-source models, the landscape of Large Language Models evolves rapidly. Due to constraints on computational resources and budget, our evaluation cannot exhaustively cover every emerging model variant, intermediate checkpoint, or specialized fine-tune. Consequently, our findings reflect snapshot of the current state-of-the-art and may not capture the full performance spectrum of models released subsequent to our experiments. Domain Specificity Our benchmark focuses on high-complexity tasks within digital environments, such as game development, software engineering, web research, etc. While these scenarios encompass 32 real-world situations, they are confined to software-based agents operating within computer interface. The current framework does not extend to embodied agents or tasks requiring physical world interaction (e.g., robotics), leaving the evaluation of such multimodal physical agency for future research."
        },
        {
            "title": "Ethical Considerations",
            "content": "Human Subjects and Compensation The construction of AGENCYBENCH involved surveys and data validation by human experts, specifically computer science researchers and developers. We strictly adhere to the ACL Code of Ethics regarding human participant research. All contributors were fully informed of the projects scope, their data was anonymized to protect privacy, and they were compensated at rate significantly exceeding the local hourly minimum wage to ensure fair and ethical treatment. Safety and Usage Given that our benchmark involves agents generating executable code and performing shell operations, there are inherent risks associated with autonomous execution. To mitigate this, all evaluations are strictly confined within isolated Docker containers (Remote Sandbox) with controlled network access, preventing any potential harm to host systems. We will release this benchmark to foster the development of reliable and safe autonomous agents and explicitly oppose the application of these capabilities for malicious purposes, such as automated cyber-attacks. 10 References"
        },
        {
            "title": "References",
            "content": "[1] Pierre Andrews, Amine Benhalloum, Gerard Moreno-Torres Bertran, Matteo Bettini, Amar Budhiraja, Ricardo Silveira Cabral, Virginie Do, Romain Froger, Emilien Garreau, Jean-Baptiste Gaya, et al. 2025. Are: Scaling up agent environments and evaluations. arXiv preprint arXiv:2509.17158. [2] Anthropic. 2025a. claude-4.5. https://www.anthropic.com/news/claude-opus-4-5. [3] Anthropic. 2025b. claude-agent-sdk-python. Claude-agent-sdk-python. https://github.com/anthropics/ [4] Anthropic. 2025c. claude code. https://claude.com/product/claude-code. [5] Victor Barres, Honghua Dong, Soham Ray, Xujie Si, and Karthik Narasimhan. 2025. τ 2-bench: Evaluating conversational agents in dual-control environment. arXiv preprint arXiv:2506.07982. [6] Cursor. 2025. cursor-cli. https://cursor.com/cn/cli. [7] DeepMind. 2025. gemini-3. https://deepmind.google/models/gemini/pro/. [8] DesignArena Team. 2025. Designarena. https://www.designarena.ai/. [9] GLM Team. 2025. glm-4.6. https://x.ai/news/grok-4-1. [10] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2023. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770. [11] Thomas Kwa, Ben West, Joel Becker, Amy Deng, Katharyn Garcia, Max Hasin, Sami Jawhar, Megan Kinniment, Nate Rush, Sydney Von Arx, et al. 2025. Measuring ai ability to complete long tasks. arXiv preprint arXiv:2503.14499. [12] Junlong Li, Wenshuo Zhao, Jian Zhao, Weihao Zeng, Haoze Wu, Xiaochen Wang, Rui Ge, Yuxuan Cao, Yuzhen Huang, Wei Liu, et al. 2025a. The tool decathlon: Benchmarking language agents for diverse, realistic, and long-horizon task execution. arXiv preprint arXiv:2510.25726. [13] Keyu Li, Mohan Jiang, Dayuan Fu, Yunze Wu, Xiangkun Hu, Dequan Wang, and Pengfei Liu. 2025b. Datasetresearch: Benchmarking agent systems for demand-driven dataset discovery. arXiv preprint arXiv:2508.06960. [14] Aixin Liu, Aoxue Mei, Bangcai Lin, Bing Xue, Bingxuan Wang, Bingzheng Xu, Bochao Wu, Bowei Zhang, Chaofan Lin, Chen Dong, et al. 2025. Deepseek-v3. 2: Pushing the frontier of open large language models. arXiv preprint arXiv:2512.02556. [15] Haotian Luo, Huaisong Zhang, Xuelin Zhang, Haoyu Wang, Zeyu Qin, Wenjie Lu, Guozheng Ma, Haiying He, Yingsha Xie, Qiyang Zhou, et al. 2025. Ultrahorizon: Benchmarking agent capabilities in ultra long-horizon scenarios. arXiv preprint arXiv:2509.21766. [16] Samuel Miserendino, Michele Wang, Tejal Patwardhan, and Johannes Heidecke. Swe-lancer: Can frontier llms earn 1 million from real-world freelance software engineering?, 2025. URL https://arxiv. org/abs/2502.12115. [17] OpenAI. 2025a. codex. https://openai.com/zh-Hans-CN/codex/. [18] OpenAI. 2025b. gpt-5.2. https://cdn.openai.com/pdf/ 3a4153c8-c748-4b71-8e31-aecbde944f8d/oai_5_2_system-card.pdf. [19] OpenAI. 2025c. Openai-agents-sdk. https://github.com/openai/openai-agents-python. [20] OpenRouter. 2025. State of ai: An empirical 100 trillion token study with openrouter. https:// openrouter.ai/state-of-ai. [21] Melissa Pan, Negar Arabzadeh, Riccardo Cogo, Yuxuan Zhu, Alexander Xiong, Lakshya Agrawal, Huanzhi Mao, Emma Shen, Sid Pallerla, Liana Patel, et al. 2025. Measuring agents in production. arXiv preprint arXiv:2512.04123. [22] Tejal Patwardhan, Rachel Dias, Elizabeth Proehl, Grace Kim, Michele Wang, Olivia Watkins, Simon Posada Fishman, Marwan Aljubeh, Phoebe Thacker, Laurance Fauconnet, et al. 2025. Gdpval: Evaluating ai model performance on real-world economically valuable tasks. arXiv preprint arXiv:2510.04374. [23] Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. 2025. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534. [24] Terminal-bench Team. 2025. terminal-bench. https://www.tbench.ai/. 11 References [25] Xingyao Wang, Boxuan Li, Yufan Song, Frank Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et al. 2024. Openhands: An open platform for ai software developers as generalist agents. arXiv preprint arXiv:2407.16741. [26] Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. 2025. Browsecomp: simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516. [27] Yunze Wu, Dayuan Fu, Weiye Si, Zhen Huang, Mohan Jiang, Keyu Li, Shijie Xia, Jie Sun, Tianze Xu, Xiangkun Hu, et al. 2025a. Innovatorbench: Evaluating agents ability to conduct innovative llm research. arXiv preprint arXiv:2510.27598. [28] Zijian Wu, Xiangyan Liu, Xinyuan Zhang, Lingjun Chen, Fanqing Meng, Lingxiao Du, Yiran Zhao, Fanshi Zhang, Yaoqi Ye, Jiawei Wang, et al. 2025b. Mcpmark: benchmark for stress-testing realistic and comprehensive mcp use. arXiv preprint arXiv:2509.24002. [29] xAI. 2025. grok-4.1. https://x.ai/news/grok-4-1. [30] Yang Xiao, Mohan Jiang, Jie Sun, Keyu Li, Jifan Lin, Yumin Zhuang, Ji Zeng, Shijie Xia, Qishuo Hua, Xuefeng Li, et al. 2025a. Limi: Less is more for agency. arXiv preprint arXiv:2509.17567. [31] Yang Xiao, Jiashuo Wang, Qiancheng Xu, Changhe Song, Chunpu Xu, Yi Cheng, Wenjie Li, and Pengfei Liu. 2025b. Towards dynamic theory of mind: Evaluating llm adaptation to temporal evolution of human states. arXiv preprint arXiv:2505.17663. [32] Yang Xiao, Jiashuo Wang, Ruifeng Yuan, Chunpu Xu, Kaishuai Xu, Wenjie Li, and Pengfei Liu. 2025c. Limopro: Reasoning refinement for efficient and effective test-time scaling. arXiv preprint arXiv:2505.19187. [33] Yang Xiao, Chunpu Xu, Ruifeng Yuan, Jiashuo Wang, Wenjie Li, and Pengfei Liu. 2025d. Scale: Selective resource allocation for overcoming performance bottlenecks in mathematical test-time scaling. arXiv preprint arXiv:2512.00466. [34] Tianze Xu, Pengrui Lu, Lyumanshan Ye, Xiangkun Hu, and Pengfei Liu. 2025. Researcherbench: Evaluating deep ai research systems on the frontiers of scientific inquiry. arXiv preprint arXiv:2507.16280. [35] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. 2025. Qwen3 technical report. arXiv preprint arXiv:2505.09388. [36] John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. 2024. Swe-agent: Agent-computer interfaces enable automated software engineering. Advances in Neural Information Processing Systems, 37:5052850652. [37] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations. 12 A. Appendix"
        },
        {
            "title": "A Appendix",
            "content": "Scenarios Tasks Game Front-end Back-end Code Research MCP Total 10 3 3 9 5 2 32 50 15 15 29 19 138 Table 5: Distribution of scenarios and tasks across the six core agentic capabilities in AGENCYBENCH. In this appendix, we provide supplementary details to support the main findings of AGENCYBENCH. We first present the detailed statistics of the dataset composition. We then provide granular analysis of tool usage frequency across different models. Following the statistical data, we detail the specific prompts used for our Text-based Judge, Vision-based Judge, and User Simulation Agent. Finally, we provide concrete examples of scenario of game development to illustrate the multi-turn and long-horizon nature of the tasks. A.1 Dataset and Tool Statistics In this section, we provide comprehensive quantitative assessment of the AGENCYBENCH dataset composition and granular analysis of model-specific tool usage behaviors. These statistics not only validate the diversity of the benchmark but also reveal distinct cognitive styles across different LLMs. Dataset Composition and Domain Diversity The structural distribution of AGENCYBENCH, as detailed in Table 5, encompasses 32 distinct scenarios and 138 specific tasks. To ensure the benchmark evaluates broad spectrum of agentic capabilities, tasks are categorized into six core agentic capabilities. The Game Development domain constitutes the largest segment, accounting for approximately 36.2% of the total tasks (50 tasks across 10 scenarios). This heavy weighting reflects the unique complexity of game environments, which require agents to manage continuous state, simulate physics, and handle complex logic simultaneously. The Code domain follows with 29 tasks, focusing on algorithmic purity. Notably, the dataset explicitly balances full-stack development skills with an equal split between Front-end and Back-end tasks (15 tasks each). Furthermore, we include 10 tasks dedicated to the emerging Model Context Protocol (MCP), ensuring the benchmark remains relevant to cutting-edge agent interface standards. Tool Usage and Behavioral Fingerprints Perhaps the most revealing insights come from the tool usage frequency analysis presented in Table 6. We identified three distinct behavioral archetypes: 1. The Navigators vs. The Executors: There is striking divergence in how models orient themselves. GLM-4.6 exhibits unique navigator strategy, invoking list directory 158 timesnearly triple the average of other models. This indicates strong preference for gathering environmental context before taking action. Conversely, GPT-5.2 and Claude-4.5-Sonnet act as executors, prioritizing the run shell command tool (425 and 362 invocations, respectively) to empirically test code and run scripts, rather than passively observing the file structure. 2. Editing Styles: Surgeons vs. Rewriters: The data reveals fundamental difference in code modification philosophies. GPT-5.2 acts as surgeon, heavily utilizing the replace tool (146 invocations) to make precise, localized edits to existing files. In sharp contrast, GLM-4.6 overwhelmingly prefers the write file tool (381 invocations), suggesting tendency to overwrite entire files rather than attempting risky partial edits. While the rewrite strategy is safer, it is significantly less token-efficient. 3. Memory Utilization: Gemini-3-Pro stands out as the sole model to effectively leverage long-term memory capabilities. It is the only model to record significant usage of update memory bank (22 times) and initialize memory bank (7 times). While other models rely entirely on their context window, Gemini attempts to persist state and key information externally, behavior that theoretically scales better for longhorizon tasks. 4. Information Retrieval: For external knowledge acquisition, GLM-4.6 again shows distinct profile, using web fetch 96 times, whereas models like Claude-4.5-Opus and GPT-5.2 rely more on their internal knowledge or specific search queries (search file content). A.2 Evaluation Prompts Claude-4.5-O Claude-4.5-S Gemini-3 GPT-5.2 Grok-4.1 GLM-4.6 Deepseek-V3.2 Qwen3 Kimi-K2 agent tool get database name glob initialize memory bank list directory read file read many files read memory bank replace run shell command save memory search file content deep research hybrid search web fetch web search todo write update memory bank write file 0 0 3 0 53 55 16 0 13 191 0 0 0 1 0 7 0 0 81 0 0 0 0 47 106 0 0 29 362 0 1 0 4 0 6 0 0 169 0 0 0 7 54 86 24 1 42 140 2 0 0 0 0 5 0 22 0 0 19 0 45 147 13 0 146 425 0 37 0 0 0 12 15 0 117 4 0 4 1 42 67 1 0 1 141 0 0 1 7 6 25 0 1 109 0 0 1 0 158 142 0 0 15 371 0 0 0 1 96 4 0 0 381 0 0 4 0 25 41 1 0 14 86 0 4 1 9 0 0 0 0 41 0 10 0 2 8 6 0 0 2 40 0 0 0 0 0 1 0 5 185 0 0 2 0 82 147 0 0 49 301 0 20 5 3 7 6 0 0 Table 6: Frequency of Tool Invocations Across Different Models. Distinct behavioral patterns are observed, such as high shell usage by Claude/GPT and specific memory tool usage by Gemini. Claude-4.5-O denotes Claude-4.5-Opus; Claude-4.5-S denotes Claude-4.5-Sonnet; Gemini-3 denotes Gemini-3-Pro; Grok-4.1 denotes Grok-4.1-Fast; Qwen3 denotes Qwen3-235B-A22B-Thinking; Kimi-K2 denotes Kimi-K2-Thinking. A.2 Evaluation Prompts To ensure rigorous, reproducible, and automated evaluation, we designed specialized prompts for three distinct agentic roles: the Text-based Judge, the Vision-based Judge, and the User Simulation Agent. The prompts are structured to enforce strict adherence to evaluation rubrics and minimize subjective variance. The following boxes illustrate the finalized prompts used in our framework. Evaluation Roles & Prompts # Agent 1: Text-based Judge Role: You act as Senior Code Compliance Auditor and Quality Assurance Specialist. Your objective is to systematically evaluate software deliverables against strict set of functional and non-functional requirements. Input Data: {Code Files}: dictionary containing filenames and source code content. {Task ID}: unique identifier for the specific engineering task. {Rubrics}: list of constraints, functional requirements, and acceptance criteria. Evaluation Protocol: 1. Static Analysis: Examine the logical structure, syntax validity, and dependency management of the {Code Files}. 2. Requirement Mapping: Verify the implementation of every item listed in {Rubrics}. 3. Defect Identification: Detect logical errors, missing functionalities, or violations of best practices. Output Specification: Return single valid JSON object with the following keys: score (Integer, 0-10): 0-2 (Critical Failure): Code is non-functional or fails to address the core problem definition. 3-5 (Substantial Deficiency): Major features are missing; code executes but fails significant rubrics. 6-7 (Marginal Acceptance): Core functionality is operational, but edge cases are unhandled or minor constraints are ignored. 14 A.2 Evaluation Prompts 8-9 (High Compliance): Meets all functional requirements with high code quality; only trivial stylistic issues remain. 10 (Full Specification Alignment): Flawless execution adhering to all rubrics and robustness standards. confidence (Float, 0.0-1.0): Assessment of certainty based on the evidence available in the static code. comment (String): concise technical justification. You must explicitly reference specific files or lines of code when identifying failures. Example Output: { \"score\": 6, \"confidence\": 0.9, \"comment\": \"The implementation correctly handles...\" } # Agent 2: Vision-based Judge Role: You act as Visual Grounding and UI/UX Verification Specialist. Your objective is to validate system functionality and design fidelity based strictly on visual evidence. Input Data: {Visual Assets}: Screenshots or video frames capturing the system execution. {Task ID}: Identifier for the visual verification task. {Rubrics}: Visual, functional, and aesthetic criteria required for acceptance. Evaluation Protocol: 1. Visual Inspection: Analyze {Visual Assets} for UI elements, layout consistency, and text rendering. 2. Evidence Corroboration: Cross-reference visual features against {Rubrics}. 3. Strict Verification: Any feature not explicitly visible in the assets must be marked as Not Demonstrated. Output Specification: Return single valid JSON object with the following keys: score (Integer, 0-10): 0-2 (No Evidence): Visual assets are missing, irrelevant, or show broken system. 3-5 (Significant Deviation): UI loads but fails to demonstrate key interactions or diverges significantly from design specs. 6-7 (Partial Compliance): Primary elements are functional; secondary visual polish or specific UI states are missing. 8-9 (High Fidelity): Visually correct and functional; clear evidence exists for almost all rubrics. 10 (Pixel-Aligned Compliance): Visual output exactly matches all descriptions and requirements. confidence (Float, 0.0-1.0): Reflects the clarity and completeness of the visual evidence. comment (String): detailed justification. Explicitly state which rubrics were satisfied or failed based on visual artifacts. Example Output: { \"score\": 4, \"confidence\": 0.8, \"comment\": \"Layout matches the wireframe. However, ...\" } 15 A.3 Scenario Example # Agent 3: User Simulation Agent Role: You act as an Acceptance Testing Supervisor and Feedback Generator. Your objective is to provide actionable, structured feedback for iterative refinement when submission fails to meet the acceptance threshold. Input Data: {Evaluation Result}: The JSON output from the Judge (Score, Confidence, Comment). {Threshold}: The minimum passing score. {Rubrics}: The original requirement list. {Deliverables}: The submitted code or text. {Artifacts}: Visual outputs (if any). Task: Perform Gap Analysis between the {Deliverables} and the {Rubrics} based on the {Evaluation Result}. Isolate specific discrepancies that caused the score to fall below the {Threshold}. Output Specification: Generate structured textual report (not JSON) containing: 1. Status Declaration: State clearly that the submission is rejected due to the score. 2. Failure Diagnosis: deeply analyze the evaluation comments to list exactly which rubrics were not met. 3. Root Cause Analysis: Explain the failure from user requirement perspective (e.g., The requirement specified responsive layout, but the provided CSS uses fixed width). 4. Directives for Revision: Provide explicit instructions on what must be rectified in the next iteration. Tone: Constructive, objective, and directive. Example Output Pattern: Submission Rejected (Score: 5/10). The following critical rubrics were not met: 1. [Rubric Name]: The parser crashes on nested keys. 2. [Rubric Name]: The Submit button is visually missing. Action Required: Implement recursive parsing logic and ensure the footer component renders correctly. A.3 Scenario Example We present the complete definitions for scenario: developing Gomoku game. The example highlight the hierarchical structure of tasks, explicit deliverables, and rubrics. Example 1: Game Development (Gomoku) Task 1: Static Board Initialization Query: Create static Gomoku launch view that renders centered 1515 board with labelled axes (AO and 115), prominent #start-btn labelled Start Match, and legend explaining black and white stones. Automatically call window.app.initializeBoard() on load so the markup appears without manual interaction. Deliverables: index.html that links styles.css and app.js via relative paths and contains containers #board, #legend, and #start-btn.- styles.css defining 640px square grid, positioning the legend beneath the board, and styling the start button.- app.js exposing window.app with methods initializeBoard() and describeLayout(); the latter returns an object with keys startButton, legend, and cells, where cells maps coordinates like H8 to x: number, y: number viewport positions. Rubric: Visual grid: automation captures layout.png and confirms the board is exactly 6404px wide and tall, centered within 6px both horizontally and vertically, and decorated with 1515 intersection markers.- Controls placement: describeLayout() must report startButton roughly at x: 966, y: 966 relative to the viewport and ensure the legend top edge sits within 806px of the board bottom.- DOM contract: the page must render 225 elements with class .intersection each carrying data-cell=letternumber, alongside elements #board, #legend, and #start-btn.- API bootstrapping: invoking window.app.initializeBoard() twice must be idempotent, leaving exactly one board instance and 16 A.3 Scenario Example populated description payload. Task 2: Interactive Game Logic Query: Extend the board so players alternate black and white stones when clicking intersections, starting with black. Display status line inside #status-bar and highlight the latest stone with pulsing ring. Keep all assets from task1. Deliverables: Continue shipping index.html, styles.css, and app.js, updating the markup to include #status-bar beneath the legend.- Extend window.app with debugState() (returning move history and the current player) and setDemoMoves(sequence) where sequence is an array of coord: H8, color: black objects for automation to preload moves. Rubric: Turn order: automation clicks the coordinates reported by describeLayout() corresponding to H8, H9, and then attempts H9 again. debugState().moves must record alternating colors for the first two moves and refuse the third with an unchanged move list.- Last move halo: after the second valid move the element #last-move-indicator must surround the latest stone with 263px animated ring captured in moves turns.webm.- Status updates: #status-bar text must read Black to move at load, switch to White to move after the first click, then revert to Black to move when the duplicated coordinate is rejected.- Layout regression: the screenshot and describeLayout() metrics from task1 must remain within the same tolerances. Task 3: Victory & Replay System Query: Introduce automatic victory detection, banner announcing the winner, undo/redo controls, and replay feature that animates the finished game. Preserve the interactive flow from task2. Deliverables: Maintain the three primary files and append session.log file storing comma-separated history (timestamp,player,coord).- Add toolbar #controls containing buttons button[data-action=undo], button[data-action=redo], and button[data-action=replay] displayed in that order beneath the status bar.- Extend window.app with methods undo(), redo(), startReplay(), checkWinner(), and exportLog() returning the log contents. Rubric: Win scenario: automation plays the sequence H8, H9, I8, I9, J8, J9, K8, K9, L8 (black begins). After the final move checkWinner() must return Black, #winner-banner must display Black wins, and further manual clicks must be disabled. Replay controls: activating the toolbar buttons (undo redo replay) must modify the board accordingly while replay.webm shows stones animating in chronological order at more than 300ms per move. During replay manual clicks remain blocked until completion.- Logging: session.log must append one line per move plus replay markers (if any). exportLog() must match the file content exactly.- Continuity: turn enforcement, last move highlighting, and layout expectations from tasks 12 must still hold. Task 4: Persistence Layer Query: Add persistence drawer that lets reviewers save and restore finished games without losing the log or replay tools. Introduce panel #persistence beneath the controls containing buttons #save-game, #load-game, and #reset-match plus read-only textarea id=state-json. Saving writes the current match state to the textarea and localStorage[gomoku-state]. Loading parses the textarea and rehydrates board, log, and indicators. Reset clears the board but retains totals for wins/draws shown inside #scoreboard with counters .black-wins, .white-wins, .draws. Deliverables: Continue shipping the three primary files. The new panel must sit below #controls and adopt responsive styling that matches the existing layout.- Extend window.app with serializeState(), applyState(serialized), resetMatch(), and getScoreboard() returning black: number, white: number, draws: number . Persist the latest save so reloading the page reenacts the saved board automatically. Rubric: Save flow: after several moves, #save-game updates #state-json with JSON containing moves array and writes the same payload to localStorage[gomoku-state]. serializeState() must return the identical data.- Restore flow: invoking resetMatch() empties the board while keeping scoreboard totals. Calling applyState() with the previously saved JSON must rebuild stones, last-move halo, and debugState() history exactly.- Scoreboard: when checkWinner() declares winner followed by resetMatch(), getScoreboard() increments the winner count without altering the opponent tally. Visual labels in #scoreboard must reflect the same totals.- Regression: undo/redo/replay, logging, and layout tolerances from tasks 13 remain satisfied. 17 A.3 Scenario Example Task 5: Diagnostics & Stress Testing Query: Layer diagnostics sidebar and scripted scenarios to stress-test persistence features. Add toggle button #toggle-diagnostics that slides right-aligned aside id=diagnostics into view displaying current depth, total moves, elapsed milliseconds, and table with headers Metric and Value. Load scenario data from hard cases.json containing at least two entries with id, label, moves, and winner fields. Provide playback controls within the sidebar to preview scenarios and inspect metrics. Deliverables: Keep existing files and supply hard cases.json. Sidebar must reveal aggregated stats for the active game and scenario preview (e.g., longest line, capture streaks) in structured list.- Extend window.app with loadScenario(id), playScenario(id, options), getDiagnostics(), summarizeScenario(id), and estimateHeap(). playScenario should return Promise resolving after the animation completes while blocking manual clicks. Rubric: Scenario import: calling loadScenario(0) must position stones per the fixture, update the banner using winner, and log the moves. summarizeScenario(0) returns an object containing total stones and the longest contiguous run for each color.- Playback: invoking playScenario(1, { intervalMs: 220 }) replays the scenario in order with more than 200ms spacing, reusing undo/redo state guards and re-enabling manual clicks afterward.- Diagnostics: after either scenario, getDiagnostics() supplies keys depth, elapsedMs, nodes, and topLines (array). While the sidebar is visible, the table lists these values and stays 2606px wide at 606px from the right edge, captured in diagnostics.png.- Stability: calling estimateHeap() five times in succession yields non-decreasing integers less than 64000000. All expectations from tasks 14 continue to hold."
        }
    ],
    "affiliations": [
        "GAIR",
        "PolyU",
        "SII",
        "SJTU"
    ]
}