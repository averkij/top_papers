{
    "paper_title": "A Review of 3D Object Detection with Vision-Language Models",
    "authors": [
        "Ranjan Sapkota",
        "Konstantinos I Roumeliotis",
        "Rahul Harsha Cheppally",
        "Marco Flores Calero",
        "Manoj Karkee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This review provides a systematic analysis of comprehensive survey of 3D object detection with vision-language models(VLMs) , a rapidly advancing area at the intersection of 3D vision and multimodal AI. By examining over 100 research papers, we provide the first systematic analysis dedicated to 3D object detection with vision-language models. We begin by outlining the unique challenges of 3D object detection with vision-language models, emphasizing differences from 2D detection in spatial reasoning and data complexity. Traditional approaches using point clouds and voxel grids are compared to modern vision-language frameworks like CLIP and 3D LLMs, which enable open-vocabulary detection and zero-shot generalization. We review key architectures, pretraining strategies, and prompt engineering methods that align textual and 3D features for effective 3D object detection with vision-language models. Visualization examples and evaluation benchmarks are discussed to illustrate performance and behavior. Finally, we highlight current challenges, such as limited 3D-language datasets and computational demands, and propose future research directions to advance 3D object detection with vision-language models. >Object Detection, Vision-Language Models, Agents, VLMs, LLMs, AI"
        },
        {
            "title": "Start",
            "content": "A Review of 3D Object Detection with Vision-Language Models RANJAN SAPKOTA, Cornell University, USA KONSTANTINOS I. ROUMELIOTIS, University of Peloponnese, Greece RAHUL HARSHA CHEPPALLY, Kansas State University, USA MARCO FLORES CALERO, Universidad de las Fuerzas Armadas, Ecuador MANOJ KARKEE, Cornell University, USA 5 2 0 2 5 2 ] . [ 1 8 3 7 8 1 . 4 0 5 2 : r This paper presents groundbreaking and comprehensive review, the first of its kind, focused on 3D object detection with Vision-Language Models (VLMs), rapidly advancing frontier in multimodal AI. Using hybrid search strategy combining academic databases and AI-powered engines, we curated and analyzed over 100 state-of-the-art papers. Our study begins by contextualizing 3D object detection within traditional pipelines, examining methods like PointNet++, PV-RCNN, and VoteNet that utilize point clouds and voxel grids for geometric inference. We then trace the shift toward VLM-driven systems, where models such as CLIP, PaLM-E, and RoboFlamingo-Plus enhance spatial understanding through language-guided reasoning, zero-shot generalization, and instruction-based interaction. We investigate the architectural foundations enabling this transition, including pretraining techniques, spatial alignment modules, and cross-modal fusion strategies. Visualizations and benchmark comparisons reveal VLMs unique capabilities in semantic abstraction and open-vocabulary detection, despite trade-offs in speed and annotation cost. Our comparative synthesis highlights key challenges such as spatial misalignment, occlusion sensitivity, and limited real-time viability, alongside emerging solutions like 3D scene graphs, synthetic captioning, and multimodal reinforcement learning. This review not only consolidates the technical landscape of VLM-based 3D detection but also provides forward-looking roadmap, identifying promising innovations and deployment opportunities. It serves as foundational reference for researchers seeking to harness the power of language-guided 3D perception in robotics, AR, and embodied AI. project associated with this review and evaluation has been created at Github Link: https://github.com/r4hul77/Awesome-3DDetection-Based-on-VLMs Additional Key Words and Phrases: 3D Object Detection, Vision-Language Models, VLMs, Object Detection with VLMs, 3D Object Detection with VLMs, Large Language Models, Object Detection with Large Language Models, Vision-Language, VLM, Artificial Intelligence ACM Reference Format: Ranjan Sapkota, Konstantinos I. Roumeliotis, Rahul Harsha Cheppally, Marco Flores Calero, and Manoj Karkee. 2025. Review of 3D Object Detection with First and corresponding author. Corresponding author. Authors Contact Information: Ranjan Sapkota, Cornell University, Ithaca, USA, rs2672@cornell.edu; Konstantinos I. Roumeliotis, University of Peloponnese, Tripoli, Greece; Rahul Harsha Cheppally, Kansas State University, Manhattan, USA; Marco Flores Calero, Universidad de las Fuerzas Armadas, Sangolquı, Ecuador; Manoj Karkee, Cornell University, Ithaca, USA, mk2684@cornell.edu. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM 1557-7368/2025/8-ART111 https://doi.org/10.1145/nnnnnnn.nnnnnnn Vision-Language Models. ACM Trans. Graph. 37, 4, Article 111 (August 2025), 23 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn Fig. 1. An illustration of VLM-based 3D detection on an apple, showing key advantages including semantic-awareness, zero-shot learning, multimodal fusion, human-aligned queries, and scalable data utilization. ACM Trans. Graph., Vol. 37, No. 4, Article 111. Publication date: August 2025. 111:2 Sapkota et al. 2025, Review of 3D Object Detection with VLMs"
        },
        {
            "title": "1\nFigure 1 provides a visual example of how object detection has\nevolved from traditional 2D methods to more advanced 3D detection\ntechniques. The image shows an apple detected in 3D space using\na bounding box that captures not just the object’s position on the\nX and Y axes, but also its depth (Z-axis). This highlights the core\nbenefit of 3D detection: the ability to perceive and localize objects\nwithin a spatial environment more accurately than 2D approaches,\nwhich are limited to flat image coordinates. As shown in the figure,\n2D bounding boxes cannot fully represent an object’s volume, size,\nor its relation to nearby objects, important factors for tasks such as\nrobotic manipulation or autonomous navigation.",
            "content": "Object detection itself is foundational task in computer vision, used in wide range of applications including autonomous vehicles, robotics, surveillance, and augmented reality [Ghasemi et al. 2022]. The primary goal is to identify and localize meaningful objects within images or sensory data by drawing class-labeled boxes around them [Zou et al. 2023]. Traditional object detection systems have relied heavily on 2D images, with early models using handcrafted features [Agarwal and Roth 2002; Papageorgiou et al. 1998]. Modern breakthroughs in deep learning led to the development of powerful real-time detectors like YOLO [Redmon et al. 2016], SSD [Liu et al. 2016], and Faster R-CNN [Ren et al. 2016], which achieve high accuracy even under challenging conditions [Sapkota and Karkee 2024; Sapkota et al. 2024c]. However, without depth information, these 2D systems struggle with tasks that require spatial reasoning or object interaction in 3D world. To overcome these spatial limitations, the field has seen growing shift toward 3D object detection methods that operate on volumetric data such as LiDAR point clouds, depth maps, and RGB-D inputs [Wang et al. 2021]. These models provide fuller representation of object geometry and spatial relationships, often utilizing voxelization, point-based representations, or 3D convolutions [Arnold et al. 2019; Caglayan and Can 2018]. However, while these traditional 3D deep learning models offer significant improvements in spatial reasoning, they come with their own set of constraints [Qian et al. 2022; Wang et al. 2022c]. They are annotation-heavy [Meng et al. 2021; Xiang et al. 2014], requiring large-scale 3D datasets with detailed manual labels that are costly and labor-intensive to create [Brazil et al. 2023; Sølund et al. 2016; Tremblay et al. 2018]. Moreover, they frequently suffer from poor generalization across domains [Eskandar 2024], lack semantic flexibility [Zhang et al. 2021], and exhibit rigid retraining requirements [Mao et al. 2023; Peng et al. 2015] to adapt to new object categories or deployment settings. Further complicating their deployment, traditional models are often sensor-dependent, relying on carefully calibrated multimodal systems such as LiDAR-camera fusion setups [Alaba and Ball 2022]. This makes them brittle in uncontrolled or resource-constrained environments. Additionally, these systems are limited in their interpretability and flexibility [Wang et al. 2022a], lacking the capability to incorporate high-level, task-oriented instructions [Chen et al. 2023a]. For example, they cannot interpret human commands like detect only ripe apples within arms reach, nor adapt in real-time to shifting ACM Trans. Graph., Vol. 37, No. 4, Article 111. Publication date: August 2025. task goals or user intents. The advent of VLMs offers transformative solution to these challenges. By combining visual perception with natural language understanding, VLMs inject new level of semantic reasoning into 3D object detection. As demonstrated again in Figure 1, our illustrated example features realistic apple being detected in 3D space, annotated not only with spatial precision but also contextualized through semantic understanding. The figure also highlights the dual sets of benefits: five key advantages of direct 3D detection over 2D methods, and five unique benefits offered by VLMbased 3D detection systems over traditional CNN-based 3D detectors. Direct 3D detection methods outperform their 2D counterparts by offering enhanced spatial accuracy, depth-aware localization, volumetric context, occlusion handling, and metric-scale reasoning. These capabilities are crucial for real-world robotics applications, where full understanding of object placement and interaction is necessary. At the same time, VLM-based approaches further elevate this by enabling prompt-based control [Tang et al. 2025b], zero-shot generalization [Zhang et al. 2024c], semantic awareness [Zhang et al. 2024f], multimodal integration [Zang et al. 2025], and interpretability [Raza et al. 2025; Yellinek et al. 2025]. Unlike CNN-based systems, VLMs can process natural language queries and integrate high-level reasoning into the detection process [Chen et al. 2024d; Fu et al. 2025], allowing for tasks like find the nearest apple suitable for picking without needing specific training for each new task. Despite the advances in traditional 3D object detection methods, often based on point cloud processing with convolutional neural networks (CNNs), these approaches still face critical limitations in terms of semantic understanding, data efficiency, and adaptability. As highlighted earlier, CNN-based 3D detectors often require expensive sensor calibration, large annotated datasets, and rigid retraining procedures to adapt to new environments. More importantly, they lack the semantic interpretability needed for complex reasoning tasks, such as identifying specific object attributes or responding to user-defined prompts in natural language. In recent years, VLMs have emerged as transformative solution to the inherent limitations of traditional 2D and geometry-only 3D object detection methods. VLMs integrate the visual pattern recognition strengths of computer vision with the semantic reasoning capabilities of LLMs, allowing for richer, multimodal understanding of scenes [Ma et al. 2024]. As illustrated in Figure 2, the public launch of ChatGPT on November 30, 2022, sparked an unprecedented surge in interest in LLMs, rapidly establishing them as foundational tools in natural language understanding. While the rise of VLMs has been more gradual, the influence of LLMs has catalyzed increased attention toward multimodal systems. This growing interest is especially evident in domains requiring high-level reasoning and contextual comprehension, such as 3D object detection in robotics, augmented reality, and autonomous navigation. Unlike conventional 3D detectors that rely heavily on point cloud data and annotated datasets, VLMs enable flexible querying, few-shot or zero-shot learning, and semantic task generalizationall without the need for retraining on each new task. This paradigm shift is exemplified in several recent works. For instance, Agent3D [Zhang et al. 2024c] uses VLMs to perform openvocabulary 3D detection based on user-defined queries, enabling robots to locate objects like \"the red cup behind the chair\" in complex Review of 3D Object Detection with Vision-Language Models 111: Fig. 2. Temporal trend analysis highlighting the surge in global attention toward LLMs and VLMs. Following the public launch of ChatGPT on November 30, 2022, there has been marked increase in interest and adoption of VLMs, particularly in domains such as 3D object detection, reflecting shift toward multimodal and prompt-driven AI systems. indoor scenes. Similarly, SpatialVLM [Chen et al. 2024d] introduces spatially-grounded vision-language framework that reasons over RGB-D inputs to detect and describe objects in 3D with spatial context. These models not only identify objects but also reason about spatial relationships, affordances, and human-centric goals. This elevates 3D perception from purely geometric task to cognitively informed one, where the model can understand instructions like find the ripe apple near the bottom-left of the tree. While VLM trends have not spiked with the same intensity as LLMs, they show steady and promising upward trajectory, marking the early stages of broader shift toward truly intelligent, semantically aware 3D systems. As the capabilities of multimodal models expand, VLMs are poised to play central role in bridging perception and cognition across AI-driven applications. Objective of the Review. The objective of this review is to systematically investigate the evolving landscape of 3D object detection using VLMs and to assess their growing role in bridging spatial perception with semantic understanding. We begin by examining the foundational concepts of 3D object detection, its evolution from geometry-based methods to multimodal frameworks, and the distinction from conventional 2D detection. Next, we explore traditional 3D detection architecturesincluding PointNet++ [Sheshappanavar and Kambhamettu 2020], VoxelNet [Chen et al. 2023b; Sindagi et al. 2019], and PV-RCNN [Shi et al. 2020] to establish baseline for comparison. Building upon this, we review state-ofthe-art VLM-based approaches, highlighting their open-vocabulary capabilities, semantic grounding, and cross-modal alignment. We delve into the underlying architectures, pretraining and fine-tuning strategies, and the visualization of detection outputs to understand how VLMs perceive and reason in 3D space. This review further compares traditional and VLM-based paradigms, identifying the strengths, limitations, and trade-offs of each. Crucially, we analyze current challenges in data availability, grounding accuracy, and computational scalability, while proposing potential solutions such as multimodal dataset expansion and hybrid model integration. By reviewing over 100 papers, this study aims to provide comprehensive roadmap for researchers, offering insights into the present capabilities and future directions of 3D object detection empowered by VLMs."
        },
        {
            "title": "2.1 Search Strategy\nTo conduct a comprehensive and systematic review of the 3D ob-\nject detection landscape, particularly at the intersection with VLMs,\nwe adopted a novel dual-strategy approach that integrates both\nacademic databases and AI-powered search engines—illustrated\nin Figure 4. This marks the first effort in the domain to leverage\nthis hybrid methodology for collecting state-of-the-art literature.\nWe began by querying twelve prominent platforms, including aca-\ndemic databases such as Google Scholar, IEEE Xplore, Scopus, arXiv,\nScienceDirect, PubMed, and Web of Science, along with advanced\nAI-based engines like Hugging Face, ChatGPT, DeepSeek, Grok,\nand Perplexity. Our core search terms included combinations and\nBoolean expressions such as “3D Object Detection,” “Vision Lan-\nguage Models,” “Large Vision Language Models,” “VLMs,” “LLMs,”\nand “Robotics.” Key queries such as “3D Object Detection + VLMs +\nRobotics” were designed to capture literature that spans traditional\ndeep learning paradigms and their extension into multimodal per-\nception systems. The initial search yielded 459 papers. In the first\nfiltering stage, papers were assessed for relevance based on titles\nand abstracts, narrowing the pool to 208. A second round of filter-\ning, focused on methodological soundness and clarity of application,\nreduced the count to 134. Finally, a rigorous evaluation of novelty,\ncompleteness, and technical relevance led to a curated selection of\n105 papers. Among these, 43 focused on traditional neural network-\nbased 3D object detection, while 62 addressed approaches using\nVision-Language Models. This refined dataset formed the basis for\nour structured analysis and review.",
            "content": "ACM Trans. Graph., Vol. 37, No. 4, Article 111. Publication date: August 2025. 111:4 Sapkota et al. 2025, Review of 3D Object Detection with VLMs Fig. 3. Conceptual overview of the methodological and analytical framework used in this review. The mindmap illustrates the studys structure: starting with comparison of 3D detection methods, followed by VLM-specific architectures, strengths and trade-offs, and ending with discussion on challenges and future directions. Fig. 4. Methodology diagram of this review paper illustrating the hybrid academic and AI-based search strategy, filtering process, and final paper selection, reducing 459 initial papers to 105."
        },
        {
            "title": "2.2 Literature Evaluation and Inclusion Criteria\nEach selected paper was reviewed for the following criteria: (1) clear\nmethodological contribution to 3D object detection, (2) empirical\nvalidation or real-world demonstration, (3) architectural or compu-\ntational novelty, and (4) relevance to robotics, automation, or agri-\ncultural contexts. Specific attention was paid to the subset of papers\nthat used VLMs or multimodal transformers to enhance 3D spatial",
            "content": "understanding. We also included several foundational works from 20172021 (e.g., PointNet++, VoteNet, SECOND, PV-RCNN, 3DSSD) as the comparative baseline for traditional neural network methods. This helped ground our discussion of the **seven limitations** seen in conventional 3D detection systemsAnnotation-Heavy, Poor Generalization, No Semantics, Rigid Training, Sensor Dependency, No Prompting, and Limited Flexibilityrelative to more recent VLM-powered approaches. ACM Trans. Graph., Vol. 37, No. 4, Article 111. Publication date: August 2025."
        },
        {
            "title": "2.3 Synthesis and Comparative Analysis\nThis review systematically synthesizes and compares the current\nstate of 3D object detection through the lens of VLMs, follow-\ning a structured flow from literature selection to architectural and\napplication-based evaluations. The analysis begins with the identi-\nfication of relevant studies using a hybrid academic and AI-based\nsearch strategy, as detailed in the methodology. Based on inclusion\ncriteria focused on technical clarity, novelty, and domain relevance,\n105 high-quality papers were selected and analyzed. Our results are\norganized to show the evolution from traditional deep learning ap-\nproaches to modern VLM-based systems. Traditional methods—such\nas PointNet++, VoteNet, and PV-RCNN—primarily rely on geometric\nfeatures derived from point clouds or voxelized data. These methods\noffer high accuracy in structured environments but face limitations\nin semantic understanding and generalization.",
            "content": "In contrast, VLM-based approaches (e.g., DetGPT-3D, ContextDET, and task-conditioned LVLMs) integrate textual prompts with visualspatial inputs, enabling open-vocabulary detection, instruction following, and zero-shot generalization. These systems show particular promise in robotics applications requiring nuanced reasoning, such as navigation, grasp planning, and task-specific manipulation, especially in unstructured settings. The architecture-focused section highlights foundational components of VLMs for 3D detection, including multimodal fusion layers, pretraining strategies, and finetuning mechanisms. Visualization techniques are also discussed to explain how VLMs ground language into spatial contexts. Our comparative analysis evaluates both traditional and VLM-based systems across key criteriaannotation dependence, interpretability, data efficiency, and computational cost. While VLMs excel in semantic grounding and flexibility, they introduce new challenges such as cross-modal alignment, increased computational demands, and sensitivity to noisy inputs. The discussion section outlines these limitations and explores potential solutions such as lightweight VLM architectures, better multimodal grounding techniques, and more robust pretraining datasets."
        },
        {
            "title": "3 Results and Analysis\n3.1 Evolution of 3D Object Detection with VLMs",
            "content": "3D Object Detection with Traditional Methods: 3D object de3.1.1 tection emerged as significant area of research in computer vision with the increasing availability of depth sensors and LiDAR technology in the early 2010s [Taylor and Kleeman 2008; Zhang 2010]. Initially driven by applications in robotics and autonomous vehicles, the field gained traction with the introduction of datasets like KITTI in 2012 [Geiger et al. 2013], which provided annotated LiDAR and camera data for benchmarking 3D perception tasks [Yebes et al. 2015]. Early methods relied heavily on hand-crafted features and geometric reasoning to infer object locations and shapes in threedimensional space [Jin et al. 2014]. However, the field witnessed surge in popularity and progress with the rise of deep learning around 2015, particularly convolutional neural networks (CNNs) adapted for processing point clouds and voxelized data [Oshea and Nash 2015]. Architectures like VoxelNet (2017) [Zhou and Tuzel 2018] and PointNet/PointNet++ (2017) [Qi et al. 2017] marked turning points by directly learning from raw 3D data without requiring Review of 3D Object Detection with Vision-Language Models 111:5 complex preprocessing. These innovations enabled more accurate and scalable 3D detection systems, fueling research in autonomous driving, augmented reality, and robotic manipulation. The growing demand for spatial awareness and environmental understanding in real-world scenarios continues to push the boundaries of 3D object detection, making it critical component in the advancement of intelligent systems and multimodal AI. Its popularity in academic and industrial research remains strong, with continued development of benchmarks, datasets, and novel architectures. All major models and their processing strategies across with neural networks are presented in Table 1. Table 1. Timeline and classification of Traditional Methods 3D object detection (with convolution neural networks) Model (Release Date) & Reference VoxelNet [Zhou and Tuzel 2018] PointNet [Qi et al. 2017] MV3D [Chen et al. 2017] SECOND [Yan et al. 2018] Frustum PointNet [Qi et al. 2018] MV3D [Chen et al. 2017] AVOD [Ku et al. 2018] PointFusion [Xu et al. 2018] MVX-Net [Sindagi et al. 2019] PointPainting [Vora et al. 2020] 3D-CVF [Yoo et al. 2020] EPNet [Huang et al. 2020] LiDAR-RCNN [Li et al. 2021b] DepthFusionNet [Shivakumar et al. 2019] PointAugment [Li et al. 2020] Stereo3DNet [Chen et al. 2020] SVGA Net VoxelGraph [He et al. 2022] FusionPainting [Xu et al. 2021] FusionTransformer citefent2024dpft CrossModal3D [Jaritz et al. 2022] SparseVoxelNet [Chen et al. 2023b] DepthSparse [Wu et al. 2022] DeepFusion [Li et al. 2022b] TransFusion [Bai et al. 2022] UVTR [Li et al. 2022a] Deep Interaction [Yang et al. 2022] LiDARFormer [Zhou et al. 2024] CMT [Yan et al. 2023] BEVFusion [Zhao et al. 2024a] GA-Fusion [Li et al. 2024b](Aug 2023) IS-Fusion [Yin et al. 2024] Uni3D [Zhang et al. 2023] VCD [Huang et al. 2023a] Vista [Deng et al. 2022] VoxelFusionNet [Song et al. 2023] PointDiffusion [Chen et al. 2024b] BEVDet++ [Huang et al. 2021] FusionFormer [Hu et al. 2023] 3DiffTection [Xu et al. 2024] OVM3D-Det [Huang et al. 2024b] 3D-SparseCNN [Shi et al. 2022] MonoDepth3D [Wang et al. 2022b] Panoptic3DNet [Lee et al. 2024] HyperVoxel [Noh et al. 2021] SparseVoxFormer [Son et al. 2025] Omni3D+ [Brazil et al. 2023] PillarFocusNet [Gao et al. 2025b] UniDet3D [Kolodiazhnyi et al. 2025] MonoTAKD [Liu et al. 2024] Type Non-Attention Non-Attention Non-Attention Non-Attention Non-Attention Non-Attention Non-Attention Non-Attention Attention-Based Attention-Based Attention-Based Attention-Based Non-Attention Non-Attention Attention-Based Attention-Based Non-Attention Attention-Based Attention-Based Attention-Based Attention-Based Attention-Based Attention-Based Attention-Based Attention-Based Attention-Based Attention-Based Attention-Based Attention-Based Attention-Based Attention-Based Attention-Based Attention-Based Attention-Based Attention-Based Diffusion-Based Attention-Based Attention-Based Diffusion-Based Attention-Based Attention-Based Attention-Based Attention-Based Attention-Based Attention-Based Attention-Based Attention-Based Attention-Based Attention-Based Processing Method Voxel-wise Point-wise ROI-wise (Multi-view Fusion) Voxel-wise Point-wise (RGB-D Frustum Proposal) ROI-wise ROI-wise Point-wise Voxel-wise Point-wise Point-wise Point-wise ROI-wise (LiDAR-Camera Fusion) Voxel-wise (Depth-Guided Fusion) Point-wise (Data Augmentation) Monocular (Stereo Vision) Voxel-wise (Graph Neural Networks) Voxel-wise Hybrid (LiDAR + Camera + Radar) Multi-Modal Distillation Sparse Voxel-wise Monocular (Depth-Aware Sparsity) Voxel-wise ROI-wise Voxel-wise Point-wise Voxel-wise + BEV Fusion Voxel-wise ROI-wise Point-wise Voxel-wise Multi-dataset Multi-modal Distillation cross-view spatial attention Voxel-wise + Multi-modal Point-wise (LiDAR Denoising) BEV Grids + ROI-wise Hybrid (Voxel + Point) Single-image Monocular (Pseudo-LiDAR) Sparse Voxel-wise Monocular (Self-Supervised) Voxel-wise + Panoptic Segmentation Hypernetwork-Driven Voxel-wise Voxel-wise Benchmark-driven Pillar-based Voxel-wise Monocular (KD) Although neural network-based processing methods such as VoxelNet and PointNet have shown promising results, they still face several inherent limitations. These limitations are outlined in the following points: Limitations in Early Fusion and ROI-Based Models: Early 3D object detection models such as MV3D (2017) and AVOD (2019) adopted ROI-wise processing strategies based ACM Trans. Graph., Vol. 37, No. 4, Article 111. Publication date: August 2025. 111:6 Sapkota et al. 2025, Review of 3D Object Detection with VLMs on hand-engineered features and multi-view fusion [Chen et al. 2017; Yuan et al. 2023]. primary challenge for these models was inefficient spatial reasoning, as the 2D region proposals often failed to align accurately with 3D geometry, especially in occluded or cluttered scenes [Chen and Gupta 2017]. Their performance heavily depended on camera calibration and multi-view consistency, which degraded in dynamic or sensor-imprecise environments. PointFusion (2019) aimed to mitigate some of these issues using point-wise fusion of RGB and depth features, yet it struggled with misalignment in feature spaces and lacked unified mechanism for robust cross-modal attention [Xu et al. 2018]. Likewise, LiDAR-RCNN (2020) introduced camera-LiDAR fusion but still relied on handcrafted heuristics and lacked contextual integration, resulting in limited generalization to novel scenes or weather conditions [Li et al. 2021b]. Furthermore, all these models required rigid sensor setups and suffered from heavy computation when scaling to larger scenes or datasets. In general, these models were constrained by low adaptability, poor cross-modal coordination, and dependence on external proposal generation modules, which limited their scalability and real-time deployment in dynamic environments like autonomous driving. Voxelization Bottlenecks and Sparsity : Voxel-based approaches such as VoxelNet (2017) [Sindagi et al. 2019] and SECOND (2018) [Yan et al. 2018]represented significant shift by enabling 3D CNNs to directly learn from LiDAR data. While these models improved detection accuracy by capturing geometric structure in 3D, voxelization introduced several key limitations. First, quantization errors and loss of fine-grained information were common due to fixed grid resolutions, which impaired performance on small or thin objects. Second, voxel grids were inherently sparse, yet early models processed them densely, leading to computational inefficiency. VoxelGraph (2020) attempted to exploit voxel sparsity using graph neural networks [Lu et al. 2023], but this added architectural complexity and introduced optimization instability in training. MonoDFNet (2025) used depth-guided fusion to improve voxel representation, but it was highly dependent on accurate depth maps, which are often noisy in low-light or reflective scenes [Gao et al. 2025a]. Across these voxel-centric models, common bottleneck was the trade-off between spatial resolution and efficiency. Moreover, since they lacked attention mechanisms, these models struggled with context modeling across long spatial ranges, making them less effective in scenes with multiple object interactions or partial occlusions. Challenges in Point-Wise and Frustum-Based Models: Point-based networks, especially PointNet (2017) and its successor PointNet++, pioneered direct learning on raw 3D point clouds [Qi et al. 2017]. These models eliminated the need for voxelization [Zhou et al. 2020], preserving spatial fidelity [Zhu et al. 2022], but encountered critical limitations. PointNets architecture was order-invariant but spatially myopic, meaning it could not model local context effectively. PointNet++ improved this by introducing hierarchical grouping, but still suffered from non-uniform sampling issues and high sensitivity to point cloud density [Li et al. 2021a; Sheshappanavar and Kambhamettu 2020]. Frustum PointNet (2018) [Qi et al. 2018]combined 2D object proposals with 3D point processing, achieving decent results in constrained environments, but was highly dependent on the accuracy of 2D detectors, making it unsuitable for autonomous applications with noisy camera inputs. EPNet (2020) [Huang et al. 2020] and PointAugment (2020) [Li et al. 2020] incorporated attention and augmentation strategies to enhance point-wise reasoning, but their reliance on manually tuned fusion mechanisms and limited receptive fields restricted their generalization in cluttered or unstructured scenes. These models also lacked dynamic adaptability to unseen object classes, as their training was anchored in closed-set object categories. Collectively, point-based models before 2022 were constrained by scalability, context integration, and real-time performance bottlenecks [Sapkota et al. 2025b], particularly in large-scale outdoor environments with varying lighting and weather conditions. Sensor Dependence, Monocular Instability, and Fusion Complexity: Multi-modal fusion models like FusionTransformer (2021) and CrossModal3D (2021) [Zhao et al. 2022] aimed to improve robustness by integrating data from LiDAR, cameras, and even radar. However, this came with significant fusion complexity and synchronization challenges. Aligning features across modalities with different spatial and temporal resolutions required careful calibration and preprocessing. Likewise, DepthSparse (2021) [Fan et al. 2022] relied on stereo vision and monocular depth estimation, which are inherently unstable under varying lighting or textureless surfaces, leading to depth estimation errors and false positives. FusionPainting (2021) and SparseVoxelNet (2021) attempted to incorporate sparse attention and temporal fusion but often required large computational resources and showed degraded performance on long-range or partially occluded objects. Another limitation was the lack of semantic reasoning capabilityfusion mechanisms were primarily geometric or photometric, unable to leverage textual or contextual cues from the environment. These models also exhibited rigid data modality requirements, limiting their deployment in scenarios where not all sensors are available or calibrated. The limitations in adaptive fusion, computational load, and semantic understanding in these pre-2022 models created clear need for more flexible, unified, and context-aware approachesgaps later addressed by Vision-Language models. 3D Object Detection with Vision Language Models: Building 3.1.2 on early innovations in 3D object detection, the period from 2019 to 2025 marked transformative shift in methodology, driven by advances in multimodal learning and the integration of VLMs. Initially, models such as MV3D[Chen et al. 2017], AVOD, and PointFusion relied on non-attention mechanisms and simple ROI or point-wise processing. However, with the rise of attention-based frameworks like MVX-Net and PointPainting in 20192020, 3D detection began leveraging cross-modal reasoning and spatial alignment through ACM Trans. Graph., Vol. 37, No. 4, Article 111. Publication date: August 2025. transformers and multi-modal fusion. The evolution accelerated post-2022, with voxel-wise architectures such as UVTR and DeepFusion optimizing spatial granularity and computation. Notably, recent years have seen the emergence of VLM-powered 3D models, which integrate textual context and visual perception. Models like PaLM-E [Driess et al. 2023], BLIP-2 [Li et al. 2023a], InternVL, and CogVLM pioneered the use of vision-language embedding alignment, enabling instruction-following, scene understanding, and open-vocabulary detection. By 20242025, this trend matured with models like VLM3D-Guide, Instruct3D, and OmniVLM3D, incorporating natural language prompts for more adaptive and interactive 3D understanding. These approaches reflect broader movement toward sensor-agnostic, context-aware, and language-informed 3D perception systems. All state-of-the-art VLMs-based 3D object detection are presented in Table 2. VLMs have recently emerged as transformative solutions to longstanding limitations in traditional 3D object detection pipelines. Unlike earlier models that struggled with rigid sensor dependencies, modality-specific architectures, and limited semantic reasoning, VLM-based systems integrate textual context and visual features within unified, cross-modal framework [Luo et al. 2024; Zhang et al. 2024a]. This enables richer understanding of scenes, where detection is guided not only by geometry but also by high-level language priors. Models like PaLM-E (2023) and CogVLM (2023) [Wang et al. 2024d] exemplify this shift, supporting embodied perception by aligning linguistic commands or descriptions with visual and spatial input for downstream 3D tasks. Similarly, BLIP-2 (2023) and InternVL (2023) introduced powerful encoder-decoder architectures that enhance 3D object grounding through multimodal fusion, improving performance in open-vocabulary and zero-shot settings. In 2024, models like TextVoxelNet and Instruct3D further extended these capabilities by incorporating language-guided voxel reasoning and instruction-tuned 3D detection, enabling more controllable and human-aligned outputs [Kamata et al. 2023]. These VLMs address key challenges such as poor generalization, lack of semantic understanding, and the need for multi-sensor alignment by offering flexible, text-conditioned object proposals and scene interpretations. Furthermore, recent VLMs like ZeroVL3D (2025) push boundaries in zero-shot 3D detection, functioning effectively in novel environments without retraining. Collectively, these models mark paradigm shift toward more adaptive, interpretable, and semantically enriched 3D perception systems. The integration of vision-language pretraining, cross-modal attention, and textual prompting positions VLMs as promising foundation for the next generation of 3D object detection. The next sections will delve deeper into their mechanisms, capabilities, and limitations. The VLMs serve as the backbone for variety of multimodal tasks, including object detection, image captioning, visual question answering (VQA), and image segmentation [Li et al. 2025]. Their ability to generalize across diverse visual inputs and interpret complex textual prompts makes them highly adaptable across domains [Du et al. 2022]. At the core of typical VLM are three primary components: an image encoder, multimodal fusion module, and text decoder. The image encodercommonly Vision Transformer orCNNencodes visual inputs into rich embeddings. These are then aligned with textual embeddings through cross-attention mechanisms or projection Review of 3D Object Detection with Vision-Language Models 111:7 Table 2. Timeline and classification of 3D object detection with VisionLanguage Models Type Processing Method Model (Release Date) & Reference PaLM-E [Driess et al. 2023] LLaVA-1.5 [Zhu et al. 2024] BLIP-2 [Li et al. 2023a] InternVL [Zhu et al. 2025] CogVLM [Tian et al. 2024; Wang et al. 2024d] CLIP3D-Det [Hegde et al. 2023] Instruct3D [Kamata et al. 2023] M3D-LaMed [Bai et al. 2024] Attention-Based Attention-Based Encoder-Decoder Encoder-Decoder Attention-Based Attention-Based Attention-Based Encoder-Decoder Qwen2-VL [Wang et al. 2024a] Find Propagate [Etchegaray et al. 2024] Decoder-Only Hybrid down/Bottom-up TopEmbodied 3D Perception Multimodal Visual Grounding Image-Text 3D Localization Visual-Language Spatial Reasoning Vision-Language Spatial Alignment Monocular (CLIP Feature Fusion) ROI-wise (Instruction Tuning) 3D Medical Image Understanding via Multi-modal Instruction Tuning Multimodal 3D Scene Understanding Open-Vocabulary 3D Detection using Frustum Search, Cross-Modal Propagation, and Remote Simulation Open-Vocabulary, Vision-Language Embedding Alignment Multi-Dataset + Language Alignment LiDAR-Radar 3D Visual Grounding with Multi-Stage Cross-Modal Attention and Graph Fusion Open-Vocabulary 3D Scene Understanding via CLIP Feature Embedding Text-Embedded Feature Alignment for Large-Vocabulary 3D Segmentation 3D-Aware Prompting with Multi-View Rendering and VLM Integration for Broad 3D Understanding Coarse-to-Fine Transformer-Based 3D Localization with Contrastive Learning and Matching-Free Refinement Voxel-wise Zero-Shot 3D Instance Masking via Multi-View CLIP Feature Fusion Automated 2D-to-3D Prompt Synthesis with SLM-Supervised VLM Reasoning Transformer + CLIP Attention-Based Prompt-Guided Sensor Fusion CLIP-Based ZeroShot Language-Guided Segmentation Multi-Task 3D Reasoning Language-Guided Localization Attention-Based Open-Vocabulary Segmentation Task Planning Cross-Modal Alignment Open-Vocabulary 3D Object Detection with Semantic Consistency Active Learning Vision-Language Embedding Diversity for Novel Object Detection Multimodal Training PreUnified Representation of 3D Point Clouds, Images, and Language for 3D Classification Multimodal Contrastive Learning Open-World 3D Shape Understanding via Multi-Modal Representation Alignment OWL-ViT Link to paper OmniVLM3D [Chen et al. 2024a] Talk2PC (2025) [Guan et al. 2025] OpenScene (2023) [Peng et al. 2023] Language-Grounded Indoor 3D Semantic Segmentation (2022) [Rozenberszki et al. 2022] 3D-LLM (2023) [Hong et al. 2023] Text2Loc (2024) [Xia et al. 2024] PromptDet [Guo and Ling 2025] OpenMask3D (2023) [Takmaz et al. 2023] 3D-Grounded VLM Framework (2025) [Tang et al. 2025b] OV-SCAN: Semantically Consistent Alignment for Novel Object Discovery (2025) [Chow et al. 2025] Language-Driven Active Learning for Diverse Open-Set 3D Object Detection (2025) [Greer et al. 2025] ULIP: Learning Unified Representation of Language, Images, and Point Clouds for 3D Understanding (2023) [Xue et al. 2023] OpenShape: Scaling Up 3D Shape Representation Towards OpenWorld Understanding (2023) [Liu et al. 2023] layers within the fusion module, facilitating meaningful cross-modal interaction [Khan et al. 2022; Li et al. 2023b]. The final component, LLM, decodes this integrated representation to produce coherent and contextually appropriate language outputs. VLMs go beyond traditional visual systems by enabling open-vocabulary object detection, where objects can be identified based on arbitrary language queries rather than predefined class labels. This is accomplished by mapping both image regions and text into shared embedding space, which allows flexible and context-aware reasoning. As illustrated in Figure 5, VLM analyzing an image of two cats can determine spatial relationships (Is one cat behind the other?), segment visual entities based on descriptive prompts (Segment: striped cat), and answer contextual questions (What is the breed of these cats?). ACM Trans. Graph., Vol. 37, No. 4, Article 111. Publication date: August 2025. 111:8 Sapkota et al. 2025, Review of 3D Object Detection with VLMs Furthermore, it can adapt through instruction, such as learning that striped cats are called tabby cats, and applying that information to reclassify visual inputs. This example underscores the comprehensive reasoning capabilities of VLMs, showcasing their role as dynamic interpreters of multimodal content."
        },
        {
            "title": "3.2.1 Pretraining and Fine-tuning Structure of VLMs for Object De-\ntection: The architectural pipeline of VLMs, as illustrated in Figure 6,\nfollows a sequential design with distinct pretraining and fine-tuning\nphases. During pretraining, the model learns cross-modal alignment\nthrough a three-stage framework: an image encoder (e.g., CLIP-ViT)\nprocesses input images into visual embeddings, a multimodal pro-\njector (e.g., dense neural networks) maps these embeddings into\nthe text token space, and a text decoder (e.g., Vicuna) generates\ncaptions or answers autoregressively [Huang et al. 2023b; Li and\nTang 2024]. The core technical challenge lies in unifying image and\ntext representations. To achieve this, visual tokens are projected into\nthe decoder’s embedding space and concatenated with text tokens,\nforming a fused input sequence [Chen et al. 2022]. For example,\nLLaVA employs a CLIP-based encoder to extract image features,\nwhich are linearly projected into Vicuna’s token dimension [Zhu\net al. 2024]. These projected visual tokens are prepended to text\nembeddings (e.g., questions or prompts), enabling the decoder to\nprocess multimodal inputs seamlessly.",
            "content": "Pretraining strategies vary, but common approach involves two stages. First, the multimodal projector is trained to align visual and textual features while keeping the image encoder and text decoder frozen. For instance, LLaVA leverages GPT-4 to synthesize instruction-response pairs from image-caption datasets: images and captions are fed to GPT-4 to generate contextual questions, which serve as training data [Zhu et al. 2024]. The projector is optimized using cross-entropy loss to align visual embeddings with the decoders expected text space. In the second stage, the text decoder is unfrozen, and both the projector and decoder are jointly fine-tuned on task-specific data (e.g., visual QA pairs), refining cross-modal reasoning. During fine-tuning, the pretrained image encoder and projector remain fixed or lightly updated, while the text decoder is adapted to downstream tasks. For detection or grounding, textual instructions (e.g., Output bounding boxes for all cars) are concatenated with projected visual tokens, conditioning the decoder to generate structured outputs like coordinates or masks. This phased training ensures computational efficiencyonly fraction of parameters (projector and decoder) are updatedwhile preserving the encoders generalizable visual features. Key innovations include the use of lightweight projectors (e.g., linear layers) for parameter efficiency and spatial-aware architectures that preserve pixel-level details for localization tasks. The modular designfreezing encoders, training projectors, and adapting decodersenables VLMs to scale across diverse applications, from captioning to 3D detection, while maintaining robust zero-shot capabilities. ACM Trans. Graph., Vol. 37, No. 4, Article 111. Publication date: August 2025. 2D Object Detection with VLMs: VLMs detect objects in 2D environments through multimodal process that combines visual feature extraction and semantic alignment  (Fig. 7)  . First, an image encoder (e.g., Vision Transformer) processes the input image into grid of spatial embeddings, capturing hierarchical features such as edges, textures, and object parts. These embeddings are projected into joint vision-language space via multimodal projector (e.g., linear layers), aligning them with textual token embeddings[Jain et al. 2024; Zang et al. 2025]. The text decoder then interprets these fused representations to generate bounding boxes or segmentation masks. Three primary strategies govern this process: Zero-Shot Prediction: The model matches visual features to textual labels (e.g., \"car\") without fine-tuning, leveraging pretrained semantic relationships. Visual Fine-Tuning: The visual encoder is adapted to domainspecific data (e.g., urban scenes) to improve detection accuracy for rare objects. Text Prompting: Descriptive prompts (e.g., \"a red truck near the curb\") guide the decoder to focus on contextual details, enhancing localization precision. The output includes 2D bounding boxes and class labels, generated autoregressively by correlating textual queries with activated visual regions. This process excels in generalizability but lacks depthaware reasoning. 3D Object Detection with VLMs: Extending VLMs to 3D detection involves multi-stage pipeline  (Fig. 7)  , where 2D proposals are fused with 3D geometric data to predict spatially grounded objects. The steps are as follows: (1) 2D Object Proposals: The VLM first generates 2D bounding boxes and class labels using its vision-language alignment capabilities. For example, it identifies \"pedestrian\" regions in an image through zero-shot prediction or text prompts like \"highlight all pedestrians.\" (2) 2D-to-3D Projection: The 2D proposals are projected into 3D space using depth maps or LiDAR-Camera calibration matrices. Each 2D region is mapped to frustum in 3D, creating candidate 3D volumes. For instance, 2D \"car\" bounding box is extruded into 3D cuboid based on estimated depth. (3) Hierarchical Feature Alignment: The VLM aligns 2D visual tokens (from the image encoder) with 3D point cloud features (from LiDAR or depth sensors). This is achieved through cross-modal attention mechanisms, where 3D points within the projected frustum attend to relevant 2D regions. For example, points in \"tree\" frustum aggregate features from image patches showing leaves or branches. (4) Refinement and Filtering: The initial 3D proposals are refined using geometric constraints (e.g., size priors) and semantic feedback from the VLM. The text decoder validates proposals by correlating 3D features with textual queries (e.g., \"a sedan with dimensions 4.5m 1.8m\"). Outliers are filtered using confidence scores derived from vision-language similarity."
        },
        {
            "title": "3.2.2 Visualization of 3D Detection with VLMs: Recent advance-\nments in 3D object detection have been significantly influenced by",
            "content": "A Review of 3D Object Detection with Vision-Language Models 111:9 Fig. 5. Illustration of Vision-Language Model (VLM) performing multimodal reasoningdetecting, segmenting, and describing objects from images using textual prompts, demonstrating open-vocabulary 3D detection and contextual understanding. Fig. 6. Overview of VLM architecture showing pretraining and fine-tuning stages. Visual embeddings from an image encoder are projected into shared space and concatenated with text tokens. During pretraining, only the projector is trained to align visual and textual representations. In fine-tuning, the decoder is adapted to downstream tasks like object detection. This modular design supports flexible multimodal learning, enabling VLMs to generalize across vision-language tasks such as grounding, captioning, and 3D object detection. the integration of VLMs, enabling models to understand and reason about 3D scenes in more versatile and semantically grounded manner. In study by Chen et al. [Chen et al. 2024d], the SpatialVLM model demonstrates how VLMs can leverage large-scale vision-language pretraining and spatial question-answering tasks to implicitly learn 3D relationships from monocular images, bypassing the need for traditional annotated 3D data or depth sensors. This innovative approach allows for both quantitative and qualitative reasoning in 3D environments. However, its reliance on synthetic question-answering data may limit real-world applicability. Further developments by Hong et al. [Hong et al. 2023] and Jiao et al. [Jiao et al. 2024] push the boundaries of VLM integration by combining multi-view rendered point cloud data with large language models. The 3D-LLM framework [Hong et al. 2023] and hierarchical alignment methods in [Jiao et al. 2024] enable zero-shot and few-shot generalization for novel object detection tasks, addressing limitations of traditional methods that often require extensive annotated datasets. Additionally, models like CoDA [Cao et al. 2023] and MSSG [Cheng et al. 2023] showcase how combining geometry, semantics, and language can enhance open-vocabulary detection and improve interpretability, marking transformative shift in 3D perception research. In recent study by Brazil et al. [Brazil et al. 2023] introduces OMNI3D, large-scale benchmark for 3D object detection that repurposes existing datasets to address the limitations of smaller, domain-specific datasets in 3D recognition. OMNI3D consists of ACM Trans. Graph., Vol. 37, No. 4, Article 111. Publication date: August 2025. 111:10 Sapkota et al. 2025, Review of 3D Object Detection with VLMs Fig. 7. Illustration of 2D and 3D object detection using VLMs in commercial orchard setting. The robot uses VLMs to perform 2D detection by aligning visual features with textual prompts, enabling zero-shot object identification. These 2D proposals are projected into 3D space using depth data to generate volumetric bounding boxes. Cross-modal fusion with LiDAR or depth inputs refines object localization, allowing context-aware reasoning about apple position and ripeness. This pipeline highlights how VLMs bridge semantic understanding and geometric reasoning for real-world agricultural tasks, enabling robots to detect and interact with fruit in complex, dynamic environments. 234,000 images and over 3 million annotated instances across 98 object categories, making it 20 times larger than existing 3D benchmarks like KITTI [Geiger et al. 2013] and SUN RGB-D [Song et al. 2015]. key challenge in 3D object detection is the diversity in camera intrinsics and object types. To overcome this, the authors propose Cube R-CNN [Piekenbrinck et al. 2024], unified model designed to generalize across varying camera and scene types. Cube R-CNN predicts 3D object locations, sizes, and rotations from single image, leveraging novel approach called virtual depth to mitigate scale-depth ambiguity caused by variations in camera focal lengths. This model outperforms previous methods, such as ImVoxelNet and GUPNet, on both urban and indoor benchmarks, demonstrating strong generalization across different domains. OMNI3Ds large scale and diversity also enable improvements in single-dataset performance and accelerate learning on smaller datasets through pre-training. Figure 8a demonstrates the performance of Cube R-CNN on the OMNI3D test set, showcasing its ability to predict 3D objects across diverse environments. The upper section of the figure displays the detection of objects in room, such as bed, table, and cupboard, accurately predicted in 3D space. These predictions are overlaid on the input image, providing clear visualization of how Cube R-CNN recognizes and locates these objects within the scene [Piekenbrinck et al. 2024]. The 3D bounding boxes are also shown from topdown perspective, where the base is composed of 1m1m tiles, offering an additional view of the spatial arrangement of these objects. The middle part of the figure focuses on the detection of kitchen items, including washing machines and racks, with 3D bounding boxes accurately identifying their positions and sizes in ACM Trans. Graph., Vol. 37, No. 4, Article 111. Publication date: August 2025. the image. This demonstrates the models versatility in handling diverse object categories. In the lower section of Figure 8a, Cube R-CNN performs 3D detection on traffic road scene, identifying vehicles and pedestrians in dynamic, outdoor environment. This variety of scenes, from indoor to outdoor, highlights Cube R-CNNs robust ability to generalize across different domains, making it powerful tool for large-scale 3D object detection. Likewise, CoDA [Cao et al. 2023] introduces novel approach to open-vocabulary 3D object detection (OV-3DDet), addressing the challenges of localizing and classifying novel objects in 3D scenes with limited base categories. The paper proposes unified framework that integrates two key components: the 3D Novel Object Discovery (3D-NOD) strategy and cross-modal alignment module. The 3D-NOD strategy leverages both 3D geometry priors and 2D semantic priors from the CLIP model to localize novel objects, creating pseudo-labels for objects not seen during training. To improve classification, CoDA employs the Discovery-Driven CrossModal Alignment (DCMA), which aligns 3D object features with 2D image/text features. This alignment consists of class-agnostic distillation and class-specific contrastive learning, which iteratively enhance the models ability to detect novel objects by leveraging the discovered novel object boxes. The framework works without relying on pre-annotated 2D open-vocabulary detection models, using CLIP-based vision-language model to transfer rich openworld knowledge into 3D object detection. CoDAs iterative process of novel object discovery and feature alignment significantly improves performance, outperforming the best alternative methods by 80% in mean Average Precision (mAP) on benchmark datasets like SUN-RGBD and ScanNet. This method demonstrates the ability to localize and classify novel objects by continuously expanding pool of discovered novel boxes and refining feature alignment, leading to better detection and classification in an open-vocabulary setting. As shown in Figure 8b, the model demonstrates its strong capability to discover more novel objects from point clouds. For instance, our method successfully identifies the dresser in the first scene and the nightstand in the second scene. This highlights the frameworks ability to effectively localize and classify novel objects that were not part of the base categories. The performance comparison further indicates that our method generally exhibits superior open-vocabulary detection ability on novel objects, outperforming alternative methods in terms of precision and detection accuracy. The figure illustrates the 3D detection process, showcasing how the model identifies these objects with high accuracy in diverse 3D environments. Wang et al. [Wang et al. 2024b] propose G3-LQ, model for 3D visual grounding in Embodied AI that integrates geometric and semantic cues using modules like PAGE and Flan-QS. By introducing Poincaré Semantic Alignment loss and evaluating on ScanRefer and Nr3d/Sr3d, G3-LQ achieves superior alignment between text and 3D features, outperforming prior methods. As shown in Figure 9, we present examples of 3D detection proposed by the author in G3-LQ on the ScanRefer dataset. The figure illustrates the models remarkable grounding performance in interpreting both geometric attributes and complex language queries. In Figure 9a, the query \"this is chair by the wall. it is the fifth chair from the right wall surface\" is correctly grounded, showcasing the models ability to recognize object positions based on spatial Review of 3D Object Detection with Vision-Language Models 111:11 relationships. Figure 9b displays the description \"the table is the left-most one in the center of the room. the table is dark orange and has four legs,\" highlighting G3-LQs proficiency in discerning object size, color, and orientation. Additionally, Figure 9c illustrates \"it is black office chair. the black office chair is the fourth office chair on the left side of the table,\" demonstrating the models capacity to effectively parse complex utterances and spatial ordering. Likwise, Zhang et al. (2024) [Zhang et al. 2024a] present 3DVLP, vision-language pre-training framework tailored for semantic 3D scene understanding. Their goal is to learn universal embedding that generalizes across downstream tasks like 3D object detection, grounding, and captioning. Unlike prior task-specific models, 3DVLP adopts visual grounding as proxy task and introduces three object-level modules to improve proposal quality and semantic alignment. First, the Object-level IoU-guided Detection (OID) loss uses DIoU and label smoothing to generate accurate 3D bounding boxes. Second, Object-level Cross-Contrastive alignment (OCC) aligns proposal features with text embeddings by maximizing similarity between matched pairs while pushing mismatches apart. Likewise, Object-level Self-Contrastive learning (OSC) enhances intra-modal discrimination by ensuring distinct proposal features for different objects [Zhang et al. 2024a]. All three modules rely on an IoU-based filtering mechanism to define positive and negative training pairs. These components enable the model to handle issues of modality misalignment, overlapping semantic information, and fine-grained object differentiation. During fine-tuning, 3DVLPs backbone is transferred to task-specific heads, demonstrating strong generalization and competitive performance. Notably, the framework supports efficient fine-tuning, making it highly adaptable. Figure 9d shows an example 3D detection output from 3DVLP, identifying the object described as There is tall chair pulled up to the table in the room. It is the second from the right. Likewise, Figure 9e presents 3D visual grounding result where the model accurately detects the brown ottoman positioned in front of brown sofa. The ottoman is correctly identified along with detailed attributes, including black backpack, black duffel bag, and box of tissues placed on it. This qualitative result underscores the effectiveness of the 3DVLP model trained on the ScanRefer dataset, where 3DVLP achieves state-of-the-art performance with an accuracy of 51.70% at IoU@0.25 and 40.51% at IoU@0.5. These findings highlight the models superior grounding ability, particularly in complex scenes with multiple, visually similar objects. 3.3 Strengths and Challenges of 3D Object Detection with VLMs Recent advances in 3D object detection via VLMs have shown impressive potential, particularly in zero-shot and open-vocabulary scenarios [Sapkota et al. 2024b; Sapkotaa and Karkeea 2025]. Unlike traditional 3D detectors that depend heavily on large-scale annotated datasets, modern VLM-based approaches harness pretrained 2D vision-language priors (e.g., CLIP, GPT-3/4) to bridge the modality gap between images, point clouds, and language. Notably, PointCLIP V2 [Zhu et al. 2023] demonstrates how realistic projection modules and GPT-generated prompts can achieve strong 3D performance without any 3D training data. Similarly, 3DVLP [Zhang ACM Trans. Graph., Vol. 37, No. 4, Article 111. Publication date: August 2025. 111: Sapkota et al. 2025, Review of 3D Object Detection with VLMs Fig. 8. Examples of 3D Object detection with VLMs:(a) Cube R-CNN with VLM-based 3D object detection on OMNI3D, showcasing predictions in room, kitchen, and traffic scenes with 3D bounding boxes.[Brazil et al. 2023] ; b) Illustrates CoDAs 3D detection process [Cao et al. 2023], showcasing its ability to localize and classify novel objects with high accuracy in diverse scenes. ACM Trans. Graph., Vol. 37, No. 4, Article 111. Publication date: August 2025. Review of 3D Object Detection with Vision-Language Models 111: Fig. 9. Examples of 3D Object Detection with VLMs: a) shows 3D detection with VLMs for \"this is chair by the wall, the fifth chair from the right wall\" by [Wang et al. 2024b]; b) illustrates \"the left-most table in the center\" bby [Wang et al. 2024b]; c) depicts \"a black office chair.\" by[Wang et al. 2024b] ; de) [Zhang et al. 2024a] et al. 2024a] leverages object-level contrastive learning to boost transferability across tasks such as detection, grounding, and captioning. Language-free 3DVG [Zhang et al. 2024d] further reduces reliance on human annotations by synthesizing pseudo-language features from multi-view images, achieving effective grounding without textual supervision. Other works like Uni3DL [Li et al. 2024a] and CoDA [Cao et al. 2023] focus on architectural unification and cross-modal alignment to enhance scalability and generalization in real-world 3D scenes. However, several challenges remain. Many methods rely on complex training pipelines (e.g., rendering, projection, pseudo-language modeling), increasing computational cost and limiting real-time applicability. Others face generalization issues when exposed to sparse data, unfamiliar object categories, or diverse environments. Furthermore, performance can be sensitive to prompt design, quality of multi-view imagery, or LiDAR calibration accuracy. Some models also depend heavily on pretrained components and handcrafted alignment mechanisms, which may not scale efficiently across datasets or applications. These recent approaches demonstrate diverse range of methodologies for tackling 3D object detection using vision-language models. PointCLIP V2 [Zhu et al. 2023] adopts dual-prompting strategy that bridges 3D point clouds with 2D vision-language priors via depth-based projection and language synthesis, allowing for strong zero-shot performance without 3D supervision. Languagefree 3DVG [Zhang et al. 2024d] replaces text entirely with pseudolanguage embeddings derived from multi-view images, reinforcing spatial relations using cross-modality consistency modules. 3DVLP [Zhang et al. 2024a] takes contrastive learning route, defining object-level alignment tasks like OID, OCC, and OSC to build robust multimodal embeddings. Meanwhile, SpatialVLM [Chen et al. 2024d] frames detection as spatial question-answering task, using large-scale VQA datasets aligned with monocular depth predictions. 3D-LLM [Hong et al. 2023] integrates multi-view rendered features directly into LLMs and supplements them with point cloud-based localization modules. Some works, like CoDA [Cao et al. 2023], focus on open-vocabulary detection by aligning 3D geometry with textual/image priors to discover unseen classes. Others such as MSSG [Cheng et al. 2023] introduce novel tasks like 3D referring expression comprehension, merging language grounding with LiDAR-based detection in real-time settings. Unsupervised methods like those by [Fruhwirth-Reisinger et al. 2024] and [Najibi et al. 2023] exploit CLIP-based classification over LiDAR clusters or auto-labeling with vision-language distillation, removing reliance on labeled 3D data. Furthermore, Uni3DL [Li et al. 2024a] proposes unified transformer framework that operates directly on point clouds, dynamically routing tasks across 3D vision-language objectives through shared token space. These varied methodologies highlight the flexibility of VLMs in adapting to the unique demands of 3D understanding, from geometric projections to task-specific grounding. Table below summarizes the key contributions, strengths, and limitations of recent VLM-based 3D object detection methods, offering comparative perspective on this rapidly evolving research landscape. Comparison of VLM-based 3D object detection methods, highlighting strengths, innovations, and key limitations: ACM Trans. Graph., Vol. 37, No. 4, Article 111. Publication date: August 2025. 111:14 Sapkota et al. 2025, Review of 3D Object Detection with VLMs Author (Year) [Chen et al. 2024d] SpatialVLM (2024) [Hong et al. 2023] 3D-LLM (2023) [Fruhwirth-Reisinger et al. 2024] Vision-Language Guidance for LiDAR-based Unsupervised 3D Object Detection (2024) [Jiao et al. 2024] Unlocking Textual and Visual Wisdom: OpenVocabulary 3D Object Detection Enhanced by Comprehensive Guidance from Text and Image (2024) [Li et al. 2024a] Uni3DL: Unified Model for 3D Vision-Language Understanding (2024) [Najibi et al. 2023] Unsupervised 3D Perception with 2D VisionLanguage Distillation for Autonomous Driving (2023) [Brazil et al. 2023] Omni3D (2023) 3D Object Detection Method Key Contribution and Strengths Spatial VQA using internetscale spatial data with visionlanguage pretraining Introduced large-scale spatial QA dataset enabling quantitative and qualitative 3D reasoning from 2D images Drawbacks / Limitations Limited to synthetic QA data, relies heavily on monocular depth, potential bias in templated questions Injecting 3D point clouds into large language models for holistic 3D understanding Vision-language-guided unsupervised 3D detection using LiDAR point clouds with CLIP for classifying point clusters Proposed new family of 3D-LLMs that perform diverse 3D tasks using 3D-language data; uses multi-view rendered features aligned with 2D VLMs and 3D localization mechanism for better spatial reasoning Requires 3D rendering and feature extraction pipeline; training depends on alignment with 2D backbones; limited availability of largescale 3D-language data compared to image-language datasets Introduced vision-language-guided approach to detect both static and moving objects in LiDAR point clouds using CLIP, achieving state-of-the-art results on Waymo and Argoverse 2 datasets (+23 AP3D and +7.9 AP3D) Relies on multi-view projections and temporal refinement, requires large-scale LiDAR datasets, and may face difficulties with distant and incomplete objects due to LiDARs inherent limitations Open-vocabulary 3D object detection enhanced by visionlanguage model guidance for novel object discovery in 3D scenes Introduced hierarchical alignment approach using vision-language models to discover novel classes in 3D scenes, showing significant improvements in accuracy and generalization in real-world scenarios Relies on pretrained VLMs and object detection models, limited by availability of 3D scene data for training and generalization to unseen categories in certain complex environments Unified 3D Vision-Language model operating directly on point clouds, supporting wide range of 3D tasks, including segmentation, detection, grounding, and captioning Developed multi-modal autolabeling pipeline to generate amodal 3D bounding boxes and tracklets for unsupervised openvocabulary 3D detection and tracking of traffic participants in autonomous driving scenarios Cube R-CNN for large-scale 3D object detection on the Omni3D benchmark Introduced query transformer for taskagnostic learning and task router for taskspecific outputs, achieving performance on par with or surpassing SOTA models in 3D vision-language tasks Limited by the complexity of the model architecture, and the performance may be constrained in highly specialized tasks not covered in the evaluation Introduced novel approach combining motion cues from LiDAR sequences with vision-language model knowledge distillation for open-set 3D detection, achieving state-of-the-art performance in unsupervised 3D perception tasks on the Waymo Open Dataset The method depends on the quality of motion cues and the performance of the pre-trained vision-language model, potentially leading to limitations in highly dynamic or complex environments Introduced the Omni3D dataset with 234k images and 3 million instances across 98 categories. Cube R-CNN generalizes 3D object detection across cameras and scene types, uses IoUness and Virtual Depth for robustness to camera variations. Focus on large-scale benchmarks, but may face challenges in handling highly diverse real-world scenes outside of the benchmarks scope. Sensitivity to annotation quality and scale-depth ambiguities in complex environments. ACM Trans. Graph., Vol. 37, No. 4, Article 111. Publication date: August 2025. [Cao et al. 2023] CoDA (2023) Collaborative Novel Box Discovery and Cross-modal Alignment for Open-vocabulary 3D Object Detection (OV-3DDet) [Cheng et al. 2023] MSSG (2023)"
        },
        {
            "title": "Single",
            "content": "Shot Multi-modal Grounding (MSSG) for 3D Referring Expression Comprehension (REC) in autonomous driving [Zhang et al. 2024a] 3DVLP (2024) Pre3D Vision-Language training with Object Contrastive Learning (3DVLP) [Zhu et al. 2023] PointCLIP V2 (2023) PointCLIP V2: Prompting CLIP and GPT for Powerful 3D Openworld Learning [Zhang et al. 2024d] Language-free 3DVG (2024) Towards Language-free Grounding via lational Enhancement Consistency CLIP-driven 3D Visual 2D-3D Reand Review of 3D Object Detection with Vision-Language Models 111:15 Introduced CoDA, unified framework for open-vocabulary 3D object detection. CoDA addresses novel object localization and classification using 3D geometry priors and cross-modal alignment between 3D pointcloud and image/text modalities. Achieved 80% mAP improvement over the best alternative methods on SUN-RGBD and ScanNet datasets. Proposed novel LiDAR Grounding task and an efficient MSSG approach that jointly learns LiDAR-based detection and language grounding with direct region prediction. Enables flexible integration of image features for enhanced multimodal comprehension. Demonstrated strong performance on the Talk2Car dataset. Introduces unified 3D vision-language pre-training framework using objectlevel contrastive learning, incorporating three innovative tasks: Object-level IoUguided Detection (OID) loss, Object-level Cross-Contrastive alignment (OCC), and Object-level learning (OSC). Demonstrates strong generalization across multiple downstream tasks including visual grounding, dense captioning, and question answering on ScanRefer, Scan2Cap, and ScanQA datasets. Self-Contrastive Proposes unified zero-shot 3D visionlanguage framework combining CLIP and GPT-3. Introduces two key prompting modules: realistic shape projection module to improve visual alignment, and 3Daware textual prompt generator using GPT3 to enhance language-vision matching. Achieves strong zero-shot classification gains (+42.90% on ModelNet10, +40.44% on ModelNet40, +28.75% on ScanObjectNN), and extends easily to few-shot classification, part segmentation, and object detection. Presents language-free training framework for 3D visual grounding by leveraging CLIPs image-text embedding to generate pseudo-language features from multiview images. Introduces Neighboring Relation-aware Modeling module and Cross-modality Relation Consistency module to enhance and align 2D-3D relational structures. Demonstrates competitive results across ScanRefer, Nr3D, and Sr3D without requiring textual annotations during training. Potential challenges with handling large, diverse real-world datasets beyond the tested ones. Sensitivity to the quality of initial base categories and pseudo label generation for novel objects. single dataset Focuses on (Talk2Car), potential limitations in generalizing to broader driving scenes or different sensors. Realtime performance and robustness under varying driving conditions require further validation. The training framework is complex, requiring extensive computation and large-scale annotated data. Pretraining and tuning require careful design of IoU thresholds and contrastive setups, which may limit applicability in low-resource or realtime settings. Relies on non-trivial prompting strategies and auxiliary models (GPT-3, voxelization), which may introduce runtime and scalability limitations. Performance heavily depends on projection quality and prompt engineering. Relies on high-quality multi-view imagery and the robustness of CLIP embeddings; may face challenges generalizing in sparse-view or textpoor environments. Additional relation modeling modules increase model complexity and training cost. ACM Trans. Graph., Vol. 37, No. 4, Article 111. Publication date: August 2025. 111: Sapkota et al. 2025, Review of 3D Object Detection with VLMs 3D Detection: Traditional vs VLM Approaches. Traditional 3D 3.3.1 object detectors such as VoxelNet [Zhou and Tuzel 2018], PointNet [Qi et al. 2017], SECOND [Yan et al. 2018], and Frustum PointNet [Qi et al. 2018] rely on handcrafted voxelor point-wise feature extraction and spatial priors to predict bounding boxes. ROI-wise methods like MV3D [Chen et al. 2017] and AVOD [Ku et al. 2018] fuse LiDAR and camera data but remain limited to fixed class sets. Attention-based architectures including MVX-Net [Sindagi et al. 2019], PointPainting [Vora et al. 2020], and 3D-CVF [Yoo et al. 2020] improved contextual aggregation, yet they still require extensive annotated 3D data and struggle with domain shifts. In contrast, VLMbased detectors embed rich semantic priors directly from language, enabling flexible, open-vocabulary reasoning. For example, CLIP3DDet leverages CLIP feature fusion for monocular detection, grounding novel categories zero-shot [Hegde et al. 2023], while OWL-ViT aligns vision and language embeddings for open-vocabulary bounding box prediction (OWL-ViT). Models like Find Propagate use frustum search and cross-modal propagation to discover novel objects without retraining [Etchegaray et al. 2024], and OpenScene achieves zero-shot segmentation and detection by co-embedding point clouds and text in CLIP space [Peng et al. 2023]. These architectures bypass the need for per-task 3D annotations, demonstrating superior flexibility and scalability over traditional pipelines. Additionally, Fixed-vocabulary neural methods such as PointFusion [Xu et al. 2018] and FusionTransformer [Fent et al. 2024] require retraining to handle new classes, while TransFusion [Bai et al. 2022] and LiDARFormer [Zhou et al. 2024] adapt BEV representations but remain constrained by labeled categories. Even multi-modal distillation approaches like CrossModal3D [Jaritz et al. 2022] and VCD [Huang et al. 2023a] depend on base-class supervision. In contrast, VLM-empowered frameworks generalize zeroand few-shot. OmniVLM3D aligns LiDAR, camera, and text across multiple datasets for open-vocabulary detection [Chen et al. 2024a], and OpenMask3D segments unseen instances via multi-view CLIP embeddings [Takmaz et al. 2023]. OV-SCAN enforces semantic consistency to discover novel classes robustly [Chow et al. 2025], and language-driven active learning (VisLED) selects informative samples to improve open-set detection [Greer et al. 2025]. ULIP unifies images, text, and point clouds into single embedding space, boosting both standard and zero-shot classification [Xue et al. 2023], while OpenShape scales 3D representations to over thousand categories, outperforming supervised baselines in zero-shot tasks [Liu et al. 2023]. These VLM-based methods demonstrate markedly stronger generalization to novel classes and domains. Moreover, Traditional fusion techniquesDepthFusionNets depthguided voxels [Shivakumar et al. 2019], LiDAR-RCNNs cameraLiDAR proposals [Li et al. 2021b], and FusionPaintings voxel-wise attention [Xu et al. 2021]lack explicit language grounding. By contrast, modern VLM-integrated architectures perform rich multimodal reasoning. Instruct3D leverages instruction tuning to refine ROI proposals via language prompts [Kamata et al. 2023], while Talk2PC fuses LiDAR and radar through prompt-guided cross-attention for precise 3D grounding in driving scenes [Guan et al. 2025]. PaLME extends embodied perception by conditioning 3D proposals on language queries [Driess et al. 2023], and LLaVA-1.5 performs multimodal visual grounding with unified transformer [Zhu et al. 2024]. ACM Trans. Graph., Vol. 37, No. 4, Article 111. Publication date: August 2025. Text2Loc uses hierarchical transformer to localize point clouds from natural language hints [Xia et al. 2024], and the 3D-LLM framework injects point-cloud features into LLMs for diverse 3D tasks, including detection [Hong et al. 2023]. These models enable finegrained spatial reasoning, open-vocabulary queries, and interactive applicationscapabilities unattainable by purely geometric neural networks. Table 3 shows the detailed comparison between the neural networkbased 3D detectors and vision-language modeldriven 3D detection."
        },
        {
            "title": "3.3.2 Trade-offs between Traditional and VLM-Based 3D Object De-\ntection. Traditional neural network-based 3D object detection mod-\nels, such as [Qi et al. 2017] (PointNet) and [Yan et al. 2018] (SECOND),\nprioritize geometric feature extraction through voxel-wise [Zhou\nand Tuzel 2018] or point-wise [Qi et al. 2018] processing. These\nmethods excel in structured environments with high-quality LiDAR\ndata, leveraging efficient spatial aggregation (e.g., [Li et al. 2022b])\nand multi-sensor fusion (e.g., [Sindagi et al. 2019]). Models like [Bai\net al. 2022] (TransFusion) refine region-of-interest (ROI) propos-\nals using attention, while diffusion-based approaches like [Xu et al.\n2024] (3DiffTection) denoise sparse inputs. However, they require ex-\ntensive labeled 3D datasets and struggle with open-vocabulary gen-\neralization. For instance, [Zhao et al. 2024a] (BEVFusion) achieves\nstate-of-the-art accuracy on nuScenes but cannot interpret free-\nform textual queries. Their strengths lie in real-time performance\nand geometric precision, but they lack semantic reasoning capabili-\nties, limiting adaptability to novel objects or unstructured scenarios\n[Huang et al. 2023a].",
            "content": "Vision-language models like [Driess et al. 2023] (PaLM-E) and [Peng et al. 2023] (OpenScene) address these limitations by aligning 3D geometric features with language embeddings. For example, [Hegde et al. 2023] (CLIP3D-Det) projects CLIPs text-image alignment into 3D space for zero-shot detection, while [Rozenberszki et al. 2022] uses text prompts to guide indoor segmentation. VLMs such as [Hong et al. 2023] (3D-LLM) integrate multi-view rendering with LLMs for contextual reasoning, enabling tasks like \"detect all chairs near windows.\" However, they incur higher computational costs and depend on noisy text-image-point cloud triplets for training [Xue et al. 2023]. Models like [Takmaz et al. 2023] (OpenMask3D) demonstrate strong open-vocabulary performance but lag in realtime applications compared to traditional methods like [Yan et al. 2023] (CMT). The tradeoff here is interpretability and generalization versus latency and hardware requirements. Hybrid architectures, such as [Jaritz et al. 2022] (CrossModal3D) and [Hu et al. 2023] (FusionFormer), attempt to bridge these gaps by combining voxel-based geometric processing with language-guided attention. For instance, [Xia et al. 2024] (Text2Loc) uses contrastive learning to align LiDAR features with textual queries, achieving 12% improvement in novel object detection over [Li et al. 2024b] (GA-Fusion). However, VLMs like [Wang et al. 2024d] (CogVLM) require costly textual annotations and struggle with ambiguous spatial references (e.g., \"object behind the tree\"). Traditional methods dominate in latency-critical applications (e.g., [Shi et al. 2022] processes 50 FPS vs. [Kamata et al. 2023]s 8 FPS), while VLMs excel in human-in-the-loop systems [Guan et al. 2025]. Key tradeoffs include: Table 3. Comparison of traditional CNN-based and VLM-based 3D object detection highlights key trade-offs: while conventional methods rely on dense 3D supervision and fixed categories, VLMs enable open-vocabulary, zero-shot reasoning by leveraging language and vision pretraining. VLMs offer richer semantic understanding and flexibility, though often at higher computational cost, making them ideal for broader, instruction-driven 3D tasks Review of 3D Object Detection with Vision-Language Models 111:"
        },
        {
            "title": "Aspect",
            "content": "Traditional 3D Neural Networks (e.g., PointNet, VoxelNet, DeepFusion)"
        },
        {
            "title": "Object Vocabulary",
            "content": "Primarily use LiDAR point clouds or fused 2D/3D inputs (RGB-D, voxel grids). Limited to closed-set categories defined by training datasets. Learning Paradigm Supervised learning with task-specific VLM-Based 3D Object Detection (e.g., 3D-LLM, OV-SCAN, PaLM-E, LLaVA, CogVLM) Combine point clouds, images, and natural language through multimodal fusion. Enable open-vocabulary detection by aligning text and visual features. Often zero-shot or few-shot learning via pre-trained VLMs. Rich semantic understanding via language grounding and visual prompts. Leverages pre-trained models, enabling weakly supervised learning. Offers natural captions, and scene descriptions. language explanations, Scene Understanding Annotation Dependency Explainability"
        },
        {
            "title": "Integration with\nLanguage",
            "content": "datasets. Focused on geometric/spatial features; less semantic context. Heavy reliance on dense 3D labels, expensive to annotate. Limited interpretability; primarily geometric. Relatively efficient, optimized for real-time. Higher complexity due to large languagePoor generalization to unseen categories or new domains. Requires large labeled datasets. Primarily used in structured scenarios like autonomous driving. Minimal to none. vision backbones. Strong zero-shot generalization across tasks and categories. Pretraining enables higher data efficiency. Extends to instruction following, navigation, and 3D QA. Tightly integrated, allowing language queries or dialogue-based tasks. Data Efficiency: Traditional models ([Chen et al. 2017], [Li et al. 2021b]) use labeled LiDAR; VLMs ([Zhu et al. 2024], [Li et al. 2023a]) need paired text-3D data. Generalization: VLMs ([Chen et al. 2024a], [Tang et al. 2025b]) handle open-world queries; traditional models ([Lee et al. 2024], [Gao et al. 2025b]) specialize in closed-set accuracy. Compute: Attention-based VLMs ([Bai et al. 2024], [Kolodiazhnyi et al. 2025]) demand 25 more GPU resources than voxel networks [Son et al. 2025]. The comparative analysis of traditional CNN-based and VLMdriven 3D detection reveals inherent trade-offs. Traditional methods (e.g., VoxelNet, SECOND) prioritize computational efficiency and real-time performance, leveraging geometric priors for LiDARcentric tasks, but lack semantic adaptability to novel objects or unstructured environments. In contrast, VLMs (e.g., OpenScene, 3DLLM) enable open-vocabulary detection through language grounding, yet suffer from higher latency and hardware demands. Beyond Table 4, VLMs face challenges in spatial precision due to noisy text-3D alignments, while CNNs struggle with domain shifts (e.g., adverse weather). Hybrid approaches aim to balance these aspects but inherit complexity. Critical considerations for deployment include sensor dependencies, annotation scalability, and task-specific adaptability, as summarized in Table 4. Table 4. Trade-offs between Traditional and VLM-Based 3D Object Detection"
        },
        {
            "title": "Advantages",
            "content": "Speed & Simplicity"
        },
        {
            "title": "Semantic Richness",
            "content": "Open-Vocabulary Support Annotation Cost 3D and Traditional NN Methods Simpler faster inference Label-limited Fixed class set High manual effort Real-Time Deployment Optimized imple-"
        },
        {
            "title": "Generalization",
            "content": "mentations Domain-specific 3D and VLM-Based Methods Heavier slower grounded Detects novel classes Uses pretraining LanguageComputationally heavy Strong zero-shot capability"
        },
        {
            "title": "4.0.1 Current Challenges and Limitations in 3D Object Detection\nwith VLMs. Despite rapid advancements, VLMs for 3D object de-\ntection still face a host of unresolved challenges that limit their\napplicability in real-world robotics and automation systems. Unlike",
            "content": "ACM Trans. Graph., Vol. 37, No. 4, Article 111. Publication date: August 2025. 111:18 Sapkota et al. 2025, Review of 3D Object Detection with VLMs traditional geometry-driven detectors, VLMs often exhibit subpar spatial reasoning capabilities and struggle with accurate depth estimation and object localization in cluttered or occluded environments [Chen et al. 2024d; Li et al. 2025; Ma et al. 2024]. Additionally, aligning multimodal embeddingsespecially the mapping of 3D geometric structures into language-informed spacesintroduces significant fidelity loss, leading to errors in scenes with complex layouts or limited visibility [Tang et al. 2025a; Xue and Zhu 2025]. Real-time deployment remains another obstacle due to the high computational load and low inference speed of transformer-based VLMs [Gopalkrishnan et al. 2024; Yeh et al. 2024]. Further, domain shifts and prompt ambiguities introduce semantic hallucinations that jeopardize system reliability [Chen et al. 2024c; Xing et al. 2024]. These limitations underscore the necessity for more robust architectures, efficient annotation techniques, and stronger cross-modal reasoning. The following points outline the most pressing technical bottlenecks currently hindering the full potential of VLM-based 3D detection systems: Spatial Reasoning Limitations: VLMs struggle with 3D spatial interpretation [Chen et al. 2024d], often misidentifying relative positions and object orientations [Li et al. 2025; Meng et al. 2024]. Their depth understanding is limited, leading to inaccuracies in bounding box placement and segmentation [Ma et al. 2024; Sharma 2024]. Cross-Modal Misalignment: Projecting high-dimensional 3D features into language embedding spaces causes loss of geometric detail [Tang et al. 2025a; Xue and Zhu 2025]. This weakens performance on occluded or structurally complex scenes. High Annotation Overhead: VLMs demand richly annotated 3D-text datasets, which are expensive to produce. Manual alignment, as seen in models like OpenScene, hinders scalability [Huang et al. 2024a]. Limited Real-Time Viability: Transformer-heavy VLMs (e.g., Instruct3D) operate at low frame rates (815 FPS), contrasting with efficient voxel-based detectors that exceed 50 FPS in real-time scenarios [Gopalkrishnan et al. 2024; Yeh et al. 2024]. Vulnerability to Occlusion: Without depth priors, VLMs often fail to localize partially obscured objects [Sarch et al. 2024; Xing et al. 2025], whereas traditional LiDAR-based methods handle such cases more robustly [Zhang et al. 2024e,b]. Weak Domain Generalization: VLMs overfit to training domains and underperform under domain shifts (e.g., lighting, sensor variance), unlike more stable geometry-driven models [Addepalli et al. 2024; Vogt-Lowell et al. 2023]. Semantic Hallucinations: Prompt-driven VLMs can infer incorrect detections when text inputs are ambiguous [Xing et al. 2024], reducing reliability in safety-critical applications [Chen et al. 2024c; Ke et al. 2024]. Lack of Explicit 3D Structure: VLMs frequently omit explicit 3D modeling (e.g., NeRFs), resulting in multiview inconsistencies and poor spatial coherence across frames [Zhang et al. 2025; Zhao et al. 2024b]. ACM Trans. Graph., Vol. 37, No. 4, Article 111. Publication date: August 2025."
        },
        {
            "title": "4.0.2 Potential Solutions to overcome challenges in 3D Object Detec-\ntion with VLMs. Addressing the identified limitations in VLM-based\n3D object detection demands innovative strategies that combine\nadvances in spatial modeling, multimodal alignment, data efficiency,\nand computational optimization. Recent research efforts have ex-\nplored a variety of promising directions, such as integrating 3D scene\ngraphs and geometric priors for more accurate spatial reasoning\n[Cheng et al. 2024; Wang 2025], and introducing modality-specific\nencoders to improve cross-modal consistency [Song et al. 2025; Ye\net al. 2024]. Synthetic data pipelines, leveraging generative AI and\nlarge language models, are also emerging as scalable alternatives\nto manual annotation, drastically reducing overhead [Kabra et al.\n2023; Sapkota and Karkee 2025; Sapkota et al. 2024a,c,d; Sharifzadeh\net al. 2024]. Meanwhile, reinforcement learning-based methods and\nregion-aware planning modules show potential to bring VLM in-\nference closer to real-time performance requirements [Cheng et al.\n2024; Pan and Liu 2025]. To enable robust generalization, research\nhas also emphasized training on diverse and dynamic environments\nto counteract prompt drift and semantic ambiguity [Eskandar 2024;\nLehner et al. 2022; Sapkota et al. 2025a]. The following solutions\nhighlight concrete approaches that can significantly enhance the\nperformance and reliability of 3D object detection systems grounded\nin vision-language modeling.",
            "content": "Improving Spatial Reasoning: Integrate 3D scene graphs (e.g., SpatialRGPT) [Cheng et al. 2024] and depth-aware plugin such as RoboFlamingo-Plus [Wang 2025] to capture object relationships and spatial orientation. This enhances bounding box precision and depth disambiguation [Chen et al. 2024d]. Resolving Cross-Modal Misalignment: Use modality-specific encoders (CrossOver) to preserve structural integrity in shared embedding spaces [Wang et al. 2024c; Ye et al. 2024]. Multistage training with scene-level alignment maintains 3D detail across views [Lyu et al. 2024; Song et al. 2025]. Reducing Annotation Overhead: Employ synthetic data generation (e.g., Synth2) [Sharifzadeh et al. 2024], where LLMs generate captions and VQ-GAN synthesizes embeddings. This scalable pipeline bypasses manual 3D-text labeling [Kabra et al. 2023]. Boosting Real-Time Performance: Apply reinforcement learning (MetaSpatial) [Pan and Liu 2025] and adaptive batching guided by SpatialRGPTs region proposals to achieve 30+ FPS, aligning VLMs with real-time constraints [Cheng et al. 2024]. Handling Occlusions: Fuse metric depth priors into visual encoders, and utilize region-aware reward schemes to enable robust occlusion reasoning comparable to LiDAR-based models [Jiao et al. 2024; Wang et al. 2023]. Enhancing Domain Generalization: Train on diverse synthetic environments and LLM-generated QA (CrossOver), improving adaptability across lighting and sensor variations without retraining [Eskandar 2024; Lehner et al. 2022]. Reducing Semantic Hallucinations: Combine templatebased and LLM-generated QA with explicit region tags (SpatialRGPT) to anchor prompts [Cheng et al. 2024] and reduce ambiguous predictions [Chen et al. 2024c]. Embedding Explicit 3D Structure: Condition VLMs on 3D scene graphs and physics constraints (MetaSpatial) [Pan and Liu 2025], using multi-turn RL to enforce coherent multi-view object representations."
        },
        {
            "title": "5 Conclusion\nThis review systematically examined the trajectory and current\nlandscape of 3D object detection with Vision-Language Mod-\nels, offering a comprehensive and structured synthesis of their\nevolution, methodology, and application in multimodal perception.\nThrough a rigorous paper collection process combining both aca-\ndemic databases and modern AI search engines (Figure 4), we cu-\nrated 105 high-quality papers that span traditional approaches and\nstate-of-the-art VLM-based systems. The literature was carefully\nfiltered and evaluated based on methodological clarity, application\nscope, and relevance to robotics and embodied intelligence.",
            "content": "The synthesis began by mapping the evolution of 3D object detection, highlighting the progression from traditional geometric and point-based methods like PointNet++, PV-RCNN, and VoteNet to newer architectures that leverage VLMs. These earlier models established foundational practices in processing LiDAR data and point clouds but lacked the semantic abstraction required for reasoning in complex, unstructured environments. Our analysis showed how VLM-based approaches overcome these limitations by fusing visual and textual modalities to allow language-conditioned perception. This shift enables zero-shot reasoning, improved adaptability, and instruction-driven task completion, making VLMs particularly suitable for real-world robotic scenarios. We then delved into the technical foundations of these models, focusing on the pretraining and fine-tuning processes that bridge 2D vision-language datasets with 3D perception tasks. Notable advancements include the use of spatial tokenization, cross-modal transformers, and point-text alignment strategies. We also discussed how visualization frameworks in these models allow better interpretability of 3D bounding boxes through textual queries, enhancing transparency in model outputs. The core of our review addressed the comparative strengths and limitations of VLM-based versus traditional 3D detectors. We found that while traditional methods remain more efficient and interpretable in structured environments, VLMs significantly outperform them in open-vocabulary and dynamically changing contexts. Trade-off analyses revealed that the computational costs and annotation requirements of VLMs are major bottlenecks, yet these are actively being mitigated through synthetic data generation, model distillation, and RL-based policy training. In our final discussion, we outlined major current limitations, including weak spatial reasoning, poor real-time performance, semantic hallucinations, and domain generalization gaps. These issues were contextualized with proposed solutions like 3D scene graphs, lightweight transformers, and multimodal reward shaping, indicating strong direction for future improvements. The findings from this review highlight several key takeaways: (1) Semantic Evolution: The transition from geometry-only to multimodal systems marks critical paradigm shift. VLMs Review of 3D Object Detection with Vision-Language Models 111:19 enrich 3D object detection by enabling instruction-based and zero-shot understanding of spatial environments. (2) Technical Innovation: Architectural designs such as spatial reasoning modules (e.g., PAGE, Flan-QS), hyperbolic alignment losses (e.g., PSA), and pretraining-finetuning pipelines demonstrate the role of language grounding in enhancing 3D spatial cognition. (3) Performance Trade-offs: VLMs deliver state-of-the-art semantic accuracy and generalization, especially in unstructured environments, yet still lag behind traditional voxelbased systems in real-time inference speed (1520% lower FPS). (4) Challenges and Future Directions: Spatial hallucinations, misalignment, and high annotation costs remain prominent. Solutions leveraging 3D scene graph distillation, synthetic captioning, and reinforcement learning are promising, with several models like RoboFlamingo-Plus and MetaSpatial already exploring these techniques. (5) Deployment Readiness: With advancements in neuromorphic hardware and efficient cross-modal attention mechanisms, VLMs are increasingly viable for deployment in autonomous navigation, industrial robotics, and AR-based interaction systems. In conclusion, 3D object detection with vision-language models is redefining the landscape of spatial perception and multimodal reasoning. By integrating language, vision, and 3D geometry, these systems offer new capabilities for intelligent interaction with the physical world. This review not only maps the state-of-the-art but also charts course for future exploration, paving the way for robust, scalable, and interpretable VLM-based 3D perception systems that are foundational to next-generation robotics and AI. Acknowledgement This work was supported by the National Science Foundation and the United States Department of Agriculture, National Institute of Food and Agriculture through the Artificial Intelligence (AI) Institute for Agriculture Program under Award AWD003473, and AWD004595, Accession Number 1029004, \"Robotic Blossom Thinning with Soft Manipulators\". Declarations The authors declare no conflicts of interest. Statement on AI Writing Assistance ChatGPT and Perplexity were utilized to enhance grammatical accuracy and refine sentence structure; all AI-generated revisions were thoroughly reviewed and edited for relevance. Additionally, ChatGPT-4o was employed to generate realistic visualizations. References Sravanti Addepalli, Ashish Ramayee Asokan, Lakshay Sharma, and Venkatesh Babu. 2024. Leveraging vision-language models for improving domain generalization in image classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2392223932. Shivani Agarwal and Dan Roth. 2002. Learning sparse representation for object detection. In Computer VisionECCV 2002: 7th European Conference on Computer ACM Trans. Graph., Vol. 37, No. 4, Article 111. Publication date: August 2025. 111:20 Sapkota et al. 2025, Review of 3D Object Detection with VLMs Vision Copenhagen, Denmark, May 2831, 2002 Proceedings, Part IV 7. Springer, 113127. Simegnew Yihunie Alaba and John Ball. 2022. survey on deep-learning-based lidar 3d object detection for autonomous driving. Sensors 22, 24 (2022), 9577. Eduardo Arnold, Omar Al-Jarrah, Mehrdad Dianati, Saber Fallah, David Oxtoby, and Alex Mouzakitis. 2019. survey on 3d object detection methods for autonomous driving applications. IEEE Transactions on Intelligent Transportation Systems 20, 10 (2019), 37823795. Fan Bai, Yuxin Du, Tiejun Huang, Max Q-H Meng, and Bo Zhao. 2024. M3d: Advancing 3d medical image analysis with multi-modal large language models. arXiv preprint arXiv:2404.00578 (2024). Xuyang Bai, Zeyu Hu, Xinge Zhu, Qingqiu Huang, Yilun Chen, Hongbo Fu, and ChiewLan Tai. 2022. Transfusion: Robust lidar-camera fusion for 3d object detection with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 10901099. Garrick Brazil, Abhinav Kumar, Julian Straub, Nikhila Ravi, Justin Johnson, and Georgia Gkioxari. 2023. Omni3d: large benchmark and model for 3d object detection in the wild. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1315413164. Ali Caglayan and Ahmet Burak Can. 2018. Volumetric object recognition using 3-D CNNs on depth data. IEEE Access 6 (2018), 2005820066. Yang Cao, Zeng Yihan, Hang Xu, and Dan Xu. 2023. Coda: Collaborative novel box discovery and cross-modal alignment for open-vocabulary 3d object detection. Advances in Neural Information Processing Systems 36 (2023), 7186271873. Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. 2024d. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1445514465. Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David Fleet, and Geoffrey Hinton. 2022. unified sequence interface for vision tasks. Advances in Neural Information Processing Systems 35 (2022), 3133331346. Wei Chen, Yan Li, Zijian Tian, and Fan Zhang. 2023a. 2D and 3D object detection algorithms from images: Survey. Array 19 (2023), 100305. Wei Chen, Zhiyuan Li, and Shuo Xin. 2024a. OmniVLM: Token-Compressed, SubBillion-Parameter Vision-Language Model for Efficient On-Device Inference. arXiv preprint arXiv:2412.11475 (2024). Xinlei Chen and Abhinav Gupta. 2017. Spatial memory for context reasoning in object detection. In Proceedings of the IEEE international conference on computer vision. 40864096. Xiangyu Chen, Zhenzhen Liu, Katie Luo, Siddhartha Datta, Adhitya Polavaram, Yan Wang, Yurong You, Boyi Li, Marco Pavone, Wei-Lun Harry Chao, et al. 2024b. Diffubox: Refining 3d object detection with point diffusion. Advances in Neural Information Processing Systems 37 (2024), 103681103705. Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia. 2017. Multi-view 3d object detection network for autonomous driving. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. 19071915. Xuweiyi Chen, Ziqiao Ma, Xuejun Zhang, Sihan Xu, Shengyi Qian, Jianing Yang, David Fouhey, and Joyce Chai. 2024c. Multi-object hallucination in vision language models. Advances in Neural Information Processing Systems 37 (2024), 4439344418. Yukang Chen, Jianhui Liu, Xiangyu Zhang, Xiaojuan Qi, and Jiaya Jia. 2023b. Voxelnext: Fully sparse voxelnet for 3d object detection and tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2167421683. Yilun Chen, Shu Liu, Xiaoyong Shen, and Jiaya Jia. 2020. Dsgn: Deep stereo geometry network for 3d object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1253612545. An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. 2024. SpatialRGPT: Grounded Spatial Reasoning in Vision Language Models. arXiv preprint arXiv:2406.01584 (2024). Wenhao Cheng, Junbo Yin, Wei Li, Ruigang Yang, and Jianbing Shen. 2023. Languageguided 3d object detection in point cloud for autonomous driving. arXiv preprint arXiv:2305.15765 (2023). Adrian Chow, Evelien Riddell, Yimu Wang, Sean Sedwards, and Krzysztof Czarnecki. 2025. OV-SCAN: Semantically Consistent Alignment for Novel Object Discovery in Open-Vocabulary 3D Object Detection. arXiv preprint arXiv:2503.06435 (2025). Shengheng Deng, Zhihao Liang, Lin Sun, and Kui Jia. 2022. Vista: Boosting 3d object detection via dual cross-view spatial attention. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 84488457. Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. 2023. PaLM-E: An Embodied Multimodal Language Model. In International Conference on Machine Learning. PMLR, 84698488. Yu Du, Fangyun Wei, Zihe Zhang, Miaojing Shi, Yue Gao, and Guoqi Li. 2022. Learning to prompt for open-vocabulary object detection with vision-language model. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1408414093. ACM Trans. Graph., Vol. 37, No. 4, Article 111. Publication date: August 2025. George Eskandar. 2024. An empirical study of the generalization ability of lidar 3d object detectors to unseen domains. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2381523825. Djamahl Etchegaray, Zi Huang, Tatsuya Harada, and Yadan Luo. 2024. Find nPropagate: Open-Vocabulary 3D Object Detection in Urban Environments. In European Conference on Computer Vision. Springer, 133151. Lue Fan, Feng Wang, Naiyan Wang, and Zhao-Xiang Zhang. 2022. Fully sparse 3d object detection. Advances in Neural Information Processing Systems 35 (2022), 351363. Felix Fent, Andras Palffy, and Holger Caesar. 2024. Dpft: Dual perspective fusion transformer for camera-radar-based object detection. IEEE Transactions on Intelligent Vehicles (2024). Christian Fruhwirth-Reisinger, Wei Lin, Dušan Malić, Horst Bischof, and Horst Possegger. 2024. Vision-Language Guidance for LiDAR-based Unsupervised 3D Object Detection. arXiv preprint arXiv:2408.03790 (2024). Rao Fu, Jingyu Liu, Xilun Chen, Yixin Nie, and Wenhan Xiong. 2025. Scene-LLM: Extending Language Model for 3D Visual Reasoning. In 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). IEEE, 21952206. Yuhan Gao, Peng Wang, Xiaoyan Li, Bo Sun, Mengyu Sun, Liangliang Li, and Ruohai Di. 2025b. PillarFocusNet for 3D object detection with perceptual diffusion and key feature understanding. Scientific Reports 15, 1 (2025), 8776. Yuhan Gao, Peng Wang, Xiaoyan Li, Mengyu Sun, Ruohai Di, Liangliang Li, and Wei Hong. 2025a. MonoDFNet: Monocular 3D Object Detection with Depth Fusion and Adaptive Optimization. Sensors 25, 3 (2025), 760. Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. 2013. Vision meets robotics: The kitti dataset. The international journal of robotics research 32, 11 (2013), 12311237. Yalda Ghasemi, Heejin Jeong, Sung Ho Choi, Kyeong-Beom Park, and Jae Yeol Lee. 2022. Deep learning-based object detection in augmented reality: systematic review. Computers in Industry 139 (2022), 103661. Akshay Gopalkrishnan, Ross Greer, and Mohan Trivedi. 2024. Multi-frame, lightweight & efficient vision-language models for question answering in autonomous driving. arXiv preprint arXiv:2403.19838 (2024). Ross Greer, Bjørk Antoniussen, Andreas Møgelmose, and Mohan Trivedi. 2025. Language-driven active learning for diverse open-set 3d object detection. In Proceedings of the Winter Conference on Applications of Computer Vision. 980988. Runwei Guan, Jianan Liu, Ningwei Ouyang, Daizong Liu, Xiaolou Sun, Lianqing Zheng, Ming Xu, Yutao Yue, and Hui Xiong. 2025. Talk2PC: Enhancing 3D Visual Grounding through LiDAR and Radar Point Clouds Fusion for Autonomous Driving. arXiv preprint arXiv:2503.08336 (2025). Kun Guo and Qiang Ling. 2025. PromptDet: Lightweight 3D Object Detection Framework with LiDAR Prompts. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 39. 32663274. Qingdong He, Zhengning Wang, Hao Zeng, Yi Zeng, and Yijun Liu. 2022. Svga-net: Sparse voxel-graph attention network for 3d object detection from point clouds. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36. 870878. Deepti Hegde, Jeya Maria Jose Valanarasu, and Vishal Patel. 2023. Clip goes 3d: Leveraging prompt tuning for language grounded 3d recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 20282038. Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 2023. 3d-llm: Injecting the 3d world into large language models. Advances in Neural Information Processing Systems 36 (2023), 2048220494. Chunyong Hu, Hang Zheng, Kun Li, Jianyun Xu, Weibo Mao, Maochun Luo, Lingxuan Wang, Mingxia Chen, Qihao Peng, Kaixuan Liu, et al. 2023. FusionFormer: Multisensory Fusion in Birds-Eye-View and Temporal Consistent Transformer for 3D Object Detection. arXiv preprint arXiv:2309.05257 (2023). Haifeng Huang, Yilun Chen, Zehan Wang, Rongjie Huang, Runsen Xu, Tai Wang, Luping Liu, Xize Cheng, Yang Zhao, Jiangmiao Pang, et al. 2024a. Chat-scene: Bridging 3d scene and large language models with object identifiers. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Junjie Huang, Guan Huang, Zheng Zhu, Yun Ye, and Dalong Du. 2021. Bevdet: Highperformance multi-camera 3d object detection in bird-eye-view. arXiv preprint arXiv:2112.11790 (2021). Jiaxing Huang, Jingyi Zhang, Kai Jiang, Han Qiu, and Shijian Lu. 2023b. Visual instruction tuning towards general-purpose multimodal model: survey. arXiv preprint arXiv:2312.16602 (2023). Linyan Huang, Zhiqi Li, Chonghao Sima, Wenhai Wang, Jingdong Wang, Yu Qiao, and Hongyang Li. 2023a. Leveraging vision-centric multi-modal expertise for 3d object detection. Advances in Neural Information Processing Systems 36 (2023), 3850438519. Rui Huang, Henry Zheng, Yan Wang, Zhuofan Xia, Marco Pavone, and Gao Huang. 2024b. Training an Open-Vocabulary Monocular 3D Object Detection Model without 3D Data. arXiv preprint arXiv:2411.15657 (2024). Tengteng Huang, Zhe Liu, Xiwu Chen, and Xiang Bai. 2020. Epnet: Enhancing point features with image semantics for 3d object detection. In Computer visionECCV 2020: 16th European conference, Glasgow, UK, August 2328, 2020, proceedings, part XV 16. Springer, 3552. Jitesh Jain, Jianwei Yang, and Humphrey Shi. 2024. Vcoder: Versatile vision encoders for multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2799228002. Maximilian Jaritz, Tuan-Hung Vu, Raoul De Charette, Émilie Wirbel, and Patrick Pérez. 2022. Cross-modal learning for domain adaptation in 3d semantic segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence 45, 2 (2022), 1533 1544. Pengkun Jiao, Na Zhao, Jingjing Chen, and Yu-Gang Jiang. 2024. Unlocking textual and visual wisdom: Open-vocabulary 3d object detection enhanced by comprehensive guidance from text and image. In European Conference on Computer Vision. Springer, 376392. Lu Jin, Shenghua Gao, Zechao Li, and Jinhui Tang. 2014. Hand-crafted features or machine learnt features? together they improve RGB-D object recognition. In 2014 IEEE International Symposium on Multimedia. IEEE, 311319. Rishabh Kabra, Loic Matthey, Alexander Lerchner, and Niloy Mitra. 2023. Leveraging vlm-based pipelines to annotate 3d objects. arXiv preprint arXiv:2311.17851 (2023). Hiromichi Kamata, Yuiko Sakuma, Akio Hayakawa, Masato Ishii, and Takuya Narihira. 2023. Instruct 3d-to-3d: Text instruction guided 3d-to-3d conversion. arXiv preprint arXiv:2303.15780 (2023). Junjie Ke, Lihuo He, Bo Han, Jie Li, Di Wang, and Xinbo Gao. 2024. VLDadaptor: Domain Adaptive Object Detection With Vision-Language Model Distillation. IEEE Transactions on Multimedia (2024). Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak Shah. 2022. Transformers in vision: survey. ACM computing surveys (CSUR) 54, 10s (2022), 141. Maksim Kolodiazhnyi, Anna Vorontsova, Matvey Skripkin, Danila Rukhovich, and Anton Konushin. 2025. Unidet3d: Multi-dataset indoor 3d object detection. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 39. 43654373. Jason Ku, Melissa Mozifian, Jungwook Lee, Ali Harakeh, and Steven Waslander. 2018. Joint 3d proposal generation and object detection from view aggregation. In 2018 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE, 18. Jeho Lee, Chanyoung Jung, Jiwon Kim, and Hojung Cha. 2024. Panopticus: Omnidirectional 3D Object Detection on Resource-constrained Edge Devices. In Proceedings of the 30th Annual International Conference on Mobile Computing and Networking. 12071221. Alexander Lehner, Stefano Gasperini, Alvaro Marcos-Ramiro, Michael Schmidt, Mohammad-Ali Nikouei Mahani, Nassir Navab, Benjamin Busam, and Federico Tombari. 2022. 3d-vfield: Adversarial augmentation of point clouds for domain generalization in 3d object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1729517304. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023a. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning. PMLR, 1973019742. Jianheng Li, Bin Pan, Evgeny Cherkashin, Linke Liu, Zhenyu Sun, Manlin Zhang, and Qinqin Li. 2021a. 3D point cloud multi-target detection method based on PointNet++. In The 10th International Conference on Computer Engineering and Networks. Springer, 12791290. Ruihui Li, Xianzhi Li, Pheng-Ann Heng, and Chi-Wing Fu. 2020. Pointaugment: an auto-augmentation framework for point cloud classification. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 63786387. Songtao Li and Hao Tang. 2024. Multimodal Alignment and Fusion: Survey. arXiv preprint arXiv:2411.17040 (2024). Xiang Li, Jian Ding, Zhaoyang Chen, and Mohamed Elhoseiny. 2024a. Uni3DL: unified model for 3D vision-language understanding. In European Conference on Computer Vision. Springer, 7492. Xiaotian Li, Baojie Fan, Jiandong Tian, and Huijie Fan. 2024b. Gafusion: Adaptive fusing lidar and camera with multiple guidance for 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2120921218. Yanwei Li, Yilun Chen, Xiaojuan Qi, Zeming Li, Jian Sun, and Jiaya Jia. 2022a. Unifying voxel-based representation with transformer for 3d object detection. Advances in Neural Information Processing Systems 35 (2022), 1844218455. Yong Li, Naipeng Miao, Liangdi Ma, Feng Shuang, and Xingwen Huang. 2023b. Transformer for object detection: Review and benchmark. Engineering Applications of Artificial Intelligence 126 (2023), 107021. Yingwei Li, Adams Wei Yu, Tianjian Meng, Ben Caine, Jiquan Ngiam, Daiyi Peng, Junyang Shen, Yifeng Lu, Denny Zhou, Quoc Le, et al. 2022b. Deepfusion: Lidar-camera deep fusion for multi-modal 3d object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1718217191. Zhichao Li, Feng Wang, and Naiyan Wang. 2021b. Lidar r-cnn: An efficient and universal 3d object detector. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 75467555. Zongxia Li, Xiyang Wu, Hongyang Du, Huy Nghiem, and Guangyao Shi. 2025. Benchmark evaluations, applications, and challenges of large vision language models: survey. arXiv preprint arXiv:2501.02189 (2025). Hou-I Liu, Christine Wu, Jen-Hao Cheng, Wenhao Chai, Shian-Yun Wang, Gaowen Liu, Jenq-Neng Hwang, Hong-Han Shuai, and Wen-Huang Cheng. 2024. MonoTAKD: Review of 3D Object Detection with Vision-Language Models 111:21 Teaching assistant knowledge distillation for monocular 3D object detection. arXiv preprint arXiv:2404.04910 (2024). Minghua Liu, Ruoxi Shi, Kaiming Kuang, Yinhao Zhu, Xuanlin Li, Shizhong Han, Hong Cai, Fatih Porikli, and Hao Su. 2023. Openshape: Scaling up 3d shape representation towards open-world understanding. Advances in neural information processing systems 36 (2023), 4486044879. Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, ChengYang Fu, and Alexander Berg. 2016. Ssd: Single shot multibox detector. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part 14. Springer, 2137. Bin Lu, Yang Sun, and Zhenyu Yang. 2023. Voxel graph attention for 3-D object detection from point clouds. IEEE Transactions on Instrumentation and Measurement 72 (2023), 112. Sheng Luo, Wei Chen, Wanxin Tian, Rui Liu, Luanxuan Hou, Xiubao Zhang, Haifeng Shen, Ruiqi Wu, Shuyi Geng, Yi Zhou, et al. 2024. Delving into multi-modal multitask foundation models for road scene understanding: From learning paradigm perspectives. IEEE Transactions on Intelligent Vehicles (2024). Ruiyuan Lyu, Jingli Lin, Tai Wang, Xiaohan Mao, Yilun Chen, Runsen Xu, Haifeng Huang, Chenming Zhu, Dahua Lin, and Jiangmiao Pang. 2024. Mmscan: multimodal 3d scene dataset with hierarchical grounded language annotations. Advances in Neural Information Processing Systems 37 (2024), 5089850924. Xianzheng Ma, Yash Bhalgat, Brandon Smart, Shuai Chen, Xinghui Li, Jian Ding, Jindong Gu, Dave Zhenyu Chen, Songyou Peng, Jia-Wang Bian, et al. 2024. When llms step into the 3d world: survey and meta-analysis of 3d tasks via multi-modal large language models. arXiv preprint arXiv:2405.10255 (2024). Jiageng Mao, Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. 2023. 3D object detection for autonomous driving: comprehensive survey. International Journal of Computer Vision 131, 8 (2023), 19091963. Qinghao Meng, Wenguan Wang, Tianfei Zhou, Jianbing Shen, Yunde Jia, and Luc Van Gool. 2021. Towards weakly supervised framework for 3D point cloud object detection and annotation. IEEE Transactions on Pattern Analysis and Machine Intelligence 44, 8 (2021), 44544468. Zaiqiao Meng, Hao Zhou, and Yifang Chen. 2024. Know About\" Up\"! Enhancing Spatial Reasoning in Visual Language Models Through 3D Reconstruction. arXiv preprint arXiv:2407.14133 (2024). Mahyar Najibi, Jingwei Ji, Yin Zhou, Charles Qi, Xinchen Yan, Scott Ettinger, and Dragomir Anguelov. 2023. Unsupervised 3d perception with 2d vision-language distillation for autonomous driving. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 86028612. Jongyoun Noh, Sanghoon Lee, and Bumsub Ham. 2021. Hvpr: Hybrid voxel-point representation for single-stage 3d object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1460514614. Keiron Oshea and Ryan Nash. 2015. An introduction to convolutional neural networks. arXiv preprint arXiv:1511.08458 (2015). Zhenyu Pan and Han Liu. 2025. MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse. arXiv preprint arXiv:2503.18470 (2025). Constantine Papageorgiou, Michael Oren, and Tomaso Poggio. 1998. general framework for object detection. In Sixth international conference on computer vision (IEEE Cat. No. 98CH36271). IEEE, 555562. Songyou Peng, Kyle Genova, Chiyu Jiang, Andrea Tagliasacchi, Marc Pollefeys, Thomas Funkhouser, et al. 2023. Openscene: 3d scene understanding with open vocabularies. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 815824. Xingchao Peng, Baochen Sun, Karim Ali, and Kate Saenko. 2015. Learning deep object detectors from 3d models. In Proceedings of the IEEE international conference on computer vision. 12781286. Jens Piekenbrinck, Alexander Hermans, Narunas Vaskevicius, Timm Linder, and Bastian Leibe. 2024. Rgb-d cube r-cnn: 3d object detection with selective modality dropout. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 19972006. Charles Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas Guibas. 2018. Frustum pointnets for 3d object detection from rgb-d data. In Proceedings of the IEEE conference on computer vision and pattern recognition. 918927. Charles Qi, Hao Su, Kaichun Mo, and Leonidas Guibas. 2017. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition. 652660. Rui Qian, Xin Lai, and Xirong Li. 2022. 3D object detection for autonomous driving: survey. Pattern Recognition 130 (2022), 108796. Shaina Raza, Ashmal Vayani, Aditya Jain, Aravind Narayanan, Vahid Reza Khazaie, Syed Raza Bashir, Elham Dolatabadi, Gias Uddin, Christos Emmanouilidis, Rizwan Qureshi, et al. 2025. Vldbench: Vision language models disinformation detection benchmark. arXiv preprint arXiv:2502.11361 (2025). Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. 2016. You only look once: Unified, real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition. 779788. ACM Trans. Graph., Vol. 37, No. 4, Article 111. Publication date: August 2025. 111: Sapkota et al. 2025, Review of 3D Object Detection with VLMs Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2016. Faster R-CNN: Towards IEEE transactions on real-time object detection with region proposal networks. pattern analysis and machine intelligence 39, 6 (2016), 11371149. David Rozenberszki, Or Litany, and Angela Dai. 2022. Language-grounded indoor 3d semantic segmentation in the wild. In European Conference on Computer Vision. Springer, 125141. Ranjan Sapkota and Manoj Karkee. 2024. Comparing YOLOv11 and YOLOv8 for instance segmentation of occluded and non-occluded immature green fruits in complex orchard environment. arXiv preprint arXiv:2410.19869 (2024). Ranjan Sapkota and Manoj Karkee. 2025. Improved yolov12 with llm-generated synthetic data for enhanced apple detection and benchmarking against yolov11 and yolov10. arXiv preprint arXiv:2503.00057 (2025). Ranjan Sapkota, Zhichao Meng, and Manoj Karkee. 2024a. Synthetic meets authentic: Leveraging llm generated datasets for yolo11 and yolov10-based apple detection through machine vision sensors. Smart Agricultural Technology 9 (2024), 100614. Ranjan Sapkota, Achyut Paudel, and Manoj Karkee. 2024b. Zero-shot automatic annotation and instance segmentation using llm-generated datasets: Eliminating field imaging and manual annotation for deep learning model development. arXiv preprint arXiv:2411.11285 (2024). Ranjan Sapkota, Rizwan Qureshi, Marco Flores Calero, Chetan Badjugar, Upesh Nepal, Alwin Poulose, Peter Zeno, Uday Bhanu Prakash Vaddevolu, Sheheryar Khan, Maged Shoman, et al. 2024c. YOLOv10 to its genesis: decadal and comprehensive review of the you only look once (YOLO) series. arXiv preprint arXiv:2406.19407 (2024). Ranjan Sapkota, Rizwan Qureshi, Marco Flores Calero, Chetan Badjugar, Upesh Nepal, Alwin Poulose, Peter Zeno, Uday Bhanu Prakash Vaddevolu, Sheheryar Khan, Maged Shoman, et al. 2024d. YOLOv10 to its genesis: decadal and comprehensive review of the you only look once (YOLO) series. arXiv preprint arXiv:2406.19407 (2024). Ranjan Sapkota, Shaina Raza, and Manoj Karkee. 2025a. Comprehensive analysis of transparency and accessibility of chatgpt, deepseek, and other sota large language models. arXiv preprint arXiv:2502.18505 (2025). RANJAN Sapkota, SHAINA Raza, MAGED Shoman, Paudel, and Karkee. 2025b. Multimodal large language models for image, text, and speech data augmentation: survey. arXiv preprint arXiv:2501.18648 (2025). Ranjan Sapkotaa and Manoj Karkeea. 2025. Object Detection with MultiTechRxiv. DOI: modal Large Vision-Language Models: An In-depth Review. 10.36227/techrxiv.174559706.69198342/v1 (2025). Gabriel Sarch, Lawrence Jang, Michael Tarr, William Cohen, Kenneth Marino, and Katerina Fragkiadaki. 2024. Vlm agents generate their own memories: Distilling experience into embodied programs of thought. Advances in Neural Information Processing Systems 37 (2024), 7594275985. Sahand Sharifzadeh, Christos Kaplanis, Shreya Pathak, Dharshan Kumaran, Anastasija Ilic, Jovana Mitrovic, Charles Blundell, and Andrea Banino. 2024. Synth2: Boosting Visual-Language Models with Synthetic Captions and Image Embeddings. arXiv preprint arXiv:2403.07750 (2024). Aditya Sharma. 2024. Advancing AI Understanding in Language & Vision. University of California, Santa Barbara. Shivanand Venkanna Sheshappanavar and Chandra Kambhamettu. 2020. novel local geometry capture in pointnet++ for 3d classification. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops. 262263. Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. 2020. Pv-rcnn: Point-voxel feature set abstraction for 3d object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1052910538. Yining Shi, Jingyan Shen, Yifan Sun, Yunlong Wang, Jiaxin Li, Shiqi Sun, Kun Jiang, and Diange Yang. 2022. Srcn3d: Sparse r-cnn 3d for compact convolutional multi-view 3d object detection and tracking. arXiv preprint arXiv:2206.14451 (2022). Shreyas Shivakumar, Ty Nguyen, Ian Miller, Steven Chen, Vijay Kumar, and Camillo Taylor. 2019. Dfusenet: Deep fusion of rgb and sparse depth information for image guided dense depth completion. In 2019 IEEE Intelligent Transportation Systems Conference (ITSC). IEEE, 1320. Vishwanath Sindagi, Yin Zhou, and Oncel Tuzel. 2019. Mvx-net: Multimodal voxelnet for 3d object detection. In 2019 International Conference on Robotics and Automation (ICRA). IEEE, 72767282. Thomas Sølund, Anders Glent Buch, Norbert Krüger, and Henrik Aanæs. 2016. large-scale 3D object recognition dataset. In 2016 Fourth International Conference on 3D Vision (3DV). IEEE, 7382. Hyeongseok Son, Jia He, Seung-In Park, Ying Min, Yunhao Zhang, and ByungIn Yoo. 2025. SparseVoxFormer: Sparse Voxel-based Transformer for Multi-modal 3D Object Detection. arXiv preprint arXiv:2503.08092 (2025). Shezheng Song, Xiaopeng Li, Shasha Li, Shan Zhao, Jie Yu, Jun Ma, Xiaoguang Mao, Weimin Zhang, and Meng Wang. 2025. How to Bridge the Gap between Modalities: Survey on Multimodal Large Language Model. IEEE Transactions on Knowledge and Data Engineering (2025). Shuran Song, Samuel Lichtenberg, and Jianxiong Xiao. 2015. Sun rgb-d: rgb-d scene understanding benchmark suite. In Proceedings of the IEEE conference on computer vision and pattern recognition. 567576. ACM Trans. Graph., Vol. 37, No. 4, Article 111. Publication date: August 2025. Ziying Song, Guoxin Zhang, Jun Xie, Lin Liu, Caiyan Jia, Shaoqing Xu, and Zhepeng Wang. 2023. VoxelNextFusion: Simple, Unified, and Effective Voxel Fusion Framework for Multimodal 3-D Object Detection. IEEE Transactions on Geoscience and Remote Sensing 61 (2023), 112. doi:10.1109/tgrs.2023.3331893 Ayça Takmaz, Elisabetta Fedele, Robert Sumner, Marc Pollefeys, Federico Tombari, and Francis Engelmann. 2023. Openmask3d: Open-vocabulary 3d instance segmentation. arXiv preprint arXiv:2306.13631 (2023). Guoqin Tang, Qingxuan Jia, Zeyuan Huang, Gang Chen, Ning Ji, and Zhipeng Yao. 2025b. 3D-Grounded Vision-Language Framework for Robotic Task Planning: Automated Prompt Synthesis and Supervised Reasoning. arXiv preprint arXiv:2502.08903 (2025). Yiwen Tang, Zoey Guo, Zhuhao Wang, Ray Zhang, Qizhi Chen, Junli Liu, Delin Qu, Zhigang Wang, Dong Wang, Xuelong Li, et al. 2025a. Exploring the Potential of Encoder-free Architectures in 3D LMMs. arXiv preprint arXiv:2502.09620 (2025). Geoffrey Taylor and Lindsay Kleeman. 2008. Visual perception and robotic manipulation: 3D object recognition, tracking and hand-eye coordination. Vol. 26. Springer. Yu Tian, Tianqi Shao, Tsukasa Demizu, Xuyang Wu, and Hsin-Tai Wu. 2024. Hpecogvlm: New head pose grounding task exploration on vision language model. arXiv preprint arXiv:2406.01914 (2024). Jonathan Tremblay, Thang To, and Stan Birchfield. 2018. Falling things: synthetic dataset for 3d object detection and pose estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops. 20382041. Kevin Vogt-Lowell, Noah Lee, Theodoros Tsiligkaridis, and Marc Vaillant. 2023. Robust fine-tuning of vision-language models for domain generalization. In 2023 IEEE High Performance Extreme Computing Conference (HPEC). IEEE, 17. Sourabh Vora, Alex Lang, Bassam Helou, and Oscar Beijbom. 2020. Pointpainting: Sequential fusion for 3d object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 46044612. Junyin Wang, Chenghu Du, Hui Li, and Shengwu Xiong. 2023. DLFusion: PaintingDepth Augmenting-LiDAR for Multimodal Fusion 3D Object Detection. In Proceedings of the 31st ACM International Conference on Multimedia. 37653776. Junhong Wang, Yun Li, Zhaoyu Zhou, Chengshun Wang, Yijie Hou, Li Zhang, Xiangyang Xue, Michael Kamp, Xiaolong Luke Zhang, and Siming Chen. 2022a. When, where and how does it fail? spatial-temporal visual analytics approach for interpretable object detection in autonomous driving. IEEE Transactions on Visualization and Computer Graphics 29, 12 (2022), 50335049. Jiahao Wang, Fang Liu, Licheng Jiao, Hao Wang, Shuo Li, Lingling Li, Puhua Chen, and Xu Liu. 2024c. Multi-modal visual tracking based on textual generation. Information Fusion 112 (2024), 102531. Ke Wang, Tianqiang Zhou, Xingcan Li, and Fan Ren. 2022c. Performance and challenges of 3D object detection methods in complex scenes for autonomous driving. IEEE Transactions on Intelligent Vehicles 8, 2 (2022), 16991716. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. 2024a. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191 (2024). Sheng Wang. 2025. RoboFlamingo-Plus: Fusion of Depth and RGB Perception with Vision-Language Models for Enhanced Robotic Manipulation. arXiv preprint arXiv:2503.19510 (2025). Tai Wang, Jiangmiao Pang, and Dahua Lin. 2022b. Monocular 3d object detection with depth from motion. In European Conference on Computer Vision. Springer, 386403. Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Song XiXuan, et al. 2024d. Cogvlm: Visual expert for pretrained language models. Advances in Neural Information Processing Systems 37 (2024), 121475121499. Yuan Wang, Yali Li, and Shengjin Wang. 2024b. G3-LQ: Marrying Hyperbolic Alignment with Explicit Semantic-Geometric Modeling for 3D Visual Grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1391713926. Yangfan Wang, Chen Wang, Peng Long, Yuzong Gu, and Wenfa Li. 2021. Recent advances in 3D object detection based on RGB-D: survey. Displays 70 (2021), 102077. Xiaopei Wu, Liang Peng, Honghui Yang, Liang Xie, Chenxi Huang, Chengqi Deng, Haifeng Liu, and Deng Cai. 2022. Sparse fuse dense: Towards high quality 3d detection with depth completion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 54185427. Yan Xia, Letian Shi, Zifeng Ding, Joao Henriques, and Daniel Cremers. 2024. Text2loc: 3d point cloud localization from natural language. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1495814967. Yu Xiang, Roozbeh Mottaghi, and Silvio Savarese. 2014. Beyond pascal: benchmark for 3d object detection in the wild. In IEEE winter conference on applications of computer vision. IEEE, 7582. Jialu Xing, Jianping Liu, Jian Wang, Lulu Sun, Xi Chen, Xunxun Gu, and Yingfei Wang. 2024. survey of efficient fine-tuning methods for vision-language modelsprompt and adapter. Computers & Graphics 119 (2024), 103885. Wenpeng Xing, Minghao Li, Mohan Li, and Meng Han. 2025. Towards Robust and Secure Embodied AI: Survey on Vulnerabilities and Attacks. arXiv preprint arXiv:2502.13175 (2025). Chenfeng Xu, Huan Ling, Sanja Fidler, and Or Litany. 2024. 3difftection: 3d object detection with geometry-aware diffusion features. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1061710627. Danfei Xu, Dragomir Anguelov, and Ashesh Jain. 2018. Pointfusion: Deep sensor fusion for 3d bounding box estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition. 244253. Shaoqing Xu, Dingfu Zhou, Jin Fang, Junbo Yin, Zhou Bin, and Liangjun Zhang. 2021. Fusionpainting: Multimodal fusion with adaptive attention for 3d object detection. In 2021 IEEE International Intelligent Transportation Systems Conference (ITSC). IEEE, 30473054. Le Xue, Mingfei Gao, Chen Xing, Roberto Martín-Martín, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, and Silvio Savarese. 2023. Ulip: Learning unified representation of language, images, and point clouds for 3d understanding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 11791189. Xizhe Xue and Xiao Xiang Zhu. 2025. Regression in EO: Are VLMs Up to the Challenge? arXiv preprint arXiv:2502.14088 (2025). Junjie Yan, Yingfei Liu, Jianjian Sun, Fan Jia, Shuailin Li, Tiancai Wang, and Xiangyu Zhang. 2023. Cross modal transformer: Towards fast and robust 3d object detection. In Proceedings of the IEEE/CVF international conference on computer vision. 18268 18278. Yan Yan, Yuxing Mao, and Bo Li. 2018. Second: Sparsely embedded convolutional detection. Sensors 18, 10 (2018), 3337. Zeyu Yang, Jiaqi Chen, Zhenwei Miao, Wei Li, Xiatian Zhu, and Li Zhang. 2022. Deepinteraction: 3d object detection via modality interaction. Advances in Neural Information Processing Systems 35 (2022), 19922005. Hanrong Ye, De-An Huang, Yao Lu, Zhiding Yu, Wei Ping, Andrew Tao, Jan Kautz, Song Han, Dan Xu, Pavlo Molchanov, et al. 2024. X-vila: Cross-modality alignment for large language model. arXiv preprint arXiv:2405.19335 (2024). Javier Yebes, Luis Bergasa, and Miguel Ángel García-Garrido. 2015. Visual object recognition with 3D-aware features in KITTI urban scenes. Sensors 15, 4 (2015), 92289250. Chen Yeh, You-Ming Chang, Wei-Chen Chiu, and Ning Yu. 2024. T2Vs Meet VLMs: Scalable Multimodal Dataset for Visual Harmfulness Recognition. Advances in Neural Information Processing Systems 37 (2024), 112950112961. Nir Yellinek, Leonid Karlinsky, and Raja Giryes. 2025. 3VL: Using Trees to Improve Vision-Language Models Interpretability. IEEE Transactions on Image Processing (2025). Junbo Yin, Jianbing Shen, Runnan Chen, Wei Li, Ruigang Yang, Pascal Frossard, and Wenguan Wang. 2024. Is-fusion: Instance-scene collaborative fusion for multimodal 3d object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1490514915. Jin Hyeok Yoo, Yecheol Kim, Jisong Kim, and Jun Won Choi. 2020. 3d-cvf: Generating joint camera and lidar features using cross-view spatial feature fusion for 3d object detection. In Computer visionECCV 2020: 16th European conference, Glasgow, UK, August 2328, 2020, proceedings, part XXVII 16. Springer, 720736. Di Yuan, Haiping Zhang, Xiu Shu, Qiao Liu, Xiaojun Chang, Zhenyu He, and Guangming Shi. 2023. An attention mechanism based AVOD network for 3D vehicle detection. IEEE Transactions on Intelligent Vehicles (2023). Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, and Chen Change Loy. 2025. Contextual object detection with multimodal large language models. International Journal of Computer Vision 133, 2 (2025), 825843. Bo Zhang, Jiakang Yuan, Botian Shi, Tao Chen, Yikang Li, and Yu Qiao. 2023. Uni3d: unified baseline for multi-dataset 3d object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 92539262. Hu Zhang, Jianhua Xu, Tao Tang, Haiyang Sun, Xin Yu, Zi Huang, and Kaicheng Yu. 2024e. Opensight: simple open-vocabulary framework for lidar-based object detection. In European Conference on Computer Vision. Springer, 119. Haochen Zhang, Nader Zantout, Pujith Kachana, Zongyuan Wu, Ji Zhang, and Wenshan Wang. 2024f. VLA-3D: dataset for 3D semantic scene understanding and navigation. arXiv preprint arXiv:2411.03540 (2024). Jixian Zhang. 2010. Multi-source remote sensing data fusion: status and trends. International journal of image and data fusion 1, 1 (2010), 524. Jianshu Zhang, Dongyu Yao, Renjie Pi, Paul Pu Liang, and Yi Fung. 2025. VLM2-Bench: Closer Look at How Well VLMs Implicitly Link Explicit Matching Visual Cues. arXiv preprint arXiv:2502.12084 (2025). Sha Zhang, Di Huang, Jiajun Deng, Shixiang Tang, Wanli Ouyang, Tong He, and Yanyong Zhang. 2024c. Agent3d-zero: An agent for zero-shot 3d understanding. In European Conference on Computer Vision. Springer, 186202. Taolin Zhang, Sunan He, Tao Dai, Zhi Wang, Bin Chen, and Shu-Tao Xia. 2024a. Visionlanguage pre-training with object contrastive learning for 3d scene understanding. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 72967304. Yifan Zhang, Junhui Hou, and Yixuan Yuan. 2024b. comprehensive study of the robustness for lidar-based 3d object detectors against adversarial attacks. International Journal of Computer Vision 132, 5 (2024), 15921624. Review of 3D Object Detection with Vision-Language Models 111:23 Yunpeng Zhang, Jiwen Lu, and Jie Zhou. 2021. Objects are different: Flexible monocular 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 32893298. Yuqi Zhang, Han Luo, and Yinjie Lei. 2024d. Towards CLIP-driven Language-free 3D Visual Grounding via 2D-3D Relational Enhancement and Consistency. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1306313072. Haocheng Zhao, Runwei Guan, Taoyu Wu, Ka Lok Man, Limin Yu, and Yutao Yue. 2024a. Unibevfusion: Unified radar-vision bevfusion for 3d object detection. arXiv preprint arXiv:2409.14751 (2024). Taijin Zhao, Heqian Qiu, Yu Dai, Lanxiao Wang, Hefei Mei, Fanman Meng, Qingbo Wu, and Hongliang Li. 2024b. VLM-guided Explicit-Implicit Complementary novel class semantic learning for few-shot object detection. Expert Systems with Applications 256 (2024), 124926. Ziyu Zhao, Zhenyao Wu, Xinyi Wu, Canyu Zhang, and Song Wang. 2022. Crossmodal few-shot 3d point cloud semantic segmentation. In Proceedings of the 30th ACM international conference on multimedia. 47604768. Yin Zhou, Pei Sun, Yu Zhang, Dragomir Anguelov, Jiyang Gao, Tom Ouyang, James Guo, Jiquan Ngiam, and Vijay Vasudevan. 2020. End-to-end multi-view fusion for 3d object detection in lidar point clouds. In Conference on Robot Learning. PMLR, 923932. Yin Zhou and Oncel Tuzel. 2018. Voxelnet: End-to-end learning for point cloud based 3d object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition. 44904499. Zixiang Zhou, Dongqiangzi Ye, Weijia Chen, Yufei Xie, Yu Wang, Panqu Wang, and Hassan Foroosh. 2024. Lidarformer: unified transformer-based multi-task network for lidar perception. In 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 1474014747. Chenming Zhu, Tai Wang, Wenwei Zhang, Jiangmiao Pang, and Xihui Liu. 2024. Llava3d: simple yet effective pathway to empowering lmms with 3d-awareness. arXiv preprint arXiv:2409.18125 (2024). Hanqi Zhu, Jiajun Deng, Yu Zhang, Jianmin Ji, Qiuyu Mao, Houqiang Li, and Yanyong Zhang. 2022. VPFNet: Improving 3D object detection with virtual point based LiDAR and stereo data fusion. IEEE Transactions on Multimedia 25 (2022), 52915304. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. 2025. InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models. arXiv preprint arXiv:2504.10479 (2025). Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyu Guo, Ziyao Zeng, Zipeng Qin, Shanghang Zhang, and Peng Gao. 2023. Pointclip v2: Prompting clip and gpt for powerful 3d open-world learning. In Proceedings of the IEEE/CVF international conference on computer vision. 26392650. Zhengxia Zou, Keyan Chen, Zhenwei Shi, Yuhong Guo, and Jieping Ye. 2023. Object detection in 20 years: survey. Proc. IEEE 111, 3 (2023), 257276. ACM Trans. Graph., Vol. 37, No. 4, Article 111. Publication date: August 2025."
        }
    ],
    "affiliations": [
        "Cornell University, USA",
        "Kansas State University, USA",
        "Universidad de las Fuerzas Armadas, Ecuador",
        "University of Peloponnese, Greece"
    ]
}