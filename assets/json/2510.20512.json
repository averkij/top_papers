{
    "paper_title": "EchoDistill: Bidirectional Concept Distillation for One-Step Diffusion Personalization",
    "authors": [
        "Yixiong Yang",
        "Tao Wu",
        "Senmao Li",
        "Shiqi Yang",
        "Yaxing Wang",
        "Joost van de Weijer",
        "Kai Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in accelerating text-to-image (T2I) diffusion models have enabled the synthesis of high-fidelity images even in a single step. However, personalizing these models to incorporate novel concepts remains a challenge due to the limited capacity of one-step models to capture new concept distributions effectively. We propose a bidirectional concept distillation framework, EchoDistill, to enable one-step diffusion personalization (1-SDP). Our approach involves an end-to-end training process where a multi-step diffusion model (teacher) and a one-step diffusion model (student) are trained simultaneously. The concept is first distilled from the teacher model to the student, and then echoed back from the student to the teacher. During the EchoDistill, we share the text encoder between the two models to ensure consistent semantic understanding. Following this, the student model is optimized with adversarial losses to align with the real image distribution and with alignment losses to maintain consistency with the teacher's output. Furthermore, we introduce the bidirectional echoing refinement strategy, wherein the student model leverages its faster generation capability to feedback to the teacher model. This bidirectional concept distillation mechanism not only enhances the student ability to personalize novel concepts but also improves the generative quality of the teacher model. Our experiments demonstrate that this collaborative framework significantly outperforms existing personalization methods over the 1-SDP setup, establishing a novel paradigm for rapid and effective personalization in T2I diffusion models."
        },
        {
            "title": "Start",
            "content": "EchoDistill: Bidirectional Concept Distillation for One-Step Diffusion Personalization Shiqi Yang3, Yixiong Yang1, , Yaxing Wang3, Joost van de Weijer2, Kai Wang4,5,2, , , Tao Wu2,*, Senmao Li3, 5 2 0 2 3 ] . [ 1 2 1 5 0 2 . 0 1 5 2 : r 1Harbin Institute of Technology (Shenzhen), China 2Computer Vision Center, Universitat Autònoma de Barcelona, Spain 3VCIP, CS, Nankai University, China 4Program of Computer Science, City University of Hong Kong (Dongguan), China 5City University of Hong Kong, HK SAR, China https://liulisixin.github.io/EchoDistill-page/"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in accelerating text-to-image (T2I) diffusion models have enabled the synthesis of high-fidelity images even in single step. However, personalizing these models to incorporate novel concepts remains challenge due to the limited capacity of one-step models to capture new concept distributions effectively. We propose bidirectional concept distillation framework, EchoDistill, to enable one-step diffusion personalization (1-SDP). Our approach involves an end-toend training process where multi-step diffusion model (teacher) and one-step diffusion model (student) are trained simultaneously. The concept is first distilled from the teacher model to the student, and then echoed back from the student to the teacher. During the EchoDistill, we share the text encoder between the two models to ensure consistent semantic understanding. Following this, the student model is optimized with adversarial losses to align with the real image distribution and with alignment losses to maintain consistency with the teachers output. Furthermore, we introduce the bidirectional echoing refinement strategy, wherein the student model leverages its faster generation capability to feedback to the teacher model. This bidirectional concept distillation mechanism not only enhances the student ability to personalize novel concepts but also improves the generative quality of the teacher model. Our experiments demonstrate that this collaborative framework significantly outperforms existing personalization methods over the 1-SDP setup, establishing novel paradigm for rapid and effective personalization in T2I diffusion models."
        },
        {
            "title": "Introduction",
            "content": "Recently, large-scale generative models [52, 85, 103, 108] dominate high-quality text-to-image (T2I) generation and have been widely applied in diverse downstream tasks [30, 54, 88, 109]. Among these applications, personalized text-to-image generation, also referred to as new concept learning [15, 39, 100], has emerged as particularly important task. It involves adapting T2I model to recognize and synthesize novel concept from user-provided reference images. Recent T2I personalization methods [21, 39, 68] generally adapt pretrained T2I models using few-shot reference images and bind the novel concept to pseudo-token so that the adapted model can synthesize Equal contribution. Visiting researcher in Nankai University. Corresponding author Preprint. Under review. Figure 1: Comparison with existing new concept learning methods for one-step personalization: Textual Inversion [21] and Custom Diffusion [39] for SDTurbo [73], and IP-Adapter [105] for TCD [114]. various renditions of the new concept guided by text prompts. Despite their success, the adapted T2I models still face notable limitation which lies in their slow inference speed. To address inference inefficiency for T2I models, recent research has turned to distillation-based acceleration techniques [45, 73, 114, 104]. These techniques have matured considerably in the context of T2I diffusion models [50, 72, 73], as we focus on in this paper. In general, training-based distillation methods aim to learn fast student generator [49, 73, 79, 114] from multi-step T2I diffusion teacher model. Representative acceleration methods [17, 49, 73] achieve impressive acceleration by reducing the number of sampling steps to four or even fewer than one step for image generation. However, existing personalization methods often overlook this critical requirement for few-step diffusion models. Directly applying conventional personalization techniques to these acceleration models frequently results in failure cases. As an example illustrated in Fig. 1, the word-inversion method Textual Inversion applied to the one-step diffusion model SDTurbo [73] is unable to learn the textual concept tokens, which indicates that the one-step diffusion model struggles to independently update the text encoder. This presents the first challenge in 1-SDP: 1) Student inadaptability: The student cannot learn text tokens independently and effectively. This issue is further exemplified in our experiments with the weight-optimization method Custom Diffusion [39] in one-step models  (Fig. 1)  , where jointly updating the text encoder and the diffusion backbone fails to improve performance and instead degrades generation quality. For existing encoder-based personalization methods, they struggle to generalize to one-step diffusion due to its unique architectural and optimization properties (IP-Adapter in Fig. 1). naive way to leverage the distillation technique for the 1-SDP problem is to first fine-tune multi-step teacher model on the target concept, then use it to generate diverse samples to distill the one-step student. However, this results in two additional challenges: 2) Inefficiency: The multi-step generation process and non-end-to-end teacher-student distillation will significantly slow down learning. 3) Teacher irreliability: The teacher itself can also fail to capture certain concepts, limiting its effectiveness as guiding signal for the student. These issues contribute to significant failure cases of current methods in concept personalization for one-step diffusion student models. In this paper, we address the above challenges in 1-SDP by introducing our EchoDistill framework. The EchoDistill jointly trains multi-step T2I teacher model and one-step student model in an endto-end manner. The framework consists of two collaborative learning stages. In the first, distillation stage, the pretrained multi-step T2I teacher model concurrently learns the target concept while transferring knowledge to the student via concept distillation. In the second, echoing stage, the fast-generating student model produces images that are leveraged to further refine and enhance the teachers generative performance. More specifically, to address the above challenges in 1-SDP, we propose the following strategies along the distillation stage: 1) Shared text encoder (STE): To ensure semantic consistency and improve knowledge transfer between models, the student directly inherits the text encoder from the teacher model; 2) End-to-end joint training (E2E): Rather than relying on sequential training paradigm, we adopt unified optimization framework where the teacher and student models are trained simultaneously on the target concept. This promotes stable knowledge distillation and faster convergence. To support training, we employ two types of loss functions: alignment losses to ensure consistency with the teacher model, and adversarial losses to align the students outputs with the real image distribution of the novel concept. 3) Echoing stage (Echo): Follwing the first distillation stage, we further introduce bidirectional refinement 2 echoing stage to improve the performance. We exploit the student models fast generation ability by using its high-quality outputs to reverse-guide the learning of the teacher and student models. After one-step personalization, our method EchoDistill is also able to achieve few-step (2-step, 4-step, etc.) customized generations as bonus. To assess the performance of our method, EchoDistill, we compare it against several personalization approaches using range of evaluation schemes from DreamBench [68]. The results, both qualitative and quantitative, highlight the superior effectiveness of our approach. To summarize, we make the following contributions: To the best of our knowledge, we are the first to identify and formalize the problem of one-step diffusion personalization, termed 1-SDP, which significantly accelerates generation compared to conventional multi-step diffusion models while maintaining competitive image quality. We identify three core challenges in this setup: student inadaptability, inefficiency, and teacher irreliability. To tackle these challenges, we introduce set of novel solutions: 1) shared text encoder (STE) to mitigate student inadaptability by aligning semantic understanding between teacher and student; 2) joint end-to-end framework (E2E) that personalizes the one-step student model alongside the multi-step teacher model; and 3) an echoing stage (Echo), where the students fast image generation is leveraged to refine and stabilize the teachers output. We conduct extensive experiments on the DreamBench [68] benchmark, evaluating both qualitative and quantitative performance. Our proposed EchoDistill outperforms existing personalization techniques under the 1-SDP setting, demonstrating its ability to quickly adapt to novel concepts while preserving generation quality in one-step student models."
        },
        {
            "title": "2 Related works",
            "content": "Text-to-image personalization, also known as new concept learning, focuses on adapting model to user-provided novel concept using few reference images. This technique has been extensively studied in T2I diffusion models [8, 21, 46, 68] and recently studies in the AR domain [15, 100]. Tuning-based methods [44] leverage reference images of the same concept to fine-tune either the T2I diffusion model or its learnable embeddings. Depending on the optimization targets, these methods can be categorized into word-inversion and weight-optimization approaches. Word-inversion methods focus on learning new concept tokens without modifying the parameters of generative models. Textual Inversion [21] is pioneering approach that introduces pseudo-words by performing personalization in the text embedding space. Other works [3, 20, 87, 113] continually enable fine-grained and robust concept representation by employing designed loss functions to ensure that each token captures distinct aspect of the reference images. While these methods maintain high semantic consistency by keeping the generative model frozen, they suffer from limited identity fidelity due to the compression of rich image features into the low-dimensional text embedding space. Weight-optimization methods advance beyond token-level personalization by fine-tuning the models internal weights, enabling richer and more faithful concept learning. One of the most prominent methods is DreamBooth [68], which fine-tunes pre-trained text-to-image diffusion model to associate unique identifier with target subject using just 35 reference images. Following that, several methods such as Custom Diffusion (CD) [39] and Cones 2 [46] propose optimizing only subset of model parameters, significantly reducing both training time and memory consumption while preserving generation fidelity. Along similar lines, variety of approaches [13, 22, 28, 112] have emerged to improve visual quality and efficiency. In addition to partial weight tuning, recent works [2, 101] introduce parameter-efficient strategies using Adapter modules, Low-Rank Adaptation (LoRA), or their variants, including Hyper-E4T [5], DisenBooth [12], etc. Tuning-Free methods have proposed encoder-based alternatives that significantly reduce or eliminate the need for fine-tuning backbones by leveraging pre-trained image encoders. These methods [43, 67, 75, 89, 102, 105] enable efficient concept learning by extracting informative features from reference images using models trained on large-scale, diverse datasets. Some of them also specify in human face generation [16, 26, 43, 89, 99]. recent advanced research is IP-Adapter [105], which utilizes the ViT image encoder from CLIP [64] to extract reference image features. These features are then integrated into the diffusion models U-Net backbone through cross-attention mechanisms, resulting in more coherent and faithful renditions. While encoder-based methods are effective for general personalization from single reference image, they are mainly tailored for large-scale, multistep T2I models and require expensive retraining for each new backbone. Their limited ability to capture concept diversity [44] and lack of adaptation to few-step architectures remain key limitations. Applying such pretrained encoders or adapters to one-step diffusion models yields poor concept fidelity and image quality, highlighting critical gap in current research."
        },
        {
            "title": "3 Methodology",
            "content": "In this section, we begin by introducing the preliminaries in Section 3.1, followed by revealing the key challenges of this 1-SDP setup in Section 3.2. We then present our framework, EchoDistill, which incorporates three novel techniques designed to address these challenges in Section 3.3. 3.1 Preliminaries Latent Diffusion Models. LDM [65] is the most widely applied T2I diffusion models and the distillation teacher model for current few-step diffusion models [49, 73]. It is conditioned on textual input τ (P), where τ is the text encoder and is the prompt. The backbone ϵtc θ is conditional UNet [66] which predicts the added noise. After predicting the noise, diverse schedulers [48, 78] are used to denoise. Here, we use SD2.1 as the teacher model ϵtc θ as in T2I acceleration approaches [56, 73]. One-Step Diffusion Model. To accelerate diffusion inference, various methods distill the sampling steps Ttc = [1, ] of the teacher into few student anchor steps (NFEs) Tst = {υ1, . . . , υn} where is typically set to 1, 2, or 4. Specifically, one-step diffusion model Gst aims to transform noise xT (0, 1) directly into an image without iterative denoising steps, hence we denote this noise to image process as xst ϕ (xT , T, C). In this paper, we build on the one-step SD-Turbo [73] as the student model Gst ϕ for new concept learning under the 1-SDP setup. 0 = Gst 3.2 Core Challenges in 1-SDP As previously illustrated qualitatively in Section 1 and supported quantitatively in Section 4, conventional T2I personalization methods fail to learn new concepts in one-step diffusion models (such as SDTurbo [73]). We identify three main challenges in this 1-SDP setting. Student Inadaptability. We begin by applying the word-inversion Textual Inversion [21] to the onestep diffusion model SD-Turbo[73]. Observing from Fig. 1 and Table 1, this naive adaptation fails to capture or reproduce the target concept, revealing key limitation: one-step diffusion models cannot be effectively personalized by text encoder tuning alone, indicating the need for additional supervision or architectural changes. We further evaluate weight-optimization methods Custom Diffusion on SD-Turbo. This not only fails to enhance performance but also degrades image quality and concept fidelity, as proven in Fig. 1. These findings suggest that excessive flexibility in updating the backbone of few-step models may disrupt the generative prior. We hypothesize that this limitation arises from the inherent differences in the distillation objectives. Traditional diffusion models distill the entire generative process, preserving detailed noise-to-image mappings. In contrast, few-step models are typically trained using distribution alignment losses [63, 95] rather than reconstructing individual denoising trajectories. As result, applying conventional diffusion losses during personalization leads to ineffective learning and poor visual fidelity. Teacher Irreliability and Inefficiency. naive approach to tackle the 1-SDP problem is two-stage distillation strategy, which we refer to as the teacher-first distillation paradigm. In this setup, multi-step teacher model is first fine-tuned on the target concept, and then used to generate diverse supervision samples for training the one-step student model. However, this paradigm faces two fundamental limitations: 1) Inefficiency: The pipeline requires the teacher to first complete concept learning before it can supervise the student. Furthermore, the supervision involves multi-step generation (e.g., using 25 or 50 NFEs), making the process slow and computationally expensive. 2) Teacher Irreliability: The teacher model may struggle to accurately learn certain visual concepts, particularly under sparse supervision, as can be seen from the Custom Diffusion baseline (SD2.1) in Table 1 and Fig. 3. When this occurs, the generated samples used for distillation are suboptimal or NFEs denote the number of function evaluations, from the view of diffusion ODE trajectories. Figure 2: Overview of EchoDistill. The student and teacher jointly learn the new concept with shared text encoder. The teacher learns from real images xr 0 (green line), and the text encoder is updated accordingly. The student is optimized with two objectives (gold line): an adversarial loss to match real data distribution and alignment losses to match the denoised outputs of the teacher. The discriminators are trained to distinguish between the students outputs and real images. even misleading, thereby degrading the student models performance due to poor supervision. Results for the teacher-first paradigm and additional discussions are provided in Appendix. H. 3.3 EchoDistill: Bidirectional Concept Distillation Based on the above observations, we propose the first end-to-end bidirectional concept distillation framework, termed EchoDistill, specifically designed for personalizing one-step diffusion models (1-SDP). Unlike the above teacher-first paradigm that sequentially updates the teacher (multi-step diffusion model) and then distills knowledge to the student (one-step diffusion model), our approach adopts gradual distillation and echoing bidirectional concept distillation strategy, where the student learns progressively in tandem with the teacher. In addition, the student provides feedback to the teacher during the following echoing stage. To ensure memory efficiency and prevent overfitting, we adopt the lightweight adaptation strategy from Custom Diffusion [39], updating only the key and value projections in both teacher and student models. To tackle the aforementioned challenges, EchoDistill includes three targeted strategies to address the above challenges. 3.3.1 Distillation Stage: Shared Text Encoder and End-to-End Distillation To address the first challengestudent adaptabilitywe propose the use of shared text encoder (STE) between the teacher and student models. Motivated by the observation that the student can benefit from consistent semantic grounding, we directly inherit the teachers text encoder for the student during distillation. This design ensures unified language-vision alignment across both models. To facilitate effective training under this shared encoder and also to address the second challenge of inefficient training, we adopt bidirectional gradual distillation strategy. In this scheme, the student model is progressively trained alongside the teacher. To maintain memory efficiency and mitigate overfitting during distillation, we adopt the lightweight adaptation strategy from Custom Diffusion, updating only the key and value projections in both teacher and student networks. Our training procedure is illustrated in Fig. 2 and the detailed algorithm pipeline is in Appendix. F. EchoDistill consists of three steps in each iteration. First, the real image xr 0 is fed into the teacher model. The teacher is trained following the Custom Diffusion paradigm, where both the text encoder and the UNet are optimized using the noise prediction loss Lrec: Lrec = Exr 0,y,t,ϵN (0,1)ϵ ϵtc θ (xt, t, τ (P)2 2 (1) Second, the student receives random noise xT (0, 1) input and generates an output xst 0 = Gst(xT , T, C). This output is guided by combination of two objectives: (1) alignment losses between the student and teacher, and (2) adversarial losses between the student and real images. For the alignment objective, xst 0 is passed through the teachers forward diffusion process to obtain noisy version xst 0 + σtϵ, ϵ (0, 1), which is then denoised by the teacher to yield the predicted xtc 0 is detached via stop-gradient operation and serves as the supervision target for computing the alignment losses against xst 0 . For the adversarial objective, the student is optimized to fool an ensemble of discriminators, which are trained to distinguish the student-generated image 0 from real images xr xst 0. Third, the discriminators are optimized to enhance their discriminative performance. The detailed loss functions are defined as follows. 0 . This xtc = αtxst Alignment losses encourage the student-generated images to be semantically consistent with those from the teacher model, capturing both low-level pixel details and high-level perceptual alignment. It is composed of three components: Identity Feature Loss, adapted from IP-Adapter [105] (IPA), extracts identity-preserving features from the image space x0 using CLIP image encoder followed by projection network. Here the xtc 0 is the estimated teacher image, computed from the teacher prediction xtc {xt [(1 αt) ϵtc /(1 αt)1/2]} while the xst 0 is the image from student direct generation Gst(xT , T, C). This loss is computed as cosine similarity: 0 = α1/ Lid(xst 0 , xtc 0 ) = 1 cos (cid:0)IPA(xst 0 )), IPA(xtc 0 ))(cid:1) MSE Loss minimizes the distance between student and teacher latent representations: 0 , xtc 0 ) = (cid:13) (3) Multi-Scale Sliced Wasserstein Distance [29], compares multi-scale feature distributions in the image space to align structural and color information. This loss is proposed to alleviate the unstable color distribution during the distillation and defined as: Lmse(xst 0 xtc 0 (cid:13)xst (cid:13) 2 (cid:13) Lswd(xst 0 , xtc 0 ) = MS-SWD (cid:0)xst 0 , xtc 0 (cid:1) (2) (4) Lalign = c(t) (cid:2)λid Lid(xst The full alignment loss scales these components by weighting factors and time-dependent term: 0 ) + λmse Lmse(xst (5) Inspired by ADD [73], we introduce timestep-dependent exponential weighting factor c(t) = α(t), where denotes the randomly sampled timestep in the teachers noisingdenoising process and α(t) is the same as defined in DDPM [31]. At higher noise levels (i.e., larger t), the teachers predictions become increasingly unreliable, and the c(t) is accordingly decreased. This design helps stabilize the students training by reducing the influence of noisy supervision. 0 ) + λms Lswd(xst 0 , xtc 0 , xtc 0 , xtc 0 )(cid:3) Adversarial losses are designed to reduce the distribution gap between student-generated outputs and real concept images. Specifically, we ensemble multiple discriminators [10, 40], each operating from different semantic perspective, to improve training stability and achieve better results. We employ = 3 discriminators, each using different pretrained backboneDINOv1, DINOv2, and CLIPas feature extractors. Each backbone is followed by two-layer trainable projection head to distinguish between real and generated images, while the feature extractors remain frozen during training. The adversarial loss for the student is defined as: LG GAN = (cid:88) k=1 λk Exst 0 (cid:2) log(Dk(xst 0 ))(cid:3) (6) where Dk denotes the k-th discriminator (based on DINOv1 [9], DINOv2 [57] and CLIP [64]), and 0 is the student output image. Denoting xr xst 0 as the real image, the discriminator loss is defined as: LD GAN = (cid:88) (cid:104) k= Exr 0 [log Dk(xr 0)] + Exst 0 (cid:2)log(1 Dk(xst 0 ))(cid:3)(cid:105) (7) In summary, the integration of shared text encoder (STE), alignment losses and adversarial loss collectively enhances the adaptability and generalization capacity of the student model within the first distillation stage of our EchoDistill framework. It is worth noting that these loss formulations are specifically designed for the one-step student model, whose fast image generation allows efficient access to final outputs. In contrast, applying such losses to the multi-step teacher is impractical due to the computational cost of obtaining real image outputs across iterative denoising steps. 6 Figure 3: Our method EchoDistill (last column) compared with existing methods applied to the 1-SDP setup with SDTurbo [73] as the one-step diffusion backbone. One representive concept image is shown on the left-most column. 3.3.2 Echoing Stage: Student improves the Teacher We interpret the one-step student model as GAN-like generator, and hypothesize that it can benefit from aligning with the few-shot target data distribution via adversarial training. Specifically, incorporating adversarial loss [73, 92, 94, 106] as in Eq. 6 and Eq. 7 helps the student model generate samples that better match the distribution of concept images, even enabling it to outperform the teacher model in terms of visual qualities, as demonstrated in Table 1. This finding also aligns with insights from ADD [73], which emphasizes the critical role of the discriminator loss in boosting generative fidelity. Building on this insight, we propose an additional echoing stage, which leverages the student models rapid generation capability to provide constructive feedback to both itself and the teacher model. The echoing stage mirrors the distillation stage, with the only difference being the definition of the real trainig examples xr 0. Specifically, we replace real images with randomly generated samples 0 = Gst(xT , T, C). The training from the updated student model after the first-stage distillation: ˆxr objectives and update rules for both the student and teacher models remain unchanged from the initial distillation stage. The motivation behind this design is further discussed in Appendix F."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental setups Comparison methods and evaluation metrics. We compare our method EchoDistill with the following T2I personalization approaches: 1) word-inversion: Textual Inversion [21], Cones 2 [46]; 7 Table 1: Quantitative comparisons with existing text-to-image (T2I) personalization methods. Methods Custom Diffusion Custom Diffusion Textual Inversion Cones 2 DreamBooth TextBoost DisenBooth IP-Adapter EchoDistill Model SD 2.1 SD Turbo SD Turbo SD Turbo SD Turbo SD Turbo SD Turbo TCD SD Turbo Train NFEs Infer NFEs CLIP-T CLIP-I DINO 1000 25 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0.269 0.205 0.252 0.273 0.188 0.217 0.251 0.204 0.252 0. 0.518 0.564 0.619 0.536 0.570 0.564 0.628 0.783 0.519 0.058 0.166 0.204 0.111 0.167 0.231 0.325 0.637 Table 2: Ablation Study (Part 1). Methods CLIP-T CLIP-I DINO Table 3: Ablation Study (Part 2). Methods CLIP-T CLIP-I DINO Ablate Components w/o teacher w/o discriminators Full model Ablate Infer-NFEs 1 step 2 steps 4 steps 0.240 0.200 0. 0.252 0.255 0.256 0.719 0.566 0.783 0.783 0.783 0.772 0.505 0.105 0.637 0.637 0.635 0.610 Ablate 1-Step Backbone Hyper-SD1.5 SD-turbo Ablate Echo Stage Teacher Student Before Echo After Echo Before Echo After Echo 0.211 0. 0.269 0.265 0.252 0.236 0.709 0.783 0.463 0.637 0.752 0.764 0.783 0. 0.519 0.571 0.637 0.673 2) optimization-based: Custom Diffusion [39], DreamBooth [68], DisenBooth [12], TextBoost [60]; 3) Encoder-based: IP-Adapter [105]. For Disenbooth and Cones2 approach, the reference images were taken from training dataset in TextBoost. For Custom Diffusion, we also include baseline variant using SD2.1 [65] as the backbone, which also serves as the base model for SDTurbo. For IP-Adapter, due to the absence of an available implementation compatible with the one-step SDTurbo model, we adopt the TCD-based practice [114] for comparison. We follow the default configurations in their papers or open-source implementations. In the experiments, we evaluate our method on the DreamBooth [68] dataset, which contains 30 distinct concepts for personalized learning. To measure the alignment between generated images and textual prompts, we employ the CLIP-T score. Additionally, we assess cosine visual similarity between generated images and reference images using DINO [9] and CLIP-I [64] metrics, following standard practices in prior works [39, 60]. More details of these comparisons and experiments are in the Appendix. G. Implementation Details. During training of our method EchoDistill, the teacher model employs sampling schedule with 1000 denoising steps (NFEs), while the student model performs denoising in single step. For the hyperparameters, the loss weights in Eq. 5 and Eq. 6 are set such that λms = 0.1, while all other weights are set to 1.0. The model is trained with learning rate of 2 105 and batch size of 2. All experiments are conducted on single NVIDIA A40 GPU. 4.2 Experimental results Qualitative results. The main qualitative comparisons are presented in Fig. 3. Among the seven baseline methods, Custom Diffusion, DreamBooth, TextBoost, and Textual Inversion fail to perform effective denoising or learn target concepts under the one-step inference setting. DisenBooth and Cones2 struggle to capture precise concepts. Although IP-Adapter preserves some identity consistency, its results are often blurry, misaligned with the prompts, and affected by the reference image background. In contrast, our proposed method, EchoDistill, achieves both precise concept learning and strong semantic alignment between the generated images and input texts. Quantitative results. The detailed numeric results are presented in Table 1. EchoDistill maintains the text-image alignment quality competitive to the baselines, as evidenced by the CLIP-T Score. In terms of image similarity (CLIP-I, DINO scores), EchoDistill significantly outperforms the one-step or multi-step based methods. We note that the CLIP-T is not that relevant in T2I personalization since it is computing the alignment with the generated image with the input text (without the conditional 8 Figure 4: (a) Abating the Infer-NFEs; (b) Ablating the one-step diffusion backbone; (c) Ablating the teacher and discriminators; (d) Ablating the echoing stage. token), and therefore does not capture the alignment with the input concept images or the intended new concept. Therefore the CLIP-I and DINO are more convincing to demonstrate the effectiveness of personalization methods: on these scores our method shows the best performance. Moreover, EchoDistill even surpasses the Custom Diffusion (SD2.1) in the first row, further supporting our argument that the teacher model is not fully reliable under the 1-SDP setup. Additionally, the low CLIP-I and DINO scores of the Textual Inversion baseline highlight the students inability to independently learn the concept without effective supervision. These results collectively validate the key challenges we identified in adapting concept learning to the 1-SDP setup. 4.3 Ablation Study Main components. Table 2 presents the analysis of removing key components from EchoDistill, namely the teacher model and the discriminators. As shown, removing the teacher or the discriminators leads to noticeable decline across all evaluation metrics. These results indicate that both components play crucial role in effectively learning new concepts. Qualitative ablation results are shown in Fig. 4-(c). Our full model generates high-quality and semantically consistent outputs. In contrast, omitting the teacher results in images that lack fine details specific to the target concept. When the discriminators are removed, the outputs are significantly more noisy. This degradation likely stems from the teachers use of single denoising step, which produces x0 predictions with residual noise that are subsequently propagated to the student model during training. Inference Steps. Although EchoDistill is trained for 1-step setting, we further evaluate its performance for 2-step and 4-step denoising, using the same trained model without re-training. As reported in Table 2, CLIP-T scores show slight improvements with additional steps, whereas CLIP-I and DINO scores exhibit marginal declines. Overall, the variations across quantitative metrics remain minimal. The qualitative comparisons in Fig. 4-(a) reveal more perceptible differences. Notably, increasing the number of inference steps enhances image fidelity, especially by producing richer background details and finer textures. This generalizability emerges as beneficial byproduct of the training process in EchoDistill. 1-Step Backbones. We perform backbone ablation study to assess the adaptability of our method to alternative one-step backbones. In particular, we replace the student model with Hyper-SD1.5 and adjust the teacher model accordingly to SD1.5. The quantitative results are summarized in Table 3. Our findings indicate that this alternative backbone yields inferior performance compared to the SDTurbo backbone. However, as illustrated in Fig.4-(b), Hyper-SD1.5 still remains capable of generating reasonable outputs in such cases. Notably, our choice of SDTurbo as the primary backbone is motivated by two main factors: it is one of the few one-step models capable of generating 9 high-quality images, and its distillation-based training process is well aligned with our framework, which likely contributes to its superior performance over Hyper-SD1.5 when used as the backbone. Echoing Stage. In Table 3, we compare the student and teacher performance after the echoing stage. The teacher model exhibits significant improvements in CLIP-I and DINO scores, while CLIP-T scores experience slight decline. These results suggest that the students output can effectively enhance the performance of both teacher and student models, particularly in terms of identity and visual similarity. Qualitative examples in Fig. 4-(d) further support this observation: when the teacher model struggles to learn certain concepts, leveraging the students output as an additional supervisory signal enables the teacher to better capture and reproduce those challenging concepts."
        },
        {
            "title": "5 Conclusions",
            "content": "In this paper, we introduced the novel task of one-step diffusion personalization (1-SDP), significant step toward bridging the gap between fast generative inference and concept-personalized image synthesis. We identified three major challenges that prevent conventional personalization methods from being directly applicable to one-step diffusion models. To overcome these limitations, we proposed unified framework, EchoDistill, which jointly trains multi-step teacher and one-step student through an end-to-end distillation process and bidirectional echoing stage. By leveraging shared text encoder, end-to-end optimization, and student-guided echoing stage, EchoDistill enables effective adaptation to new visual concepts. Extensive experiments on the DreamBench benchmark confirm that EchoDistill consistently outperforms existing personalization approaches, setting new foundation for rapid and reliable concept learning in diffusion-based generation."
        },
        {
            "title": "Acknowledgements",
            "content": "We acknowledge project PID2022-143257NB-I00, financed by MCIN/AEI/10.13039/501100011033 and ERDF/EU, and the Generalitat de Catalunya CERCA Program, and ELLIOT project funded by the European Union under Grant Agreement 101214398. This work was also supported by NSFC (NO. 62225604) and Youth Foundation (62202243). We acknowledge Science and Technology Yongjiang 2035 key technology breakthrough plan project and Chinese government-guided local science and technology development fund projects (scientific and technological achievement transfer and transformation projects) (254Z0102G). Kai Wang acknowledges the funding from Guangdong and Hong Kong Universities 1+1+1 Joint Research Collaboration Scheme and the start-up grant B01040000108 from CityU-DG."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Panos Achlioptas, Alexandros Benetatos, Iordanis Fostiropoulos, and Dimitris Skourtis. Stellar: systematic evaluation of human-centric personalized text-to-image methods. arXiv preprint arXiv:2312.06116, 2023. [3] Aishwarya Agarwal, Srikrishna Karanam, Tripti Shukla, and Balaji Vasan Srinivasan. An image is worth multiple words: Multi-attribute inversion for constrained text-to-image synthesis. International Conference on Machine Learning, 2024. [4] Yuval Alaluf, Elad Richardson, Gal Metzer, and Daniel Cohen-Or. neural space-time representation for text-to-image personalization. ACM Transactions on Graphics (TOG), 42(6):110, 2023. [5] Moab Arar, Rinon Gal, Yuval Atzmon, Gal Chechik, Daniel Cohen-Or, Ariel Shamir, and Amit H. Bermano. Domain-agnostic tuning-encoder for fast personalization of text-to-image models. In SIGGRAPH Asia 2023 Conference Papers, pages 110, 2023. [6] Vladimir Arkhipkin, Andrei Filatov, Viacheslav Vasilev, Anastasia Maltseva, Said Azizov, Igor Pavlov, Julia Agafonova, Andrey Kuznetsov, and Denis Dimitrov. Kandinsky 3.0 technical report. arXiv preprint arXiv:2312.03511, 2023. 10 [7] Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-Or, and Dani Lischinski. Break-ascene: Extracting multiple concepts from single image. SIGGRAPH Asia 2023, 2023. [8] Muhammad Atif Butt, Kai Wang, Javier Vazquez-Corral, and Joost van de Weijer. Colorpeel: Color prompt learning with diffusion models via color and shape disentanglement. In European Conference on Computer Vision, 2024. [9] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. [10] Eric Chan, Connor Lin, Matthew Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometryaware 3d generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1612316133, 2022. [11] Bowen Chen, Mengyi Zhao, Haomiao Sun, Li Chen, Xu Wang, Kang Du, and Xinglong Wu. Xverse: Consistent multi-subject control of identity and semantic attributes via dit modulation. arXiv preprint arXiv:2506.21416, 2025. [12] Hong Chen, Yipeng Zhang, Simin Wu, Xin Wang, Xuguang Duan, Yuwei Zhou, and Wenwu Zhu. Disenbooth: Identity-preserving disentangled tuning for subject-driven text-to-image generation. International Conference on Learning Representations, 2024. [13] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Rui, Xuhui Jia, Ming-Wei Chang, and William Cohen. Subject-driven text-to-image generation via apprenticeship learning. Advances in Neural Information Processing Systems, 2023. [14] Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William Cohen. Re-imagen: Retrievalaugmented text-to-image generator. In The Eleventh International Conference on Learning Representations, 2022. [15] Jiwoo Chung, Sangeek Hyun, Hyunjun Kim, Eunseo Koh, MinKyu Lee, and Jae-Pil Heo. Fine-tuning visual autoregressive models for subject-driven generation. Proceedings of the International Conference on Computer Vision, 2025. [16] Siying Cui, Jia Guo, Xiang An, Jiankang Deng, Yongle Zhao, Xinyu Wei, and Ziyong Feng. Idadapter: Learning mixed features for tuning-free personalization of text-to-image models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 950959, 2024. [17] Trung Dao, Thuan Hoang Nguyen, Thanh Le, Duc Vu, Khoi Nguyen, Cuong Pham, and Anh Tran. Swiftbrush v2: Make your one-step diffusion model better than its teacher. European Conference on Computer Vision, 2024. [18] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, and Haoqi Fan. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. [19] Ganggui Ding, Canyu Zhao, Wen Wang, Zhen Yang, Zide Liu, Hao Chen, and Chunhua Shen. Freecustom: Tuning-free customized image generation for multi-concept composition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 90899098, 2024. [20] Ziyi Dong, Pengxu Wei, and Liang Lin. Dreamartist: Towards controllable one-shot text-toimage generation via contrastive prompt-tuning. arXiv preprint arXiv:2211.11337, 2022. [21] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. International Conference on Learning Representations, 2023. [22] Rinon Gal, Moab Arar, Yuval Atzmon, Amit Bermano, Gal Chechik, and Daniel CohenOr. Designing an encoder for fast personalization of text-to-image models. arXiv preprint arXiv:2302.12228, 2023. [23] Rinon Gal, Or Lichter, Elad Richardson, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. Lcm-lookahead for encoder-based text-to-image personalization. In European Conference on Computer Vision, pages 322340. Springer, 2024. [24] Daniel Garibi, Shahar Yadin, Roni Paiss, Omer Tov, Shiran Zada, Ariel Ephrat, Tomer Michaeli, Inbar Mosseri, and Tali Dekel. Tokenverse: Versatile multi-concept personalization in token modulation space. Proceedings of the International Conference on Computer Vision, 2025. [25] Yuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun Shi, Yunpeng Chen, Zihan Fan, Wuyou Xiao, Rui Zhao, Shuning Chang, Weijia Wu, et al. Mix-of-show: Decentralized low-rank adaptation for multi-concept customization of diffusion models. Advances in Neural Information Processing Systems, 36, 2024. [26] Zinan Guo, Yanze Wu, Chen Zhuowei, Peng Zhang, Qian He, et al. Pulid: Pure and lightning id customization via contrastive alignment. Advances in neural information processing systems, 37:3677736804, 2024. [27] Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2025. [28] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, and Feng Yang. Svdiff: Compact parameter space for diffusion fine-tuning. Proceedings of the International Conference on Computer Vision, 2023. [29] Jiaqi He, Zhihua Wang, Leon Wang, Tsein-I Liu, Yuming Fang, Qilin Sun, and Kede Ma. Multiscale sliced wasserstein distances as perceptual color difference measures. In European Conference on Computer Vision, pages 425442. Springer, 2024. [30] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. International Conference on Learning Representations, 2023. [31] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:68406851, 2020. [32] Jiannan Huang, Jun Hao Liew, Hanshu Yan, Yuyang Yin, Yao Zhao, Humphrey Shi, and Yunchao Wei. Classdiffusion: More aligned personalization tuning with explicit class guidance. International Conference on Learning Representations, 2025. [33] Jiehui Huang, Xiao Dong, Wenhui Song, Hanhui Li, Jun Zhou, Yuhao Cheng, Shutao Liao, Long Chen, Yiqiang Yan, Shengcai Liao, et al. Consistentid: Portrait generation with multimodal fine-grained identity preserving. arXiv preprint arXiv:2404.16771, 2024. [34] Qihan Huang, Siming Fu, Jinlong Liu, Hao Jiang, Yipeng Yu, and Jie Song. Resolving multi-condition confusion for finetuning-free personalized image generation. Proceedings of the Conference on Artificial Intelligence, 2025. [35] Xuhui Jia, Yang Zhao, Kelvin CK Chan, Yandong Li, Han Zhang, Boqing Gong, Tingbo Hou, Huisheng Wang, and Yu-Chuan Su. Taming encoder for zero fine-tuning image customization with text-to-image diffusion models. arXiv preprint arXiv:2304.02642, 2023. [36] Liming Jiang, Qing Yan, Yumin Jia, Zichuan Liu, Hao Kang, and Xin Lu. Infiniteyou: Flexible photo recrafting while preserving your identity. Proceedings of the International Conference on Computer Vision, 2025. [37] Zhe Kong, Yong Zhang, Tianyu Yang, Tao Wang, Kaihao Zhang, Bizhu Wu, Guanying Chen, Wei Liu, and Wenhan Luo. Omg: Occlusion-friendly personalized multi-concept generation in diffusion models. In European Conference on Computer Vision, pages 253270. Springer, 2024. [38] Nupur Kumari, Xi Yin, Jun-Yan Zhu, Ishan Misra, and Samaneh Azadi. Generating multiimage synthetic data for text-to-image customization. arXiv preprint arXiv:2502.01720, 2025. [39] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multiconcept customization of text-to-image diffusion. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2023. [40] Nupur Kumari, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Ensembling off-the-shelf models for gan training. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1065110662, 2022. [41] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [42] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. International Conference on Machine Learning, 2023. [43] Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, Ming-Ming Cheng, and Ying Shan. Photomaker: Customizing realistic human photos via stacked id embedding. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2024. [44] Ziqiang Li, Jun Li, Lizhi Xiong, Zhangjie Fu, and Zechao Li. comprehensive survey on visual concept mining in text-to-image diffusion models. arXiv preprint arXiv:2503.13576, 2025. [45] Enshu Liu, Xuefei Ning, Yu Wang, and Zinan Lin. Distilled decoding 1: One-step sampling of image auto-regressive models with flow matching. International Conference on Learning Representations, 2025. [46] Zhiheng Liu, Ruili Feng, Kai Zhu, Yifei Zhang, Kecheng Zheng, Yu Liu, Deli Zhao, Jingren Zhou, and Yang Cao. Cones: Concept neurons in diffusion models for customized generation. International Conference on Machine Learning, 2023. [47] Zhiheng Liu, Yifei Zhang, Yujun Shen, Kecheng Zheng, Kai Zhu, Ruili Feng, Yu Liu, Deli Zhao, Jingren Zhou, and Yang Cao. Customizable image synthesis with multiple subjects. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [48] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:57755787, 2022. [49] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. [50] Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick von Platen, Apolinário Passos, Longbo Huang, Jian Li, and Hang Zhao. Lcm-lora: universal stable-diffusion acceleration module. arXiv preprint arXiv:2311.05556, 2023. [51] Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu. Subject-diffusion: Open domain personalized text-to-image generation without test-time fine-tuning. Proceedings of the ACM SIGGRAPH Conference on Computer Graphics, 2024. [52] Siwei Ma, Junlong Gao, Ruofan Wang, Jianhui Chang, Qi Mao, Zhimeng Huang, and Chuanmin Jia. Overview of intelligent video coding: from model-based to learning-based approaches. Visual Intelligence, 1(1):15, 2023. [53] Yiyang Ma, Huan Yang, Wenjing Wang, Jianlong Fu, and Jiaying Liu. Unified multi-modal latent diffusion for joint subject and text conditional image generation. arXiv preprint arXiv:2303.09319, 2023. [54] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. Proceedings of the Conference on Artificial Intelligence, 2024. [55] Chong Mou, Yanze Wu, Wenxu Wu, Zinan Guo, Pengze Zhang, Yufeng Cheng, Yiming Luo, Fei Ding, Shiwen Zhang, Xinghui Li, et al. Dreamo: unified framework for image customization. SIGGRAPH Asia, 2025. [56] Thuan Hoang Nguyen and Anh Tran. Swiftbrush: One-step text-to-image diffusion model with variational score distillation. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2024. [57] Maxime Oquab, Timothée Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023. [58] Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, and Furu Wei. KosmosInternational g: Generating images in context with multimodal large language models. Conference on Learning Representations, 2024. [59] Lianyu Pang, Jian Yin, Baoquan Zhao, Feize Wu, Fu Lee Wang, Qing Li, and Xudong Mao. Attndreambooth: Towards text-aligned personalized text-to-image generation. Advances in Neural Information Processing Systems, 37:3986939900, 2024. [60] NaHyeon Park, Kunhee Kim, and Hyunjung Shim. Textboost: Towards one-shot personalization of text-to-image models via fine-tuning text encoder. arXiv preprint arXiv:2409.08248, 2024. [61] Maitreya Patel, Sangmin Jung, Chitta Baral, and Yezhou Yang. lambda-eclipse: Multi-concept personalized text-to-image diffusion models by leveraging clip latent space. arXiv preprint arXiv:2402.05195, 2024. [62] Ryan Po, Guandao Yang, Kfir Aberman, and Gordon Wetzstein. Orthogonal adaptation for modular customization of diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 79647973, 2024. [63] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. DreamFusion: Text-to-3D using 2D Diffusion. International Conference on Learning Representations, 2023. [64] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. [65] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, 06 2022. [66] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted InterventionMICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages 234241. Springer, 2015. [67] Ciara Rowles, Shimon Vainer, Dante De Nigris, Slava Elizarov, Konstantin Kutsy, and Simon Donné. Ipadapter-instruct: Resolving ambiguity in image-based conditioning using instruct prompts, 2024. [68] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2250022510, 2023. [69] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, and Kfir Aberman. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models. arXiv preprint arXiv:2307.06949, 2023. [70] Amirmojtaba Sabour, Sanja Fidler, and Karsten Kreis. Align your flow: Scaling continuoustime flow map distillation. arXiv preprint arXiv:2506.14603, 2025. 14 [71] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 2022. [72] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. International Conference on Learning Representations, 2022. [73] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. European Conference on Computer Vision, 2024. [74] Viraj Shah, Nataniel Ruiz, Forrester Cole, Erika Lu, Svetlana Lazebnik, Yuanzhen Li, and Varun Jampani. Ziplora: Any subject in any style by effectively merging loras. arXiv preprint arXiv:2311.13600, 2023. [75] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instantbooth: Personalized text-to-image generation without test-time finetuning. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2024. [76] Enis Simsar, Thomas Hofmann, Federico Tombari, and Pinar Yanardag. Loraclr: Contrastive adaptation for customization of diffusion models. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2025. [77] James Seale Smith, Yen-Chang Hsu, Lingyu Zhang, Ting Hua, Zsolt Kira, Yilin Shen, and Hongxia Jin. Continual diffusion: Continual customization of text-to-image diffusion with c-lora. arXiv preprint arXiv:2304.06027, 2023. [78] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. [79] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In International Conference on Machine Learning, pages 3221132252. PMLR, 2023. [80] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. [81] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, et al. Generative multimodal models are in-context learners. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2024. [82] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2025. [83] Yoad Tewel, Rinon Gal, Gal Chechik, and Yuval Atzmon. Key-locked rank one editing for text-to-image personalization. In ACM SIGGRAPH 2023 Conference Proceedings, pages 111, 2023. [84] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2024. [85] Xiaoguang Tu, Zhi He, Yi Huang, Zhi-Hao Zhang, Ming Yang, and Jian Zhao. An overview of large ai models and their applications. Visual Intelligence, 2(1):122, 2024. [86] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19211930, 2023. [87] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir Aberman. p+: Extended textual conditioning in text-to-image generation. arXiv preprint arXiv:2303.09522, 2023. 15 [88] Kai Wang, Fei Yang, Shiqi Yang, Muhammad Atif Butt, and Joost van de Weijer. Dynamic prompt learning: Addressing cross-attention leakage for text-based image editing. Advances in Neural Information Processing Systems, 2023. [89] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, and Anthony Chen. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024. [90] Xierui Wang, Siming Fu, Qihan Huang, Wanggui He, and Hao Jiang. Ms-diffusion: Multisubject zero-shot image personalization with layout guidance. International Conference on Learning Representations, 2025. [91] Yaxing Wang, Abel Gonzalez-Garcia, David Berga, Luis Herranz, Fahad Shahbaz Khan, and Joost van de Weijer. Minegan: effective knowledge transfer from gans to target domains with few images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 93329341, 2020. [92] Yaxing Wang, Abel Gonzalez-Garcia, Chenshen Wu, Luis Herranz, Fahad Shahbaz Khan, Shangling Jui, Jian Yang, and Joost van de Weijer. Minegan++: Mining generative models for efficient knowledge transfer to limited data domains. International Journal of Computer Vision, 132(2):490514, 2024. [93] Yaxing Wang, Chenshen Wu, Luis Herranz, Joost van de Weijer, Abel Gonzalez-Garcia, and B. Raducanu. Transferring gans: generating images from limited data. In ECCV, 2018. [94] Yaxing Wang, Chenshen Wu, Luis Herranz, Joost Van de Weijer, Abel Gonzalez-Garcia, and Bogdan Raducanu. Transferring gans: generating images from limited data. In Proceedings of the European conference on computer vision (ECCV), pages 218234, 2018. [95] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in Neural Information Processing Systems, 36:84068441, 2023. [96] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 1589715907, 2023. [97] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. [98] Shaojin Wu, Mengqi Huang, Wenxu Wu, Yufeng Cheng, Fei Ding, and Qian He. Less-tomore generalization: Unlocking more controllability by in-context generation. arXiv preprint arXiv:2504.02160, 2025. [99] Yi Wu, Ziqiang Li, Heliang Zheng, Chaoyue Wang, and Bin Li. Infinite-id: Identity-preserved personalization via id-semantics decoupling paradigm. In European Conference on Computer Vision, pages 279296. Springer, 2024. [100] Yi Wu, Lingting Zhu, Lei Liu, Wandi Qiao, Ziqiang Li, Lequan Yu, and Bin Li. Proxytuning: Tailoring multimodal autoregressive models for subject-driven image generation. arXiv preprint arXiv:2503.10125, 2025. [101] Chendong Xiang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. closer look at parameterefficient tuning in diffusion models. arXiv preprint arXiv:2303.18181, 2023. [102] Guangxuan Xiao, Tianwei Yin, William Freeman, Frédo Durand, and Song Han. Fastcomposer: Tuning-free multi-subject image generation with localized attention. International Journal of Computer Vision, 2023. [103] Zhen Xing, Qijun Feng, Haoran Chen, Qi Dai, Han Hu, Hang Xu, Zuxuan Wu, and Yu-Gang Jiang. survey on video diffusion models. ACM Computing Surveys, 57(2):142, 2024. 16 [104] Chenkai Xu, Xu Wang, Zhenyi Liao, Yishun Li, Tianqi Hou, and Zhijie Deng. Show-o turbo: Towards accelerated unified multimodal understanding and generation. arXiv preprint arXiv:2502.05415, 2025. [105] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. Proceedings of the Conference on Artificial Intelligence, 2024. [106] Tianwei Yin, Michaël Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and William Freeman. Improved distribution matching distillation for fast image synthesis. Advances in Neural Information Processing Systems, 2024. [107] Yu Zeng, Vishal Patel, Haochen Wang, Xun Huang, Ting-Chun Wang, Ming-Yu Liu, and Yogesh Balaji. Jedi: Joint-image diffusion models for finetuning-free personalized text-toimage generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 67866795, 2024. [108] Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang, and In So Kweon. Text-to-image diffusion models in generative ai: survey. arXiv preprint arXiv:2303.07909, 2023. [109] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. [110] Yuxin Zhang, Weiming Dong, Fan Tang, Nisha Huang, Haibin Huang, Chongyang Ma, TongYee Lee, Oliver Deussen, and Changsheng Xu. Prospect: Expanded conditioning for the personalization of attribute-aware image generation. SIGGRAPH Asia 2023, 2023. [111] Yuxuan Zhang, Yiren Song, Jiaming Liu, Rui Wang, Jinpeng Yu, Hao Tang, Huaxia Li, Xu Tang, Yao Hu, Han Pan, et al. Ssr-encoder: Encoding selective subject representation for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 80698078, 2024. [112] Zhixing Zhang, Ligong Han, Arnab Ghosh, Dimitris Metaxas, and Jian Ren. Sine: Single image editing with text-to-image diffusion models. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2023. [113] Ruoyu Zhao, Mingrui Zhu, Shiyin Dong, De Cheng, Nannan Wang, and Xinbo Gao. Catversion: Concatenating embeddings for diffusion-based text-to-image personalization. IEEE Transactions on Circuits and Systems for Video Technology, 2025. [114] Jianbin Zheng, Minghui Hu, Zhongyi Fan, Chaoyue Wang, Changxing Ding, Dacheng Tao, and Tat-Jen Cham. Trajectory consistency distillation. arXiv preprint arXiv:2402.19159, 2024. [115] Yufan Zhou, Ruiyi Zhang, Tong Sun, and Jinhui Xu. Enhancing detail preservation for customized text-to-image generation: regularization-free approach. arXiv preprint arXiv:2305.13579, 2023."
        },
        {
            "title": "A Broader Impacts",
            "content": "Our method, EchoDistill, enables efficient and high-fidelity personalized image generation, which holds significant potential for range of creative applications, including design, education, and virtual content creation. By reducing the need for extensive data and computation, EchoDistill democratizes access to advanced generative tools, empowering users with minimal resources to produce customized visual content. However, as with other powerful generative models, our approach also introduces potential risks. These include the unauthorized generation of content, impersonation, and the creation of misleading or harmful imagery. We acknowledge these risks and stress the importance of deploying appropriate safeguards, such as content moderation, usage auditing, and user authentication mechanisms, particularly in real-world applications. We advocate for the responsible use of this technology and encourage the research community and stakeholders to collaborate on developing ethical guidelines and technical solutions to mitigate potential misuse. Limitations. Our proposed method, EchoDistill, marks an initial step toward enabling one-step diffusion models to learn novel concepts efficiently. While the experimental results are promising, several limitations remain and warrant further investigation: (1) Training efficiency: The current training pipeline is hindered by the computational overhead introduced by the multi-discriminator architecture. Optimizing or rethinking this component could significantly improve training speed. (2) Limited one-shot personalization: The discriminators reliance on multiple reference samples to model the underlying data distribution makes true one-shot personalization challenging. Designing more robust discriminator or alternative mechanisms to enable faithful learning from single image remains an open problem. (3) Training instability: As with many GAN-based methods, our approach may exhibit instability across runs, particularly for challenging concepts, where achieving optimal results may require few trials. Enhancing training stability remains promising direction for future work. We leave these challenges as compelling avenues for future research, aiming to build upon this initial framework to support broader generalization, compositionality, and efficiency."
        },
        {
            "title": "C Code Release and Reproducibility Statement",
            "content": "Our project page is available at https://liulisixin.github.io/EchoDistill-page/. We will release the complete codebase, including training scripts and model checkpoints, upon publication."
        },
        {
            "title": "D Ethical and LLM Statements",
            "content": "We acknowledge the potential ethical implications of deploying generative models, including concerns related to privacy, data misuse, and the propagation of biases. All models used in this paper are publicly available, and we will release the modified codes to enable reproduction of our results. We also emphasize the potential misuse of customization approaches in generating misinformation, and we strongly encourage and support their responsible usage. Regarding the use of LLMs, we clarify that in this work they were only minimally employed, specifically for correcting grammatical errors. Overview of Text-to-Image Personalization Methods In this section, we present comprehensive comparison of representative text-to-image personalization methods, expanding upon the overview introduced in λ-Eclipse [61]. Table 4 provides an extended summary that systematically contrasts these approaches across several key dimensions, including support for singleor multi-subject personalization, training-free versus training-based paradigms, number of input images required, inference efficiency, etc. 18 Table 4: We provide an overview of representative text-to-image personalization methods by extending the summary introduced in λ-Eclipse [61]. The base models listed correspond to those used in their original papers. For fair comparison with the highlighted methods in our study, we re-implemented and adapted all approaches using the same base model configuration as described in our main paper. ChilloutMix is community-contributed variant of the Stable Diffusion model [65]. Inifinity refers to variant of the Text-to-Image VAR model [84], while LlamaGen denotes text-to-image autoregressive (AR) model [80]. Method Textual Inversion [21] P+ [87] ProsPect [110] MATTE [3] Cones 2 [47] DreamBooth [68] ClassDiffusion [32] DisenBooth [12] CatVersion [113] AttnDreamBooth [59] ViCo [86] TextBoost [60] NeTI [4] HyperDreamBooth [69] E4T [22] Hyper-E4T [5] ARBooth [15] Proxy-Tuning [100] Continual Diffusion [77] Perfusion [83] Custom Diffusion [39] Cones [46] SVDiff [28] FreeCustom [19] Mix-of-Show [25] LoRACLR [76] Orthogonal [62] OMG [37] Zip-LoRA [74] Break-A-Scene [7] TokenVerse [24] EchoDistill (Ours) PhotoMaker [43] ConsistentID [33] InstantID [89] Profusion [115] PuLID [26] Infinite-ID [99] LCM-Lookahead [23] InfiniteYou [36] IP-Adapter [105] ELITE [96] UMM-Diffusion [53] InstantBooth [75] BLIP-Diffusion [42] JeDi [107] Re-Imagen [14] SuTi [13] Taming [35] Kosmos-G [58] SSR-Encoder [111] λ-Eclipse [61] FastComposer [102] Subject-Diffusion [51] RMCC [34] Emu2 [81] MS-Diffusion [90] MultiSubject TuningFree Base Model Input Images Inference Steps Note SDv1.4 SDv1.4 SDv1.4 SDv1.4 SDv2.1 SDv1.4 SDv1.5 SDv2.1 SDv1.5 SDv2.1 SDv1.4 SDv1.5 SDv1.4 SDv1.5 SD SD Infinity [27] LlamaGen [80] SD SDv1.5 SDv1.4 SDv1.4 SDv1.5 SDv1.5 Chilloutmix Chilloutmix Chilloutmix SDXL SDXL SDv2.1 Flux [41] 1-shot 1-shot 1-shot 1-Shot 1-Shot 1-shot 1-shot 1-shot Few-Shot Multi-Step Word-Inversion Few-Shot Multi-Step Word-Inversion Multi-Step Word-Inversion Multi-Step Word-Inversion Few-Shot Multi-Step Word-Inversion Few-Shot Multi-Step Few-Shot Multi-Step Multi-Step Few-Shot Multi-Step Multi-Step Few-Shot Multi-Step Multi-Step Few-Shot Multi-Step Multi-Step Multi-Step Multi-Step Few-Shot Multi-Step Few-Shot Multi-Step Few-Shot Multi-Step Few-Shot Multi-Step Few-Shot Multi-Step Multi-Step Few-Shot Multi-Step Multi-Step Few-Shot Multi-Step Few-Shot Multi-Step Few-Shot Multi-Step Few-Shot Multi-Step Few-Shot Multi-Step Multi-Step Multi-Step 1-shot 1-shot 1-Shot 1-shot Human Face Human Face Human Face Human Face Human Face Human Face Human Face Human Face SDTurbo [73] Few-Shot 1-step 1-shot 1-shot 1-shot 1-shot 1-shot 1-shot 1-shot 1-shot 1-shot 1-shot 1-shot Multi-Step Multi-Step Multi-Step Multi-Step Multi-Step Multi-Step Multi-Step Multi-Step Multi-Step Multi-Step Multi-Step Few-Shot Multi-Step Multi-Step Few-Shot Multi-Step Multi-Step Few-Shot Multi-Step Multi-Step Multi-Step Multi-Step Multi-Step Multi-Step Multi-Step Multi-Step Multi-Step Multi-Step 1-shot 1-shot 1-shot 1-shot 1-shot 1-shot 1-shot 1-shot 1-shot 1-shot 1-shot SDXL SDv1.5 SDXL SDv2 SDXL SDXL SDXL Flux [41] SDv1.5 SDv1.4 SDv1.5 SDv1.4 SDv1.5 SDv1.4 Imagen [71] Imagen [71] Imagen [71] SDv1.5 SDv1.5 Kandinsky [6] SDv1.5 SDv2.1 SDXL SDXL SDXL 19 Algorithm 1 Training Pipeline of One-step Personalization in EchoDistill 1: Input: Real image dataset = {(xr θ ; student model Gst; discriminators k=1; text encoder τ (); diffusion steps ; noise schedule functions {αt, σt}; weighting 0, y)}; teacher model ϵtc {Dk}K functions c(t), λid, λmse, λms, {λk}K k=1 0 + σtϵ 0, y) 2: for each training iteration do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: Sample real image and prompt: (xr Random noise ϵ (0, 1) Encode prompt: τ (y) Step 1: Teacher training Sample timestep U(1, ) Generate noisy input: xt = αtxr Predict noise: ˆϵ ϵtc θ (xt, t, C) Compute loss: Lrec = ϵ ˆϵ2 2 Update teacher model and the text encoder using Lrec Step 2: Student training Sample latent: xT (0, 1) Generate image: xst // Alignment loss Forward diffuse: xst Denoise: xtc Compute alignment loss: Lalign = c(t) [λid Lid(xst // Adversarial loss Compute adversarial loss: LG Update student model using: Lst = Lalign + LG Step 3: Discriminator training for each discriminator Dk do = αtxst 0 stopgrad(ϵtc 0 Gst(xT , T, stopgrad(C)) k=1 λk Exst 0 + σtϵ θ (xst GAN = (cid:80)K , t, C)) 0 , xtc GAN 0 19: 20: 21: 22: 23: 24: (cid:104) Compute the discriminator loss: LDk Exr Update Dk using: LDk GAN [log Dk(xr GAN = 0 0)] + Exst 0 end for 25: 26: 27: end for 28: Output: Trained teacher model ϵtc 0 ) + λmse Lmse(xs, xt) + λms Lswd(xst 0 , xtc 0 )] [ log(Dk(xst 0 ))] [log(1 Dk(stopgrad(xst 0 )))] (cid:105) θ , student model Gst, and text encoder τ stopgrad() denotes stop-gradient"
        },
        {
            "title": "F Algorithmic Description of EchoDistill Training",
            "content": "Distillation Stage. The full training procedure of one-step personalization in EchoDistill is described in Section 3.3.1 of the main paper. For completeness, Algorithm 1 provides the detailed step-by-step implementation. Each iteration consists of three steps: (1) the teacher model and text encoder are jointly optimized via the noise prediction loss Lrec following the Custom Diffusion paradigm; (2) training of the student model through combination of alignment losses with the teachers outputs and adversarial losses against real image data; and (3) updating the discriminators to improve their capacity to differentiate between real and synthesized samples. Echoing Stage. During the echoing stage, the training procedure remains identical to the previous phase, except that the training examples are replaced with one-step inference samples generated by the student model. This design is beneficial in multiple ways. (1) In the initial distillation stage, the availability of real images is limited, and this constrained data scale impedes the training of the teacher model. By contrast, the student model exhibits the ability to learn data distributions from few images. That is capability endowed by the discriminator, as validated in few-shot GAN frameworks such as TransferGAN[93] and MineGAN[91]. Consequently, during the echoing stage, our objective is to sample from the data distribution learned by the student model, using these samples as training examples to enhance the teacher models performance. (2) Notably, the 1-step diffusion student model learns distributions distinct from those acquired by the teacher model. Similar observation can be found in ADD[73], where the discriminator loss primarily shapes the data distribution of the student model, while the distillation loss facilitates convergence and enhances conceptual alignment 20 Table 5: Quantitative comparisons with existing text-to-image (T2I) personalization methods. NFEs indicates the number of function evaluations. Methods Model Train NFEs Inference NFEs CLIP-T CLIP-I DINO Train Time (s) Inference Time (s) iterations SD 2.1 SD Turbo SD Turbo SD Turbo SD Turbo SD Turbo SD Turbo SD Turbo SD Turbo SD Turbo SD Turbo SD Turbo SD Turbo SD Turbo TCD Flux SD Turbo 1000 1000 1000 1000 1 1 4 4 1 1 1 1 1 1 / / Custom diffusion Textual Inversion Cones 2 DreamBooth TextBoost DisenBooth Lora IP-Adapter OminiControl EchoDistill 25 1 4 25 1 4 1 4 1 1 1 1 1 1 1 1 0.264 0.207 0.257 0.276 0.205 0.246 0.206 0.258 0.252 0.273 0.188 0.217 0.251 0.212 0.204 0.279 0.252 0.761 0.530 0.597 0.647 0.518 0.556 0.532 0.600 0.564 0.619 0.536 0.570 0.564 0.585 0.628 0.727 0. 0.555 0.097 0.235 0.337 0.058 0.109 0.105 0.244 0.166 0.204 0.111 0.167 0.231 0.160 0.325 0.455 0.637 345 541 541 541 543 543 543 543 2269 2446 281 64 905 141 / / 2.73 0.22 0.54 1.57 0.22 0.53 0.23 0.54 0.13 0.51 0.14 0.15 0.18 0.15 0.39 2.48 0.18 1000 1000 1000 1000 1000 1000 1000 1000 4000 4000 1000 500 2000 800 / / with the teachers outputs. Supporting evidence for this ablation study can be found in Table 8. (3) By shifting the training examples from few real image inputs to images generated by the 1-step student model, the teacher model is enabled to learn from the students distribution, distribution partially shaped by the discriminators design and characterized by image features not inherently present in the teacher model, thereby yielding beneficial effects."
        },
        {
            "title": "G Additional results on method comparison",
            "content": "In this section, we present additional quantitative and qualitative results to further validate the effectiveness and efficiency of our proposed method. Table 5 extends the comparisons from the main paper by reporting both training and inference time (in seconds), along with the number of optimization iterations required by each method. These results underscore the efficiency of our approach as one-step diffusion model, achieving image generation in just 0.18 seconds per instance during inference. Figures 6 through 9 provide additional qualitative comparisons against representative baseline methods. Each figure presents concept reference image (left) followed by results from various approaches. Our method, EchoDistill, consistently delivers superior visual fidelity while maintaining rapid inference, highlighting its practical advantages for real-time or resource-constrained applications in one-step diffusion-based image generation (1-SDP). Discussion on runtime cost. About the training time, this limitation is inherent to optimizationbased customization approaches, which universally require additional runtime computation when encountering novel concepts. Conversely, existing optimization-free methods, including encoderbased frameworks (IP-Adatper [105], DreamO [55], Xverse [11], UNO [98], InfiniteYou [36], etc.) and unified models (BAGEL [18], GPT-4o [1], OmniGen2 [97], etc.), demand extensive datasets for training. Furthermore, no encoder-based or unified model to date fully supports few-step (or even one-step) diffusion models. This leaves the integration of one-step models speed advantages with the versatile capabilities of unified models as an underexplored research direction. In this work, we aim to be the first to investigate the realization of one-step personalization via an optimization-based approach, with optimization-free alternatives designated as future work. Comparison with Flux+OminiControl.We further compare EchoDistill with the recent Flux [41] model combined with the OminiControl [82]. It is important to note that OminiControl is trained on large-scale datasets similar to IP-Adapter, which makes the evaluation against our method not entirely equitable. The evaluation is conducted on the DreamBooth dataset under the 1-step setup, and the results are summarized in the lower part of Table 5. As shown, Flux+OminiControl outperforms other baselines reported in the table; however, it remains significantly inferior to our proposed EchoDistill. This performance gap can be attributed to the weaker generation capability of Flux constrained to 1-step inference. 21 Table 6: Additional results for alternative designs on the DreamBooth dataset."
        },
        {
            "title": "Method",
            "content": "CLIP-T CLIP-I DINO Teacher-first Remove STE Ours (EchoDistill) 0.266 0.242 0.252 0.725 0.742 0.783 0.503 0.551 0.637 Table 7: User study for diverse methods."
        },
        {
            "title": "Method",
            "content": "Preference Rate (%) Custom Diffusion [39] Cones 2 [46] DisenBooth [12] EchoDistill 32.26% 0.48% 0.36% 66.90%"
        },
        {
            "title": "H Additional discussions for alternative designs",
            "content": "We further compare our method with several alternative designs in order to clarify the motivation and validity of our proposed framework. All experiments in this section are performed on the DreamBooth dataset. Teacher-first paradigm. In this design, the teacher is first trained, and the teacher-generated samples for the target concept (with varying text prompts) are directly used as supervised training data for the student. The identity loss (Eq. 2 in the main paper) is applied between the teacher-generated samples and the student outputs, allowing the student to learn identity-related features from the teacher model. As shown in Table 6, this design performs worse than our proposed approach. Moreover, it suffers from several inherent limitations: (1) Computational overhead: teacher inference requires multiple steps, which is inefficient; (2) Teacher irreliability: as discussed in the main paper, the teacher does not always successfully learn the target concepts; (3) Limited image diversity: the generated images consistently feature highly similar visual appearances; and (4) Performance ceiling: the students performance is inherently bounded by the capabilities of the teacher. In addition, we also attempted to directly apply VSD [95], SDS [63], and MSE losses to distill teacher-learned concepts into the student model under this paradigm. However, we observed that this approach was insufficient for transferring the teachers personalization capabilities to the student. Discussion on feed-forward customization methods. Beyond the teacher-first paradigm, an alternative direction is to build on feed-forward customization methods such as SynCD [38] and JeDi [107], and then distill these models into few-step diffusion framework. However, most existing distillation techniques for diffusion models are primarily designed to align the data distributions of few-step models with those of their teacher models. This emphasis stems from the inherent difficulty that few-step diffusion models face in replicating the full denoising trajectory of their teacher. As result, subtle discrepancies in concept-specific details are often introduced, as observed in prior works such as AYF [70], ADD [73], and LCM [49]. Remove STE. We further investigate the effect of sharing the text encoder between the teacher and student models. Removing the shared text encoder (STE) results in clear performance drop, demonstrating that STE not only simplifies the training framework but also improves learning efficiency."
        },
        {
            "title": "I User study",
            "content": "To evaluate alignment with human preferences, we conducted user study involving 15 participants, yielding 840 preference annotations per method. In each trial, participants were presented with set of generated images and instructed to select the best image from each group, considering both text-image alignment and object identity consistency. The methods evaluated in our study include Custom Diffusion, Cones 2, and DisenBooth, which demonstrate superior performance compared to other baseline approaches based on both qualitative and quantitative experimental results. As 22 Table 8: Ablation study of our method EchoDistill. Methods"
        },
        {
            "title": "Full model",
            "content": "w/o the teacher w/o all the discriminators w/o Identity Feature Loss w/o MSE loss w/o MS-SWD loss w/o the discriminator of the Dino v1 w/o the discriminator of the Dino v2 w/o the discriminator of the Clip CLIP Score CLIP-I DINO 0.252 0.240 0.200 0.248 0.242 0.249 0.231 0.246 0. 0.783 0.719 0.566 0.769 0.739 0.754 0.689 0.736 0.678 0.637 0.505 0.105 0.618 0.528 0.553 0.441 0.534 0. Table 9: One-shot performance of our EchoDistill. Methods CLIP-T CLIP-I DINO One-shot Few-shot 0.231 0.252 0.713 0.783 0.470 0.637 Figure 5: The qualitative results of the 1-shot performance. summarized in Table 7, our approach, EchoDistill, significantly outperformed all baselinesreceiving at least 34% more user votes than the second-best method. These findings underscore the strong alignment between EchoDistill outputs and human perceptual judgments."
        },
        {
            "title": "J Extended Ablation Study",
            "content": "In Section 4.3 of the main paper, we explore the contributions of key components in EchoDistill, namely the teacher model and discriminators. more detailed ablation study is presented in Table 8, wherein individual loss terms and discriminators are systematically removed. The results indicate that omitting the ID loss, latent MSE loss, or MSSWD loss causes notable performance degradation, particularly reflected in reduced DINO scores, underscoring their critical role in maintaining alignment with the teacher model. Furthermore, removal of any single discriminator leads to more pronounced declines across all evaluation metrics. Collectively, these findings demonstrate the complementary nature of the various loss functions and discriminators in improving generation fidelity and semantic consistency. Qualitative comparisons provided in Fig. 10 further illustrate the visual impact of removing each component. Beyond the general decline in generation quality, we observe that omission of certain components can induce training instability or divergence for specific concepts. 23 1-shot performance. We further evaluate our method under one-shot supervision setting, wherein only single image is utilized for training. As summarized in Table 9, performance declines across all evaluation metrics relative to the few-shot scenario. This degradation is anticipated, given that our approach is not explicitly optimized for one-shot learning, and the scarcity of supervisory data increases the likelihood of training instability. Qualitative results illustrated in Fig. 5 demonstrate that, although one-shot training can yield visually plausible outputs, the generated images occasionally lack fine-grained details corresponding to the novel concept. Results on the CustomConcept101 Dataset We further evaluate our method on the CustomConcept101 dataset [39]. Qualitative results, presented in Fig. 11 and Fig. 12, demonstrate that our approach generalizes effectively across diverse set of concepts and prompt types, consistently generating high-quality outputs. 24 Figure 6: Our method EchoDistill (last column) compared with existing methods applied to the 1-SDP setup with SDTurbo [73] as the one-step diffusion backbone. One representive concept image is shown on the left-most column. (Part 1) 25 Figure 7: Our method EchoDistill (last column) compared with existing methods applied to the 1-SDP setup with SDTurbo [73] as the one-step diffusion backbone. One representive concept image is shown on the left-most column. (Part 2) 26 Figure 8: Our method EchoDistill (last column) compared with existing methods applied to the 1-SDP setup with SDTurbo [73] as the one-step diffusion backbone. One representive concept image is shown on the left-most column. (Part 3) 27 Figure 9: Our method EchoDistill (last column) compared with existing methods applied to the 1-SDP setup with SDTurbo [73] as the one-step diffusion backbone. One representive concept image is shown on the left-most column. (Part 4) 28 Figure 10: Qualitative results of the extended ablation study. denotes the discriminator. 29 Figure 11: Qualitative results of EchoDistill on the CustomConcept101 dataset. Our method demonstrates strong generalization across variety of concepts and prompt styles. (Part 1) 30 Figure 12: Qualitative results of EchoDistill on the CustomConcept101 dataset. Our method demonstrates strong generalization across variety of concepts and prompt styles. (Part 2)."
        }
    ],
    "affiliations": [
        "City University of Hong Kong, HK SAR, China",
        "Computer Vision Center, Universitat Autònoma de Barcelona, Spain",
        "Harbin Institute of Technology (Shenzhen), China",
        "Program of Computer Science, City University of Hong Kong (Dongguan), China",
        "VCIP, CS, Nankai University, China"
    ]
}