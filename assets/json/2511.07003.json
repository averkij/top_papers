{
    "paper_title": "Beyond English: Toward Inclusive and Scalable Multilingual Machine Translation with LLMs",
    "authors": [
        "Yingfeng Luo",
        "Ziqiang Xu",
        "Yuxuan Ouyang",
        "Murun Yang",
        "Dingyang Lin",
        "Kaiyan Chang",
        "Tong Zheng",
        "Bei Li",
        "Peinan Feng",
        "Quan Du",
        "Tong Xiao",
        "Jingbo Zhu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models have significantly advanced Multilingual Machine Translation (MMT), yet the broad language coverage, consistent translation quality, and English-centric bias remain open challenges. To address these challenges, we introduce \\textbf{LMT}, a suite of \\textbf{L}arge-scale \\textbf{M}ultilingual \\textbf{T}ranslation models centered on both Chinese and English, covering 60 languages and 234 translation directions. During development, we identify a previously overlooked phenomenon of \\textbf{directional degeneration}, where symmetric multi-way fine-tuning data overemphasize reverse directions (X $\\to$ En/Zh), leading to excessive many-to-one mappings and degraded translation quality. We propose \\textbf{Strategic Downsampling}, a simple yet effective method to mitigate this degeneration. In addition, we design \\textbf{Parallel Multilingual Prompting (PMP)}, which leverages typologically related auxiliary languages to enhance cross-lingual transfer. Through rigorous data curation and refined adaptation strategies, LMT achieves SOTA performance among models of comparable language coverage, with our 4B model (LMT-60-4B) surpassing the much larger Aya-101-13B and NLLB-54B models by a substantial margin. We release LMT in four sizes (0.6B/1.7B/4B/8B) to catalyze future research and provide strong baselines for inclusive, scalable, and high-quality MMT \\footnote{\\href{https://github.com/NiuTrans/LMT}{https://github.com/NiuTrans/LMT}}."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 3 0 0 7 0 . 1 1 5 2 : r Beyond English: Toward Inclusive and Scalable Multilingual Machine Translation with LLMs Yingfeng Luo1*, Ziqiang Xu1*, Yuxuan Ouyang1, Murun Yang1, Dingyang Lin1 Kaiyan Chang1, Tong Zheng1, Bei Li1, Peinan Feng1, Quan Du2, Tong Xiao1,2, Jingbo Zhu1,2 1 School of Computer Science and Engineering, Northeastern University, Shenyang, China 2 NiuTrans Research, Shenyang, China luoyingfeng_neu@outlook.com {xiaotong,zhujingbo}@mail.neu.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Large language models have significantly advanced Multilingual Machine Translation (MMT), yet the broad language coverage, consistent translation quality, and English-centric bias remain open challenges. To address these challenges, we introduce LMT, suite of Large-scale Multilingual Translation models centered on both Chinese and English, covering 60 languages and 234 translation directions. During development, we identify previously overlooked phenomenon of directional degeneration, where symmetric multi-way finetuning data overemphasize reverse directions (X En/Zh), leading to excessive many-toone mappings and degraded translation quality. We propose Strategic Downsampling, simple yet effective method to mitigate this degeneration. In addition, we design Parallel Multilingual Prompting (PMP), which leverages typologically related auxiliary languages to enhance cross-lingual transfer. Through rigorous data curation and refined adaptation strategies, LMT achieves SOTA performance among models of comparable language coverage, with our 4B model (LMT-60-4B) surpassing the much larger Aya-101-13B and NLLB-54B models by substantial margin. We release LMT in four sizes (0.6B/1.7B/4B/8B) to catalyze future research and provide strong baselines for inclusive, scalable, and high-quality MMT 1."
        },
        {
            "title": "Introduction",
            "content": "Multilingual machine translation (MMT) seeks to support translation among multiple languages with unified model, has long been pursued for its parameter efficiency and strong potential for crosslingual transfer (Johnson et al., 2017; Arivazhagan et al., 2019; Aharoni et al., 2019; Fan et al., 2021; Costa-jussà et al., 2022). The emergence of large * Equal contribution. Corresponding author. 1https://github.com/NiuTrans/LMT 1 LLM for MT #CPT #Langs Zhcentric Base Model BigTranslate (Yang et al., 2023) ALMA (Xu et al., 2024) TowerInstruct (Alves et al., 2024) Guo et al. (2024) X-ALMA (Xu et al., 2025) GemmaX2 (Cui et al., 2025) Hunyuan-MT (Zheng et al., 2025a) Seed-X (Cheng et al., 2025) LMT (Ours) 90B 20B 20B 120G 40B 54B - 200B 90B 102 6 10 3 50 28 33 28 60 LLaMA LLaMA-2 LLaMA-2 LLaMA-2 LLaMA-2 Gemma-2 Hunyuan-7B - QwenTable 1: Comparison of typical LLM-based MMT models. We summarize their total number of continued pretraining (CPT) data, supported languages, support for Zh-centric translation, and the base models used. language models (LLMs) has significantly accelerated these goals due to their powerful generative and cross-lingual generalization capabilities (Brown et al., 2020; Dubey et al., 2024; Yang et al., 2025). This momentum has spurred an active line of work on adapting LLMs for MMT (Yang et al., 2023; Alves et al., 2024; Xu et al., 2025; Cui et al., 2025; Luo et al., 2025; Zheng et al., 2025b), as summarized in Table 1. While these efforts have advanced LLM-based MMT, challenges remain in scaling language coverage and maintaining consistent quality across languages, which are two critical attributes of an ideal MMT system. These persistent limitations often stem from combination of factors, including the scarcity and imbalance of high-quality training data, suboptimal adaptation strategies, and dependence on outdated or less capable backbone models (Zhu et al., 2024b; Xu et al., 2024; Richburg and Carpuat, 2024). Beyond these issues lies more fundamental, often overlooked problem: the pervasive English-centric bias inherited from pre-training corpora dominated by English. Consequently, research attention and system design in the MMT community remain excessively English-centric, leaving many languages underserved. Chinese, in particular, despite being one of the most widely spoken languages with substantial real-world translation demand, illustrates this gap in todays LLM-based MMT landscape. Addressing these gaps is non-trivial and involves interdependent challenges in both data and training. On the data side, the long-tail distribution of internet corpora leads to foundational LLMs developing uneven modeling competence across languages, with pronounced weaknesses in low-resource ones (see Figure 1, top). This imbalance is further amplified in bilingual settings, where available parallel data are particularly scarce for Chinese-centric pairs (see Figure 1, bottom), thereby constraining the effectiveness of supervised adaptation. On the training side, how to design strategies to effectively leverage data to build robust and consistent translation capabilities is still under investigation. 59 languages and Chinese To tackle these challenges, we build on the esSupertablished Continued Pre-training (CPT) vised Fine-tuning (SFT) pipeline and present LMT, Large-scale Multilingual machine Translation model centered on both Chinese and English. LMT covers 60 languages and 234 translation directions, including English 58 languages. To support effective CPT, we developed comprehensive multi-stage data pipeline comprising large-scale collection, pseudo-parallel synthesis, and multi-dimensional filtering, which lays solid foundation for multilingual adaptation. During SFT, we uncover previously overlooked issue, directional degeneration, where training with symmetric multi-way data degrades performance in En/Zh directions. Our analysis traces this degeneration to excessive many-to-one mappings, and we propose simple yet effective Strategic Downsampling method to mitigate it. In addition, to enhance cross-lingual transfer, we design Parallel Multilingual Prompting (PMP), which augments the instruction with parallel sentence from either English or typologically similar auxiliary language, as illustrated in Figure 3. By combining rigorous data curation with refined adaptation strategies, LMT achieves SOTA performance among models with similar language coverage. Notably, our LMT-60-4B surpasses the much larger Aya-101-13B and NLLB-54B models by substantial margin. In summary, our contributions are threefold: We identify and analyze previously overlooked issue, directional degeneration, in large-scale multilingual SFT with multi-way 2 Figure 1: Top: Performance of base LLMs (orange) on the Belebele benchmark across 108 languages, plotted against their data ratios in the CulturaX (blue). Bottom: Bilingual data volume (million sentence pairs) from the OPUS corpus for 60 languages in our study, covering English-centric (blue) and Chinese-centric (orange) directions. Languages are grouped into high-, medium-, and low-resource tiers. data, and propose Strategic Downsampling, simple yet practical mitigation strategy. We introduce Parallel Multilingual Prompting (PMP), an intuitive and effective method that enhances cross-lingual transfer by augmenting instructions with auxiliary language. We release LMT, suite of ChineseEnglishcentric multilingual translation models in four sizes (0.6B/1.7B/4B/8B) covering 60 languages and 234 directions, providing strong baselines for future MMT research."
        },
        {
            "title": "2 Related Work",
            "content": "The Paradigm Shift: From NMT to LLM-based MT Machine translation has undergone major paradigm shift over the past decade. The introduction of the Transformer architecture (Vaswani et al., 2017) marked the beginning of the Neural Machine Translation (NMT) era, leading to the development of large-scale MMT systems such as M2M-100 (Fan et al., 2021) and NLLB (Costajussà et al., 2022). More recently, the landscape has been reshaped by the rise of pre-trained Large Language Models (LLMs) (Brown et al., 2020; Dubey et al., 2024; Yang et al., 2025). Unlike task-specific NMT systems, LLMs are general-purpose foundation models that can be adapted for wide range Figure 2: An overview of our methodology for LMT. The pipeline consists of two main stages: hybrid data curation process (top) to build the training corpus, and two-stage adaptation (bottom) involving CPT and SFT. of tasks, including translation. This has catalyzed paradigm shift from training bespoke NMT models to adapting existing pre-trained LLMs (Yang et al., 2023; Alves et al., 2024; Xu et al., 2025; Cui et al., 2025), bringing not only stronger translation quality but also advanced capabilities such as context awareness (Wang et al., 2024, 2025b) and reasoning (Wang et al., 2025a; Chen et al., 2025). LLM Adaptation for MMT The standard practice for adapting LLMs to MMT involves multistage pipeline, typically consisting of Continued Pre-training (CPT) and Supervised Fine-tuning (SFT). In CPT, studies explore different data formulations. Some emphasize monolingual data to enhance language modeling (Xu et al., 2024), while others integrate bilingual data to inject translationspecific knowledge (Alves et al., 2024; Cui et al., 2025). SFT further aligns models with humancurated datasets, often through instruction tuning (Wei et al., 2022). Despite steady progress, several gaps persist. First, the potential pitfalls of symmetric multi-way data, which are commonly used for broad language coverage during SFT, remain underexplored, particularly their impact on directional robustness. Second, while instruction tuning is widely adopted, prompting strategies that explicitly encourage cross-lingual transfer are still limited. Finally, much of the current research remains English-centric, with only few recent works extending support to Chinese-centric translation (Cui et al., 2025; Zheng et al., 2025a; Cheng et al., 2025). However, these models still fall short in both language coverage and directional diversity. Our work aims to bridge these gaps through systematic data curation and targeted adaptation strategies for largescale, ChineseEnglish-centered MMT."
        },
        {
            "title": "3.1 The LMT Adaptation Framework",
            "content": "LMT is designed to cover 60 languages across diverse linguistic families, scripts, and resource conditions. The model is Chinese-English-centric, supporting 234 translation directions in total. To promote inclusivity, we include major regional and minority languages of China: Uyghur, Tibetan, Mongolian (traditional script), and Cantonese (Yue). complete list is available in the Appendix Section C. Motivated by our preliminary analysis (as shown in Figure 1, top), which demonstrates Qwen3 (Yang et al., 2025) offers more comprehensive multilingual performance than Llama3.1 (Dubey et al., 2024) and Gemma2 (Rivière et al., 2024), we selected Qwen3 as the backbone across sizes. We adopt the two-stage adaptation frameworkCPT followed by SFT, as illustrated in Figure 2. CPT operates on large-scale mixture of monolingual and parallel corpora to integrate wide-coverage translation knowledge. SFT then fine-tunes the model on carefully curated dataset of high-quality, humantranslated parallel sentences formatted as instructions to further improve faithfulness and adequacy."
        },
        {
            "title": "3.2 Data Curation",
            "content": "The foundation of an effective MMT model relies on high-quality, large-scale, and diverse corpus. 3 Recognizing that constructing such corpus is major challenge, we designed systematic, multistage data curation pipeline to ensure both linguistic breadth and quality consistency. CPT Monolingual Data To achieve comprehensive coverage and diversity, our monolingual CPT corpus aggregates several sources. For English and Chinese, we utilized the well-curated SlimPajama (Soboleva et al., 2023) and Skywork (Wei et al., 2023) corpora, respectively. For the remaining languages, we collected from CulturaX (Nguyen et al., 2024), OpenDataLab (Yu et al., 2025), and Wikimedia, with full source details in Appendix Table 5. CPT Bilingual Data The foundation of the bilingual corpus is curated from OPUS 2 sub-corpora. To significantly scale this up, we employed pseudoparallel synthesis using open-source models in two ways: (1) direct synthesis, creating synthetic En/Zh data by translating monolingual corpora, and (2) pivoted synthesis via English, leveraging typically higher-quality En Zh models and En data. Finally, all data was unified to obtain Zh and subjected to rigorous quality control pipeline, using OpusFilter (Aulamo et al., 2020) for heuristic cleaning and CometKiwi (Rei et al., 2022b) for quality-based scoring and selection (see Figure 2). This yields approximately 2.1B sentence pairs for English-centric and 2.9B for Chinese-centric directions, with the vast majority comprising over 10M high-quality pairs across the 117 targeted directions. Appendix Figures 10 and 11 visualize their final statistics and score distributions. SFT Data Prior studies emphasize that data quality and diversity are crucial for successful SFT (Zhou et al., 2023; Zhu et al., 2024a). Guided by these findings, our SFT dataset compiles translation set from carefully curated human-translated data. We begin with the widely used multi-way parallel datasets Flores-200 devset (Costa-jussà et al., 2022) and NTREX (Federmann et al., 2022) to ensure broad language coverage. We then complement the training data with SMol (Caswell et al., 2025), which provides additional coverage for underrepresented language pairs. Finally, we include the WMT1423 and IWSLT1724 test sets to enhance style and domain diversity. The resulting SFT dataset contains approximately 596K pairs, strategically allocating 3K20K high-quality examples per direction across 117 directions. 2https://opus.nlpl.eu/ Figure 3: Examples of the three prompt formats for the CPT and SFT stages of LMT adaptation. The underlined text indicates the part used for loss computation during training."
        },
        {
            "title": "3.3 Parallel Multilingual Prompting",
            "content": "An advantage of MMT models is their ability to facilitate cross-lingual transfer, where knowledge learned from high-resource languages can benefit low-resource ones. However, this mechanism typically emerges as an implicit and subtle consequence of joint training on large, mixed-language corpora. To harness this potential, we introduce Parallel Multilingual Prompting (PMP), which explicitly enables knowledge transfer through auxiliary parallel context. PMP is applied specifically to mediumand low-resource translation directions, where an auxiliary parallel sentence is selected from high-resource languages that are typologically related for English-centric cases and English for Chinese-centric cases. This auxiliary context serves as semantic and lexical anchor, guiding the model toward higher-fidelity translations. Formulation. Let be source sentence in LS and the target in LT . standard translation prompt (STP) optimizes (cid:0)T (cid:12) (cid:12) S; τLSLT log Pθ (cid:1), max θ where τLSLT denotes the translation direction. PMP augments the context with an auxiliary sentence in LA: log Pθ (cid:0)T (cid:12) (cid:12) S, A; τLSLALT (cid:1), max θ yielding many-to-one mapping + [A] . Figure 3 compares these two prompting strategies. 4 Selecting the Auxiliary Language LA We select LA with two complementary policies: (1) For En directions, we select LA from the high-resource language that is typologically simX directions, we consisilar to X. (2) For Zh tently set LA = En to leverage strong English priors as universal semantic anchor. This strategy echoes the classic pivot translation paradigm, where English serves as an intermediary between non-English pairs. However, unlike traditional twostep pipelines (e.g., Zh), which discard original source information in the second step, PMP integrates the English translation as an in-context hint alongside the source, allowing the model to use the English as semantic guide while preserving the sources nuances. PMP is applied only when suitable high-resource counterpart exists; otherwise, the model defaults to STP. The full list of participating languages and their auxiliary partners is detailed in the Table 7."
        },
        {
            "title": "En then En",
            "content": "Training and Inference During SFT, PMP and STP are mixed with probability, thereby balancing direct translation proficiency and cross-lingual transfer. The PMP data are easily constructed from multi-way parallel data (e.g., Flores-200), allowing us to reuse the same underlying data to create these context-enriched training examples. At inference, PMP is optional: by default, the model translates using standard STP-style prompts. When an auxiliary sentence is provided, either from high-quality external MT system or self-generated, PMP can be activated to further enhance translation robustness and adequacy."
        },
        {
            "title": "Multilingual SFT",
            "content": ""
        },
        {
            "title": "X and X",
            "content": "We start from the Qwen3-4B-Base model and perform standard supervised fine-tuning on our SFT data to establish baseline. Following common practice, each parallel pair is used in both directions (e.g., En En). While this standard approach yielded expected improvements in directions, it unexpectedly resulted the En/Zh in significant performance drop in the reverse En/Zh directions. Qualitative analysis revealed that while the generated outputs were fluent, they frequently exhibited unfaithfulness, hallucinating content not grounded in the source (see Table 3). We term this phenomenon directional degeneration. We hypothesize that it arises from ShalFigure 4: The impact of the Strategic Downsampling proportion (p). Dashed lines represent the use of sepaEn/Zh directions. rate, non-symmetric data for the low Mapping Trap, induced by the multi-way data structure in which each English or Chinese sentence can be repeated up to 59 times as the target of different sources. This excessive many-to-one structure in the training data incentivizes the model to learn shortcut: mapping diverse source sentences to limited, high-frequency set of English or Chinese target patterns, thereby undermining source faithfulness and adequacy. To probe the hypothesis, we conduct an oracle experiment: we replace the reverse En/Zh portion of the SFT data with separately sampled, nonoverlapping subset from our bilingual CPT data, unchanged. As shown while keeping En/Zh by the dashed curves in Figure 4, En/Zh performance returns to stable level relative to the fully symmetric baseline (i.e., p=100), suggesting that the collapse stems from symmetric reuse rather than the inherent difficulty of direction itself."
        },
        {
            "title": "4.2 Mitigation via Strategic Downsampling",
            "content": "Although the oracle experiment addresses this issue, it depends on an external data pool, which limits its practicality. To overcome this constraint, we propose simpler and more practical method: Strategic Downsampling. Let be the retention En/Zh samples. During SFT, probability for instances (100%), while we retain all En/Zh including each En/Zh instance with probability p. As shown by the solid lines in Figure 4, varying reveals that even very small reverse retention (e.g., p=, 0.5%) is sufficient to prevent degeneration. This indicates that only minimal alignment signal is needed for En/Zh, given the strong target-side priors in English/Chinese. Performance peaks around 5%, and then gradually declines as target-side repetition increases. Based on this observation, we adopt p=5% for all SFT models. 5 Prior work (Zheng et al., 2025b) reports similar asymmetric degradation but does not analyze its underlying cause, and addresses the issue through model-level remedies (direction-aware training and group-wise model merging). Going beyond this, we identify the root cause of the problem, namely the excessive many-to-one mappings induced by symmetric multi-way data, and show that simple data-level solution is sufficient to mitigate it. comprehensive analysis is provided in Appendix A."
        },
        {
            "title": "5.1 Setup",
            "content": ""
        },
        {
            "title": "X and X",
            "content": "Training Details For CPT, we use 90B tokens balanced at 1:1:1 ratio across monolingual, Chinesecentric bilingual, and English-centric bilingual data. The bilingual samples are used equally in both directions (En/Zh En/Zh; 50/50), and adopt an Informative Formatting with explicit direction tags and target-language separator (as shown in Figure 3), which we find performs better than naïve newline sourcetarget concatenation (Guo et al., 2024; Iyer et al., 2024). During SFT, for forward directions (En/Zh X), STP and PMP each account for 50%. For reverse directions (X En/Zh), we apply strategic downsampling with total retention of 5%, split evenly between formats (STP 2.5%, PMP 2.5%). We train four model sizes (0.6B, 1.7B, 4B, and 8B) on 16 NVIDIA H200 GPUs using the ms-swift framework (Zhao et al., 2024). The 4B and 8B models consume over 2,500 and 4,000 GPU-hours, respectively. Detailed hyperparameters are provided in Appendix Table 4. Evaluation Data and Metrics We evaluate on FLORES-200 devtest (Costa-jussà et al., 2022). To address the lack of Chinese Mongolian testset, we translated the Chinese side of FLORES into Mongolian using native annotators under rigorous protocol 3. We adopt COMET-22 (Rei et al., 2022a) as our primary evaluation metric, and report SacreBLEU (Post, 2018) in the Appendix. In addition, we also provide comparison against the WMT24++ (Deutsch et al., 2025) in the Appendix. Models for Comparison We compare LMT against two categories of models: (1) Generalpurpose Multilingual LLMs capable of instructionfollowing translation, including Aya-Expanse-8B (Dang et al., 2024), Aya-101-13B (Üstün et al., 2024), and LLaMAX3-8B-Alpaca (Lu et al., 3We will release this dataset to fill gap in MT Benchmark. (2) Dedicated MMT Models, including 2024). TowerInstruct-13B (Alves et al., 2024), GemmaX228-9B (Cui et al., 2025), X-ALMA-13B (Xu et al., 2025), Hunyuan-MT-7B (Zheng et al., 2025a), Seed-X-PPO-7B (Cheng et al., 2025) , and NLLB54B (Costa-jussà et al., 2022). For fairness, evaluation is conducted on the intersection of language pairs supported by each baseline and LMT, covering Chinese-centric directions when applicable. For brevity, we present LMT-60-4/8B in the main text, with full results for all four sizes (0.6B/1.7B/4B/8B) provided in the Appendix"
        },
        {
            "title": "5.2 Overall Performance",
            "content": "Table 2 summarizes the overall evaluation results. Our proposed LMT models demonstrate strong and consistent performance across most translation directions, placing them among the top-tier of opensource MMT models. Within the 5060 language scale, our model achieves SOTA performance. Notably, the LMT-60-4B surpasses substantially larger models such as Aya-101-13B and NLLB-54. For inZh direction, LMT-60-4B outperstance, in the forms the 13 larger NLLB-54B by 7.5 COMET points, demonstrating exceptional parameter efficiency. When narrowed to 1030 medium-scale languages, LMT still maintains leading accuracy in most translation directions. Our LMT-60-8B ranks just behind Seed-X-PPO-7B. However, this gap can largely be explained by the difference in training data: LMT covers more than twice as many languages (60 vs. 27) while being trained on less than half the total data (90B vs. 200B tokens), resulting in roughly 20% of the data per language. Taken together, these results demonstrate that LMT delivers robust quality at scale, covering 60 languages and 234 directions, substantially improving performance on mediumand low-resource languages while maintaining strong results on highresource ones. This finding indicates that ChineseEnglish-centric large-scale MMT with LLMs is both practical and competitive, even compared with larger or more narrowly focused systems."
        },
        {
            "title": "5.3 Ablation Study",
            "content": "To quantify the contributions of each component in our adaptation pipeline, we conduct an ablation study on our 4B model. Figure 5 presents the results of incrementally adding Strategic Downsampling (SD), Continued Pre-training (CPT), and Parallel Multilingual Prompting (PMP) to naïve Base + SFT baseline. Each bar in the figure repre- # Langs (Hig./Med./Low) 10 (7/3/0) 23 (13/9/1) 27 (13/12/2) 28 (13/8/7) 35 (13/9/13) 40 (13/16/11) 54 (13/18/23) 55 (13/18/24) 59 (13/18/28) High Resource Medium Resource Low Resource Model En En Zh Zh En En Zh Zh En TowerInstruct-13B 88.78 88.79 LMT-60-4B 89.05 LMT-60-8B 88.51 88.48 88.64 86.15 86.63 86.87 86.83 87.86 87.92 89.92 89.77 90.20 88.53 88.75 88. 87.44 87.68 88.06 86.66 87.67 87.87 - - - - - - En Zh Zh - - - - - - Aya-expanse-8B LMT-60-4B LMT-60-8B Seed-X-PPO-7B LMT-60-4B LMT-60-8B GemmaX2-28-9B LMT-60-4B LMT-60-8B Hunyuan-MT-7B LMT-60-4B LMT-60-8B X-ALMA-13B LMT-60-4B LMT-60-8B Aya-101-13B LMT-60-4B LMT-60-8B 88.60 89.10 89.41 89.91 89.10 89. 89.34 89.10 89.41 89.43 89.10 89.41 89.41 89.10 89.41 87.00 89.10 89.41 LLaMAX3-Alpaca 85.22 89.10 LMT-60-4B 89.41 LMT-60-8B NLLB-54B LMT-60-4B LMT-60-8B 87.95 89.10 89.41 88.16 88.38 88.57 88.59 88.38 88.57 88.62 88.38 88.57 87.56 88.38 88.57 88.51 88.38 88. 86.55 88.38 88.57 87.19 88.38 88.57 88.17 88.38 88.57 86.44 86.85 87.14 87.73 86.85 87.14 86.99 86.85 87. 87.08 86.85 87.14 - - - 84.34 86.85 87.14 82.28 86.85 87.14 85.82 86.85 87.14 86.32 87.57 87. 87.94 87.57 87.67 87.66 87.57 87.67 87.38 87.57 87.67 - - - 83.29 87.57 87.67 82.49 87.57 87. 80.06 87.57 87.67 88.82 89.26 89.61 91.58 90.75 91.03 88.71 88.54 88.86 89.10 88.75 89.06 90.37 89.95 90. 87.54 89.86 90.13 84.80 89.86 90.13 88.95 89.86 90.13 88.50 88.77 88.94 89.48 89.21 89.33 89.46 88.93 89. 87.76 88.80 88.98 89.29 89.06 89.21 87.32 89.02 89.18 87.87 89.02 89.18 88.85 89.02 89.18 86.19 86.52 86. 89.25 88.43 88.76 85.82 85.60 85.91 86.29 85.94 86.26 - - - 84.26 87.04 87.42 80.90 87.04 87. 85.58 87.04 87.42 86.13 87.14 87.27 88.07 87.64 87.72 87.59 87.30 87.50 86.77 87.24 87.42 - - - 83.26 87.32 87.45 82.23 87.32 87.45 80.69 87.32 87.45 87.82 88.49 88.98 91.27 90.63 90.81 86.42 86.35 86. 82.74 87.01 87.34 87.15 86.94 87.27 81.68 86.26 86.58 76.69 86.43 86.75 85.12 86.92 87.23 88.41 88.58 88. 89.22 88.97 89.14 88.93 87.90 88.32 84.43 86.71 87.17 87.98 88.12 88.39 85.71 87.31 87.72 84.60 87.26 87. 86.81 86.77 87.20 84.76 85.06 85.51 88.78 88.02 88.40 82.81 82.85 83.36 79.27 83.96 84.31 - - - 77.92 82.68 83.10 71.97 82.95 83.37 82.33 83.81 84.20 85.28 86.61 86.71 87.39 87.23 87.34 86.63 85.84 86. 83.50 85.10 85.54 - - - 81.58 85.19 85.59 79.38 85.25 85.61 80.56 85.06 85.48 Table 2: COMET-22 scores of our LMT models compared with range of general-purpose multilingual LLMs and dedicated MMT models . Languages are grouped into high-, medium-, and low-resource categories. Evaluation is conducted only on the intersection of language pairs supported by each baseline and our models. The first column (# Langs) denotes the number of overlapping languages, followed by their distribution across resource tier (high/medium/low). Bold numbers indicate the best in each group, and the underlined numbers the second best. sents the COMET score for specific translation direction, averaged across all 60 languages. The Base + SFT model first reaffirms the directional degeneration identified in Section 4. Introducing SD yields remarkable improvement Zh direction and +5.83 +11.45 points in the in the En direction, highlighting its effectiveness in addressing degeneration. Next, adding the CPT stage provides substantial, foundational uplift across all directions, with gains ranging from +3.80 to +8.23 points, underscoring its central contribution in strengthening the models broad translation capabilities 4. Finally, PMP provides steady boost on top of the CPT-enhanced model across all directions, confirming its broad effectiveness. Overall, this component-wise analysis demonstrates that LMTs overall performance stems from the synergistic effect among three core elements: 4Detailed analyses are provided in Appendix C. 7 robust CPT foundation, critical SFT mitigation for degeneration, and an effective prompting strategy. Together, these components are essential for adapting foundation LLMs into high-coverage, robust MMT systems."
        },
        {
            "title": "5.4 Understanding the Effect of PMP",
            "content": "Inference-Time Strategy Analysis To examine how PMP behaves under different inference-time configurations, we evaluate four decoding strategies on languages involved in PMP training: (1) Pivot Translation (PT)-the traditional two-step pipeline via English pivot; (2) Direct Translation (DT)-the standard baseline; (3) PMP-O (Oracle)- using gold auxiliary translations; and (4) PMPS (Self-Generated)-using self-generated auxiliary translations. Average results are reported in Figure 6 (left). Both PMP-O and PMP-S outperform the PT and DT baselines, but the improvements Figure 5: Ablation study on the impact of each component: Strategic Downsampling (SD), Continued Pretraining (CPT), and Parallel Multilingual Prompting (PMP). The annotated values quantify the performance gain of each component over the preceding one. are modare asymmetric: gains for En/Zh est, whereas gains for En/Zh are substantially larger, suggesting that the auxiliary parallel context mainly enhances source-side comprehension and semantic grounding. Besides, while PMP-O leads in the En/Zh directions as expected, PMP-S surprisingly achieves the best performance for En/Zh. This reversal likely stems from the Strategic Downsampling applied to these directions, which creates sparsity in PMP training examples. Consequently, the model appears to benefit more from self-generated hint aligned with its internal biases, rather than perfect but out-of-distribution oracle. Moreover, since PMP-S requires no external resources, it can be readily applied at inference time to further enhance translation performance. Zero-Shot Transfer Effect Analysis To evaluate the zero-shot transfer effects of PMP, we compare our model with counterpart trained without PMP. We categorize languages into three groups based on their level of involvement in PMP: (1) In-Group: between PMP-enhanced languages and their corresponding high resource language (HRL) auxiliaries; (2) Out-of-Group: between PMP-enhanced languages and the HRLs not used as auxiliary, testing generalization beyond direct PMP supervision; (3) Baseline Group: among HRLs not involved in PMP, serving as baseline for zero-shot gain (dashed line). As shown in Figure 6 (right), the most pronounced improvements appear in the HRL LRL direction. The gains are strongest for InGroup pairs (+1.8), while also extending notably to Out-of-Group settings (+0.7), indicating that the 8 Figure 6: Analysis of the Parallel Multilingual Prompting (PMP). Left: Comparison of different inferencetime strategies. Right: Zero-shot transfer gains from PMP training. The bars show the performance difference between our final model and the model trained without PMP. learned alignment generalizes beyond directly supervised language pairs. Interestingly, the Baseline Group (dashed line) exhibits positive gain, suggesting that PMP induces global enhancement to cross-lingual mappings. Apart from slight deviation in the LRL HRL direction within the In-Group tier, the overall results confirm that PMP functions as broadly beneficial mechanism for boosting zero-shot translation capabilities."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we take step toward overcoming the prevailing English-centric bias in MMT, achieving both broad language coverage and consistently high translation quality. We introduce LMT, Chinese-English-centric MMT model, covering 60 languages across 234 translation directions and achieving SOTA performance among models with similar language coverage. Our adaptation pipeline begins with the construction of large and rigorously curated multilingual corpus, followed by extensive continued pre-training to integrate broad translation knowledge into the model. We then identify directional degeneration, salient yet previously overlooked issue in SFT with multi-way data, and mitigate it by introducing practical strategic downsampling method. Furthermore, we propose Parallel Multilingual Prompting, simple but effective technique to enhance cross-lingual transfer. Together, these contributions establish LMT as competitive and robust baseline for largescale multilingual translation. We release the LMT model suite to support future research toward more inclusive, scalable, and high-quality MMT."
        },
        {
            "title": "Limitations",
            "content": "While our LMT models demonstrate strong and robust performance, we acknowledge several limitations that primarily point to exciting avenues for future research. First, our evaluation, while comprehensive, is primarily conducted on academic benchmarks with COMET. Future work could extend this evaluation to wider range of real-world scenarios to further assess generalization and capture more nuanced aspects of translation quality, such as cultural appropriateness. Second, our bicentric design, which already moves beyond English, could be extended in future work to trior multi-centric configurations to better serve other major linguistic or geopolitical regions and further the goal of truly inclusive MMT ecosystem. Third, although LMT currently supports 60 languages, this still represents small fraction of the worlds linguistic diversityover 7,000 languages exist globally, with only few hundred having substantial written corpora. Expanding support to additional and underrepresented languages remains crucial step toward more equitable and inclusive multilingual translation systems."
        },
        {
            "title": "References",
            "content": "Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019. Massively multilingual neural machine translation. In NAACL-HLT (1), pages 38743884. Association for Computational Linguistics. Duarte M. Alves, José Pombal, Nuno Miguel Guerreiro, Pedro Henrique Martins, João Alves, M. Amin Farajian, Ben Peters, Ricardo Rei, Patrick Fernandes, Sweta Agrawal, Pierre Colombo, José G. C. de Souza, and André F. T. Martins. 2024. Tower: An open multilingual large language model for translation-related tasks. CoRR, abs/2402.17733. Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, Maxim Krikun, Mia Xu Chen, Yuan Cao, George F. Foster, Colin Cherry, Wolfgang Macherey, Zhifeng Chen, and Yonghui Wu. 2019. Massively multilingual neural machine translation in the wild: Findings and challenges. CoRR, abs/1907.05019. Mikko Aulamo, Sami Virpioja, and Jörg Tiedemann. 2020. OpusFilter: configurable parallel corpus In Proceedings of the 58th Anfiltering toolbox. nual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 150156. Association for Computational Linguistics. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, and 12 others. 2020. Language models are few-shot learners. In NeurIPS. Laurie Burchell, Ona de Gibert, Nikolay Arefyev, Mikko Aulamo, Marta Bañón, Mariia Fedorova, Liane Guillou, Barry Haddow, Jan Hajiˇc, Erik Henriksson, and 1 others. 2025. An expanded massive multilingual dataset for high-performance language technologies. arXiv preprint arXiv:2503.10267. Isaac Caswell, Elizabeth Nielsen, Jiaming Luo, Colin Cherry, Geza Kovacs, Hadar Shemtov, Partha Talukdar, Dinesh Tewari, Baba Mamadi Diane, Koulako Moussa Doumbouya, Djibrila Diane, Solo Farabado Cissé, Edoardo Ferrante, Alessandro Guasoni, Mamadou K. Keita, Sudhamoy DebBarma, Ali Kuzhuget, David Anugraha, Muhammad Ravi Shulthan Habibi, and 3 others. 2025. SMOL: Professionally translated parallel data for 115 underrepresented languages. Preprint, arXiv:2502.12301. Andong Chen, Yuchen Song, Wenxin Zhu, Kehai Chen, Muyun Yang, Tiejun Zhao, and Min Zhang. 2025. Evaluating o1-like llms: Unlocking reasoning for translation through comprehensive analysis. CoRR, abs/2502.11544. Shanbo Cheng, Yu Bao, Qian Cao, Luyang Huang, Liyan Kang, Zhicheng Liu, Yu Lu, Wenhao Zhu, Jingwen Chen, Zhichao Huang, Tao Li, Yifu Li, Huiying Lin, Sitong Liu, Ningxin Peng, Shuaijie She, Lu Xu, Nuo Xu, Sen Yang, and 7 others. 2025. Seed-x: Building strong multilingual translation LLM with 7b parameters. CoRR, abs/2507.13618. Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Y. Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loïc Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, and 19 others. 2022. No language left behind: Scaling human-centered machine translation. CoRR, abs/2207.04672. Menglong Cui, Pengzhi Gao, Wei Liu, Jian Luan, and Bin Wang. 2025. Multilingual machine translation with open large language models at practical scale: An empirical study. In NAACL (Long Papers), pages 54205443. Association for Computational Linguistics. John Dang, Shivalika Singh, Daniel Dsouza, Arash Ahmadian, Alejandro Salamanca, Madeline Smith, Aidan Peppin, Sungjin Hong, Manoj Govindassamy, Terrence Zhao, Sandra Kublik, Meor Amer, Viraat Aryabumi, Jon Ander Campos, Yi-Chern Tan, Tom Kocmi, Florian Strub, Nathan Grinsztajn, Yannis FletBerliac, and 26 others. 2024. Aya expanse: Combining research breakthroughs for new multilingual frontier. Preprint, arXiv:2412.04261. 9 Daniel Deutsch, Eleftheria Briakou, Isaac Caswell, Mara Finkelstein, Rebecca Galor, Juraj Juraska, Geza Kovacs, Alison Lui, Ricardo Rei, Jason Riesa, Shruti Rijhwani, Parker Riley, Elizabeth Salesky, Firas Trabelsi, Stephanie Winkler, Biao Zhang, and Markus Freitag. 2025. WMT24++: Expanding the Language Coverage of WMT24 to 55 Languages & Dialects. Preprint, arXiv:2502.12404. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, and 82 others. 2024. The llama 3 herd of models. CoRR, abs/2407.21783. Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Michael Auli, and Armand Joulin. 2021. Beyond english-centric multilingual machine translation. J. Mach. Learn. Res., 22:107:1107:48. Christian Federmann, Tom Kocmi, and Ying Xin. 2022. NTREX-128 news test references for MT evaluation of 128 languages. In Proceedings of the First Workshop on Scaling Up Multilingual Evaluation, pages 2124, Online. Association for Computational Linguistics. Jiaxin Guo, Hao Yang, Zongyao Li, Daimeng Wei, Hengchao Shang, and Xiaoyu Chen. 2024. novel paradigm boosting translation capabilities of large language models. In NAACL-HLT (Findings), pages 639649. Association for Computational Linguistics. HuggingFaceFW. 2024. fineweb (revision af075be). Vivek Iyer, Bhavitvya Malik, Pavel Stepachev, Pinzhen Chen, Barry Haddow, and Alexandra Birch. 2024. Quality or quantity? on data scale and diversity in adapting large language models for low-resource translation. In WMT, pages 13931409. Association for Computational Linguistics. Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda B. Viégas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2017. Googles multilingual neural machine translation system: Enabling zero-shot translation. Trans. Assoc. Comput. Linguistics, 5:339351. Sneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier Garcia, Christopher A. Choquette-Choo, Katherine Lee, Derrick Xin, Aditya Kusupati, Romi Stella, Ankur Bapna, and Orhan Firat. 2023. Madlad-400: multilingual and document-level large audited dataset. Preprint, arXiv:2309.04662. Yinquan Lu, Wenhao Zhu, Lei Li, Yu Qiao, and Fei Yuan. 2024. LLaMAX: Scaling linguistic horizons of LLM by enhancing translation capabilities beyond 100 languages. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1074810772, Miami, Florida, USA. Association for Computational Linguistics. Yingfeng Luo, Tong Zheng, Yongyu Mu, Bei Li, Qinghong Zhang, Yongqi Gao, Ziqiang Xu, Peinan Feng, Xiaoqian Liu, Tong Xiao, and Jingbo Zhu. 2025. Beyond decoder-only: Large language models can be good encoders for machine translation. Preprint, arXiv:2503.06594. Thuat Nguyen, Chien Van Nguyen, Viet Dac Lai, Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, Ryan A. Rossi, and Thien Huu Nguyen. 2023. Culturax: cleaned, enormous, and multilingual dataset for large language models in 167 languages. Preprint, arXiv:2309.09400. Thuat Nguyen, Chien Van Nguyen, Viet Dac Lai, Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, Ryan A. Rossi, and Thien Huu Nguyen. 2024. CulturaX: cleaned, enormous, and multilingual dataset for large language models in 167 languages. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 4226 4237, Torino, Italia. ELRA and ICCL. Matt Post. 2018. call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186 191, Belgium, Brussels. Association for Computational Linguistics. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of Machine Learning Research, 21(140):167. Ricardo Rei, José G. C. de Souza, Duarte M. Alves, Chrysoula Zerva, Ana C. Farinha, Taisiya Glushkova, Alon Lavie, Luísa Coheur, and André F. T. Martins. 2022a. COMET-22: unbabel-ist 2022 submission for the metrics shared task. In WMT, pages 578585. Association for Computational Linguistics. Ricardo Rei, Marcos V. Treviso, Nuno Miguel Guerreiro, Chrysoula Zerva, Ana C. Farinha, Christine Maroti, José G. C. de Souza, Taisiya Glushkova, Duarte M. Alves, Luísa Coheur, Alon Lavie, and André F. T. Martins. 2022b. Cometkiwi: Ist-unbabel 2022 submission for the quality estimation shared task. In WMT, pages 634645. Association for Computational Linguistics. Aquia Richburg and Marine Carpuat. 2024. How multilingual are large language models fine-tuned for translation? CoRR, abs/2405.20512. Morgane Rivière, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, 10 Alexandre Ramé, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton Tsitsulin, and 80 others. 2024. Gemma 2: Improving open language models at practical size. CoRR, abs/2408.00118. Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob Steeves, Joel Hestness, and Nolan Dey. 2023. SlimPajama: 627B token cleaned and deduplicated version of RedPajama. https://www.cerebras.n et/blog/slimpajama-a-627b-token-cleaned-a nd-deduplicated-version-of-redpajama. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 59986008. Jiaan Wang, Fandong Meng, Yunlong Liang, and Jie Zhou. 2025a. DRT: deep reasoning translation via long chain-of-thought. In Findings of the Association for Computational Linguistics, ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pages 67706782. Association for Computational Linguistics. Longyue Wang, Zefeng Du, Wenxiang Jiao, Chenyang Lyu, Jianhui Pang, Leyang Cui, Kaiqiang Song, Derek F. Wong, Shuming Shi, and Zhaopeng Tu. 2024. Benchmarking and improving long-text translation with large language models. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 71757187. Association for Computational Linguistics. Yutong Wang, Jiali Zeng, Xuebo Liu, Derek F. Wong, Fandong Meng, Jie Zhou, and Min Zhang. 2025b. Delta: An online document-level translation agent based on multi-level memory. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2022. Finetuned language models are zero-shot learners. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lü, Rui Hu, Chenxia Li, Liu Yang, Xilin Luo, Xuejie Wu, Lunan Liu, Wenjun Cheng, Peng Cheng, Jianhao Zhang, Xiaoyu Zhang, and 11 others. 2023. Skywork: more open bilingual foundation model. Preprint, arXiv:2310.19341. Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla. 2024. paradigm shift in machine translation: Boosting translation performance of large language models. In ICLR. OpenReview.net. Haoran Xu, Kenton Murray, Philipp Koehn, Hieu Hoang, Akiko Eriguchi, and Huda Khayrallah. 2025. X-ALMA: plug & play modules and adaptive rejection for quality translation at scale. In ICLR. OpenReview.net. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, and 41 others. 2025. Qwen3 technical report. CoRR, abs/2505.09388. Wen Yang, Chong Li, Jiajun Zhang, and Chengqing Zong. 2023. Bigtrans: Augmenting large language models with multilingual translation capability over 100 languages. CoRR, abs/2305.18098. Jia Yu, Fei Yuan, Rui Min, Jing Yu, Pei Chu, Jiayang Li, Wei Li, Ruijie Zhang, Zhenxiang Li, Zhifei Ren, Dong Zheng, Wenjian Zhang, Yan Teng, Lingyu Meng, ZhenJiang Jin, Jiantao Qiu, ShaSha Wang, Zhongying Tu, Dahua Lin, and 4 others. 2025. Wanjuansilu: high-quality open-source webtext dataset for low-resource languages. Preprint, arXiv:2501.14506. Biao Zhang, Philip Williams, Ivan Titov, and Rico Sennrich. 2020. Improving massively multilingual neural machine translation and zero-shot translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1628 1639, Online. Association for Computational Linguistics. Yuze Zhao, Jintao Huang, Jinghan Hu, Xingjun Wang, Yunlin Mao, Daoze Zhang, Zeyinzi Jiang, Zhikai Wu, Baole Ai, Ang Wang, Wenmeng Zhou, and Yingda Chen. 2024. Swift:a scalable lightweight infrastructure for fine-tuning. Preprint, arXiv:2408.05517. Mao Zheng, Jiaqiang Wang, Haoran Xiong, Hongda Liu, Canrong He, Ziyue Jiang, and Dingkai Long. 2025a. Hunyuan-MT Technical Report. Preprint, arXiv:2509.05209. Tong Zheng, Yan Wen, Huiwen Bao, Junfeng Guo, and Heng Huang. 2025b. Asymmetric conflict and synergy in post-training for llm-based multilingual machine translation. CoRR, abs/2502.11223. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023. LIMA: less is more for alignment. In NeurIPS. Dawei Zhu, Pinzhen Chen, Miaoran Zhang, Barry Haddow, Xiaoyu Shen, and Dietrich Klakow. 2024a. Fine-tuning large language models to translate: Will touch of noisy data in misaligned languages suffice? In EMNLP, pages 388409. Association for Computational Linguistics. 11 Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, and Lei Li. 2024b. Multilingual machine translation with large language models: Empirical results and analysis. In Findings of the Association for Computational Linguistics: NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pages 27652781. Association for Computational Linguistics. Ahmet Üstün, Viraat Aryabumi, Zheng-Xin Yong, WeiYin Ko, Daniel Dsouza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh Fadaee, Julia Kreutzer, and Sara Hooker. 2024. Aya model: An instruction finetuned open-access multilingual language model. arXiv preprint arXiv:2402.07827."
        },
        {
            "title": "Models and Languages",
            "content": "This section provides more extensive analysis of the Directional Degeneration pitfall identified in the main paper (Section 4), exploring its generality across different models and its relationship with the scale of multilingualism. A.1 Generality Across Foundation Models To verify that the observed directional degeneration is not specific to our chosen model (Qwen34B-Base), we replicated the Strategic Downsampling experiment on three other foundation models: Qwen3-8B-Base, Llama3.1-8B, and Gemma2-9B. As shown in Figure 7, all three models exhibit the same pattern identified in our main analysis: the Zh directions experience significant degeneration under 100% usage, and are substantially restored by minuscule 1% or 5% sampling proportion. This consistent suggests that directional degeneration is general and systemic phenomenon when fine-tuning contemporary LLMs with symmetric multi-way data, rather than model-specific characteristic."
        },
        {
            "title": "En and X",
            "content": "A."
        },
        {
            "title": "Impact of Language Scale on\nDegeneration",
            "content": "One may wonder to what extent directional degeneration manifests as the number of languages increases. To examine how the degree of multilingualism affects the severity of this phenomenon, we conducted controlled experiment using the Qwen3-4B-Base model. To isolate the effect of scale from potential language-specific biases, we first randomized the order of the 60 languages and then formed cumulative subsets containing 10, 20, 30, 40, 50, and all 60 languages for SFT. As shown 12 in Figure 8, the degradation remains modest in the 10-language setup but becomes increasingly pronounced as the language scale expands. At 30 languages, the decline at 100% usage becomes substantial, and by 60 languages, it escalates into clear breakdown of translation quality. This trend supports our hypothesis that the Shallow Mapping Trap intensifies with higher degrees of multilingualism, as larger language scales amplify target-side repetition."
        },
        {
            "title": "Bilingual Corpus",
            "content": "X) and Chinese-centric (Zh Figures 10 and 11 present the COMETKiwi score (Rei et al., 2022b) distributions for the Englishcentric (En X) portions of our curated CPT bilingual corpus, respectively. primary observation is that the score distributions for low-resource languages are noticeably skewed to the left compared to the highresource counterparts. This skew is particularly pronounced for the Chinese-centric low-resource pairs (Figure 11), highlighting the challenge of sourcing non-English-centric data. We attribute this phenomenon to two factors. First, it likely reflects the real scarcity of clean, high-quality bilingual data for many low-resource languages, especially those paired with Chinese. Second, it may reveal bias in the quality estimation (QE) model itselfmodels like COMETKiwi may yield less reliable scores for underrepresented or non-English language pairs due to limited exposure during training. This could lead to systematically lower scores for some language pairs, irrespective of their true quality. Overall, these findings highlight the dual challenge of both data scarcity and model bias in multilingual MT research, and emphasize the need for more equitable QE models capable of handling diverse, non-English-centric scenarios."
        },
        {
            "title": "C CPT Gains Across Languages",
            "content": "This section details the language-specific impact of Continued Pre-training (CPT), extending the aggregated ablation results in Section 5.3. Figure 9 presents COMET score comparisons between Base+SFT+SD (without CPT) and Base+CPT+SFT+SD (with CPT) across four translation directions per language. Across nearly all languages and directions, incorporating CPT yields consistent performance imFigure 7: Generality analysis of Directional Degeneration across multiple foundation models. The x-axis represents the Strategic Downsampling proportion (p) for En/Zh directions. Figure 8: The impact of multilingual scale (number of languages) on the Directional Degeneration. The x-axis represents the Strategic Downsampling proportion (p) for En/Zh directions provements, underscoring its role as fundamental adaptation step. However, the magnitude of this gain is not uniform, and is more pronounced for mediumand low-resource languages. This trend aligns with the expectation that CPT is particularly beneficial for strengthening foundational linguistic knowledge where the base model lacks sufficient prior exposure. Additionally, for many highand medium-resource languages, the baseline model exhibits lower performance in the En/Zh directions than in En/Zh, consistent with prior observations (Arivazhagan et al., 2019) that generating into diverse target languages is inherently more challenging. CPT substantially improves performance in these directions, highlighting its effectiveness in strengthening multilingual generation capabilities. Figure 9: Performance improvements brought by Continued Pre-training (CPT). Languages are grouped by resource level: high , medium , and low. The orange portion of the bar shows the performance of the model without CPT (Base+SFT+SD), while the total height of the bar represents the performance after including the CPT stage (Base+CPT+SFT+SD). The blue portion visually represents the performance gain (COMET) contributed by CPT."
        },
        {
            "title": "Reference",
            "content": "Content 他补充道我们现在有4 个月大没有糖尿病的老鼠但它们曾经得过该病 \"We now have 4-month-old mice that are non-diabetic that used to be diabetic,\" he added. Prediction He added: \"We now have four-month-old mice that have never had diabetes but were given the disease.\" Table 3: Translation error cases: hallucinations and fabricated outputs Hyperparameter Learning Rate Adam β LR Scheduler Number of Epochs Global Batch Size Max Length Train Steps Warmup Ratio Weight Decay CPT Stage 2e-5 (0.9, 0.999) cosine 1 1536 2048 40,000 0.05 0.01 SFT Stage 2e-5 (0.9, 0.999) cosine 1 1024 1024 500 0.01 0.01 Table 4: Hyperparameter configuration during training."
        },
        {
            "title": "Languages",
            "content": "en zh ar, th, vi cs, fa, hi, de, fr, it, pt, es, el, uk, fi sv, no, da, ro, hu, sk, bg, nl, pl, tr"
        },
        {
            "title": "Data Source",
            "content": "SlimPajama-6B (Soboleva et al., 2023) Skywork (Wei et al., 2023) WanJuanSiLu (Yu et al., 2025) CulturaX(Nguyen et al., 2023) hr, sw ne MADLAD-400 (Kudugunta et al., 2023) CulturaX, MADLAD-400, MrBinit/Nepali bn, he, id, ms, az, kk, ps, ta, ur, uz, CulturaX, MADLAD-400, Opus-Corpora(Zhang et al., 2020), am, jv fineweb-2(HuggingFaceFW, 2024) km, lo, my, tl ja, ko, ru ug, bo, mn_cn yue CulturaX, MADLAD-400, fineweb-2, Opus-Corpora, C4 (Raffel et al., 2020), OSCAR WanJuanSiLu (Yu et al., 2025) In-house dataset AlienKevin/yue_and_zh_sentences si, te, mr, is, tg, ky, ka, hy HPLT (Burchell et al., 2025), OSCAR, Wikipedia Table 5: Monolingual data sources information for 60 languages. ahttps://huggingface.co/datasets/MrBinit/nepali_dataset_text_cleaned bhttps://oscar-project.github.io/documentation/versions/oscar-2301/ chttps://huggingface.co/datasets/AlienKevin/yue_and_zh_sentences"
        },
        {
            "title": "English\nArabic\nSpanish\nGerman\nFrench\nItalian\nJapanese\nDutch\nPolish\nPortuguese\nRussian\nTurkish\nChinese",
            "content": "ISO Code 13 High-resource Languages en ar es de fr it ja nl pl pt ru tr zh 18 Medium-resource Languages bg bn cs da el fa fi hi hu id ko no ro sk sv th uk vi 29 Low-resource Languages am az bo he hr hy is jv ka kk km ky lo"
        },
        {
            "title": "Resource",
            "content": "Indo-European Afro-Asiatic Indo-European Indo-European Indo-European Indo-European Japonic Indo-European Indo-European Indo-European Indo-European Turkic Sino-Tibetan Indo-European Indo-European Indo-European Indo-European Indo-European Indo-European Uralic Indo-European Uralic Austronesian Koreanic Indo-European Indo-European Indo-European Indo-European Tai-Kadai Indo-European Austroasiatic Afro-Asiatic Turkic Sino-Tibetan Afro-Asiatic Indo-European Indo-European Indo-European Austronesian Kartvelian Turkic Austroasiatic Turkic Tai-Kadai"
        },
        {
            "title": "Cyrillic\nBengali\nLatin\nLatin\nGreek\nArabic\nLatin\nDevanagari\nLatin\nLatin\nHangul\nLatin\nLatin\nLatin\nLatin\nThai\nCyrillic\nLatin",
            "content": "Geez Latin Tibetan Hebrew Latin Armenian Latin Latin Georgian Cyrillic Khmer Cyrillic Lao 16 ISO Code mn_cn mr ms my ne ps si sw ta te tg tl ug ur uz yue"
        },
        {
            "title": "Language\nChinese Mongolian Mongolian\nDevanagari\nMarathi\nLatin\nMalay\nMyanmar\nBurmese\nDevanagari\nNepali\nArabic\nPashto\nSinhala\nSinhala\nLatin\nSwahili\nTamil\nTamil\nTelugu\nTelugu\nCyrillic\nTajik\nLatin\nTagalog\nArabic\nUighur\nArabic\nUrdu\nLatin\nUzbek\nHan\nYue Chinese",
            "content": "Family Mongolic Indo-European Austronesian Sino-Tibetan Indo-European Indo-European Indo-European Atlantic-Congo Dravidian Dravidian Indo-European Austronesian Turkic Indo-European Turkic Sino-Tibetan"
        },
        {
            "title": "Resource\nLow\nLow\nLow\nLow\nLow\nLow\nLow\nLow\nLow\nLow\nLow\nLow\nLow\nLow\nLow\nLow",
            "content": "Table 6: Detailed information of 60 languages. The languages are grouped into categories based on their data ratios in the CulturaX (Nguyen et al., 2024): High Resource (>1%), Medium Resource (0.1%1%], and Low Resource (0.1%)."
        },
        {
            "title": "ISO Code",
            "content": "bg da fa no ro sk sv uk vi az hr is kk ky ps tg tl ur uz"
        },
        {
            "title": "Cyrillic\nLatin\nArabic\nLatin\nLatin\nLatin\nLatin\nCyrillic\nLatin\nLatin\nLatin\nLatin\nCyrillic\nCyrillic\nArabic\nCyrillic\nLatin\nArabic\nLatin",
            "content": "Indo-European Indo-European Indo-European Indo-European Indo-European Indo-European Indo-European Indo-European Austroasiatic Turkic Indo-European Indo-European Turkic Turkic Indo-European Indo-European Austronesian Indo-European Turkic"
        },
        {
            "title": "Russian\nGerman\nArabic\nGerman\nItalian\nCzech\nGerman\nRussian\nFrench\nTurkish\nPolish\nGerman\nRussian\nRussian\nArabic\nRussian\nSpanish\nPersian\nTurkish",
            "content": "ru de ar de it cs de ru fr tr pl de ru ru ar ru es fa tr Table 7: The set of languages that utilize the Parallel Multilingual Prompting (PMP) method and their corresponding auxiliary languages. The auxiliary language for each target was strategically selected from pool of 11 high-resource languages (excluding English and Chinese), guided by two primary principles: shared language family and, where possible, shared script. 17 # Langs (Hig./Med./Low) Model High Resource Medium Resource Low Resource En En Zh Zh En En Zh Zh En TowerInstruct-13B 82.27 80.84 LMT-60-4B 81.70 LMT-60-8B 83.33 82.90 83.46 81.78 81.58 82.38 81.06 83.37 83.92 84.28 84.58 85.64 81.29 82.70 84.28 82.14 83.90 84. 78.12 82.83 84.73 En Zh - - - - - - Zh - - - - - - Aya-expanse-8B LMT-60-4B LMT-60-8B GemmaX2-28-9B LMT-60-4B LMT-60-8B Hunyuan-MT-7B LMT-60-4B LMT-60-8B X-ALMA-13B LMT-60-4B LMT-60-8B Aya-101-13B LMT-60-4B LMT-60-8B 82.50 81.08 81.88 75.49 81.08 81.88 85.35 81.08 81.88 80.15 81.08 81. 75.14 81.08 81.88 83.03 82.45 82.90 76.42 82.45 82.90 83.32 82.45 82.90 79.31 82.45 82.90 77.68 82.45 82. 82.85 81.60 82.22 74.40 81.60 82.22 84.21 81.60 82.22 - - - 82.08 82.70 83.27 76.26 82.70 83. 85.30 82.70 83.27 - - - 77.85 81.60 82.22 75.00 82.70 83.27 83.38 81.32 82.17 74.92 80.82 81. 86.59 80.92 81.79 81.60 82.47 83.31 76.64 82.31 83.18 83.78 82.17 83.57 68.62 82.27 83.50 84.40 82.04 83. 78.87 82.75 83.86 78.69 82.57 83.78 83.38 81.33 82.29 75.34 80.56 81.55 84.55 80.71 81.79 - - - 82.10 81.27 83.19 75.97 81.41 83.39 85.11 81.19 83.21 - - - 77.94 81.85 82.84 75.06 81.53 83. 82.92 81.61 81.80 75.04 80.62 81.10 81.29 77.97 79.41 75.35 76.81 77.70 71.80 78.58 79.94 83.46 83.11 83. 58.88 82.33 83.49 83.58 80.30 82.98 75.98 81.59 83.35 77.87 80.31 82.73 81.59 80.13 81.34 73.21 77.94 78. 77.83 74.59 76.08 - - - 80.98 81.15 82.62 76.92 79.93 82.08 83.23 77.90 81.26 - - - 70.82 75.82 77.12 74.04 78.41 81.35 10 (9/1/0) 23 (13/9/1) 24 (13/8/3) 28 (13/9/6) 33 (13/16/4) 39 (13/18/8) Table 8: COMET-22 results on the WMT24++ benchmark (document-level). Only models without WMT24++ training are included."
        },
        {
            "title": "BLEU",
            "content": "0.6B 1.7B 4B 8B 0.6B 1.7B 4B 8B"
        },
        {
            "title": "13 High-resource Languages",
            "content": "82.97 84.77 83.58 85.70 86.17 90.09 84.75 84.48 88.03 86.49 86.72 87.92 85.82 86.24 87.04 87.68 88.13 91.32 87.20 88.08 89.53 88.80 89.01 88."
        },
        {
            "title": "18 Medium-resource Languages",
            "content": "enar enes ende enfr enit enja ennl enpl enpt enru entr enzh enbg enbn encs enda enel enfa enfi enhi enhu enid enko enno enro ensk ensv enth enuk envi enam enaz enbo enhe enhr enhy enis enjv enka enkk enkm enky enlo enmn_cn enmr enms enmy enne enps ensi ensw enta ente entg entl enug enur enuz enyue 87.25 87.06 88.01 88.66 88.87 91.69 88.43 89.72 90.02 89.80 90.18 89.49 91.28 87.12 91.29 91.47 89.63 88.04 92.42 81.22 89.95 92.09 89.77 90.45 91.08 91.09 91.29 88.96 90.39 89.86 87.58 88.18 92.22 88.49 91.16 90.68 86.60 87.84 87.96 91.02 83.56 89.26 85.70 96.39 75.91 90.11 87.26 84.35 78.65 90.24 85.97 89.65 86.56 77.54 85.81 88.36 83.51 90.91 88. 87.66 87.22 88.53 88.86 89.16 92.01 88.72 90.11 90.28 90.14 90.76 89.47 91.36 87.21 91.79 91.55 89.99 88.46 92.84 81.69 90.29 92.23 90.20 90.58 91.44 91.40 91.45 89.29 90.60 90.05 87.82 88.72 92.57 88.98 91.52 90.89 87.14 87.90 88.79 91.25 83.82 89.48 85.89 96.31 76.18 90.11 88.28 84.70 78.69 90.47 86.22 89.96 87.03 77.77 86.11 88.52 83.57 91.06 88.99 17.64 24.82 28.92 40.79 25.70 30.69 22.32 15.58 42.78 24.92 20.36 44.12 31.84 13.47 25.48 38.27 19.39 19.56 15.25 24.11 17.04 43.23 26.61 28.57 32.33 25.60 36.37 11.56 20.86 39.10 5.38 11.80 1.71 17.16 22.48 17.23 18.77 23.92 10.89 18.53 6.52 11.35 11.57 15.12 11.20 39.25 2.72 15.79 9.36 10.33 30.52 10.79 15.33 15.19 32.19 12.13 17.40 15.18 5. 21.73 27.22 35.09 46.15 28.78 34.79 25.49 19.45 46.76 29.30 23.63 47.16 36.54 14.98 28.77 42.44 23.29 22.82 19.23 28.47 21.66 46.59 29.62 31.34 36.33 29.84 40.77 12.92 25.50 41.66 8.49 13.14 3.33 22.95 25.81 20.62 22.70 26.94 12.82 21.48 7.49 13.09 13.17 19.94 14.22 41.44 3.23 18.50 11.26 13.66 34.90 13.42 18.10 18.98 35.39 15.01 19.70 18.04 8.30 25.83 29.49 38.47 48.87 31.03 37.40 26.86 21.60 49.34 32.33 26.98 49.26 39.53 17.65 30.87 46.01 25.90 26.05 23.96 31.88 24.10 48.54 30.72 32.72 39.96 32.43 44.20 14.56 30.08 43.69 12.03 14.53 3.67 27.31 30.60 23.44 25.48 29.86 15.41 24.07 7.86 15.18 14.62 24.63 16.45 43.06 4.33 20.79 12.48 17.29 38.10 15.19 20.85 21.63 38.08 17.84 23.27 21.85 8. 27.45 29.95 39.87 50.41 32.13 38.62 27.99 22.17 50.96 32.79 28.44 49.52 40.70 18.48 32.82 46.75 27.03 27.25 25.28 33.25 25.61 49.05 32.37 33.51 40.89 33.61 45.26 15.18 31.32 44.13 12.51 15.27 3.07 29.94 31.09 23.52 26.63 30.18 15.84 25.21 7.85 15.85 14.80 26.66 17.48 43.48 4.51 22.13 12.92 17.96 38.84 15.87 22.24 22.40 39.12 19.73 23.11 22.23 9.31 87.90 83.20 87.19 88.27 84.59 82.58 85.85 76.81 83.81 90.62 87.55 87.89 86.68 85.90 87.40 85.83 85.08 88.25 79.76 84.31 87.37 81.79 86.02 86.42 81.43 85.89 81.89 88.46 79.32 85.84 80.52 95.25 70.80 88.98 77.47 81.45 75.67 82.90 82.88 85.42 83.10 75.64 83.16 84.05 78.67 88.52 85.88 90.27 85.70 90.09 90.46 88.05 86.29 90.11 79.65 88.28 91.59 88.97 89.59 89.76 89.45 90.18 87.69 89.01 89. 84.63 86.66 90.98 86.40 89.55 89.23 85.26 87.10 85.78 90.21 82.44 87.89 83.30 96.07 74.18 89.84 82.95 83.11 77.51 87.40 85.07 88.19 85.03 76.71 85.07 86.71 81.66 90.01 87."
        },
        {
            "title": "29 Low-resource Languages",
            "content": "Table 9: COMET-22 and SacreBLEU scores of LMT on the FLORES-200 devtest set (En X)."
        },
        {
            "title": "BLEU",
            "content": "0.6B 1.7B 4B 8B 0.6B 1.7B 4B 8B"
        },
        {
            "title": "13 High-resource Languages",
            "content": "84.53 86.38 88.14 88.43 87.09 86.85 86.27 84.47 88.54 85.50 87.12 86.73 87.09 87.28 89.15 89.21 88.21 87.94 87.37 86.02 89.46 86.72 89.12 87."
        },
        {
            "title": "18 Medium-resource Languages",
            "content": "aren esen deen fren iten jaen nlen plen pten ruen tren zhen bgen bnen csen daen elen faen fien hien huen iden koen noen roen sken sven then uken vien amen azen boen heen hren hyen isen jven kaen kken kmen kyen loen mn_cnen mren msen myen neen psen sien swen taen teen tgen tlen ugen uren uzen yueen 87.88 87.67 89.54 89.55 88.47 88.44 87.82 86.59 89.81 87.18 89.83 87.83 88.32 88.86 88.78 90.31 88.07 88.35 90.17 89.76 88.64 89.87 88.75 89.37 89.50 88.57 90.28 88.93 87.71 88.16 86.32 87.26 71.85 88.58 88.32 88.69 86.79 86.11 87.20 88.22 87.38 86.33 87.78 78.93 88.61 89.61 86.10 90.74 85.19 88.53 86.13 87.19 88.69 78.87 88.12 87.16 87.74 88.21 87. 88.24 87.86 89.67 89.58 88.57 88.74 87.99 86.74 90.01 87.34 90.00 88.06 88.46 89.20 89.00 90.49 88.18 88.62 90.26 90.10 88.83 89.99 88.82 89.40 89.72 88.71 90.36 89.01 87.81 88.24 87.28 87.46 73.46 88.82 88.47 88.94 87.27 86.50 87.48 88.47 87.88 86.53 88.31 80.52 88.80 89.81 86.78 90.96 85.51 89.04 86.57 87.70 89.13 79.63 88.50 87.64 88.14 88.58 88.08 33.56 29.17 39.99 41.93 31.12 25.51 29.92 27.50 45.49 31.91 32.26 28.62 36.89 25.12 35.38 44.61 30.94 30.48 27.84 31.72 30.87 41.19 25.94 41.13 39.91 34.59 44.68 28.22 35.01 35.14 15.76 19.75 6.20 35.83 33.44 30.29 28.44 32.90 22.45 27.53 23.47 20.17 25.63 10.00 27.81 41.94 17.59 30.23 24.03 19.28 34.29 22.40 26.63 26.06 38.80 21.31 26.55 28.58 28. 39.41 31.53 43.97 44.89 34.98 28.71 32.54 30.76 48.69 35.88 37.46 31.85 40.89 31.79 39.65 48.13 35.92 35.53 33.22 38.97 35.53 45.10 30.51 44.04 44.09 39.26 47.94 33.16 39.22 37.78 26.09 24.30 9.83 41.76 38.13 34.75 34.14 39.00 26.55 33.13 29.89 23.86 33.65 28.38 33.00 45.18 23.94 37.53 29.64 26.38 40.53 29.12 35.78 32.08 44.51 26.32 32.75 33.97 32.44 42.74 33.45 45.65 46.71 36.56 30.45 34.41 32.69 50.74 38.13 40.32 32.93 42.79 35.49 41.97 49.77 38.10 38.58 36.61 41.84 37.45 47.70 32.70 46.20 45.12 41.63 50.07 35.24 41.92 39.48 32.75 27.03 15.80 45.37 39.79 39.57 37.34 42.86 30.55 35.96 34.15 26.32 38.19 42.65 37.62 48.28 26.89 41.02 33.08 34.46 44.50 32.96 39.06 35.30 48.78 28.70 36.33 36.71 33. 44.28 34.83 46.24 47.54 37.15 32.32 34.98 33.42 51.32 39.33 41.03 34.41 43.60 37.28 43.20 50.23 39.49 39.42 37.00 42.82 38.42 47.78 33.77 46.50 46.18 42.70 50.36 36.30 42.81 39.94 35.65 28.12 18.10 46.42 40.56 40.87 38.62 43.93 31.13 36.79 35.88 27.35 39.42 48.75 38.77 48.87 29.12 42.56 34.26 35.68 46.40 34.03 40.60 38.07 50.46 29.97 38.16 38.02 35.22 86.72 85.21 86.78 88.86 85.09 85.32 86.46 86.97 86.30 88.61 86.62 87.57 88.00 86.32 88.52 86.78 85.55 86.70 77.78 84.26 64.15 84.98 85.83 85.31 82.28 81.57 83.54 84.96 83.37 83.17 83.32 69.36 84.96 88.02 80.74 87.81 80.52 81.72 81.64 82.22 84.09 73.45 84.73 83.68 83.64 85.05 86.53 87.91 87.66 88.28 89.98 87.33 87.60 89.15 89.11 88.07 89.54 88.05 88.87 89.27 87.92 89.79 88.36 87.17 87. 83.26 86.19 68.66 87.47 87.81 87.53 85.55 84.60 86.20 87.31 86.05 85.48 86.17 75.56 87.45 89.03 84.26 89.75 83.54 85.02 84.56 85.74 87.42 77.39 87.13 86.54 86.56 87.34 87."
        },
        {
            "title": "29 Low-resource Languages",
            "content": "Table 10: COMET-22 and SacreBLEU scores of LMT on the FLORES-200 devtest set (X En)."
        },
        {
            "title": "BLEU",
            "content": "0.6B 1.7B 4B 8B 0.6B 1.7B 4B 8B"
        },
        {
            "title": "13 High-resource Languages",
            "content": "79.71 86.73 82.67 80.41 82.06 83.75 88.87 81.67 83.08 84.89 84.76 81.09 82.24 87.48 84.58 84.05 84.24 86.07 89.74 84.92 86.92 86.51 87.46 84."
        },
        {
            "title": "18 Medium-resource Languages",
            "content": "zhar zhen zhes zhde zhfr zhit zhja zhnl zhpl zhpt zhru zhtr zhbg zhbn zhcs zhda zhel zhfa zhfi zhhi zhhu zhid zhko zhno zhro zhsk zhsv zhth zhuk zhvi zham zhaz zhbo zhhe zhhr zhhy zhis zhjv zhka zhkk zhkm zhky zhlo zhmn_cn zhmr zhms zhmy zhne zhps zhsi zhsw zhta zhte zhtg zhtl zhug zhur zhuz zhyue 84.04 87.83 85.38 85.21 85.28 87.35 90.65 86.01 88.64 87.25 88.71 85.85 88.80 82.55 89.95 88.38 86.80 85.40 89.35 74.73 87.04 88.87 87.68 87.77 87.88 88.91 88.41 86.88 88.62 88.77 83.38 85.39 92.72 85.06 89.51 87.44 84.28 84.37 85.39 88.19 81.23 86.61 82.21 96.97 69.02 86.53 84.08 77.16 76.02 87.06 82.13 85.92 81.72 76.24 81.93 84.79 78.90 88.02 91. 84.48 88.06 85.69 85.65 85.49 87.46 90.86 86.36 89.04 87.39 88.90 86.32 89.26 82.92 90.15 88.74 87.41 85.75 89.79 75.09 87.26 89.06 88.06 88.20 88.26 89.57 88.75 87.21 89.08 89.00 83.95 85.88 92.68 85.51 89.97 87.98 84.55 84.87 86.03 88.41 81.70 87.01 83.01 96.96 69.62 86.83 84.41 77.75 76.26 87.33 82.37 86.14 82.07 76.36 82.41 85.13 79.62 88.16 91.53 8.71 28.62 16.32 14.97 21.16 15.70 24.08 13.11 9.86 21.17 14.21 10.47 17.14 6.72 13.30 18.89 10.69 11.50 8.83 12.42 9.78 23.24 18.88 13.90 17.83 13.10 17.00 8.20 10.96 27.63 2.61 7.95 1.61 7.97 12.13 9.16 10.50 11.06 6.61 10.15 5.01 7.62 7.26 25.34 6.15 20.51 1.60 7.17 5.10 5.71 13.87 5.95 7.20 8.47 16.05 9.20 9.16 8.46 5. 11.17 31.85 18.94 18.60 24.93 18.55 27.34 16.14 13.19 23.95 17.35 13.11 20.32 7.95 16.73 21.69 13.17 14.19 11.99 15.78 12.78 25.83 21.79 16.38 20.68 15.82 20.38 8.85 14.79 30.13 4.16 9.73 2.11 10.99 15.60 11.30 13.07 13.75 8.89 12.33 5.87 9.08 8.66 33.57 7.95 22.26 2.26 8.56 6.70 7.72 16.73 7.11 8.81 10.85 20.07 11.31 11.56 10.51 6.73 13.72 32.93 20.45 21.78 28.04 21.03 29.80 18.23 15.37 26.49 19.26 15.53 22.57 9.75 19.33 24.16 15.55 16.19 14.82 18.42 16.50 27.43 23.41 17.78 23.63 18.83 22.96 10.30 17.09 32.04 5.92 10.63 2.28 14.44 18.44 13.41 15.15 16.17 10.67 14.76 6.92 11.04 9.24 41.17 9.77 23.46 2.59 10.13 8.16 10.64 19.81 8.68 10.72 13.81 21.65 13.48 14.15 13.14 6. 14.58 34.41 20.95 22.56 29.03 22.02 30.90 19.06 15.49 27.40 20.09 16.31 23.77 10.39 20.07 25.16 16.68 17.05 15.04 19.36 16.67 28.77 24.32 18.62 24.43 19.63 24.22 10.80 18.30 33.11 6.65 11.23 2.10 15.69 19.62 13.84 16.23 17.01 11.47 15.46 6.98 11.13 9.82 42.45 9.70 24.37 2.83 10.94 8.25 11.01 20.39 8.94 11.31 14.07 22.83 14.50 14.82 13.37 6.34 84.63 78.06 84.72 84.95 81.00 79.75 81.87 69.07 79.62 87.07 85.14 84.44 83.24 83.23 84.32 83.82 82.86 86.97 74.21 80.11 88.72 78.05 83.28 82.24 78.16 81.64 77.88 84.72 75.69 82.14 76.58 96.15 63.03 85.16 71.97 73.18 70.92 78.49 78.23 80.41 75.90 73.88 78.69 80.12 73.01 84.90 90.19 87.43 80.86 88.06 87.15 85.05 83.34 86.67 72.53 84.49 88.33 86.94 86.80 86.45 87.22 86.94 85.60 86.96 88. 79.25 83.46 91.10 82.65 87.64 85.60 82.15 83.52 82.77 86.76 79.08 85.01 79.53 96.60 66.84 86.17 78.79 75.34 74.26 83.54 80.78 83.74 79.55 74.64 81.24 83.41 76.87 86.67 90."
        },
        {
            "title": "29 Low-resource Languages",
            "content": "Table 11: COMET-22 and SacreBLEU scores of LMT on the FLORES-200 devtest set (Zh X)."
        },
        {
            "title": "BLEU",
            "content": "0.6B 1.7B 4B 8B 0.6B 1.7B 4B 8B"
        },
        {
            "title": "13 High-resource Languages",
            "content": "82.33 87.92 86.04 85.80 86.55 86.18 86.80 84.78 83.95 86.46 84.87 83.99 84.95 88.83 87.31 87.43 87.44 87.48 88.17 86.34 85.88 87.93 86.26 85."
        },
        {
            "title": "18 Medium-resource Languages",
            "content": "arzh enzh eszh dezh frzh itzh jazh nlzh plzh ptzh ruzh trzh bgzh bnzh cszh dazh elzh fazh fizh hizh huzh idzh kozh nozh rozh skzh svzh thzh ukzh vizh amzh azzh bozh hezh hrzh hyzh iszh jvzh kazh kkzh kmzh kyzh lozh mn_cnzh mrzh mszh myzh nezh pszh sizh swzh tazh tezh tgzh tlzh ugzh urzh uzzh yuezh 86.04 89.49 87.69 88.01 87.85 87.84 88.48 86.93 86.49 88.30 86.80 86.94 86.91 86.37 87.42 88.32 86.16 86.70 87.65 86.62 86.84 87.58 87.67 87.69 87.45 87.21 88.32 88.10 86.71 87.96 83.21 85.80 72.44 86.61 87.28 86.45 85.35 83.85 86.03 86.54 85.78 85.27 85.99 87.36 85.51 87.18 84.18 87.22 83.54 85.98 84.15 84.49 85.21 78.84 85.65 86.16 85.46 86.40 91. 86.26 89.47 87.88 88.02 87.88 87.85 88.71 87.00 86.61 88.38 86.84 87.14 87.11 86.90 87.25 88.36 86.35 86.91 87.78 87.15 87.03 87.71 87.87 87.76 87.43 87.13 88.40 88.22 86.83 87.95 84.08 85.90 74.47 86.71 87.28 86.84 85.81 84.29 86.36 86.75 85.94 85.51 86.46 88.33 86.11 87.40 84.68 87.67 84.01 86.82 84.31 85.03 85.73 79.27 85.66 86.33 86.14 86.67 91.07 31.79 44.12 34.09 36.81 37.54 34.67 31.92 32.49 31.89 37.45 35.11 32.56 35.67 27.02 34.77 37.27 30.55 30.68 30.90 30.19 32.40 36.96 32.16 34.91 36.41 34.06 36.53 32.22 34.54 35.73 16.53 27.51 7.84 32.04 33.55 30.52 28.44 29.27 25.37 30.63 26.41 25.83 25.25 24.01 27.00 36.14 14.78 29.26 23.07 19.46 27.18 22.72 24.11 27.95 32.58 28.28 25.39 30.56 43. 36.86 47.16 37.19 40.31 40.66 38.30 35.59 35.67 35.63 40.65 38.43 37.25 39.01 32.68 38.70 40.90 35.50 35.93 36.00 35.36 36.83 40.49 35.77 38.94 39.94 38.07 40.16 36.11 38.81 38.24 24.61 31.25 13.73 37.37 38.05 35.03 33.66 34.26 32.70 34.70 30.93 30.35 31.53 50.27 33.42 39.72 24.02 34.29 29.32 26.80 33.95 29.91 32.50 33.30 38.30 33.78 32.34 35.88 45.10 39.59 49.26 38.67 41.40 41.94 39.37 37.27 37.62 37.21 42.05 39.82 39.91 40.88 35.63 40.94 42.88 38.00 37.67 38.72 37.94 38.72 41.50 38.02 40.76 41.95 39.90 42.61 38.47 40.58 39.73 31.23 33.64 19.40 40.38 40.18 38.58 36.75 36.92 35.97 37.79 34.43 32.32 35.86 67.48 36.18 41.15 29.95 37.53 32.98 34.21 36.99 33.18 35.67 37.04 40.66 35.70 35.91 37.88 44. 40.39 49.52 39.14 41.94 41.80 39.87 38.22 37.73 37.55 42.44 40.49 40.19 41.34 37.29 40.50 42.10 38.69 38.84 38.56 39.18 39.36 42.22 38.05 40.69 41.88 40.12 42.38 38.98 41.03 40.17 33.02 34.15 22.06 40.85 39.81 39.37 37.73 37.69 35.97 38.23 35.58 32.99 36.84 72.57 36.93 41.77 30.89 38.17 33.70 35.81 37.77 34.49 36.76 37.74 40.76 36.73 36.89 38.50 44.62 84.40 81.62 85.06 85.99 82.17 83.02 83.35 82.65 83.67 85.98 85.44 85.26 84.94 84.61 85.93 85.72 83.86 86.53 73.26 82.36 64.72 82.45 84.01 81.99 80.51 79.12 80.88 82.87 81.26 81.23 80.42 77.98 80.23 84.87 76.46 83.16 77.75 78.07 78.09 78.56 78.81 73.04 81.68 81.64 80.30 82.42 90.67 86.39 84.87 86.65 87.68 85.21 86.00 86.51 85.64 86.05 87.13 87.14 87.13 86.78 86.28 87.68 87.44 86.03 87. 79.39 84.59 69.65 85.30 86.24 84.64 84.08 82.67 84.65 85.19 84.08 84.41 83.98 84.13 84.43 86.54 81.64 85.95 81.29 82.15 82.24 82.95 83.20 76.91 84.41 84.92 83.74 85.29 91."
        },
        {
            "title": "29 Low-resource Languages",
            "content": "Table 12: COMET-22 and SacreBLEU scores of LMT on the FLORES-200 devtest set (X Zh). 22 Figure 10: COMETKiwi score distributions for bilingual sentence pairs (En-X) are shown as histograms. Vertical lines indicate quality thresholds at 0.6 (red), 0.7 (green), and 0.8 (magenta), with the legend specifying the number and proportion of sentence pairs exceeding each threshold. Some language pairs are excluded due to COMETKiwis limited language support. Figure 11: COMETKiwi score distributions for bilingual sentence pairs (Zh-X) are shown as histograms. Vertical lines indicate quality thresholds at 0.6 (red), 0.7 (green), and 0.8 (magenta), with the legend specifying the number and proportion of sentence pairs exceeding each threshold. Some language pairs are excluded due to COMETKiwis limited language support."
        }
    ],
    "affiliations": [
        "NiuTrans Research, Shenyang, China",
        "School of Computer Science and Engineering, Northeastern University, Shenyang, China"
    ]
}