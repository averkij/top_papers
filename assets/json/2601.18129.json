{
    "paper_title": "Typhoon-S: Minimal Open Post-Training for Sovereign Large Language Models",
    "authors": [
        "Kunat Pipatanakul",
        "Pittawat Taveekitworachai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have progressed rapidly; however, most state-of-the-art models are trained and evaluated primarily in high-resource languages such as English and Chinese, and are often developed by a small number of organizations with access to large-scale compute and data. This gatekeeping creates a practical barrier for sovereign settings in which a regional- or national-scale institution or domain owner must retain control and understanding of model weights, training data, and deployment while operating under limited resources and strict transparency constraints. To this end, we identify two core requirements: (1) adoptability, the ability to transform a base model into a general-purpose assistant, and (2) sovereign capability, the ability to perform high-stakes, region-specific tasks (e.g., legal reasoning in local languages and cultural knowledge). We investigate whether these requirements can be achieved without scaling massive instruction corpora or relying on complex preference tuning pipelines and large-scale reinforcement fine-tuning (RFT). We present Typhoon S, a minimal and open post-training recipe that combines supervised fine-tuning, on-policy distillation, and small-scale RFT. Using Thai as a representative case study, we demonstrate that our approach transforms both sovereign-adapted and general-purpose base models into instruction-tuned models with strong general performance. We further show that small-scale RFT with InK-GRPO -- an extension of GRPO that augments the GRPO loss with a next-word prediction loss -- improves Thai legal reasoning and Thai-specific knowledge while preserving general capabilities. Our results suggest that a carefully designed post-training strategy can reduce the required scale of instruction data and computation, providing a practical path toward high-quality sovereign LLMs under academic-scale resources."
        },
        {
            "title": "Start",
            "content": "Typhoon-S: Minimal Open Post-Training for Sovereign Large Language Models Kunat Pipatanakul, Pittawat Taveekitworachai Typhoon, SCB 10X 6 2 0 2 6 ] . [ 1 9 2 1 8 1 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have progressed rapidly; however, most state-of-the-art models are trained and evaluated primarily in high-resource languages such as English and Chinese. In addition, they are often developed by small number of organizations with access to large-scale compute and data. This gatekeeping creates practical barrier for sovereign settings in which regionalor national-scale institution or domain owner must retain control and understanding of model weights, training data, and deployment while operating under limited resources and strict transparency constraints. To this end, we identify two core requirements: (1) adoptability, the ability to transform base model into general-purpose assistant, and (2) sovereign capability, the ability to perform high-stakes, region-specific tasks (e.g., legal reasoning in local languages and cultural knowledge). We investigate whether these requirements can be achieved without scaling massive general-purpose instruction corpora or relying on complex preference tuning pipelines and large-scale reinforcement fine-tuning (RFT). We present Typhoon S, minimal and open post-training recipe that combines supervised fine-tuning, on-policy distillation, and small-scale RFT stages. Using Thai as representative case study, we demonstrate that our approach successfully addresses adoptability by transforming both sovereign-adapted and general-purpose base models into instruction-tuned models with strong general performance. We further show that small-scale RFT with InK-GRPOan extension of GRPO that augments the GRPO loss with next-word prediction lossenables sovereign capability by improving Thai legal reasoning and Thai-specific knowledge while preserving general capabilities. Our results suggest that carefully designed post-training strategy can reduce the required scale of instruction data and computation, providing practical path toward high-quality sovereign LLMs under academic-scale resources (approximately two days of 8-GPU training for an 8B model for adoptability, and one day of 4-GPU training for sovereign capability). Project Resources: Collections: Typhoon-S-Collections Models: Typhoon-S-8B-Instruct, Typhoon-S-4B-Legal-Agent Datasets: Typhoon-S-Instruct-Dataset, Typhoon-S-Sovereign-Capability-Dataset Github: https://github.com/scb-10x/typhoon-s"
        },
        {
            "title": "Introduction",
            "content": "Recent frontier large language models (LLMs) can be broadly categorized into three types: (1) proprietary systems, (2) open-weight models, and (3) fully open initiatives. While proprietary models have historically defined the state-of-the-art, recent open-weight models such as Qwen (Yang et al., 2025), Gemma (Team et al., 2025), and DeepSeek (DeepSeek-AI et al., 2025) have made rapid progress, delivering performance competitive with top-tier closed systems. Simultaneously, fully open efforts like OLMo (Olmo et al., 2025), 1 Apertus (Hernández-Cano et al., 2025), and Nemotron (Bercovich et al., 2025) aim to democratize scientific understanding by releasing not just weights, but also training pipelines and datasets. However, the development of state-of-the-art LLMs remains concentrated in small number of organizations, with most models trained primarily on Englishand Chinese-centric data or supported by large-scale compute resources and complex post-training pipelines. Even fully open models typically rely on massivescale compute (e.g., OLMo 3 used cluster of 1,024 H100 GPUs for several months, totaling $2.75M (Olmo et al., 2025)) and complex post-training pipelines that are inaccessible to smaller research groups or national initiatives. This creates form of resource gatekeeping, where the standard recipe for achieving high performance requires scale that exceeds the capacity of most academic or public sector entities. This concentration presents significant challenges for sovereign setting. We define sovereign setting as scenario in which developers must retain control over and understanding of model weights, data, and training methodologies, while ensuring strong alignment with specific regional, cultural, and in-domain requirements. Unlike general commercial developments one-size-fits-all models, sovereign adaptation often operates under limited resources, constrained by both compute budgets and human expertise. As result, smaller research groups and national-level initiativesespecially in resource-constrained countries face diﬀiculties in adapting these models to regionalor domain-specific needs. In practice, sovereign-adapted base models often demonstrate strong regional knowledge (e.g., strong Thai performance on MMLU-style benchmarks such as ThaiExam (Pipatanakul et al., 2023) or the M3/M6 exams (Yuenyong et al., 2025a)), but they lag behind leading proprietary and open-weight models in general instruction following, tool use, and agentic behaviors, limiting their practical adoption. common approach has been to scale general-purpose instruction data and post-training pipelines. Contemporary post-training frameworks typically combine supervised fine-tuning (SFT), preference optimization (e.g., DPO), and reinforcement fine-tuning (RFT), supported by large, curated instruction datasets. While effective, this strategy poses two major challenges in sovereign settings. First, scaling general-purpose instruction data tends to prioritize high-resource languages and generic tasks, increasing dependence on external data quality and availability. Second, the engineering complexity and data requirements of these pipelines can exceed the capacity of resource-constrained teams. As result, there remains no clear, validated post-training recipe tailored to sovereign deployment under limited-resource conditions. To this end, we propose Typhoon S, minimal and open post-training recipe addressing the aforementioned challenges. Specifically, Typhoon is designed to address this central research question: Which post-training strategy can enable competitive performance with frontier models under academic-level resource constraints? We breakdown this research question into two aspects: 1) Adoptability, defined as the ability to transform base model into general-purpose assistant capable of instruction following, mathematical, code generation, and tool use with competitive performance; and 2) Sovereign Capability, defined as an ability to perform high-stakes, region-specific tasks (e.g., local legal reasoning, cultural knowledge, and language-specific logic) that are often under-represented in general pretraining data. We use Thai as representative sovereign setting. In this technical report, we study the two aforementioned core capabilities into separate experimental setups to reduce cross-contamination effects. For adoptability, we demonstrate that multi-stage post-training frameworkconsisting of lightweight SFT on general instructions and on-policy distillation (OPD) from teacher modelscan effectively transform sovereignadapted base models into competitive instruction-tuned assistants. For sovereign capability, we introduce Injected Knowledge GRPO (InK-GRPO), an extension of GRPO that augments the GRPO loss with cross-entropy (next-token prediction) loss. We show that InK-GRPO effectively improves target performance while teaching new knowledge in parallel. We apply this technique in both standard RFT settings and agentic RFT setups; in the latter, the model has access to tools for retrieving external knowledge during both training and inference. We observe particularly strong improvements in Thai legal reasoning scenarios. We summarize the contributions of our paper as follows: 2 1. We identify two complementary requirements for sovereign post-training: adoptability as generalpurpose assistant and sovereign capability on regionalor cultural-specific tasks. 2. To address adoptability, we present minimal base-to-instruct recipe that combines lightweight SFT with OPD, using open-source instruction data together with target-language data. 3. To enhance sovereign capability, we introduce InK-GRPO, which optimizes domain-specific performance while teaching new knowledge in parallel within an agentic RFT setup on reasoning and knowledge benchmarks."
        },
        {
            "title": "2 Adoptability",
            "content": "In this section, we assume scenario in which national-level initiative needs to develop Thai instructiontuned model from base model to enable instruction-following capabilities under limited budgetfor example, using less than one 8 H100 for under week wall-clock time. Our approach is two-stage post-training pipeline consisting of (1) SFT followed by (2) OPD. For training datasets, we uses standard monolingual English dataset as the foundation, augmented with small amount of target-language data to enable instruction-following ability in the target language (Pipatanakul et al., 2024; Taveekitworachai et al., 2025b; Pipatanakul et al., 2025; Ong and Limkonchotiwat, 2023). Given the assumed scenario, key data constraint is the exclusive use of open-source English datasets, reflecting practical assumption for sovereign developers: to leverage existing resources rather than reinventing the wheel. Specifically, we use the Tulu 3 SFT dataset (Lambert et al., 2025) as the primary instruction corpus and Toucan (Xu et al., 2025) as the tool-use dataset due to it openness, diversity and credibility. For augmented small target-language datasets used to improve language-specific benchmarks, we follow well-established approach (Pipatanakul et al., 2024). The remainder of this section details target-language data construction, SFT, OPD, training and evaluation procedures, as well as results and discussion. 2.1 Target Language Dataset To support Thai instruction following, we construct dedicated target-language dataset following our previous work (Pipatanakul et al., 2024), as shown in Figure 1, by performing the following steps: Figure 1 Overview of the target-language dataset construction pipeline for Thai. Prompt sourcing We draw prompts from two sources: (1) We use Thai translated real-user English prompts from WildChat (Zhao et al., 2024) due to the lack of publicly available Thai real-user prompt data. (2) We aggregate prompts from available Thai instruction datasets, including WangchanThaiInstruct (Limkonchotiwat et al., 2025), Han (Phatthiyaphaibun, 2024), and Typhoon Instruct (Pipatanakul et al., 2023). Response generation and filtering Once we have Thai prompts, we generate responses from those prompts using the AutoIF framework (Dong et al., 2024). We define code-verifiable constraints in both English and Thai using small set of manually curated few-shot examples in Thai and English; these constraints are combined with the sourced prompts to guide response generation. Candidate responses are produced using Qwen3 235B A22B Instruct (2507). Quality control is enforced through rejection sampling based on AutoIF test cases and model self-evaluation. Responses with scores below 7 are discarded. Lightweight augmentation Prompts in the AutoIF format consist of two components: (1) the main instruction (the user query) and (2) constraints, which specify rules and requirements for the response. Constraints can be placed either in the system prompt or included in the user prompt together with the main instruction. In monolingual settings, all prompt components typically use the same language. In multilingual settings, however, prompts often involve code mixing. For example, user may provide the main instruction in Thai while specifying constraints in English. Moreover, multilingual LLMs tend to perform internal reasoning in English regardless of the prompt language (Schut et al., 2025). To mitigate these issues and encourage cross-lingual alignment, as well as robustness to constraint placement (system vs. user prompts), we use two simple augmentation approaches: 1. Randomly translating constraints between English and Thai. 2. Randomly assigning constraints either to the system message or concatenating them with the user instruction. From our pilot experiments, we find that mixing English constraints with Thai user instructions yields performance improvements across tasksincluding mathematics, code generation, and agentic behaviorswhile preserving data eﬀiciency. Therefore, we construct the dataset using this approach. 2.2 SFT SFT is selected as the first stage in the training pipeline to equip the base model with basic instructionfollowing and tool-use abilities. We follow standard SFT setup, optimizing cross-entropy objective over instructionresponse pairs. SFT Dataset We construct unified SFT corpus by combining datasets corresponding to three complementary objectives: English-centric general instruction following: We sample 200k instances from the Tulu 3 SFT dataset (Lambert et al., 2025), which contains over one million examples spanning diverse task domains. English-centric tool use: We incorporate the Toucan Tool dataset (Xu et al., 2025), which provides supervision for structured tool usage. Thai language instruction: We include Thai instruction dataset (Section 2.1) to improve Thai instruction-following quality beyond English-only supervision. The final data mixture is summarized in Table 1. This design targets balanced skill profilegeneral instruction following, tool use, and Thai-language capabilitywhile keeping the pipeline simple and minimizing computational requirements. 2.3 On-Policy Distillation Following SFT, we apply OPD using the Generalized Knowledge Distillation (GKD) (Agarwal et al., 2024; Yang et al., 2025; Lu and Lab, 2025). The motivation is to reduce the train-inference distribution mismatch of standard (offline) distillation: if the student is only trained on fixed teacher outputs, it may not learn how 4 Source Dataset Description #Samples Tulu 3 SFT Toucan Tool Typhoon AutoIF General instruction tuning across diverse tasks Tool-use and agentic interaction examples Thai language alignment (AutoIF-style) Total 200k 100k 40k 340k Table 1 Summary of data sources and statistics for the SFT dataset. to recover from its own mistakes at inference time. In GKD, the teacher instead provides token-level feedback on student-generated trajectories, so the corrective signal better matches the students current behavior. Concretely, GKD alternates between two data sources using student-data fraction λ [0, 1]. At each training step, with probability λ we (i) sample an input and generate an output pS( x) from the student (on-policy data); otherwise, we (ii) sample (x, y) from reference dataset (e.g., SFT data). We then query the teacher for token probabilities along the chosen sequence and update the student to minimize divergence between teacher and student token-level distributions, averaged over time steps. OPD can be implemented in two variants: full-distribution (full-logits) distillation, which uses the teachers complete next-token distribution, and top-K distillation, computationally eﬀicient approximation that retains only the teachers top-K token probabilities. In our experiments, we use full-logits distillation, unless state otherwise. We additionally compare both variants to study the trade-offs between performance and eﬀiciency, with detailed results presented in Section 2.5.2. 2.3.1 Distillation Pipeline In common RL training frameworks for LLMs (e.g., veRL (Sheng et al., 2024) and OpenRLHF (Hu et al., 2025), the teacher model is often treated as remote worker, which introduces substantial overhead due to the serialization and transfer of logits across Ray processes. This communication cost can dominate training latency, in some cases exceeding the time required for forward pass and response generation during on-policy rollouts. In our single-node setup, these overheads provide limited benefit and instead become bottleneck. To address this issue and enable eﬀicient full-logits OPD, we implement lightweight training framework based on the HuggingFace Transformers and TRL (von Werra et al., 2020) stack. Drawing inspiration from the hybrid flow model proposed in veRL (Sheng et al., 2024), we integrate teacher forward passes directly into the training loop while preserving memory eﬀiciency. Specifically, our training framework implements dynamic model swapping, in which inactive models are offloaded to RAM when not in use, reducing GPU VRAM pressure without sacrificing model availability. This design enables the orchestration of large teacher models under limited hardware budgets. In addition, our pipeline incorporates: FSDP with CPU offloading to enable full fine-tuning of an 8B-parameter student model under limited GPU memory. vLLM as the inference backend for high-throughput, low-latency student inference within the training loop. Hybrid scheduling to coordinate model swapping scheduling, only working model is loaded into GPU VRAM, to optimize the throughput and offload to RAM after used. Using this pipeline, we train the 8B model on 4 H100 GPUs, demonstrating that teacher-student full-logits OPD can be executed eﬀiciently without large-scale distributed computing resources. 2.3.2 OPD Dataset For the OPD dataset, we subsample from the SFT mixture to construct smaller corpus. We downsample the English subset while retaining the full Thai dataset. The final OPD dataset contains 160k examples with the mixture shown in Table 2. 5 Source Dataset Task Type #Samples Tulu 3 (subset) Typhoon AutoIF Toucan Tool (subset) Agentic & tool-use tasks General instruction-following Thai language alignment (AutoIF) Total 100k 40k 20k 160k Table 2 Summary of data sources and statistics for the OPD dataset. 2.4 Experimental Setup 2.4.1 Training SFT is conducted using AdamW optimizer with learning rate of 2 105, batch size of 32 and sequence packing up to total length of 16,384 tokens, and it is run for two epochs. OPD uses AdamW optimizer with learning rate of 1 106. We set λ = 0.25, meaning that 25% of training steps use student-generated (on-policy) sequences for distillation, while the remaining 75% use sequences sampled from the reference dataset. OPD is performed for single epoch. The OPD training is driven solely by the distillation loss with the forward KL divergence (teacher-to-student) as the distillation objective. We use Qwen/Qwen3-4B-Base as the primary base model for our experiments. For RQ4, we use ThaiLLM/ ThaiLLM-8B as the base model. All experiments are conducted on H100 GPUs. The 4B-scale experiments run on 4H100 GPUs and complete in under two days, while the 8B-scale experiments run on 8H100 GPUs and take approximately two days. Full hyperparameters are provided in Table 12. 2.4.2 Evaluation Benchmarks We evaluate all models on diverse suite of benchmarks covering chat, instruction following, knowledge, mathematical reasoning, code reasoning, tool use, and agentic retrieval. We report results separately for English (EN), Thai (TH), and Thai code-switching (CS) when applicable. Chat We use MT-Bench as an LLM-as-a-judge evaluation of multi-turn conversational quality, focusing on helpfulness, correctness, and instruction adherence. For English, we follow the standard LMSYS MT-Bench setup (Zheng et al., 2023). For Thai, we use Thai MT-Bench released by ThaiLLMLeaderboard (10X et al., 2024; Payoungkhamdee et al., 2024), which is newly created in Thai and does not rely on translation; instead, the questions are written natively in Thai linguistic and cultural context. Scores are reported separately for English (MTB EN) and Thai (MTB TH). Instruction following We evaluate verifiable instruction adherence using IFEval (Zhou et al., 2023), reporting accuracy over predefined constraints. We use both the original English IFEval and Thai-translated version, scb10x/ifeval-th1. For the Thai setting, translations are manually verified to preserve instruction semantics, and the constraints are adapted natively to Thai. Instruction-following results are reported separately for English (IFE EN) and Thai (IFE TH). Code-switching sampling robustness To measure robustness under language mixing in generation settings, we include dedicated Thai code-switching (CS) evaluation. We follow the evaluation protocol of Pipatanakul et al. (2025), which probes models ability to correctly follow instructions and generate coherent outputs using the characters commonly used by Thai speakersnamely Thai script with subtle English 1https://huggingface.co/datasets/scb10x/ifeval-th 6 mixing. The evaluation focuses on realistic ThaiEnglish code-switching patterns that are prevalent in everyday communication, rather than artificial or rarely occurring language mixtures. This metric is particularly sensitive to distributional mismatches in sampling and is therefore useful for diagnosing model brittleness. (1) Knowledge benchmarks We evaluate factual and scientific knowledge using three benchmarks: GPQA Diamond (Rein et al., 2023), challenging multiple-choice benchmark in biology, physics, and chemistry, used in English only. (2) MMLU Pro (Thai) (Xuan et al., 2025), Thai-translated version of MMLU Pro (Wang et al., 2024), evaluating broad academic knowledge. (3) OpenThaiEval (OTE) (Yuenyong et al., 2025b), native Thai benchmark covering Thai exam-style questions, natural language inference, and regional knowledge. These benchmarks are used to assess whether alignment methods preserve or degrade knowledge inherited from the base model. Mathematical reasoning We evaluate mathematical reasoning using MATH500 (Lightman et al., 2023). The benchmark consists of 500 English problems; we additionally include Thai-translated version, resulting in 1,000 total problems. Scores are reported separately for English (M500 EN) and Thai (M500 TH). Code reasoning We assess code generation and reasoning ability using LiveCodeBench (Jain et al., 2024), contamination-resistant benchmark drawn from LeetCode, AtCoder, and Codeforces. We use the release_v3 split containing 612 problems and report pass@1 accuracy (LCB EN). Tool use and agentic reasoning We evaluate tool use and multi-step agent with two benchmarks: (1) BFCL v4 (Patil et al., 2025), function-calling benchmark. We evaluate only the single-tool and multi-tool scenarios, comprising 4,441 problems in total. Results are reported as BFCL EN. (2) HotpotQA (HPQA) (Yang et al., 2018), used as an end-to-end agentic retrieval benchmark. We evaluate on the medium subset, subsampled to 100 questions, using simple ReAct-style agent connected to the Wikipedia API, following Liu et al. (2024). We report results separately for English (HPQA EN) and Thai (HPQA TH). Metrics We use accuracy as our primary metric, where correctness is defined according to each dataset and evaluated using greedy decoding. The average score is computed as the unweighted mean across all reported benchmarks, with MT-Bench multiplied by 10 to ensure equal weighting. Unless otherwise stated, higher scores indicate better performance. 2.5 Results & Discussion To evaluate the effectiveness of our proposed recipe, we investigate whether post-trained models can achieve competitive performance with globally developed open-source models while remaining strong on local-language tasks under academic-scale resource constraints. To this end, we assess the effectiveness of our recipe by ablating our design choices in RQ1RQ3 and evaluate the generalization of the proposed recipe in RQ4. RQ1 Does SFT alone, without OPD, suﬀice to achieve strong and robust performance? RQ2 Is full-logits distillation truly necessary? Is it more effective than top-K distillation across scenarios? RQ3 Is target-language dataset necessary at all post-training stages of our recipe to achieve strong performance on downstream tasks? RQ4 Is our proposed recipe effective with sovereignty-adapted base model? 2.5.1 A1: SFT Alone Is Insufficient to Achieve Strong and Robust Performance To answer the question of whether SFT alone is suﬀicient to achieve strong and robust performance without OPD, we use Qwen3 8B Instruct as our baseline model and compare model fine-tuned using SFT alone with model fine-tuned using our full recipe (SFT + OPD) across benchmarks. Table 3 suggests that SFT alone is not suﬀicient to achieve strong overall performance across our evaluation suite, as it yields 7 lower average performance compared to the SFT+OPD model. While SFT produces functional instructionfollowing model, it substantially underperforms on several capability axes and exhibits clear brittleness under distribution shift. The degradation is most visible in code-switching and tool use: the SFT model drops sharply on Thai code-switching (CS: 65.4 vs. 93.4 from the SFT+OPD model) and achieves zero scores on HPQA in both languages. This behavior is consistent with the fact that standard standard SFT maximizes the likelihood of the observed target token but does not explicitly encourage probability mass over other plausible or semantically equivalent tokens, resulting in sharply peaked and brittle token distribution (Kim and Rush, 2016). In contrast, our recipe (SFT+OPD) improves performance over SFT across most benchmarks, raising the average score by +6.49 points (37.45 43.94). We hypothesize that these gains come from two factors. First, distillation provides richer training signal than cross-entropy against single ground-truth sequence by leveraging the full distribution of the teacher (Hinton et al., 2015). Second, because distillation is performed on-policy, the training targets are computed on student-generated trajectories, reducing mismatch between training contexts and the models current behavior (Agarwal et al., 2024). Together, these factors are potentially improve generalization on open-ended generation tasks, consistent with gains on MT-Bench, MATH500, HotpotQA, and code-switching. In contrast, performance on knowledge-focused benchmarks (e.g., MMLU, OpenThaiEval, and GPQA) remains similar to the SFT-only model, suggesting these capabilities are largely inherited from the base model and that OPD is less effective at improving them. Model CS Chat TH EN TH MTB MTB CS IF EN IFE Knowledge TH Math Code EN Tool / Agent TH EN TH IFE GPQA MMLU OTE M500 M500 LCB HPQA HPQA BFCL TH EN EN EN Average Qwen3 Instruct 8.65 7.20 96.20 87.42 79.68 39. 34.18 67.23 87.20 78.76 33.50 14.00 8.00 31.53 48. SFT SFT+OPD 5.67 7.20 29.29 8.24 6.44 93.40 83.18 75.98 33.84 65.40 70.94 73.35 31.97 61.28 50.40 70.94 33.50 29.76 0.00 61.19 74.00 66.93 31.21 15. 0.00 24.36 9.00 26.95 37.45 43.94 Table 3 Performance comparison of the instruction-tuned baseline model, SFT, and SFT with on-policy distillation (SFT+OPD) across chat, instruction following, knowledge, math reasoning, code, and tool-use benchmarks in English and Thai. Results show that SFT alone underperforms the base model and suffers from poor generalization, while on-policy distillation consistently improves robustness and overall performance. 2.5.2 A2: Full-Logits Distillation Is Not Always Necessary, but It Improves Sampling Robustness We compare the SFT+OPD (OPD Full) model from the previous experiment, which uses full-logits distillation, with model trained using top-K OPD (OPD Top-K) initialized from the SFT-only model in the previous section, in order to examine the differences between the two OPD paradigms. As shown in Table 4, full-logits distillation achieves higher average score (43.94 vs. 42.81), although the differences are not consistent across tasks. The most pronounced gap appears in Thai code-switching, where OPD with full logits substantially outperforms OPD Top-K (CS: 93.4 vs. 69.8). In contrast, several benchmarks show comparable performance or even slight improvements under Top-K distillation, particularly on tasks with single correct answer, MATH500 and HotpotQA. These results suggest that full-logits distillation provides better robustness under sampling in open-ended generation, especially for multilingual and less well-represented languages. We hypothesize that the full teacher distribution provides richer token-level probabilities over long-tail tokens, (e.g., linguistically plausible alternatives, transliterations, and mixed-language function words), which helps reduce spurious characterlevel errors during stochastic decoding and is often lost with Top-K distillation. However, Top-K distillation appears suﬀicient for tasks with more constrained output spaces and singlecorrect-answer objectives, where the primary challenge lies in evidence selection and reasoning rather than modeling fine-grained token-level uncertainty. In summary, while full-logits distillation is not strictly necessary to achieve reasonable overall performance, it provides clear advantage for long-tail tokens robustness. We therefore adopt full-logits distillation for the rest of the experiments, as it enables higher accuracy on code-switching tasks and is more appropriate for sovereignty-focused setting. 8 Model CS Chat EN TH TH MTB MTB CS IF EN IFE EN TH IFE GPQA MMLU OTE M500 M500 LCB HPQA HPQA BFCL TH EN EN EN Math Code EN Tool / Agent TH Knowledge TH OPD Full OPD Top-K 8.24 6.38 8.24 6.44 93.40 83.18 75.98 33.84 29.76 61.19 74.00 66.93 31.21 15. 26.95 28.91 62.13 74.60 68.34 30.07 20.00 14.00 25.36 9.00 69.80 83.55 76.70 31.31 Average 43.94 42.81 Table 4 Comparison between OPD using full-logits distillation and Top-K distillation across the evaluation suite. Full-logits distillation achieves higher overall average and substantially improves robustness on Thai code-switching, while Top-K distillation performs comparably on more constrained, single-correct-answer tasks such as math reasoning and HotpotQA. 2.5.3 A3: Target-Language Data Is Essential for SFT, While Primarily Benefits OPD In Thai Native Tasks, With Limited Impact on Thai Translated Tasks To assess the impact of the target-language dataset at each stage of the training recipe, we compare models trained at each stage with and without the target-language data, with Thai consistently excluded across the SFT and OPD training stages in the latter setting. Table 5 shows that the inclusion of Thai target-language dataset plays different roles for each training stage. For SFT, Thai data is clearly important. Removing Thai data (SFT w/o Thai) causes large regressions across Thai-facing evaluations, including Thai chat (MTB TH: 5.67 4.36), Thai instruction following (IFE TH: 73.35 57.44), and especially code-switching (CS: 65.4 34.4). These results indicate that SFT is highly sensitive to data coverage: without explicit Thai supervision, the model fails to learn proper Thai alignment. Interestingly, adding Thai data also benefits some English benchmarks, suggesting that broader multilingual instruction coverage may improve overall alignment and robustness. For OPD, the gap between using and removing Thai data is smaller in terms of absolute average score (43.94 vs. 42.02), but the effect is more targeted. Thai data primarily improves Thai Native tasksThai chat (MTB TH: 6.28 6.44), OpenThaiEval (59.32 61.19), and code-switching (80.8 93.4), as well as instructionfollowing (IFE TH: 72.62 75.98), which we attribute to greater diversity in constraint scenarios. In contrast, Thai translated or language-agnostic tasks such as MATH500 and MMLU Pro show little change. Englishonly evaluations and knowledge-heavy multiple-choice benchmarks remain largely stable (e.g., GPQA). Overall, this pattern suggests that OPD already transfers much of the general interaction behavior from the teacher even without Thai data, while Thai supervision remains valuable for refining Thai-specific language capabilities. Model CS Chat EN TH TH MTB MTB CS IF EN IFE EN TH IFE GPQA MMLU OTE M500 M500 LCB HPQA HPQA BFCL TH EN EN EN Math Code EN Tool / Agent TH Knowledge TH SFT SFT w/o Thai 7.20 5.67 65.40 70.94 73.35 29.29 34.40 66.33 57.44 35.35 7.14 4.36 31.97 61.28 50.40 70.94 33.50 30.44 62.13 51.20 57.31 31. 0.00 1.00 0.00 24.36 0.00 24.70 OPD OPD w/o Thai 8.26 6.28 8.24 6.44 93.40 83.18 75.98 33.84 32.32 80.80 82.04 72.62 29.76 61.19 74.00 66.93 31.21 15.00 8.00 32.65 59.32 73.60 69.34 29. 9.00 6.00 26.95 27.19 Average 37.45 33.07 43.94 42.02 Table 5 Effect of Thai general-data inclusion on SFT and OPD. Removing Thai data severely degrades SFT performance on Thai-facing and code-switching tasks, while OPD is more robust and shows smaller overall regressions, with Thai data primarily improving Thai-specific and Thai-adjacent evaluations. 2.5.4 A4: Our Recipe Is Effective With Sovereignty-Adapted Base Model In all preceding experiments (RQ1-3), our recipeSFT followed by OPDhas shown strong effectiveness in adapting Qwen3 8B Instruct. However, an open question remains: how effective is our recipe when applied to sovereignty-adapted base model? 9 To answer this question, we apply our recipe to ThaiLLM-8B2, sovereign base model from the ThaiLLM3 project obtained by continuing pretraining Qwen3-8B-Base on 64B tokens of Thai corpus. Because ThaiLLM-8B is base model, it does not natively exhibit instruction-following behavior, making it strong test of whether our method can reliably produce an instruction-tuned model while preserving local-language strengths. We release the resulting model as Typhoon-S-ThaiLLM-8B45. Because ThaiLLM-8B is base model and does not reliably follow instructions, we do not report its scores on instruction-tuned benchmarks; instead, we evaluate the instruction-tuned model produced by our pipeline (Typhoon-S-8B) against strong instruction-tuned baseline (Qwen3-8B). Model Chat CS MTB CS IF Knowledge Agent HPQA IFE OTE Average 95.40 80.47 Qwen3-8B Typhoon-S-8B 7.89 96.60 76.45 7.08 63.66 67. 23.00 37.00 66.66 71.20 Table 6 Thai-only evaluation (higher is better) comparing Typhoon-S-8Btrained from the sovereignty-focused base ThaiLLM-8B using our SFT+OPD (full-logits) recipeagainst Qwen3-8B. Typhoon-S-8B improves Thai chat, codeswitching, Thai knowledge (OTE), and agentic QA, yielding higher Thai average (71.20 vs. 66.66). Table 6 shows that the resulting instruction-tuned model (Typhoon-S-8B6) surpasses Qwen3-8B on the strict Thai evaluation suite (i.e., tasks written originally in Thai rather than translated) and Agentic task. In particular, Typhoon-S-8B improves Thai chat quality, code-switching robustness, OpenThaiEval, and retrievalstyle QA, yielding higher overall Thai average (71.20 vs. 66.66). This suggests that starting from Thaioptimized base model and applying our post-training recipe can produce model that is both aligned and strong on local-language tasks. An interesting observation is that, on agentic tasks, performance improves to some extent when our method is applied to sovereignty-focused model. This finding raises the question of whether local knowledge alone is suﬀicient to improve performance in low-resource languages under unbiased post-training, which we leave for future work. When evaluated on the full English+Thai benchmark suite  (Table 7)  , Typhoon-S-8B reaches performance comparable to Qwen3-8B overall, though gap remains on two categories: (1) hard/scientific knowledge (e.g., GPQA and MMLU) and (2) mathematics (MATH500) (3) coding (LiveCodeBench). We also observe that several translated benchmarks may be biased toward Qwen3 8B, which is highly multilingual model (119 languages), likely due to cross-lingual ability with English-centric data. In contrast, on sovereignty-relevant evaluations emphasizing native Thai usage and local knowledge (e.g., OpenThaiEval and MT-Bench TH), the sovereignty-focused model maintains clear advantage over the highly multilingual model. Overall, these results show that our method adapts well to sovereignty-focused base model, yielding strong instructionfollowing model while preserving local-language performance."
        },
        {
            "title": "3 Sovereign Capability",
            "content": "Although the recipe introduced in the previous section is effective at transforming general-purpose base models into instruction-tuned models with strong performance across wide range of Thai and English tasks, such models often fall short on more complex problemsespecially those requiring regional, cultural, or domain-specific knowledge and reasoning that are underrepresented in general-domain data. common approach to mitigating this limitation is to introduce additional post-training stages targeted at specific capabilities. Reinforcement-learning-based approaches, such as RFT (Guo et al., 2025), have recently become standard choice for this purpose, as they enable targeted optimization for specific tasks while offering better 2https://huggingface.co/ThaiLLM/ThaiLLM-8B 3https://huggingface.co/ThaiLLM 4https://huggingface.co/scb10x/typhoon-s-thaillm-8b-instruct-research-preview 5To further improve performance, we replace the teacher model with stronger one (Hinton et al., 2015): Qwen/Qwen3-30BA3B-Instruct-2507. 6For conciseness, we use Typhoon-S-ThaiLLM-8B interchangeably with Typhoon-S-8B throughout the rest of the paper. 10 Model CS Chat EN TH TH MTB MTB CS IF EN IFE EN TH IFE GPQA MMLU OTE M500 M500 LCB HPQA HPQA BFCL TH EN EN EN Math Code EN Tool / Agent TH Knowledge TH Qwen3-8B Typhoon-S-8B 8.19 7.89 96.60 79.28 76.45 95.40 87.64 80.47 41.41 42.18 63.66 81.20 73.95 63.39 52.00 23.00 36.17 47.00 37.00 27.39 34.18 67.06 71.60 65.93 33.49 8.69 7.08 31. Average 54.02 49.99 Table 7 Full-suite evaluation in English and Thai comparing TyphoonS-8B (SFT+OPD with full-logits on the sovereignty-focused base ThaiLLM-8B) against Qwen3-8B. Typhoon-S-8B is stronger on Thai-centric benchmarks (e.g., MT-Bench TH, OpenThaiEval, and HPQA TH), but lags on English scientific/ knowledge benchmarks (GPQA, MMLU) and mathematics (MATH500), resulting in slightly lower overall average (51.88 vs. 54.72). Scores are reported per benchmark; higher is better unless otherwise noted. generalization than many alternative post-training techniques (Chu et al., 2025; Zhu et al., 2025; Chen et al., 2025; Shenfeld et al., 2025). However, standard RFT pipelines are typically designed to optimize task performance under fixed data and interaction assumptions (Guo et al., 2025; Zhao et al., 2025). As result, they exhibit two important limitations in domain-specialized settings: they are often ineffective at introducing new domain knowledge that is absent from the base model, and they provide limited support for agentic reasoning behaviors such as multi-turn decision-making and tool use. These limitations motivate the need for extensions to the standard RFT recipe that explicitly target both knowledge injection and agentic capabilities. In this section, we introduce two such extensions to standard RFT pipeline, both built on GRPO (Shao et al., 2024). Specifically, we propose Injected Knowledge GRPO (InK-GRPO), which augments the GRPO objective with stochastic next-token prediction, and Agentic RFT, which enables multi-step reasoning with external tools in controlled retrieval-augmented environment. We describe each extension in turn. 3.1 Injected Knowledge GRPO (InK-GRPO) Prior work suggests that RFT primarily amplifies knowledge already present in model, rather than introducing new factual information (Zhao et al., 2025). This limitation is especially problematic for sovereign and domain-specialized settings, where relevant domain knowledge and region-specific content are often underrepresented in both the base model and general-domain instruction data. To mitigate this issue, we propose InK-GRPO, which augments the standard GRPO objective with an auxiliary next-token prediction loss computed on separate in-domain text corpus. The key idea is to stochastically inject domain knowledge during RFT while preserving the benefits of task-focused reinforcement learning. Data Sources InK-GRPO training uses two distinct data sources: 1. Task prompt datasets: Used to generate GRPO rollouts and compute the GRPO loss (LGRPO) 2. In-domain text corpus: Used to compute the auxiliary next-token prediction loss (LCE) Training Procedure At each optimization step, we stochastically apply an auxiliary cross-entropy (CE) loss with probability ρ. Formally, the training objective is: = LGRPO + λ LCE, (1) where Bernoulli(ρ) determines whether the auxiliary loss is applied, and λ controls the relative weight of LCE when active. This stochastic scheduling allows the model to alternate between domain knowledge acquisition (via LCE) and task-specific optimization (via LGRPO), while maintaining focus on the latter through standard RFT. Pseudo-code for InK-GRPO is provided in Algorithm 1. 11 3.2 Agentic RFT While InK-GRPO addresses limitations in domain knowledge acquisition, standard RFT pipelines remain limited in their ability to support agentic reasoning behaviors such as multi-turn decision-making and tool use. To address this complementary limitation, we explore training an agentic reasoning model using RFT, with final-answer accuracy as the primary reward signal. During both training and inference, the model interacts with controlled retrieval-augmented generation (RAG) environment through two tools: (1) search, which performs semantic retrieval over an in-domain corpus, (2) read, which returns the full in-domain document given its identifier. The model learns to interleave natural language reasoning with tool calls across multiple interaction turns in order to gather relevant information and produce final answer. In this setup, GRPO is applied over entire interaction trajectories, enabling joint optimization of reasoning steps, tool-use decisions, and final-answer correctness within unified RFT framework. Since tool outputs are not generated by the model, tool responses are masked during gradient computation. Environment We formulate the RAG setup as partially observable environment in which the agent incrementally acquires information through tool interactions. At each step, the agent produces reasoning trace and then either issues tool call or emits final answer. Observations returned by the environment consist of retrieved document identifiers in response to search actions, or full document contents in response to read actions. An episode terminates when the agent outputs final answer or performs no further tool calls, at which point the reward is computed based on answer accuracy. 3.3 Experimental Setup 3.3.1 Training We follow standard RFT setup similar to DeepSeek-R1 (Guo et al., 2025). Specifically, we use GRPO as the base optimization algorithm integrated with our two aforementioned extensions: InK-GRPO and Agentic Training. Our training framework is built on veRL (Sheng et al., 2024). Reward Function Our reward function consists of two components: format reward and an accuracy reward. Format: We apply format reward to encourage the model to place its reasoning within the <thinking></ thinking> tags, with the final response generated outside these tags. We use the <thinking> tag instead of <think> because <think> is special token in the Qwen3 model but is not learned by the Instruct model. This results in uninitialized embeddings, which substantially increases training resource requirements. Accuracy: We evaluate the correctness of the generated response by comparing it to reference answer. Due to the open-ended nature of the outputs, we employ an LLM-as-a-Judge approach (Taveekitworachai et al., 2026) to determine answer correctness. We use reasoning-based judge model (e.g., gpt-5-nano) for correctness evaluation. In pilot experiments, instruction-tuned judge models (e.g., gpt-4o-mini) led to reward hacking, incentivizing the agent to generate excessively long responses; this behavior is mitigated by switching to reasoning judge model. The final reward (r [0, 1]) is weighted sum of the accuracy (racc) and format (rformat) rewards. The raw accuracy score is assigned in the range [0, 2], where 0 indicates an incorrect answer, 1 partially correct answer, and 2 fully correct answer. This score is then normalized to [0, 1], while the format reward is binary in [0, 1]. The weights of accuracy and format rewards are 0.9 and 0.1, respectively. This weighting follows our prior work in RFT (Taveekitworachai et al., 2025a, 2026). Formally, the reward function is = 0.9 racc + 0.1 rformat (2) 12 For agentic RFT, we use only the accuracy reward, i.e., we do not apply any format reward, as text-before tool calling already serves as implicit reasoning. We do not incorporate rewards for successful tool calls or any other complex reward components. Training Data The RAG environment operates over static in-domain document collection indexed as vector database. Documents are embedded using Qwen/Qwen3-Embedding-0.6B7 and stored in FAISS index with an IVF-SQ8 configuration. For each search action, the environment returns the top three documents based on vector similarity, while the read action returns the full content of specified document. No additional re-ranking or post-processing is applied in the retrieval pipeline. As InK-GRPO requires two sources of training data, RFT data and CE data, we prepare them as follows: RFT: Each sample is structured as question-answering task, where the model is prompted with question, generates responses, and receives reward feedback by comparing its generated response against reference answer CE: Each sample consists of unstructured text obtained by each in-domain textual evidence from the dataset, without including the question itself. This form of semi-supervision allows the model to acquire new knowledge (Chang et al., 2024) without being forced to generate specific response, as in supervised learning. At the same time, the model already possesses the knowledge required to answer the question, which RFT should focus on identifying and effectively utilizing. We primarily conduct our training and evaluation on NitiBench (Akarajaradwong et al., 2025), Thai legal dataset that represents sovereignty-focused setting. As NitiBench is provided as document collection suitable for RAG use cases (i.e., context-query-answer triples), we construct the training dataset by leveraging the contextual information associated with each example while omitting this context from the query itself. For NitiBench, we conduct experiments on the ccl split and treat the positive_contexts field as in-domain text for the CE loss computation, while the question and positive_answer fields are used for the GRPO loss computation. To evaluate the generalizability of our approach beyond single domain, we additionally conduct experiments in RQ5 on MIRAGE-Bench (Thakur et al., 2025), multilingual RAG benchmark designed to cover broader and more general settings. In this case, we use the training split of MIRAGE-BenchInstruct8 and focus on the Thai subset of the benchmark. To avoid potential interference between domains, each model is trained separately on single dataset, ensuring that injected in-domain knowledge remains consistent and non-conflicting. Training Details For our main experiments, we use Qwen3-4B-Instruct-2507 (Yang et al., 2025) as the base model. We perform full fine-tuning using AdamW optimizer with learning rate of 1 106. The sampling temperature is 0.7. We using gpt-5-nano as the judge model for all experiments. The full hyperparameters is in Table 13. GRPO Loss We adopt the DAPOs decoupled-clip-higher strategy (Yu et al., 2025), setting the high clip ratio to 0.24 and the low clip ratio to 0.20. This choice is motivated by our observation of entropy collapse when using the standard single clipping value (0.20) in pilot experiments. We also use overlong reward shaping (Yu et al., 2025) to penalize excessively long responses and getting cut off. CE Loss We set ρ to 0.6 and λ to 0.1, which were selected empirically based on pilot experiments. We did not perform dedicated ablation study on these hyperparameters; systematic investigation of their impact is left for future work. The batch size for calculating CE loss is set to twice the mini-batch size, and the updates are performed at the mini-batch level. 7https://huggingface.co/Qwen/Qwen3-Embedding-0.6B 8https://huggingface.co/datasets/nthakur/mirage-bench-instruct 13 3.3.2 Evaluation Benchmarks We evaluate our methods on benchmarks that mirror the training environments and emphasize underrepresented languages and domain-specific knowledge, settings in which base models typically underperform. Our primary evaluation benchmark is NitiBench (Akarajaradwong et al., 2025), which targets Thai legal reasoning and requires both precise reasoning and knowledge of local law. This benchmark serves as the main testbed for assessing the benefits of InK-GRPO and its integration with agentic RFT in sovereign settings. To assess generalization beyond single domain, we additionally conduct RQ5 experiments on MIRAGEBench (Thakur et al., 2025), Wikipedia-based multilingual RAG benchmark derived from TyDi QA and MIRACL (Clark et al., 2020; Zhang et al., 2023). These evaluation settings align with the corresponding training datasets and allow us to isolate generalization effects without introducing mismatched retrieval domains. To reduce inference costs, we randomly sample 10% of the evaluation data (approximately = 300), which pilot experiments show yields comparable results. For all evaluations, we use greedy decoding. Metrics We evaluate each test split corresponding to the training datasets and report task-specific accuracy. In RQ8, general performance is measured using the metrics defined in Section 2.4.2. To quantify general capability retention after domain-specific fine-tuning, we report general performance as the unweighted average across all evaluations described in Section 2.4.2, ensuring equal weight across tasks. 3.4 Results & Discussion To evaluate the effectiveness of our method, we seek to answer the following research questions: RQ5 Does InK-GRPO improve performance over GRPO in sovereignty-focused settings? RQ6 In InK-GRPO, is pretraining-style data or SFT-style instruction data more effective for CE loss? RQ7 Does agentic RFT further improve performance, and can the InK-GRPO extend to modern multi-turn agentic RFT settings? RQ8 How does InK-GRPO affect general capabilities and catastrophic forgetting? 3.4.1 A5: InK-GRPO Improves Performance in Sovereignty-Focused Settings We examine whether InK-GRPOby jointly optimizing task performance objective and pre-training (next-token prediction) objective during RFTenhances sovereignty-focused, task-specific performance. We compare InK-GRPO with standard GRPO on both NitiBench and MIRAGE-Bench. As described in our training setup, we train separate models for each dataset, resulting in four models in total. Table 8 shows that InK-GRPO variants outperform their GRPO-only counterparts on both datasets. On NitiBench, InK-GRPO yields 4% absolute improvement in accuracy (19.30% vs. 15.82%), while on MIRAGEBench, it achieves consistent gain of 1.6% (22.63% vs. 20.99%). These results indicate that augmenting RFT with stochastic in-domain next-token prediction objective, as in InK-GRPO, yields consistent performance gains. We further observe that the magnitude of improvement varies across datasets due to their differing characteristics. NitiBench, which emphasizes legal reasoning and multi-step reasoning, benefits substantially from GRPO itselfshowing more than 100% relative improvement over the base model. In contrast, MIRAGEBench, which focuses on multilingual understanding, exhibits smaller but consistent gains. Nonetheless, the additional improvement achieved by InK-GRPO indicates that integrating in-domain knowledge positively influences both datasets, regardless of task type. Finally, We further compare against GPT-5, which remains substantially stronger across both benchmarks, largely due to its much larger model scale compared to our 4B backbone. The gap is especially pronounced 14 on MIRAGE-Bench, which emphasizes general and multilingual knowledge, while on NitiBench GPT5 exhibits relative performance drop, reflecting the increased diﬀiculty of sovereignty-focused reasoning tasks. These results highlight the need for targeted adaptation methods such as InK-GRPO for improving performance in specialized domains. Model GPT-5 Qwen3 4B Instruct 2507 GRPO InK-GRPO NitiBench Accuracy MIRAGE Accuracy 28.79% 5.90% 15.82% 19.30% 53.91% 17.70% 20.99% 22.63% Table 8 Comparison of GRPO and InK-GRPO trained separately on NitiBench and MIRAGE-Bench. InK-GRPO consistently improves performance across datasets. 3.4.2 A6: Pretraining In-Domain Data Is More Effective Than SFT Data For InK-GRPO CE Loss Since the InK-GRPO CE loss is computed from training source separate from that used for the GRPO objective, we investigate whether the CE data should be drawn from pretraining-style data (as in RQ6) or SFT-style instruction data. To this end, we construct an SFT-style variant by using the question field as the prompt and the positive_answer field as the target the NitiBench training set. As shown in Table 10, both InK-GRPO variants outperform the GRPO baseline on NitiBench. However, InK-GRPO trained with pretraining-style data achieves the highest accuracy (19.30%), whereas the variant trained with SFT-style data yields smaller improvement (16.89%). These results suggest that, for joint optimization during RFT, pretraining-style in-domain text is more effective than SFT-style instruction data. One possible explanation is that SFT data may overly constrain the policy by biasing it toward narrow set of preferred responses, thereby limiting the exploration necessary for effective RFT. In contrast, pretraining-style language modeling offers broader domain exposure without directly optimizing the same behavioral space, which may better preserve exploration while still injecting domain knowledge. Model GRPO InK-GRPO PT InK-GRPO SFT NitiBench Accuracy 15.82% 19.30% 16.89% Table 9 NitiBench accuracy comparing GRPO with InK-GRPO variants that jointly optimize in-domain text using either pretraining-style corpora (PT) or SFT-style instruction data (SFT). Both variants improve over GRPO; however, the PT variant achieves the highest accuracy, suggesting that pretraining-style in-domain optimization more effectively complements RFT. 3.4.3 A7: Agentic RFT Further Improves Performance, and InK-GRPO Benefits Agentic Settings Table 10 shows that InK-GRPO outperforms the GRPO baseline in an agentic RFT setup, where the model operates in multi-turn agentic scenario. Notably, this approach enables 4B-parameter model to achieve task-specific performance exceeding that of GPT-5level baselines. We further observe that, despite the availability of externalized knowledge through tool use (e.g., search), incorporating in-domain pretrainingstyle data as in InK-GRPO yields additional gains over model trained with GRPO alone. We hypothesize that these improvements arise because in-domain corpora facilitate the acquisition of atomic task-relevant skills, enabling the model to more effectively learn complex behaviors necessary for diﬀicult tasks during RFT (Yuan et al., 2025). Model NitiBench Accuracy Qwen3-4B-Instruct-2507 + Agent GPT-5 + Search GPT-5 + Agent Agentic GRPO Agentic InK-GRPO 46.11% 38.07% 75.34% 73.73% 78.02% Table 10 Agentic (multi-turn) evaluation on NitiBench: InK-GRPO with agentic RFT outperforms GRPO under the same agentic retrieval setup and achieves higher accuracy than GPT-5 + Agent, which was evaluated with comparable tool-augmented configurations, and much higher than GPT-5 + Search, which is GPT-5 with built-in search. 3.4.4 A8: InK-GRPO Preserves General Capabilities Without Evidence of Severe Catastrophic Forgetting To assess general capability retention and potential catastrophic forgetting, we evaluate all post-trained models from RQ5RQ7 using the same general evaluation suite described in Section 2.5. This suite spans English and Thai chat, instruction following, knowledge, math, code, and tool/agent benchmarks. Overall, Table 11 shows that performance remains stable: across all settings, average scores fall within narrow range (48.08%-49.55%) relative to the base Qwen3 Instruct baseline (48.07%), regardless of the training algorithm. We emphasize that these models are trained exclusively on either NitiBench or MIRAGE-Bench, rather than on general-purpose benchmarks. These results indicate that neither GRPO nor InK-GRPO induces broad degradation of general capabilities, and in several cases both methods yield net improvements. We observe that performance changes are largely targeted rather than global. For example, GRPO-NitiBench achieves the highest overall average (49.55%) and improves chat and code-switching metrics without degrading performance on math or knowledge benchmarks. InK-GRPO variants typically closely track their GRPO counterparts, suggesting that the additional in-domain next-token objective does not harm generalization. This trend aligns with prior observations that RFT methods generally preserve existing knowledge and exhibit limited catastrophic forgetting (Chu et al., 2025; Zhu et al., 2025; Chen et al., 2025; Shenfeld et al., 2025). Model CS Chat EN TH TH MTB MTB CS IF EN IFE Knowledge TH Math Code EN Tool / Agent TH EN TH IFE GPQA MMLU OTE M500 M500 LCB HPQA HPQA BFCL TH EN EN EN Average Qwen3 4B Instruct 8.65 7.20 96.20 87.42 79.68 39.39 34.18 67.23 87.20 78.76 33.50 14.00 8. 31.53 48.07 GRPO-MIRAGE InK-GRPO-MIRAGE 8.56 8.47 7.04 7.07 95.00 88.03 79.44 96.00 86.61 78. 43.43 41.41 33.50 33.16 67.40 87.00 79.76 39.71 67.23 86.00 79.76 39.54 19.00 9.00 6.00 10.00 30.95 30. 41.41 8.84 6.96 98.20 87.66 77.50 GRPO-NitiBench 95.60 87.36 78.37 InK-GRPO-NitiBench (PT) 37.88 8.66 95.80 88.09 80.50 42.42 InK-GRPO-NitiBench (SFT) 8.72 7.05 7.07 34.35 67.91 86.40 79.16 39.05 36.05 66.98 84.80 79.16 38.40 67.32 84.40 80.36 36.27 35.20 16.00 19.00 31.27 30.66 17.00 16.00 30.89 10.00 16.00 GRPO + Agent InK-GRPO + Agent 8.63 8. 7.04 6.95 94.20 87.57 79.08 40.40 94.60 87.72 78.85 44.44 33.84 34.01 67.06 83.80 77.76 46.08 3.00 65.36 83.80 77.76 42.48 24.00 12.00 9.00 31.29 30. 48.92 48.08 49.55 48.86 48.79 47.77 49.35 Table 11 Evaluation for general capability retention and catastrophic forgetting. Higher is better. Across settings, GRPO and InK-GRPO preserve general performance with no evidence of severe catastrophic forgetting."
        },
        {
            "title": "4 Summary: Typhoon-S Recipe",
            "content": "This section summarizes the final Typhoon-S post-training recipes for: (1) transforming base model to an instruct model for improved adoptability and (2) building legal RAG agent to support sovereign capability. Both recipes are intentionally minimal, using only few training stages, openly available datasets, 16 transparent training details, and academic-scale compute. These recipes result in the following open releases of the models as: Typhoon-S-8B Instruct9 and Typhoon-S-4B Legal Agent10. 4.1 Typhoon-S-8B Instruct (Adoptability) We aim to develop recipe for transforming sovereignty-adapted base model into strong general-purpose instruction-tuned assistant while preserving Thai-native strengths. After validating the SFT+OPD recipe on Qwen3-4B in RQ1-RQ4 (Section 2), we apply the same approach to sovereignty-adapted base model to obtain Typhoon-S-8B Instruct. 4.1.1 Recipe We start from the sovereignty-adapted base model ThaiLLM/ThaiLLM-8B, an open model obtained by continued pretraining of Qwen3-8B-Base on 64B tokens of Thai corpus. We then apply two-stage post-training recipeSFT followed by OPDusing Qwen/Qwen3-30B-A3B-Instruct-2507 as the OPD teacher, which provides token-level distillation targets. Data We use open English instruction data as the foundation and add small, targeted Thai dataset for Thai alignment. The SFT data mixture is shown in Table 1, and the OPD mixture is shown in Table 2. In line with RQ3, Thai data is included at both stages to improve Thai-native performance. The full training datasets for each stage are openly available on Hugging Face. Training Stages 1. Stage 1: SFT Full fine-tuning on mixed instruction corpus (general instructions, tool use, and Thai alignment), optimizing cross-entropy loss on instruction-response pairs. 2. Stage 2: OPD Full-logits OPD on student-generated trajectories and sequences sampled from the SFT corpus, minimizing the forward KL divergence (teacher-to-student) at the token level. Hyperparameters We use the hyperparameters listed in Table 12. In brief, SFT uses AdamW optimizer with learning rate of 2 105 for 2 epochs with sequence packing up to 16,384 tokens. OPD uses AdamW optimizer with learning rate of 1 106 for 1 epoch with student-data fraction λ = 0.25. To make full-logits OPD feasible under limited GPU memory, we employ dynamic model swapping, FSDP with CPU offloading, and vLLM-backed rollouts. 4.1.2 Result The resulting model, Typhoon-S-8B Instruct, improves Thai-native chat quality, Thai code-switching robustness, Thai knowledge (OTE), and Thai agentic retrieval QA compared to strong multilingual instruction-tuned baseline (Qwen3-8B), while remaining competitive on general English benchmarks Detailed performance results are reported inTables 6 and 7. 4.2 Typhoon-S-4B Legal Agent (Sovereign Capability) We aim to improve Thai legal reasoning performance in sovereignty-focused setting by training RAG agent capable of multi-turn tool use to retrieve and reason over an in-domain legal corpus. 4.2.1 Recipe We start from Qwen3-4B-Instruct-2507 as general-purpose instruction-tuned backbone and adapt it to the Thai legal RAG setting via single agentic RFT stage. Specifically, we apply the recipe developed in RQ5-RQ8, namely the Agentic InK-GRPO algorithm Section 3. 9https://huggingface.co/typhoon-ai/typhoon-s-thaillm-8b-instruct-research-preview 10https://huggingface.co/typhoon-ai/typhoon-s-4b-nitibench-ccl-legal-agent-research-preview 17 Data Training is performed on the ccl split of NitiBench.11 The question field provides the agent prompt, the positive_answer field serves as the reference answer for reward computation, and the positive_contexts field provides in-domain legal content used as pretraining-style data for CE loss computation by InK-GRPO. The training dataset is available on Hugging Face. Environment and Tools Training and evaluation are conducted in controlled RAG environment with two tools: search: semantic retrieval over an in-domain legal corpus; read: returns the full content of selected document. Documents are embedded using Qwen/Qwen3-Embedding-0.6B and indexed in FAISS using an IVF-SQ8 configuration. Each search call returns the top-3 documents, and read returns the full document content. Tool outputs are masked from gradient computation during RFT. Agentic InK-GRPO Typhoon-S-4B Legal Agent is trained using single-stage Agentic InK-GRPO algorithm that combines: 1. Trajectory-level Agentic GRPO. The model interacts with the RAG environment over multiple tooluse turns, and GRPO is applied over entire trajectories using an accuracy-based reward. 2. Stochastic in-domain next-token prediction. At each optimization step, an auxiliary cross-entropy loss on in-domain legal text is activated with probability ρ, injecting legal-domain knowledge during RFT. Formally, the training loss at each step is = LGRPO + λ LCE, Bernoulli(ρ), (3) where LGRPO is computed from on-policy agent trajectories, and LCE is computed on separate in-domain, pretraining-style dataset. Reward Function Agentic training uses an accuracy-only reward based on an LLM-as-a-judge comparison between the models final answer and the reference answer. The judge model operates on full generated trajectories but the reward is assigned solely based on the final answer; no additional tool-specific rewards are used. Hyperparameters We use the RFT hyperparameters listed in Table 13. Key settings include: AdamW optimizer with learning rate of 1 106, sampling temperature of 0.7, DAPO-style decoupled clipping (clip-high 0.24, clip-low 0.20), overlong reward shaping, and InK-GRPO mixing with ρ = 0.6 and λ = 0.1. Training is performed on 4H100 GPUs. 4.3 Result The resulting model, Typhoon-S-4B Legal Agent, is trained with single-stage Agentic InK-GRPO and achieves the strongest NitiBench agentic accuracy among our 4B variants  (Table 10)  , surpassing both GRPOonly training and GPT-5 with comparable agentic setups, while preserving general capabilities without evidence of severe catastrophic forgetting  (Table 11)  ."
        },
        {
            "title": "5 Conclusion",
            "content": "We present Typhoon S, minimal and open post-training recipe for sovereign LLMs that enables both adoptability and sovereign capability under limited resources. Using Thai as case study, we show that SFT and OPD are suﬀicient to transform base models into competitive instruction-tuned assistants, while 11https://huggingface.co/datasets/airesearch/WangchanX-Legal-ThaiCCL-RAG 18 small-scale RFT with InK-GRPO substantially improves Thai legal reasoning and agentic retrieval without degrading general capabilities. These results are achieved with academic-scale compute, demonstrating that careful post-training designnot scale alonecan support practical, transparent, and sovereign LLMs. Limitations & Future Work While we extensively experiment with modern post-training approaches such as SFT, OPD, and RFT in this report, we do not explore pre-training or mid-training, as these experiments require resources at larger scale than are currently available to us. Future studies can extend our findings by investigating the dynamics of preand mid-training under our proposed approaches. Our study focuses on Thai because we have access to the necessary data and domain expertise and, as language owners, possess the cultural and linguistic knowledge required for effective evaluation. However, we believe that our proposed approaches and findings generalize to other settings and are not specific to Thai (i.e., they are not tied to fixed Thai dataset). Accordingly, we aim to examine how well the approach generalizes to other languages and cultural contexts in future work. All experiments were conducted using 4H100 GPUs, except for the final run of section 2 (Typhoon-S-8BInstruct), which was trained on 8H100 GPUs at scale comparable to those used by many national and academic laboratories. Scaling to larger setups, or systematically studying the scaling behavior of these techniques, remains an important direction for future work."
        },
        {
            "title": "Acknowledgments",
            "content": "Beyond the primary authors, we gratefully acknowledge the Typhoon Team members at SCB 10X. We also extend our appreciation to the SCBx R&D Team for their support, resources, and valuable insights. Lastly, we are grateful to the global and local AI communities for open-sourcing resources and sharing knowledge."
        },
        {
            "title": "References",
            "content": "SCB 10X, VISTEC, and SEACrowd. Thai llm leaderboard, 2024. URL https://huggingface.co/spaces/ ThaiLLM-Leaderboard/leaderboard. Rishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Stanczyk, Sabela Ramos, Matthieu Geist, and Olivier Bachem. On-policy distillation of language models: Learning from self-generated mistakes, 2024. URL https://arxiv.org/ abs/2306.13649. Pawitsapak Akarajaradwong, Pirat Pothavorn, Chompakorn Chaksangchaichot, Panuthep Tasawong, Thitiwat Nopparatbundit, Keerakiat Pratai, and Sarana Nutanong. NitiBench: Benchmarking LLM frameworks on Thai legal question answering capabilities. In Christos Christodoulopoulos, Tanmoy Chakraborty, Carolyn Rose, and Violet Peng, editors, Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 3429234315, Suzhou, China, November 2025. Association for Computational Linguistics. ISBN 979-8-89176-332-6. doi: 10.18653/v1/2025.emnlp-main.1739. URL https://aclanthology.org/2025.emnlp-main.1739/. Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, Ido Shahaf, Oren Tropp, Ehud Karpas, Ran Zilberstein, Jiaqi Zeng, Soumye Singhal, Alexander Bukharin, Yian Zhang, Tugrul Konuk, Gerald Shen, Ameya Sunil Mahabaleshwarkar, Bilal Kartal, Yoshi Suhara, Olivier Delalleau, Zijia Chen, Zhilin Wang, David Mosallanezhad, Adi Renduchintala, Haifeng Qian, Dima Rekesh, Fei Jia, Somshubra Majumdar, Vahid Noroozi, Wasi Uddin Ahmad, Sean Narenthiran, Aleksander Ficek, Mehrzad Samadi, Jocelyn Huang, Siddhartha Jain, Igor Gitman, Ivan Moshkov, Wei Du, Shubham Toshniwal, George Armstrong, Branislav Kisacanin, Matvei Novikov, Daria Gitman, Evelina Bakhturina, Prasoon Varshney, Makesh Narsimhan, Jane Polak Scowcroft, John Kamalu, Dan Su, Kezhi Kong, Markus Kliegl, Rabeeh Karimi Mahabadi, Ying Lin, Sanjeev Satheesh, Jupinder Parmar, Pritam Gundecha, Brandon Norick, Joseph Jennings, Shrimai Prabhumoye, Syeda Nahida Akter, Mostofa Patwary, Abhinav Khattar, Deepak Narayanan, Roger Waleffe, Jimmy Zhang, Bor-Yiing Su, Guyue Huang, Terry Kong, Parth Chadha, Sahil Jain, Christine Harvey, Elad Segal, Jining Huang, Sergey Kashirsky, Robert McQueen, Izzy Putterman, George Lam, Arun Venkatesan, Sherry Wu, Vinh Nguyen, Manoj Kilaru, Andrew Wang, Anna Warno, Abhilash Somasamudramath, Sandip Bhaskar, Maka Dong, Nave Assaf, Shahar Mor, Omer Ullman Argov, Scot Junkin, Oleksandr Romanenko, Pedro Larroy, Monika Katariya, Marco Rovinelli, Viji Balas, Nicholas Edelman, Anahita Bhiwandiwalla, Muthu Subramaniam, Smita 19 Ithape, Karthik Ramamoorthy, Yuting Wu, Suguna Varshini Velury, Omri Almog, Joyjit Daw, Denys Fridman, Erick Galinkin, Michael Evans, Shaona Ghosh, Katherine Luna, Leon Derczynski, Nikki Pope, Eileen Long, Seth Schneider, Guillermo Siman, Tomasz Grzegorzek, Pablo Ribalta, Monika Katariya, Chris Alexiuk, Joey Conway, Trisha Saar, Ann Guan, Krzysztof Pawelec, Shyamala Prayaga, Oleksii Kuchaiev, Boris Ginsburg, Oluwatobi Olabiyi, Kari Briski, Jonathan Cohen, Bryan Catanzaro, Jonah Alben, Yonatan Geifman, and Eric Chung. Llamanemotron: Eﬀicient reasoning models, 2025. URL https://arxiv.org/abs/2505.00949. Hoyeon Chang, Jinho Park, Seonghyeon Ye, Sohee Yang, Youngkyung Seo, Du-Seong Chang, and Minjoon Seo. How do large language models acquire factual knowledge during pretraining? In Proceedings of the 38th International Conference on Neural Information Processing Systems, NIPS 24, Red Hook, NY, USA, 2024. Curran Associates Inc. ISBN 9798331314385. Howard Chen, Noam Razin, Karthik Narasimhan, and Danqi Chen. Retaining by doing: The role of on-policy data in mitigating forgetting, 2025. URL https://arxiv.org/abs/2510.18874. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. SFT memorizes, RL generalizes: comparative study of foundation model post-training. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/forum?id= dYur3yabMj. Jonathan H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. TyDi QA: benchmark for information-seeking question answering in typologically diverse languages. Transactions of the Association for Computational Linguistics, 8:454470, 2020. doi: 10.1162/tacl_a_00317. URL https://aclanthology.org/2020.tacl-1.30/. DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Yang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, and Zizheng Pan. Deepseek-v3 technical report, 2025. URL https://arxiv.org/abs/2412.19437. Guanting Dong, Keming Lu, Chengpeng Li, Tingyu Xia, Bowen Yu, Chang Zhou, and Jingren Zhou. Self-play with execution feedback: Improving instruction-following capabilities of large language models, 2024. URL https: //arxiv.org/abs/2406.13542. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Hanwei Xu, Honghui Ding, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jingchang Chen, Jingyang Yuan, Jinhao Tu, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaichao You, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong 20 Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingxu Zhou, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081):633638, September 2025. ISSN 1476-4687. doi: 10.1038/s41586-025-09422-z. URL http://dx.doi.org/10.1038/s41586-025-09422-z. Alejandro Hernández-Cano, Alexander Hägele, Allen Hao Huang, Angelika Romanou, Antoni-Joan Solergibert, Barna Pasztor, Bettina Messmer, Dhia Garbaya, Eduard Frank Ďurech, Ido Hakimi, Juan García Giraldo, Mete Ismayilzada, Negar Foroutan, Skander Moalla, Tiancheng Chen, Vinko Sabolčec, Yixuan Xu, Michael Aerni, Badr AlKhamissi, Ines Altemir Marinas, Mohammad Hossein Amani, Matin Ansaripour, Ilia Badanin, Harold Benoit, Emanuela Boros, Nicholas Browning, Fabian Bösch, Maximilian Böther, Niklas Canova, Camille Challier, Clement Charmillot, Jonathan Coles, Jan Deriu, Arnout Devos, Lukas Drescher, Daniil Dzenhaliou, Maud Ehrmann, Dongyang Fan, Simin Fan, Silin Gao, Miguel Gila, María Grandury, Diba Hashemi, Alexander Hoyle, Jiaming Jiang, Mark Klein, Andrei Kucharavy, Anastasiia Kucherenko, Frederike Lübeck, Roman Machacek, Theofilos Manitaras, Andreas Marfurt, Kyle Matoba, Simon Matrenok, Henrique Mendoncça, Fawzi Roberto Mohamed, Syrielle Montariol, Luca Mouchel, Sven Najem-Meyer, Jingwei Ni, Gennaro Oliva, Matteo Pagliardini, Elia Palme, Andrei Panferov, Léo Paoletti, Marco Passerini, Ivan Pavlov, Auguste Poiroux, Kaustubh Ponkshe, Nathan Ranchin, Javi Rando, Mathieu Sauser, Jakhongir Saydaliev, Muhammad Ali Sayfiddinov, Marian Schneider, Stefano Schuppli, Marco Scialanga, Andrei Semenov, Kumar Shridhar, Raghav Singhal, Anna Sotnikova, Alexander Sternfeld, Ayush Kumar Tarun, Paul Teiletche, Jannis Vamvas, Xiaozhe Yao, Hao Zhao Alexander Ilic, Ana Klimovic, Andreas Krause, Caglar Gulcehre, David Rosenthal, Elliott Ash, Florian Tramèr, Joost VandeVondele, Livio Veraldi, Martin Rajman, Thomas Schulthess, Torsten Hoefler, Antoine Bosselut, Martin Jaggi, and Imanol Schlag. Apertus: Democratizing Open and Compliant LLMs for Global Language Environments. https://arxiv.org/abs/2509.14233, 2025. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network, 2015. URL https: //arxiv.org/abs/1503.02531. Jian Hu, Xibin Wu, Wei Shen, Jason Klein Liu, Zilin Zhu, Weixun Wang, Songlin Jiang, Haoran Wang, Hao Chen, Bin Chen, Weikai Fang, Xianyu, Yu Cao, Haotian Xu, and Yiming Liu. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework, 2025. URL https://arxiv.org/abs/2405.11143. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code, 2024. URL https://arxiv.org/abs/2403.07974. Yoon Kim and Alexander M. Rush. Sequence-level knowledge distillation. In Jian Su, Kevin Duh, and Xavier Carreras, editors, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1317 1327, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1139. URL https://aclanthology.org/D16-1139/. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. Tulu 3: Pushing frontiers in open language model post-training, 2025. URL https://arxiv.org/abs/2411.15124. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step, 2023. URL https://arxiv.org/abs/2305.20050. 21 Peerat Limkonchotiwat, Pume Tuchinda, Lalita Lowphansirikul, Surapon Nonesung, Panuthep Tasawong, Alham Fikri Aji, Can Udomcharoenchaikit, and Sarana Nutanong. Wangchanthaiinstruct: An instruction-following dataset for culture-aware, multitask, and multi-domain evaluation in thai, 2025. URL https://arxiv.org/abs/2508.15239. Zhiwei Liu, Weiran Yao, Jianguo Zhang, Liangwei Yang, Zuxin Liu, Juntao Tan, Prafulla K. Choubey, Tian Lan, Jason Wu, Huan Wang, Shelby Heinecke, Caiming Xiong, and Silvio Savarese. Agentlite: lightweight library for building and advancing task-oriented llm agent system, 2024. Kevin Lu and Thinking Machines Lab. On-policy distillation. Thinking Machines Lab: Connectionism, 2025. doi: 10.64434/tml.20251026. https://thinkingmachines.ai/blog/on-policy-distillation. Team Olmo, :, Allyson Ettinger, Amanda Bertsch, Bailey Kuehl, David Graham, David Heineman, Dirk Groeneveld, Faeze Brahman, Finbarr Timbers, Hamish Ivison, Jacob Morrison, Jake Poznanski, Kyle Lo, Luca Soldaini, Matt Jordan, Mayee Chen, Michael Noukhovitch, Nathan Lambert, Pete Walsh, Pradeep Dasigi, Robert Berry, Saumya Malik, Saurabh Shah, Scott Geng, Shane Arora, Shashank Gupta, Taira Anderson, Teng Xiao, Tyler Murray, Tyler Romero, Victoria Graf, Akari Asai, Akshita Bhagia, Alexander Wettig, Alisa Liu, Aman Rangapur, Chloe Anastasiades, Costa Huang, Dustin Schwenk, Harsh Trivedi, Ian Magnusson, Jaron Lochner, Jiacheng Liu, Lester James V. Miranda, Maarten Sap, Malia Morgan, Michael Schmitz, Michal Guerquin, Michael Wilson, Regan Huff, Ronan Le Bras, Rui Xin, Rulin Shao, Sam Skjonsberg, Shannon Zejiang Shen, Shuyue Stella Li, Tucker Wilde, Valentina Pyatkin, Will Merrill, Yapei Chang, Yuling Gu, Zhiyuan Zeng, Ashish Sabharwal, Luke Zettlemoyer, Pang Wei Koh, Ali Farhadi, Noah A. Smith, and Hannaneh Hajishirzi. Olmo 3, 2025. URL https://arxiv.org/ abs/2512.13961. David Ong and Peerat Limkonchotiwat. SEA-LION (Southeast Asian languages in one network): family of Southeast Asian language models. In Liling Tan, Dmitrijs Milajevs, Geeticka Chauhan, Jeremy Gwinnup, and Elijah Rippeth, editors, Proceedings of the 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023), pages 245245, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. nlposs-1.26. URL https://aclanthology.org/2023.nlposs-1.26/. Shishir G. Patil, Huanzhi Mao, Charlie Cheng-Jie Ji, Fanjia Yan, Vishnu Suresh, Ion Stoica, and Joseph E. Gonzalez. The berkeley function calling leaderboard (bfcl): From tool use to agentic evaluation of large language models. In Forty-second International Conference on Machine Learning, 2025. Patomporn Payoungkhamdee, Wannaphong Phatthiyaphaibun, Surapon Nonesung, Chalermpun Mai-On, Lalita Lowphansirikul, Parinthapat Pengpun, and Peerat Limkonchotiwat. Mt-bench thai, 2024. URL https://huggingface. co/datasets/ThaiLLM-Leaderboard/mt-bench-thai. Version 1.0. Wannaphong Phatthiyaphaibun. Han instruct dataset, July 2024. URL https://doi.org/10.5281/zenodo.13145164. Kunat Pipatanakul, Phatrasek Jirabovonvisut, Potsawee Manakul, Sittipong Sripaisarnmongkol, Ruangsak Patomwong, Pathomporn Chokchainant, and Kasima Tharnpipitchai. Typhoon: Thai large language models, 2023. URL https://arxiv.org/abs/2312.13951. Kunat Pipatanakul, Potsawee Manakul, Natapong Nitarach, Warit Sirichotedumrong, Surapon Nonesung, Teetouch Jaknamon, Parinthapat Pengpun, Pittawat Taveekitworachai, Adisai Na-Thalang, Sittipong Sripaisarnmongkol, Krisanapong Jirayoot, and Kasima Tharnpipitchai. Typhoon 2: family of open text and multimodal thai large language models, 2024. URL https://arxiv.org/abs/2412.13702. Kunat Pipatanakul, Pittawat Taveekitworachai, Potsawee Manakul, and Kasima Tharnpipitchai. Adapting languagespecific llms to reasoning model in one day via model merging an open recipe, 2025. URL https://arxiv.org/ abs/2502.09056. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: graduate-level google-proof q&a benchmark, 2023. URL https: //arxiv.org/abs/2311.12022. Lisa Schut, Yarin Gal, and Sebastian Farquhar. Do multilingual llms think in english?, 2025. URL https://arxiv. org/abs/2502.15603. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300. 22 Idan Shenfeld, Jyothish Pari, and Pulkit Agrawal. RLs razor: Why on-policy reinforcement learning forgets less. In AI That Keeps Up: NeurIPS 2025 Workshop on Continual and Compatible Foundation Model Updates, 2025. URL https://openreview.net/forum?id=GO0X9nQot3. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and eﬀicient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Pittawat Taveekitworachai, Potsawee Manakul, Sarana Nutanong, and Kunat Pipatanakul. Prior prompt engineering for reinforcement fine-tuning, 2025a. URL https://arxiv.org/abs/2505.14157. Pittawat Taveekitworachai, Potsawee Manakul, Kasima Tharnpipitchai, and Kunat Pipatanakul. Typhoon t1: An open thai reasoning model, 2025b. URL https://arxiv.org/abs/2502.09042. Pittawat Taveekitworachai, Natpatchara Pongjirapat, Krittaphas Chaisutyakorn, Piyalitt Ittichaiwong, Tossaporn Saengja, and Kunat Pipatanakul. On the robustness of answer formats in medical reasoning models, 2026. URL https://arxiv.org/abs/2509.20866. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, Gaël Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Robert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, JanThorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, Ravin Kumar, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner, Abe Friesen, Abhanshu Sharma, Abheesht Sharma, Adi Mayrav Gilady, Adrian Goedeckemeyer, Alaa Saade, Alex Feng, Alexander Kolesnikov, Alexei Bendebury, Alvin Abdagic, Amit Vadi, András György, André Susano Pinto, Anil Das, Ankur Bapna, Antoine Miech, Antoine Yang, Antonia Paterson, Ashish Shenoy, Ayan Chakrabarti, Bilal Piot, Bo Wu, Bobak Shahriari, Bryce Petrini, Charlie Chen, Charline Le Lan, Christopher A. Choquette-Choo, CJ Carey, Cormac Brick, Daniel Deutsch, Danielle Eisenbud, Dee Cattle, Derek Cheng, Dimitris Paparas, Divyashree Shivakumar Sreepathihalli, Doug Reid, Dustin Tran, Dustin Zelle, Eric Noland, Erwin Huizenga, Eugene Kharitonov, Frederick Liu, Gagik Amirkhanyan, Glenn Cameron, Hadi Hashemi, Hanna Klimczak-Plucińska, Harman Singh, Harsh Mehta, Harshal Tushar Lehri, Hussein Hazimeh, Ian Ballantyne, Idan Szpektor, Ivan Nardini, Jean Pouget-Abadie, Jetha Chan, Joe Stanton, John Wieting, Jonathan Lai, Jordi Orbay, Joseph Fernandez, Josh Newlan, Ju yeong Ji, Jyotinder Singh, Kat Black, Kathy Yu, Kevin Hui, Kiran Vodrahalli, Klaus Greff, Linhai Qiu, Marcella Valentine, Marina Coelho, Marvin Ritter, Matt Hoffman, Matthew Watson, Mayank Chaturvedi, Michael Moynihan, Min Ma, Nabila Babar, Natasha Noy, Nathan Byrd, Nick Roy, Nikola Momchev, Nilay Chauhan, Noveen Sachdeva, Oskar Bunyan, Pankil Botarda, Paul Caron, Paul Kishan Rubenstein, Phil Culliton, Philipp Schmid, Pier Giuseppe Sessa, Pingmei Xu, Piotr Stanczyk, Pouya Tafti, Rakesh Shivanna, Renjie Wu, Renke Pan, Reza Rokni, Rob Willoughby, Rohith Vallu, Ryan Mullins, Sammy Jerome, Sara Smoot, Sertan Girgin, Shariq Iqbal, Shashir Reddy, Shruti Sheth, Siim Põder, Sijal Bhatnagar, Sindhu Raghuram Panyam, Sivan Eiger, Susan Zhang, Tianqi Liu, Trevor Yacovone, Tyler Liechty, Uday Kalra, Utku Evci, Vedant Misra, Vincent Roseberry, Vlad Feinberg, Vlad Kolesnikov, Woohyun Han, Woosuk Kwon, Xi Chen, Yinlam Chow, Yuvein Zhu, Zichuan Wei, Zoltan Egyed, Victor Cotruta, Minh Giang, Phoebe Kirk, Anand Rao, Kat Black, Nabila Babar, Jessica Lo, Erica Moreira, Luiz Gustavo Martins, Omar Sanseviero, Lucas Gonzalez, Zach Gleicher, Tris Warkentin, Vahab Mirrokni, Evan Senter, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, Yossi Matias, D. Sculley, Slav Petrov, Noah Fiedel, Noam Shazeer, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Jean-Baptiste Alayrac, Rohan Anil, Dmitry, Lepikhin, Sebastian Borgeaud, Olivier Bachem, Armand Joulin, Alek Andreev, Cassidy Hardin, Robert Dadashi, and Léonard Hussenot. Gemma 3 technical report, 2025. URL https://arxiv.org/abs/2503.19786. Nandan Thakur, Suleman Kazi, Ge Luo, Jimmy Lin, and Amin Ahmad. Mirage-bench: Automatic multilingual benchmark arena for retrieval-augmented generation systems, 2025. URL https://arxiv.org/abs/2410.13716. Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouédec. Trl: Transformer reinforcement learning. https://github.com/ huggingface/trl, 2020. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. Mmlu-pro: more robust and challenging multi-task language understanding benchmark, 2024. URL https://arxiv.org/abs/2406.01574. Zhangchen Xu, Adriana Meza Soria, Shawn Tan, Anurag Roy, Ashish Sunil Agrawal, Radha Poovendran, and 23 Rameswar Panda. Toucan: Synthesizing 1.5m tool-agentic data from real-world mcp environments, 2025. URL https://arxiv.org/abs/2510.01179. Weihao Xuan, Rui Yang, Heli Qi, Qingcheng Zeng, Yunze Xiao, Aosong Feng, Dairui Liu, Yun Xing, Junjue Wang, Fan Gao, Jinghui Lu, Yuang Jiang, Huitao Li, Xin Li, Kunyu Yu, Ruihai Dong, Shangding Gu, Yuekang Li, Xiaofei Xie, Felix Juefei-Xu, Foutse Khomh, Osamu Yoshie, Qingyu Chen, Douglas Teodoro, Nan Liu, Randy Goebel, Lei Ma, Edison Marrese-Taylor, Shijian Lu, Yusuke Iwasawa, Yutaka Matsuo, and Irene Li. Mmlu-prox: multilingual benchmark for advanced large language model evaluation, 2025. URL https://arxiv.org/abs/2503.10497. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering, 2018. URL https://arxiv. org/abs/1809.09600. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale, 2025. URL https: //arxiv.org/abs/2503.14476. Lifan Yuan, Weize Chen, Yuchen Zhang, Ganqu Cui, Hanbin Wang, Ziming You, Ning Ding, Zhiyuan Liu, Maosong Sun, and Hao Peng. From (x) and g(x) to (g(x)): Llms learn new skills in rl by composing old ones, 2025. URL https://arxiv.org/abs/2509.25123. Sumeth Yuenyong, Thodsaporn Chay-intr, and Kobkrit Viriyayudhakorn. Openthaigpt 1.6 and r1: Thai-centric open source and reasoning large language models, 2025a. URL https://arxiv.org/abs/2504.01789. Sumeth Yuenyong, Kobkrit Viriyayudhakorn, Apivadee Piyatumrong, and Jillaphat Jaroenkantasima. Openthaigpt 1.5: thai-centric open source large language model, 2025b. URL https://arxiv.org/abs/2411.07238. Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-Hermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin. MIRACL: multilingual retrieval dataset covering 18 diverse languages. Transactions of the Association for Computational Linguistics, 11:11141131, 2023. doi: 10.1162/tacl_a_00595. URL https://aclanthology.org/2023.tacl-1.63/. Rosie Zhao, Alexandru Meterez, Sham Kakade, Cengiz Pehlevan, Samy Jelassi, and Eran Malach. Echo chamber: Rl post-training amplifies behaviors learned in pretraining, 2025. URL https://arxiv.org/abs/2504.07912. Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. Wildchat: 1m chatgpt interaction logs in the wild, 2024. URL https://arxiv.org/abs/2405.01470. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. URL https://arxiv.org/abs/2306.05685. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models, 2023. URL https://arxiv.org/abs/2311.07911. Hanqing Zhu, Zhenyu Zhang, Hanxian Huang, DiJia Su, Zechun Liu, Jiawei Zhao, Igor Fedorov, Hamed Pirsiavash, Jinwon Lee, David Z. Pan, Zhangyang Wang, Yuandong Tian, and Kai Sheng Tai. The path not taken: RLVR provably learns off the principals. In NeurIPS 2025 Workshop on Eﬀicient Reasoning, 2025. URL https:// openreview.net/forum?id=N75EWQQnb3."
        },
        {
            "title": "A Hyperparameters",
            "content": "This section summarizes the hyperparameter configurations used in our post-training experiments. Unless otherwise noted, hyperparameters were selected based on preliminary tuning and held fixed across benchmarks to ensure comparability of results. A.1 Hyperparameter for SFT & OPD Table 12 reports the hyperparameters used for SFT and OPD. These settings are shared across all benchmarks unless explicitly stated. Hyperparameter SFT OPD Training Configuration Optimizer Learning rate Epochs Global batch size LR scheduler Warmup ratio Max sequence length OPD Specific λ (student-data fraction) Sampling temperature Top-p Max completion length AdamW AdamW 1 106 2 105 1 2 64 32 Cosine Cosine 0.05 0.05 4096 16384 0.25 1.0 0.95 2048 Table 12 Hyperparameters used for SFT and OPD experiments. In RQ 2, we specifically apply top-k = 10 distillation, while other steps utilize the full distribution. A.2 Hyperparameter for RFT Table 13 lists the hyperparameters used for RFT experiments based on GRPO. In addition to standard GRPO settings, we include parameters for the stochastic auxiliary CE objective used in InK-GRPO, as well as agentic-specific configurations where applicable."
        },
        {
            "title": "B Prompt Templates",
            "content": "This section provides the complete prompt templates used throughout our experiments. These prompts define how we evaluate model responses and compute reward signals during training. B.1 RL Reward Calculation Prompt We use an LLM-as-a-judge approach to compute rewards during RFT. The prompt in Figure 2 instructs judge model to evaluate responses on 3-point scale (0-2) based on task-specific criteria. The judge assesses whether responses meet all specified requirements and returns structured feedback including both numerical score and explanation. 25 Hyperparameter NitiBench NitiBench-Agentic MIRAGE-Bench GRPO Optimizer Learning rate Epochs GRPO epoch per step Optimization steps Global batch size Mini-batch size KL with ref policy LR scheduler Sampling temperature Max response length Clip-high ratio Clip-low ratio Overlong reward shaping CE ρ λ CE batch size CE update frequency Agentic Max user turns Max tool response length AdamW 1 106 2 1 512 32 32 No Constant 0.7 4096 tokens 0.24 0.20 AdamW 1 106 1 1 256 32 8 No Constant 0.7 8192 tokens 0.24 0.20 AdamW 1 106 3 1 204 32 32 No Constant 0.7 4096 tokens 0.24 0.20 0.6 0.1 64 Per mini-batch 0.6 0.1 16 Per mini-batch 0.6 0.1 64 Per mini-batch 5 1024 tokens Table 13 Hyperparameters used for RFT experiments across NitiBench, NitiBench-Agentic, and MIRAGE-Bench. GRPO denotes standard RFT settings; CE denotes the stochastic auxiliary cross-entropy objective used in InKGRPO. Agentic-specific hyperparameters are applied only to NitiBench-Agentic. 26 You are an expert evaluator. Your task is to evaluate how well response addresses given prompt according to specific evaluation criteria. # Task Evaluate the response below using 3-level scoring system: - **Score 0**: The response is incorrect, irrelevant, or does not address the requirements - **Score 1**: The response partially addresses the requirements but has significant gaps, errors, or missing information - **Score 2**: The response fully addresses all requirements correctly and completely # Evaluation Criteria {llm_judge_prompt} # User Prompt {prompt} # Response to Evaluate {response_to_judge} # Instructions 1. Carefully check if the response meets ALL requirements specified in the evaluation criteria 2. Assign score of 0, 1, or 2 based on how well it meets the criteria 3. Provide brief explanation justifying your score 4. Return your evaluation in the following JSON format: {{ ''score'': <0, 1, or 2>, ''explanation'': }} ''<Brief explanation of why you gave this score>'' {ground truth} Figure 2 Prompt template for calculating RFT rewards using LLM-as-a-judge evaluation. B.2 LLM Judge Prompt We evaluate model responses using an LLM-based judge that determines correctness by comparing generated answer against reference solution. The judge is instructed to assess whether the model output matches the ground-truth answer exactly, without partial credit or subjective interpretation. Figure 3 shows the prompt template used for this evaluation. To correctly answer this question, the response must match the following answer: {ground_truth} Figure 3 LLM-judge prompt used to compute an accuracy score by comparing model responses against groundtruth answer. Pseudo-code For InK-GRPO Algorithm 1 details the InK-GRPO training procedure, which combines on-policy GRPO with stochastic in-domain language modeling (CE objective). At each training step, the algorithm: (1) collects on-policy rollouts and computes the GRPO objective, and (2) stochastically mixes in cross-entropy loss on in-domain text data with probability ρ and weight λ. This approach optimizes task-specific rewards while teaching knowledge. 27 Algorithm 1 InK-GRPO Training (On-Policy GRPO with Stochastic In-Domain CE Augmentation) Require: Policy model πθ; optional reference policy πref; reward function R(); prompt distribution Dprompt; in-domain text corpus DCE; mixing probability ρ; CE weight λ; rollouts per prompt K; number of training steps ; GRPO epochs per step E; GRPO minibatches per step ; learning rate η 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 1: for = 1 to do 2: 3: // On-policy GRPO data collection Sample prompts Dprompt For each P, sample responses: {y(k)}K πθ( p) Compute rewards: r(k) R(p, y(k)) for all P, [K] Construct GRPO training buffer DGRPO {(p, y(k), r(k))} k=1 // Optimize on collected data with minibatches for = 1 to do Shuffle DGRPO for = 1 to do Sample GRPO minibatch BGRPO DGRPO if πref is provided then LGRPO GrpoObjective(πθ, BGRPO, πref) else LGRPO GrpoObjective(πθ, BGRPO) end if // Stochastic in-domain CE augmentation (per minibatch update) Sample Bernoulli(ρ) if = 1 then Sample in-domain batch BCE DCE LCE NextTokenLogLik(πθ, BCE) LGRPO + λ LCE else LGRPO end if // Update parameters θ θ + η θL 26: 27: 28: 29: 30: end for end for end for"
        }
    ],
    "affiliations": [
        "Typhoon, SCB 10X"
    ]
}