{
    "paper_title": "Advances in Speech Separation: Techniques, Challenges, and Future Trends",
    "authors": [
        "Kai Li",
        "Guo Chen",
        "Wendi Sang",
        "Yi Luo",
        "Zhuo Chen",
        "Shuai Wang",
        "Shulin He",
        "Zhong-Qiu Wang",
        "Andong Li",
        "Zhiyong Wu",
        "Xiaolin Hu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The field of speech separation, addressing the \"cocktail party problem\", has seen revolutionary advances with DNNs. Speech separation enhances clarity in complex acoustic environments and serves as crucial pre-processing for speech recognition and speaker recognition. However, current literature focuses narrowly on specific architectures or isolated approaches, creating fragmented understanding. This survey addresses this gap by providing systematic examination of DNN-based speech separation techniques. Our work differentiates itself through: (I) Comprehensive perspective: We systematically investigate learning paradigms, separation scenarios with known/unknown speakers, comparative analysis of supervised/self-supervised/unsupervised frameworks, and architectural components from encoders to estimation strategies. (II) Timeliness: Coverage of cutting-edge developments ensures access to current innovations and benchmarks. (III) Unique insights: Beyond summarization, we evaluate technological trajectories, identify emerging patterns, and highlight promising directions including domain-robust frameworks, efficient architectures, multimodal integration, and novel self-supervised paradigms. (IV) Fair evaluation: We provide quantitative evaluations on standard datasets, revealing true capabilities and limitations of different methods. This comprehensive survey serves as an accessible reference for experienced researchers and newcomers navigating speech separation's complex landscape."
        },
        {
            "title": "Start",
            "content": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1 Advances in Speech Separation: Techniques, Challenges, and Future Kai Li1, , , Guo Chen1, , Wendi Sang2, Yi Luo3, Zhuo Chen4, Shuai Wang5, Shulin He6, Zhong-Qiu Wang6, Andong Li7, Zhiyong Wu8, and Xiaolin Hu1, *"
        },
        {
            "title": "Trends",
            "content": "5 2 0 2 4 1 ] . [ 1 0 3 8 0 1 . 8 0 5 2 : r AbstractThe field of speech separation, targeting the challenging cocktail party problem, has witnessed revolutionary advances along with the development of deep neural networks (DNNs). Speech separation can be employed in standalone applications, enhancing speech clarity in complex acoustic enit can function as crucial previronments. Additionally, processing method for other speech processing tasks, such as speech recognition and speaker recognition. Despite numerous publications addressing this challenge, current literature surveys and reviews tend to focus narrowly on specific architectural designs or isolated learning approaches, creating fragmented understanding of this rapidly evolving domain. This fragmentation underscores the urgent need for unified, comprehensive survey that captures the fields breadth and recent innovations. This survey addresses this critical gap by providing systematic and holistic examination of DNN-based speech separation techniques. Our work differentiates itself from existing surveys and reviews: (I) Comprehensive perspective: Instead of only cataloging architectural variations, we systematically investigated higher-level learning paradigms, comprehensively covering separation scenarios with known or unknown number of speakers, comparative analysis of supervised, self-supervised, and unsupervised learning frameworks, and detailed examination of various architectural components from front-end encoders to back-end estimation strategies. (II) Rigorous timeliness: By covering the cutting-edge developments from the most recent years, we provide up-to-date coverage to ensure researchers have access to the most current methodological innovations and performance benchmarks. (III) Unique insights: Beyond mere summarization, we critically evaluate technological trajectories, identify emerging research patterns, and highlight promising directions including domain-robust frameworks, computationally efficient architectures, multimodal integration approaches, and novel self-supervised paradigms. (IV) Fair quantitative evaluation: We provide fair quantitative evaluations on standard datasets, aiming to provide clear and reliable performance benchmarks for Manuscript received April 19, 2021; revised August 16, 2021. This work was supported by the National Key Research and Development Program of China under Grant 2021ZD0200301 and the National Natural Science Foundation of China under Grant U2341228. 1Kai Li, Guo Chen, and Xiaolin Hu are with the Department of Computer Science and Technology, Tsinghua University, Beijing, China. 2Wendi Sang is with the School of Computer Technology and Application, Qinghai University, Xining, China. independent 3Yi Luo an y.luo@columbia.edu). is author, Shenzhen, China (e-mail: 4Zhuo Chen is with ByteDance. 5Shuai Wang is with Nanjing University, Suzhou, China. 6Shulin He and Zhong-Qiu Wang are with the Southern University of Science and Technology, Shenzhen, China. 7Andong Li is with the Institute of Acoustics, Chinese Academy of Sciences, Beijing, China. 8Zhiyong Wu is with the Shenzhen International Graduate School, Tsinghua University, Shenzhen, China. Project leader. Equal contribution. (Kai Li and Guo Chen contributed equally to this work.) *Corresponding author: Xiaolin Hu (e-mail: xlhu@tsinghua.edu.cn). the research community, thereby revealing the true capabilities and limitations of different methods. This comprehensive survey aims to serve as an accessible reference for both experienced researchers and newcomers navigating the complex landscape of speech separation. Index Termscocktail party problem, speech separation, deep neural networks, large-scale datasets, toolkit. I. INTRODUCTION the human auditory system, the cocktail party effect exemplifies our remarkable ability to selectively attend to target speaker while suppressing interfering sources in noisy environments [1][3]. However, endowing machines with comparable level of separation capability remains one of the most challenging topics in the field of speech processing. Over the past decades, conventional approaches have primarily relied on statistical models and signal processing techniques, such as Independent Component Analysis (ICA) [4], Non-negative Matrix Factorization (NMF) [5], and heuristic spatial filtering methods [6], [7]. Although these methods perform well in certain scenarios, their separation performance and generalization ability are severely limited in real-world settings characterized by high nonlinearity, complex environments, and dynamically varying numbers of speech sources [8]. In recent years, data-driven paradigmspearheaded by deep neural networks (DNNs)has triggered significant shift, greatly advancing the state-of-the-art in speech separation systems. host of architectures and methodologies have been proposed, encompassing Convolutional Neural Networks (CNNs) [9][11], Recurrent Neural Networks (RNNs) [12] [15], and more recently, variants based on self-attention mechanisms such as Transformers [16][18], as well as generative models like Generative Adversarial Networks (GANs) [19], [20] and diffusion models [21][23]. These modern DNNbased approaches automatically learn deep hierarchical features and, through direct exposure to large-scale mixtures, substantially boost the performance of speech separation tasks. As visually illustrated in Figure 1, performance curves on the canonical WSJ0-2mix dataset highlight the remarkable progress driven by the evolution of architectures and algorithms [24]. Nevertheless, alongside rapid technological advances, application demands for speech separation have become increasingly diverse and complex. For example, meeting transcription and real-time communication require extremely low latency and online output; assistive hearing devices and mobile applications demand lightweight, efficient models with constrained resources. Meanwhile, real-world scenarios present persistent JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 Fig. 1. Speech Separation Model Performance on WSJ0-2mix over Time. The size of the points represents the number of parameters of the model. TABLE COMPARATIVE ANALYSIS OF RECENT SURVEYS AND REVIEWS ON DEEP LEARNING-BASED SPEECH SEPARATION. : COMPREHENSIVE COVERAGE, : PARTIAL COVERAGE, : LIMITED OR NO COVERAGE. Surveys and reviews (Year) Deep Learning Methods Architecture Topics Evaluation Datasets Platforms Results [29] (2018) [30] (2018) [31] (2022) [32] (2023) [33] (2023) [34] (2023) Ours (2025) challenges such as an unknown and dynamically varying number of speakers, frequent speaker switching, as well as non-stationary noise and reverberationall of which remain significant bottlenecks impeding the large-scale deployment of speech separation algorithms [25][28]. Given the rapid evolution of this field, comprehensive and systematic survey of deep learning-based speech separation is not just necessary but urgently needed. This paper aims to provide an authoritative guide for researchers and practitioners, clarifying the current research landscape and critically assessing the strengths, limitations, and future trajectories of cutting-edge technologies in addressing real-world challenges. Specifically, early seminal surveys and reviews like Qian et al. [29] and Wang and Chen [30] established solid foundation by explaining the cocktail party problem and initial deep learning approaches [12], [24]. However, their historical context is also their primary limitation. They entirely miss the disruptive innovations of the last five years, such as advanced end-to-end models, unsupervised paradigms, and pre-trained models. Furthermore, they severely lack comprehensive discussions on practical aspects like evaluation metrics (e.g., subjective metrics beyond SDR [35] and PESQ [36]), datasets, and comparative performance analysis, rendering their utility limited for contemporary research. While existing surveys and reviews [29][34] offer valuable summaries, they struggle to keep pace with the relentless innovation in this domain. As illustrated in Table I, the literature still lacks single, up-to-date survey that systematically integrates the latest complex model architectures, emerging modeling paradigms (e.g., unsupervised and self-supervised learning), standardized evaluation frameworks, and critical dataset platforms. More critically, even the most recent surveys and reviews and reviews by Agrawal et al. [32] and Ansari et al. [34] suffer from fundamental flaw. While broader in scope, their approach to results analysis is often simple aggregation of performance metrics reported in original papers. Due to disparate experimental setups, training data, and evaluation scripts, these metrics, gathered from non-standardized environments, are not directly comparable and can even be misleading. This JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 3 creates significant confusion for researchers seeking objective assessments and technology selection. Therefore, establishing fair comparison framework under unified standard is an urgent, unmet need in this field. Finally, the existing literature has also largely overlooked in-depth discussions on several topics crucial for real-world applications: the trade-off between model compactness and real-time deployment, the inherent challenges of different learning paradigms, and critical analysis of scenario coverage and biases in existing datasets. To address all the aforementioned challenges and fill these gaps, this work presents comprehensive and systematic survey of deep neural networkbased speech separation technologies. We provide an in-depth analysis that traces the evolution of the field from fundamental theories to cutting-edge architectures, with particular emphasis on the core designs and technical strengths of mainstream models such as dual-path networks [13], U-Net variants [10], [37], and Transformer-based architectures [16], [18]. Distinct from prior surveys and reviews, our primary contribution lies in the implementation of rigorous and reproducible benchmarking of representative models within unified experimental framework. By conducting fair quantitative evaluations on both standard datasets (e.g., LibriMix [38], WHAM! [39], REAL-M [40]) and newly introduced challenging datasets (e.g., LRS2-2Mix [41], SonicSet [42]), we aim to provide the research community with clear and reliable performance benchmark1, thereby revealing the true capabilities and limitations of different methodological approaches. In addition, this survey systematically summarizes the functionalities of opensource toolkits such as Asteroid [43], SpeechBrain [44], and WeSep [45], and discusses key challenges and future directions. We anticipate that this work will offer comprehensive knowledge map serving as both an introductory guide for new researchers and decision-making reference for experienced engineers in applications such as meeting assistants, wearable hearing devices, and augmented reality. II. PROBLEM FORMULATION A. The Cocktail Party Problem The cocktail party effect refers to the human ability to selectively focus on single auditory source, such as particular voice or sound, amidst competing background noise [1], [46]. This capacity is critical for navigating real-world auditory environments, where sound sources often overlap in both time and space. The phenomenon is underpinned by the brains ability to suppress irrelevant auditory inputs while isolating and prioritizing meaningful acoustic signalsa fundamental process in auditory scene analysis [2], [47]. At its core, the cocktail party effect exemplifies the interaction between bottom-up sensory processing and top-down attentional modulation, mediated by the dynamic interplay within hierarchical brain networks [3], [48][51]. The brain integrates temporally synchronized auditory features to form coherent perceptual objects [52]. Neuronal populations encoding sound source attributeslike pitch, location, 1https://cslikai.cn/Speech-Separation-Paper-Tutorial/ and harmonicsshow phase-locked oscillations, binding these features into unified stream while segregating asynchronous distractors [53]. This involves feedforward encoding of features in primary auditory cortex (A1) [54] and feedback from higher regions (e.g., prefrontal and parietal cortices) that modulate integration based on attention [55]. For example, focusing on speaker enhances neural gain for their voice traits [56], amplifying its representation amid noise and enabling target speech tracking. Top-down attentional control enables selective auditory processing [57], [58]. The prefrontal cortex (PFC) and anterior cingulate cortex (ACC) produce modulatory signals that bias competition among auditory streams, favoring behaviorally relevant ones [59]. This bias manifests as enhanced phase alignment of theta (48 Hz) and gamma (3080 Hz) oscillations in auditory cortex, synchronizing neural activity with attended speech rhythms [60]. For instance, speechs syllabic rhythm (38 Hz) entrains theta oscillations in the superior temporal gyrus (STG), creating temporal template that tracks the target speakers acoustic envelope [61]. Concurrently, gamma oscillations bind fine spectral-temporal features (e.g., formants, phonemes) into unified percept [61], [62]. Notably, attention amplifies target voice representations while suppressing unattended stimuli, as seen in attenuated N1 event-related potentials for ignored streams, reducing neural responsiveness below baseline. Hierarchical processing in the auditory system refines selective attention [63][65]. Early stages in the brainstem and midbrain [66][69], especially the superior olivary complex, use binaural cuessuch as interaural time differences (ITDs) and interaural level differences (ILDs)to spatially segregate sound sources. These pre-attentive processes boost the signalto-noise ratio (SNR) of targets via spatial unmasking. In the auditory cortex, processing advances from spectrotemporal analysis in A1 to advanced feature extraction in non-primary areas like the planum temporale (PT) [70]. The PT integrates spatial and spectral data to separate overlapping streams and shows enhanced sensitivity to auditory edgessudden acoustic energy shifts signaling new sourcesenabling the parsing of complex scenes into distinct perceptual streams. The cocktail party effect arises from cascade of neural operations: temporal coherence binds acoustic features into perceptual streams, attentional gain modulates their cortical representation, hierarchical auditory processing separates spatial and spectrotemporal cues, and large-scale networks integrate these processes into coherent perceptual experience. These mechanisms collectively enable the brain to resolve the ambiguity of complex auditory scenes, prioritizing meaningful signals while filtering out noisean ability that remains unmatched by even the most advanced computational models. B. Speech Separation As stated in the previous subsection, humans possess natural ability to distinguish different sound sources in noisy environments, phenomenon known as the cocktail party effect [1]. In speech signal processing, this corresponds to the task of speech separation [30]. The objective of this task is JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4 Fig. 2. Overview of speech separation with known/unknown source counts. to separate individual speech source signals {si R1T [1, C]} from mixed audio input R1T , where denotes the length of the audio signal, and represents the number of speakers in the mixture. The mixed signal typically contains multiple sound sources that overlap substantially in both time and frequency domains, thereby significantly increasing the complexity of the separation process. Formally, the task can be represented as: = (cid:88) i=1 si + (1) where R1T denotes background noise. the signal The input of the speech separation task can be represented in the time domain or frequency domain. In the timedomain representation, the raw audio waveform is processed directly, usually using one-dimensional convolutional layers to extract high-dimensional embedding features [71]. In the frequency-domain representation, is transformed into time-frequency representation via short-time Fourier transform (STFT) [24]. Regardless of the input representation used, current mainstream speech separation methods mostly adopt modular processing pipeline, typically consisting of an encoder, separator, audio estimation, and decoder. The encoder first maps the input signal to high-dimensional feature space; the separator decouples and distinguishes information from different sound sources in this space; then, through audio estimation (e.g., generating masks or directly estimating source signal features), representations of each source are obtained; finally, the decoder is responsible for reconstructing these representations into independent time-domain audio signals. This architecture provides foundation for tackling subsequent challenges. For example, the input of speech separation methods can be single-channel or multi-channel audio data. In the single-channel scenario, the separation task is particularly challenging due to the lack of spatial information that helps differentiate between different sound sources. In contrast, multi-channel settings utilize microphone arrays to exploit spatial features, thereby improving separation performance. However, this setting also brings challenges related to hardware deployment and increased algorithmic complexity. Additionally, speech separation systems need to generate independent audio signals for each source and be capable of handling both known and unknown numbers of sources. This requirement is especially important in applications such as real-time voice communication [53], conference systems [72], and human-computer interaction [73], where systems must demonstrate adaptability in dynamic environments. Therefore, speech separation methods must strike balance between signal quality, computational efficiency, and model generalization ability to meet the demands of diverse application scenarios. 1) Speech Separation with Known Number of Sound Sources: In the speech separation with the known number of sound sources, the objective is to separate independent speech signals from mixed audio input, as shown in Figure 2, where represents the known number of speakers. Specifically, this task involves learning mapping fθ(x) {ˆs1, ˆs2, . . . , ˆsC} via separation network fθ(), such that the separated signals ˆsi closely approximate the corresponding ground-truth source signals si. In deep learning-based approaches, criterion-driven loss functions are often employed to enhance separation performance [35]. For instance, the ScaleInvariant Signal-to-Distortion Ratio (SI-SDR) loss function [35] optimizes at the signal level to improve the perceptual quality of the separated signals. The design of separation networks typically utilizes deep neural network architectures (see Section IV), such as temporal convolutional networks (TCNs) [74], recurrent neural networks (RNNs) [75] and Transformerbased models [76]. These architectures effectively capture long-term dependencies by modeling contextual information. In this task, the output layer of the network is constrained to produce fixed number of output channels C. To address the speaker permutation ambiguity problem, strategies such as deep clustering [24] or Permutation-Invariant Training (PIT) [12] (detailed in Section III) are commonly employed. These strategies enable accurate modeling and, when combined with advanced neural network architectures and optimization techniques, significantly improve the separation performance. 2) Speech Separation with Unknown Number of Sound Sources: The task of single-channel speech separation with an unknown number of speakers aims to recover independent speaker signals {si}C i=1 from mixture x, as shown in Figure 2. Unlike conventional separation tasks with fixed source counts, the core challenge lies in the joint optimization of dynamic output dimensions and separation termination conditions. In practical scenarios (e.g., meeting transcription, multiparty dialogue), the unknown and time-varying nature of renders traditional fixed-output-channel methods ineffective. Specific challenges include [25]: 1) Exponential expansion of permutation ambiguity: When handling variable outputs, conventional PIT must extend to asymmetric output configurations. 2) Trade-off between separation quality and termination robustness: The separation process requires the adaptive generJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5 TABLE II SUMMARY OF SPEECH SEPARATION METHODS BASED ON DIFFERENT LEARNING APPROACHES. METHODS ARE SORTED IN CHRONOLOGICAL ORDER. FOR DETAILED CONTENT, REFER TO SECTION III. Learning Approaches Methods Unsupervised Learning MixIT [79], VAE [80], TS-MixIT [81], UNSSOR [82] Supervised Learning Clustering DPCL [24], DANet [8], ADAN [83], Chimera++ Network [84], Chimera++ sign [85], WaveSplit [86] PIT PIT [12], uPIT-BLSTM [87], TaSNet [71], Wave-UNet [88], SSGAN [89], SSGAN-PIT [90], CBLDNN-GAT [91], Conv-TasNet [9], Deep CASA [92], OR-PIT [25], FurcaNeXt [93], VSUNOS [26], DPRNN [13], DPTNet [14], Conv-TasSAN [94], Two-Step TCN [95], SudoRM-RF [10], Multi-Decoder Dprnn [27], DPTCN-ATPP [96], MSGT-TasNet [97], TS-MixIT [81], SepFormer [16], A-FRCNN [11], Sandglasset [98], CDGAN [20], UnknowSS [77], QDPN [99], SFSRNet [100], MTDS [101], TDANet [102], SkiM [103], TFPSNet [104], SSL-SS [105], SepEDA [28], MossFormer [17], SepDiff [22], DiffSep [106], Separate And Diffuse [23], PGSS [107], pSkiM [108], Diff-Refiner [109], CycleGAN-SS [110], HuBERT [111], S4M [112], TF-GridNet [15], MossFormer2 [18], TCodecSS [113], CodecSS [114], DIP [115], Fast-GeCo [116], SepTDA [78], Conv-TasNet GAN [117], ReSepFormer [118], EDSep [119], TIGER [37], SPMamba [120] ation of C-matched channels while avoiding under-separation (incomplete source extraction) or over-separation (redundant noise channels). Existing approaches primarily address these challenges through recursive separation frameworks [25], [26] and dynamic network architectures [77]. The former iteratively extracts individual speakers while updating residual signals (e.g., OR-PIT [25]), formalized as: rk = fθ(rk1), r0 = (2) where fθ denotes the single-step separation network and rk represents the residual signal at the k-th iteration. This approach dynamically terminates separation through threshold detection (e.g., energy ratio or confidence score), demonstrating generalization capability to unseen values (e.g., = 4) during testing. The latter category employs dynamic network architectures for flexible output generation. Representative methods include multi-decoder mixture-of-experts systems, where shared encoder extracts mixed features before multiple expert decoders generate different output counts, with speaker quantity classifier selecting optimal results [27]. Another line of work adapts speaker diarization concepts through dynamic attractor computation, where LSTMs generate candidate attractors followed by similarity thresholding for effective source count estimation [28], [78]. Current methods still face challenges in computational efficiency (e.g., error accumulation in recursive separation) and noise robustness (e.g., attractor selection sensitivity to reverberation). III. LEARNING PARADIGMS A. Unsupervised Learning and its performance relies on strong statistical assumptions that may not hold in real acoustic environments. Subsequent extensions such as Independent Vector Analysis (IVA) model dependencies among source vectors to enhance the processing of multi-channel signals, yet these methods still inherit dependence on statistical priors and require multi-microphone input [121]. In parallel, matrix factorization-based methods, such as Non-negative Matrix Factorization (NMF), have emerged as alternatives [5]. NMF operates by decomposing the magnitude spectrum of the mixed signal into set of basis vectors (i.e., spectral templates) and the corresponding activations. core limitation of NMF is its restricted model expressiveness; the learned bases are typically static and struggle to capture the complex temporal variations present in speech, which impairs separation quality. With the advent of deep learning, unsupervised methods have been revitalized via more powerful generative models. For instance, frameworks based on Variational Autoencoders (VAEs) achieve separation by learning probabilistic priors over speech signals within latent space [80]. Recently, the mixture of mixtures (MixIT) paradigm has made notable advances by training on mixtures of mixtures without access to source references [79]. However, MixIT suffers from overseparation issues and requires single-speaker data for convergence. To address this, semi-supervised extensions such as the teacher-student paradigm incorporate limited supervision [81], albeit still relying on portion of annotated data. UNSSOR exploits multi-channel signals in over-determined settings to achieve unsupervised separation [82], optimizing filters through convolutional prediction, but generalizing to underdetermined conditions remains challenging. Speech separation aims to extract individual source signals from mixed recording and constitutes long-standing challenge in the field of audio signal processing. Early explorations in this domain primarily pursued unsupervised solutions, seeking to uncover latent sources in the absence of accessible ground-truth signals. As pioneering approach, Independent Component Analysis (ICA) leverages the statistical independence among source signals to perform blind separation [4]. Despite its foundational role, ICA demonstrates limited effectiveness in highly underdetermined, single-channel scenarios, Although unsupervised approachesby leveraging statistical priors or consistency objectivesoffer elegant and dataefficient solutions, they are fundamentally constrained by the strength of their underlying assumptions. Consequently, their performance in complex acoustic scenarios often fails to meet the fidelity requirements of high-quality separation. These inherent limitations have driven the research community towards significant paradigm shift to supervised learning, wherein models are directly trained on large-scale datasets of paired mixtures and their corresponding source signals. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6 frequency (T-F) unit (indexed by = (t, )) in the mixed speech spectrogram (typically obtained through Short-Time Fourier Transform, STFT) to high-dimensional embedding space, generating embedding vectors vi RD [24]. The core idea is to ensure that embedding vectors from the same source (e.g., the same speaker) are positioned close to each other in the embedding space, while those from different sources remain distant from each other, as shown in Fig. 4. To achieve this, Deep Clustering typically optimizes loss function based on the difference between an affinity matrix constructed from embedding vectors and an ideal affinity matrix. Specifically, all T-F unit embedding vectors (N = ) are stacked into matrix RN D, and the corresponding ideal source assignments (e.g., binary labels based on which source dominates in that T-F unit) are represented as matrix {0, 1}N (where Yic = 1 indicates that the i-th TF unit belongs to source c). The objective function of Deep Clustering is typically defined as minimizing the Frobenius norm distance between the affinity matrices derived from these two matrices: (3) LDC = VVT YYT 2 , where VVT and YYT represent the affinity (inner product) matrices between embedding vectors and between ideal labels, respectively. Since the matrix product AAT is invariant to the column order of matrix (i.e., the permutation order of source labels), the loss function LDC is permutation-invariant. This allows the network to learn discriminative embedding representations without knowing the specific correspondence between outputs and targets, thus elegantly solving the label permutation problem. The separation process is completed during the inference phase by applying clustering algorithms (such as K-means, specifying the number of clusters as C) to the learned embedding vectors to partition T-F units into different clusters, each corresponding to mask ˆMc for separated source. Finally, the estimated signals are obtained by masking the mixed spectrogram X(t, ) and performing inverse STFT: ˆsc = iSTFT( ˆMc X). Speech separation methods based on deep clustering exhibit diverse forms and applications [8], [83][86], [122], [123]. The most direct form strictly follows its original definition [8], [122]: first, using deep neural network (typically recurrent neural network such as BLSTM) to learn embedding vectors for time-frequency units, with the training objective of optimizing the embedding space structure to satisfy the characteristic that elements from the same source cluster together while those from different sources remain separated; then, during the inference phase, applying standard clustering algorithms (such as K-means or spectral clustering) to the embedding vectors output by the network to obtain estimated masks for each source, thereby reconstructing the speech signals. Beyond this basic application, deep clustering has also inspired many improvements and hybrid methods. For instance, the Chimera network [84] and its improved version Chimera++ [85] combine deep clustering with Mask Inference (MI), setting up two output heads in one network: one head outputs embedding vectors for clustering (optimizing LDC), while the other directly predicts separation masks ˆM Fig. 3. The supervised speech separation workflow. The diagram presents two mainstream approaches: Deep Clustering (DPCL) and Permutation Invariant Training (PIT). The number of speakers is assumed to be 2. Fig. 4. The pipeline of the deep clustering method. B. Supervise Learning Supervised speech separation constitutes dominant approach in the domain of audio signal processing, aiming to extract independent source signals from mixed acoustic inputs through model training, as shown in Fig. 3. These methods rely on parallel dataset composed of mixed signals and their corresponding clean signals, enabling the model to learn mapping function from the mixed signals to the constituent sources. One of the main challenges in this domain is the correct assignment of the components within mixed speech signal to their corresponding source signals. This challenge arises from the inherent ambiguity in the order of speech components within mixed speech, leading to what is known as the label permutation problem [12], [24]. Taking the multispeaker scenario as an example: due to the arbitrary nature of speaker order within mixed speech, there is no predetermined correspondence between the output channels of the separation network and the target speakers."
        },
        {
            "title": "Current approaches to address this challenge primarily fall",
            "content": "into two categories: 1. Deep clustering (DC) [24], which transform the separation problem into high-dimensional embedding space, allowing time-frequency units belonging to the same source to cluster together; 2. Permutation Invariant Training (PIT) [12], which evaluates all possible output-target permutations during training and selects the permutation scheme that minimizes the loss function. These methods tackle the label permutation problem in supervised speech separation from different perspectives. 1) Clustering Methods: Unlike conventional clustering algorithms which do not require teaching signals, these deep clustering methods are supervised learning methods. Deep Clustering [24] circumvents this problem through an innovative approach. Instead of directly predicting separated signals or masks, it trains deep neural network to map each timeJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 7 (optimizing mask-related losses LM , such as mean squared error). The total loss function for this hybrid architecture is typically weighted sum of both: Ltotal = αLDC + (1 α)LMI, (4) where α is weighting factor. This approach allows the deep clustering loss LDC to serve as regularization means, assisting the mask prediction task, and typically achieves better performance than using either method alone. Another important evolution is the Deep Attractor Network (DAN) [8], [83], which borrows the embedding concept from deep clustering but explicitly learns attractor points ac RD representing each source in the embedding space. These attractors are typically defined as the centroids of embedding vectors for TF units corresponding to that source. Then, by calculating the distance (e.g., Euclidean distance) between each T-F units embedding vector vi and various attractors ac, the unit is assigned to the source represented by the nearest attractor, thereby generating separation masks: ˆci = arg minc vi ac2. The training objective of DAN is typically to minimize signal reconstruction error, making it an end-to-end method while utilizing the attractor mechanism to maintain permutation invariance. Additionally, researchers have explored different deep clustering objective functions [85], [86] and strategies that combine deep clustering with other techniques (such as signal enhancement post-processing [122], iterative phase reconstruction [85], etc.), further expanding the separation framework based on deep clustering ideas."
        },
        {
            "title": "Its most",
            "content": "Deep clustering, as pioneering speech separation technique, exhibits evident advantages. fundamental contribution lies in providing, for the first time, an effective framework for addressing the label permutation problem in deep learning-based single-channel, speaker-independent speech separation, significantly advancing developments in this field. Additionally, deep clustering demonstrates potential for handling variable number of sources [24], [122], as the embedding and clustering framework is not inherently limited to fixed number of output channels. The number of separated sources can be determined during testing by specifying the number of clusters, and models trained on mixtures with fewer speakers can potentially be applied to separation tasks involving more speakers. However, deep clustering also presents some inherent limitations. Its performance is constrained by the quality of the embedding vectors and the effectiveness of the subsequent clustering algorithm, constituting two-stage process that may not be end-to-end optimal. The original loss function based on affinity matrices does not directly optimize final speech separation evaluation metrics (such as SDR) [124], although subsequent research has attempted improvements by introducing end-to-end objectives. Furthermore, the clustering step during inference (especially spectral clustering) may incur high computational costs, and if clustering is performed on short segments, additional steps are needed to resolve cluster label consistency issues across segments. Despite the subsequent emergence of end-to-end timedomain models (such as TasNet [71]) and novel architectures incorporating clustering concepts (such as WaveSplit [86]) that Fig. 5. The pipeline of the permutation invariant training method. have achieved superior performance on specific benchmarks, the pioneering idea of deep clusteringutilizing embedding spaces to address permutation invariancecontinues to exert profound influence. Future separation models are likely to continue borrowing and developing this embedding learning paradigm, exploring more effective embedding representations, more optimized clustering or assignment mechanisms, and integrating these ideas into more powerful end-to-end trainable frameworks to address more complex and realistic speech separation scenarios, such as those involving reverberation, strong noise, and numerous overlapping speakers. 2) Permutation Invariant Training Methods: Unlike deep clustering, Permutation Invariant Training (PIT) resolves this issue through an ingenious strategy: instead of presetting fixed output order, it considers all C! possible permutations between outputs and targets at each training step [12], [87], as shown in Fig. 5. Specifically, for permutation σ SC (where SC is the set of all permutations of {1, . . . , C}), it calculates the total loss under that permutation. The objective function of PIT is to select the permutation that minimizes the total loss and update the network parameters θ based on this minimum loss. The loss function is formally expressed as: LPIT = min σSC (cid:88) i=1 L(si, ˆsσ(i)), (5) where L(, ) is loss function measuring the difference between an individual source signal and its estimated signal, such as negative Signal-to-Noise Ratio (SNR) [125] or the commonly used Scale-Invariant Signal-to-Noise Ratio (SISNR) [35], and σ(i) represents the output index corresponding to the i-th target source si under permutation σ. This strategy directly optimizes separation as the objective, avoiding the label permutation problem and significantly improving model performance on speaker-independent separation tasks [12]. The original PIT [12] computes optimal permutations at the frame or segment level, which can lead to inconsistent speakerto-channel assignments across long utterance, disrupting temporal continuity. To mitigate this, several extensions have been proposed, evolving PIT to handle longer contexts, variable speaker counts, and complex scenarios like meetings. Utterance-level PIT (uPIT) [87] extends permutation selection to the entire utterance, identifying single optimal mapping that ensures consistent speaker assignments throughout the signal. This promotes learning of long-term dependencies and speaker traits. Key advantages include improved continuity and performance in fixed-speaker scenarios; however, it assumes the number of speakers does not exceed output channels JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 8 (K C), limiting flexibility in dynamic environments. Building on this, Graph-PIT [126] relaxes the constraint, allowing more speakers overall as long as simultaneously active ones do not exceed C. It models speech segments as graph, with nodes for segments and edges for temporal overlaps, then frames permutation as graph coloring problem to minimize loss while ensuring non-overlapping segments can share channels. This is particularly useful for conferencelike recordings with intermittent speakers. Advantages include scalability to variable and larger speaker sets; drawbacks involve higher computational complexity due to graph optimization, potentially scaling poorly with many nodes. Alternatively, One-and-Rest PIT (OR-PIT) [25] employs recursive approach, iteratively separating one speaker and feeding the residual mixture back into the network until all are extracted. The recursion depth adapts to the (unknown) number of speakers. This offers flexibility for varying without fixed assumptions. Strengths lie in handling unknown speaker counts and recursive refinement; limitations include potential error accumulation across iterations and increased inference time. These extensions highlight PITs evolution from local to global and adaptive handling, addressing real-world challenges like speaker variability, but often at the cost of added complexity or assumptions. PIT successfully addresses the fundamental challenge of label permutation, enabling end-to-end deep learning models to effectively tackle speaker-independent speech separation tasks and achieve breakthrough performance improvements [15], [17], [18]. Models trained with PIT and its variants (such as uPIT) demonstrate good generalization capabilities, able to separate voices of speakers not present in the training set, and even generalize across languages, indicating that the models learn relatively universal acoustic separation cues. As training strategy, PIT can be flexibly combined with various complex neural network architectures (such as RNN [12], LSTM [13], [14], [71], [87], CNN [9][11], [37], [102], and Transformers [15][18]). For uPIT, the additional permutation computation overhead during training is typically acceptable relative to complex deep models, while there is no extra computational burden during the inference phase. However, PIT methods also have some disadvantages. Basic frame-level PIT may lead to inconsistent correspondence between output channels and speakers over time; although uPIT alleviates this issue, enforcing globally consistent permutations for very long audio might be overly restrictive. The main limitation lies in computational complexity: PIT and uPIT need to evaluate C! permutations, which becomes impractical when the number of speakers increases (e.g., greater than 3 or 4) as the computational cost grows dramatically [26], [27]. While Graph-PIT handles more potential speakers through graph modeling, it introduces the complexity of graph construction and coloring algorithms. OR-PIT, though avoiding the C! computation, may employ suboptimal recursive/iterative separation approach and potentially suffer from error accumulation problems. Future research should focus on addressing key limitations of PIT-based methods. This includes developing efficient permutation handling for long audio and dynamic speaker scenarios [127] (e.g., via online processing or segmented Fig. 6. Overall pipeline of speech separation. PIT), and reducing computational complexity for more than three speakers through approximation algorithms or hybrid approaches like clustering guidance [86]. Additionally, improving robustness in noisy, reverberant environments and enabling end-to-end optimization with downstream tasks such as ASR [128] and speaker diarization [129] will enhance practical applicability. IV. ARCHITECTURES In the field of speech separation, the encoder-separatoraudio estimation-decoder architecture is the unique framework, as illustrated in Fig. 6. The encoder is responsible for converting the raw audio signal into more informative representation, the separator processes these representations to isolate different sound sources, and the decoder converts the separated representations back into time-domain signals. The advantage of this architecture lies in its ability to decompose the complex speech separation task into three relatively independent modulesfeature extraction, feature separation, and signal reconstructionwhich facilitates model design and optimization. This section presents comprehensive chronological analysis of speech separation models as categorized in Table III, offering insights into the evolution of various encoder-separator-audio estimation-decoder architectures across different methodological paradigms. In this section, unless otherwise specified, we restrict our discussion to scenarios where the number of speakers is known. A. Encoder & Decoder Currently, the mainstream encoders and decoders can be broadly categorized into the following types, as shown in Fig. 7. The first category relies on the short-time Fourier transform (STFT) and its inverse (iSTFT). In this approach, the STFT converts time-domain signal into time-frequency domain representation, leveraging two-dimensional depiction of time and frequency to differentiate the characteristics of various sound sources. This method has been widely used in early studies as well as in some current research [8], [15], [24], [37], [83], [120]. Its advantages lie in its clear physical interpretation, ease of understanding and implementation, and compatibility with many traditional signal processing techniques. However, as STFT/iSTFT are fixed, non-learnable transforms, their time-frequency resolution is inherently limited by the uncertainty principle related to window function and stride selection. Consequently, they may not provide the optimal feature representation for separation tasks, and larger window lengthoften required in low-latency scenarioscan introduce significant delay [103]. Moreover, when reconstructing the time-domain signal using iSTFT, typically only the JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 9 TABLE III OVERVIEW OF SPEECH SEPARATION MODELS CATEGORIZED BY ENCODER/DECODER ARCHITECTURE (STFT/CONVOLUTION/PRE-TRAINED), SEPARATOR (RNN/CNN/ATTENTION/MIXTURE-BASED), AND AUDIO ESTIMATED METHOD (MASK-BASED VS. MAPPING-BASED). Methods DPCL [24] PIT [12] uPIT-BLSTM [87] DANet [8] ADAN [83] Chimera++ [84] TaSNet [71] SSGAN-PIT [90] CBLDNN-GAT [91] Wave-UNet [88] Chimera++ sign [85] Conv-TasNet [9] Deep CASA [92] OR-PIT [25] WaveSplit [86] DPTNet [14] Conv-TasSAN [94] DPRNN [13] VSUNOS [26] Two-Step TCN [95] SudoRM-RF [10] MixIT [79] FurcaNeXt [93] TS-MixIT [81] SepFormer [16] MSGT-TasNet [97] Multi-Decoder Dprnn [27] DPTCN-ATPP [96] Unknow-SS [77] A-FRCNN [11] Sandglasset [98] CDGAN [20] SSL-SS [105] SkiM [103] TDANet [102] MTDS [101] QDPN [99] SFSRNet [100] TFPSNet [104] SepEDA [28] SepDiff [22] S4M [112] HuBERT [111] pSkiM [108] PGSS [107] Separate And Diffuse [23] DiffSep [106] TF-GridNet [15] UNSSOR [82] MossFormer [17] ReSepFormer [118] Conv-TasNet GAN [117] SepTDA [78] SPMamba [120] Fast-GeCo [116] DIP [115] TIGER [37] CodecSS [114] TCodecSS [113] MossFormer2 [18] EDSep [119] Encoder/Decoder STFT STFT STFT STFT STFT STFT Convolution STFT STFT Convolution STFT Convolution STFT Convolution Convolution Convolution Convolution Convolution Convolution Convolution Convolution Convolution Convolution Convolution Convolution Convolution Convolution Convolution Convolution Convolution Convolution STFT STFT Convolution Convolution Convolution Convolution Convolution STFT Convolution STFT Convolution Pre-trained Model Convolution STFT STFT STFT STFT STFT Convolution Convolution Convolution Convolution STFT STFT Pre-trained Model STFT Pre-trained Model Pre-trained Model Convolution STFT Separator RNN-Based RNN-Based RNN-Based RNN-Based RNN-Based RNN-Based RNN-Based RNN-Based RNN-Based CNN-Based Mixture-Based CNN-Based CNN-Based RNN-Based RNN-Based Mixture-Based CNN-Based RNN-Based RNN-Based CNN-Based CNN-Based CNN-Based CNN-Based CNN-Based Attention-Based Attention-Based RNN-Based CNN-Based CNN-Based Mixture-Based Mixture-Based RNN-Based RNN-Based RNN-Based Mixture-Based RNN-Based Mixture-Based Attention-Based Attention-Based Mixture-Based CNN-Based Mixture-Based Attention-Based RNN-Based Mixture-Based Attention-Based CNN-Based Mixture-Based Attention-Based Attention-Based Attention-Based CNN-Based Mixture-Based Mixture-Based CNN-Based Mixture-Based Mixture-Based Attention-Based Attention-Based Attention-Based CNN-Based Audio Estimation mask mask mask mask mask mask mask mask mask mapping mask mask mask mask mapping mask mask mask mapping mask mask mapping mapping mapping mask mask mapping mask mapping mask mask mapping mask mapping mask mask mapping mask mask mask mapping mask mapping mapping mapping mapping mapping mapping mapping mask mask mask mapping mapping mapping mask mask mapping mapping mask mapping magnitude spectrum is employed [8], [24], [83], while the phase information is often estimated via the Griffin-Lim algorithm [130] or directly taken from the mixed signal, potentially leading to degradation in the quality of the reconstructed signal. Recently, certain methods have converted the complexdomain features obtained from STFT into the real domain so that the separation network can jointly estimate the real and imaginary components of the target signal, thereby enabling joint estimation of magnitude and phase information [15], [37], [131]. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 10 Fig. 7. The process of feature extraction by the encoder and waveform reconstruction by the decoder in speech separation. The second category is based on learnable convolutional neural networks (CNNs). For example, one-dimensional convolutions (1D Conv) can be applied directly in the time domain to encode the waveform, using multiple convolutional layers and downsampling operations to extract features. Corresponding decoders then employ one-dimensional transposed convolutions (1D Transposed Conv) or upsampling operations to reconstruct the waveform [9], [71]. This method integrates the processes of feature extraction and reconstruction into an end-to-end learning framework, allowing the network to automatically learn an optimal basis of feature representations according to the requirements of the separation task [10], [11], [13], [14], [16], [102]. Its advantages include high flexibility and the ability to directly optimize time-domain separation metrics (such as SI-SNR [35]), thereby circumventing the phase estimation issues and resolution limitations of the STFT. The learnable encoder and decoder used in Conv-TasNet [9] are typical examples within this direction. Nevertheless, the learned features often lack an explicit physical interpretation, leading to relatively low model interpretability; moreover, larger corpus of training data may be required to learn effective feature transformations [132]. Future research might achieve breakthroughs either by designing more efficient convolutional structures to capture long-term dependencies or by incorporating physical priors into the feature learning process. The third category involves the use of pre-trained models or neural network-based codecs as encoders. In recent years, with the advancement of self-supervised learning, models such as Wav2Vec 2.0 [133] and HuBERTpre-trained [134] on large-scale unlabeled speech datahave demonstrated the ability to extract rich acoustic and speech information. By employing the encoder component of these pre-trained models or their extracted embeddings as the encoder in speech separation system [80], [115], one can leverage their powerful representational capabilities to potentially enhance generalization performance in complex acoustic scenarios. Similarly, neural network-based codecs (e.g., EnCodec [135], SoundStream [136]) can compress speech into low-bitrate discrete or continuous representations, while still enabling highquality reconstruction. Utilizing such codec-based encoders for feature extraction, along with appropriate decoders for reconstruction, has emerged as new research direction [113], [114]. The advantages of this approach include leveraging the robust feature extraction capability afforded by largescale pre-training, which may provide enhanced robustness to noise and reverberation. On the other hand, pre-trained models often have large number of parameters and high computational complexity, rendering them unsuitable for devices with limited resources [137]. Moreover, discrepancies between the objectives of pre-training tasks (such as mask prediction or contrastive learning) and those of the speech separation task may necessitate careful interface design or fine-tuning [116]. Additionally, the use of neural networkbased codecs may introduce compression artifacts. Future work is expected to focus on efficiently transferring pre-trained knowledge to the domain of speech separation, designing lightweight pre-trained models or codecs, and developing pretraining strategies specifically tailored for separation tasks. B. Separator Architectures The separator constitutes the core component of speech separation models, with its architectural design directly determining the models capacity to extract and model target speech characteristics from mixed signals, thereby affecting overall separation performance. Throughout the evolution of speech separation techniques (as shown in Figure 8), separator architectures have progressed from single-structure designs to hybrid integrations, giving rise to several mainstream paradigms. These can be categorized into RNN-based, CNNbased, Attention-based, and mixed architecture (Mixturebased) approaches. In this section, we provide detailed description of these four principal separator architectures, discussing their core concepts, representative models, current challenges, and future directions. 1) RNN-Based Methods: Recurrent Neural Networks (RNNs) have occupied an important position in the field of speech separation due to their inherent sequence modeling capabilities. Speech signals are essentially time-series data, where temporal dependencies are crucial for understanding and separating mixed sound sources. RNNs, especially Long Short-Term Memory networks (LSTMs) and Gated Recurrent Units (GRUs), can effectively capture and model these longdistance temporal dependencies, making them ideal choices for handling speech separation tasks [12], [24]. With the development of deep learning techniques, the application of JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 11 Fig. 8. Overview of the evolution of separator architectures in speech separation. The figure illustrates the development from single-structure models to hybrid architectures, highlighting the emergence of representative paradigms including RNN-based, CNN-based, Attention-based, and Mixture-based approaches. Note: As many publications involve authors from multiple institutions, the affiliation for each method is determined by the corresponding authors institution, or the first authors institution if corresponding author is not specified. RNNs in speech separation has gradually expanded from initial frequency-domain feature processing to time-domain direct modeling, further improving separation performance and processing efficiency [14], [15], [71], [103], [120]. RNN-based speech separation methods still face several inherent limitations that hinder their performance and applicability in real-world scenarios. One major issue is the vanishing or exploding gradient problem during training, particularly in standard RNNs, which makes it difficult to capture very longrange dependencies in speech signals [16]. This is exacerbated in speech separation tasks involving extended audio sequences, where temporal context spans minutes or more, leading to Fig. 9. Overall pipeline of DPRNN. degraded separation quality for distant sound sources or overlapping speakers [138], [139]. Another significant drawback is the high computational burden, as RNNs process sequences JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 sequentially, resulting in linear time complexity with respect to sequence length [140], [141]. This makes them inefficient for long inputs, often requiring substantial memory and processing power, which is problematic for resource-constrained devices like mobile phones or edge computing systems. Additionally, RNNs struggle with real-time requirements in practical applications, such as live conversations or streaming audio, due to their non-parallelizable nature and high latency in causal (online) processing modes. These limitations not only reduce separation accuracy in complex, noisy, or multi-speaker environments but also limit scalability, making RNNs less competitive against more parallel architectures like Transformers in certain scenarios [13], [16]. Addressing these challenges is crucial for advancing RNNs in speech separation, as they represent key research opportunities for improving efficiency, robustness, and adaptability. To mitigate these drawbacks, researchers have introduced innovative strategies that enhance RNNs without sacrificing their core temporal modeling strengths. For instance, DPRNN divides long sequences into shorter chunks and employs dual-path structure for intra-chunk and inter-chunk processing, effectively alleviating the long-range dependency and computational issues [13]. Similarly, SkiM incorporates skipmemory mechanism to reduce redundant computations by approximately 75%, maintaining separation performance while lowering overhead [103]. Building on this, Predictive SkiM integrates contrastive predictive coding with local context encoding-decoding to boost accuracy and real-time capabilities in causal modeling [108]. These approaches improve RNNs handling of complex scenarios like multi-speaker dialogues, providing foundation for more efficient and robust systems. Building on these improvements, there are still many potential directions for the development of RNNs in speech separation. For example, combining RNNs with other architectures, such as convolutional neural networks [142], attention mechanisms [15], or diffusion models, could create hybrid models that leverage complementary strengths for better local detail capture and global context understanding. Additionally, optimizing techniques like contrastive predictive coding, skip-memory mechanisms, and autoregressive strategies may further reduce latency and computational costs, enabling deployment on mobile or low-power devices. For ultra-long sequences or varying speaker counts, integrating RNNs with advanced frameworks like Transformers [76] or State-Space Models [120], [140] holds promise for groundbreaking improvements. 2) CNN-Based Methods: Convolutional Neural Networks (CNNs) have become one of the primary network architectures used in speech separation models due to their inherent feature extraction capabilities and efficiency in capturing local temporal dependencies in audio signals. The widespread adoption of this technology primarily stems from the rise of end-to-end models that operate directly in the time domain [9], circumventing the complexity of phase reconstruction in traditional time-frequency methods [10], [11], [94], [143]. CNNs provide powerful mechanism for learning complex representations from raw waveforms, enabling models to directly map mixed audio signals to separated sources. Furthermore, advancements in CNN architectures such as depthwise separable convolutions and dilated convolutions have made it possible to construct efficient models that simultaneously capture both short-term and long-term temporal dynamics, perfectly suited for the complex task of audio source separation [9]. CNN-based speech separation techniques present diverse landscape, encompassing various architectural designs that effectively utilize convolutional layers. Among these, encoderdecoder structures are particularly prominent, such as WaveU-Net [88] and SUDO RM-RF [10], inspired by U-Net [144] from the image segmentation domain. These models progressively downsample the input through an encoder to extract hierarchical features, then reconstruct separated signals via decoder with upsampling, typically incorporating skip connections to preserve fine-grained details. Another important category includes models like Conv-TasNet [9] and DPTCNATPP [96], which utilize stacked one-dimensional convolutional blocks to learn feature representations and generate separation masks. These models frequently employ dilated convolutions to expand the temporal receptive field and capture multi-scale features through parallel processing paths [37], thereby enhancing their ability to model complex timefrequency patterns in speech. Additionally, attempts like learnable fbank to explore learnable filterbanks within the CNN framework demonstrate efforts to optimize front-end processing of audio signals [143], moving beyond fixed filterbanks toward data-driven representations that can improve separation performance, particularly in noisy environments. Despite the significant achievements of CNN methods in speech separation tasks, they still face several critical challenges that limit their performance and generalization. Foremost, CNNs often struggle to adequately capture long-range dependencies in audio signals while maintaining sensitivity to local features, as their convolutional kernels are inherently limited in receptive field size, potentially leading to suboptimal modeling of global temporal contexts in complex mixtures [11]. Secondly, in encoder-decoder structures like Wave-UNet and SUDO RM-RF, the upsampling process can introduce artifacts or distortions, such as aliasing or phase inconsistencies, which degrade reconstruction quality and affect the perceptual fidelity of separated speech [143]. Furthermore, direct time-domain modeling imposes high demands on network parameter convergence and feature matching, often resulting in training instability, sensitivity to hyperparameters, and poor robustness in noisy or reverberant environments [10]. These limitations highlight key research gaps: for instance, how to extend receptive fields without exponentially increasing computational costs, how to mitigate reconstruction errors in upsampling, and how to stabilize training for real-world deployment. Addressing these issues is crucial for advancing CNN-based models, as they represent fertile ground for future innovations in speech separation. To mitigate these challenges, researchers have explored several strategies. For example, models like Conv-TasNet incorporate non-linear encoders and parallel multi-scale separation modules to better integrate local and global information, while improved training strategies such as multi-task learning enhance model robustness [10], [11], [102]. Similarly, DPTCN-ATPP employs dilated convolutions and dual-path parallel encoding to expand receptive JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 13 fields and reduce instability in time-domain processing [96]. The development of CNNs in speech separation will primarily focus on further optimization of network structures and training mechanisms to overcome the aforementioned limitations. On one hand, the fusion of multi-scale, multiresolution information, attention mechanisms, and adaptive filterbanks (as proposed in Learnable fbank) is expected to improve separation effects and noise resistance performance [143]. On the other hand, leveraging emerging technologies such as Transformers, dual-path modeling, and self-supervised learning to combine CNNs with other deep models may enable more precise capture of long-distance dependencies and handling of complex noise scenarios; for instance, Two-Step TCN provides insights for low-latency and real-time applications through strategy of first learning latent spaces before separation [95]. Finally, the design of lightweight CNN models will receive increasing attention to meet requirements for real-time processing and deployment on resource-constrained devices [11], [37], [102]. 3) Attention-Based Methods: In recent years, with the success of Transformer in sequence modeling tasks [76], [145], [146], attention-based speech separation methods have gradually become research hotspot [14][18]. Although these they generally methods differ in specific implementations, share two significant characteristics: First, they utilize selfattention mechanisms to enable direct interaction of global information, breaking the limitations of traditional RNNs or CNNs in capturing long-distance dependencies [16][18]; Second, through segmentation or dual-path [15], multi-scale strategies [97], they consider both local details and effectively integrate long temporal information, thereby better accomplishing the separation of mixed speech signals. The Transformer architecture also allows parallel processing, avoiding the inherent sequential computation limitations in RNNs, which significantly improves the training and inference efficiency of models [97]. Attention-based models have achieved state-of-the-art separation performance on standard datasets such as WSJ0-2mix, demonstrating their effectiveness in the field of speech separation [17], [18]. It is crucial to distinguish that the attention mechanism central to these computational models is conceptually different from the top-down cognitive attention discussed in Section II-A. The latter refers to selective, goal-driven process in the human brain, where listener actively focuses on target speaker using prior knowledge [57] (e.g., the speakers voice characteristics). In contrast, the self-attention mechanism in Transformers can be understood as form of bottom-up attention. It is data-driven computational process that dynamically calculates the relevance of every element in an input sequence to every other element. This focus is not guided by an external, pre-defined goal but rather emerges from the statistical patterns learned from the data itself. Thus, it is mechanistic weighting scheme for feature integration, distinct from the brains high-level selective auditory processing. Attention-based speech separation methods can be categorized into several types based on their architecture and application of attention mechanisms. One approach is the dual-path processing strategy, such as the Dual-Path Transformer Network (DPTNet) [13] and SepFormer [16], which segment the input signal along the time axis and apply Transformers to capture local temporal features (Intra-Transformer) and long-term dependencies across blocks (Inter-Transformer). This dualpath structure enables the model to effectively process long sequences while maintaining sensitivity to short-term patterns. Another approach introduces special attention mechanisms to reduce computational complexity, such as MSGT-TasNet [97], which employs group self-attention mechanisms and multiscale fusion to reduce complexity while maintaining modeling capability for long-term dependencies. Sandglasset [98] introduces an hourglass-shaped architecture with multi-granularity features, where the temporal granularity of features gradually coarsens in the first half of the network and then gradually refines toward the original signal level. This unique structure allows the model to capture both broad and detailed temporal information. TFPSNet [104] combines time-frequency domain analysis, where multi-path blocks are designed not only to scan individually on time and frequency domains but also include blocks that scan along the time-frequency domain diagonal. The MossFormer series models [17], [18] adopt gated singlehead Transformer architecture and convolution-enhanced joint self-attention mechanism, through joint local and global self-attention mechanisms, performing full-computation selfattention on local blocks and linearized low-cost self-attention across the entire sequence. However, these methods are not without significant drawbacks, which pose substantial challenges for practical applications and represent key areas for future research. The most prominent issue is the high computational complexity, especially when processing long sequences. In the standard Transformers self-attention mechanism, the complexity grows quadratically with sequence length (i.e., O(n2) where is the sequence length), making it computationally prohibitive for speech sequences that can extend to thousands of frames [16]. This not only increases training and inference times but also demands substantial memory resources, limiting deployment on resource-constrained devices like edge hardware [14] [16], [104]. Additionally, attention mechanisms can suffer from overparameterization, leading to overfitting in low-data scenarios, and may struggle with capturing fine-grained local features without additional hybrid components [37], [102]. These limitations highlight critical research gaps: how to scale attention for ultra-long sequences without exponential resource demands, how to mitigate energy inefficiency in realtime systems, and how to balance global modeling with local precision in dynamic noise environments. To mitigate these challenges, researchers have explored various improvement strategies. For instance, multi-group self-attention (MSGT) divides the input into smaller groups to compute intra-group correlations, maintaining complexity within acceptable bounds as long as group sizes are controlled [97]. Sparse Transformers employ sparse attention patterns to reduce unnecessary computations, while hierarchical attention networks and segment-level recursive mechanisms enable learning of dependencies beyond fixed lengths [147]. 4) Mixture-Based Methods: Speech separation methods based on mixed architectures typically integrate multiple JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 14 temporal context, network structures to address different subtasks such as multi-scale information extraction [11], [37], [102] and timefrequency features [15], [85], [91]. The common characteristic is the incorporation of various modules including CNN, RNN, or Transformer to fully utilize local spectral features and global thereby achieving complementary advantages in performance and convergence stability [15], [37]. These methods adopt hybrid strategy because single networks often struggle to simultaneously account for spectral details and long-term dependencies, whereas mixed architectures can balance feature extraction and temporal modeling to meet comprehensive requirements for separation quality, realtime processing, and computational resources across different application scenarios. Mixed architecture approaches encompass various types, with design philosophies primarily reflected in the mixing and information interaction between different modules or branches. For instance, CBLDNN-GAT [91] achieves enhanced separation effects through hierarchical fusion of convolutional layers and bidirectional LSTM, combined with generative adversarial training to fully utilize log-mel filterbank and pitch information. The Sign Prediction Network [85] integrates clustering-based, RNN, and CNN techniques while employing geometric constraints to specifically address phase sign ambiguity issues arising from accurate amplitude estimation. Additionally, A-FRCNN [11] implements an asynchronous update strategy with bottom-up, top-down, and lateral connections to achieve collaborative processing of multi-temporal scale information, effectively compressing model parameters while improving separation performance. TF-GridNet [15] features multi-path structure comprising intra-frame spectral modules, sub-band temporal modules, and full-frequency self-attention modules, not only achieving complex spectral mapping but also ensuring that separated speech strictly satisfies energy conservation requirements of the mixed signal through novel loss terms. Mixed architecture methods face significant challenges that limit their practical deployment. The primary issue is the substantial computational overhead introduced by combining multiple architectural components. For instance, TF-GridNet alternates between bidirectional LSTM and full-frequency selfattention mechanisms, resulting in extremely high computational resource requirements that hinder real-time deployment or implementation on edge devices [102]. The model parameter explosion is another critical challengethese hybrid approaches typically combine convolutional, recurrent, and self-attention modules to capture both local spectral and global temporal information, leading to models with hundreds of millions of parameters. Additionally, training complexity poses significant difficulties. Hybrid models based on generative adversarial training suffer from unstable convergence and mode collapse issues inherent to GANs, making the training process complex and difficult to debug [148]. The multi-task learning frameworks commonly employed in these architectures further complicate the optimization landscape, requiring careful hyperparameter tuning and often leading to suboptimal trade-offs between different objectives. These computational and training challenges severely restrict the applicability of mixed architecture methods in resource-constrained scenarios and real-world deployments. Recent research has explored lightweight alternatives to address these limitations. State space models (SSM) have emerged as one potential direction, replacing traditional RNN or Transformer architectures by modeling input signals as linear ordinary differential equations [112], [120]. Some approaches also investigate the integration of auditory mechanisms, such as using pitch contours as prior information for conditional generative networks [82], [149]. However, these solutions remain largely experimental and have yet to demonstrate consistent performance improvements across diverse acoustic conditions. C. Audio Estimation Methods 1) Masking Methods: In the field of speech separation, mask-based approaches [24], [71] have been pivotal in the development of the encoder-separator-decoder architecture. In this setup, the models goal is to produce one or more masks that, when applied to the high-dimensional representation output by the encoder, separates individual source signals from the original mixed signal. Each mask corresponds to specific source, and when element-wise multiplied with the encoded representation of the mixed signal, it extracts the desired source component or suppresses the noise. This method relies on the ability of deep learning models to learn to extract useful features from the mixed signal and use these features to generate effective masks. Under this framework, the overall formula of the model is as follows: = Encoder(x), = Separator(W), ˆsi = Decoder(Mi W), for each source i, (6) where is the input of original speech signal, is the highdimensional representation output by the encoder, is the mask which is to extract specific sources or to suppress noise, and denotes element-wise multiplication (also known as the Hadamard product or pointwise multiplication). Mask-based methods demonstrate significant advantages in practical applications, offering intuitive interpretability as they can be viewed as selective filters that transparently show how the model differentiates and extracts target source signals [24]. However, masking operations may lead to irreversible information loss, particularly when mask values are too low or zero, potentially causing important signal components to be erroneously suppressed [15]. Another common issue is that signals processed with masks might introduce artificial artifacts during the decoding process, affecting the naturalness and quality of the final output. Considering these limitations of masking methods, researchers in recent years have focused on developing more advanced separator structures and optimization strategies to extract more discriminative signal representations, thereby maintaining the advantages of masking methods while mitigating their drawbacks [17], [18]. 2) Mapping Methods: In mapping-based methods [15], [119], the goal is not to generate masks but to directly map (or transform) from the encoded representation of the mixed JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 15 TABLE IV COMPARISON OF SUBJECTIVE AND OBJECTIVE METRICS. INTRU DENOTES THE METRIC IS INTRUSIVE (I.E. REQUIRES CLEAN REFERENCE); SCALE DENOTES THE METRICS VALUE RANGE; UNIT DENOTES THE UNIT OR INTERPRETATION OF THE METRIC. Subjective Metrics Metric MOS Intru Scale No 15 Unit Score Advantages/Limits Intuitive; high cost, time-consuming Metric Intru Scale Unit Advantages/Limits Objective Metrics SDR [125] SIR [125] SAR [125] SISDR [35] SDRi [125] SISDRi [35] CISDR [125] SASDR [150] SASISDR [151] SACISDR [151] PESQ (NB) [36] PESQ (WB) [36] STOI [152] ESTOI [153] DNSMOS [154] SIGMOS [155] Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No No Ratio Ratio Ratio Ratio Gain Gain Ratio Ratio Ratio Ratio Easy to compute; ignores semantics/intelligibility Interference suppression; no distortion measure Artifact evaluation; ignores residual interference Scale-invariant; robust to convolutive distortion Relative gain; no absolute quality measure Relative + invariant; only measures improvement Covers noise + artifacts; complex definition Multi-channel; aggregation-dependent Multi + invariant; computationally heavy Multi + convolution-invariant; filter estimation hard dB dB dB dB dB dB dB dB dB dB 0.54.5 MOS-like Models auditory masking; standard narrowband range 0.54.5 MOS-like 01 01 15 1 Supports wideband speech; improved fidelity Accurate intelligibility; noise-sensitive Extended; better correlation under modulated noise Non-intrusive; needs large-scale annotations End-to-end; complex training/deployment Corr. Corr. Score Score signal to the representations of the individual sources. This approach typically involves direct mapping function that learns how to recover the signals of independent sources from the mixed signal over time. major advantage of mapping-based methods is that they can directly optimize the feature space used for separation, rather than indirectly operating through masks.The overall formula for mappingbased method can be stated as: = Encoder(x), Si = Separator(W), ˆsi = Decoder(Si), for each source i, (7) for each source i. core advantage of mapping methods lies in their ability to directly optimize the feature space used for separation, avoiding the indirect operations of mask-based methods [15], [119]. This direct transformation mechanism can theoretically learn more complex source signal disentanglement approaches and potentially circumvent signal distortion or artifact issues caused by imprecise masks, as they do not rely on elementwise multiplication operations that may introduce information bottlenecks or artifacts. However, compared to mask-based methods, the physical significance of mapping methods is often less intuitive, with their internal separation mechanism resembling more of black box, reducing model interpretability. Moreover, learning direct mapping functions from mixed signals to multiple independent source signal representations may require more complex network structures and larger parameter counts, especially when processing highly overlapping or nonlinearly mixed signals, potentially leading to higher computational costs and training optimization challenges [15], [22], [119]. Therefore, although mapping methods theoretically provide more direct optimization path, in practice, trade-offs must be made between model complexity, interpretability, and separation performance. V. EVALUATION METRICS Speech separation evaluation is critical step for measuring performance, guiding algorithm optimization, and facilitating practical applications. With the widespread application of speech separation technology in communication [53], humancomputer interaction [156], hearing assistance [157], and other fields, establishing scientific, comprehensive, and reliable evaluation system has become particularly important. An ideal speech separation system should accurately separate target speech while maintaining speech naturalness and intelligibility, and minimizing interference, noise, and processing-induced distortion. Evaluation metrics are generally classified into two categories: subjective evaluation metrics and objective evaluation metrics. Subjective evaluation relies on human listeners direct perceptual assessment of separation results, reflecting the actual perceptual quality of speech separation. Objective evaluation, on the other hand, provides quantitative analysis of separation results through mathematical models and algorithms, offering reproducible and efficient assessment methods. As shown in Table IV, these two evaluation approaches have their respective advantages, complement each other, and together constitute complete system for evaluating speech separation technology. A. Subjective Metrics Subjective evaluation methods primarily rely on human listeners direct judgment of speech quality, with the Mean Opinion Score (MOS) rating system being the most widely listeners typically score speech used. In MOS evaluation, samples on scale of 1-5 according to the ITU-T P.800 standard [158], where 1 indicates extremely poor quality and 5 indicates excellent quality. MOS ratings can be subdivided into multiple dimensions, such as Speech Quality MOS (SIG), Background Noise MOS (BAK), and Overall MOS (OVRL) JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 16 [159]. Additionally, the ITU-T P.835 standard [160] is specifically designed for evaluating the performance of noise suppression algorithms, while the MUSHRA (MUltiple Stimuli with Hidden Reference and Anchor) test [161] is used for assessing high-quality audio systems. The primary advantage of subjective evaluation is that it directly reflects end-users perceptual experience, making the evaluation results highly valuable for practical applications. However, subjective evaluation also faces numerous challenges: first, organizing large-scale subjective tests requires test results substantial human resources and time; second, can be influenced by individual differences among listeners, testing environments, cultural backgrounds, and auditory fatigue, leading to subjective bias and consistency issues; third, subjective testing is difficult to automate and standardize, making it unsuitable for real-time optimization guidance during algorithm development. Nevertheless, subjective evaluation is still considered the golden standard for assessing speech separation technology, and the effectiveness of objective evaluation metrics is typically validated through their correlation with subjective evaluation results. B. Objective Metrics Objective evaluation metrics quantitatively analyze the relationship between separated speech and reference signals through mathematical models, providing an automated, reproducible, and efficient assessment method. In signal-level evaluation, the Signal-to-Noise Ratio (SNR) is the most fundamental metric, used to measure the energy ratio between the target speech signal and background noise [125]. The Signal-to-Distortion Ratio (SDR) and its improved versions offer more comprehensive assessment, measuring the energy ratio between the reference signal and the estimated signal. The SDR improvement (SDRi) evaluates algorithm performance by comparing the changes in SDR before and after processing [125]. To address the sensitivity of traditional SDR to signal scaling, the Scale-Invariant Signal-toDistortion Ratio (SI-SDR) and the Scale-Invariant SDR improvement (SI-SDRi) have been proposed [35], introducing an optimal scaling factor to ensure fairer evaluations. In multisource separation scenarios, the Convolution-Invariant Signalto-Distortion Ratio (CI-SDR) considers various error terms, including interference, noise, and distortion introduced by the algorithm [125]. For conference-type multi-talker speech data, the Source-Aggregated Signal-to-Distortion Ratio (SASDR) [150] evaluates separation performance by aggregating multi-channel signals, with derived metrics SA-SI-SDR and SA-CI-SDR [151] targeting scale-invariant and convolutioninvariant applications, respectively. Additionally, the Signalto-Interference Ratio (SIR) and the Signal-to-Artifacts Ratio (SAR) [125] measure the extent of target signal interference suppression and algorithm-induced distortion, often used in conjunction with SDR to form more comprehensive evaluation. On the perceptual level, the Perceptual Evaluation of Speech Quality (PESQ) [36] assesses speech quality by simulating the human auditory system, while the Short-Time Objective Intelligibility (STOI) [152] predicts speech intelligibility by calculating short-term correlation. The Perceptual Objective Listening Quality Assessment (POLQA) [162], as third-generation speech quality assessment algorithm, supports wideband speech evaluation and accurately measures nonlinear distortion, time-varying distortion, and time-scaling distortion. With the advancement of deep learning technologies, no-reference metrics such as the Deep Noise Suppressor Mean Opinion Score (DNSMOS) [154] and SigMOS [155] directly predict subjective ratings through neural network analysis without requiring reference signals, demonstrating broad prospects in practical applications. The main advantages of objective evaluation metrics lie in their automation, low cost, and high reproducibility; however, they also have certain limitations: many objective metrics struggle to fully capture the complexity of human auditory perception, particularly regarding subjective experiences such as speech naturalness and auditory comfort; different metrics are designed for different application scenarios, and no single metric is suitable for all speech separation tasks; some metrics (e.g., PESQ-NB) are primarily designed for narrowband speech and may not be accurate enough in wideband speech or music scenarios. While objective metrics are automated, low-cost, and reproducible, they have limitations. Many struggle to fully capture the complexity of human auditory perception, such as naturalness and comfort. Furthermore, the sheer number of metrics can make direct comparison across different studies difficult if evaluation standards are not shared. To address this and promote consistency, it is highly recommended that studies report core set of metrics that provide multi-faceted view of performance. For comprehensive evaluation, the standard practice should include SI-SDR (or SI-SDRi) as the primary metric for separation accuracy, alongside traditional SDR for backward compatibility. To assess perceptual impact, reporting PESQ for speech quality and STOI (or its successor, ESTOI) for speech intelligibility is essential. Reporting this combination of metrics (SI-SDR, SDR, PESQ, and STOI/ESTOI) provides balanced assessment covering both signal fidelity and human perception, enabling more meaningful and reliable comparisons across different algorithms. VI. DATASETS Speech separation, as crucial task in the field of speech processing, heavily relies on the availability of high-quality training data. Most speech separation methods employ supervised learning, which requires that the training data provide both clean source signals and mixture signals, enabling the model to effectively learn to separate mixed speech into individual speaker voices [24], [40]. Existing speech separation datasets can be classified into single-channel and multichannel categories, as illustrated in Table V. These datasets capture various aspects of real-world acoustic environments, including reverberation, device noise, speech overlap, and background interference. A. Monaural Single-channel speech separation tasks require high-quality multi-speaker mixed speech data, comprising both clean JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 17 TABLE COMPARATIVE OVERVIEW OF MONAURAL AND MULTI-CHANNEL DATASETS CATEGORIZED BY ACOUSTIC CHARACTERISTICS AND RECORDING CONDITIONS. THE TABLE HIGHLIGHTS KEY PARAMETERS INCLUDING NOISE CONDITIONS, REVERBERATION PROPERTIES, SPEAKER OVERLAP PATTERNS, AND LICENSES. Dataset Noise Reverb Overlap License WSJ0 [24] WHAM! [39] WHAMR! [163] LibriMix [38] DNS Challenge [164] REAL-M [40] Lombard-GRID [165] LibriheavyMix [166] LRS2-2Mix [41] SonicSet [42] SMS-WSJ [167] LibriCSS [127] Kinect-WSJ [168] AISHELL-4 [169] Monaural 100% 100% 100% 0100% 100% 100% 100% 0100% 100% 0100% LDC CC BY-NC 4.0 CC BY-NC 4.0 MIT MIT Apache-2.0 Apache-2.0 Apache-2.0 Academic CC BY-SA 4.0 Multi-channel 100% 040% 0100% 0100% LDC MIT LDC CC BY-SA 4.0 source signals and mixture signals, in order to enable models to learn to separate mixed speech into individual speaker voices. Existing datasets can be categorized into three types: (1) simple synthetic mixtures based on clean speech, such as WSJ0-2mix [24] and WSJ0-3mix; (2) complex mixtures incorporating environmental noise and reverberation, such as WHAM! [39] and WHAMR! [163]; and (3) large-scale datasets simulating real-world scenarios, such as LibriMix [38] and LibriheavyMix [166]. The early dataset WSJ0-2mix includes only 101 speakers and limited vocabulary, resulting in inadequate model generalization. To address this limitation, LibriMix leverages the 1,252 speakers from LibriSpeech to construct larger-scale mixed speech dataset, and additionally introduces sparse overlap patterns to better simulate real conversational scenarios. With the progression of research, dataset design has gradually shifted towards greater diversity. For instance, LibriheavyMix provides 20,000 hours of multi-speaker speech with accurate time-stamped annotations; LRS2-2Mix [41] is constructed based on BBC television programs, encompassing variety of accents and environmental variations; SonicSet [42] utilizes the SonicSim tool to simulate moving sound sources in 90 distinct scenarios, thereby enhancing spatial acoustic features; Lombard-GRID-2mix [165] focuses on the effect of speaker articulation changes under noisy environments on speech separation performance. The latest trend is the development of real-recorded speech mixture datasets, such as REAL-M [40],and DNS Challenge datasets [164] tailored for noise suppression tasks. In the future, the development of datasets may focus on: (1) recording at larger scales and with greater diversity in real-world scenarios; (2) providing fine-grained annotations, including speaker emotion, accent, age, and other attributes; (3) constructing multimodal datasets by combining visual, textual, and other information to facilitate speech separation; and (4) building continuous speech separation datasets in dynamic scenarios to support the development of real-time application systems. 1) WSJ0: The Wall Street Journal (WSJ0) corpus is highquality, read-speech collection originally created for automatic speech recognition research [170]. Due to its clean recordings, it has become the standard foundation for creating benchmark speech separation datasets. The most prominent of these is wsj0-2mix [24], which generates mixtures of two speakers by combining their signals at various signal-to-noise ratios. To address more complex scenarios, the wsj0-3mix dataset extends this by creating mixtures from three distinct speakers. Collectively, these datasets have been instrumental as standard benchmarks for developing and evaluating speech separation algorithms in multi-talker environments. 2) WHAM! & WHAMR!: The WHAM! (Wall Street Journal 0 Habitat multi-channel) dataset extends the WSJ0 2-mix dataset by adding environmental noise from the real world to simulate speech separation problems in various noise scenarios [39]. It includes different noise conditions representing indoor and outdoor environments, public spaces, and transportation vehicles, all sourced from real-world recordings. The WHAMR! dataset is an extension of the WHAM dataset, featuring not only background noise but also reverberation effects [163]. This provides data support for speech separation research under more complex acoustic conditions. 3) LibriMix: LibriMix [38] is foundational open-source speech separation dataset, created to overcome the significant generalization limitations of prior benchmarks like wsj0-2mix [24], which suffer from limited speaker and vocabulary diversity and unrealistic, fully-overlapped mixtures. To address this, LibriMix leverages the large-scale LibriSpeech corpus [171] and WHAM! noise [39] to generate much more diverse and challenging dataset. Its construction pipeline features baseline version that mixes speakers using perceptually-aware LUFS normalization [172] and adds environmental noise. it also introduces sparse version designed to Crucially, mimic natural conversation, which uses forced aligner [173] to create longer mixtures with varying degrees of temporal overlap (0%100%). This dual approach, offering both clean and noisy mixtures with dense and sparse overlaps, provides comprehensive and scalable benchmark for training and evaluating more generalizable speech separation models. 4) DNS Challenge: The DNS (Deep Noise Suppression) Challenge dataset [164] is key resource in the field of speech separation and noise suppression. Developed to support the DNS Challenge hosted by Microsoft, this dataset includes diverse collection of real and synthetic noises designed to mimic variety of acoustic environments. It also contains clean speech recordings from multiple speakers in various languages, which are mixed with background noises to create challenging audio scenarios for testing and enhancing noise suppression algorithms. The DNS dataset is widely utilized by researchers and developers to train and evaluate their models, facilitating advancements in the domain of speech processing technologies by providing rich and varied set of audio samples for comprehensive testing and development. 5) REAL-M: REAL-M [40] is real-world mixed speech dataset collected through crowdsourcing, specifically designed for two-speaker scenarios. This dataset comprises over 1,436 mixed speech samples, sourced from 50 speakers, including both native and non-native English speakers, with total duration of approximately 3 hours. The mixed speech samples JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 are generated by participants synchronously reading paired texts randomly selected from the LibriSpeech test set, with sentence lengths strictly constrained between 5 and 15 words. distinguishing characteristic of the mixing process is the diversity of recording conditions: participants utilize various consumer-grade devices, such as laptops and smartphones, and perform recordings in different acoustic environments. These settings encompass near-field and far-field scenarios, as well as varying levels of environmental noise and reverberation effects. Each mixed audio segment is accompanied by precise text transcriptions, providing crucial benchmark for subsequent automatic speech recognition (ASR) performance evaluation."
        },
        {
            "title": "The",
            "content": "6) Lombard-GRID-2mix: Lombard-GRID-2mix dataset [165] is specifically designed to investigate the impact of the Lombard effectchanges in articulation due to noisy environmentson speech separation systems. Derived from the Audio-Visual Lombard GRID corpus [174], it comprises two parallel subsets containing two-speaker mixtures: one with normal speech and another with Lombard speech. Both subsets are constructed by mixing two speakers at an 8 kHz sampling rate with relative signal-to-noise ratio between 05 dB, using min clipping mode to to simulate ensure maximum temporal overlap. Critically, the conditions that induce the Lombard effect, the Lombard subset further incorporates speech-shaped background noise (SSN) into the mixtures. This dual-subset design, which includes meticulously partitioned training, validation, and test sets, provides controlled environment to systematically analyze how speech style variations affect the performance of speech separation algorithms. 7) LibriheavyMix: LibriheavyMix [166] is massive 20,000-hour, single-channel dataset of far-field overlapping speech, designed to overcome the data scarcity and lack of realism in prior corpora for complex multi-speaker tasks. Built it employs sophistiupon the Libriheavy [175] corpus, cated synthesis pipeline: first, dynamic overlap algorithm, informed by the statistical properties of real meetings [176], generates complex multi-turn dialogues for 1 to 4 speakers. these dialogues are convolved with simulated room Next, impulse responses using the FAST-RIR toolkit [177] to create diverse and realistic far-field reverberant conditions. Critically, the dataset provides precise, time-aligned annotations including speaker identity, text, and punctuation, establishing it as foundational resource for developing and evaluating jointly optimized, end-to-end systems for speech separation, recognition, and speaker diarization. 8) LRS2-2Mix: The LRS2-2Mix dataset [41] is synthetic two-speaker mixed speech benchmark derived from the LRS2BBC audio-visual corpus [178], which comprises real-world video clips from an extensive collection of BBC programs. It is created by mixing speech signals from different speakers found in the LRS2-BBC validation and test sets at signalto-noise ratios (SNRs) ranging from -5 dB to 5 dB. The resulting dataset provides approximately 11 hours of training data, along with 3-hour validation and test sets. By retaining the associated visual information from the source, LRS2-2Mix serves as valuable resource for developing and evaluating audio-visual speech separation and recognition models in multi-talker scenarios. 9) SonicSet: SonicSet [42] is large-scale, high-quality dataset designed to provide realistic and diverse training data of moving sound sources for speech separation and enhancement tasks. It integrates 360 hours of speech from LibriSpeech [171] and various noises into 90 distinct 3D scenes sourced from Matterport3D [179]. The dataset was created using the SonicSim synthesis tool [42], which simulates the movement of sound sources by generating trajectories within the 3D environments. Along these paths, it computes time-varying Room Impulse Responses (RIRs) and convolves them with source audio to produce dynamic, spatialized recordings that closely mimic real-world acoustics. In addition to the audio, SonicSet provides rich metadata in JSON files detailing source trajectories and positions, making it an exceptionally valuable resource not only for separation and enhancement, but also for related tasks like sound source localization and voice activity detection. B. Multi-channel Multi-channel speech signal processing holds critical significance in real-world applications. Compared with singlechannel methods, multi-channel methods can effectively exploit spatial information, thereby markedly enhancing speech separation performance. To advance research in this field, various multi-channel datasets with diverse characteristics have been constructed. Existing multi-channel speech separation datasets can be broadly categorized into two types: simulated reverberant datasets (e.g., SMS-WSJ [167]) and datasets collected with real devices (e.g., Kinect-WSJ [168], AISHELL4 [169], and LibriCSS [127]). These datasets capture different aspects of real-world acoustic environments. SMS-WSJ [167] creates multi-speaker environments with simulated reverberation by combining the WSJ0 corpus with simulated room acoustics; Kinect-WSJ [168] focuses on noise and spatial characteristics captured by real devices; AISHELL-4 records complex acoustic environments from genuine meeting scenarios, including short pauses, speech overlaps, and background noise; LibriCSS targets continuous speech separation scenes, providing speech data with varying overlap ratios. Although current datasets have significantly propelled research in multichannel speech separation, future dataset development still faces several challenges and opportunities, such as constructing larger-scale and more diverse datasets, collecting data from more complex and dynamic scenarios (e.g., speaker motion and multiple noise sources), and exploring multimodal datasets that integrate auxiliary information such as vision and motion to assist speech separation. These directions will better reflect the complexity of practical application scenarios and further bridge the gap between laboratory research and real-world deployment. 1) SMS-WSJ: The SMS-WSJ dataset is multi-channel speech separation and recognition dataset that combines audio signals from the WSJ0 corpus with simulated room acoustics [167]. In particular, SMS-WSJ leverages multi-channel room that more acoustic simulations to create speech dataset JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 closely resembles real-life scenarios. It is particularly wellsuited for the development and evaluation of algorithms for speech separation and recognition in complex acoustic environments. The dataset includes not only the original clean speech signals but also reverberant speech signals simulated in multi-talker environment, posing additional challenges for speech separation and recognition systems. 2) LibriCSS: The LibriCSS dataset [127] is designed to evaluate Continuous Speech Separation (CSS) by simulating realistic conversational scenarios. It was created by concatenating speech segments from LibriSpeech [171] to form 10 hours of continuous audio, which was then re-recorded in meeting room using seven-channel circular microphone array to capture authentic room acoustics. The dataset is structured into one-hour sessions, each containing mini-sessions with controlled speaker overlap ratios varying from 0% to 40%. Each mini-session features eight speakers randomly selected from LibriSpeechs test clean subset, with speakers positioned at fixed but random locations. To enable systematic assessment, LibriCSS is equipped with Kaldi-based [180] ASR evaluation protocol, allowing for the direct measurement of recognition performance on separated speech. 3) Kinect-WSJ: The Kinect-WSJ dataset [168] is challenging audio dataset that consists of speech recordings from the WSJ0 corpus captured using the Microsoft Kinect device. The aim of this dataset is to facilitate speech recognition research in real-world noise conditions and potential room reverberation. The Kinect-WSJ dataset places particular emphasis on the characteristics of the Kinect microphone array when capturing speech, which include higher noise levels and spatial audio capturing capabilities, making this dataset particularly relevant for automatic speech recognition and sound source localization studies. 4) AISHELL-4: The AISHELL-4 [169] is challenging dataset designed to bridge the gap between research and real-world meeting applications. It comprises 120 hours of audio from 211 real meetings, recorded with an 8-channel circular microphone array and featuring 4 to 8 native Mandarin speakers per session. The dataset captures complex acoustic environments, including speech overlaps, rapid speaker turns, and diverse background noises (e.g., keyboard typing, fan noise), to increase processing difficulty. To ensure realism, recordings were conducted in rooms of varying sizes and materials, with speaker-to-microphone distances ranging from 0.6m to 6.0m; speakers were static in the training set but mobile in the evaluation set. For supervised learning, the data is meticulously annotated with explicit markings of overlapping speech regions. All information, including speaker IDs, gender, precise timestamps, and transcriptions, is provided in the TextGrid format, making AISHELL-4 highly suitable for developing and evaluating end-to-end multi-speaker processing pipelines, from signal processing to ASR and diarization. VII. RESULTS WITH DIFFERENT MODELS AND DIFFERENT DATASETS In recent years, the field of speech separation has witnessed rapid development, with improvements in model performance largely propelled by the advent of high-quality, TABLE VI PERFORMANCE COMPARISON OF SPEECH SEPARATION MODELS ON THE WSJ0-2MIX DATASET. THE TYPE COLUMN DENOTES THE MODEL PARADIGM: FOR GENERATIVE AND FOR DETERMINISTIC. SAME CONVENTIONS ARE USED IN SUBSEQUENT TABLES. Model SI-SDRi SDRi Params (M) Type SPMamba [120] EDSep [119] ReSepFormer [118] SepTDA [78] MossFormer2 [18] Separate And Diffuse [23] S4M [112] pSkiM [108] DiffSep [106] TF-GridNet [15] MossFormer [17] TDANet [102] SepEDA [28] SkiM [103] MTDS [101] QDPN [99] SFSRNet [100] TFPSNet [104] Unknow-SS [77] A-FRCNN [11] Sandglasset [98] SepFormer [16] WaveSplit [86] MSGT-TasNet [97] Multi-Decoder Dprnn [27] DPTCN-ATPP [96] DPTNet [14] Conv-TasSAN [94] DPRNN [13] VSUNOS [26] Two-Step TCN [95] SudoRM-RF [10] Deep CASA [92] Conv-TasNet [9] OR-PIT [25] Chimera++ sign [85] ADAN [83] TaSNet [71] Chimera++ Network [84] DANet [8] uPIT-BLSTM [87] DPCL [24] 22.5 15.9 18.6 24.0 24.1 23.9 20.5 15.5 14.3 23.5 22.8 18.6 21.2 18.3 21.5 23.6 24.0 21.1 19.4 18.3 20.8 22.3 22.3 17.0 19.1 19.6 20.2 15.1 18.8 20.1 16.1 17.0 17.7 15.3 14.8 15.3 10.4 13.2 11.5 10.5 9.8 10.8 22.7 18.9 23.9 20.7 23.6 18.9 21.4 18.7 21.7 24.1 21.3 18.6 21.0 22.4 22.4 17.3 19.9 20.6 15.4 19.0 20.4 17.3 18.0 15.6 15.0 15.6 10.8 13.6 12.0 10.0 6.1 8.0 21.2 55.7 3.6 8.5 14.5 42.1 2.3 12.5 5.9 4.0 200.0 59.0 2.7 6.1 2.3 26.0 29.0 66.8 4.7 2.7 5.0 2.9 7.5 8.6 2.7 12.8 5.1 9.1 23.6 32.9 9.1 92.7 D D D D D D G D D D large-scale benchmark datasets. The currently widely adopted standard datasets include WSJ0-2Mix [24], WHAM! [39], LibriMix [38], and WHAMR! [163], which cover scenarios ranging from clean speech to complex acoustic conditions with intense noise and reverberation. Furthermore, newly proposed datasets such as LRS2-2Mix [37] and SonicSet [42] present authentic challenges under realistic acoustic environments. We have compiled the performance of various approaches across several mainstream datasets, focusing particularly on their improvements in standard metrics such as SI-SNRi and SDRi (see Tables VI, VII, IX, VIII, X, XI), and we also compare model efficiency in terms of parameter count. Most experimental results are directly reproduced from the original publications. For works that do not openly report specific scores, we adopt evaluation data from subsequent related studies to ensure fairness in horizontal comparison."
        },
        {
            "title": "For",
            "content": "the scenario of clean speech separation, WSJ02Mix [24] and LibriMix [38] are the most representative JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 20 TABLE VII PERFORMANCE COMPARISON OF SPEECH SEPARATION MODELS ON THE WHAM! DATASET. TABLE IX PERFORMANCE COMPARISON OF SPEECH SEPARATION MODELS ON THE LIBRIMIX (TRAIN-100 MIN) DATASET. Model SI-SDRi SDRi Params (M) Type Model SI-SDRi SDRi Params (M) Type SPMamba [120] ReSepFormer [118] Fast-GeCo [116] MossFormer2 [18] MossFormer [17] TDANet [102] Unknow-SS [77] A-FRCNN [11] SepFormer [16] WaveSplit [86] MSGT-TasNet [97] DPTNet [14] DPRNN [13] VSUNOS [26] SudoRM-RF [10] Conv-TasNet [9] TaSNet [71] Chimera++ Network [84] 17.4 14.1 12.6 18.1 17.3 15.2 11.5 14.5 16.4 16.0 13.1 14.9 13.7 15.2 12.9 12.7 12.0 10.0 17.6 14.4 15.4 14.8 16.5 15.3 14.1 13.3 6.1 8.0 55.7 42.1 2.3 6.1 26.0 29.0 66.8 2.7 2.9 7.5 2.7 5.1 23.6 32.9 D D D D TABLE VIII PERFORMANCE COMPARISON OF SPEECH SEPARATION MODELS ON THE WHAMR! DATASET. Model SI-SDRi SDRi Params (M) Type SPMamba [120] MossFormer2 [18] TF-GridNet [15] MossFormer [17] QDPN [99] SepFormer [16] WaveSplit [86] DPTNet [14] DPRNN [13] VSUNOS [26] SudoRM-RF [10] Conv-TasNet [9] TaSNet [71] 16.6 17.0 17.3 16.3 14.4 14.0 13.2 11.2 10.3 12.2 13.5 8.3 10.9 15.2 15.8 13.0 12.2 10.6 6.1 55.7 14.5 42.1 200.0 26.0 29.0 2.7 2.9 7.5 2.7 5.1 23. D D D datasets. Recent dual-path network architectures (such as SepTDA [78], SFSRNet [100], TF-GridNet [15], and MossFormer2 [18], etc.) generally attain SI-SDRi values of 20 dB or higher on WSJ0-2Mix, with some methods (such as SepTDA and SFSRNet) even surpassing 24 dB, which is substantial improvement over earlier approaches like ConvTasNet [9] and DPRNN [13]. similar trend is observed on the LibriMix (train-100) subset, where methods such as MossFormer2 [18], Separate And Diffuse [23], TF-GridNet [15], and TFPSNet [104] all achieve remarkable results, with SISDRi values typically ranging from 18 to 21 dB, outperforming conventional methods by significant margin. It is noteworthy that on datasets with broader speaker coverage such as LibriMix, parameter-efficient networks (e.g., TIGER [37], TDANet [102]) can still deliver competitive performance with extremely low model complexity (for instance, TIGER comprises just 0.8M parameters), thereby demonstrating the dual advantages of U-Net structures in generalizability and efficiency. More [39], WHAMR! [163], LRS2-2Mix [37], and SonicSet [15], impose stricter demands on models due to the incorporation of real challenging datasets, such as WHAM! SPMamba [120] TIGER [37] Conv-TasNet GAN [117] Fast-GeCo [116] DIP [115] MossFormer2 [18] Separate And Diffuse [23] S4M [112] DiffSep [106] TF-GridNet [15] MossFormer [17] TDANet [102] SSL-SS [105] SFSRNet [100] TFPSNet [104] A-FRCNN [11] SepFormer [16] WaveSplit [86] DPTNet [14] DPRNN [13] Two-Step TCN [95] SudoRM-RF [10] Conv-TasNet [9] TaSNet [71] Chimera++ Network [84] uPIT-BLSTM [87] 19.9 18.0 12.2 13.0 15.8 21.7 21.5 16.9 9.6 19.2 19.7 17.4 11.1 16.4 19.7 16.7 16.5 15.8 16.7 14.1 12.0 13.5 12.2 7.9 6.3 7.6 20.4 18.3 12.6 16.1 17.4 19.6 17.9 16.9 19.9 17.2 17.0 15.9 17.1 14.6 12.5 14.0 12.7 8.7 7.0 8.2 6.1 0.8 55.7 3.6 14.5 42.1 2.3 59.0 2.7 6.1 26.0 29.0 2.7 2.9 8.6 2.7 5.1 23.6 32.9 92.7 G D D D D D TABLE PERFORMANCE COMPARISON OF SPEECH SEPARATION MODELS ON THE LRS2-2MIX DATASET. Model SI-SDRi SDRi Params (M) Type EDSep [119] DIP [115] TIGER [37] S4M [112] TDANet [102] A-FRCNN [11] SepFormer [16] DPTNet [14] DPRNN [13] SudoRM-RF [10] Conv-TasNet [9] 9.6 12.0 15.1 15.3 14.2 13.0 13.5 13.3 12.7 11.0 10. 12.4 15.3 15.5 14.9 13.3 13.8 13.6 13.0 11.4 11.0 0.8 3.6 2.3 6.1 26.0 2.7 2.9 2.7 5.1 D D TABLE XI PERFORMANCE COMPARISON OF SPEECH SEPARATION MODELS ON THE SONICSET DATASET. Model SI-SDRi SDRi Params (M) Type TF-GridNet [15] MossFormer [17] TDANet [102] A-FRCNN [11] DPRNN [13] SudoRM-RF [10] Conv-TasNet [9] 15.4 14.7 9.3 9.2 4.9 8.0 4.8 16.8 16.0 11.0 10.6 6.7 9.7 7. 14.5 42.1 2.3 6.1 2.9 2.7 5.1 D noise, reverberation, far-field, and other adverse conditions. On WHAM! and WHAMR!, the latest models including MossFormer2 [18], SPMamba [120], and TF-GridNet [15], continue to maintain notable superiority. For instance, MossFormer2 and TF-GridNet achieve SI-SDRi scores of 17.0 and 17.3 dB on WHAMR!, respectively, which significantly surpasses the 8.3 dB achieved by Conv-TasNet [9], thus illustrating JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 21 TABLE XII COMPARISON OF OPEN-SOURCE SPEECH SEPARATION TOOLKITS. Name nussl [181] PL Python DL Datasets Pre-trained Models Deployment PyTorch MUSDB18, WSJ0-2Mix, WHAM!, FUSS DPCL, OpenUnmix, DPRNN Chimera, TasNet, Python API License MIT ONSSEN [182] Python PyTorch WSJ0-2Mix, DAPS, None Python API GPL-3.0 ESPNet-SE [183], [184] Python PyTorch SLURP-S, LT-S, WSJ02Mix Edinburgh-TTS DANet, DPRNN, DPTNet, GridNet Conv-TasNet, SVoice, TFSkiM, Python API Apache-2.0 Asteroid [43] Python PyTorch WSJ0-2Mix, WHAM!, LibriMix, FUSS Conv-TasNet, DPRNN Python API MIT SpeechBrain [44], [185] Python PyTorch LibriMix, WSJ0-2Mix, SepFormer, Conv-TasNet Python API Apache-2.0 WHAM! ClearerVoice-Studio [186] Python PyTorch SLURP-S, LT-S MossFormer2 Python API, JIT, ONNX Apache-2.0 WeSep [45] Python, C++ PyTorch Libri2Mix outstanding robustness. The separation difficulty is further elevated in LRS2-2Mix and SonicSet, where the SI-SDRi scores of mainstream models decline substantially. For example, TDANet [102] and A-FRCNN [11] obtain only 911 dB on SonicSet, while earlier methods typically fall below 7 dB. It is worth noting that some models, such as TF-GridNet [15] and MossFormer [17], still maintain leading performance on SonicSet, attesting to the generalization capability of nextgeneration architectures under extreme noise conditions. Despite the improved performance and generalization of the latest approaches in complex real-world scenarios compared to traditional architectures, there remains substantial room for progress under highly diverse conditions, which poses ongoing challenges for future research. VIII. PLATFORMS Open-source toolkits in the field of speech separation have greatly accelerated the advancement and application of related research. By providing unified frameworks, standardized evaluation metrics, and convenient dataset interfaces, these toolkits have significantly lowered the barriers to entry and improved the reproducibility of experiments, as illustrated in Table XII. Representative tools, such as nussl [181], ONSSEN [182], ESPNet-SE [183], [184], Asteroid [43], SpeechBrain [44], [185], ClearerVoice [186], and WeSep [45], are primarily developed in Python and predominantly adopt PyTorch [187] as the deep learning backend, reflecting the widespread dominance of this framework within the community. common feature among these toolkits is their modular design, which enables researchers to conveniently replace or extend components such as data processing, model architectures, and loss functions according to specific research requirements. Furthermore, the provision of pretrained models and standardized evaluation pipelinessuch as the integration of BSSEval [125], mir_eval [188], or SI-SNRis another shared characteristic, facilitating rapid reproduction of baseline results and enabling fair comparisons. (Conv-TasNet), Spex+ pBSRNN, pDPCCN, TF-GridNet (JIT/ONNX) Python API, JIT, ONNX, C++ Apache-2.0 Despite these similarities, toolkits still vary distinctly in design philosophy, core functionalities, and application focus. nussl [181], an early and comprehensive library, covers range of techniques from traditional methods (e.g., mask estimation, matrix factorization) to deep learning approaches, with particular emphasis on standardized evaluation and data management. ONSSEN [182] is dedicated to deep-learningbased speech separation and enhancement, employing modular design that utilizes Librosa and Numpy for feature processing. ESPNet-SE [183], [184] distinguishes itself by seamlessly integrating front-end speech processing (separation and enhancement) with downstream tasks such as ASR, supporting multi-task joint training and focusing on multichannel and far-field scenarios. Asteroid [43] is recognized for its comprehensive functionality and retention of native PyTorch features, introducing Kaldi-style recipes to enhance reproducibility; it is primarily oriented toward monaural tasks. SpeechBrain [44], [185], as general-purpose speech processing toolkit, includes speech separation as one of its many supported tasks and introduces innovative training strategies such as dynamic mixing to enhance model generalization. ClearerVoice [186] places greater emphasis on practical applications, providing unified inference platform and supporting audio-visual target speaker extraction. WeSep [45] is specialized for target speaker extraction (TSE), integrating advanced speaker modeling techniques (such as combination with WeSpeaker [189]), employing online data simulation strategies to improve generalization to unseen data, and offering model export (JIT/ONNX) functionalities for efficient deployment. The collective efforts represented by these toolkits have driven considerable progress in speech separation technologies. Nonetheless, future toolkits are expected to further enhance model robustness and generalization under realistic and complex acoustic conditions (e.g., reverberation, noise, far-field), to develop more efficient and lightweight models suitable for edge computing, to strengthen the utilization of multimodal information (such as audio-visual integration, as explored by ClearerVoice [186] and WeSep [45]), and to deJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 22 liver more user-friendly deployment solutions and interfaces. Additionally, future developments may expand to broader audio processing tasks, such as universal source separation or deeper joint optimization with higher-level speech understanding in end-to-end frameworks. detailed introduction to these toolkits is provided below. A. nussl The Northwestern University Source Separation Library (nussl) [181] is designed to provide unified and extensible research platform for source separation. Implemented in Python, nussl adopts an object-oriented design paradigm, ensuring high scalability and ease of extension. This library integrates various source separation techniques, including maskbased estimation methods, matrix decomposition approaches, and deep learning-based models. Additionally, it offers comprehensive suite of standardized evaluation tools, enabling researchers to compare algorithms within consistent framework. B. ONSSEN ONSSEN [182] is an open-source library specifically designed for speech separation and enhancement tasks, aiming to provide flexible and extensible platform that facilitates researchers in implementing state-of-the-art deep learning algorithms and conducting fair comparisons. ONSSEN leverages Librosa and Numpy for feature extraction and employs PyTorch as the backend for model training. ONSSEN adopts modular design, consisting of data module, neural network module, and loss computation module. The data module is responsible for feature extraction and data loading, the neural network module implements various speech separation algorithms, and the loss computation module defines different loss functions. This modular structure enables researchers to effortlessly replace or extend individual components to accommodate diverse experimental requirements. C. ESPNet-SE ESPNet-SE [183], [184] is toolkit specifically designed for speech enhancement and speech separation, while also integrating downstream tasks such as automatic speech recognition (ASR). ESPNet-SE provides comprehensive suite of both single-channel and multi-channel speech enhancement and separation methods, enabling joint training with tasks such as ASR, speech translation, and spoken language understanding. The framework supports multi-task learning strategies, allowing for the joint optimization of different training objectives, such as signal-level loss and ASR loss, thereby improving overall system performance. Furthermore, ESPNetSE introduces two novel multi-channel datasets, SLURP-S and LT-S, to facilitate research on far-field speech understanding tasks. The potential applications of enhancement front-ends in these tasks are also demonstrated, highlighting their value in improving system robustness. D. Asteroid Asteroid [43] is an open-source audio source separation toolkit based on PyTorch [187], specifically designed for researchers and practitioners. It aims to provide both robust functionality and ease of use to support research and applications in monaural audio source separation and speech enhancement tasks. The development of this toolkit is inspired by state-of-the-art neural network-based source separation systems and integrates complete workflow encompassing data preparation, model training, and evaluation. Compared to existing toolkits such as nussl [181], ONSSEN [182], and Open-Unmix [190], Asteroid distinguishes itself through its comprehensiveness and flexibility. It not only supports wide range of datasets and diverse network architectures but also reproduces key results from important research papers using Kaldi-style recipes [180]. This significantly enhances research reproducibility and facilitates experimental workflows. The design philosophy of Asteroid emphasizes maintaining balance between abstraction and usability, preserving the native characteristics of PyTorch code whenever possible. Additionally, it enables seamless integration of third-party code with minimal modifications while ensuring experiment reproducibility through commandline configuration. E. SpeechBrain SpeechBrain [44], [185] is an open-source and generalpurpose speech processing toolkit implemented in PyTorch. It is designed to provide concise, flexible, and efficient framework that supports multiple speech-related tasks, including speech recognition, speech enhancement, speaker recognition, and speech separation. This toolkit not only offers wide range of pre-trained models and experimental recipes but also possesses strong extensibility, enabling researchers to rapidly construct, compare, and optimize various speech processing systems. The core architecture of SpeechBrain is built around modular design, allowing seamless integration of models across different tasks and thereby facilitating endto-end speech processing workflows. SpeechBrain implements variety of state-of-the-art speech separation models and supports training and evaluation on different speech corpora. It incorporates the dynamic mixing training strategy [16], which dynamically generates mixed speech samples during training rather than relying on fixed training dataset. This approach effectively enhances the models generalization capability and mitigates the risk of overfitting to specific data distributions. The speech separation module in SpeechBrain is fully implemented in PyTorch and supports end-to-end training, allowing researchers to explore methodologies such as joint training and transfer learning. F. ClearerVoice ClearVoice-Studio [186] is unified inference platform that integrates speech enhancement, speech separation, and audiovisual target speaker extraction. It is designed to streamline the application of pre-trained models in speech processing tasks JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 and facilitate their integration into real-world projects. The platform provides collection of pre-trained models covering various speech processing tasks and supports automatic model loading for inference, eliminating the need for manual downloads. In the speech separation module of ClearVoice-Studio, the system incorporates the MossFormer2 SS 16K model [18], which is specifically optimized for speech data sampled at 16 kHz. Additionally, ClearVoice-Studio offers flexible inference interface, supporting batch processing modes for single audio files, audio directories, and audio list files (.scp). Users can invoke MossFormer2 SS 16K for speech separation tasks through simple Python code and have the option to save the separated speech files online. This functionality provides researchers, engineers, and developers with convenient experimental environment, enabling seamless integration of the model into various speech processing pipelines and application systems. G. Wesep WeSep [45] is an open-source toolkit designed for Target Speaker Extraction (TSE), providing an efficient, flexible, and scalable solution for both academic research and real-world applications. The TSE task focuses on accurately extracting the speech signal of specific target speaker from mixture of speech signals. It finds widespread applications in personalized human-computer interaction, hearing aids, as well as downstream tasks such as automatic speech recognition and speaker recognition. However, current TSE research faces challenges such as poor generalization to unseen data and target speaker modeling methods, with lack of efficient relatively few open-source toolkits available. To bridge this gap, WeSep integrates mainstream TSE models, introduces advanced speaker modeling techniques, and offers efficient data management and online data simulation to facilitate advancements in TSE research. IX. CHALLENGES & EXPLORATIONS A. Long-Form Audio Processing Long-form audio processing presents multiple challenges in the field of speech separation, significantly constraining the practical applicability of existing models. When handling long sequential inputs, speech separation models often encounter computational resource bottlenecks [14][16], [100]. This issue is particularly pronounced in Transformer-based architectures that employ self-attention mechanisms, such as SepFormer [16] and TF-GridNet [15], whose computational complexity scales quadratically with input length. Consequently, these models suffer from excessive computational overhead when processing long-duration audio. Moreover, many speech separation techniques rely on fixed-length windowing approaches to mitigate computational demands. However, such methods frequently result in the loss of crucial information at window boundaries, leading to degraded separation performance [15]. In particular, when speech content spans across window boundaries, models may struggle to correctly identify and separate speaker characteristics within continuous speech segments. Another critical challenge in long-sequence processing is the degradation of long-range dependency modeling capabilities. As the audio length increases, the models ability to capture temporal dependencies across distant time points deteriorates [127]. This limitation is particularly detrimental in accurately separating speech from different speakers, especially in scenarios with frequent speaker alternations or similar vocal characteristics. In real-world applications, such as meeting telemedicine consultations, and multi-speaker transcription, conversation analysis, speech content often spans extended durations. The limitations of conventional models severely affect the quality and usability of the separation results in such cases. To address these challenges, researchers have proposed various solutions. Dual-path processing architectures, such as DPRNN [13] and DPTNet [14], alternate between intra-chunk and inter-chunk processing, effectively balancing local feature extraction and global context modeling. The MSGT-TasNet [97] incorporates hierarchical attention mechanism that applies attention computation at different abstraction levels, thereby reducing overall computational complexity while preserving long-range modeling capabilities. Additionally, MTDS [101] leverages sliding window approach combined with an overlap-add strategy, widely adopted for processing arbitrarylength audio while alleviating boundary effects through interwindow information sharing. More advanced solutions include sparse attention mechanisms, such as linear attention [191] [193], local attention [194], [195], and axial attention [196], [197], which restrict the scope of attention computation or modify its operation, reducing complexity from O(n2) to O(n log n) or even O(n). The MossFormer series [17], [18] employs hybrid local-global self-attention strategy, significantly alleviating computational burdens while maintaining global modeling capabilities. Furthermore, hybrid architectures that combine recurrent neural networks (RNNs) and convolutional neural networks (CNNs) have demonstrated advantages in long-sequence processing. For instance, models such as SkiM [103] and pSkiM [108] leverage skip memory mechanism to efficiently handle long-duration sequences. B. Lightweight Models Deploying lightweight models for speech separation remains significant challenge, particularly in embedded systems, mobile applications, and edge computing platforms. Many high-performance models [14][18], [98] are typically characterized by large number of parameters and complex network architectures, resulting in high computational complexity and substantial memory consumption, which hinder deployment on resource-constrained devices. While model compression techniques such as pruning [198] and quantization [199], [200] can mitigate computational demands, excessive compression may degrade separation performance. Efforts to optimize attention module complexity are prevalent. For instance, S4M [112] and SPMamba [120] leverage linear time-invariant systems for sequence modeling, decomposing input signals into multiscale representations to achieve speech separation with fewer JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 24 Fig. 10. Causal speech separation pipeline and causal network architectures. (a) Causal convolutional layers, (b) unidirectional recurrent neural networks, and Transformer models with causal masks. Causal convolutional layers apply masked convolutions along the temporal dimension to ensure that each output frame depends only on the current and previous frames. Unidirectional RNNs capture local sequential dependencies through forward recurrence. The Transformer module employs triangular causal mask in the self-attention mechanism to block access to future frames. trainable parameters. TDANet [102] reduces computational cost by placing computationally intensive modules at locations with minimal temporal resolution through downsamplingupsampling strategy. TIGER [37] partitions frequency domain signals into multiple sub-bands for parallel processing, compressing the attention dimension from time-frequency domain perspective. C. Causal Speech Separation Causal speech separation aims to address the practical requirements of speech separation technology in real-time applications [201], as shown in Figure 10. Traditional non-causal speech separation systems typically rely on bidirectional processing mechanisms, allowing access to future frames when generating the separation output for the current time step [9], [13], [15], [18], [71]. While such approaches achieve superior separation performance, they inevitably introduce processing latency, significantly limiting their feasibility for deployment in applications such as hearing aids, real-time communication systems, and human-computer interaction. In contrast, causal speech separation constrains the model to utilize only current and past information, fundamentally eliminating the dependency on future frames [103], [108], [118]. This enables low-latency or even zero-latency speech separation, making it well-suited for real-time processing scenarios. However, this design also introduces substantial performance challenges. The absence of contextual information from future frames often results in degraded performance when handling rapidly changing speech signals or complex overlapping scenarios [103]. This issue is particularly exacerbated in non-stationary noise and reverberant environments, where the difficulty of the separation task further increases. Additionally, the causality constraint limits the receptive field of the model, weakening its ability to capture long-range dependencies, which may lead to suboptimal performance in tasks requiring broader contextual information. Causal speech separation is typically achieved by making structural modifications to key components of non-causal systems. First, bidirectional recurrent neural networks (BiRNNs) or bidirectional long short-term memory networks (BiLSTMs) need to be replaced with unidirectional architectures to ensure that information propagates only forward in the temporal sequence [13]. Prior research has demonstrated that methods such as SkiM [103] and pSkiM [108] improve the performance of causal speech separation by preserving historical hidden states in LSTM layers and propagating them to subsequent layers. Additionally, in convolution-based models, causal convolution or appropriate padding strategies must be adopted to prevent information leakage [9]. For Transformerbased architectures, carefully designed masking mechanisms are required to restrict self-attention to focus exclusively on current and past information [16]. In recent years, streaming attention mechanisms have been extensively studied and have been shown to enhance the ability of causal models to leverage past information, thereby improving separation performance [118]. Beyond the aforementioned approaches, some studies have explored the application of state space models (SSMs) [112], [120], such as Mamba [140], in causal speech separation. These models maintain causality while effectively capturing long-range dependencies, thereby improving separation quality. Furthermore, compromise approach allows for limited amount of future information (e.g., few milliseconds) to improve separation performance while maintaining low latency these strategies drive the advancement [202]. Collectively, of causal speech separation technology, ensuring that realtime requirements are met while minimizing the performance degradation imposed by causality constraints. D. Generative Approaches In recent years, generative methods have garnered significant attention in speech separation due to their superior generalization and data modeling capabilities compared to traditional discriminative approaches. While discriminative models directly optimize evaluation metricssuch as scale-invariant signal-to-noise ratio (SI-SNR) [35] and perceptual speech quality (PESQ) [36]they often struggle to generalize to unseen conditions, especially in noisy, far-field, or reverberant environments [116]. Moreover, discriminative methods may introduce perceptually unnatural artifacts that compromise the structural integrity of speech, adversely affecting downstream tasks. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 25 Generative models, in contrast, learn the prior distribution of data without imposing rigid parametric assumptions on the output density [22], [89]. This allows them to capture complex characteristics of speech signalsincluding both magnitude and phase information, which is crucial for maintaining naturalness in low signal-to-noise ratio (SNR) and reverberant conditions [22], [107]. As result, generative approaches are better equipped to produce high-quality, natural-sounding speech even in challenging acoustic scenarios. widely explored family of generative methods is based on Generative Adversarial Networks (GANs) [19]. In GAN-based speech separation, generator networkoften implemented with convolutional neural networks (CNNs) or time-domain architectures such as Conv-TasNet [9]synthesizes speech signals, while discriminator network evaluates the realism of the generated outputs [117]. Variants like LSGAN [203] and MetricGAN [21] have been shown to enhance metrics such as PESQ and speech intelligibility (STOI) [152], particularly under adverse SNR conditions [91]. The Conditional Generative Adversarial Network (Conditional GAN) framework is employed to map mixed speech signals to individual sources, often integrating permutation-invariant training (PIT) to resolve the speaker permutation problem [20], [90], [94], [204]. However, challenges such as mode collapse and vanishing gradients remain inherent to GAN training [110]. More recently, diffusion models have emerged as promising alternative. These models employ stochastic differential equations (SDEs) to gradually add Gaussian noise to clean speech samples in forward process and learn reverse process to denoise and recover the original signal [22], [23], [106], [109], [205]. Diffusion models offer more stable training process, effectively mitigating issues like mode collapse and generating speech signals with higher naturalness and fewer artifacts [116], [119]. Nevertheless, their standard formulation requires numerous reverse sampling steps, leading to increased computational complexitya limitation that researchers are addressing through fast inference techniques such as FastGeCo [116]. The shift from discriminative to generative paradigms in speech separation has led to improved quality and robustness, making generative approaches promising avenue for addressing the challenges inherent in complex and noisy audio environments. Speech separation approaches based on pre-trained models have attracted extensive attention in recent years, as they effectively mitigate the scarcity of high-quality training data. Self-supervised pre-trained models, such as wav2vec 2.0 [133] and HuBERT [134], acquire rich speech representations by learning from large amounts of unlabeled speech data. These representations can be transferred to the speech separation task, thereby reducing the need for task-specific labeled data [105]. In addition, the domain gap between synthetic and real data is an important factor affecting separation performance. To this end, researchers have proposed Domain-Invariant Pre-training (DIP) front-end, which pre-trains the feature extractor on mixed data to ensure consistent representation across real and synthetic data, thereby enhancing the generalization performance of the model [115]. In practical applications, the pre-trained model is typically employed as feature extractor to convert the mixed audio into context-dependent embedding features that are subsequently used by the separation model to predict masks or embeddings for each speaker, or it is fine-tuned to better adapt the model to the requirements of the speech separation task [105]. This approach not only capitalizes on the robust capabilities of pre-trained models in speech processing tasks but also reduces training costs through transfer learning. Concurrently, autoencoder-based speech separation methods offer an efficient alternative by compressing the audio signal into low-dimensional embedding space. Neural Audio Codecs (NAC), pre-trained on large-scale data, produce embeddings that capture features beneficial for speech processing, making separation in the compressed space feasible [113], [114]. Due to the significant temporal compression achieved by the neural codec, the sequence length in the embedding space is substantially reduced, thereby lowering the memory and computational demands of the separation model [114]. However, methods for speech separation based on pretrained models and autoencoders also face certain challenges. For example, autoencoders are typically trained on clean speech data and may not adequately represent overlapping speech mixtures, which limits separation performance [113], [114]. Additionally, the distortion introduced during the compression process can negatively impact separation quality, particularly in complex acoustic environments [105], [113]. E. Pre-training Methods F. Target Speaker Extraction In real-world cocktail party scenarios, obtaining training data with target references is particularly challenging, which has led most separation models to rely on synthetic data during training. This reliance often results in decline in model performance when applied to real data [113], [115]. Moreover, traditional speech separation methods typically process highdimensional waveform data directly, incurring high computational costs and further limiting their deployment on resourceconstrained devices [111], [113], [115]. To address these issues, researchers have proposed speech separation methods based on pre-trained models and autoencoders, which leverage the knowledge acquired from large-scale pre-training while reducing computational complexity. Target Speaker Extraction (TSE), field closely related to speech separation, aims to isolate specific speakers voice from multi-speaker mixture [206][208]. To identify the target, TSE systems leverage auxiliary cues such as pre-recorded anchor utterance [206][209], spatial information [210], [211], or visual data like lip movements [212]. Compared to generic speech separation, TSE inherently bypasses critical issues like permutation ambiguity and speaker count dependency, making it highly active research area with significant value for robust speech recognition and automated meeting transcription [213], [214]. Current TSE methods predominantly rely on audio-based cues, where an effective speaker embedding is extracted from JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 26 an anchor utterance to guide the separation network [208], [209], [215]. Implementations vary, spanning time-domain models like the SpEx+ series [216], [217] and frequencydomain techniques such as VoiceFilter [218] and SpeakerBeam [213]. central challenge lies in learning discriminative and robust speaker embeddings from limited or lowquality anchors, as simplistic single-vector representations can lead to speaker identity confusion [219]. To mitigate this, researchers have explored more advanced representations, including hierarchical structures [207] and sparse LDA transformations [206], to enhance discriminative power. While audioonly methods are convenient to deploy, their performance is critically dependent on the anchor quality. Beyond audio-only approaches, research has expanded to multi-modal and spatial cues to handle more challenging scenarios. Audio-visual speaker extraction (AVSE) leverages visual information like lip movements, offering strong target indication in highly noisy or overlapped conditions [41], [212], [220][223]. However, AVSE requires high-quality, synchronized audio-visual data and incurs additional computational costs [221]. Alternatively, methods utilizing spatial information from microphone arrays, such as L-SPEX [210], provide strong physical discriminability but often assume the targets location is known or necessitate an extra localization module [224]. These diverse strategies highlight trade-off between performance robustness, implementation complexity, and scenario applicability. Future TSE research holds several promising directions. The joint modeling of speaker extraction with diarization could enable systems to comprehensively answer who spoke what and when. Adopting generative frameworks, particularly conditional diffusion models, may offer enhanced robustness and generalization to unseen speakers and noise. Furthermore, expanding multi-modal fusion to include gestures or semantic cues could address extreme conditions like severe overlap or occlusion. Finally, leveraging self-supervised [225] and fewshot learning will be crucial for reducing reliance on highquality anchor data, thereby improving the practicality of TSE for diverse, real-world applications. G. Joint Modeling with Other Tasks Speech processing in complex auditory scenes has recently attracted significant attention. Traditionally, speech separation and downstream taskssuch as automatic speech recognition (ASR) [226], speaker identification (SID) [227], and speaker diarization (SD) [228]were handled in cascaded pipeline. This approach, however, suffers from error accumulation and mismatched optimization objectives, hindering overall performance. Consequently, joint end-to-end modeling of speech separation with downstream tasks has become major research trend. These tasks prominently include ASR [129], [229][234], SD [129], [233], [235][237], SID [227], speaker counting [235], and broader speech enhancement [138]. The core motivation is mutual benefit: separation provides cleaner inputs for downstream modules, while downstream tasks impose beneficial constraints (e.g., linguistic or speaker cues) to guide the separation process. This synergy can even lead to unified solutions for the complex who spoke when and what problem [233], [235]. Existing joint training methods fall into three main categories: cascade-based, monolithic, and hybrid. Cascadebased methods [229], [230] maintain modular independence while enabling global end-to-end optimization. Monolithic approaches [231] use single network to bypass explicit separation, avoiding potential errors but at the cost of increased model complexity. Hybrid methods, such as EENDSS [235], TS-SEP [237], and PixIT [236], aim to balance the interpretability of separation with the performance gains of joint optimization. However, joint training introduces new challenges: balancing multi-task loss functions, managing increased computational complexity and cost, and addressing inference latency for real-time applications [138], [229]. Looking forward, this research area is expected to advance in several key directions. First, foundational model adaptation methodscharacterized by large models in combination with separation, adaptation/fine-tuning (e.g., soft prompts, side adapters, parameter-efficient learning)will enable large pre-trained ASR models such as Whisper to acquire multispeaker and target speaker recognition capabilities efficiently, transfer to multilingual and multiwith zeroor few-shot task scenarios [234], [238][240]. Second, joint modeling with broader array of downstream tasks, such as speech translation, emotion recognition, and speech synthesis [232], may give rise to new applications. Furthermore, improving the joint processing capabilities for long-duration, continuous conversations is necessary, including the development of more efficient online streaming solutions and more robust speaker tracking [233], [236]. Finally, with the advancement of selfsupervised and weakly supervised learning, future joint models may reduce their dependence on labeled data, thus becoming more practical and effective for real-world deployment [236]. X. CONCLUSIONS In this survey, we systematically survey deep neural network-based speech separation technologies, encompassing their core learning paradigms, key model architectures, mainstream evaluation metrics, datasets, and open-source tools. We not only categorize and summarize various methodologies, but also provide an in-depth analysis of the complete technical pipeline from encoder to separator to decoder. Furthermore, we conduct comparative analysis of different learning frameworks, including supervised, self-supervised, and unsupervised approaches, with the aim of establishing clear and comprehensive body of knowledge in this domain. Notably, we identify several existing challenges and technological gaps in current research, such as the effective handling of longduration audio, the design of lightweight models suitable for resource-constrained devices, and the fulfillment of real-time causal processing requirements while maintaining separation performance. Based on these observations, we further delineate potential future research directions, including the exploration of more efficient generative models (e.g., diffusion models), leveraging large-scale pre-trained models and neural codecs to bridge the gap between synthetic data and real-world scenarJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 27 ios, as well as strengthening the joint modeling of speech separation with downstream tasks (such as speech recognition and speaker diarization). We believe this survey offers researchers and practitioners comprehensive knowledge map and clear technical roadmap, thereby inspiring and advancing further exploration and development in the field of speech separation across theoretical innovation, model optimization, and realworld application."
        },
        {
            "title": "REFERENCES",
            "content": "[1] E. C. Cherry, Some experiments on the recognition of speech, with one and with two ears, Journal of the Acoustical Society of America, vol. 25, no. 5, pp. 975979, 1953. [2] B. Arons, review of the cocktail party effect, Journal of the American Voice I/O society, vol. 12, no. 7, pp. 3550, 1992. [3] A. S. Bregman, Auditory Scene Analysis: The Perceptual Organization of Sound. MIT press, 1994. [4] A. Hyvarinen and E. Oja, Independent component analysis: Algorithms and applications, Neural Networks, vol. 13, no. 45, pp. 411 430, 2000. [5] A. Cichocki, R. Zdunek, and S.-i. Amari, New algorithms for nonnegative matrix factorization in applications to blind source separation, in IEEE International Conference on Acoustics Speech and Signal Processing Proceedings. IEEE, 2006, pp. VV. [6] K. Tesch and T. Gerkmann, Multi-channel speech separation using spatially selective deep non-linear filters, IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 32, pp. 542553, 2023. [7] M. I. Mandel and J. P. Barker, Multichannel spatial clustering using model-based source separation, in New Era for Robust Speech Recognition: Exploiting Deep Learning. Springer, 2017, pp. 5177. [8] Z. Chen, Y. Luo, and N. Mesgarani, Deep attractor network for singlemicrophone speaker separation, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2017, pp. 246250. ideal timefrequency magnitude masking for speech separation, IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 27, no. 8, pp. 12561266, 2019. Conv-tasnet: Surpassing and N. Mesgarani, [9] Y. Luo [10] E. Tzinis, Z. Wang, and P. Smaragdis, Sudo rm -rf: Efficient networks for universal audio source separation, in International Workshop on Machine Learning for Signal Processing. IEEE, 2020, pp. 16. [11] X. Hu, K. Li, W. Zhang, Y. Luo, J.-M. Lemercier, and T. Gerkmann, Speech separation using an asynchronous fully recurrent convolutional neural network, in Advances in Neural Information Processing Systems. NeurIPS, 2021, pp. 22 50922 522. [12] D. Yu, M. Kolbæk, Z.-H. Tan, and J. Jensen, Permutation invariant training of deep models for speaker-independent multi-talker speech separation, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2017, pp. 241245. [13] Y. Luo, Z. Chen, and T. Yoshioka, Dual-path rnn: Efficient long sequence modeling for time-domain single-channel speech separation, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2020, pp. 4650. [14] J. Chen, Q. Mao, and D. Liu, Dual-path transformer network: Direct context-aware modeling for end-to-end monaural speech separation, in Annual Conference of the International Speech Communication Association. ISCA, 2020, pp. 26422646. [15] Z.-Q. Wang, S. Cornell, S. Choi, Y. Lee, B.-Y. Kim, and S. Watanabe, Tf-gridnet: Integrating fulland sub-band modeling for speech separation, IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 31, pp. 32213236, 2023. [16] C. Subakan, M. Ravanelli, S. Cornell, M. Bronzi, and J. Zhong, Attention is all you need in speech separation, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2021, pp. 2125. [17] S. Zhao and B. Ma, Mossformer: Pushing the performance limit of monaural speech separation using gated single-head transformer with convolution-augmented joint self-attentions, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2023, pp. 15. [18] S. Zhao, Y. Ma, C. Ni, C. Zhang, H. Wang, T. H. Nguyen, K. Zhou, J. Q. Yip, D. Ng, and B. Ma, Mossformer2: Combining transformer and rnn-free recurrent network for enhanced time-domain monaural speech separation, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2024, pp. 10 35610 360. [19] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, Generative adversarial networks, Communications of the ACM, vol. 63, no. 11, pp. 139144, 2020. [20] Y. Li, W.-T. Zhang, and S.-T. Lou, Generative adversarial networks for single channel separation of convolutive mixed speech signals, Neurocomputing, vol. 438, pp. 6371, 2021. [21] S.-W. Fu, C.-F. Liao, Y. Tsao, and S.-D. Lin, Metricgan: Generative adversarial networks based black-box metric scores optimization for speech enhancement, in International Conference on Machine Learning. PmLR, 2019, pp. 20312041. [22] B. Chen, C. Wu, and W. Zhao, Sepdiff: Speech separation based on denoising diffusion model, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2023, pp. 15. [23] S. Lutati, E. Nachmani, and L. Wolf, Separate and diffuse: Using pretrained diffusion model for improving source separation, 2023. [24] J. R. Hershey, Z. Chen, J. Le Roux, and S. Watanabe, Deep clustering: Discriminative embeddings for segmentation and separation, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2016, pp. 3135. [25] N. Takahashi, S. Parthasaarathy, N. Goswami, and Y. Mitsufuji, Recursive speech separation for unknown number of speakers, in Annual Conference of the International Speech Communication Association. ISCA, 2019, pp. 13481352. [26] E. Nachmani, Y. Adi, and L. Wolf, Voice separation with an unknown number of multiple speakers, in International Conference on Machine Learning. PMLR, 2020, pp. 71647175. [27] J. Zhu, R. A. Yeh, and M. Hasegawa-Johnson, Multi-decoder dprnn: Source separation for variable number of speakers, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2021, pp. 34203424. [28] S. R. Chetupalli and E. A. P. Habets, Speech separation for an unknown number of speakers using transformers with encoder-decoder attractors, in Annual Conference of the International Speech Communication Association. ISCA, 2022, pp. 53935397. [29] Y.-m. Qian, C. Weng, X.-k. Chang, S. Wang, and D. Yu, Past review, current progress, and challenges ahead on the cocktail party problem, Frontiers of Information Technology & Electronic Engineering, vol. 19, pp. 4063, 2018. [30] D. Wang and J. Chen, Supervised speech separation based on deep learning: An overview, IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2018. [31] M. Mirbeygi, A. Mahabadi, and A. Ranjbar, Speech and music separation approaches-a survey, Multimedia Tools and Applications, vol. 81, no. 15, pp. 21 15521 197, 2022. [32] J. Agrawal, M. Gupta, and H. Garg, review on speech separation in cocktail party environment: Challenges and approaches, Multimedia Tools and Applications, vol. 82, no. 20, pp. 31 03531 067, 2023. [33] P. Ochieng, Deep neural network techniques for monaural speech enhancement and separation: State of the art analysis, Artificial Intelligence Review, vol. 56, no. Suppl 3, pp. 36513703, 2023. [34] S. Ansari, A. S. Alatrany, K. A. Alnajjar, T. Khater, S. Mahmoud, D. Al-Jumeily, and A. J. Hussain, survey of artificial intelligence approaches in blind source separation, Neurocomputing, vol. 561, p. 126895, 2023. [35] J. Le Roux, S. Wisdom, H. Erdogan, and J. R. Hershey, Sdr: Halfbaked or well done? in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2019, pp. 626630. [36] A. W. Rix, J. G. Beerends, M. P. Hollier, and A. P. Hekstra, Perceptual evaluation of speech quality (pesq) new method for speech quality assessment of telephone networks and codecs, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2001, pp. 749752. [37] M. Xu, K. Li, G. Chen, and X. Hu, Tiger: Time-frequency interleaved gain extraction and reconstruction for efficient speech separation, in The Thirteenth International Conference on Learning Representations, 2025. [38] J. Cosentino, M. Pariente, S. Cornell, A. Deleforge, and E. Vincent, Librimix: An open-source dataset for generalizable speech separation, 2020. [39] G. Wichern, J. Antognini, M. Flynn, L. R. Zhu, E. McQuinn, D. Crow, E. Manilow, and J. Le Roux, Wham!: Extending speech separation to noisy environments, in Annual Conference of the International Speech Communication Association. ISCA, 2019, pp. 13681372. [40] C. Subakan, M. Ravanelli, S. Cornell, and F. Grondin, Real-m: Towards speech separation on real mixtures, in IEEE International JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 Conference on Acoustics, Speech and Signal Processing. pp. 68626866. IEEE, 2022, [41] K. Li, F. Xie, H. Chen, K. Yuan, and X. Hu, An audio-visual speech separation model inspired by cortico-thalamo-cortical circuits, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [42] K. Li, W. Sang, C. Zeng, R. Yang, G. Chen, and X. Hu, Sonicsim: customizable simulation platform for speech processing in moving sound source scenarios, in International Conference on Learning Representations, 2025. [43] M. Pariente, S. Cornell, J. Cosentino, S. Sivasankaran, E. Tzinis, J. Heitkaemper, M. Olvera, F.-R. Stoter, M. Hu, J. M. Martın-Donas, D. Ditter, A. Frank, A. Deleforge, and E. Vincent, Asteroid: The pytorch-based audio source separation toolkit for researchers, in 21st Annual Conference of the International Speech Communication Association. ISCA, 2020, pp. 26372641. [44] M. Ravanelli, T. Parcollet, P. Plantinga, A. Rouhe, S. Cornell, L. Lugosch, C. Subakan, N. Dawalatabad, A. Heba, J. Zhong, J.-C. Chou, S.-L. Yeh, S.-W. Fu, C.-F. Liao, E. Rastorgueva, F. Grondin, W. Aris, H. Na, Y. Gao, R. De Mori, and Y. Bengio, SpeechBrain: generalpurpose speech toolkit, 2021. [45] S. Wang, K. Zhang, S. Lin, J. Li, X. Wang, M. Ge, J. Yu, Y. Qian, and H. Li, Wesep: scalable and flexible toolkit towards generalizable target speaker extraction, in Annual Conference of the International Speech Communication Association, 2024, pp. 42734277. [46] S. Haykin and Z. Chen, The cocktail party problem, Neural Computation, vol. 17, no. 9, pp. 18751902, 2005. [47] J. H. McDermott, The cocktail party problem, Current Biology, vol. 19, no. 22, pp. R1024R1027, 2009. [48] R. P. Carlyon, How the brain separates sounds, Trends in Cognitive Sciences, vol. 8, no. 10, pp. 465471, 2004. [49] R. Cusack, J. Decks, G. Aikman, and R. P. Carlyon, Effects of location, frequency region, and time course of selective attention on auditory streaming, Journal of Experimental Psychology: Human Perception and Performance, vol. 30, no. 4, p. 643, 2004. [50] R. P. Carlyon, R. Cusack, J. M. Foxton, and I. H. Robertson, Effects of attention and unilateral neglect on auditory stream segregation, Journal of Experimental Psychology: Human Perception and Performance, vol. 27, no. 1, p. 115, 2001. [51] T.-Y. Kuo, Y. Liao, K. Li, B. Hong, and X. Hu, Inferring mechanisms of auditory attentional modulation with deep neural networks, Neural Computation, vol. 34, no. 11, pp. 22732293, 2022. [52] M. Elhilali, L. Ma, C. Micheyl, A. J. Oxenham, and S. A. Shamma, Temporal coherence in the perceptual organization and cortical representation of auditory scenes, Neuron, vol. 61, no. 2, pp. 317329, 2009. [53] P. Divenyi, Speech Separation by Humans and Machines. Springer Science & Business Media, 2004. [54] L.-Y. Li, X.-Y. Ji, F. Liang, Y.-T. Li, Z. Xiao, H. W. Tao, and L. I. Zhang, feedforward inhibitory circuit mediates lateral refinement of sensory representation in mammalian olfactory bulb, Journal of Neuroscience, vol. 34, no. 41, pp. 13 67013 683, 2014. [55] A. J. Power, J. J. Foxe, E.-J. Forde, R. B. Reilly, and E. C. Lalor, At what time is the cocktail party? late locus of selective attention to natural speech, European Journal of Neuroscience, vol. 35, no. 9, pp. 14971503, 2012. [56] J. F. Houde and S. S. Nagarajan, Speech production as state feedback control, Frontiers in Human Neuroscience, vol. 5, p. 11333, 2011. [57] E. Awh, A. V. Belopolsky, and J. Theeuwes, Top-down versus bottomup attentional control: failed theoretical dichotomy, Trends in Cognitive Sciences, vol. 16, no. 8, pp. 437443, 2012. [58] A. Mai, S. A. Hillyard, and D. J. Strauss, Linear modeling of brain activity during selective attention to continuous speech: the critical role of the n1 effect in event-related potentials to acoustic edges, Cognitive Neurodynamics, vol. 19, no. 1, pp. 118, 2025. [59] M. P. Milham, M. T. Banich, and V. Barad, Competition for priority in processing increases prefrontal cortexs involvement in top-down control: An event-related fmri study of the stroop task, Cognitive Brain Research, vol. 17, no. 2, pp. 212222, 2003. [60] B. Gourevitch, C. Martin, O. Postal, and J. J. Eggermont, Oscillations in the auditory system and their possible role, Neuroscience & Biobehavioral Reviews, vol. 113, pp. 507528, 2020. [61] P. Patel, K. van der Heijden, S. Bickel, J. L. Herrero, A. D. Mehta, and N. Mesgarani, Interaction of bottom-up and top-down neural mechanisms in spatial multi-talker speech perception, Current Biology, vol. 32, no. 18, pp. 39713986, 2022. [62] Z. Li and D. Zhang, How does the human brain process noisy speech in real life? insights from the second-person neuroscience perspective, Cognitive Neurodynamics, vol. 18, no. 2, pp. 371382, 2024. [63] B. G. Shinn-Cunningham and V. Best, Selective attention in normal and impaired hearing, Trends in Amplification, vol. 12, no. 4, pp. 283 299, 2008. [64] P. Lakatos, G. Musacchia, M. N. OConnel, A. Y. Falchier, D. C. Javitt, and C. E. Schroeder, The spectrotemporal filter mechanism of auditory selective attention, Neuron, vol. 77, no. 4, pp. 750761, 2013. [65] E. S. Sussman, Auditory scene analysis: An attention perspective, Journal of Speech, Language, and Hearing Research, vol. 60, no. 10, pp. 29893000, 2017. [66] N. Mesgarani and E. F. Chang, Selective cortical representation of attended speaker in multi-talker speech perception, Nature, vol. 485, no. 7397, p. 233, 2012. [67] J. C. Dahmen, P. Keating, F. R. Nodal, A. L. Schulz, and A. J. King, Adaptation to stimulus statistics in the perception and neural representation of auditory space, Neuron, vol. 66, no. 6, pp. 937948, 2010. [68] A. W. Bronkhorst, The cocktail-party problem revisited: Early processing and selection of multi-talker speech, Attention, Perception, & Psychophysics, vol. 77, no. 5, pp. 14651487, 2015. [69] C. Du, K. Fu, J. Li, and H. He, Decoding visual neural representations by multimodal learning of brain-visual-linguistic features, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 9, pp. 10 76010 777, 2023. [70] L. S. Hamilton, Y. Oganian, and E. F. Chang, Topography of speechrelated acoustic and phonological feature encoding throughout the human core and parabelt auditory cortex, 2020. [71] Y. Luo and N. Mesgarani, Tasnet: Time-domain audio separation network for real-time, single-channel speech separation, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2018, pp. 696700. [72] W. Rao, Y. Fu, Y. Hu, X. Xu, Y. Jv, J. Han, Z. Jiang, L. Xie, Y. Wang, S. Watanabe et al., Conferencespeech challenge: Towards far-field multi-channel speech enhancement for video conferencing, in IEEE Automatic Speech Recognition and Understanding Workshop. IEEE, 2021, pp. 679686. [73] S. P. Panda, Automated speech recognition system in advancement of human-computer interaction, in International Conference on Computing Methodologies and Communication. IEEE, 2017, pp. 302306. [74] C. Lea, R. Vidal, A. Reiter, and G. D. Hager, Temporal convolutional networks: unified approach to action segmentation, in 14th European Conference on Computer Vision. Springer, 2016, pp. 4754. [75] Y. Yu, X. Si, C. Hu, and J. Zhang, review of recurrent neural networks: Lstm cells and network architectures, Neural Computation, vol. 31, no. 7, pp. 12351270, 2019. [76] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, Attention is all you need, in Advances in Neural Information Processing Systems, 2017, pp. 5998 6008. [77] S. E. Chazan, L. Wolf, E. Nachmani, and Y. Adi, Single channel voice separation for unknown number of speakers under reverberant and noisy settings, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2021, pp. 37303734. [78] Y. Lee, S. Choi, B.-Y. Kim, Z. Wang, and S. Watanabe, Boosting unknown-number speaker separation with transformer decoder-based attractor, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2024, pp. 446450. [79] S. Wisdom, E. Tzinis, H. Erdogan, R. Weiss, K. Wilson, and J. Hershey, Unsupervised sound separation using mixture invariant training, in Advances in neural information processing systems, vol. 33, 2020, pp. 38463857. [80] J. Neri, R. Badeau, and P. Depalle, Unsupervised blind source separation with variational auto-encoders, in 29th European Signal Processing Conference. IEEE, 2021, pp. 311315. [81] J. Zhang, C. Zorila, R. Doddipatla, and J. Barker, Teacher-student mixit for unsupervised and semi-supervised speech separation, arXiv preprint arXiv:2106.07843, 2021. [82] Z.-Q. Wang and S. Watanabe, Unssor: Unsupervised neural speech separation by leveraging over-determined training mixtures, in Advances in Neural Information Processing Systems, vol. 36, 2023, pp. 34 02134 042. [83] Y. Luo, Z. Chen, and N. Mesgarani, Speaker-independent speech separation with deep attractor network, IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 26, no. 4, pp. 787796, 2018. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 29 [84] Z.-Q. Wang, J. Le Roux, and J. R. Hershey, Alternative objective functions for deep clustering, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2018, pp. 686690. [85] Z.-Q. Wang, K. Tan, and D. Wang, Deep learning based phase reconstruction for speaker separation: trigonometric perspective, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2019, pp. 7175. [86] N. Zeghidour and D. Grangier, Wavesplit: End-to-end speech separation by speaker clustering, IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 28402849, 2021. [87] M. Kolbæk, D. Yu, Z.-H. Tan, and J. Jensen, Multitalker speech separation with utterance-level permutation invariant training of deep recurrent neural networks, IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 25, no. 10, pp. 19011913, 2017. [88] D. Stoller, S. Ewert, and S. Dixon, Wave-u-net: multi-scale neural network for end-to-end audio source separation, in International Society for Music Information Retrieval Conference, 2018, pp. 334 340. [89] Y. C. Subakan and P. Smaragdis, Generative adversarial source separation, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2018, pp. 2630. [90] L. Chen, M. Yu, Y. Qian, D. Su, and D. Yu, Permutation invariant training of generative adversarial network for monaural speech separation, in 19th Annual Conference of the International Speech Communication Association. ISCA, 2018, pp. 302306. [91] C. Li, L. Zhu, S. Xu, P. Gao, and B. Xu, Cbldnn-based speakerindependent speech separation via generative adversarial training, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2018, pp. 711715. [92] Y. Liu and D. Wang, Divide and conquer: deep casa approach to talker-independent monaural speaker separation, IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 27, no. 12, pp. 20922102, 2019. [93] L. Zhang, Z. Shi, J. Han, A. Shi, and D. Ma, Furcanext: End-toend monaural speech separation with dynamic gated dilated temporal convolutional networks, in International Conference on Multimedia Modeling. Springer, 2020, pp. 653665. [94] C. Deng, Y. Zhang, S. Ma, Y. Sha, H. Song, and X. Li, Conv-tassan: Separative adversarial network based on conv-tasnet, in 21st Annual Conference of the International Speech Communication Association. ISCA, 2020, pp. 26472651. [95] E. Tzinis, S. Venkataramani, Z. Wang, Y. C. Subakan, and P. Smaragdis, Two-step sound source separation: Training on learned latent targets, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2020, pp. 3135. [96] Y. Zhu, X. Zheng, X. Wu, W. Liu, L. Pi, and M. Chen, Dptcn-atpp: Multi-scale end-to-end modeling for single-channel speech separation, in International Conference on Communication and Information Systems. IEEE, 2021, pp. 3944. [97] Y. Zhao, C. Luo, Z.-J. Zha, and W. Zeng, Multi-scale group transformer for long sequence modeling in speech separation, in International Joint Conference on Artificial Intelligence. IJCAI, 2020, pp. 32513257. [98] M. W. Y. Lam, J. Wang, D. Su, and D. Yu, Sandglasset: light multigranularity self-attentive network for time-domain speech separation, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2021, pp. 57595763. [99] J. Rixen and M. Renz, Qdpn: Quasi-dual-path network for singlechannel speech separation, in Annual Conference of the International Speech Communication Association. ISCA, 2022, pp. 53535357. [100] , Sfsrnet: Super-resolution for single-channel audio source separation, in AAAI Conference on Artificial Intelligence. AAAI Press, 2022, pp. 11 22011 228. [101] S.-Q. Qian, L. Gao, H. Jia, and Q. Mao, Efficient monaural speech separation with multiscale time-delay sampling, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2022, pp. 68476851. [102] K. Li, R. Yang, and X. Hu, An efficient encoder-decoder architecture with top-down attention for speech separation, in International Conference on Learning Representations. OpenReview, 2023, pp. 110. [103] C. Li, L. Yang, W. Wang, and Y. Qian, Skim: Skipping memory lstm for low-latency real-time continuous speech separation, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2022, pp. 681685. [104] L. Yang, W. Liu, and W. Wang, Tfpsnet: Time-frequency domain path scanning network for speech separation, in IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 68426846. IEEE, 2022, [105] Z. Huang, S. Watanabe, S.-W. Yang, P. Garcıa, and S. Khudanpur, Investigating self-supervised learning for speech enhancement and separation, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2022, pp. 68376841. [106] R. Scheibler, Y. Ji, S.-W. Chung, J. Byun, S. Choe, and M.-S. Choi, Diffusion-based generative speech source separation, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2023, pp. 15. [107] X. Li, Y. Wang, Y. Sun, X. Wu, and J. Chen, Pgss: Pitch-guided speech separation, in Thirty-Seventh AAAI Conference on Artificial Intelligence. AAAI Press, 2023, pp. 13 13013 138. [108] C. Li, Y. Wu, and Y. Qian, Predictive skim: Contrastive predictive coding for low-latency online speech separation, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2023, pp. 15. [109] M. Hirano, K. Shimada, Y. Koyama, S. Takahashi, and Y. Mitsufuji, Diffusion-based signal refiner for speech separation, 2023. [110] S. Joseph and R. Rajan, Cycle gan-based audio source separation using timefrequency masking, Circuits, Systems, and Signal Processing, vol. 42, no. 2, pp. 11631180, 2023. [111] M. Fazel-Zarandi and W.-N. Hsu, Cocktail hubert: Generalized selfsupervised pre-training for mixture and single-source speech, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2023, pp. 15. [112] C. Chen, C.-H. H. Yang, K. Li, Y. Hu, P.-J. Ku, and E. S. Chng, neural state-space modeling approach to efficient speech separation, in Annual Conference of the International Speech Communication Association. ISCA, 2023, pp. 37843788. [113] J. Q. Yip, S. Zhao, D. Ng, E. S. Chng, and B. Ma, Towards audio codec-based speech separation, in Annual Conference of the International Speech Communication Association. ISCA, 2024, pp. 21902194. [114] J. Q. Yip, C. Y. Kwok, B. Ma, and E. S. Chng, Speech separation using neural audio codecs with embedding loss, in Asia Pacific Signal and Information Processing Association Annual Summit and Conference. IEEE, 2024, pp. 16. [115] W. Wang, Z. Pan, X. Li, S. Wang, and H. Li, Speech separation with pretrained frontend to minimize domain mismatch, IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2024. [116] H. Wang, J. Villalba, L. Moro-Velazquez, J. Hai, T. Thebaud, and N. Dehak, Noise-robust speech separation with fast generative correction, in Annual Conference of the International Speech Communication Association. ISCA, 2024, pp. 21652169. [117] A. Lakandri, Exploring gans with conv-tasnet: Adversarial training for speech separation, Masters thesis, Ohio University, 2024. [118] L. Della Libera, C. Subakan, M. Ravanelli, S. Cornell, F. Lepoutre, and F. Grondin, Resource-efficient separation transformer, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2024, pp. 761765. [119] J. Dong, X. Wang, and Q. Mao, Edsep: An effective diffusionbased method for speech source separation, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2025, pp. 15. [120] K. Li, G. Chen, R. Yang, and X. Hu, Spmamba: State-space model is all you need in speech separation, in IEEE International Conference on Multimedia and Exposition. IEEE, 2024. [121] D. Kitamura, N. Ono, H. Sawada, H. Kameoka, and H. Saruwatari, Determined blind source separation unifying independent vector analysis and nonnegative matrix factorization, IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 9, pp. 1626 1641, 2016. [122] Y. Z. Isik, J. Le Roux, Z. Chen, S. Watanabe, and J. R. Hershey, Single-channel multi-speaker separation using deep clustering, in Annual Conference of the International Speech Communication Association. ISCA, 2016, pp. 545549. [123] Y. Luo, Z. Chen, J. R. Hershey, J. Le Roux, and N. Mesgarani, Deep clustering and conventional networks for music separation: Stronger together, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2017, pp. 6165. [124] Y. Luo, Z. Chen, and N. Mesgarani, Speaker-independent speech separation with deep attractor network, IEEE/ACM Transactions on Audio, Speech and Language Processing, vol. 26, no. 4, pp. 787796, 2018. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 [125] E. Vincent, R. Gribonval, and C. Fevotte, Performance measurement in blind audio source separation, IEEE Transactions on Audio, Speech, and Language Processing, vol. 14, no. 4, pp. 14621469, 2006. [126] T. von Neumann, K. Kinoshita, C. Boeddeker, M. Delcroix, and R. Haeb-Umbach, Graph-pit: Generalized permutation invariant training for continuous separation of arbitrary numbers of speakers, in Annual Conference of the International Speech Communication Association. ISCA, 2021, pp. 34903494. [127] Z. Chen, T. Yoshioka, L. Lu, T. Zhou, Z. Meng, Y. Luo, J. Wu, X. Xiao, and J. Li, Continuous speech separation: Dataset and analysis, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2020, pp. 72847288. [128] A. Narayanan and D. Wang, Investigation of speech separation as front-end for noise robust speech recognition, IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 22, no. 4, pp. 826 835, 2014. [129] D. Raj, P. Denisov, Z. Chen, H. Erdogan, Z. Huang, M. He, S. Watanabe, J. Du, T. Yoshioka, Y. Luo et al., Integration of speech separation, diarization, and recognition for multi-speaker meetings: System description, comparison, and analysis, in IEEE Spoken Language Technology Workshop. IEEE, 2021, pp. 897904. [130] D. Griffin and J. Lim, Signal estimation from modified short-time fourier transform, IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 32, no. 2, pp. 236243, 1984. [131] Y. Luo and J. Yu, Music source separation with band-split rnn, IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 31, pp. 18931901, 2023. [132] D. Michelsanti, Z.-H. Tan, S.-X. Zhang, Y. Xu, M. Yu, D. Yu, and J. Jensen, An overview of deep-learning-based audio-visual speech enhancement and separation, IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 13681396, 2021. [133] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, wav2vec 2.0: framework for self-supervised learning of speech representations, in Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, 2020. [134] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, Hubert: Self-supervised speech representation learning by masked prediction of hidden units, IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 34513460, 2021. [135] A. Defossez, J. Copet, G. Synnaeve, and Y. Adi, High fidelity neural audio compression, Transactions on Machine Learning Research, vol. 2023. [136] N. Zeghidour, A. Luebs, A. Omran, J. Skoglund, and M. Tagliasacchi, Soundstream: An end-to-end neural audio codec, IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 30, pp. 495 507, 2021. [137] J. W. Yoon, B. J. Woo, and N. S. Kim, Hubert-ee: Early exiting hubert for efficient speech recognition, in Annual Conference of the International Speech Communication Association. ISCA, 2024, pp. 24002404. [138] C. Quan and X. Li, Spatialnet: Extensively learning spatial information for multichannel joint speech separation, denoising and dereverberation, IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 32, pp. 13101323, 2024. [139] , Multichannel long-term streaming neural speech enhancement for static and moving speakers, IEEE Signal Processing Letters, vol. 31, pp. 22952299, 2024. [140] A. Gu and T. Dao, Mamba: Linear-time sequence modeling with selective state spaces, in First Conference on Language Modeling, 2024. [141] T. Lei, Y. Zhang, S. I. Wang, H. Dai, and Y. Artzi, Simple recurrent units for highly parallelizable recurrence, in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2018, pp. 44704481. [142] Y. Luo, E. Ceolini, C. Han, S.-C. Liu, and N. Mesgarani, Fasnet: Lowlatency adaptive beamforming for multi-microphone audio processing, in IEEE Workshop on Automatic Speech Recognition and Understanding. IEEE, 2019. [143] M. Pariente, S. Cornell, A. Deleforge, and E. Vincent, Filterbank design for end-to-end speech separation, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2020, pp. 63646368. [144] O. Ronneberger, P. Fischer, and T. Brox, U-net: Convolutional networks for biomedical image segmentation, in Medical Image Computing and Computer-Assisted Intervention. Springer, 2015, pp. 234241. [145] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., Gpt-4 technical report, 2023. [146] A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan et al., Deepseek-v3 technical report, 2024. [147] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. Le, and R. Salakhutdinov, Transformer-xl: Attentive language models beyond fixed-length context, in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 2019, pp. 29782988. [148] D. Bang and H. Shim, Mggan: Solving mode collapse using manifoldguided training, in IEEE/CVF International Conference on Computer Vision Workshops. IEEE, 2021, pp. 23472356. [149] H. Zhang, V. Sindagi, and V. M. Patel, Image de-raining using conditional generative adversarial network, IEEE Transactions on Circuits and Systems for Video Technology, vol. 30, no. 11, pp. 3943 3956, 2019. [150] T. von Neumann, K. Kinoshita, C. Boeddeker, M. Delcroix, and R. Haeb-Umbach, Sa-sdr: novel loss function for separation of meeting style data, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2022, pp. 60226026. [151] , All-neural online source separation, counting, and diarization for meeting analysis, IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 31, pp. 576589, 2022. [152] C. H. Taal, R. C. Hendriks, R. Heusdens, and J. Jensen, An algorithm for intelligibility prediction of time-frequency weighted noisy speech, IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 7, pp. 21252136, 2011. [153] J. Jensen and C. H. Taal, An algorithm for predicting the intelligibility of speech masked by modulated noise maskers, IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 11, pp. 20092022, 2016. [154] C. K. A. Reddy, V. Gopal, R. Cutler, E. Beyrami, R. Cheng, H. Dubey, S. Matusevych, R. Aichner, A. Aazami, S. Braun, P. Rana, S. Srinivasan, and J. Gehrke, The interspeech 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results, in Annual Conference of the International Speech Communication Association. ISCA, 2020, pp. 24922496. [155] N.-C. Ristea, B. Naderi, A. Saabas, R. Cutler, S. Braun, and S. Branets, Icassp 2024 speech signal improvement challenge, IEEE Open Journal of Signal Processing, 2025. [156] L. Fadil, A. K. Abdul Hassan, and H. B. Alwan, review of isolating speakers in multi-speaker environments for human-computer interaction, in AIP Conference Proceedings, vol. 3232, no. 1. AIP Publishing, 2024. [157] D. Wang, Time-frequency masking for speech separation and its potential for hearing aid design, Trends in Amplification, vol. 12, no. 4, pp. 332353, 2008. [158] T. Union, International telecommunication union, Yearbook of Statistics 19912000, 2001. [159] Y. Hu and P. C. Loizou, Evaluation of objective quality measures for speech enhancement, IEEE Transactions on Audio, Speech, and Language Processing, vol. 16, no. 1, pp. 229238, 2007. [160] P. ITU-T, 835: Subjective test methodology for evaluating speech communication systems that include noise suppression algorithms, ITU-T Recommendation, 2003. [161] B. Series, Method for the subjective assessment of intermediate quality level of audio systems, International Telecommunication Union Radiocommunication Assembly, vol. 2, 2014. [162] J. G. Beerends, C. Schmidmer, J. Berger, M. Obermann, R. Ullmann, J. Pomy, and M. Keyhl, Perceptual objective listening quality assessment (polqa), the third generation itu-t standard for end-to-end speech quality measurement part itemporal alignment, Journal of the Audio Engineering Society, vol. 61, no. 6, pp. 366384, 2013. [163] M. Maciejewski, G. Wichern, E. McQuinn, and J. Le Roux, Whamr!: Noisy and reverberant single-channel speech separation, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2020, pp. 696700. [164] C. K. A. Reddy, H. Dubey, K. Koishida, A. A. Nair, V. Gopal, R. Cutler, S. Braun, H. Gamper, R. Aichner, and S. Srinivasan, Interspeech 2021 deep noise suppression challenge, in 22nd Annual Conference of the International Speech Communication Association. ISCA, 2021, pp. 27962800. [165] I. Ewert, M. Borsdorf, H. Li, and T. Schultz, Does the lombard effect matter in speech separation? introducing the lombard-grid-2mix dataset, in Annual Conference of the International Speech Communication Association, 2024, pp. 577581. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 31 [166] Z. Jin, Y. Yang, M. Shi, W. Kang, X. Yang, Z. Yao, F. Kuang, L. Guo, L. Meng, L. Lin et al., Libriheavymix: 20,000-hour dataset for single-channel reverberant multi-talker speech separation, asr and speaker diarization, in Annual Conference of the International Speech Communication Association. ISCA, 2024, pp. 702706. [167] L. Drude, J. Heitkaemper, C. Boeddeker, and R. Haeb-Umbach, Sms-wsj: Database, performance measures, and baseline recipe for multi-channel source separation and recognition, arXiv preprint arXiv:1910.13934, 2019. [168] S. Sivasankaran, E. Vincent, and D. Fohr, Analyzing the impact of speaker localization errors on speech separation for automatic speech recognition, in 28th European Signal Processing Conference. IEEE, 2021, pp. 346350. [169] Y. Fu, L. Cheng, S. Lv, Y. Jv, Y. Kong, Z. Chen, Y. Hu, L. Xie, J. Wu, and H. Bu, Aishell-4: An open source dataset for speech enhancement, separation, recognition and speaker diarization in conference scenario, in Annual Conference of the International Speech Communication Association, 2021, pp. 36653669. [170] J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus, and D. S. Pallett, Darpa timit acoustic-phonetic continuous speech corpus, NASA STI/Recon Technical Report N, vol. 93, 1993. [171] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, Librispeech: An asr corpus based on public domain audio books, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2015, pp. 52065210. [172] ITU-R, Algorithms to measure audio programme loudness and truepeak audio level, ITU-R Recommendation BS.1770-2, International Telecommunication Union Radiocommunication Assembly, March 2011. [173] M. McAuliffe, M. Socolof, S. Mihuc, M. Wagner, and M. Sonderegger, Montreal forced aligner: Trainable textspeech alignment using kaldi, in Annual Conference of the International Speech Communication Association. ISCA, 2017, pp. 498502. [174] N. Alghamdi, S. Maddock, R. Marxer, J. Barker, and G. J. Brown, corpus of audio-visual lombard speech with frontal and profile views, The Journal of the Acoustical Society of America, vol. 143, no. 6, pp. EL523EL529, 2018. [175] W. Kang, X. Yang, Z. Yao, F. Kuang, Y. Yang, L. Guo, L. Lin, and D. Povey, Libriheavy: 50,000 hours asr corpus with punctuation casing and context, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2024, pp. 10 99110 995. [176] F. Yu, S. Zhang, Y. Fu, L. Xie, S. Zheng, Z. Du, W. Huang, P. Guo, Z. Yan, B. Ma, X. Xu, and H. Bu, M2met: The icassp 2022 multi-channel multi-party meeting transcription challenge, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2022, pp. 61676171. [177] Y. Luo and J. Yu, Fra-rir: Fast random approximation of the imagesource method, in Annual Conference of the International Speech Communication Association. ISCA, 2023, pp. 38843888. [178] T. Afouras, J. S. Chung, A. Senior, O. Vinyals, and A. Zisserman, Deep audio-visual speech recognition, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 12, pp. 87178727, 2018. [179] A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niebner, M. Savva, S. Song, A. Zeng, and Y. Zhang, Matterport3d: Learning from rgbd data in indoor environments, in International Conference on 3D Vision, 2017. [180] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz et al., The kaldi speech recognition toolkit, in IEEE Workshop on Automatic Speech Recognition and Understanding. IEEE Signal Processing Society, 2011. [181] E. Manilow, P. Seetharaman, and B. Pardo, The northwestern university source separation library, in International Society for Music Information Retrieval Conference, 2018, pp. 297305. [182] Z. Ni and M. I. Mandel, Onssen: An open-source speech separation and enhancement library, 2019. [183] C. Li, J. Shi, W. Zhang, A. S. Subramanian, X. Chang, N. Kamo, M. Hira, T. Hayashi, C. Boeddeker, Z. Chen et al., Espnet-se: Endto-end speech enhancement and separation toolkit designed for asr integration, in IEEE Spoken Language Technology Workshop. IEEE, 2021, pp. 785792. [184] Y.-J. Lu, X. Chang, C. Li, W. Zhang, S. Cornell, Z. Ni, Y. Masuyama, B. Yan, R. Scheibler, Z.-Q. Wang, Y. Tsao, Y. Qian, and S. Watanabe, Espnet-se++: Speech enhancement for robust speech recognition, translation, and understanding, in 23rd Annual Conference of the International Speech Communication Association. 54585462. ISCA, 2022, pp. [185] M. Ravanelli, T. Parcollet, A. Moumen, S. de Langen, C. Subakan, P. Plantinga, Y. Wang, P. Mousavi, L. D. Libera, A. Ploujnikov, F. Paissan, D. Borra, S. Zaiem, Z. Zhao, S. Zhang, G. Karakasidis, S.- L. Yeh, P. Champion, A. Rouhe, R. Braun, F. Mai, J. Zuluaga-Gomez, S. M. Mousavi, A. Nautsch, X. Liu, S. Sagar, J. Duret, S. Mdhaffar, G. Laperriere, M. Rouvier, R. D. Mori, and Y. Esteve, Open-source conversational ai with SpeechBrain 1.0, 2024. [186] S. Zhao, Z. Pan, and B. Ma, Clearervoice-studio: Bridging advanced speech processing research and practical deployment, 2025. [187] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala, Pytorch: An imperative style, highperformance deep learning library, in Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, 2019, pp. 80248035. [188] C. Raffel, B. McFee, E. J. Humphrey, J. Salamon, O. Nieto, D. Liang, D. P. W. Ellis, and C. C. Raffel, Mir eval: transparent implementation of common mir metrics, in International Society for Music Information Retrieval Conference, 2014, pp. 367372. [189] H. Wang, C. Liang, S. Wang, Z. Chen, B. Zhang, X. Xiang, Y. Deng, and Y. Qian, Wespeaker: research and production oriented speaker embedding learning toolkit, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2023, pp. 15. [190] F.-R. Stoter, S. Uhlich, A. Liutkus, and Y. Mitsufuji, Open-unmix - reference implementation for music source separation, Journal of Open Source Software, vol. 4, no. 41, p. 1667, 2019. [191] Z. Shen, M. Zhang, H. Zhao, S. Yi, and H. Li, Efficient attention: Attention with linear complexities, in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2021, pp. 35313539. [192] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret, Transformers are rnns: Fast autoregressive transformers with linear attention, in Proceedings of the 37th International Conference on Machine Learning, 2020, pp. 51565165. [193] S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma, Linformer: Selfattention with linear complexity, 2020. [194] S. Mirsamadi, E. Barsoum, and C. Zhang, Automatic speech emotion recognition using recurrent neural networks with local attention, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2017, pp. 22272231. [195] C.-F. Chen, R. Panda, and Q. Fan, Regionvit: Regional-to-local attention for vision transformers, in The Tenth International Conference on Learning Representations, 2022. [196] J. Ho, N. Kalchbrenner, D. Weissenborn, and T. Salimans, Axial attention in multidimensional transformers, 2019. [197] J. M. J. Valanarasu, P. Oza, I. Hacihaliloglu, and V. M. Patel, Medical transformer: Gated axial-attention for medical image segmentation, in Medical Image Computing and Computer Assisted Intervention MICCAI 2021: 24th International Conference. Springer, 2021, pp. 3646. [198] M. Stamenovic, N. L. Westhausen, L.-C. Yang, C. Jensen, and A. Pawlicki, Weight, block or unit? exploring sparsity tradeoffs for speech enhancement on tiny neural accelerators, 2021. [199] J. Xu, J. Yu, X. Liu, and H. Meng, Mixed precision dnn quantization for overlapped speech separation and recognition, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2022, pp. 72977301. [200] Y. Wu, C. Li, and Y. Qian, Light-weight visualvoice: Neural network quantization on audio visual speech separation, in IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops. IEEE, 2023, pp. 15. [201] E. W. Healy, H. Taherian, E. M. Johnson, and D. Wang, causal and talker-independent speaker separation/dereverberation deep learning algorithm: Cost associated with conversion to real-time capable operation, The Journal of the Acoustical Society of America, vol. 150, no. 5, pp. 39763986, 2021. [202] H. Schroter, T. Rosenkranz, A.-N. Escalante-B, and A. Maier, Low latency speech enhancement for hearing aids using deep filtering, IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 30, pp. 27162728, 2022. [203] X. Mao, Q. Li, H. Xie, R. Y. Lau, Z. Wang, and S. Paul Smolley, Least squares generative adversarial networks, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2017, pp. 27942802. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 32 [224] R. Gu and Y. Luo, Rezero: Region-customizable sound extraction, IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2024. [225] D. Hu, Y. Wei, R. Qian, W. Lin, R. Song, and J.-R. Wen, Class-aware sounding objects localization via audiovisual correspondence, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 12, pp. 98449859, 2021. [226] J. Wu, Z. Chen, S. Chen, Y. Wu, T. Yoshioka, N. Kanda, S. Liu, and J. Li, Investigation of practical aspects of single channel speech separation for asr, in Annual Conference of the International Speech Communication Association. ISCA, 2021, pp. 30663070. [227] P. Mowlaee, R. Saeidi, M. G. Christensen, Z.-H. Tan, T. Kinnunen, P. Franti, and S. H. Jensen, joint approach for single-channel speaker identification and speech separation, IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, no. 9, pp. 25862601, 2012. [228] T. J. Park, N. Kanda, D. Dimitriadis, K. J. Han, S. Watanabe, and S. Narayanan, review of speaker diarization: Recent advances with deep learning, Computer Speech & Language, vol. 72, p. 101317, 2022. [229] J. Shi, X. Chang, S. Watanabe, and B. Xu, Train from scratch: Singlestage joint training of speech separation and recognition, Computer Speech & Language, vol. 76, p. 101387, 2022. [230] Z.-Q. Wang and D. Wang, Joint training of speech separation, filterbank and acoustic model for robust automatic speech recognition, in Annual Conference of the International Speech Communication Association, 2015, pp. 28392843. [231] S. Berger, P. Vieting, C. Boddeker, R. Schluter, and R. Haeb-Umbach, Mixture encoder for joint speech separation and recognition, in 24th Annual Conference of the International Speech Communication Association. ISCA, 2023, pp. 35273531. [232] X. Jiang, Y. A. Li, A. N. Florea, C. Han, and N. Mesgarani, Speech slytherin: Examining the performance and efficiency of mamba for speech separation, recognition, and synthesis, in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2025, pp. 15. [233] S. Cornell, J.-W. Jung, S. Watanabe, and S. Squartini, One model to rule them all? towards end-to-end joint speaker diarization and speech recognition, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2024, pp. 11 85611 860. [234] L. Meng, J. Kang, Y. Wang, Z. Jin, X. Wu, X. Liu, and H. Meng, Empowering whisper as joint multi-talker and target-talker speech recognition system, in Annual Conference of the International Speech Communication Association. ISCA, 2024, pp. 46534657. [235] S. Maiti, Y. Ueda, S. Watanabe, C. Zhang, M. Yu, S.-X. Zhang, and Y. Xu, Eend-ss: Joint end-to-end neural speaker diarization and speech separation for flexible number of speakers, in 2022 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2023, pp. 480487. [236] J. Kalda, C. Pages, R. Marxer, T. Alumae, and H. Bredin, Pixit: Joint training of speaker diarization and speech separation from real-world multi-speaker recordings, in The Speaker and Language Recognition Workshop. ISCA, 2024, pp. 115122. [237] C. Boeddeker, A. S. Subramanian, G. Wichern, R. Haeb-Umbach, and J. Le Roux, Ts-sep: Joint diarization and separation conditioned on estimated speaker embeddings, IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 32, pp. 11851197, 2024. [238] A. Polok, D. Klement, M. Wiesner, S. Khudanpur, J. ˇCernocky, and L. Burget, Target speaker asr with whisper, in ICASSP 20252025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2025, pp. 15. [239] P. Guo, X. Chang, H. Lv, S. Watanabe, and L. Xie, Sqwhisper: Speaker-querying based whisper model for target-speaker asr, IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2024. [240] H. Ma, R. Chen, X.-L. Zhang, J. Liu, and X. Li, Enhancing intelligibility for generative target speech extraction via joint optimization with target speaker asr, IEEE Signal Processing Letters, 2025. [204] Z.-C. Fan, Y.-L. Lai, and J.-S. R. Jang, Svsgan: Singing voice separation via generative adversarial network, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2018, pp. 726730. [205] J. Ho, A. Jain, and P. Abbeel, Denoising diffusion probabilistic models, in Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, 2020. [206] K. Zhang, J. Li, S. Wang, Y. Wei, Y. Wang, Y. Wang, and H. Li, Multilevel speaker representation for target speaker extraction, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2025, pp. 15. [207] S. He, H. Zhang, W. Rao, K. Zhang, Y. Ju, Y. Yang, and X. Zhang, Hierarchical speaker representation for target speaker extraction, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2024, pp. 10 36110 365. [208] K. ˇZmolıkova, M. Delcroix, T. Ochiai, K. Kinoshita, J. ˇCernocky, and D. Yu, Neural target speech extraction: An overview, IEEE Signal Processing Magazine, vol. 40, no. 3, pp. 829, 2023. [209] S. He, H. Li, and X. Zhang, Speakerfilter: Deep learning-based target speaker extraction using anchor speech, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2020, pp. 376380. [210] M. Ge, C. Xu, L. Wang, E. S. Chng, J. Dang, and H. Li, L-spex: Localized target speaker extraction, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2022, pp. 7287 7291. [211] R. Gu, L. Chen, S.-X. Zhang, J. Zheng, Y. Xu, M. Yu, D. Su, Y. Zou, and D. Yu, Neural spatial filter: Target speaker speech separation assisted with directional information, in 20th Annual Conference of the International Speech Communication Association. ISCA, 2019, pp. 42904294. [212] Z. Pan, R. Tao, C. Xu, and H. Li, Muse: Multi-modal target speaker extraction with visual cues, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2021, pp. 6678 6682. [213] K. ˇZmolıkova, M. Delcroix, K. Kinoshita, T. Ochiai, T. Nakatani, ˇCernocky, Speakerbeam: Speaker aware neural L. Burget, and J. network for target speaker extraction in speech mixtures, IEEE Journal of Selected Topics in Signal Processing, vol. 13, no. 4, pp. 800814, 2019. [214] P. Shen, K. Chen, S. He, P. Chen, S. Yuan, H. Kong, X. Zhang, and Z.- Q. Wang, Listen to extract: Onset-prompted target speaker extraction, 2025. [215] J. Ao, M. S. Yıldırım, R. Tao, M. Ge, S. Wang, Y. Qian, and H. Li, Used: Universal speaker extraction and diarization, IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2024. [216] C. Xu, W. Rao, E. S. Chng, and H. Li, Spex: Multi-scale time domain speaker extraction network, IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, pp. 13701384, 2020. [217] M. Ge, C. Xu, L. Wang, E. S. Chng, J. Dang, and H. Li, Multi-stage speaker extraction with utterance and frame-level reference signals, in IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2021, pp. 61096113. [218] Q. Wang, H. Muckenhirn, K. W. Wilson, P. Sridhar, Z. Wu, J. R. Hershey, R. A. Saurous, R. J. Weiss, Y. Jia, and I. Lopez-Moreno, Voicefilter: Targeted voice separation by speaker-conditioned spectrogram masking, in Annual Conference of the International Speech Communication Association. ISCA, 2019, pp. 27282732. [219] A. Pandey and D. Wang, On cross-corpus generalization of deep learning based speech enhancement, IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, pp. 24892499, 2020. [220] J. Li, M. Ge, Z. Pan, L. Wang, and J. Dang, Vcse: Time-domain visualcontextual speaker extraction network, in Annual Conference of the International Speech Communication Association. ISCA, 2022, pp. 906910. [221] K. Li, R. Yang, F. Sun, and X. Hu, Iianet: An intraand inter-modality attention network for audio-visual speech separation, in Forty-first International Conference on Machine Learning, 2024. [222] S. Pegg, K. Li, and X. Hu, Rtfs-net: Recurrent time-frequency modelling for efficient audio-visual speech separation, in The Twelfth International Conference on Learning Representations, 2024. [223] , Tdfnet: An efficient audio-visual speech separation model with top-down fusion, in 13th International Conference on Information Science and Technology (ICIST). IEEE, 2023, pp. 243252. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 33 Kai Li (Student Member, IEEE) received the B.S. degree from the Department of Computer Technology and Application, Qinghai University, Xining, China, in 2020, and the M.S. degree from the Department of Computer Science and Technology, Tsinghua University, Beijing, China, in 2024, where he is currently pursuing the Ph.D. degree under the supervision of Prof. Xiaolin Hu. His current research interests include speech/music separation, multi-modal speech separation, and audio large language models. He serves as reviewer for several prestigious conferences and journals, including NeurIPS, ICLR, ICASSP, Interspeech, AAAI, TASLP, and TPAMI. Guo Chen with Bachelors and Masters degrees in Computer Science and Technology from Tsinghua University, currently works at Hypergryph, where involved in the development of voice-related features in game applications. My research focused on speech technologies, including speaker separation, automatic speech recognition, and speech synthesis. Wendi Sang received the B.E. degree from Bengbu University, China, in 2021. He is currently pursuing the M.S. degree under the joint supervision of Qinghai University and Tsinghua University, China, with the School of Computer Technology and Application, Qinghai University. His research interests include audio processing and audio-visual learning, especially multi-modal speech separation. Yi Luo is currently an independent researcher. Yi obtained his Ph.D. degree from Columbia University in 2021 and then worked as senior research scientist at Tencent AI Lab between 2021 and 2024. His main research focuses are speech and audio understanding and generation, including source separation, microphone array processing, and speech and audio generation. Yi received the 2021 IEEE Signal Processing Society Best Paper Award with his work on end-to-end speech separation. Zhuo Chen serves as research manager at ByteDance, leading team focused on audio and speech generation, multi-media interaction and continual learning. Before joining ByteDance, Zhuo obtained his PhD from Columbia University in 2017 and then worked as principal applied scientist at Microsoft. He has published over 150 research papers and patents, advancing wide range of speech tasks including speech recognition and translation, speech separation and enhancement, speaker identification and diarization, multi-channel processing and beamforming, speech self supervised learning and speech generation. Shuai Wang (Member, IEEE) received his Ph.D. degree from Shanghai Jiao Tong University (SJTU), Shanghai, China, in 2020. He is currently an Associate Professor with Nanjing University, Suzhou, China. He was formerly Research Scientist with the Shenzhen Research Institute of Big Data, The Chinese University of Hong Kong (Shenzhen), where he continues to hold an adjunct appointment. Prior to that, he worked at Tencents Lightspeed & Quantum Studios as Senior Application Research Scientist, where he led the speech team in developing game-oriented speech technologies. Dr. Wangs research interests include speaker modeling, target speaker separation, and speech generation. He has authored over 70 papers in premier conferences and journals in the speech processing field. He was the recipient of the IEEE Ganesh N. Ramaswamy Memorial Student Grant Award at ICASSP 2018. He was also the main contributor to the champion systems of VoxSRC 2019 and DIHARD 2019. He is member of ISCA, SPS and IEEE, serving as regular reviewer for conferences and journals including ICASSP, INTERSPEECH, ASRU, TASLP and CSL. He initiated the popular Wespeaker and WeSep projects, utilized by numerous research groups across academia and industry. Shulin He received the B.Eng. degree in Computer Science from Inner Mongolia University (IMU), Hohhot, China, in 2019, and the Ph.D. degree in Computer Science and technology from IMU in 2025. From June 2021 to September 2021, he was Visiting Researcher with the Key Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China. From May 2022 to May 2023, he joined the Tencent RhinoBird Elite Talent Program as joint Ph.D. trainee and received its Excellent Student Award. Between October 2023 and April 2024, he was Visiting Ph.D. Student in the Division of Emerging Interdisciplinary Areas, Hong Kong University of Science and Technology. He is currently Postdoctoral Researcher with the SUSTech Audio Intelligence Lab, Department of Computer Science and Engineering, Southern University of Science and Technology (SUSTech), Shenzhen, China. His research interests include target speaker extraction, speech enhancement, and deep learning for speech and audio, with focus on advancing robust speech technology in complex acoustic environments. Zhong-Qiu Wang received the B.E. degree in computer science and technology from Harbin Institute of Technology, Harbin, China, in 2013, and the M.S. and Ph.D. degrees in computer science and engineering from The Ohio State University, Columbus, OH, USA, in 2017 and 2020, respectively. He is currently tenure-track associate professor in the Department of Computer Science and Engineering at Southern University of Science and Technology, Shenzhen, Guangdong, China. He was Postdoctoral Research Associate with Carnegie Mellon University, Pittsburgh, PA, USA, from 2021 to 2024, and visiting research scientist at Mitsubishi Electric Research Laboratories, Cambridge, MA, USA, from 2020 to 2021. His research interests include speech separation, robust automatic speech recognition, microphone array processing, and deep learning, aiming at solving the cocktail party problem. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 34 Andong Li (Member, IEEE) received the B.S. degree in information engineering from Southeast University, Nanjing, China, in 2018, and the Ph.D. degree in signal and information processing from the Institute of Acoustics, Chinese Academy of Sciences, Beijing, China, in 2023. From 2023 to 2024, he was Senior Researcher with Tencent AI Lab. He is currently an Associate Researcher with the Institute of Acoustics, Chinese Academy of Sciences. His research interests include speech enhancement, audio coding, array signal processing, and audio generation. He is also an active reviewer for multiple leading conferences and journals, such as INTERSPEECH, ICASSP, ICLR, ACMMM, IJCAI, AAAI, ASRU, IEEE Signal Processing Letters, Speech Communication, Neural Networks, Pattern Recognition, and IEEE/ACM Transactions on Audio, Speech, and Language Processing. Zhiyong Wu (Member, IEEE) received the B.S. and Ph.D. degrees in computer science and technology from Tsinghua University, Beijing, China, in 1999 and 2005, respectively. From 2005 to 2007, he was Postdoctoral Fellow with the Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong (CUHK), Hong Kong. He then joined the Graduate School at Shenzhen (now Shenzhen International Graduate School), Tsinghua University, Shenzhen, China, and is currently Professor. He is also Coordinator with the Tsinghua-CUHK Joint Research Center for Media Sciences, Technologies and Systems. His research interests include intelligent speech interaction, more specially, speech processing, audiovisual bimodal modeling, text-to-audio-visual-speech synthesis, and natural language understanding and generation. He is Member of International Speech Communication Association (ISCA) and China Computer Federation (CCF). Xiaolin Hu (S01-M08-SM13) received B.E. and M.E. degrees in automotive engineering from the Wuhan University of Technology, Wuhan, China, in 2001 and 2004, respectively, and Ph.D. degree in automation and computer-aided engineering from the Chinese University of Hong Kong, Hong Kong, in 2007. He is currently an Associate Professor at the Department of Computer Science and Technology, Tsinghua University, Beijing, China. His current research interests include deep learning and computational neuroscience. At present, he is an Associate Editor of the IEEE Transactions on Pattern Analysis and Machine Intelligence and Cognitive Neurodynamics. Previously he was an Associate Editor of the IEEE Transactions on Image Processing, IEEE Transactions on Neural Networks and Learning Systems."
        }
    ],
    "affiliations": [
        "ByteDance",
        "Department of Computer Science and Technology, Tsinghua University, Beijing, China",
        "Institute of Acoustics, Chinese Academy of Sciences, Beijing, China",
        "Nanjing University, Suzhou, China",
        "School of Computer Technology and Application, Qinghai University, Xining, China",
        "Shenzhen International Graduate School, Tsinghua University, Shenzhen, China",
        "Shenzhen, China",
        "Southern University of Science and Technology, Shenzhen, China"
    ]
}