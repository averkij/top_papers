{
    "paper_title": "Distilled-3DGS:Distilled 3D Gaussian Splatting",
    "authors": [
        "Lintao Xiang",
        "Xinkai Chen",
        "Jianhuang Lai",
        "Guangcong Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "3D Gaussian Splatting (3DGS) has exhibited remarkable efficacy in novel view synthesis (NVS). However, it suffers from a significant drawback: achieving high-fidelity rendering typically necessitates a large number of 3D Gaussians, resulting in substantial memory consumption and storage requirements. To address this challenge, we propose the first knowledge distillation framework for 3DGS, featuring various teacher models, including vanilla 3DGS, noise-augmented variants, and dropout-regularized versions. The outputs of these teachers are aggregated to guide the optimization of a lightweight student model. To distill the hidden geometric structure, we propose a structural similarity loss to boost the consistency of spatial geometric distributions between the student and teacher model. Through comprehensive quantitative and qualitative evaluations across diverse datasets, the proposed Distilled-3DGS, a simple yet effective framework without bells and whistles, achieves promising rendering results in both rendering quality and storage efficiency compared to state-of-the-art methods. Project page: https://distilled3dgs.github.io . Code: https://github.com/lt-xiang/Distilled-3DGS ."
        },
        {
            "title": "Start",
            "content": "Distilled-3DGS: Distilled 3D Gaussian Splatting Lintao Xiang1,2 Xinkai Chen2 Jianhuang Lai3 Guangcong Wang2 2Vision, Graphics, and Group, Great Bay University, 3Sun Yat-Sen University 1The University of Manchester, 5 2 0 2 9 1 ] . [ 1 7 3 0 4 1 . 8 0 5 2 : r Figure 1. Compared with state-of-the-art 3DGS-based methods on Mip360 dataset, our Distilled-3DGS introduces novel lightweight framework for high-quality view synthesis, achieving better detail preservation with lower storage."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction 3D Gaussian Splatting (3DGS) has exhibited remarkable efficacy in novel view synthesis (NVS). However, it suffers from significant drawback: achieving high-fidelity rendering typically necessitates large number of 3D Gaussians, resulting in substantial memory consumption and storage requirements. To address this challenge, we propose the first knowledge distillation framework for 3DGS, featuring various teacher models, including vanilla 3DGS, noiseaugmented variants, and dropout-regularized versions. The outputs of these teachers are aggregated to guide the optimization of lightweight student model. To distill the hidden geometric structure, we propose structural similarity loss to boost the consistency of spatial geometric distributions between the student and teacher model. Through comprehensive quantitative and qualitative evaluations across diverse datasets, the proposed Distilled-3DGSa simple yet effective framework without bells and whistlesachieves promising rendering results in both rendering quality and storage efficiency compared to state-of-theart methods. Project page: https://distilled3dgs.github.io. Code: https://github.com/lt-xiang/Distilled-3DGS. Equal contribution. Corresponding author. 1 Novel view synthesis (NVS) is fundamental task [40] in computer vision and computer graphics, serving as cornerstone in many applications, e.g., VR/AR and autonomous driving. The goal of NVS is to generate photorealistic images from novel, previously unseen viewpoints. This process typically begins by constructing 3D representation from set of existing 2D observations. 3D Gaussian Splatting (3DGS) [13] has recently demonstrated remarkable effectiveness in novel view synthesis. This approach employs point-based representation augmented with 3D Gaussian attributes and utilizes rasterization-based rendering pipeline to synthesize images. However, 3DGS necessitates large number of 3D Gaussians to ensure high-fidelity image rendering, particularly in the presence of complex scenes. This limits their applicability on platforms and devices with constrained computational resources and limited memory capacity. On the other hand, knowledge distillation [3] has proven effective in compressing neural networks across various vision tasks. However, applying it to 3D Gaussian Splatting (3DGS) introduces unique challenges. First, 3DGS is an explicit and unstructured representation composed of variable 3D Gaussians, lacking the consistent latent feature spaces typically leveraged in conventional KD. Second, the Gaussian primitives are scene-dependent and unordered, preventing straightforward correspondence between teacher and student components. Third, since rendering outputs are view-dependent and non-differentiable w.r.t. individual Gaussians, designing stable and informative distillation losses becomes nontrivial. These challenges necessitate careful design of both teacher model ensembles and geometry-aware distillation strategies, as introduced in our Distilled-3DGS framework. To address this challenge, we propose lightweight 3D Gaussian representation framework based on knowledge distillation, termed Distilled-3DGS. This approach enhances the performance of compact student model by distilling knowledge from multiple complex teacher models. The overall pipeline of Distilled-3DGS comprises two main stages: multi-teacher training and student training via distillation. In the multi-teacher training stage, we begin by training standard 3DGS model. Subsequently, we introduce random perturbations and dropout strategies separately to obtain two additional diverse teacher models. During the distillation-based student training stage, we first aggregate predictions from the teacher ensemble to synthesize pseudo image. The student model is then supervised by enforcing similarity between its rendered output and this pseudo image. This strategy effectively transfers rich knowledge priors from the teacher models, providing more comprehensive and robust supervisory signal for optimizing the student model. In the context of Distilled-3DGS, the teacher model typically contains dense and high-quality point clouds, while the student model is compressed to obtain much sparser points due to efficiency or deployment considerations. Despite its sparsity, the student model is trained to reconstruct the same underlying 3D scene as the teacher. So it is expected to preserve the essential spatial layout and local geometric patterns present in the teacher model. To facilitate this, we propose spatial distribution distillation strategy that guides the student to align with the teachers point distribution in space. Rather than enforcing exact point-wise correspondence, this structure-aware supervision encourages the student to learn how the teacher organizes points, focusing on global and local geometric consistency. In this way, structural knowledge from the teacher can be effectively and comprehensively distilled into the student model. In summary, our main contributions are as follows: 1) We propose novel distillation-based 3DGS framework, termed as Distilled-3DGS, which is the first method to leverage multi-teacher knowledge priors to optimize 3DGS and boost rendering quality and storage efficiency. 2) We propose spatial distribution consistency distillation to enable the student model to learn similar geometric structure distributions from the teacher model. 3) Extensive experiments on several real-world datasetsincluding Mip-NeRF 360, Tanks&Temples, and Deep Blendingdemonstrate that the proposed Distilled-3DGS achieves promising performance in both rendering quality and efficiency compared to existing methods. 2. Related Work 3D Representation. Radiance fields have been extensively employed for 3D scene reconstruction, particularly in the context of novel view synthesis. Neural Radiance Fields (NeRFs) have achieved remarkable progress by learning neural volumetric representations of 3D scenes, enabling high-fidelity image synthesis via volumetric rendering techniques. After that, many works have focused on improving the rendering quality [1, 2] and accelerating the efficiency [6, 11, 31, 39] of NeRFs. Nevertheless, NeRF-based approaches continue to rely on numerous MLP queries during rendering, thereby limiting their applicability in scenarios with real-time constraints. To enhance the training and rendering efficiency, Plenoxels [6] improve NeRF efficiency by optimizing sparse voxel grid and removing the need for MLPs, while Instant NGP [39] uses hash-grid encodings to boost expressivity. However, despite these improvements, grid-based methods still struggle to achieve real-time rendering. Recently, 3D Gaussian Splatting (3DGS) [13] has gained significant attention as an efficient and effective approach for 3D scene representation. 3DGS represents 3D scenes explicitly with millions of anisotropic Gaussians and utilizes differentiable rasterization, enabling real-time, photorealistic view synthesis. When 3DGS overfits the scene by optimizing Gaussian properties, it typically produces many redundant Gaussians, thereby reducing rendering efficiency and substantially increasing memory usage. To tackle these issues, several subsequent approaches have aimed to prune redundant Gaussians based on handcrafted importance criteria. Mini-Splatting [5] addresses overlapping and reconstruction artifacts by employing blur splitting, depth reinitialization, and stochastic sampling. Radsplatting [24] enhances robustness by applying max operator to derive importance scores from ray contributions. Taming-3DGS [21] leverages pixel saliency and gradient information for selective densification, while LP-3DGS [38] utilizes learned binary mask for efficient Gaussian pruning. Additionally, Scaffold-GS [20] proposes structured dual-layer hierarchical scene representation to better regulate the distribution of 3D Gaussian primitives. Overall, the aforementioned methods that prioritize efficiency generally achieve faster performance, but this often comes at the expense of rendering quality compared to the standard 3DGS. Conversely, approaches that focus on enhancing rendering quality tend to demand substantially higher computational resources. To address this trade-off, we propose knowledge distillation-based 3DGS framework that simultaneously improves storage efficiency and rendering fidelity. Knowledge Distillation.Knowledge distillation (KD) 2 to transfers knowledge from large teacher model compact student model. Initially proposed for model compression [3, 10], KD began with matching teacher outputs and was later extended to mimic intermediate representations [27, 32]. KD has since been applied to various tasks, including detection [18, 35],segmentation [12, 19], and generation [17, 36]. To overcome the limitations of singleteacher KD, multi-teacher distillation (MKD) is proposed to aggregate diverse knowledge from multiple teachers. While early approaches assign equal weights [7, 30], recent methods adopt adaptive strategies, such as entropy-based weighting (EB-KD [15]) and confidence-aware distillation (CA-MKD [33]). MMKD [34] further introduces metalearning to jointly distill features and logits. These methods often rely on CNNs for structured feature spaces, facilitating effective alignment via soft labels or intermediate supervision. However, extending KD to 3D Gaussian Splatting (3DGS) poses new challenges. 3DGS uses an explicit and unstructured representation composed of variable set of discrete Gaussian primitives. These primitives are unordered, scene-dependent, and lack shared latent space, making it infeasible to directly align elements between teacher and student. As result, existing KD strategies must be fundamentally rethought to accommodate the unique properties of 3DGS. Based on the above analysis, we propose to utilize multiple pre-trained 3DGS teacher models to render high-quality images as supervision targets and optimize the student model. Besides, we propose spatial distribution distillation strategy that guides the student to align with the teachers point distribution in space. 3. Method In this section, we present Distilled-3DGS, an efficient 3D Gaussian Splatting framework that distills knowledge from powerful teacher models to small student model. 3.1. Preliminaries 3DGS [13] is cutting-edge method for novel view synthesis, which fundamentally depends on an explicit pointbased representation to achieve high-fidelity rendering from arbitrary viewpoints. Specifically, 3DGS models scene as set of Gaussian distributions. The ith Gaussian primitive is denoted as Gi = (µi, Σi, oi, fi), where µi is the 3D position, Σi is the covariance matrix, fi represents spherical harmonics (SH) coefficients associated with the Gaussian, and oi indicates opacity. The effect of the ith Gaussian primitive at position is represented by Gi(x) = 1 (xµi), where Σi can be factorized as Σi = RSST RT , with as rotation matrix and as scaling matrix, both being learnable. Subsequently, the Gaussians are mapped to the 2D image plane via the projection matrix , resulting in the projected 2D covariance matrix 2 (xµi)T Σ1 = JW ΣiW , where represents the Jacobian of the Σ affine projection. The pixel color is computed through alpha blending as follows: = (cid:88) i=1 i1 (cid:89) ciαi (1 αj), j=1 (1) where is the number of Gaussians covering the pixel, the color ci is derived from the spherical harmonics (SH) coefficients of each Gaussian, while αi is determined by the projected 2D covariance matrices Σ and the associated opacity oi. The Gaussian parameters are optimized using photometric loss [13] function, with the posed training images providing the ground-truth supervision. 3.2. Distilled 3D Gaussian Splatting 3DGS has enabled highly detailed and accurate 3D scene reconstruction, yet such state-of-the-art models are often extremely large and computationally expensive, limiting their practicality in real-time and resource-constrained scenarios. Knowledge distillation (KD) has emerged as highly effective and popular approach for model compression in image classification, semantic segmentation [19], and object detection [4]. Inspired by these observations, one could ask if knowledge distillation works for 3DGS. Different from conventional KD in neural networks, it is an explicit 3D representation with variable unstructured 3D Gaussians, which remains unexplored. To address this problem, we first provide an overview of the proposed Distilled-3DGS, and then detail the design of diverse teacher models and the distillation method, as discussed in the following. 3.3. Overview of Distill-3DGS As shown in Fig.2, we firstly train three independent 3DGS models with diverse strategies to obtain cumbersome yet high-quality teacher models with millions of Gaussian primitives, optimized by the standard photometric loss. Then we leverage the optimized teacher 3DGS representation to generate pseudo labels by fusing multiple teachers outputs. In the training process of the student model, pseudo labels are leveraged to transfer prior knowledge from multiple teachers to single student. To obtain lightweight student model, we prune the number of Gaussians based on the importance score proposed in Mini-Splatting [5]. To distill knowledge hidden in geometric structure of 3D Gaussians, we propose structural knowledge distillation for unstructured 3D Gaussians to encourage the similar spatial distribution between teacher and student models. 3.4. Training Diverse Teacher models To provide the student model with richer supervision signals and facilitate better understanding of 3D scene structures and details, we train the base 3DGS model multiple 3 Figure 2. The architecture of multi-teacher knowledge distillation framework for 3DGS. It consists of two stages. First, standard teacher model Gstd is trained, along with two variants: Gperb with random perturbation and Gdrop with random dropout. Then, pruned student model Gstd is supervised by the outputs of these teachers. Additionally, spatial distribution distillation strategy is introduced to help the student learn structural patterns from the teachers. times using diverse strategies to enhance the robustness and generalization ability of the teacher models. Regular training. First, we train vanilla 3DGS model Gstd with the same settings in [13]. The training loss is defined as: Lcolor = (1 λ)L1( ˆI, Igt) + λLD-SSIM( ˆI, Igt) (2) Feature Perturbation. Then, we train 3DGS model Gperb with random perturbations on Gaussian parameters, following the same optimization and density control strategies introduced in 3DGS. At each training iteration t, each Gaussian is perturbed as: Gt perb = Gt std + δt, (3) where random noises are added to corresponding Gaussian parameters including 3D positions µp, 3D rotations Rp, scales Sp, and opacities op. Since the representation of rotation as 3D matrix is discontinuous, we instead perturb its continuous 6D representation as : ˆRt = 1 (cid:0)f (Rt p) + δR where and 1 are the forward and inverse mappings between the rotation matrix and its 6D representation. By R6 δt (cid:1) , (4) introducing parameter perturbations during training, the model is compelled to learn scene structures that are less dependent on the precise positions and shapes of Gaussian primitives, thereby enhancing its generalization capability. Random Dropout. Dropout [8, 26] is recognized as one of the most effective techniques for improving model robustness by randomly deactivating subset of neurons during training. Inspired by the remarkable success of dropout, we introduce Random Dropout Strategy to further enhance both the robustness and redundancy of the representation of model Gdrop. Specifically, during training, each Gaussian primitive is randomly deactivated with probability p, while the remaining primitives are optimized to fit the observed views. During inference, all Gaussian primitives are activated to facilitate novel view synthesis. By randomly dropping subset of Gaussian primitives during training, our approach encourages the model to learn collaborative and distributed scene representation, rather than relying on limited set of critical Gaussian primitives. Inspired by [25], the dropping rate rt is updated based on the current iteration index as follows: rt = rinit (t t0)/(t1 t0), (5) where t0 and t1 are the starting and end iterations of in4 troducing random dropout strategy. rinit is the initial drop rate. 3.5. Training Efficient Student Model Knowledge distillation is technique that transfers knowledge from larger teacher model to smaller, faster student model. This approach is particularly useful when deploying deep neural networks in resource-constrained environments. The student model, trained under the guidance of the teacher, can achieve comparable performance with significantly fewer parameters. This process mainly consists of pseudo label generation and student training. Pseudo labeling with teacher model. By evaluating the optimized diverse teachers Gstd, Gperb and Gdrop, we can render per-view image denoted as Istd, Iperb and Idrop, these rendered images as prior knowledge are further aggregated by average strategy to generate pseudo label Itea and guide the learning of the student model. Conventional Knowledge Distillation. In the training process of the student network, we utilize the ground-truth labels and the pseudo label of multiple teachers as additional knowledge to jointly guide the optimization of student model Gstu. Following the conventional knowledge distillation loss, we formulate our objective by incorporating fused knowledge from multiple teachers, as follows: Lkd = Lcolor(Istu, Igt) + λkdLcolor(Istu, Itea) (6) Spatial Distribution Distillation. In the context of 3DGS, these optimized teacher models provide structure-rich and dense 3D point distribution. In contrast, the student model operates under sparse or limited sampling conditions and aims to reconstruct the similar scene representation. Therefore, we hope to design structural similarity loss to encourage the student model to capture spatial geometric distributions similar to those of the teacher. However, challenges arise due to varying point densities, sampling noise, and non-uniform point distributions between student and teacher models. Direct coordinate-based distance measures are often insufficiently robust to these variations. To address this problem, we leverage the voxel histogram representation shown in Fig. 3, which discretizes the 3D space into regular voxels and counts the number of points within each voxel. This approach encodes the spatial distribution of points as high-dimensional structural feature, inherently robust to point count and density variations. Comparing voxel histograms thus enables efficient and structureaware similarity evaluation between different point clouds. To this end, we propose voxel histogram-based structural loss to enhance the structural learning capability of the student model. During the training phase of the student model, we firstly obtain point cloud Ptea and Pstu from the optimized standard teacher model Gstd and student model Gstu. Then, we Figure 3. Overview of Spatial Distribution Distillation. determine common 3D bounding box that encompasses both sets of points.The bounding box is partitioned into regular voxel grid with resolution of 128 . Each point from both clouds is assigned to corresponding voxel cell based on its spatial coordinates. For point cloud Ptea and Pstu, we count the number of points falling into every voxel separately, resulting in two high-dimensional voxel occupancy histograms htea and hstu. These histograms are then normalized to form probability distributions that capture the spatial structure of each cloud, independent of point count or density. Finally, we compute the cosine similarity between their normalized voxel occupancy histograms. The cosine similarity loss is given by: Lhist = 1 htea hstu htea2 hstu2 (7) This loss quantitatively reflects how closely the Student point cloud matches the Teachers structural distribution. The final loss function during the student training phase is defined as: = Lkd + Lhist (8) 4. Experiments Datasets. We conducted experiments on three widely used datasets: LLFF [22], Mip360 [2] and two scenes from the Tanks and Temples(T&T) [14]. LLFF contains eight scenes with forward-facing camera. Mip-NeRF360 comprises nine distinct scenes that encompass both expansive outdoor scenes and intricate indoor settings. These scenes exhibit wide range of capture styles and encompass both bounded indoor environments as well as large, unbounded outdoor settings. To partition the dataset into training and test sets, we follow the protocol of 3DGS by allocating every eight image to the test set. The resolution of all images is kept consistent with that used in 3DGS. 5 Figure 4. Visualized comparison on the Bicycle, Garden, and Kitchen scenes. As shown in the rendered images and corresponding local regions, the proposed method can better capture fine details. Type Method Quality Efficiency Plenoxels (CVPR22) INGP-Big (SIGGRAPH22) Mip-NeRF360 (CVPR22) 3D-GS (TOG23) 3D-GS ScaffoldGS (CVPR24) CompactGaussian (CVPR24) LP-3DGS (NIPS24) MiniSplatting (CVPR24) EAGLES(ECCV24) Taming 3DGS(SIGGRAPH Asia24) CompGS (ECCV24) Ours PSNR 23.08 25.59 27.69 27.26 27.39 27.60 27.08 27.47 27.25 27.20 27.71 27.12 27. Mip-NeRF 360 SSIM LPIPS 0.463 0.626 0.331 0.699 0.237 0.792 0.214 0.815 0.219 0.819 0.222 0.812 0.247 0.798 0.227 0.812 0.217 0.820 0.232 0.809 0.207 0.820 0.240 0.806 0.202 0.827 #G(106) - - - 3.5 3.43 0.6 1.388 1.959 0.5 1.3 0.63 0.84 0.49 PSNR 21.08 21.92 22.22 23.14 23.61 24.08 23.32 23.60 23.21 23.26 23.95 23.44 23.76 Tanks & Temples SSIM LPIPS 0.379 0.719 0.305 0.745 0.257 0.759 0.183 0.841 0.180 0.849 0.165 0.854 0.201 0.831 0.188 0.842 0.203 0.836 0.201 0.837 0.201 0.837 0.198 0.838 0.179 0.845 #G(106) - - - 2.0 1.84 0.6 0.836 1.244 0.32 0.7 0.29 0.52 0.25 PSNR 23.06 24.96 29.40 29.41 29.55 30.25 29.79 - 29.98 29.86 29.82 29.90 29. Deep Blending SSIM LPIPS 0.510 0.795 0.390 0.817 0.245 0.901 0.243 0.903 0.241 0.912 0.245 0.907 0.258 0.901 - - 0.253 0.908 0.246 0.910 0.237 0.904 0.251 0.907 0.916 0.251 #G(106) - - - 3.2 3.24 0.40 1.06 - 0.40 1.20 0.27 0.55 0.33 Table 1. Quantitative evaluations across the Mip-NeRF 360, Tanks&Temples, and Deep Blending datasets. Best and second-best results are highlighted for each. * denotes our re-runs of the existing codebase to ensure fair evaluation. Evaluation Metrics. For the evaluation of comparative view synthesis quality, we adopt several widely used quantitative metrics, including Peak Signal-to-Noise Ratio (PSNR), Learned Perceptual Image Patch Similarity (LPIPS)[37], and Structural Similarity Index Measure PSNR and SSIM primarily assess pixel- (SSIM)[28]. level fidelity and structural consistency, respectively, while LPIPS reflects more human-aligned assessment of visual quality. In addition, we present the average memory usage associated with storing the optimized parameters. Implementations. Our implementations are based on the official 3DGS codebase. All models were trained on single NVIDIA RTX 3090 GPU. Training process of Distilled3DGS contains two stages: In the training phase of teacher models, these teacher models are trained for 30k iterations by following 3DGS. Gaussian densification is stopped after the 15000th iteration as in 3DGS. For the teacher training with random perturbation, random noise δt is applied to those Gaussian primitives exhibiting large view-space positional gradients, as these typically correspond to regions that have not yet been well reconstructed, starting from 500th to 15000th iteration with interval 500. Introducing appropriate perturbations in this manner can enhance the robustness of the model. For the teacher training with random dropout, rinit, t0 and t1 are respectively set 0.2, 500 and 15000. Each Gaussian primitive is randomly deactivated with probability p, and the remaining primitives are optimized to fit the observed views. For inference, all Gaussian primitives are activated to enable novel view synthesis. In the training phase of student model, the total number of optimization steps is set to 30K. Densification is applied up to the 15000th iteration, after which simplification is car6 ried out at both the 15000th iterations. Subsequently, the structural similarity loss is computed and applied every 500 iterations throughout the optimization process. 4.1. Comparisons with State-of-the-Arts We evaluate model performance across several real-world datasets, including Mip-NeRF 360, Tanks&Temples, and Deep Blending. For NeRF-based methods, we compare with the state-of-the-art Mip-NeRF 360 [2] and two efficient NeRF variants, INGP [23] and Plenoxels [6]. For 3DGSbased methods, our comparisons include the vanilla 3DGS as well as leading Gaussian simplification techniques such as CompactGaussian [16], LP-3DGS [38], EAGLES [9], MiniSplatting [5], and Taming 3DGS [21]. For vanilla 3D-GS, we include both the metrics reported in [13] and those obtained through our own experimental runs. The quantitative results for all datasets are presented in Table 1. Our method surpasses both the voxel grid-based approach, Plenoxels, and the fast NeRF-based method, INGP, across all datasets and evaluation metrics. Compared to the Mip-NeRF360 baseline, Distilled-3DGS yields PSNR improvements of 0.12 dB, 1.54 dB, and 0.47 dB on the Mip-NeRF360, Tanks&Temples, and Deep Blending datasets, respectively, verifying its effectiveness across diverse datasets. Compared to other 3DGS-based methods, our proposed Distilled-3DGS consistently outperforms the baseline 3DGS across all three metrics while using significantly fewer Gaussians on Mip360 dataset. We attribute this improvement to the comprehensive knowledge supervision provided by the diverse teacher models. Specifically, compared to the vanilla 3DGS, our Distilled-3DGS achieves PSNR improvements of 0.55 dB on Mip-NeRF360, 0.62 dB on Tanks&Temples, and 0.46 dB on Deep Blending. The number of Gaussians is also reduced by 86.0%, 87.5%, and 89.6% on these three datasets, respectively. Compared with these 3DGS simplification methods, such as Taming 3DGS, our method also can improve rendering quality while maintaining comparable number of Gaussians. Besides, visual comparison is illustrated in Fig. 4, compared with existing simplification approaches such as Taming-3DGS, Mini-Splatting, and the vanilla 3DGS, our Distilled-3DGS achieves rendering results that preserve fine details most faithfully to the ground truth while utilizing significantly reduced number of Gaussian primitives. removing the Perturbation-based, Dropout-based 3DGS teachers, respectively. The quantitative results are shown in Table 2. Specifically, the student model distilled from all three teachers consistently achieves the best performance, indicating that each teacher provides complementary knowledge. Teacher Gperb enhances the students robustness to input variations, while teacher Gdrop prevents overfitting and encourages generalization. The regular 3DGS teacher Gstd serves as strong baseline, providing high-fidelity supervision. The progressive decrease in performance with the removal of these specialized teachers underscores their critical roles in enriching the distilled knowledge, validating the effectiveness of leveraging diverse teacher models for optimal student performance. Method Ours Without Gdrop Without Gperb Without Lhist Deep Blending Tanks&Temples PSNR SSIM LPIPS PSNR SSIM LPIPS 29.87 29.71 29.63 29. 0.916 0.899 0.878 0.871 0.251 0.257 0.262 0.263 23.76 23.58 23.43 23. 0.845 0.840 0.838 0.836 0.179 0.186 0.195 0.197 Table 2. Ablation study on two datasets."
        },
        {
            "title": "Grid size",
            "content": "PSNR SSIM LPIPS Train Mem.(MB) 32 64 128 256 27.51 27.62 27.81 27.92 0.819 0.821 0.827 0.829 0.198 0.199 0.202 0. 9564 10232 12235 15456 Table 3. The impact of different grid size in spatial distribution distillation."
        },
        {
            "title": "Method",
            "content": "Teachers (Gstd+Gdrop+Gperb) 3DGS Student(Base) Student(Big) Student(Small) Room (Mip360) SSIM LPIPS #G(106) 0. 0.920 0.927 0.934 0.923 0.185 0.200 0.193 0.189 0.194 1. 1.50 0.46 1.13 0.21 PSNR 32.15 31.59 31.54 31.89 31. Table 4. The impact of the number of Gaussians."
        },
        {
            "title": "Student",
            "content": "Gstd/Gperb/Gdrop Gstd/Gstd/Gstd Gstd Gperb Gdrop"
        },
        {
            "title": "Gstu",
            "content": "PSNR 31.54 31.36 31.19 31.31 31.23 Room (Mip360) SSIM LPIPS 0.927 0.923 0.918 0.921 0.919 0.193 0.191 0.187 0.186 0.189 #G(106) 0.460 0.469 0.465 0.453 0.459 4.2. Ablation Studies and Further Analyses Table 5. The impact of different teachers To study the contribution of each component in the proposed framework, we conducted series of ablation experiments on the Deep Blending and Tanks&Temples datasets. Effect of the number of teachers. To further understand the contribution of each teacher model in our distillation framework, we conducted ablation studies by gradually Effect of Spatial Distribution Distillation. The results presented in Table 2 verify that spatial distribution distillation plays crucial role in enhancing rendering quality. Without this, performance in PSNR is decreased by 0.16 dB. In addition, we investigate the impact of varying grid 7 sizes on the student model training using the Mip360 dataset shown in Table 3. Generally, larger grid size produces smaller voxel dimensions and greater number of voxels, leading to more detailed scene representation. While increasing the grid size can improve PSNR performance, it also incurs substantial increase in GPU memory. Impact of the number of Gaussians. We conduct experiments on the Room scene from Mip360 to evaluate the impact of Gaussian count. Table 4 reports the reconstruction quality and the number of Gaussians for different model variants. The ensemble of three diverse teacher models achieves the highest PSNR of 32.15 dB. Compared to vanilla 3DGS, the student (Base) modeltrained via multiteacher distillationpreserves comparable reconstruction quality while significantly reducing the number of Gaussians. Although the student (Big) model achieves higher PSNR, it uses nearly as many Gaussians as the teacher models. In contrast, the student (Small) model applies further pruning, resulting in only slight PSNR drop of 0.15 dB. Impact of different teachers. We analyze the effects of various teacher models on the performance of the student model. As shown in Table 5, employing multiple diverse teachers (Gstd, Gperb, Gdrop) to distill the student yields the best overall performance. In contrast, using three standard teachers(Gstd) results in lower PSNR (31.36), and single-teacher configurations perform even worse compared to these teacher ensembles. These results highlight that diversity among teachers provides richer and more complementary supervisory signals, thereby enhancing student model performance. 5. Conclusion In this paper, we proposed multi-teacher distillation framework for 3DGS, aiming to preserve reconstruction quality under significantly reduced Gaussian counts. By leveraging knowledge from multiple teacher models, our approach effectively transfers both scene geometry and appearance priors to more compact student representation. Besides, we leverage spatial distribution distillation strategy to encourage the student to learn spatial geometric distributions consistent with those of standard teacher model. Extensive experiments across different scenes demonstrate that our distilled student model-Distilled-3DGS achieves promising performance with substantially fewer Gaussians, highlighting the potential of our method for deployment in memory-constrained or real-time scenes. Limitation. Distilled-3DGS also has some drawbacks: first, it requires pre-training multiple high-performance teacher models, increasing training time and computational resources by at least N-fold compared to single model; second, generating distillation soft labels via multi-model inference significantly increases GPU memory usage. Future work could explore end-to-end distillation pipelines or adaptive pruning strategies for Gaussian parameters to further improve efficiency and generalization."
        },
        {
            "title": "Acknowledgement",
            "content": "The computational resources are supported by SongShan Lake HPC Center (SSL-HPC) in Great Bay University. This work was also supported by Guangdong Research Team for Communication and Sensing Integrated with Intelligent Computing (Project No. 2024KCXTD047)."
        },
        {
            "title": "References",
            "content": "[1] Jonathan Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, and Ricardo Martin-Brualla. Mip-nerf: multiscale representation for anti-aliasing neural radiance fields. In Proceedings of the IEEE/CVF international conference on computer vision, pages 58555864, 2021. 2 [2] Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded In Proceedings of anti-aliased neural radiance fields. the IEEE/CVF conference on computer vision and pattern recognition, pages 54705479, 2022. 2, 5, 7 [3] Cristian Buciluˇa, Rich Caruana, and Alexandru NiculescuMizil. Model compression. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 535541, 2006. 1, 3 [4] Guobin Chen, Wongun Choi, Xiang Yu, Tony Han, and Manmohan Chandraker. Learning efficient object detection models with knowledge distillation. Advances in neural information processing systems, 30, 2017. 3 [5] Guangchi Fang and Bing Wang. Mini-splatting: Representing scenes with constrained number of gaussians. In European Conference on Computer Vision, pages 165181. Springer, 2024. 2, 3, 7 [6] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 55015510, 2022. 2, [7] Takashi Fukuda, Masayuki Suzuki, Gakuto Kurata, Samuel Thomas, Jia Cui, and Bhuvana Ramabhadran. Efficient knowledge distillation from an ensemble of teachers. In Interspeech, pages 36973701, 2017. 3 [8] Yarin Gal and Zoubin Ghahramani. Dropout as bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning, pages 10501059. PMLR, 2016. 4 [9] Sharath Girish, Kamal Gupta, and Abhinav Shrivastava. Eagles: Efficient accelerated 3d gaussians with lightweight encodings. In European Conference on Computer Vision, pages 5471. Springer, 2024. 7 [10] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. DistillarXiv preprint ing the knowledge in neural network. arXiv:1503.02531, 2015. 3 [11] Wenbo Hu, Yuling Wang, Lin Ma, Bangbang Yang, Lin Gao, Xiao Liu, and Yuewen Ma. Tri-miprf: Tri-mip represenIn tation for efficient anti-aliasing neural radiance fields. 8 Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1977419783, 2023. 2 [12] Deyi Ji, Haoran Wang, Mingyuan Tao, Jianqiang Huang, Xian-Sheng Hua, and Hongtao Lu. Structural and statistical texture knowledge distillation for semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1687616885, 2022. 3 [13] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 1, 2, 3, 4, 7 [14] Arno Knapitsch, Jaesik Park, and Qian-Yi Zhou. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics (ToG), 36(4):113, 2017. 5 [15] Kisoo Kwon, Hwidong Na, Hoshik Lee, and Nam Soo Kim. Adaptive knowledge distillation based on entropy. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 74097413. IEEE, 2020. 3 [16] Joo Chan Lee, Daniel Rho, Xiangyu Sun, Jong Hwan Ko, and Eunbyung Park. Compact 3d gaussian representation for radiance field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21719 21728, 2024. [17] Muyang Li, Ji Lin, Yaoyao Ding, Zhijian Liu, Jun-Yan Zhu, and Song Han. Gan compression: Efficient architecIn Proceedings of tures for interactive conditional gans. the IEEE/CVF conference on computer vision and pattern recognition, pages 52845294, 2020. 3 [18] Quanquan Li, Shengying Jin, and Junjie Yan. Mimicking very efficient network for object detection. In Proceedings of the ieee conference on computer vision and pattern recognition, pages 63566364, 2017. 3 [19] Yifan Liu, Ke Chen, Chris Liu, Zengchang Qin, Zhenbo Luo, and Jingdong Wang. Structured knowledge distillation for In Proceedings of the IEEE/CVF semantic segmentation. conference on computer vision and pattern recognition, pages 26042613, 2019. 3 [20] Tao Lu, Mulin Yu, Linning Xu, Yuanbo Xiangli, Limin Wang, Dahua Lin, and Bo Dai. Scaffold-gs: Structured 3d In Proceedings of gaussians for view-adaptive rendering. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2065420664, 2024. 2 [21] Saswat Subhajyoti Mallick, Rahul Goel, Bernhard Kerbl, Markus Steinberger, Francisco Vicente Carrasco, and Fernando De La Torre. Taming 3dgs: High-quality radiance fields with limited resources. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. 2, 7 [22] Ben Mildenhall, Pratul Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. ACM Transactions on Graphics (ToG), 38(4):114, 2019. [23] Thomas Muller, Alex Evans, Christoph Schied, and AlexanInstant neural graphics primitives with mulder Keller. tiresolution hash encoding. ACM transactions on graphics (TOG), 41(4):115, 2022. 7 [24] Michael Niemeyer, Fabian Manhardt, Marie-Julie Rakotosaona, Michael Oechsle, Daniel Duckworth, Rama Gosula, Keisuke Tateno, John Bates, Dominik Kaeser, and Federico Tombari. Radsplat: Radiance field-informed gaussian splatting for robust real-time rendering with 900+ fps. arXiv preprint arXiv:2403.13806, 2024. 2 [25] Hyunwoo Park, Gun Ryu, and Wonjun Kim. Dropgaussian: Structural regularization for sparse-view gaussian splatting. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2160021609, 2025. 4 [26] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):19291958, 2014. 4 [27] Yonglong Tian, Dilip Krishnan, Contrastive representation distillation. arXiv:1910.10699, 2019. and Phillip Isola. arXiv preprint [28] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. 6 [29] Xiaoyang Wu, Daniel DeTone, Duncan Frost, Tianwei Shen, Chris Xie, Nan Yang, Jakob Engel, Richard Newcombe, Hengshuang Zhao, and Julian Straub. Sonata: Selfsupervised learning of reliable point representations. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2219322204, 2025. 11 [30] Shan You, Chang Xu, Chao Xu, and Dacheng Tao. Learning from multiple teacher networks. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, pages 12851294, 2017. 3 [31] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. Plenoctrees for real-time rendering of neural radiance fields. In Proceedings of the IEEE/CVF international conference on computer vision, pages 5752 5761, 2021. 2 [32] Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. arXiv preprint arXiv:1612.03928, 2016. 3 [33] Hailin Zhang, Defang Chen, and Can Wang. ConfidenceIn ICASSP aware multi-teacher knowledge distillation. 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 44984502. IEEE, 2022. [34] Hailin Zhang, Defang Chen, and Can Wang. Adaptive multiteacher knowledge distillation with meta-learning. In 2023 IEEE International Conference on Multimedia and Expo (ICME), pages 19431948. IEEE, 2023. 3 [35] Linfeng Zhang and Kaisheng Ma. Improve object detection with feature-based knowledge distillation: Towards accurate and efficient detectors. In International conference on learning representations, 2020. 3 [36] Linfeng Zhang, Xin Chen, Xiaobing Tu, Pengfei Wan, Ning Xu, and Kaisheng Ma. Wavelet knowledge distillation: Towards efficient image-to-image translation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1246412474, 2022. 3 9 [37] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 6 [38] Zhaoliang Zhang, Tianchen Song, Yongjae Lee, Li Yang, Cheng Peng, and Rama Chellappa. Lp-3dgs: Learning to prune 3d gaussian splatting. Advances in Neural Information Processing Systems, 37:122434122457, 2024. 2, 7 [39] Yang Katie Zhao, Shang Wu, Jingqun Zhang, Sixu Li, Chaojian Li, and Yingyan Celine Lin. Instant on-device neural radiance field training via algorithmIn 2023 accelerator co-designed near-memory processing. 60th ACM/IEEE Design Automation Conference (DAC), pages 16. IEEE, 2023. Instant-nerf: [40] Tinghui Zhou, Shubham Tulsiani, Weilun Sun, Jitendra Malik, and Alexei Efros. View synthesis by appearance flow. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 286301. Springer, 2016. 1 10 A. Additional ablation experiments A.1. Point cloud structural similarity Methods for evaluating the similarity of spatial geometric distribution between two point clouds can be broadly categorized into three types: distance-based, distribution matching-based, and learning-based approaches. Our proposed spatial distribution distillation strategy, which utilizes voxel histograms, falls under the distribution matching category. For the distance-based approach, we employ Chamfer Distance (CD) to directly measure the spatial distance between the two point sets. For the learning-based approach, we adopt Sonata [29], state-of-the-art point cloud representation learning method, to extract point features and compute the similarity loss between the two point clouds. Method Tanks&Temples PSNR SSIM LPIPS Mem.(GB) Student Time(min) Distance-based Feature-based Ours 23.69 23.78 23.76 0.832 0.847 0.845 0.187 0. 0.179 23.82 40.00 13.83 50 60 30 Table 6. Impact of different structural similarity strategy. As shown in Table 6, both the distance-based and feature-based methods consume considerable memory and time(student model training) yet fail to deliver substantial performance gains. In contrast, our proposed voxel histogram-based method outperforms these two approaches while requiring significantly less memory and computation time. B. Per-scene breakdown results To provide more detailed evaluation of our model, we present the per-scene breakdown results of Mip-NeRF360, Tanks&Temples and Deep Blending datasets."
        },
        {
            "title": "Method",
            "content": "PSNR SSIM LPIPS #G(106)"
        },
        {
            "title": "Average",
            "content": "EAGLES Mini-Splatting 3D-GS Ours EAGLES Mini-Splatting 3D-GS Ours EAGLES Mini-Splatting 3D-GS Ours 30.38 30.62 28.79 30.45 29.35 29.36 30.31 29.29 29.86 29.99 29.55 29. 0.910 0.915 0.911 0.926 0.900 0.903 0.913 0.906 0.910 0.909 0.912 0.916 0.250 0.249 0.241 0.243 0.240 0.260 0.241 0.259 0.250 0.255 0.241 0. 0.80 0.41 3.39 0.26 1.57 0.38 3.08 0.39 1.19 0.40 3.24 0.33 Figure 6. Quantitative per-scene breakdown results on Deep Blending dataset. Scene Method PSNR SSIM LPIPS Num.(M) Bicycle Bonsai Counter Flowers Garden Kitchen Room Stump Treehill Average EAGLES Mini-Splatting 3D-GS Ours EAGLES Mini-Splatting 3D-GS Ours EAGLES Mini-Splatting 3D-GS Ours EAGLES Mini-Splatting 3D-GS Ours EAGLES Mini-Splatting 3D-GS Ours EAGLES Mini-Splatting 3D-GS Ours EAGLES Mini-Splatting 3D-GS Ours EAGLES Mini-Splatting 3D-GS Ours EAGLES Mini-Splatting 3D-GS Ours EAGLES Mini-Splatting 3D-GS Ours 25.04 25.21 25.03 24.97 31.32 31.41 31.99 32.79 28.40 28.32 28.89 29. 21.29 21.31 21.30 21.45 26.91 26.67 27.32 27.58 30.77 31.24 31.43 31.65 31.47 31.21 31.59 31.54 26.78 27.32 26.53 27.73 22.69 22.58 22.43 22. 27.23 27.25 27.39 27.81 0.750 0.760 0.740 0.777 0.940 0.940 0.960 0.946 0.900 0.913 0.920 0.914 0.58 0.614 0.600 0.617 0.840 0.844 0.870 0. 0.930 0.924 0.930 0.932 0.920 0.920 0.920 0.927 0.770 0.804 0.770 0.811 0.640 0.656 0.660 0.645 0.810 0.820 0.819 0.827 0.240 0.246 0.241 0. 0.190 0.182 0.170 0.179 0.200 0.181 0.190 0.181 0.370 0.334 0.359 0.313 0.150 0.153 0.125 0.108 0.130 0.123 0.120 0.117 0.200 0.191 0.200 0. 0.240 0.215 0.240 0.193 0.340 0.331 0.325 0.314 0.240 0.217 0.219 0.202 2.26 0.60 5.67 0.59 0.64 0.33 1.64 0.31 0.56 0.36 1.58 0. 1.33 0.62 3.67 0.62 1.65 0.69 5.92 0.68 1.00 0.38 2.01 0.37 0.67 0.32 1.99 0.31 2.22 0.61 4.68 0.60 1.60 0.63 3.67 0. 1.33 0.50 3.43 0.49 Table 7. Quantitative per-scene breakdown results on MiPNeRF360 dataset."
        },
        {
            "title": "Method",
            "content": "PSNR SSIM LPIPS #G(106)"
        },
        {
            "title": "Average",
            "content": "EAGLES Mini-Splatting 3D-GS Ours EAGLES Mini-Splatting 3D-GS Ours EAGLES Mini-Splatting 3D-GS Ours 21.65 21.28 21.94 22.14 25.09 25.13 25.31 25.37 23.37 23.21 23.62 23. 0.800 0.801 0.810 0.812 0.870 0.871 0.880 0.878 0.835 0.836 0.845 0.845 0.240 0.238 0.200 0.207 0.160 0.166 0.150 0.150 0.200 0.203 0.175 0. 0.46 0.29 1.11 0.23 0.83 0.35 2.54 0.29 0.64 0.32 1.83 0.25 Table 8. Tanks&Temples dataset. Quantitative per-scene breakdown results on Figure 5. Visual comparison with different teacher models. Without the guidance of diverse teacher models, the rendering quality of the student 3DGS model gradually deteriorates."
        }
    ],
    "affiliations": [
        "Sun Yat-Sen University",
        "The University of Manchester",
        "Vision, Graphics, and Group, Great Bay University"
    ]
}