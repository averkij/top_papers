{
    "paper_title": "Speculative Jacobi-Denoising Decoding for Accelerating Autoregressive Text-to-image Generation",
    "authors": [
        "Yao Teng",
        "Fuyun Wang",
        "Xian Liu",
        "Zhekai Chen",
        "Han Shi",
        "Yu Wang",
        "Zhenguo Li",
        "Weiyang Liu",
        "Difan Zou",
        "Xihui Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As a new paradigm of visual content generation, autoregressive text-to-image models suffer from slow inference due to their sequential token-by-token decoding process, often requiring thousands of model forward passes to generate a single image. To address this inefficiency, we propose Speculative Jacobi-Denoising Decoding (SJD2), a framework that incorporates the denoising process into Jacobi iterations to enable parallel token generation in autoregressive models. Our method introduces a next-clean-token prediction paradigm that enables the pre-trained autoregressive models to accept noise-perturbed token embeddings and predict the next clean tokens through low-cost fine-tuning. This denoising paradigm guides the model towards more stable Jacobi trajectories. During inference, our method initializes token sequences with Gaussian noise and performs iterative next-clean-token-prediction in the embedding space. We employ a probabilistic criterion to verify and accept multiple tokens in parallel, and refine the unaccepted tokens for the next iteration with the denoising trajectory. Experiments show that our method can accelerate generation by reducing model forward passes while maintaining the visual quality of generated images."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 4 9 9 8 0 . 0 1 5 2 : r Speculative Jacobi-Denoising Decoding for Accelerating Autoregressive Text-to-image Generation Yao Teng1 Yu Wang4 Zhenguo Li3 Weiyang Liu2 Difan Zou1 Xihui Liu1 Fuyun Wang2 Xian Liu2 Zhekai Chen1 Han Shi3 1The University of Hong Kong 2CUHK 3Huawei Noahs Ark Lab 4Tsinghua University"
        },
        {
            "title": "Abstract",
            "content": "As new paradigm of visual content generation, autoregressive text-to-image models suffer from slow inference due to their sequential token-by-token decoding process, often requiring thousands of model forward passes to generate single image. To address this inefficiency, we propose Speculative Jacobi-Denoising Decoding (SJD2), framework that incorporates the denoising process into Jacobi iterations to enable parallel token generation in autoregressive models. Our method introduces next-clean-token prediction paradigm that enables the pre-trained autoregressive models to accept noise-perturbed token embeddings and predict the next clean tokens through low-cost fine-tuning. This denoising paradigm guides the model towards more stable Jacobi trajectories. During inference, our method initializes token sequences with Gaussian noise and performs iterative next-cleantoken-prediction in the embedding space. We employ probabilistic criterion to verify and accept multiple tokens in parallel, and refine the unaccepted tokens for the next iteration with the denoising trajectory. Experiments show that our method can accelerate generation by reducing model forward passes while maintaining the visual quality of generated images."
        },
        {
            "title": "Introduction",
            "content": "Autoregressive models have emerged as cornerstone of visual generative tasks through next-token prediction [14]. However, the autoregressive paradigm suffers from significant inference latency due to its sequential, token-by-token decoding process. For instance, generating single high-resolution image often requires thousands of sequential forward passes. To address this challenge, we focus on accelerating autoregressive text-to-image generation models via parallel token decoding. Jacobi decoding [5] is an iterative method that accelerates the inference of autoregressive models through parallel token decoding without any training. This method operates on sequence of randomly initialized tokens and iteratively calls the neural network to refine the tokens until the convergence (i.e., tokens are correctly decoded). Its variant, Speculative Jacobi Decoding (SJD) [6], improves Jacobi decoding with probabilistic criterion tailored for accelerating text-to-image generation using discrete tokens. The core of SJD is verification-refinement process. Specifically, given sequence of tokens, in each Jacobi iteration, SJD first predicts the probability for each input token, and then these probabilities enable the criterion to determine the acceptance of prefix of the tokens (i.e., verification) while also guiding the resampling of unaccepted tokens for the next iteration (i.e., refinement). The above verification-refinement process operates within fixed-length sliding Jacobi window where the accepted tokens are removed and newly initialized tokens are appended. By accepting multiple tokens (at least one token) per iteration, SJD reduces model forward passes, speeding up generation compared to token-by-token decoding. Corresponding Author 39th Conference on Neural Information Processing Systems (NeurIPS 2025). Figure 1: We propose Speculative Jacobi-Denoising Decoding to accelerate autoregressive text-toimage generation via multi-token prediction. On Lumina-mGPT, the number of model forward passes for inference (denoted as steps) is reduced. The inference step for our decoding is marked in green. Although SJD accelerates autoregressive text-to-image generation by non-negligible margin, the token refinement process in SJD is inherently unconstrained, making it difficult to control the refinement to achieve the correct token predictions. Consequently, some tokens undergo many refinement iterations before being accepted, resulting in low speedup ratio. In contrast, diffusion models explicitly define trajectory for the iterative input refinement, known as the denoising process, which is governed by the principles of stochastic differential equations [7]. More importantly, previous works [810] have demonstrated that this trajectory can be remarkably short (as few as tens of iterations) for any length of inputs (i.e., various image resolutions). Inspired by this intuition, to further accelerate the autoregressive text-to-image generation, we leverage the denoising process from the diffusion models to assist the Jacobi decoding and fine-tune the autoregressive model to adapt to the noise perturbation. In this paper, we propose Speculative Jacobi-Denoising Decoding (SJD2), along with fine-tuning strategy, to enable pre-trained autoregressive text-to-image generation models to perform parallel token-level denoising decoding. To achieve this, we introduce task called next-clean-token prediction, where an autoregressive model accepts noisy input tokens and predicts the clean tokens at one-position offset (i.e., noise-free next tokens). Our noise-augmented fine-tuning strategy 2 equips pre-trained autoregressive models with the denoising ability, i.e., we randomly select segments of input tokens and add Gaussian noise to their token embeddings during training. The training supervision remains consistent with conventional autoregressive models, i.e., the one position-offset discrete tokens are taken as ground-truth supervision and the model is trained with cross-entropy loss. After fine-tuning, the model has the ability to process noisy inputs and thus perform Jacobi-Denoising decoding during inference. The decoding process begins with sequence (Jacobi window) of token embeddings randomly initialized with Gaussian noise. In each Jacobi-Denoising iteration, this sequence is fed into the autoregressive model to predict the probability distribution of clean tokens at each token position. The noisy tokens are refined for the next iteration through the denoising process using the probability predicted in the current iteration. If token is sufficiently denoised, it becomes clean token and undergoes the standard Jacobi refinement. The iterative process is repeated until all tokens are accepted. To validate our method, we conduct experiments on two open-source large autoregressive models, Lumina-mGPT [11] and Emu3 [4]. Experimental results demonstrate that our method reduces the number of forward passes by about 4 on Lumina-mGPT and more than 5 on Emu3 and thus achieves latency speedup by more than 2. We further verify image quality to show that our method accelerates autoregressive text-to-image generation without compromising image quality."
        },
        {
            "title": "2 Related Work",
            "content": "Integration of Autoregression and Continuous Diffusion The AR-diffusion language model [12] integrates the concept of autoregression into an embedding diffusion language model [1318]. The embedding diffusion performs the standard denoising process on (normalized) token embeddings instead of next-token prediction, and use the regression loss [12] or cross-entropy loss [14] for training. The recent diffusion-forcing model and its variants [1925] share similar pipeline as the AR-diffusion model and introduce the autoregressive sampling into the temporal dimension of the continuous latent video diffusion models. They employ causal masks to support history-conditioned video generation, but have independent noise levels across input tokens. Moreover, for the diffusion process on the temporal dimension, works like the rolling diffusion model [26] further introduce the sliding window mechanism on the time series. Transfusion and its variants [2729] utilize single backbone model to jointly perform diffusion-based image generation in continuous latent space and autoregressive language generation in discrete space, using separate loss, different lightweight decoders, and specific attention masks for training. MAR and its variants [3032] propose the use of an efficient MLP diffusion head to decode features from autoregressive backbones, enabling autoregression in continuous latent space. Several multimodal large language models (MLLMs) [33 38] generate images via pre-trained diffusion models such as SDXL [39] which take the output features of autoregressive models as conditions. Parallel Token Decoding in Autoregressive Models Blockwise Parallel Decoding [40] and Medusa [41] employ auxiliary modules to predict multiple next tokens simultaneously, thereby accelerating language models. Speculative decoding [42, 43, 41, 4447] enhances inference efficiency by using smaller model to generate candidate tokens, which are then verified and accepted by the larger model in parallel. DiffuLLaMA [48] fine-tunes the large autoregressive language models into discrete diffusion models [49] for parallel decoding. Jacobi Decoding [5], initially applied to pixel-level autoregressive generation models, iteratively decodes tokens in parallel until their values converge, often in training-free manner. Lookahead Decoding [50] employs the token trajectories of Jacobi decoding to form pool of n-grams generated using greedy sampling to accelerate language models. CLLMs [51] collect the Jacobi trajectories into dataset and then distill the language models with it. Speculative Jacobi Decoding [6] revisits the original Jacobi Decoding in image generation and adapts it for modern autoregressive text-to-image generation based on discrete tokens by simply introducing probabilistic criterion. Spatially parallel image autoregressive decoding [52, 53] decodes multiple tokens simultaneously by leveraging spatial dependencies. Distilled-Decoding [17] distills an autoregressive model into consistency model [54] via an embedding-prediction head with millions of prepared noise-token pairs and hundreds of training epochs. Discussion In contrast to the above works, this paper integrates the denoising process into discrete autoregressive models while preserving the properties like next-token prediction. Our method enables flexible modulation between the autoregression, the speculative decoding, and the denoising process. 3 We fine-tune pre-trained autoregressive text-to-image generation models to achieve our goal with few epochs and off-the-shelf image data while avoiding adding additional modules."
        },
        {
            "title": "3 Preliminaries",
            "content": "Autoregressive Generation Let {x1, . . . , xN } denote sequence of discrete tokens, where is the sequence length and xi is an integer from vocabulary of size . In autoregressive models, the joint probability of the sequence is factorized as: P(x1, x2, . . . , xN ) = P(x1) (cid:81)N i=2 P(xix1, . . . , xi1), which assumes each discrete token xi depends only on its preceding tokens, following causal structure. Autoregressive models parameterize the conditional probability as Pθ(xix1, . . . , xi1) and sequentially decode (i.e., sampling token based on the predicted probability) each discrete token xi based on the preceding outputs. Jacobi Decoding and Speculative Jacobi Decoding Jacobi Decoding treats the autoregressive decoding as solving non-linear equation in triangular system via fixed-point iteration [5]. Instead of sequentially generating tokens by the rule xi Pθ(xx1, , xi1), Jacobi Decoding introduces an iteration index to enable parallel updates across all the sequential positions: x(j+1) denotes token at position at iteration j. Jacobi Decodi ing starts with randomly initialized tokens and iterates across the dimension until convergence, i.e., the tokens remain unchanged between two consecutive iterations. Since the number of iterations is proven to be not greater than the number of tokens [5], the acceleration can be achieved. i1) where x(j) 1 , , x(j) Pθ(xx(j) To adapt Jacobi Decoding for the modern autoregressive text-to-image generation which is based on large range of discrete tokens, Speculative Jacobi Decoding (SJD) [6] improves it by introducing the probabilistic criterion from speculative sampling [42, 43] to determine the convergence of tokens: (cid:33) (cid:32) 1, x(j) x(j) x(j) converged if < min Pθ(x(j) Pθ(x(j) where U[0, 1] denotes uniform distribution between 0 and 1, and is set to 1 by default, i.e., the tokens generated in previous Jacobi iterations serve as draft tokens for the current Jacobi iteration. SJD also maintains fixed-length Jacobi window within which Jacobi iterations are performed and token convergence is determined. In each iteration, prefix of the tokens is determined to be converged and removed from the window (i.e., the tokens are accepted), while the remaining tokens are resampled and newly initialized tokens are appended to the window. 1 , , x(j) i1) , , x(j) i1) , U[0, 1], (1) Continuous Diffusion Models Continuous diffusion models [5559, 8, 60] generate data by learning to reverse noise-corruption process. In this process, clean continuous input x0 is gradually corrupted into pure Gaussian noise ϵ. This can be formulated as: at any timestep (normalized to the range of [0, 1]) the noise perturbation can be written as xt = αtx0 +σtϵ where αt and σt are manually defined functions. As increases, αt monotonically decreases but σt increases. The reverse denoising trajectory between two timesteps can be solved by: xt = σt Dθ(xs, s) [61]. Here, σs Dθ is neural network trained via regression loss to predict x0 given noisy input and function of timestep. In this paper, we incorporate the denoising process into the Jacobi decoding. xs + αs (cid:16) αt αs σt σs (cid:17)"
        },
        {
            "title": "4 Method",
            "content": "In Jacobi Decoding, the refinement of tokens is unconstrained and difficult to control, with no guarantee that tokens will follow the fastest path to reach the correct values. In contrast, diffusion models have been demonstrated to generate high-resolution, high-quality images through short trajectories (as few as dozens of iterations) [810]. Motivated by this, we propose integrating the denoising process from the diffusion models into Jacobi Decoding. Overview Our decoding process, illustrated in Figure 2, comprises the following steps: (1) Token initialization: Given sequence of noisy normalized token embeddings (illustrated as blue-bordered patches in the first row) and prefilling/already-accepted tokens (depicted as green circles in the first row), the noise levels of the token embeddings are configured to be non-strictly monotonically 4 Figure 2: Overview of our decoding process. The noisy token embeddings with increasing noise levels undergo parallel forward pass with causal attention mask, predicting conditional probabilities and then sampling clean tokens. probabilistic criterion selects prefix of tokens for acceptance (green area). For unaccepted tokens, if clean, the next-token prediction performs on them (green solid arrows marked with AR). If noisy, they are denoised with one-position offset (blue solid ˆx0 arrows and dash arrows). Figure 3: Overview of our training strategy and model process. The input token embedding sequence is randomly divided into segments and the adjacent segments are perturbed with the noise of consecutive levels. The noisy embeddings with timestep tokens are fed into transformer blocks and prediction head. During training, the predicted probability is used to compute the cross-entropy loss for next-token prediction. During inference, the probability is for token sampling and then generating embeddings. increasing. Before the iterations start, the sequence is initialized with pure Gaussian noise. (2) Parallel forward: After initialization, these token embeddings undergo Jacobi-denoising iteration, where they are fed into the neural network along with timestep encodings for single parallel forward pass using causal attention mask. The network predicts conditional probability and performs token sampling for the next clean token at each position. Sampled tokens from noisy embedding inputs are denoted as the one-position-offset ˆx0-predictions (marked by the down-right blue solid arrows) while those from prefilling or accepted token inputs are denoted as autoregressive (AR) predictions, i.e., the standard next-token prediction (marked by green solid arrows). (3) Verification: After the parallel forward pass, prefix of sampled tokens is accepted based on the probabilistic criterion outlined in Equation (1). For example, in the second row of Figure 2, the first two sampled tokens are accepted and marked with green borders. (4) Refinement for unaccepted tokens: After the verification, the refinement is performed for unaccepted tokens. For each unaccepted noisy token, denoising step, outlined in Equation (3), is performed on its embedding. Specifically, linear combination is performed on the token embedding from the previous iteration at the same spatial position (indicated by vertical blue dashed lines) and the embedding of the predicted clean tokens from the one-position offset (the down-right blue solid arrows). If an unaccepted token has been sufficiently denoised to be clean in previous iterations, there is no further denoising needed and the refinement follows the standard next-token prediction. This iterative process repeats until all required tokens are accepted, serving as the final outputs. 4.1 Model Parameterization To seamlessly integrate the denoising process with next-token prediction, we propose paradigm of next-clean-token prediction based on noisy normalized token embeddings. In this paradigm, the autoregressive neural network learns to accept noisy input tokens and predict the clean tokens at one-position offset (i.e., noise-free next tokens). Let denote normalized token embedding and θ 5 denote the autoregressive model, our paradigm can be formulated as follows: (cid:0)xe0 ˆx0 1, , e0 i+1 Pθ i+1 = We One-hot(ˆx0 ˆm0 i+1 = Normalize( ˆm0 ˆe0 i+1), i1, et0 i+1), , , etk (cid:1), (2) 1, , i , , etk where et is the normalized token embedding at spatial position in the sequence and at timestep t, and ˆe0 i+1 represents the predicted clean normalized token embedding at position + 1, (cid:0)xe0 (cid:1) denotes the predicted conditional probability of discrete toi1, et0 Pθ i1, et0 ken with the embedding sequence {e0 } as conditions, One-hot() denotes transforming token category into one-hot vector, We RDV denotes the learned embedding i+1 RD denotes the D-dimensional predicted token embedding weight matrix of the model, and ˆm0 at position + 1. Normalize() is normalizing embeddings with their statistics (details in Section 4.3). The timesteps {t0, , tk}, i.e., noise levels, are configured to be non-strictly monotonically increasing, and we discuss the specific selection of timesteps for decoding in Section 4.2. In this paradigm, the pre-trained autoregressive models still predict the categorical probabilities of tokens instead of the continuous token embeddings, and we employ these probabilities to generate the embeddings. , , etk 1, , e0 4.2 Speculative Jacobi-Denoising Decoding Jacobi Window and token initialization In practice, our method employs sliding Jacobi window during the decoding phase, rather than directly performing the decoding process at fixed token positions. The Jacobi window is fixed-length sliding window containing noisy normalized token embeddings as the draft. Initially, the window is filled with pure Gaussian noise. In each iteration, prefix of accepted tokens is removed, and an equal number of new tokens, sampled from pure Gaussian noise, is appended. Consequently, the noise levels of the tokens within the window are non-strictly monotonically increasing across iterations. Parallel Forward and Verification The model takes draft token embeddings as the inputs and predicts the conditional probability of their next clean tokens in parallel. Then, Equation (1) is employed to determine whether to accept or reject each draft token which is transformed from the input token embedding. This transformation is finding the nearest discrete tokens in the vocabulary through cosine similarity [47]. Refinement with Denoising After verification, the unaccepted tokens will go through refinement process to refine the token embeddings for the next iteration. The refinement of noisy tokens is realized by the denoising process. For denoising, we first define fixed monotonically decreasing timestep sequence {tK, , tk , t0, 0}, where t0 is set very close to zero [62]. The values of these timesteps can follow the Karras timestep scheduler [62]. Then, we show the specific denoising formula based on these timesteps when > 0: etk1 = σtk1 σtk etk + αtk (cid:18) αtk1 αtk σtk1 σtk (cid:19) ˆe0 , (3) where ˆe0 is the token embedding predicted according to Equation (2). In the above equation, we perform denoising step like the diffusion models to obtain the embedding with smaller noise level at position with timestep tk. If = 0, i.e., the denoising process is just complete, Equation (3) reduces into the standard Jacobi iteration: e0 . Additionally, for the unaccepted sufficiently denoised tokens, as no further denoising is required, we resample the tokens whose threshold from Equation (1) is below 0.5 but retain the others for the next Jacobi iteration. := 0 + 1 ˆe0 By introducing the denoising trajectories into the decoding process of autoregressive models, our method stabilizes the token trajectories in the decoding process, accelerating the token convergence. 4.3 Fine-tuning Strategy In this section, we introduce the strategy of fine-tuning pretrained autoregressive text-to-image generation model to accept noisy input tokens for Speculative Jacobi-Denoising Decoding. Figure 3 illustrates an overview of our training strategy and the specific process of the neural network: During training, the normalized embeddings (initially clean) are first transformed into noisy embeddings. For example, in the right-side image of Figure 3, noise levels increase non-monotonically across patches (i.e., token positions) in raster scan order (left to right, top to bottom). When reaching randomly determined position, the noise level stops increasing and the noise level of the next position resets to zero, forming segments with non-monotonically increasing noise levels in the token sequence. Next, these noisy embeddings are appended with timestep encodings, which indicate the noise level at each position. Together, the embeddings and encodings are denormalized and are fed into transformer blocks and prediction head to produce logits for each position. The cross-entropy loss is then applied to each position, using the clean token indices as labels, with one-position offset (shown by the dotted frame in Figure 3) for next-clean-token prediction. During inference, the input normalized embeddings are already noisy and are not further perturbed. These embeddings are also appended with timestep encodings and processed to generate logits through the denormalization, the transformer blocks and the prediction head. As described in Equation (2), these logits are used for token sampling, and these sampled clean tokens are then transformed into normalized token embeddings. Noise Perturbation We add noise to the embeddings of these discrete input tokens, bypassing the discrete values. Although we can directly perform linear combination of the embedding and Gaussian random variable, the distribution of the pre-trained embeddings may deviate from the scale of the standard Gaussian distribution. For instance, if the variance of the embedding is small, the pretrained transformer blocks may only handle small values, making the values from standard Gaussian distribution unsuitable for these blocks. To address this issue, we add noise to the normalized token embeddings. Subsequently, these noisy normalized embeddings are de-normalized and then fed into the transformer blocks. The detailed procedure is as follows: = Normalize(m) = 1 σe (cid:0)m µe (cid:1), et = αte + σtϵ, mt = De-normalize(et) = σe et + µe, (4) where RD denotes the D-dimensional embedding of ground-truth token. µe RD and σe RD denote the mean and standard deviation of the learned embedding weight We RV D, respectively. In practice, we directly average this weight across its first dimension to compute the RD represents the mean, and the standard deviation is also obtained across this dimension. 1 σe element-wise reciprocal of σe, and is the element-wise product. αt and σt are the hyper-parameters for the denoising timestep t, and ϵ RD denotes the standard Gaussian noise. With Equation (4), we can transform clean embedding into noisy embedding mt. For fine-tuning, the input sequence is divided into randomly sized segments, and we add the identical level of noise to the tokens from the same segment. Finetuning Objective and Loss Function We fine-tune pre-trained autoregressive text-to-image model to predict the next clean token from the inputs of noisy token embeddings. The model accepts noisy token embeddings and outputs logits representing the categorical probability distribution of the next clean token. The cross-entropy loss is computed between these logits and the ground truth token categories, optimizing the model to denoise inputs while maintaining autoregressive prediction. Specifically, analogous to standard next-token prediction, the cross-entropy loss is performed as follows: = (cid:80)N denotes the one-hot label of the groundtruth token at position i, is the total number of tokens, and ˆp0 i+1 denotes the conditional probability in Equation (2). Here, we assume that only the first token is prefilled with text conditioning. With this training objective and parameterization, the model learns to decode or verify clean tokens even when noisy tokens are present in the input conditions. i=0 Cross-Entropy(cid:0)x (cid:1), where i+1, ˆp0 i+1 Timestep Injection Injecting the information of timesteps into noisy inputs is common design for the denoising process [56, 63, 64]. To avoid introducing additional modules like AdaLN [56], we take the sinusoidal encodings of timesteps as sequence of special token embeddings and append them to the sequence of input token embeddings during fine-tuning and decoding. Then, the sequence, which comprises input token embeddings and timestep encodings, is fed into the transformer blocks. Within the attention modules of these blocks, we use the attention mask to force each noisy token embedding to attend to the corresponding timestep encoding which indicates its noise level. To ensure 7 Figure 4: Correlation between denoising iterations and Jacobi window length on the latency. Circle areas represent absolute latency values, and the lowest latency (a difference within 3 seconds is allowed) is in orange. Figure 5: Study on embedding normalization for denoising process. Left: Denoising output without embedding normalization, failing to generate coherent image. Right: Denoising output with embedding normalization, generating semantically meaningful image. Table 1: Evaluation on the validation sets of MSCOCO [65]. Configuration COCO2017 (5k) COCO2014 (30k) Average Steps () Step Compression () FID CLIPScore Average Steps () Step Compression () FID CLIPScore Lumina-mGPT [11] SJD [6] SJD2 Emu3 [4] SJD [6] SJD2 2357 1060 592 8193 3528 1461 1.00 30.8 31.3 2.23 31.1 31.3 4.02 31.4 31.8 1.00 31.1 31.0 2.32 30.6 30.9 5.62 31.5 30. 2357 1057 599 8193 3535 1461 1.00 21.0 31.3 2.23 20.8 31.3 3.93 21.1 31.9 1.00 19.3 31.2 2.32 21.4 31.0 5.63 21.8 30.8 that the distribution of the timestep encodings aligns with that of the token embeddings, we apply normalization-then-denormalization process to these encodings similar to Equation (4)."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Implementation Details We perform experiments on two baselines, Lumina-mGPT [11] and Emu3 [4]. When generating an image at least 720 720, Lumina-mGPT only needs about 2k tokens while Emu3 requires more than 8k tokens because of the difference between their tokenizers. For each fine-tuning, 8 GPUs with 80G memory are required for each model. Since all model parameters are used for fine-tuning, we leverage DeepSpeed ZeRO-3 or FSDP with gradient checkpointing to save GPU memory at the cost of increased training time. The global batch size is set to 64, and the learning rate is set to 2 105 with the AdamW optimizer. We tune each model only within 6 epochs, costing approximately 14 8 A100 hours for Lumina-mGPT and 26 8 H100 hours for Emu3. By default, we set classifier-free guidance to 3.0 and use top-2000 for the quantitative results of our method. Evaluation metrics. To assess visual quality, we employ two key metrics: FID [66] and CLIPScore [67] on COCO benchmark [65] and GenEval benchmark [68]. To quantify the efficiency of the decoding process, we introduce the step compression ratio [50]: = # generated tokens # decoding steps , which serves as theoretical measure of parallelization and thus reflects the acceleration. For each benchmark, we compute the average step compression ratio across all generated images. Additionally, this ratio is included alongside individual image samples in qualitative comparisons to highlight the performance differences between our method and other approaches. More importantly, we provide the latency of model forward passes on single GPU to evaluate the practical speedup achieved. 8 Table 2: Visual quality on GenEval benchmark [68]."
        },
        {
            "title": "Colors Position Counting Two Color Attri Single Overall",
            "content": "Lumina-mGPT [11] 0.83 Lumina-mGPT + SJD [6] 0.82 0.81 Lumina-mGPT (tuned) Lumina-mGPT (tuned) + SJD [6] 0.81 Lumina-mGPT (tuned) + SJD2 0.79 Emu3 [4] Emu3 + SJD [6] Emu3 (tuned) + SJD2 0.78 0.79 0.73 0.09 0.08 0.13 0.12 0.11 0.15 0.12 0.14 0.26 0.23 0.27 0.31 0. 0.33 0.28 0.28 0.60 0.63 0.65 0.68 0.64 0.69 0.61 0.61 0.15 0.12 0.27 0.24 0.23 0.16 0.13 0.24 0.96 0.96 0.98 0.97 0. 0.98 0.97 0.96 0.48 0.47 0.52 0.52 0.51 0.52 0.48 0.49 5.2 Qualitative Results In Figure 1 and Figure 6, we compare the standard autoregressive decoding and our method on Lumina-mGPT [11]. The results show that our method can achieve about 4 fewer steps for autoregressive text-to-image generation and the visual quality is preserved. We also compare different decoding methods on Emu3 [4] in Figure 7. 5.3 Quantitative Results Our SJD2 significantly reduces the steps for autoregressive text-to-image generation while maintaining visual quality, as demonstrated by our evaluation on MS-COCO [65] validation sets in Table 1. We also find that our method achieves higher step compression ratio on Emu3 [4] than that on LuminamGPT [11]. Specifically, SJD2 can achieve step compression about 4.0 on Lumina-mGPT and about 5.6 on Emu3. For visual quality, we further compare our method to autoregressive decoding and SJD [6] on the GenEval benchmark [68] with Lumina-mGPT [11] as the baseline in Table 2. Our method achieves an overall score of 0.51, nearly matching the 0.52 of tuned AR and SJD, demonstrating preserved visual quality. More importantly, following [47], we select 100 COCO prompts on the identical A100 server to compare the practical average speedup and visual quality among these decoding methods. According to the latency reported in Table 3, our method is still faster than other decoding methods on the real server by more than 2. We also provide the GPU memory usage during inference. While our parallel decoding method achieves significant latency reductions, we acknowledge it incurs an additional memory overhead of about 3GB compared to autoregressive decoding, because of the variables for denoising like the timestep tokens. 5.4 Ablation Studies We perform experiments for our method with Lumina-mGPT as the baseline and on one RTX 4090 by default. The selected 100 prompts used in Table 3 are for the evaluation in the ablation studies. We also include specific and detailed analysis for our denoising process in Section B. Study on the sampling timesteps and Jacobi window length. While contemporary diffusion models typically require dozens of denoising iterations to achieve satisfactory results, extending Jacobi window lengths introduces computational overhead. This establishes trade-off between denoising iteration counts and Jacobi window length for the minimization of latency. According to the results in Figure 4, when constraining denoising steps to 20 while maintaining Jacobi window lengths beyond 80, the latency reduction converges to minimum. Study on the embedding normalization. When verifying the usefulness of embedding normalization, we focus on our denoising process by enforcing the denoised tokens to be immediately accepted (as detailed in Section B). As shown in Figure 5, deactivating embedding normalization results in complete failure of the denoising process, yielding pure noise instead of coherent images. Conversely, activating normalization enables the denoising process to generate reasonable outputs, demonstrating the critical role of embedding normalization in our denoising process. Table 3: The computational cost of decoding methods on subset of COCO prompts."
        },
        {
            "title": "Methods",
            "content": "Steps Latency CLIP-Score GPU Memory Lumina-mGPT [11] SJD [6] SJD2 Emu3 [4] SJD [6] SJD2 2357 1058 596 8193 3537 1470 88.55s 41.99s 33.64s 375.29s 207.60s 147.65s 32.0 31.5 32.2 30.9 31.2 30.7 17G 17G 20G 20G 20G 23G Figure 6: The comparison of the original autoregressive decoding, SJD [6], and our method with Lumina-mGPT [11] as the baseline and on one RTX 4090. Figure 7: The comparison of the original autoregressive decoding, SJD [6], and our method with Emu3 [4] as the baseline and on one RTX 4090."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper introduces Speculative Jacobi-Denoising Decoding, new algorithm integrating the continuous denoising process into Jacobi decoding to accelerate autoregressive text-to-image generation. By extending next-token prediction into next-clean-token prediction with noisy inputs, we enable pre-trained autoregressive models to learn to denoise noise-perturbed token embeddings through fine-tuning strategy. The proposed Jacobi-Denoising decoding initializes token sequences with Gaussian noise. It then iteratively refines them using process that combines denoising steps with Jacobi decoding and an improved probabilistic prefix acceptance criterion. Experiments show that our method can reduce the number of model forward passes for acceleration while keeping the visual quality of generated images."
        },
        {
            "title": "Acknowledgment",
            "content": "This work is supported by the National Nature Science Foundation of China (No. 62402406)."
        },
        {
            "title": "References",
            "content": "[1] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022. 1 [2] Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan Huang, Rachel Hornung, Hartwig Adam, Hassan Akbari, Yair Alon, Vighnesh Birodkar, et al. Videopoet: large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023. [3] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. [4] Emu3 Team BAAI. Emu3: Next-token prediction is all you need, 2024. 1, 3, 8, 9, 10, 16, 17 [5] Yang Song, Chenlin Meng, Renjie Liao, and Stefano Ermon. Accelerating feedforward computation via parallel nonlinear equation solving. In International Conference on Machine Learning, pages 97919800. PMLR, 2021. 1, 3, 4, 17, [6] Yao Teng, Han Shi, Xian Liu, Xuefei Ning, Guohao Dai, Yu Wang, Zhenguo Li, and Xihui Liu. Accelerating auto-regressive text-to-image generation with training-free speculative jacobi decoding. arXiv preprint arXiv:2410.01699, 2024. 1, 3, 4, 8, 9, 10, 17, 18 [7] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR. OpenReview.net, 2021. 2 [8] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR. OpenReview.net, 2021. 2, 4 [9] Qiang Liu. Rectified flow: marginal preserving approach to optimal transport. arXiv preprint arXiv:2209.14577, 2022. [10] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ODE solver for diffusion probabilistic model sampling in around 10 steps. In NeurIPS, 2022. 2, 4 [11] Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu Qiao, Hongsheng Li, and Peng Gao. Lumina-mgpt: Illuminate flexible photorealistic text-to-image generation with multimodal generative pretraining. arXiv preprint arXiv:2408.02657, 2024. 3, 8, 9, 10, 16, 17, 18 [12] Tong Wu, Zhihao Fan, Xiao Liu, Hai-Tao Zheng, Yeyun Gong, Jian Jiao, Juntao Li, Jian Guo, Nan Duan, Weizhu Chen, et al. Ar-diffusion: Auto-regressive diffusion model for text generation. Advances in Neural Information Processing Systems, 36:3995739974, 2023. 3 [13] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. Diffusion-lm improves controllable text generation. Advances in Neural Information Processing Systems, 35:43284343, 2022. 3 [14] Sander Dieleman, Laurent Sartran, Arman Roshannai, Nikolay Savinov, Yaroslav Ganin, Pierre Richemond, Arnaud Doucet, Robin Strudel, Chris Dyer, Conor Durkan, et al. Continuous diffusion for categorical data. arXiv preprint arXiv:2211.15089, 2022. 3 [15] Zhujin Gao, Junliang Guo, Xu Tan, Yongxin Zhu, Fang Zhang, Jiang Bian, and Linli Xu. Difformer: Empowering diffusion models on the embedding space for text generation. arXiv preprint arXiv:2212.09412, 2022. [16] Ishaan Gulrajani and Tatsunori Hashimoto. Likelihood-based diffusion language models. Advances in Neural Information Processing Systems, 36, 2024. [17] Enshu Liu, Xuefei Ning, Yu Wang, and Zinan Lin. Distilled decoding 1: One-step sampling of image auto-regressive models with flow matching, 2024. 3 11 [18] Vincent Hu, Di Wu, Yuki Asano, Pascal Mettes, Basura Fernando, Björn Ommer, and Cees Snoek. Flow matching for conditional text generation in few sampling steps. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 2: Short Papers), pages 380392, 2024. [19] Boyuan Chen, Diego Marti Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. arXiv preprint arXiv:2407.01392, 2024. 3 [20] Kaifeng Gao, Jiaxin Shi, Hanwang Zhang, Chunping Wang, Jun Xiao, and Long Chen. Ca2vdm: Efficient autoregressive video diffusion model with causal generation and cache sharing. arXiv preprint arXiv:2411.16375, 2024. [21] Jiatao Gu, Yuyang Wang, Yizhe Zhang, Qihang Zhang, Dinghuai Zhang, Navdeep Jaitly, Josh Susskind, and Shuangfei Zhai. Dart: Denoising autoregressive transformer for scalable text-to-image generation. arXiv preprint arXiv:2410.08159, 2024. [22] Chaorui Deng, Deyao Zh, Kunchang Li, Shi Guan, and Haoqi Fan. Causal diffusion transformers for generative modeling. arXiv preprint arXiv:2412.12095, 2024. [23] Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self forcing: Bridging the train-test gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025. [24] Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tianning Zhang, WQ Zhang, Weifeng Luo, et al. Magi-1: Autoregressive video generation at scale. arXiv preprint arXiv:2505.13211, 2025. [25] Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2296322974, 2025. 3 [26] David Ruhe, Jonathan Heek, Tim Salimans, and Emiel Hoogeboom. Rolling diffusion models. In Proceedings of the 41st International Conference on Machine Learning, 2024. 3 [27] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. 3 [28] Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Liang Zhao, et al. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. arXiv preprint arXiv:2411.07975, 2024. [29] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. 3 [30] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024. 3 [31] Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen Sun, Michael Rubinstein, Deqing Sun, Kaiming He, and Yonglong Tian. Fluid: Scaling autoregressive text-to-image generative models with continuous tokens. arXiv preprint arXiv:2410.13863, 2024. [32] Haoge Deng, Ting Pan, Haiwen Diao, Zhengxiong Luo, Yufeng Cui, Huchuan Lu, Shiguang Shan, Yonggang Qi, and Xinlong Wang. Autoregressive video generation without vector quantization. arXiv preprint arXiv:2412.14169, 2024. 3 [33] Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan. Planting seed of vision in large language model. arXiv preprint arXiv:2307.08041, 2023. 3 12 [34] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, et al. Dreamllm: Synergistic multimodal comprehension and creation. arXiv preprint arXiv:2309.11499, 2023. [35] Canyu Zhao, Mingyu Liu, Wen Wang, Weihua Chen, Fan Wang, Hao Chen, Bo Zhang, and Chunhua Shen. Moviedreamer: Hierarchical generation for coherent long visual sequence. arXiv preprint arXiv:2407.16655, 2024. [36] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. [37] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1439814409, 2024. [38] Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, et al. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025. 3 [39] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 3, 18 [40] Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Blockwise parallel decoding for deep autoregressive models. Advances in Neural Information Processing Systems, 31, 2018. 3 [41] Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason Lee, Deming Chen, and Tri Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads. arXiv preprint arXiv:2401.10774, 2024. 3 [42] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pages 1927419286. PMLR, 2023. 3, 4 [43] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023. 3, [44] Jacob Christopher, Brian Bartoldson, Bhavya Kailkhura, and Ferdinando Fioretto. Speculative diffusion decoding: Accelerating language generation through diffusion. arXiv preprint arXiv:2408.05636, 2024. 3 [45] Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ahmad Beirami, Himanshu Jain, and Felix Yu. Spectr: Fast speculative decoding via optimal transport. Advances in Neural Information Processing Systems, 36, 2024. [46] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle: Speculative sampling requires rethinking feature uncertainty. arXiv preprint arXiv:2401.15077, 2024. 16, 18 [47] Doohyuk Jang, Sihwan Park, June Yong Yang, Yeonsung Jung, Jihun Yun, Souvik Kundu, Sung-Yub Kim, and Eunho Yang. Lantern: Accelerating visual autoregressive models with relaxed speculative decoding. arXiv preprint arXiv:2410.03355, 2024. 3, 6, 9, 16, 18 [48] Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, et al. Scaling diffusion language models via adaptation from autoregressive models. arXiv preprint arXiv:2410.17891, 2024. 3 [49] Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:1798117993, 2021. 13 [50] Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. Break the sequential dependency of llm inference using lookahead decoding. arXiv preprint arXiv:2402.02057, 2024. 3, 8 [51] Siqi Kou, Lanxiang Hu, Zhezhi He, Zhijie Deng, and Hao Zhang. Cllms: Consistency large language models. arXiv preprint arXiv:2403.00835, 2024. 3 [52] Yuqing Wang, Shuhuai Ren, Zhijie Lin, Yujin Han, Haoyuan Guo, Zhenheng Yang, Difan Zou, Jiashi Feng, and Xihui Liu. Parallelized autoregressive visual generation. arXiv preprint arXiv:2412.15119, 2024. [53] Yefei He, Feng Chen, Yuanyu He, Shaoxuan He, Hong Zhou, Kaipeng Zhang, and Bohan Zhuang. Zipar: Accelerating autoregressive image generation through spatial locality. arXiv preprint arXiv:2412.04062, 2024. 3, 16, 18 [54] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint arXiv:2303.01469, 2023. 3 [55] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In CVPR, pages 1067410685. IEEE, 2022. 4, 18 [56] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. [57] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740, 2024. [58] Shuai Wang, Zexian Li, Tianhui Song, Xubin Li, Tiezheng Ge, Bo Zheng, and Limin Wang. Flowdcn: Exploring dcn-like architectures for fast image generation with arbitrary resolution. arXiv preprint arXiv:2410.22655, 2024. [59] Shuai Wang, Zhi Tian, Weilin Huang, and Limin Wang. Ddt: Decoupled diffusion transformer. arXiv preprint arXiv:2504.05741, 2025. 4 [60] Yao Teng, Yue Wu, Han Shi, Xuefei Ning, Guohao Dai, Yu Wang, Zhenguo Li, and Xihui Liu. Dim: Diffusion mamba for efficient high-resolution image synthesis. arXiv preprint arXiv:2405.14224, 2024. 4 [61] Cheng Lu and Yang Song. Simplifying, stabilizing and scaling continuous-time consistency models. arXiv preprint arXiv:2410.11081, 2024. 4 [62] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In NeurIPS, 2022. 6 [63] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. 7 [64] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2266922679, 2023. 7 [65] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740755. Springer, 2014. 8, 9, [66] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 8 [67] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, volume 139 of Proceedings of Machine Learning Research, pages 87488763. PMLR, 2021. 8 14 [68] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. 8, 9, 16, 17 [69] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing visionlanguage models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 16 [70] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. arXiv preprint arXiv:2403.03206, 2024. 16, [71] Peng Gao, Le Zhuo, Ziyi Lin, Chris Liu, Junsong Chen, Ruoyi Du, Enze Xie, Xu Luo, Longtian Qiu, Yuhang Zhang, et al. Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers. arXiv preprint arXiv:2405.05945, 2024. 16 [72] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. 16, 17,"
        },
        {
            "title": "A More Implementation Details",
            "content": "Experimental settings. We perform experiments on two baselines, Lumina-mGPT [11] and Emu3 [4]. We have collected approximately 80k high-resolution (at least 720 720) images from the Internet. For images lacking text descriptions, we caption them with Qwen2-VL [69]. We follow the advanced flow matching setting [70, 71] with αt = 1 and σt = for our denoising process. By default, the length of the Jacobi window of our method is set to 96 for Lumina-mGPT and 128 for Emu3. The number of denoising steps in SJD2 is set to 25."
        },
        {
            "title": "B Further Analysis",
            "content": "The pure denoising process also generates reasonable images. In our approach, the speculative Jacobi decoding is applied after the denoising steps, so we need to recognize that the acceleration benefits stem not only from the Jacobi iterations but also from the effectiveness of the denoising process. To systematically verify that the denoising stage alone can produce valid tokens, we conduct the following experiment: when token is successfully denoised and its distance from the first point of the Jacobi window is smaller than (where represents the length of the Jacobi window and denotes the number of denoising iterations), we immediately accept it. The ratio guarantees that the last token in the Jacobi window completes exactly denoising iterations when it reaches the left part of the window. The results of this experiment are presented in Figure 9a. According to these results, we observe that our denoising process can generate reasonable images in certain cases, particularly those featuring characters. Additionally, the immediate acceptance of denoised tokens significantly reduces the number of steps and overall latency. In comparison, as shown in Figure 9b, we reintroduce Jacobi iterations into the decoding process. These iterations serve as refinement mechanism, enhancing the image generation with more intricate details and reducing the artifacts. However, this improvement comes at the cost of increased computational steps. Therefore, to preserve image quality, we opt to keep the Jacobi iterations in our method. Analysis of unifying noise perturbation for discrete and continuous inputs. We demonstrate the feasibility of unifying noise perturbation for both the discrete and continuous inputs in the state-ofthe-art transformer-based models. Since the noise perturbation is commonly used in diffusion models, we first analyze the behavior of noisy inputs in diffusion transformer (DiT) architecture. Although noise perturbation appears to occur in the latent space, the learnable linear transformation : RdHW RDHS WS (usually implemented by 2D convolution without receptive field overlapping) before the transformer blocks causes the perturbation to actually happen in the feature space: xt = αt(W x) + σtn, where RDHS WS is random Gaussian variable formed by linear weighted sum of independent Gaussian noises with the elements of as weights, and D, HS, WS, denote feature dimension, latent height, latent width and latent dimension, respectively. Therefore, based on the above equation, the noise perturbation can be interpreted as linear combination of clean feature vector and noise vector. Since both DiTs and autoregressive models rely on transformer blocks operating in the feature space, we aim to align their noise perturbation on the input embedding. Analysis of Flops in inference. We present the average Flops per output token in Table 4. The results reveal that autoregressive decoding requires fewer Flops than SJD2. Although more Flops are used for decoding, the practical latency becomes lower. Actually, this Flops overload stems from the paradigm of all the speculative decoding methods. Their drafting-and-verification mechanism, which inherently introduces computational overhead: the number of accepted tokens per sampling step is substantially lower than the number of input draft tokens. Smaller baseline. We also implement SJD2 on Janus-pro-1B [72], an advanced autoregressive model much smaller than Lumina-mGPT [11]. The results are in Table 5 and Table 6. From the results, we observe that SJD2 still can accelerate Janus-pro without sacrificing on generated image quality, as evidenced by the following Geneval metrics [68]. Comparison to other accelerating methods. We compare our method with the recent and classic speculative/parallel decoding methods, including Lantern [47], ZipAR [53], Eagle [46] and Jacobi Table 4: The inference Flops of autoregressive models with different decoding methods."
        },
        {
            "title": "Method",
            "content": "GFlops () Latency () Lumina-mGPT [11] Lumina-mGPT + SJD2 Emu3 [4] Emu3 + SJD2 18.72 219.60 18.15 465.92 88.55s 33.64s 375.29s 147.65s Table 5: Inference performance comparison on Janus-Pro-1B [72]."
        },
        {
            "title": "Method",
            "content": "Latency () Steps () Janus-Pro-1B [72] Janus-Pro-1B + SJD2 9.1s 2.5s 576 144 Decoding [5], on COCO2017 validation set [65] with Lumina-mGPT [11] as baseline. As shown in Table 7, our approach achieves superior acceleration while maintaining comparable visual quality. Comparison to diffusion models. While many autoregressive models currently underperform state-of-the-art diffusion models in image quality and face acceleration challenges, our SJD2 narrows the speed gap. In Table 8, we evaluate inference latency for several commonly-used diffusion models (smaller than 3B) and Janus-Pro [72] at the same resolution (384 384). We set the number of sampling steps for diffusion models as 50. Results demonstrate that our SJD2 reduces latency of Janus-Pro-1B from 9.1s to 2.5s, narrowing the gap between Janus-pro and the advanced diffusion models like SD3. Moreover, this result means Janus-Pro-1B with SJD2 already outperforms SDXL in speed (2.5s vs. 4.3s). Investigation on the refinement. In this paragraph, we demonstrate that SJD2 stabilizes the refinement trajectory, as illustrated in Figure 8. Specifically, we apply SJD2 and SJD [6] on Emu3 [4] respectively, and then we examine the first five tokens from the Jacobi window in one iteration, computing the times of the token change between adjacent steps and the cumulative changes. As shown in the first five figures, SJD2 yields identical initial predictions across the 25 sampling steps. As the noise level decreases, token predictions diversify but then become unchanged at several steps, indicating the stabilization of the token trajectory. In contrast, for SJD, the tokens consistently change, appearing to oscillate and remain unstable. The last figure of Figure 8 also shows that the SJD causes more times of token change than SJD2."
        },
        {
            "title": "C Limitations and Future Work",
            "content": "Although SJD2 can achieve similar step compression in various models, the improvements on actual latency are not consistent, shown by Figure 6 and Figure 7. We speculate that this is caused by the different sizes of KV cache in different autoregressive models (the model whose tokenizer has low image compression ratio leads to large KV cache). promising direction is to stabilize the latency acceleration of Jacobi-based acceleration methods across the models with different KV caches."
        },
        {
            "title": "D Impact Statement",
            "content": "Our paper proposes new method of autoregressive text-to-image generation for research purposes. Real-world deployment would require additional safeguards beyond technical implementation. We Table 6: Visual quality on Geneval benchmark [68] with Janus-pro-1B [72] as the baseline. Method Colors Position Counting Two Color Attri Single Overall Janus-Pro-1B (tuned) [72] Janus-Pro-1B (tuned) + SJD2 0.82 0. 0.39 0.45 0.39 0.37 0.55 0.59 0.42 0.38 0.94 0.96 0.59 0. 17 Table 7: Comparison to other accelerating methods with Lumina-mGPT [11] as baseline."
        },
        {
            "title": "Configuration",
            "content": "Acceleration Latency () Acceleration Step () CLIP-Score () Autoregressive Decoding Jacobi Decoding [5] SJD [6] EAGLE [46] LANTERN [47] ZipAR [53] SJD2 1.00 1.02 2.05 2.10 2.56 1.82 2.63 1.00 1.04 2.23 2.94 3.63 4.00 4.02 31.3 31.4 31.3 33.3 32.7 31.2 31.8 Table 8: Efficiency comparison with the diffusion model. Method Latency () Steps () Janus-Pro-1B [72] SD1.5 [55] SDXL [39] SD3-Medium [70] Janus-Pro-1B + SJD2 9.1s 1.7s 4.3s 1.7s 2.5s 576 50 50 50 Figure 8: The trajectories of token difference. 18 recognize that this open technical accessibility could lead to potential societal risks, including misuse for generating misleading content or harmful biases. Fortunately, this problem can be alleviated by strict dataset filtering to exclude harmful content. (a) The images generated with the immediate acceptance of denoised tokens (without Jacobi iterations). (b) The images generated with the combination of Jacobi iterations and denoising. Figure 9: Analysis of our denoising process. (a) Our denoising process without Jacobi iterations can generate reasonable images with few model forward passes and small latency. (b) The further Jacobi iterations refine the results of our denoising process, resulting in more details and fewer artifacts."
        }
    ],
    "affiliations": [
        "CUHK",
        "Huawei Noahs Ark Lab",
        "The University of Hong Kong",
        "Tsinghua University"
    ]
}