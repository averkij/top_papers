{
    "paper_title": "Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning",
    "authors": [
        "Guanting Dong",
        "Yifei Chen",
        "Xiaoxi Li",
        "Jiajie Jin",
        "Hongjin Qian",
        "Yutao Zhu",
        "Hangyu Mao",
        "Guorui Zhou",
        "Zhicheng Dou",
        "Ji-Rong Wen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, large language models (LLMs) have shown remarkable reasoning capabilities via large-scale reinforcement learning (RL). However, leveraging the RL algorithm to empower effective multi-tool collaborative reasoning in LLMs remains an open challenge. In this paper, we introduce Tool-Star, an RL-based framework designed to empower LLMs to autonomously invoke multiple external tools during stepwise reasoning. Tool-Star integrates six types of tools and incorporates systematic designs in both data synthesis and training. To address the scarcity of tool-use data, we propose a general tool-integrated reasoning data synthesis pipeline, which combines tool-integrated prompting with hint-based sampling to automatically and scalably generate tool-use trajectories. A subsequent quality normalization and difficulty-aware classification process filters out low-quality samples and organizes the dataset from easy to hard. Furthermore, we propose a two-stage training framework to enhance multi-tool collaborative reasoning by: (1) cold-start fine-tuning, which guides LLMs to explore reasoning patterns via tool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with hierarchical reward design, which reinforces reward understanding and promotes effective tool collaboration. Experimental analyses on over 10 challenging reasoning benchmarks highlight the effectiveness and efficiency of Tool-Star. The code is available at https://github.com/dongguanting/Tool-Star."
        },
        {
            "title": "Start",
            "content": "Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning Guanting Dong1, Yifei Chen1, Xiaoxi Li1, Jiajie Jin1, Hongjin Qian2, Yutao Zhu1 Hangyu Mao3, Guorui Zhou3, Zhicheng Dou1, Ji-Rong Wen1 2BAAI 3Kuaishou Technology 1Renmin University of China {dongguanting, dou}@ruc.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Recently, large language models (LLMs) have shown remarkable reasoning capabilities via large-scale reinforcement learning (RL). However, leveraging the RL algorithm to empower effective multi-tool collaborative reasoning in LLMs remains an open challenge. In this paper, we introduce Tool-Star, an RL-based framework designed to empower LLMs to autonomously invoke multiple external tools during stepwise reasoning. Tool-Star integrates six types of tools and incorporates systematic designs in both data synthesis and training. To address the scarcity of tool-use data, we propose general tool-integrated reasoning data synthesis pipeline, which combines tool-integrated prompting with hint-based sampling to automatically and scalably generate tool-use trajectories. subsequent quality normalization and difficulty-aware classification process filters out low-quality samples and organizes the dataset from easy to hard. Furthermore, we propose two-stage training framework to enhance multi-tool collaborative reasoning by: (1) cold-start fine-tuning, which guides LLMs to explore reasoning patterns via toolinvocation feedback; and (2) multi-tool self-critic RL algorithm with hierarchical reward design, which reinforces reward understanding and promotes effective tool collaboration. Experimental analyses on over 10 challenging reasoning benchmarks highlight the effectiveness and efficiency of Tool-Star. The code is available at https://github.com/dongguanting/Tool-Star. 5 2 0 2 2 2 ] . [ 1 0 1 4 6 1 . 5 0 5 2 : r Figure 1: Overview result comparison of Tool-Star and other TIR baselines. Left: Average tooluse accuracy (TE) on knowledge-intensive and computational reasoning datasets. Right: The performance overview on 10 challenging reasoning datasets. Corresponding author. Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in large language models (LLMs) have demonstrated remarkable reasoning capabilities empowered by large-scale reinforcement learning (RL) [59, 21, 57, 36, 79]. Advanced models exemplified by Deepseek R1 [9] and OpenAI o1 [41] exhibit diverse emergent behaviors in Chain-of-Thought (CoT) reasoning, including deep thinking, self-reflection, thereby improving problem-solving performance on complex reasoning tasks [48, 40]. However, real-world reasoning scenarios often require models to integrate multiple capabilities through interaction with the environment, such as deep information seeking, long-term knowledge memory, and precise computation. To achieve this, Tool-Integrated Reasoning (TIR) [17, 28, 67, 72, 42] approaches encourage LLMs to interact with external tools during the reasoning process, substantially expanding the paradigm from language-only reasoning to broader scope of applications. Current foundational work on TIR primarily focuses on distilling tool-use trajectories from strong models and applying supervised fine-tuning (SFT) to guide weaker models via imitation learning [17, 27, 76, 22]. As large reasoning models exhibit emergent capabilities, subsequent efforts aim to enhance long chain-of-thought reasoning in LLMs through tool-integrated prompting design [2, 30, 31, 69, 62]. However, these approaches rarely enable LLMs to autonomously discover effective reasoning patterns of tool usage. To address this limitation, recent studies introduce outcome-based rewards [4, 51], extending reinforcement learning to the TIR domain [53, 23, 1, 16, 35, 64, 46]. While RL-based methods encourage exploration of efficient tool-use behaviors, they primarily focus on interacting with the individual tool. Notably, real-world reasoning tasks often require LLMs to possess both accurate computation and dynamic information-seeking capabilities.2 This necessitates deeper integration of feedback from multiple tools, particularly search engines and code generators. The lack of systematic investigation into multi-tool collaborative reasoning presents major bottleneck for practical TIR deployment. In this work, we aim to bridge this gap by addressing the following research questions: 1. Rationality and efficiency of tool usage. How to enable the model to perform efficient toolaugmented reasoning while balancing the cost of tool invocation. 2. Multi-tool collaborative reasoning. How to effectively enable models to integrate the functionalities of multiple tools throughout the reasoning process. In this paper, we propose Tool-Star, reinforcement learning-based framework designed to empower LLMs to autonomously invoke multiple external tools during stepwise reasoning. Specifically, Tool-Star integrates six types of tools into the reasoning process (three for training and three for inference-time optimization) and incorporates systematic designs in both data synthesis and training algorithms. To address the scarcity of tool-use data, we first design general Tool-Integrated Reasoning Data Synthesis Pipeline that combines tool-integrated prompting with hint-based sampling to automatically generate large-scale tool-use trajectories. We then introduce quality normalization and difficulty-aware classification process to effectively filter out unreasonable tool-use samples and partition the data in curriculum-like manner from easy to hard [54, 11, 12]. Leveraging this pipeline, we construct high-quality datasets for both cold-start fine-tuning and reinforcement learning in staged manner, laying solid foundation for subsequent TIR training. To incentivize the models capability for multi-tool collaboration, we propose two-stage TIR training framework that progressively aligns reasoning abilities in an easy-to-hard manner. (1) In the first stage, we introduce Cold-Start Supervised Fine-Tuning strategy, allowing LLMs to initially explore reasoning patterns with feedback from tool invoking. (2) In the second stage, we develop Multi-Tool Self-Critic Reinforcement Learning Algorithm. Unlike prior RL approaches that focus on single-tool usage, our method employs hierarchical reward mechanism that not only evaluates answer correctness and tool-use format but also assigns additional rewards for effective multi-tool collaboration. To further improve the models understanding of this complex reward structure, we interleave self-critic reward fine-tuning phase within the standard RL process, facilitating the internalization of reward principles. To comprehensively evaluate the capability of Tool-Star, we design experiments across more than 10 challenging computational reasoning tasks (e.g., AIME24 and MATH500 [37]) and knowledge2https://openai.com/index/introducing-o3-and-o4-mini/ intensive reasoning tasks (e.g., WebWalker [68] and HotpotQA [73]). As illustrated in Figure 1, Tool-Star demonstrates strong overall reasoning performance while ensuring both efficiency and reliability in tool usage. In summary, our main contributions are as follows: We propose Tool-Star, two-stage training framework to enhance multi-tool collaborative reasoning in LLMs: (1) Cold-Start fine-tuning enables LLMs to explore reasoning patterns guided by toolinvocation feedback; (2) Multi-Tool self-critic RL algorithm with hierarchical reward design reinforces the LLMs grasp of reward principles and promotes effective multi-tool collaboration. To address the scarcity of tool-use data, we propose general TIR data synthesis pipeline that combines tool-integrated prompting with hint-based sampling to automatically and scalably generate tool-use trajectories. We further introduce quality normalization and difficulty-aware classification process to filter out unreasonable samples and organize the dataset from easy to hard. Experiments on more than 10 challenging reasoning datasets validate the effectiveness of Tool-Star. Further quantitative analyses demonstrate its high efficiency, offering insights for incentivizing multi-tool collaborative reasoning."
        },
        {
            "title": "2 Related Work",
            "content": "Tool-Integrated Reasoning. Tool-Integrated Reasoning (TIR) has emerged as promising paradigm for enhancing LLM reasoning by enabling autonomous invocation of external tools. Existing TIR approaches can be broadly categorized into three streams: (1) Prompting-based methods guide models to use tools via carefully crafted prompts without additional training [2, 30, 38, 31, 69, 25, 62]. While easy to implement, they often suffer from instability and limited accuracy in tool usage. (2) SFT-based methods apply supervised fine-tuning to teach weaker models tool use by distilling trajectories from stronger models [17, 27, 76, 22, 33, 10, 13, 28]. Though effective, their performance is constrained by the quality of demonstrations and limited generalization beyond seen examples. (3) RLbased methods optimize tool-use behavior via outcome-driven rewards [53, 56, 23, 1, 16, 35, 32, 52], enabling models to autonomously discover effective tool-use strategies through exploration. This often leads to more robust and adaptive tool utilization than imitation-based approaches. Despite these advances, most work focuses on single-tool settings. Multi-tool collaborative reasoningrequiring coordination across heterogeneous tools (e.g., search engines, code interpreters)remains underexplored. While systems like OTC [64] and ToolRL [45] have made initial progress, there is still significant gap in developing specialized RL algorithms and reward mechanisms tailored for multi-tool collaboration. Reinforcement Learning for Reasoning. Recent advances in reinforcement learning (RL) have significantly enhanced reasoning capabilities in language models. Large-scale RL implementations such as DeepSeek-R1 [9], OpenAI-o1 [41], and QwQ [60] have demonstrated the effectiveness of RL for improving reasoning performance. Concurrently, several studies have explored achieving emergent reasoning capabilities with minimal resources, either through high-quality but small-scale datasets or smaller parameter models [79, 74, 34]. Notable examples include SimpleRL-Zoo [79], LIMO [74], LIMR [34], and Qwens mathematical reasoning approach [71], with additional insights from works examining RL effectiveness in both small [7] and large models with minimal examples [66]. series of research has focused on analyzing RL training strategies and reward function design for reasoning tasks, providing practical insights for broader applications. Key contributions include PRIME [6], which explores process reinforcement through implicit rewards, empirical studies on improving R1-like reasoning models [3, 75], and DAPOs scalable RL system [77]. Another research direction has concentrated on optimizing reasoning efficiency through specialized RL algorithms, as demonstrated in works on concise reasoning [15, 47, 55]. More recently, researchers have begun integrating external tools to enhance reasoning accuracy, with notable examples including ToRL [35], ReTool [16], ToolRL [46], and OTC [64]. However, exploration of multi-tool collaborative reasoning remains relatively limited, highlighting significant gap in current research."
        },
        {
            "title": "3 Methodology",
            "content": "Tool-Star is an RL-based framework that empowers LLMs to autonomously invoke multiple external tools during reasoning. As shown in Figure 2 and Figure 3, Tool-Star includes two key components: (1) tool-integrated data synthesis pipeline that generates large-scale, high-quality reasoning 3 trajectories with reasonable tool usage (3.1); (2) two-stage training paradigm that facilitates multi-tool collaborative reasoning, comprising cold-start fine-tuning and multi-tool self-critic RL algorithm (3.2). We delve into the specifics of our approach as follows. Problem Formulation. Multi-tool Collaborative Reasoning aims to enable LLMs to perform multistep reasoning through interaction and collaboration with multiple external tools. Specifically, given task query and an external tool set , Tool-Star autonomously invokes tools during the generation of the reasoning chain Rc, guided by the tool-integrated instruction . The real-time tool-call feedback is dynamically concatenated into the reasoning chain to facilitate ongoing inference until the final output is produced. This process can be modeled as: Pθ(Rc, IT , q, ) = (cid:89)Tc t=1 (cid:124) Pθ(Rc <t, , q, {F }<t) Rc (cid:125) (cid:123)(cid:122) Multi-Tool Integrated Reasoning (cid:89)Ty t= (cid:124) Pθ(yt y<t, Rc, , q) (cid:125) (cid:123)(cid:122) Answer Generation , (1) is the token at position t, and <t represents all tokens generated before t. {F }<t denotes the feedback of all tool calls prior to where Tc denotes the number of tokens in the reasoning chain Rc, Rc Rc t. Ty is the length of the Answer y, with yt as the token at position t. Tool Design. In this work, we design six tools to enhance the reasoning capabilities of LLMs within the Tool-Star framework. During the two-stage training process, we introduce three core tools that enable LLMs to autonomously invoke external functionalities during reasoning: Search Engine: Executes search queries to retrieve relevant information, supporting both local and web-based search modes. Web Browser Agent: Parses web search results by visiting URLs, extracting relevant content, and summarizing key information in response to the query. Code Interpreter: Executes code snippets generated by the LLM in sandbox environment, returning either the execution results or error messages based on code correctness. During inference, we introduce three Inference-time tools to optimize the reliability of TIR process: Code Debugger: Automatically corrects LLM-generated erroneous code by leveraging the original code and compiler error messages to guide revisions. Tool-Use Backtracer: Locates and rolls back to the reasoning step preceding failed tool invocation, enabling the model to resume and revise its reasoning path. Reasoning Chain Refiner: When the output exceeds the maximum length, refiner prunes and optimizes redundant steps in the reasoning process, replacing original reasoning chain with more concise and coherent version to complete the task. 3.1 Tool-Integrated Reasoning Data Synthesis In this section, we present our tool-integrated reasoning data synthesis pipeline, aiming to enable automated and scalable construction of high-quality tool-use datasets (Figure 2). 3.1.1 Step-1: Data Collection and Sampling To balance data scale and diversity while ensuring accessibility, we curate high-quality training set from open-source knowledge-based and computational reasoning datasets, comprising approximately 90K text-based reasoning data (Dtext) and 1K existing TIR datasets (Dtool), as summarized in Table 5. To further expand TIR trajectories, we introduce two complementary sampling strategies: (1) TIR Prompting-based Sampling. We design tool-integrated prompt to guide the LLM Pθ in sampling responses for queries in Dtext. Following the Eq.(2), LLM decodes tool-invoking requests within special tokens (e.g., <search> ... </search> , <python> ... </python> ) during trajectory generation. We then automatically parse and extract these requests, invoke external tools to obtain feedback , and insert the tool feedbackenclosed within <result> and </result> tagsback into the reasoning chain as additional context for subsequent generation steps. This process iterates until either (1) the maximum number of tool calls or length is reached, or (2) the model generates final answer, enclosed by designated tokens <answer> and </answer> . After inference on the entire Dtext, we filter for correct samples to obtain the dataset DP tool. 4 Figure 2: The overview of 3-step tool-integrated reasoning data synthesis pipeline. (2) Hint-based Sampling. To further diversify tool invoking patterns, we employ hint-based method [29] that inserts hint tool-call tokens into language-only reasoning trajectories. We first prompt the LLM to perform language-only reasoning on queries from Dtext. Following the START [28], we propose two hint instructionsLogical Verification and Answer Reflectionto insert tool-invoking hints into the original reasoning chains. As shown in Figure 2, logical verification hints randomly replace uncertain expressions (e.g., maybe, wait, not sure) in the chain, while reflection hints are inserted after the answer. These diverse hints facilitate the model to invoke tools when information is insufficient or after answer generation, enabling information completion and answer verification. After inserting hints, we truncate the original reasoning chain at the hint position, prompting the model to perform tool-augmented reasoning in response to the hint, which can be formulated as: Pθ(Rc >tH , IT , q, Rc tH , ) = (cid:89)Tc t=tH Pθ(Rc t, , q, {F }t) Rc (cid:125) (cid:123)(cid:122) Hint-based Tool-Integrated Reasoning (cid:124) (cid:89)Ty t=1 (cid:124) Pθ(yt y<t, Rc, , q) , (cid:125) (cid:123)(cid:122) Answer Generation Unlike Eq.(2), tH is the end position of the hint insertion, and Rc reasoning chain before the hint position. The LLM continues reasoning from Rc the tool-integrated reasoning chain Rc filtering the samples for the correct answers, we obtain the hint-based TIR dataset DH merge two datasets to obtain the final dataset Dv (2) represents the language-only with to derive >tH . Other variables are defined consistently with Eq.(2). After tool. Finally, we tool = {DH tool DM tool Dtool}. 3.1.2 Step-2: Tool-Use Quality Normalization. To ensure the rationality of tool usage within each sample, we implement the following 3 TIR normalization strategies for tool-use data quality control: (1) Tool-call Frequency Control: Remove samples with tool-call frequency exceeding predefined threshold β to alleviate excessive tool invocation. (2) Duplicate Tool-call Removal: Eliminate samples containing redundant tool calls, such as repeated generation of identical search queries or code snippets in the same response. (3) Format Normalization: Standardize tool call formats in reasoning chains by unifying special tokens for invocation, feedback, and final answers, while ensuring balanced usage of start and end tokens. By applying these criteria, we obtain quality-filtered dataset denoted as Dv tool. 3.1.3 Step-3: Difficulty-Aware Data Classification. Considering the computational overhead of tool-use and the multi-stage nature of TIR training, we argue that high-quality tool-use dataset should meet the following criteria: (1) Invoke tools only when necessary: Tool calls should be avoided when the model is capable of solving the problem through direct reasoning. (2) Organize samples from easy to hard: As emphasized in prior RL tool learning [53, 81], stage-wise training based on sample difficulty is crucial for effective learning. To achieve the above objectives, we first perform language-only reasoning pass on each question in the dataset Dv2 text. Based on the correctness of both direct reasoning (DR) and tool-integrated reasoning (TIR), each sample is categorized into four tool, yielding direct reasoning result denoted as Dv2 types (Figure 2). For categories 1 and 2, where the model already produces correct answers through direct reasoning, tool use is deemed unnecessary. These samples are extracted from Dv2 text to form subset Dsub text. For category 3, which clearly illustrates the advantages of tool use over language-only tool to construct the subset Dsub reasoning, we directly sample from Dv2 tool. tool by combining Dsub To support curriculum learning paradigm [11] from easy to hard, we construct cold-start finetuning dataset DSFT tool. For Category-4 samples, which are challenging for both DR and TIR, are treated as hard examples and reserved for reinforcement learning, forming the dataset DRL tool. This design enables the LLM to acquire basic tool-use capabilities through coldstart fine-tuning, and subsequently generalize to more complex scenarios during the RL phase (e.g. multi-tool collaboration), thereby facilitating progressive learning trajectory. text and Dsub 3.2 Tool-Star: Two-stage Multi-tool Collaborative Training Framework In this section, we propose two-stage multi-tool collaborative training framework that generalize the LLMs TIR capability from single to multiple tool-use paradigm, and from easy to hard learning. Cold-Start Supervised Fine-tuning. To equip the LLM with an initial understanding of tool usage for problem solving, given (xi, yi) DSFT tool , we apply the standard Supervised Fine-tuning objective on the backbone model Pθ with parameters θ: L(θ) = (cid:80) log Pθ(yi xi) , where xi denotes the i-th input. Ultimately, we obtain cold-start LLM ˆπθ with initial TIR capability. (xi,yi)DSFT tool Multi-Tool Self-Critic Reinforcement Learning. We will introduce how the LLM learn to autonomously invoke tools, including Code interpreter, search engine and web browser agent. (1) Memory-based Roll-Out with Tools. As shown in Figure 3, we employ multi-tool invocation instructions to guide the model in decoding tool-use requests into special tokens (e.g., <python> ) during the roll-out process. Upon detecting these tokens, the corresponding tool is automatically invoked, and the resulting feedback is integrated back into the reasoning chain. To reduce latency caused by frequent tool calls, we incorporate memory mechanism that caches the mapping between each tool request and its output. This allows the model to retrieve responses for repeated requests directly from memory, thereby improving efficiency. (2) Hierarchical Reward Design. Reward signals serve as the optimization objective and directly guide the behavior of the policy model during training. Distinct from previous tool-use RL approaches, we not only design correctness and format rewards for LLMs, but also introduce multi-tool collaborative rewards. This design aims to encourage multiple tool usage while maintaining LLM correctness. Specifically, when both the answer and tool invocation format are correct, and the model employs multiple tools (i.e., both <search> and <python> appear in the reasoning chain), an extra reward rM is granted. Formally, the overall reward is defined as follows: = max(Acc. + rM, Acc.) 0 1 If Format is Good & Acc.>0 If Format is Good & Acc.=0 Otherwise (cid:40) , rM = 0.1 If ( <search> & <python> ) 0 Otherwise (3) (3) Self-Critic RL Algorithm. Multi-tool RL algorithms involve complex reward structures, making it challenging for LLMs to discover optimal behavior. To address this, we propose self-critic RL algorithm that enables LLMs to better align with the design principles of reward mechanisms. As illustrated in Figure 3, we begin by performing steps of vanilla RL training on the cold-start model ˆπθ. To optimize the policy, we adopt Group Relative Policy Optimization (GRPO) [51] as our RL algorithm, which estimates the baseline using group of rollouts. LGRPO(θ) = E[q DRL tool, {oi}G i=1 ˆπθold (Oq)] 1 G (cid:88) i=1 1 oi oi (cid:88) t=1 (cid:26) min (cid:20) ˆπθ(oi,tq, oi,<t) ˆπθold (oi,tq, oi,<t) ˆAi,t, clip (cid:18) ˆπθ(oi,tq, oi,<t) ˆπθold (oi,tq, oi,<t) , 1 ϵ, 1 + ϵ (cid:19) (cid:21) ˆAi,t βDKL [ˆπθˆπref ] (4) (cid:27) , where ϵ and β are hyper-parameters, and denote query and models output with tools. Ai is the normalized advantage of the i-th rollout in group. Then we can obtain an RL-initialized model πRL θ . Subsequently, we detail our self-critic reward finetuning phrase, which help LLMs better internalize the reward structure. As shown in Figure 3, we start by randomly rejection sampling examples 6 Figure 3: The overall framework of Multi-Tool Self-Critic Reinforcement Learning. tool to construct DS tool [61, 78]. For each query DS from the RL training set DRL tool, the RL initialized self-samples candidate responses, forming diverse QA set (q, {ai}N model πRL i=1). Notably, θ our hierarchical rule-based reward function acts as an executable program that automatically assigns reward label to each response. This enables on-policy reasoning by providing each ai with corresponding reward. We then construct reward-aware dataset Dcritic i=1, where each input xi is query q, and each output yi includes candidate response ai, its reasoning trace pi, and reward score ri. We treat samples with ri 1 as positive and those with ri < 1 as negative, forming preference dataset (x, yw, yl) Dcritic tool . Finally, we fine-tune the RL-initialized model using DPO objective as: tool = {(xi, yi)}N LDPO(πRL θ ; πref) = E(x,yw,yl)D[logσ(βlog πRL θ (ywx) πref(ywx) βlog πRL θ (ylx) πref(ylx) )], (5) where the reference model πref is set to πRL initially and remains fixed throughout training. β is θ hyperparameter and σ is the sigmoid function. LDPO encourages higher log-probability for preferred responses yw over dispreferred ones yl. We interleave self-critic reward fine-tuning every steps during standard RL training. This iterative process allows the model to progressively learn behavioral distinctions via self-sampling and evaluation, implicitly capturing the hierarchical reward structure. The full algorithm is detailed in Algorithm 1. 3.3 Multi-tool Collaborative Inference During inference, we follow Eq.(2), enabling the model to autonomously use the search engine, web browser agent, and Python interpreter. To improve TIR robustness, we introduce three inference-time mechanisms targeting common failure scenarios: Code Execution Error: When generated code contains syntax errors, the Code Debugger uses compiler messages and the original code to produce executable fixes. Tool Invocation Failure: If tool calls yield no useful output or debugging fails, the Backtracer identifies the statement before the tool-invoking token and restarts inference from there. Inference Length Overflow: When tool outputs exceed the context limit, the Chain Refiner compresses and restructures the reasoning chain by removing redundant or irrelevant content, allowing continued inference without tools. These mechanisms enhance tool invocation reliability and provide an effective inference-time scaling strategy. Further details are listed in the Appendix B.2. Table 1: Overall results on 10 challenging reasoning tasks. The top two results are highlighted in bold and underlined. All baseline methods use Qwen2.5-3B-Instruct as the backbone. For baselines lacking 3B Instruct version, we replicate them using the exact parameter settings from their opensource code. To ensure fairness, we report the RAG setting (Top-5 Docs) for the code-assistant model in knowledge-intensive reasoning. The abbreviations of the datasets: HQA (HotpotQA), 2Wiki. (2wikiMultiHopQA), MuSi. (MuSiQue), and Bamb (Bamboogle). Method Computational Reasoning Knowledge-Intensive Reasoning AIME24 AIME25 MATH500 GSM8K MATH WebWalker HQA 2Wiki. MuSiQ. Bamb. Avg. Qwen2.5-3B-Instruct Llama3.2-3B-Instruct 10.0 0.0 6.7 3.3 20.0 3.3 Code-Assistant Reasoning Methods ToRL DotaMath Search-Enhanced Reasoning Methods RAG Search-o1 Search-R1 13.3 16.7 0. 10.0 3.3 3.3 10.0 6.7 Multi-Tool Integrated Reasoning Methods Multi-Tool Prompting ReCall Tool-Star(Qwen2.5-3B) Tool-Star(Llama3-3B) 3.3 16.6 20.0 10.0 3.3 6.7 16.7 10.0 63.0 40. 72.0 56.2 54.0 69.0 26.0 54.2 63.0 72.0 54.8 75.0 71.2 71.6 58.2 0.5 0. 9.7 12.5 9.4 9.2 84.4 78.2 46.0 34.0 43.0 48.8 77.8 85.0 77.8 81.0 71. 56.0 63.0 44.0 59.6 74.2 82.6 70.6 12.0 11.5 14.6 13.0 14.4 9.2 13.0 20.8 24.0 37.9 27.0 35.6 31. 39.4 31.2 34.9 28.9 43.2 25.5 15.9 16.2 43.5 38.9 51.9 40.0 48.9 43.2 3.6 4.0 8.3 7.5 10.3 9.6 16.5 6.5 16.5 19.3 16. 11.7 26.1 18.3 21.7 25.4 37.8 23.8 32.6 17.4 29.0 35.1 30.2 40.8 25.7 17.8 23.4 40.8 39.1 52.5 46.1 54.7 41."
        },
        {
            "title": "4 Experiments",
            "content": "Datasets. To comprehensively evaluate the tool-use capabilities of our model, we conduct experiments on two types of reasoning datasets: (1) Math reasoning benchmarks, including AIME2024, AIME2025 3, MATH500 [37], MATH [18], and GSM8K. (2) Knowledge reasoning benchmarks, including three complex reasoning QA tasks: GAIA [39], HLE [43], and WebWalker [68]; and three Wikipedia-based open-domain QA tasks: HotpotQA [73], 2WikiMultihopQA [19], and Musique [63]. For all tasks, we follow previous work [31] and extract answers from the model output enclosed in box{}. Evaluation Metrics. For computational reasoning tasks, we follow Search-o1 [31] employ LLMbased judging (Qwen2.5-72B=Instruct) to ensure answer correctness. For open-domain QA tasks, we adopt token-level F1 score as the evaluation metric. To assess tool usage efficiency, we propose the Tool-use Efficiency metric: TE = 1 denote the number of correct answers and the number of total samples for the i-th dataset when using tools. where is the number of datasets, Si and (cid:80)N Si i= Baselines. Our experiments include the following three categories of baselines: (1) Proprietary Models: Qwen2.5 [49], Llama3.2 [14]. (2) Single-tool Enhanced Baselines: Code-enhanced models such as ToRL [35] and DotaMath [27], and search-enhanced models such as Vanilla RAG [26], Search-o1 [31] and Search-R1 [23]. (3) Multi-tool Enhanced Baselines: Multiple tools integrated prompting and ReCall [1]. More details of implementations are provided in the Appendix C. 4.1 Results on Challenging Reasoning Tasks Our main results are presented in Table 1. Overall, Tool-Star consistently outperforms all baselines, decisively establishing the superiority. Furthermore, we have identified the following insights: (1) TIR prompting fails to explore the better tool-use behavior. Focusing on Search-o1 and Multi-Tool Prompting, their performance on comprehensive reasoning tasks remains suboptimal. Notably, multi-tool prompting strategies even exhibit lower consistency compared to their backbone 3https://huggingface.co/datasets/AI-MO/aimo-validation-aime Figure 4: Left: Tool-use efficiency comparison across TIR methods. Middle: Performance analysis of TIR methods with inference-time tool. Right: Error statistics of tool invocation. model. This reveals that relying solely on prompt engineering to elicit tool usage is insufficient for guiding the LLM toward effective tool utilization, thereby limiting its reasoning capabilities. (2) Single-tool RL-based methods exhibit strong domain specialization but limited generalizability. Search-enhanced RL approaches perform well on knowledge-intensive tasks (e.g., 40+% F1 on HQA), but their accuracy drops sharply by over 20% on computation-heavy tasks like MATH500 and AIME24 compared to the backbone model. Conversely, code-assistant methods like ToRL underperform on knowledge-based tasks, even falling behind RAG baselines. These trends underscore the specialization bias of single-tool RL methods and their limited cross-domain transferability. (3) Tool-Star demonstrates strong and versatile reasoning, along with robust plug-and-play capability. Tool-Star consistently outperforms both singleand multi-tool baselines, achieving over 40% average accuracy across 10 datasets while maintaining competitive performance on individual tasks. Notably, it also delivers substantial gains across different backbone models, with average improvements nearing 20% on both Qwen and LLaMA. These results highlight Tool-Stars generality, efficiency, and strong adaptability across models and tasks. To further validate Tool-Stars generalizability, we present results on deep web exploration and case study in Appendix A.1 and E. 4.2 Quantitative Analysis Method Knowledge. Table 2: Ablation study on Tool-Star(3B). Ablation Study. In this section, we conduct systematic ablation study in Table 2, where w/o denotes removing specific component. The results demonstrate that: (1) Removing any single module leads to performance degradation, highlighting the critical role of each part of design. (2) Excluding either results in significant performance drops. the Cold-Start phase helps the model initially acquire tooluse capabilities, while the RL phase promotes generalization in multi-tool reasoning. Both stages are indispensable and complementary. (3) Incorporating hierarchical rewards and selfcritic mechanism on top of vanilla RL consistently brings further improvements, confirming the effectiveness of our tailored RL strategy. w/o Cold-Start 43.5 (-8.4) 40.8 (-11.7) 77.8 (-7.2) 74.2 (-8.4) w/o RL stage 47.5 (-4.4) 43.9 (-8.6) 80.2 (-4.8) 78.4 (-4.2) w/o Reward 50.4 (-1.5) 50.3 (-2.2) 83.1 (-1.9) 80.2 (-2.4) w/o Self-Critic 49.8 (-2.1) 48.3 (-4.2) 82.8 (-2.2) 77.8 (-4.8) Bamb. GSM8K MATH Computational. Tool-Star (3B) HQA 51.9 52.5 85.0 82. Tool-Use Efficiency Analysis. To validate whether Tool-Star can efficiently invoke tools during stepwise reasoning, we compare the Tool-use Accuracy TE of various TIR methods on knowledgeintensive and computation-based reasoning datasets, as shown in Figure 4 (left). Results indicate that while Search-R1 and ToRL achieve high accuracy in knowledge and computation tasks respectively, they underperform in the other domainhighlighting the tendency of single-tool RL methods to specialize in specific reasoning types. In contrast, Tool-Star consistently maintains high tool-use accuracy across both task types and baselines, demonstrating its efficiency in tool utilization. 9 Local Search VS. Web Search. Inspired by R1Searcher [53] and DeepResearch [81], we further compared the differences between local search and web search of Tool-Star-3B in knowledge-intensive reasoning tasks. As shown in Figure 5, we found that after using web search, except for HotpotQA which did not show obvious improvement, the other three domains all demonstrated significant improvements, especially 2wiki (+13% F1) and Bamboogle (+8% F1). Regarding the above phenomena, we believe that the performance differences between the two mainly stem from: (1)The retrieval library of wiki is subset of the web page corpus. Moreover, when combined with the browser agent for web page information, it can provide more accurate and concise information compared to wiki documents. (2) We use the Bing API 4 for web search, and the retrieval models and strategies of their services perform better than the E5-base we deployed offline for local search. Figure 5: Performance Comparison of ToolStar(3B) in Local and Web Search Modes Inference-Time Tool-Use Analysis. To verify the effectiveness of inference-time tool design, we further analyze its impact on the performance of Tool-Star and DotaMath across two datasets. As shown in Figures 4 (middle) and (right), both models exhibit notable reduction in tool-use errors during reasoning after applying inference-time tool optimizations, accompanied by significant performance improvements. These results not only underscore the detrimental effect of incorrect tool usage on reasoning accuracy, but also reinforces the motivation for incorporating inference-time tools. Notably, the performance gain for DotaMath is substantially larger than that for Tool-Star, with improvements exceeding 20% on GSM8K. This suggests that inference-time tools are particularly beneficial for models with weaker coding capabilities, while models with stronger coding proficiency, such as Tool-Star, naturally make fewer tool-related errors and thus have less room for improvement. Scaling Analysis on Parameter Scales. To investigate the effectiveness of Tool-Star across different parameter scales, we display the RL training curves of Qwen2.5 models with 0.5B, 1.5B, and 3B parameters in Figure 6. Our key observations are as follows: (1) All models achieve good reward scores at the beginning of training (step 0), benefiting from the cold-start SFT stage, which significantly reduces the exploration burden in early RL; (2) As training progresses, the average reward scores steadily improve across all model sizes, indicating that our self-critic RL framework further improve TIR capabilities on top of the cold-start initialization. (3) Notably, the average reward shows signs of emergent improvement around step 10, while no clear inflection point (i.e., aha moment) is observed in response length. Instead, response lengths gradually stabilize, suggesting convergence toward optimized response patterns. More training details can be found in Appendix A.3. Figure 6: Training curves of reward and response length for models with different parameters. Scaling Analysis on the RL Training Curve. To better understand the training dynamics of the reinforcement learning process, we present the trends of KL penalty, average reward score, average response length, and validation score over the first 100 steps RL training (Figure 7). Our key observations are as follows: All metricsincluding reward score, KL penalty, and validation scoreincrease steadily with training steps, indicating the stability of our self-critic RL training method. The initial reward scores of the 3B and 1.5B models are already above 0 (within possible range of -1 to 1.1), with corresponding validation scores starting at approximately 0.375 and 0.29 respectivelysubstantially higher than the vanilla instruct models score of below 0.15. These results demonstrate the effectiveness of our cold-start strategy. In contrast to RL methods trained entirely from scratch [35, 53, 23], the cold-start approach endows the model with baseline level of TIR capability from the outset, significantly reducing the need for extensive black-box exploration during the early stages of RL training. 4https://www.microsoft.com/en-us/bing/apis/bing-web-search-api 10 Figure 7: The scaling relationship of kl loss, reward, response length and validation score for Tool-Star(Qwen2.5-3B & 1.5B in instruct version). After cold-start SFT, the reward score shows mild emergence within the first 020 steps, though less pronounced than in from-scratch RL (i.e., no sudden rise-and-drop). Similarly, the response length does not fluctuate sharply but gradually converges to stable value, further supporting the stability benefits of cold-start training."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we introduce Tool-Star, an RL-based framework designed to enable LLMs to autonomously invoke multiple external tools during stepwise reasoning. To address the scarcity of tool-use data, Tool-Star employs scalable TIR data synthesis pipeline, incorporating normalization and difficulty-aware data classification process. Furthermore, we propose two-stage training framework to enhance multi-tool collaborative reasoning in LLMs, consisting of Cold-Start Finetuning phase and Multi-Tool Self-Critic RL stage. This framework progressively fosters effective multi-tool collaboration and strengthens the LLMs understanding of reward principles. Extensive experiments across over ten challenging benchmarks demonstrate that Tool-Star significantly improves both the effectiveness and efficiency of tool-augmented reasoning in LLMs."
        },
        {
            "title": "References",
            "content": "[1] Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Z. Pan, Wen Zhang, Huajun Chen, Fan Yang, Zenan Zhou, and Weipeng Chen. Research: Learning to reason with search for llms via reinforcement learning, 2025. [2] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Trans. Mach. Learn. Res., 2023, 2023. [3] Zhipeng Chen, Yingqian Min, Beichen Zhang, Jie Chen, Jinhao Jiang, Daixuan Cheng, Wayne Xin Zhao, Zheng Liu, Xu Miao, Yang Lu, Lei Fang, Zhongyuan Wang, and Ji-Rong Wen. An empirical study on eliciting and improving r1-like reasoning models. CoRR, abs/2503.04548, 2025. [4] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021. 11 [5] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [6] Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, Jiarui Yuan, Huayu Chen, Kaiyan Zhang, Xingtai Lv, Shuo Wang, Yuan Yao, Xu Han, Hao Peng, Yu Cheng, Zhiyuan Liu, Maosong Sun, Bowen Zhou, and Ning Ding. Process reinforcement through implicit rewards. CoRR, abs/2502.01456, 2025. [7] Quy-Anh Dang and Chris Ngo. Reinforcement learning for reasoning in small llms: What works and what doesnt. CoRR, abs/2503.16219, 2025. [8] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR, 2023. [9] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. [10] Guanting Dong, Keming Lu, Chengpeng Li, Tingyu Xia, Bowen Yu, Chang Zhou, and Jingren Zhou. Self-play with execution feedback: Improving instruction-following capabilities of large language models. CoRR, abs/2406.13542, 2024. [11] Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, and Jingren Zhou. How abilities in large language models are affected by supervised fine-tuning data composition. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 177198. Association for Computational Linguistics, 2024. [12] Guanting Dong, Chenghao Zhang, Mengjie Deng, Yutao Zhu, Zhicheng Dou, and Ji-Rong Wen. Progressive multimodal reasoning via active retrieval. CoRR, abs/2412.14835, 2024. 12 [13] Guanting Dong, Yutao Zhu, Chenghao Zhang, Zechen Wang, Zhicheng Dou, and Ji-Rong Wen. Understand what LLM needs: Dual preference alignment for retrieval-augmented generation. CoRR, abs/2406.18676, 2024. [14] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [15] Mehdi Fatemi, Banafsheh Rafiee, Mingjie Tang, and Kartik Talamadupula. Concise reasoning via reinforcement learning, 2025. [16] Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong. Retool: Reinforcement learning for strategic tool use in llms, 2025. [17] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. Tora: tool-integrated reasoning agent for mathematical problem solving. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. [18] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Joaquin Vanschoren and Sai-Kit Yeung, editors, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021. [19] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing multihop QA dataset for comprehensive evaluation of reasoning steps. In Donia Scott, Núria Bel, and Chengqing Zong, editors, Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020, pages 66096625. International Committee on Computational Linguistics, 2020. [20] Jian Hu. Reinforce++: simple and efficient approach for aligning large language models. arXiv preprint arXiv:2501.03262, 2025. [21] Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: survey. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, pages 10491065. Association for Computational Linguistics, 2023. [22] Jinhao Jiang, Jiayi Chen, Junyi Li, Ruiyang Ren, Shijie Wang, Wayne Xin Zhao, Yang Song, and Tao Zhang. Rag-star: Enhancing deliberative reasoning with retrieval augmented verification and refinement. CoRR, abs/2412.12881, 2024. [23] Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. Searchr1: Training llms to reason and leverage search engines with reinforcement learning. CoRR, abs/2503.09516, 2025. [24] Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, and Zhicheng Dou. Flashrag: modular toolkit for efficient retrieval-augmented generation research. CoRR, abs/2405.13576, 2024. [25] Shanglin Lei, Guanting Dong, Xiaoping Wang, Keheng Wang, and Sirui Wang. Instructerc: Reforming emotion recognition in conversation with retrieval multi-task llms framework. CoRR, abs/2309.11911, 2023. [26] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Hugo Larochelle, MarcAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. 13 [27] Chengpeng Li, Guanting Dong, Mingfeng Xue, Ru Peng, Xiang Wang, and Dayiheng Liu. Dotamath: Decomposition of thought with code assistance and self-correction for mathematical reasoning. CoRR, abs/2407.04078, 2024. [28] Chengpeng Li, Mingfeng Xue, Zhenru Zhang, Jiaxi Yang, Beichen Zhang, Xiang Wang, Bowen Yu, Binyuan Hui, Junyang Lin, and Dayiheng Liu. START: self-taught reasoner with tools. CoRR, abs/2503.04625, 2025. [29] Chengpeng Li, Mingfeng Xue, Zhenru Zhang, Jiaxi Yang, Beichen Zhang, Xiang Wang, Bowen Yu, Binyuan Hui, Junyang Lin, and Dayiheng Liu. START: self-taught reasoner with tools. CoRR, abs/2503.04625, 2025. [30] Chengshu Li, Jacky Liang, Andy Zeng, Xinyun Chen, Karol Hausman, Dorsa Sadigh, Sergey Levine, Li Fei-Fei, Fei Xia, and Brian Ichter. Chain of code: Reasoning with language model-augmented code emulator. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. [31] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models. CoRR, abs/2501.05366, 2025. [32] Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng Dou. Webthinker: Empowering large reasoning models with deep research capability. arXiv preprint arXiv:2504.21776, 2025. [33] Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yongkang Wu, Zhonghua Li, Qi Ye, and Zhicheng Dou. Retrollm: Empowering large language models to retrieve fine-grained evidence within generation. CoRR, abs/2412.11919, 2024. [34] Xuefeng Li, Haoyang Zou, and Pengfei Liu. LIMR: less is more for RL scaling. CoRR, abs/2502.11886, 2025. [35] Xuefeng Li, Haoyang Zou, and Pengfei Liu. Torl: Scaling tool-integrated RL. CoRR, abs/2503.23383, 2025. [36] Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, Yingying Zhang, Fei Yin, Jiahua Dong, Zhijiang Guo, Le Song, and Cheng-Lin Liu. From system 1 to system 2: survey of reasoning large language models. CoRR, abs/2502.17419, 2025. [37] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. [38] Pan Lu, Bowen Chen, Sheng Liu, Rahul Thapa, Joseph Boen, and James Zou. Octotools: An agentic framework with extensible tools for complex reasoning. CoRR, abs/2502.11271, 2025. [39] Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. GAIA: benchmark for general AI assistants. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. [40] Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, et al. Imitate, explore, and self-improve: reproduction report on slow-thinking reasoning systems. arXiv preprint arXiv:2412.09413, 2024. [41] OpenAI. Learning to reason with llms, September 2024. [42] Bhargavi Paranjape, Scott M. Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and Marco Túlio Ribeiro. ART: automatic multi-step reasoning and tool-use for large language models. CoRR, abs/2303.09014, 2023. 14 [43] Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, Adam Khoja, Ryan Kim, Jason Hausenloy, Oliver Zhang, Mantas Mazeika, Daron Anderson, Tung Nguyen, Mobeen Mahmood, Fiona Feng, Steven Y. Feng, Haoran Zhao, Michael Yu, Varun Gangal, Chelsea Zou, Zihan Wang, Jessica P. Wang, Pawan Kumar, Oleksandr Pokutnyi, Robert Gerbicz, Serguei Popov, John-Clark Levin, Mstyslav Kazakov, Johannes Schmitt, Geoff Galgon, Alvaro Sanchez, Yongki Lee, Will Yeadon, Scott Sauers, Marc Roth, Chidozie Agu, Søren Riis, Fabian Giska, Saiteja Utpala, Zachary Giboney, Gashaw M. Goshu, Joan of Arc Xavier, Sarah-Jane Crowson, Mohinder Maheshbhai Naiya, Noah Burns, Lennart Finke, Zerui Cheng, Hyunwoo Park, Francesco Fournier-Facio, John Wydallis, Mark Nandor, Ankit Singh, Tim Gehrunger, Jiaqi Cai, Ben McCarty, Darling Duclosel, Jungbae Nam, Jennifer Zampese, Ryan G. Hoerr, Aras Bacho, Gautier Abou Loume, Abdallah Galal, Hangrui Cao, Alexis C. Garretson, Damien Sileo, Qiuyu Ren, Doru Cojoc, Pavel Arkhipov, Usman Qazi, Lianghui Li, Sumeet Motwani, Christian Schröder de Witt, Edwin Taylor, Johannes Veith, Eric Singer, Taylor D. Hartman, Paolo Rissone, Jaehyeok Jin, Jack Wei Lun Shi, Chris G. Willcocks, Joshua Robinson, Aleksandar Mikov, Ameya Prabhu, Longke Tang, Xavier Alapont, Justine Leon Uro, Kevin Zhou, Emily de Oliveira Santos, Andrey Pupasov Maksimov, Edward Vendrow, Kengo Zenitani, Julien Guillod, Yuqi Li, Joshua Vendrow, Vladyslav Kuchkin, and Ng Ze-An. Humanitys last exam. CoRR, abs/2501.14249, 2025. [44] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 56875711. Association for Computational Linguistics, 2023. [45] Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tür, Gokhan Tur, and Heng Ji. Toolrl: Reward is all tool learning needs. arXiv preprint arXiv:2504.13958, 2025. [46] Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tür, Gokhan Tur, and Heng Ji. Toolrl: Reward is all tool learning needs, 2025. [47] Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma Gongque, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, Runfeng Qiao, Yifan Zhang, Xiao Zong, Yida Xu, Muxi Diao, Zhimin Bao, Chen Li, and Honggang Zhang. We-math: Does your large multimodal model achieve human-like mathematical reasoning? CoRR, abs/2407.01284, 2024. [48] Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, et al. O1 replication journey: strategic progress reportpart 1. arXiv preprint arXiv:2410.18982, 2024. [49] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2024. [50] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. KDD 20, 2020. [51] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. [52] Joykirat Singh, Raghav Magazine, Yash Pandya, and Akshay Nambi. Agentic reasoning and tool integration for llms via reinforcement learning. arXiv preprint arXiv:2505.01441, 2025. [53] Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. CoRR, abs/2503.05592, 2025. [54] Petru Soviany, Radu Tudor Ionescu, Paolo Rota, and Nicu Sebe. Curriculum learning: survey. Int. J. Comput. Vis., 130(6):15261565, 2022. [55] Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, and Xia Ben Hu. Stop overthinking: survey on efficient reasoning for large language models. CoRR, abs/2503.16419, 2025. [56] Hao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan Hou, Yong Jiang, Pengjun Xie, Yan Zhang, Fei Huang, and Jingren Zhou. Zerosearch: Incentivize the search capability of llms without searching, 2025. [57] Jiankai Sun, Chuanyang Zheng, Enze Xie, Zhengying Liu, Ruihang Chu, Jianing Qiu, Jiaqi Xu, Mingyu Ding, Hongyang Li, Mengzhe Geng, Yue Wu, Wenhai Wang, Junsong Chen, Zhangyue Yin, Xiaozhe Ren, Jie Fu, Junxian He, Wu Yuan, Qi Liu, Xihui Liu, Yu Li, Hao Dong, Yu Cheng, Ming Zhang, Pheng-Ann Heng, Jifeng Dai, Ping Luo, Jingdong Wang, Ji-Rong Wen, Xipeng Qiu, Yike Guo, Hui Xiong, Qun Liu, and Zhenguo Li. survey of reasoning with foundation models. CoRR, abs/2312.11562, 2023. [58] Shuang Sun, Huatong Song, Yuhao Wang, Ruiyang Ren, Jinhao Jiang, Junjie Zhang, Fei Bai, Jia Deng, Wayne Xin Zhao, Zheng Liu, Lei Fang, Zhongyuan Wang, and Ji-Rong Wen. Simpledeepsearcher: Deep information seeking via web-powered reasoning trajectory synthesis. 2025. [59] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [60] Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown. Hugging Face, 2024. [61] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023. [62] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. arXiv preprint arXiv:2212.10509, 2022. [63] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. musique: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539554, 2022. [64] Hongru Wang, Cheng Qian, Wanjun Zhong, Xiusi Chen, Jiahao Qiu, Shijue Huang, Bowen Jin, Mengdi Wang, Kam-Fai Wong, and Heng Ji. Otc: Optimal tool calls via reinforcement learning. arXiv preprint arXiv:2504.14870, 2025. [65] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training, 2024. [66] Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Lucas Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, Weizhu Chen, Shuohang Wang, Simon Shaolei Du, and Yelong Shen. Reinforcement learning for reasoning in large language models with one training example, 2025. [67] Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, Eli Gottlieb, Monica Lam, Yiping Lu, Kyunghyun Cho, Jiajun Wu, Li Fei-Fei, Lijuan Wang, Yejin Choi, and Manling Li. Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning, 2025. 16 [68] Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu Zhou, Pengjun Xie, and Fei Huang. Webwalker: Benchmarking llms in web traversal. CoRR, abs/2501.07572, 2025. [69] Junde Wu, Jiayuan Zhu, and Yuyuan Liu. Agentic reasoning: Reasoning llms with tools for the deep research. CoRR, abs/2502.04644, 2025. [70] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report. CoRR, abs/2407.10671, 2024. [71] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. CoRR, abs/2409.12122, 2024. [72] Dayu Yang, Tianyang Liu, Daoan Zhang, Antoine Simoulin, Xiaoyi Liu, Yuwei Cao, Zhaopu Teng, Xin Qian, Grey Yang, Jiebo Luo, and Julian J. McAuley. Code to think, think to code: survey on code-enhanced reasoning and reasoning-driven code intelligence in llms. CoRR, abs/2502.19411, 2025. [73] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: dataset for diverse, explainable multi-hop question answering. In EMNLP, pages 23692380, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. [74] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. LIMO: less is more for reasoning. CoRR, abs/2502.03387, 2025. [75] Chao Yu, Xuejing Zheng, Hankz Hankui Zhuo, Hai Wan, and Weilin Luo. Reinforcement learning with knowledge representation and reasoning: brief survey. CoRR, abs/2304.12090, 2023. [76] Dian Yu, Baolin Peng, Ye Tian, Linfeng Song, Haitao Mi, and Dong Yu. Siam: Self-improving code-assisted mathematical reasoning of large language models. CoRR, abs/2408.15565, 2024. [77] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Weinan Dai, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. DAPO: an open-source LLM reinforcement learning system at scale. CoRR, abs/2503.14476, 2025. [78] Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. Scaling relationship on learning mathematical reasoning with large language models. CoRR, abs/2308.01825, 2023. [79] Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild. CoRR, abs/2503.18892, 2025. [80] Yuxiang Zhang, Yuqi Yang, Jiangming Shu, Xinyan Wen, and Jitao Sang. Agent models: Internalizing chain-of-action generation into reasoning models. arXiv preprint arXiv:2503.06580, 2025. [81] Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. Deepresearcher: Scaling deep research via reinforcement learning in real-world environments. arXiv preprint arXiv:2504.03160, 2025."
        },
        {
            "title": "Appendix",
            "content": "A Supplement Experimental Results A.1 Deep Web Exploration Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Different RL Algorithm: REINFORCE++ VS. GRPO . . . . . . . . . . . . . . . . A.3 Analysis of Tool-Call Masking . . . . . . . . . . . . . . . . . . . . . . . . . . . . Details of Tool-Star Framework B.1 Training Algorithm of Tool-Star . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Details of Tool Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Implementation Details C.1 The Statistics of Data Sources . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Training Details of Tool-Star . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 Details of Retrieval C.4 Baseline Details . C.5 Datasets Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Instruction Templates D.1 Instructions for Tool-Star . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Instructions for Code Debugger . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 Instructions for Reasoning Chain Refiner . . . . . . . . . . . . . . . . . . . . . . . D.4 Instructions for Standard RAG . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.5 Instructions for Knowledge-Intensive Reasoning Model . . . . . . . . . . . . . . . D.6 Instructions for Computational Reasoning Model . . . . . . . . . . . . . . . . . . D.6.1 Instructions for Direct Sampling and Reasoning Computational Datasets . . 19 19 19 20 20 20 22 22 23 23 24 25 25 26 26 26 26 D.6.2 Instructions for Direct Sampling and Reasoning Knowledge-Intensive Datasets 26 Case Study E.1 TIR Cases of Tool-Star . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Cases of Self-Critic DPO Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . Limitations and Future Directions Broader Impacts 27 27 27 28 18 Table 3: The performance of Tool-Star and other baseline methods on the GAIA and HLE datasets. The experimental settings are consistent with those in Table 1. Here, Qwen refers to the use of the Qwen2.5-3B-Instruct model, and Llama refers to the use of the Llama3.2-3B-Instruct model. Dataset Qwen Llama ToRL DotaMath RAG Search-o1 Search-R1 Multi-Tool Prompting Tool-Star GAIA HLE 3.9 7.4 5.8 5.0 8.7 7.2 4.9 4.8 8.7 7.2 7.8 6. 9.7 7.8 5.8 5.6 15.5 8."
        },
        {
            "title": "A Supplement Experimental Results",
            "content": "A.1 Deep Web Exploration Result In this section, we further evaluate the performance of the Tool-Star framework on web exploraton tasks. Specifically, we assess it on the General AI Assistants (GAIA) and Humanitys Last Exam (HLE) datasets with the same experimental settings as in main result. As shown in Table 3, In detail, we observe that the Tool-Star framework achieves strong performance even on web search tasks. Specifically, on the GAIA dataset, our method achieves the highest score, outperforming the second-best by over 60%. On the HLE dataset, our method also demonstrates strong competitiveness. This fully validates the effectiveness of our approach in the deep web exploration task. In terms of task difficulty, GAIA consists of complex deep web exploration tasks that typically require both information retrieval and reasoning. To address such challenges, Tool-Star often employs multi-tool collaboration strategy to accomplish the subtasks. Examples of multi-tool collaborative reasoning are shown in Table 6. A.2 Different RL Algorithm: REINFORCE++ VS. GRPO As two widely-used rule-based reinforcement learning algorithms, Tool-Star with GRPO [51] and Reinforce++ [20] are compared in Table 4 in terms of training curves and experimental performance. It can be seen that the overall differences between GRPO and Reinforce++ are relatively minor. Table 4: Performance comparison of RL Algorithm on ToolStar (Qwen2.5-3B-Instruct). Models GSM8K MATH HQA. Bamb. Tool-Star (SFT) 80 79. 47.5 48.3 + GRPO + REINFORCE++ 85.0 84.4 81 80.5 51.9 48. 52.5 53.1 Experimental results show that Tool-Star with GRPO slightly outperforms REINFORCE++ on computational reasoning tasks such as GSM8K and MATH. However, when it comes to knowledgeintensive reasoning tasks, REINFORCE++ slightly outperforms GRPO on Bamboogle. These results suggest that while the performance of the two methods fluctuates slightly across different domains, the overall difference remains within 3%. Figure 8: The result of tool-call masking A.3 Analysis of Tool-Call Masking In our work, we apply masking to the retrieved and python execution results when computing the loss to prevent the training policy from being biased by tool outputs. Therefore, all tool call results in Tool-Star are masked. Based on this design, we conduct ablation studies on Tool-Star (Qwen2.5-3B-Instruct) with or without tool-call masking in RL training. As shown in Figure 8, removing the masking leads to rapid collapse of the models reward score to -1, with persistent strategy hacking. This stands in contrast to prior findings in single-tool reinforcement learning [80], where, 19 Algorithm 1 Multi-Tool Self-Critic Reinforcement Learning Require: Reasoning Model πθ, External Tools , Reward Function Rϕ, Cycle Times C, GRPO"
        },
        {
            "title": "Steps S",
            "content": "Start Vanilla RL phrase (GRPO) for step {1, ..., S} do Sample batch Db from for each query Db do 1: Input: Datasets D, Task instruction 2: Initialize πθ old πθ 3: for {1, ..., C} do 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: Randomly sample Dsample from for each query Dsample do Sample sequences for with : {oj}G Calculate the optimization objective function Jθ of GRPO according to Eq.(4) Update the weight parameters of πθ: θ θ + η θJθ j=1 πθ( q, ) Start Self-Critic DPO Sample the query for times with to generate different results: {oj}G j=1 πθ( 15: q, ) oreject Filter one correct result and one incorrect result from results to obtain ochosen and 16: 17: 18: Output: Trained Model πθ new Calculate the optimization objective function Jθ of DPO according to Eq.(5) Update the weight parameters of πθ: θ θ + η θJθ according to their experiments, masking in single-tool RL did not lead to result hacking. We consider that maybe tool feedback in multi-tool reasoning can introduce significantly greater instability during RL training. Details of Tool-Star Framework B.1 Training Algorithm of Tool-Star In this section, we delve into the overall workflow of the our Multi-Tool Self-Critic Reinforcement Learning algorithm. The training process consists of two alternating phases over multiple cycles, which are illustrated in algorithm 1: (1) Vanilla RL stage: In each cycle, the model πθ is first optimized via GRPO for fixed number of steps. For each query sampled from the dataset, the task instruction is prepended, and the model generates multiple output sequences using external tools. These outputs are used to compute the GRPO objective (Eq. 4), and the model parameters are updated accordingly. (2) Self-Critic DPO stage: After GRPO, self-critic phase is performed to refine the models decision-making. For each sampled query, the model generates multiple outputs, from which one preferred (correct) and one rejected (incorrect) response are selected. These are used to compute the DPO objective (Eq. 5), further updating the model via gradient ascent. This alternating training scheme enables the model to not only improve through reinforcement learning with tool-augmented outputs but also self-correct by distinguishing between good and bad generations. The final output is tool-aware reasoning model πnew optimized for both performance and efficiency. θ B.2 Details of Tool Design To provide more detailed illustration of the design and usage guideline of our tools, we present the specifics of both the training tools and inference-time tools below. Training Tools: 20 Table 5: The statistics of data sources in our main result. Dataset NuminaMath HotpotQA [53] 2WikiMultiHopQA [19] SimpleDeepSearch [58] WebWalker [68] Type Language-only Language-only Language-only TIR Language-only Task # Counts Computational Reasoning Knowledge-Intensive Reasoning Knowledge-Intensive Reasoning Knowledge-Intensive Reasoning Knowledge-Intensive Reasoning 72.4K 8.1k 10k 0.9k 3k Search Engine: During inference, the search engine executes queries to retrieve relevant information, supporting both local and web-based retrieval modes. Inspired by DeepResearch [81], we adopt web search during training. Empirically, we find that both using webpage snippets and incorporating browser agent consistently improve RL performance. However, the former significantly reduces efficiency. During evaluation, we use Wikipedia5 as the retrieval corpus and E5 [65] as the retriever for four open-domain QA tasks. For all other tasks, we combine web search with the browser agent to obtain higher-quality retrieval results. Web Browser Agent: This component accesses URLs from search results, parses webpage content, extracts information relevant to the query, and summarizes it. The process includes webpage parsing, regex-based cleaning, and an auxiliary model that refines the cleaned content based on the search query. The summarized output is then fed back into the reasoning chain. Notably, the browser agent is optionalwhen it is not used, the search engine directly returns webpage snippets or raw text. Code Interpreter: Code snippets generated by the language model are executed in sandboxed environment, returning either the execution result or error messages based on correctness. Our Python interpreter implementation follows the design of ToRA [17], ensuring secure and accurate execution. Inference-time tool: Code Debugger: This module automatically corrects erroneous code generated by large language models and guides the code revision process. Specifically, we design dedicated debug prompt ( D.2) that incorporates the model-generated code and compiler error messages, enabling the model to understand and fix the errors for automated code correction. Tool-Use Backtracer: This module identifies and reverts to the reasoning step prior to failed tool call, allowing the model to recover and revise its reasoning path. In real experiments, we compared model-based and rule-based rewinding strategies. As the model often struggles to accurately locate the faulty tool call, we adopt the rule-based approach. Concretely, we set the rewind position at the newline token preceding the special token of the erroneous tool call, prompting the model to regenerate the reasoning chain from that point. Reasoning Chain Refiner: When the model output exceeds the maximum length limit, this module prunes and refines redundant steps in the reasoning process, replacing the original chain with more concise and coherent version to complete the task. We design refinement prompt ( D.3) that enables the model to identify and remove errors or repetitive thoughts in the original reasoning chain and generate streamlined version. Template examples can be found in Table D.3. To ensure fairness, all deployed models of tools in the experiments (e.g., the browser agents information refinement process, code debugger, and Reasoning Chain Refiner) use instruct-version models with the same parameter size as the test model. Therefore, our inference-time tool can also be regarded as self-play inference-time scaling process. 5https://archive.org/details/enwiki-"
        },
        {
            "title": "C Implementation Details",
            "content": "C.1 The Statistics of Data Sources In this section, we sample data from sources including Numina-MATH, 2Wiki, HotpotQA, with the detailed breakdown and corresponding data sizes presented in Table 5. We strictly adhere to all copyright and licensing terms associated with the datasets and open-source code. For the computational reasoning dataset, we use 72.4K samples aligned with Numina-MATH TIR. For knowledge-based reasoning, we include 10.6K samples from HotpotQA and 2Wiki, aligned with R1 Searcher and Research respectively, along with 0.9K samples from the SimpleDeepSearch dataset and 3K samples randomly selected from the Webwalker training set. Finally, we obtained approximately 100K samples containing 1% TIR data. It is important to note that we only use the queries from these language-only datasets for reasoning chain sampling, without relying on the reasoning chains generated by stronger models of original datasets. Our cold-start data is sampled using Qwen2.5-3B-Instruct, which serves as the backbone model to match the parameter scale of the main table. Following the START settings [28], we set the temperature to 0.7 and the top-p value to 0.95 during sampling. Due to resource constraints, each question is sampled three times only. In the tool-use quality normalization stage, our tool-call frequency threshold β is set to 5. Ultimately, following multiple stages of quality filtering in Section 3.1, we curated approximately 54K samples from categories 1, 2, and 3 for cold-start training. Furthermore, 10K challenging samples were randomly selected from categories-4 samples for the RL phase to enable progressive alignment from simpler to more difficult tasks. C.2 Training Details of Tool-Star Cold-Start Supervised Finetuning. After applying our sampling strategies to the original datasets, we use approximately 54k samples for cold-start supervised fine-tuning. During the supervised fine-tuning stage, we fine-tune Qwen2.5-3B-Instruct model on the training data with learning rate of 7e-6. We utilize DeepSpeed ZeRO-3 [50] and FlashAttention2 [8] for training. The batch size is set to 128, weight decay to 0.1, and we train for 3 epoch. Mixed precision training with BF16 is adopted, and the maximum input length is set to 4096. Multi-Tool Self-Critic Reinforcement Learning. After SFT, we further train the model using the GRPO algorithm under the VERL framework 6. Similarly, we randomly select around 10k samples from DRL tool for reinforcement learning. The learning rate is set to 8, with 8 rollouts per sample. The total training batch size is 128, and the mini-batch size is 16. The maximum output length is set to 4096 to ensure completeness of exploration. To accelerate the training speed, following DAPO [77], we set the maximum number of tool invocations during training to 3. To stabilize training, we set the KL divergence coefficient in GRPO to 0. The reinforcement learning phase lasts for 2 epochs. All experiments are conducted on 8 NVIDIA A800 GPUs. To ensure reproducibility, all experimental results are averages of three independent trials. We have approximately 300 samples in the validation set during the RL stage, which includes AIME25, 25, MATH500, HotpotQA, 2wikiMultiHopQA, MuSiQue and Bamboogle. It is worth noting that all tool invocation results in Tool-Star are masked. As illustrated in the Figure 3, these masked segments are excluded from the loss computation to prevent the training policy from being biased toward the tool outputs. Only the tokens from the text-based reasoning process and tool requests are considered during loss calculation. Self-Critic DPO Training. In this phase, the learning rate is set to 5e-7 with cosine scheduler and 0.1 warm-up ratio. We use DeepSpeed ZeRO Stage 3 and Flash-Attention 2 for efficiency, with global batch size of 64. Training utilizes sigmoid loss function with beta value of 0.3 and spans 2 epochs. Mixed precision training with bf16 is employed, and the maximum context length is 4096 tokens. We perform self-critic training every 0.5 epoch during the standard reinforcement learning 6https://github.com/volcengine/verl process (approximately every 50 steps), and use the resulting model to resume the subsequent vanilla reinforcement learning training. The detailed data format are listed in Appendix E.2 C.3 Details of Retrieval During training, we use the Bing Web Search API as the retriever, configured with the US-EN region setting. For each query, we retrieve 10 web pages as supporting documents. During inference, we use Bing Web Search for the Math Reasoning and Complex Reasoning datasets. For open-domain QA, we employ Wikipedia as the retriever. The Wikipedia retriever is based on the Wikipedia 2018 Corpus provided by FlashRAG [24] and uses E5-base-v2 as the retriever. C.4 Baseline Details Proprietary Models Qwen2.5 Series [70, 49], developed by Alibaba, is available in several sizes: Qwen2.5-0.5B /1.5B /7B and 72B. This model is trained on data sources spanning 29 kinds of languages, enabling it to perform exceptionally well in multilingual tasks. Additionally, Qwen2.5 exhibits strong capabilities in coding and mathematics. Qwen2.5-72B-Instruct is notable for its ability to handle input windows of up to 128K tokens in length, making it exceptionally well-suited for processing long texts and tackling complex tasks. LLaMA 3 Series [14], developed by Meta AI, is released in multiple sub-series, including LLaMA 3, LLaMA 3.1, LLaMA 3.2 and LLaMA 3.3. LLaMA series models offer two primary variants for each model size: the base model and the instruct model. These models are trained on mixture of datasets, covering wide range of domains and languages. The LLaMA series models achieve strong performance across reasoning, coding, and multilingual understanding benchmarks. With continued improvements in alignment and context length, they are increasingly suitable for real-world tasks. Search-Enhanced Baselines RAG [26] is basic baseline that demonstrates the models ability to directly utilize retrieved information. In our implementation, we directly used the retriever to obtain the top-5 results and inserted them into carefully designed prompt to let the model generate the answer. Search-o1 [31] is reasoning-model-driven RAG method. Its core idea is to allow the reasoning model to autonomously decide whether to perform search during its reasoning process. It generates queries, retrieves relevant information, organizes the retrieved results, and incorporates them into the models reasoning chain. The model then continues generation based on this chain until final answer is reached. In our experiments, we modified the official code of Search-o1 by replacing its base model with one that has the same number of parameters as ours, and we also used the same retriever to ensure consistency. Search-R1 [23] is reinforcement learning method that trains model from scratch to call the search engine during reasoning. Its overall process is similar to Search-o1 [31]: the model generates necessary queries during reasoning, retrieves relevant information, and inserts it into the reasoning chain. We follow the original prompt used in the method to ensure its effectiveness. Code-Enhanced Baselines ToRL [35] is tool-integrated framework trained with reinforcement learning methods. This framework enables the model to autonomously invoke code interpreter to solve mathematical reasoning problems. Compared to prompt-based methods and supervised fine-tuning training approaches, ToRL encourages the model to explore extensively to find optimal tool usage strategies. This approach shows significant improvements over traditional methods. At the same time, the framework demonstrates the broad potential applications of tool integration in mathematical reasoning. DotaMath [27] models are fine-tuned on mathematical instruction datasets. After training, the DotaMath model demonstrates strong capabilities in solving complex mathematical reasoning problems. By training on the DotaMathQA dataset built for mathematical problems, the model 23 has acquired the abilities of thought decomposition and self-correction during reasoning. This paradigm opens up new direction for solving complex math problems. Multi-tool Enhanced Baselines Multi-Tool Prompting method refers to the approach where the model learns to use external tools to solve problems solely through prompts. In this setup, we primarily aim to explore whether the model has the capability to autonomously call relevant tools to complete tasks. For the prompt format, we use the same structure as the one employed by Tool-Star. The specific prompt can be found in Appendix D.1. ReCall [1] method makes significant improvements over the ReSearch framework. Specifically, compared to the purely search-based framework, it further allows the model to call code interpreter to solve complex reasoning problems. This setup expands the tasks that ReSearch could handle to mathematical reasoning, while still maintaining the original capability of ReSearch to call search engines. Additionally, ReCall introduces new method for generating data towards complex multi-step tasks, enabling the LLM to develop sophisticated reasoning abilities based on multiple tools. C.5 Datasets Details We provide detailed introduction to the computational reasoning benchmarks evaluated by Tool-Star: AIME24 7 dataset of 30 challenging math problems from the 2024 American Invitational Mathematics Examination (AIME & II), covering algebra, geometry, number theory, and combinatorics. Each JSONL-formatted entry includes the problem ID, statement, detailed solution steps, and the final integer answer. Designed to evaluate large language models on complex mathematical reasoning, the dataset features high difficulty, rigorous multi-step solutions, and unique answersmaking it strong benchmark for assessing advanced problem-solving capabilities. AIME25 8 is dataset comprising 30 challenging mathematical problems sourced from the 2025 American Invitational Mathematics Examination (AIME & II). These problems encompass diverse range of topics, including algebra, combinatorics, geometry, number theory, and probability. AIME25 is designed to evaluate the mathematical reasoning capabilities of LLMs. It serves as benchmark for assessing how well these models can tackle complex mathematical problems requiring multi-step reasoning and understanding of advanced mathematical concepts. GSM8K [5] is widely adopted benchmark for assessing the mathematical reasoning abilities of large language models through grade-school level word problems. In our experiments, we utilize its standard test set and report performance based on greedy decoding accuracy (maj@1). MATH [18] is dataset consisting of challenging high school-level mathematics problems, spanning topics such as Prealgebra, Algebra, Number Theory, Counting & Probability, Geometry, Intermediate Algebra, and Precalculus. Compared to GSM8K, MATH offers greater difficulty and topic diversity. MATH500 [37] is subset of 500 problems drawn from the MATH benchmark developed by OpenAI. This dataset is designed to assess the mathematical reasoning capabilities of large language models. The problems in MATH500 cover range of mathematical topics including algebra, geometry, trigonometry, and probability. Each entry in the dataset includes the problem statement, detailed step-by-step solution, and the final answer, allowing for thorough model training and evaluation. Furthermore, we also conduct series experiments on follow knowledge-intensive reasoning benchmarks: WebWalker [68] dataset consisting of queries derived from various web pages, where answering single query may require gathering information from one or multiple pages. In our experiments, we adopt an open-domain setting by using search engine for retrieval rather than the root URLs provided in the dataset. 7https://huggingface.co/datasets/HuggingFaceH4/aime_2024 8https://huggingface.co/datasets/math-ai/aime25 HotPotQA [73] is widely-used question answering dataset that features multi-hop questions built from Wikipedia articles. In our experiments, we use its validation set for evaluation. 2WikiMultihopQA [19] is large-scale multi-hop QA dataset that combines structured knowledge from Wikidata and unstructured text from Wikipedia. It includes diverse question types and annotated reasoning paths to support explainable multi-step inference. Bamboogle [44] is challenging multi-hop QA dataset that contains hand-crafted questions requiring up to four reasoning hops. It tests models ability to perform complex and compositional reasoning across multiple facts. Musique [63] is multi hop dataset that requires inference. The problem inside is dependency based multi hop problem, where one inference step depends on the information obtained from the previous inference step, making the dataset very difficult. We conducted experiments using Musiques dev set."
        },
        {
            "title": "D Instruction Templates",
            "content": "D.1 Instructions for Tool-Star Instruction for Tool-Star You are helpful assistant that can solve the given question step by step with the help of the wikipedia search tool and python interpreter tool. Given question, you need to first think about the reasoning process in the mind and then provide the answer. During thinking, you can invoke the wikipedia search tool to search and python interpreter tool to calculate the math problem for fact information about specific topics if needed. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags respectively, and the search query and result are enclosed within <search> </search> and <result> </result> tags respectively. After receiving the search or python result, you should continue your reasoning process begin with <think>. For example, <think> This is the reasoning process. </think> <search> search query here </search> <result> search result here </result> <think> This is the reasoning process. </think> <python> python code here </python> <result> python interpreter result here </result> <think> This is the reasoning process. </think> <answer> The final answer is answer here </answer>. In the last part of the answer, the final exact answer is enclosed within boxed{} with latex format. D.2 Instructions for Code Debugger Instruction for Code Debugger You are code expert. need you to debug the following code. Below are the code originally generated by the model and the error information that occurred during code execution. Please output ONLY the corrected Python code, without any explanation or markdown formatting: **Inputs:** **Original Code:** {code} **Execution Error:** {error} Output the corrected Python code only, without any explanation or markdown formatting: 25 D. Instructions for Reasoning Chain Refiner Instruction for Reasoning Chain Refiner You are an expert in response refinement. Given prompt and its corresponding response, your task is to compress and restructure the response by removing redundant, repetitive, or irrelevant content. Preserve all key information needed to directly and accurately address the original prompt. Only output your revised response and do not output anything else. **Original Prompt:** {prompt} **Original Response:** {response} **Revised Response:** D.4 Instructions for Standard RAG Instruction for Standard RAG Find the useful content from the provided documents, then answer the question. Answer the question directly. Your response should be very concise. Please provide use So the final answer is: as prefix for the final answer. The following are given documents. Documents: {reference} Question: {question} D.5 Instructions for Knowledge-Intensive Reasoning Model Instruction for Sampling and Inferencing Knowledge-Intensive Datasets You are helpful assistant that can solve the given question step by step with the help of the python interpreter tool. Given question, you need to first think about the reasoning process in the mind and then provide the answer. During thinking, you can invoke the python interpreter tool to calculate the math problem for fact information about specific topics if needed. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags respectively. For example, <think> This is the reasoning process. </think> <python> python code here </python> <result> python interpreter result here </result> <think> This is the reasoning process. </think> <answer> The final answer is </answer>. In the last part of the answer, the final exact answer is enclosed within boxed{} with latex format. answer here D.6 Instructions for Computational Reasoning Model D.6.1 Instructions for Direct Sampling and Reasoning Computational Datasets Instruction for Sampling and Reasoning Computational Datasets conversation between User and Assistant. The user asks question, and the Assistant solves it. Please integrate natural language reasoning with programs to solve the problem above, and put your final answer within boxed{}. {question} D.6.2 Instructions for Direct Sampling and Reasoning Knowledge-Intensive Datasets Instructions for Sampling and Reasoning Knowledge-Intensive Datasets conversation between User and Assistant. The user asks question, and the Assistant solves it. Please refer to the given document to solve the problem above, and put your final answer within boxed{}. Question: {question} Reference documents: {reference}"
        },
        {
            "title": "E Case Study",
            "content": "E.1 TIR Cases of Tool-Star To better understand how Tool-Star performs tool-based collaborative reasoning, we conducted case study to clearly illustrate its reasoning process. The model-generated think process are enclosed within <think> and </think> . The model-generated search queries are enclosed within <search> and </search> , and the model-generated Python codes are enclosed within <python> and </python> . Search results, Python execution results and final answers are enclosed within <result> , </result> , <answer> and </answer> . Our analysis reveals the following: In terms of task difficulty, GAIA is complex deep web exploration tasks that typically involve both information retrieval and reasoning. When handling such tasks, Tool-Star often adopts multitool collaboration strategy to complete the subtasks (as shown in the Table 6). In contrast, for domain-specific datasets like HotpotQA and AIME, which are more focused on knowledge retrieval or mathematical computation, Tool-Star can accurately identify the task type and select the most appropriate single tool for problem-solving (as shown in the Table 7 & 8). These observations demonstrate that Tool-Star can flexibly choose between tool combinations or single tools based on task requirements, showing strong task understanding and tool selection strategies. This highlights the effectiveness and autonomy of our approach in multi-tool collaborative reasoning. E.2 Cases of Self-Critic DPO Data To more comprehensively illustrate the data format used in our self-critic reinforcement learning framework, we provide several reward data example (positive & negative pairs). As shown in the Table 10 to 16, we can observe that the question and reasoning chain in the example are consistent with those in vanilla reinforcement learning data, ensuring no impact on model performance during subsequent training. The only difference in our approach lies in the addition of two key fields after the original reasoning chain: Reward principle and Reward score. The Reward principle is automatically generated by rule-based reward function, which identifies specific mismatches leading to the score, aiming to provide interpretability for the reward. For instance, in one case, reward score of -1 is assigned because the number of special tokens in the model output does not align with the reference, resulting in formatting inconsistency."
        },
        {
            "title": "F Limitations and Future Directions",
            "content": "Despite our best efforts to optimize the overall Tool-Star pipeline, there remain certain limitations and areas for improvement that warrant further exploration. Expanding Tool Diversity: Currently, the Tool-Star framework integrates three tools during training and three tools during inference. Compared to prior work that relies on single tool, our approach represents pioneering and timely attempt. However, given the growing diversity of available tools, we believe many more remain to be explored for enhancing reasoning capabilities. For instance, incorporating vision-language models (VLMs) as external tools could further unlock visual understanding, and adopting the Model Context Protocol may enable more flexible tool invocation. We plan to continue expanding the types and integration of tools in future work, advancing multi-tool collaborative reasoning. Scaling Backbone Model Parameters: Due to limited computational resources and the timeconsuming rollout process in multi-tool reasoning, this work focuses on models with 0.5B, 1.5B, and 3B parameters for RL training and evaluation. Nonetheless, our Scaling Analysis on Parameter Scales (see Table 4) provides preliminary evidence of the Tool-Star frameworks scalability. In future work, we aim to extend our experiments to larger models, such as 7B and 32B, to assess generalization across more complex tasks and model capacities. Outlook: Achieving reliable and robust step-by-step reasoning remains challenging task that demands continued research and innovation. We sincerely hope to collaborate with researchers in 27 the fields of alignment and reasoning to jointly address these shared challenges and further advance tool-integrated reasoning systems."
        },
        {
            "title": "G Broader Impacts",
            "content": "This study presents the Tool-Star framework, which empowers large language models with multi-tool collaborative reasoning, enhancing their adaptability and effectiveness in complex tasks. Through reinforcement learning and scalable data synthesis, Tool-Star offers generalizable and flexible reasoning paradigm, demonstrating strong performance across benchmarks and broad potential in domains such as education, science, and engineering. However, as models gain greater autonomy in tool use, concerns arise regarding inappropriate tool selection, opaque reasoning, and bias propagationespecially in high-stakes scenarios. To ensure responsible deployment, future development should prioritize controllability, interpretability, and robust oversight, aligning technological progress with ethical standards and societal values. 28 Table 6: An example from Tool-Star on GAIA dataset, with special symbols used in think content, search queries, Python codes, returned results and final answer highlighted with purple box , green box , blue box and red box , respectively. Example #1 from GAIA Dataset Question: The longest-lived vertebrate is named after an island. According to Wikipedia, what is the 2023 population of that island, to the nearest thousand? Labeled Answer: Model Output by Tool Star <think> To find out which island this refers to and its corresponding population from 2020, will start off by identifying the longest-lived vertebrate mentioned in Wikipedia. </think> <search> search query: What is the longest-lived vertebrate according to Wikipedia? </search> <result> An international team of scientists led by the University of Copenhagen and including the University of Oxford has found that the Greenland shark has life expectancy of at least 272 years. This discovery shows it is the longest living vertebrate known to science, exceeding even bowhead whales, turtles and tortoises.Our results demonstrate that the Greenland shark is among the longest-lived vertebrate species surpassing even the bowhead whale (Balaena mysticetus, estimated longevity of 211 years), Dr. Nielsen and co-authors said. The life expectancy of the Greenland shark is exceeded only by that of the ocean quahog (Arctica islandica, 507 years). </result> <think> Now having identified the longest-lived vertebrate - the Greenland Shark, my next task would be finding out more details regarding the associated island specifically referred to in the context. Since the name Greenlanddirectly connects to the island, let me proceed further with determining the population data related to this location. </think> <search> search query: Population of Greenland as per the latest available statistics </search> <result> The current population of Greenland is 55,764 as of April 26, 2025, based on interpolation of the latest United Nations data. The population of Greenland is projected at 55,745, or 56k, as of July 1, 2025. The total population in Greenland is projected at 55,840, or 56k people, for the year 2024. ***More reasoning contentare omitted here***) After the 1960s, the population in the towns grew rapidly. As of January 1, 2023, the population of Greenland amounted to 56,609. The majority of the population held the Danish citizenship, as the country is an autonomous region of the Kingdom of...As of July 1, 2024 the population of Greenland reached 55,840, of which 29,360 people are male and 26,480 female. </result> <think> Having gathered sufficient evidence through searching, it appears clear that the relevant piece of information comes from the statement mentioning the population of Greenland as of January 1, 2023 being approximately 56,609. Considering the requirement asks for rounding off to the nearest thousand, we should round this figure accordingly. Lets finalize the calculation and presentation of the result using Python code for accuracy. </think> <python> # Given data population = 55840 # Rounding to the nearest thousand rounded_population = round(population, -3) print(rounded_population) </python> <result> 56000 </result> <think> After performing the necessary calculations and ensuring alignment with the required precision level, the estimated population of Greenland to the nearest thousand is determined. </think> <answer> 56000 </answer> Table 7: An example from Tool-Star on HotpotQA dataset, with special symbols used in think content, search queries, returned results and final answer highlighted with purple box , green box and red box , respectively. Example #2 from HotpotQA Dataset Question: D1NZ is series based on what oversteering technique? Labeled Answer: Drifting Model Output by Tool Star <think> Okay, so need to figure out what D1NZ is series based on an oversteering technique. Hmm, first off, Im not too familiar with D1NZ. Let me start by breaking down the term. The name might be related to some racing or driving terminology. The acronym could stand for something like Driving 1 NZ or maybe its play on words. Oversteer is when the rear wheels lose traction, causing the car to slide outward. Common techniques include understeer (front losing grip) and oversteer. But how does this relate to series? should probably look up what D1NZ refers to. Maybe its motorsport series? Like rallycross or drag racing? Alternatively, perhaps its video game series involving cars where oversteer is key mechanic. Wait, there was game called Need for Speed: Most Wanted, but thats more about drifting. Or maybe Forza Horizon? Not sure. Alternatively, D1NZ might be an abbreviation used in real-world racing categories. Let me try searching for D1NZ oversteering technique. That seems direct. Wait, another thought: sometimes series have names combining letters with numbers or other codes. Maybe D1 stands for something else. For example, in some contexts, D1 could refer to division number. If its racing series, maybe D1 is category, and NZ indicates New Zealand. So perhaps D1NZ is New Zealand-based racing series focused on oversteer techniques. Let me structure my searches properly. First, check what D1NZ is. Search terms could be What is D1NZ racing series? or D1NZ oversteering technique. So Ill start with the first search query: </think> <search> D1NZ racing series oversteering technique </search> <result> Drifting (motorsport) Drifting (motorsport) Drifting is driving technique where the driver intentionally oversteers, with loss of traction in the rear wheels or all tires, while maintaining control and driving the car through the entirety of corner. Car drifting is caused when the rear slip angle is greater than the front slip angle, to such an extent that often the front wheels are pointing in the opposite direction to the turn (e.g. car is turning left, wheels are pointed right or vice versa, also known as opposite lock or counter-steering). As motoring discipline, drifting competitions were first popularized in 1970s ***More reasoning contentare omitted here***) of Driftworks embarked on journey to Japan where they would shoot documentary about the local drifting scene, which would become known to the public as Outsiders. Drifting (motorsport) Drifting is driving technique where the driver intentionally oversteers, with loss of traction in the rear wheels or all tires, while maintaining control and driving the car through the entirety of corner. Car drifting is caused when the rear slip angle is greater than the front slip angle, to such an extent that often the front wheels are pointing in the opposite direction to the turn (e.g. car </result> <think> From the search results, it looks like D1NZ is New Zealand drift racing series. The description mentions that its production car drifting series, focusing on oversteer techniques. Specifically, the explanation talks about intentional oversteer maneuvers where the driver loses traction in the rear wheels, allowing the car to spin or slide around corners. This aligns with the concept of drift racing, which relies heavily on oversteer to execute controlled slides and spins. Therefore, the oversteering technique associated with D1NZ must involve intentionally inducing rear-wheel loss of traction to achieve controlled drifts during races or competitions. </think> <answer> The final answer is drif ting </answer> 30 Table 8: An example from Tool-Star on AIME24 dataset. Example #3 from AIME24 Dataset Question: Every morning Aya goes for 9-kilometer-long walk and stops at coffee shop afterwards. When she walks at constant speed of kilometers per hour, the walk takes her 4 hours, including minutes spent in the coffee shop. When she walks + 2 kilometers per hour, the walk takes her 2 hours and 24 minutes, including minutes spent in the coffee shop. Suppose Aya walks at + 1 2 kilometers per hour. Find the number of minutes the walk takes her, including the minutes spent in the coffee shop. Labeled Answer: 204 Model Output by Tool Star <think> To solve this problem, we need to set up system of equations based on the information provided. Lets break down the problem step-by-step. 1. **Define Variables:** - s: Ayas walking speed in kilometers per hour. - t: Time spent in the coffee shop in minutes (which needs to be converted to hours since the other times are in hours). - Distance walked each time: 9 kilometers. 2. **Formulate Equations Based on Given Information:** - When Aya walks at km/h, the total time taken is 4 hours, which includes minutes spent in the coffee shop. Converting minutes to hours, we get 60 . So the equation becomes: = 4 9 + - When Aya walks at + 2 km/h, the total time taken is 2 hours and 24 minutes, which is 2 + 24 Converting minutes to hours again, we get 60 . So the equation becomes: 60 = 2.4 hours. 9 + 2 + 60 = 2.4 3. **Solve the System of Equations:** We now have two equations: (cid:40) 60 = 4 + s+2 + 9 60 = 2.4 Lets use Python to solve these equations. </think> <python> from sympy import symbols, Eq, solve # Define the variables s, = symbols(s t) # Define the equations based on the given information eq1 = Eq(9/s + t/60, 4) eq2 = Eq(9/(s + 2) + t/60, 2.4) # Solve the system of equations solution = solve((eq1, eq2), (s, t)) print(solution) </python> <result> </result> (4.50000000000000, 360.000000000000), (2.50000000000000, 24.0000000000000) <think> ***More reasoning contentare omitted here***) </think> <answer> The final answer is 204 </answer> 31 Table 9: Example 1 with positive output of our self-critic reward finetuning dataset, including the question, model output, corresponding reward principle and reward score. The special symbols used in think content, Python codes, returned results and final answer highlighted with purple box , blue box and red box , respectively. Example #1 from Training Dataset 13 + 1 Question: Find the sum 1 the fraction is in its simplest form. Please give the value of + m. Labeled Answer: 385 (2n1)(2n+1) + + 35 + + 1 255257 . The original answer is in format, where Positive Output of Tool-Star <think> Okay, so need to find the sum of this series: 1/(1 3) + 1/(3 5) + + 1/(255 257). Hmm, the general term seems to be 1/[(2n 1)(2n + 1)] and we are summing up to some where 2n 1 = 255. Let me figure out how many terms there are first. If the last term is 1/(255 257), then 2n 1 = 255, so solving for n: </think> <python> = (255 + 1) // 2 print(n) </python> <result> 128 </result> <think> So there are 128 terms in total. Now, remember that for fractions like 1/[(2n 1)(2n + 1)], partial fractions might be helpful here. Let me try to decompose the general term. Suppose that: (2n1)(2n+1) = Multiplying both sides by (2n 1)(2n + 1) gives: 1 = A(2n + 1) + B(2n 1) ***More reasoning contents are omitted here*** <python> 2n1 + 2n+1 1 total = (1 - 1/257) / 2 numerator = total.numerator denominator = total.denominator print(f\"{numerator}/{denominator}\") </python> <result> 128/257 </result> <think> So the sum is 128/257. Let me check if this can be simplified. </think> <python> from fractions import Fraction frac = Fraction(128, 257) print(f\"Simplified: {frac.numerator}/{frac.denominator}\") </python> <result> Simplified: 128/257 </result> <think> Since 128 and 257 share no common divisors other than 1, the fraction is already in simplest form. Therefore, the answer is 128/257, and + = 128 + 257 = 385. </think> <answer> 385 </answer> Reward Principle of Output The response format is correct. The final answer is correct. The reasoning chain contains single tool usage. Reward Score of Output So the reward score is: 1 32 Table 10: Example 1 with negative output of our self-critic reward finetuning dataset, including the question, model output, corresponding reward principle and reward score. The special symbols used in think content, Python codes, returned results and final answer highlighted with purple box , blue box and red box , respectively. Example #1 from Training Dataset 13 + 1 Question: Find the sum 1 the fraction is in its simplest form. Please give the value of + m. Labeled Answer: (2n1)(2n+1) + + 1 35 + + 1 255257 . The original answer is in format, where Positive Output of Tool-Star 1 1 (2n1)(2n+1) + + 1 13 + 1 35 + + 255257 . And the <think> Okay, so have this series to compute: problem says that the answer should be in the form , which is already simplified, and we need to find + m. Alright, lets start breaking this down. First, notice that each term in the series has the denominator as product of two consecutive odd numbers. Specifically, the general term seems like (2n1)(2n+1) . Maybe theres way to simplify this term using partial fractions? Let me recall: when dealing with denominators like (a b)(a + b), we can use partial fractions to split 2n+1 . To find and B, Ill combine the right-hand it into simpler terms. So, side over common denominator: A(2n+1)+B(2n1) (2n1)(2n+1) . So the numerator must equal 1: A(2n + 1) + B(2n 1) = 1. Expanding this gives: 2An + + 2Bn = 1, which simplifies to (2A + 2B)n + (A B) = 1. For this equation to hold true for all n, the coefficients of and the constant term must both match on both sides. Therefore, we get the system of equations: (1) 2A + 2B = 0, and (2) = 1. From equation (1), we get = B. Substituting into equation (2): = 1 implies 2B = 1, so = 1 2 and = 1 2 . . This is much cleaner! 2n1 + (2n1)(2n+1) = (2n1)(2n+1) = (cid:16) (cid:17) 1 1 1 1 (2n1)(2n+1) = Therefore, the partial fraction decomposition is: Each term can be rewritten as half the difference between reciprocals of adjacent odd numbers. Now, the entire series becomes: (cid:80)128 (2n1)(2n+1) = 1 1 scoping series! When expanded, many terms will cancel out. Lets verify: when written out, the series is 1 becomes: </think> <python> 35 + + 1 2n1 1 13 + 1 (cid:80)128 n=1 2n1 (cid:16) 1 2n+1 2n+1 n=1 (cid:17) 2 255257 , and using the telescoping identity, this . This looks like tele- # Calculate the sum using telescoping series formula total_terms = 128 term_start = 1 term_end = 257 # Compute the telescoping sum telescoping_sum = (1 / term_start) - (1 / term_end) print(f\"Telescoping sum: {telescoping_sum}\") </python> <result> Telescoping sum: 0.9961089494163424 </result> <think> The result from the code execution shows the telescoping sum converges to approximately 0.9961089494163424, which is very close to 256 257 . Checking the code confirms this approximation is accurate enough, considering floating-point precision limitations. Finally, since 256 and 257 are coprime (their greatest common divisor is 1), the fraction 256 257 is already in its simplest form. Hence, = 256 and = 257, leading to + = 513. </think> <answer> 513 </answer> 257 . Thus, the sum simplifies to 256 Reward Principle of Output The response format is correct. The answer is incorrect. Reward Score of Output So the reward score is: 0 Table 11: Example 2 with positive output of our self-critic reward finetuning dataset, including the question, model output, corresponding reward principle and reward score. The special symbols used in think content, Python codes, returned results and final answer highlighted with purple box , blue box and red box , respectively. Example #1 from Training Dataset Question: How many four-digit positive integers are multiples of 7? Labeled Answer: 1286 Positive Output of Tool-Star <think> To determine how many four-digit positive integers are multiples of 7, we consider all integers from 1000 to 9999 and find those divisible by 7. The smallest four-digit number is 1000. Since 1000 7 142.857, the smallest multiple of 7 greater than or equal to 1000 is 143 7 = 1001. Similarly, the largest four-digit number is 9999. Since 9999 7 1428.42857, the largest multiple of 7 less than or equal to 9999 is 1428 7 = 9996. These numbers form an arithmetic sequence starting at 1001 and ending at 9996, with common difference of 7. The total number of terms is given by the formula = ana1 +1, where a1 = 1001, an = 9996, and = 7. Substituting, we get = 99961001 7 + 1 = 1285 + 1 = 1286. </think> <python> + 1 = 7 # Calculate the smallest four-digit multiple of 7 smallest_multiple = (1000 + 6) // 7 * 7 # Calculate the largest four-digit multiple of 7 largest_multiple = (9999 // 7) * 7 # Calculate the number of four-digit multiples of 7 number_of_multiples = (largest_multiple - smallest_multiple) // 7 + 1 print(number_of_multiples) </python> <result> 1286 </result> <answer> The number of four-digit positive integers that are multiples of 7 is 1286 . </answer> Reward Principle of Output The response format is correct. The final answer is correct. The reasoning chain contains single tool usage. Reward Score of Output So the reward score is: 1 34 Table 12: Example 2 with negative output of our self-critic reward finetuning dataset, including the question, model output, corresponding reward principle and reward score. The special symbols used in think content, Python codes, returned results and final answer highlighted with purple box , blue box and red box , respectively. Example #1 from Training Dataset Question: How many four-digit positive integers are multiples of 7? Labeled Answer: 1286 Positive Output of Tool-Star <think> Okay, so need to figure out how many four-digit positive integers are multiples of 7. four-digit number ranges from 1000 to 9999. The smallest four-digit multiple of 7 should be 1001, since 1000 7 142.857, and rounding up gives 143 7 = 1001. The largest one would be 9996, since 9999 7 1428.428, and rounding down gives 1428 7 = 9996. So, we have the range starting at 1001 and ending at 9996 inclusive. Now, the problem reduces to finding the number of terms in an arithmetic sequence where the first term is 1001, the common difference is 7, and the last term is 9996. The formula for the nth term of an arithmetic sequence is an = a1 + (n 1)d, where an is the last term, a1 is the first term, is the common difference, and is the number of terms. Solving for n, we get = ana1 + 1. Substituting the values an = 9996, a1 = 1001, and = 7, we get = 99961001 7 + 1 = 1285 + 1 = 1286. </think> <python> <python> + 1 = 8995 7 # Calculate the number of terms in the arithmetic sequence first_term = 1001 last_term = 9998 common_difference = = ((last_term - first_term) // common_difference) + 1 print(n) </python> <result> 1286 </result> <answer> The calculation shows there are 1286 four-digit multiples of 7 between 1000 and 9999. </answer> Reward Principle of Output The response format is incorrect, <python> and </python> are not matched. Reward Score of Output So the reward score is: - 35 Table 13: Example 3 with positive output of our self-critic reward finetuning dataset, including the question, model output, corresponding reward principle and reward score. The special symbols used in think content, search queries, returned results and final answer highlighted with purple box , blue box and red box , respectively. Example #1 from Training Dataset Question: The main symptoms of overtraining syndrome in athletes include which of the following? A. Improved Fitness B. Mental Clarity C. Muscle Strength D. Weight Gain Labeled Answer: Positive Output of Tool-Star <think> Okay, so the question is about the main symptoms of overtraining syndrome in athletes, and the options are through D. Overtraining syndrome occurs when an athlete trains excessively without adequate recovery, leading to negative effects on performance and health. The key is to identify which of the given options are consistent with known symptoms. Option is Improved Fitness, which contradicts the nature of overtraining, since performance typically declines. Option is Mental Clarity, but overtraining is associated with mental fatigue, irritability, and poor concentration, so mental clarity is unlikely. Option is Muscle Strength, but chronic overtraining impairs recovery and can lead to decreased strength rather than improvement. Option is Weight Gain, which at first seems counterintuitive, as overtraining might lead to weight loss due to increased energy expenditure. However, hormonal imbalances and stress responses could result in fluid retention or metabolic changes that cause weight gain. To confirm, should search for the main symptoms of overtraining syndrome. </think> <search> overtraining syndrome main symptoms </search> <result> Overtraining syndrome main symptoms include: fatigue, persistent muscle soreness, unexplained weight loss or gain, increased susceptibility to illness and injury, gastrointestinal issues, sleep disturbances, elevated resting heart rate, decreased maximal heart rate during exercise, psychological symptoms like anxiety, irritability, depression, burnout, mood swings, emotional exhaustion, and performance decline. These symptoms worsen with inadequate recovery and may require medical evaluation. </result> <think> Based on the search result, overtraining syndrome includes both physical and psychological symptoms. Among the physical symptoms, unexplained weight gain is explicitly mentioned, making Option valid. Option (Improved Fitness) contradicts the performance decline seen in overtraining. Option (Mental Clarity) is inconsistent with the psychological symptoms like irritability and burnout. Option (Muscle Strength) is unlikely, as overtraining typically leads to decreased strength due to insufficient recovery. Since weight gain is listed symptom, and the others are not, the correct answer is D. </think> <answer> </answer> Reward Principle of Output The response format is correct. The final answer is correct. The reasoning chain contains single tool usage. Reward Score of Output So the reward score is: 1 36 Table 14: Example 3 with negative output of our self-critic reward finetuning dataset, including the question, model output, corresponding reward principle and reward score. The special symbols used in think content, search queries, returned results and final answer highlighted with purple box , blue box and red box , respectively. Example #1 from Training Dataset Question: The main symptoms of overtraining syndrome in athletes include which of the following? A. Improved Fitness B. Mental Clarity C. Muscle Strength D. Weight Gain Labeled Answer: Positive Output of Tool-Star <think> In this case study, we are tasked with identifying the main symptom of overtraining syndrome among four options: through D. Overtraining syndrome occurs when an athlete engages in excessive training without adequate recovery, leading to decline in performance and various physical and psychological issues. Lets analyze each option: Option A: \"Improved Fitness\" This is generally result of balanced training and recovery. However, overtraining leads to plateau or decline in fitness, not improvement. Therefore, improved fitness is not symptom of overtraining. Option B: \"Mental Clarity\" Overtraining is associated with mood disturbances, irritability, anxiety, and difficulty concentrating. These symptoms reflect decline in mental clarity, making \"mental clarity\" an unlikely symptom. Option C: \"Muscle Strength\" While proper training enhances muscle strength, overtraining without sufficient recovery can lead to muscle breakdown and decreased strength. Thus, increased muscle strength is not symptom of overtraining. Option D: \"Weight Gain\" This is more nuanced. Overtraining can sometimes lead to hormonal imbalances, particularly elevated cortisol, which may result in fluid retention or increased fat storage. Additionally, some athletes may overeat in response to chronic fatigue or stress, potentially leading to weight gain. While not the most common symptom, weight gain can occur in certain cases. ***More reasoning contents are omitted here, over 15K tokens*** To verify, key symptoms of overtraining syndrome include: persistent fatigue, decreased performance, mood changes (e.g., depression, irritability), sleep disturbances, elevated resting heart rate, frequent illnesses, and hormonal imbalances. None of the options perfectly match these core symptoms, but among the given choices, weight gain (D) is the most plausible indirect symptom, while the others describe positive outcomes that are contrary to overtraining effects. Therefore, based on the evidence and reasoning, the best answer is: </think> <answer> </answer> Reward Principle of Output The response format is incorrect, the response over max length. Reward Score of Output So the reward score is: -1 37 Table 15: Example 4 with positive output of our self-critic reward finetuning dataset, including the question, model output, corresponding reward principle and reward score. The special symbols used in think content, search queries, Python codes, returned results and final answer highlighted with purple box , green box , blue box and red box , respectively. Example #1 from Training Dataset Question: We now define an algorithm: The definition of a(n) is: The nearest integer to the imaginary part of the n-th nontrivial zero of the Riemann zeta function, where the zeros are listed in order of increasing positive imaginary parts. Given the input list: xlist = [50, 51, 52, 53, 54, 55, 56, 57, 58, 59] Determine the corresponding output sequence ylist. Which of the following options is correct? Option A:[145, 148, 149, 152, 153, 155, 158, 160, 161, 163] Option B:[143, 146, 147, 150, 151, 153, 156, 158, 159, 161] Option C:[147, 150, 151, 154, 155, 157, 160, 162, 163, 165] Option D:[139, 142, 143, 146, 147, 149, 152, 154, 155, 157] Labeled Answer: Positive Output of Tool-Star <think> To find the correct sequence, we need to know the nearest integers to the imaginary parts of the 50th through 59th nontrivial zeros of the Riemann zeta function. will search for this information or use reliable source to find these values. </think> <search> nearest integer to the imaginary part of the 50th through 59th nontrivial zeros of the Riemann zeta function </search> <result> </result> <think> It seems like the direct search did not yield straightforward list of the required values. To get the exact values, will use Python library that can compute the zeros of the Riemann zeta function, such as mpmath. will use the mpmath library to compute the 50th through 59th nontrivial zeros of the Riemann zeta function and extract the nearest integers to their imaginary parts. </think> <python> import mpmath # Set the precision mpmath.mp.dps = 50 # Function to get the nearest integer to the imaginary part of the n-th zero def nearest_integer_imaginary_part(n): zero = mpmath.zetazero(n) return round(zero.imag) # Input list x_list = [50, 51, 52, 53, 54, 55, 56, 57, 58, 59] # Compute the output sequence y_list = [nearest_integer_imaginary_part(n) for in x_list] # Display the result y_list </python> <result> [143, 146, 147, 150, 151, 153, 156, 158, 159, 161] </result> <think> The computed sequence of the nearest integers to the imaginary parts of the 50th through 59th nontrivial zeros of the Riemann zeta function is [143, 146, 147, 150, 151, 153, 156, 158, 159, 161]. This matches option B. </think> <answer> The correct answer is . </answer> Reward Principle of Output The response format is correct. The final answer is correct. The reasoning chain contains multiple tool usage. Reward Score of Output So the reward score is: 1.1 Table 16: Example 4 with negative output of our self-critic reward finetuning dataset, including the question, model output, corresponding reward principle and reward score. The special symbols used in think content, Python codes, returned results and final answer highlighted with purple box , blue box and red box , respectively. Example #1 from Training Dataset Question: We now define an algorithm: The definition of a(n) is: The nearest integer to the imaginary part of the n-th nontrivial zero of the Riemann zeta function, where the zeros are listed in order of increasing positive imaginary parts. Given the input list: xlist = [50, 51, 52, 53, 54, 55, 56, 57, 58, 59] Determine the corresponding output sequence ylist. Which of the following options is correct? Option A:[145, 148, 149, 152, 153, 155, 158, 160, 161, 163] Option B:[143, 146, 147, 150, 151, 153, 156, 158, 159, 161] Option C:[147, 150, 151, 154, 155, 157, 160, 162, 163, 165] Option D:[139, 142, 143, 146, 147, 149, 152, 154, 155, 157] Labeled Answer: Positive Output of Tool-Star <think> To solve this, need to compute the imaginary parts of the 50th through 59th nontrivial zeros of the Riemann zeta function and round them to the nearest integers. will use the mpmath library to perform these computations. </think> <python> import mpmath # Set high precision for accurate computation mpmath.mp.dps = 50 # Decimal places of precision # Function to get the nearest integer to the imaginary part of the n-th nontrivial zero of the Riemann zeta function def nearest_integer_imag_part(n): zero = mpmath.zetazero(n) return round(zero.imag) # List of indices for which to compute the zeros x_list = list(range(50, 60)) # Compute the nearest integer imaginary parts for the given indices y_list = [nearest_integer_imag_part(n) for in x_list] # Output the result print(y_list) </python> <result> [143, 146, 147, 150, 151, 153, 156, 158, 159, 161] </result> <think> The computed values are [143, 146, 147, 150, 151, 153, 156, 158, 159, 161]. However, due to misreading of the result, will now incorrectly match this to option instead of the correct option B. </think> The correct answer is . Reward Principle of Output The response format is incorrect. <answer> and </answer> are not matched. Reward Score of Output So the reward score is: -"
        }
    ],
    "affiliations": [
        "BAAI",
        "Kuaishou Technology",
        "Renmin University of China"
    ]
}