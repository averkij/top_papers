{
    "paper_title": "FilmAgent: A Multi-Agent Framework for End-to-End Film Automation in Virtual 3D Spaces",
    "authors": [
        "Zhenran Xu",
        "Longyue Wang",
        "Jifang Wang",
        "Zhouyi Li",
        "Senbao Shi",
        "Xue Yang",
        "Yiyu Wang",
        "Baotian Hu",
        "Jun Yu",
        "Min Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Virtual film production requires intricate decision-making processes, including scriptwriting, virtual cinematography, and precise actor positioning and actions. Motivated by recent advances in automated decision-making with language agent-based societies, this paper introduces FilmAgent, a novel LLM-based multi-agent collaborative framework for end-to-end film automation in our constructed 3D virtual spaces. FilmAgent simulates various crew roles, including directors, screenwriters, actors, and cinematographers, and covers key stages of a film production workflow: (1) idea development transforms brainstormed ideas into structured story outlines; (2) scriptwriting elaborates on dialogue and character actions for each scene; (3) cinematography determines the camera setups for each shot. A team of agents collaborates through iterative feedback and revisions, thereby verifying intermediate scripts and reducing hallucinations. We evaluate the generated videos on 15 ideas and 4 key aspects. Human evaluation shows that FilmAgent outperforms all baselines across all aspects and scores 3.98 out of 5 on average, showing the feasibility of multi-agent collaboration in filmmaking. Further analysis reveals that FilmAgent, despite using the less advanced GPT-4o model, surpasses the single-agent o1, showing the advantage of a well-coordinated multi-agent system. Lastly, we discuss the complementary strengths and weaknesses of OpenAI's text-to-video model Sora and our FilmAgent in filmmaking."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 9 0 9 2 1 . 1 0 5 2 : r a"
        },
        {
            "title": "Work in Progress",
            "content": "FILMAGENT: MULTI-AGENT FRAMEWORK FOR END-TO-END FILM AUTOMATION IN VIRTUAL 3D SPACES Zhenran Xu1 Longyue Wang Jifang Wang1 Zhouyi Li2 Senbao Shi1 Xue Yang Yiyu Wang Baotian Hu1 Jun Yu1 Min Zhang1 1Harbin Institute of Technology (Shenzhen) 2Tsinghua University Figure 1: We introduce FILMAGENT, multi-agent collaborative framework for end-to-end film automation powered by large language models (LLMs). team of LLM-based agents takes on film crew roles, and simulates the human workflow in 3D virtual spaces by sequentially engaging in idea development, scriptwriting, and cinematography, finally completing the filmmaking process."
        },
        {
            "title": "ABSTRACT",
            "content": "Virtual film production requires intricate decision-making processes, including scriptwriting, virtual cinematography, and precise actor positioning and actions. Motivated by recent advances in automated decision-making with language agentbased societies, this paper introduces FILMAGENT, novel LLM-based multiagent collaborative framework for end-to-end film automation in our constructed 3D virtual spaces. FILMAGENT simulates various crew rolesdirectors, screenwriters, actors, and cinematographers, and covers key stages of film production workflow: (1) idea development transforms brainstormed ideas into structured story outlines; (2) scriptwriting elaborates on dialogue and character actions for each scene; (3) cinematography determines the camera setups for each shot. Baotian Hu is the corresponding author: hubaotian@hit.edu.cn."
        },
        {
            "title": "Work in Progress",
            "content": "team of agents collaborates through iterative feedback and revisions, thereby verifying intermediate scripts and reducing hallucinations. We evaluate the generated videos on 15 ideas and 4 key aspects. Human evaluation shows that FILMAGENT outperforms all baselines across all aspects and scores 3.98 out of 5 on average, showing the feasibility of multi-agent collaboration in filmmaking. Further analysis reveals that FILMAGENT, despite using the less advanced GPT-4o model, surpasses the single-agent o1, showing the advantage of well-coordinated multiagent system. Lastly, we discuss the complementary strengths and weaknesses of OpenAIs text-to-video model Sora and our FILMAGENT in filmmaking 1."
        },
        {
            "title": "INTRODUCTION",
            "content": "Virtual film production entails methodical and disciplined approach to the directing, camera placement and actor positioning (He et al., 1996). Recent advancements in deep learning have started to automate film production practices, where sophisticated neural networks enable the movement of virtual cameras through 3D environments (Jiang et al., 2020). However, films are not only about moving pictures; they are crafted through language. They are produced through the dialogues spoken by the characters, the screenplays that outline the story, the shooting scripts that instruct the cinematographers, and, undeniably, the guidance given by directors (Jiang et al., 2024). Therefore, filmmaking is fundamentally communication-driven collaborative task, motivating our design of multi-agent system based on large language models (LLMs). In this paper, we propose FILMAGENT, the first LLM-based multi-agent collaborative framework designed to automate end-to-end virtual film production. In this framework, LLM-based agents fulfill various film crew roles, including director, screenwriter, actor, and cinematographer, to collectively create film. As shown in Figure 1, the collaborative method emulates the human workflow and divides the process into three sequential stages: idea development, scriptwriting, and cinematography. In the first stage, given brief story idea, the director develops character profiles and expands the idea into detailed scene outline, specifying the where, what, and who of each segment. During scriptwriting, the director, screenwriter, and actors collaborate on dialogue development and choreograph movements. In the cinematography stage, the cinematographers and director work together to design camera setups for each line, selecting between static and dynamic shots to effectively convey the narrative visually. In addition, we propose two multi-agent collaboration algorithms, CritiqueCorrect-Verify and Debate-Judge, applied in scriptwriting and cinematography stages respectively, to refine the script and camera settings. Finally, once the script is fully annotated, the film is shot within our meticulously constructed 3D spaces. The virtual 3D spaces include 15 locations, 65 designated actor positions, 272 shots covering 9 types of static and dynamic shots, 21 actor actions depicting expressive gestures and emotions, and speech audio generation. Human evaluations of the generated videos across 15 ideas validate the effectiveness of our framework. The results show that the collaborative FILMAGENT achieves an average score of 3.98 out of 5, significantly outperforming single-agent efforts across four aspects: plot coherence, alignment between dialogue and actor profiles, appropriateness of camera setting, and accuracy of actor actions. Further preference analysis underscores the importance of multi-agent collaboration in addressing hallucinations, enhancing plot coherence and improving camera choices. We also experiment with OpenAIs large reasoning model o1 and find that FILMAGENT, despite using less advanced GPT4o as foundational model, outperforms the single-agent o1. This highlights that well-coordinated multi-agent system can exceed the performance of more advanced foundational model. Case study with the OpenAIs text-to-video model Sora reveals the complementary strengths and weaknesses of Sora and FILMAGENT. While Sora shows great adaptability, it struggles with consistency and narrative delivery. In contrast, FILMAGENT produces coherent, physics-compliant videos with strong storytelling capabilities, due to its foundation on pre-designed 3D spaces and characters within collaborative workflow. As an early exploration of LLM-based multi-agent systems in virtual film production, we hope that this project lays the groundwork for end-to-end film automation, showing the potential of collaborative AI agents in this creative domain. 1For more information, including open-source Unity 3D spaces, codes and videos, please visit our project page at https://filmagent.github.io/."
        },
        {
            "title": "Work in Progress",
            "content": "In summary, our main contributions are as follows: We present FILMAGENT, novel LLM-based multi-agent collaborative framework for endto-end film automation, which mirrors the traditional film set process within meticulously crafted 3D virtual spaces. We incorporate two multi-agent collaboration strategies within the workflow, which substantially reduces hallucinations and enhances the quality of scripts and camera settings. Extensive human evaluations validate the effectiveness of FILMAGENT, indicating LLMbased multi-agent system as promising avenue for automating film production."
        },
        {
            "title": "2.1 VIRTUAL FILM PRODUCTION",
            "content": "Virtual film production is defined as broad term referring to spectrum of computer-aided production and visualization filmmaking methods (Bodini et al., 2024). This method supports remote collaboration and enhances accessibility due to its virtual nature (Nebeling et al., 2021). It has gained substantial attention in the entertainment industry, following its prominent use in The Mandalorian television series (Kavakli & Cremona, 2022). Recently, game engines are revolutionizing filmmaking with Virtual Camera Plugin, which allows real-time rendering of simulated environments. This enables filmmakers to play around in virtual environment before shooting, potentially replacing traditional pre-visualization methods like storyboards (Legato & Deschanel, 2019). Deep learning-based virtual production. Virtual film production covers wide spectrum of problems, from narrative aspects (de Lima et al., 2009), camera control (Li & Cheng, 2008; Christie et al., 2008) and even cutting and editing problems (Leake et al., 2017). In recent years, the field has embraced deep neural networks due to their remarkable generalization ability. When applying cinematography in computer graphics environments, Jiang et al. (2020) combine the Toric coordinate system (Lino & Christie, 2015) with Mixture-of-Experts model to generate styled camera motions based on different video references. Jiang et al. (2021) further introduce keyframing for finer control of camera motions with an LSTM-based backbone. In this work, based on the understanding that filmmaking is communication-driven collaborative process (Jiang et al., 2024), we design multi-agent system that uses large language models (LLMs) to enhance this collaboration. Preliminary exploration with LLMs. Recent works in virtual production have begun to utilize the emergent reasoning and planning capabilities of LLMs (Wei et al., 2022). Qing et al. (2023) address the Story-to-Motion task, which requires characters to navigate to locations and perform specific actions based on textual descriptions. Here, LLMs are utilized as text-driven motion schedulers, extracting sequences of (text, position, duration) tuples from long text. VideoDirectorGPT (Lin et al., 2023) and Anim-Director (Li et al., 2024b) employ LLMs to plan videos, generating detailed scene descriptions, along with the positioning and layout of entities, for consistent multi-scene video production. In our work, we expand the use of LLMs to cover all aspects of film production, fully automating tasks from plot planning to cinematography within 3D virtual spaces. 2.2 MULTI-AGENT FRAMEWORK Recently, LLM-based autonomous agents have gained tremendous interest in both industry and academia (Wang et al., 2024). Voyager (Wang et al., 2023), AppAgent (Zhang et al., 2023) and Claude 3.5 Computer Use (Hu et al., 2024a) are typical task-oriented agents that can autonomously interact with the environment and solve simple tasks. However, single agents struggle to achieve effective, coherent, and accurate problem-solving processes, particularly when there is need for meaningful collaborative interaction (Zhuge et al., 2023; Qian et al., 2023). In the transition from single-agent frameworks into multi-agent frameworks, the pioneering research on Generative Agents (Park et al., 2023) has laid the groundwork for the development of Simulated Society. These societies are conceptualized as dynamic systems where multiple agents engage in intricate interactions within well-defined environment (Xi et al., 2023). This approach aligns with the Society of Mind (SoM) theory (Minsky, 1988), which suggests that intelligence arises from the interaction of computational modules, achieving collective goals beyond the capabilities"
        },
        {
            "title": "Work in Progress",
            "content": "Figure 2: vertical view of one of the 3D spaces (the living room) in FILMAGENT built with Unity. The environment is pre-configured with designated positions for actors and various camera setups for cinematography. These include static shots from multiple distances and dynamic shots that either follow or orbit around characters. Full camera setup of this space is provided in Figure 8. of individual modules. To this end, many works (Xu et al., 2023; Zhang et al., 2024; Cohen et al., 2023) have improved reasoning and factuality of LLMs by integrating discussions among multiple agents. Furthermore, ChatDev (Qian et al., 2023), MetaGPT (Hong et al., 2024) TransAgents (Wu et al., 2024a;b) and Agent Laboratory (Schmidgall et al., 2025) have successfully implemented multi-agent collaborative schemes throughout simulating standard human practices and workflows such as requirement design, coding and testing. Motivated by the promising outcomes of multiagent collaboration, we have developed multi-agent system called FILMAGENT to replicate human workflows and automate the end-to-end film production process."
        },
        {
            "title": "3 FILMAGENT",
            "content": "FILMAGENT is an LLM-based multi-agent framework for end-to-end film automation in 3D sandbox environment. The basic process is illustrated in Figure 1. An introduction of our constructed virtual 3D spaces for filmmaking is in Sec. 3.1. We describe the overview of FILMAGENT in Sec. 3.2, the core collaboration strategies in Sec. 3.3, and the production workflow in Sec. 3.4. 3.1 ENVIRONMENT SETUP We have meticulously built virtual 3D spaces ready for filmmaking. This Unity spaces include 15 locations that reflect everyday settings, such as living rooms, kitchens, offices and roadside, thus providing versatile backdrops for wide range of narratives. screenshot of the living room is presented in Figure 2. Each scene is pre-configured with actor positions and camera setups. All locations are listed in Figure 7 in Appendix A. Positions. The environment includes 32 standing points and 33 sitting points, each accompanied by human-written description indicating its position. For example, Position in Figure 2 is described as near the sofa, sittable, between Positions and C, allowing easy communication with characters at these positions. Actions. Each character can perform 21 different actions, selected from Mixamo2. These actions range from basic movements like sitting down and walking to more expressive gestures, such as joyful jumping and annoyed head-shaking. All actions are listed in Appendix A. Cameras. Following the principles of the language of film (Wohl, 2004), we define 9 types of shots, including 3 static shots from various distances (e.g., close-up, medium, and long shots as 2https://www.mixamo.com/"
        },
        {
            "title": "Work in Progress",
            "content": "No. Shot Type Description View ① Close-up Shot ② Medium Shot Close-up (CU) Shot should be close to the subject, typically including the collar, encapsulating the identity. Medium Shot (MS) should include the posture (such as body language) and physical movement (like walking). ③"
        },
        {
            "title": "Long Shot",
            "content": "Long shot (LS) contains the human body, showing where the subject is located. Table 1: Examples of 3 types of static shots in Figure 2, targeted at Position B. shown by Camera 1-3 in Figure 2) and 6 dynamic shots that track or orbit around character (e.g., pan shot represented by Camera 4 in Figure 2, zoom shot, arc shot, etc.). The descriptions and views of these static and dynamic shots in Figure 2 are shown in Table 1 and 4 in Appendix A. In total, all virtual 3D spaces contains 165 static shots and 107 dynamic shots. Audio. To create more natural and expressive audio, we utilize ChatTTS3 to generate the speech for each line in the script. The duration of each camera shot and action in the video is synchronized with the length of the corresponding audio segment. With these configurations in place, our virtual 3D spaces can support automatic film production. 3.2 OVERVIEW Clear role specialization allows for the breakdown of complex work into smaller and more specific tasks (Li et al., 2023; Hong et al., 2024). In FILMAGENT, we define four main characters: Director, Screenwriter, Actor and Cinematographer, as shown in Figure 3. Each of these roles carries its own set of responsibilities. The Director initiates and oversees the entire filmmaking project. This role includes setting character profiles, developing video outlines, providing feedback on the script, engaging in discussions with other crew members, and making final decisions when conflicts arise. The Screenwriter works closely under the Directors guidance. Its responsibilities go beyond writing dialogue; they also specify the positioning and actions for each line, and continuously update the script to ensure it is coherent, captivating, and well-structured, based on the Directors critiques. Actors are responsible for making minor adjustments to their lines based on their character profiles, ensuring the dialogue aligns with the characters, and communicating any necessary changes to the Director. Cinematographers select the camera settings for each line according to shot usage guidelines, collaborate with peer cinematographers to compare and discuss these choices, and ensure the appropriateness of camera settings. 3.3 AGENT COLLABORATION STRATEGIES In this section, we introduce two collaboration strategies used in this work, including CritiqueCorrect-Verify (Algorithm 1) and Debate-Judge (Algorithm 2). 3https://github.com/2noise/ChatTTS"
        },
        {
            "title": "Work in Progress",
            "content": "Figure 3: Workflow of FILMAGENT. Given story idea and 3D virtual spaces, the director creates character profiles and scene outline. Actors, the screenwriter, and the director then collaborate on dialogue and movements. Cinematographers annotate camera setups for each line. Finally, the film is shot within the 3D spaces. LLM-based agents take on various film crew roles, collaborating through Critique-Correct-Verify and Debate-Judge strategies. Critique-Correct-Verify Collaboration. As outlined in Algorithm 1, this strategy involves two agents working collaboratively. First, the Action agent generates response based on the given context and instruction I. Next, the Critique agent reviews the response and writes critiques highlighting potential areas for improvement. The Action agent then integrates the critiques and corrects the response. Finally, the Critique agent evaluates the updated response to determine whether the critiques have been adequately addressed or if further iterations are necessary. Algorithm 1: Critique-Correct-Verify Collaboration Input : Context C; Instruction I; Maximum number of iterations M; Action agent P; Critique agent Q; Current round; Initialize the conversation history; Output: The final response that is approved by the Critique agent Q; [C; I] 0 while do + 1; P(H) if > 1 then Generate new response; Q(C, I, R, F) addressed critiques; if = TRUE then The Critique agent verifies whether the response has Break Stop iterating if the Critique agent thinks the response is already of high quality; Q(H, R) + [R; F] Generate critiques; Append and to the conversation history H; Return the final response R; Debate-Judge Collaboration involves multiple agents who propose their responses and then engage in debate to persuade each other. third-party agent ultimately summarizes the discussion and delivers the final judgment. We present the details of our collaboration strategy in Algorithm 2. Two peer agents and independently generate their responses, and then provide feedback on each others work about the discrepancy during each iteration. After several rounds of debate, the Judgment agent concludes the discussion and makes the final decision R."
        },
        {
            "title": "Work in Progress",
            "content": "Algorithm 2: Debate-Judge Collaboration Input : Context C; Instruction I; Number of iterations M; Peer agents P, Q; Judgment agent J; Current round; Initialize the conversation history; Agents and generate responses; Output: The final judgment that finalizes the debate; [C; I] 0 RP P(H) RQ Q(H) FQ P(H, RP, RQ) FP Q(H, RP, RQ) while < do + 1; FQ P(H, RP, RQ, FP) FP Q(H, RP, RQ, FQ) J(H, RP, RQ, FP, FQ) Return the final judgment R; Agents and provide feedback on each others response; Agents and continue the debate; Judgment agent formulates the final judgment; Figure 4: The responsibilities of screenwriter extend beyond writing dialogues; they also involve annotating the corresponding action for each line. 3.4 WORKFLOW As shown in Figure 3, following the traditional film set workflow, we divide the whole film production process into three sequential stages: idea development, scriptwriting and cinematography, and apply the collaboration strategies discussed earlier in Sec. 3.3. The prompts for each stage are detailed in Appendix D. Idea development. From brief story idea, the director generates various character profiles that could be relevant to the story. The profiles include key attributes such as gender, occupation, and personality traits. Using these profiles and set of 15 predefined locations in our 3D virtual spaces, the director expands the initial story idea into detailed scene outline, specifying the where, what, and who of each segment (as illustrated in Figure 1). Scriptwriting is collaborative stage involving the screenwriter, director, and actors and is divided into three parts: 1. Initial Draft: The screenwriter drafts the initial script, including character dialogue, positioning and actions. Positioning refers to assigning each character to specific spots (e.g., Position A-D in Figure 2). Actions are annotated from the action space for each line, as shown in Figure 4. 2. Director-Screenwriter Discussion: The director and screenwriter then engage in CritiqueCorrect-Verify cycle. The director (the Critique agent Q) thoroughly reviews the script and provides suggestions on the plot coherence and character actions. Take Figure 3 as an example. the director identifies an inappropriate action and suggests better one to convey the characters surprise. The screenwriter (Action agent P) revises the script accordingly, and the director verifies the updated script to determine if further adjustments are needed. 3. Actor-Director-Screenwriter Discussion: Actors provide feedback based on their understanding of characters to ensure consistency between the script and character profiles. In the example in Figure 3, the actor Dana suggests more empathetic tone to be align with her character profile. The director filters and aggregates such feedback, then, in collabo-"
        },
        {
            "title": "Work in Progress",
            "content": "ration with the screenwriter, employs the same Critique-Correct-Verify cycle to refine the script. Once the director confirms no further changes are needed, the script is finalized. Cinematography is collaborative process among two peer cinematographers and the director in the Debate-Judge manner to ensure diverse and appropriate camera choices. Cinematographers (agents and Q) independently assign camera choices to each line of the script. They then engage in debate to address any discrepancies in their choices. Consider Figure 3 as an example. In this scenario, the cinematographers debate over the best shot, with one preferring medium shot to capture body language, while the other favors zoom shot to emphasize Danas surprise. Through this debate, the pros and cons of each option are thoroughly explored. After several rounds, the director (the Judgment agent J) summarizes the debate process, resolves any remaining conflicts, and finalizes the camera setup based on the discussion. After these stages, each line in the script is specified with the positions of the actors, their actions, and the chosen camera shots. An example of fully annotated script is displayed in Appendix C. We can simulate the entire script within the constructed 3D environment and begin filming. The duration of each line in the video corresponds to the length of its speech audio."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTAL SETUP Data. We manually brainstorm 15 story ideas that can be implemented within the constraints of our constructed virtual 3D spaces, such as quarrel and breakup scene, late night brainstorming for startup and casual meet-up with an old friend. Evaluation Scheme. We evaluate the generated videos across five key aspects: the scripts fidelity to the intended theme, the appropriateness of camera settings, the alignment of the script with actor profiles, the accuracy of actor actions, and the overall plot coherence. In our preliminary study, we found that all scripts faithfully adhered to the intended story ideas. Therefore, we conduct comprehensive human annotations on the remaining four aspects of the videos. We use 5-point Likert scale to assess the scripts alignment with actor profiles, the appropriateness of camera settings, and the overall plot coherence. The evaluation guideline is in Appendix B. To evaluate the accuracy of actor actions, we randomly select 50 actions from the generated scripts and annotate their accuracy. Finally, we normalize the action accuracy scores to 0-5 scale and calculate average scores across the four aspects. Baselines. Following the experimental setup of AgentVerse (Chen et al., 2024), to validate the superiority of FILMAGENT in facilitating agent collaboration over standalone agents, we compare it against the following baselines: (1) CoT: single agent, guided by hints about key stages in the prompt, directly generates the chain-of-thought rationale and produces the complete script. (2) Solo: single agent is responsible for idea development, scriptwriting, and cinematography, representing our FILMAGENT framework without multi-agent collaboration algorithms. the full FILMAGENT framework, utilizing multi-agent collaboration. All the experiments are done in zero-shot setting. (3) Group, i.e. Implementation Details. Our experiments employ the gpt-4o-2024-05-13 version of OpenAI API to simulate multi-agent virtual film production. For the o1-preview model, we access it through the ChatGPT webpage. The maximum number of iterations in multi-agent collaboration algorithms is set to 3. 4.2 RESULTS The human evaluation results in Table 2 show that FILMAGENT achieves an average score of 3.98 out of 5, validating the effectiveness of FILMAGENT. Agents configured using FILMAGENT (both Solo and Group setups) consistently outperform the standalone CoT agent. This demonstrates the efficacy of decomposing complex tasks into manageable sub-tasks. Comparative analysis between the Solo and Group configurations of FILMAGENT highlights the benefits of the multi-agent framework. FILMAGENT facilitates iterative feedback and revisions through multiple collaboration algorithms, leading to significant improvements across all aspects, especially in plot coherence and the appropri-"
        },
        {
            "title": "Work in Progress",
            "content": "Method LLM Action Plot Profile Camera Avg. GPT-4o CoT o1 CoT GPT-4o FILMAGENT (Solo) FILMAGENT (Group) GPT-4o 0.68 0.80 0.80 0.88 1.60 2.73 1.87 3.53 3.84 3.60 4.20 4. 1.67 2.86 2.07 3.53 2.63 3.30 3.04 3.98 Table 2: Comparison of baselines using human annotations for actor actions, overall plot coherence, script alignment with actor profiles, and appropriateness of camera settings. The evaluation metric for Action is accuracy (0-1), while the others use 5-point Likert scale. ateness of camera settings. Further analysis and detailed case studies underscoring the importance of multi-agent collaboration are provided in Section 4.3. Comparison with o1. Recently OpenAI has released large reasoning model called o1, optimized for complex multi-step tasks and achieving superior performance compared to GPT-4o (Xu et al., 2025). This adavantage is reflected in Table 2: an o1-based CoT agent not only outperforms GPT-4o-based CoT agent, but also surpasses the single-agent version of FILMAGENT in certain aspects. This highlights o1s ability to autonomously decompose complex tasks and solve sub-tasks step by step. However, our findings also show that the multi-agent FILMAGENT framework, despite being built on less advanced GPT-4o foundational model, outperforms the single-agent o1. This demonstrates that well-coordinated multi-agent system can exceed the performance of more advanced underlying model. 4.3 PREFERENCE ANALYSIS To further analyze the effectiveness of multi-agent collaboration, we compare 15 scripts before and after Critique-Correct-Verify, including the Director-Screenwriter Discussion (referred to as Scriptwriting #2, representing the second stage of scriptwriting) and the Actor-DirectorScreenwriter Discussion (referred to as Scriptwriting #3, representing the third stage of scriptwriting). Additionally, we examine 50 randomly selected modifications on the camera choices before and after Debate-Judge in the Cinematography Stage (denoted as Cinematography). For each case, we determine whether the updated version \"wins,\" \"loses,\" or \"ties\" compared to the original version. 44 66.7 73.4 20 13.3 13.3 13.3 Scriptwriting #2 Scriptwriting #3 Cinematography Figure 5 presents the winning rates of revised scripts and reveals clear preference by human evaluators for the revised scripts over the original versions. These results highlight the effectiveness of iterative feedback and verification in multi-agent collaboration strategies, as demonstrated by the four cases in Table 3. For the scriptwriting stage, as illustrated by Case #1, the Director-Screenwriter discussion reduces hallucinations of non-existent actions (e.g., standing suggest), enhances plot coherence, and ensures consistency across scenes. Case #2 shows that Actor-Director-Screenwriter discussion improves the alignment of dialogue with character profiles. For the Debate-Judge method in cinematography, Case #3 demonstrates the correction of an inappropriate dynamic shot, which is replaced with medium shot to better convey body language. Case #4 replaces series of identical static shots with mix of dynamic and static shots, resulting in more diverse camera setup. Figure 5: Compared with the original version, the win, tie, and lose rates of the updated script and camera choices after multi-agent collaboration. 50 Percentage (%) loses wins Tie 100"
        },
        {
            "title": "5 DISCUSSION AND FUTURE WORK",
            "content": "Comparison with Sora. Sora is video generation tool developed by OpenAI, designed to create high-quality videos from text prompts, images or existing videos (Cho et al., 2024). We experiment"
        },
        {
            "title": "Work in Progress",
            "content": "Before Multi-Agent Collaboration After Multi-Agent Collaboration Scene #1 (Roadside) Emma: Id love that. Where should we meet? Alex: (Standing suggest) Theres cafe just around the corner from here. How about tomorrow at 3? Emma: (Standing happy) Perfect! See you tomorrow. Scene #2 (Alexs living room) Alex: (Standing greeting) Welcome to my humble abode! Make yourself comfortable. (Standing thinking) How about at my Scene #1 (Roadside) Emma: Id love that. Where should we meet? Alex: place? Tomorrow at 3? Emma: (Standing happy) Perfect! See you tomorrow. Scene #2 (Alexs living room) Alex: (Standing greeting) Welcome to my humble abode! Make yourself comfortable. Critiques from the Director: For the reasonableness of actions, {\"dialogue\": \"Theres cafe . . . ?\", \"correct_action\": \"Standing suggest\", \"suggested_revision\": \"Standing thinking\"}. For the fluency of the script, the dialogue in Scene 1 mentions meeting up in cafe, but Scene 2 shows them at Alexs house instead. Consider changing Alexs dialogue to mention catching up at his place to make Scene 2 more natural. Brooke: Alex said was always overreacting. It really hurt me. Dana: Sounds rough. There was time felt ignored too but chose to let it go. Maybe we should all lay it out. Brooke: Alex said was always overreacting. It really hurt me. Dana: That must have been really tough for you. There was time felt overlooked too, but talking about it openly could help us all. {\"name\": Danas profile: \"therapist\",\"personality traits\": \"empathetic, patient\",\"speaking style\": \"soothing, deliberate, therapeutic\"}. Critiques from the Actor Dana: It would be more effective to say That must have been really tough for you. This reinforces my empathetic and patient traits. \"female\",\"occupation\": \"34\",\"gender\": \"Dana\",\"age\": Here are the selected shots for the last line in Case #1. Debate from one Cinematographer: Tracking Shot is not applicable as Alex is not moving, violating the guideline of Tracking Shot usage. Instead, the Medium Shot correctly shows Alexs body language. Mia: (Standing Arguing) What is this? found messages between you and Lily. (Medium Shot of Mia) Alex: (Standing Thinking) Mia, can explain. These conversations were some unfinished matters from the past. (Medium Shot of Alex) Mia: (Standing Angry) Past? These are from just last week! How could you hide this from me? (Medium Shot of Mia) Alex: (Standing Deny) didnt think it was important. didnt want to upset you.(Medium Shot of Alex) Mia: (Standing Arguing) What is this? found messages between you and Lily. (Medium Shot of Mia) Alex: (Standing Thinking) Mia, can explain. These conversations were some unfinished matters from the past. (Pan Shot of Alex) Mia: (Standing Angry) Past? These are from just last week! How could you hide this from me? (Pan Shot of Mia) Alex: (Standing Deny) didnt think it was important. didnt want to upset you. (Close-up Shot of Alex) Debate from one Cinematographer about the third line: The Medium Shot is used again to capture Mias body language. However, having consecutive static medium shots might make the scene feel dull. Consider replacing this shot with Pan Shot to create some dynamic tension. Case # Case #2 Case #3 Case #4 Table 3: Comparisons of the scripts and camera settings before (left) and after (right) multi-agent collaboration, with excerpts from their discussion process. Case #1 and #2 are from the CritiqueCorrect-Verify method in Scriptwriting #2 and #3 stages respectively. Case #3 and #4 are from the Debate-Judge method in Cinematography. Sora with the storyboard function on its official website4, which allows users to describe what 4https://sora.com/storyboard"
        },
        {
            "title": "Work in Progress",
            "content": "Figure 6: Comparison of videos showing quarrel and breakup scene produced by FILMAGENT and Sora. Sora demonstrates excellent adaptability to various scenes, styles, and shots, while FILMAGENT can produce coherent, physics-compliant videos with storytelling capabilities. you want to happen at specific time in the generated video. Specifically we utilize the directors planned scenes in FILMAGENT as prompts for each segment of the video. Given that the maximum duration for Soras generated video is currently 10 seconds, we allocate 34 seconds per scene on the storyboard timeline. As illustrated in Figure 6, we compare the videos produced by FILMAGENT with those generated by Sora, and analyze their complementary strengths and weaknesses: Sora excels at quickly adapting to diverse scenes, realistic styles, and various shots (e.g., the close-up of the woman in the first scene to convey her anger). This makes Sora useful tool for video creators seeking rapid brainstorming and idea validation. In contrast, FILMAGENT requires pre-built virtual 3D spaces, characters and cameras. However, Sora faces several challenges: (1) Inconsistencies: The generated videos sometimes fail to align with the text instructions. For example, in the second scene, the prompt specifies that only two main characters should be involved, yet there are four. Additionally, we observe character inconsistencies across frames during our tests. (2) Non-compliance with physics: There are strange artifacts that defy real-world physics. For example, in the second scene, the womans face and right hand blend together unnaturally, and then another phone suddenly appears in her right hand. (3) Limited storytelling capability: Due to short video durations and lack of variation, Sora struggles to convey complete stories. In the third scene, only close-up of the man talking is shown, with minimal variation between frames (just lips moving), and no subtitles or audio to indicate the dialogue, making it hard to follow the plot. In contrast, FILMAGENT effectively addresses these issues by utilizing 3D spaces in game engines and collaborative workflow, ensuring coherence and more comprehensive storytelling capability. Limitations. The primary limitation of our system is its reliance on predefined virtual 3D spaces with limited action spaces and preset camera settings. Recent advancements in 3D scene synthesis, motion, and camera adjustments driven by textual instructions (Qing et al., 2023; Jiang et al., 2024; Hu et al., 2024b) provide more flexible and dynamic alternatives. Future research could integrate these adaptable components into the FILMAGENT framework. Additionally, there are other important areas for improvement: (1) Fine-Grained Control: The current system lacks precise control over actions and camera settings. Annotating actions and camera movements at the line level is too coarse-grained, as single line of script may involve multiple character actions and camera transitions. (2) Multimodal LLM Integration: Film automation is inherently multimodal task requiring visual inputs. Incorporating multimodal LLMs presents promising direction for improving the accuracy of feedback and verification processes (Xu et al., 2024; Li et al., 2024a). (3) Expanded Crew Roles: To create video that meets the standards of film, essential crew roles such as music composition, color grading, and video editing need to be included."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We present FILMAGENT, an LLM-based multi-agent framework that automates end-to-end film production in virtual 3D spaces. This framework incorporates our constructed 3D spaces, simulates efficient human workflows, and employs multi-agent collaboration strategies. Extensive human evaluations rate the videos produced by FILMAGENT with an average score of 3.98 out of 5, underscoring its effectiveness. Further analysis shows that multi-agent collaboration significantly enhances script quality, improves camera selection, and reduces hallucination errors. These findings demonstrate the potential of FILMAGENT to advance film automation through multi-agent systems."
        },
        {
            "title": "REFERENCES",
            "content": "Aimone Bodini, Arthi Manohar, Federico Colecchia, David Harrison, and Vanja Garaj. Envisioning the future of virtual production in filmmaking: remote co-design study. Multimedia Tools and Applications, 83(7):1901519039, February 2024. Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi Lu, Yi-Hsin Hung, Chen Qian, Yujia Qin, Xin Cong, Ruobing Xie, Zhiyuan Liu, Maosong Sun, and Jie Zhou. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=EHg5GDnyq1. Joseph Cho, Fachrina Dewi Puspitasari, Sheng Zheng, Jingyao Zheng, Lik-Hang Lee, Tae-Ho Kim, Choong Seon Hong, and Chaoning Zhang. Sora as an agi world model? complete survey on text-to-video generation, 2024. URL https://arxiv.org/abs/2403.05131. Marc Christie, Patrick Olivier, and Jean-Marie Normand. Camera control in computer graphics. Computer Graphics Forum, 27(8):21972218, 2008. doi: https://doi.org/10.1111/j.1467-8659. 2008.01181.x. URL https://onlinelibrary.wiley.com/doi/abs/10.1111/j. 1467-8659.2008.01181.x. Roi Cohen, May Hamri, Mor Geva, and Amir Globerson. LM vs LM: Detecting factual errors In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of via cross examination. the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1262112640, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. emnlp-main.778. URL https://aclanthology.org/2023.emnlp-main.778. Edirlei E. S. de Lima, Cesar T. Pozzer, Marcos C. dOrnellas, Angelo E. M. Ciarlini, Bruno Feijó, and Antonio L. Furtado. Virtual cinematography director for interactive storytelling. In Proceedings of the International Conference on Advances in Computer Entertainment Technology, ACE ISBN 09, pp. 263270, New York, NY, USA, 2009. Association for Computing Machinery. 9781605588643. doi: 10.1145/1690388.1690432. URL https://doi.org/10.1145/ 1690388.1690432. Li-wei He, Michael F. Cohen, and David H. Salesin. The virtual cinematographer: paradigm In Proceedings of the 23rd Annual for automatic real-time camera control and directing. Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 96, pp. 217224, New York, NY, USA, 1996. Association for Computing Machinery. ISBN 0897917464. doi: 10.1145/237170.237259. URL https://doi.org/10.1145/237170.237259. Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber. MetaGPT: Meta programming for multi-agent collaborative framework. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=VtmBAGCN7o. Siyuan Hu, Mingyu Ouyang, Difei Gao, and Mike Zheng Shou. The dawn of gui agent: preliminary case study with claude 3.5 computer use, 2024a. URL https://arxiv.org/abs/ 2411.10323."
        },
        {
            "title": "Work in Progress",
            "content": "Ziniu Hu, Ahmet Iscen, Aashi Jain, Thomas Kipf, Yisong Yue, David A. Ross, Cordelia Schmid, and Alireza Fathi. Scenecraft: An llm agent for synthesizing 3d scene as blender code, 2024b. URL https://arxiv.org/abs/2403.01248. Hongda Jiang, Bin Wang, Xi Wang, Marc Christie, and Baoquan Chen. Example-driven virtual cinematography by learning camera behaviors. ACM Trans. Graph., 39(4), aug 2020. ISSN 07300301. doi: 10.1145/3386569.3392427. URL https://doi.org/10.1145/3386569. 3392427. Hongda Jiang, Marc Christie, Xi Wang, Libin Liu, Bin Wang, and Baoquan Chen. Camera keyframing with style and control. ACM Trans. Graph., 40(6), December 2021. ISSN 0730-0301. doi: 10.1145/3478513.3480533. URL https://doi.org/10.1145/3478513.3480533. Hongda Jiang, Xi Wang, Marc Christie, Libin Liu, and Baoquan Chen. Cinematographic camera diffusion model, 2024. Manolya Kavakli and Cinzia Cremona. The virtual production studio concept an emerging game changer in filmmaking. In 2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR), pp. 2937, 2022. doi: 10.1109/VR51125.2022.00020. Mackenzie Leake, Abe Davis, Anh Truong, and Maneesh Agrawala. Computational video editing for dialogue-driven scenes. ACM Trans. Graph., 36(4), jul 2017. ISSN 0730-0301. doi: 10.1145/ 3072959.3073653. URL https://doi.org/10.1145/3072959.3073653. Rob Legato and Caleb Deschanel. Disney presents: the making of the lion king. In ACM SIGGRAPH 2019 Production Sessions, SIGGRAPH 19, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450361958. doi: 10.1145/3292423.3310247. URL https://doi.org/10.1145/3292423.3310247. Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. CAMEL: Communicative agents for mind exploration of large language model society. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https:// openreview.net/forum?id=3IyL2XWDkG. Tsai-Yen Li and Chung-Chiang Cheng. Real-time camera planning for navigation in virtual enIn Proceedings of the 9th International Symposium on Smart Graphics, SG 08, vironments. pp. 118129, Berlin, Heidelberg, 2008. Springer-Verlag. ISBN 9783540854104. doi: 10.1007/ 978-3-540-85412-8_11. URL https://doi.org/10.1007/978-3-540-85412-8_ 11. Yunxin Li, Shenyuan Jiang, Baotian Hu, Longyue Wang, Wanqi Zhong, Wenhan Luo, Lin Ma, and Min Zhang. Uni-moe: Scaling unified multimodal llms with mixture of experts, 2024a. URL https://arxiv.org/abs/2405.11273. Yunxin Li, Haoyuan Shi, Baotian Hu, Longyue Wang, Jiashun Zhu, Jinyi Xu, Zhen Zhao, and Min Zhang. Anim-director: large multimodal model powered agent for controllable animation video generation, 2024b. URL https://arxiv.org/abs/2408.09787. Han Lin, Abhay Zala, Jaemin Cho, and Mohit Bansal. Videodirectorgpt: Consistent multi-scene video generation via llm-guided planning, 2023. Christophe Lino and Marc Christie. Intuitive and efficient camera control with the toric space. ACM Trans. Graph., 34(4), jul 2015. ISSN 0730-0301. doi: 10.1145/2766965. URL https: //doi.org/10.1145/2766965. M. Minsky. Society Of Mind. Touchstone book. Simon & Schuster, 1988. ISBN 9780671657130. URL https://books.google.com/books?id=bLDLllfRpdkC. Michael Nebeling, Shwetha Rajaram, Liwei Wu, Yifei Cheng, and Jaylin Herskovitz. Xrstudio: In Provirtual production and live streaming system for immersive instructional experiences. ceedings of the 2021 CHI Conference on Human Factors in Computing Systems, CHI 21, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450380966. doi: 10.1145/3411764.3445323. URL https://doi.org/10.1145/3411764.3445323."
        },
        {
            "title": "Work in Progress",
            "content": "Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, UIST 23, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400701320. doi: 10.1145/3586183.3606763. URL https://doi.org/10.1145/3586183.3606763. Chen Qian, Xin Cong, Wei Liu, Cheng Yang, Weize Chen, Yusheng Su, Yufan Dang, Jiahao Li, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. Communicative agents for software development, 2023. Zhongfei Qing, Zhongang Cai, Zhitao Yang, and Lei Yang. Story-to-motion: Synthesizing inIn SIGGRAPH Asia 2023 Technifinite and controllable character animation from long text. cal Communications, SA 23, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400703140. doi: 10.1145/3610543.3626176. URL https://doi.org/10. 1145/3610543.3626176. Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, and Emad Barsoum. Agent laboratory: Using llm agents as research assistants, 2025. URL https://arxiv.org/abs/2501.04227. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models, 2023. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji-Rong Wen. survey on large language model based autonomous agents, 2024. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. URL https://openreview.net/forum?id=yzkSU5zdwD. Survey Certification. M. Wohl. Editing Techniques with Final Cut Pro. Peachpit Press, 2004. ISBN 9780321168870. URL https://books.google.com/books?id=Q9VTAAAAMAAJ. Minghao Wu, Jiahao Xu, and Longyue Wang. TransAgents: Build your translation company with language agents. In Delia Irazu Hernandez Farias, Tom Hope, and Manling Li (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 131141, Miami, Florida, USA, November 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-demo.14. URL https://aclanthology.org/ 2024.emnlp-demo.14/. Minghao Wu, Yulin Yuan, Gholamreza Haffari, and Longyue Wang. (perhaps) beyond human translation: Harnessing multi-agent collaboration for translating ultra-long literary texts, 2024b. URL https://arxiv.org/abs/2405.11804. Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui. The rise and potential of large language model based agents: survey, 2023. Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, Chenyang Shao, Yuwei Yan, Qinglong Yang, Yiwen Song, Sijian Ren, Xinyuan Hu, Yu Li, Jie Feng, Chen Gao, and Yong Li. Towards large reasoning models: survey on scaling llm reasoning capabilities, 2025. URL https://arxiv.org/ abs/2501.09686. Zhenran Xu, Senbao Shi, Baotian Hu, Jindi Yu, Dongfang Li, Min Zhang, and Yuxiang Wu. Towards reasoning in large language models via multi-agent peer review collaboration, 2023."
        },
        {
            "title": "Work in Progress",
            "content": "Zhenran Xu, Senbao Shi, Baotian Hu, Longyue Wang, and Min Zhang. MultiSkill: Evaluating large multimodal models for fine-grained alignment skills. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 15061523, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp.81. URL https://aclanthology.org/ 2024.findings-emnlp.81/. Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users, 2023. Jintian Zhang, Xin Xu, Ningyu Zhang, Ruibo Liu, Bryan Hooi, and Shumin Deng. Exploring collaboration mechanisms for llm agents: social psychology view, 2024. Mingchen Zhuge, Haozhe Liu, Francesco Faccio, Dylan R. Ashley, Róbert Csordás, Anand Gopalakrishnan, Abdullah Hamdi, Hasan Abed Al Kader Hammoud, Vincent Herrmann, Kazuki Irie, Louis Kirsch, Bing Li, Guohao Li, Shuming Liu, Jinjie Mai, Piotr Piekos, Aditya Ramesh, Imanol Schlag, Weimin Shi, Aleksandar Stanic, Wenyi Wang, Yuhui Wang, Mengmeng Xu, Deng-Ping Fan, Bernard Ghanem, and Jürgen Schmidhuber. Mindstorms in natural languagebased societies of mind, 2023."
        },
        {
            "title": "A ENVIRONMENT DETAILS",
            "content": "There are 15 locations in our constructed virtual 3D spaces. Figure 7 displays the screenshots of each location. Based on Figure 2, nine types of camera shots are annotated in Figure 8. The descriptions and views of these static and dynamic shots are shown in Table 1 and 4. Characters can perform 21 actions from the Mixamo website. The complete list of these actions is as follows: \"Joyful Jump\", \"Sit Down\", \"Sitting Clapping\", \"Sitting Laughing\", \"Sitting Talking\", \"Stand Up\", \"Standing Agree\", \"Standing Angry\", \"Standing Arguing\", \"Standing Bored\", \"Standing Crying\", \"Standing Deny\", \"Standing Depressed\", \"Standing Greeting\", \"Standing Happy\", \"Standing Normal\", \"Standing Puzzled\", \"Standing Surprise\", \"Standing Talking\", \"Standing Thinking\", \"Walking\""
        },
        {
            "title": "B HUMAN EVALUATION GUIDELINES",
            "content": "Annotators evaluate the generated videos across 5 key aspects. They assign binary score (yes/no) to evaluate the scripts fidelity to the intended theme and the accuracy of the actors actions. For the other three aspects (i.e., the appropriateness of camera settings, the script alignment with actor profiles, and the overall plot coherence), annotators use Likert scale from 1 to 5, following the guidelines provided in Table 5."
        },
        {
            "title": "C SCRIPT EXAMPLE",
            "content": "Figure 9 presents fully annotated script excerpt that is prepared for simulation within the constructed 3D environment and is ready to begin filming. Since the walking action involves the position change, both the initial and actor positions for each line are documented, and the walking action is separately represented in the \"move\" dictionary of the JSON file. We can simulate the entire script within our constructed 3D virtual spaces and begin filming."
        },
        {
            "title": "D PROMPT LIST",
            "content": "The workflow of FILMAGENT is divided into three stages, each guided by specific prompts. Table 6 provides an overview of these stages, the corresponding prompts from Figure 10 to Figure 24, and their usage."
        },
        {
            "title": "Work in Progress",
            "content": "(a) Apartment kitchen (b) Apartment living room (c) Beverage room (d) Billiard room (e) Dining room (f) Gaming room (g) Large kitchen (h) Meeting room (i) Office (j) Reception room (k) Relaxing room (l) Roadside (m) Sofa corner (n) Storehouse (o) Work room Figure 7: Locations in our constructed 3D spaces."
        },
        {
            "title": "Work in Progress",
            "content": "Figure 8: vertical view of the living room in FILMAGENT, with additional annotations of 9 shot types based on Figure 2. The letters indicate designated actor positions, while the numbers represent camera positions for cinematography. The views from these cameras are shown in Table 1 and 4."
        },
        {
            "title": "Work in Progress",
            "content": "No. Shot Type Description View ④"
        },
        {
            "title": "Pan Shot",
            "content": "⑤ Zoom Shot ⑥ Tracking Shot ⑦ ⑧ Curve Surround Shot 360-Degree Arc Shot ⑨ Truck Shot pan shot smoothly rotates horizontally from one side to the other while remaining stationary. The view follows the subjects movement from to D. the subject Zooming brings closer, effectively magnifying specific focus point in the frame. The view shows the zoom shot from position B. tracking shot involves moving camera that follows one or more characters. The view of the example follows the characters back from position to D. Curve Surround Shot is an Arc Shot orbiting the camera around character from feet to head. The character often makes an entrance as the camera circles it. 360-degree Arc Shot revolves the camera around character at fixed height, typically with the character stationary as the camera circles it. Trucking involves the camera moving side to side along fixed point, effective for conveying scene dynamics. The view in the example provides comprehensive view of the entire location. Table 4: Examples of 6 types of dynamic shots in Figure 8."
        },
        {
            "title": "Work in Progress",
            "content": "Plot Profile Camera 5 4 2 1 that The story is highly engaging, with rich emotional profoundly depth touches the audience. The story is logically flawless, progresses very smoothly, and is rich in detail and refinement. Different parts of the story are tightly interconnected, displaying high level of coherence and creativity. The story is quite engaging and emotionally deep, resonating well with the audience. The story is logically clear, progresses fluidly, and handles details well. Different parts of the story correspond well, showing reasonable coherence and depth. improvement. The story is moderately engaging and emotionally there is room deep, but for The story generally progresses smoothly but has some minor logical errors or lacks Different compactness. parts of the story generally correspond, but the depth and cleverness of the references are average. The story has some appeal, but insufficient emotional expression and lacks depth. The storys development has logical discontinuities and poor fluidity. The connection between different parts of the story is weak, with only minor references. The story lacks appeal and the emotional expression is The storys probland. gression lacks logic, making it hard to understand or follow. Different parts of the story are almost unconnected, lacking internal consistency. The dialogue perfectly reflects the unique professional attributes of the character. The dialogue deeply reveals the characters personality traits. The dialogue fully maintains the characters specific linguistic style. The dialogue well reflects the unique professional attributes of the character. The dialogue well reveals the characters personality traits. The dialogue maintains good consistency with the characters linguistic style. The dialogue moderately reflects the unique professional attributes of the character. The dialogue moderately reveals the characters personality traits. The dialogue is moderately consistent with the characters linguistic style. The script demonstrates excellent creativity and diversity in shot usage. Dynamic and static shots are used effectively, perfectly supporting the rhythm and emotions of the storyline. All shots are carefully chosen, resulting in an excellent visual experience. The script features diverse range of covering shots, various dynamic and static shots, reflecting good visual rhythm. Each shot is used appropriately, enhancing the expression of the plot. Almost no repetition of the same static shots, overall smooth flow. The script includes moderate variety of shots, covering both dynamic and static types. shots are Most used appropriately to support the plot, but there is room for improvement. Frequent use of the same static shots is rare, but not entirely avoided. The dialogue minimally reflects the unique professional attributes of the charThe dialogue minacter. imally reveals the characters personality traits. The dialogue somewhat deviates from the characters linguistic style. The dialogue does not reflect the unique professional attributes of the character. The dialogue does not reveal the characters personality traits. The dialogue significantly deviates from the characters linguistic style. The script features limited variety of shots, with minimal variation. Insufficient use of both dynamic and static shots, failing to effectively support plot development. Some shots are repeated, though there is occasional inclusion of different shots. The script uses very limited and repetitive range of shots, with almost no variation. Lacks balanced use of dynamic or static shots. Repeated and frequent use of the same shots leads to visual fatigue. Table 5: Details of the 5-point Likert scale for overall plot coherence, script alignment with actor profiles, and appropriateness of camera settings."
        },
        {
            "title": "Work in Progress",
            "content": "[ { \"scene information\": { \"who\": [ \"Alex\", \"Mia\" ], \"where\": \"Apartment living room\", \"what\": \"Mia finds out that Alex has been hiding something significant from her-a secret correspondence with his ex, Lily. She is furious, feeling betrayed and (cid:44) confused. The scene begins with Mia confronting Alex, asking him to explain (cid:44) the messages she stumbled upon. Mia's anger intensifies as Alex tries to (cid:44) explain, leading to heated exchange.\" (cid:44) }, \"initial position\": [ { }, { } \"character\": \"Mia\", \"position\": \"Position D\" \"character\": \"Alex\", \"position\": \"Position B\" ], \"scene\": [ { \"move\": { \"character\": \"Mia\", \"destination\": \"Position A\" }, \"shot\": \"Track Shot\", \"current position\": [ { }, { } \"character\": \"Mia\", \"position\": \"Position D\" \"character\": \"Alex\", \"position\": \"Position B\" ] }, { \"speaker\": \"Mia\", \"actions\": [ { } \"character\": \"Mia\", \"state\": \"standing\", \"action\": \"Standing Arguing\" ], \"content\": \"Alex, what is this? found messages between you and Lily.\", \"shot\": \"Medium Shot\", \"current position\": [ { }, { } \"character\": \"Mia\", \"position\": \"Position A\" \"character\": \"Alex\", \"position\": \"Position B\" ] }, ..., ] }, ..., ] Figure 9: script excerpt ready for simulation in our 3D virtual spaces."
        },
        {
            "title": "Work in Progress",
            "content": "Stage"
        },
        {
            "title": "Idea Development",
            "content": "Scriptwriting (Stage 1) Scriptwriting (Stage 2) Scriptwriting (Stage 3) Cinematography Prompt Figure 10 Figure 11 Figure 12 Figure 13 Figure 14 Figure 15 Figure 16 Figure Figure 18 Figure 19 Figure 20 Figure 17, Figure 21 Figure 22 Figure 23 Figure 24 Description The director generates character profiles. The director expands the story idea into scene outline. The screenwriter drafts the dialogue in the script. The screenwriter assigns positions for characters. The screenwriter annotates actions for each line. The screenwriter adds movements between positions. The director provides critiques on plot coherence and action appropriateness. The screenwriter revises the script based on the directors critiques. The director verifies the updated script to determine if further adjustments are needed. Actors provide feedback based on character profiles. The director filters and aggregates actor feedback. The same Critique-Correct-Verify cycle is utilized to refine the script. Cinematographers assign camera choices to each line of the script. Debate between cinematographers over camera choices. Director resolves conflicts and finalizes the camera setup. Table 6: The stages, corresponding prompts, and their usage of FILMAGENT. You are tasked with directing film based on provided topic. You need to brainstorm the main characters in the film and provide profile for each character. ### Film topic: {topic} ### Requirements: 1. Profile content: - The profile should include the name, gender (male or female), age, occupation, personality traits and speaking style. - The name should only have one word. 2. Number of characters: - Include no more than 4 main characters. Your response should only contain the following JSON content: [{\"name\": \"...\", \"age\": \"...\", \"gender\": \"...\", \"occupation\": \"...\", \"personality traits\": \"...\", \"speaking style\": \"...\" }, ...] Figure 10: Prompt for the idea development stage: The director generates character profiles."
        },
        {
            "title": "Work in Progress",
            "content": "You are tasked with directing film based on provided topic. You need to plan several coherent scenes according to the topic. ### Film Topic: {topic} ### Main Characters: 1. Male: {male_characters} 2. Female: {female_characters} ### Main Locations: 1. Apartment living room: maximum capacity: 5 2. Apartment kitchen: maximum capacity: 5 3. Roadside: maximum capacity: 2 4. Gaming room: maximum capacity: 4 5. Meeting room: maximum capacity: 7 6. Storehouse: maximum capacity: 3 7. Relaxing room: maximum capacity: 5 8. Reception room: maximum capacity: 5 9. Sofa corner: maximum capacity: 5 10. Large kitchen: maximum capacity: 5 11. Beverage room: maximum capacity: 3 12. Office: maximum capacity: 3 13. Dining room: maximum capacity: 4 14. Billiard room: maximum capacity: 4 15. Work room: maximum capacity: 5 ### Planning Steps: 1. Determine the number of scenes and assign simple sub-topic to each scene. - The number of scenes should be no more than 3. 2. Based on the sub-topic, select the location from the Main Locations for each scene. 3. Based on the sub-topic, select several characters from the Main Characters for each scene. - The number of characters selected for each scene can not exceed the maximum capacity of the selected location. - The number of characters selected for each scene is at least two. - All the main characters must be chosen at least once. 4. Based on the sub-topic, write story plot for each scene. - The story plot for each scene must include only the characters selected for this scene in step 3. - All the story plots should be story-coherent, and the more dramatic and specific the story, the better. - All the story plots should be detailed and give adequate background information. 5. Based on the story plot, give final dialogue goal so that the dialogue between the characters in this scene can end naturally. Your response should only contain the following JSON content: [{\"sub-topic\": \"...\", \"selected-characters\": [\"...\",\"...\",...], \"selected-location\": \"...\", \"story-plot\": \"...\", \"dialogue-goal\": \"...\" }, ...] Figure 11: Prompt for the idea development stage: The director generates the scene outline."
        },
        {
            "title": "Work in Progress",
            "content": "You are tasked as screenwriter to create specific dialogues based on the provided script outline. Please use your creativity and understanding of the plot to write vivid dialogues that drive the story forward, making the script rich and engaging. ### Script Outline: {scene_outline} ### Requirements: - Keep the dialogue natural, concise, and vivid, avoiding repetition, clichés, and the use of numbers. - In each scene, the characters participating in the dialogue can only include those specified in the script for that scene. - The number of dialogues in each scene should not be excessive. - The dialogue in each scene should ultimately achieve the given dialogue-goal, allowing the scene to end naturally. Your response should only contain the following JSON content: [{\"scene-topic\": \"...\", \"scene-plot\": \"...\", \"scene-dialogue\": [{\"speaker\": \"...\", \"content\": \"...\"}, ...] }, ...] Figure 12: Prompt for the scriptwriting stage: The screenwriter drafts the dialogue in the script. You are screenwriter. You need to choose an appropriate position for each character in every scene of the script. ### Script Information: {scene_outline} ### Optional Positions: {position_description} ### Requirements: 1. In scene, each characters position must be different. 2. You need to provide reason for your choice of position. Your response should only contain the following JSON content: [{\"scene-id\": \"...\", \"scene-location\": \"...\", \"reason\": \"...\", \"scene-position\": [{\"character\": \"...\", \"position\": \"...\"}, ...] }, ...] Figure 13: Prompt for the scriptwriting stage: The screenwriter assigns positions for characters."
        },
        {
            "title": "Work in Progress",
            "content": "You are screenwriter. You need to read the dialogues and story plot in the script, then add appropriate actions for the characters based on your understanding. ### [Script Information]: 1. Plot: {scene_outline} 2. Dialogues: {dialogue_draft} 3. Characters Initial Positions and States: {initial_position} (e.g., Alex: Position A, sittable, standing) ### [Complete List of Actions]: 1. Actions performed in standing state: - Standing Talking: simply show the character is talking. - Standing Thinking: show the character is thinking. - Standing Depressed: convey the characters depressed emotion. - Standing Crying: show the character is crying. - Standing Angry: convey the characters angry emotion. - Standing Happy: convey the characters happy emotion. - Standing Surprise: convey the characters surprise emotion. - Standing Puzzled: show the characters confusion. - Standing Greeting: only used for the character to greet others. - Standing Bored: show the characters boredom. - Standing Normal: merely depict the character standing. - Standing Arguing: show the character is arguing with others. - Standing Agree: convey the characters approval. - Standing Deny: convey the characters rejection or disapproval. - Joyful Jump: demonstrate the character is extremely happy. - Sit Down: Only when you need to change the characters state from standing to sitting, you must perform this action. 2. Actions performed in sitting state: - Sitting Talking: simply show the character is talking. - Sitting Laughing: convey the characters happy emotion. - Sitting Claping: indicate the character agrees or is happy. - Stand Up: Only when you need to change the characters state from sitting to standing, you must perform this action. ### [Action Selection Requirements]: 1. Basic Requirements: - Note that all actions should be selected from Complete List of Actions. - Each character can only add one action at time. - Unnecessary actions should not be added to avoid cluttering the scene. 2. Key Requirements: - In one scene, you cannot overuse certain action. - When character is in the standing state, you can only choose the action performed in standing state. When character is in the sitting state, you can only choose the action performed in sitting state. - You can choose \"Sit Down\" only when the character is at sittable position. - Only by using \"Stand Up\" or \"Sit Down\" can the characters state be changed; otherwise, the characters state should remain the same as before. ### [Action Selection Steps]: - Step 1: Understand the emotions expressed in the dialogues based on the story plot, and consider the emotional states of the characters in the conversation. - Step 2: According to the considerations from Step 1 and Action Selection Requirements, choose the most appropriate actions from the Complete List of Actions and add them after each line of dialogue, including the speaker and some other (not all) characters actions."
        },
        {
            "title": "Work in Progress",
            "content": "(continued from the previous page) ### [Output Content]: Each action added should consist of the following parts: - action: The action selected from Complete List of Actions. - character: The character that performs the action. - state: The state before the action is performed, including standing or sitting. - reasoning: The logical reasoning process that includes the complete two action selection steps mentioned above. Your response should only contain the following JSON content: [{\"speaker\": \"...\", \"content\": \"...\", \"actions\": [{\"reasoning\": \"...\", \"character\": \"...\", \"state\": \"...\", \"action\": \"...\"}, ...] }, ...] Figure 14: Prompt for the scriptwriting stage: The screenwriter annotates actions for each line. You are screenwriter. Please first read the script information and decide whether to add character movement to the script, and if so, add an appropriate movement. ### Movement Information: 1. Movable Characters: {characters_in_standing_state} 2. Optional Destinations: {position_description} ### Script Information: 1. Plot: {scene_outline} 2. Dialogues: {dialogue_with_insertion} (e.g., [\"<Insertion Position 0>\",{\"speaker\":\"Alex\",\"content\":\"How are you?\"},\"<Insertion Position 1>\",{\"speaker\":\"Taylor\",\"content\":\"Im fine\"}, ...]) 3. The current positions of characters: {initial_position} ### Requirements: 1. You should not add any unnecessary character movement. 2. If you need to add character movement, you must provide sufficient and necessary reason. 3. Output Requirements: - If no character movement is required, return: {\"reason\": \"...\", \"move\": \"None\"} - If you need to add character movement, please specify the character who will move, the destination of the movement, the reason for adding this movement, the best insertion position of the movement in the Dialogues and the corresponding reason for insertion, return: {\"move\": {\"reason\": \"...\", \"character\": \"...\", \"destination\": \"...\" }, \"insertion\": {\"insertion reason\": \"...\", \"insertion position\": \"...\" } } Your response should only contain the Output in JSON format. Figure 15: Prompt for the scriptwriting stage: The screenwriter adds movements between positions."
        },
        {
            "title": "Work in Progress",
            "content": "You are director. Your task is to thoroughly review the original script and provide detailed and specific feedback for potential improvements. ### [Film Theme]: {topic} ### [Original Script]: {draft_script} (formatted as Appendix C) ### [Complete List of Actions]: {action_list} (same as prompt in Figure 14) ### [Detailed Feedback]: 1. Action Reasonableness: - Check whether the actions used in the script do not exist in Complete List of Actions. - Check whether the actions are appropriate. If you find it inappropriate, suggest better action in Complete List of Actions. - Check whether any character has violated the rule that character can only add one action at time. - Check whether any character transitions from standing state to sitting state without using \"Sit Down\", or transitions from sitting state to standing state without using \"Stand Up\". - Check whether any character uses \"Sit Down\" at an unsittable position. 2. Theme Consistency: - Evaluate whether the theme is clearly and strongly presented in the script. 3. Script Coherence: - Evaluate whether the script is coherent and captivating, with pacing that neither drags nor rushes. If not, first identify the dialogues in the script that lead to this outcome, then provide detailed revision suggestions. - Assess whether the script flows smoothly and is well-structured, ensuring there are no abrupt jumps or sudden plot twists that disrupt the flow. If not, first identify the dialogues in the script that lead to this outcome, then provide detailed revision suggestions. Please support your feedback with detailed logical reasoning, specific elements of the script. linking observations to Your response should be formatted as the following JSON content: {\"action-reasonableness\": \"...\", \"theme-consistency\": \"...\", \"script-fluency\": \"...\" } Figure 16: Prompt for the scriptwriting stage: The director provides feedback on action appropriateness, theme consistency and script coherence."
        },
        {
            "title": "Work in Progress",
            "content": "You are screenwriter. You have received feedback on your script from the director. Now, you need to take this feedback into consideration and provide an updated script. ### [Film Theme]: {topic} ### [Directors feedback]: {director_critique} ### [Original Script]: {draft_script} ### [Complete List of Actions]: {action_list} ### [Position Information]: {initial_position} ### [Basic Requirements]: 1. Dialogue Requirements: - Please keep the dialogue natural, concise, and vivid, avoiding repetition, clichés, and the use of numbers. - In each scene, the characters participating in the dialogue can only include those specified in the script for that scene. - The number of dialogues in each scene should not be excessive. 2. Action Requirements: - Note that all actions in updated script should be selected from Complete List of Actions. - In scene, you cannot overuse certain action. - When character is in the standing state, you can only choose the action performed in standing state. When character is in the sitting state, you can only choose the action performed in sitting state. - You can choose \"Sit Down\" only when the character is at sittable position. - Each character can only add one action at time. - Only by using \"Stand Up\" or \"Sit Down\" can the characters state be changed; otherwise, the characters state should remain the same as before. - Each action should consist of the following parts: reason: The reason for your choice of action. action: The action selected from Complete List of Actions. character: The character that performs the action. state: The state before the action is performed, including standing or sitting. You must make corresponding updates in the script for each piece of feedback provided by director. Your response should only contain the following JSON content: [{\"scene_information\": {\"who\": \"...\",\"where\": \"...\", \"what\": \"...\"}, \"initial position\": [{\"character\": \"...\", \"position\": \"...\"}, ...], \"dialogues\": [{\"speaker\": \"...\", \"content\": \"...\", \"actions\": [{\"reason\": \"...\", \"character\": \"...\", \"state\": \"...\", \"action\": \"...\"}, ...]}, ...] }, ...] Figure 17: Prompt for the scriptwriting stage: The screenwriter updates the script based on the directors feedback."
        },
        {
            "title": "Work in Progress",
            "content": "You are director. You have provided feedback on the script, and then the script has been revised. ### Your Feedback: {director_critique} ### Updated Script: {updated_script} Please review the updated script to determine if it fully addresses your feedback. Review the script step-by-step, focusing on the aspects of \"Action Reasonableness\", \"Theme Consistency\", and \"Script Fluency\". Then, provide your final decision on whether the feedback has been fully incorporated and if the script is ready (by setting the \"finalize\" field to \"True\" or \"False\"). Your response should only contain the following JSON content: {\"action-reasonableness\": \"...\", \"theme-consistency\": \"...\", \"script-fluency\": \"...\", \"finalize\": \"...\"} Figure 18: Prompt for the scriptwriting stage: In Director-Screenwriter discussion, the director verifies the updated script to determine whether further adjustments are needed. You are an actor. Now you are playing the role of {character}. Your task is to review your dialogue content (i.e., the lines of {character}) in the script, then provide necessary feedback for script improvement based on your personal profile. ### Your Personal Profile: {character_profile} ### Script: {draft_script} ### Requirements: - Note that you only provide feedback on your own lines (i.e., the lines of {character}). - The number of your feedback should not exceed 3. - Link your feedback to specific elements of the script. Your response should only contain the following JSON content (The \"content\" field should be consistent with the script. You only need to write your suggestions in the \"feedback\" field.): [{\"speaker\": \"{character}\", \"content\": \"...\", \"feedback\": \"...\"}, ...] Figure 19: Prompt for the scriptwriting stage: Actors provide feedback based on character profiles."
        },
        {
            "title": "Work in Progress",
            "content": "You are director. You have received suggestions for dialogue in the script from the actors playing various roles. Now you need to rigorously consider whether to adopt these suggestions from the following three aspects: ### Main Characters: {character_profiles} ### Script: {draft_script} #### Detailed Consideration: - Dialogue Fluency: Consider whether these suggestions will improve the pace and fluency of the dialogue. Ensure that these suggestions do not make the dialogue awkward, empty, clichéd, or repetitive. - Plot Coherence: Evaluate whether these suggestions impact the overall coherence and logic of the plot, and whether they help in advancing the story. - Character Authenticity: Consider whether these suggestions can enhance the authenticity of the character, better convey the characters emotions and motivations, and make the character more vivid. ### Actors suggestions: {actor_critique} ### Requirements: - Based on the considerations above, filter out the suggestions you need to adopt (you can also choose not to adopt any suggestions). - Make sure the number of suggestions you adopt is NOT too many. - You need to provide specific reasons for your decision. ### Output Format: - If you want to adopt some suggestions provided by actors, return the suggestions you adopted in the following JSON format: {\"adopted-suggestions\": [{\"reason\": \"...\", \"speaker\": \"...\", \"content\": \"...\", \"feedback\": \"...\"}, ...] } - If you do not want to adopt any suggestions, return: {\"reason\": \"...\", \"adopted-suggestions\": \"None\"} Figure 20: Prompt for the scriptwriting stage: The director filters and aggregates the feedback from actors."
        },
        {
            "title": "Work in Progress",
            "content": "You are director. You have provided feedback on the script, and then the script has been revised. ### Feedback: {filtered_critique} ### Updated Script: {updated_script} Please review the updated script to determine if it fully addresses your feedback. Review the script step-by-step. Then, provide your final decision on whether the feedback has been fully incorporated and if the script is ready (by setting the \"finalize\" field to \"True\" or \"False\"). You need to provide the specific reason for your decision. Your response should only contain the following JSON content: {\"reason\": \"...\", \"finalize\": \"...\"} Figure 21: Prompt for the scriptwriting stage: In Actor-Director-Screenwriter discussion, the director verifies the updated script to determine whether further adjustments are needed."
        },
        {
            "title": "Work in Progress",
            "content": "You are cinematographer. You need to complete the shot annotations for provided script. Please first read the script thoroughly. Then, based on your insights and the specific usage conditions for each type of shot, select the most suitable shots from [Complete List of Shots]. ### [Provided Script]: {final_script} ### [Complete List of Shots]: Dynamic Shots: Zoom Shot: Appears to move closer to the subject, following Long Shot. Usage Condition: 1. When the character is talking, gradually focus on the character. 2. When the character is talking, highlight key comedic or dramatic moments. Pan Shot: Pivots horizontally from fixed position to track character movement. Usage Condition: 1. When the character is talking, use this shot multiple times in row to create tense atmosphere. 2. When the character is moving, use this shot to display both the character and surroundings. Tracking Shot: Continuously follows moving character. Usage Condition: This shot can only be used when the character is moving, immersing the audience in the characters perspective during movement. 360-Degree Arc Shot: Circles completely around standing character. Usage Condition: 1. This shot can be used when the character first appears in the script. 2. When the character is talking, create unease and heighten scene tension. Curve Surround Shot: Moves from the characters feet to head, with the character standing. Usage Condition: This shot can only be used when the character first appears in the script. Truck Shot: Switches dynamically between long shots. Usage Condition: This shot can only be used in the opening shot of each scene to showcase characters and their environments. Static Shots: Long Shot: Displays the entire scene. Usage Condition: Providing background and showing interactions during dialogue. Close-up Shot: Focuses on the upper body of character. Usage Condition: Captures the characters emotions during dialogue. Medium Shot: Captures the full body of character. Usage Condition: Showcasing body language and interactions."
        },
        {
            "title": "Work in Progress",
            "content": "(continued from the previous page) ### [Shot Annotation Requirements]: - Each scene should not have too many close-up shots and medium shots. - Each shot used must meet its usage conditions. - For the dialogue-starting scene, you should choose between [Truck Shot, Long Shot] to set the context. - If you want to use Zoom Shot, you must ensure the preceding shot is Long Shot. - If you want to use Pan Shot during dialogue, it should be used multiple times in row. ### [Shot Annotation Steps]: - Step 1: Focus on the usage conditions of each shot in the [Complete List of Shots], then allocate them to the shot annotation points in the script according to different usage conditions. - Step 2: Check if the shot annotations in Step 1 meet the [Shot Annotation Requirements]. If not, identify which requirements are not met, then update the shot annotations to meet these requirements. Please support your decision with detailed logical reasoning. The \"reasoning\" field should include the complete two shot annotation steps mentioned above. Your response should only contain the following JSON content: { \"scene 1\": { \"selected-shot-1\": {\"reasoning\": \"...\", \"shot\": \"Name of shot\"}, \"selected-shot-2\": {\"reasoning\": \"...\", \"shot\": \"Name of shot\"}, ... }, ... } Figure 22: Prompt for the cinematography stage: Cinematographer annotates camera setups for each line."
        },
        {
            "title": "Work in Progress",
            "content": "You are cinematographer. Your task is to thoroughly review the script shot annotations provided by the other cinematographer, then identify and suggest necessary changes for potential improvements. ### [Provided Materials]: 1. Script Content: {final_script_with_own_annotation} 2. Another Cinematographers Shot Annotations: {peer_annotation} 3. Complete List of Shots: {shot_list} (same as prompt in Figure 22) ### [Annotation Requirements]: - If there are too many close shots or mid shots in one scene, suggest appropriate dynamic shots to replace some, but not all, based on the usage conditions. - If the first shot of dialogue-starting scene is neither Track Shot nor Long Shot, suggest shot in [Track Shot, Long Shot] to replace it. - If Zoom Shot is used in scene, check whether the preceding shot is Long Shot. If it is not, replace the preceding shot with Long Shot and this Zoom Shot remains unchanged. - If Track Shot is not used as the first shot of scene, suggest an appropriate shot to replace it. - If Pan Shot appears only once in scene, replace the preceding shot and the next shot with Pan Shot. - If Follow Shot is used on character who is not moving, suggest an appropriate shot to replace it. - If Curve Surround Shot is not used for the characters first appearance in the script, suggest another appropriate dynamic shot to replace it. - If 360 Degrees Shot is neither used for the characters first appearance in the script nor to create tense atmosphere, suggest another appropriate dynamic shot to replace it. ### [Review Steps]: - Step 1. Carefully review each shot annotation according to the [Annotation Requirements]. Check whether the shot annotation violates any requirements, and if it does, specify which guideline it violates. - Step 2. If step 1 finds that shot annotation violates the requirements, then set the \"need update\" field to \"True\"; otherwise, set it to \"False\". If \"need update\" field is \"True\", suggest better updated shot to replace the - Step 3. original shot according to the [Annotation Requirements]; otherwise, just set the \"updated shot\" field to \"None\". Please support your decision and suggestion with detailed logical reasoning. \"reasoning\" field should include the complete three review steps mentioned above. The Your response should only contain the following JSON content: { \"scene 1\": { \"selected-shot-1\": {\"shot\": \"...\", \"reasoning\": \"...\", \"need update\": \"...\", \"updated shot\": \"...\"}, \"selected-shot-2\": {\"shot\": \"...\", \"reasoning\": \"...\", \"need update\": \"...\", \"updated shot\": \"...\"}, ... }, ... } Figure 23: Prompt for the cinematography stage: Cinematographers debate over camera choices."
        },
        {
            "title": "Work in Progress",
            "content": "You are director. Two cinematographers have discussed about their shot annotations, now you need to settle any discrepancies and choose the better one. ### Script: {final_script} ### Cinematographer #1 Annotations: {peer_annotation_1} ### Cinematographer #2 Annotations: {peer_annotation_2} ### Complete List of Shots: {shot_list} (same as prompt in Figure 22) ### Key Requirements: {shot_annotation_requirements} (same as prompt in Figure 22) ### Output Format: - If you think cinematographer #1 annotations are better, return: {\"reason\": \"...\", \"better\": \"1\"} - If you think cinematographer #2 annotations are better, return: {\"reason\": \"...\", \"better\": \"2\"} Figure 24: Prompt for the cinematography stage: The director judges the final camera setup."
        }
    ],
    "affiliations": [
        "Harbin Institute of Technology (Shenzhen)",
        "Tsinghua University"
    ]
}