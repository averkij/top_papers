{
    "paper_title": "The Curious Case of Analogies: Investigating Analogical Reasoning in Large Language Models",
    "authors": [
        "Taewhoo Lee",
        "Minju Song",
        "Chanwoong Yoon",
        "Jungwoo Park",
        "Jaewoo Kang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Analogical reasoning is at the core of human cognition, serving as an important foundation for a variety of intellectual activities. While prior work has shown that LLMs can represent task patterns and surface-level concepts, it remains unclear whether these models can encode high-level relational concepts and apply them to novel situations through structured comparisons. In this work, we explore this fundamental aspect using proportional and story analogies, and identify three key findings. First, LLMs effectively encode the underlying relationships between analogous entities; both attributive and relational information propagate through mid-upper layers in correct cases, whereas reasoning failures reflect missing relational information within these layers. Second, unlike humans, LLMs often struggle not only when relational information is missing, but also when attempting to apply it to new entities. In such cases, strategically patching hidden representations at critical token positions can facilitate information transfer to a certain extent. Lastly, successful analogical reasoning in LLMs is marked by strong structural alignment between analogous situations, whereas failures often reflect degraded or misplaced alignment. Overall, our findings reveal that LLMs exhibit emerging but limited capabilities in encoding and applying high-level relational concepts, highlighting both parallels and gaps with human cognition."
        },
        {
            "title": "Start",
            "content": "The Curious Case of Analogies: Investigating Analogical Reasoning in Large Language Models Taewhoo Lee1,2, Minju Song1, Chanwoong Yoon1, Jungwoo Park1,2, Jaewoo Kang1,2* 1Korea University 2AIGEN Sciences {taewhoo, minjusong, cwyoon99, jungwoo-park, kangj}@korea.ac.kr 5 2 0 2 5 2 ] . [ 1 4 4 3 0 2 . 1 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Analogical reasoning is at the core of human cognition, serving as an important foundation for variety of intellectual activities. While prior work has shown that LLMs can represent task patterns and surface-level concepts, it remains unclear whether these models can encode high-level relational concepts and apply them to novel situations through structured comparisons. In this work, we explore this fundamental aspect using proportional and story analogies, and identify three key findings. First, LLMs effectively encode the underlying relationships between analogous entities; both attributive and relational information propagate through mid-upper layers in correct cases, whereas reasoning failures reflect missing relational information within these layers. Second, unlike humans, LLMs often struggle not only when relational information is missing, but also when attempting to apply it to new entities. In such cases, strategically patching hidden representations at critical token positions can facilitate information transfer to certain extent. Lastly, successful analogical reasoning in LLMs is marked by strong structural alignment between analogous situations, whereas failures often reflect degraded or misplaced alignment. Overall, our findings reveal that LLMs exhibit emerging but limited capabilities in encoding and applying high-level relational concepts, highlighting both parallels and gaps with human cognition. Code https://github.com/dmis-lab/analogical-reasoning"
        },
        {
            "title": "Introduction",
            "content": "Analogical reasoning is fundamental aspect of human cognition, enabling humans to navigate unfamiliar situations by drawing parallels to familiar concepts (Hofstadter 2001; Holyoak, Gentner, and Kokinov 2001; Hofstadter and Sander 2013). This ability serves as the foundation for wide range of cognitive functions, including knowledge adaptation (Keane 1996), problem solving, and creative thinking (Gentner and Markman 1996). Among various types of analogies, proportional analogies are widely used to assess ones ability to extract semantic relationships and apply them to new contexts (Brown 1989). For example, given the query Persuasion is to Jane Austen as 1984 is to, one would first focus on the first pair of entities (Persuasion, *Corresponding author. Copyright 2026, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Figure 1: An overview of the mechanism behind analogical reasoning in LLMs. (A) LLMs effectively encode relational information and apply it during correct analogical reasoning, but applying the relation often remains as much bottleneck as encoding it. (B) Identifying analogous situations is strongly associated with structural alignment, which we quantify using the Mutual Alignment Score (MAS). Jane Austen) to identify the semantic relationship (author of ), and apply it to the third entity (1984) to obtain the correct answer (George Orwell). Extending this rudimentary setting, the ability to draw parallels between situations can be evaluated using story analogies. For instance, despite different surface details between missing train but encountering dear friend and getting injured but coming back stronger, we find corresponding elements binded under the same theme: that every cloud has silver lining. Meanwhile, the advent of large language models (LLMs) and their remarkable performance on various tasks have spurred interest in the research community. Trained on massive text corpora with billions of parameters, modern LLMs have shifted the paradigm of problem-solving from taskspecific fine-tuning to leveraging instructions and examples in the input prompt (Brown et al. 2020). This emergent ability has motivated researchers to explore LLMs for complex reasoning tasks in diverse domains (Kojima et al. 2022; Yao et al. 2023; Imani, Du, and Shrivastava 2023; Jin et al. 2023). Recently, there has been growing interest in the analogical reasoning capabilities of LLMs, focusing on evaluating (Wijesiriwardene et al. 2023; Webb, Holyoak, and Lu 2023) or advancing (Wijesiriwardene et al. 2024) these capabilities. However, the inner mechanisms behind LLMs and their ability to perform analogical reasoning remains unexplored. How do models extract relationships and apply them to predict the correct answer? Moreover, how do they draw parallels between semantically disparate, yet analogous context? In this work, we take closer look at how modern LLMs perform analogical reasoning. We first examine how information that bridges different entities is extracted and applied using proportional analogies. To understand this information flow, we analyze where critical signals are processed within the input. By blocking the final token from attending to different token positions, we find that mid-upper layers within the second and third entities (e.g., Jane Austen and 1984 in Figure 1) carry essential information; disrupting these positions leads to noticeable drops in performance. Further analysis shows that these positions encode both attributive and relational information, with relational content showing significant gap between correct and incorrect cases. This suggests that, much like humans, models can not only represent individual entities but also abstract the underlying relation that connects them, highlighting relational reasoning as central mechanism in analogical understanding. Extending this analysis, we find that applying relational information poses an additional challenge beyond merely identifying it. Replacing the initial entity pairs in incorrect cases with those from correct ones changes model behavior in up to 38.4% of cases, suggesting that models still struggle to transfer relational structure in the remaining cases. Building on earlier insights about the role of linking positions (e.g., as), we conduct patching interventions to facilitate information flow between entity pairs. These adjustments lead to successful answer revisions in up to 38.1% of the remaining cases, highlighting that failures in analogical reasoning stem not only from representational gaps but also from limitations in relational application. To deepen our understanding of how models perform analogical reasoning, we turn to the question of structural alignment, i.e., how models identify and map high-level relational parallels between seemingly unrelated concepts. Using story analogies, we reveal that analogical structure becomes increasingly linearly separable in the middle layers, and that successful reasoning is associated with stronger token-level alignment between source and target stories, despite minimal lexical overlap. These findings suggest that, beyond encoding entity-level information, LLMs develop abstract relational representations and perform alignment operations that mirror core aspects of human analogical reasoning. In summary, our contributions include: We investigate the internal mechanisms of LLMs in analogical reasoning, focusing on how models succeed (or fail) to extract and apply relational information. We analyze how structural alignment emerges in model representations, associating it with deeper token-level alignment between analogous situations. We contextualize model behavior by comparing it with human cognition, highlighting both parallels in relational abstraction and limitations in alignment and application."
        },
        {
            "title": "2.1 Analogical Reasoning\nAnalogical reasoning is a cognitive process that requires\nidentifying relational similarities to understand new situa-\ntions, form abstract concepts, and draw on past experiences\nto tackle novel problems (Boteanu and Chernova 2015).\nAnalogies can take several forms, including word analo-\ngies (Gladkova, Drozd, and Matsuoka 2016; Yuan et al.\n2024), proportional analogies (Mikolov, Yih, and Zweig\n2013), story analogies (Jiayang et al. 2023), and long-text\nanalogies (Sultan and Shahaf 2022). In this work, we focus\non two types that best represent the cognitive requirements\nof analogical reasoning: proportional analogies, which re-\nquire extracting and applying semantic relationships in the\nform “A is to B as C is to D”; and story analogies, which\ndemand structural alignment between semantically distinct\nnarratives or situations.",
            "content": "In the field of natural language processing (NLP), analogical reasoning has been explored through both benchmark construction (Ye et al. 2024; Jiayang et al. 2023) and behavioral evaluation (Webb, Holyoak, and Lu 2023). Others propose prompting strategies to leverage analogical capabilities more effectively, such as self-generated exemplars (Yasunaga et al. 2024) or knowledge-enhanced prompts (Wijesiriwardene et al. 2024). In related line of work, several studies have examined how LLMs encode abstract task-level information when presented with in-context examples (Hendel, Geva, and Globerson 2023; Todd et al. 2024; Opiełka, Rosenbusch, and Stevenson 2025). These works identify task or function vectors, i.e., compact representations that reflect the operation demonstrated in ICL settings. While these studies provide evidence that models can internally represent conceptual relations, they are primarily limited to simple tasks (e.g., color matching, antonyms) and focus on detecting the presence of these vectors rather than analyzing what they represent and how they are used in more complex reasoning scenarios. In contrast, our work directly targets analogical reasoning behavior, offering comprehensive view on how models extract, apply, and structurally align relational information."
        },
        {
            "title": "2.2 Mechanistic Interpretability\nUnderstanding the internal mechanisms of LLMs has been a\ncentral focus of recent research (Bereska and Gavves 2024).\nAmong the various techniques developed to analyze inter-\nmediate activations and their causal roles in model behavior,\ntwo broad categories are particularly relevant to our work:\nrepresentational analysis (nostalgebraist 2020; Belrose et al.\n2023; Pal et al. 2023), which investigates what types of in-\nformation are encoded in hidden states; and intervention-\nbased methods (Vig et al. 2020; Zhang and Nanda 2024;\nPochinkov et al. 2024), which manipulate internal activa-\ntions to examine their functional impact on model outputs.",
            "content": "Our study builds on both paradigms to probe the internal computations that support analogical reasoning. Below, we introduce key methods employed in our experiments: (1) Attention Knockout (Wang et al. 2023b; Geva et al. 2023): This method involves selectively disabling attention heads to examine their contribution to predicting outputs. By removing specific attention pathways, we can assess whether specific tokens are responsible for prediction correct outputs and identify which components are crucial for resolving relational information. (2) Linear Probing (Alain and Bengio 2018; Belinkov 2022): This technique assesses whether specific types of information are linearly separable within models hidden representations. Given labeled examples, we extract activation vectors from particular layer and train linear classifier to predict the labels. High probe accuracy suggests that the relevant information is explicitly encoded in the representation space at that layer. (3) Patchscopes (Ghandeharioun et al. 2024): leverages recent extension of activation patching that to interpret what the models generative capabilities information is encoded in its hidden representations. Specifically, source prompt is first passed through the model, and the hidden representation of the token we wish to inspect is recorded. Next, the same model processes target prompt, which is used to induce natural language descriptions regarding the representation. For example, when using the target prompt constructed by Ghandeharioun et al. (2024): Syria: Country in the Middle East, Leonardo DiCaprio: American actor, Samsung: South Korean multinational major appliance and consumer electronics corporation, x, the representation in is replaced with the previously recorded representation, resulting in the description of that representation. Throughout this work, we systematically construct diverse target prompts suitable for extracting different types of information."
        },
        {
            "title": "3.1 Dataset Construction\nFor proportional analogies, we manually construct a test\nset that contains both correct and incorrect analogies for\neach model. We begin by retrieving entity pairs from Anal-\nogyKB (Yuan et al. 2024), a million-scale analogy knowl-\nedge base that contains entity pairs of the same relation 1.",
            "content": "1We use the Wikidata subset. Next, to ensure clear distinction between correct and incorrect cases for evaluation, we manually filter out relations that can lead to multiple answers (e.g., interested in) or change over time (e.g., head of state). Finally, we iteratively combine different entity pairs (e1-e2, e3-e4) that share the same relation, generating total of 50k analogies to be used for evaluation. In the evaluation phase, we set up series of additional filters to confine our experiments to analogical reasoning. First, we ensure that each model is equipped with the necessary knowledge. Formally, for each (ei, ej) pair, we check whether models can predict ej given ei and the relation. As an illustrative example, for the analogy Persuasion is to Jane Austen as 1984 is to George Orwell, we construct two queries with the relation as follows: The author of Persuasion is and The author of 1984 is. If model fails to answer both queries correctly, we exclude the analogy, as we cannot determine whether the incorrect predictions stem from incorrect analogical reasoning or from lack of prior knowledge. Second, we prevent models from relying on reasoning shortcuts (Xu et al. 2022; Wang et al. 2023a). We define reasoning shortcuts as instances where models return the correct answer without e2 or e1 is to e2. For example, we construct two queries as follows: Persuasion is to as 1984 is to and 1984 is to. If the model correctly predicts George Orwell in these cases, this suggests that the answer entity is strongly correlated with e3, bypassing the need to perform analogical reasoning. In such cases, we exclude the analogy to ensure that models are genuinely engaging in relational reasoning rather than leveraging direct associations. We sample 500 analogies each from the remaining collection of correct and incorrect cases for our experiments. For story analogies, we use the StoryAnalogy (Jiayang et al. 2023) dataset, which contains 360 multiple-choice questions. Each question involves selecting the target story that is analogous to given source story. The incorrect options originally consist of two randomly selected stories and one distractor story with high nounal similarity to the source story. To focus our analysis on structural alignment in the presence of surface-level distractors, we discard the random options and adopt two-option format. To minimize positional bias, we present each question twice with reversed indices and consider response correct only if the model selects the target story in both trials. We report detailed statistics for both datasets in the Appendix."
        },
        {
            "title": "3.2 Models",
            "content": "For proportional analogies, presented as simple nexttoken prediction task, we investigate the following opensource models: Llama-2-13b (Touvron et al. 2023), Gemma7B (Team et al. 2024a), and Qwen2.5-14B (Yang et al. 2024). For story analogies, we use instruction-tuned models that demonstrate sufficient performance for analysis: Llama-2-13b-chat, Gemma-2-9B-it (Team et al. 2024b), and Qwen2.5-14B-Instruct. We mainly report results for Qwen2.5-14B models, as they exhibit representative behavior, and provide results for other models in the Appendix. (a) Correct case (b) Incorrect case Figure 2: Results of applying attention knockout to different positions on Qwen2.5-14B. Mid-upper layers of e2 and e3 are critical for answer resolution in both correct and incorrect cases. In incorrect cases, information from the link strongly influences model output, suggesting that the link may contribute to reasoning failures."
        },
        {
            "title": "4.1 Methods\nWe first apply attention knockout to identify positions that\nare critical for resolving the answer. We focus on four posi-\ntions that precede the resolution token: e1, e2, link, and e3.\nFor correct cases, we report the accuracy of the generated re-\nsponse. For incorrect cases, we check whether the knockout\nresults in a change in the generated text to assess the impact\nof blocked layers (Biran et al. 2024). We keep a window of\nk layers around each layer to account for information that\npropagates across multiple layers (Geva et al. 2023), where\nk is set to one-fifth of the total number of layers.",
            "content": "Next, to analyze what information is encoded in the hidden representations of these positions, we first categorize information into two types: attributive and relational information. Attributive information reflects how well the representation captures the inherent attributes of an entity, while relational information indicates whether the representation encodes the relation. To analyze attributive information within each entity, we employ Patchscopes and use the same target prompt used in Section 2.2 to obtain descriptions of hidden representations in natural language. Next, to check whether each description involves the correct attributes, we take inspiration from Geva et al. (2023) and construct set of tokens highly related to the entity of interest. Specifically, for each entity, we retrieve 100 paragraphs from Wikipedia 2 using BM25 (Robertson et al. 1994), and extract related entities using en core web trf (Honnibal and Montani 2017). We consider hidden representation to have encoded attributive information if the corresponding description contains one or more entities related to the entity of interest. For relational information, our goal is to inspect whether specific entity encodes the correct relation. To achieve this, we design three target prompts for each entity, encouraging models to explicitly output the encoded relation while considering their respective positions. For e2, we use the following prompt: Japan is to Tokyo: capital of, Theory of Evolution is to Charles Darwin: founder of, Peace is to olive branch: symbol of, {} is to x, where curly brackets are replaced with e1. Similarly, for e3, we use the same exemplars but replace the final phrase with is to {}, where curly brackets are replaced with e4. Finally, for the resolution token (to), we use {} is for the final phrase. Note that we use two prompts, where curly brackets are replaced with either e3 or e4. We consider hidden representation to have encoded relational information if the corresponding description contains the correct relation. We provide example descriptions generated from each custom prompt in the Appendix."
        },
        {
            "title": "4.2 Results\nFigure 2 shows the results of applying attention knockout\nto different positions preceding the resolution token, from\nwhich we identify three notable patterns. First, for both cor-\nrect and incorrect cases, blocking attention edges from the\nresolution token to e1 has little impact on model perfor-\nmance or generation. This indicates that e1 plays a limited\nrole in retaining information that is essential within the first\npair. Second, blocking attention edges to either e2 or e3 re-",
            "content": "2We use the dump from December 20, 2024. (a) Relational information across layers (b) Attributive information across layers Figure 3: Proportion of cases where relational or attributive information is successfully decoded using Patchscopes. Attributive information persists across mid-upper layers regardless of correctness, while relational information shows sharp decline in incorrect cases. This underscores the critical role of relational information in accurate answer resolution. sults in noticeable performance drops or fluctuations in generation, mainly around the mid-upper layers. This suggests that information propagating directly from e2 and e3 has strong influence on model behavior, with information from e2 propagating in slightly earlier layers than that from e3. Third, information propagating from the link heavily affects model generations in incorrect cases, particularly in the early to middle layers. This either indicates an incorrect encoding of information passed to the link, or failure of the link to transfer information to the target element. Based on this observation, we conduct further experiments to better understand incorrect cases in Section 5. Figure 3 displays the proportion of cases where relational and attributive information is successfully decoded from each source layer. We see that attributive information is consistently encoded within e2 and e3, persisting until the midupper layers before declining sharply in the upper layers. Given that we ensure models are equipped with the necessary knowledge (Section 3.1), we confirm that attributive information remains intact for e2 and e3 in both correct and incorrect cases. However, significant gap is observed between these cases in terms of relational information. This suggests that relational information encoded in e2 and e3 serves as key factor in answer resolution. Moreover, while both types of information follow similar trend for e2, relational information in e3 remains consistent up to the upper layers, implying its role in answer resolution at these layers."
        },
        {
            "title": "5 Application as a Hurdle",
            "content": "For humans, the primary difficulty in solving analogies lies in extracting the underlying relation; once retrieved or cued, mapping it onto new context is relatively straightforward (Kubricht, Lu, and Holyoak 2017). In the previous section, we have identified two potential explanations for model failures: incorrect encoding of information passed to the link, or ineffective transfer of information through the"
        },
        {
            "title": "Model",
            "content": "Exp 1 Exp"
        },
        {
            "title": "Overall",
            "content": "Llama-2-13B +32.3% +25.9% +49.8% Gemma-7B +38.4% +38.1% +61.9% Qwen2.5-14B +35.6% +30.5% +55.3% Table 1: Results from error analysis experiments. Exp 1 indicates setting where we evaluate models using correct first pairs. Exp 2 indicates setting where we patch representations for the remaining incorrect cases. link itself. In this section, we aim to deepen our understanding of how models fail at analogical reasoning, focusing on the observed influence of the link in incorrect generations and the pivotal role of e2, which encodes both attributive and relational information. We begin by re-evaluating model performance when provided with the correct first pair. For cases where the model still fails, we then intervene by patching the representations of e2 into the linking position to better facilitate the propagation of critical relational information."
        },
        {
            "title": "5.1 Methods",
            "content": "For the first experiment, we replace the first pairs of incorrect cases with those from correct cases. To ensure sufficient number of samples for replacement, we select three representative relations from our test set: official language of , author of , and composer of . For each incorrect input analogy, we randomly choose correct analogy from the same relation and swap their first pairs. We evaluate models using this newly constructed test set. For the second experiment, we patch the hidden representations of each layer in e2 to each layer in the link to see if models can benefit from directly injecting critical information encoded in e2. We report the performance improvement from the combination of layers that yields the highest gain."
        },
        {
            "title": "5.2 Results\nTable 1 shows the performance gains observed from each\nexperiment. We find that model responses can be rectified\nby replacing the first pair in up to 38.4% of incorrect cases.\nThis indicates that a non-negligible portion of model errors\nstem from insufficient extraction of information within the\nfirst pair. This also highlights the importance of information\nencoded in e2, as we have previously confirmed that the res-\nolution token strongly attends to e2 for answer resolution.",
            "content": "Interestingly, for cases where replacing the first pairs did not result in correct answers, we observe that patching the representations of e2 to the link leads to noticeable performance gains up to 38.1%. This indicates that even if the model correctly extracts the necessary information from the first pair, the extent to which the link effectively conveys that information to subsequent positions can significantly impact model generation. Moreover, we inspect the generation results across different layers for both e2 and the link. For e2, we find that patching representations up to the middle layers is mainly effective in rectifying model responses. Given that both relational and attributive information is strongly formed up to the mid-upper layers of e2 in correct cases, we see that injecting information encoded from these regions into the link assists in propagating these information to subsequent positions. For the link, where patching is performed, applying the patched representation to the early layers proves to be effective, suggesting that the representation need to pass through certain number of layers to be properly contextualized with relational information."
        },
        {
            "title": "6.1 Methods\nFor the first experiment, we extract the source, target, and\ndistractor stories from each sample in the StoryAnalogy\ndataset. We construct a probing dataset by pairing each\nsource story with both the target and distractor stories. For\neach input pair, we extract the activation at the final token\nfrom every attention head in each layer, yielding a prob-\ning dataset {(x(h,ℓ)\ndenotes the acti-\nvation from head h in layer ℓ for the i-th input pair. We train\na binary linear classifier on these representations to assess",
            "content": "i=1, where x(h,ℓ) , yi)}N Algorithm 1: Mutual Alignment Score Input: Source token representations = {s1, . . . , sm}, Candidate token representations = {c1, . . . , cn} Output: Mutual alignment score 1: Normalize each vector in and to unit norm 2: Compute similarity matrix Mij = cos(si, cj) for all [1, m], [1, n] // Best-matching in for si // Best-matching in for cj arg maxj Mij arg maxi Mij if = then 3: Initialize counter mutual matches 0 4: for = 1 to do 5: 6: 7: 8: end if 9: 10: end for 11: mutual matches / min(m, n) 12: return mutual matches mutual matches + 1 whether analogical structure is linearly separable from lexical similarity in the models internal representations. To ensure robust performance estimates and mitigate overfitting, we apply 5-fold cross-validation, reporting the average validation accuracy across folds as the final probe accuracy. For the second experiment, we assess whether structural alignment is reflected in the models internal geometry during analogical reasoning, and how it diverges between correct and incorrect cases in the presence of distractor stories. To this end, we define the Mutual Alignment Score (MAS) as the proportion of mutual best matches between contextualized token representations from the source and candidate spans (Algorithm 1), computed at each layer for both the target and distractor stories. token pair (si, cj) forms mutual best match if each is the others most similar token based on cosine similarity between their layer-specific representations. By computing MAS across layers, we trace the emergence of structural alignment and examine its relationship with successful analogical reasoning."
        },
        {
            "title": "6.2 Results",
            "content": "Figure 4 presents linear probe accuracies for distinguishing analogical from lexically similar stories across all layers of the model. The heatmap reveals clear progression in representational quality across depth. Early to middle layers begin to show accuracies above chance, suggesting an initial emergence of analogical structure at relatively shallow depths. marked increase in probe accuracy extends through the middle layers, with layers 20 through 30 showing an average accuracy of 82.9%. This pattern indicates that analogical distinctions are not immediately encoded at the input level but instead develop gradually across layers, reaching maximal discriminability in the middle layers of the model. These findings imply that models develop an internal representation of analogical structure that becomes linearly separable from lexical similarity as processing deepens. Figure 5 shows the relative MAS between the source and target stories versus the source and distractor stories, measured as the difference in MAS across layers. For correct Figure 4: Linear probe accuracy across layers. High accuracy in the middle layers indicates the internal representation of analogical structure in these regions. Figure 5: Relative Mutual Alignment Score (MAS) across layers, computed as the difference between the MAS of source-target pairs and source-distractor pairs. cases, the MAS between source and target stories consistently exceeds that between source and distractor stories, suggesting that models encode deeper structural alignment beyond surface-level lexical cues. This is especially notable given that target stories are designed to have minimal entity overlap with the source, indicating that models are capturing underlying relational structure, similar to how humans seek one-to-one alignments that maximize relational similarity during analogical reasoning. The relative gap peaks in the middle layers, suggesting that structural alignment between correct analogical pairs is strongest at these depths. This aligns with our probing results, which show that analogical distinctions become most linearly separable in these layers. In contrast, for incorrect cases, the model constructs stronger alignment between Figure 6: Sample heatmap of average similarity scores across layers between source and target stories. Black boxes indicate mutual best matches. Analogous token pairs (e.g., Water-air, house-lungs) form mutual best matches with high similarity scores, despite surface-level disparities. source and distractor stories across most layers, with only slight preference for sourcetarget alignment in the middle layers. The overall gap is much less pronounced than in correct cases, indicating that the model fails to reliably identify the intended analogical structure. Overall, these results indicate that successful analogical reasoning in the model is strongly associated with higher token-level structural alignment between source and target stories. In contrast, incorrect cases exhibit much smaller alignment gap, with distractors often receiving greater alignment, suggesting that the model fails to clearly differentiate the intended relational structure. This highlights structural alignment as key internal signal for analogical success and reveals the models vulnerability to surface-level interference when the relational mapping is not robustly encoded."
        },
        {
            "title": "7 Conclusion\nIn this work, we study the internal mechanisms of LLMs in\nanalogical reasoning. Using proportional analogies, we find\nthat correct reasoning is associated with the encoding of ab-\nstract relational information in the mid-upper layers. While\nmodels are capable of abstracting these relations, we find\nthat applying them remains a major bottleneck. By patch-\ning the representation of the second entity into the link, we\nuncover the link’s role in transferring relational informa-\ntion to downstream positions, and show that failure at this\nstage leads to incorrect generations. Finally, our analysis of\nstory analogies shows that successful reasoning aligns with\nstrong structural mapping between source and target stories,\nwhile failures often reflect weak or distractor-biased align-\nment. Overall, our work paves the way for future research\ninto understanding and improving the analogical reasoning\ncapabilities of LLMs.",
            "content": "Acknowledgments We thank Minbyul Jeong, Hyeon Hwang, and Yein Park for their invaluable feedback on this work. This research was supported by the National Research Foundation of Korea (NRF-2023R1A2C3004176), the Ministry of Health & Welfare, Republic of Korea (HR20C002103), the Ministry of Science and ICT (MSIT) (RS-2023-00262002), the ICT Creative Consilience program through the Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the MSIT (IITP-2025-RS-2020II201819), and the Culture, Sports and Tourism R&D Program through the Korea Creative Content Agency (KOCCA) grant funded by the Ministry of Culture, Sports and Tourism (MCST) in 2023 (Project Name: Development of storytelling AI technology for cultural heritage tailored to the various interests of users, Project Number: RS-2023-00220195, Contribution Rate: 100%). References Alain, G.; and Bengio, Y. 2018. Understanding intermediate layers using linear classifier probes. arXiv:1610.01644. Belinkov, Y. 2022. Probing Classifiers: Promises, Shortcomings, and Advances. Computational Linguistics, 48(1): 207 219. Belrose, N.; Furman, Z.; Smith, L.; Halawi, D.; Ostrovsky, I.; McKinney, L.; Biderman, S.; and Steinhardt, J. 2023. Eliciting latent predictions from transformers with the tuned lens. arXiv preprint arXiv:2303.08112. Bereska, L.; and Gavves, E. 2024. Mechanistic InarXiv preprint terpretability for AI SafetyA Review. arXiv:2404.14082. Biran, E.; Gottesman, D.; Yang, S.; Geva, M.; and Globerson, A. 2024. Hopping Too Late: Exploring the Limitations of Large Language Models on Multi-Hop Queries. In AlOnaizan, Y.; Bansal, M.; and Chen, Y.-N., eds., Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, 1411314130. Miami, Florida, USA: Association for Computational Linguistics. Boteanu, A.; and Chernova, S. 2015. Solving and explaining analogy questions using semantic networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 29. Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877 1901. Brown, W. R. 1989. Two traditions of analogy. Logic, 11(3). French, R. M. 2002. The computational modeling of analogy-making. Trends in cognitive Sciences, 6(5): 200 205. Gentner, D.; and Forbus, K. D. 2011. Computational models of analogy. Wiley interdisciplinary reviews: cognitive science, 2(3): 266276."
        },
        {
            "title": "Informal",
            "content": "Gentner, D.; and Markman, A. B. 1996. Keith J. Holyoak and Paul Thagard, Mental Leaps: Analogy in Creative Thought. Pragmatics &amp; Cognition, 4(2): 407409. Geva, M.; Bastings, J.; Filippova, K.; and Globerson, A. 2023. Dissecting Recall of Factual Associations in AutoRegressive Language Models. In Bouamor, H.; Pino, J.; and Bali, K., eds., Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 12216 12235. Singapore: Association for Computational Linguistics. Ghandeharioun, A.; Caciularu, A.; Pearce, A.; Dixon, L.; and Geva, M. 2024. Patchscopes: unifying framework for inspecting hidden representations of language models. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org. Gladkova, A.; Drozd, A.; and Matsuoka, S. 2016. Analogybased detection of morphological and semantic relations with word embeddings: what works and what doesnt. In Andreas, J.; Choi, E.; and Lazaridou, A., eds., Proceedings of the NAACL Student Research Workshop, 815. San Diego, California: Association for Computational Linguistics. Hendel, R.; Geva, M.; and Globerson, A. 2023. In-Context Learning Creates Task Vectors. In Bouamor, H.; Pino, J.; and Bali, K., eds., Findings of the Association for Computational Linguistics: EMNLP 2023, 93189333. Singapore: Association for Computational Linguistics. Hofstadter, D. R. 2001. Epilogue: Analogy as the core of cognition. Surfaces and Hofstadter, D. R.; and Sander, E. 2013. essences: Analogy as the fuel and fire of thinking. Basic books. Holyoak, K.; Gentner, D.; and Kokinov, B. 2001. The place of analogy in cognition. The analogical mind: Perspectives from cognitive science, 119. Honnibal, M.; and Montani, I. 2017. spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing. To appear. Imani, S.; Du, L.; and Shrivastava, H. 2023. Mathprompter: Mathematical reasoning using large language models. arXiv preprint arXiv:2303.05398. Jiayang, C.; Qiu, L.; Chan, T.; Fang, T.; Wang, W.; Chan, C.; Ru, D.; Guo, Q.; Zhang, H.; Song, Y.; Zhang, Y.; and Zhang, Z. 2023. StoryAnalogy: Deriving Story-level Analogies from Large Language Models to Unlock Analogical Understanding. In Bouamor, H.; Pino, J.; and Bali, K., eds., Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 1151811537. Singapore: Association for Computational Linguistics. Jin, Z.; Chen, Y.; Leeb, F.; Gresele, L.; Kamal, O.; LYU, Z.; Blin, K.; Gonzalez Adauto, F.; Kleiman-Weiner, M.; Sachan, M.; and Scholkopf, B. 2023. CLadder: Assessing Causal Reasoning in Language Models. In Oh, A.; Naumann, T.; Globerson, A.; Saenko, K.; Hardt, M.; and Levine, S., eds., Advances in Neural Information Processing Systems, volume 36, 3103831065. Curran Associates, Inc. Interpreting GPT: The Logit Lens. Keane, M. T. 1996. On Adaptation in Analogy: Tests of Pragmatic Importance and Adaptability in Analogical Problem Solving. The Quarterly Journal of Experimental Psychology Section A, 49(4): 10621085. Kojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y.; and Iwasawa, Y. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35: 2219922213. Kubricht, J. R.; Lu, H.; and Holyoak, K. J. 2017. Individual differences in spontaneous analogical transfer. Memory & Cognition, 45(4): 576588. Markman, A.; and Gentner, D. 1993. Structural Alignment during Similarity Comparisons. Cognitive Psychology, 25(4): 431467. Mikolov, T.; Yih, W.-t.; and Zweig, G. 2013. Linguistic Regularities in Continuous Space Word Representations. In Vanderwende, L.; Daume III, H.; and Kirchhoff, K., eds., Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 746751. Atlanta, Georgia: Association for Computational Linguistics. nostalgebraist. 2020. LessWrong. Opiełka, G.; Rosenbusch, H.; and Stevenson, C. E. 2025. Analogical reasoning inside large language models: Concept vectors and the limits of abstraction. arXiv preprint arXiv:2503.03666. Pal, K.; Sun, J.; Yuan, A.; Wallace, B.; and Bau, D. 2023. Future Lens: Anticipating Subsequent Tokens from Single Hidden State. In Jiang, J.; Reitter, D.; and Deng, S., eds., Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL). Singapore: Association for Computational Linguistics. Pochinkov, N.; Benoit, A.; Agarwal, L.; Majid, Z. A.; and Ter-Minassian, L. 2024. Extracting Paragraphs from LLM Token Activations. In MINT: Foundation Model Interventions. Robertson, S.; Walker, S.; Jones, S.; Hancock-Beaulieu, M.; and Gatford, M. 1994. Okapi at TREC-3. 0. Sultan, O.; and Shahaf, D. 2022. Life is Circus and We are the Clowns: Automatically Finding Analogies between Situations and Processes. In Goldberg, Y.; Kozareva, Z.; and Zhang, Y., eds., Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, 3547 3562. Abu Dhabi, United Arab Emirates: Association for Computational Linguistics. Team, G.; Mesnard, T.; Hardin, C.; Dadashi, R.; Bhupatiraju, S.; Pathak, S.; Sifre, L.; Rivi`ere, M.; Kale, M. S.; Love, J.; et al. 2024a. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295. Team, G.; Riviere, M.; Pathak, S.; Sessa, P. G.; Hardin, C.; Bhupatiraju, S.; Hussenot, L.; Mesnard, T.; Shahriari, B.; Rame, A.; Ferret, J.; Liu, P.; Tafti, P.; Friesen, A.; Casbon, M.; Ramos, S.; Kumar, R.; Lan, C. L.; Jerome, S.; Tsitsulin, A.; Vieillard, N.; Stanczyk, P.; Girgin, S.; Momchev, N.; Hoffman, M.; Thakoor, S.; Grill, J.-B.; Neyshabur, B.; Bachem, O.; Walton, A.; Severyn, A.; Parrish, A.; Ahmad, A.; Hutchison, A.; Abdagic, A.; Carl, A.; Shen, A.; Brock, A.; Coenen, A.; Laforge, A.; Paterson, A.; Bastian, B.; Piot, B.; Wu, B.; Royal, B.; Chen, C.; Kumar, C.; Perry, C.; Welty, C.; Choquette-Choo, C. A.; Sinopalnikov, D.; Weinberger, D.; Vijaykumar, D.; Rogozinska, D.; Herbison, D.; Bandy, E.; Wang, E.; Noland, E.; Moreira, E.; Senter, E.; Eltyshev, E.; Visin, F.; Rasskin, G.; Wei, G.; Cameron, G.; Martins, G.; Hashemi, H.; Klimczak-Plucinska, H.; Batra, H.; Dhand, H.; Nardini, I.; Mein, J.; Zhou, J.; Svensson, J.; Stanway, J.; Chan, J.; Zhou, J. P.; Carrasqueira, J.; Iljazi, J.; Becker, J.; Fernandez, J.; van Amersfoort, J.; Gordon, J.; Lipschultz, J.; Newlan, J.; yeong Ji, J.; Mohamed, K.; Badola, K.; Black, K.; Millican, K.; McDonell, K.; Nguyen, K.; Sodhia, K.; Greene, K.; Sjoesund, L. L.; Usui, L.; Sifre, L.; Heuermann, L.; Lago, L.; McNealus, L.; Soares, L. B.; Kilpatrick, L.; Dixon, L.; Martins, L.; Reid, M.; Singh, M.; Iverson, M.; Gorner, M.; Velloso, M.; Wirth, M.; Davidow, M.; Miller, M.; Rahtz, M.; Watson, M.; Risdal, M.; Kazemi, M.; Moynihan, M.; Zhang, M.; Kahng, M.; Park, M.; Rahman, M.; Khatwani, M.; Dao, N.; Bardoliwalla, N.; Devanathan, N.; Dumai, N.; Chauhan, N.; Wahltinez, O.; Botarda, P.; Barnes, P.; Barham, P.; Michel, P.; Jin, P.; Georgiev, P.; Culliton, P.; Kuppala, P.; Comanescu, R.; Merhej, R.; Jana, R.; Rokni, R. A.; Agarwal, R.; Mullins, R.; Saadat, S.; Carthy, S. M.; Cogan, S.; Perrin, S.; Arnold, S. M. R.; Krause, S.; Dai, S.; Garg, S.; Sheth, S.; Ronstrom, S.; Chan, S.; Jordan, T.; Yu, T.; Eccles, T.; Hennigan, T.; Kocisky, T.; Doshi, T.; Jain, V.; Yadav, V.; Meshram, V.; Dharmadhikari, V.; Barkley, W.; Wei, W.; Ye, W.; Han, W.; Kwon, W.; Xu, X.; Shen, Z.; Gong, Z.; Wei, Z.; Cotruta, V.; Kirk, P.; Rao, A.; Giang, M.; Peran, L.; Warkentin, T.; Collins, E.; Barral, J.; Ghahramani, Z.; Hadsell, R.; Sculley, D.; Banks, J.; Dragan, A.; Petrov, S.; Vinyals, O.; Dean, J.; Hassabis, D.; Kavukcuoglu, K.; Farabet, C.; Buchatskaya, E.; Borgeaud, S.; Fiedel, N.; Joulin, A.; Kenealy, K.; Dadashi, R.; and Andreev, A. 2024b. Gemma 2: Improving Open Language Models at Practical Size. arXiv:2408.00118. Todd, E.; Li, M.; Sharma, A. S.; Mueller, A.; Wallace, B. C.; and Bau, D. 2024. Function Vectors in Large Language Models. In The Twelfth International Conference on Learning Representations. Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Vig, J.; Gehrmann, S.; Belinkov, Y.; Qian, S.; Nevo, D.; Singer, Y.; and Shieber, S. 2020. Investigating Gender Bias in Language Models Using Causal Mediation Analysis. In Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.; and Lin, H., eds., Advances in Neural Information Processing Systems, volume 33, 1238812401. Curran Associates, Inc. Wang, F.; Mo, W.; Wang, Y.; Zhou, W.; and Chen, M. 2023a. Causal View of Entity Bias in (Large) Language Models. In Bouamor, H.; Pino, J.; and Bali, K., eds., Findings of the Association for Computational Linguistics: EMNLP 2023, 1517315184. Singapore: Association for Computational Linguistics. Wang, K. R.; Variengien, A.; Conmy, A.; Shlegeris, B.; and Steinhardt, J. 2023b. Interpretability in the Wild: Circuit In The for Indirect Object Identification in GPT-2 Small. Eleventh International Conference on Learning Representations. Webb, T.; Holyoak, K. J.; and Lu, H. 2023. Emergent analogical reasoning in large language models. Nature Human Behaviour, 7(9): 15261541. Wijesiriwardene, T.; Wickramarachchi, R.; Gajera, B. G.; Gowaikar, S. M.; Gupta, C.; Chadha, A.; Reganti, A. N.; Sheth, A.; and Das, A. 2023. ANALOGICALA Novel Benchmark for Long Text Analogy Evaluation in Large Language Models. arXiv preprint arXiv:2305.05050. Wijesiriwardene, T.; Wickramarachchi, R.; Vennam, S.; Jain, V.; Chadha, A.; Das, A.; Kumaraguru, P.; and Sheth, A. 2024. Exploring the Abilities of Large Language Models to Solve Proportional Analogies via Knowledge-Enhanced Prompting. arXiv preprint arXiv:2412.00869. Xu, N.; Wang, F.; Li, B.; Dong, M.; and Chen, M. 2022. Does Your Model Classify Entities Reasonably? Diagnosing and Mitigating Spurious Correlations in Entity Typing. In Goldberg, Y.; Kozareva, Z.; and Zhang, Y., eds., Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, 86428658. Abu Dhabi, United Arab Emirates: Association for Computational Linguistics. Yang, A.; Yang, B.; Zhang, B.; Hui, B.; Zheng, B.; Yu, B.; Li, C.; Liu, D.; Huang, F.; Wei, H.; et al. 2024. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. Yao, S.; Zhao, J.; Yu, D.; Du, N.; Shafran, I.; Narasimhan, K.; and Cao, Y. 2023. ReAct: Synergizing Reasoning and In International Conference Acting in Language Models. on Learning Representations (ICLR). Yasunaga, M.; Chen, X.; Li, Y.; Pasupat, P.; Leskovec, J.; Liang, P.; Chi, E. H.; and Zhou, D. 2024. Large Language In The Twelfth InternaModels as Analogical Reasoners. tional Conference on Learning Representations. Ye, X.; Wang, A.; Choi, J.; Lu, Y.; Sharma, S.; Shen, L.; Tiyyala, V. M.; Andrews, N.; and Khashabi, D. 2024. AnaloBench: Benchmarking the Identification of Abstract and Long-context Analogies. In Al-Onaizan, Y.; Bansal, M.; and Chen, Y.-N., eds., Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, 1306013082. Miami, Florida, USA: Association for Computational Linguistics. Yuan, S.; Chen, J.; Sun, C.; Liang, J.; Xiao, Y.; and Yang, D. 2024. ANALOGYKB: Unlocking Analogical Reasoning of Language Models with Million-scale Knowledge Base. In Ku, L.-W.; Martins, A.; and Srikumar, V., eds., Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 12491265. Bangkok, Thailand: Association for Computational Linguistics. Zhang, F.; and Nanda, N. 2024. Towards Best Practices of Activation Patching in Language Models: Metrics and Methods. In The Twelfth International Conference on Learning Representations. Dataset Statistics In Table and B, we report detailed statistics of data samples used in proportional and story analogies per model, respectively. For proportional analogies, Total refers to the total number of samples after the knowledge filtering process. We then sample 500 instances from both correct and incorrect cases for analysis. Example Descriptions of Target Prompts In Table C, D, and E, we provide example descriptions generated using Patchscopes across different token positions and relations. Specifically, we apply different versions of custom prompts to encourage models to output relational information encoded in each entity position. For attributive information, we apply fixed default prompt on each entity position, as represented in resolution (default)."
        },
        {
            "title": "Resolution Token",
            "content": "Figure depicts the heatmap of layers in the resolution token where e4 is successfully decoded using Patchscopes for all models. We confirm that the answer is mostly resolved in the upper layers of the resolution token, and look for positions that propagate information critical for this process. Attention Knockout on Different Positions Figure shows the results of applying attention knockout to different positions for all models."
        },
        {
            "title": "Positions",
            "content": "Figure and describe the proportion of cases where relational and attributive information is successfully decoded using Patchscopes for all models. Heatmap of Intervention Experiments Figure visualizes head-wise contributions of the intervention experiment for all models. Patching representations to the early layers of the link proves to be particularly effective, suggesting that early contextualization of information from e2 is critical for correctly applying information. Heatmap of Probing Experiments Figure shows heatmaps of probing experiments for all models. High accuracy in the middle layers (early-mid layers for Llama-2-13B-chat) indicates that models are capable of representing analogical structure in these regions. Relative Mutual Alignment Score Figure shows the relative mutual alignment score (MAS) for all models, computed as the difference between the MAS of source-target pairs and source-distractor pairs."
        },
        {
            "title": "Total",
            "content": "Llama-2-13B Gemma-7B 809 756 924 Qwen2.5-14B 1478 1122 1733 1753 Table A: Statistics for proportional analogy samples."
        },
        {
            "title": "Total",
            "content": "Llama-2-13b-chat Gemma-2-9B-it Qwen2.5-14B-Instruct 150 219 210 141 163 360 360 Table B: Statistics for story analogy samples. (a) Llama-2-13B (b) Gemma-7B (c) Qwen2.5-14B Figure A: Heatmap of layers in the resolution token where e4 is successfully decoded using Patchscopes. Prompt Type Prompt Model Output Analogy: The Sign of Four is to Arthur Conan Doyle as Don Quixote is to (answer: Miguel de Cervantes) e1: The Sign of Four, e2: Arthur Conan Doyle, e3: Don Quixote, e4: Miguel de Cervantes e2 e3 resolution resolution (default) Japan is to Tokyo: capital of, Theory of Evolution is to Charles Darwin: founder of, Peace is to olive branch: symbol of, The Sign of Four is to Japan is to Tokyo: capital of, Theory of Evolution is to Charles Darwin: founder of, Peace is to olive branch: symbol of, is to Miguel de Cervantes Japan is to Tokyo: capital of, Theory of Evolution is to Charles Darwin: founder of, Peace is to olive branch: symbol of, Don Quixote is Syria: Country in the Middle East, Leonardo DiCaprio: American actor, Samsung: South Korean multinational major appliance and consumer electronics corporation, , Arthur Conan: author of, ... : author of, The Sign of Four is to ... : author of Don Quixote, and so on. ... : author of, is to Don Quixote: ... Cervantes: author of Don Quixote, ... Miguel de Cervantes: author of, ... Miguel de Cervantes: Miguel de Cervantes was Spanish writer, ... Table C: Prompts and model outputs for the relation \"author of\". Prompt Type Prompt Model Output Analogy: Avatar 3 is to James Cameron as Heaven & Earth is to (answer: Oliver Stone) e1: Avatar 3, e2: James Cameron, e3: Heaven & Earth, e4: Oliver Stone e2 e3 resolution resolution (default) Japan is to Tokyo: capital of, Theory of Evolution is to Charles Darwin: founder of, Peace is to olive branch: symbol of, Avatar 3 is to Japan is to Tokyo: capital of, Theory of Evolution is to Charles Darwin: founder of, Peace is to olive branch: symbol of, is to Oliver Stone Japan is to Tokyo: capital of, Theory of Evolution is to Charles Darwin: founder of, Peace is to olive branch: symbol of, Heaven & Earth is Syria: Country in the Middle East, Leonardo DiCaprio: American actor, Samsung: South Korean multinational major appliance and consumer electronics corporation, , James Cameron: director of, ... : director of, Avatar 3 is to ... : director of Heaven & Earth, and so on. ... : director of, is to Heaven & Earth: ... Oliver Stone: director of, Oliver Stone is to JFK: director of, ... Oliver Stone: Oliver Stone is an American filmmaker,director of many political films, ... Table D: Prompts and model outputs for the relation \"director of\". Prompt Type Prompt Model Output Analogy: True Romance is to Hans Zimmer as Fugue in minor, Little, BWV 578 is to (answer: Johann Sebastian Bach) e1: True Romance, e2: Hans Zimmer, e3: Fugue in minor, Little, BWV 578, e4: Johann Sebastian Bach e2 e3 resolution resolution (default) Japan is to Tokyo: capital of, Theory of Evolution is to Charles Darwin: founder of, Peace is to olive branch: symbol of, True Romance is to Japan is to Tokyo: capital of, Theory of Evolution is to Charles Darwin: founder of, Peace is to olive branch: symbol of, is to Johann Sebastian Bach Japan is to Tokyo: capital of, Theory of Evolution is to Charles Darwin: founder of, Peace is to olive branch: symbol of, Fugue in minor, Little, BWV 578 is Syria: Country in the Middle East, Leonardo DiCaprio: American actor, Samsung: South Korean multinational major appliance and consumer electronics corporation, , Hans Zimmer: composer of, ... : composer of, True Romance is to ... : composer of Fugue in minor, and so on. ... : composer of, is to Fugue in minor: ... Johann Sebastian Bach: composer of, ... J. S. Bach: composer of, ... Johann Sebastian Bach: Bach was German composer and musician of the Baroque period, ... Table E: Prompts and model outputs for the relation \"composer of\". (a) Llama-2-13B (b) Gemma-7B (c) Qwen2.5-14B Figure B: Attention knockout results for all models. (a) Llama-2-13B (b) Gemma-7B (c) Qwen2.5-14B Figure C: Relational information for all models. (a) Llama-2-13B (b) Gemma-7B (c) Qwen2.5-14B Figure D: Attributive information for all models. (a) Llama-2-13B (b) Gemma-7B (c) Qwen2.5-14B Figure E: Visualization of intervention experiment for all models. Source Layer refers to layers in e2, and Target Layer refers to layers in the link, into which representations from e2 are injected. (a) Llama-2-13B-chat (b) Gemma-2-9B-it (c) Qwen2.5-14B-Instruct Figure F: Linear probe accuracy for all models. (a) LLama-2-13B-chat (b) Gemma-2-9B-it (c) Qwen2.5-14B-Instruct Figure G: Relative mutual alignment score for all models."
        }
    ],
    "affiliations": [
        "AIGEN Sciences",
        "Korea University"
    ]
}