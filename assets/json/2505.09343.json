{
    "paper_title": "Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures",
    "authors": [
        "Chenggang Zhao",
        "Chengqi Deng",
        "Chong Ruan",
        "Damai Dai",
        "Huazuo Gao",
        "Jiashi Li",
        "Liyue Zhang",
        "Panpan Huang",
        "Shangyan Zhou",
        "Shirong Ma",
        "Wenfeng Liang",
        "Ying He",
        "Yuqing Wang",
        "Yuxuan Liu",
        "Y. X. Wei"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid scaling of large language models (LLMs) has unveiled critical limitations in current hardware architectures, including constraints in memory capacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3, trained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model co-design can effectively address these challenges, enabling cost-efficient training and inference at scale. This paper presents an in-depth analysis of the DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting key innovations such as Multi-head Latent Attention (MLA) for enhanced memory efficiency, Mixture of Experts (MoE) architectures for optimized computation-communication trade-offs, FP8 mixed-precision training to unlock the full potential of hardware capabilities, and a Multi-Plane Network Topology to minimize cluster-level network overhead. Building on the hardware bottlenecks encountered during DeepSeek-V3's development, we engage in a broader discussion with academic and industry peers on potential future hardware directions, including precise low-precision computation units, scale-up and scale-out convergence, and innovations in low-latency communication fabrics. These insights underscore the critical role of hardware and model co-design in meeting the escalating demands of AI workloads, offering a practical blueprint for innovation in next-generation AI systems."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 3 4 3 9 0 . 5 0 5 2 : r Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures Chenggang Zhao, Chengqi Deng, Chong Ruan, Damai Dai, Huazuo Gao, Jiashi Li, Liyue Zhang, Panpan Huang, Shangyan Zhou, Shirong Ma, Wenfeng Liang, Ying He, Yuqing Wang, Yuxuan Liu, Y.X. Wei DeepSeek-AI Beijing, China Abstract The rapid scaling of large language models (LLMs) has unveiled critical limitations in current hardware architectures, including constraints in memory capacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3, trained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model co-design can effectively address these challenges, enabling cost-efficient training and inference at scale. This paper presents an in-depth analysis of the DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting key innovations such as Multi-head Latent Attention (MLA) for enhanced memory efficiency, Mixture of Experts (MoE) architectures for optimized computation-communication trade-offs, FP8 mixed-precision training to unlock the full potential of hardware capabilities, and Multi-Plane Network Topology to minimize cluster-level network overhead. Building on the hardware bottlenecks encountered during DeepSeek-V3s development, we engage in broader discussion with academic and industry peers on potential future hardware directions, including precise low-precision computation units, scale-up and scale-out convergence, and innovations in low-latency communication fabrics. These insights underscore the critical role of hardware and model co-design in meeting the escalating demands of AI workloads, offering practical blueprint for innovation in next-generation AI systems. CCS Concepts Computer systems organization Architectures. Keywords Large Language Model, Mixture-of-Experts, Deep Learning, FP8 Mixed-Precision Training, Multi-Plane Network, Co-Design ACM Reference Format: Chenggang Zhao, Chengqi Deng, Chong Ruan, Damai Dai, Huazuo Gao, Jiashi Li, Liyue Zhang, Panpan Huang, Shangyan Zhou, Shirong Ma, Wenfeng Liang, Ying He, Yuqing Wang, Yuxuan Liu, Y.X. Wei . 2025. Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures. In Proceedings of the 52nd Annual International Symposium on Computer Architecture (ISCA 25), June 2125, 2025, Tokyo, Japan. ACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/3695053.3731412 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. ISCA 25, June 2125, 2025, Tokyo, Japan 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-1261-6/2025/06 https://doi.org/10.1145/3695053."
        },
        {
            "title": "1 Introduction\n1.1 Background\nLarge Language Models (LLMs) have undergone rapid evolution\nin recent years, driven by iterative advancements in model design,\ncomputational power, and data availability. In 2024, groundbreak-\ning models such as GPT4o [59], LLaMa-3 [3], Claude 3.5 Sonnet [8],\nGrok-2 [73], Qwen2.5 [75], Gemini-2 [37] and our DeepSeek-V3 [26]\nhave showcased remarkable progress, further narrowing the gap to-\nwards Artificial General Intelligence (AGI). As the Scaling Laws [45]\nshows, increasing model size, training data, and computational re-\nsources leads to substantial improvements in model performance,\nunderscoring the pivotal role of scaling in advancing AI capabilities.\nCollectively, these developments have ushered in an era where\nscaling model size and computational power is seen as the key to\nunlocking higher levels of intelligence.",
            "content": "Recent developments, reasoning models such as OpenAIs o1/o3 series models [60, 61], DeepSeek-R1 [28], Claude-3.7 Sonnet [9], Gemini 2.5 Pro [38], Seed1.5-Thinking [68] and Qwen3 [71] have demonstrated not only the benefits conferred by large-scale architectures, but also the necessity of improving inference efficiency, particularly in handling longer contexts and achieving greater reasoning depth. These advancements underscore the need for faster and more efficient inference, consequently placing ever-increasing demands on computational resources. To meet these challenges, industry leaders such as Alibaba, ByteDance, Google, xAI and Meta have deployed colossal training clusters [33, 42, 43, 56, 62, 74], featuring tens or even hundreds of thousands of GPUs or TPUs. While such massive infrastructures have enabled the development of state-of-the-art models, their exorbitant costs present significant barriers for smaller research teams and organizations. Despite these barriers, open-source startups such as DeepSeek [2326, 28] and Mistral [41, 55] are also striving to develop state-of-the-art models. Among them, DeepSeek has especially demonstrated that effective software-hardware co-design can enable cost-efficient training of large models, leveling the playing field for smaller teams. Building on this tradition, DeepSeek-V3 [26] represents new milestone in cost-effective training. By leveraging just 2,048 NVIDIA H800 GPUs, DeepSeek-V3 achieves state-of-the-art performance. This achievement aligns with the commitment to advance AI through practical and scalable solutions, as previously demonstrated in the cost-effective architecture of Fire-Flyer AI-HPC [7]. The practices and insights derived from DeepSeek-V3 demonstrate how existing hardware resources can be harnessed to their fullest potential, offering valuable lessons for the broader AI and HPC communities. Authors are listed in alphabetical order of their first names. Yuqing Wang and Liyue Zhang are the corresponding authors of this paper (e-mail: research@deepseek.com). This is the authors version of the work. It is posted here for your personal use. Not for redistribution. The definitive version will appear as part of the Industry Track in Proceedings of the 52nd Annual International Symposium on Computer Architecture (ISCA 25). ISCA 25, June 2125, 2025, Tokyo, Japan DeepSeek-AI"
        },
        {
            "title": "1.2 Objectives\nThis paper does not aim to reiterate the detailed architectural and\nalgorithmic specifics of DeepSeek-V3, which are extensively docu-\nmented in its technical report [26]. Instead, it adopts a dual perspec-\ntive—spanning hardware architecture and model design—to explore\nthe intricate interplay between them in achieving cost-efficient\nlarge-scale training and inference. By examining this synergy, we\naim to provide actionable insights for scaling LLMs efficiently with-\nout sacrificing performance or accessibility.",
            "content": "Specifically, the paper focuses on: Hardware-Driven Model Design: Analyze how hardware features, such as FP8 low-precision computation and scale-up/scaleout network properties, informed the architectural choices in DeepSeek-V3. Mutual Dependencies Between Hardware and Models: Investigate how hardware capabilities shape model innovation and how the evolving demands of LLMs drive the need for nextgeneration hardware. Future Directions for Hardware Development: Derive actionable insights from DeepSeek-V3 to guide the co-design of future hardware and model architectures, paving the way for scalable, cost-efficient AI systems."
        },
        {
            "title": "2 Design Principles for DeepSeek Models\nThe development of DeepSeek-V3 exemplifies a hardware-aware\napproach to scaling LLMs, where each design decision was carefully\naligned with hardware constraints to optimize performance and\ncost efficiency.",
            "content": "As shown in Figure 1, DeepSeek-V3 employs the DeepSeekMoE [27] and Multi-head Latent Attention (MLA) [25] architectures that have been proven effective in DeepSeek-V2 [25]. DeepSeekMoE unlocks the potential of MoE architecture, while MLA drastically reduces memory consumption by compressing Key-Value (KV) caches. In addition, DeepSeek-V3 incorporates FP8 mixedprecision training, significantly lowering computational costs and making large-scale training more practical without compromising model quality. To improve the inference speed, DeepSeek-V3 integrates speculative decoding based on its Multi-Token Prediction Module, which significantly increases the generation speed. Beyond model architecture, we also explored cost-efficient AI infrastructure by deploying Multi-Plane two-layer Fat-Tree network to 2 replace traditional three-layer Fat-Tree topology, reducing cluster networking costs. These innovations aim to address three core challenges in scaling LLMsmemory efficiency, cost-effectiveness, and inference speedwhich are explored in detail in the following subsections."
        },
        {
            "title": "2.1 Memory Efficiency\nLLMs generally require significant memory resources, with memory\ndemands increasing by more than 1000% per year. In contrast, the\ngrowth rate of high-speed memory (e.g., HBM) capacity is much\nslower, typically less than 50% per year [35]. While multi-node\nparallelism is a viable solution to address memory limitations, opti-\nmizing memory usage at the source remains a crucial and effective\nstrategy.\nLow-Precision Models. Compared to models that utilize BF16\n2.1.1\nfor weights, FP8 significantly reduces memory consumption by half,\neffectively alleviating the AI memory wall challenge. A detailed\ndiscussion of low-precision techniques is provided in Section 3\nLow-Precision Driven Design.\n2.1.2 Reducing KV Cache with MLA. For LLM inference, user re-\nquests often involve multi-turn conversations. To handle these\nefficiently, the context from previous requests is cached in what is\ncommonly referred to as the KV cache. KV cache addresses this chal-\nlenge by caching the Key and Value vectors of previously processed\ntokens, eliminating the need to recompute them for subsequent to-\nkens. During each inference step, the model only computes the Key\nand Value vectors for the current token and performs attention com-\nputation by combining them with the cached Key-Value pairs from\nthe history. This incremental computation reduces the complexity\nof generating each token to 𝑂 (𝑁 ), making it efficient when pro-\ncessing long sequences or multi-turn inputs. However, it introduces\na memory-bound bottleneck because the computation shifts from\nGEMM to GEMV, which has a much lower compute-to-memory\nratio. With modern hardware offering hundreds of TFLOPS, GEMV\nquickly becomes limited by memory bandwidth, making memory\naccess the primary bottleneck.",
            "content": "To address this bottleneck, we employ Multi-head Latent Attention (MLA) [25] that compresses the KV representations of all attention heads into smaller latent vector using projection matrix, which is jointly trained with the model. During inference, only the latent vector needs to be cached, significantly reducing memory consumption compared to storing the KV cache for all attention heads. In addition to MLA, several other approaches have been proposed to reduce the size of the KV cache. These methods are highly valuable and provide significant inspiration for advancements in memory-efficient attention mechanisms: Shared KV (Grouped-Query Attention, GQA; Multi-Query Attention, MQA): Instead of maintaining separate KV pairs for each attention head, multiple heads share single set of KV pairs, significantly compressing KV storage. Representative methods include GQA [5] and MQA [70]. Windowed KV: For long sequences, only sliding window of KV pairs is retained in the cache, discarding results outside the window. While this reduces storage, it compromises long-context reasoning. Representative methods include Longformer [11] and related architectures. Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures ISCA 25, June 2125, 2025, Tokyo, Japan Figure 1: Basic architecture of DeepSeek-V3. Built upon DeepSeek-V2s MLA and DeepSeekMoE, Multi-Token Prediction Module and FP8 mixed-precision training are introduced to enhance inference and training efficiency. The figure indicates the precision used for computations in different parts of the architecture. All components take inputs and outputs in BF16. Quantized Compression: KV pairs are stored using low-bit representations [40, 44, 52], further reducing memory usage. Quantization achieves significant compression with minimal impact on model performance. Table 1 compares the KV cache memory usage per token among DeepSeek-V3, Qwen-2.5 72B [75], and LLaMA-3.1 405B [4]. By adopting MLA, DeepSeek-V3 achieves significant reduction in KV cache size, requiring only 70 KB per token, substantially less than LLaMA-3.1 405Bs 516 KB and Qwen-2.5 72Bs 327 KB. This reduction highlights the efficiency of MLA in compressing KV representations compared to GQA-based methods. The ability to achieve such significant reduction in memory consumption makes DeepSeekV3 particularly well-suited for scenarios involving long-context processing and resource-constrained environments, enabling more scalable and cost-effective inference. Future Directions and Perspectives on Resource-Efficient Tech2.1.3 niques. While reducing the size of the KV cache is promising method for improving memory efficiency, the quadratic complexity inherent in Transformer-based autoregressive decoding remains formidable challenge, especially for extremely long contexts. Recent research efforts, such as Mamba-2 [21] and Lightning Attention[63], investigate linear-time alternatives that offer new possibilities for balancing computational cost and model performance. In addition, approaches such as sparse attention [76], which seek to compress and sparsely activate attention keys and values, represent another attempt at overcoming the computational challenges associated with attention. We look forward to collaborative progress with the broader community toward breakthroughs in this area."
        },
        {
            "title": "2.2 Cost-Effectiveness of MoE Models\nFor sparse computing, we have developed DeepSeekMoE, an ad-\nvanced Mixture of Experts (MoE) architecture, which is illus-\ntrated in the lower right part of Figure 1. The advantages of MoE\nmodels lie in two folds.",
            "content": "2.2.1 Reducing Computational Requirements for Training. The primary advantage of the MoE architecture lies in its ability to significantly reduce training costs. By selectively activating only subset of expert parameters, MoE models allow the total parameter count to scale up dramatically while keeping computational requirements modest. For example, DeepSeek-V2 features 236B parameters, but only 21B parameters are activated per token. Similarly, DeepSeek-V3 expands to 671B parametersnearly three times the size of V2while keeping the activation per token at just 37B. In comparison, dense models such as Qwen2.5-72B and LLaMa3.1-405B require all parameters to be active during training. 3 ISCA 25, June 2125, 2025, Tokyo, Japan DeepSeek-AI Table 1: KV cache size comparison (BF16 precision): DeepSeek-V3 (MLA) largely reduces KV cache size compared to other models using GQA. Table 2: Comparison of computational costs for training MoE and dense models: Computational cost per token is measured, assuming sequence length of 4096. Model DeepSeek-V3 (MLA) Qwen-2.5 72B (GQA) LLaMA-3.1 405B (GQA) KV Cache Per Token Multiplier 70.272 KB 327.680 KB 516.096 KB 1x 4.66x 7.28x As shown in Table 2, the total computational cost for DeepSeekV3 is approximately 250 GFLOPS per token, whereas the 72B dense model requires 394 GFLOPS and the 405B dense model requires 2448 GFLOPS. This demonstrates that MoE models achieve comparable or even superior performance to dense models while consuming an order of magnitude less computational resources. 2.2.2 Advantages for Personal Use and On-Premises Deployment. In future where personalized LLM agents [53] become ubiquitous, MoE models offer unique advantages in single-request scenarios. Because only subset of parameters is activated per request, memory and computational demands are greatly reduced. For example, DeepSeek-V2 (236B parameters) activates just 21B parameters during inference. This enables PCs with AI SoC chips [6, 10, 58] to achieve nearly 20 tokens per second (TPS), or even twice that speed, which is more than sufficient for personal use. In contrast, dense models of similar capability (e.g., 70B parameters) typically reach only single-digit TPS on similar hardware. Notably, the increasingly popular KTransformers [39] inference engine allows the complete DeepSeek-V3 model to run on lowcost server equipped with consumer GPU (costing approximately $10,000), while still achieving nearly 20 TPS. This efficiency makes MoE architectures suitable for local deployments and single-user scenarios, where hardware resources are often limited. By minimizing memory and computational overhead, MoE models can deliver high-quality inference performance without requiring expensive infrastructure."
        },
        {
            "title": "2.3 Increasing Inference Speed\n2.3.1 Overlapping Computation and Communication: Maximizing\nThroughput. Inference speed encompasses both system-wide maxi-\nmum throughput and single-request latency. To maximize through-\nput, our model is architected from the outset to leverage dual micro-\nbatch overlap [31, 78], intentionally overlapping communication\nlatency with computation. As demonstrated in our online infer-\nence system and supported by open-source profiling data [31],\nwe decouple the computation of MLA and MoE into two distinct\nstages. While one micro-batch executes a portion of MLA or MoE\ncomputation, the other micro-batch simultaneously performs the\ncorresponding dispatch communication. Conversely, during the\ncomputation phase of the second micro-batch, the first micro-batch\nundergoes the combine communication step. This pipelined ap-\nproach enables seamless overlap of all-to-all communication with\nongoing computation, ensuring that the GPU remains fully utilized\nat all times. Moreover, in production, we adopt a prefill and decode\ndisaggregation architecture [80], assigning large batch size prefill\nand latency-sensitive decode requests to different expert parallelism\ngroup sizes. This strategy ultimately maximizes system throughput\nunder real-world service conditions.",
            "content": "4 Model DeepSeek-V2 MoE DeepSeek-V3 MoE Qwen-72B Dense LLaMa-405B Dense Size 236B 671B 72B 405B Training Cost 155 GFLOPS/Token 250 GFLOPS/Token 394 GFLOPS/Token 2448 GFLOPS/Token Inference Speed Limits. This section focuses on the decode 2.3.2 output speed of LLM services, typically measured in Time Per Output Token (TPOT). TPOT is critical metric for user experience, and it also directly impacts the responsiveness of reasoning models such as OpenAIs o1/o3 and DeepSeek-R1, which rely on the inference length to enhance their intelligence. For MoE models, achieving high inference speed relies on efficiently deploying expert parameters across computing devices. To achieve the fastest possible inference speed, each device should ideally perform computations for single expert (or multiple devices should collaboratively compute single expert if necessary). However, Expert Parallelism (EP) requires routing tokens to the appropriate devices, which involves all-to-all communication across the network. As result, the upper limit of MoE inference speed is dictated by interconnection bandwidth. Consider system where each device holds one experts parameters and processes approximately 32 tokens at time. This token count strikes balance between compute-to-memory ratio and communication latency. And this token count ensures that each device processes an equal batch size during expert parallelism, allowing the communication time to be easily calculated. For system interconnected with CX7 400Gbps InfiniBand (IB) NICs, the time required for the two all-to-all communications in EP is calculated as follows: Comm. Time = (1Byte + 2Bytes) 32 9 7K/50GB/s = 120.96𝜇𝑠 Here, dispatch uses FP8 (1 byte), while combine uses BF16 (2 bytes), and the hidden size of each token is approximately 7K. The factor 9 indicates that each token is transferred to 8 routed experts and 1 shared expert. As discussed in Section 2.3.1, maximizing throughput necessitates the use of dual micro-batch overlap. In this strategy, our theoretical best-case analysis assumes that computation overhead is minimized, so the upper bound on performance is determined by communication latency. In practical inference workloads, however, request contexts are often much longer, and MLA computations typically dominate execution time. Thus, this analysis represents an idealized scenario under dual micro-batch overlap. Under this assumption, the total time per layer can be formulated as: Total Time Per Layer = 2 120.96𝜇𝑠 = 241.92𝜇𝑠 With 61 layers in DeepSeek-V3, the total inference time is: Total Inference Time = 61 241.92𝜇𝑠 = 14.76ms Thus, the theoretical upper limit for this system is approximately 14.76 ms TPOT, equivalent to 67 tokens per second. However, in practice, factors such as communication overhead, latency, incomplete bandwidth utilization, and computational inefficiencies reduce this number. Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures ISCA 25, June 2125, 2025, Tokyo, Japan By contrast, if high-bandwidth interconnect like GB200 NVL72 (900GB/s unidirectional bandwidth across 72 GPUs) were used, the communication time per EP step drops to: Comm. Time = (1Byte + 2Bytes) 32 9 7K/900GB/s = 6.72𝜇𝑠 Assuming the computation time is equal to the communication time, this reduces the total inference time significantly, enabling theoretical upper limit of over 0.82 ms TPOT, approximately 1200 tokens per second. While this figure is purely theoretical and has not been empirically validated, it vividly illustrates the transformative potential of high-bandwidth scale-up networks in accelerating large-scale model inference. While MoE models exhibit good scalability, achieving high inference speeds by increasing hardware resources alone is costprohibitive. Therefore, software and algorithms must also contribute to improving inference efficiency. 2.3.3 Multi-Token Prediction. Inspired by Gloeckle et al. [36], DeepSeek-V3 introduces Multi-Token Prediction (MTP) framework, which simultaneously enhances model performance and improves inference speed. During inference, traditional autoregressive models generate one token at decoding step, leading to sequential bottlenecks. MTP mitigates this issue by enabling the model to generate additional candidate tokens at lower cost and verify them in parallel, similar to previous self-drafting-based speculative decoding approaches [14, 48]. This framework significantly accelerates inference without compromising accuracy. As illustrated in the top part of Figure 1, each MTP module uses single layer, which is much more lightweight than the full model, to predict additional tokens, enabling parallel verification of multiple candidate tokens. Although slightly hurting the throughput, this approach significantly improves the end-to-end generation latency. The real world practice data demonstrates that an MTP module achieves an acceptance rate of 80% to 90% for predicting the second subsequent token, which increases the generation TPS by 1.8x compared to the scenario without the MTP module. Moreover, by predicting multiple tokens per step, MTP increases the inference batch size, which is crucial for boosting EP computational intensity and hardware utilization. Such algorithmic innovations are vital for fast and cost-effective inference in DeepSeek-V3. 2.3.4 High Inference Speed for Reasoning Models and Test-Time Scaling. Test-time scaling in LLMs, exemplified by OpenAIs o1/o3 series [60, 61], has enabled significant advances in mathematical reasoning, programming, and general reasoning by dynamically adjusting computational resources during inference. Subsequent modelsincluding DeepSeek-R1 [28], Claude-3.7 Sonnet [9], Gemini 2.5 Pro [38], Seed1.5-Thinking [68], and Qwen3 [71]have adopted similar strategies and achieved notable improvements in these tasks. For these reasoning models, high token output speed is of paramount importance. In reinforcement learning (RL) workflowssuch as PPO [67], DPO [64] and GRPO [69]the necessity to rapidly generate large numbers of samples makes inference throughput critical bottleneck. Likewise, prolonged reasoning sequences can increase user wait times, reducing the practical usability of such models. As result, optimizing inference speed through synergistic hardware and software innovations is indispensable for advancing the efficiency of reasoning models. However, effective strategies for 5 accelerating inference and expediting RL training remain active areas of investigation, as discussed in Section 2.1.3. We encourage the broader community to collaboratively explore and develop novel solutions to these ongoing challenges."
        },
        {
            "title": "2.4 Technique Validation Methodology\nEach acceleration technique undergoes rigorous empirical valida-\ntion to evaluate its accuracy impact, including MLA, FP8 mixed-\nprecision computation, and network co-designed MoE gate rout-\ning. Given the prohibitive cost of exhaustive ablation on full-scale\nmodels, we adopt a hierarchical and resource-efficient validation\npipeline. Each technique is first validated extensively on small-\nscale models, followed by minimal large-scale tuning, and finally\nintegrated in a single, comprehensive training run.",
            "content": "For instance, we first conducted fine-grained FP8 training ablation studies on both 16B and 230B DeepSeek-V2 models before final integration. Under these controlled settings, the relative accuracy loss compared to BF16 remains below 0.25%, attributable to our use of high-precision accumulation and fine-grained quantization strategies."
        },
        {
            "title": "3 Low-Precision Driven Design\n3.1 FP8 Mix-Precision Training\nQuantization techniques such as GPTQ [32] and AWQ [51] have\nbeen widely used to reduce bit-widths to 8-bit, 4-bit, or even lower,\nsignificantly reducing memory requirements. However, these tech-\nniques are primarily applied during inference to save memory,\nrather than in the training phase. NVIDIA’s Transformer Engine\nhas supported FP8 mixed-precision training for some time, but\nprior to DeepSeek-V3, there were no open-source large models\nleveraging FP8 for training. Through deep collaboration between\nour infrastructure and algorithm teams, and after extensive experi-\nmentation and innovation, we developed an FP8-compatible train-\ning framework for MoE models. Figure 1 shows the computational\ncomponents where FP8-precision forward and backward processes\nare utilized in the training pipeline. Fine-grained quantization is\napplied, i.e., tile-wise 1x128 quantization for activations and block-\nwise 128x128 quantization for model weights. Further technical\ndetails of our FP8 framework are documented in the DeepSeek-V3\ntechnical report [26], and our fine-grained FP8 GEMM implemen-\ntation has been open-sourced in DeepGEMM [77].",
            "content": "3.1.1 Limitations: While FP8 has great potential for accelerating training, several hardware limitations need to be addressed to fully exploit its capabilities: FP8 Accumulation Precision: FP8 uses constrained accumulation precision in Tensor Cores, affecting the stability for training large models, particularly on NVIDIA Hopper GPUs. After aligning 32 mantissa products by right-shifting based on the maximum exponent, the Tensor Core only maintains their highest 13 fraction bits for addition, and truncates bits exceeding this range. Addition results are accumulated to FP22 registers (1 sign bit, 8 exponent bits, and 13 mantissa bits). Fine-Grained Quantization Challenges: Fine-grained quantization such as tile-wise and block-wise quantization introduces large dequantization overhead in transporting the partial results ISCA 25, June 2125, 2025, Tokyo, Japan DeepSeek-AI from Tensor Cores to CUDA Cores for scaling factor multiplication. This incurs frequent data movements, reducing computational efficiency and complicating hardware utilization. Suggestions: To address the limitations of existing hardware, 3.1.2 we have the following suggestions for future designs: Increased Accumulation Precision: Hardware should improve the accumulation register precision to an appropriate value (e.g. FP32), or support configurable accumulation precision, enabling trade-off between performance and accuracy for different requirements of training and inference in various models. Native Support for Fine-Grained Quantization: Hardware should natively support fine-grained quantization, enabling Tensor Cores to receive scaling factors and implement matrix multiplication with group scaling. In this way, the whole partial sum accumulation and dequantization can be completed directly inside Tensor Cores until the final result is produced, avoiding frequent data movements to reduce dequantization overhead. notable industrial implementation of this approach is NVIDIA Blackwells support for microscaling data format [66], which exemplifies the practical benefits of native quantization at scale."
        },
        {
            "title": "3.2 LogFMT: Communication Compression\nIn the current DeepSeek-V3 architecture, we employ low-precision\ncompression for network communication. During EP parallelism,\ntokens are dispatched using fine-grained FP8 quantization, reducing\ncommunication volume by 50% compared to BF16. This significantly\nlowers communication time. While the combine stage still uses\nhigher precision (e.g., BF16) due to accuracy requirements, we are\nactively testing FP8, custom precision formats (e.g., E5M6) and\nmixing FP8-BF16 for further reductions.",
            "content": "Besides these traditional floating point formats, we also tried new data type, named Logarithmic Floating-Point Formats (LogFMT-nBit), where 𝑛 is the number of bits with the leading 1 bit as the sign bit 𝑆. By mapping the activations from the original Linear space to the Log space, the distribution of the activations is more uniform. To be specific, given tile of elements, [𝑥1, , 𝑥𝑚], which is 1x128 in our implementation, we take the absolute values and compute the logarithm of all the elements, and find the minimum 𝑚𝑖𝑛 = 𝑙𝑜𝑔(𝑎𝑏𝑠 (𝑥𝑖 )) and maximum 𝑚𝑎𝑥 = 𝑙𝑜𝑔(𝑎𝑏𝑠 (𝑥 𝑗 )). The minimum is encoded as 𝑆.00 01 and the maximum is encoded as 𝑆.11 11, with an interval representing 𝑆𝑡𝑒𝑝 = 𝑚𝑎𝑥 𝑚𝑖𝑛 2𝑛1 2 . Zero values are represented by 𝑆.00 00, specially. The left values are rounded to the nearest integer 𝐾 multiples of 𝑆𝑡𝑒𝑝. The decoding process is simple by combining the sign bit and 𝑒𝑥𝑝𝑚𝑖𝑛+𝑆𝑡𝑒𝑝 (𝐾 1) . By locally calculating the 𝑚𝑖𝑛 and 𝑆𝑡𝑒𝑝, this data type supports dynamic representation range for different blocks, covering larger ranges or providing more precision, compared to static floating point formats. Besides, we find it is important to round in the original Linear space, instead of the Log space, for the unbiased activation quantization. We also constrain the 𝑚𝑖𝑛 to be larger than 𝑚𝑎𝑥 𝑙𝑜𝑔(232), which means that the max representation range is similar to E5, floating point with 5 exponents. We validate our LogFMT-nBit on dense language models with around 7 billion parameters, by quantifying the output of the residual branch to simulate the combine stage in MoE models. When setting 𝑛 = 8, sharing the same bits with FP8, the LogFMT-8Bit shows superior Figure 2: H800 node interconnection. training accuracy compared to E4M3 or E5M2. After increasing the 𝑛 to 10 bits, we find its similar to the BF16 combine stage. Limitations: The initial purpose of using LogFMT is to apply 3.2.1 it to activations during transmission or near activation functions, as it offers higher precision than FP8 with the same bit width. However, subsequent computations require reconversion to BF16 or FP8 to accommodate the Hopper GPU tensor cores data type. Due to insufficient GPU bandwidth for log/exp operations and excessive register pressure during encode/decode, if encode/decode operations are fused with all-to-all communication, the overhead can be substantial (50%100%). Therefore, although experimental results validate the effectiveness of this format, we do not employ it eventually. Suggestions: Providing native support for compression and 3.2.2 decompression units tailored to FP8 or custom precision formats represents viable approach for future hardware. This could help minimize bandwidth requirements and streamline communication pipelines. The reduced communication overhead is particularly helpful in bandwidth-intensive tasks like MoE training."
        },
        {
            "title": "4 Interconnection Driven Design\n4.1 Current Hardware Architecture\nThe NVIDIA H800 GPU SXM architecture we currently use, illus-\ntrated in Figure 2, is built on the Hopper architecture, similar to\nthe H100 GPU. However, it features reduced FP64 computational\nperformance and NVLink bandwidth for regulatory compliance.\nSpecifically, the NVLink bandwidth in H800 SXM nodes is reduced\nfrom 900 GB/s to 400 GB/s. This significant reduction in intra-node\nscale-up bandwidth presents a challenge for high-performance\nworkloads. To compensate, each node is equipped with eight 400G\nInfiniband (IB) CX7 NICs, enhancing scale-out capabilities to miti-\ngate the bandwidth deficit.",
            "content": "To address these hardware constraints, the DeepSeek-V3 model incorporates several design considerations that align with the hardwares strengths and limitations."
        },
        {
            "title": "4.2 Hardware-Aware Parallelism\nTo align with the constraints of the H800 architecture, the following\nparallelism strategies were considered to optimize the performance\nof DeepSeek-V3:",
            "content": "6 Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures ISCA 25, June 2125, 2025, Tokyo, Japan Avoidance of Tensor Parallelism (TP): Tensor Parallelism is avoided during training due to its inefficiency under limited NVLink bandwidth. However, during inference, TP can still be selectively used to reduce latency and improve TPOT performance. Enhanced Pipeline Parallelism (PP): DualPipe [29] is employed to overlap attention and MoE computation with MoE communication. This also reduces pipeline bubbles and balances memory usage across GPUs, improving overall throughput. Additional details are available in the technical report [26]. Accelerated Expert Parallelism (EP): With eight 400Gbps InfiniBand (IB) NICs, the system achieves all-to-all communication at speeds exceeding 40GB/s. Notably, our all-to-all EP implementation, DeepEP [78], is open-sourced, enabling highly efficient expert parallelism as discussed in the following subsection."
        },
        {
            "title": "4.3 Model Co-Design: Node-Limited Routing\nThe bandwidth disparity between scale-up (intra-node) and scale-\nout (inter-node) communication in the H800 architecture is approx-\nimately 4:1. Specifically, NVLink provides 200GB/s bandwidth (of\nwhich about 160GB/s can actually be achieved), while each 400Gbps\nIB NIC delivers only 50GB/s bandwidth (we consider small message\nsize and latency influence, use 40GB/s for effective bandwidth).\nTo balance and fully utilize the higher intra-node bandwidth, the\nmodel architecture is co-designed with hardware, particularly in\nthe TopK Expert Selection Strategy.",
            "content": "Consider setup with 8 nodes (64 GPUs in total) and 256 routed experts (4 experts per GPU). For DeepSeek-V3, each token is routed to one shared expert and 8 routed experts. If its 8 target experts are distributed across all 8 nodes, the communication time over IB would be 8𝑡, where 𝑡 represents the time to send one token over IB. However, by leveraging the higher NVLink bandwidth, tokens routed to the same node can be sent once over IB and then forwarded via NVLink to other intra-node GPUs. The NVLink forwarding enables deduplication of the IB traffic. When the target experts for given token are distributed across 𝑀 nodes, the deduplicated IB communication cost will be reduced to 𝑀𝑡 (𝑀 < 8). Since the IB traffic depends on only 𝑀, DeepSeek-V3 introduces Node-Limited Routing for the TopK expert selection strategy. Specifically, we group 256 routed experts into 8 groups, with 32 experts per group, and deploy each group on single node. On top of this deployment, we algorithmically ensure that each token will be routed to up to 4 nodes. This approach mitigates the bottleneck of IB communication and enhances the effective communication bandwidth during training."
        },
        {
            "title": "4.4 Scale-Up and Scale-Out Convergence\n4.4.1 Limitations of Current Implementations. While the Node-\nLimited Routing strategy reduces communication bandwidth re-\nquirements, it complicates communication pipeline kernel imple-\nmentations due to the disparity in bandwidth between intra-node\n(NVLink) and inter-node (IB) interconnects. In practice, GPU Stream-\ning Multiprocessors (SM) threads are used for both network mes-\nsage handling (e.g., filling QPs and WQEs) and data forwarding over\nNVLink, consuming computational resources. For example, during\ntraining, up to 20 of the SMs on the H800 GPU are allocated for",
            "content": "7 communication-related operations, leaving fewer resources available for actual computation. To maximize throughput in online inference, we perform EP all-to-all communication entirely through NIC RDMA, avoiding SM resource contention and improving compute efficiency. This highlights the advantage of RDMAs asynchronous communication model in overlapping computation and communication. The following are key tasks currently performed by SMs during EP communication, particularly for the combine stages reduce operations and data type conversions. Offloading these tasks to dedicated communication hardware could free up SMs for computation kernels, significantly improving overall efficiency: Forwarding Data: Aggregating IB traffic destined for multiple GPUs within the same node between the IB and NVLink domains. Data Transport: Moving data between RDMA buffers (registered GPU memory regions) and input/output buffers. Reduce Operations: Executing reduce operations required for EP all-to-all combine communications. Managing Memory Layouts: Handling fine-grained memory layouts for chunked data transfers across the IB and NVLink domains. Data Type Cast: Converting data type before and after all-toall communications. Suggestions: To address these inefficiencies, we strongly rec4.4.2 ommend that future hardware should integrate intra-node (scaleup) and inter-node (scale-out) communication into unified framework. By incorporating dedicated co-processors for network traffic management and seamless forwarding between NVLink and IB domains, such designs can reduce software complexity and maximize bandwidth utilization. For example, node-limited routing strategies employed in DeepSeek-V3 can be further optimized with hardware support for dynamic traffic deduplication. We also recognize emerging interconnect protocols such as the Ultra Ethernet Consortium (UEC) [17, 18], Ultra Accelerator Link (UALink) [16], both of which are poised to drive advancements in scale-up and scale-out communication. More recently, Unified Bus (UB) [49] has introduced novel approach to scale-up and scale-out convergence. Section 6 further explores several technical innovations proposed by UEC and UALink. However, in this section, our primary focus is on achieving scale-up and scale-out convergence at the programming framework level.: (1) Unified Network Adapter: Design NICs (Network Interface Cards) or I/O Dies that are connected to unified scale-up and scale-out networks. These adapters should also support basic switch functionality, such as forwarding packets from the scaleout network to specific GPUs within the scale-up network. This could be achieved using single LID (Local Identifier) or IP address with policy-based routing. (2) Dedicated Communication Co-Processor: Introduce dedicated co-processor or programmable componentsuch as an I/O diefor handling network traffic. This component would offload packet processing from GPU SMs, preventing performance degradation. Besides, it should include hardware-accelerated memory copy capabilities for efficient buffer management. (3) Flexible Forwarding, Broadcast and Reduce Mechanisms: Hardware should support flexible forwarding, broadcast operations (for EP dispatch), and reduce operations (for EP combine) ISCA 25, June 2125, 2025, Tokyo, Japan DeepSeek-AI across scale-up and scale-out networksmirroring our current GPU SM-based implementation. This would not only improve effective bandwidth but also reduce the computational complexity of network-specific operations. (4) Hardware Synchronization Primitives: Provide fine-grained hardware synchronization instructions to handle memory consistency issues or out-of-order packet arrivals at the hardware level. This would eliminate the need for software-based synchronization mechanisms like RDMA completion events, which introduce extra latency and increase programming complexity. Memory-semantic communication with an acquire/release mechanism is promising implementation. By implementing these recommendations, future hardware designs can significantly enhance the efficiency of large-scale distributed AI systems while simplifying software development."
        },
        {
            "title": "4.5 Bandwidth Contention and Latency\n4.5.1 Limitations: Besides, current hardware lacks the flexibility\nto dynamically allocate bandwidth between different types of traffic\non NVLink and PCIe. For example, during inference, transferring\nKV cache data from CPU memory to GPU can consume tens of\nGB/s, saturating PCIe bandwidth. If the GPU simultaneously uses IB\nfor EP communication, this contention between KV cache transfers\nand EP communication can degrade overall performance and cause\nlatency spikes.",
            "content": "Suggestions: 4.5.2 Dynamic NVLink/PCIe Traffic Prioritization: Hardware should support dynamic prioritization of traffic based on its type. For example, traffic related to EP, TP, and KV cache transfers should be assigned different priorities to maximize interconnect efficiency. For PCIe, exposing the traffic class (TC) to user-level programming would suffice. I/O Die Chiplet Integration: Integrating NICs directly into the I/O die and connecting them to the compute die in the same package, rather than through conventional PCIe, would substantially reduce communication latency and alleviate PCIe bandwidth contention. CPUGPU Interconnects within the Scale-Up Domain: To further optimize intra-node communication, CPUs and GPUs should be interconnected using NVLink or similar dedicated high-bandwidth fabrics, rather than relying solely on PCIe. Similar to the benefits provided by integrating NICs into the I/O die, this approach can significantly improve scenarios such as offloading parameters or KV cache between GPU and CPU memory during training and inference."
        },
        {
            "title": "5 Large Scale Network Driven Design\n5.1 Network Co-Design: Multi-Plane Fat-Tree\nDuring the training of DeepSeek-V3, we deployed a Multi-Plane\nFat-Tree (MPFT) scale-out network, as shown in Figure 3. Each\nnode is equipped with eight GPUs and eight IB NICs, with each\nGPU–NIC pair assigned to a distinct network plane. Additionally,\neach node has a 400 Gbps Ethernet RoCE NIC connected to a sepa-\nrate storage network plane for accessing the 3FS [30] distributed file\nsystem. In the scale-out network, we used 64-port 400G IB switches,\nenabling the topology theoretically supports up to 16,384 GPUs",
            "content": "Figure 3: Eight-plane two-layer fat-tree scalue-out network: Each GPU and IB NIC pair belongs to one network plane. Cross-plane traffic must use another NIC and PCIe or NVLink for intra-node forwarding. while retaining the cost and latency advantages of two-layer network. However, due to policy and regulatory constraints, just over two thousand GPUs were ultimately deployed. Furthermore, due to the current limitations of IB ConnectX-7, our deployed MPFT network does not fully realize the envisioned architecture. Ideally, as depicted in Figure 4, each NIC would feature multiple physical ports, each connected to separate network plane, yet collectively exposed as single logical interface to the user through port bonding. From user perspective, single Queue Pair (QP) could seamlessly transmit and receive messages across all available ports, akin to packet spraying. As consequence, packets originating from the same QP may traverse distinct network paths and arrive at the receiver out of order, thereby necessitating native support for out-of-order placement within the NIC to guarantee message consistency and preserve the correct ordering semantics. For example, InfiniBand ConnectX-8 natively supports four plane. It would be advantageous for future NICs to fully support advanced multi-plane capabilities, allowing two-tier fat-tree networks to scale effectively to much larger AI clusters. Overall, the multi-plane architecture offers significant advantages in fault isolation, robustness, load balancing, and large-scale system scalability. 5.1.1 Advantages of Multi-Plane Fat-Tree Network. Subset of Multi-Rail Fat-Tree (MRFT): The MPFT topology constitutes specific subset of the broader MRFT architecture. As result, existing optimizations developed by NVIDIA and NCCL for Multi-Rail networks can be seamlessly leveraged within Multi-Plane network deployments. Furthermore, NCCLs support for PXN [54] technology addresses the inherent challenge of inter-plane isolation, enabling efficient communication even when direct interconnectivity between planes is absent. Cost Efficiency: As shown in Table 3, the multi-plane network enables over 10k endpoints using two-layer fat-tree (FT2) topology, significantly reducing network costs compared to threelayer fat tree (FT3). The cost per endpoint is even slightly more competitive than the cost-efficient Slim Fly (SF) topology [12]. Traffic Isolation: Each plane operates independently, ensuring that congestion in one plane does not affect others. This isolation improves overall network stability and prevents cascading performance degradation. Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures ISCA 25, June 2125, 2025, Tokyo, Japan Figure 4: Ideal Multi-Plane Network: Each NIC is equipped with multiple physical ports, each connected to distinct network plane. single queue pair (QP) can simultaneously utilize all available ports for transmitting and receiving packets, which necessitates native support for out-of-order placement within the NIC. Table 3: Network topology comparison. Cost estimates are derived from the methodology in the Slim Fly (SF) paper [12]. DF denotes the canonical dragonfly topology [22, 46, 65]. Metric Endpoints Switches Links Cost [M$] Cost/Endpoint [k$] FT2 MPFT 16,384 2,048 768 96 16,384 2,048 72 9 4.39 4. FT3 65,536 5,120 131,072 491 7.5 SF 32,928 1,568 32,928 146 4.4 DF 261,632 16,352 384,272 1,522 5.8 Latency Reduction: The two-layer topology achieves lower latency than three-layer fat trees, as demonstrated in our experiments. This makes it particularly suitable for latency-sensitive applications such as MoE-based training and inference. Robustness: As shown in Figure 4, multi-port NICs provide multiple uplinks, so single-port failures do not disrupt connectivity and rapid, transparent fault recovery is possible. It is important to note that, due to current 400G NDR InfiniBand limitations, cross-plane communication requires intra-node forwarding, which introduces additional latency during inference. If future hardware can achieve scale-up and scale-out network convergence as discussed earlier, this latency can be significantly reduced, further enhancing the viability of multi-plane networks. 5.1.2 Performance Anlaysis. To verify the effectiveness of the MultiPlane Network design, we conducted real-world experiments on our cluster, modifying the clusters network topology to compare the performance of the Multi-Plane Two-Layer Fat Tree (MPFT) and the Single-Plane Multi-Rail Fat Tree (MRFT). Below are the key findings from our experiments: 1. All-to-All Communication and EP Scenarios: As illustrated in Figure 5, the all-to-all performance of the multi-plane network is very similar to that of the single-plane multi-rail network. This performance parity can be attributed to NCCLs PXN [54] mechanism, which optimizes traffic forwarding via NVLink in multi-rail topologies. The multi-plane topology also benefits from this mechanism. As shown in Figure 6, the results of all-toall communication tests conducted on 16 GPUs reveal negligible differences in latency between the MPFT and MRFT topologies. To evaluate MPFTs performance of all-to-all communication in practical training scenarios, we tested the EP communication patterns commonly used during training. As shown in Figure 7, each Figure 5: NCCL all-to-all performance from 32 to 128 GPUs for MRFT and MPFT networks. GPU achieves high bandwidth exceeding 40GB/s in multi-plane network, providing reliable performance that meets the demands of training. 2. Training Throughput for DeepSeek-V3 Model: We also compare the training metrics of the DeepSeek-V3 model between MPFT and MRFT in Table 4. MFU (Model Flops Utilization) is calculated based on BF16 peak performance. Causal MFU only takes into account the flops of the lower triangle of the attention matrix (in line with FlashAttention[19, 20]), while non-causal MFU includes the flops of the whole attention matrix (in line with Megatron [47]). 1F, 1B, and 1W denote forward time, input backward time, and weight backward time, respectively. When training the V3 model on 2048 GPUs, the performance of MPFT is nearly identical to that of MRFT, with observed differences falling within normal fluctuations and measurement error."
        },
        {
            "title": "5.2 Low Latency Networks\nIn our model inference, large-scale EP relies heavily on all-to-all\ncommunication, which is highly sensitive to both bandwidth and\nlatency. Consider a typical scenario discussed in Section 2.3.2, with a\nnetwork bandwidth of 50GB/s, the data transfer should ideally take\napproximately 120 𝜇s . Therefore, the intrinsic network latencies on\nthe order of microseconds can critically impact system performance,\nmaking their effects non-negligible.",
            "content": "IB or RoCE. As shown in Table 5, IB consistently achieves 5.2.1 lower latency, making it the preferred choice for latency-sensitive 9 ISCA 25, June 2125, 2025, Tokyo, Japan DeepSeek-AI Table 4: Training metric comparison between MPFT and MRFT networks. Metric tokens/day (B) time/step (s) 1F (s) bubble (s) 1B (s) 1W (s) 1F1B (s) opt (s) TFLOPS (non-causal) TFLOPS (causal) MFU (non-causal) MFU (causal) MPFT MRFT 272.52 272.80 19.946 19.926 1.13 1.13 2.03 2.06 1.99 1.99 0.48 0.48 14.00 13.95 0.31 0.29 432 432 385 385 43.68% 43.73% 38.90% 38.94% Table 5: CPU side end-to-end latency comparison between IB, RoCE, and intra-node NVLink for 64B data transmission. Link Layer RoCE InfiniBand NVLink Same Leaf Cross Leaf 3.6us 2.8us 3.33us 5.6us 3.7us - (2) Optimized Route Policy: As shown in Figure 8, the default Equal-Cost Multi-Path (ECMP) routing policy in RoCE struggles to distribute traffic efficiently across interconnects, leading to severe congestion performance degradation in NCCL collective communication tests. LLM training traffic, such as in DP (Data Parallelism), tends to lack randomness, causing multiple flows to converge on the same interconnect link. In contrast, Adaptive Routing (AR) [34] can significantly enhance network performance by dynamically spraying packets across multiple paths. While static routingbased on manually configured route tablescan avoid link conflicts for specific destinations, it lacks flexibility. For large-scale all-to-all communication, adaptive routing offers superior performance and scalability. (3) Improved Traffic Isolation or Congestion Control Mechanisms: Current RoCE switches support only limited number of priority queues, which are insufficient for complex AI workloads involving concurrent communication patterns such as EPs allto-all and DPs all-reduce. In such mixed workloads, all-to-all traffic can cause incast congestion due to bursty many-to-one transfers, potentially degrading overall network performance. To address incasts influence on other traffic, one approach is to adopt virtual output queuing (VOQ), assigning dedicated virtual queue to each QP to isolate traffic flows. Alternatively, more effective congestion control (CC) mechanisms such as RTT-based CC (RTTCC) or user-programmable CC (PCC) can be employed, enabling NICswitch co-optimization to maintain low latency and high throughput under dynamic traffic conditions. InfiniBand GPUDirect Async (IBGDA). We utilize IBGDA [2, 5.2.3 57] to reduce latency in network communications. Traditionally, network communication involves the creation of CPU proxy thread: once the GPU has prepared the data, it must notify the CPU proxy, which then populates the control information for the Figure 6: Latency comparison between MPFT and MRFT networks in NCCL all-to-all test under different message sizes, showing that their performance is nearly identical. Figure 7: DeepEP performance on MPFT: The EP dispatch and combine kernel communicates across 16 to 128 GPUs using all-to-all. Each GPU processes 4096 tokens. The observed throughput nearly saturates the 400Gps NIC bandwidth. workloads such as distributed training and inference. Although IB has superior latency performance compared to RDMA over Converged Ethernet (RoCE), it comes with certain limitations: Cost: IB hardware is significantly more expensive than RoCE solutions, which limits its widespread adoption. Scalability: IB switches typically support only 64 ports per switch, compared to the 128 ports commonly found in RoCE switches. This restricts the scalability of IB-based clusters, particularly for large-scale deployments. 5.2.2 Recommendations for RoCE Improvements. While RoCE has the potential to be cost-effective alternative to IB, its current limitations in latency and scalability prevent it from fully meeting the demands of large-scale AI systems. Below, we outline specific recommendations for improving RoCE: (1) Specialized Low-Latency RoCE Switches: We recommend that Ethernet vendors develop RoCE switches specifically optimized for RDMA workloads by removing unnecessary Ethernet features. The Slingshot architecture [22] exemplifies how Ethernet-based designs can achieve latency performance comparable to IB. Similarly, recent innovations from Broadcom [13], including the AI Forwarding Header (AIFH) and upcoming lowlatency Ethernet switches, demonstrate the feasibility of highperformance Ethernet fabrics tailored for AI. We are looking forward to continuing innovation in this direction. 10 Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures ISCA 25, June 2125, 2025, Tokyo, Japan Single Hardware Failures: Node crashes, GPU failures, or ECC (Error-Correcting Code) memory errors can compromise longrunning training jobs, often requiring costly restarts. The impact of such failures escalates in large-scale deployments, where the probability of single-point failure increases proportionally with system size. Silent Data Corruption: Errors undetected by ECC mechanisms, such as multi-bit memory flips or computational inaccuracies, pose significant risk to model quality. These errors are particularly insidious in long-running tasks, as they can propagate undetected and corrupt downstream computations. Current mitigation strategies rely on application-level heuristics, which are insufficient for ensuring system-wide robustness. Suggestions for Advanced Error Detection and Correction. To 6.1.2 mitigate risks associated with silent corruption, hardware must incorporate advanced error detection mechanisms beyond traditional ECC. Techniques such as checksum-based validation or hardwareaccelerated redundancy checks can provide higher reliability for large-scale deployments. Furthermore, hardware vendors should deliver comprehensive diagnostic toolkits to end users, empowering them to rigorously verify the integrity of their systems and proactively identify any latent silent data corruption. Such toolkits, when embedded as part of the standard hardware package, foster transparency and enable continuous validation throughout the operational lifecycle, thereby bolstering overall system trustworthiness."
        },
        {
            "title": "6.2 CPU Bottlenecks and Interconnects\nWhile accelerator design often takes center stage, CPUs remain\nessential for coordinating computation, managing I/O, and sustain-\ning system throughput. However, current architectures face several\ncritical bottlenecks:",
            "content": "First, as discussed in Section 4.5, the PCIe interface between CPUs and GPUs often becomes bandwidth bottleneck, particularly during large-scale parameter, gradient, or KV cache transfers. To mitigate this, future systems should adopt direct CPUGPU interconnectssuch as NVLink or Infinity Fabricor integrate both CPUs and GPUs into the scale-up domain, thereby eliminating intra-node bottlenecks. In addition to PCIe limitations, sustaining such high data transfer rates also requires exceptionally high memory bandwidth. For example, saturating 160 lanes of PCIe 5.0 demands over 640 GB/s per node, translating to memory bandwidth requirement of approximately 1 TB/s per nodeposing significant challenge for conventional DRAM architectures. Lastly, latency-sensitive tasks such as kernel launches and network processing demand high single-core CPU performance, typically requiring base frequencies above 4 GHz. Furthermore, modern AI workloads require sufficient CPU cores per GPU to prevent control-side bottlenecks. For chiplet-based architectures, additional cores are needed to support cache-aware workload partitioning and isolation."
        },
        {
            "title": "6.3 Toward Intelligent Networks for AI\nTo meet the demands of latency-sensitive workloads, future inter-\nconnects must prioritize both low latency and intelligent networks:",
            "content": "Figure 8: RoCE network bandwidth of AllGather and ReduceScatter communication primitives under different routing methods (ECMP, AR, Static Routing) and TP dimensions. work request (WR) and signals the NIC via doorbell mechanism to initiate data transmission. This process introduces additional communication overhead. IBGDA addresses this issue by allowing the GPU to directly fill the WR content and write to the RDMA doorbell MMIO address. By managing the entire control plane within the GPU, IBGDA eliminates the significant latency overhead associated with GPU-CPU communication. Moreover, when sending large number of small packets, the control plane processor can easily become bottleneck. Since GPUs have multiple parallel threads, the sender can leverage these threads to distribute the workload, thereby avoiding such bottlenecks. range of worksincluding our DeepEP [78]have leveraged IBGDA and reported substantial performance gains [1, 15, 79]. We therefore advocate for such capabilities to be widely supported across accelerator devices."
        },
        {
            "title": "Architecture Design",
            "content": "Building on the previous sections, we summarize key architectural insights and outline future directions for hardware design tailored to large-scale AI workloads. Section 2.3.2 highlighted the importance of large-scale scale-up networks for accelerating model inference. Section 3 discussed the necessity of efficient support for low-precision computation and communication. Section 4 explored the convergence of scale-up and scale-out architectures, along with several proposed enhancements. Section 5 focused on multi-plane network topologies and identified key improvements needed for Ethernet-based interconnects. Together, these sections identify hardware limitations in concrete application contexts and offer corresponding suggestions. Building on that foundation, this section expands the discussion to broader considerations and proposes forward-looking directions for future hardware architecture design."
        },
        {
            "title": "6.1 Robustness Challenges\n6.1.1 Limitations:\n• Interconnect Failures: High-performance interconnects (e.g.,\nIB and NVLink) are prone to intermittent disconnections, which\ncan disrupt node-to-node communication. This is especially\nharmful in communication-heavy workloads like EP, where even\nbrief interruptions may lead to significant performance drops or\njob failures.",
            "content": "11 ISCA 25, June 2125, 2025, Tokyo, Japan DeepSeek-AI Co-Packaged Optics: Incorporating silicon photonics enables scalable higher bandwidth scalability and enhanced energy efficiency, both are critical for large-scale distributed systems. Lossless Network: Credit-Based Flow Control (CBFC) mechanisms ensures lossless data transmission, yet naively triggering flow control can induce severe head-of-line blocking. Therefore, it is imperative to deploy advanced, endpoint-driven congestion control (CC) algorithms that proactively regulate injection rates and avert pathological congestion scenarios. Adaptive Routing: As underscored in Section 5.2.2, future network should standardize the adoption of dynamic routing schemessuch as packet spraying and congestion-aware path selectionthat continuously monitor real-time network conditions and intelligently redistribute traffic. These adaptive strategies are particularly effective in alleviating hotspots and mitigating bottlenecks during collective communication workloads, including all-to-all and reduce-scatter operations. Efficient Fault-Tolerant Protocols: Robustness against failures can be significantly enhanced through the deployment of self-healing protocols, redundant ports, and rapid failover techniques. For instance, link-layer retry mechanisms and selective retransmission protocols prove indispensable in scaling reliability across large networks, minimizing downtime and ensuring seamless operation despite intermittent failures. Dynamic Resource Management: To handle mixed workloads effectively, future hardware should enable dynamic bandwidth allocation and traffic prioritization. For example, inference tasks should be isolated from training traffic in unified clusters, ensuring responsiveness for latency-sensitive applications."
        },
        {
            "title": "Communication and Ordering Issue",
            "content": "Inter-node communication using load/store memory semantics is efficient and programmer-friendly, but current implementations are hampered by memory ordering challenges. For example, after writing data, the sender must issue an explicit memory barrier (fence) before updating flag to notify the receiver, ensuring data consistency. This strict ordering introduces additional round-trip time (RTT) latency and can stall the issuing thread, impeding inflight stores and reducing throughput. Similar out-of-order synchronization issues arise in message-semantic RDMA; for instance, performing RDMA atomic add operations with packet spraying after regular RDMA writes on InfiniBand or NVIDIA BlueField-3 can incur additional RTT latency. To address these, we advocate for hardware support that offers built-in ordering guarantees for memory-semantic communication. Such consistency should be enforced both at the programming level (e.g., via acquire/release semantics) and by hardware at the receiver, enabling in-order delivery without added overhead. Several approaches are possible. For instance, the receiver could buffer atomic messages and use packet sequence numbers for inorder processing. However, an acquire/release mechanism is both more elegant and efficient. We suggest simple conceptual mechanism, Region Acquire/Release (RAR) mechanism, wherein receiver hardware maintains bitmap to track the state of the RNR memory region, and acquire/release operations are scoped to the 12 RAR address range. With minimal bitmap overhead, this enables efficient, hardware-enforced ordering, eliminating explicit senderside fences and delegating ordering to hardwareideally on the NIC or I/O die. Importantly, the RAR mechanism benefits not only memory-semantic operations but also message-semantic RDMA primitives, thus broadening its practical applicability."
        },
        {
            "title": "6.5 In-Network Computation and Compression\nEP involves two critical all-to-all stages—dispatch and com-\nbine—that present significant opportunities for in-network opti-\nmization. The dispatch stage resembles a small-scale multicast\noperation, where a single message must be forwarded to multi-\nple target devices. A hardware-level protocol enabling automatic\npacket replication and forwarding to multiple destinations could\ndrastically reduce communication overhead and improve efficiency.\nThe combine stage, acting as a small-scale reduction operation,\ncould benefit from in-network aggregation techniques. However,\ndue to the small reduction scope and imbalanced workload in EP\ncombine, implementing in-network aggregation in a flexible man-\nner is challenging.",
            "content": "Moreover, as highlighted in Section 3.2, LogFMT enables lowprecision token transmission with minimal impact on model performance. Incorporating LogFMT natively within network hardware could further optimize communication by increasing entropy density and reducing bandwidth usage. Hardware-accelerated compression and decompression would allow seamless integration of LogFMT into distributed systems, enhancing overall throughput."
        },
        {
            "title": "6.6 Memory-Centric Innovations\n6.6.1 Limitations of Memory Bandwidth. The exponential growth\nin model sizes has outpaced advancements in high-bandwidth mem-\nory (HBM) technology. This disparity creates a memory bottleneck,\nparticularly in attention-heavy architectures like Transformers.\n6.6.2",
            "content": "Suggestions: DRAM-Stacked Accelerators: Leveraging advanced 3D stacking technologies, DRAM dies can be vertically integrated atop logic die, thereby enabling exceptionally high memory bandwidth, ultra-low latency, and practical memory capacity (though stack-limited). This architectural paradigm proves remarkably advantageous for ultra-fast inference in MoE models, where memory throughput is critical bottleneck. Architectures such as SeDRAM[72] exemplify the potential of this approach, delivering unprecedented performance for memory-bound workloads. System-on-Wafer (SoW): Wafer-scale integration [50] can maximize computational density and memory bandwidth, addressing the needs of ultra-large-scale models."
        },
        {
            "title": "7 Conclusion\nDeepSeek-V3 exemplifies the transformative potential of hardware-\nsoftware co-design in advancing the scalability, efficiency, and ro-\nbustness of large-scale AI systems. By addressing the limitations of\ncurrent hardware architectures and proposing actionable recom-\nmendations, this paper provides a roadmap for the next generation\nof AI-optimized hardware. These innovations will be critical as AI\nworkloads continue to grow in complexity and scale, driving the\nfuture of intelligent systems.",
            "content": "Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures ISCA 25, June 2125, 2025, Tokyo, Japan References [1] Elena Agostini, Davide Rossetti, and Sreeram Potluri. 2017. Offloading Communication Control Logic in GPU Accelerated Applications. In 2017 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID). 248 257. https://doi.org/10.1109/CCGRID.2017.29 [2] E. Agostini, D. Rossetti, and S. Potluri. 2018. GPUDirect Async: Exploring GPU synchronous communication techniques for InfiniBand clusters. J. Parallel and Distrib. Comput. 114 (2018), 2845. https://doi.org/10.1016/j.jpdc.2017.12.007 [3] AI@Meta. 2024. Llama 3 Model Card. https://github.com/meta-llama/llama3/ blob/main/MODEL_CARD.md [4] AI@Meta. 2024. Llama 3.1 Model Card. https://github.com/meta-llama/llamamodels/blob/main/models/llama3_1/MODEL_CARD.md [5] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. 2023. GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. arXiv preprint arXiv:2305.13245 (2023). [6] AMD. 2025. AMD Ryzen AI Max+ PRO 395: Designed to power https: new generation of compact Copilot+ PC workstations. //www.amd.com/en/products/processors/laptop/ryzen-pro/ai-max-pro300-series/amd-ryzen-ai-max-plus-pro-395.html [7] Wei An, Xiao Bi, Guanting Chen, Shanhuang Chen, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Wenjun Gao, Kang Guan, Jianzhong Guo, Yongqiang Guo, Zhe Fu, Ying He, Panpan Huang, Jiashi Li, Wenfeng Liang, Xiaodong Liu, Xin Liu, Yiyuan Liu, Yuxuan Liu, Shanghao Lu, Xuan Lu, Xiaotao Nie, Tian Pei, Junjie Qiu, Hui Qu, Zehui Ren, Zhangli Sha, Xuecheng Su, Xiaowen Sun, Yixuan Tan, Minghui Tang, Shiyu Wang, Yaohui Wang, Yongji Wang, Ziwei Xie, Yiliang Xiong, Yanhong Xu, Shengfeng Ye, Shuiping Yu, Yukun Zha, Liyue Zhang, Haowei Zhang, Mingchuan Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, and Yuheng Zou. 2024. Fire-Flyer AIHPC: Cost-Effective Software-Hardware Co-Design for Deep Learning. In SC24: International Conference for High Performance Computing, Networking, Storage and Analysis. 123. https://doi.org/10.1109/SC41406.2024.00089 [8] Anthropic. 2024. Claude 3.5 Sonnet. https://www.anthropic.com/news/claude3-5-sonnet [9] Anthropic. 2025. Claude 3.7 Sonnet and Claude Code. https:// www.anthropic.com/news/claude-3-7-sonnet [10] Apple. 2024. Apple introduces M4 Pro and M4 Max. https://www.apple.com/ newsroom/2024/10/apple-introduces-m4-pro-and-m4-max/ [11] Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The LongDocument Transformer. arXiv:2004.05150 (2020). [12] Nils Blach, Maciej Besta, Daniele De Sensi, Jens Domke, Hussein Harake, Shigang Li, Patrick Iff, Marek Konieczny, Kartik Lakhotia, Ales Kubicek, Marcel Ferrari, Fabrizio Petrini, and Torsten Hoefler. 2025. high-performance design, implementation, deployment, and evaluation of the slim fly network. In Proceedings of the 21st USENIX Symposium on Networked Systems Design and Implementation (Santa Clara, CA, USA) (NSDI24). USENIX Association, USA, Article 57, 20 pages. [13] Broadcom. 2025. Scale Up Ethernet Framework. https://docs.broadcom.com/ doc/scale-up-ethernet-framework [14] Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, and Tri Dao. 2024. Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. https: //openreview.net/forum?id=PEpbUobfJv [15] Shaoyuan Chen, Wencong Xiao, Yutong Lin, Mingxing Zhang, Yingdi Shan, Jinlei Jiang, Kang Chen, and Yongwei Wu. 2025. Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation. arXiv:2405.01814 [cs.LG] https://arxiv.org/abs/2405. [16] ULTRA ACCELERATOR LINK CONSORTIUM. 2025. Introducing UALink 200G 1.0 Specification. https://ualinkconsortium.org/wp-content/uploads/2025/04/ UALink-1.0-White_Paper_FINAL.pdf [17] Ultra Ethernet Consortium. 2023. Overview of and Motivation for the Forthcoming Ultra Ethernet Consortium Specification. https://ultraethernet.org/wpcontent/uploads/sites/20/2023/10/23.07.12-UEC-1.0-Overview-FINAL-WITHLOGO.pdf [18] Ultra Ethernet Consortium. 2024. UEC Progresses Towards v1.0 Set of Spechttps://ultraethernet.org/uec-progresses-towards-v1-0-set-ofifications. specifications/ [19] Tri Dao. 2023. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. [20] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. In Advances in Neural Information Processing Systems. [21] Tri Dao and Albert Gu. 2024. Transformers are SSMs: generalized models and efficient algorithms through structured state space duality. In Proceedings of the 41st International Conference on Machine Learning (Vienna, Austria) (ICML24). JMLR.org, Article 399, 31 pages. [22] Daniele De Sensi, Salvatore Di Girolamo, Kim H. McMahon, Duncan Roweth, and Torsten Hoefler. 2020. An In-Depth Analysis of the Slingshot Interconnect. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. 114. https://doi.org/10.1109/SC41405.2020.00039 [23] DeepSeek-AI. 2024. DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source https://doi.org/ Models in Code Intelligence. CoRR abs/2406.11931 (2024). 10.48550/arXiv.2406.11931 [24] DeepSeek-AI. 2024. DeepSeek LLM: Scaling Open-Source Language Models https://doi.org/10.48550/ with Longtermism. CoRR abs/2401.02954 (2024). arXiv.2401. [25] DeepSeek-AI. 2024. DeepSeek-V2: Strong, Economical, and Efficient Mixture-ofExperts Language Model. CoRR abs/2405.04434 (2024). https://doi.org/10.48550/ arXiv.2405.04434 [26] DeepSeek-AI. 2024. DeepSeek-V3 Technical Report. arXiv:2412.19437 [cs.CL] https://arxiv.org/abs/2412.19437 (2024). [27] DeepSeek-AI. 2024. DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models. CoRR abs/2401.06066 (2024). https: //doi.org/10.48550/arXiv.2401.06066 [28] DeepSeek-AI. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948 [cs.CL] https://arxiv.org/abs/ 2501.12948 [29] DeepSeek-AI. 2025. DualPipe: bidirectional pipeline parallelism algorithm for computation-communication overlap in V3/R1 training. https://github.com/ deepseek-ai/dualpipe. [30] DeepSeek-AI. 2025. Fire-Flyer File System. https://github.com/deepseek-ai/3FS https://github.com/ [31] DeepSeek-AI. 2025. Profiling Data in DeepSeek Infra. deepseek-ai/profile-data?tab=readme-ov-file#inference [32] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2022. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323 (2022). [33] Adithya Gangidi, Rui Miao, Shengbao Zheng, Sai Jayesh Bondu, Guilherme Goes, Hany Morsy, Rohit Puri, Mohammad Riftadi, Ashmitha Jeevaraj Shetty, Jingyi Yang, Shuqiang Zhang, Mikel Jimenez Fernandez, Shashidhar Gandham, and Hongyi Zeng. 2024. RDMA over Ethernet for Distributed Training at Meta Scale. In Proceedings of the ACM SIGCOMM 2024 Conference (Sydney, NSW, Australia) (ACM SIGCOMM 24). Association for Computing Machinery, New York, NY, USA, 5770. https://doi.org/10.1145/3651890. [34] Patrick Geoffray and Torsten Hoefler. 2008. Adaptive Routing Strategies for Modern High Performance Networks. In 2008 16th IEEE Symposium on High Performance Interconnects. 165172. https://doi.org/10.1109/HOTI.2008.21 [35] Amir Gholami, Zhewei Yao, Sehoon Kim, Coleman Hooper, Michael W. Mahoney, and Kurt Keutzer. 2024. AI and Memory Wall . IEEE Micro 44, 03 (May 2024), 3339. https://doi.org/10.1109/MM.2024.3373763 [36] Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozière, David Lopez-Paz, and Gabriel Synnaeve. 2024. Better & Faster Large Language Models via Multi-token Prediction. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. https://openreview.net/ forum?id=pEWAcejiU2 [37] Google. 2024. Introducing Gemini 2.0: our new AI model for the agentic era. https://blog.google/technology/google-deepmind/google-gemini-aiupdate-december-2024 [38] Google. 2025. Gemini 2.5: Our most intelligent AI model. https://blog.google/ technology/google-deepmind/gemini-model-thinking-updates-march-2025/ [39] MADSys group and Approaching.AI. 2025. Flexible Framework for Experiencing Cutting-edge LLM Inference Optimizations. https://github.com/kvcacheai/ktransformers [40] Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael Mahoney, Yakun Sophia Shao, Kurt Keutzer, and Amir Gholami. 2024. KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization. arXiv preprint arXiv:2401.18079 (2024). [41] Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7B. arXiv preprint arXiv:2310.06825 (2023). [42] Ziheng Jiang, Haibin Lin, Yinmin Zhong, Qi Huang, Yangrui Chen, Zhi Zhang, Yanghua Peng, Xiang Li, Cong Xie, Shibiao Nong, Yulu Jia, Sun He, Hongmin Chen, Zhihao Bai, Qi Hou, Shipeng Yan, Ding Zhou, Yiyao Sheng, Zhuo Jiang, Haohan Xu, Haoran Wei, Zhang Zhang, Pengfei Nie, Leqi Zou, Sida Zhao, Liang Xiang, Zherui Liu, Zhe Li, Xiaoying Jia, Jianxi Ye, Xin Jin, and Xin Liu. 2024. MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs. http://arxiv.org/abs/2402.15627 arXiv:2402.15627 [cs]. [43] Norm Jouppi, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan, Lifeng Nai, Nishant Patil, Suvinay Subramanian, Andy Swing, Brian Towles, Clifford Young, Xiang Zhou, Zongwei Zhou, and David Patterson. 2023. TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings. In Proceedings of the 50th Annual International Symposium on Computer Architecture (Orlando, FL, USA) (ISCA 23). Association for Computing Machinery, New York, NY, USA, Article 82, 14 pages. https://doi.org/10.1145/ 13 ISCA 25, June 2125, 2025, Tokyo, Japan DeepSeek-AI 3579371.3589350 [44] Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar Krishna, and Tuo Zhao. 2024. GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM. arXiv:2403.05527 [cs.LG] [45] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling Laws for Neural Language Models. CoRR abs/2001.08361 (2020). arXiv:2001.08361 https://arxiv.org/abs/2001.08361 [46] John Kim, Wiliam J. Dally, Steve Scott, and Dennis Abts. 2008. TechnologyDriven, Highly-Scalable Dragonfly Topology. In 2008 International Symposium on Computer Architecture. 7788. https://doi.org/10.1109/ISCA.2008.19 [47] Vijay Anand Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad Shoeybi, and Bryan Catanzaro. 2023. Reducing activation recomputation in large transformer models. Proceedings of Machine Learning and Systems 5 (2023). [48] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. 2024. EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. https://openreview.net/forum?id=1NdN7eXyb4 [49] Heng Liao, Bingyang Liu, Xianping Chen, Zhigang Guo, Chuanning Cheng, Jianbing Wang, Xiangyu Chen, Peng Dong, Rui Meng, Wenjie Liu, Zhe Zhou, Ziyang Zhang, Yuhang Gai, Cunle Qian, Yi Xiong, Zhongwu Cheng, Jing Xia, Yuli Ma, Xi Chen, Wenhua Du, Shizhong Xiao, Chungang Li, Yong Qin, Liudong Xiong, Zhou Yu, Lv Chen, Lei Chen, Buyun Wang, Pei Wu, Junen Gao, Xiaochu Li, Jian He, Shizhuan Yan, and Bill McColl. 2025. UB-Mesh: Hierarchically Localized nD-FullMesh Datacenter Network Architecture. arXiv:2503.20377 [cs.AR] https: //arxiv.org/abs/2503. [50] Sean Lie. 2022. Cerebras Architecture Deep Dive: First Look Inside the HW/SW Co-Design for Deep Learning : Cerebras Systems. In 2022 IEEE Hot Chips 34 Symposium (HCS). 134. https://doi.org/10.1109/HCS55958.2022.9895479 [51] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. 2024. AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration. In MLSys. [52] Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. 2024. KIVI: Tuning-Free Asymmetric 2bit Quantization for KV Cache. arXiv preprint arXiv:2402.02750 (2024). [53] Junyu Luo, Weizhi Zhang, Ye Yuan, Yusheng Zhao, Junwei Yang, Yiyang Gu, Bohan Wu, Binqi Chen, Ziyue Qiao, Qingqing Long, Rongcheng Tu, Xiao Luo, Wei Ju, Zhiping Xiao, Yifan Wang, Meng Xiao, Chenwu Liu, Jingyang Yuan, Shichang Zhang, Yiqiao Jin, Fan Zhang, Xian Wu, Hanqing Zhao, Dacheng Tao, Philip S. Yu, and Ming Zhang. 2025. Large Language Model Agent: Survey on Methodology, Applications and Challenges. arXiv preprint arXiv:2503.21460 (2025). [54] Karthik Mandakolathur and Sylvain Jeaugey. 2022. Doubling all2all Performance with NVIDIA Collective Communication Library 2.12. https://developer.nvidia.com/blog/doubling-all2all-performance-withnvidia-collective-communication-library-2-12/ [55] Mistral. 2024. Cheaper, Better, Faster, Stronger: Continuing to push the frontier of AI and making it accessible to all. https://mistral.ai/news/mixtral-8x22b [56] Dheevatsa Mudigere, Yuchen Hao, Jianyu Huang, Zhihao Jia, Andrew Tulloch, Srinivas Sridharan, Xing Liu, Mustafa Ozdal, Jade Nie, Jongsoo Park, Liang Luo, Jie Amy Yang, Leon Gao, Dmytro Ivchenko, Aarti Basant, Yuxi Hu, Jiyan Yang, Ehsan K. Ardestani, Xiaodong Wang, Rakesh Komuravelli, Ching-Hsiang Chu, Serhat Yilmaz, Huayu Li, Jiyuan Qian, Zhuobo Feng, Yinbin Ma, Junjie Yang, Ellie Wen, Hong Li, Lin Yang, Chonglin Sun, Whitney Zhao, Dimitry Melts, Krishna Dhulipala, K. R. Kishore, Tyler Graf, Assaf Eisenman, Kiran Kumar Matam, Adi Gangidi, Guoqiang Jerry Chen, Manoj Krishnan, Avinash Nayak, Krishnakumar Nair, Bharath Muthiah, Mahmoud khorashadi, Pallab Bhattacharya, Petr Lapukhov, Maxim Naumov, Ajit Mathews, Lin Qiao, Mikhail Smelyanskiy, Bill Jia, and Vijay Rao. 2023. Software-Hardware Co-design for Fast and Scalable Training of Deep Learning Recommendation Models. http://arxiv.org/abs/2104.05158 arXiv:2104.05158 [cs]. [57] NVIDIA. 2022. Improving Network Performance of HPC Systems Using NVIDIA Magnum IO NVSHMEM and GPUDirect Async. https://developer.nvidia.com/blog/improving-network-performance-ofhpc-systems-using-nvidia-magnum-io-nvshmem-and-gpudirect-async/ [58] NVIDIA. 2025. NVIDIA DGX Spark: Grace Blackwell AI supercomputer on your desk. https://www.nvidia.com/en-us/products/workstations/dgx-spark/ [59] OpenAI. 2024. Hello GPT-4o. https://openai.com/index/hello-gpt-4o/ [60] OpenAI. 2024. Introducing OpenAI o1. https://openai.com/o1/ [61] OpenAI. 2025. Introducing OpenAI o3 and o4-mini. https://openai.com/index/ introducing-o3-and-o4-mini/. [62] Kun Qian, Yongqing Xi, Jiamin Cao, Jiaqi Gao, Yichi Xu, Yu Guan, Binzhang Fu, Xuemei Shi, Fangbo Zhu, Rui Miao, Chao Wang, Peng Wang, Pengcheng Zhang, Xianlong Zeng, Eddie Ruan, Zhiping Yao, Ennan Zhai, and Dennis Cai. 2024. Alibaba HPN: Data Center Network for Large Language Model Training. In Proceedings of the ACM SIGCOMM 2024 Conference (Sydney, NSW, Australia) (ACM SIGCOMM 24). Association for Computing Machinery, New York, NY, USA, 691706. https://doi.org/10.1145/3651890.3672265 [63] Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, and Yiran Zhong. 2024. Various lengths, constant speed: efficient language modeling with lightning attention. In Proceedings of the 41st International Conference on Machine Learning (Vienna, Austria) (ICML24). JMLR.org, Article 1688, 19 pages. [64] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. 2024. Direct Preference Optimization: Your Language Model is Secretly Reward Model. arXiv:2305.18290 [cs.LG] https://arxiv.org/ abs/2305.18290 [65] Md Shafayat Rahman, Saptarshi Bhowmik, Yevgeniy Ryasnianskiy, Xin Yuan, and Michael Lang. 2019. Topology-custom UGAL routing on dragonfly. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (Denver, Colorado) (SC 19). Association for Computing Machinery, New York, NY, USA, Article 17, 15 pages. https://doi.org/10.1145/ 3295500.3356208 [66] Bita Darvish Rouhani, Ritchie Zhao, Ankit More, Mathew Hall, Alireza Khodamoradi, Summer Deng, Dhruv Choudhary, Marius Cornea, Eric Dellinger, Kristof Denolf, Stosic Dusan, Venmugil Elango, Maximilian Golub, Alexander Heinecke, Phil James-Roxby, Dharmesh Jani, Gaurav Kolhe, Martin Langhammer, Ada Li, Levi Melnick, Maral Mesmakhosroshahi, Andres Rodriguez, Michael Schulte, Rasoul Shafipour, Lei Shao, Michael Siu, Pradeep Dubey, Paulius Micikevicius, Maxim Naumov, Colin Verrilli, Ralph Wittig, Doug Burger, and Eric Chung. 2023. Microscaling Data Formats for Deep Learning. arXiv:2310.10537 [cs.LG] https://arxiv.org/abs/2310.10537 [67] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. arXiv:1707.06347 [cs.LG] 2017. Proximal Policy Optimization Algorithms. https://arxiv.org/abs/1707.06347 [68] ByteDance Seed. 2025. Seed1.5-Thinking: Advancing Superb Reasoning Models with Reinforcement Learning. arXiv:2504.13914 [cs.CL] https://arxiv.org/abs/ 2504.13914 [69] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. arXiv:2402.03300 [cs.CL] https://arxiv.org/abs/2402.03300 [70] Noam Shazeer. 2019. Fast Transformer Decoding: One Write-Head is All You Need. CoRR abs/1911.02150 (2019). http://arxiv.org/abs/1911.02150 [71] Qwen Team. 2025. Qwen3: Think Deeper, Act Faster. QwenLM/Qwen3 https://github.com/ [72] Song Wang, Bing Yu, Wenwu Xiao, Fujun Bai, Xiaodong Long, Liang Bai, Xuerong Jia, Fengguo Zuo, Jie Tan, Yixin Guo, Peng Sun, Jun Zhou, Qiong Zhan, Sheng Hu, Yu Zhou, Yi Kang, Qiwei Ren, and Xiping Jiang. 2023. 135 GBps/Gbit 0.66 pJ/bit Stacked Embedded DRAM with Multilayer Arrays by Fine Pitch Hybrid Bonding and Mini-TSV. In 2023 IEEE Symposium on VLSI Technology https://doi.org/10.23919/ and Circuits (VLSI Technology and Circuits). 12. VLSITechnologyandCir57934.2023.10185427 [73] xAI. 2024. Grok-2 Beta Release. https://x.ai/news/grok-2. [74] xAI. 2024. Our Gigafactory of Compute:Colossus. https://x.ai/colossus. [75] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2024. Qwen2.5 Technical Report. arXiv preprint arXiv:2412.15115 (2024). [76] Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Y. X. Wei, Lean Wang, Zhiping Xiao, Yuqing Wang, Chong Ruan, Ming Zhang, Wenfeng Liang, and Wangding Zeng. 2025. Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention. https: //arxiv.org/abs/2502.11089 [77] Chenggang Zhao, Liang Zhao, Jiashi Li, and Zhean Xu. 2025. DeepGEMM: clean and efficient FP8 GEMM kernels with fine-grained scaling. https://github.com/ deepseek-ai/DeepGEMM. [78] Chenggang Zhao, Shangyan Zhou, Liyue Zhang, Chengqi Deng, Zhean Xu, Yuxuan Liu, Kuai Yu, Jiashi Li, and Liang Zhao. 2025. DeepEP: an efficient expert-parallel communication library. https://github.com/deepseek-ai/DeepEP. [79] Size Zheng, Jin Fang, Xuegui Zheng, Qi Hou, Wenlei Bao, Ningxin Zheng, Ziheng Jiang, Dongyang Wang, Jianxi Ye, Haibin Lin, Li-Wen Chang, and Xin Liu. 2025. TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives. arXiv:2503.20313 [cs.DC] https://arxiv.org/abs/ 2503.20313 [80] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. 2024. DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving. In 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24). USENIX Association, Santa Clara, CA, 193210. https://www.usenix.org/conference/osdi24/ presentation/zhong-yinmin"
        }
    ],
    "affiliations": [
        "DeepSeek-AI Beijing, China"
    ]
}