{
    "paper_title": "TextCrafter: Accurately Rendering Multiple Texts in Complex Visual Scenes",
    "authors": [
        "Nikai Du",
        "Zhennan Chen",
        "Zhizhou Chen",
        "Shan Gao",
        "Xi Chen",
        "Zhengkai Jiang",
        "Jian Yang",
        "Ying Tai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper explores the task of Complex Visual Text Generation (CVTG), which centers on generating intricate textual content distributed across diverse regions within visual images. In CVTG, image generation models often rendering distorted and blurred visual text or missing some visual text. To tackle these challenges, we propose TextCrafter, a novel multi-visual text rendering method. TextCrafter employs a progressive strategy to decompose complex visual text into distinct components while ensuring robust alignment between textual content and its visual carrier. Additionally, it incorporates a token focus enhancement mechanism to amplify the prominence of visual text during the generation process. TextCrafter effectively addresses key challenges in CVTG tasks, such as text confusion, omissions, and blurriness. Moreover, we present a new benchmark dataset, CVTG-2K, tailored to rigorously evaluate the performance of generative models on CVTG tasks. Extensive experiments demonstrate that our method surpasses state-of-the-art approaches."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 1 6 4 3 2 . 3 0 5 2 : r TextCrafter: Accurately Rendering Multiple Texts in Complex Visual Scenes Nikai Du1, , Zhennan Chen1, , Zhizhou Chen1 , Shan Gao2 , Xi Chen2 , Zhengkai Jiang3 , Jian Yang1 and Ying Tai1, *: Equal Contribution. 1Nanjing University 2China Mobile 3The Hong Kong University of Science and Technology 502024710004@smail.nju.edu.cn Figure 1: TextCrafter enables precise multi-region visual text rendering, addressing the challenges of long, small-size,various numbers, symbols and styles in visual text generation. We illustrate the comparisons among TextCrafter with three state-of-the-art models, i.e., FLUX, TextDiffuser-2 and 3DIS. Abstract This paper explores the task of Complex Visual Text Generation (CVTG), which centers on generating intricate textual content distributed across diverse regions within visual images. In CVTG, image generation models often rendering distorted and blurred visual text or missing some visual To tackle these challenges, we propose text. TextCrafter, novel multi-visual text rendering method. TextCrafter employs progressive strategy to decompose complex visual text into distinct components while ensuring robust alignment between textual content and its visual carrier. Additionally, it incorporates token focus enhancement mechanism to amplify the prominence of visual text during the generation process. TextCrafter effectively addresses key challenges in CVTG tasks, such as text confusion, omissions, and blurriness. Moreover, we present new benchmark dataset, CVTG-2K, tailored to rigorously evaluate the performance of generative models on CVTG tasks. Extensive experiments demonstrate that our method surpasses state-of-the-art approaches."
        },
        {
            "title": "1\nIn recent years, diffusion models [Ho et al., 2020; Podell et\nal., 2023; Rombach et al., 2022; Ramesh et al., 2021; Ramesh",
            "content": "et al., 2022; Zhao et al., 2024; Dhariwal and Nichol, 2021; Chen et al., 2023] have emerged as the forefront technology in visual image generation. Recent studies, represented by general-purpose text-to-image models such as Flux [BlackForest, 2024] and SD 3.5 [Esser et al., 2024], have acquired the ability to render simple visual text through large-scale pretraining. However, when confronted with complex realworld visual text scenarios, these models often struggle with critical challenges such as text distortion, omission, and blurriness. These shortcomings significantly hinder their practical applicability in real-world use cases. Achieving precise, legible, and contextually accurate visual text rendering in complex scenes remains essential yet highly challenging. Recent studies in multi-instance control [Feng et al., 2022; Zhou et al., 2024; Chen et al., 2024b; Zhou et al., 2025] primarily focus on generating multiple instances within single image simultaneously, striving to achieve precise regulation over factors such as quantity, position and attributes. However, crucial yet often overlooked category in the visual domain is visual text. Text is an indispensable component of the real world, distinguished by its unparalleled complexity and delicacy compared to conventional objects, flora, or fauna. Unlike these entities, text possesses an exceptionally intricate internal structure, where even minor alterations can cause significant changes in visual appearance, potentially leading to misrecognition or complete illegibility. Existing multi-instance approaches, such as MIGC [Zhou et al., 2024] and 3DIS[Zhou et al., 2025], primarily focus on sentencelevel information and interaction, offering limited understanding of text at finer granularity. Moreover, RPG [Yang et al., 2024a] and RAG-Diffusion [Chen et al., 2024b] adjust instance structures during attention fusion but fall short of the precision and stability needed for visual text rendering. Recent studies on visual text rendering primarily focus on accurately generating single text region. For instance, methods like AnyText [Tuo et al., 2024], Glyph-byT5 [Liu et al., 2025], SceneTextGen [Zhangli et al., 2024], Diff-Text [Zhang et al., 2024b], and TextDiffuser [Chen et al., 2024a] often employ specialized modules to encode visual text representations and condition the diffusion model accordingly. However, these methods face two major limitations: 1) They rely on predefined rules to synthesize visual text training datasets. Due to the need for highly accurate text annotations, manual verification is often required, making the creation of complex visual text datasets both labor-intensive and costly. 2) Most approaches depend on fine-tuned text encoders (e.g., Glyphby-T5) or trained conditional control encoders (e.g., AnyText, Diff-Text, TextDiffuser, SceneTextGen) to facilitate text generation. In complex multi-text generation, however, these approaches often cause interference among the control features of different targets, making it difficult to balance fine-grained local control with global consistency across multiple texts. This paper explores the task of Complex Visual Text Generation (CVTG), which aims to generate intricate text content distributed across different regions. To tackle the challenges of CVTG, we propose TextCrafter, novel trainingfree framework for rendering multiple texts in complex visual scenes. TextCrafter enhances multi-text control, ensuring accurate text presentation and logical layout. Our framework comprises three key stages: 1) Instance Fusion: This stage strengthens the connection between visual text and its corresponding carrier, ensuring strong consistency between the text content and its surrounding environment. It effectively prevents text from appearing in incorrect positions. 2) Region Insulation: Leveraging the positional priors of the pretrained DiT model [Peebles and Xie, 2023] (e.g., FLUX), this step initializes the layout information for each text instance while separating and denoising text prompts across different regions. This process prevents early interference between text areas, reducing confusion and the risk of content omission in multi-text scenarios. 3) Text Focus: An attention control mechanism is introduced to enhance the attention maps of visual text, refining the fidelity of text rendering. This stage ensures textual accuracy and addresses issues of blurriness, particularly in smaller text. We present some examples in Figure 1. Our contributions are summarized as follows: We propose TextCrafter, novel training-free framework for rendering multiple texts in complex visual scenes, consisting of three key stages: instance fusion, region insulation, and token focus. We construct new CVTG-2K dataset, which encompasses diverse range of visual text prompts varying in position, quantity, length, and attributes, serving as robust benchmark for evaluating the performance of generation models in CVTG tasks. We conduct extensive quantitative and qualitative experiments on the CVTG-2K benchmark. The results demonstrate that TextCrafter delivers exceptional performance, proving its superior effectiveness and robustness in tackling the CVTG task."
        },
        {
            "title": "2 Related Works\nMulti-instance Compositional Generation. Attend-and-\nExcite [Chefer et al., 2023] and BOX-Diffusion [Xie et\nal., 2023] guide pre-trained diffusion models to align gen-\nerated instances with prompts. GLIGEN [Li et al., 2023],\nMIGC [Zhou et al., 2024], and CreativeLayout [Zhang et al.,\n2024a] enhance control by incorporating bounding box con-\nditions into trainable layers. RPG [Yang et al., 2024a] and\nRAG-Diffusion [Chen et al., 2024b] break the generation pro-\ncess into regional tasks for compositional generation. How-\never, these methods often overlook a critical element: visual\ntext. As a key component of visual scenes, accurate text sig-\nnificantly enhances the realism of generated images.\nVisual Text Generation. DiffSTE [Ji et al., 2023], Glyph-\nByT5 [Liu et al., 2025], and UDiffText [Zhao and Lian, 2025]\nuse character-level encoders to incorporate word appearance\ninto embeddings. TextDiffuser [Chen et al., 2024a] and\nGlyphControl [Yang et al., 2024b] generate text layout masks\nfor injection into the latent space. Approaches like Diff-\nText [Zhang et al., 2024b] improve accuracy with attention\nmap restrictions. GlyphDraw [Ma et al., 2023] uses two mod-\nels for text position prediction and rendering. TextDiffuser-\n2 [Chen et al., 2025] leverages large language models for lay-\nout planning. However, these approaches often optimize text\nrendering for single region or on individual object, leading to\npoor generalization in complex visual text scenes.",
            "content": "Figure 2: Overview of our TextCrafter. TextCrafter consists of three steps. (a) Instance Fusion: Strengthen the connection between visual text and its corresponding carrier. (b) Region Insulation: Leverage the positional priors of the pre-trained DiT model to initialize the layout information for each text instance while separating and denoising text prompts across different regions. (c) Text Focus: Enhance the attention maps of visual text, refing the fidelity of text rendering. Benchmark for Visual Text. Earlier works like GlyphControl introduced SimpleBench and CreativeBench [Yang et al., 2024b], but these fixed-template datasets offer limited diversity and contain only single-word scenes. MARIOEval [Chen et al., 2024a] suffers from low-quality prompts and unclear semantics. DrawTextExt Creative [Ma et al., 2023] collects prompts for generating natural scenes but still lack sufficient coverage. AnyText-benchmark [Tuo et al., 2024] allows multi-word prompts but treats each word separately and appends them directly to the caption, limiting data distribution. Existing visual text benchmarks focus on text generation within single object or location, falling short of capturing the complexity of real-world scenes. Thus, we propose benchmark CVTG-2K to incorporate diverse positions, quantities, lengths, and attributes for comprehensively evaluate models in complex visual text scenarios."
        },
        {
            "title": "3.1 Overview",
            "content": "Problem Definition. In CVTG, the user provides global prompt that contains multiple descriptions of visual texts = {d1, d2, . . . , dn}, where each description includes the main body of the visual text and other descriptors such as the position and attributes of the visual text. The main body of the visual text is defined as = {vt1, vt2, . . . , vtn}, where each vti corresponds to all of its descriptors di. The goal of the model is to generate an image based on the provided prompt , where each visual text vti is presented with its corresponding description di. Figure 3: Illustration of tokenizing the prompt sidewalk poster with Register Now for IJCAI 2025. along with the attention map corresponding to each token. The use of preceding quotation marks can reinforce the relationship between text tokens and carrier tokens. Challenges in CVTG. When handing complex prompts, diffusion models encounter interference among visual texts. This interference leads to several degradation issues. 1) Text confusion: Visual texts (e.g., visual text vti and visual text vtj) intertwine, leading to the generation of erroneous character or word. 2) Text omission: Only visual texts related to description di are generated, neglecting the visual text vtj in description dj. 3) Text blurriness: Visual text in description di is small in scale and fails to draw sufficient attention, causing vti to appear blurred in generated image. Motivation. Current text-to-image models excel at handing individual visual text in visual generation. By breaking down CVTG into simpler sub-tasks, overall complexity is reduced. Solving these sub-tasks separately and combining their soluFigure 4: For pre-trained DiT model, only few denoising steps are required to approximate the layout of the image and the relative positions of the main subjects. After 8 denoising steps, the layout closely resembles that of full 50-step process, with subsequent steps primarily refining image details. Benchmark CreativeBench MARIOEval DrawTextExt AnyText-benchmark Number Word 1.00 2.92 3.75 4.18 400 5414 220 1000 CVTG-2K 2000 8.10 Char 7.29 15.47 17.01 21. 39.47 Attribute size/color/font Region 2/3/4/5 Table 1: Comparison of CVTG-2K with existing public visual text Benchmarks in terms of Number of Samples(Number), Average Word Count (Word), Average Character Count (Char), Attribute, and number of Regions (Region). tions effectively accomplishes the CVTG task. This approach directs the model to progressively shift focus from the entire task to individual sub-tasks and then to specific visual texts, enabling precise rendering. Based on this coarse-to-fine strategy, we present TextCrafter, as shown in Figure 2, with detailed explanations provided in the following sections."
        },
        {
            "title": "3.3 Region Insulation\nPre-generation.\nIn CVTG, multiple texts often influence\neach other’s glyphs and layout [Zheng et al., 2023]. To sim-\nplify, we decouple them into independent instances. Specif-\nically, we assign a rectangular bounding box to each visual\ntext to represent its layout, initialized based on the positional\npreferences of the pre-trained DiT model. This allows us\nto effectively isolate the texts, preventing interference during\ngeneration. Instead of manually specifying layouts or relying\non large language models, we leverage the model’s learned\npositional knowledge [Tumanyan et al., 2023], as enforcing a",
            "content": "zt1, At DiT(zt, c, t) Algorithm 1 TextCrafter image generation Input: The encoded embedding of and is denoted as and {ˆc1, ˆc2, . . . , ˆcn} Parameter: the proportion of the addition λ, the steps of region insulation, the pregeneration steps τ Output: generated latent x0 1: {Step 1 Instance Fusion} 2: = InstanceFusion(c, λ) 3: {Step 2 Region Insulation} 4: zt (0, 1) 5: for = τ, τ 1, . . . , 1 do 6: 7: end for 8: for = 1, 2, . . . , do 9: pi (0, 0) 10: pmax,i arg maxpi At(pi, i) 11: end for 12: bbx1, bbx2, . . . , bbxn MILP(pmax,1, pmax,2, . . . , pmax,n) 13: xT (0, 1) 14: for = 1, 2, . . . , do 15: ˆxi 16: end for 17: for = T, 1, . . . , + 1 do 18: xt1 xt ϵθ(xt, c) 19: for = 1, 2, . . . , do 20: ˆxi t, ˆci) t1 ˆxi 21: xt1 Reinsert(xt1, ˆxi 22: 23: end for 24: {Step 3 Text Focus} 25: for = r, 1, . . . , 1 do 26: 27: 28: 29: 30: 31: 32: end for 33: return x0 for AttentionLayeri ϵθ do Ki Mt = Qi Mt Focus(Mt) Attentioni end for xt1 xt ϵθ(xt, c) Initialize(N (0, 1), bbxi) softmax(Mt/ ϵθ(ˆxi t1, bbxi) d) Vi end for fixed layout may compromise quality. As shown in Figure 4, during the early denoising steps, we capture attention maps and identify the points of maximum attention for each visual text. This pre-generation process guides layout initialization. Assume that the attention map obtained during this process is At, where represents the current step, and stands for pixel: pmax = arg max At(p). (1) Layout Optimizer. Next, the rectangular bounding boxes for each visual text are optimized using Mixed-Integer Linear Programming (MILP) to minimize the Manhattan distance between the bounding box center and the attention point. Let pmax,i be the maximum attention point of vti and ci the rectangular bounding box center. The optimization objective is: (cid:88) min DManhattan(pmax,i, ci). (2) Simultaneously incorporate set of constraints to ensure that the layout of each visual text is reasonable and nonoverlapping. The solution to this optimization problem can be represented as assigning bounding box bbxi (moffset, noffset, mscale, nscale) to each description. Then, we encode the prompt and the corresponding descriptions of visual text = {d1, d2, . . . , dn}, obtaining values for and {ˆc1, ˆc2, . . . , ˆcn}. needs to be processed through instance fusion introduced in Section 3.2. The latent ˆxi is conditioned on ˆci, and the initial latent is conditioned on the complete c. Model SD3.5 Large (ICML 2024) FLUX.1 dev (2024) AnyText (ICLR 2024) TextDiffuser-2 (ECCV 2024) RAG-Diffusion (Arxiv 2024) 3DIS (ICLR 2025) TextCrafter (Ours) 2 regions 0.7293 0.6089 0.0513 0.5322 0.4388 0.4495 0.7628 Word Accuracy 4 regions 0.6574 0.4661 0.1948 0.1787 0.2116 0.3880 0.7406 3 regions 0.6825 0.5531 0.1739 0.3255 0.3316 0.3959 0. 5 regions 0.5940 0.4316 0.2249 0.0809 0.1910 0.3303 0.6977 average 0.6548 0.4965 0.1804 0.2326 0.2648 0.3813 0.7370 NED CLIPScore 0.8470 0.6879 0.4675 0.4353 0.4498 0.6505 0.8679 0.7797 0.7401 0.7432 0.6765 0.7797 0.7767 0. Table 2: Quantitative results on the proposed CVTG-2K. The results highlight TextCrafters superiority across three metrics. Step 1 Step 2 Step 3 Word Accuracy NED 0.6495 0.4422 0.7750 0.6116 0.8055 0.6351 0.7762 0.6121 0.8651 0.7328 0.8679 0.7370 Table 3: Ablation on CVTG-2K of Instance Fusion (Step 1), Region Insulation (Step 2), and Text Focus (Step 3). When > (i.e., during the first steps of the denoising process), denoising follows the formula: xt1 = xt ϵθ(xt, c) t, ˆci), t1 = ˆxi ˆxi (3) where ϵθ represents the noise predictor. After each denoising step, ˆxi is reinserted into the corresponding region in xt. ϵθ(ˆxi"
        },
        {
            "title": "3.4 Text Focus\nRegion insulation creates distinct visual boundaries between\nvisual texts. To enhance harmony, regions are merged into a\nsingle latent variable for denoising, but this may blur smaller\ntexts. To preserve structure, we boost attention scores [Hertz\net al., 2022] for texts and preceding quotation marks, capping\nenhancement at 2× to prevent overamplification. To ensure\ncontrol, we employ the tanh function. The k token sequence\nis denoted as F = {f1, . . . , fk}. The enhancement ratio is:\nratio = 1 + tanh(0.5 · k).",
            "content": "(4) We use smaller ratio for single-word cases. Unlike UNets cross-attention, MM-DiT employs full attention matrix with four regions: image-to-image, prompt-to-prompt, prompt-toimage, and image-to-prompt [Cai et al., 2024] . We re-weight the image-to-text matrix, which controls image modality and enhances focus on visual text. At step t, the operation for each element in Mt is expressed as: Focus(Mt)i,j = (cid:26)ratio (Mt)i,j (Mt)i,j if otherwise. (5) The overall algorithmic process is illustrated in Algorithm 1."
        },
        {
            "title": "4 Experiments\n4.1 Benchmark\nCurrently, there is a lack of publicly available benchmark\ndatasets specifically designed for complex visual text gen-\neration tasks. Therefore, we propose CVTG-2K, a chal-\nlenging public benchmark dataset for complex visual text.",
            "content": "All prompts are generated through the OpenAIs O1-mini API [OpenAI, 2024]. These prompts encompass variety of scenes containing complex visual texts, including but not limited to street views, book covers, advertisements, posters, notes, memes, logos, and movie frames. Unlike previous standard datasets synthesized using fixed rules, CVTG-2K ensures the diversity and rationality of the data distribution. We first prompt the O1 model to conceive scene and then imagine the spossible visual texts that may appear in that scene, utilizing Chain-of-Thought (CoT) [Wei et al., 2022] techniques to ensure the quality of the generated prompts. For further details on the dialogue template with O1, refer to the supplementary material. Through rigorous filtering rules and meticulous post-processing, we have generated 2,000 prompts containing complex visual texts. On average, the visual texts in CVTG-2K contain 8.10 words and 39.47 characters, surpassing all previously published visual text benchmark datasets in terms of visual text length. Table 1 presents comparison of the data in CVTG-2K with currently available benchmark datasets. Furthermore, CVTG-2K is the first benchmark dataset to include the number of multiple visual text regions, with the number of regions ranging from 2 to 5. We have assigned different proportions to these region numbers, approximately 20%, 30%, 30%, and 20%. With its diverse region numbers and visual text lengths, CVTG-2K provides comprehensive evaluation of model performance in complex visual text generation tasks. Through careful human review and filtering, CVTG-2K ensures that it does not contain any discriminatory or inflammatory content. To further enhance the challenge of CVTG-2K, we randomly added attributes to each visual text in half of the data. The attributes include size, color, and font. The sizes are categorized as large, medium, and small, and the fonts include regular, bold, italic, and cursive. All attributes are labeled using natural language to ensure that the text encoder can process them without the need for special designs. Each visual text will randomly have one or more of these attributes. This design will promote further research into the stylization and customization of visual texts in future studies. Additionally, we provide more fine-grained information. We decouple the prompts using O1-mini, separating descriptions = {d1, d2, . . . , dn} containing multiple visual texts and expressing the relationship between each visual text and its position through the most critical words, which is the carrier of the visual text. Through meticulous manual review, we ensured the accuracy of this fine-grained information. CVTG2K will be publicly released alongside our code, and the dataset will be available for any users adhering to the usage Figure 5: Qualitative comparison of TextCrafter with other baselines on CVTG-2K. TextCrafter excels in delivering harmonious and aesthetically pleasing images. It also accurately renders multiple visual texts while maintaining stability in complex scenarios. guidelines. For more examples of CVTG-2K, please refer to the supplementary material."
        },
        {
            "title": "4.2 Evaluation Metrics and Baselines\nThe quality of CVTG are evaluated from three metrics:",
            "content": "1. Word Accuracy: The generated image is input into an OCR tool for prediction. Here, we use the SOTA opensource OCR model PPOCR-v4 [PaddlePaddle, 2023]. 2. Normalized Edit Distance (NED): NED [Tuo et al., 2024] is more lenient metric than word accuracy, used to measure character-level similarity. 3. CLIPScore: We introduce CLIPScore, which is used to evaluate the models ability to follow the text prompt during the generation process [Hessel et al., 2021]. We compare our method on CVTG-2K with several stateof-the-art models for visual text generation, including Stable Diffusion 3.5 Large [Esser et al., 2024], FLUX.1 dev, AnyText [Tuo et al., 2024] and TextDiffuser2 [Chen et al., 2025]. To highlight the difference between CVTG and multiinstance generation tasks, we also compare with methods like RAG-Diffusion [Chen et al., 2024b] and 3DIS [Zhou et al., 2025]. All experiments use official or author-provided codes and configurations for optimal baseline performance. The parameter λ described in Section 3.2 uses 0.4, and = 5 from Section 3.3. Notably, TextCrafter is built on FLUX.1 dev and requires no training or fine-tuning, enabling all experiments to run on single A6000 GPU."
        },
        {
            "title": "4.3 Quantitative Results\nAs shown in Table 2, quantitative experiments on the\nCVTG-2K demonstrate that TextCrafter outperforms com-",
            "content": "peting methods in both OCR accuracy (Word Accuracy and NED) and prompt-following ability (CLIPScore). Specifically, TextCrafter improves OCR accuracy by over 45% compared to FLUX. While general image generation models like Stable Diffusion 3.5 and FLUX perform well in simple visual text scenarios, their performance significantly degrades as the number of visual text regions increases. Methods like AnyText and TextDiffuser-2, trained on rule-based data and focusing on single-region generation, fail to generalize well to complex visual text tasks. AnyText performs better when the mask covers larger portion of the image, leading to slight improvement in accuracy as the number of regions increases. However, its overall performance remains 55% behind that of TextCrafter. Multi-instance control methods such as RAG-Diffusion and 3DIS, although capable of rendering complex scenes, struggle with the specific requirements of visual text generation, resulting in poor performance as complexity rises. Detailed quantitative results are provided in the supplementary material. Overall, the experiments confirm that TextCrafter excels in robustness, maintaining high accuracy and prompt-following performance even in complex visual text scenarios."
        },
        {
            "title": "4.4 Qualitative Results\nFigure 5 presents visual results comparing TextCrafter with\nseveral state-of-the-art text-to-image generation models, in-\ncluding SD3.5, FLUX, AnyText, TextDiffuser-2, RAG-\nDiffusion and 3DIS, on the CVTG-2K. While SD3.5 and\nFLUX produce high-quality images and can generate some\nvisual texts, they struggle with omissions and confusion as\nthe number of regions increases. AnyText performs subopti-\nmally when the text consists of two or more words. TextD-",
            "content": "Figure 6: Qualitative results of ablation studies on each step individually. The incorrectly rendered visual text is highlighted with red box, while the corresponding correctly rendered visual text is highlighted with blue box. iffuser sacrifices background information, leading to lower CLIPScore. RAG-Diffusion suffers from merging issues during the refinement phase. 3DIS weakens visual text information due to its layout-to-depth conversion. In contrast, TextCrafter maintains high image harmony and accurately renders multiple visual texts, adhering to prompt specifications while avoiding attribute confusion. More qualitative results are available in the supplementary material."
        },
        {
            "title": "4.5 Ablation Study\nTable 3 presents quantitative ablation results of each step in-\ndividually or in combinations.",
            "content": "Instance Fusion. Disabling instance fusion has small impact on overall metrics, as it primarily establishes the correlation between visual text and its location, which doesnt directly affect accuracy. However, without instance fusion, visual text often appears in incorrect locations. For instance, in the first column of the Figure 6, disabling instance fusion causes the text Hot Dogs to appear in the wrong area. Enabling instance fusion also suppresses hallucinated text, ensuring correct placement and preventing irrelevant text from appearing, as shown in the second column of the Figure 6. Region Insulation. Enabling region insulation improves performance across all metrics by decoupling complex visual text, reducing CVTG complexity and maximizing the pre-trained models ability. Region insulation achieves over 60% word accuracy on its own and reduces interference between texts. In Figure 6, region insulation prevents interference, improving visual text clarity and accuracy, as seen in the corrected rendering of Arrival in the fourth column. Text Focus. Text focus has the most significant impact on improving metrics, especially for small-size visual text. It enhances both clarity and accuracy, as seen in the clear generation of Next Stop on the train in the fifth column in Figure 6, and VIP Access in the last column. When enabled alone, text focus achieves 63.51% word accuracy but is about 10% lower than the optimal performance, indicating that it requires support from instance fusion and region insulation for stability, particularly in complex visual text scenarios."
        },
        {
            "title": "5 Conclusion\nThis paper delves into the complex visual text generation\n(CVTG) task, which involves generating textual content\nacross multiple regions of an image. We propose TextCrafter,\na novel training-free framework based on DiT framework,\nemploying step-by-step refinement from prompts to clauses,\nand finally to specific visual text for precise generation. Key\ninnovations in TextCrafter include instance fusion to link text\nwith its spatial carrier, the region insulation strategy to reduce\ncomplexity and prevent text loss, and a text focus approach\nfor accurate small-text rendering. We also construct a new\nCVTG-2K benchmark dataset for the CVTG task. Exten-\nsive experiments demonstrate TextCrafter’s clear advantages\nagainst the existing competitors.",
            "content": "Ethical Statement Any users of our model are prohibited from using it to create false or malicious information. References [BlackForest, 2024] BlackForest. Black forest labs; frontier ai lab, 2024. [Cai et al., 2024] Minghong Cai, Xiaodong Cun, Xiaoyu Li, Wenze Liu, Zhaoyang Zhang, Yong Zhang, Ying Shan, and Xiangyu Yue. Ditctrl: Exploring attention control in multi-modal diffusion transformer for tuning-free arXiv preprint multi-prompt longer video generation. arXiv:2412.18597, 2024. [Chefer et al., 2023] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM Transactions on Graphics (TOG), 42(4):110, 2023. [Chen et al., 2023] Zhennan Chen, Rongrong Gao, Tian-Zhu Xiang, and Fan Lin. Diffusion model for camouflaged object detection. In ECAI 2023, pages 445452. IOS Press, 2023. [Chen et al., 2024a] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei. Textdiffuser: Diffusion models as text painters. Advances in Neural Information Processing Systems, 36, 2024. [Chen et al., 2024b] Zhennan Chen, Yajie Li, Haofan Wang, Zhibo Chen, Zhengkai Jiang, Jun Li, Qian Wang, Jian Yang, and Ying Tai. Region-aware text-to-image generation via hard binding and soft refinement. arXiv preprint arXiv:2411.06558, 2024. [Chen et al., 2025] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei. Textdiffuser-2: Unleashing the power of language models for text rendering. In European Conference on Computer Vision, pages 386402. Springer, 2025. [Dhariwal and Nichol, 2021] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. [Esser et al., 2024] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for highIn Forty-first International resolution image synthesis. Conference on Machine Learning, 2024. [Feng et al., 2022] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Reddy Akula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for In The Eleventh compositional text-to-image synthesis. International Conference on Learning Representations, 2022. [Hertz et al., 2022] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-or. Prompt-to-prompt image editing with cross-attention control. In The Eleventh International Conference on Learning Representations, 2022. [Hessel et al., 2021] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 75147528, 2021. [Ho et al., 2020] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [Hu et al., 2024] Taihang Hu, Linxuan Li, Joost van de Weijer, Hongcheng Gao, Fahad Shahbaz Khan, Jian Yang, Ming-Ming Cheng, Kai Wang, and Yaxing Wang. Token merging for training-free semantic binding in text-toimage synthesis. arXiv preprint arXiv:2411.07132, 2024. [Ji et al., 2023] Jiabao Ji, Guanhua Zhang, Zhaowen Wang, Bairu Hou, Zhifei Zhang, Brian Price, and Shiyu Chang. Improving diffusion models for scene text editing with dual encoders. Transactions on Machine Learning Research, 2023. [Li et al., 2023] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded textIn Proceedings of the IEEE/CVF to-image generation. Conference on Computer Vision and Pattern Recognition, pages 2251122521, 2023. [Liu et al., 2025] Zeyu Liu, Weicong Liang, Zhanhao Liang, Chong Luo, Ji Li, Gao Huang, and Yuhui Yuan. Glyphbyt5: customized text encoder for accurate visual text rendering. In European Conference on Computer Vision, pages 361377. Springer, 2025. [Ma et al., 2023] Jian Ma, Mingjun Zhao, Chen Chen, Ruichen Wang, Di Niu, Haonan Lu, and Xiaodong Lin. Glyphdraw: Seamlessly rendering text with intricate spatial structures in text-to-image generation. arXiv preprint arXiv:2303.17870, 2023. [OpenAI, 2024] OpenAI. O1mini: Advancing cost-efficient reasoning, 2024. [PaddlePaddle, 2023] PaddlePaddle. Pp-ocrv4, 2023. [Peebles and Xie, 2023] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. [Podell et al., 2023] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations, 2023. [Raffel et al., 2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):1 67, 2020. [Zhangli et al., 2024] Qilong Zhangli, Jindong Jiang, Di Liu, Licheng Yu, Xiaoliang Dai, Ankit Ramchandani, Guan Pang, Dimitris Metaxas, and Praveen Krishnan. Layoutagnostic scene text image synthesis with diffusion models. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 74967506. IEEE Computer Society, 2024. [Zhao and Lian, 2025] Yiming Zhao and Zhouhui Lian. Udifftext: unified framework for high-quality text synthesis in arbitrary images via character-aware diffusion models. In European Conference on Computer Vision, pages 217 233. Springer, 2025. [Zhao et al., 2024] Chen Zhao, Weiling Cai, Chenyu Dong, and Chengwei Hu. Wavelet-based fourier information interaction with frequency diffusion adjustment for underwater image restoration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82818291, 2024. [Zheng et al., 2023] Guangcong Zheng, Xianpan Zhou, Xuewei Li, Zhongang Qi, Ying Shan, and Xi Li. Layoutdiffusion: Controllable diffusion model for layout-to-image In Proceedings of the IEEE/CVF Confergeneration. ence on Computer Vision and Pattern Recognition, pages 2249022499, 2023. [Zhou et al., 2024] Dewei Zhou, You Li, Fan Ma, Xiaoting Zhang, and Yi Yang. Migc: Multi-instance generation controller for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 68186828, 2024. [Zhou et al., 2025] Dewei Zhou, Ji Xie, Zongxin Yang, and Yi Yang. 3dis: Depth-driven decoupled instance synthesis for text-to-image generation. In The Twelfth International Conference on Learning Representations, 2025. [Ramesh et al., 2021] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image In International conference on machine generation. learning, pages 88218831. Pmlr, 2021. [Ramesh et al., 2022] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. [Rombach et al., 2022] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent the IEEE/CVF diffusion models. conference on computer vision and pattern recognition, pages 1068410695, 2022."
        },
        {
            "title": "In Proceedings of",
            "content": "[Tumanyan et al., 2023] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19211930, 2023. [Tuo et al., 2024] Yuxiang Tuo, Wangmeng Xiang, Jun-Yan He, Yifeng Geng, and Xuansong Xie. Anytext: Multilingual visual text generation and editing. In The Twelfth International Conference on Learning Representations, 2024. [Wei et al., 2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [Xie et al., 2023] Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng Zheng, and Mike Zheng Shou. Boxdiff: Text-to-image synthesis with training-free box-constrained diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 74527461, 2023. [Yang et al., 2024a] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and CUI Bin. Mastering text-to-image diffusion: Recaptioning, planning, and In Forty-first Internagenerating with multimodal llms. tional Conference on Machine Learning, 2024. [Yang et al., 2024b] Yukang Yang, Dongnan Gui, Yuhui Yuan, Weicong Liang, Haisong Ding, Han Hu, and Kai Chen. Glyphcontrol: Glyph conditional control for visual text generation. Advances in Neural Information Processing Systems, 36, 2024. [Zhang et al., 2024a] Hui Zhang, Dexiang Hong, Tingwei Gao, Yitong Wang, Jie Shao, Xinglong Wu, Zuxuan Wu, and Yu-Gang Jiang. Creatilayout: Siamese multimodal diffusion transformer for creative layout-to-image generation. arXiv preprint arXiv:2412.03859, 2024. [Zhang et al., 2024b] Lingjun Zhang, Xinyuan Chen, Yaohui Wang, Yue Lu, and Yu Qiao. Brush your text: Synthesize any scene text on images via diffusion model. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 72157223, 2024."
        }
    ],
    "affiliations": [
        "China Mobile",
        "Nanjing University",
        "The Hong Kong University of Science and Technology"
    ]
}