{
    "paper_title": "Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks",
    "authors": [
        "Cheng Yang",
        "Haiyuan Wan",
        "Yiran Peng",
        "Xin Cheng",
        "Zhaoyang Yu",
        "Jiayi Zhang",
        "Junchi Yu",
        "Xinlei Yu",
        "Xiawu Zheng",
        "Dongzhan Zhou",
        "Chenglin Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video Models have achieved remarkable success in high-fidelity video generation with coherent motion dynamics. Analogous to the development from text generation to text-based reasoning in language modeling, the development of video models motivates us to ask: Can video models reason via video generation? Compared with the discrete text corpus, video grounds reasoning in explicit spatial layouts and temporal continuity, which serves as an ideal substrate for spatial reasoning. In this work, we explore the reasoning via video paradigm and introduce VR-Bench -- a comprehensive benchmark designed to systematically evaluate video models' reasoning capabilities. Grounded in maze-solving tasks that inherently require spatial planning and multi-step reasoning, VR-Bench contains 7,920 procedurally generated videos across five maze types and diverse visual styles. Our empirical analysis demonstrates that SFT can efficiently elicit the reasoning ability of video model. Video models exhibit stronger spatial perception during reasoning, outperforming leading VLMs and generalizing well across diverse scenarios, tasks, and levels of complexity. We further discover a test-time scaling effect, where diverse sampling during inference improves reasoning reliability by 10--20%. These findings highlight the unique potential and scalability of reasoning via video for spatial reasoning tasks."
        },
        {
            "title": "Start",
            "content": "Reasoning via Video: The First Evaluation of Video Models Reasoning Abilities through Maze-Solving Tasks Cheng Yang1 Haiyuan Wan2,3 Yiran Peng1 Xin Cheng4 Zhaoyang Yu1 Jiayi Zhang1,8 Junchi Yu5 Xinlei Yu6 Xiawu Zheng7 Dongzhan Zhou3 Chenglin Wu1 3Shanghai Artificial Intelligence Laboratory 1DeepWisdom 2Tsinghua University 4Renmin University of China 5University of Oxford 6National University of Singapore 7Xiamen University 8Hong Kong University of Science and Technology (GuangZhou) (cid:128) https://imyangc7.github.io/VRBench Web Core contributors Corresponding authors 5 2 0 N 4 2 ] . [ 2 5 6 0 5 1 . 1 1 5 2 : r Figure 1. Overview of VR-Bench. (A) Maze Types. VR-Bench comprises five maze typesRegular Maze, Irregular Maze, 3D Maze, Trapfield, and Sokobancovering both 2D and 3D settings as well as diverse task structures, yielding broad range of spatial reasoning scenarios. (B) Reasoning via Video Paradigm. VR-Bench adopts chain-of-frame reasoning paradigm [45], requiring models to produce frame-by-frame inferences that capture sequential visual reasoning. (C) Benchmark Performance. Leading VLMs and video models are evaluated on four core metrics across all maze types, revealing clear differences in spatial reasoning capability. (D) Additional Analysis. VR-Bench also supports evaluations on difficulty generalization, texture generalization, maze-type generalization, and test-time scaling, enabling comprehensive assessment of model robustness and generalization."
        },
        {
            "title": "Abstract",
            "content": "Video Models have achieved remarkable success in highfidelity video generation with coherent motion dynamics. Analogous to the development from text generation to textbased reasoning in language modeling, the development of video models motivates us to ask: Can video models reason via video generation? Compared with the discrete text corpus, video grounds reasoning in explicit spatial layouts and temporal continuity, which serves as an ideal substrate for spatial reasoning. In this work, we explore the reasoning via video paradigm and introduce VR-Bencha comprehensive benchmark designed to systematically evaluate video models reasoning capabilities. Grounded in mazesolving tasks that inherently require spatial planning and multi-step reasoning, VR-Bench contains 7,920 procedurally generated videos across five maze types and diverse visual styles. Our empirical analysis demonstrates that SFT can efficiently elicit the reasoning ability of video model. Video models exhibit stronger spatial perception during reasoning, outperforming leading VLMs and generalizing well across diverse scenarios, tasks, and levels of complexity. We further discover test-time scaling effect, where diverse sampling during inference improves reasoning reliability by 1020%. These findings highlight the unique potential and scalability of reasoning via video for spatial reasoning tasks. 1. Introduction With the rapid development of diffusion-based and autoregressive-based generative architectures, video models have witnessed tremendous success in high-fidelity video generation. Previous works, such as Stable Video Diffusion [3] and Imagen Video [15], showcase the capability of video models to generate physically realistic and temporally consistent videos conditioned on their input instructions. Recent studies further reveal that advanced video models are capable of performing diverse range of visual tasks beyond generation itself, including perception, understanding, and even reasoning. These findings suggest that video models are evolving from pure generative models into generalpurpose visual intelligence models. Analogous to the evolution of language models from text generation to text-based reasoning, the development of video models leads to question: Can video models reason via video generation? Crucially, the spatiotemporal nature of video modality offers new perspective on reasoning. The traditional paradigm, which we term reasoning via text, uses language as the medium for expressing intermediate reasoning steps. Representative work, such as Chain-of-Thought prompting [39, 44, 4952, 54], achieves this by eliciting large language models (LLMs) to generate coherent textual reasoning chain. Recently, this reasoning via text paradigm has been introduced to visual domains, including multimodal question answering and video understanding. However, even in these multimodal settings, current paradigms still express reasoning through textual continuation instead of visual or physical dynamics. In contrast, video represents reasoning as process of visual continuation over time. Each frame in video builds upon its previous ones, capturing the dynamics of motion, spatial consistency, and temporal causality within 2D and 3D space. The continuous and structured nature of frames makes video an ideal substrate for multimodal reasoning. Building on this insight, we propose reasoning via video, where reasoning emerges through nextframe generation rather than next-token prediction. However, comprehensive testbed for reasoning via video is lacking. To this end, we introduce VR-Bench, dedicated benchmark designed to systematically assess the reasoning capabilities of video generation models. As shown in Figure 1, we ground our benchmark in the mazesolving task, natural fit for visual reasoning due to its open-ended solution space and rich trajectory-based supervision. Each instance inherently demands spatial planning, dynamic tracking, and multi-step reasoning, making it an ideal testbed for evaluating model inference quality over time. Our dataset comprises 7,920 procedurally generated maze-centric videos, each paired with corresponding Trace Reasoning Task that requires models to infer the optimal path. To ensure broad generalizability and challenge model robustness, VR-Bench spans five distinct maze typesRegular Maze, Irregular Maze, 3D Maze, Sokoban, and Trapfieldcovering wide spectrum of spatial structures and decision patterns. Additionally, each maze is rendered in diverse visual styles across more than dozen themes, enabling fine-grained analysis of how well models generalize across varied visual domains and increasing the realism and complexity of the reasoning tasks. Building upon the proposed VR-Bench, we conduct systematic study of the reasoning via video paradigm. We construct instruction-following datasets derived from VRBench to elicit the reasoning capability of open-source video models. After supervised fine-tuning (SFT), these models exhibit significant performance gain across all reasoning tasks in VR-Bench. Moreover, SFT endows video models with strong out-of-domain generalization under diverse distribution shifts, including task difficulty, background style, and task type. Compared with visionlanguage models (VLMs) [2, 5, 6, 19, 22] that reason via text, video models consistently outperform their counterparts on high-complexity reasoning tasks, showing greater stability and even superior performance as task difficulty increases, across diverse scenarios and tasks. This finding confirms that videos serve as more expressive substrate for spatial reasoning, which facilitates video models to leverage temporal continuity and dynamic visual context. 2 Interestingly, we further observe that video models exhibit test-time scaling effect analogous to that of LLMs. As the inference budget increases, their performance improves substantially. By employing diverse sampling strategies at test time, video models effectively explore multiple reasoning trajectories, reducing uncertainty and achieving an average performance gain of 1020%. These empirical results highlight the unique potential and scalability of the reasoning via video paradigm. Our contributions are summarized as follows: We make an early and systematic exploration of the reasoning via video paradigm, where reasoning emerges from sequential frame generation rather than token prethis diction. Compared with text-based approaches, paradigm naturally captures temporal continuity and spatial causality, offering more expressive and scalable substrate for solving spatial reasoning tasks. We construct VR-Bench, comprehensive benchmark grounded in maze-solving tasks with diverse spatial structures, difficulty levels, and texture styles. It provides finegrained trajectory-level supervision and supports evaluations on path accuracy, rule compliance, generalization. Through extensive experiments, we demonstrate that video-based reasoning outperforms text-based reasoning (e.g., VLMs) on complex tasks, especially under distribution shifts in maze type, visual style, and difficulty. Finetuned video models exhibit stronger performance, lower path redundancy, and higher structural fidelity. We reveal test-time scaling effect for video models, where performance consistently improves with larger inference budgets. Similar to that in LLMs, diverse sampling unlocks multi-path exploration and yields up to 20% performance gains across metrics and difficulty levels. 2. Related Works 2.1. Video Generation Video models have advanced rapidly in both understanding and generation. Early understanding methods, such as MViT [10], Video Swin Transformer [23], and VideoMAE [37], focused on learning robust video representations for downstream tasks. With LLMs [1, 3, 29], recent approaches tokenize videos and leverage language backbones for captioning [36], event localization [33], and reasoning [16]. On the generation side, Sora-2 [4] achieved controllable, physically consistent outputs with synchronized dialogue and sound. Proprietary systems such as Runways Gen-3 [31], Pika Labs [30], Luma AI [24], and Google DeepMinds Veo series [7, 8] further enhance video quality and realism but remain closed-source. In contrast, open-source frameworks such as Stable Video Diffusion [3], OpenSora[55], HunyanVideo [21], and the Wan series [40] democratize access, offering efficient architectures and scalable training for stateof-the-art video synthesis. 2.2. Evolution of Reasoning Paradigms Chain-of-Thought (CoT) prompting has significantly enhanced the reasoning abilities of language models [12, 41, 44]. Reinforcement learning further integrates CoT-style reasoning into model training, enabling models to internalize multi-step thought processes. More recently, such paradigms have been extended to vision-language models (VLMs). Systems like o3 and o4-mini [27] introduce the Think with Image framework, where reasoning is grounded in visual operations such as zooming and cropping. This allows the model to dynamically interact with image regions as part of the CoT process, thereby improving multimodal reasoning [32, 56, 57]. In parallel, the rise of unified models for both generation and understanding has given birth to new reasoning paradigm centered on interleaved vision-language outputs. Instead of purely textual reasoning traces, these models generate coherent sequences that alternate between textual and visual elements [9, 39, 46, 47], providing more grounded and expressive format for complex multimodal reasoning. 2.3. Evaluation of Video Generation Reasoning Previous benchmarks for video generation models have predominantly focused on assessing visual quality, temporal coherence, and alignment with human preferences [14, 17, 18, 53]. However, these evaluations largely neglect the reasoning capabilities of video models. Recent works have begun to explore reasoning via video generationthe ability of models to solve reasoning tasks through the generation process itself [13, 34, 45]. For instance, models like Veo 3 demonstrate zero-shot competence in tasks such as maze navigation and symmetry recognition. These tasks require perceiving, modeling, and manipulating the visual world, indicating that video generation can inherently support spatial-temporal reasoning. Despite these promising directions, current benchmarks for video reasoning still suffer from several limitations: (1) Lack of fine-grained and objective evaluation: Current evaluations rely heavily on manual inspection or coarse metrics, without capturing the reasoning trajectory embedded in the video; (2) Absence of modality comparisons: There is lack of systematic comparison with think with text or think with image paradigms, making it unclear whether video generation truly provides unique advantages for reasoning; (3) Neglect of tuning and scaling analysis: Unlike language or multimodal models, video reasoning benchmarks seldom explore whether supervised fine-tuning (SFT) or test-time scaling can improve performance. These gaps call for new benchmark that evaluates not only generation quality but also the reasoning process in videos, using rigorous metrics, multimodal comparisons, and extensible settings. 3 3. VR-Bench 3.1. Dataset Construction The VR-Bench dataset is Visual Trace Reasoning (VTR) dataset that constructs various Maze Puzzles into visual reasoning tasks. Its construction process comprises two steps: Maze Generation and Video Generation. Figure 2. Variations of difficulty level and maze texture Maze Generation. The maze generation process in our work encompasses five distinct types, with all 7,920 maze instances in the dataset generated programmatically through custom code [35]. Each type is tailored to assess specific visual reasoning capabilities, as elaborated below: 1. Regular Maze. We generate mazes with grid-based layout to focus on the models ability to perceive basic maze structures and its path-finding and problemsolving competence, serving as fundamental testbed for maze reasoning tasks. 2. Trapfield. This type transforms the walls of traditional mazes into grid-shaped trap regions, reversing the logic from finding feasible paths to avoiding traps. Beyond altering the problem-solving logic, the more flexible movement space also challenges the models ability to plan and find optimal paths. 3. Irregular Maze. Moving away from regular blockshaped paths, we adopt curve-based path designs. This design prevents the model from relying on coordinatebased position encoding, thereby rigorously evaluating its pure visual perception of maze layouts. It also explicitly decouples visual reasoning from text-based reasoning, focusing on the video models capability to reason via video itself. 4. Sokoban. We modify the underlying rules of traditional mazes by introducing the Sokoban task mechanism. Models need to comprehend and apply Sokoban logic on top of path finding, increasing task complexity and emphasizing the models ability to internalize and apply logic. 5. 3D Maze. By extending the maze to 3D space, we employ stereoscopic structural design to test the models 4 spatial perception ability in 3D environments and its cross-dimensional path reasoning capability. Maze Variations. To evaluate the generalization ability on the VTR task and enhance robustness in adapting to diverse maze scenarios, we introduce variations across two key dimensions: (1) Difficulty Level: We define three difficulty grades (Easy, Medium, and Hard) by adjusting the maze size (e.g., expanding from 55 to 77), modifying the number of maze branches, and adding obstacles; (2) Maze Texture: We vary the textures of maze obstacles, paths, and other components using textures generated via procedural methods and generative models, as shown in Figure 6, which exposes the policies to broad visual distribution and mitigates overfitting to clean, synthetic environments. Video Generation. To generate solution videos from maze images, we use Breadth-First Search solver to compute the optimal path for each maze. These paths are rendered into videos at 24 fps and standardized to 192 frames (8 seconds) by adjusting playback speed, producing consistent imagevideo pairs for training and evaluation. 3.2. Metric Design We selected two different evaluation paradigms to comprehensively assess our task, as detailed below. Path Matching. To objectively and comprehensively evaluate the VTR task, we perform target tracking across each frame of the model-generated videos to record the motion trajectory of the target. By comparing and analyzing these trajectories against the optimal path for each task [48], we propose the following four evaluation metrics. 1. Exact Match (EM) j=1 Defined as EMi = (cid:81)ni I(ˆvij = vij). This metric measures whether the model successfully generates the complete and correct trajectory that aligns with the shortest optimal valid path. One step of deviation from the optimal solution is considered incorrect. 2. Success Rate (SR) (cid:17) end Bgoal (cid:16) p(gen) Defined as SRi = . SR measures whether the generated trajectory successfully reaches the designated goal region. It reflects the models capability to complete the task by arriving at the target position, with value of 1 indicating successful goal attainment and 0 indicating failure to reach the goal. (cid:80)ni (cid:104)(cid:81)j 3. Precision Rate (PR) (cid:105) Defined as PRi = 1 . PR j=1 ni quantifies the proportion of consecutively correct steps along the optimal path. It offers softer metric than EM, reflecting the models ability to make steady, meaningful progress toward the complete correct trajectory. I(ˆvik = vik) k="
        },
        {
            "title": "Method",
            "content": "EM ()"
        },
        {
            "title": "Irreg Trap",
            "content": "3D Soko Base SR () Irreg Trap 3D Soko Base PR () Irreg Trap 3D Soko Base"
        },
        {
            "title": "Irreg Trap",
            "content": "3D"
        },
        {
            "title": "Soko",
            "content": "SD ()"
        },
        {
            "title": "2.8\nGemini-2.5-pro\n13.9\nGpt-5 high\nQwen2.5-VL-7B♡\n0.0\nQwen2.5-VL-7B-SFT 12.5",
            "content": "36.1 31.9 1.3 29.2 13.9 18.1 0.0 22.2 2.8 0.0 0.0 31.9 25.0 23.6 1.4 29.8 4.2 69.4 1.4 52.8 37.5 33.3 6.9 34. 13.9 27.8 1.4 52.8 2.8 1.4 1.4 36.1 31.9 34.7 2.8 37.5 9.5 11.8 6.5 32.5 47.9 43.6 12.4 45.1 57.6 53.7 14.3 71. 19.4 23.7 11.3 59.3 33.8 35.6 7.8 43.0 1.9 25.0 31.0 2.1 300.0 26.7 2.3 52.6 0.0 6.9 66.7 11.3 1.1 0.5 0.0 20.0 80.0 1150.0 4. 3.0 +12.5 +27.9 +22.2 +31.9 +28.4 +51.4 +27.8 +51.4 +34.7 +34.7 +26.0 +32.7 +57.3 +48.0 +35.2 -247.4 -24.4 -55.4 -75.5 -1147.0 o d a e Veo-3.1-fast Veo-3.1-pro Sora-2 kling-v1 Seedance-1.0-pro MiniMax-Hailuo-2. Wan2.5-i2v-preview Wan2.2-TI2V-5B Wan-R1 0.0 0.0 1.4 0.0 0.0 0.0 0.0 0.0 0.0 4.2 5.6 0.0 2.8 1. 2.8 0.0 0.0 1.4 0.0 0.0 2.8 2.8 4.2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2.8 0.0 4.2 0.0 0.0 0. 0.0 0.0 40.3 47.2 75.0 2.8 75.0 68.1 36.1 36.1 72.2 0.0 45.8 40.3 58.3 6.9 26.4 12.5 38.9 59.7 83.0 1.4 59.7 70. Closed-Source 48.6 50.0 37.5 27.8 77.8 55.6 Open-Source 24.5 31.9 77.8 0.0 43.1 37.5 43.1 12.5 13.9 45.8 20.2 24.6 45.1 6.3 12.8 23.2 24.8 33.9 45.7 8.8 35.8 24.2 28.2 39.1 46.6 10.4 42.7 30. 13.4 18.0 19.3 11.7 23.6 20.3 21.7 21.4 27.4 9.0 17.1 15.5 33.5 195.3 111.5 80.7 140.7 94.5 40.1 85.4 302.9 187.0 145.1 92.4 69.7 25.2 84.4 162.3 143.4 99.1 50.1 464.0 170.0 90.9 112.3 141.8 138.7 356.1 241.9 165.5 22.4 11. 14.3 6.6 21.8 9.1 34.4 7.1 24.5 12.8 17.1 9.2 378.4 281.8 73.2 119.9 278.0 176.6 388.7 66. 5.4 56.9 100.0 100.0 69.4 33.3 2.4 +33.3 +56.9 +38.9 +65.3 +4.2 +69.5 +56.9 +100.0 +68.1 +58.3 +54.0 +62.5 +72.0 +80.7 +35.1 -12.8 -25.1 69.4 10.3 60. 71.6 44.3 93.5 65.3 38.9 76. 79.1 4.2 3.9 3.9 -7.8 10.2 -100.1 Table 1. The five tasks of VR-Bench correspond to Base (Regular Maze), Irrg (Irregular Maze), Trap (TrapField), 3D (3D Maze), and Soko (Sokoban). The best and second-best results in each column are bolded and underlined, respectively. indicates that the model produced no successful cases for the corresponding task, making SD undefined. denotes the base model of Wan-R1, for comparisons. 4. Step Deviation (SD) 1. SD quantifies the relaDefined as SDi = L(gen) L(gt) tive path-length redundancy of the generated trajectory, representing how much longer the models path is compared to the optimal one. smaller SD indicates higher efficiency and closer adherence to the optimal solution. ines five key dimensions: (1) Motion Continuity of the main subject, (2) Temporal Consistency of the subject, (3) Trajectory Rationality of the main subject, (4) Structural Consistency of the maze, and (5) Interactional Rationality of subjectmaze interactions. Each generated video is interpreted by VLM, which identifies and scores potential violations of these rules. Specifically, each dimension is assigned binary score0 if the behavior is deemed unreasonable and 1 if it is reasonable. The scores from all five dimensions are then aggregated to compute unified VLM-score, ranging from 0 to 5, which provides quantitative measure of the overall rule compliance in the generated videos. (cid:16) i= (cid:80)M Since Structural Consistency offers only binary judgment of whether the maze layout has changed, we introduce Maze Fidelity (MF) to quantitatively measure the degree of structural consistency across frames, which defined as 1 p:I0(p)Ii(p)>τ MF = 1 Ni Here, is the number of sampled frames; I0 and Ii denote the background regions of the first and the i-th frames; τ is the pixel-difference threshold; and Ni is the number of valid overlapping pixels. MF quantifies background stability across frames, with higher values indicating better preservation of the static maze layout. 4. Experiment (cid:17) . To comprehensively evaluate the reasoning capability of video models, we conduct experiments on our proposed VR-Bench. We evaluate both state-of-the-art proprietary and open-source video models on this benchmark. To highlight the advantages of video models over traditional multimodal approaches, we also include representative VLMs in our evaluation. In addition, we fine-tune the open-source 5 Figure 3. Bad case visualization and VLM-as-judge schematic Rule Compliance. Not all generated videos faithfully depict the ball following the maze path to complete its task. As shown in Figure 6, we observed numerous failure cases during testing, including the ball breaking through maze walls, disappearing and reappearing, and inconsistent maze layouts across frames. To consistently evaluate the models adherence to spatial and physical rules [18], we designed prompt-based assessment protocol that exam-"
        },
        {
            "title": "Method",
            "content": "Closed-Source MF () VLM-Score () Base Irreg Trap 3D Soko Base Irreg Trap 3D Soko o d l n 43.2 86.3 22.5 69.1 63.5 Veo-3.1-fast 80.5 89.3 82.2 73.4 95.8 Veo-3.1-pro 96.5 97.2 97.2 95.4 95.2 Sora-2 55.5 73.8 54.4 84.9 72.9 kling-v1 Seedance-1.0-pro 87.7 97.2 64.3 86.3 83.0 MiniMax-Hailuo-2.3 92.4 93.4 91.4 94.9 93.3 Open-Source Wan2.5-i2v-preview 69.2 74.8 70.6 82.7 90.4 Wan2.2-TI2V-5B 85.5 97.4 83.7 94.7 93.5 0.8 2.5 3.9 1.4 3.0 3.5 1.2 2. 2.5 2.8 4.1 2.5 3.8 3.4 1.8 3.1 1.1 1.5 3.9 1.9 2.4 3.2 2.6 1.5 1.7 2.0 3.3 2.8 2.7 3.7 2.6 3. 2.0 2.5 3.2 2.8 3.2 3.6 2.9 3.1 Wan-R1 4.1 -Wan2.2-TI2V-5B +5.7 +0.7 +9.6 +1.0 +0.6 +0.3 +0.2 +0.5 +0.7 +0.5 91.2 98.1 93.3 95.7 94.1 4. 4.3 4.4 4.0 Table 2. MF and VLM-Score denote Maze Fidelity and the rulecompliance score evaluated by VLM. The best and second-best results in each column are bolded and underlined. video model Wan2.2-TI2V-5B using VR-Bench to investigate their generalization ability on reasoning tasks. This allows us to assess whether such reasoning capabilities can emerge via supervised fine-tuning, and whether they can generalize across different settings. 4.1. Training Configurations To investigate how well open-source video models can acquire and generalize reasoning abilities through fine-tuning, we trained Wan-R1 based on the proposed dataset. Specifically, we used the first scene from each of the five game types in our benchmark. For each game, we created two training settings: one using only easy samples, and another using mixture of easy, medium, and hard samples. In each case, the data was split into 80% for training and 20% for validation. All models were fine-tuned using the Accelerate framework on A100 GPUs. We adopted LoRA-based training strategy on the Wan2.2-TI2V-5B architecture, with learning rate of 1e-4, image resolution of 512512, and video length of 193 frames. We applied LoRA (rank 32) to key attention and feedforward modules (q, k, v, o, ffn.0, ffn.2) of the Dit backbone. Each model was trained for 5 epochs, with dataset repetition factor of 100. Other training parameters such as batch size and GPU days are in Appendix. 4.2. Baseline Model We compare the Wan-R1 against wide range of baselines: 1) 6 Closed-source video models: Veo-3.1-fast, Veo-3.1pro [8], Sora-2 [28], Kling-v1 [20], Seedance-1.0-pro [11], MiniMax-Hailuo-2.3 [25]. 2) 2 Open-source video models: Wan2.5-i2v-preview [38], Wan2.2-TI2V-5B [40]. 3) 3 VLMs: Gemini-2.5-pro [6], Gpt-5 high [26], Qwen2.5-VL7B [2]. The settings of the baseline are in the appendix. Task Base Irrg Trap 3D Soko EM Easy Med. Hard SR Easy Med. Hard PR Easy Med. Hard SD Easy Med. Hard 0.0 0.0 (+0.0) 0.0 83.3 (+83.3) 0.0 62.5 (+62.5) 0.0 41.7 (+41.7) 0.0 4.2 (+4.2) 0.0 4.2 (+4.2) 0.0 66.7 (+66.7) 0.0 0.0 (+0.0) 0.0 0.0 (+0.0) 0.0 0.0 (+0.0) 0.0 0.0 (+0.0) 0.0 54.2 (+54.2) 0.0 12.5 (+12.5) 0.0 4.2 (+4.2) 0.0 0.0 (+0.0) 8.3 83.3 (+75.0) 29.2 95.8 (+66.6) 0.0 100.0 (+100.0) 54.2 100.0 (+45.8) 20.8 83.3 (+62.5) 8.3 41.7 (+33.4) 4.2 87.5 (+83.3) 0.0 95.8 (+95.8) 16.7 79.2 (+62.5) 8.3 45.8 (+37.5) 4.2 58.3 (+54.1) 4.2 62.5 (+58.3) 0.0 62.5 (+62.5) 25.0 83.3 (+58.3) 4.2 33.3 (+29.1) 13.2 40.1 (+26.9) 13.4 88.0 (+74.6) 6.0 86.7 (+80.7) 7.6 78.7 (+71.1) 14.6 62.2 (+47.6) 3.8 26.1 (+22.3) 8.1 74.8 (+66.7) 6.4 43.7 (+37.3) 9.8 47.4 (+37.6) 8.3 12.5 (+4.2) 2.7 6.0 (+3.3) 5.8 68.4 (+62.6) 8.9 56.7 (+47.8) 15.2 58.7 (+43.5) 5.6 10.4 (+4.8) 154.8 28.2 (-126.6) 48.3 3.5 (-44.8) 2.1 () 53.0 4.6 (-48.4) 354.2 18.8 (-335.4) 10.1 () 7.8 () 5.9 () 87.9 10.2 (-77.7) 61.4 84.5 (+23.1) () 39.3 3.1 (-36.2) 3.5 () 33.6 13.8 (-19.8) 16.1 () Table 3. Difficulty generalization of Wan-R1 on VR-Bench. Each task block compares the baseline (Wan2.2-TI2V-5B) and the finetuned model (trained only on Easy level) across difficulty levels (Easy, Medium, Hard) and four metrics (EM, SR, PR, SD). Green indicates improvements, red indicates degradation, gray denotes no change or undefined cases. 5. Insights and Discussions In this section, we discuss the observations and insights we draw from our comprehensive evaluation experiments. Wan-R1 Outperforms Prior Models on VR-Bench. As shown in Table 1, our method consistently achieves top performance across nearly all tasks and evaluation metrics, demonstrating both high accuracy and rollout efficiency. Notably, Wan-R1 attains perfect SR of 100.0 on the Trap and 3D maze tasks, highlighting its robust success capabilities even in complex environments. Compared to its base model Wan2.2-TI2V-5B, Wan-R1 achieves remarkable EM improvement of +65.3 on 3D, and reduces SD by 100.1 on Soko. These gains underscore the effectiveness of our fine-tuning strategy in enhancing both correctness and trajectory quality across diverse reasoning settings. Success Alone Does Not Guarantee Efficient Reasoning. Some models manage to complete tasks, but their rollouts remain highly inefficient. For instance, Sora-2 and MiniMax-Hailuo-2.3 achieve strong SR of 75.0 and 68.1 on the Base task, yet their corresponding SD values reach 302.9 and 464.0, revealing substantial path redundancy. Even more striking, the open-source VLM Qwen2.5-VL-7B produces very low SR of 1.4, but still yields high SD of 300.0, indicating unstable or erratic generation. As shown in Table 1, Wan-R1 achieves comparable or better SR while reducing SD to just 10.3, demonstrating its ability to generate correct and efficient trajectories consistently. Reasoning via Video Outperforms Reasoning via Text. Under the same training data and settings, we fine-tune both the vision-language model (Qwen2.5-VL-7B) and the video model (Wan2.2-TI2V-5B). As shown in Table 1, the videobased model (Wan-R1) yields significantly larger gains across all metrics and tasks, especially in challenging settings like Trap and 3D. In contrast, Qwen2.5-VL-7B-SFT Task Model Base Irreg Wan2.2-TI2V-5B Baseline 0.0 Regular Maze Fine-tuned Irregular Maze Fine-tuned 3D Maze Fine-tuned Sokoban Fine-tuned Trapfield Fine-tuned 33.3 (+33.3) 0.0 (+0.0) 0.0 (+0.0) 0.0 (+0.0) 0.0 (+0.0) 0.0 5.6 (+5.6) 56.9 (+56.9) 5.6 (+5.6) 1.4 (+1.4) 0.0 (+0.0) EM Trap 0. 1.4 (+1.4) 0.0 (+0.0) 0.0 (+0.0) 1.4 (+1.4) 38.9 (+38.9) 3D 0.0 0.0 (+0.0) 0.0 (+0.0) 65.3 (+65.3) 0.0 (+0.0) 0.0 (+0.0) Soko Base Irreg 0.0 0.0 (+0.0) 0.0 (+0.0) 0.0 (+0.0) 4.2 (+4.2) 0.0 (+0.0) 6.9 76.4 (+69.5) 11.1 (+4.2) 38.9 (+32.0) 0.5 (-6.4) 93.1 (+86.2) 12.5 8.3 (-4.2) 69.4 (+56.9) 31.9 (+19.4) 22.2 (+9.7) 40.3 (+27.8) SR Trap 0.0 88.9 (+88.9) 52.8 (+52.8) 30.6 (+30.6) 18.1 (+18.1) 100.0 (+100.0) 3D Soko Base Irreg 31.9 11.1 6.6 9.1 69.4 (+37.5) 79.2 (+47.3) 100.0 (+68.1) 22.2 (-9.7) 79.2 (+47.3) 30.6 (+19.5) 12.5 (+1.4) 20.8 (+9.7) 69.4 (+58.3) 6.9 (-4.2) 60.6 (+54.0) 16.6 (+10.0) 6.8 (+0.2) 15.7 (+9.1) 10.9 (+4.3) 22.7 (+13.6) 71.6 (+62.5) 20.6 (+11.5) 23.7 (+14.6) 12.9 (+3.8) PR Trap 7.1 25.2 (+18.1) 18.1 (+11.0) 6.7 (-0.4) 36.2 (+29.1) 79.1 (+72.0) 3D Soko Base SD Irreg Trap 12. 13.7 (+0.9) 16.8 (+4.0) 93.5 (+80.7) 15.7 (+2.9) 14.7 (+1.9) 9. 388.7 66.1 19.0 (+9.8) 15.5 (+6.3) 15.0 (+5.8) 44.3 (+35.1) 10.0 (+0.8) 10.3 (-378.4) 35.8 (-352.9) 108.2 (-280.5) 46.3 (-342.4) 57.5 (-331.2) 51.7 (-14.4) 2.4 (-63.7) 10.9 (-55.2) 34.4 (-31.7) 16.8 (-49.3) 3.8 () 6.9 () 8.8 () 39.9 () 3.9 () 3D 5.4 12.7 (+7.3) 9.0 (+3.6) 3.9 (-1.5) 20.1 (+14.7) 11.4 (+6.0) Soko 176.6 49.3 (-127.3) 40.7 (-135.9) 80.6 (-96.0) 10.2 (-166.4) 57.8 (-118.8) Table 4. Comparison between the baseline (Wan2.2-TI2V-5B) and task-specific fine-tuned models across five game types (Base, Irreg, Trap, 3D, Soko). Each cell reports absolute performance and relative change over the baseline on four metrics: EM, SR, PR, and SD. Green indicates improvement, red indicates degradation, and gray denotes no change or undefined difference. shows only moderate improvements. This highlights the advantage of reasoning via video in learning temporal reasoning and efficient path planning over static VLMs. Rule Compliance and Structural Fidelity. As shown in Table 2, our model Wan-R1 consistently achieves the highest VLM-Score across all maze types, and performs competitively in MF, ranking top-2 in most categories: 1) it consistently attains the highest VLM-Score across all maze types ( 4.0), indicating superior rule-following behavior in motion continuity, physical plausibility, and environmental interactions; 2) it ranks among the top performers in MF, especially excelling on structurally complex mazes like Irreg and 3D. 3) it shows consistent improvements over its base model Wan2.2-TI2V-5B, confirming the effectiveness of our training paradigm in enhancing both visual stability and behavioral correctness. Reasoning via Video Scales Better. 1) As illustrated in Figure 4, model performance consistently declines with increasing maze difficulty on the Trap and Irre Maze tasks. In Easy settings, VLMs often match or even surpass state-ofthe-art video models. However, as the maze complexity escalates, VLMs experience sharper performance drop compared to video models. On large-scale hard mazes, even toptier VLMs such as Gemini-2.5-Pro and GPT-5 are outperformed by leading video models like Sora-2 and Seedance1.0-pro. 2) We attribute this trend to fundamental differences in the reasoning paradigms of the two model families. VLMs rely on encoding static visual observations into textual tokens and performing reasoning within languagedominant latent space. As maze size increases, the number of visual tokens grows substantially, leading to contextlength saturation and degraded long-horizon reasoning. In contrast, video models reason via visual dynamics, constructing temporally grounded CoF that maintains spatial continuity across time. This video-centric reasoning mechanism preserves efficiency as scene complexity increases, since the number of visual tokens per frame remains stable, with visual tokens carrying significantly higher information density than textual tokens, finding validated by DeepSeek OCR [43] through optical context compression. Remarkably, models such as Sora-2 even exhibit improved performance in the Irreg Maze under higher difficulty levels, particularly in the SR metric. 3) These observations suggest that reasoning via video constitutes more native and scalable paradigm for visual reasoning, enabling temporalspatial problem solving that remains robust under increasing environmental complexity. Figure 4. Model performance (PR and SR) on Irregular Maze and Trapfield across difficulty levels. Each curve represents baseline, while the dashed and dotted lines indicate VLM and Video Model averages. Results for other maze types are in the Appendix. Test-Time Scaling for Video Models. 1) Test-time scaling (TTS), exemplified by self-consistency [42], has shown strong effectiveness in text-based reasoning tasks. Its key intuition is that complex reasoning problems often admit multiple valid solution trajectories, and sampling diverse paths increases the likelihood of converging to correct answer. Maze-solving naturally shares this property: the solution space is open-ended, and multiple routes can lead to the goal. Motivated by this, we apply TTS to video models 7 by perturbing the sampling noise to generate diverse rollouts, and evaluate performance using Pass@K, which selects the best solution among independent attempts. 2) Figure 5 shows the scaling behavior of Wan-R1 on the Irregular Maze benchmark. As increases from 1 to 16, the model achieves steady gains across all difficulty levels, with improvements of roughly 1020% depending on metric and difficulty. On Easy mazes, performance rises almost monotonically and nears saturation at higher K. Medium difficulty shows clear early gains, often improving by 5 10% from = 1 to = 4, followed by continued smaller increases. Even on Hard mazes, where Pass@1 scores are lower, TTS still yields consistent upward trends, recovering solutions that single attempts often miss. 3) Overall, these results demonstrate that TTS significantly enhances videomodel reasoning in VTR tasks. By initializing generation from different noise conditions, the model explores multiple solution pathways within the mazes open-ended search space. This multi-path exploration effectively unlocks additional reasoning capacity, enabling video models to produce trajectories that are more accurate, more reliable, and consistently closer to the desired solution than those obtained under standard single-sample inference. Difficulty Generalization The results in Table 3 demonstrate the strong difficulty generalization capability of WanR1 across all five maze tasks. Although the model is finetuned only on the Easy level, it consistently delivers substantial improvements over the Wan2.2-TI2V-5B baseline on Medium and Hard mazes as well. This indicates that WanR1 does not simply memorize small or low-complexity layouts; rather, it internalizes more principled and transferable reasoning procedure. The gains observed across unseen difficulty tiers show that fine-tuning on small mazes induces broad generalization: the model acquires mazesolving strategy that scales to larger, more intricate structures without additional supervision. Such behavior reflects deeper structural understanding of the environment and verifies that Wan-R1s improvements stem from reasoningpattern internalization rather than task-specific overfitting. Maze Type Generalization. As shown in Table 4, finetuning on single game (e.g., Regular Maze, Trapfield, or 3D Maze) not only improves in-domain performance but also yields substantial gains on unseen games across all metrics (EM, SR, PR, SD). This highlights the emergence of transferable video reasoning capabilities. Notably, models fine-tuned on 3D Maze exhibit strong generalization, this overall transfer pattern underscores that training on complex 3D structures fosters general reasoning skills applicable to other maze types. Texture Generalization. Although fine-tuned only on the Raw skin of each game type, the model shows consistent and often substantial gains on unseen textures (Skin2 and Skin3). As shown in Table 5, this pattern holds across Task EM Raw Skin2 Skin3 Raw SR Skin2 Skin3 PR Raw Skin2 Skin3 SD Raw Skin2 Skin Base Irreg 3D Soko Trap 0.0 33.3 (+33.3) 0.0 56.9 (+56.9) 0.0 65.3 (+65.3) 0.0 4.2 (+4.2) 0.0 38.9 (+38.9) 0.0 1.4 (+1.4) 0.0 22.2 (+22.2) 0.0 43.1 (+43.1) 0.0 0.0 (+0.0) 0.0 9.7 (+9.7) 0.0 23.6 (+23.6) 0.0 15.3 (+15.3) 0.0 50 (+50.0) 0.0 1.4 (+1.4) 0.0 0.0 (+0.0) 6.9 76.4 (+69.5) 12.5 69.4 (+56.9) 0.0 100.0 (+100.0) 31.9 69.4 (+37.5) 11.1 100.0 (+88.9) 4.2 38.9 (+34.7) 4.2 15.3 (+11.1) 4.2 97.2 (+93.0) 2.8 34.7 (+31.9) 0.0 38.9 (+38.9) 2.8 68.1 (+65.3) 2.1 23.6 (+21.5) 4.2 100 (+95.8) 7.1 58.3 (+51.2) 0.0 1.4 (+1.4) 6.6 60.6 (+54.0) 9.1 71.6 (+62.5) 7.1 93.5 (+86.4) 12.8 44.3 (+31.5) 9.2 79.1 (+69.9) 4.6 12.3 (+7.7) 12.0 36.5 (+24.5) 8.9 83.8 (+74.9) 9.3 21.0 (+11.7) 9.0 29.0 (+20.0) 9.4 51.0 (+41.6) 47.8 26.2 (-21.6) 8.3 86.8 (+78.5) 8.1 14.1 (+6.0) 7.5 9.9 (+2.4) 388.7 10.3 (-378.4) 66.1 2.4 (-63.7) 3.9 () 5.4 10.2 (+4.8) 176.6 3.9 (-172.7) 11.7 19.0 (+7.3) 39.5 5.1 (-34.4) 73.6 6.4 (-67.2) 58.6 () 8.7 () 14.9 9.0 (-5.9) 42.0 7.7 (-34.3) 59.3 5.9 (-53.4) 82.6 () 18.5 () Table 5. Texture generalization under different skin. For each task, the table reports the baseline performance, the results after finetuning on the Raw texture, and the relative change across three texture conditions (Raw, Skin2, Skin3) on EM, SR, PR, and SD. Green denotes improvement, red denotes degradation, and gray indicates no change or undefined differences. all five task domains. For example, in the Base task, Skin3never encountered during trainingimproves by +23.6 in EM and +41.6 in PR. In the more challenging 3D Maze, the generalization is even stronger, reaching +50.0 EM and +78.5 PR on Skin3. These results demonstrate the models strong texture-level generalization and indicate that the learned spatio-temporal reasoning transfers robustly to new visual styles. Figure 5. Performance on Irregular Maze using Wan-R1 under test-time scaling. Results are shown across different sampling numbers (K 1, 4, 8, 12, 16) and difficulty levels (Easy, Medium, Hard). Results for other maze types are in the Appendix. 6. Conclusion In this work, we take step forward in evaluating whether video models can reason via video generation. We propose VR-Bench, comprehensive benchmark grounded in 8 maze-solving tasks to assess the spatial reasoning ability of video models. Our experiments demonstrate that fine-tuned video models exhibit strong spatial reasoning and consistently outperform leading vision-language models. Moreover, our analysis reveals test-time scaling effect akin to self-consistency in language models, underscoring the scalable potential of video-based reasoning. Limitations and Future Work. While VR-Bench provides focused and rigorous testbed for spatial reasonFuit currently emphasizes maze-centric tasks. ing, ture iterations of VR-Bench will explore broader and more challenging reasoning scenarios. For instance, we plan to incorporate Olympiad-level problem-solving tasks, such as solving complex physics or mathematics competition problems via video-based visual reasoning. In addition, we aim to support embodied reasoning settings where models are required to predict or simulate coherent action sequences within interactive environments."
        },
        {
            "title": "References",
            "content": "[1] Rohan Anil, Andrew Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023. 3 [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and JunarXiv preprint yang Lin. Qwen2.5-vl technical report. arXiv:2502.13923, 2025. 2, 6 [3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2, 3 [4] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators. OpenAI Blog, 1(8):1, 2024. 3 [5] Xinyan Chen, Renrui Zhang, Dongzhi Jiang, Aojun Zhou, Shilin Yan, Weifeng Lin, and Hongsheng Li. Mint-cot: Enabling interleaved visual tokens in mathematical chain-ofthought reasoning. arXiv preprint arXiv:2506.05331, 2025. 2 [6] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 2, 6 [7] Google DeepMind. Veo 2, 2024. Accessed: 2024. 3 [8] Google DeepMind. Veo-3 technical report. Technical report, [9] Chengqi Duan, Rongyao Fang, Yuqing Wang, Kun Wang, Linjiang Huang, Xingyu Zeng, Hongsheng Li, and Xihui Liu. Got-r1: Unleashing reasoning capability of mllm for visual generation with reinforcement learning. arXiv preprint arXiv:2505.17022, 2025. [10] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph FeichtenIn Proceedings of hofer. Multiscale vision transformers. the IEEE/CVF international conference on computer vision, pages 68246835, 2021. 3 [11] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. 6 [12] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 3 [13] Ziyu Guo, Xinyan Chen, Renrui Zhang, Ruichuan An, Yu Qi, Dongzhi Jiang, Xiangtai Li, Manyuan Zhang, Hongsheng Li, and Pheng-Ann Heng. Are video models ready as zero-shot reasoners? an empirical study with the mme-cof benchmark. arXiv preprint arXiv:2510.26802, 2025. 3 [14] Hui Han, Siyuan Li, Jiaqi Chen, Yiwen Yuan, Yuling Wu, Yufan Deng, Chak Tou Leong, Hanwen Du, Junchen Fu, Youhua Li, et al. Video-bench: Human-aligned video generation benchmark. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1885818868, 2025. 3 [15] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. [16] Kairui Hu, Penghao Wu, Fanyi Pu, Wang Xiao, Yuanhan Zhang, Xiang Yue, Bo Li, and Ziwei Liu. Video-mmmu: Evaluating knowledge acquisition from multi-discipline professional videos. arXiv preprint arXiv:2501.13826, 2025. 3 [17] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. 3 [18] Ziqi Huang, Fan Zhang, Xiaojie Xu, Yinan He, Jiashuo Yu, Ziyue Dong, Qianli Ma, Nattapol Chanpaisit, Chenyang Si, Yuming Jiang, et al. Vbench++: Comprehensive and versatile benchmark suite for video generative models. arXiv preprint arXiv:2411.13503, 2024. 3, 5 [19] Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, et al. Mme-cot: Benchmarking chain-of-thought in large multimodal models for reasoning quality, robustness, and efficiency. arXiv preprint arXiv:2502.09621, 2025. 2 [20] Kling AI. Kling Video Generation Platform, 2025. Accessed Google DeepMind, 2025. 3, 6 on November 25, 2025. 9 [21] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 3 [22] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 2 [23] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 32023211, 2022. 3 [24] LumaLabs. Dream machine, 2024. Accessed: 2024. 3 [25] MiniMax. MiniMax Hailuo 2.3, 2024. Accessed on November 25, 2025. 6 [26] OpenAI. GPT-5, 2025. Accessed on November 25, 2025. 6 [27] OpenAI. OpenAI o3 and o4-mini System Card. Technical report, OpenAI, 2025. Accessed: 2025-11-01. 3 [28] OpenAI. Sora 2 system card. Technical report, OpenAI, 2025. 6 [29] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. 3 [30] PikaLabs. Pika 1.5, 2024. Accessed: 2024. 3 [31] Runway. Introducing gen-3 alpha: new frontier for video generation, 2024. Accessed: 2024. [32] Alex Su, Haozhe Wang, Weiming Ren, Fangzhen Lin, and Wenhu Chen. Pixel reasoner: Incentivizing pixel-space reasoning with curiosity-driven reinforcement learning. arXiv preprint arXiv:2505.15966, 2025. 3 [33] Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, and Chenliang Xu. Audio-visual event localization in unconstrained videos. In Proceedings of the European conference on computer vision (ECCV), pages 247263, 2018. 3 [34] Jingqi Tong, Yurong Mou, Hangcheng Li, Mingzhe Li, Yongzhuo Yang, Ming Zhang, Qiguang Chen, Tianyi Liang, Xiaomeng Hu, Yining Zheng, et al. Thinking with video: Video generation as promising multimodal reasoning paradigm. arXiv preprint arXiv:2511.04570, 2025. 3 [35] Jingqi Tong, Jixin Tang, Hangcheng Li, Yurong Mou, Ming Zhang, Jun Zhao, Yanbo Wen, Fan Song, Jiahao Zhan, Yuyang Lu, et al. Code2logic: Game-code-driven data synthesis for enhancing vlms general reasoning. arXiv preprint arXiv:2505.13886, 2025. 4 [36] Tony Cheng Tong, Sirui He, Zhiwen Shao, and Dit-Yan Yeung. G-veval: versatile metric for evaluating imIn Proceedings of age and video captions using gpt-4o. the AAAI Conference on Artificial Intelligence, pages 7419 7427, 2025. 3 [37] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. Advances in neural information processing systems, 35:1007810093, 2022. 3 [38] Wan. Wan2.5-i2v-preview, 2025. Accessed on November 25, 2025. 6 [39] Haiyuan Wan, Chen Yang, Junchi Yu, Meiqi Tu, Jiaxuan Lu, Di Yu, Jianbao Cao, Ben Gao, Jiaqing Xie, Aoran Wang, et al. Deepresearch arena: The first exam of llms research abilities via seminar-grounded tasks. arXiv preprint arXiv:2509.01396, 2025. 2, 3 [40] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 3, 6 [41] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. 3 [42] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. 7 [43] Haoran Wei, Yaofeng Sun, and Yukun Li. DeepseekarXiv preprint ocr: Contexts optical compression. arXiv:2510.18234, 2025. 7 [44] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 2, 3 [45] Thaddaus Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, and Robert Geirhos. Video models are zero-shot learners and reasoners. arXiv preprint arXiv:2509.20328, 2025. 1, 3 [46] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. 3 [47] Jinheng Xie, Zhenheng Yang, and Mike Zheng Shou. ShowarXiv Improved native unified multimodal models. o2: preprint arXiv:2506.15564, 2025. [48] Yi Xu, Chengzu Li, Han Zhou, Xingchen Wan, Caiqi Zhang, Anna Korhonen, and Ivan Vulic. Visual planning: Lets think only with images. arXiv preprint arXiv:2505.11409, 2025. 4 [49] Cheng Yang, Jiaxuan Lu, Haiyuan Wan, Junchi Yu, and Feiwei Qin. From what to why: multi-agent system for evidence-based chemical reaction condition reasoning. arXiv preprint arXiv:2509.23768, 2025. 2 [50] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. [51] Junchi Yu, Ran He, and Zhitao Ying. Thought propagation: An analogical approach to complex reasoning with large language models. In The Twelfth International Conference on Learning Representations, 2024. 10 [52] Zhaoyang Yu, Jiayi Zhang, Huixue Su, Yufan Zhao, Yifan Wu, Mingyi Deng, Jinyu Xiang, Yizhang Lin, Lingxiao Tang, Yingchao Li, et al. Recode: Unify plan and arXiv preprint action for universal granularity control. arXiv:2510.23564, 2025. 2 [53] Shenghai Yuan, Jinfa Huang, Yongqi Xu, Yaoyang Liu, Shaofeng Zhang, Yujun Shi, Rui-Jie Zhu, Xinhua Cheng, Jiebo Luo, and Li Yuan. Chronomagic-bench: benchmark for metamorphic evaluation of text-to-time-lapse video generation. Advances in Neural Information Processing Systems, 37:2123621270, 2024. [54] Jiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng, Xionghui Chen, Jiaqi Chen, Mingchen Zhuge, Xin Cheng, Sirui Hong, Jinlin Wang, et al. Aflow: Automating agentic workflow generation. arXiv preprint arXiv:2410.10762, 2024. 2 [55] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. 3 [56] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing thinking with images via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025. 3 [57] Muzhi Zhu, Hao Zhong, Canyu Zhao, Zongze Du, Zheng Huang, Mingyu Liu, Hao Chen, Cheng Zou, Jingdong Chen, Ming Yang, et al. Active-o3: Empowering multimodal large arXiv language models with active perception via grpo. preprint arXiv:2505.21457, 2025. 3 11 Reasoning via Video: The First Evaluation of Video Models Reasoning Abilities through Maze-Solving Tasks"
        },
        {
            "title": "Supplementary Material",
            "content": "7. Experiment Details 7.1. Implementation Details of Baselines Video Models. To ensure fair comparison across different video models, we standardized input preprocessing and output postprocessing. Since most maze images are square (1 : 1), we applied black-border padding to satisfy modelspecific resolution or aspect ratio requirements, followed by center cropping to restore the maze region. Generated videos were temporally aligned according to each models fixed or adjustable duration settings. Veo-3.1 and Veo-3.1-Pro generate fixed 8-second videos in 9 : 16 aspect ratio, with black-border padding and postcropping applied. Doubao outputs 10-second, 1 : 1 videos, padding and cropping inputs smaller than 300 300 pixels. Kling produces 10-second videos without further adjustments. MiniMax also yields 10-second videos at its default 768p resolution, using the same paddingcropping scheme for small inputs. Sora-2 generates 10-second, 9 : 16 videos, followed by black-border padding and cropping to maintain maze integrity. VLM. Given an initial observation image I0, the VLM predicts an action sequence apred = [a1, . . . , aT ], representing its intended movements in the environment. The actions are sequentially executed in the simulator to verify trajectory validity against the optimal reference aopt. For each task type, we define its corresponding action space. In TrapField, Sokoban, and Regular Maze, actions correspond to four-directional moves {up, down, left, right}, and the VLM outputs sequences such as [up, In the Irregular Maze, actions are deright, down]. fined over irregular graph nodes (A, B, . . . ), where the VLM outputs node transition sequences like [A, C, E]; the model is evaluated on whether these transitions correspond to valid path connections. For the 3D Maze, actions include six directional movements covering both horizontal and vertical axes, and the VLM outputs sequences such as [forward left, up, forward right]. 7.2. Training Details We fine-tuned one model for each of the five maze categories in our benchmark. For each category, the fine-tuning set consists of 80% of the data from the first skin, covering the easy, medium, and hard difficulty levels. All models were trained using the DiffSynth-Studio framework. The training hyperparameters are summarized in Table 6."
        },
        {
            "title": "Frame resolution\nNumber of frames\nDataset repeat factor\nBase model\nLoRA rank\nLearning rate\nEpochs",
            "content": "512 512 193 100 Wan2.2-TI2V-5B 32 1 104 5 Table 6. Key training parameters used in our fine-tuning setup. Figure 6. Dynamic visualization of trajectory tracking across five maze types. For each task, the green polyline denotes the groundtruth trajectory, while the blue polyline represents the trajectory tracked from the generated video. Each column shows temporally ordered frame sampled from the tracking process, illustrating how the agents motion evolves over time. 8. Evaluation Details 8.1. Path Matching Setup For each generated video clip we load its UnifiedState description and enumerate all candidate ground-truth (GT) videos sharing the same scene identifier. The generated clips spatial resolution and frame rate define unified evaluation specification; both generated and GT videos are resized and temporally resampled to this specification before comparison. Trajectory Extraction The controllable agents initial bounding box is read from the state and scaled to the evaluation resolution. An object tracker (priority order: CSRT, with fallbacks KCF/MOSSE) is initialized on the first frame. Tracking proceeds frame by frame; sampling follows fixed temporal interval derived from the unified frame rate. When tracking fails on sampling frame, the last valid center is reused to preserve temporal alignment. The output is pixel-space polyline of agent centers across time, paired with the first frame for later visualization. Normalization and Resampling Extracted pixel trajectories are normalized to the unit square using the evaluated videos width and height. Physical-distance resampling is applied: the GT trajectory is resampled into fixed number of equidistant points along arc length, and the generated trajectory is interpolated at the same cumulative distances (clipped to its total length). This produces speed-invariant, sparsity-controlled trajectory pairs. GT Selection and Outputs Each generated clip is evaluated against all candidate GT trajectories using score based solely on trajectory length consistency; the GT with the highest score is selected as the match. For every generated clip, we record the matched GT identity and per-clip summary statistics, and we produce visual diagnostics that overlay GT (green) and generated (blue) resampled trajectories on the first frame, along with start (yellow) and goal (red) bounding boxes. Batch evaluation yields per-difficulty summaries and overall totals, enabling direct comparison of trajectory performance across difficulty levels. 8.2. Rule Compliance Human Alignment To verify that our VLM-as-Judge evaluation faithfully reflects human judgment across all five diagnostic dimensions, we conducted large-scale human preference study over diverse set of generated videos. The annotation protocol follows standard practices used in prior video-evaluation benchmarks, with controlled instructions, representative examples, and multiple rounds of cross-checking to ensure consistent and reliable human annotations. In line with established evaluation frameworks, we computed the correlation between the VLM-as-Judge scores and human preference outcomes. Figure 7 presents the alignment plots, showing the relationship between human win ratios and our VLM-based scores for each dimension, including Motion Continuity, Temporal Consistency, Trajectory Rationality, Structural Consistency, and Interactional Rationality. linear regression is fitted for visualization, and the Pearson correlation coefficient (ρ) is reported for each dimension. Across all five dimensions, VLM-score aligns closely with human judgment, demonstrating that our proposed evaluation protocol provides reliable and humanconsistent assessment of video reasoning quality. 9. Additional Analysis Test-Time Scaling We evaluate how increasing the number of test-time trajectory samples {1, 4, 8, 12, 16} influences model performance across different maze types and difficulty levels. As illustrated in Figure 9, larger values generally lead to improved performance across four evaluation metrics: Exact Match (EM), Success Rate (SR), Precision Rate (PR), and Step Deviation (SD). For Regular Maze, performance improvements are consistent and significant across difficulty levels. All metricsespecially EM and PRdemonstrate strong upward trends with increasing K, particularly for hard instances, indicating enhanced trajectory fidelity and goal alignment. In Maze3D, higher yields substantial gains on EM and PR across all difficulty levels. Notably, SR remains saturated (close to 1.0), suggesting that while reaching the goal is easy, fine-grained precision benefits from more diverse samples. SD decreases with K, highlighting trajectory smoothness gains. Sokoban exhibits the most challenging dynamics: absolute scores remain low, especially on EM and PR. However, performance steadily improves as increases, most noticeably on the easy setting. This reflects the high complexity introduced by object manipulation and interaction constraints. For Trapfield, metrics show moderate but consistent gains, with gradual rise in EM and PR as increases. SR is already saturated at lower values, and SD maintains stability, implying that while high-level goals are achievable, detailed path planning benefits from greater sample diversity. Overall, the results suggest that test-time scaling is an effective strategy for enhancing model robustness, particularly in structurally complex or interaction-heavy environments. The diminishing returns observed on easy levels also motivate future directions such as difficulty-adaptive sampling strategies. 2 Figure 7. Human Alignment of VLM-as-Judge Evaluation. Each plot corresponds to Motion Continuity, Temporal Consistency, Trajectory Rationality, Structural Consistency, or Interactional Rationality. Dots show the human preference win ratio (horizontal axis) and the VLM-score (vertical axis) for nine video generation models. linear fit visualizes the correlation, and Pearsons correlation coefficient (ρ) is reported for each dimension, indicating close alignment between VLM-as-Judge scores and human judgment across all five dimensions. Figure 8. Model performance across all five maze types and difficulty levels. Each subplot corresponds to specific maze type (Base, Irregular, Trapfield, 3D, Sokoban) and evaluation metric (Exact Match, Success Rate, Precision Rate, or Step Deviation). Each curve represents baseline model, while the dashed and dotted lines denote the VLM and Video Model averages. Full metric trends across difficulty levels illustrate the degradation patterns and performance differences among models. Model Performance Across Difficulty Levels To better understand model behavior under varying spatial structures and difficulty levels, we conduct comprehensive analysis over all five maze types using all four evaluation metrics (EM, SR, PR, SD). As shown in Fig. 8, this section highlights global performance trends, examines cross-model and cross-metric differences, and discusses how structural characteristics of each maze family amplify specific failure modes. Together, these observations provide detailed view of the strengths and limitations of both video models and VLMs in long-horizon trajectory reasoning tasks. 1) Overall Difficulty Trends. Performance consistently declines with increasing difficulty across all maze types and metrics. Across all five maze types (Base, Irregular, Trapfield, 3D, Sokoban), model performance decreases steadily from Easy to Hard difficulty, and this pattern is consistent across EM, SR, PR, and SD. The decline is most pronounced in Irregular and Trapfield, where complex geometry and branching paths increase long-horizon planning difficulty. On Easy mazes, several VLMs perform 3 Figure 9. Performance across four maze types using Wan-R1 under test-time scaling. Results are shown across different sampling numbers (K {1, 4, 8, 12, 16}) and difficulty levels (Easy, Medium, Hard). Each subplot corresponds to one maze type (Regular Maze, Irregular Maze, Trapfield, and Sokoban) and one evaluation metric (Exact Match, Success Rate, Precision Rate, or Step Deviation), illustrating how test-time scaling influences trajectory accuracy and stability across diverse maze structures. comparably to video models, but their performance drops much faster as maze complexity increases. On Hard mazes, even strong VLMs such as Gemini-2.5-pro and GPT-5-high are frequently surpassed by video models like Sora-2 and Seedance-1.0-pro. In contrast, Base and Sokoban show milder degradation: Base Maze remains structurally simple, allowing more stable PR and SR, while Sokobans gridbased layout reduces positional ambiguity despite strict EM constraints. 2) Cross-Metric and Cross-Model Patterns. Video models display stronger robustness, while VLMs show high variance across maze structures. Across metrics, video models such as Sora-2, Seedance-1.0-pro, and Veo-3.1pro consistently rank highest, especially in PR and SR, which rely on spatial alignment and correct goal attainment. VLMs including Gemini-2.5-pro, Qwen2.5-VL-7B, and GPT-5-high exhibit large performance variance: they perform well on simpler layouts like Base Maze but degrade sharply on Trapfield and 3D Maze. The difference becomes particularly apparent in SD, where VLM trajectories drift substantially as maze complexity grows. EM remains the strictest metric, and most models fail to achieve meaningful EM on Hard variants of Irregular, Trapfield, 3D Maze, and Sokoban, reflecting the difficulty of producing fully correct long-horizon trajectories. 3) Structural Effects Across Maze Families. Different maze structures stress models in distinct ways, highlighting complementary challenges. Irregular Maze and Trapfield serve as the strongest discriminators of model capability: only few video models maintain usable SR and PR at 4 Across the five maze families, these templates provide video models with precise generative instructions, while giving VLMs structured reasoning prompts for consistent evaluation. Figure 10. Failure case: incorrect reasoning leads to unreachable path. Although the model attempts to reach the target, its predicted trajectory (in blue) passes through an infeasible region, resulting in failure to arrive at the goal (red). The ground-truth trajectory is shown in green. Figure 11. Failure case: duplicated object during reasoning. During inference, the reasoning object unexpectedly appears in two locations simultaneously, indicating visual inconsistency likely caused by faulty temporal coherence. This distracts the model and leads to an invalid reasoning path (in blue), which fails to reach the correct goal (red). Figure 12. Failure case: suboptimal path to target. The model starts at the green point and eventually reaches the red goal, but takes significantly longer path than the shortest possible route. This suggests limitations in global path planning and trajectory efficiency. Hard difficulty, while most VLMs collapse almost entirely. 3D Maze introduces challenges related to depth perception and occlusion, resulting in instability in PR and SD even at Medium difficulty. Sokoban emphasizes rule consistency: minor violations lead directly to failure, causing sharp drops in EM and SR on Hard difficulty. Overall, these results show that video models maintain stronger spatial-temporal consistency as difficulty increases, whereas VLMs perform well primarily when maze structures are simple or when short-horizon reasoning dominates. The diversity of maze structures provides comprehensive stress test for evaluating trajectory fidelity. 10. Failure cases To illustrate typical failure cases, we adopt three-frame visualization format for each example: Left: The initial frame of the reasoning video, showing the static scene before any agent action. Middle: representative intermediate frame capturing the failure behavior during the reasoning process. Right: trajectory visualization comparing the predicted and ground-truth reasoning paths. The green line denotes the ground-truth trajectory, the blue line represents the models predicted path, the yellow square indicates the starting point, and the red square marks the final destination. These visualization conventions are applied consistently in all subsequent failure case illustrations (see Figures 10 20). 11. Prompt Template As shown in Tables, we design unified prompt templates for all five maze types (Base, Irregular, Trapfield, 3D, and Sokoban), each provided in two variants tailored for video models and VLMs. The video model prompts are presented in gray, reflecting their focus on motion-generation constraints, while the VLM prompts are shown in light blue, highlighting their emphasis on trajectory interpretation and rule compliance. Each maze type follows consistent prompt structure composed of system-level prompt and task-specific instructions. For video models, the system prompt specifies the maze layout, agent behavior rules, and rendering constraints, followed by an execution prompt that defines: (1) the valid movement space; (2) expected trajectory behavior; and (3) visual requirements for temporal and spatial consistency across frames. For VLMs, the system prompt defines the evaluation context, and the task prompt formulates: (1) the extracted agent trajectory; (2) rule-checking dimensions; and (3) the expected reasoning format for judging motion validity. 5 Figure 13. Failure case: invalid path through obstacles. The model starts from the blue point and attempts to reach the green goal, but its predicted trajectory crosses red obstacle regions, violating the environments physical constraints. Failure case: moving object disappears midFigure 17. sequence. The model begins reasoning from the blue object and generates few initial frames correctly. However, the moving object then vanishes unexpectedly, leaving only the path trace without visible agent completing the trajectory. Figure 14. Failure case: spurious object motion and goal inactivity. During reasoning, the model introduces moving object unrelated to the task, while the actual target object remains static. This reflects failure to correctly ground the reasoning process on the intended object and leads to an incorrect trajectory. Figure 18. Failure case: reversed motion direction and object deformation. The task requires the puck to move toward the goal hole, but the model instead makes the hole move toward the puck. Moreover, the goal object is mistakenly deformed into green sphere, indicating failure in both physical role understanding and object appearance consistency. Figure 15. Failure case: target object replaced by incorrect appearance. The target object is initially carrot, but during reasoning it is incorrectly replaced by red dot. This severe visual inconsistency indicates the models failure to maintain object identity, undermining its ability to reason about the correct target. incorrect goal identification. The Figure 19. Failure case: model starts correctly from the purple ball but mistakenly identifies the wrong goal position at the top-left green region. It generates valid pathyet toward the incorrect endpointresulting in complete failure in target localization. Figure 16. Failure case: incorrect understanding of object interaction. The model misinterprets the intended interaction between the character and the box. Instead of pushing the box toward the target area, the model attempts to approach the goal by pulling the box, which violates the correct physical reasoning for this task. Figure 20. Failure case: misunderstanding of reasoning target roles. The model misinterprets the intended interaction: instead of moving the green ball toward the red target, it moves both objects simultaneously. This reveals failure in understanding asymmetric goal-directed behavior and role assignment. 6 Regular Maze Create 2D animation based on the provided image of maze. The red circle slides smoothly along the white path, stopping perfectly on the green square. The red circle never slides or crosses into the blue areas of the maze. The camera is static, top-down view showing the entire maze. Maze: The maze paths are white, the walls are blue. The red circle moves to the goal position, represented by green square. The red circle slides smoothly along the white path. The red circle never slides or crosses into the blue areas of the maze. The red circle stops perfectly on the green square. Scene: No change in scene composition. No change in the layout of the maze. The red circle travels along the white path without speeding up or slowing down. Camera: Static camera. No zoom. No pan. No glitches, noise, or artifacts. Irregular Maze Create 2D animation based on the provided image of maze. The green circle slides smoothly along the white path, stopping perfectly on the red circle. The green circle never slides or crosses into the black areas of the maze. The camera is static, top-down view showing the entire maze. Maze: The maze paths are white, the walls are black. The green circle moves to the goal position, represented by red circle. The green circle slides smoothly along the white path. The red circle never slides or crosses into the black areas of the maze. The green circle stops perfectly on the red circle. Scene: No change in scene composition. No change in the layout of the maze. The green circle travels along the white path without speeding up or slowing down. Camera: Static camera. No zoom. No pan. No glitches, noise, or artifacts. 7 3D Maze Create 3D animation based on the provided image of cube maze.A yellow ball slides smoothly along the gray cube pathway, climbs up the vertical ladders step by step, and finally stops perfectly on the red cube at the top.The yellow ball never touches or passes through the blue cube or any non-gray areas of the maze.The camera remains static in an isometric, top-down angle showing the entire structure. Maze: The maze consists of stacked transparent gray cubes forming 3D pathway. The red cube represents the goal position. The blue cube marks the starting platform where the yellow ball begins. The yellow ball moves upward along the gray path, climbing vertically via the ladders. The ball slides smoothly without sudden changes in direction or speed. The ball stops exactly on top of the red cube at the end. Scene: No structural or color changes during animation. The maze layout and cube arrangement remain unchanged. The yellow ball moves continuously at constant speed along the 3D path. Camera: Static camera. No zoom. No pan. No glitches, noise, or artifacts. Trapfield Create 2D animation based on the provided image of maze. The blue circle slides smoothly along the gray path, stopping perfectly on the green circle. The blue circle never slides into or crosses the red areas with crosses (trap areas). The camera is static, top-down view showing the entire maze. Maze: The maze paths are gray, and the trap areas are red with crosses. The blue circle moves to the goal position, represented by the green circle. The blue circle slides smoothly along the gray path. The blue circle never slides into or crosses the red areas with crosses of the maze. The blue circle stops perfectly on the green circle. Scene: No change in scene composition. No change in the layout of the maze. The blue circle travels along the gray path without speeding up or slowing down. Camera: Static camera. No zoom. No pan. No glitches, noise, or artifacts. 8 Sokoban Create 2D animation based on the provided image of grid puzzle. The blue ball moves into position behind the yellow square and smoothly pushes it toward the red goal square. The yellow square only slides when pushed from behind by the blue ball and moves in straight line along the white floor tiles. When the direction of the yellow squares movement needs to change, the blue ball must reposition itself to new side of the yellow square.The square never crosses or overlaps any gray walls. Maze: The floor area is white, and the walls are gray. The yellow square can only move when pushed by the blue ball from behind. The blue ball cannot pull the square or move through walls. The yellow square slides smoothly in one direction until it reaches the red goal square. The animation stops perfectly when the yellow square aligns with the red goal square. Scene: No change in scene composition. No change in the layout of the maze. The movement is smooth, with no speed variation. Camera: Static camera. No zoom. No pan. No glitches, noise, or artifacts. Sokoban You are given an image of grid-based Sokoban puzzle. Gray tiles represent walls and cannot be crossed. White tiles represent open floor tiles that can be moved through. The blue ball represents the player or agent. The yellow square represents the box that needs to be pushed. The red square represents the goal destination for the box. Task: Infer the complete movement sequence required for the blue ball to push the yellow square onto the red goal square. The blue ball moves in four directions: up, down, left, right. When the blue ball moves into box, it automatically pushes the box if there is space behind it. The box and the blue ball cannot cross or overlap any gray walls. Diagonal movement is not allowed, and the camera remains fixed from top-down view. Output Format: Return the entire movement sequence as JSON array of directional actions, where each element is one of up, down, left, or right. Do not include any explanations or additional text. Example of expected output: { actions: [right, right, down, left, down] } 9 Regular Maze You are given an image of grid-based maze. Black tiles represent walls and cannot be crossed. White tiles represent open paths that can be moved through. The green circle represents the starting point of the path. The red circle represents the goal or destination. Task: Infer the shortest valid path from the green starting point to the red goal circle. Movement can only occur between adjacent open tiles up, down, left, or right. Diagonal movement is not allowed, and the path must not cross or touch any black walls. Output Format: Return the entire movement sequence of the green circle as JSON array of directions, where each element is one of up, down, left, or right. Do not include any explanations or additional text. Example of expected output: { path: [up, up, left, down, right, right] } Irregular Maze You are given an image of pathfinding puzzle. The image shows network of curved paths connecting various waypoints. Each waypoint (intersection or junction) is labeled with letter or letter combination (A, B, C, ..., Z, AA, AB, etc.). The green circle represents the starting point. The red circle represents the goal or destination. Task: Find the shortest valid path from the green starting point to the red goal. The path must follow the visible roads/paths in the image. You can only move along the connected paths shown in the image. Output Format: You MUST return JSON object with path field containing an array of waypoint labels. The array should start with the label closest to the starting point and end with the label closest to the goal. Do not include any explanations or additional text. Important: The path field MUST be an array of strings, not single string. Example of expected output: { path: [A, B, C, D, E] } 10 Maze3D You are given an image of 3D maze composed of gray cubes that represent walkable platforms suspended in space. Each cube represents solid tile that the ball can stand on or move across. The yellow sphere represents the starting point. The blue cubes represent the initial platform where the ball begins. The red cube represents the goal or destination. Task: Infer the shortest valid 3D path for the yellow sphere to move from its starting position to the red goal cube. Movement Rules: Horizontal movements (forward left, forward right, backward left, backward right): each move spans 2 grid units horizontally. Vertical movements (up, down): each move spans 3 grid units vertically via ladder; ladder must be present at the starting position. The sphere cannot move through empty space or overlap any cube structure. All movements must follow valid cube surfaces and ladder connections. The six valid directions of movement are: forward left move diagonally forward and to the left (2 units) within the same layer. forward right move diagonally forward and to the right (2 units) within the same layer. backward left move diagonally backward and to the left (2 units) within the same layer. backward right move diagonally backward and to the right (2 units) within the same layer. up move vertically upward (3 units) via ladder. down move vertically downward (3 units) via ladder. Output Format: Return the full sequence of movement directions as JSON array, where each step is one of the six valid directions. Do not include any explanations, reasoning, or extra text. Example of expected output: { path: [up, forward right, forward left, up, forward right] } Trapfield You are given an image of grid-based maze. Red tiles marked with an represent trap zones that must be avoided. White tiles represent open paths that can be moved through. The blue circle represents the starting point of the path. The green circle represents the goal or destination. Task: Infer the shortest valid path for the blue circle to reach the green circle. Movement can only occur between adjacent open tiles up, down, left, or right. Diagonal movement is not allowed. The path must not cross or touch any red trap tiles. Output Format: Return the full movement sequence of the blue circle as JSON array of directions, where each element is one of up, down, left, or right. Do not include any explanations, reasoning, or extra text. Example of expected output: { path: [left, left, down, down] }"
        }
    ],
    "affiliations": [
        "DeepWisdom",
        "Hong Kong University of Science and Technology (GuangZhou)",
        "National University of Singapore",
        "Renmin University of China",
        "Shanghai Artificial Intelligence Laboratory",
        "Tsinghua University",
        "University of Oxford",
        "Xiamen University"
    ]
}