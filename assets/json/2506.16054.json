{
    "paper_title": "PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and Quantized Attention in Visual Generation Models",
    "authors": [
        "Tianchen Zhao",
        "Ke Hong",
        "Xinhao Yang",
        "Xuefeng Xiao",
        "Huixia Li",
        "Feng Ling",
        "Ruiqi Xie",
        "Siqi Chen",
        "Hongyu Zhu",
        "Yichong Zhang",
        "Yu Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In visual generation, the quadratic complexity of attention mechanisms results in high memory and computational costs, especially for longer token sequences required in high-resolution image or multi-frame video generation. To address this, prior research has explored techniques such as sparsification and quantization. However, these techniques face significant challenges under low density and reduced bitwidths. Through systematic analysis, we identify that the core difficulty stems from the dispersed and irregular characteristics of visual attention patterns. Therefore, instead of introducing specialized sparsification and quantization design to accommodate such patterns, we propose an alternative strategy: *reorganizing* the attention pattern to alleviate the challenges. Inspired by the local aggregation nature of visual feature extraction, we design a novel **Pattern-Aware token ReOrdering (PARO)** technique, which unifies the diverse attention patterns into a hardware-friendly block-wise pattern. This unification substantially simplifies and enhances both sparsification and quantization. We evaluate the performance-efficiency trade-offs of various design choices and finalize a methodology tailored for the unified pattern. Our approach, **PAROAttention**, achieves video and image generation with lossless metrics, and nearly identical results from full-precision (FP) baselines, while operating at notably lower density (~20%-30%) and bitwidth (**INT8/INT4**), achieving a **1.9x** to **2.7x** end-to-end latency speedup."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 4 5 0 6 1 . 6 0 5 2 : r PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and Quantized Attention in Visual Generation Models Tianchen Zhao 1,2, Ke Hong1, Xinhao Yang1, Xuefeng Xiao2, Huixia Li2, Feng Ling2, Ruiqi Xie1, Siqi Chen1, Hongyu Zhu1, Zhang Yichong1, Yu Wang 1 1Tsinghua University, 2ByteDance Seed Figure 1: PAROAttention unifies the diverse attention patterns through token reorder, which benefits both the sparsification and quantization. It achieves nearly identical generation result from fullprecision baseline without metrics degradation, under lower density (20%-30%) and bitwidth (INT8/INT4), achieving 1.92.7 end-to-end latency speedup."
        },
        {
            "title": "Abstract",
            "content": "In visual generation, the quadratic complexity of attention mechanisms results in high memory and computational costs, especially for longer token sequences required in high-resolution image or multi-frame video generation. To address this, prior research has explored techniques such as sparsification and quantization. However, these techniques face significant challenges under low density and reduced bitwidths. Through systematic analysis, we identify that the core difficulty stems from the dispersed and irregular characteristics of visual attention patterns. Therefore, instead of introducing specialized sparsification and quantization design to accommodate such patterns, we propose an alternative strategy: reorganizing the attention pattern to alleviate the challenges. Inspired by the local aggregatin nature of visual feature extraction, we design novel Pattern-Aware token ReOrdering (PARO) technique, which unifies the diverse attention patterns into hardware-friendly block-wise pattern. This unification substantially simplifies and enhances both sparsification and quantization. We evaluate the performanceefficiency trade-offs of various design choices and finalize methodology tailored for the unified pattern. Our approach, PAROAttention, achieves video and image generation with lossless metrics, and nearly identical results from full-precision Equal contribution. work done when Tianchen Zhao intern at Bytedance Corresponding author: Yu Wang (yu-wang@tsinghua.edu.cn). Preprint. Under review. Figure 2: The Motivation of PAROAttention. (a) The computational flow of transformer. (b) The challenge for sparsification and quantization caused by visual attention pattern, and how PAROAttention addresses it. (c) The illustration of 3D feature, and 1D token sequence with different orders. (FP) baselines, while operating at notably lower density (20%-30%) and bitwidth (INT8/INT4), achieving 1.92.7 end-to-end latency speedup."
        },
        {
            "title": "Introduction",
            "content": "Diffusion transformers (DiTs [35]) have garnered significant research interest in visual generation tasks. However, their excessive resource cost poses challenges for broader applications. The adoption of \"3D full attention\" in models like CogVideoX [56] further increases token length. The quadratic complexity of attention mechanisms in token length results in substantial memory consumption and computational overhead when processing such long sequences. For instance, generating 49-frame 6-second 720P video involves 17K token length2. Attention computation contributes to the majority of the total latency, making it the primary bottleneck and requiring urgent optimization. As shown in Fig. 2(a), the two matrix multiplications QK and (P is the attention map after softmax) has computation cost quadratic to token length, making them the primary bottleneck in attention. Previous research has explored sparse attention mechanisms [58, 55] and quantization [61, 59, 60] to accelerate attention. While these techniques achieve notable success in language models [3, 8, 20, 18], they cannot be directly applied to visual generative models due to distinct attention patterns. As shown in Fig. 2-(b), different attention heads exhibit diverse patterns. These patterns vary not only in type (e.g., blockwise, multi-diagonal) but also in their structural characteristicssuch as the number, width, and spacing of the diagonals. Recent sparse attention methods [57, 64, 45] explore designing specialized sparse masks for visual models, but still face significant challenges in maintaining quality at lower density rates (< 50%). Quantization techniques tailored to visual generation remain underexplored. Existing methods struggle to efficiently quantize computations to lower-bit integers (e.g., INT8/INT4), and often remain at FP16/FP8. We conduct systematic analysis of the underlying reason for subpotimal performance of existing techniques under low density and bitwidths (as discussed in Sec. 3). Our findings reveal that the core challenge of sparsification and quantization stem from distinct characteristics of visual attention patterns. For sparsification, the dispersed and dynamically changing attention patterns lead to the absence of locally sparse regions, resulting in sparsification error. For quantization, the presence of multiple \"diagonal\" values act as outliers within data group, thereby increasing quantization error. Different from existing methods that design specialized sparsification and quantization techniques to accommodate the diverse patterns, we propose an alternative direction: To reorganize the attention patterns to ease the difficulty the design of both sparsification and quantization methodologies. To design proper technique to reorganize the attention pattern, we further analyze the underlying causes of the diverse visual attention patterns. Prior literature [33] reveals the local aggregation nature of visual attention, which suggest that attention tends to capture relationships between neighboring pixels. In vision transformers, the 3D physical space are flattened into 1D token sequences, which disrupts the data adjacency. For example, as shown in Fig. 2-(c), the neighboring tokens along -dimension in 3D space are not consecutive, and have the same interval of HW in 1D token sequence of default order [F, H, ]. The aggregation of these tokens with equal intervals forms 1All videos in the figure are provided in the supplementary. 2We use this setting as example for most of the description below, the image token length = 17550 = = 13 30 45, where F, H, stands for the frame number, height, and width in the latent space. the multi-diagonal pattern. Therefore, the multi-diagonal and block-wise patterns are intrinsically the same, representing local aggregation in different dimensions. By applying token reordering, we rearrange the tensor layout (e.g., permute from [F, H, ] to [H, W, ])) for each head to keep elements of the local aggregation dimension contiguous. The Pattern-Aware token ReOrder (PARO) could transform diverse patterns into unified, hardware-friendly block-wise pattern. Furthermore, we analyze the trade-offs among design choices in terms of accuracy, and efficiency, and design specialized sparsification and quantization techniques tailored to the unified block-wise pattern, constructing the PAROAttention method. We summarize our contributions as follows: We analyze and identify the key challenges of attention sparsification and quantization as unique attention pattern characteristic, and propose to address it from novel direction of token reorder to reorganize and unify the attention pattern. We compare the strength and weakness of design choices, and develop specialized methodologies tailored for the unified pattern, along with CUDA kernels for practical acceleration. PAROAttention, achieves generation with lossless metrics, and nearly identical results from full-precision (FP) baselines, while operating at notably lower density (20%-30%) and bitwidth (INT8/INT4), achieving 1.92.7 end-to-end latency speedup."
        },
        {
            "title": "2.1 Visual Generative Models",
            "content": "Diffusion transformers [35], which leverage transformer architectures [40, 9], have achieved outstanding performance and are widely adopted by recent image generation models such as PixArt [6] and Flux [19]. For video generation, earlier approaches, such as OpenSORA [15], apply \"spatialtemporal\" attention, which performs attention separately along the spatial and temporal dimensions. Other recent models like CogVideoX [56], Wan [41] adopt 3D full attention\" instead, processing all spatial tokens across all frames simultaneously. The increased model size and token length pose significant challenges for efficient deployment."
        },
        {
            "title": "2.2 Sparse Attention for Generative Models",
            "content": "Existing sparse attention research [3, 8, 5, 48, 29, 65] primarily focuses on designing sparse masks aligned with specific attention patterns, such as sliding window patterns [67, 49, 50] and attention sink patterns [51, 11] commonly observed in language models. However, visual generation involves unique attention patterns that require new forms of specialized sparse mask design. Recent studies have explored various mask designs tailored for visual generation, including window-based approaches (e.g., DiTFastAttn [57], STA [64]), spatial temporal patterns (e.g., SparseVideoGen [46, 54]), and hybrid mask combinations (e.g., MInference [18]). In contrast, SpargeAttention [62] does not rely on predefined sparse masks but instead generates them online based on QK embeddings. Despite these advancements, the dispersed and diverse distribution of attention values in visual tasks prevents these mask patterns from maintaining quality under low density. In this work, we address this challenge by reorganizing attention patterns through token reordering."
        },
        {
            "title": "2.3 Quantization for Generative Models",
            "content": "Quantization [37, 66, 22, 31, 23] has proven to be highly effective across wide range of applications. For visual generation, prior research [69, 36, 24, 70, 21, 16, 26] has identified unique challenges associated with quantizing DiTs, they primarily focused on quantizing the linear layers in transformers. However, the increasing token length has shifted the bottleneck to the attention mechanism. Recent advancements [61, 59], have explored quantizing QK to INT4/8 while employing FP8 (8 bit floating-point) for . In this work, we address the unique challenges of attention quantization in visual generation, analyzing the underlying reasons behind the difficulty of quantizing the matrix. We further extend our investigation to explore INT4/8 quantization for the computation."
        },
        {
            "title": "3 Preliminary Analysis",
            "content": "Key Challenge for Sparse Attention: The goals of sparse attention design are two-folded: (1) preserving algorithmic performance by avoiding the removal of important attention values, and (2) enabling practical hardware acceleration. Since arbitrary sparsity patterns could not achieve practical acceleration [7], proper structured sparsity is necessary. However, as illustrated in Fig. 2, the dispersed and diverse nature of visual attention patterns presents significant challenges for designing structured sparsity. Firstly, diverse attention patterns vary in type (block-wise, multi-diagonal, and diagonal-inblock), as well as in structural characteristics (the number, width, and spacing of diagonals). These patterns also change dynamically across different timesteps and prompts. Designing proper structured sparsity pattern that accommodates all these variations and generalizes across different scenarios is extremely difficult. Secondly, the dispersed attention distribution makes it challenging to form fully sparse regions, thereby inevitably introducing errors in structured sparsity. Given these challenges, improving sparse mask design to accommodate diverse patterns may have limited effectiveness. Instead, we propose an alternative direction: reorganizing the attention pattern. Key Challenge for Attention Quantization: The goal of quantization design is to minimize quantization error, and we begin by analyzing its sources. As shown in prior work [69], major source of quantization error arises from large variations within data group. In such cases, the computed scaling factor becomes excessively large, compressing the majority of values toward zero and leading to significant quantization error. Upon analysis, we find that the distributions of Q, K, and do not exhibit substantial variation. The primary challenge lies in properly quantizing the attention matrix . As illustrated in Fig. 2, in the \"diagonal-like\" patterns, the larger values along the diagonals act as outliers within each local region (i.e., quantization group), leading to substantial rounding errors. Addressing this issue is crucial for reducing quantization error. This also points to the need for reorganizing the attention distribution to reduce outliers."
        },
        {
            "title": "4.1 Pattern-aware Token Reordering (PARO)",
            "content": "As concluded in Sec. 3, it is crucial to introduce technique that reorganizes the attention distribution to mitigate challenging characteristics and promote structure more favorable to sparsification and quantization. To explore this, we investigate the root causes of the diverse attention patterns. Inspired by the local aggregation nature discussed in Sec. 1, we observe that diverse patterns actually represents local aggregation along different dimensions. They could potentially be transformed into localized block-wise pattern by gathering the locally aggregated tokens together through token reordering. However, determining the optimal reorder strategy for each attention head is non-trivial challenge. First, the large number of tokens (Ntoken) leads to vast search space of possible reorders, making optimization difficult. Second, the reorder strategy must be carefully designed to minimize hardware overhead. Third, it should simultaneously satisfy the distinct requirements of sparsification and quantization: sparsification benefits from fully sparse local regions, whereas quantization requires balanced distributions within local regions. To address these challenges, we focus on specific subset of reorder - permutations, inspired by the observation that certain attention heads tend to aggregate information locally along particular dimension. In the case of 3D video generation, where tokens are structured along three dimensions [F, H, ], we limit the reordering space to the six possible permutations (P 3 3 = 6). We verify that the optimal permutation is sufficient to produce unified, block-wise patterns through empirical analysis as seen in Fig. 3. We further elaborate on how we address the above mentioned challenges as follows: Minimize Hardware Overhead: We first verify that the optimal permutation remains consistent across different timesteps and prompts. Based on this observation, we determine the permutation order offline, eliminating the need for runtime overhead associated with generating the reordering strategy. Consequently, the only remaining overhead is the online application of the permutation itself, which primarily involves data movement. This cost can be significantly reduced by fusing the permutation operation with preceding kernels. After fusion, explicit data movement from global memory on the GPU is avoidedonly the output write-back addresses need to be adjusted. The resulting overhead is less than 1% of the preceding kernel (e.g., LayerNorm), and thus negligible compared to the cost of attention computation. 4 Figure 3: The overall framework of PAROAttention. The pattern-aware token reordering (PARO) is applied to unify the attention pattern into hardware-friendly block pattern. Sparse attention and quantization techniques are designed tailored for this pattern. Metric for Permutation Order: As discussed above, sparsification and quantization prefer different distributions. We design separate metrics and combine them to to determine the permutation order for each head. Given the post-softmax attention map RN , = b, where is the block size. is tiled into sub-matrices Pij Rbb. For sparsification, we choose relatively small value ϵ (e.g., 1e-3), and classify the block as sparse if the vast majority of values (over threshold σ, e.g., 90%)) within the block are smaller than ϵ. The percentage of such sparse blocks is adopted as the sparse metric Msparse, it could be calculated as follows, where stands for the indicator function. ij = (cid:80)b n<ϵ m=1 (cid:80)b n=1 (Pij(m, n) < ϵ) , Msparse = 1 kk (cid:80)k i=1 (cid:80)k j=1 (cid:16) n<ϵ ij bb σ (cid:17) . (1) For quantization, we follow the previous methods [69] and adopt incoherence Ψ (maximum value divided by the mean absolute value) as an indicator of quantization difficulty within data group Rg. The quantization metric Mquant is defined as follows: (cid:80)k (cid:80)k Ψ(x) = max(x) (cid:80) , Mquant = 1 kk 1 i=1 j=1 Ψ(Pij). (2) Finally, the Msparse and Mquant are normalized across all possible permutations Θi {Θ1, ..., Θ6}, and combined as the final metric . The weighting coefficient α controls the relative importance of the two aspects, and the permutation with the lowest Θi is chosen. Θi = α Θi + (1 α) Θi (3) . sparse Θj sparse (cid:80) Θj quant Θj quant (cid:80) Θj"
        },
        {
            "title": "4.2 Block-wise Sparse Attention",
            "content": "After applying permutation, we obtain attention maps with unified and regular block-wise patterns. We further elaborate on comparing the strengths and limitations of different design choices to conclude the final PAROAttention sparsification design. Static vs. Dynamic Sparse Attention: There are two major schemes for sparse attention: the dynamic approach, which predicts sparse masks online, and the static approach, which calibrates sparse masks offline. Since the PARO attention reorganization is compatible with both schemes, we summarize their respective strengths and limitations below and justify our final selection: (1) In terms of preserving algorithmic performance, the dynamic approach bases on QK embeddings to predict the sparse mask. However, as shown in Fig. 3, the pre-softmax attention map (QK ) contains relatively uniform values and lacks distinguishable sparse patterns, making accurate prediction of the attention pattern difficult. Furthermore, the QK embeddings often need to be downsampled to reduce computational overhead, which further compromises prediction accuracy. The static approach, 5 on the other hand, benefits from access to the more informative post-softmax attention patterns. Nevertheless, it still faces challenges in designing sparse masks that accommodate the highly diverse, and dynamically changing attention distributions. Fortunately, with our token reordering strategy that transforms attention into unified block-wise structure, these challenges are significantly alleviated. (2) In terms of hardware efficiency, the dynamic approach introduces runtime overhead for online sparse mask prediction. Reducing this cost often comes at the expense of prediction accuracy. The static approach, while avoiding this cost, incurs memory overhead for storing sparse masks and offline calibration overhead. To address this, we develop techniques to minimize these costs. Minimize Hardware Overhead: We ensure that the sparsification design of PAROAttention introduces minimal overhead by incorporating the following techniques: (1) Timestep-Aware Sparse Mask Sharing: As discussed above, offline determined static sparse mask face challenges when generalizing to dynamically changing sparse patterns. To address this, we systematically analyze the similarity of attention maps across multiple dimensions. Since sparsification is applied only to image tokens (account for 99% of all tokens), we observe extremely high similarity across different prompts (cosine similarity 0.99). However, lower similarity is witnessed across the timestep dimension and we adopt timestep-wise sparse masks. Despite improving accuracy, timestep-wise sparse masks increase memory usage. We observe that attention patterns change most during the early timesteps. Thus, we apply distinct timestep-wise masks only for the first half of timesteps and reuse common mask for the remainer. We further reduce runtime memory cost by prefetching sparse masks for each head during inference, eliminating the need to store all masks simultaneously. When stored as binary bitmasks, each heads sparse mask requires only 9.2 KB, making the overall storage and data movement overhead negligible. (2) Block-Aligned Sparse Granularity for Efficient CUDA Execution: FlashAttention processes attention in block-wise manner, we align our sparsification granularity with the FlashAttention block size. This allows for an extremely simple CUDA implementation, where entire blocks can be skipped without additional branching or logic overhead. (3) Efficient Offline Mask Generation: Although static methods allow sophisticated sparse metric design, we find that simple block sum thresholding is sufficient. It also enables explicit control over density. Due to strong generalization across prompts, only 12 prompts are needed to determine the permutation order and generate the sparse masks, which only involves minute-level cost."
        },
        {
            "title": "4.3 Block-wise Quantized Attention",
            "content": "As discussed in Sec. 3, the major source of quantization error comes from the data variation within quantization group. Prior literature [4, 69] introduces metric metric incoherence Ψ, as shown in eq. (2), to measure the relative sharpness of the data distribution within group. Data groups with higher incoherence are more challenging to quantize. We conclude and justify our design choice for PAROAttention quantization design to reduce incoherence and ensure hardware efficiency as follows: (1) Block-Aligned Quantization Granularity (Grouping): We emphasize the importance of aligning the quantization grouping with the FlashAttention block size, considering both algorithmic performance and hardware efficiency. As illustrated in Fig. 3, adopting naive per-row quantization scheme (analogous to per-token grouping for and K) is not only incompatible with the blockwise processing paradigm of FlashAttention but also introduces substantial incoherence due to the inherently \"diagonal\" structure of visual attention patterns. This highlights the necessity of block-wise quantization grouping. However, even within local blocks, the diagonal distribution persists, leading to high incoherence (e.g., Ψ = 93), which necessitates further optimization to reduce quantization error. (2) Token Reorder for Incoherence Reduction: Prior work has proposed distribution-balancing quantization techniques for linear layers, such as scaling [27] and rotation [4]. However, these approaches are not applicable to the computation in FlashAttention due to the iterative update mechanism of , which is not explicitly materialized in order to save memory. Instead, we explore novel direction: tuning the attention distribution via token reordering, which groups similar attention values together. As shown in Fig. 3, this reordering significantly reduces incoherence, thereby effectively mitigating quantization error. 6 Table 1: Performance of PAROAttention CogVideoX text-to-video generation on VBench prompts. Baselines are evaluated using their official codebases. For fair comparison, we configure SparseVideoGen without skipping sparsification during the first 30% of timesteps. The SpargeAttn (0.5 + PARO) denotes the SpargeAttention method augmented with token reordering (PARO)."
        },
        {
            "title": "Method",
            "content": "- FP16 Full Attn. DiTFastAttn (0.5) MInference (0.5) SpargeAttn (0.5) SpargeAttn (0.5 + PARO) SparseVideoGen (0.5) PAROAttn (0.5) SpargeAttn (0.3) SpargeAttn (0.3 + PARO) SparseVideoGen (0.3) PAROAttn (0.3) PAROAttn (0.2) PAROAttn (0.125)"
        },
        {
            "title": "Efficiency",
            "content": "Dense Rate / Bitwidth 100.0% 50.0% 50.0% 50.0% 50.0% 50.0% 50.0% 30.0% 30.0% 30.0% 30.0% 20.0% 12.5% RTN (INT8) RTN (INT4) SageAttn SageAttnV QK (INT8), PV (INT8) QK (INT4), PV (INT4) QK (INT8), PV (FP16) QK (INT4), PV (FP8) PAROAttn (INT8) PAROAttn (INT4) QK (INT8), PV (INT8) QK (INT4), PV (INT4) Sparse +Quant PAROAttn (0.3+INT8) PAROAttn (0.5+INT4) 30% + QK, PV (INT8) 50% + QK, PV (INT4)"
        },
        {
            "title": "Video Quality Metrics",
            "content": "FP Diff. Metrics CLIPSIM VQA FScore FVD-FP16 PSNR SSIM CosSim 0. 0.197 0.197 0.198 0.198 0.198 0.203 0.197 0.197 0.197 0.204 0.203 0.201 0.190 0.184 0.203 0.200 0.203 0.200 0.201 0. 92.53 90.43 86.02 87.72 89.26 90.14 92.56 86.74 89.96 89.54 92.66 92.42 90.21 92.09 69.25 92.24 88.79 92.57 89. 91.68 90.42 0.000 0.740 2.250 1.154 0.671 0.568 0.103 1.231 1.142 0.589 0.101 0.151 0.203 0.571 3.360 0.131 2. 0.096 0.876 0.884 0.967 0.000 0.904 0.368 0.347 0.259 0.186 0.068 0.375 0.339 0.241 0.153 0.151 0. 0.480 1.446 0.047 1.750 0.026 1.382 0.533 1.431 15.40 16.54 16.80 17.32 18.50 29.14 15.22 16.89 17.73 22. 19.39 15.93 18.88 11.99 29.58 24.46 29.01 24.16 21.49 24.34 1.000 0.603 0.696 0.683 0.693 0.755 0. 0.642 0.683 0.725 0.829 0.744 0.690 0.750 0.500 0.927 0.824 0.935 0.822 0.779 0.827 1. 0.920 0.945 0.938 0.948 0.960 0.997 0.912 0.946 0.954 0.984 0.962 0.937 0.956 0.905 0.997 0.979 0.996 0.985 0.976 0. Figure 4: Qualitative Results of CogVideoX generated videos for PAROAttention and baselines."
        },
        {
            "title": "5.1 Experimental Setup",
            "content": "Video and Image Generation: For video generation, we apply PAROAttn to the CogVideoX-5B [56] and Wan [41] (see Appendix Sec.1) model for 720P 6/10-second video with 30 sampling steps. To verify generalization, we collect calibration data with the first 2 prompts of the CogVideo example dataset [38] and evaluate on subset of prompts collected from VBench [17], covering all subtasks. For image generation, we apply PAROAttn to the Flux.1.Dev [19] model for 1024 resolution, with 30 sampling steps. The calibration prompts are the same as video generation and the first 1024 prompts from the COCO [28] dataset are used for evaluation. The block size for sparsification and quantization are chosen as 64 to align with the FlashAttention. Unlike prior work [45, 64], which avoids compressing the initial 25% of timesteps, our techniques are applied to all timesteps. Evaluation Metrics: We employ two types of metrics: (1) Quality Metrics: They measure the absolute quality of videos or images. For videos, we adopt CLIPSIM [43], VQA [44], and FlowScore [32] to measure text-video alignment, quality, and temporal consistency respectively. For images, we adopt CLIPScore [13] and ImageReward [53] to measure text-image alignment, and human preference. 7 Table 2: Performance of Flux text-to-image generation on COCO prompt set. Type Method - FP16 Full Attn. Sparse DiTFastAttn (0.75) MInference (0.75) SpargeAttn (0.75) PAROAttn (0.75) DiTFastAttn (0.5) MInference (0.5) SpargeAttn (0.5) PAROAttn (0.5) Efficiency Dense Rate / Bitwidth 100.0% 75.0% 75.0% 75.0% 75.0% 50.0% 50.0% 50.0% 50.0% Quant SageAttn SageAttnV2 QK (INT8), PV (FP16) QK (INT4), PV (FP8) PAROAttn (INT8) PAROAttn (INT4) QK (INT8), PV (INT8) QK (INT4), PV (INT4) Sparse +Quant PAROAttn (0.5+INT8) PAROAttn (0.75+INT4) 50% + QK, PV (INT8) 75% + QK, PV (INT4) Quality Image Quality Metrics FP Diff. Metrics CLIPScore ImageReward FID-FP16 PSNR SSIM CosSim 0.258 0.258 0.260 0.260 0. 0.255 0.255 0.255 0.259 0.258 0.257 0.258 0.258 0.259 0.259 1. 0.96 0.97 0.97 1.01 0.81 0.89 0.91 1.04 1.00 1.00 1.00 1.00 1.04 1. 0.00 28.31 34.88 29.04 19.47 53.95 42.58 55.47 30.39 14.77 20.11 15.34 19. 29.56 22.45 16.73 14.68 16.76 20.95 12.49 13.42 14.62 16.20 23.47 20. 23.04 20.16 16.28 19.26 1.000 0.687 0.602 0.680 0.812 0.537 0.583 0.602 0. 0.863 0.814 0.856 0.793 0.680 0.770 1.000 0.956 0.933 0.951 0.947 0.890 0.908 0. 0.944 0.986 0.979 0.986 0.975 0.947 0.971 Figure 5: Qualitative Results of Flux generated images for PAROAttention and baseline methods. (2) Relative Difference Metrics: They quantify the difference between FP16 generation. For both video and image generation, PSNR and cosine similarity are used to measure low-level pixel-space differences. SSIM [42] evaluates structural similarity, while FVD-FP16 [39], and FID-FP16 [14] assess feature-space differences. In practice, we find that relative difference metrics are more sensitive and better reflect the quality of compression techniques (discussed in Appendix Sec.2). Baseline Methods: For sparsification, we select baselines of different schemes, including: DiTFastAttn [57] (dynamic window mask), SpargeAttn [62] (dynamic block-wise mask), and MInference [18], SparseVideoGen [45] (multiple static mask, dynamic selection). PAROAttn adopts static mask, and achieve superior performance under lower dense rate. For quantization, we compare with naive RTN [34], SageAttn [61] (INT8 QK, FP16 ), and SageAttnV2 (INT4 QK , FP8 ). PAROAttn quantizes to lower bitwidth (QK, INT8/INT4) with comparable performance. CUDA Kernel Implementation: We implement PAROAttn based on the SageAttnV2 [59] kernel, incorporating customized designs for sparsity and quantization. Sparsification comparison are conducted on an NVIDIA A100 with CUDA 11.8, due to support limitations of baseline methods. The quantization comparison is conducted on NVIDIA RTX 4090 for FP8 and INT4 support."
        },
        {
            "title": "5.2 Main Results",
            "content": "Text-to-video generation: We present the evaluation metrics in Tab. 1 and ualitative comparisons in Fig. 4, using challenging prompt that features complex scene with multiple objects (e.g., artworks and people). We conclude our findings as follows: (1) Baseline sparsification methods exhibit notable performance degradation for multiple metrics, even at relatively high dense rate of 50%, resulting in visible content distortion or blurred outputs. (2) In contrast, the PAROAttn sparsification method can generate images nearly identical to the FP16 full attention baseline, even at 20% dense rate, and achieves metric scores that surpass those of the 50% baseline. (3) The PARO token reordering is compatible with dynamic sparsity approaches. Simply combining PARO with SpargeAttn at 30% density (PSNR: 16.89) achieves performance comparable to SpargeAttn at 50% (PSNR: 16.8), yielding speedup improvement from 1.67 to 2.22, as shown in Fig. 6. (4) The PAROAttn quantization method maintains comparable performance while further quantizing 8 Figure 6: Normalized latency speedup (bar plot) and PSNR (line plot) trade-off under different (a) quantization and (b) sparse configurations. the to lower-bit integers (e.g., PAROAttn (INT8) vs. SageAttn, and PAROAttn (INT4) vs. SageAttnV2). (5) The PAROAttn sparsification and quantization techniques could be combined together for improved speedup. With aligned metric scores, the most aggresive plan PAROAttn (0.5+INT4) achieves nearly 10x speedup compared with baseline methods with 1.5-2x speedup. Text-to-image generation: We present the evaluation metrics in Tab. 2 and qualitative results in Fig. 5. The conclusions observed in previous sections hold consistently. Due to the shorter token lengths, sparsification becomes more challenging, and most baseline methods introduce noticeable artifacts and content distortion at 50% density rate. In contrast, PAROAttn effectively preserves both visual quality and content, even at low density rates and when combined with quantization. Hardware Resource Savings: We compare the latency speedup and performance-efficiency tradeoff with baseline methods CUDA kernel implementation in Fig. 6. We conclude the key findings as follows: (1) The PAROAttn kernel achieves both superior speedup and algorithmic performance with aligned sparsity. For instance, at 50% density rate, PAROAttn achieves 1.73 speedup, outperforming SpargeAttn (1.67) and SparseVideoGen (1.42), attributed to its simplified design and reduced overhead. (2) PAROAttns sparsification achieves speedups approaching the theoretical upper bound for computation reduction (e.g., 1.73 at 50% density, 2.71 at 30%), demonstrating its hardware-friendly nature. (3) PAROAttn introduces minimal runtime overhead (<1%), compared with SpargeAttn (69%) and SparseVideoGen (1015%). (4) At just 20% density, PAROAttn matches the performance of baselines at 50% density, while delivering significantly higher speedup (3.81 vs. 1.41.7). (5) PAROAttn supports quantization of to lower-bit formats (e.g., INT8/INT4), achieving similar performance to SageAttn while notably improving speedup (e.g., from 1.72 to 1.83, and from 2.56 to 2.97)."
        },
        {
            "title": "6 Analysis",
            "content": "We conduct extensive analyses to demonstrate the effectiveness of PAROAttn, we highlight key results here and provide additional details in the Appendix. Ablation Studies: We present ablation studies of PAROAttns techniques in Tab. 3. Removing token reorder leads to significant metric degradation for both sparsity and quantization. Similarly, eliminating timestep sharing and storing sparse masks for all timesteps does not improve performance, demonstrating that later timesteps can effectively share sparse masks. Additionally, replacing the block-wise quantization group with row-wise approach for PV quantization results in notable degradation, highlighting the importance of the block-wise quantization group. Overhead Analysis: The additional cost of PAROAttn is two-fold: The runtime overhead is minimized, as shown in Fig. 6. The offline mask generation incurs only minute-level cost, which is faster than the hyperparameter tuning required for SpargeAttn or the mask generation in SparseVideoGen. Effectiveness of PARO permute metric: We visualize the attention map of the 5th head in the 1st transformer block under six different token permutation orders in Fig. 7. The permutation successfully transforms the multi-diagonal patterns into block-wise patterns. Notably, for the permutation [H, F, ], although the values are uniformly distributed within the block, insufficient sparsity is observed. In contrast, the selected permutation [H, W, ] exhibits both sparse and uniform blocks, demonstrating the effectiveness of the metrics for permutation order. 9 Table 3: Ablation studies. (- Token-Reorder) denotes PAROAttn without the token-reorder technique. Type Method FP Diff. Metrics PSNR SSIM LPIPS CosSim Sparse Quant PAROAttn (0.5) (- Token-Reorder) (- Timestep-Share) PAROAttn (PV INT8) (- Token-Reorder) (- Block-Group) 29.14 26.25 29.09 30.17 29.00 27. 0.936 0.907 0.937 0.940 0.930 0.906 0.045 0.069 0.044 0.039 0.049 0.063 0.997 0.992 0.997 0.995 0.995 0. Figure 7: Attention patterns under different permute orders."
        },
        {
            "title": "References",
            "content": "[1] Tensor core. https://resources.nvidia.com/en-us-tensor-core,. 19 [2] Xilinx dsp. https://docs.amd.com/r/2021.2-English/ug1483-model-composer-sys-gen-user-guide/ DSP48E,. 19 [3] Moonshot AI. Moba: Mixture of block attention. Technical report, MoonshotAI, 2025. 2, 3 [4] Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian Croci, Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoefler, and James Hensman. Quarot: Outlier-free 4-bit inference in rotated llms. arXiv preprint arXiv:2404.00456, 2024. 6 [5] Aiyue Chen, Bin Dong, Jingru Li, Jing Lin, Yiwu Yao, and Gongyi Wang. Rainfusion: Adaptive video generation acceleration via multi-dimensional visual redundancy. arXiv preprint arXiv:2505.21036, 2025. [6] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis, 2023. 3 [7] Shail Dave, Riyadh Baghdadi, Tony Nowatzki, Sasikanth Avancha, Aviral Shrivastava, and Baoxin Li. Hardware acceleration of sparse and irregular tensor computations of ml models: survey and insights. Proceedings of the IEEE, 109(10):17061752, 2021. 4 [8] DeepSeek. Nested sparse attention. arXiv preprint arXiv:2502.11089, 2025. 2, 3 [9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 3 [10] Tianyu Fu, Yi Ge, Yichen You, Enshu Liu, Zhihang Yuan, Guohao Dai, Shengen Yan, Huazhong Yang, and Yu Wang. R2r: Efficiently navigating divergent reasoning paths with small-large model token routing. arXiv preprint arXiv:2505.21600, 2025. 21 [11] Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan Zhang, Boju Chen, Tianqi Wu, Hongyi Wang, Zixiao Huang, Shiyao Li, Shengen Yan, Guohao Dai, Huazhong Yang, and Yu Wang. Moa: Mixture of sparse attention for automatic large language model compression, 2024. [12] Tianyu Fu, Tengxuan Liu, Qinghao Han, Guohao Dai, Shengen Yan, Huazhong Yang, Xuefei Ning, and Yu Wang. Framefusion: Combining similarity and importance for video token reduction on large visual language models. arXiv preprint arXiv:2501.01986, 2024. 21 [13] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Bras, and Choi Yejin. Clipscore: reference-free evaluation metric for image captioning. pages 75147528, 01 2021. 7 [14] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS17, page 66296640, Red Hook, NY, USA, 2017. Curran Associates Inc. 8 [15] HPC-AI. Open-Sora. https://github.com/hpcaitech/Open-Sora, 2024. 3 [16] Yushi Huang, Ruihao Gong, Jing Liu, Yifu Ding, Chengtao Lv, Haotong Qin, and Jun Zhang. Qvgen: Pushing the limit of quantized video generative models. arXiv preprint arXiv:2505.11497, 2025. 10 [17] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. arXiv preprint arXiv:2311.17982, 2023. 7 [18] Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir H. Abdi, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention, 2024. 2, 3, 8, 17 [19] Black Forest Labs. Flux.1: high-quality text-to-image model. https://github.com/ black-forest-labs/flux, 2024. Accessed [current date]. 3, 7 [20] Xunhao Lai, Jianqiao Lu, Yao Luo, Yiyuan Ma, and Xun Zhou. Flexprefill: context-aware sparse attention mechanism for efficient long-sequence inference, 2025. 2 [21] Muyang Li, Yujun Lin, Zhekai Zhang, Tianle Cai, Xiuyu Li, Junxian Guo, Enze Xie, Chenlin Meng, Jun-Yan Zhu, and Song Han. Svdqunat: Absorbing outliers by low-rank components for 4-bit diffusion models. arXiv preprint arXiv:2411.05007, 2024. 3 [22] Shiyao Li, Yingchun Hu, Xuefei Ning, Xihui Liu, Ke Hong, Xiaotao Jia, Xiuhong Li, Yaqi Yan, Pei Ran, Guohao Dai, et al. Mbq: Modality-balanced quantization for large vision-language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 41674177, 2025. 3 [23] Shiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng Shi, Shengen Yan, Guohao Dai, Huazhong Yang, and Yu Wang. Evaluating quantized large language models. In Proceedings of the 41st International Conference on Machine Learning, pages 2848028524, 2024. [24] Xiuyu Li, Yijiang Liu, Long Lian, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang, and Kurt Keutzer. Q-diffusion: Quantizing diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1753517545, 2023. 3 [25] Haokun Lin, Teng Wang, Yixiao Ge, Yuying Ge, Zhichao Lu, Ying Wei, Qingfu Zhang, Zhenan Sun, and Ying Shan. Toklip: Marry visual tokens to clip for multimodal comprehension and generation. arXiv preprint arXiv:2505.05422, 2025. 21 [26] Haokun Lin, Haobo Xu, Yichen Wu, Jingzhi Cui, Yingtao Zhang, Linzhan Mou, Linqi Song, Zhenan Sun, and Ying Wei. Duquant: Distributing outliers via dual transformation makes stronger quantized llms. Advances in Neural Information Processing Systems, 37:8776687800, 2024. 3 [27] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023. 6 [28] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár. Microsoft coco: Common objects in context, 2015. 7 [29] Akide Liu, Zeyu Zhang, Zhexin Li, Xuehai Bai, Yizeng Han, Jiasheng Tang, Yuanjie Xing, Jichao Wu, Mingyang Yang, Weihua Chen, et al. Fpsattention: Training-aware fp8 and sparsity co-design for fast video diffusion. arXiv preprint arXiv:2506.04648, 2025. [30] Jiacheng Liu, Chang Zou, Yuanhuiyi Lyu, Junjie Chen, and Linfeng Zhang. From reusing to forecasting: Accelerating diffusion models with taylorseers. arXiv preprint arXiv:2503.06923, 2025. 21 [31] Tengxuan Liu, Shiyao Li, Jiayi Yang, Tianchen Zhao, Feng Zhou, Xiaohui Song, Guohao Dai, Shengen Yan, Huazhong Yang, and Yu Wang. Pm-kvq: Progressive mixed-precision kv cache quantization for long-cot llms. arXiv preprint arXiv:2505.18610, 2025. 3 [32] Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. Evalcrafter: Benchmarking and evaluating large video generation models. arXiv preprint arXiv:2310.11440, 2023. 7 [33] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. 2 [34] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart Van Baalen, and Tijmen Blankevoort. white paper on neural network quantization. arXiv preprint arXiv:2106.08295, 2021. 8 [35] William Peebles and Saining Xie. Scalable diffusion models with transformers, 2023. 2, 3 11 [36] Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and Yan Yan. Post-training quantization on diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19721981, 2023. 3 [37] Keda Tao, Haoxuan You, Yang Sui, Can Qin, and Huan Wang. Plug-and-play 1.x-bit kv cache quantization for video large language models. arXiv preprint arXiv:2503.16257, 2025. [38] THUDM. Cogvideox-5b. https://huggingface.co/THUDM/CogVideoX-5b, 2025. 7 [39] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphaël Marinier, Marcin Michalski, and Sylvain Gelly. Fvd: new metric for video generation. 2019. 8 [40] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 3 [41] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 3, 7, 14 [42] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: From error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. 8 [43] Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, and Nan Duan. Godiva: Generating open-domain videos from natural descriptions. arXiv preprint arXiv:2104.14806, 2021. 7 [44] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin. Exploring video quality assessment on user generated contents from aesthetic and technical perspectives. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2014420154, 2023. 7 [45] Haocheng Xi, Shuo Yang, Yilong Zhao, Chenfeng Xu, Muyang Li, Xiuyu Li, Yujun Lin, Han Cai, Jintao Zhang, Dacheng Li, Jianfei Chen, Ion Stoica, Kurt Keutzer, and Song Han. Sparse videogen: Accelerating video diffusion transformers with spatial-temporal sparsity, 2025. 2, 7, 8 [46] Haocheng Xi, Shuo Yang, Yilong Zhao, Chenfeng Xu, Muyang Li, Xiuyu Li, Yujun Lin, Han Cai, Jintao Zhang, Dacheng Li, et al. Sparse videogen: Accelerating video diffusion transformers with spatial-temporal sparsity. arXiv preprint arXiv:2502.01776, 2025. 3, 17 [47] Yifei Xia, Fangcheng Fu, Wentao Zhang, Jiawei Jiang, and Bin Cui. Efficient multi-task llm quantization and serving for multiple lora adapters. Advances in Neural Information Processing Systems, 37:63686 63714, 2024. [48] Yifei Xia, Suhan Ling, Fangcheng Fu, Yujie Wang, Huixia Li, Xuefeng Xiao, and Bin Cui. Training-free and adaptive sparse attention for efficient long video generation, 2025. 3 [49] Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, and Maosong Sun. Infllm: Training-free long-context extrapolation for llms with an efficient context memory, 2024. 3 [50] Guang Xuan Xiao, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Haotian Tang, Yao Fu, and Song Han. Duoattention: Efficient long-context llm inference with retrieval and streaming heads. arXiv preprint arXiv:2410.10819, 2024. 3 [51] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks, 2024. [52] Xingrun Xing, Zheng Liu, Shitao Xiao, Boyan Gao, Yiming Liang, Wanpeng Zhang, Haokun Lin, Guoqi Li, and Jiajun Zhang. Efficientllm: Scalable pruning-aware pretraining for architecture-agnostic edge language models. arXiv preprint arXiv:2502.06663, 2025. 21 12 [53] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation, 2023. 7 [54] Shuo Yang, Haocheng Xi, Yilong Zhao, Muyang Li, Jintao Zhang, Han Cai, Yujun Lin, Xiuyu Li, Chenfeng Xu, Kelly Peng, et al. Sparse videogen2: Accelerate video generation with sparse attention via semantic-aware permutation. arXiv preprint arXiv:2505.18875, 2025. 3 [55] Zhuoyi Yang et al. Seerattention: Learning intrinsic sparse attention in your llms. arXiv preprint arXiv:2410.13276, 2024. 2 [56] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2, 3, 7 [57] Zhihang Yuan, Hanling Zhang, Lu Pu, Xuefei Ning, Linfeng Zhang, Tianchen Zhao, Shengen Yan, Guohao Dai, and Yu Wang. DiTFastattn: Attention compression for diffusion transformer models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 2, 3, 8, 17 [58] Manzil Zaheer, Guru Guruganesh, Avinava Dubey, et al. Big bird: Transformers for longer sequences. arXiv preprint arXiv:2007.14062, 2020. [59] Jintao Zhang, Haofeng Huang, Pengle Zhang, Jia Wei, Jun Zhu, and Jianfei Chen. Sageattention2: Efficient attention with thorough outlier smoothing and per-thread int4 quantization, 2024. 2, 3, 8 [60] Jintao Zhang, Jia Wei, Pengle Zhang, Xiaoming Xu, Haofeng Huang, Haoxu Wang, Kai Jiang, Jun Zhu, and Jianfei Chen. Sageattention3: Microscaling fp4 attention for inference and an exploration of 8-bit training. arXiv preprint arXiv:2505.11594, 2025. 2 [61] Jintao Zhang, Jia Wei, Pengle Zhang, Jun Zhu, and Jianfei Chen. Sageattention: Accurate 8-bit attention for plug-and-play inference acceleration. In International Conference on Learning Representations (ICLR), 2025. 2, 3, 8 [62] Jintao Zhang, Chendong Xiang, Haofeng Huang, Jia Wei, Haocheng Xi, Jun Zhu, and Jianfei Chen. Spargeattn: Accurate sparse attention accelerating any model inference. arXiv preprint arXiv:2502.18137, 2025. 3, 8 [63] Jintao Zhang, Chendong Xiang, Haofeng Huang, Jia Wei, Haocheng Xi, Jun Zhu, and Jianfei Chen. Spargeattn: Accurate sparse attention accelerating any model inference, 2025. 14, 17, 21 [64] Peiyuan Zhang, Yongqi Chen, Runlong Su, Hangliang Ding, Ion Stoica, Zhenghong Liu, and Hao Zhang. Fast video generation with sliding tile attention. arXiv preprint arXiv:2502.04507, 2025. 2, 3, 7 [65] Peiyuan Zhang, Haofeng Huang, Yongqi Chen, Will Lin, Zhengzhong Liu, Ion Stoica, Eric Xing, and Hao Zhang. Faster video diffusion with trainable sparse attention. arXiv preprint arXiv:2505.13389, 2025. 3 [66] Pengle Zhang, Jia Wei, Jintao Zhang, Jun Zhu, and Jianfei Chen. Accurate int8 training through dynamic block-level fallback. arXiv preprint arXiv:2503.08040, 2025. [67] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, Zhangyang Wang, and Beidi Chen. H2o: Heavy-hitter oracle for efficient generative inference of large language models, 2023. 3 [68] Lin Zhao, Tianchen Zhao, Zinan Lin, Xuefei Ning, Guohao Dai, Huazhong Yang, and Yu Wang. Flasheval: Towards fast and accurate evaluation of text-to-image diffusion generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1612216131, 2024. 21 [69] Tianchen Zhao, Tongcheng Fang, Haofeng Huang, Enshu Liu, Rui Wan, Widyadewi Soedarmadji, Shiyao Li, Zinan Lin, Guohao Dai, Shengen Yan, Huazhong Yang, Xuefei Ning, and Yu Wang. Vidit-q: Efficient and accurate quantization of diffusion transformers for image and video generation, 2025. 3, 4, 5, 6 [70] Tianchen Zhao, Xuefei Ning, Tongcheng Fang, Enshu Liu, Guyue Huang, Zinan Lin, Shengen Yan, Guohao Dai, and Yu Wang. Mixdq: Memory-efficient few-step text-to-image diffusion models with metric-decoupled mixed precision quantization. In European Conference on Computer Vision, pages 285302. Springer, 2024. 3 13 Experimental Results for Wan 2.1 Model We present the results of applying PAROAttn and baseline sparsification and quantization methods to the Wan-2.1[41] 14B T2V model in Tab. 4, along with qualitative results in Fig. 8. Notably, when applying SpargeAttention[63], we observed numerical instability leading to NaN outputs; hence, its results are omitted from the table. These findings are consistent with those presented for the CogVideoX model in Table 1 of the main paper. We summarize our key observations as follows: (1) PAROAttn consistently outperforms baseline sparsification methods across varying density. As shown in Tab. 4, PAROAttn significantly outperforms the baseline method SparseVideoGen across different metrics and settings. Remarkably, PAROAttn with 0.3 density still surpasses SparseVideoGen at 0.5 density. (2) PAROAttn preserves both visual quality and content even under more aggressive sparsity. As illustrated in Fig. 8, PAROAttn at 0.3 density produces frames nearly identical to the dense baseline. In contrast, PAROAttn at 0.5 density introduces noticeable degradation with changes in both content and style. (3) PAROAttn achieves comparable performance to baseline quantization methods with higher speedups. As demonstrated in Tab. 4 and Fig. 8, PAROAttns quantization scheme further compresses the computation to INT8/INT4 on top of the SageAttn baseline, maintaining performance while delivering greater speedup. Table 4: Performance of PAROAttention Wan 2.1 text-to-video generation on VBench prompts. Baselines are evaluated using their official codebases. For fair comparison, we configure SparseVideoGen without skipping sparsification during the first 30% of timesteps."
        },
        {
            "title": "Method",
            "content": "- FP16 Full Attn."
        },
        {
            "title": "Sparse",
            "content": "SparseVideoGen (0.5) PAROAttn (0.5) SparseVideoGen (0.3) PAROAttn (0.3)"
        },
        {
            "title": "Efficiency",
            "content": "Dense Rate / Bitwidth 100.0% 50.0% 50.0% 30.0% 30.0% SageAttn SageAttnV2 QK (INT8), PV (FP16) QK (INT4), PV (FP8)"
        },
        {
            "title": "Quant",
            "content": "PAROAttn (INT8) PAROAttn (INT4) QK (INT8), PV (INT8) QK (INT4), PV (INT4)"
        },
        {
            "title": "Video Quality Metrics",
            "content": "FP Diff. Metrics CLIPSIM VQA FScore FVD-FP16 PSNR SSIM CosSim 0. 0.199 0.213 0.196 0.208 0.201 0.200 0.213 0.206 93.49 91.56 92. 90.13 91.97 92.24 88.53 92.89 89.77 0.000 0.468 0.114 0.612 0. 0.126 1.260 0.128 0.896 0.000 0.476 0.251 0.679 0.278 0.209 0. 0.362 0.412 15.32 22.02 13.17 21.73 20.43 17.86 20.13 19. 1.000 0.613 0.806 0.475 0.786 0.720 0.678 0.706 0.741 1. 0.900 0.978 0.839 0.978 0.970 0.954 0.967 0.965 Additional Qualitative Results and Analysis of Metrics Selection: Analysis of Additional Qualitative Results: We present additional qualitative comparisons of sparsification methods, along with their corresponding metric scores, in Fig. 9. We compare PAROAttn against SpargeAttn and SparseVideoGen on the CogVideoX model under density levels of 50% and 30%. As shown in the figure, PAROAttn produces nearly identical frames at both density levels, whereas SpargeAttn and SparseVideoGen introduce noticeable blurriness and content distortion. In particular, SpargeAttn at 30% density exhibits prominent square-shaped color blocks, and the generated content becomes barely recognizable. We further analyze the corresponding changes in metric scores in the next paragraph. Findings and Recommendation of Metrics: In Fig. 9, we observe that quality-related metric (VQA) and FP difference metric (PSNR) exhibit different trends as the dense rate varies. Specifically, for PAROAttn, the quality-related VQA metric remains stable even with 30% density, while the FP difference-related PSNR metric gradually decays. It reveals that the FP difference-related metrics are more challenging to maintain because they focus on low-level differences and can detect very minor detail changes that quality-related metrics might miss. As result, they are suitable for scenarios where only minor differences are expected. However, they may not be reliable when comparing samples with significant differences, in which case quality-related metrics are more appropriate. 14 Figure 8: Qualitative results of Wan 2.1 model video generation. Figure 9: Additional qualitative results of sparsification for CogVideoX. Ablation study on skipping scheme in SparseVideoGen. In the original SparseVideoGen paper and its official code release, sparsification is deliberately omitted for the first two Transformer blocks and the initial 30% of timesteps. In the main paper, for fair comparison, we donot adopt such skipping scheme for SparseVideoGen. We conduct an ablation study on the effect of this \"skipping\" scheme, with results shown in appendix and Fig. 11. As observed, removing the skipping strategy leads to notable degradation in generation quality (PSNR drops from 25.37 to 18.50), significant content distortion, and visibly blurred outputs. In contrast, PAROAttn maintains high generation quality even without skipping early timesteps or transformer blocks."
        },
        {
            "title": "C Additional Results for CUDA Kernel Efficiency Improvement",
            "content": "We provide detailed results comparing different CUDA kernel implementations in appendix and appendix C. The reported speedups are measured based on attention computation alone (excluding the QKVO projections), using token length of 17,750corresponding to 6-second 720P video generation with CogVideoX. Experiments for sparsification baselines are conducted on an NVIDIA 15 Method PSNR SSIM CosSim SparseVideoGen (0.5, w.o. skip) SparseVideoGen (0.5, w. skip) PAROAttention (0.5) 18.50 25.37 29.14 0.755 0.871 0.936 0.960 0. 0.997 Figure 10: Ablation of skipping timestep and transformer blocks for SparseVideoGen. Figure 11: Qualitative examples from the ablation study on skipping timesteps and Transformer blocks in SparseVideoGen. A100 GPU (consistent with the supported hardware in the baseline code release), while quantization results are evaluated on an RTX 4090 GPU to leverage support for INT4 and FP8 quantization. Comparison of Latency Speedups: We detailedly present the experimental results for PAROAttns CUDA kernel implementation with baseline sparsification and quantization methods in appendix C. We discuss the findings in Sec. 5.2 Hardware Resource Savings of the main paper in detail as follows: Baseline Sparsification methods exhibit notable performance degradation. Both the SparseVideoGen and SpargeAttn achieve PSNR lower than 20, even with relative high density of 50%, worse than the PAROAttn with both sparsification and quantization applied (0.3+INT8 with PSNR 21.49, 0.5+INT4 with PSNR 24.34). PAROAttns sparsification method can generate nearly identical frames, even with lower dense rate. As seen in appendix and Fig. 9, the PAROAttn generation result highly resembles the full-precision baseline, while achieveing substantial speedups. The PARO token reordering is compatible with dynamic sparsification approaches. In appendix C, the SpargeAttention (0.3 + PARO) means adopting the PARO token reordering with SpargeAttn, it notably improves the performance to exceeding the SpargeAttn (0.5), and improve the speedup from 1.67x to 2.11x. PAROAttn introduces minimal overhead. The overhead of sparsification is presented in appendix C, since the PAROAttn adopts static sparse scheme, it avoids the online static mask generation overhead. The remaining overhead is the online permutation, and loading of the sparse mask, which are also diminished with the kernel fusion and prefetch techniques, discussed further in the overhead of permutation/prefetch paragraph below. PAROAttn supports quantization of PV to lower-bit formats Comparing the PAROAttn (INT8/4) with SageAttn (V1/V2), PAROAttn could further quantizes the computation from FP16/FP8 to INT8/INT4 with similar performance, and notable better speedup (1.72x to 1.83x, and 2.56x to 2.97x). Overhead of Permutation: We present an overhead analysis of integrating permutation within the Rotary Position Embedding (RoPE) operator. As shown, this integration introduces only negligible overhead (0.03%), demonstrating that permutation can be fused with prior operators without performance impact. Overhead of Prefetch: As discussed in Section 4.2 of the main paper, static sparse attention introduces additional memory overhead due to the need to store the sparse mask in GPU memory. Since we adopt timestep-wise and transformer-block-wise sparse masks, the memory cost of storing the binary sparse mask is approximately 1GB. To mitigate this cost, we introduce prefetch scheme that only loads the sparse mask for the current transformer block and timestep, reducing memory usage to the KB level. Additionally, we employ double-buffering pipeline technique that allows for simultaneous sparse mask loading and attention computation, minimizing the time spent on sparse mask loading. Overall, the prefetching incurs only 0.33% of the total latency. 16 Method FlashAttention SpargeAttention (0.5) SparseVideoGen (0.5) PAROAttention (0.5) SpargeAttention (0.3) SpargeAttention (0.3 + PARO) SparseVideoGen (0.3) PAROAttention (0.3) PSNR - 16.80 18.50 29.14 15.22 16.89 17.53 22.90 Speedup Overhead 1.00x 1.67x 1.42x 1.73x 2.62x 2.62x 2.11x 2.71x - 6% 10% 0% 9% 9% 15% 0% Method FlashAttention SageAttnV1 PAROAttn (INT8) SageAttnV2 PAROAttn (INT4) PAROAttn (0.3 + INT8) PAROAttn (0.5 + INT4) PSNR - 29.58 29.01 24.46 24.16 21.49 24.34 Speedup 1.00x 1.72x 1.83x 2.56x 2.97x 5.72x 9.28x Table 5: Comparison of latency speedup for sparsification methods on NVIDIA A100. latency Table 6: Comparison of speedup for quantization methods on NVIDIA RTX4090. Table 7: Overhead of permutation. The latency comparison of whether adopting permutation to rope operator. The w. and w.o. stands for with and without w.o. permute w. permute Latency (ms) 1.2488 1.2492 overhead 0.03%"
        },
        {
            "title": "D Implementation Details and Analysis of Baseline Sparsification Methods",
            "content": "Implementation details: We select MInference [18], DiTFastAttn [57], SparseVideoGen [46], and SpargeAttention [63] for video generation. We visualize the sparse mask generated by baseline sparse methods in Fig. 12. MInference: We adapt the sparse attention scheme designed for language models to visual attention masks, making the following modifications: First, since we skip sparsification for the larger text tokens, the \"attention sink\" phenomenonwhere the first few tokens are significantly larger than the restis not observed. As result, the -shaped pattern degrades to single diagonal pattern, which can be viewed as special case of the \"verticalslash\" pattern. We select between the remaining \"vertical-slash\" and \"block-wise\" patterns based on cosine similarity, following the original implementation. Consistent with the original paper, the vertical-slash pattern is determined by selecting the top K% of vertical and diagonal lines with the largest summed values, where can be tuned to adjust the sparsity rate. For the \"block-wise\" pattern, we use 88 blocks and determine whether to retain each block based on its summed value. DiTFastAttn: Consistent with the original paper, we determine the window length by selecting the smallest window where the sum of values within the window reaches K% of the total attention values. SparseVideoGen: We use the official code implementation, the num-sampled-rows are chosen as the default value 32 and 64 for CogVideo and Wan. Specifically, for fair comparison, we donot adopt skipping sparsification for the first timesteps, and set first-times-fp as 0. We also present the ablation of such skipping scheme in appendix C. SpargeAttn: We use the official code implementation to test the performance of the CogVideoX model. When adapting SpargeAttn to Wan, numerical stability issue arises, causing the attention computation to produce NaN values; therefore, we omit these results. The hyperparameters for SpargeAttn are tuned using the script provided in the official code. Table 8: Overhead of prefetching. The latency comparison of whether adopting prefetch for attention. w.o prefetch w. prefetch Latency (ms) 1296.5 1300.8 overhead 0.33% For density of 50%, we set l1 = 0.09 and pvl1 = 0.095. For density of 30%, we choose l1 = 0.13 and pvl1 = 0.135. Visualization of attention masks: We present comparison of sparse masks for PAROAttention and baseline sparse attention methods. The first column shows the post-softmax attention patterns. As can be seen, for the SparseVideoGen method, while it successfully identifies the diagonal in block temporal attention pattern, the diagonal selection within the block remains inaccurate even at relatively high dense rate. In contrast, PAROAttn effectively preserves attention values while exploiting sparsity. For DiTFastAttn, the window-based attention struggles with the multiple diagonal pattern and fails to capture diagonals located far from the center. Similarly, MInferences diagonal pattern is unable to accurately preserve the naturally block-wise attention pattern. Figure 12: Comparison of attention masks for PAROAttention and baseline sparse attention methods. We present the relative difference metrics (L1 Norm, RMSE, CosSim) to measure the difference between the original and masked attention map."
        },
        {
            "title": "E The Effectiveness of PAROAttention Quantization Technique",
            "content": "Incoherence Analysis: To demonstrate the effectiveness of PAROAttentions quantization technique, we present the data distribution within the quantization group (a 6464 block) in Fig. 13. As shown, similar values are successfully aggregated into localized blocks, and the outliers present in the original data distribution are significantly reduced. This reduces the incoherence range from 200-1200 to 12-20, and thus significantly reducing the quantization error. Comparison with FP8 quantization: In SageAttnV2  (Table 6)  , the authors analyze the cosine similarity between the quantized matrix and its original FP counterpart, concluding that INT8 quantization introduces too much error and thus opting for FP8. However, after applying patternaware token reordering, the incoherence within the attention map data groups is significantly reduced, 18 Figure 13: Incoherence for data within the quantization group before and after permutation. Figure 14: Quantization error with respect to FP for quantization of the attention map . The red FP stands for the FP8 quantization error. leading to notable decrease in quantization error. As shown in Fig. 14, the INT8 quantized attention map achieves significantly higher FP difference metric scores compared to its FP8 counterpart. This is because INT8 provides more mantissa bits to accurately represent subtle value differences. Reasons for exploring interger quantization: Despite FP8 quantization achieves good performance and valid acceleration with easy deployment. We summarize the reasons for exploring integer quantization as follows: Lower quantization error: Low-bit floating-point formats consist of both exponent bits and mantissa bits. For example, the E5M2 FP8 format has 5 exponent bits and 2 mantissa bits. The reduced number of mantissa bits limits its ability to represent small value differences, potentially leading to performance degradation. In contrast, with the same bitwidth, integer formats provide more mantissa bits (e.g., equivalently 7 mantissa bits for INT8), enabling them to preserve subtle data differences and achieve lower quantization error. By applying proper preprocessing to remove outliers within data groups, the need for additional exponent bits to handle large dynamic variations is reduced. This advantage becomes even more pronounced at lower bitwidths, such as 4-bit, where FP4 formats have only 1-2 mantissa bits. As presented in Fig. 5 in the main paper, the all INT4 PAROAttention quantized Flux model could still generate images with high quality. To conclude, integer quantizations representation power is essential for lower bitwidth quantization. Support non-GPU hardware platforms. Despite Nvidia TensorCore [1] demonstrate simialr computing power for FP8 and INT8 matrix multiplication. However, for domainspecific accelerator design [2], adopting INT8 matrix multiplication could be more resourceefficient than FP8. Therefore, integer quantization is valuable for AI accelerator hardware design beyond GPU."
        },
        {
            "title": "F Generalization of Sparse Attention Mask",
            "content": "We present visualization of the post-softmax attention patterns across different timesteps, prompts, and classifier-free guidance (CFG) settings in Fig. 15. The relative metric scores (L1 Norm, RMSE, cosine similarity) are calculated based on the attention pattern at timestep 5, as indicated by the red text. As shown, the type of attention pattern remains consistent across timesteps, prompts, and 19 CFG. However, the detailed attention pattern may vary over timesteps, gradually stabilizing in later timesteps. To address this, we design timestep-wise sparse masks and share the sparse mask for later timesteps. Figure 15: Visualization of post softmax attention pattern for different timesteps, prompts, and classifier-free-guidance (CFG). The metric scores are calculated relative to the attention pattern with red text."
        },
        {
            "title": "G Additional Visualization of Permutation for Flux",
            "content": "We present the attention pattern for flux under different permutations. The permutation also effectively produces concentrated and regular block-wise pattern. Figure 16: Visualization of the attention pattern for flux under different permutation."
        },
        {
            "title": "H Discussion of application of PAROAttn",
            "content": "As discussed in the \"Discussion of Adaptability\" section of the main paper, we introduce patternaware token reordering (permutation) as universal and efficient preprocessing step for attention patterns. Its effectiveness stems from the unique properties of vision transformers, where 3D (or 2D) spatial information is flattened into 1D token sequence, disrupting local adjacency. By concentrating attention patterns into more regular and block-wise structures, it benefits wide range of application scenarios. For Compression Techniques: The advantages of this approach extend beyond the specific design of PAROAttention and are applicable to various compression techniques, such as timestep-wise sharing [30], efficient reasoning [68, 10, 12], efficient architecture design [25, 52, 47]. For dynamic sparse attention methods, such as SpargeAttn [63], the relative importance of blocks becomes more apparent, simplifying the task of generating sparse masks from QK embeddings. Additionally, the concentrated patterns can improve caching strategies for feature reuse. For Model Training: PAROAttentions permutation design also sheds light on the meaning of attention patterns, which could inspire future improvements in model training. For instance, it could encourage different attention heads to focus on aggregating information along different dimensions, leading to more specialized and efficient learning. Beyond Visual Generative Models: The effectiveness of permutation arises from the unique properties of vision transformers, but its applicability is not limited to visual generative models. It could potentially be extended to multi-modal large language models and large vision models for perception tasks, offering similar benefits in these domains."
        },
        {
            "title": "I Limitations and Broader Impacts",
            "content": "The methodology can be further improved from several perspectives. Permutation represents constrained subset of possible token reordering, and we adopt simple block sum as sparse metric, exploring more advanced reordering techniques or sparse metric could further enhance performance. PAROAttn introduces novel direction by leveraging token reordering to reorganize attention patterns. The idea is not limited to post-training compression, and could be extended to broader applications, such as enabling native sparse attention or training acceleration."
        }
    ],
    "affiliations": [
        "ByteDance",
        "Tsinghua University"
    ]
}