{
    "paper_title": "LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization",
    "authors": [
        "Xingyu Wu",
        "Yuchen Yan",
        "Shangke Lyu",
        "Linjuan Wu",
        "Yiwen Qiu",
        "Yongliang Shen",
        "Weiming Lu",
        "Jian Shao",
        "Jun Xiao",
        "Yueting Zhuang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large reasoning models have achieved remarkable performance through extended chain-of-thought sequences, yet this computational freedom leads to excessive token generation even for simple problems. We present Length-Adaptive Policy Optimization (LAPO), a novel framework that transforms reasoning length control from an external constraint into an intrinsic model capability. Unlike existing approaches that impose rigid limits or rely on post-hoc interventions, LAPO enables models to internalize an understanding of appropriate reasoning depth through a two-stage reinforcement learning process. In the first stage, models learn natural reasoning patterns by discovering the statistical distribution of successful solution lengths. The second stage leverages these patterns as meta-cognitive guidance, embedding them directly within the model's reasoning context to ensure inference-time flexibility. Experiments on mathematical reasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\\% while improving accuracy by 2.3\\%. Our analysis reveals that models trained with LAPO develop emergent abilities to allocate computational resources based on problem complexity, achieving efficient reasoning without sacrificing quality."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 8 5 7 5 1 . 7 0 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "LAPO: INTERNALIZING REASONING EFFICIENCY VIA LENGTH-ADAPTIVE POLICY OPTIMIZATION Xingyu Wu1, Yuchen Yan1, Shangke Lyu1, Linjuan Wu1, Yiwen Qiu1, Yongliang Shen1, Weiming Lu1, Jian Shao1, Jun Xiao1, Yueting Zhuang1 1Zhejiang University {wuxingyu, syl}@zju.edu.cn GitHub: https://github.com/zju-real/lapo (cid:128) Project: https://zju-real.github.io/lapo"
        },
        {
            "title": "ABSTRACT",
            "content": "Large reasoning models have achieved remarkable performance through extended chain-of-thought sequences, yet this computational freedom leads to excessive token generation even for simple problems. We present Length-Adaptive Policy Optimization (LAPO), novel framework that transforms reasoning length control from an external constraint into an intrinsic model capability. Unlike existing approaches that impose rigid limits or rely on post-hoc interventions, LAPO enables models to internalize an understanding of appropriate reasoning depth through two-stage reinforcement learning process. In the first stage, models learn natural reasoning patterns by discovering the statistical distribution of successful solution lengths. The second stage leverages these patterns as metacognitive guidance, embedding them directly within the models reasoning context to ensure inference-time flexibility. Experiments on mathematical reasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9% while improving accuracy by 2.3%. Our analysis reveals that models trained with LAPO develop emergent abilities to allocate computational resources based on problem complexity, achieving efficient reasoning without sacrificing quality."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advances in large reasoning models have demonstrated remarkable capabilities through extended chain-of-thoughts (Wei et al., 2022; Jaech et al., 2024; Guo et al., 2025). However, this computational freedom comes with critical limitation: these models often generate excessively verbose reasoning chains regardless of problem complexity, leading to significant computational overhead, phenomenon known as overthinking (Min et al., 2024). While beneficial for complex problems, this verbosity introduces substantial inefficiencies, making practical deployment challenging. Existing approaches to address this challenge fall into three main categories, each with inherent limitations. Direct length reduction methods either rely on reward design (Yang et al., 2025; Huang et al., 2025) that can cause over-shortening and accuracy degradation, or impose hard length constraints (Aggarwal & Welleck, 2025; Hou et al., 2025) that lack adaptability across problem types. Dynamic early-stopping approaches (Qiao et al., 2025; Muennighoff et al., 2025) make real-time termination decisions but often truncate mid-reasoning, disrupting the thinking process. Adaptive thinking methods (Lou et al., 2025; Zhang et al., 2025; Fang et al., 2025) enable models to switch between thinking and non-thinking modes but operate at coarse granularity. The fundamental limitation of these approaches lies in their treatment of length control as an external constraint imposed upon the reasoning process. This paradigm inherently conflicts with the nature of mathematical reasoning, where each problem possesses its own intrinsic complexity that naturally determines the required reasoning depth. Current methods fail to recognize that when models successfully solve problems, they naturally converge to certain reasoning lengths that reflect this"
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Overview of Length-Adaptive Policy Optimization (LAPO) and its superior performance. The LAPO framework (left) trains model in two stages: first discovering natural reasoning lengths, then internalizing them as self-proposed budgets. This process enables our models (LAPO-I) to achieve state-of-the-art balance between accuracy and efficiency (right), surpassing existing methods by operating in the desirable top-left region of the performance plot. intrinsic complexity. The challenge is not to impose arbitrary limits, but to help models discover and internalize these natural reasoning patterns. We propose paradigm shift: instead of constraining reasoning through external mechanisms, we enable models to learn from their own successful reasoning patterns and develop an internal sense of appropriate reasoning depth. Our key insight is that the distribution of reasoning lengths in correct solutions contains valuable information about how much thinking each problem genuinely requires. By capturing these patterns during training and teaching models to anticipate the appropriate reasoning budget before they begin solving, we can transform length control from an external limitation into an intrinsic capability. We introduce Length-Adaptive Policy Optimization (LAPO), two-stage reinforcement learning framework that progressively builds this adaptive capability. In the first stage, we design lengthaware rewards that encourage efficiency while maintaining accuracy. During this process, we collect statistical patterns from successful solutions, specifically focusing on the reasonable length range where most correct answers naturally fall. This reveals the inherent reasoning requirements In the second stage, we leverage these of each problem without imposing artificial constraints. discovered patterns to provide explicit guidance to the model. By incorporating target length information directly into the problem prompt, we enable models to plan their reasoning trajectory before beginning the solution process. Crucially, to ensure models can reason adaptively without requiring predefined lengths at inference time, we embed the length constraint as self-declarative statement immediately after the think token. This technique reframes the budget not as an external command, but as part of the models own internal reasoning plan. By learning to generate solution that aligns with its self-proposed budget, the model is incentivized to internalize the link between problem complexity and resource allocation, allowing it to reason flexibly and efficiently when deployed. LAPO fundamentally differs from existing approaches by recognizing that efficient reasoning requires understanding problem-specific computational needs rather than following rigid rules. Our two-stage design enables natural progression: models first learn what constitutes appropriate reasoning depth through experience, then develop the ability to anticipate these requirements proactively. This approach mirrors how human experts develop intuition about problem complexity, allocating mental effort proportionally to task demands. Extensive experiments validate the effectiveness of our approach. LAPO achieves remarkable efficiency gains, reducing token usage by up to 40.9% while simultaneously improving accuracy by 2.3% on mathematical reasoning benchmarks (see Figure 1). Our analysis reveals that this improvement stems from the models ability to distinguish between problems requiring elaborate derivations versus those needing only brief calculations. The training dynamics demonstrate smooth convergence in both stages, with models maintaining stable accuracy even as they learn increasingly"
        },
        {
            "title": "Preprint",
            "content": "precise length control. These results confirm that when models learn from their own successful patterns rather than arbitrary constraints, they develop more robust and efficient reasoning strategies. Our main contributions are: We propose LAPO, novel two-stage reinforcement learning framework that transforms length control from an external constraint into an intrinsic reasoning capability, enabling models to adaptively allocate computational resources based on problem complexity. We introduce training methodology that combines statistical analysis of successful reasoning patterns with contextual length guidance embedded within the reasoning process, allowing models to internalize length-adaptive behaviors while maintaining inference-time flexibility. We demonstrate through extensive experiments that LAPO achieves substantial efficiency gains (up to 40.9% token reduction) while improving accuracy, revealing that models can develop emergent meta-cognitive abilities for reasoning budget allocation."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "2.1 TEST-TIME SCALING IN LARGE LANGUAGE MODELS Increasing test-time computation has consistently been shown to improve performance in complex reasoning tasks, mathematical problem-solving, and code generation (Wu et al., 2024; Wang et al., 2022; Wei et al., 2022; Guo et al., 2025). Test-time scaling laws indicate predictable performance gains from increasing inference computation, either by generating more reasoning chains or longer ones (Wu et al., 2024; Snell et al., 2024; Jaech et al., 2024). Prominent approaches include parallel sampling of multiple reasoning paths (Wang et al., 2022), tree-based search (Yao et al., 2023; Wu et al., 2024), and iterative refinement techniques (Snell et al., 2024; Welleck et al., 2024). Recent reasoning models such as OpenAIs O1 and DeepSeeks R1-style models (Jaech et al., 2024; Guo et al., 2025) simplify test-time scaling by generating extended reasoning traces through reinforcement learning with verifiable rewards (RLVR), encouraging deep thinking behaviors such as broad exploration and feasibility checks (Gandhi et al., 2025). However, these extended reasoning behaviors often lead to much longer reasoning traces, sometimes several times longer than those produced by short CoT models (Sui et al., 2025; Chen et al., 2024), creating an overthinking issue that largely increases inference costs (Kumar et al., 2025). Several recent works have shown that this extended reasoning often includes redundant or unnecessary verification and reflection, even on simple problems (Wang et al., 2025). Despite their promising results, existing methods lack precise and dynamic control over the length of generated reasoning chains, resulting in often suboptimal performance or unrealized potential efficiency gains. 2.2 EFFICIENT LONG CHAIN-OF-THOUGHT LLM While test-time scaling with long CoT significantly improves accuracy, it comes at the cost of computational inefficiency. In particular, reasoning models often produce verbose and unnecessary reasoning when solving simple problemsa phenomenon commonly referred to as overthinking (Sui et al., 2025). To address the overthinking phenomenon in reasoning models, various methods have been proposed following three main strategies. Prompt-based methods attempt to control response length by incorporating instructions directly into prompts (Xu et al., 2025a), but cannot achieve precise length control. Training-based methods include supervised fine-tuning approaches that collect datasets with variable lengths (Han et al., 2024; Kang et al., 2025; Ma et al., 2025; Xia et al., 2025) and RL-based methods that incorporate length penalties into reward functions (Muennighoff et al., 2025; Yeo et al., 2025; Luo et al., 2025a; Xu et al., 2025b). However, these methods fail to control length according to users requirements or problem complexity. Routerbased methods train separate classifiers to route queries between fast and reasoning models (Chuang et al., 2024; Ong et al., 2024), but require additional computational overhead. Recent advances in token budget control have introduced more sophisticated approaches. Works like L1 (Aggarwal & Welleck, 2025) and Elastic Reasoning (Xu et al., 2025b) can more precisely control output length under given token budgets, yet they fail to enable autonomous estimation of appropriate response lengths for different problems."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: The LAPO framework consists of two stages: (1) Discovery stage learns natural reasoning patterns by rewarding efficient correct solutions and collecting length statistics; (2) Internalization stage embeds these statistics as self-proposed plans within the models reasoning context, teaching models to internalize efficient reasoning. In contrast to these prior approaches, our LAPO framework uniquely combines autonomous budget estimation and precise length control capabilities through two-stage reinforcement learning design. Unlike existing methods that rely on external truncation mechanisms or require manual budget specification, LAPO trains models to intrinsically learn appropriate reasoning lengths while maintaining reasoning completeness and logical coherence. This endogenous length control capability enables problem-adaptive token budget allocation, achieving significant efficiency improvements while maintaining or enhancing reasoning performance."
        },
        {
            "title": "3 METHOD",
            "content": "We present Length-Adaptive Policy Optimization (LAPO), framework that enables reasoning models to internalize efficient reasoning as an intrinsic capability. Our approach fundamentally differs from existing methods by teaching models to develop an internal understanding of appropriate reasoning depth, rather than imposing external constraints. We achieve this through carefully designed two-stage training process that first discovers natural reasoning patterns, then transforms these patterns into an internalized capability. 3.1 OVERVIEW Consider reasoning model generating response for problem q. While current models produce high-quality solutions, they lack awareness of computational efficiency, often generating responses far exceeding necessary length. Our goal is to train models that autonomously determine appropriate reasoning lengths while maintaining solution quality. Our key insight is that successful problem solutions naturally exhibit certain length distributions that reflect intrinsic problem complexity. Rather than viewing these patterns as constraints to enforce, we treat them as signals that teach models about reasoning depth requirements. LAPO employs two-stage approach illustrated in Figure 2: the Discovery stage explores natural reasoning patterns through length-aware rewards, while the Internalization stage transforms these patterns into adaptive reasoning behavior. 3.2 DISCOVERY STAGE: LEARNING NATURAL REASONING PATTERNS The Discovery stage aims to uncover inherent relationships between problems and their natural reasoning lengths through GRPO training with carefully designed reward mechanism that encourages efficient exploration while maintaining correctness."
        },
        {
            "title": "Preprint",
            "content": "Extracting Statistics from GRPO Rollouts. During GRPO training, we generate rollout responses for each problem in the training batch. From these rollouts, we collect the lengths of responses that produce correct answers: Lq = {ri : I(yi = ygold) = 1, [1, ]} where yi is the predicted answer from the i-th rollout response ri. This collection, extracted directly from the GRPO sampling process, represents natural variation in successful reasoning lengths. (1) We derive two key statistics from these rollouts. First, we establish reasonable length range using percentiles to filter outliers while preserving central tendencies: [Lmin, Lmax] = [Percentile30(Lq), Percentile70(Lq)] (2) Second, we create problem-to-length mapping that will guide the Internalization stage: : (cid:55) Lmedian(q) = Median(Lq) (3) For problems without correct solutions in the current rollouts, we set M(q) = 4096 (maximum sequence length) to encourage comprehensive exploration in subsequent episodes. Length-Aware Reward Design. We employ composite reward function balancing accuracy and efficiency: R1(ri, q) = I(yi = ygold) + α Rlength-1(ri, q) (4) The length component operates on crucial principleonly correct responses receive length-based rewards: Rlength-1(ri, q) = 1.0 max(0, 1 di/100) 0 if correct and Lmin ri Lmax, if correct and ri / [Lmin, Lmax], if incorrect; (5) where di = min(ri Lmin, ri Lmax). This design creates gradients guiding models toward efficient lengths while allowing flexibility for complex problems. Throughout the Discovery stage, we continuously update after each GRPO training step to reflect evolving model capabilities. 3.3 INTERNALIZATION STAGE: LENGTH-AWARE EFFICIENT REASONING The Internalization stage transforms discovered patterns into internalized capabilities through continued GRPO training with modified prompts and rewards. Length-Conditioned Rollout. We augment each problem prompt with explicit length guidance: prompt = promptq + <think> will answer the question with tokens. (6) where = M(q) from the Discovery stage. This embeds length awareness within the reasoning context, helping models perceive computational budgets as intrinsic to thinking rather than external constraints. Length-Adherence Reward. To encourage the model to follow its self-declared reasoning budget from Equation 6, the Internalization stage employs precision-focused reward function. This function is designed to reward the alignment between the models output length and its self-declared budget n. The total reward is defined as: R2(ri, q) = I(yi = ygold) + β RLength-2(ri, q) where the adherence component, RLength-2, is only granted for correct solutions: RLength-2(ri, n) = (cid:40) exp (cid:16) (rin)2 2σ2 (cid:17) 0 if correct, otherwise; (7) (8) This Gaussian-inspired reward positively reinforces solutions that are both correct and consistent with the intended reasoning depth. By rewarding adherence to the self-proposed plan, this mechanism guides the model to internalize the relationship between problem complexity and an appropriate computational budget, rather than merely tracking an external signal."
        },
        {
            "title": "Preprint",
            "content": "Sample batch for each problem do Generate rollouts: {r1, ..., rN } πθ(q) Collect correct lengths: Lq = {ri : yi = ygold} Compute range: [Lmin, Lmax] = [P30(Lq), P70(Lq)] Update mapping: M(q) = Median(Lq) Compute rewards: R1(ri, q) = I(yi = ygold) + α Rlength-1(ri, q) Algorithm 1 Length-Adaptive Policy Optimization (LAPO) 1: Input: Base model πθ, training data D, hyperparameters α, β, σ, E1, E2 2: Output: Length-adaptive model π θ 3: 4: // Discovery Stage 5: for episode = 1 to E1 do 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: end for 16: 17: // Internalization Stage 18: for episode = 1 to E2 do 19: 20: 21: 22: 23: 24: 25: 26: 27: end for 28: return π θ end for Update πθ using GRPO with rewards R1 end for Update πθ using GRPO with rewards R2 Sample batch for each problem do Augment prompt: = + <think> will answer the question with M(q) tokens. Generate rollouts: {r1, ..., rN } πθ(q) Compute rewards: R2(ri, q) = I(yi = ygold) + β RLength-2(ri, q) Update mapping M(q) using dual-strategy (Eq. 9) Internalization via In-Context Guidance. cornerstone of our framework is how it fosters genuine internalization, enabling inference-time flexibility without explicit length targets. The key lies in the design of the augmented prompt (Equation 6). By placing the length guidance will answer the question with tokens. immediately after the <think> token, we transform an external constraint into an intrinsic part of the models cognitive process. During the Internalization stage, we refine based on new GRPO rollouts with dual-strategy update: M(q) = (cid:40) Median(L(t) ) min(M(q), Median(L(t) )) if previously unsolved if previously solved (9) This ensures newly solved problems establish reasonable benchmarks while previously solved problems gravitate toward more efficient solutions. 3.4 TRAINING PIPELINE We present the complete LAPO training procedure in Algorithm 1. LAPO employs GRPO across both stages with the following pipeline: Discovery Stage (Lines 4-13): The model explores natural reasoning patterns through GRPO training with length-aware rewards. For each problem in the training batch, we generate multiple rollouts and extract statistics from successful responses. The mapping is continuously updated to capture the evolving understanding of appropriate reasoning lengths. This stage runs for E1 epochs, allowing the model to discover problem-specific length patterns through self-supervised exploration. Internalization Stage (Lines 15-24): The model learns to internalize efficient reasoning by incorporating discovered length patterns into the training process. Each problem prompt is augmented with target length information derived from the Discovery stage. The placement of"
        },
        {
            "title": "Preprint",
            "content": "this guidance within the <think> block encourages the model to treat the budget as part of its own reasoning plan, which fosters genuine length awareness rather than rote instruction following. The dual-strategy update mechanism refines the mapping throughout training, allowing newly solved problems to establish benchmarks while encouraging efficiency improvements for previously solved ones. This progressive design mirrors cognitive development: first gaining experience about appropriate reasoning depth through practice, then learning to anticipate these requirements proactively. The embedding of guidance as self-declared plan is the key mechanism that bridges this gap from experience to proactive anticipation. By making efficiency an intrinsic part of reasoning, LAPO creates models that naturally adapt computational investment to match problem demands."
        },
        {
            "title": "4 EXPERIMENT SETUP",
            "content": "Training Details. We train our models on mixed dataset of 10,000 mathematical problems. This dataset comprises 6,000 examples randomly sampled from the DeepScaleR-Preview-Dataset (Luo et al., 2025b), which sources problems from competitive math contests (e.g., AIME, AMC) and benchmarks (e.g., Omni-Math (Gao et al., 2024), STILL (Min et al., 2024)). The remaining 4,000 examples are drawn from the intermediate levels (3-5) of the MATH dataset (Hendrycks et al., 2020) to ensure balanced difficulty distribution. For the base models, we experiment with DeepSeekR1-1.5B(Guo et al., 2025), strong instruction-tuned reasoner, and DeepScaleR-1.5B-Preview(Luo et al., 2025b), an advanced RL-finetuned version known for high-quality but verbose outputs. All models are trained using the Group Relative Policy Optimization (GRPO) algorithm. The LAPO framework is applied in two sequential stages. Stage 1 (LAPO-Discovery) is trained for 240 steps (approx. 3 epochs) with the length-aware reward R1 (Eq. 4), where hyperparameter α is 0.7. The resulting model then serves as the initialization for Stage 2 (LAPO-Internalization), which is trained for another 240 steps using the plan-adherence reward R2 (Eq. 7) with β set to 0.8. Due to computational constraints, the maximum context length is limited to 4,096 tokens during training. More training details are provided in Appendix A.1. Evaluation Details. We evaluate the trained models on four challenging benchmarks representing spectrum of mathematical difficulty: MATH-500 (Hendrycks et al., 2020), AIME2024, AMC23, and Olympiad-Bench (He et al., 2024). Following standard practices (Guo et al., 2025), we use sampling temperature of 0.6, top-p of 0.95, and maximum generation length of 32,768 tokens. For each benchmark, we report Pass@1 accuracy and the average number of tokens per response. The number of samples per query is 4 for MATH-500 and Olympiad-Bench, and 32 for the more competitive AIME2024 and AMC23. Baselines. We benchmark LAPO against two classes of baselines: the foundational models we build upon and existing methods designed for efficient reasoning. First, we evaluate the Base Models to establish performance starting point. These include DeepSeek-R1-1.5B(Guo et al., 2025), strong instruction-tuned model, and DeepScaleR-1.5B-Preview(Luo et al., 2025b), powerful RLfinetuned reasoner known for its high-quality but often verbose outputs. Both are evaluated without any length-control modifications. Second, we compare against state-of-the-art Efficient Reasoning Baselines that represent two distinct technical approaches. For methods focusing on direct length budgeting, we evaluate L1-Exact and L1-Max(Aggarwal & Welleck, 2025). Both train model to adhere to user-specified length constraint by penalizing deviations from the target budget, with L1-Exact enforcing precise match and L1-Max treating the budget as soft upper limit. For the iterative pruning strategy, we compare against ThinkPrune(Hou et al., 2025). This method trains model to shorten its reasoning by imposing hard token limit during its iterative RL process, where any response exceeding the limit receives zero reward. We evaluate its two publicly available variants, ThinkPrune-iter2k and ThinkPrune-4k."
        },
        {
            "title": "5 RESULTS AND ANALYSIS",
            "content": "We present comprehensive experimental results and analysis to validate LAPOs effectiveness and understand its underlying mechanisms. We begin with the main results (Section 5.1), benchmarking"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Main results on MATH500, AIME2024, AMC, and OlympiadBench. We report Pass@1 accuracy (%) and the average number of generated tokens (#Tok). In each base model group, bold indicates the best and underline indicates the second-best Pass@1 score. MATH-500 AIME2024 AMC-23 OlympiadBench Average Pass@1 #Tok Pass@1 #Tok Pass@1 #Tok Pass@1 #Tok Pass@1 #Tok Base model: DeepSeek-R1-distill-Qwen-1.5B Base + LAPO-D + LAPO-I 83.1 84.7 84.3 4031 2566 2354 30.3 28.5 29. 12150 8415 8318 68.3 72.2 71.2 Base model: DeepScaleR-1.5B-Preview L1-Exact L1-Max ThinkPrune-I2k ThinkPrune-4k Base + LAPO-D + LAPO-I 80.6 81.9 85.5 86.6 85.8 86.4 86.3 1953 1673 1707 2042 3280 2365 2168 24.4 24.9 34.9 35. 35.5 37.6 38.1 2625 3638 5095 6488 9246 5945 5371 70.9 72.7 74.3 76.3 74.2 77.6 78.3 7222 4132 2177 2705 2913 3839 6416 3655 3765 50.0 51.3 51.7 48.8 50.5 54.7 55.7 54.6 56.1 56.3 8942 5595 2357 2151 3498 4010 5974 4499 4024 57.9 59.2 59.1 56.2 57.5 62.3 63.5 62.5 64.4 64.8 8086 5177 2278 2541 3303 4094 6229 4116 3832 LAPO against baselines and state-of-the-art methods. We then conduct in-depth ablation studies on key design choices, including the the form of length guidance (Section 5.2) and the statistical metrics for target length selection (Section 5.3). Finally, we provide mechanistic analysis of how LAPO works, examining its emergent ability for difficulty-aware resource allocation (Section 5.4), its qualitative refinement of reasoning patterns (Section 5.5). 5.1 MAIN RESULTS As shown in Table 1, LAPO consistently improves reasoning efficiency across all benchmarks, achieving substantial token reduction while simultaneously enhancing or maintaining accuracy. Our key findings are summarized below. LAPO simultaneously enhances reasoning performance and reduces test-time computes. Our framework demonstrates clear superiority over both the foundational models and state-of-the-art baselines. When applied to DeepScaleR-1.5B-Preview, LAPO-I achieves remarkable 37.8% token reduction while boosting the average accuracy by 2.3 percentage points. This trend holds for DeepSeek-R1-1.5B, where it cuts token usage by 40.9% and improves accuracy by 1.2 points. These results validate that LAPO enables models to generate more concise yet effective reasoning chains. LAPO surpasses existing efficient reasoning optimization approaches. Compared to existing methods, LAPO establishes more effective accuracy-efficiency frontier. On the MATH-500 benchmark, our LAPO-I model (86.3%) substantially outperforms both L1-Max (81.9%) and L1Exact (80.6%). While L1-Max achieves aggressive token compression, its over-shortening appears to compromise reasoning quality on complex problems. In contrast, LAPO strikes more effective balance. Against ThinkPrune-4k, which shares our 4k training window, LAPO demonstrates superior performance in both token efficiency and accuracy, confirming its ability to find better trade-off. Both Discovery and Internalization stages contribute to the final performance. The effectiveness of LAPO stems from its two-stage design, where performance progresses verbeter from the Discovery stage (LAPO-D) to the Internalization stage (LAPO-I). LAPO-D first establishes strong foundation, achieving significant 36.0% token reduction on its own by learning natural reasoning length distributions. Building on this, LAPO-I achieves an additional 6.9% efficiency gain by internalizing these patterns through in-context guidance. This progressive refinement, combined with LAPOs consistent performance improvements across diverse benchmarks, confirms that our framework learns generalizable principles of adaptive reasoning rather than task-specific heuristics."
        },
        {
            "title": "Preprint",
            "content": "Table 2: Experimental results within different length guidance for LAPO-I. Bold and underline indicate the best and second-best Pass@1 scores. w/ indicates length guidance used in LAPO-I. Method MATH-500 AIME2024 AMC-23 OlympiadBench Average Pass@1 #Tok Pass@1 #Tok Pass@1 #Tok Pass@1 #Tok Pass@1 #Tok Base model: DeepScaleR-1.5B-Preview Base LAPO-D w/ Exact w/ Range w/ Implicit 85.8 86.4 86.3 86.6 86.9 3280 2365 2168 2153 35.5 37.6 38.1 36.5 36.2 9246 5945 5371 6095 5963 74.2 77.6 78.3 76.9 76.1 6416 3655 3765 3600 4002 54.6 56.1 56.3 56.2 55.1 5974 4499 4024 4011 62.5 64.4 64.8 64.1 63.6 6229 4116 3832 3964 4088 Table 3: Experimental results within different statistical metrics used for target length selection in LAPO-I. Bold and underline indicate the best and second-best Pass@1 scores. w/ indicates statistical metrics used for target length selection in LAPO-I. Method MATH-500 AIME AMC-23 OlympiadBench Average Pass@1 #Tok Pass@1 #Tok Pass@1 #Tok Pass@1 #Tok Pass@1 #Tok Base model: DeepScaleR-1.5B-Preview Base LAPO-D w/ Median w/ Mean w/ Minimum 85.8 86.4 86.3 85.6 85.9 3280 2365 2168 2308 2031 35.5 37.6 38.1 36.8 36.3 9246 5945 5371 6030 6080 74.2 77.6 78.3 77.4 76.7 6416 3655 3765 3658 54.6 56.1 56.3 56.6 55.0 5974 4499 4024 4164 3851 62.5 64.4 64.8 64.1 63.5 6229 4116 3832 4040 3821 5.2 ABLATION STUDY ON THE LENGTH GUIDANCE FOR LAPO-I To investigate the effectiveness of the in-context length guidance, we conduct an ablation study on the prefill content used in LAPO-I. We compare our default method, Exact Length Guidance (which provides precise target n), against two variants: Range-based Guidance (providing target range) and Implicit Guidance (no length information, relying only on the reward). As shown in Table 2, explicit and precise guidance is crucial for performance. Our default method, LAPO-I, achieves the highest average accuracy (64.8%). However, switching to range-based guidance results in 0.7% accuracy drop (to 64.1%), and removing the guidance entirely causes further drop to 63.6%. This experimental results directly validate the effectiveness of the problemto-length mapping learned in LAPO-D; the more precisely the model is guided to follow these discovered patterns, the better it performs. Notably, the Implicit Guidance variant not only suffers from lower accuracy but also sees its token consumption revert to higher 4,088 tokens, nearly identical to the LAPO-D baseline. This demonstrates that the reward signal alone is insufficient to internalize efficient reasoning behaviors. The explicit, in-context guidance acts as critical bridge, enabling the model to access and leverage the reasoning templates discovered in LAPO-D. 5.3 ABLATION STUDY ON THE STATISTICAL METRICS FOR TARGET LENGTH SELECTION The choice of statistical measure to derive the target length from the distribution of successful solutions is critical. We conduct an ablation study comparing three strategies for this selection: using the median (our default), the mean, and the minimum length. As shown in Table 3, the median proves to be the most effective choice, achieving the best balance between accuracy and efficiency. Using the median as the target yields the highest average accuracy (64.8%) with an efficient token count of 3,832. This validates our hypothesis that the median, due to its robustness to outliers, provides the most representative signal of typically effective reasoning depth. In contrast, the mean is susceptible to few excessively long, successful solutions, leading it to set overly generous budgets, resulting in higher token usage (4,040) and slightly lower accuracy."
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Reasoning length allocation across mathematical problem difficulty levels. LAPO learns to scale computation with complexity. Figure 4: Keyword usage of reasoning behaviors across different stages. LAPO selectively prunes hesitant and exploratory thought patterns. The minimum, while achieving the most aggressive compression (3,821 tokens), suffers significant accuracy drop (to 63.5%), suggesting it promotes an over-shortening strategy that discards necessary reasoning steps. These findings underscore the importance of robust statistical measures for learning well-calibrated reasoning-efficiency trade-off. 5.4 DIFFICULTY-AWARE COMPUTATIONAL ALLOCATION To understand the mechanisms behind LAPOs efficiency gains, we first examine its macro-level capability to allocate computational resources in proportion to problem complexity. We investigate LAPOs capacity for difficulty-aware reasoning length allocation by conducting systematic evaluations across mathematical reasoning benchmarks with inherent difficulty gradients: MATH Level 1-5 and AIME 2024. As shown in Figure 3, our experiments reveal consistent positive correlation between problem complexity and reasoning length across all model variants. This data demonstrates LAPOs fundamental capacity to proportionally scale computational investment with task demands. Specifically, both LAPO-D and LAPO-I exhibit near-linear progression in average reasoning length from MATH Level 1 (approximately 1,000-1,200 tokens) to AIME 2024 (5,0008,500 tokens), validating the proposed LAPO frameworks emergent capability in intrinsic difficulty awareness. The consistent scaling behaviors across different base models (DeepSeek-R1-1.5B and DeepScaleR-1.5B-Pre) and training stages underscore that LAPO develops robust, generalizable heuristics for computational resource management rather than model-specific optimizations. 5.5 QUALITATIVE REFINEMENT OF REASONING BEHAVIORS Beyond quantitatively adjusting how much computation the model uses, we next investigate how it qualitatively refines the reasoning process to achieve this efficiency. To do this, we analyzed the frequency of keywords indicative of different cognitive behaviors, such as Self-Correction and Verification, Exploration and Alternatives, Context Setting, and Conclusion Drawing, in the generated responses (Figure 4). The results reveal significant shift in the models reasoning style. The most significant change is dramatic reduction in keywords associated with Self-Correction and Exploration. The base model (DeepScaleR-1.5B-Pre) frequently employs these terms, indicating verbose and deliberative reasoning style. After LAPO-D training, and further refined in LAPOI, the model significantly curtails this internal monologue. This suggests that LAPO effectively discourages redundant verification loops and inefficient exploration of the solution space. Crucially, this efficiency gain is not achieved by indiscriminately shortening the response. The frequency of keywords related to Context Setting and Conclusion Drawing remains stable across training stages. This demonstrates that LAPO selectively prunes inefficient and hesitant thought patterns while preserving the essential scaffolding of coherent logical argument. The model learns to maintain its ability to properly frame problem and articulate its deductions."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we introduce Length-Adaptive Policy Optimization (LAPO), two-stage reinforcement learning framework that enables language models to autonomously adjust reasoning length based on problem complexity. Unlike existing approaches that impose uniform constraints, LAPO recognizes that efficient reasoning requires understanding problem-specific computational needs rather than following rigid rules. Our two-stage design enables natural progression: models first learn what constitutes appropriate reasoning depth through experience, then develop the ability to anticipate these requirements proactively. This approach mirrors how human experts develop intuition about problem complexity, allocating mental effort proportionally to task demands. Extensive experiments validate LAPOs effectiveness, achieving remarkable efficiency gains with up to 40.9% reduction in token usage while simultaneously improving accuracy by 2.3% on mathematical reasoning benchmarks. Our analysis reveals that this improvement stems from the models ability to distinguish between problems requiring elaborate derivations versus those needing only brief calculations. These results confirm that when models learn from their own successful patterns rather than arbitrary constraints, they develop more robust and efficient reasoning strategies."
        },
        {
            "title": "REFERENCES",
            "content": "Pranjal Aggarwal and Sean Welleck. L1: Controlling how long reasoning model thinks with reinforcement learning. arXiv preprint arXiv:2503.04697, 2025. Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187, 2024. Yu-Neng Chuang, Helen Zhou, Prathusha Kameswara Sarma, Parikshit Gopalan, John Boccio, Sara Bolouki, and Xia Hu. Learning to route llms with confidence tokens. arXiv preprint arXiv:2410.13284, 2024. Gongfan Fang, Xinyin Ma, and Xinchao Wang. Thinkless: Llm learns when to think. arXiv preprint arXiv:2505.13379, 2025. Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. arXiv preprint arXiv:2503.01307, 2025. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, et al. Omni-math: universal olympiad level mathematic benchmark for large language models. arXiv preprint arXiv:2410.07985, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyu Zhao, Shiqing Ma, and Zhenyu Chen. Tokenbudget-aware llm reasoning. arXiv preprint arXiv:2412.18547, 2024. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and arXiv preprint Jacob Steinhardt. Measuring massive multitask language understanding. arXiv:2009.03300, 2020. Bairu Hou, Yang Zhang, Jiabao Ji, Yujian Liu, Kaizhi Qian, Jacob Andreas, and Shiyu Chang. Thinkprune: Pruning long chain-of-thought of llms via reinforcement learning. arXiv preprint arXiv:2504.01296, 2025."
        },
        {
            "title": "Preprint",
            "content": "Chengyu Huang, Zhengxin Zhang, and Claire Cardie. Hapo: Training language models to reason concisely via history-aware policy optimization. arXiv preprint arXiv:2505.11225, 2025. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Yu Kang, Xianghui Sun, Liangyu Chen, and Wei Zou. C3ot: Generating shorter chain-of-thought In Proceedings of the AAAI Conference on Artificial without compromising effectiveness. Intelligence, volume 39, pp. 2431224320, 2025. Abhinav Kumar, Jaechul Roh, Ali Naseh, Marzena Karpinska, Mohit Iyyer, Amir Houmansadr, and Eugene Bagdasarian. Overthink: Slowdown attacks on reasoning llms. arXiv preprint arXiv:2502.02542, 2025. Chenwei Lou, Zewei Sun, Xinnian Liang, Meng Qu, Wei Shen, Wenqi Wang, Yuntao Li, Qingping Yang, and Shuangzhi Wu. Adacot: Pareto-optimal adaptive chain-of-thought triggering via reinforcement learning. arXiv preprint arXiv:2505.11896, 2025. Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and Dacheng Tao. O1-pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning. arXiv preprint arXiv:2501.12570, 2025a. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Li Erran Li, et al. Deepscaler: Surpassing o1-preview with 1.5 model by scaling rl. Notion Blog, 2025b. Xinyin Ma, Guangnian Wan, Runpeng Yu, Gongfan Fang, and Xinchao Wang. Cot-valve: Lengthcompressible chain-of-thought tuning. arXiv preprint arXiv:2502.09601, 2025. Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, et al. Imitate, explore, and self-improve: reproduction report on slow-thinking reasoning systems. arXiv preprint arXiv:2412.09413, 2024. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. Isaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph Gonzalez, Waleed Kadous, and Ion Stoica. Routellm: Learning to route llms with preference data. arXiv preprint arXiv:2406.18665, 2024. Ziqing Qiao, Yongheng Deng, Jiali Zeng, Dong Wang, Lai Wei, Fandong Meng, Jie Zhou, Ju Ren, and Yaoxue Zhang. Concise: Confidence-guided compression in step-by-step efficient reasoning. arXiv preprint arXiv:2505.04881, 2025. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, et al. Stop overthinking: survey on efficient reasoning for large language models. arXiv preprint arXiv:2503.16419, 2025. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Yue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Linfeng Song, Dian Yu, Juntao Li, Zhuosheng Zhang, et al. Thoughts are all over the place: On the underthinking of o1-like llms. arXiv preprint arXiv:2501.18585, 2025. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022."
        },
        {
            "title": "Preprint",
            "content": "Sean Welleck, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf, Alex Xie, Graham Neubig, Ilia Kulikov, and Zaid Harchaoui. From decoding to meta-generation: Inference-time algorithms for large language models. arXiv preprint arXiv:2406.16838, 2024. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models. arXiv preprint arXiv:2408.00724, 2024. Heming Xia, Chak Tou Leong, Wenjie Wang, Yongqi Li, and Wenjie Li. Tokenskip: Controllable chain-of-thought compression in llms. arXiv preprint arXiv:2502.12067, 2025. Silei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He. Chain of draft: Thinking faster by writing less. arXiv preprint arXiv:2502.18600, 2025a. Yuhui Xu, Hanze Dong, Lei Wang, Doyen Sahoo, Junnan Li, and Caiming Xiong. Scalable chain of thoughts via elastic reasoning. arXiv preprint arXiv:2505.05315, 2025b. Junjie Yang, Ke Lin, and Xing Yu. Think when you need: Self-adaptive chain-of-thought learning. arXiv preprint arXiv:2504.03234, 2025. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chainof-thought reasoning in llms. arXiv preprint arXiv:2502.03373, 2025. Jiajie Zhang, Nianyi Lin, Lei Hou, Ling Feng, and Juanzi Li. Adaptthink: Reasoning models can learn when to think. arXiv preprint arXiv:2505.13417, 2025."
        },
        {
            "title": "A TECHNICAL APPENDICES AND SUPPLEMENTARY MATERIAL",
            "content": "A."
        },
        {
            "title": "IMPLEMENTATION DETAILS",
            "content": "System prompt used for training. The system prompts used for the two-stage training are shown in the boxes below. The prompt titled LAPO-D-prompt was used for DeepSeek-R1-Distill-Qwen1.5B, and LAPO-I-prompt was used for DeepScaleR. This approach maintains consistency with the original RL training of DeepSeek-R1. LAPO-D-prompt You are helpful assistant. conversation between User and Assistant. The user asks question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process is enclosed within <think> and </think> tags, respectively, i.e., <think> reasoning process here </think> answer here. User: {question} Please think step by step and output the final answer within boxed{}. Assistant: <think> LAPO-I-prompt You are helpful assistant. conversation between User and Assistant. The user asks question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process is enclosed within <think> and </think> tags, respectively, i.e., <think> reasoning process here </think> answer here. User: {question} Please think step by step and output the final answer within boxed{}. Assistant: <think> will answer the question with {length} tokens. Training and Reproduction Details. We trained the model on the OpenRLHF framework. During training, we sampled 8 responses for each query in the batch with temperature of 1.0, set the kl parameter to 0.0001, used learning rate of 1e-6 and batch size of 128, and set the maximum context length to 4K tokens during training. Both LAPO-D and LAPO-I training were conducted for 3 episodes, approximately 240 steps. The α and β parameters in R1 and R2 were 0.7 and 0.8, respectively. All experiments were conducted using 4 A800 GPUs. We provide training hyperparameters in Table 4. Table 4: Training Hyperparameters Hyperparameter Epochs Episodes Learning Rate Train Batch Size Temperature Rollout per Prompt Prompt Max Length Generation Max Length KL Coefficient Precision α β Value 1 3 1e-6 128 1.0 8 1024 4096 0.0001 BF16 0.7 0.8 A.2 TRAINING DYNAMICS We analyze the training dynamics by periodically evaluating model checkpoints on the MATH-500 validation set to understand the learning mechanisms of our two-stage framework. As illustrated in Figures 5a and 5b, LAPO achieves superior balance between efficiency and accuracy across both training stages."
        },
        {
            "title": "Preprint",
            "content": "(a) Average Length on MATH-500 (b) Accuracy on MATH-500 Figure 5: Training dynamics evaluated on the MATH-500 validation set. Checkpoints were saved periodically during training on our mixed dataset. (a) Both LAPO-D and LAPO-I policies learn to significantly reduce the average response length. (b) These efficiency gains are achieved while maintaining or even improving accuracy over the baseline. Continuous Efficiency Gains. Figure 5a shows clear, two-step reduction in token generation. In Stage 1, the LAPO-D policy rapidly becomes more concise, with its average length decreasing from verbose baseline of 3,280 tokens to stable 2,365 tokens, driven by the length-aware reward (R1). Building on this, the LAPO-I policy achieves further compression, reducing the length to below 2,200 tokens. This demonstrates that the plan-adherence reward (R2), combined with incontext guidance, effectively encourages the model to execute its self-proposed reasoning plans more precisely. Accuracy Maintenance and Refinement. Crucially, these efficiency gains do not compromise performance. As shown in Figure 5b, accuracy on MATH-500 is consistently maintained or improved. The LAPO-D policys accuracy climbs from 85.8% to over 86.4%, suggesting the reward mechanism prunes redundant or error-prone reasoning steps. The LAPO-I policy sustains this high accuracy level even on much tighter token budget. Notably, it exhibits transient performance peak, key finding that suggests the in-context guidance actively steers the model toward more focused and effective reasoning, rather than merely acting as constraint. In summary, the training dynamics validate our two-stage design. LAPO-D establishes robust foundation for efficient reasoning, which LAPO-I then refines to achieve superior performancecost balance. The smooth convergence on challenging validation set confirms that by learning from its own successful patterns, the model develops transferable and efficient reasoning strategies. A.3 SELECTION OF TRAINING DATASET As mentioned in section 4 Experiment Setup, we chose mixed dataset for training in our In this section, we provide detailed analysis of the impact of different dataset experiments. selections on model performance. Table 5 shows the test results on various benchmarks after two-stage training using different training datasets. Several important findings can be observed from the experimental results. Combined-data achieved the best performance in terms of average accuracy, showing clear advantage over single-dataset training. This indicates that dataset with balanced difficulty distribution helps enhance the models generalization ability across different types of questions. In terms of token usage efficiency, the model trained on combined-data also performed the best. This suggests that problems with different difficulty gradients help establish more accurate complexity-length mapping relationship. By exposing the model to wider range of problem difficulties, it can better learn the optimal thinking range for different questions. Taking all these factors into consideration, we selected the mixed dataset as the training data to expose the model to more diverse set of problems and enable it to deeply learn the optimal reasoning patterns for different questions."
        },
        {
            "title": "Preprint",
            "content": "Table 5: Ablation study on the training dataset. This table compares performance when trained on different data sources. For each metric column, bold indicates the best score and underline indicates the second-best score across all configurations. Method MATH500 AIME2024 AMC-23 OlympiadBench Average Pass@1 #Tok Pass@1 #Tok Pass@1 #Tok Pass@1 #Tok Pass@1 #Tok Training Data: Combined (Ours) LAPO-D LAPO-I 86.4 86.3 2365 2168 37.6 38.1 5945 77.6 78.3 3655 3765 56.1 56.3 4499 4024 64.4 64.8 4116 Training Data: DeepScaleR-only LAPO-D LAPO-I 86.1 86.1 2397 2210 36.8 36.5 6153 76.8 77.0 3983 3791 55.5 55.6 4258 3933 63.8 63.8 4197 Training Data: MATH-only LAPO-D LAPO-I 86.5 86.1 2398 2340 38.0 35.5 7034 77.3 75.8 4060 4021 55.8 54.5 4494 4194 64.4 63.0"
        }
    ],
    "affiliations": [
        "Zhejiang University"
    ]
}