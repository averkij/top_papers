{
    "paper_title": "Light of Normals: Unified Feature Representation for Universal Photometric Stereo",
    "authors": [
        "Hong Li",
        "Houyuan Chen",
        "Chongjie Ye",
        "Zhaoxi Chen",
        "Bohan Li",
        "Shaocong Xu",
        "Xianda Guo",
        "Xuhui Liu",
        "Yikai Wang",
        "Baochang Zhang",
        "Satoshi Ikehata",
        "Boxin Shi",
        "Anyi Rao",
        "Hao Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Universal photometric stereo (PS) aims to recover high-quality surface normals from objects under arbitrary lighting conditions without relying on specific illumination models. Despite recent advances such as SDM-UniPS and Uni MS-PS, two fundamental challenges persist: 1) the deep coupling between varying illumination and surface normal features, where ambiguity in observed intensity makes it difficult to determine whether brightness variations stem from lighting changes or surface orientation; and 2) the preservation of high-frequency geometric details in complex surfaces, where intricate geometries create self-shadowing, inter-reflections, and subtle normal variations that conventional feature processing operations struggle to capture accurately."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 2 8 8 8 1 . 6 0 5 2 : r Light of Normals: Unified Feature Representation for Universal Photometric Stereo Hong Li1,2 Houyuan Chen1,3 Chongjie Ye1,4 Zhaoxi Chen1 Bohan Li1 Baochang Zhang2 2BUAA 3NJU 4FNii, CUHKSZ 1BAAI Shaocong Xu1 Xianda Guo1 Xuhui Liu2 Yikai Wang5 Satoshi Ikehata6 Boxin Shi8 Anyi Rao7 Hao Zhao1,9 5BNU 6NII 7HKUST 8PKU 9AIR, THU Figure 1: (Left) Given multiple images from the same viewpoint under varying lighting, our method outperforms existing universal photometric stereo methods in accuracy and detail, even exceeding the professional 3D scanner. (Right) On the DiliGenT benchmark, with similar decoders, higher feature consistency (SSIM and CSIM) in encoder features corresponds to better accuracy, which is measured by the percentage of pixels with angular error under 9.25."
        },
        {
            "title": "Abstract",
            "content": "Universal photometric stereo (PS) aims to recover high-quality surface normals from objects under arbitrary lighting conditions without relying on specific illumination models. Despite recent advances such as SDM-UniPS and Uni MS-PS, two fundamental challenges persist: 1) the deep coupling between varying illumination and surface normal features, where ambiguity in observed intensity makes it difficult to determine whether brightness variations stem from lighting changes or surface orientation; and 2) the preservation of high-frequency geometric details in complex surfaces, where intricate geometries create self-shadowing, inter-reflections, and subtle normal variations that conventional feature processing operations struggle to capture accurately. To address these challenges, we propose LINO-UniPS, featuring three key innovations: (1) learnable light register tokens with global cross-image attention that effectively decouple lighting from normal features; (2) wavelet transform-based sampling that preserves highfrequency surface details; and (3) normal-gradient confidence loss that prioritizes accurate reconstruction in regions with high normal variations. Additionally, we introduce PS-Verse, synthetic dataset with graduated geometric complexity and diverse lighting conditions to progressively train models on increasingly challenging scenarios. Extensive experiments demonstrate that our method achieves state-of- *Equal contribution. Part of project lead. Corresponding author. the-art performance on public benchmarks while exhibiting strong generalization capabilities across various material properties and lighting scenarios."
        },
        {
            "title": "Introduction",
            "content": "Photometric stereo (PS) [55] aims to recover surface normals from multiple images under varying lighting conditions. Traditional methods [29, 30, 24, 40, 19] rely on assumptions of specific light sources or physical models, making them difficult to adapt to complex natural scenes. Until recently, to overcome the dependence of traditional methods on physical lighting models, the Universal PS approaches [27, 31, 20] employ an encoder-decoder architecture. The encoder extracts high-dimensional representations of pixel-wise, spatially varying illumination from multiple images taken under different lighting conditions, thereby producing so-called global lighting context that replaces traditional physical parameter modeling. Subsequently, the decoder, functioning as calibration network [26], leverages this context to estimate surface normals, achieving robust normal recovery in complex and spatially non-uniform lighting environments, thereby significantly advancing the field. From the design perspective of previous Universal PS methods [31, 27], the features extracted by the encoder are expected to vary significantly with changes in illumination distribution, and the encoder typically processes larger capacity than the decoder. However, we observe that as the similarity of the lighting context features generated by different encoders increases, the performance of normal estimation also correspondingly improves, as shown in Fig. 1. This indicates that the encoder effectively couples lighting and normal features within the global lighting context, while the decoder is tasked with disentangling the invariant normal features from this varying context, which imposes an increased learning burden on the encoder. Therefore, the encoder should possess the capability to attenuate the coupling between illumination variations and intrinsic surface normal attributes, extracting ideal normal features that are consistent across images and rich in detail from diverse lighting contexts to ensure robust and high-quality normal recovery. The synergy of these two factors ultimately determines the effectiveness of Universal PS in estimating surface normal under complex illumination. However, Universal PS methods still face significant challenges in encoding such ideal normal features. First, most current methods rely on single image frames and patch-position-aligned attention mechanisms, hindering the full integration of global lighting information from multiple images, resulting in instability and inconsistency in normal feature estimation. Second, commonly used upsampling and downsampling operations (such as bilinear interpolation [28] and pixel shuffling [31]), while computationally efficient, tend to degrade high-frequency details, compromising the fine recovery of normals. Finally, although existing synthetic training datasets cover various types of light sources, they fail to adequately simulate the impact of complex geometry on the spatial distribution of illumination, limiting the models generalization ability. To address the above challenges, we propose LINO-UniPS, general photometric stereo method based on the ViT [42, 13] architecture. Our approach specifically targets the two fundamental challenges in Universal PS: the deep coupling between illumination and normal features, and the preservation of high-frequency geometric details. Inspired by [9, 53], we use learnable light register tokens to store global illumination (see Fig. 3), effectively alleviating the coupling between lighting and normal features. Combined with global cross-image attention mechanism, the encoder can generate more consistent and discriminative normal features. By reducing ambiguity in the observed intensity, the model can more effectively distinguish between variations in lighting and actual changes in surface orientation. Furthermore, we use wavelet transform [14, 10] and integrate normal gradient perception loss along with enhanced feature resolution to ensure rich detail in the normal features, thereby improving the recovery of high-quality surface normals.This ensures the preservation of high-frequency components, which are essential for capturing intricate geometries. Moreover, we propose dataset graded by surface complexity, significantly improving the models generalization ability to real-world complex lighting scenarios. Extensive experiments on synthetic and real datasets demonstrate that our method outperforms state-of-the-art techniques in both accuracy and robustness. Overall, our contributions can be summarized as the following three points: Figure 2: Overview of the LiNo-UniPS architecture, featuring Light-Normal Contextual Encoder, Decoder, and loss computation. The encoder includes light registered wavelet-aware downsampler, an enhanced LightNormal Contextual Attention Module, DPT-Based [45] Fusion Module, and WaveUpSampler, which together encode and fuse pixel and wavelet domain features for normal-consistent outputs. The Decoder is similar to SDM-UniPS [31]. 1. We use learnable light register tokens for flexible decoupling of illumination and normal features, combined with global cross-image attention mechanism to enhance multi-view lighting representation and normal feature consistency. 2. We adopt wavelet transform for up/downsampling, combined with normal-gradient perception loss and point-wise decoder, significantly improving fine normal detail recovery. 3. We create synthetic dataset with graded normal complexity to enrich training, and experiments confirm our methods superior performance and generalization."
        },
        {
            "title": "2 Related Work",
            "content": "Calibrated Photometric Stereo: Calibrated PS [55] deduces surface normals by assuming meticulously pre-calibrated illumination, often relying on precise light source parameters. This stringent requirement Initial methods successfully addressed Lambertian surfaces under such known lighting [55]. Subsequent research extended capabilities to handle complex non-Lambertian reflectance, including varied BRDFs and specular highlights [18, 29, 3, 16, 17], albeit always presupposing perfectly known illumination conditions. More recently, deep learning techniques [47, 4, 33, 32] have significantly enhanced the ability to model intricate materials within this calibrated framework. However, the reliance of Calibrated PS on precise light source parameters severely restricts its applications to controlled laboratory environments, thereby limiting its real-world utility. Uncalibrated Photometric Stereo: To alleviate photometric stereos dependence on pre-calibrated light sources, the task of Uncalibrated Photometric Stereo (Uncalibrated PS) was introduced. Uncalibrated PS methods [21, 2, 38, 6, 50, 34, 36] commonly adopt two-stage pipeline: an initial stage where network module estimates the unknown lighting parameters, followed by second stage where these estimates are utilized as known inputs, like conventional Calibrated PS framework. While such Uncalibrated PS techniques have demonstrated success in handling more complex scenarios [36], their application under unconstrained natural or ambient illumination, encounters 3 significant challenges. These challenges primarily stem from the inherent difficulty in accurately modeling the physics of such arbitrary and often intricate lighting environments. Universal Photometric Stereo: The Universal PS task, as recently formulated by Ikehata in [28], leverages purely data-driven approach to solve the photometric stereo problem. This methodology enables operation under arbitrary and unknown lighting environments, critically obviating the need for complex predefined assumptions regarding the illumination. Building on this foundation, methods such as SDM-UniPS [31] further advanced Universal PS by employing an encoder to extract features from multi-illumination images; these features are then fused to create global lighting context, which subsequent decoder utilizes in conjunction with pixel-level information to estimate surface normals [20] has explored multi-scale strategies to address the Universal PS problem; however, such approaches often fall short of reconstructing surface normals that are simultaneously detailed and accurate. We posit that the essence of PS fundamentally involves resolving consistent surface normals from multiple images captured under diverse lighting conditions. Our analysis of prior methods like UniPS and SDM-UniPS indicates that an encoders ability to extract features with greater consistency significantly facilitates the decoders task of producing higher-fidelity normal maps. Motivated by this observation, we propose our method, LiNO-UniPS, which is designed to learn superior Unified Feature Representation for Universal Photometric Stereo."
        },
        {
            "title": "3 Method",
            "content": "The core task of Universal PS is to accurately recover the surface normal map RHW 3 of =1, where If RHW 3, captured under single an object or scene from set of images {If }F viewpoint but multiple unknown lighting conditions [28]. This task faces two key challenges: unknown and spatially varying lighting conditions that are deeply entangled with normal features and difficult to disentangle; and non-Lambertian reflectance from complex surface geometry and materials, making accurate perception difficult. We address these challenges from both methodological and data perspectives. On the methodological side, we propose LINO-UniPS (see Sec. 3.1), whose architecture is shown in Fig. 2. It introduces light registers and global attention mechanism to decouple lighting from normal features during the encoder stage, as well as wavelet transform and normal gradient-aware loss to enhance the models sensitivity to high-frequency details. On the data side, we present the large-scale graded PS-Verse dataset (see Sec. 3.2), which for the first time introduces normal maps to enhance geometric complexity and lighting variation, thereby enriching the diversity of training data. 3.1 LINO-UniPS Network Architecture Overview: Given the multi-light images as input to the Light-Normal Contextual Encoder (illustrated in the top pipeline of Fig. 2), they first undergo Light Registered Wavelet-aware DownSampler (leftmost block in the encoder in Fig. 2), which simultaneously generates bilinearly downsampled global components as well as low-frequency and high-frequency wavelet decompositions [10, 14]. Subsequently, the global components and wavelet components are independently converted into patch-based token sequences. These token sequences are prepended with additional light register tokens to aggregate lighting information before being fed into ViT-based backbone, which is initialized with DINOv2 [42, 13], to extract independent visual representations for each image. Next, the visual representations are processed by four cascaded interleaved attention blocks(central block in the encoder in Fig. 2). Feature tokens, excluding the light register tokens, are then subjected to multi-level aggregation(block before WaveUpSampler in Fig. 2). Using WaveUpSampler(rightmost block in Fig. 2), features from the wavelet domain and pixel domain are fused to produce what we term the Light-Normal Contextual representation. Finally, this Light-Normal Contextual representation is decoded by decoder to predict the surface normal map. To maintain clarity, we emphasize the discussion of high-level concepts instead of detailed explanations. full description of the detailed implementation can be found in the appendix. Strengthening Lighting-Normal Decoupling: Previous universal photometric stereo methods [28, 31] encode spatially varying global lighting context per pixel from multi-illuminated images and pass it as physical lighting parameters to self-calibrating decoder for normal prediction. Usually, the encoder is far more powerful than the decoder. However, as observed in Fig. 4, this approach can lead to the encoder deeply coupling lighting and normal features, putting too much pressure on the decoder Figure 3: We visualize the attention maps of lighting registers on the encoders final-layer feature maps for both real (left) and synthetic (right) data. The first and second rows correspond to HDRI and point light registers, respectively, showing attention regions that rotate from left to right, demonstrating the models ability to capture light from different directions. The HDRI register exhibits broader attention regions, while the point light register focuses more sharply, consistent with the low-frequency nature of HDRI and the high-frequency characteristics of point lights. Figure 4: Features from different method encoders; rightmost column is variance. Figure 5: Results of object inference with masks in Luces and DiLiGenT datasets. to disentangle them. This motivates our approach to decouple lighting and normal features within the encoder, allowing the decoder to focus solely on estimating normals from consistent normal features, thus easing the learning process.The success of this decoupling is quantitatively shown by improved CSIM and SSIM scores in Tab. 2 and Tab. 3. Specifically, our training process consists of three key components: Light Registers Tokens, Interleaved Attention, and the Light Aligner. Inspired by the register mechanism in DINOv2 [42], we designed Light Registers to collect global information during model training, which can be discarded during inference. Through supervised learning, the encoders output is guided to encode features consistent with surface normals, achieving an initial decoupling of lighting and normal features. In contrast, UniPS and SDM-UniPS only use Frame Attention and Light Axis Attention to enable local information flow and global lighting context communication, which is insufficiently efficient to disentangle the complex coupling of lighting variations and normal features. Inspired by VGGT [53], we introduce global attention mechanism that attends to all input image tokens, facilitating efficient interaction between global lighting context and normal features, thus achieving more effective information decoupling. Additionally, drawing on the feature alignment methods used in VAVAE [59] and REPA [60] to accelerate generative model training, we encode three of the light registers along with information about the types of light sources in the training datasetincluding the HDRI map, as well as the intensities, sizes, and positions of area lights and point lights (see Fig. 2)and constrain each of them separately with cosine similarity loss. We summarize the form of the loss functions as follows: Llight = λ1Lhdri + λ2Lpoint + λ3Larea Where λ1, λ2, and λ3 are hyperparameters. Through the combined effect of these three components, we significantly enhance the decoupling of lighting and normal features; detailed results are shown in Fig. 4 and Tab. 3. Extracting Fine-grained Contexts: Current feature extraction techniques often discard highfrequency surface details, which are vital for accurate normal reconstruction. Especially in complex surface regions, the precision and detail of predicted surface normals improve significantly when the encoder can capture finer-grained contextual information. Although prior methods have recognized this challenge, they have not effectively resolved it. UniPS [28] extracts features from downsampled images, inherently introducing blur. While SDM-UniPS [31] attempted to mitigate this blur through 5 Table 1: Comparison of PS-Verse with Other Photometric Stereo Datasets. The term Normal indicates whether the normal map is used to enhance surface details during data rendering. Light denotes the presence of light type annotations, while Complexity quantifies the average magnitude of surface normal gradients within the dataset, where higher values signify greater geometric complexity. Dataset Intrinsic Normal Light Complexity # Shapes # HDRI # Scenes CyclesPS-Train [25] PS-Wild [28] PS-Mix [31] PS-Uni MS-PS [20] PS-Verse 4.9 3.5 11.5 8.6 26.7 15 410 410 11,000 17,805 0 31 31 1100 2423 45 10,099 34,927 100,000 100,000 \"split-and-merge\" operation, it disrupted the original high-frequency semantics [41]. To address these shortcomings and overcome these limitations, enabling our encoder to effectively capture and preserve fine-grained details, we introduce the wavelet transform and normal gradient perception loss. To enable the encoder to extract finer-grained contexts, we replace the original downsampling operation with discrete wavelet transform [10] to extract the high-frequency and low-frequency components of multi-light images, thereby mitigating information loss during downsampling. At the same time, to retain global image-domain semantic information, similar to SDM-UniPS, we also preserve bilinear downsampling. Refer to Fig. 2 for the light-registered wavelet-aware downsampler. Additionally, during feature upsampling, we use discrete wavelet inverse transform in the wavelet upsampler to upsample features in the wavelet domain. To overcome the limitations of existing methods in handling complex geometric and richly textured regions, we introduce Normal Gradient Perception Loss. This loss guides the network by using the predicted normal gradient to generate confidence map that weights the normal prediction error, thereby emphasizing high-frequency detail areas. The combined loss is formulated as: LN = λ4 (cid:88) (N )2 + λ5 (cid:88) ( G)2, where the confidence map = is derived from the predicted normal gradient G, = is the ground truth gradient, and λ4, λ5 are weighting coefficients balancing the normal prediction and gradient supervision terms. This design effectively enhances the networks sensitivity to fine surface details in challenging regions. 3.2 PS-Verse Dataset [27] pioneers the design of large-scale synthetic dataset, PS-Wild, for Universal PS tasks. It utilizes single-material 3D models from the commercial AdobeStock collection [1], rendering over 10,000 scenes augmented through rotation and color variations. However, the lighting setup is limited to HDRI environment maps, lacking high-frequency point light sources that provide richer illumination effects. Building on this dataset, [31] extends the lighting configuration by combining area lights, environment lights, and point lights to better simulate natural illumination. Additionally, it replaces single objects with multi-object compositions to disrupt semantic cues and enhance model generalization. Nonetheless, these objects lack fine-grained surface normals and complex geometric structures, resulting in relatively limited lighting variations and self-shadowing effects, which restrict further improvements in model performance. Using the availability of publicly available large-scale 3D asset datasets [12, 11, 51], we propose the PS-Verse dataset, which naturally expands the training data required by Universal PS. To increase the lighting complexity in rendered scenes, we carefully select 17,805 textured 3D models with UV coordinates and PBR materials from the Objaverse dataset [12], which contains nearly 800,000 models. Following Dora [7], which quantifies geometric complexity based on the density of sharp edges, we categorize the objects into four complexity levels. We then construct scenes by recursively selecting 4 to 6 objects from each level, and classify scenes into four corresponding complexity tiers. Regarding materials, textures are randomly sampled from the MatSynth large-scale PBR material database [51], consisting of 806 metallic, 1,226 specular, and 3,321 diffuse material groups. To balance the distribution of specular and diffuse lighting effects, we assign object materials according to the ratio of 1:4:2.5:2.5 across original object textures, diffuse, specular, and metallic materials. 6 To more realistically simulate fine surface detail interactions with light, we introduce normal maps for objects for the first time in such data, defining this as fifth complexity level that injects rich high-frequency lighting details. In terms of lighting setup, we incorporate uniform environment light to simulate non-dark ambient conditions. Detailed lighting configurations are provided in the appendix. Overall, the PS-Verse dataset consists of 100,000 scenes generated using the Cycles renderer in Blender [15]. Each scene includes two rendering outputs: with and without normal maps. We render 20 16-bit images at resolution of 512x512 per scene. Specifically, for quantitative metric assessment, we chose 10 scenes from each level (Level 1 to Level 5), collectively constituting what we term our PS-Verse Testdata."
        },
        {
            "title": "4 Experiments",
            "content": "Method Table 2: Ablation results on architecture trained on PS-Mix [31]. Implementation Details: To maintain data volume comparable to that of PS-Mix [31], we limited our selection from the PS-Verse dataset to 40K scenes for training (to align with PSMix [31], which is about 35K). We start training on PS-Verse Level 1 data, adding higher levels every 10 epochs up to Level 4, for about 150 epochs without ground truth normals. Then, we fine-tune on Level 5 data with ground truth normals until 200 epochs to improve fine surface detail reconstruction. We use AdamW Optimizer with 1e4 initial learning rate, 0.05 weight decay and step decay of 0.8 every ten epochs. Input images per batch vary randomly from 3 to 6. The total loss combines multiple components: MAE DiLiGenT LUCES DiLiGenT102 SDM-UniPS 0.84 0.90 Ours CSIM SSIM 14.96 13.84 13.50 12.70 0.72 0.91 5.80 5.60 = Llight + LN Training runs on eight NVIDIA H100 GPUs for roughly seven days. Inference takes around 2 seconds for 16 input images at 512512 on H100. Evaluation Metric: We evaluates the results with three metrics. Normal accuracy is measured by Mean Angular Error (MAE), with lower values indicating better performance. Feature similarity is assessed by Cosine Similarity (CSIM) and Structural Similarity Index (SSIM) [54]: encoder-extracted features are normalized, CSIM is computed by cosine similarity, and SSIM is calculated after PCA reduces features to three dimensions. Higher CSIM and SSIM indicate better similarity. Evaluation Dataset: For quantitative evaluation, we use three public datasets: DiLiGenT [48], LUCES [37] and DiLiGenT102 [46]. We also use real-world data from UniPS [28] and SDMUniPS [31] to qualitatively assess generalization. For ablation study 4.1, we use PS-Verse Testdata 3.2. Table 3: Main ablation with 20 multi-lights input images. The evaluation metrics were measured on our PS-Verse Testdata 3.2. Method Dataset CSIM SSIM Level 1 Level 2 Level 3 Level 4 Level MAE SDM-UniPS [31] SDM-UniPS [31] PS-Mix [31] PS-Verse PS-Verse Baseline w/ tokens PS-Verse w/ global attention PS-Verse w/ align PS-Verse w/ wavelet transform PS-Verse w/ grad perception loss PS-Verse 0.61 0.68 0.71 0.74 0.80 0.86 0.85 0. 0.63 0.65 0.69 0.73 0.78 0.82 0.82 0.83 9.27 7.35 7.52 7.01 5.64 4.35 4.32 4.18 10.34 8.15 12.64 8. 13.89 9.47 8.03 8.15 6.41 5.60 5.10 4.90 8.41 8.34 7.52 5.47 4.96 4.77 9.12 8.43 7.12 6.10 5.71 5.54 14.87 11.69 10.55 8.65 6.51 5.97 5.64 5. Avg. 12.02 8.82 8.73 8.13(0.60 ) 6.44 (2.29 ) 5.58 (3.15 ) 5.15 (3.58 ) 4.84 (3.89 ) 4.1 Ablation Study To illustrate the effectiveness of our proposed method, LINO-UniPS was trained to convergence (approx. 140 epochs) on the PS-Mix dataset [31]. As presented in Tab. 2, our LINO-UniPS surpasses SDM-UniPS, also trained to convergence on PS-Mix, across both evaluated public datasets. Notably, LINO-UniPS yields higher CSIM and SSIM scores, quantitatively demonstrating that features extracted by our encoder exhibit greater similarity compared to those from SDM-UniPS [31]. It is noteworthy that our LINO-UniPS utilizes decoder 7 Figure 6: Results on the DiLiGenT102 dataset: matrix comparing performance where rows/columns correspond to shapes/materials. Values indicate Mean Angular Error (MAE) (mean/median). For enhanced detail visibility, viewing the electronic version in color is recommended. Table 4: Evaluation on LUCES[39]. Uses all 52 images unless otherwise noted (K). Method Ball Bell Bowl Buddha Bunny Cup Die Hippo House Jar Owl Queen Squirrel Tool Avg. UniPS [28] 11.01 24.12 23.84 SDM-UniPS [31] 13.30 12.76 8.44 10.29 10.51 6.79 Uni MS-PS [20] 9.09 12.00 10.09 Ours w/ mlp 8.28 10.05 8.48 Ours Ours (K=15) Ours (K=6) Ours (K=3) 10.23 7.35 8.32 10.84 7.85 8.43 11.55 9.53 10.43 27.90 18.58 12.57 16.63 14.45 14.73 16.41 19. 23.51 28.64 16.24 21.41 35.93 14.53 32.87 28.36 8.30 12.67 15.97 8.53 6.10 11.38 15.97 9.60 7.65 11.77 13.62 9.87 9.16 5.87 9.84 6.38 19.67 7.25 13.35 6.27 15.97 6.86 6.30 7.87 26.07 25.46 25.37 22.67 8.86 8.44 9.44 5.87 6.26 6.43 11.11 7.76 8.16 8. 7.01 9.24 8.83 5.91 6.39 7.01 22.78 24.34 28.79 9.78 9.93 7.02 6.38 11.02 9.96 5.44 11.72 13.80 25.36 16.01 11.37 16.57 9.81 9.70 10.49 13. 19.03 23.77 12.54 13.50 12.22 11.10 13.22 12.62 9.48 7.66 7.53 9.59 8.22 10.30 9.35 12.08 architecture identical to that of SDM-UniPS [31]. This architectural commonality strongly suggests that the observed performance improvements are primarily attributable to our encoders enhanced capability to more effectively decouple illumination from geometry. Such effective decoupling fosters stronger normal consistency within the learned features, consequently boosting the final normal reconstruction accuracy and capability. The main ablation results are presented in Tab. 3. Firstly, by comparing the performance of SDMUniPS when trained to convergence on the PSMix dataset versus our PS-Verse dataset, we observe that training on PS-Verse yields superior feature similarity scores (CSIM and SSIM) as well as better MAE for normal reconstruction. This indicates that training on our PS-Verse dataset, compared to PS-Mix, more effectively enhances the capability to reconstruct normals for fine-grained details. Secondly, we conducted ablations of the various modules within our LINOUniPS method, starting from baseline model where all our proposed enhancements are removed. Our initial finding is that by merely incorporating unsupervised light tokens, both the feature similarity metrics and the normal reconstruction results show an improvement. This suggests that these additional tokens can often capture global lighting information even without direct supervision, thereby aiding the disentanglement of normals from illumination. Progressively incorporating our global attention mechanism and lighting alignment strategy leads to more significant improvements in both feature similarity and normal reconstruction performance. This further validates our central conclusion: alleviating the coupling between lighting and normal features effectively enhances the quality of normal reconstruction. Finally, the last two rows in Tab. 3 pertain to the wavelet transform and the gradient perception loss, components we introduced for extracting fine-grained contexts. While these two modules exhibit comparatively modest contribution to the overall feature similarity metrics, their substantial performance improvements on data with complex geometries, particularly for Level 4 and Level 5 of the PS-Verse dataset, underscore their justification and effectiveness in handling intricate surface details. Figure 7: Our method outperforms other universal PS methods in challenging synthetic scenes with no masks, detailed textures, complex geometry, and high reflectivity. Table 5: Evaluation on DiLiGenT[48]. Uses all 96 images unless otherwise noted (K). Method Ball Bear Buddha Cat Cow Goblet Harvest Pot1 Pot2 Reading Avg. UniPS [28] 4.90 9.10 SDM-UniPS [31] 1.50 3.60 1.84 3.14 Uni MS-PS [20] 1.21 3.62 Ours w/ mlp 1.77 2.64 Ours Ours(K=32) Ours(K=16) Ours(K=8) Ours(K=4) 1.80 2.69 1.58 2.81 1.88 2.60 2.59 3. 19.40 7.50 6.04 7.36 6.24 6.10 6.42 6.92 7.50 13.00 11.60 24.20 8.50 4.50 5.40 3.99 6.49 3.45 6.11 4.94 4.83 5.17 4.29 3.39 3.34 3.66 3.70 5.93 4.11 4.80 5.28 5.67 5.14 5.37 5.29 6. 25.20 10.20 8.90 10.71 8.56 8.69 8.93 9.77 10.24 10.80 9.90 4.70 4.10 4.12 4.70 5.37 5.23 4.07 4.59 4.19 4.71 4.04 4.66 4.34 5.22 5.34 5.52 18.80 8.20 7.00 7.54 6.71 6.97 7.26 7.06 9. 14.70 5.80 4.97 5.69 4.74 4.77 4.95 5.21 6.17 4.2 Quantitative Results On the DiLiGenTs [48] 10 objects, our LiNO-UniPS obtains the largest number of SOTA results on individual objects, while its mean performance metrics also ranks SOTA (results can be seen in Tab. 5). On LUCES [37], dataset predominantly featuring high-frequency information, our quantitative results are presented in Tab. 4. Compared to all evaluated Universal Photometric Stereo (PS) methods, our LINO-UniPS consistently attains SOTA performance.Remarkably, for the House object, which features particularly intricate geometry, our LINO-UniPS outperforms all other compared methods. Furthermore, our approach demonstrates excellent results even with very limited number of input images (e.g., = 3). We attribute this robustness to our strategy of strengthening Lighting-Normal decoupling; this enables our method to extract features with high normal consistency even from sparse inputs, consequently leading to superior reconstruction quality. Our LINO-UniPS not only achieves the best performance among all Universal PS methods evaluated but also significantly surpasses representative Calibrated PS [44] and Uncalibrated PS approaches [5]. Specifically, for objects exhibiting difficult material properties (e.g., ACRYLIC) or complex shapes (e.g., PENTAGON), our LINO-UniPS demonstrates markedly superior performance. We also conduct comparative evaluation of existing Universal Photometric Stereo (PS) methods on PS-Verse Testdata 3.2, with the results (detailed in Fig. 7) clearly demonstrating that our approach significantly surpasses other contemporary Universal PS techniques, especially in these more demanding cases. Finally, to further underscore the efficacy of our encoder, we conducted an additional experiment. In this setup, we replaced the standard decoder in our LINO-UniPS framework with significantly simpler Multi-Layer Perceptron (MLP) and then fine-tuned this variant until convergence. Our findings were twofold: First, the consistency of the extracted features was further enhanced, as detailed in Fig. 4. Second, although the reconstruction performance saw slight degradation compared to LINO-UniPS with its original, more sophisticated decoder, this MLP-decoder variant still markedly outperformed SDM-UniPS (which utilizes complex encoder architecture), with comparative results presented in Tab. 4. These results align with our central hypothesis that the superior performance of LINO-UniPS is primarily driven by its encoders advanced capability to generate highly consistent and well-disentangled features. Figure 8: High-resolution (>2K) surface normal recovery from real captured images under varying lighting conditions. Top: Canandwood from [28], masked object; bottom: Coins and keyboard from [31], unmasked scene. 4.3 Qualitative Results Fig. 1 and Fig. 8 showcase our methods ability to reconstruct highly detailed and accurate surface normals for diverse real-world objects and scenes, highlighting its strong generalization capabilities. Additional reconstruction results on real-world scenarios can be found in appendix."
        },
        {
            "title": "5 Conclusion",
            "content": "In this study, we propose LINO-UniPS, universal photometric stereo solution. We identify the core challenge of this task as decoupling consistent normal information from spatially varying lighting features, achieving invariance amid variation. Specifically, we utilize lighting register and global attention mechanism to capture global lighting and enhance depth-context awareness across multiple images, enabling effective separation of lighting and normal features. To improve the recovery of high-frequency normal details, we replace traditional up/downsampling with wavelet transforms to preserve high-frequency information and introduce normal gradient-aware loss to boost the models sensitivity to details. Finally, we contribute complex and large-scale photometric stereo dataset, hoping to provide valuable reference for future research."
        },
        {
            "title": "References",
            "content": "[1] Adobe. Adobe stock. https://stock.adobe.com/. [2] P. Belhumeur and D. Kriegman. What is the set of images of an object under all possible lighting conditions? In Proceedings of IEEE/CVF Computer Vision and Pattern Recognition, 1996. [3] M. Chandraker, F. Kahl, and D. Kriegman. Reflections on the generalized bas-relief ambiguity. Proceedings of IEEE/CVF Computer Vision and Pattern Recognition, 2005. [4] G. Chen, K. Han, and K-Y. K. Wong. Ps-fcn: flexible learning framework for photometric stereo. Proceedings of European Conference on Computer Vision, 2018. [5] Guanying Chen, Kai Han, Boxin Shi, Yasuyuki Matsushita, and Kwan-Yee K. Wong. Sdps-net: Selfcalibrating deep photometric stereo networks. In Proceedings of IEEE/CVF Computer Vision and Pattern Recognition, 2019. [6] Guanying Chen, Michael Waechter, Boxin Shi, Kwan-Yee Wong, and Yasuyuki Matsushita. What is learned in deep uncalibrated photometric stereo? In Proceedings of European Conference on Computer Vision, 2020. [7] Rui Chen, Jianfeng Zhang, Yixun Liang, Guan Luo, Weiyu Li, Jiarui Liu, Xiu Li, Xiaoxiao Long, Jiashi Feng, and Ping Tan. Dora: Sampling and benchmarking for 3d shape variational auto-encoders. In Proceedings of IEEE/CVF Computer Vision and Pattern Recognition, 2024. [8] Pointcept Contributors. Pointcept: codebase for point cloud perception research. https://github. com/Pointcept/Pointcept, 2023. [9] Timothée Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers. In International Conference on Learning Representations, 2024. [10] I. Daubechies. The wavelet transform, time-frequency localization and signal analysis. IEEE Transactions on Information Theory, 36(5):9611005, 1990. [11] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli VanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia Gkioxari, Kiana Ehsani, Ludwig Schmidt, and Ali Farhadi. Objaverse-xl: universe of 10m+ 3d objects. arXiv:2307.05663, 2023. [12] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. arXiv:2212.08051, 2022. [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021. [14] Shahaf Finder, Roy Amoyal, Eran Treister, and Oren Freifeld. Wavelet convolutions for large receptive fields. In European Conference on Computer Vision, 2024. [15] Blender Foundation. Blender. https://www.blender.org/. [16] D. Goldman, B. Curless, A. Hertzmann, and S. Seitz. Shape and spatially-varying brdfs from photometric stereo. In Proceedings of IEEE/CVF International Conference on Computer Vision, October 2005. [17] D. B. Goldman, B. Curless, A. Hertzmann, and S. M. Seitz. Shape and spatially-varying brdfs from photometric stereo. IEEE Trans. Pattern Anal. Mach. Intell., 32(6):10601071, 2010. [18] M. Grossberg and S. Nayar. Determining the camera response from images: What is knowable? IEEE Trans. Pattern Anal. Mach. Intell., 25(11):14551467, 2003. [19] Bjoern Haefner, Zhenzhang Ye, Maolin Gao, Tao Wu, Yvain Quéau, and Daniel Cremers. Variational uncalibrated photometric stereo under general lighting. In Proceedings of IEEE/CVF Computer Vision and Pattern Recognition, 2019. [20] Clément Hardy, Yvain Quéau, and David Tschumperlé. Uni ms-ps: multi-scale encoder-decoder transformer for universal photometric stereo. Computer Vision and Image Understanding, 248:104093, 2024. [21] H. Hayakawa. Photometric stereo under light souce with arbitary motion. JOSA, 11(11):30793089, 1994. [22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. arXiv preprint arXiv:1512.03385, 2015. [23] Huaibo Huang, Ran He, Zhenan Sun, and Tieniu Tan. Wavelet-srnet: wavelet-based cnn for multi-scale face super resolution. In Proceedings of IEEE/CVF International Conference on Computer Vision, 2017. [24] S. Ikehata. Cnn-ps: Cnn-based photometric stereo for general non-convex surfaces. In Proceedings of European Conference on Computer Vision, 2018. [25] S. Ikehata. Cnn-ps: Cnn-based photometric stereo for general non-convex surfaces. In Proceedings of European Conference on Computer Vision, 2018. [26] S. Ikehata. Ps-transformer: Learning sparse photometric stereo network using self-attention mechanism. In Proceedings of British Machine Vision Conference, 2021. [27] S. Ikehata. Universal photometric stereo network using global lighting contexts. In Proceedings of IEEE/CVF Computer Vision and Pattern Recognition, 2022. [28] S. Ikehata. Universal photometric stereo network using global lighting contexts. In Proceedings of IEEE/CVF Computer Vision and Pattern Recognition, 2022. [29] S. Ikehata, D. Wipf, Y. Matsushita, and K. Aizawa. Robust photometric stereo using sparse regression. In Proceedings of IEEE/CVF Computer Vision and Pattern Recognition, 2012. [30] S. Ikehata, D. Wipf, Y. Matsushita, and K. Aizawa. Photometric stereo using sparse bayesian regression for general diffuse surfaces. IEEE Trans. Pattern Anal. Mach. Intell., 36(9):18161831, 2014. [31] Satoshi Ikehata. Scalable, detailed and mask-free universal photometric stereo. In Proceedings of IEEE/CVF Computer Vision and Pattern Recognition, 2023. [32] Yakun Ju, Junyu Dong, and Sheng Chen. Recovering surface normal and arbitrary images: dual regression network for photometric stereo. IEEE Transactions on Image Processing, 30:36763690, 2021. [33] Jiyoung Jung, Joon-Young Lee, and In So Kweon. One-day outdoor photometric stereo via skylight estimation. In Proceedings of IEEE/CVF Computer Vision and Pattern Recognition, 2015. [34] Berk Kaya, Suryansh Kumar, Carlos Oliveira, Vittorio Ferrari, and Luc Van Gool. Uncalibrated neural inverse rendering for photometric stereo of general surfaces. In Proceedings of IEEE/CVF Computer Vision and Pattern Recognition, pages 38043814, 2021. [35] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: In Proceedings of the 36th framework for attention-based permutation-invariant neural networks. International Conference on Machine Learning, 2019. [36] Daniel Lichy, Soumyadip Sengupta, and David Jacobs. Fast light-weight near-field photometric stereo. In Proceedings of IEEE/CVF Computer Vision and Pattern Recognition, 2022. [37] Fotios Logothetis, Roberto Mecca, Ignas Budvytis, and Roberto Cipolla. cnn based approach for the point-light photometric stereo problem. Int. J. Comput. Vision, 131(1):101120, October 2022. [38] S. P. Mallick, T. E. Zickler, D. J. Kriegman, and P. N. Belhumeur. Beyond lambert: reconstructing specular surfaces using color. In Proceedings of IEEE/CVF Computer Vision and Pattern Recognition, 2005. [39] Roberto Mecca, Fotios Logothetis, Ignas Budvytis, and Roberto Cipolla. Luces: dataset for near-field point light source photometric stereo. arXiv preprint arXiv:2104.13135, 2021. [40] Zhipeng Mo, Boxin Shi, Feng Lu, Sai-Kit Yeung, and Yasuyuki Matsushita. Uncalibrated photometric stereo under natural illumination. In Proceedings of IEEE/CVF Computer Vision and Pattern Recognition, 2018. [41] Augustus Odena, Vincent Dumoulin, and Chris Olah. Deconvolution and checkerboard artifacts. Distill, 2016. [42] Maxime Oquab, Timothée Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2023. [43] Yaopeng Peng, Milan Sonka, and Danny Chen. Spectral u-net: Enhancing medical image segmentation via spectral decomposition. arXiv.2409.09216, 2024. [44] PS-FCN. https://github.com/guanyingc/PS-FCN. [45] René Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. ArXiv preprint, 2021. [46] Jieji Ren, Feishi Wang, Jiahao Zhang, Qian Zheng, Mingjun Ren, and Boxin Shi. Diligent102: photometric stereo benchmark dataset with controlled shape and material variation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1258112590, June 2022. [47] H. Santo, M. Samejima, Y. Sugano, B. Shi, and Y. Matsushita. Deep photometric stereo network. In International Workshop on Physics Based Vision meets Deep Learning (PBDL) in Conjunction with IEEE International Conference on Computer Vision (ICCV), 2017. [48] Boxin Shi, Zhipeng Mo, Zhe Wu, Dinglong Duan, Sai-Kit Yeung, and Ping Tan. benchmark dataset and evaluation for non-lambertian and uncalibrated photometric stereo. IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(2):271284, 2018. [49] Ziyang Song, Zerong Wang, Bo Li, Hao Zhang, Ruijie Zhu, Li Liu, Peng-Tao Jiang, and Tianzhu Zhang. Depthmaster: Taming diffusion models for monocular depth estimation. arXiv:2501.02576, 2025. [50] T. Taniai and T. Maehara. Neural inverse rendering for general reflectance photometric stereo. In International Conference on Machine Learning, 2018. [51] Giuseppe Vecchio and Valentin Deschaintre. Matsynth: modern pbr materials dataset. In Proceedings of IEEE/CVF Computer Vision and Pattern Recognition, 2024. [52] Feishi Wang, Jieji Ren, Heng Guo, Mingjun Ren, and Boxin Shi. Diligent-pi: photometric stereo benchmark dataset with controlled shape and material variation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. [53] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of IEEE/CVF Computer Vision and Pattern Recognition, 2025. [54] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600612, 2004. [55] P. Woodham. Photometric method for determining surface orientation from multiple images. Opt. Engg, 19(1):139144, 1980. [56] Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, Xihui Liu, Yu Qiao, Wanli Ouyang, Tong He, and Hengshuang Zhao. Point transformer v3: Simpler, faster, stronger. In Proceedings of IEEE/CVF Computer Vision and Pattern Recognition, 2024. [57] Xiaoyang Wu, Yixing Lao, Li Jiang, Xihui Liu, and Hengshuang Zhao. Point transformer v2: Grouped vector attention and partition-based pooling. In Proceedings of Advances in Neural Information Processing Systems, 2022. 12 [58] Xiaoyang Wu, Zhuotao Tian, Xin Wen, Bohao Peng, Xihui Liu, Kaicheng Yu, and Hengshuang Zhao. Towards large-scale 3d representation learning with multi-dataset point prompt training. In Proceedings of IEEE/CVF Computer Vision and Pattern Recognition, 2024. [59] Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. In Proceedings of IEEE/CVF Computer Vision and Pattern Recognition, 2025. [60] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. In International Conference on Learning Representations, 2025. 13 A1 Appendix This supplementary document offers further technical details, demonstrations of the datasets employed, additional insights, and comprehensive results pertaining to our LINO-UniPS. A1.1 Network architecture details To provide readers with more in-depth understanding of our LINO-UniPS network architecture, the method primarily comprises seven key modules:(a) WaveDownSample module, (b) DINOv2-based feature extraction backbone, (c) an Enhanced Light-Normal Contextual Attention Module, (d) DPT-Based Fusion Module, (e) WaveUpSample module, (f) final Decoder and (g) the training loss. An overview of our network architecture is presented in Figure 2. In the subsequent sections, we will provide detailed account of the networks structural components and the specific transformations of tensor shapes as data progresses through the model. A1.1.1 WaveDownSample: To enable our encoder to extract more fine-grained contexts, we first incorporate the wavelet transform [10], chosen for its ability to separate an images highand low-frequency components [14, 23] while concurrently mitigating losses typically incurred during downsampling [43]. Initially, the input to our network is batch of multi-light image sets, represented by tensor RBF HW 3. In this notation, denotes the batch size, is the number of images captured under different illumination conditions for each scene instance, and represent the spatial dimensions (height and width), and the final dimension 3 corresponds to the RGB color channels. To simplify the subsequent exposition, we will assume batch size of = 1 unless otherwise specified, effectively considering the processing of single multi-illumination image set at time. As part of the preprocessing, to ensure that image pixel values lie within comparable range, each of the images within given scene instance is normalized by random scalar sampled uniformly between its maximum and mean values. Following this preprocessing, for the purpose of subsequent discussion (effectively assuming = 1), we obtain set of images {If }F Following SDM-UniPS [31], for each pre-processed input image If RHW 3, we perform two 2 3 in the image domain, and separate transformations: naive downsampling to obtain wavelet transform to yield its corresponding wavelet domain components 2 3, namely ll 2 3. =1, where each If RHW 3. R4 2 3, and hh H H 2 3, lh 2 3, hl 2 2 2 2 2 2 A1.1.2 DINOv2 backbone: 2 , lh , hl , hh (comprising ll 2 3 and the set of wavelet Subsequently, both the downsampled image representation 2 , each in 2 3) are individually processed. First, components each of these input components is partitioned into sequence of patch-based tokens. These token sequences are then fed into our feature extraction backbone. To maintain lean model architecture and leverage strong prior knowledge while minimizing the introduction of excessive additional parameters, we employ the DINOv2-small variant [42] as this backbone. This DINOv2-small backbone is initialized with its publicly available pre-trained weights and is subsequently fine-tuned during our training procedure to extract rich visual representations from these diverse inputs. In our specific implementation, we set the patch size to = 8. Consequently, for each input component with spatial dimensions H/2 W/2, the resulting sequence length is = (H/2)(W/2) tokens. Given our use of the DINOv2-small variant, its feature embedding dimension is = 384. Following processing by this DINOv2 backbone, we respectively obtain shallow visual feature representations s,f R4LD from s,f RLD from the downsampled image stream (derived from d the wavelet components stream (derived from ) and 2 ). A1.1.3 Enhanced Light-Normal Contextual Attention Module: To achieve more effective decoupling of lighting and normal features that are subsequently processed by the decoder, we introduce our Enhanced Light-Normal Contextual Attention Module. 1 Firstly, we design the light registration tokens to improve the handling of global illumination. While lighting information predominantly exhibits global characteristics across multi-light inputs [25, 28, 31], traditional attention mechanisms in existing Universal Photometric Stereo (PS) methods often fail to fully leverage this distributed information. This deficiency can hinder effective illuminationgeometry separation, motivating our specialized token-based strategy. Drawing inspiration from advancements like [9], and further considering the inherent illumination-dependency of the Universal PS task, we introduce these light registration tokens to explicitly capture and represent decoupled global lighting information within our framework. To facilitate the perception of distinct lighting components, we additionally introduce three specialized light tokens: xhdri R1C, designated for perceiving spatially-varying environment light; xpoint R1C, tailored for point lights (which often contribute high-frequency illumination effects); and xarea R1C, for directional light (typically representing low-frequency illumination sources). Subsequently, this set of three specialized light tokens is prepended to the token sequences derived from s,f (features from the wavelet s,f,r R4LD, where = components stream), respectively, leading to:F + 3. Then s,f,r are fed into our Interleaved Attention block [53] to enhance inter-intra feature communication. Specifically, our alternating attention block contains four attention layers, which can be represented as: frame light-axis global light-axis. s,f (features from the downsampled image stream) and s,f,r RLD and s,f,r and Previous work has found that feature communication within the encoder is very important [28, 31, 20], but their methods are often limited to patch-level local light-axis attention. Our alternating attention, however, breaks such limitations. On the one hand, it incorporates frame attention to enhance interimage communication. On the other hand, we have added global attention, more comprehensive global operation, allowing intra-image features to also be extended from the patch level to the image level. Upon processing by these four cascaded Interleaved Attention blocks (the number chosen to maintain manageable parameter count, although more blocks could potentially be employed), we d,f,r R4LD. It is obtain the deep feature representations denoted as worth noting that the attention operations inherent in these blocks do not alter the fundamental shapes of these tensor sequences. d,f,r RLD and To ensure the light registers tokens xhdri, xpoint, xarea effectively capture global illumination, as direct supervision of the decoded light map is challenging, we introduce light-aware feature alignment strategy during training [60, 59, 49]. Since we train LINO-UniPS on our own rendered synthetic dataset PS-Verse, for every scene within this training dataset, we have access to its corresponding lighting information from the rendering process. This includes: the environment light HDRI map, the directions and intensities of point lights, and the direction and intensity of the directional light. Mathematically, these are denoted as Lhdri RHW 3, Lpoint RM14, and Larea RM25, respectively. For Lpoint RM14, where M1 is the number of point lights, its first three dimensions denote position and the fourth denotes intensity. For the directional light component Larea RM25, where M2 denotes the number of directional lights, its first three dimensions specify position, the fourth denotes the directional light size, and the fifth indicates intensity. Subsequently, we encode the lighting components Lhdri, Lpoint, and Larea by projecting them into D-dimensional feature space, area RD. Similarly, thereby obtaining their respective representations lh the light tokens xenv, xpoint, and xdir, after being processed by the cascaded alternating attention blocks, are projected into the same D-dimensional feature space. This yields their respective high-dimensional area, all in RD. Specifically, this projection is realized using three representations: xh structurally similar two-layer Multi-Layer Perceptrons (MLPs). Within this common embedding space, we employ cosine similarity to supervise and align their respective feature distributions. Cosine similarity is chosen as it effectively measures the directional concordance between feature vectors, making it suitable for aligning representations of different lighting characteristics. This supervision translates into three distinct loss functions, denoted as Lhdri, Lpoint, and Larea. Their point RD, and lh hdri RD, lh point, and xh hdri, xh 2 respective mathematical formulations are: Lhdri = 1 Lpoint = 1 Larea = 1 (cid:88) (cid:88) (cid:88) A1.1.4 DPT-Based Fusion Module: hdri) (lh (lh hdri xh point xh area xh (lh area) point) (A1a) (A1b) (A1c) d,f The subsequent discussion details the fusion module applied to features extracted during the initial stages of our Encoder. The overall feature fusion process is DPT-based [45]. For clarity, we will first illustrate this process using the features derived from the downsampled image features d,f,r as the primary example. Within each Interleaved Attention module of the encoder, features obtained from its four internal attentionframe, light-axis, global, and ight-axisare first concatenated along the feature dimension. This operation yields an aggregated feature set for each module, denoted as d,(i) d,f RL4D, where {1, 2, 3, 4} represents the index of the i-th attention block. It is crucial to note that the three additional light registration tokens (introduced previously) do not participate in this specific feature aggregation (concatenation) process. Then, the aggregated features d,(i) from four selected Interleaved Attention blocks then undergo series of projection and resizing operations. These operations transform them into four-level feature pyramid, with hierarchical features H1, H2, H3, and H4. Their respective shapes are H0/2 W0/2, 2C H0/4 W0/4, 4C H0/8 W0/8, and 4C H0/16 W0/16. In this context, represents the number of multi-light input images, is base feature channel dimension (256), and H0, W0 refer to the spatial resolution of the original, full-sized input images. These four levels of hierarchical features (H1 through H4) are subsequently progressively fused using residual 2 convolutional blocks [22], ultimately producing fused feature representation 2 C. similar fusion process is applied to the features derived from the wavelet components path (F d,f,r 2 C. It is noteworthy that ), yielding corresponding fused representation, this strategy of selecting four feature levels for hierarchical fusion can be adapted if more than four Interleaved Attention blocks are employed in the encoders initial stages. For instance, if six such blocks are utilized, features from blocks indexed 1, 2, 4, and 6 might be selected to form the pyramid. Similarly, for an eight-block configuration, features from blocks 1, 3, 5, and 7 could be chosen as inputs to the hierarchical fusion pathway. fused R4F fused RF 2 A1.1.5 WaveUpSample: fused and those from the wavelet components path To obtain the final encoder output Fenc RF HW C, the features derived from the downsampled image path fused should be integrated. The process is as follows: first, the feature map fused RF HW C. Concurrently, for fused, which originates from the wavelet-transformed inputs, an inverse wavelet transform is applied to convert it back to the spatial domain, resulting in dwt fused RF HW C. Finally, these two processed feature sets, up fused, are element-wise summed. Gaussian blur is subsequently applied to this sum to promote smoother and more effective fusion of these potentially cross-domain features, ultimately producing the final encoder representation Fenc RF HW C. fused is upsampled, yielding an representation up fused and dwt A1.1.6 Decoder: The decoder architecture in our LINO-UniPS is largely identical to that of SDM-UniPS [31]; for clarity, we briefly outline its key components and rationale here. common initial step in PS for surface normal estimation is the pixel-wise aggregation of spatial-light features along the illumination axis, effectively reducing light channels to single representation per pixel using input images If and their corresponding encoded features enc. We introduce an approach, termed the pixel-sampling Transformer [58, 8, 57, 56], which uniquely operates on fixed count (m, e.g., = 2048) of randomly chosen pixel locations. This strategy offers distinct advantages: it maintains constant memory footprint per sample set regardless of image dimensions, thus ensuring excellent scalability; furthermore, by processing sparse, randomly distributed set 3 (xi) RC. The term of points, it substantially curtails over-smoothing artifacts often prevalent in dense convolutional operations. The practical implementation of the pixel-sampling Transformer involves selecting random pixels, denoted {xi}m i=1, from the valid (masked) region of the input image. For each sampled pixel xi, its associated features enc(xi) RC are obtained. These features enc(xi) are then combined through element-wise addition with (xi) represents high-dimensional projection of the raw pixel observations If (xi) R3, and this projection is performed by two-layer MLP with the objective of enhancing the representational power of these raw observations by mapping them to this higher-dimensional space. Notably, our strategy of first projecting the raw observations to RC and then performing addition differs from SDM-UniPS [31], which typically employ direct concatenation of features and the raw observations. These added per-pixel features add(xi) are subsequently condensed into compact descriptors A(xi) by employing Pooling by Multi-head Attention (PMA) [35]. The resulting collection of descriptors, A(xi)m i=1, is then fed into Transformer network. The processing for each of the sampled points involves applying frame attention and light-axis attention to aggregate non-local context and cross-image information. Following this, two-layer MLP is utilized to predict the surface normal vector for each location. These sparsely predicted normals are then systematically mergedfor instance, through spatial interpolation or dedicated upsampling moduleto reconstruct the full-resolution surface normal map corresponding to the original input image dimensions. In essence, the pixel-sampling Transformer facilitates the modeling of robust non-local dependencies with notable computational efficiency, while concurrently preserving fine details in the output normal map. This makes the approach particularly well-suited for physics-based vision tasks that involve high-resolution imagery. A1.1.7 Training Loss: In this part, we elaborate on the composition of our total training loss and the design of the respective weights for its constituent components. Based on the definitions provided in Eq. 4, Eq. 3.1 and Eq. A1, the overall training loss for our LiNO-UniPS method can be expressed as: = λ1Lhdri + λ2Lpoint + λ3Larea + λ4 (cid:88) (N )2 + λ5 (cid:88) ( G)2, Let us define the confidence-weighted normal reconstruction loss as Lconf and the normal gradient supervision loss as Lgradient. The overall training loss can then be expressed as: = λ1Lhdri + λ2Lpoint + λ3Larea + λ4Lconf + λ5Lgradient, Since our primary objective is surface normal reconstruction, Lconf (our confidence-weighted reconstruction loss) is established as the principal component of our total loss function. First, we provide detailed explanation for our selection of Lconf as the primary loss function. We elaborate on why this specific formulation was chosen over other potential candidates, such as direct MSE, (cid:80)(N )2, or an alternative loss weighted by eG, = , namely (cid:80)(N )2 eG. primary motivation for our LiNO-UniPS framework is to advance beyond prior Universal PS methods by specifically improving the handling of challenging high-frequency regions. We identify these regions based on large magnitudes of the surface normal gradients, as these directly reflect geometric complexity. We deliberately avoid using gradients derived from the input RGB multi-light images as the primary criterion for this identification. The rationale is that while RGB gradients are indeed large in areas of intricate geometric detail, they can also exhibit high magnitudes in regions with significant basecolor variations, which do not necessarily correspond to the geometric highfrequency features we aim to emphasize and reconstruct accurately. Consequently, our methodology incorporates loss function that is directly informed by surface normal gradients, rather than relying on naive MSE. crucial aspect of this gradient-informed loss strategy concerns the source of the gradients utilized for weighting or guidance. We opt to utilize network-estimated normal gradients for this purpose, rather than directly employing ground truth normal gradients G. This design choice is primarily motivated by two factors: Firstly, it compels the network to intrinsically estimate high-frequency components from the input, thereby fostering its inherent capability to process and represent finegrained details. Secondly, refraining from direct weighting by ground truth normal gradients typically leads to more stable and manageable training process, especially during the initial stages when network predictions may significantly deviate from the ground truth. Our design for the loss weights is as follows: λ1 = 0.1 (Lhdri/Lconf)sg , λ2 = 0.1 (Lpoint/Lconf)sg , λ3 = 0.1 (Larea/Lconf)sg , λ4 = 1, λ5 = 0.1 (Lgradient/Lconf)sg where the subscript sg denotes that the term within the parenthesis is treated as constant (i.e., its gradient is not computed during backpropagation for the purpose of this scaling factor, akin to .detach() in PyTorch). Our decision to set λ4 to 1 is because Lconf serves as the principal component in our overall loss function. The remaining auxiliary losses are then scaled using the adaptive weighting mechanism detailed in Eq. A1.1.7. This mechanism constrains their magnitudes to 0.1 times that of the primary losss detached value, (Lconf)sg, while still allowing their gradients to backpropagate fully. Such strategy effectively positions these auxiliary losses to act as regularizers to the main learning objective, rather than allowing disparate loss magnitudes to vie for dominance and potentially destabilize training. This controlled weighting is crucial for ensuring stable and efficient training of LiNO-UniPS, mitigating issues such as excessively slow convergence or even training failure that can arise from an unbalanced multi-term loss function. A1.2 Dataset Analysis and Presentation A1.2.1 Categorization Methodology To rigorously evaluate and enhance the capability of our LiNO-UniPS method for reconstructing surface normals of objects that feature high-frequency geometric details on complex surfaces, we curated dedicated set of objects exhibiting diverse geometric complexities. These objects were subsequently graded by difficulty into five distinct levels, designated Level 1 to Level 5. Specifically, Levels 14 are classified following Dora [7] criterion, based on the number of salient edges NΓ. Level 5, in contrast, is distinguished by the use of normal maps in its rendering. The specific criteria for this classification are as follows: Level 1 (Less Detail): 0 < NΓ 5000; Level 2 (Moderate Detail): 5000 < NΓ 20000; Level 3 (Rich Detail): 20000 < NΓ 50000; Level 4 (Very Rich Detail): NΓ > 50000. Level 5 : With Normal Map. Fig. A1 shows representative cases from the different defined levels. PS-Verse comprises 100,000 scenes. For each of these scenes, two distinct renderings are typically generated: one that utilizes normal map to incorporate fine geometric details, and another rendered without this normal map. Levels 1 4 consist exclusively of scenes rendered without normal map, with each of these four levels containing 25,000 scenes. Level 5 is composed entirely of the 100,000 scenes rendered with normal map enhancement. A1.2.2 The Use of Normal Map To enhance PS normal reconstruction for objects characterized by intricate, high-frequency details, training data rich in such geometric features is essential. However, 3D models genuinely possessing fine-grained geometric intricacies are often scarce and prohibitively expensive, which impedes the creation of diverse, large-scale, high-fidelity datasets. To overcome this limitation within the Universal PS framework, our work pioneers the integration of normal mapping directly into the dataset generation process. Normal mapping, 3D computer graphics technique, imbues low-polygon models with the visual appearance of high-frequency geometric details by applying specialized texture, known as normal map, which encodes fine-scale perturbations of the surface normals. During rendering, these stored normal variations are then utilized to simulate intricate surface details without actually increasing the underlying geometric complexity or polygon count of the model. 5 Figure A1: The objects are displayed sequentially from left to right, representing Level 1 to Level 5. Across Levels 1 through 5, there is progressive rise in geometric complexity. Specifically, Level 5 features exceptionally complex surface geometry due to the utilization of normal maps in its rendering process. 6 Figure A2: Effect of normal mapping on rendered surface detail. The top two rows display renderings without normal maps, while the bottom two rows showcase the same scenes rendered with normal map. It is evident that employing normal maps (bottom rows) results in significantly more high-frequency surface normal detail compared to renderings without (top rows). visual comparison of renderings with and without the use of normal maps is presented in Fig. A2. It is clearly evident from the figure that employing normal maps during the rendering process yields significantly higher level of detail in the resulting surface normals. A1.2.3 Lighting Setup When rendering PS-Verse, we use four types of light sources; (a) environment lighting, (b) directional lighting, (c) point lighting, (d) uniform background lightings. During rendering, we generate ten distinct lighting configurations by combining several base lighting components (conceptually denoted here as (a), (b), (c), and (d)). These specific configurations are as follows: (1) Component (a), (2) Component (b), (3) Component (b), (4) Components (a) + (b), (5) Components (a) + (c), (6) Components (b) + (c), (7) Components (a) + (b) + (c), (8) Components (a) + (d), (9) Components (b) + (d), (10) Components (a) + (b) + (d). The lighting setup includes: one directional light; variable number of point lights, randomly ranging from one to three; and uniform background lighting, introduced to better simulate realistic global illumination. Every scene in PS-Verse is rendered as 20 images, each employing lighting setup randomly chosen from our ten predefined lighting configurations. An example of such an image set for single scene is illustrated in Figure A3. A1.2.4 Comparison with Other Datasets Here, we primarily compare several training datasets: CyclesPS-Train [25], PS-Wild [28], PSMix [31], PS-Uni MS-PS [20], and our PS-Verse. For qualitative comparison of these datasets, please refer to Tab. 1. We now present illustrative qualitative comparisons in Fig. A4. Our comprehensive evaluation, encompassing both qualitative and quantitative aspects, leads us to conclude that PS-Verse is the premier training dataset in terms of quality for the Universal PS task. 7 Figure A3: 20 multi-light images of one scene and their respective lighting configurations. A1.3 Additional Discussion A1.3.1 Why Feature Consistency Matters The fundamental objective of the Universal PS task is to reconstruct illumination-invariant surface normals from series of multi-light input images. Consequently, the decoder must effectively aggregate contextual information provided by the encoder to derive lighting-agnostic representation of these surface normals. Existing Universal PS methods predominantly utilize encoder-decoder architectures, wherein encoders typically possess significantly larger parameter count than their decoder counterparts. Therefore, assuming relatively consistent decoder design, empowering more capable encoder to effectively disentangle lighting from normal information and refine these into robust normal-specific features is expected to yield superior normal reconstruction. Building upon approaches such as UniPS [28] and SDM-UniPS [31] that utilize comparable decoder designs, primary driver for our LiNO-UniPS is the introduction of an improved encoder. The objective of this encoder is to yield more consistent features, thereby achieving superior results. However, it is not an oversimplification that greater feature consistency directly translates to superior normal reconstruction; the decoders architecture and capabilities also represent non-negligible factor in overall performance. Nonetheless, the decoders utilized in UniPS, SDM-UniPS and our LiNO-UniPS are architecturally similar, all adhering to the pixel-sampling paradigm. Consequently, when analyzing the relationship between feature consistency (e.g., as measured by CSIM/SSIM) and normal reconstruction performance in Fig. 1, we group these three methods together to facilitate more direct comparison of the impact of their respective encoder-derived features. Fig. A8 presents Principal Component Analysis (PCA) visualizations of features extracted by the encoders of various methods, alongside their corresponding feature similarity metrics (CSIM/SSIM). In the following discussion, we primarily focus our analysis on our Ours w/mlp variant and Uni MS-PS. 8 Figure A4: Visual comparison of different datasets. The spatial resolution of the images corresponding to each column is indicated beneath it. Figure A5: Attention maps for light registers tokens, showcasing their ability to focus on illumination-related regions within the input. Ours w/MLP refers to configuration of our LiNO-UniPS where the standard decoder is replaced by simple two-layer MLP. As illustrated in Fig. A8, this variant gives the highest feature similarity. Visual inspection of the PCA plots further reveals that its extracted features have effectively disentangled lighting information. We hypothesize that the superior feature similarity of Ours w/mlp compared to the full LiNO-UniPS (with its original decoder) stems from the constraints imposed by the weaker MLP decoder; this limited decoder capacity compels the encoder to learn more consistent features to facilitate accurate normal reconstruction. While this enhanced feature consistency from the encoder may not entirely compensate for the reduced representational power of the simpler decoder in terms of absolute normal reconstruction quality (when compared to the full LiNO-UniPS), Ours w/MLP variant nevertheless significantly outperforms SDM-UniPS. This finding strongly corroborates our central hypothesis regarding the critical role of powerful and well-regularized encoder in achieving effective feature disentanglement and consistency. Uni MS-PS also demonstrates high feature similarity. However, visual analysis of its features (Fig. A8) suggests that they remain considerably entangled with lighting information. Consequently, we infer that its high reported feature similarity may be more attributable to geometric self-consistency within its representations rather than successful illumination decoupling. Despite this apparent lack of complete feature decoupling, Uni MS-PS often achieves commendable reconstruction results. We attribute this primarily to its multi-scale architecture: beyond the initial stage, each subsequent network stage in Uni MS-PS incorporates predicted normals from the preceding stage as an additional input, effectively leveraging them as strong geometric prior. This iterative refinement, guided by intermediate normal predictions, places Uni MS-PS in distinct operational paradigm compared to methods like UniPS, SDM-UniPS, and our LiNO-UniPS. Furthermore, We need to figure out that Uni MS-PS exhibits certain practical limitations. (a) Its multi-scale nature leads to considerable inference latency, particularly when processing multiple high-resolution input images (e.g., handling 16 images at 4K resolution can extend to several hours). In contrast, our LiNO-UniPS method typically completes inference within tens of seconds for similar inputs. (b) While Uni MS-PS can reconstruct detailed surface normals, its reliance on potentially lower-resolution training datasets and its patch-based inference mechanism can lead to loss of global contextual information, sometimes resulting in reconstructions that are locally detailed but globally inconsistent or erroneous 1. A1.3.2 Light Registers Tokens To achieve more effective decoupling of lighting from normal features within our encoder, we introduce light register tokens. These tokens are specifically designed with the expectation that they will primarily capture and encode illumination information. Fig. A5 presents attention maps derived from the final layer of our encoder. These visualizations demonstrate that the light register tokens indeed attend to diverse illumination-related regions within the input images, including areas characterized by highlights and shading. 10 Figure A6: Attention maps for our different light registers tokens reveals distinct focusing patterns. The HDRI register token typically attends to broader, more diffuse regions, while the point light register token and the area light register token exhibit similar behaviors, both demonstrating more localized and sharper attentional focus. We introduce three additional light registration tokens: xhdri intended for environment light, xpoint for point light sources, and xarea for area light sources. Fig. A6 shows the attention map visualizations for these different tokens. It is observable that the HDRI register typically attends to broader regions. The point light register token and the area light register token exhibit similar attentional patterns, both demonstrating more localized and sharper focus. This shared characteristic aligns well with the generally low-frequency nature of global HDRI illumination and the more localized, often high-frequency, impact typically associated with point and area light sources. A1.3.3 Limitations Despite the commendable performance demonstrated by LiNO-UniPS, certain limitations nonetheless remain, offering avenues for future research. Firstly, our incorporation of global attention in the encoder, while aimed at enhancing intra-image feature communication for more effective decoupling of lighting from normal features and successfully improving disentanglement, also introduces computational burden. key direction for future work is therefore to explore more computationally efficient mechanisms that can achieve comparable decoupling efficacy at reduced operational cost. Secondly, limitation arises in the estimation of surface normals for near-planar objects that exhibit intricate concave and convex surface details, where the performance of LiNO-UniPS can be suboptimal. We attribute this primarily to fundamental challenge inherent in the Universal Photometric Stereo (PS) paradigm: the absence of explicit light source parameters makes it difficult for the network to unambiguously determine the precise illumination direction (e.g., to distinguish between light originating from above versus below the surface). Consequently, this ambiguity can frequently, and in some instances, even lead to the estimated surface normals being inverted A7. In future investigations, we plan to explore whether more sophisticated lighting alignment strategies could effectively mitigate this specific issue. Figure A7: For near-planar objects possessing intricate concave and convex surface details, our LiNO-UniPS tends to invert the predicted surface normals. The objects are from DiLiGenT-Π [52] A1.3.4 Broader Impacts Our research offers several positive societal implications. It promises to drive technological advancements in digital content creation, including fields such as virtual reality (VR) and cinematic visual effects (VFX). Moreover, enhanced 3D reconstruction capabilities can bolster machine perception for applications like robotics and autonomous driving, and provide valuable tools for auxiliary scientific research. Nevertheless, the advanced capabilities described also entail potential negative societal effects. High-fidelity 3D reconstruction, if misused, could be exploited to generate disinformation or fabricate convincing false realities. Furthermore, such technology raises concerns regarding potential infringements on personal privacy, and could pose risks to intellectual property rights and security. 11 Figure A8: Post-PCA visualization of features extracted by different method encoders for the CowPNG from the DiLiGenT [48]. Metrics displayed to the right of each row is (CSIM/SSIM), higher values indicate higher feature similarity. Figure A9: Top row: Example from the input multi-light images. Bottom row: Surface normal map reconstructed by our LiNO-UniPS. The data utilized is from SDM-UniPS [31], and the resolution is 4K . A1.4 Extended Results In this section, we present additional results to further demonstrate the capabilities of our LiNOUniPS. A1.4.1 Real Data Fig. A9 shows reconstruction results on challenging real-world data, which was captured from diverse scenes, is mask-free, and features 4K spatial resolution. These results demonstrate that our LiNO-UniPS is also scale, mask-free and detailed. 12 Figure A10: Real-world data with masks and corresponding LiNO-UniPS reconstruction results; data sourced from UniPS [28] and SDM-UniPS [31]. 13 Figure A11: Comparison of different Universal PS methods on our PS-Verse Testdata, showcasing ground truth normals, reconstruction normals, and corresponding error maps. The error maps depict the Mean Angular Error (MAE), measured in degrees; lower MAE values signify more accurate reconstruction. Fig. A10 presents examples of real-world captured objects, the overall quality of these reconstructions underscores our approachs strong generalization capabilities. Fig. A11 presents comparison of various Universal PS methods on our PS-Verse Testdata. Given that PS-Verse Testdata is synthetic dataset, ground truth is readily available, facilitating precise quantitative evaluation. The results clearly demonstrate that our LINO-UniPS significantly outperforms contemporary approaches, including Uni MS-PS [20], SDM-UniPS [31], and UniPS [28]."
        }
    ],
    "affiliations": [
        "AIR, THU",
        "BAAI",
        "BNU",
        "BUAA",
        "FNii, CUHKSZ",
        "HKUST",
        "NII",
        "NJU",
        "PKU"
    ]
}