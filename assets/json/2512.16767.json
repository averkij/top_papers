{
    "paper_title": "Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation",
    "authors": [
        "Zhiyang Guo",
        "Ori Zhang",
        "Jax Xiang",
        "Alan Zhao",
        "Wengang Zhou",
        "Houqiang Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Posing 3D characters is a fundamental task in computer graphics and vision. However, existing methods like auto-rigging and pose-conditioned generation often struggle with challenges such as inaccurate skinning weight prediction, topological imperfections, and poor pose conformance, limiting their robustness and generalizability. To overcome these limitations, we introduce Make-It-Poseable, a novel feed-forward framework that reformulates character posing as a latent-space transformation problem. Instead of deforming mesh vertices as in traditional pipelines, our method reconstructs the character in new poses by directly manipulating its latent representation. At the core of our method is a latent posing transformer that manipulates shape tokens based on skeletal motion. This process is facilitated by a dense pose representation for precise control. To ensure high-fidelity geometry and accommodate topological changes, we also introduce a latent-space supervision strategy and an adaptive completion module. Our method demonstrates superior performance in posing quality. It also naturally extends to 3D editing applications like part replacement and refinement."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 7 6 7 6 1 . 2 1 5 2 : r Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation Zhiyang Guo1,2 Ori Zhang2 Jax Xiang2 Houqiang Li1 Alan Zhao2 Wengang Zhou1 1CAS Key Laboratory of Technology in GIPAS, EEIS Department, University of Science and Technology of China Figure 1. Given 3D humanoid model of arbitrary shape and initial pose, our method efficiently re-poses it in single feed-forward pass. Unlike auto-rigging or generative approaches that often suffer from skinning artifacts or limited controllability, our latent posing paradigm robustly handles challenging cases and produces high-fidelity animation results."
        },
        {
            "title": "Abstract",
            "content": "Posing 3D characters is fundamental task in computer graphics and vision. However, existing methods like autorigging and pose-conditioned generation often struggle with challenges such as inaccurate skinning weight pretopological imperfections, and poor pose condiction, formance, limiting their robustness and generalizability. To overcome these limitations, we introduce Make-ItPoseable, novel feed-forward framework that reformulates character posing as latent-space transformation problem. Instead of deforming mesh vertices as in traditional pipelines, our method reconstructs the character in new poses by directly manipulating its latent representation. At the core of our method is latent posing transformer that manipulates shape tokens based on skeletal motion. This process is facilitated by dense pose representation for precise control. To ensure high-fidelity geometry and accommodate topological changes, we also introduce latent-space supervision strategy and an adaptive completion module. Our method demonstrates superior perforIt also naturally extends to 3D mance in posing quality. editing applications like part replacement and refinement. 1. Introduction Posing 3D humanoid characters, i.e., controllably articulating them into novel poses, is fundamental task in computer graphics and vision, with broad applications in filming, gaming, and mixed reality. Traditional posing and animation pipelines rely on rigging and skinning, notoriously labor-intensive process. While recent auto-rigging methods [3, 6, 15, 19, 25 27, 40, 48] have shown promise in automating this process, they tend to suffer from inaccurate blend weight prediction with spatial leakage and yield suboptimal animation quality, particularly for non-rest input poses with limbs in close proximity. Moreover, these geometry-based approaches deform vertices based on fixed mesh topology, preventing them from handling deformations that necessitate topological changes, such as revealing new surfaces. This issue is especially critical for AI-generated 3D assets, which have recently seen rapid development and widespread adoption [13, 30, 37, 50], yet frequently exhibit topological imperfections such as fused limbs or self-occlusions. another Recently, line of work explores poseconditioned 3D generation [31, 41]. However, these methods typically rely on an iterative denoising process guided by sparse pose signals, resulting in slow inference and insufficient pose conformance, in addition to their inherent difficulty in preserving the identity and details of specific source character across continuous animations. In this paper, we tackle these limitations from new perspective. Our key insight is that the geometry-space posing in traditional pipelines can be reformulated as well-defined regression problem within the 3D latent space. Therefore, we propose novel end-to-end framework that reconstructs the 3D character in new poses within single feed-forward pass by directly manipulating its compact and expressive latent representation. This approach operates in skinning-free fashion, enabling topology-aware articulation without the constraints of traditional rigging, thereby sidestepping the aforementioned issues of geometry-space manipulation. To the best of our knowledge, this is the first feed-forward 3D character posing model that operates on native 3D latent representation. The core of our method is latent posing transformer. It takes the latent VecSet [46] representation of source character, along with the source and target skeletons, as inputs, and predicts the latents of the re-posed character. This design incorporates minimal skinning inductive bias, principle conceptually similar to 2D novel-view synthesis models that operate with minimal 3D inductive bias [10]. Instead of relying on explicit skinning weights that are challenging to perfectly predict and often cause artifacts, our model adaptively learns to manipulate the overall shape in response to skeletal motion entirely within the latent space. To facilitate this latent-space manipulation, we design skeleton encoder that establishes dense pose representation. Unlike sparse pose signals that can create an information bottleneck [31, 41], our encoder provides fine-grained skeletal context for the shapes latent representation. This explicit injection of dense pose information obviates the need for the model to infer complex geometric correspondences, enabling more precise and controllable pose-aware feature modulation. During training, to ensure high-fidelity results, we employ latent-space supervision strategy. By establishing canonical correspondence between source and target latent vectors, we provide strong guidance to overcome the ambiguity of the unordered VecSet representation and prevent detail loss. Furthermore, to handle newly exposed surfaces during deformations that involve topological changes, we 2 equip our model with an adaptive completion module. This module is tuned to generate new latent tokens representing previously unseen geometry, ensuring plausibility in the final posed shape. Extensive experiments demonstrate that our method significantly outperforms existing approaches in terms of fidelity, speed, and animation quality. The fine-grained control offered by our latent-space modeling also enables generalization to some 3D editing applications. By manipulating the skeleton encoding, our framework facilitates structured content modifications such as segmentation, replacement, and refinement of body parts. Our work is focused on the bipedal humanoid domain, which allows us to tackle the high degrees of freedom and complex self-contact scenarios unique to humanoids, while also leveraging available large-scale motion data. Although the current implementation is tailored for humanoids, our frameworks core principles are skeleton-agnostic and can potentially be extended to general articulated objects. 2. Related Works 2.1. Latent 3D Generation and Editing Rather than operating on explicit voxel grids or mesh representations, modern 3D generation methods increasingly operate in latent space, enabling superior geometric generalization. Early approaches reconstructed 3D geometry by lifting and fusing 2D multi-view features [16, 17, 21]. However, these 2D-lifting methods are inherently limited by their lack of 3D latent representation. Therefore, recent work has shifted toward native 3D architectures. One popular approach encodes 3D object as an unordered set of latent vectors (i.e., VecSet) [46], which can be further processed by diffusion transformers [13, 30]. In parallel, voxel-based latent grids map latent vectors to sparse voxel coordinates, preserving spatial alignment while maintaining memory efficiency [23, 37]. As 3D representations migrate into latent domains, editing methodologies have also evolved to operate directly within latent spaces. For instance, various methods perform part-aware segmentation and composition [42, 43, 51], geometry enhancement [4, 44], and localized editing [11, 12] directly on latent features. Most of these techniques rely on explicit spatial structures or iterative diffusion processes. 2.2. Humanoid Animation Humanoid animation aims to transform 3D characters with controllable and physically plausible motions. The most mainstream approach is rigging, where the input mesh is deformed via skeleton and the associated skinning weights. The laborious nature of manual rigging has driven the development of automatic techniques. Early approaches embedded skeleton into the input mesh and optimized Figure 2. Pipeline of our character posing framework. Given source shape and source/target skeletons, we encode them into latent representations with dense correspondence. latent posing transformer then predicts the target shape tokens, which are finally decoded into the posed mesh. This framework is trained in two stages. First, latent loss is established to preserve geometric details. Second, an adaptive completion module is exclusively finetuned with an SDF loss to synthesize plausible geometry for newly exposed structures. for linear skinning weights [2]. Subsequent learning-based methods make progress by using neural networks [19, 40]. More recently, feed-forward transformer-based models enable fast, accurate, and robust character rigging [3, 6, 27]. This paradigm has also been extended beyond humanoids to general objects [5, 15, 25, 26, 48]. Parallel works infer motion from videos [9] or directly model 4D mesh dynamics [36]. Despite these advances, existing data-driven autorigging algorithms still exhibit notable limitations. They often produce non-adaptive mesh topologies and suffer from skinning weight leakage across geometrically close vertices, leading to unrealistic deformations and restricting their applicability to canonical pose configurations. Alternatively, humanoid animation can also be addressed through pose-conditioned 3D reconstruction or generation. Recent approaches [31, 41] manage to integrate skeletal conditioning into 3D generative pipelines. However, they often struggle with precise and generalizable pose control due to the sparse and complex mapping between skeletal pose and geometry. In parallel, template-based frameworks [8, 14] introduce canonical human templates [18, 20] and learn deformation fields to preserve topology and motion consistency. More recent work [34] unifies geometry, pose, and appearance generation in feed-forward paradigm. Nevertheless, current pose-conditioned 3D generation models still fail to achieve an ideal balance between output quality and pose conformance. 3 2.3. Feed-forward Models in 3D Vision Recent feed-forward models have demonstrated the ability to reconstruct high-quality 3D assets in single forward pass, offering significant efficiency gains over iterative optimization or diffusion-based methods. Pioneered by Large Reconstruction Model (LRM) [7], many works [28, 29, 35, 38, 39, 49] learn to map multi-view or monocular inputs to compact 3D representations directly. This paradigm has also been extended to tasks like dense geometry estimation and novel view synthesis [10, 32, 33], leveraging transformer backbones for fast and robust 3D prediction without relying on diffusion or optimization loops. comprehensive survey of this emerging field can be found in [47]. Despite the rapid and remarkable progress, the application of feed-forward models in animation-oriented 3D generation remains far less explored. HumanRAM [45] represents one of the first attempts to endow LRM with animation capabilities in the 2D image domain. Nevertheless, the development of feed-forward paradigms for dynamic 3D content is still in its infancy. Efficiently integrating motion understanding and deformation control into these architectures remains an open challenge. 3. Method Our goal is to re-pose given 3D humanoid mesh while maintaining geometric details and shape plausibility. As illustrated in Fig. 2, our method begins by extracting skeleton from the source mesh to define the pose (Sec. 3.1). We then employ two parallel encoders, i.e., shape encoder (Sec. 3.1) and skeleton encoder (Sec. 3.2), to convert the source mesh and both the source and target skeletons into latent representations. These latents are projected and processed by latent posing transformer (Sec. 3.3) that predicts the target shape tokens, which are finally decoded to reconstruct the posed mesh. The overall model is trained with latent-space supervision (Sec. 3.4) to ensure high-quality results. Furthermore, to handle newly exposed structures during deformations, we additionally introduce an adaptive completion module and apply geometry-space finetuning (Sec. 3.5). More implementation details are provided in the supplementary material. 3.1. Preliminaries Humanoid rigging. Typically, the production of 3D humanoid animations begins with the rigging process, which generates skeleton RK6 (K bones with head and tail positions) from the mesh and assigns suitable blend weights to all mesh vertices to define the influence of the bones. Instead of applying such geometry-space rigging and some blend skinning algorithms to animate the mesh, we provide novel end-to-end solution of latent-space posing, where skeleton is still extracted by an automatic rigging method [6], but only serves as pose representation and also proxy for user-specified target motion. 3D VAE. We employ 3D Variational Autoencoder (VAE) to obtain compact, VecSet-based representation [46] of 3D shapes, which has been proven to be suitable for both 3D understanding and generating tasks [6, 13, 30]. Specifically, the shape encoder takes the coordinates and normals of point cloud RN 3 sampled from the mesh surface as inputs. It first utilizes Farthest Point Sampling (FPS) to select subset of query points RL3. These queries then interact with the full set via cross-attention and are mapped into Gaussian distribution parameterized by the mean and variance E(Zs), Var(Zs) RLd, from which an L-sized latent VecSet Zs RLd is sampled. This whole process can be formulated as E(Zs), Var(Zs) = CrossAttn( , ), (1) where the concatenated normal input and Fourier positional encoding are omitted for simplicity. Despite any explicit constraints, each latent vector in Zs typically captures the geometry context of local region around the corresponding subsampled query point [4], which establishes bridge between geometry-space and latent-space manipulations. Conditioned on Zs, transformer-based shape decoder reconstructs continuous field by predicting the original shapes Signed Distance Function (SDF). This implicit representation can then be converted into an explicit mesh using iso-surface extraction algorithms like marching cubes. 4 3.2. Skeleton Encoder As illustrated in Fig. 3 (a), to effectively condition the 3D shape on pose transformation, we propose skeleton encoder that densifies the sparse skeletal representation to match the per-point granularity of the input. Dense pose representation. The original form of pose, defined by sparse set of bone positions or transformations, tends to create an information bottleneck for conditioning. If adopted, it will place substantial learning burden on the subsequent transformer, which has to implicitly learn the complex mapping from this global pose to its different influence on every local 3D point. Inspired by multi-view vision encoders where per-pixel Plucker ray embeddings are preferred over single per-image camera matrix, our method provides per-point skeletal context. Specifically, for each sampled shape surface point in (i.e., the input of the shape encoder), we compute its geometric projection onto the surface of its associated bone. This associated bone, modeled as cylinder with radius positively correlated to the bone length, is identified by selecting the one with maximum influence from the coarse blend weights provided by an auto-rigging tool [6], though simpler segmentation prior could also be used. These projected coordinates are then processed by transformerbased encoder with subset-fullset cross attention similar to the 3D shape encoder (Eq. (1)). Specially, to produce the required query subset, we mirrors the subsampling indices from the shape encoder, so that the output source skeleRLd have one-to-one corresponton latents Zsrc dence with the shape latents Zsrc along dimension L, allowing them to be channel-wise concatenated to provide fine-grained dense pose information for the shape latents. Note that unlike the shape encoder that predicts the mean and variance of the latent VecSet as in Eq. (1), our skeleton encoder directly outputs the latent vector to represent the pose in deterministic way, which can be formulated as Zsrc = CrossAttn(Proj( ), Proj(P )), where Proj() denotes the projection operation onto the associated bones from the source skeleton. (2) As mentioned in Sec. 3.1, each latent vector in Zsrc corresponds to specific local region around its query point on the source skeleton. To establish robust correspondence between the source and target latents, we leverage the fact that the target skeleton is rigid-deformed version of the source one. Specifically, the target skeleton in another pose is also encoded into latent set Ztgt by the shared skeleton encoder. But instead of performing re-sampling or reprojecting to get the required points on the target skeleton, we directly apply the bone-wise source-to-target rigid deformation to the fullset and subset of source skeletal points to form the target ones, which are then processed as = CrossAttn(T (Proj( )), (Proj(P ))), Ztgt (3) Figure 3. Illustration of our key designs. (a) The skeleton encoder (Sec. 3.2) produces dense pose representations with latent-level oneto-one correspondence. (b) Latent-space supervision (Sec. 3.4) ensures semantically meaningful token transformation path to preserve geometric details. (c) Adaptive tokens (Sec. 3.5) are introduced in the finetuning stage to handle newly exposed structures after deformation. where () represents the rigid deformation from the source to the target pose. This approach ensures the interpretability of the target latents, which is essential for the subsequent latent-space supervision (Sec. 3.4). Discriminative embeddings. Our skeleton encoding strategy expressed by Eqs. (2) and (3) naturally preserves oneto-one correspondence between the source and target skeleton latents. As variant of VecSet-based 3D shape encoder, such skeleton encoder already has the ability to extract 3D contextual information, which is crucial for the subsequent latent posing transformer to match the source and target latents. However, we find in practice that this encoding process alone is insufficient for preserving geometric details across poses, especially for complex motions. To address this, the source-target correspondence has to be explicitly highlighted in the latent space, providing shortcut for the transformer to copy source geometries to the target place. Therefore, we introduce set of unique, learnable embeddings to build more discriminative latent representations for skeletons. We add the same embedding vector to each pair of corresponding latent vectors (one from Zsrc and one from Ztgt ) that originate from the same query point. To prevent the model from including any semantic information in these embeddings, we randomly shuffle the assignment of embeddings to query points in each training iteration. Analogous to patch-level positional embeddings in vision transformers, these discriminative embeddings act as latent-level pairing IDs to enhance the distinctiveness of the unordered latents in VecSet, enabling the model to effectively track corresponding local features across different poses. 3.3. Latent Posing Transformer At the core of our method is decoder-only transformer designed to animate the input shape within the latent space. The process begins by constructing input tokens for the transformer. To ensure deterministic representation for the latent-space supervision (introduced later in Sec. 3.4), we represent the source shape using channel-wise concatenation of the mean E(Zsrc ) and variance Var(Zsrc ) vectors of its encoded latent distribution (rather than random sample Zsrc ). We concatenate these vectors with the source skeleton latents Zsrc to form pose-aware shape representation. An MLP-based projector then transforms it into sequence of source input tokens by an MLP. Meanwhile, the target skeleton latents tgt alone are projected into target input tokens. s The source and target input tokens are then concatenated into long sequence and fed into series of transformer blocks. Through in-context attention, the model implicitly learns the complex source-to-target shape transformation guided by the skeleton proxy, completing skinning-free posing in an end-to-end latent-space manner within single forward pass. The transformer outputs an updated sequence of target tokens, which are then projected by another MLP to predict the mean and variance of the target shapes latent distribution. Finally, we sample the target shape latents Ztgt from this predicted distribution and reconstruct the fis nal posed mesh using the pretrained shape VAE decoder and marching cubes algorithm. 5 3.4. Training with Latent-Space Supervision Training regressive reconstruction models [10, 45] with an end-to-end reconstruction loss is common practice. However, in our case, directly applying SDF supervision to the output shape yields over-smoothed results that lack geometric detail. This issue arises from the unordered nature of the latent VecSet. As illustrated in Fig. 3 (b), the permutation invariance of the output tokens introduces ambiguity in the correspondence between input and output tokens of our posing transformer, since any permuted version of token set can be decoded into the exact same shape. This makes it difficult for the model to learn consistent shape transformation path and leads to smooth mean result. To address this, we propose strategy for latent-space supervision. We first establish canonical transformation path that is consistently meaningful, which ensures that each output target shape latent corresponds to the same semantic local region as its source counterpart, as depicted in Fig. 3 (b). Recall that our dense pose representation (Sec. 3) naturally creates one-to-one correspondence between the tarand the source shape latents Zsrc get skeleton latents tgt . We then leverage this correspondence to construct groundtruth target shape latents denoted by ˆZtgt . Specifically, we encode the ground-truth target shape using the pretrained shape encoder, while specifying the query points as ( ). These are the same query points from the source shape encoding process (Eq. (1)), but transformed by the groundtruth deformation (). Finally, we align the predicted Ztgt with the ground-truth latents ˆZtgt using an L1 loss. s 3.5. Finetuning with Adaptive Tokens The latent loss introduced in Sec. 3.4 relies on the assumption that the source and target shapes share similar geometric structures, allowing for one-to-one correspondence between their latent representations. However, this assumption does not hold when deformations reveal new geometric structures. For instance, as illustrated in Fig. 3 (c), lifting characters arm exposes the armpit, surface region not visible in the source pose. Representing these newly emerged structures requires additional tokens that lack counterparts in the source shape. To address this issue, we propose an adaptive completion module. It utilizes lightweight transformer architecture to generate new shape tokens conditioned on the source and target input tokens for contextual awareness. We finetune this module separately using an SDF loss with the main model frozen, which prevents the previously learned canonical transformation path from being disrupted by the geometry-space supervision (as discussed in Sec. 3.4). To produce sufficient training data for this finetuning stage, we apply an alpha-wrapping-based remeshing technique [22] to the source shapes while keeping the target shapes unchanged. This process effectively occludes in6 ternal or self-contacting surfaces from inputs, creating data pairs where deformation reveals previously unseen geometry. Consequently, the model learns to adaptively identify and complete these missing structures based on the provided context, making the output shape more plausible under complex animations. 4. Experiments 4.1. Experimental Settings Data. We generate the training and evaluation data by pairing character models with motion sequences, following [6]. Our character dataset comprises two main sources: (a) 95 high-quality, artist-designed 3D humanoid models from Mixamo [1], and (b) 10k AI-generated humanoid meshes with diverse body proportions from HumanRig [3]. For motion data, we utilize 20k sequences from Mixamo [1], each averaging 200 frames. Source-target pose pairs are randomly sampled from these sequences for training and quantitative evaluation. To assess performance on in-thewild data, we also use Hunyuan3D-2.1 [30] to generate variety of 3D characters for qualitative comparison. Baselines. We compare our method with state-of-the- (a) Make-Itart works involving different paradigms: Animatable [6] (MIA) is powerful geometry-space autorigging method that predicts skeleton and skinning weights for 3D humanoid models of arbitrary shape and pose. (b) Puppeteer [25] is recent auto-rigging method for general 3D shapes with an advanced topology-aware skinning predictor. Since it generates non-semantic skeletons that cannot be accurately driven by predefined poses, we include it only for qualitative comparison, where we manually apply poses after rigging. (c) Hunyuan3D-Omni [31] (HY3DOmni) is DiT-based 3D generative model capable of producing 3D characters from skeleton conditions. We adapt it for our 3D posing task by providing the rendered image of the textured source shape along with the target skeleton. Although this comparison is not perfectly fair due to the different settings, it serves to illustrate the inherent limitations of the pose-conditioned generation paradigm for this task. Metrics. We quantitatively evaluate posing quality against the ground-truth shapes using several metrics. Following [46], we compute Chamfer Distance (CD) and F-score on 50k points sampled from the mesh surfaces, while volumetric Intersection-over-Union (IoU) and SDF Root Mean Square Error (SDF-RMSE) are calculated on another 50k points sampled in 3D space. The average per-sample inference time is also reported. 4.2. Comparison Results Quantitative analysis. As shown in Tab. 1, our method significantly outperforms the baselines in terms of mesh fidelity across all metrics. Compared to the geometry-space Figure 4. Qualitative comparison on diverse characters and poses. We showcase results for re-posing each character into widelyIt robustly handles adopted T-pose and an additional random pose. Our method produces high-fidelity results across various cases. challenging inputs where MIA [6] and Puppeteer [25] produce significant artifacts, and gives better pose conformance and detail preservation compared to HY3D-Omni [31]. Note that HY3D-Omni takes the rendered image as input, not the 3D shape itself. Table 1. Quantitative comparison. We report the mesh fidelity metrics and per-sample inference time. CD F-score IoU SDF-RMSE Time MIA [6] HY3D-Omni [31] Ours 1.07E-3 5.42E-3 0.07E0.8788 0.5598 0.9858 0.7467 0.4712 0.8542 0.0164 0.0301 0.0161 0.45s 33.16s 0.59s rigging method [6], our approach achieves superior geometric accuracy and surface fidelity, while maintaining comparable inference speed. Against the generative model [31], our method demonstrates advantages in both quality and efficiency, being over 50 times faster. Qualitative analysis. The qualitative results in Fig. 4 further highlight the superiority of our method. For standard assets, our results are visually comparable to MIA [6]. However, in challenging cases with non-rest input poses (e.g., limbs in close proximity) or topologically imperfect input meshes, rigging-based methods [6, 25] both produce spiking artifacts due to skinning leakage. In contrast, our method remains robust and can adaptively reconstruct the mesh topology to avoid such issues. The generative model, HY3D-Omni [31], despite producing plausible results, struggles to preserve character identity and details. Moreover, its sparse pose control often fails on complex poses or out-of-distribution shapes, leading to inaccurate poses, limb misplacement, or even collapsed shapes. Thanks to the dense pose representation and the powerful latent transformer architecture, our method consistently produces high-fidelity results that excel in identity preservation, detail accuracy, and pose conformance. 4.3. Applications Animation. The most direct application of our method is character animation, which can be realized in two ways: (1) First re-pose the input mesh into rest pose with limbs fully extended (e.g., 大-pose), then rig and animate it using standard pipelines. This integrates seamlessly with existing graphics workflows while effectively resolving the skinning and topology issues. (2) Directly apply target motions to generate mesh sequence, leveraging our methods detail consistency and fast inference speed. Fig. 5 (a) shows our high-quality direct animation results. Part segmentation and replacement. Our fine-grained and flexible pose representation enables zero-shot body part segmentation and replacement, even without explicit training for these tasks. This is accomplished by manipulating the target skeleton. As illustrated in Fig. 5 (b), parts can be segmented by displacing their corresponding bones far from the body. Similarly, new parts can be integrated by simply importing them into the 3D space and then attaching their bones to the target skeleton with the corresponding original ones removed. Fig. 5 (c) shows the characters abstract hands being replaced by new shape from four-fingered 7 Figure 5. Our method enables various applications, e.g., (a) animation, (b) part segmentation, (c) replacement, and (d) refinement. Table 2. Quantitative ablation results. We evaluate some variants of our model to validate the effectiveness of our key designs. B denotes removing design and adding as an alternative. Module/Stage Setting SDF-RMSE Skeleton Encoder skeleton as cylinders lines per-latent embedding per-latent displacement embedding per-latent per-bone embedding per-latent blend-weight (GT) embedding per-latent blend-weight (Pred) embedding Training latent SDF loss Finetuning w/o finetuning SDF latent loss Ours (full model) 0.0593 0.0345 0.0229 0.0317 0.0163 0.0291 0.0442 0.0186 0. 0.0161 bunny. Our method naturally ensures smooth shape and topology at the transition boundaries for both operations. Part refinement. In practice, some character shapes are just abstract geometries lacking details, such as hands without fingers. To enable part refinement for them, we finetune our model by simply zeroing out the input source shape latents of hands. As illustrated in Fig. 5 (d), the model then becomes capable of reconstructing cohesive and detailed five-fingered hands guided by the target skeleton. This allows for subsequent fine-grained animation of the fingers. 4.4. Ablation Study We conduct ablation studies to validate our key designs. Skeleton encoder. We first ablate our pose representation from Sec. 3.2. As shown in Tab. 2, modeling the skeleton as lines significantly degrades performance. This simplification introduces rotational ambiguity along the bone axis, Figure 6. Qualitative ablation results. Our designs are crucial for handling issues including rotational ambiguity, source-target correspondence, detail preservation, and newly exposed structures. causing artifacts like twisted head, as seen in Fig. 6 (a). Our cylinder-based representation resolves this by providing richer 3D context. Next, we validate our per-latent embeddings. Removing them entirely leads to substantial performance drop, as the model struggles to establish source-target correspondence, usually resulting in missing limbs (Fig. 6 (b)). We also test several alternatives. While using ground-truth blend weights as embeddings yields comparable scores, this is impractical for in-the-wild inputs. Other configurations, such as predicted weights or simple displacement vectors, perform worse than our proposed method. These results confirm that our per-latent embeddings offer robust and effective mechanism for preserving local geometric details across poses. Latent-space supervision. As discussed in Sec. 3.4, supervising the model in the latent space is crucial. Training directly with geometry-space SDF loss yields poor results, producing over-smoothed shapes that lack fine details, as shown in Fig. 6 (c). This highlights the importance of our latent loss for preserving high-frequency details. Adaptive completion. We ablate our adaptive completion module from Sec. 3.5. Without the finetuning stage, the model fails to reconstruct newly exposed regions, leading to artifacts like the malformed armpit in Fig. 6 (d). The quantitative results also show that using an SDF loss for this stage is superior to latent loss, as the latter is ill-suited for tokens that lack source correspondence. 8 5. Conclusion and Discussion In this paper, we present novel feed-forward framework that reframes 3D humanoid character posing as latent-space transformation problem, which effectively addresses the limitations of geometry-based rigging or poseconditioned generative methods. Comprehensive experiments demonstrate the superiority of our method and its considerable potential for various applications. Despite the merits, several avenues still remain for future exploration and improvement. First, our frameworks skeleton-agnostic designs allow for direct extension to support general articulated objects like non-bipedal characters, which would require training on corresponding largescale animatable datasets. Second, extending the framework to accept multiple source shapes, e.g., individual body parts or gallery of poses, could facilitate more flexible posing through adaptive combination or selection."
        },
        {
            "title": "References",
            "content": "[1] Adobe. Mixamo, 2024. https://www.mixamo.com. 6 [2] Ilya Baran and Jovan Popovic. Automatic rigging and animation of 3D characters. ACM TOG, 26(3):72es, 2007. 3 [3] Zedong Chu, Feng Xiong, Meiduo Liu, Jinzhi Zhang, Mingqi Shao, Zhaoxu Sun, Di Wang, and Mu Xu. HumanRig: Learning automatic rigging for humanoid character in large scale dataset, 2024. 1, 3, 6 [4] Ken Deng, Yuan-Chen Guo, Jingxiang Sun, Zi-Xin Zou, Yangguang Li, Xin Cai, Yan-Pei Cao, Yebin Liu, and Ding Liang. DetailGen3D: Generative 3D geometry enhancement via data-dependent flow, 2025. 2, 4 [5] Yufan Deng, Yuhao Zhang, Chen Geng, Shangzhe Wu, and Jiajun Wu. Anymate: dataset and baselines for learning 3D object rigging. In SIGGRAPH Conference Proceedings, Vancouver, BC, Canada, 2025. Association for Computing Machinery. 3 [6] Zhiyang Guo, Jinxu Xiang, Kai Ma, Wengang Zhou, Houqiang Li, and Ran Zhang. Make-It-Animatable: An efficient framework for authoring animation-ready 3D characters. In CVPR, 2025. 1, 3, 4, 6, [7] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. LRM: Large reconstruction model for single image to 3D. In ICLR, 2024. 3 [8] Yukun Huang, Jianan Wang, Ailing Zeng, Zheng-Jun Zha, Lei Zhang, and Xihui Liu. DreamWaltz-G: Expressive 3D gaussian avatars from skeleton-guided 2D diffusion. IEEE TPAMI, 2025. 3 [9] Zehuan Huang, Haoran Feng, Yangtian Sun, Yuanchen Guo, Yanpei Cao, and Lu Sheng. AnimaX: Animating the inanimate in 3D with joint video-pose diffusion models. arXiv preprint arXiv:2506.19851, 2025. 3 [10] Haian Jin, Hanwen Jiang, Hao Tan, Kai Zhang, Sai Bi, Tianyuan Zhang, Fujun Luan, Noah Snavely, and Zexiang Xu. LVSM: large view synthesis model with minimal 3D inductive bias. In ICLR, 2025. 2, 3, 6, 1 [11] Lin Li, Zehuan Huang, Haoran Feng, Gengxiong Zhuang, Rui Chen, Chunchao Guo, and Lu Sheng. Voxhammer: Training-free precise and coherent 3D editing in native 3D space. arXiv preprint arXiv:2508.19247, 2025. 2 [12] Xiao-Lei Li, Hao-Xiang Chen, Yanni Zhang, Kai Ma, Alan Zhao, Tai-Jiang Mu, Hao-Xiang Guo, and Ran Zhang. RELATE3D: Refocusing latent adapter for targeted local enhancement and editing in 3D generation. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 112, 2025. [13] Yangguang Li, Zi-Xin Zou, Zexiang Liu, Dehu Wang, Yuan Liang, Zhipeng Yu, Xingchao Liu, Yuan-Chen Guo, Ding Liang, Wanli Ouyang, et al. TripoSG: High-fidelity 3d shape arXiv synthesis using large-scale rectified flow models. preprint arXiv:2502.06608, 2025. 2, 4 [14] Tingting Liao, Hongwei Yi, Yuliang Xiu, Jiaxiang Tang, Yangyi Huang, Justus Thies, and Michael J. Black. TADA! Text to animatable digital avatars. In 3DV, pages 15081519, 2024. 3 [15] Isabella Liu, Zhan Xu, Wang Yifan, Hao Tan, Zexiang Xu, Xiaolong Wang, Hao Su, and Zifan Shi. RigAnything: Template-free autoregressive rigging for diverse 3D assets. ACM TOG, 44(4):112, 2025. 1, 3 [16] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: In ICCV, pages 9298 Zero-shot one image to 3D object. 9309, 2023. 2 [17] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3D: SinIn CVPR, gle image to 3D using cross-domain diffusion. pages 99709980, 2024. 2 [18] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. SMPL: skinned multiperson linear model. ACM TOG, 34(6):248:1248:16, 2015. [19] Jing Ma and Dongliang Zhang. TARig: Adaptive templateaware neural rigging for humanoid characters. Computers & Graphics, 114:158167, 2023. 1, 3 [20] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. Expressive body capture: 3D hands, face, In CVPR, pages 10975 and body from single image. 10985, 2019. 3 [21] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. DreamFusion: Text-to-3D using 2D diffusion. arXiv preprint arXiv:2209.14988, 2022. 2 [22] Cedric Portaneri, Mael Rouxel-Labbe, Michael Hemmer, David Cohen-Steiner, and Pierre Alliez. Alpha wrapping with an offset. ACM TOG, 41(4):122, 2022. 6, 2 [23] Xuanchi Ren, Jiahui Huang, Xiaohui Zeng, Ken Museth, Sanja Fidler, and Francis Williams. XCube: Large-scale 3D generative modeling using sparse voxel hierarchies. In CVPR, pages 42094219, 2024. 2 [24] Tianchang Shen, Jacob Munkberg, Jon Hasselgren, Kangxue Yin, Zian Wang, Wenzheng Chen, Zan Gojcic, Sanja Fidler, 9 Nicholas Sharp, and Jun Gao. Flexible isosurface extraction for gradient-based mesh optimization. ACM TOG, 42(4):1 16, 2023. 3 [25] Chaoyue Song, Xiu Li, Fan Yang, Zhongcong Xu, Jiacheng Wei, Fayao Liu, Jiashi Feng, Guosheng Lin, and Jianfeng Puppeteer: Rig and animate your 3D models. Zhang. NeurIPS, 2025. 1, 3, 6, 7, 4 [26] Chaoyue Song, Jianfeng Zhang, Xiu Li, Fan Yang, Yiwen Chen, Zhongcong Xu, Jun Hao Liew, Xiaoyang Guo, Fayao Liu, Jiashi Feng, et al. MagicArticulate: Make your 3D models articulation-ready. In CVPR, pages 1599816007, 2025. 3 [27] Mingze Sun, Junhao Chen, Junting Dong, Yurun Chen, Xinyu Jiang, Shiwei Mao, Puhua Jiang, Jingbo Wang, Bo Dai, and Ruqi Huang. DRiVE: Diffusion-based rigging empowers generation of versatile and expressive characters. In CVPR, pages 2117021180, 2025. 1, 3 [28] Stanislaw Szymanowicz, Chrisitian Rupprecht, and Andrea Vedaldi. Splatter image: Ultra-fast single-view 3D reconstruction. In CVPR, pages 1020810217, 2024. [29] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. LGM: Large multi-view gaussian In ECCV, model for high-resolution 3D content creation. pages 118. Springer, 2024. 3 [30] Tencent Hunyuan3D Team. Hunyuan3D 2.1: From images to high-fidelity 3D assets with production-ready pbr material, 2025. 2, 4, 6, 1, 3 [31] Tencent Hunyuan3D Team. Hunyuan3D-Omni: unified framework for controllable generation of 3D assets, 2025. 2, 3, 6, 7 [32] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. VGGT: In CVPR, pages Visual geometry grounded transformer. 52945306, 2025. 3 [33] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. DUSt3R: Geometric 3D vision made easy. In CVPR, pages 2069720709, 2024. 3 [34] Zilong Wang, Zhiyang Dou, Yuan Liu, Cheng Lin, Xiao Dong, Yunhui Guo, Chenxu Zhang, Xin Li, Wenping Wang, and Xiaohu Guo. WonderHuman: Hallucinating unseen parts in dynamic 3D human reconstruction. arXiv preprint arXiv:2502.01045, 2025. [35] Xinyue Wei, Kai Zhang, Sai Bi, Hao Tan, Fujun Luan, Valentin Deschaintre, Kalyan Sunkavalli, Hao Su, and Zexiang Xu. MeshLRM: Large reconstruction model for highquality mesh. arXiv preprint arXiv:2404.12385, 2024. 3 [36] Zijie Wu, Chaohui Yu, Fan Wang, and Xiang Bai. AnimateAnyMesh: feed-forward 4D foundation model for text-driven universal mesh animation. In ICCV, 2025. 3 [37] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3D latents for scalable and versatile 3D generation. In CVPR, pages 2146921480, 2025. 2 [38] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. InstantMesh: Efficient 3D mesh generation from single image with sparse-view large arXiv preprint arxiv:2404.07191, reconstruction models. 2024. 3 [39] Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen, Ceyuan Yang, Sida Peng, Yujun Shen, and Gordon Wetzstein. GRM: Large gaussian reconstruction model for efIn ECCV, pages ficient 3D reconstruction and generation. 120. Springer, 2024. 3 [40] Zhan Xu, Yang Zhou, Evangelos Kalogerakis, Chris Landreth, and Karan Singh. RigNet: Neural rigging for articulated characters. ACM TOG, 39(4):58:58:158:58:14, 2020. 1, 3 [41] Hongyu Yan, Kunming Luo, Weiyu Li, Yixun Liang, Shengming Li, Jingwei Huang, Chunchao Guo, and Ping Tan. PoseMaster: Generating 3D characters in arbitrary poses from single image. arXiv preprint arXiv:2506.21076, 2025. 2, 3 [42] Xinhao Yan, Jiachen Xu, Yang Li, Changfeng Ma, Yunhan Yang, Chunshi Wang, Zibo Zhao, Zeqiang Lai, Yunfei Zhao, Zhuo Chen, et al. X-Part: high fidelity and structure coherent shape decomposition. arXiv preprint arXiv:2509.08643, 2025. [43] Yunhan Yang, Yuan-Chen Guo, Yukun Huang, Zi-Xin Zou, Zhipeng Yu, Yangguang Li, Yan-Pei Cao, and Xihui Liu. HoloPart: Generative 3D part amodal segmentation. arXiv preprint arXiv:2504.07943, 2025. 2 [44] Chongjie Ye, Yushuang Wu, Ziteng Lu, Jiahao Chang, Xiaoyang Guo, Jiaqing Zhou, Hao Zhao, and Xiaoguang Han. Hi3DGen: High-fidelity 3D geometry generation from images via normal bridging. arXiv preprint arXiv:2503.22236, 3:2, 2025. 2 [45] Zhiyuan Yu, Zhe Li, Hujun Bao, Can Yang, and Xiaowei Zhou. HumanRAM: Feed-forward human reconstruction In SIGGRAPH and animation model using transformers. Conference Proceedings, 2025. 3, 6 [46] Biao Zhang, Jiapeng Tang, Matthias Nießner, and Peter Wonka. 3DShape2VecSet: 3D shape representation for neural fields and generative diffusion models. ACM TOG, 42 (4):92:192:16, 2023. 2, 4, 6, 1 [47] Jiahui Zhang, Yuelei Li, Anpei Chen, Muyu Xu, Kunhao Liu, Jianyuan Wang, Xiao-Xiao Long, Hanxue Liang, Zexiang Xu, Hao Su, et al. Advances in feed-forward 3D reconstruction and view synthesis: survey. arXiv preprint arXiv:2507.14501, 2025. 3 [48] Jia-Peng Zhang, Cheng-Feng Pu, Meng-Hao Guo, Yan-Pei Cao, and Shi-Min Hu. One model to rig them all: Diverse skeleton rigging with UniRig. ACM TOG, 44(4):118, 2025. 1, [49] Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao, Kalyan Sunkavalli, and Zexiang Xu. GS-LRM: Large reconstruction model for 3D gaussian splatting. In ECCV, pages 119. Springer, 2024. 3 [50] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. CLAY: controllable large-scale generative model for creating high-quality 3D assets. ACM TOG, 43(4):120, 2024. 2 [51] Longwen Zhang, Qixuan Zhang, Haoran Jiang, Yinuo Bai, Wei Yang, Lan Xu, and Jingyi Yu. BANG: Dividing 3D assets via generative exploded dynamics. ACM TOG, 44(4): 121, 2025. 2 10 Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Implementation Details A.1.4. Adaptive Completion Module A.1. Model Details A.1.1. Shape VAE Our 3D shape VAE is adapted from the Hunyuan3D2.1 [30] architecture. The shape encoder processes an input of 32768 points uniformly sampled from the mesh surface, each with its coordinate and normal vector. These points are first downsampled to 4096 via Farthest Point Sampling (FPS). Subsequently, an 8-layer transformer with 16 attention heads performs subset-fullset cross-attention. The resulting 1024-dimensional tokens are then projected into 64-dimensional embeddings for both the mean and variance, producing VecSet [46] representation of sampled shape latents Zs RLd=409664. The shape decoder first projects these latents back to 1024-dimensional tokens, which are then processed by 16-layer, 16-head transformer using self-attention. Finally, query-based crossattention module and an MLP head decode the tokens into Signed Distance Function (SDF) values. We load the pretrained weights from Hunyuan3D-2.1 [30] and freeze the entire shape VAE during training. A.1.2. Skeleton Encoder As introduced in Sec. 3.2 of the main paper, we model skeleton as collection of bone cylinders. The radius of each cylinder is heuristically set to 25% of the corresponding bone length, value we empirically found to provide stable and expressive representation. Afterwards, to generate the skeleton latents, we project the sampled surface points onto these bone cylinders, creating per-bone point cloud. This point cloud is then processed by an encoding network that shares its architecture with the shape encoder. This architectural sharing allows for the reuse of the FPS indices from the shape encoder, thereby establishing dense correspondence between the skeleton and shape latents. A.1.3. Latent Posing Transformer The latent posing transformer consists of 24-layer transformer blocks with 8 attention heads and token dimension of 512. QK-Normalization is adopted in each attention block. It processes concatenated sequence of source tokens (projected from source shape and skeleton latents) and target tokens (projected from target skeleton latents). To handle multiple target poses in parallel, their corresponding target token sequences are stacked along the batch dimension, and the source tokens are repeated accordingly, which is similar to the implementation in [10]. The adaptive completion module has an architecture similar to the latent posing transformer but consists of fewer layers. It utilizes in-context attention to produce 128 adaptive tokens (approximately 3.1% of the main shape tokens) conditioned on the context, including source input, target input, and target output tokens. A.1.5. Training We train our model in two stages using the AdamW optimizer. In both stages, the learning rate is linearly warmed up to 1e-4 over the first 1% of training iterations, followed by cosine decay schedule to minimum of 1e-5. The first training stage runs for 5 epochs, and the subsequent finetuning stage runs for 3 epochs. A.2. Experiment Details A.2.1. Data During training, we augment our data by synthesizing random, anatomically plausible finger poses to compensate for the limited range of finger motion in the original dataset. Furthermore, following the approach in [6], we apply canonical global transformation to align the orientation of the input shape, which effectively facilitates the pose modeling by simplifying its spatial distribution. A.2.2. Applications Here we provide more implementation details about some of the applications discussed in Sec. 4.3 of the main paper. Part replacement. Consider the part replacement scenario illustrated in Fig. 5 of the main paper, where source character is to be fitted with new hands imported into the 3D space. Our framework obviates the need for manual mesh editing operations like cutting off the original hands and merging the new ones. Instead, the user only needs to perform two simple steps on the skeleton: 1) Reposition the original hand bones to align with the new hand geometries, forming modified source skeleton. It is permissible for these bones to be detached from the characters body (e.g., floating hands). 2) Roughly align the hand bones with the arm bones to form the target skeleton. This alignment is straightforward as the bones are represented by regular cylinders. During inference, the association between the hand bones and the new hand geometries ensures that the original hands are automatically ignored and replaced by the new ones in the final posed shape. Part refinement. The part refinement application is enabled by an additional finetuning stage. During this stage, Table S1. Additional ablation results. B denotes removing design and adding as an alternative. Module/Stage Setting SDF-RMSE Latent Posing Transformer mean & variance mean shape latents mean & variance sampled shape latents Finetuning 128 0 adaptive tokens (w/o finetuning) 128 0 adaptive tokens (full finetuning, SDF loss) 128 0 adaptive tokens (full finetuning, latent loss) 128 64 adaptive tokens 128 256 adaptive tokens postpre-transformer adaptive tokens Ours (full model) 0.0264 0.0345 0.0186 0.0402 0.0182 0.0179 0.0164 0.0170 0. we override the source shape latents corresponding to the specific body part to be refined (i.e., hands in our experiments) with zeros, while the ground-truth target shape remains unchanged. As result, the input tokens for the source part contain only skeletal pose information, with no shape data. The latent posing transformer learns to interpret this pattern as signal to recover the missing geometry for the target part, in addition to its primary posing task. Once finetuned, the model can refine the input characters hands by reconstructing well-articulated fingers under the guidance of the provided skeleton. A.2.3. Ablation Studies We provide more detailed explanation of the model variants mentioned in Sec. 4.4 and Tab. 2 from the main paper. Blend-weight embedding. In this variant, we assign learnable embedding to each bone. The embedding for given point on the skeleton is then computed as weighted sum of these per-bone embeddings, with the weights determined by the skinning weights. However, this design is heavily dependent on the quality of the skinning weights and restricts the framework to fixed skeleton template. Latent loss finetuning. To enable direct latent-space supervision for the newly introduced adaptive tokens, it is necessary to define sequence order for them, which is nontrivial. In our experiments, we employ simple heuristic, ordering the tokens based on the spatial coordinates of their corresponding subsampled points. This design requires the model to identify the dynamic spatial arrangement of the new tokens, which is challenging task less effective than our adopted geometry-space finetuning. B. Additional Experiments and Visualization B.1. Additional Ablation Studies We present additional ablation studies to complement those in Sec. 4.4 of the main paper. The results are summarized in Tab. S1. Shape encoding. Our latent posing transformer manipulates the latent representation of an input 3D shape, which is encoded by shape VAE. VAE typically encodes shape into distribution characterized by mean and variance vectors, from which the final shape latents are sampled. Our model utilizes both the mean and variance vectors to represent the shape. We ablate this design by using only the mean vectors (i.e., zero variance) or the sampled latents. We find that neither variant provides sufficiently rich representation for effective pose transformation. Both alternatives often result in fragmented mesh surfaces, and in some cases, the marching cubes algorithm even fails to extract valid mesh from their decoded SDFs. Adaptive completion. Our finetuning approach with 128 adaptive tokens effectively addresses the challenge of generating newly exposed structures. Direct full finetuning using geometry-space SDF loss leads to collapse in performance, suffering from the transformation path ambiguity issue discussed in Sec. 3.4 of the main paper. Meanwhile, finetuning using latent loss without new tokens forces the original shape tokens to represent more geometric areas, an ill-posed setting with marginal improvement. We also observe that using fewer tokens (e.g., 64) provides insufficient capacity to capture all necessary details. Conversely, increasing the number of tokens to 256 offers no significant benefit and sometimes introduces minor floating artifacts. Furthermore, injecting the adaptive tokens after the posing transformer (post-transformer) is more effective than injecting them before (pre-transformer). This is intuitive because the information required for the missing structures can be more accurately identified and defined after the pose transformation has been applied. B.2. Visualization of Newly Exposed Structures Fig. S1 illustrates the challenge of newly exposed structures that arise when re-posing character, particularly when limbs are initially in close contact. Our preprocessing, which involves remeshing step using the alpha wrap algorithm [22], removes internal or self-contacting surfaces from the source mesh. Consequently, regions like the armpits are not sampled by the shape encoder, as shown in the top row of Fig. S1. This issue is common for inthe-wild 3D character assets, especially AI-generated ones, which often have topological defects like fused body parts. To address this, we introduce adaptive tokens during finetuning stage. The model, already proficient in posing from its initial training, is further guided by an SDF loss during finetuning. This effectively directs the adaptive completion module to generate plausible geometry for these previously non-existent surfaces. The bottom row of Fig. S1 clearly demonstrates the effectiveness of this design. B.3. Results with Texture In this work, we primarily focus on shape transformation and leave appearance modeling for future work. Nevertheless, to demonstrate the potential of our posed shapes for primarily guides the color and style rather than the geometry. Consequently, the character pose in the reference image can differ from that of the input shape while still yielding plausible textured results. C. Skinning Issue of Geometry-Space Rigging Accurate skinning weights are crucial for high-quality character posing in traditional pipelines. However, even stateof-the-art geometry-space rigging methods struggle to produce satisfactory skinning results, as illustrated in Fig. S3. While MIA [6] introduces normal-based geometric features to improve skinning quality and outperforms previous methods [19, 40], it still suffers from skinning leakage issue that leads to unnatural deformations. The more recent work, Puppeteer [25], despite employing an advanced skinning predictor that leverages segmentation and mesh-topology priors, still exhibits substantial skinning imperfections like spiking artifacts. Moreover, while segmentation priors can mitigate skinning leakage in some scenarios, they may also introduce new artifacts due to overly strong assumptions, resulting in rigid and torn deformations (see Fig. 4 from the main paper). Therefore, robustly predicting high-quality skinning weights for arbitrary 3D characters remains an open challenge. This limitation motivates our exploration of latentspace pose manipulation, which operates in skinning-free manner. More crucially, all geometry-based rigging methods are fundamentally unable to correct or handle topological imperfections (e.g., fused limbs or self-occlusions) in the source mesh, challenge our proposed method robustly overcomes. D. More about Limitations and Future Work This section further discusses the limitations of our current work and outlines potential directions for future research, in addition to Sec. 5 of the main paper. primary limitation is that the level of detail in the output mesh is constrained by the resolution of the VecSetbased VAE. Although the adopted Hunyuan3D-2.1 [30] architecture is powerful shape VAE, any high-frequency details from the input mesh that cannot be faithfully reconstructed through its identity encoding-decoding process will also be lost during pose transformation. Integrating more advanced shape representations (e.g., [24]) presents promising solution. Our framework currently focuses on shape deformation. While we demonstrate textured results in Fig. S2, this is achieved via workaround using an external texture generation model. natural extension of our work would be to integrate appearance features into the VecSet latent space, which would enable the seamless generation of fullytextured animations. Figure S1. Visualization of newly exposed structures. Top row: The yellow areas highlight newly exposed surfaces (i.e., armpits) in the target pose that are not sampled from the source shape due to self-contact. Our adaptive tokens are trained to reconstruct these missing regions. Bottom row: comparison of inference results with and without adaptive tokens. The model with adaptive tokens successfully reconstructs plausible shapes, whereas the model without them fails to produce the complete geometry. Figure S2. Textured results. We apply textures to our generated posed shapes using Hunyuan3D-Paint [30] to demonstrate one feasible solution for realistic rendering. realistic rendering, we provide feasible solution of applying textures using Hunyuan3D-Paint [30], diffusion-based PBR material generation framework. The results are visualized in Fig. S2. Hunyuan3D-Paint [30] generates albedo, roughness, and metallic maps from multiple viewpoints, conditioned on the 3D shape (via its rendered normal map) and reference image. Although this framework is designed for static assets, we observe that the reference image Figure S3. Skinning artifacts in geometry-space rigging methods. The right hand of the tiger is positioned very close to its head (without making contact though). When re-posed, MIA [6] exhibits skinning leakage, causing some hand vertices to deform unnaturally with the head. Puppeteer [25] also suffers from noticeable spiking artifacts due to the leaked skinning weights from hand to face. In contrast, our method, which is not constrained by skinning, produces clean posing result. Tasks such as creating newly exposed surfaces or refining parts are not always regressive. Our solution recovers plausible missing geometry based on patterns observed during training, rather than truly generating novel structures. Future work could address this by incorporating generative module that leverages additional context, such as text prompts or reference images, similar to the approach in [4]. Our data-driven method implicitly learns the skinning process. Although our training data is constructed using linear blend skinning, the framework is not limited to this method. With sufficient training data, it could potentially learn more complex deformation behaviors, such as muscle dynamics. Finally, the principles of our posing method could inspire broader applications in latent-space 3D object editing. An interesting future direction is to explore the use of other dense control signals, like bounding boxes, to achieve more general and versatile shape manipulation."
        }
    ],
    "affiliations": [
        "CAS Key Laboratory of Technology in GIPAS, EEIS Department, University of Science and Technology of China"
    ]
}