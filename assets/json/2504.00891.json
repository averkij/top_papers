{
    "paper_title": "GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning",
    "authors": [
        "Jian Zhao",
        "Runze Liu",
        "Kaiyan Zhang",
        "Zhimu Zhou",
        "Junqi Gao",
        "Dong Li",
        "Jiafei Lyu",
        "Zhouyi Qian",
        "Biqing Qi",
        "Xiu Li",
        "Bowen Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in Large Language Models (LLMs) have shown that it is promising to utilize Process Reward Models (PRMs) as verifiers to enhance the performance of LLMs. However, current PRMs face three key challenges: (1) limited process supervision and generalization capabilities, (2) dependence on scalar value prediction without leveraging the generative abilities of LLMs, and (3) inability to scale the test-time compute of PRMs. In this work, we introduce GenPRM, a generative process reward model that performs explicit Chain-of-Thought (CoT) reasoning with code verification before providing judgment for each reasoning step. To obtain high-quality process supervision labels and rationale data, we propose Relative Progress Estimation (RPE) and a rationale synthesis framework that incorporates code verification. Experimental results on ProcessBench and several mathematical reasoning tasks show that GenPRM significantly outperforms prior PRMs with only 23K training data from MATH dataset. Through test-time scaling, a 1.5B GenPRM outperforms GPT-4o, and a 7B GenPRM surpasses Qwen2.5-Math-PRM-72B on ProcessBench. Additionally, GenPRM demonstrates strong abilities to serve as a critic model for policy model refinement. This work establishes a new paradigm for process supervision that bridges the gap between PRMs and critic models in LLMs. Our code, model, and data will be available in https://ryanliu112.github.io/GenPRM."
        },
        {
            "title": "Start",
            "content": "2025-4-2 GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning Jian Zhao1,3*, Runze Liu1,2*, Kaiyan Zhang1, Zhimu Zhou3, Junqi Gao4, Dong Li4, Jiafei Lyu1, Zhouyi Qian4, Biqing Qi2, Xiu Li1 and Bowen Zhou1,2 1Tsinghua University, 2Shanghai AI Laboratory, 3BUPT, 4Harbin Institute of Technology 5 2 0 2 1 ] . [ 1 1 9 8 0 0 . 4 0 5 2 : r Recent advancements in Large Language Models (LLMs) have shown that it is promising to utilize Process Reward Models (PRMs) as verifiers to enhance the performance of LLMs. However, current PRMs face three key challenges: (1) limited process supervision and generalization capabilities, (2) dependence on scalar value prediction without leveraging the generative abilities of LLMs, and (3) inability to scale the test-time compute of PRMs. In this work, we introduce GenPRM, generative process reward model that performs explicit Chain-ofThought (CoT) reasoning with code verification before providing judgment for each reasoning step. To obtain high-quality process supervision labels and rationale data, we propose Relative Progress Estimation (RPE) and rationale synthesis framework that incorporates code verification. Experimental results on ProcessBench and several mathematical reasoning tasks show that GenPRM significantly outperforms prior PRMs with only 23K training data from MATH dataset. Through test-time scaling, 1.5B GenPRM outperforms GPT-4o, and 7B GenPRM surpasses Qwen2.5-Math-PRM-72B on ProcessBench. Additionally, GenPRM demonstrates strong abilities to serve as critic model for policy model refinement. This work establishes new paradigm for process supervision that bridges the gap between PRMs and critic models in LLMs. Our code, model, and data will be available in https://ryanliu112.github.io/GenPRM. 1. Introduction Large Language Models (LLMs) have shown significant advances in recent years (OpenAI, 2023; Anthropic, 2023; OpenAI, 2024a,b; DeepSeek-AI et al., 2025). As OpenAI o1 demonstrates the great effectiveness of scaling test-time compute (OpenAI, 2024a), an increasing number of researches focus on Test-Time Scaling (TTS) methods to improve the reasoning performance of LLMs (Snell et al., 2025; Liu et al., 2025). Effective TTS requires high-quality verifiers, such as Process Reward Models (PRMs) (Liu et al., 2025). However, existing PRMs face several limitations. They exhibit limited process supervision capabilities and struggle to generalize across different models and tasks (Zheng et al., 2024; Zhang et al., 2025c; Liu et al., 2025). Furthermore, most current approaches train PRMs as classifiers that output scalar values, neglecting the natural language generation abilities of LLMs, which are pre-trained on extensive corpora. This classifier-based modeling inherently prevents PRMs from leveraging test-time scaling methods to enhance process supervision capabilities. These limitations lead us to the following research question: How can generative modeling enhance the process supervision capabilities of PRMs while enabling test-time scaling? In this work, we address these challenges through generative process verification framework that introduces three key innovations: (1) multi-step reasoning process that integrates natural * Equal contribution Project lead & Work done during an internship at Shanghai AI Laboratory Corresponding authors: Biqing Qi (qibiqing@pjlab.org.cn), Xiu Li (li.xiu@sz.tsinghua.edu.cn), and Bowen Zhou (zhoubowen@tsinghua.edu.cn) GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning Figure 1: Comparison between GenPRM (right) and previous classification-based PRMs (left). language reasoning (Chain-of-Thought (CoT) (Wei et al., 2022)) with code generation and execution, (2) Relative Progress Estimation (RPE) combined with novel rationale data synthesis framework, and (3) Flexible computational scaling through sampling and reasoning component selection. Our approach redefines process supervision as generative task rather than discriminative scoring task. comparison of our method with previous classification-based methods is presented in Figure 1. Our contributions can be summarized as follows: 1. We propose generative process reward model that performs explicit CoT reasoning and code verification and utilizes Relative Progress Estimation for accurate label prediction. 2. Empirical results on ProcessBench and common mathematical reasoning tasks demonstrate that GenPRM outperforms prior classification-based PRMs. Additionally, smaller GenPRM models can surpass larger PRMs using TTS. 3. We offer fresh perspective on PRMs, fully leveraging their TTS capabilities, reshaping their application, and providing new directions for future research in process supervision. 2. Preliminaries 2.1. Markov Decision Process Following Liu et al. (2025), we formulate the test-time scaling process with PRMs as Markov Decision Process (MDP) defined by (𝒮, 𝒜, 𝑃, 𝑟, 𝛾), where 𝒮 is the state space, 𝒜 is the action space, 𝑃 represents transition dynamics, 𝑟 : 𝒮 𝒜 is the reward function, and 𝛾 [0, 1] is the discount factor. Starting with prompt set 𝒳 and an initial state 𝑠1 = 𝑥 𝒳 , the policy model 𝜋𝜃 generates an action 𝑎1 𝜋𝜃( 𝑠1).1 Unlike traditional RL methods with stochastic transitions (Liu et al., 2022, 2024), transitions in LLMs are deterministic, i.e., 𝑠𝑡+1 = 𝑃 ( 𝑠𝑡, 𝑎𝑡) = [𝑠𝑡, 𝑎𝑡], where [, ] denotes string concatenation. This process continues until the episode terminates (i.e., generating the [EOS] token), obtaining trajectory of 𝑇 steps: 𝜏 = {𝑎1, 𝑎2, , 𝑎𝑇 }. The goal is to optimize either the reward of each step (as in search-based methods) or the reward over the full response (as in Best-of-N sampling). 1Following Snell et al. (2025); Liu et al. (2025), we refer to models that generate solutions as policy models. 2 GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning Figure 2: Overall framework of GenPRM. 2.2. Supervised Fine-Tuning Supervised Fine-Tuning (SFT) trains model to predict the next token based on prior context. For dataset 𝒟SFT = {(𝑥(𝑖), 𝑦(𝑖))}𝑁 , the SFT loss is: 𝑖=1 ℒSFT(𝜃) = E(𝑥,𝑦)𝒟SFT 𝑦 log 𝜋𝜃(𝑦𝑡 𝑥, 𝑦1:𝑡1) , (1) where 𝜋𝜃 represents model with parameters 𝜃. 𝑡=1 2.3. Test-Time Scaling In this work, we consider two test-time scaling methods, including majority voting and Best-of-N. Majority Voting. Majority voting (Wang et al., 2023) selects the answer that appears the most frequently among all solutions. Best-of-N. Best-of-N (BoN) (Brown et al., 2024; Snell et al., 2025) selects the best answer from 𝑁 candidate solutions. 3. Method In this section, we first describe how to develop GenPRM and integrate the reasoning process with code verification. We then introduce how to scale test-time compute of policy models using GenPRM and apply TTS for GenPRM. Last, we present the improved label estimation method and data generation and filtering framework of GenPRM. 3 GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning 3.1. GenPRM and Test-Time Scaling 3.1.1. From Discriminative PRM to Generative PRM Discriminative PRM. Assume we have PRM dataset 𝒟Disc = {(𝑠𝑡, 𝑎𝑡), 𝑟𝑡}, where 𝑟𝑡 {0, 1} for PRM labels with hard estimation. The discriminative PRM 𝑟𝜓 is trained via cross-entropy loss (Skywork o1 Team, 2024; Zhang et al., 2025c): ℒCE(𝜓) = E(𝑠𝑡,𝑎𝑡,𝑟𝑡)𝒟Disc [𝑟𝑡 log 𝑟𝜓(𝑠𝑡, 𝑎𝑡) + (1 𝑟𝑡) log(1 𝑟𝜓(𝑠𝑡, 𝑎𝑡))] . (2) Direct Generative PRM. With dataset 𝒟Direct-Gen = {(𝑠𝑡, 𝑎𝑡), 𝑟𝑡}, where 𝑟𝑡 is Yes for correct step and No otherwise, the direct generative PRM (Xiong et al., 2024) is trained through SFT to predict Yes or No for each step. For step 𝑡, we use the probability of the Yes token as the predicted process reward ˆ𝑟𝑡: ˆ𝑟𝑡 = 𝑟𝜓(Yes 𝑠𝑡, 𝑎𝑡). (3) Generative PRM. By equipping the direct generative PRM with an explicit reasoning process like CoT (Wei et al., 2022), we obtain generative PRM. Let 𝑣1:𝑡1 denote the rationale from step 1 to 𝑡 1 and 𝑣𝑡 denote the rationale for step 𝑡. Assume we have dataset 𝒟Gen = {(𝑠𝑡, 𝑎𝑡, 𝑣1:𝑡1), (𝑣𝑡, 𝑟𝑡)}. GenPRM learns to reason and verify each step via SFT on this dataset. The generative process reward ˆ𝑟𝑡 can be obtained via the following equation: ˆ𝑟𝑡 = 𝑟𝜓(Yes 𝑠𝑡, 𝑎𝑡, 𝑣1:𝑡1, 𝑣𝑡), where 𝑣𝑡 𝑟𝜓( 𝑠𝑡, 𝑎𝑡, 𝑣1:𝑡1) (4) If we only verify the reasoning step with CoT based on Generative PRM with Code Verification. natural language, the process may lack robustness in certain complex scenarios (Zhu et al., 2024; Gou et al., 2024). The difference between the generative PRM and the generative PRM with code verification is that the latter generates code to verify the reasoning step by executing it and provides the judgment based on the execution results. At step 𝑡, after generating the rationale 𝑣𝑡 containing CoT and code, we execute the code and obtain feedback 𝑓𝑡. Given the current state 𝑠𝑡, action 𝑎𝑡, previous rationales 𝑣1:𝑡1, and previous corresponding execution feedback 𝑓1:𝑡1, the PRM first generates the rationale 𝑣𝑡. After execution and obtaining the feedback 𝑓𝑡, we compute the final generative process reward as follows: ˆ𝑟𝑡 = 𝑟𝜓(Yes 𝑠𝑡, 𝑎𝑡, 𝑣1:𝑡1, 𝑓1:𝑡1, 𝑣𝑡, 𝑓𝑡), where 𝑣𝑡 𝑟𝜓( 𝑠𝑡, 𝑎𝑡, 𝑣1:𝑡1, 𝑓1:𝑡1) (5) In the following sections, we refer to GenPRM as this generative PRM type with code verification. The effectiveness of CoT and code verification can be found in Section 4.4. 3.1.2. Test-Time Scaling Policy Model TTS: GenPRM as Verifier. To scale the test-time compute of policy models, we can sampling multiple responses from policy models and then use GenPRM as verifier to select the final answer (Snell et al., 2025) in the way of parallel TTS. Policy Model TTS: GenPRM as Critic. By equipping the PRM with generative process supervision abilities, GenPRM can be naturally used as critic model to refine the outputs of policy models and we can scale the refinement process with multiple turns in sequential TTS manner. 4 GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning GenPRM TTS. When evaluating each solution step, we first sample 𝑁 reasoning verification paths and then use majority voting to obtain the final prediction by averaging the rewards. For GenPRM without code verification, the rewards are computed as follows: ˆ𝑟𝑡 = 1 𝑁 𝑁 𝑖=1 𝑟𝜓(Yes 𝑠𝑡, 𝑎𝑡, 𝑣𝑖 1:𝑡1, 𝑣𝑖 𝑡). (6) And we can further incorporate code verification and execution feedback into this reasoning process: ˆ𝑟𝑡 = 1 𝑁 𝑁 𝑖=1 𝑟𝜓(Yes 𝑠𝑡, 𝑎𝑡, 𝑣𝑖 1:𝑡1, 𝑓 𝑖 1:𝑡1, 𝑣𝑖 𝑡, 𝑓 𝑖 𝑡 ). (7) Then the rewards can be used for ranking the responses of policy models or be converted into binary labels through threshold 0.5 for judging the correctness of the step. The discussion of using code verification can be found at Table 5. 3.2. Synthesizing Data of GenPRM In this section, we introduce our pipeline for synthesizing training data of GenPRM. The pipeline consists of three stages: (1) generating reasoning paths and obtaining PRM labels via Monte Carlo (MC) estimation; (2) evaluating the progress of each step via Relative Progress Estimation; and (3) synthesizing rationales with CoT and code verification, and inferring LLM-as-a-judge labels with consensus filtering. 3.2.1. Solution Generation and Monte Carlo Estimation Solution Generation with Step Forcing. We utilize the 7.5K problems from the training set of the MATH dataset (Hendrycks et al., 2021) as the problem set. For each problem, we use Qwen2.5-7BInstruct (Yang et al., 2024a) as the generation model to collect multiple solutions. Since using nn for step division does not consider the semantics of each step and may result in overly fine-grained division, we apply step forcing approach to generate solutions. Specifically, we add Step 1: as the prefix for the generation model to complete the response. For response with 𝑇 reasoning steps, the format is as follows: The response format with step forcing Step 1: {step content} ... Step T: {step content} The proportion of correct paths versus incorrect paths varies significantly depending on the difficulty of the problems. To ensure sufficient number of correct and incorrect paths, we sample up to 2048 paths for both hard and easy problems. If no correct or incorrect paths are found after sampling 2048 responses, we discard the corresponding problems. Balancing the Precision and Efficiency of Monte Carlo Estimation. Following Math-Shepherd (Wang et al., 2024b), we estimate the probability of correctness for each step using completion-based sampling. For each reasoning step 𝑠𝑡, we generate 𝐾 completion trajectories using completion model, GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning specifically Qwen2.5-Math-7B-Instruct (Yang et al., 2024b), and use MC estimation to calculate the probability that the current step 𝑎𝑡 is correct (Wang et al., 2024b; Zhang et al., 2025c): 𝑀 𝐶(𝑠𝑡, 𝑎𝑡) = 𝑀 𝐶(𝑠𝑡+1) = 1 𝐾 𝐾 𝑗= 1(𝑞𝑗 = 𝑞*), (8) where 𝑞𝑗 is the answer of the 𝑗-th response, 𝑞* is the ground-truth answer, and 1 is the indicator function. However, it is difficult for the completion model to reach the correct answer for hard problems even when the original step is correct, leading to incorrect results for MC estimation. To address this and balance the computation cost, we use dynamic 𝐾 based on the estimated Pass@1 𝑀 𝐶(𝑠1): 128 𝐾 = 64 if 0 𝑀 𝐶(𝑠1) < 0.1, if 0.1 𝑀 𝐶(𝑠1) < 0.9, if 0.9 𝑀 𝐶(𝑠1) < 1. (9) 32 3.2.2. Relative Progress Estimation Previous work has shown that hard label estimation is better than soft label estimation for PRMs (Zhang et al., 2025c). However, after MC estimation, we observe that although the MC score of many steps is greater than 0, the steps are incorrect, as also noted by Zhang et al. (2025c). On the other hand, we assume that positive step should be both correct and beneficial. reasoning step is considered as beneficial one if it is easier to reach the correct answer by adding this step as the generation prefix. To address these issues, we propose Relative Progress Estimation (RPE), which shares similar idea with relative advantage estimation in GRPO (Shao et al., 2024; DeepSeek-AI et al., 2025), to improve conventional hard label estimation. Specifically, the MC score is an empirical estimation of the current state 𝑠𝑡. To evaluate the quality of the current action 𝑎𝑡, it is natural to compare the MC score of the next state 𝑠𝑡+1 with that of the current state 𝑠𝑡, since 𝑠𝑡+1 = [𝑠𝑡, 𝑎𝑡]. For each response, if the first erroneous step is step 𝑡 (i.e., 𝑀 𝐶(𝑠𝑡) = 0), we set the MC score of the following steps to 0. Our RPE 𝑃𝑡 for step 𝑡 is defined as follows: 𝑃𝑡 = 𝑀 𝐶(𝑠𝑡, 𝑎𝑡) 𝑀 𝐶(𝑠𝑡) , (10) where 𝑀 𝐶(𝑠1) is the estimated Pass@1 computed in the solution generation phase. However, we empirically find that using strict criterion where progress is always greater than 1 leads to unsatisfactory performance, as shown in Table 3. To address this, we estimate the final reward label ˆ𝑟𝑡 by introducing threshold 𝜖: ˆ𝑟𝑡 = { 1 0 if 𝑃𝑡 𝜖, otherwise. (11) We also discuss another form of relative progress 𝑃𝑡 = 𝑀 𝐶(𝑠𝑡, 𝑎𝑡) 𝑀 𝐶(𝑠𝑡) in Table 3 in Section 4.4. 3.2.3. Rationale Generation, Verification and Filtering To obtain high-quality rationale data, we use QwQ-32B (Qwen Team, 2025) as the rationale generation model and introduce three-step pipeline that automatically generates and verifies the rationale of each reasoning step. Given problem 𝑥 with ground-truth answer 𝑞* and candidate steps {𝑎1, , 𝑎𝑇 }, the generation and verification proceed as follows: 6 GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning Step 1: Code-Based Rationale Generation. To evaluate the correctness of 𝑎𝑡, we synthesize step-by-step CoT analysis. It has been shown that program-based reasoning improves verification outcomes (Zhu et al., 2024). Based on CoT analysis, we continue to synthesize code-based rationales to verify 𝑎𝑡 based on the problem and historical steps {𝑎1, , 𝑎𝑡1}. We prompt the rationale generation model to surround the CoT with <analyze> and </analyze>, and the code with <verify> and </verify>. The prompt for rationale generation is shown in Table A.2. Step 2: Code Execution and Verification. With generated code, we execute it and obtain the feedback 𝑓𝑡 for step 𝑡. The execution feedback is formatted as [Code output: {execution result}] and is concatenated to the generated CoT and code as the prefix for the subsequent generation. If the execution result is inconsistent with the generated CoT verification, we observe that QwQ-32B performs self-reflection behaviors until reaching consensus. Step 3: Label Judgment and Consensus Filtering. After generating and verifying the rationale data of all candidate steps, the rationale generation model finally outputs an number. If all steps are inferred to be correct, the number will be -1, otherwise will be the index of the first erroneous step. For each solution, if there is at least one process label with RPE is not consistent with the labels generated by LLM-as-a-judge (Zheng et al., 2023), we discard the entire solution and only retain the one with all labels consistent. After consensus filtering, we discard approximately 51% of the data and finally obtain dataset containing 23K problems with reasoning steps and rationale data. 4. Experiments In this section, we aim to answer the following questions: Q1: How does GenPRM perform compared with previous PRMs? (4.2, 4.3) Q2: How does the performance of GenPRM scale with more test-time compute? (4.2, 4.3) Q3: How does GenPRM benefit policy model test-time scaling? (4.3) Q4: How do the components and hyperparameters influence GenPRM? (4.4) 4.1. Setup Benchmarks. GenPRM and baseline methods are evaluated on ProcessBench (Zheng et al., 2024), benchmark designed to measure process supervision abilities on mathematical reasoning tasks.2 Additionally, we use MATH (Hendrycks et al., 2021), AMC23 (AI-MO, 2024b), AIME24 (AI-MO, 2024a), and Minerva Math (Lewkowycz et al., 2022) for BoN experiments. For BoN response generation, we adopt Qwen2.5-Math-7B-Instruct (Yang et al., 2024b) and Gemma-3-12b-it (Gemma Team and Google DeepMind, 2025). Baselines. We compare GenPRM with the following methods: Math-Shepherd-PRM-7B (Wang et al., 2024b): This method trains PRM using data collected through MC estimation. RLHFlow series (Xiong et al., 2024): Includes RLHFlow-PRM-Mistral-8B and RLHFlow-PRMDeepseek-8B. 2The evaluation code for ProcessBench is based on https://github.com/QwenLM/ProcessBench. 7 GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning Skywork-PRM series (Skywork o1 Team, 2024): Includes Skywork-PRM-1.5B and SkyworkPRM-7B. EurusPRM (Cui et al., 2025): EurusPRM-Stage1 and EurusPRM-Stage2 are trained as implicit PRMs (Yuan et al., 2024). Qwen2.5-Math series (Zheng et al., 2024; Zhang et al., 2025c): Qwen2.5-Math-7B-MathShepherd and Qwen2.5-Math-7B-PRM800K are trained with Math-Shepherd (Wang et al., 2024b) and PRM800K (Lightman et al., 2024), respectively. For Qwen2.5-Math-PRM-7B and Qwen2.5-Math-PRM-72B, the training data is applied consensus filtering using LLM-as-ajudge (Zheng et al., 2023). RetrievalPRM-7B (Zhu et al., 2025): The method enhances PRM with retrieved questions and corresponding steps. Universal-PRM-7B (Tan et al., 2025): The method proposes an automated framework using ensemble prompting and reverse verification. Dyve-14B (Zhong et al., 2025): This method applies adaptive verification (fast or slow) for each reasoning step. Direct Generative PRM-7B: The method trains direct generative PRM with the original language head via SFT using the same data as GenPRM, but without CoT and code verification. Implementation Details. For relative progress estimation, we use 𝜖 = 0.8 for all experiments and the ablation study on this can be found in Section 4.4. For rationale generation, we use QwQ-32B (Qwen Team, 2025). We use DeepSeek-R1-Distill series models (DeepSeek-AI et al., 2025) as the base models of GenPRM, including parameters of 1.5B, 7B, and 32B. For GenPRM training, the batch size is 64 and the learning rate is 2.0𝑒 6. For GenPRM evaluation, we use temperature of 0.6. 4.2. ProcessBench Results GenPRM outperforms classification-based PRMs on ProcessBench. As shown in Table 1, GenPRM7B significantly outperforms direct generative PRM and surpasses all previous PRMs with parameters less than 72B on ProcessBench. Also, GenPRM-1.5B outperforms Skywork-PRM-1.5B by large margin. It is noteworthy that GenPRM is trained with merely 23K data from MATH (Hendrycks et al., 2021) only. By comparing the detailed results in Table 6, we can find that the performance gain of GenPRM mainly comes from the stronger abilities of finding erroneous steps and we provide concrete cases in Appendix C. These results demonstrating the superiority of generative modeling of PRM. GenPRM enables smaller PRMs surpass 10 larger PRMs and GPT-4o via TTS. We also compare the TTS results of GenPRM in Table 1 and find that GenPRM-1.5B surpasses GPT-4 and GenPRM-7B exceeds Qwen2.5-Math-PRM-72B on ProcessBench via simply majority voting, showing that scaling test-time compute is highly effective for GenPRM. We also find that the performance improvement of scaling the test-time compute on harder problems is larger than that of easier questions. 4.3. Policy Model Test-Time Scaling Results GenPRM as Verifier. The results in Figure 3 (a)-(d) show that GenPRM outperforms the baselines on MATH, AMC23, AIME24, and Minerva Math with Qwen2.5-Math-7B-Instruct (Yang et al., 2024b) as the generation model. The advantage of GenPRM becomes larger by scaling the test-time compute of GenPRM. Figure 3 (e) demonstrates that GenPRM generalizes well to responses with Gemma-312b-it (Gemma Team and Google DeepMind, 2025) as the generation model. 8 GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning Table 1: ProcessBench results reported with F1 scores. The results of GenPRM are shaded . For 1.5B PRMs, bold indicates the best Pass@1 or scores superior to GPT-4o. For 7-8B PRMs, bold denotes the best Pass@1 or scores superior to Qwen2.5-Math-PRM-72B. # of Samples GSM8K MATH Olympiad Bench OmniMATH Avg. Model GPT-4o-0806 o1-mini Skywork-PRM-1.5B GenPRM-1.5B (Pass@1) GenPRM-1.5B (Maj@8) Proprietary LLMs (Critic) unk unk 79.2 93.2 63.6 88. PRMs (1.5B) unk 23K 23K 59.0 52.8 51.3 PRMs (7-8B) Math-Shepherd-PRM-7B RLHFlow-PRM-Mistral-8B RLHFlow-PRM-Deepseek-8B Skywork-PRM-7B EurusPRM-Stage1 EurusPRM-Stage2 Qwen2.5-Math-7B-Math-Shepherd Qwen2.5-Math-7B-PRM800K Qwen2.5-Math-PRM-7B RetrievalPRM-7B Universal-PRM-7B Direct Generative PRM-7B GenPRM-7B (Pass@1) GenPRM-7B (Maj@8) 445K 273K 253K unk 463K 30K 445K 264K 344K 404K unk 23K 23K 23K 47.9 50.4 38.8 70.8 44.3 47.3 62.5 68.2 82.4 74.6 85.8 63.9 78.7 81.0 Dyve-14B Qwen2.5-Math-PRM-72B GenPRM-32B (Pass@1) GenPRM-32B (Maj@8) PRMs (14-72B) 117K 344K 23K 23K 68.5 87.3 83.1 85.1 51.4 87. 19.3 55.1 65.3 24.8 13.8 16.9 22.9 21.7 21.2 13.7 50.7 67.5 60.2 67.6 54.5 72.2 78.4 49.0 74.3 72.8 78.9 53.5 82.4 61.9 87.9 19.2 54.5 62. 23.8 15.8 16.9 21.0 23.1 20.9 7.7 44.3 66.3 57.3 66.4 55.9 69.8 76.8 47.2 71.1 72.8 80.1 36.4 57.3 63.4 31.5 28.4 26.6 42.1 31.2 31.3 28.9 56.5 73.5 65.8 74.3 60.0 75.2 80.5 55.8 78.3 77.6 82.6 48.0 66.6 74. 29.5 33.4 33.8 53.6 35.6 35.7 31.6 62.6 77.6 71.1 77.7 65.8 80.3 85.7 58.3 80.6 81.7 86.3 (a) MATH (b) AMC23 (c) AIME24 (d) Minerva Math (e) AMC23 (Gemma) Figure 3: BoN results with different generation models. GenPRM as Critic. We also conduct experiments by using GenPRM as critic to refine the outputs of the policy model, specifically Gemma-3-12b-it (Gemma Team and Google DeepMind, 2025). The results in Table 2 show that GenPRM exhibits strong critique abilities by improving the performance of the policy model from 66.9 to 74.0 and the performance continues to increase with more refinement based on critic feedback. 9 GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning Table 2: Results of GenPRM used as critic model on AMC23. Original Turn 1 Turn 2 Turn 3 66.9 74.0 76.2 76.9 4.4. Analysis Label Estimation Method and Criterion. To explore how different label estimation influences GenPRM, we conduct experiments with the following methods: (1) hard label (Wang et al., 2024b; Zhang et al., 2025c); (2) RPE in (10); and (3) RPE variant (𝑃𝑡 = 𝑀 𝐶(𝑠𝑡, 𝑎𝑡) 𝑀 𝐶(𝑠𝑡)). For the RPE and its variant, we use different thresholds 𝜖 for evaluation and set the labels as correct by checking whether 𝑃𝑡 𝜖. The results in Table 3 show that RPE and its variant outperforms hard label estimation and RPE with 𝜖 = 0.8 achieves the best result. By scaling test-time compute with majority voting, the results in Table 4 demonstrate that RPE with 𝜖 = 0.8 still reaches the best. Table 3: Results of GenPRM with different label estimation method and threshold on ProcessBench, reported with Pass@1. The best results are shown in bold. Estimation Method Positive Label Criterion GSM8K MATH Olympiad Bench OmniMATH 𝑃𝑡 = 𝑀 𝐶(𝑠𝑡, 𝑎𝑡) (hard label) 𝑃𝑡 > 0 𝑃𝑡 = 𝑀 𝐶(𝑠𝑡, 𝑎𝑡) 𝑀 𝐶(𝑠𝑡) 𝑃𝑡 = 𝑀 𝐶(𝑠𝑡, 𝑎𝑡) 𝑀 𝐶(𝑠𝑡) 𝑃𝑡 0.1 𝑃𝑡 0.3 𝑃𝑡 0. 𝑃𝑡 0.1 𝑃𝑡 0.5 𝑃𝑡 0.8 𝑃𝑡 1.0 72.9 77.3 76.8 75.8 74.8 75.7 78.7 76.4 78.9 79.9 79.6 80. 78.7 79.2 80.3 77.4 73.2 70.8 71.1 72.8 71.6 70.4 72.2 68.1 68.0 68.5 69.0 68. 68.7 68.5 69.8 67.2 Avg. 73.2 74.1 74.1 74.3 73.5 73.5 75.2 72.3 Table 4: Results of GenPRM with different label estimation method and threshold on ProcessBench, reported with Maj@8. The best results are shown in bold. Estimation Method Positive Label Criterion GSM8K MATH Olympiad Bench OmniMATH 𝑃𝑡 = 𝑀 𝐶(𝑠𝑡, 𝑎𝑡) (hard label) 𝑃𝑡 > 𝑃𝑡 = 𝑀 𝐶(𝑠𝑡, 𝑎𝑡) 𝑀 𝐶(𝑠𝑡) 𝑃𝑡 = 𝑀 𝐶(𝑠𝑡, 𝑎𝑡) 𝑀 𝐶(𝑠𝑡) 𝑃𝑡 0.1 𝑃𝑡 0.3 𝑃𝑡 0.5 𝑃𝑡 0.1 𝑃𝑡 0.5 𝑃𝑡 0.8 𝑃𝑡 1.0 75. 79.8 80.9 78.1 77.0 78.0 81.0 81.1 83.8 85.1 86.5 85.6 84.6 85.2 85.7 84.1 80. 78.0 78.1 79.1 78.1 78.2 78.4 76.0 74.4 74.5 75.0 73.4 75.3 74.3 76.8 74.7 Avg. 78.5 79.4 80.2 79.1 78.7 78.9 80.5 79.0 Reasoning Components. To understand how each reasoning component influence GenPRM, we conduct experiments by training GenPRM with: (1) CoT data only, (2) code verification data only, and (3) full data. During inference phase, we also compare several variants. For example, GenPRM trained with full data can be used to only verify each step with CoT only by stopping generation at </analyze> token. The results in Table 5 show that: (1) the improvement of GenPRM mainly comes 10 GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning from CoT reasoning; (2) generating code and reasoning with code execution result improves the process verification performance as well. Table 5: Results on ProcessBench of GenPRM with different reasoning components, reported with Maj@8. The best results are shown in bold. Training Inference CoT Code CoT Code Code Exec. GSM8K MATH Olympiad Bench OmniMATH 63. 67.0 70.6 76.4 60.1 61.3 78.8 81.0 81.0 65.8 70.8 76.6 83.0 66.7 74.7 85.1 85.1 85.7 54. 61.6 67.3 80.5 59.9 68.1 78.7 78.1 78.4 55.9 57.4 63.9 75.4 59.2 62.0 74.9 75.5 76.8 Avg. 60.0 64.2 69.6 78.8 61.5 66.5 79.3 79.9 80.5 5. Related Work Process Reward Models. Process reward models have been proved to be effective for providing step-wise scores and are superior to outcome reward models in mathematical reasoning tasks (Uesato et al., 2022; Lightman et al., 2024). However, annotating process supervision dataset such as PRM800K (Lightman et al., 2024) requires significant human costs. To mitigate this cost, prior works utilize Monte Carlo estimation (Wang et al., 2024b) and binary search (Luo et al., 2024) for automated label generation. Subsequent research improves PRMs through methods such as advantage modeling (Setlur et al., 2025), 𝑄-value rankings (Li and Li, 2025), implicit entropy regularization (Zhang et al., 2024a), retrieval-augmented generation (Zhu et al., 2025), and fast-slow verification (Zhong et al., 2025). Furthermore, the community has developed high-quality opensource PRMs, including the RLHFlow series (Xiong et al., 2024), Math-psa (Wang et al., 2024a), Skywork series (Skywork o1 Team, 2024), and Qwen2.5-Math series (Zheng et al., 2024; Zhang et al., 2025c). Recently, line of works focus on extending PRMs to other tasks, including coding (Zhang et al., 2024b), medical tasks (Jiang et al., 2025), agentic tasks (Choudhury, 2025), general domain tasks (Zhang et al., 2025a; Zeng et al., 2025), and multimodal tasks (Wang et al., 2025). Current studies also focus on benchmarking PRMs (Zheng et al., 2024; Song et al., 2025) to systematically evaluate their performance. Large Language Model Test-Time Scaling. Scaling test-time computation is an effective method for improving performance during the inference phase (OpenAI, 2024a,b; DeepSeek-AI et al., 2025). TTS is commonly implemented with external verifiers (e.g., ORMs and PRMs) or strategies (e.g., beam search and MCTS) (Wu et al., 2025; Snell et al., 2025; Beeching et al., 2024; Liu et al., 2025). In this work, we scale the test-time computation of generative PRM with an explicit reasoning process and GenPRM can also serve as verifier or critic model in external TTS. Enhancing the Generative Abilities of Reward Models. Previous research has investigated methods to enhance the generative capabilities of reward models using CoT reasoning (Ankner et al., 2024; Zhang et al., 2025b; Mahan et al., 2024). For instance, CLoud reward models (Ankner et al., 2024) are trained to generate critiques for responses and predict rewards using an additional reward 11 GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning head. GenRM-CoT (Zhang et al., 2025b) and GenRM (Mahan et al., 2024) train generative reward models that perform CoT reasoning before making final predictions via SFT and preference learning, respectively. CTRL (Xie et al., 2025) demonstrates that critic models exhibit strong discriminative abilities when utilized as generative reward models. Prior to these works, GRM (Yang et al., 2024c) regularizes the hidden states of reward models with text generation loss. 6. Conclusion In this work, we propose GenPRM, generative process reward model that performs explicit reasoning and code verification for process supervision and enables scaling the test-time compute of PRMs. Experimental results on ProcessBench and several mathematical datasets show GenPRM outperforms prior PRMs. We also demonstrate that the performance of GenPRM increases via test-time scaling and GenPRM is effective as critic model. We believe that this work provides perspectives on PRMs by demonstrating the strong TTS abilities of PRMs and extending the applications of PRMs. Limitations. First, GenPRM provides process supervision by generative reasoning, which introduces additional computation during inference phase. Future work will investigate how to prune the reasoning process dynamically (Zhong et al., 2025). Although GenPRM focuses mainly on mathematical reasoning tasks, it is worth to explore how to apply generative reasoning on coding and general reasoning tasks in the future (Zhang et al., 2025a). Additionally, it would be interesting to leverage RL to incentivize the generative reasoning abilities of GenPRM."
        },
        {
            "title": "References",
            "content": "AI-MO. AIME 2024, 2024a. aimo-validation-aime. AI-MO. AMC 2023, 2024b. aimo-validation-amc. URL https://huggingface.co/datasets/AI-MO/ URL https://huggingface.co/datasets/AI-MO/ Zachary Ankner, Mansheej Paul, Brandon Cui, Jonathan Chang, and Prithviraj Ammanabrolu. Critique-out-Loud Reward Models. arXiv preprint arXiv:2408.11791, 2024. Anthropic. Introducing Claude, 2023. introducing-claude/. URL https://www.anthropic.com/index/ Edward Beeching, Lewis Tunstall, and Sasha Rush. Scaling Test-Time Compute with URL https://huggingface.co/spaces/HuggingFaceH4/ Open Models, 2024. blogpost-scaling-test-time-compute. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Ré, and Azalia Mirhoseini. Large Language Monkeys: Scaling Inference Compute with Repeated Sampling. arXiv preprint arXiv:2407.21787, 2024. Sanjiban Choudhury. Process Reward Models for LLM Agents: Practical Framework and Directions. arXiv preprint arXiv:2502.10325, 2025. Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process Reinforcement through Implicit Rewards. arXiv preprint arXiv:2502.01456, 2025. 12 GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv preprint arXiv:2501.12948, 2025. Gemma Team and Google DeepMind. Introducing Gemma 3: The most capable model you can run on single GPU or TPU, March 2025. URL https://blog.google/technology/developers/ gemma-3. Zhibin Gou, Zhihong Shao, Yeyun Gong, yelong shen, Yujiu Yang, Nan Duan, and Weizhu Chen. CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing. In International Conference on Learning Representations (ICLR), 2024. URL https://openreview.net/forum? id=Sx038qxjek. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring Mathematical Problem Solving With the MATH Dataset. In Advances in Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL https://openreview.net/forum?id=7Bywt2mQsCe. Shuyang Jiang, Yusheng Liao, Zhe Chen, Ya Zhang, Yanfeng Wang, and Yu Wang. MedS3: Towards Medical Small Language Models with Self-Evolved Slow Thinking. arXiv preprint arXiv:2501.12051, 2025. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving Quantitative Reasoning Problems with Language Models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning Advances in Neural Information Processing Systems (NeurIPS), volume 35, pages 38433857. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/ 2022/file/18abbeef8cfe9203fdf9053c9c4fe191-Paper-Conference.pdf. Wendi Li and Yixuan Li. Process Reward Model with Q-value Rankings. In International Conference on Learning Representations (ICLR), 2025. URL https://openreview.net/forum?id= wQEdh2cgEk. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets Verify Step by Step. In International Conference on Learning Representations (ICLR), 2024. URL https://openreview.net/forum? id=v8L0pN6EOi. Runze Liu, Fengshuo Bai, Yali Du, and Yaodong Yang. Meta-Reward-Net: Implicitly Differentiable Reward Learning for Preference-based Reinforcement Learning. In Advances in Neural Information Processing Systems (NeurIPS), volume 35, pages 2227022284, 2022. Runze Liu, Yali Du, Fengshuo Bai, Jiafei Lyu, and Xiu Li. PEARL: Zero-shot Cross-task Preference Alignment and Robust Reward Learning for Robotic Manipulation. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, International Conference on Machine Learning (ICML), volume 235 of Proceedings of Machine Learning Research, pages 3094630964. PMLR, 2127 Jul 2024. URL https://proceedings. mlr.press/v235/liu24o.html. Runze Liu, Junqi Gao, Jian Zhao, Kaiyan Zhang, Xiu Li, Biqing Qi, Wanli Ouyang, and Bowen Zhou. Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling. arXiv preprint arXiv:2502.06703, 2025. Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, et al. Improve Mathematical Reasoning in Language Models by Automated Process Supervision. arXiv preprint arXiv:2406.06592, 2024. Dakota Mahan, Duy Van Phung, Rafael Rafailov, Chase Blagden, Nathan Lile, Louis Castricato, JanPhilipp Fränken, Chelsea Finn, and Alon Albalak. Generative Reward Models. arXiv preprint arXiv:2410.12832, 2024. OpenAI. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774, 2023. OpenAI. Learning to reason with LLMs, 2024a. learning-to-reason-with-llms. URL https://openai.com/index/ OpenAI. OpenAI o3-mini, 2024b. URL https://openai.com/index/openai-o3-mini. Qwen Team. QwQ-32B: Embracing the Power of Reinforcement Learning, March 2025. URL https://qwenlm.github.io/blog/qwq-32b. Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, and Aviral Kumar. Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning. In International Conference on Learning Representations (ICLR), 2025. URL https://openreview.net/forum?id=A6Y7AqlzLW. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. arXiv preprint arXiv:2402.03300, 2024. 14 GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning Skywork o1 Team. Skywork-o1 Open Series. https://huggingface.co/Skywork, November 2024. URL https://huggingface.co/Skywork. Charlie Victor Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM Test-Time Compute Optimally Can be More Effective than Scaling Parameters for Reasoning. In International Conference on Learning Representations (ICLR), 2025. URL https://openreview.net/forum?id= 4FWAwZtd2n. Mingyang Song, Zhaochen Su, Xiaoye Qu, Jiawei Zhou, and Yu Cheng. PRMBench: Fine-grained and Challenging Benchmark for Process-Level Reward Models. arXiv preprint arXiv:2501.03124, 2025. Xiaoyu Tan, Tianchu Yao, Chao Qu, Bin Li, Minghao Yang, Dakuan Lu, Haozhe Wang, Xihe Qiu, Wei Chu, Yinghui Xu, et al. AURORA: Automated Training Framework of Universal Process Reward Models via Ensemble Prompting and Reverse Verification. arXiv preprint arXiv:2502.11520, 2025. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. Jun Wang, Meng Fang, Ziyu Wan, Muning Wen, Jiachen Zhu, Anjie Liu, Ziqin Gong, Yan Song, Lei Chen, Lionel Ni, et al. OpenR: An open source framework for advanced reasoning with large language models. arXiv preprint arXiv:2410.09671, 2024a. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 94269439, 2024b. Weiyun Wang, Zhangwei Gao, Lianjie Chen, Chen Zhe, Jinguo Zhu, Xiangyu Zhao, Yangzhou Liu, Yue Cao, Shenglong Ye, Xizhou Zhu, Lewei Lu, Haodong Duan, Yu Qiao, Jifeng Dai, and Wenhai Wang. VisualPRM: An Effective Process Reward Model for Multimodal Reasoning. arXiv preprint arXiv:2503.10291, 2025. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-Consistency Improves Chain of Thought Reasoning in Language Models. In International Conference on Learning Representations (ICLR), 2023. URL https: //openreview.net/forum?id=1PL1NIMMrw. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In Advances in neural information processing systems (NeurIPS), volume 35, pages 2482424837, 2022. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving. In International Conference on Learning Representations (ICLR), 2025. URL https://openreview.net/forum? id=VNckp7JEHn. Zhihui Xie, Liyu Chen, Weichao Mao, Jingjing Xu, Lingpeng Kong, et al. Teaching Language Models to Critique via Reinforcement Learning. arXiv preprint arXiv:2502.03492, 2025. Wei Xiong, Hanning Zhang, Nan Jiang, and Tong Zhang. An Implementation of Generative PRM. https://github.com/RLHFlow/RLHF-Reward-Modeling, 2024. 15 GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 Technical Report. arXiv preprint arXiv:2412.15115, 2024a. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement. arXiv preprint arXiv:2409.12122, 2024b. Rui Yang, Ruomeng Ding, Yong Lin, Huan Zhang, and Tong Zhang. Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs. In Advances in Neural Information Processing Systems (NeurIPS), 2024c. URL https://openreview.net/forum?id=jwh9MHEfmY. Lifan Yuan, Wendi Li, Huayu Chen, Ganqu Cui, Ning Ding, Kaiyan Zhang, Bowen Zhou, Zhiyuan Liu, and Hao Peng. Free Process Rewards without Process Labels. arXiv preprint arXiv:2412.01981, 2024. Thomas Zeng, Shuibai Zhang, Shutong Wu, Christian Classen, Daewon Chae, Ethan Ewer, Minjae Lee, Heeju Kim, Wonjun Kang, Jackson Kunde, et al. VersaPRM: Multi-Domain Process Reward Model via Synthetic Reasoning Data. arXiv preprint arXiv:2502.06737, 2025. Hanning Zhang, Pengcheng Wang, Shizhe Diao, Yong Lin, Rui Pan, Hanze Dong, Dylan Zhang, Pavlo Molchanov, and Tong Zhang. Entropy-Regularized Process Reward Model. arXiv preprint arXiv:2412.11006, 2024a. Kaiyan Zhang, Jiayuan Zhang, Haoxin Li, Xuekai Zhu, Ermo Hua, Xingtai Lv, Ning Ding, Biqing Qi, and Bowen Zhou. OpenPRM: Building Open-domain Process-based Reward Models with Preference Trees. In International Conference on Learning Representations (ICLR), 2025a. URL https://openreview.net/forum?id=fGIqGfmgkW. Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative Verifiers: Reward Modeling as Next-Token Prediction. In International Conference on Learning Representations (ICLR), 2025b. URL https://openreview.net/forum?id=Ccwp4tFEtE. Yuxiang Zhang, Shangxi Wu, Yuqi Yang, Jiangming Shu, Jinlin Xiao, Chao Kong, and Jitao Sang. o1-Coder: an o1 Replication for Coding. arXiv preprint arXiv:2412.00154, 2024b. Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. The Lessons of Developing Process Reward Models in Mathematical Reasoning. arXiv preprint arXiv:2501.07301, 2025c. Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. ProcessBench: Identifying Process Errors in Mathematical Reasoning. arXiv preprint arXiv:2412.06559, 2024. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph Gonzalez, and Ion Stoica. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems (NeurIPS), volume 36, pages 4659546623. Curran Associates, Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ 91f18a1287b398d378ef22505bf41832-Paper-Datasets_and_Benchmarks.pdf. Jianyuan Zhong, Zeju Li, Zhijian Xu, Xiangyu Wen, and Qiang Xu. Dyve: Thinking Fast and Slow for Dynamic Process Verification. arXiv preprint arXiv:2502.11157, 2025. Jiachen Zhu, Congmin Zheng, Jianghao Lin, Kounianhua Du, Ying Wen, Yong Yu, Jun Wang, and Weinan Zhang. Retrieval-Augmented Process Reward Model for Generalizable Mathematical Reasoning. arXiv preprint arXiv:2502.14361, 2025. Xuekai Zhu, Biqing Qi, Kaiyan Zhang, Xinwei Long, Zhouhan Lin, and Bowen Zhou. PaD: Programaided Distillation Can Teach Small Models Reasoning Better than Chain-of-thought Fine-tuning. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 25712597, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.142. URL https://aclanthology.org/2024.naacl-long.142/. 17 GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning A. Experimental Details A.1. Scoring and Voting Methods PRM-Last. PRM-Last takes the process reward of the last step of the whole LLM response as the final score, i.e., score = 𝑟𝑇 . PRM-Avg. PRM-Avg takes the mean process reward of all steps as the final score, i.e., score = 1 𝑇 𝑡=1 𝑟𝑡. 𝑇 PRM-Min. PRM-Min takes the minimum process reward of all steps as the final score, i.e., score = min𝑟{𝑟𝑡}𝑇 . 𝑡=1 A.2. Implementation Details Prompt for CoT and code rationale generation is shown in Table A.2. Prompt for CoT and code rationale generation [System]: You are math teacher. Your task is to review and critique the paragraphs in solution step by step with python code. [User]: The following is the math problem and solution (split into paragraphs, enclosed with tags and indexed from 1): [Math Problem] {problem} [Solution] {solution_section} Your task is to verify the correctness of paragraph in the solution. Split your verification by ### Paragraph {{ID}}. Your verification for each paragraph should be constructed by 2 parts, wrapped by <analyze></analyze> and <verify></verify> separately. 1. In <analyze></analyze> part, you need to analyze the reasoning process and explain why the paragraph is correct or incorrect in detail. In <verify></verify> part, you must write **Python code** in the form of 2. pythonn{{CODE}}n to verify every details that can be verified by code. You can import PyPI (i.e., sympy, scipy and so on) to implement complicated calculation. Make sure to print the critic results in the code. Every code will be executed automatically by system. You need to analyze the [Code Output] after code executing. GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning >Pay attention that you must follow the format of pythonn{{CODE}}n when you write the code, otherwise the code will not be executed. After all verifications, if you identify an error in paragraph, return the **index of the paragraph where the earliest error occurs**. Otherwise, return the **index of -1 (which typically denotes \"not found\")**. Please put your final answer (i.e., the index) within box in the form of $boxed{{INDEX}}$. Following Zheng et al. (2024); Zhang et al. (2025c), we use the prompt in Table A.2 to evaluate LLM-as-a-judge methods on ProcessBench (Zheng et al., 2024). Evaluation prompt for LLM-as-a-judge methods on ProcessBench will provide math problem along with solution. They will be formatted as follows: [Math Problem] <math_problem> ...(math problem)... </math_problem> [Solution] <paragraph_1> ...(paragraph 1 of solution)... </paragraph_1> ... <paragraph_n> ...(paragraph of solution)... </paragraph_n> Your task is to review each paragraph of the solution in sequence, analyzing, verifying, and critiquing the reasoning in detail. You need to provide the analyses and the conclusion in the following format: <analysis_1> ...(analysis of paragraph 1)... </analysis_1> ... <analysis_n> ...(analysis of paragraph n)... </analysis_n> <conclusion> 19 GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning Correct/Incorrect </conclusion> * When you analyze each paragraph, you should use proper verification, recalculation, or reflection to indicate whether it is logically and mathematically valid. Please elaborate on the analysis process carefully. * If an error is detected in any paragraph, you should describe the nature and cause of the error in detail, and suggest how to correct the error or the correct approach. Once paragraph is found to contain any error, stop further analysis of subsequent paragraphs (as they may depend on the identified error) and directly provide the conclusion of \"Incorrect.\" For instance, given solution of five paragraphs, if an error is found in the third paragraph, you should reply in the following format: <analysis_1> ...(analysis of paragraph 1)... </analysis_1> <analysis_2> ...(analysis of paragraph 2)... </analysis_2> <analysis_3> ...(analysis of paragraph 3; since an error is found here, also provide detailed critique and correction guideline)... </analysis_3> <conclusion> Incorrect </conclusion> Note that the analyses of paragraphs 4 and 5 should be skipped as the paragraph 3 has been found to contain an error. * Respond with your analyses and conclusion directly. The following is the math problem and the solution for you task: [Math Problem] {tagged_problem} [Solution] {tagged_response} 20 GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning Table 6: Full results of critic models and PRMs on ProcessBench. Model GPT-4-0806 o1-mini Llama-3-8B-Instruct Llama-3-70B-Instruct Llama-3.1-8B-Instruct Llama-3.1-70B-Instruct Llama-3.3-70B-Instruct Qwen2.5-Math-7B-Instruct Qwen2.5-Math-72B-Instruct Qwen2.5-Coder-7B-Instruct Qwen2.5-Coder-14B-Instruct Qwen2.5-Coder-32B-Instruct Qwen2-7B-Instruct Qwen2-72B-Instruct Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct Qwen2.5-32B-Instruct Qwen2.5-72B-Instruct QwQ-32B-Preview GSM8K MATH OlympiadBench Omni-MATH Err. Corr. F1 Err. Corr. F1 Err. Corr. F1 Err. Corr. F1 Avg. F1 Proprietary LLMs (Critic) 70.0 88.9 91.2 97.9 79.2 54.4 93.2 83.5 76.6 95.1 63.6 45.8 88.9 80.2 58.4 95. 51.4 45.2 87.2 74.8 65.6 91.7 53.5 61.9 82.4 87.9 Open-Source LLMs (Critic) 7.8 96.9 6.2 89.6 96.9 42.5 13.1 28.6 35.7 52.2 13.0 44.4 10.9 41.9 64.3 74.9 35.4 82.9 43.3 72.5 15.5 100.0 26.8 14.8 65.8 36.0 49.8 3.4 7.7 50.1 25.4 33.8 68.9 44.9 54.1 8.4 40.6 30.5 67.6 37.7 57.0 36.5 30.8 40.6 69.3 38.4 54.6 65.6 36.7 49.3 76.2 46.3 62.8 88.0 78.1 81. 96.9 100.0 14.3 96.4 94.8 4.7 82.9 33.2 94.8 97.9 96.9 95.3 9.1 93.3 2.7 75.6 94.6 96.8 94.3 98.3 92.4 90.6 13.8 70.9 45.1 87.4 95.8 93.1 79.3 13.8 27.1 22.8 12.0 5.1 32.4 48.2 35.1 59.4 31.0 25.7 7.7 52.1 19.5 6.5 2.1 39.9 20.7 60.1 33.4 19.0 22.4 49.2 34.0 36.6 26.5 53.3 31.5 53.1 25.3 61.8 38.7 78.7 61.4 2.7 92.0 1.5 69.9 94.1 91.7 97.3 99.1 94.1 91.2 10.9 55.2 33.9 78.8 95.9 92.6 54.6 4.8 26.1 21.2 11.2 2.8 32.0 46.7 30.7 46.7 28.2 14.2 6.9 32.5 19.0 4.1 0.9 34.0 15.9 48.9 31.5 14.7 20.0 42.1 32.3 29.7 26.2 45.0 28.3 40.0 24.1 54.6 36.6 57.8 55.7 8.3 91.7 0.8 61.8 90.5 88.0 96.3 98.3 94.2 87.6 8.7 53.1 28.6 76.3 92.5 90.9 68. 12.6 11.1 20.0 29.1 1.6 5.1 41.0 52.7 43.0 58.0 12.7 19.9 31.7 45.5 1.8 6.7 27.3 37.8 46.3 56.1 12.1 13.6 40.2 49.8 27.4 32.6 41.3 52.2 38.3 49.3 52.2 61.2 61.3 71.5 PRMs (1.5B) Skywork-PRM-1.5B GenPRM-1.5B (Pass@1) GenPRM-1.5B (Maj@8) 50.2 37.0 34.8 71.5 92.7 97.4 59.0 37.9 52.8 57.1 51.3 64. 65.2 80.1 87.7 48.0 15.4 66.6 47.0 74.4 57.2 26.0 66.5 76.1 19.3 13.6 55.1 45.2 65.3 51.3 32.8 68.7 80.1 19.2 36.4 54.5 57.3 62.5 63. Math-Shepherd-PRM-7B 32.4 RLHFlow-PRM-Mistral-8B 33.8 RLHFlow-PRM-Deepseek-8B 24.2 Skywork-PRM-7B 61.8 EurusPRM-Stage1 46.9 51.2 EurusPRM-Stage2 Qwen2.5-Math-7B-Math-Shepherd 46.4 53.1 Qwen2.5-Math-7B-PRM800K 72.0 Qwen2.5-Math-PRM-7B 64.7 RetrievalPRM-7B - Universal-PRM-7B 52.7 Direct Generative PRM-7B 67.7 GenPRM-7B (Pass@1) 69.6 GenPRM-7B (Maj@8) Dyve-14B Qwen2.5-Math-PRM-72B GenPRM-32B (Pass@1) GenPRM-32B (Maj@8) - 78.7 73.1 74.9 PRMs (7-8B) 47.9 18.0 50.4 21.7 38.8 21.4 70.8 43.8 44.3 33.3 47.3 36.4 62.5 18.9 68.2 48.0 82.4 68.0 74.6 67.2 85.8 63.9 55.9 78.7 74.6 81.0 80.5 - 82.0 72.2 80.0 62.2 38.2 35.0 96.6 90.1 90.4 75.6 - 80.0 87.0 91.6 29.5 15.0 33.4 8.2 33.8 10.1 53.6 17.9 35.6 23.9 35.7 25.7 31.6 7.4 62.6 35.7 77.6 55.7 71.1 56.0 77.7 65.8 44.8 80.3 68.3 85.7 74.0 - PRMs (14-72B) - 68.5 87.3 74.2 83.1 79.4 85.1 84. - 88.2 84.1 88.7 - 58.3 80.6 67.9 81.7 73.4 86.3 79.0 91.7 99.0 98.4 82.9 42.0 44.0 95.9 95.3 96.4 88.1 - 81.4 94.0 96.9 - 97.9 96.4 98.5 71.1 43.1 51.0 31.9 19.8 18.0 93.8 87.3 85.5 65.2 - 69.6 76.6 83. - 82.0 72.2 78.8 24.8 14.2 13.8 9.6 16.9 10.9 22.9 14.0 21.7 21.9 21.2 23.1 13.7 4.0 50.7 29.8 67.5 55.2 60.2 52.8 67.6 54.5 45.5 72.2 63.5 78.4 70.0 - - 49.0 74.3 64.8 72.8 70.3 78.9 76.3 73.0 45.2 51.9 41.9 24.5 19.1 95.0 86.1 83.0 62.7 - 72.6 77.4 85. - 78.8 75.5 84.2 23.8 31.5 15.8 28.4 16.9 26.6 21.0 42.1 23.1 31.2 20.9 31.3 7.7 28.9 44.3 56.5 66.3 73.5 57.3 65.8 66.4 74.3 55.9 60.0 69.8 75.2 76.8 80.5 47.2 55.8 71.1 78.3 72.8 77.6 80.1 82.6 B. Additional Results We provide full results of ProcessBench in Table 6. Model Size. To investigate how the model size influence the performance of GenPRM, we conduct experiments with parameters including 1.5B, 7B, and 32B. The results in Table 7 show that the performance of GenPRM increases significantly (57.3 75.2, 63.4 80.5) by scaling the parameters 21 GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning from 1.5B to 7B. However, there is small improvement when continue to scaling the parameters to 32B, demonstrating that GenPRM with 7B parameters is more economic choice. Table 7: Evaluation results of GenPRM with different sizes on ProcessBench. Model Size Metric GSM8K MATH OlympiadBench Omni-MATH Avg. 1.5B 7B 32B Pass@1 Maj@8 Pass@1 Maj@8 Pass@1 Maj@8 52.8 81.0 78.7 81.0 83.1 85.1 66.6 74.4 80.3 85.7 81.7 86. 55.1 65.3 72.2 78.4 72.8 78.9 54.5 62.5 69.8 76.8 72.8 80.1 57.3 63.4 75.2 80.5 77.6 82.6 Inference Tokens. We provide statistics of the reasoning tokens per step and per response in Table 8. Table 8: Statistics of the output tokens of GenPRM. MATH AMC23 AIME24 Minerva Math Step 344.7 Response 2771.4 416.2 3200.2 432.5 4112.9 503.3 4877.1 C. Cases In this section, we analyze two cases to have better understanding of GenPRM. The case in Figure 4 shows that the code execution feedback can correct the mistakes in CoT and enhance the process supervision abilities of GenPRM. The case in Figure 5 demonstrates that GenPRM provides accurate process supervision with CoT reasoning only. 22 GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning Figure 4: The first case of GenPRM. 23 GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning Figure 5: The second case of GenPRM."
        }
    ],
    "affiliations": [
        "BUPT",
        "Harbin Institute of Technology",
        "Shanghai AI Laboratory",
        "Tsinghua University"
    ]
}