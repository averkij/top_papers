{
    "paper_title": "FullFront: Benchmarking MLLMs Across the Full Front-End Engineering Workflow",
    "authors": [
        "Haoyu Sun",
        "Huichen Will Wang",
        "Jiawei Gu",
        "Linjie Li",
        "Yu Cheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Front-end engineering involves a complex workflow where engineers conceptualize designs, translate them into code, and iteratively refine the implementation. While recent benchmarks primarily focus on converting visual designs to code, we present FullFront, a benchmark designed to evaluate Multimodal Large Language Models (MLLMs) \\textbf{across the full front-end development pipeline}. FullFront assesses three fundamental tasks that map directly to the front-end engineering pipeline: Webpage Design (conceptualization phase), Webpage Perception QA (comprehension of visual organization and elements), and Webpage Code Generation (implementation phase). Unlike existing benchmarks that use either scraped websites with bloated code or oversimplified LLM-generated HTML, FullFront employs a novel, two-stage process to transform real-world webpages into clean, standardized HTML while maintaining diverse visual designs and avoiding copyright issues. Extensive testing of state-of-the-art MLLMs reveals significant limitations in page perception, code generation (particularly for image handling and layout), and interaction implementation. Our results quantitatively demonstrate performance disparities across models and tasks, and highlight a substantial gap between current MLLM capabilities and human expert performance in front-end engineering. The FullFront benchmark and code are available in https://github.com/Mikivishy/FullFront."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 9 9 3 7 1 . 5 0 5 2 : r FullFront: Benchmarking MLLMs Across the Full Front-End Engineering Workflow Haoyu Sun Tongji University Huichen Will Wang University of Washington Jiawei Gu Sun Yat-sen University Linjie Li Microsoft Yu Cheng The Chinese University of Hong Kong Figure 1: Mapping the full front-end engineering workflow to FullFronts benchmark tasks: (1) Conceptualization assessed by Webpage Design, (2) Comprehension by Webpage Perception QA, and (3) Implementation by Webpage Code Generation."
        },
        {
            "title": "Abstract",
            "content": "Front-end engineering involves complex workflow where engineers conceptualize designs, translate them into code, and iteratively refine the implementation. While recent benchmarks primarily focus on converting visual designs to code, we present FullFront, benchmark designed to evaluate Multimodal Large Language Models (MLLMs) across the full front-end development pipeline. FullFront assesses three fundamental tasks that map directly to the front-end engineering pipeline: Webpage Design (conceptualization phase), Webpage Perception QA (comprehension of visual organization and elements), and Webpage Code Generation (implementation phase). Unlike existing benchmarks that use either scraped websites with bloated code or oversimplified LLM-generated HTML, FullFront employs novel, two-stage process to transform real-world webpages into clean, standardized HTML while maintaining diverse visual designs and avoiding copyright issues. Extensive testing of state-of-the-art MLLMs reveals significant limitations in page perception, code generation (particularly for image handling and layout), and interaction implementation. Our results quantitatively demonstrate performance disparities across models and tasks, and highlight substantial gap between current MLLM capabilities and human expert performance in front-end engineering. The FullFront benchmark and code are available in https://github.com/Mikivishy/FullFront."
        },
        {
            "title": "Introduction",
            "content": "Front-end engineering, cornerstone of the modern digital experience, is an intricate process, as depicted in Figure 1. It transforms abstract concepts into initial designs (conceptualization), Figure 2: Overview of the eight subtasks FullFront covers and our data construction pipeline. involves detailed visual comprehension (perception), and culminates in functional, interactive code (implementation) for web applications. This field is poised for significant transformation with the advent of Multimodal Large Language Models (MLLMs), whose capabilities in processing visual information and generating code offer compelling potential to streamline and even automate the front-end development, aligning with the aspirational goal of an idea-to-design-to-code paradigm. Despite this burgeoning potential, benchmark to assess MLLMs across the full front-end Engineering workflow is conspicuously absent. Instead, current evaluations tend to separately address crucial yet distinct capabilities: vision perception and code generation. For instance, benchmarks like IW-Bench [1] and WebCode2M [2] scrutinize MLLMs code generation from visual inputs but often possess narrow task scope, overlooking vital aspects such as implementing interactive features or refining existing codebases. Conversely, while WebQuest [3] and Webqa [4] investigate MLLMs visual understanding of webpages, the focus frequently remains on content-level reasoning, thereby neglecting the fine-grained perceptual acuity concerning element size, positioning, and layout, which is indispensable for accurate front-end implementation. Most critically, these fragmented approaches generally omit the initial conceptual design phase of development, and therefore fall short of gauging MLLM proficiency in end-to-end front-end engineering. In this work, we introduce FullFront, benchmark meticulously designed to evaluate MLLMs across the full front-end engineering workflow. As depicted in Figure 2, FullFront distinctively offers holistic assessment through three core tasks: (1) Webpage Design (50 problems), which assesses the models ability to structure and organize visual elements to present some given content; (2) Webpage Perception QA (three subtasks and 1800 multiple-choice questions), which evaluates the perception of visual organization, element characteristics, and spatial relationships within webpage; and (3) Webpage Code Generation (4 subtasks and 400 code generation problems), which focuses on the accurate translation of visual designs into functional code, including interaction implementation and code refinement. We collect real-world webpages and develop an MLLM-driven pipeline to reconstruct them into clean, standardized, and copyright-free HTML, ensuring high controllability while preserving original visual diversity for robust benchmark data. This comprehensive task structure and our evaluation framework, incorporating fine-grained visual similarity scores and detailed code-level metrics (including structural and content-based comparisons), provide multifaceted and robust assessment of model capabilities across the full front-end engineering workflow. Benchmarking state-of-the-art open-source and proprietary MLLMs with FullFront reveals significant challenges across the board. In the Webpage Design task, current text-to-image MLLMs demonstrate an ability to produce general layout concepts but lack the precision for high-fidelity webpage designs 2 that accurately reflect detailed textual descriptions. In Webpage Perception QA, even leading models struggle to achieve human-comparable accuracy; for instance, the best-performing model, Claude 3.7 Sonnet, achieves an average accuracy below 55% across these tasks, starkly contrasting with human performance exceeding 95%. Our analysis reveals that MLLMs face considerable difficulties in accurately perceiving element alignment, size, and positioning within webpages. For Webpage Code Generation, while proprietary models like Claude 3.7 Sonnet and Gemini 2.5 Pro generally outperforme open-source alternatives, they still encounter difficulties, particularly in accurately handling complex front-end details such as image manipulation, layout fidelity, and interaction implementation. These findings underscore the critical need to enhance current MLLM capabilities within the front-end development workflow to bridge the substantial gap between their current performance and the requirements for expert-level engineering. In summary, our main contributions are as follows: Comprehensive Full Front-End Workflow Benchmark: Unifying Webpage Design (conceptualization), Perception QA (comprehension), and Code Generation (implementation) into one cohesive evaluation pipeline. Robust Multi-Faceted Evaluation Metrics: Integrating fine-grained visual similarity and detailed code-level comparisons for thorough assessment. Benchmarking State-of-the-Art MLLMs & Key Insights: Our evaluation highlights critical MLLM limitations, primarily rooted in deficient fine-grained visual perception (e.g., element alignment, sizing, spacing). This impacts their ability to accurately generate code, particularly for complex layouts, image manipulation, and interactive functionalities, with notable performance disparity between proprietary and open-source models."
        },
        {
            "title": "2 Related Work",
            "content": "Applications of MLLMs in Web Recently, the application of MLLMs in the web domain [5, 6, 7, 8] has garnered considerable research attention. Numerous innovative approaches have emerged, enabling MLLMs to navigate and manipulate websites according to user instructions [9, 10, 11]. For instance, Mind2Web [12] pioneers generalist web agent by training models on diverse web tasks, demonstrating their capability to follow complex natural language commands across various websites. Similarly, WinClick [13] focuses on GUI grounding with MLLMs, allowing for more precise interaction with web elements by understanding their visual and textual properties to execute user commands like clicking buttons or filling forms. These advancements highlight growing trend towards creating more autonomous and intelligent web interaction agents. Webpage Benchmarks and Datasets Several benchmarks and datasets have been developed to evaluate MLLMs on webpage-related tasks. For instance, significant body of work [14, 15, 3, 4, 16, 17, 18] leverages real-world webpages to assess MLLMs capabilities in element grounding and content reasoning via question-answering (QA). ScreenWords [19] focuses on screen summarization, while VisualWebBench [20] offers seven QA tasks for broader understanding assessment. Separately, research has also benchmarked MLLMs for front-end code generation from screenshots. Methodologies vary: Design2Code [21], WebCode2M [2], and IW-Bench [1] curate datasets by scraping and simplifying existing code. In contrast, Web2Code [22] and WebSight [23] employ LLMs for code generation, and Pix2Code [24] uses stochastic UI generator. Notable contributions also include MRWebs [25] resource list for external resources and Interaction2Codes [26] focus on dynamic webpage generation."
        },
        {
            "title": "3 Benchmark",
            "content": "3.1 Data Curation We now introduce the dataset composition across the three tasks and our data collection process. Webpage Design The Webpage Design task aims to evaluate text-to-image generation MLLMs as webpage designers. We provide 50 textual descriptions of synthetic webpages sampled from the Text to Code task dataset (see below). MLLMs are required to generate webpage design images based on 3 Figure 3: Comparison of the images used in our FullFront for webpage code generation tasks with those of other benchmarks. We are the first to not use single image placeholder or random images. these descriptions. This process tests how effectively models can transform textual requirements into visual designs, including their understanding of webpage layouts and element relationships. Since textual descriptions naturally cannot capture all visual design nuances, this task also assesses models ability to make reasonable design decisions where specifications are incomplete. Webpage Perception QA This task assesses MLLMs perception of webpage elements, including their position, style, spatial relationships, and overall page layout, through three subtasks. The Realworld QA subtask evaluates perceptual abilities using 625 real webpage screenshots (270 manually collected, 355 sourced from Uground [27] and IW-Bench [1]), resulting in 1,250 question-answer pairs. Complementing this, Synthetic QA assesses model performance on 400 Q/A pairs derived from 200 synthesized webpage screenshots generated via the specific methodology (detailed in the next Webpage Code Generation). Finally, Multi-window QA elevates task complexity by presenting 75 samples, each combining 2-4 screenshots from the Real-world QA set (totaling 150 Q/A pairs), thereby challenging models to accurately identify and locate the screenshot relevant to the posed question. Questions are primarily generated by GPT-4o [28], augmented with bounding boxes and OCR data extracted by OmniParser [29]. This allows GPT-4o to focus on generating challenging, high-quality multiple-choice questions based on page content and structure rather than low-level perception. All generated questions undergo rigorous manual review and modification to ensure correctness, challenge, and task validity. To mitigate ethical risks such as privacy leakage, all webpage screenshots are manually inspected and annotated to remove personal data and harmful content. Webpage Code Generation The Webpage Code Generation task evaluates models ability to translate visual page designs into executable HTML. Existing benchmarks (e.g., WebCode2M [2], Design2Code [21]) often simplify HTML from sources like Common Crawl [30] to mitigate ethical issues, remove external dependencies and redundant elements, and standardize code for comparison. Despite these benefits, the simplification process are inherently time-consuming and difficult to generalize across varied real-world webpages. Meanwhile, HTML generated with LLMs from scratch (e.g., WebSight [23]) often lacks authentic complexity. key limitation of existing datasets is their handling of images, such as using generic placeholders or random images, which hinders the assessment of nuanced image understanding and utilization crucial for high-fidelity webpage replication. To overcome these issues, we introduce the synthesis pipeline from real-world webpages. This two-stage process (detailed in Figure 2) starts with real-world webpage screenshot and its OmniParser-extracted element information. GPT-4o generates an initial HTML-v1, which Claude 3.7 Sonnet then refinesadjusting styles, positions, alignments, and layoutsinto higher-quality, more complex HTML-v2. This HTML-v2 and its rendered page serve as ground truth. For image handling, we utilize category-based strategy to best preserve the visual information from real-world webpage screenshots (see Appendix B.1). As shown in Figure 3, our method generates webpages that are demonstrably superior to other benchmarks in complexity and diversity. Unlike traditional tasks that only involve providing webpage screenshot for HTML code generation, we design four distinct subtasks to evaluate MLLMs front-end code generation capabilities under various conditions: Image to Code (200 samples) evaluates direct HTML generation from these HTML-v2 rendered screenshots; Text to Code (50 samples) assesses HTML generation based solely on manually verified textual descriptions of HTML-v2 rendered pages; Interaction Authoring (100 samples) measures the ability 4 to implement dynamic behaviors, requiring MLLMs to reproduce static page (from HTML-v1 as base) and add specified interactions based on screenshots depicting the page before and after the interaction; and Code Refinement (50 samples) simulates code optimization by requiring MLLMs to refine provided HTML-v1 code to match the quality and complexity of an HTML-v2 rendered screenshot. For more detailed task descriptions, see the Appendix B.2. 3.2 Evaluation Metrics To comprehensively evaluate MLLM performance on FullFront, we employ visual and code-level metrics, detailed below and applied specifically to each core task. Visual Level Metrics We assess MLLM generative capabilities by comparing the visual similarity of their output (rendered HTML or direct design images) against ground-truth images. This includes the CLIP Score [31], which measures high-level conceptual consistency via embedding space similarity, and the Gemini Visual Score. The latter, using Gemini 2.5 Flash, provides fine-grained evaluation across ten criteria (e.g., Alignment and Spacing Accuracy, Overall Content Representation), each scored 0-10 based on consistent guidelines (see Appendix C.1 for full details). Code Level Metrics To evaluate code similarity, we propose and design the Code Score, which assesses MLLM-generated against reference HTML. It parses both into Document Object Model (DOM) trees and extracts associated CSS, then performs weighted aggregation. This considers structural similarity, quantified by the Longest Common Subsequence (LCS) ratio of DOM tag sequences. It also assesses content-type similarity for text, images, and forms, where corresponding elements are identified and compared based on content (e.g., text via SequenceMatcher [32]), key styling attributes (e.g., color, font size, image dimensions), and critical attributes (e.g., image src, form element type). An implementation rate for each content type, reflecting the proportion of reference elements found, adjusts these similarity scores to capture both quality and completeness. The final Code Score combines structural and adjusted content-type similarities using predefined weights. Further specifics on the Code Score calculation are available in the Appendix C.2. For the Webpage Design, Visual Level Metrics assess generated design quality. For Webpage Perception QA, standard accuracy (correctly answered multiple-choice questions) is used. The Webpage Code Generation employs both Visual Level Metrics and the Code Score."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Evaluation Settings FullFront-mini Dataset To facilitate rapid iterative evaluation of MLLMs, we constructed FullFrontmini dataset. For specifics on the FullFront-mini setup, see Appendix A. Models We evaluate the performance of ten state-of-the-art MLLMs on the Webpage Perception QA and Webpage Code Generation tasks. This set includes four open-source models (Qwen2.5-VL72B-Instruct [33], InternVL2.5-78B [34], InternVL3-78B [35], and LLaVA-Onevision-72B [36]) and six proprietary models (Claude 3.7 Sonnet [37], Gemini 2.5 Flash [38], Gpt-4o [28], o4-mini [39], GPT-4.1 [40], o1 [41] and Gemini 2.5 Pro [42]). For the Webpage Design task, which targets image generation MLLMs, we test the capabilities of GPT-4o [28] and gemini-2.0-flash-exp-imagegeneration [43]. We report the results for o1 and Gemini 2.5 Pro solely on the FullFront-mini dataset. Table 1: Evaluation results of Webpage Design task. We mark the better results with bold font. Model Gemini Score CLIP Score Human Score GPT-4o gemini-2.0-flash-exp-image-generation 5.4700 2.1000 0.7644 0.7696 6.9600 6.0400 4.2 Main Results Webpage Design On the Webpage Design task, current text-to-image MLLMs exhibit foundational capability in generating general layout concepts but encounter difficulties in producing 5 Table 2: Evaluation results on three Webpage Perception QA tasks. Among the MLLM results, we mark the best results with bold font and the second best with underline. Model FullFront FullFront-mini Real-world Synthetic Multi-window Real-world Synthetic Multi-window Qwen2.5-VL-72B-Instruct InternVL2.5-78B InternVL3-78B LLaVA-Onevision-72B Claude 3.7 Sonnet Gemini 2.5 Flash GPT-4o o4-mini GPT-4.1 o1 Gemini 2.5 Pro Human Expert 0.4696 0.4696 0.4816 0.3296 0.5464 0.4800 0.4448 0.4976 0.4672 0.4950 0.5050 0.5375 0.3275 0.5325 0.4250 0.4675 0.5300 0.4650 0.4267 0.4267 0.4600 0.2733 0.4533 0.3867 0.3733 0.4400 0.3733 0.4550 0.4950 0.4700 0.3450 0.5250 0.4550 0.4450 0.4800 0.4400 0.4350 0.5200 0.9700 0.5100 0.4500 0.5100 0.2900 0.5000 0.4400 0.4400 0.5000 0.4200 0.4600 0.5800 0. 0.4000 0.3400 0.4400 0.2600 0.5600 0.4000 0.3200 0.4600 0.3600 0.4200 0.4800 0.9400 high-fidelity designs that accurately reflect detailed textual descriptions. As shown in Table 1, GPT4o outperforms gemini-2.0-flash-exp-image-generation in both Gemini Score and Human Score. Furthermore, qualitative examples in Appendix D.1 illustrate that GPT-4o demonstrates superior performance in rendering overall page structure, typography, and element implementation fidelity. Table 3: Evaluation results of different models on four Webpage Code Generation tasks. Ref: Code Refinement; Img: Image to code; Inter: Interaction Authoring; Text: Text to code. We mark the best results with bold font and the second best with underline. (mini) indicates the experimental results under the mini dataset setting. Model Code Score Gemini Visual Score CLIP Score Ref Img Inter Text Ref Img Inter Text Ref Img Inter Text Qwen2.5-VL-72B-Instruct 0.56 0.40 0.35 0.33 InternVL2.5-78B 0.49 0.42 InternVL3-78B 0.31 0.14 LLaVA-Onevision-72B 0.68 0.64 Claude 3.7 Sonnet 0.72 0.63 Gemini 2.5 Flash 0.42 0.34 GPT-4o 0.62 0.57 o4-mini 0.67 0.61 GPT-4. 0.40 0.30 0.38 0.06 0.55 0.52 0.36 0.55 0.55 0.44 6.00 4.48 0.45 4.48 4.01 0.47 5.63 4.47 0.38 4.40 1.89 0.60 8.48 8.93 0.52 9.02 8.64 0.46 6.41 5.91 0.54 8.66 8.47 0.54 9.03 8.89 6.22 3.51 4.48 0.45 9.18 8.07 5.81 8.84 9.13 4.08 0.79 0.72 4.37 0.75 0.74 4.44 0.78 0.73 3.73 0.73 0.65 7.84 0.88 0.89 6.83 0.89 0.88 5.54 0.81 0.81 6.94 0.86 0.87 7.42 0.89 0.88 0.76 0.69 0.73 0.58 0.86 0.81 0.76 0.84 0.84 0.73 0.76 0.74 0.71 0.87 0.83 0.77 0.83 0. Model (mini) Code Score Gemini Visual Score CLIP Score Enh Img Inter Text Enh Img Inter Text Enh Img Inter Text Qwen2.5-VL-72B-Instruct 0.53 0.39 0.47 0.30 InternVL2.5-78B 0.60 0.42 InternVL3-78B 0.37 0.15 LLaVA-Onevision-72B 0.73 0.60 Claude 3.7 Sonnet 0.75 0.61 Gemini 2.5 Flash 0.50 0.32 GPT-4o 0.69 0.53 o4-mini 0.68 0.59 GPT-4.1 0.68 0.53 o1 0.68 0.60 Gemini 2.5 Pro 0.41 0.29 0.41 0.04 0.56 0.53 0.33 0.57 0.51 0.41 0.53 0.41 5.77 3.86 0.44 4.61 3.96 0.43 6.25 4.43 0.32 4.93 1.88 0.62 9.00 8.66 0.55 8.88 8.48 0.47 6.98 5.68 0.58 8.66 8.05 0.51 8.60 8.15 0.50 8.44 8.36 0.56 9.17 8.55 6.05 4.17 4.71 0.19 9.08 7.76 4.95 8.95 8.40 7.05 7.99 4.69 0.79 0.72 4.64 0.70 0.72 5.94 0.75 0.74 5.23 0.68 0.62 7.72 0.88 0.90 7.13 0.90 0.87 6.65 0.84 0.80 7.05 0.86 0.86 7.36 0.87 0.88 6.56 0.87 0.89 7.66 0.92 0.88 0.70 0.70 0.73 0.61 0.88 0.81 0.76 0.86 0.81 0.81 0.84 0.78 0.78 0.78 0.75 0.87 0.83 0.78 0.84 0.85 0.79 0. Webpage Perception QA As demonstrated in Table ??, MLLMs generally exhibit weak perceptual capabilities on the Webpage Perception QA task. On the FullFront-mini subset, even the top-performing models, Claude 3.7 Sonnet and Gemini 2.5 Pro, achieve an average accuracy barely exceeding 50% across the three subtasks. Conversely, LLaVA-OneVision-72Bs accuracy remains below 35% on all QA subtasks. Critically, all models performe significantly worse than human experts, with accuracy gaps of 44.5%, 38%, and 38% on three subtasks respectively, highlighting their challenges in fine-grained page perception. Notably, this task reveals no substantial perfor6 mance disparity between closed-source and open-source models; for instance, on the full FullFront benchmark, InternVL3-78B achieves leading accuracies of 53.75% on Synthetic QA and 46.00% on Multi-window QA. Further analysis indicates nearly identical model performance on single-page Real-world and Synthetic QA, while performance degrades considerably on the more complex Multi-window QA. Table 4: Human Evaluation of MLLM-generated webpages on FullFront-mini. We mark the best results with bold font and the second best with underline. Model Ref Image Inter Text Table 5: Interaction rate results (%). We mark the best results with bold font and the second best with underline. Model Rate Rate (mini) Qwen2.5-VL-72B-Instruct InternVL2.5-78B InternVL3-78B LLaVA-Onevision-72B Claude 3.7 Sonnet Gemini 2.5 Flash GPT-4o o4-mini GPT-4.1 o1 Gemini 2.5 Pro 5.43 5.05 4.99 4.75 8.36 8.20 7.42 7.70 7.70 8.22 8. 4.35 3.33 3.78 2.56 7.93 7.82 5.81 7.25 8.05 8.40 8.45 5.17 2.72 4.14 0.57 8.21 6.92 4.90 8.17 7.26 6.80 7.71 4.46 4.88 5.28 4.33 7.96 6.92 5.65 7.71 7.50 6.25 7.43 Qwen2.5-VL-72B-Instruct 57.00 47.00 InternVL2.5-78B 48.00 InternVL3-78B 16.00 LLaVA-Onevision-72B 78.00 Claude 3.7 Sonnet 70.00 Gemini 2.5 Flash 80.00 GPT-4o 93.00 o4-mini 78.00 GPT-4.1 o1 Gemini 2.5 Pro 60.00 45.00 40.00 20.00 80.00 65.00 70.00 90.00 75.00 80.00 85.00 Webpage Code Generation In the Webpage Code Generation task, closed-source models significantly outperform their open-source counterparts across all subtasks and metrics, with no open-source model securing top-two position in any category. As detailed in Table 3, Claude 3.7 Sonnet consistently leads, closely followed by other proprietary models like Gemini 2.5 Pro, Gemini 2.5 Flash, and GPT-4.1, all demonstrating impressive, top-tier results. For instance, in the Code Refinement task on the FullFront-mini, Gemini 2.5 Pro achieves Gemini Visual Score of 9.17, indicating near-perfect visual reproduction in most cases, whereas the best-performing open-source model, InternVL3-78B, scores only 6.25 under the same settings. While Qwen2.5-VL-72B-Instruct and InternVL3-78B show relatively strong performance among open-source options, their scores are generally comparable only to GPT-4o rather than the leading closed-source models. consistent trend across models is the alignment of performance across different metrics; models excelling in one visual or code-based score typically perform similarly well in others. Subtask analysis reveals distinct patterns: providing partial HTML (Code Refinement) improves performance over image-only inputs (Image to Code). However, generating functional interactive code (Interaction Authoring) is more challenging, yielding lower scores despite simpler HTML-v1 targets, difficulty underscored by interaction implementation rates  (Table 5)  where closed-source models exceed 65% success, far surpassing open-source models like LLaVA-Onevision-72B (16%). The Text to Code task, requiring autonomous design from textual descriptions, proves the most difficult, resulting in the lowest overall model performance. Blind human evaluation on the FullFront-mini dataset, using Gemini Visual Score criteria  (Table 4)  , further confirms that closed-source models like Claude 3.7 Sonnet and Gemini 2.5 Pro are perceived as more accurate, frequently scoring above 8/10 for reproduction quality. While these models achieve high overall fidelity, illustrative examples in Appendix D.2 reveal that even top performers can exhibit minor imperfections in fine-grained details."
        },
        {
            "title": "5 Discussion",
            "content": "5.1 Where do MLLMs struggle most in perceiving webpages? By analyzing the error types of 200 questions that all MLLMs (except o1 and Gemini 2.5 Pro) fail to answer correctly, we gain insight into the primary difficulties current MLLMs face in page perception. As shown in Figure 4 (a), MLLMs exhibit particular difficulty in accurately understanding the alignment (21.5%), size (19.5%), spacing (15.5%), and precise positioning (18.5%) of page elements. These factors constitute the core reasons behind perception failures. For example, Figure (b) shows an instance where MLLMs fail to correctly identify the position of the tag labeled Human Rights Advocates relative to the main title and subtitle, while Figure (c) demonstrates an incorrect comparison of the sizes of two LEARN MORE buttons. 7 Figure 4: MLLM Errors in Webpage Perception QA. (a) Distribution of error types for 200 questions. (b) An illustrative example of Positioning Error. (c) An illustrative example of Size Error. Figure 5: Three common errors in Webpage Code Generation. (a) Abnormal Image Sizes, where an image within the rendered page is disproportionately large. (b) Blank Pages, showing an entirely blank rendered output. (c) Isolation Error, demonstrating an output consisting only of an isolated interactive element. 5.2 What is the relationship between perceptual ability and code performance? Counter-intuitively, the results in Table ?? and Table 3 indicate that models excelling in perceptual tasks dont invariably excel in code generation, despite their capacity for more detailed page comprehension. Admittedly, some models, such as Claude 3.7 Sonnet and Gemini 2.5 Pro, perform strongly across both task categories. However, InternVL3-78B, though surpassing Gemini 2.5 Flash in perceptual QA, exhibits noticeable disparity in its code generation capabilities. similar pattern is observed between InternVL2.5-78B and GPT-4o. We attempt to analyze the underlying reasons for this phenomenon. As illustrated in Figure 4 (b), all tested models incorrectly identified the position of the Human Rights Advocate tag relative to the title during the perceptual QA phase. Yet, upon analyzing their generated pages (see appendix D.3), all models correctly placed the tag directly above the title during implementation. This observation implies that even when models err in fine-grained perception, they can still produce visually coherent and structurally sound webpages. It suggests that the processes for visual perception in QA and for translating visual concepts into code might operate with different sensitivities or rely on distinct internal representations and generation strategies within MLLMs, characteristic warranting future investigation. 5.3 Can MLLMs be an excellent front-end engineer? To determine if MLLM-generated pages are superior to real-world versions, three human experts blindly evaluate 100 webpage generated by various MLLMs (except o1 and Gemini 2.5 Pro) against their real-world counterparts. Results in Figure 6 indicate that leading models (e.g., o4-mini, Gemini 2.5 Flash) are, in the vast majority of cases, superior to their real-world counterparts. However, further analysis of the generated webpages reveals that MLLMs can exhibit three prevalent error categories, illustrated in Figure 5: Abnormal Image Size (abnormally large images disrupting layout integrity), Blank Image (entirely blank screenshots despite non-empty code), and Isolation Error 8 Figure 6: Human evaluation comparing MLLMs-generated and Real-World webpages. Table 6: Counts of three error types in Webpage Code Generation tasks. Size: Abnormal Image Size; Blacnk: Blank Image; Isolation: Isonlation Error. Size Isolation Blank Model Table 7: Detailed Code-level performance (Structure, Text, Image, Form) on FullFrontmini. We mark the best results with bold font and the second best with underline. Ref Img Inter Text Ref Img Inter Text Inter Model Structure Text Image Form Qwen2.5-VL-72B-Instuct 9 1 InternVL2.5-78B 2 InternVL3-78B 0 LLaVA-Onevision-72B 1 Claude 3.7 Sonnet 0 Gemini 2.5 Flash 1 GPT-4o 5 o4-mini 0 GPT-4.1 0 o1 (mini) 0 Gemini 2.5 Pro (mini) 62 11 2 20 5 20 3 22 0 1 2 3 0 11 2 9 0 1 1 0 0 0 19 14 18 24 0 1 11 3 0 2 0 3 0 2 1 0 0 0 0 0 0 2 4 14 12 14 10 1 45 0 0 0 0 0 0 0 0 2 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 2 11 1 88 0 0 0 0 0 0 0 Qwen2.5-VL-72B-Instuct InternVL2.5-78B InternVL3-78B LLaVA-Onevision-72B Claude 3.7 Sonnet Gemini 2.5 Flash GPT-4o o4-mini GPT-4.1 o1 Claude 3.7 Pro 0.51 0.43 0.51 0.25 0.73 0.69 0.45 0.65 0.62 0.58 0.68 0.37 0.27 0.35 0.12 0.58 0.55 0.30 0.52 0.46 0.43 0. 0.44 0.40 0.59 0.16 0.65 0.72 0.47 0.63 0.63 0.61 0.44 0.38 0.33 0.39 0.20 0.50 0.45 0.32 0.55 0.60 0.45 0.38 (instances where only isolated interactive buttons are generated, neglecting page content). Each error type significantly degrades the effectiveness of the generated webpage. Table 6 shows that open-source models exhibit these errors markedly more often than closed-source counterparts; this considerably diminishes their reliability and stability. Furthermore, detailed examination of codelevel performance  (Table 7)  indicates that current MLLMs still have substantial room for improvement in text and form implementation, as similarity scores for these components do not exceed 0.6. Overall, despite certain shortcomings in fine-grained details, MLLMs do demonstrate the capability to design generally coherent webpage interfaces from textual descriptions and can generate corresponding code from webpage screenshots. However, the overall deficiencies in their perceptual abilities, coupled with the potential for critical errors during code generation, render their current reliability and stability uncertain. We believe promising future direction involves integrating MLLMs with specialized tools. This can compensate for their perceptual limitations and provide mechanisms to identify and rectify generation anomalies, thereby aiding MLLMs in their evolution towards becoming excellent front-end engineers."
        },
        {
            "title": "6 Summary",
            "content": "We introduce FullFront, pioneering and comprehensive Multimodal Front-End Benchmark. FullFront is designed to systematically evaluate the capabilities of MLLMs across the full front-end development pipeline, including design, page perception, and code generation. By constructing high-quality, diverse synthetic data and designing multi-layered evaluation system, FullFront serves as powerful tool for analyzing the strengths and limitations of current MLLMs, particularly highlighting the challenges MLLMs face in handling complex front-end details (such as image sizing and interaction implementation) and accurately perceiving webpage elements. While FullFront, like any benchmark, possesses limitations, future work can improve upon it by introducing more advanced evaluation metrics, expanding the dataset size, or exploring new task types. Nevertheless, the introduction of FullFront sets new standard for assessing MLLMs on Front-end, laying the foundation for the development of the next generation of intelligent webpage development tools."
        },
        {
            "title": "References",
            "content": "[1] H. Guo, W. Zhang, J. Chen, Y. Gu, J. Yang, J. Du, B. Hui, T. Liu, J. Ma, C. Zhou et al., Iw-bench: Evaluating large multimodal models for converting image-to-web, arXiv preprint arXiv:2409.18980, 2024. [2] Y. Gui, Z. Li, Y. Wan, Y. Shi, H. Zhang, B. Chen, Y. Su, D. Chen, S. Wu, X. Zhou et al., Webcode2m: real-world dataset for code generation from webpage designs, in Proceedings of the ACM on Web Conference 2025, 2025, pp. 18341845. [3] M. Wang, S. Sunkara, G. Baechler, J. Lin, Y. Zhu, F. Zubach, L. Shu, and J. Chen, Webquest: benchmark for multimodal qa on web page sequences, arXiv preprint arXiv:2409.13711, 2024. [4] Y. Chang, M. Narang, H. Suzuki, G. Cao, J. Gao, and Y. Bisk, Webqa: Multihop and multimodal qa, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 16 49516 504. [5] H. H. Zhao, D. Gao, and M. Z. Shou, Worldgui: Dynamic testing for comprehensive desktop gui automation, arXiv preprint arXiv:2502.08047, 2025. [6] Z. Wu, C. Han, Z. Ding, Z. Weng, Z. Liu, S. Yao, T. Yu, and L. Kong, Os-copilot: Towards generalist computer agents with self-improvement, arXiv preprint arXiv:2402.07456, 2024. [7] W. Tan, W. Zhang, X. Xu, H. Xia, Z. Ding, B. Li, B. Zhou, J. Yue, J. Jiang, Y. Li et al., Cradle: Empowering foundation agents towards general computer control, arXiv preprint arXiv:2403.03186, 2024. [8] Z. Wang, H. Xu, J. Wang, X. Zhang, M. Yan, J. Zhang, F. Huang, and H. Ji, Mobile-agent-e: Self-evolving mobile assistant for complex tasks, arXiv preprint arXiv:2501.11733, 2025. [9] B. Zheng, B. Gou, J. Kil, H. Sun, and Y. Su, Gpt-4v (ision) is generalist web agent, if grounded, arXiv preprint arXiv:2401.01614, 2024. [10] O. Yoran, S. J. Amouyal, C. Malaviya, B. Bogin, O. Press, and J. Berant, Assistantbench: Can web agents solve realistic and time-consuming tasks? arXiv preprint arXiv:2407.15711, 2024. [11] K. Cheng, Q. Sun, Y. Chu, F. Xu, Y. Li, J. Zhang, and Z. Wu, Seeclick: Harnessing gui grounding for advanced visual gui agents, arXiv preprint arXiv:2401.10935, 2024. [12] X. Deng, Y. Gu, B. Zheng, S. Chen, S. Stevens, B. Wang, H. Sun, and Y. Su, Mind2web: Towards generalist agent for the web, Advances in Neural Information Processing Systems, vol. 36, pp. 28 09128 114, 2023. [13] Z. Hui, Y. Li, T. Chen, C. Banbury, K. Koishida et al., Winclick: Gui grounding with multimodal large language models, arXiv preprint arXiv:2503.04730, 2025. [14] D. Chen, Y. Huang, S. Wu, J. Tang, L. Chen, Y. Bai, Z. He, C. Wang, H. Zhou, Y. Li et al., Guiworld: dataset for gui-oriented multimodal llm-based agents, arXiv e-prints, pp. arXiv2406, 2024. [15] W. Chen, J. Cui, J. Hu, Y. Qin, J. Fang, Y. Zhao, C. Wang, J. Liu, G. Chen, Y. Huo et al., Guicourse: From general vision language models to versatile gui agents, arXiv preprint arXiv:2406.11317, 2024. [16] X. Chen, Z. Zhao, L. Chen, D. Zhang, J. Ji, A. Luo, Y. Xiong, and K. Yu, Websrc: dataset for web-based structural reading comprehension, arXiv preprint arXiv:2101.09465, 2021. [17] J. Wu, W. Yin, Y. Jiang, Z. Wang, Z. Xi, R. Fang, L. Zhang, Y. He, D. Zhou, P. Xie et al., Webwalker: Benchmarking llms in web traversal, arXiv preprint arXiv:2501.07572, 2025. [18] Y. Hao, J. Gu, H. W. Wang, L. Li, Z. Yang, L. Wang, and Y. Cheng, Can mllms reason in multimodality? emma: An enhanced multimodal reasoning benchmark, arXiv preprint arXiv:2501.05444, 2025. [19] B. Wang, G. Li, X. Zhou, Z. Chen, T. Grossman, and Y. Li, Screen2words: Automatic mobile ui summarization with multimodal learning, in The 34th Annual ACM Symposium on User Interface Software and Technology, 2021, pp. 498510. [20] J. Liu, Y. Song, B. Y. Lin, W. Lam, G. Neubig, Y. Li, and X. Yue, Visualwebbench: How far have multimodal llms evolved in web page understanding and grounding? arXiv preprint arXiv:2404.05955, 2024. 10 [21] C. Si, Y. Zhang, R. Li, Z. Yang, R. Liu, and D. Yang, Design2code: Benchmarking multimodal code generation for automated front-end engineering, arXiv preprint arXiv:2403.03163, 2024. [22] S. Yun, H. Lin, R. Thushara, M. Q. Bhat, Y. Wang, Z. Jiang, M. Deng, J. Wang, T. Tao, J. Li et al., Web2code: large-scale webpage-to-code dataset and evaluation framework for multimodal llms, arXiv preprint arXiv:2406.20098, 2024. [23] H. Laurençon, L. Tronchon, and V. Sanh, Unlocking the conversion of web screenshots into html code with the websight dataset, arXiv preprint arXiv:2403.09029, 2024. [24] T. Beltramelli, pix2code: Generating code from graphical user interface screenshot, in Proceedings of the ACM SIGCHI symposium on engineering interactive computing systems, 2018, pp. 16. [25] Y. Wan, Y. Dong, J. Xiao, Y. Huo, W. Wang, and M. R. Lyu, Mrweb: An exploration of generating multi-page resource-aware web code from ui designs, arXiv preprint arXiv:2412.15310, 2024. [26] J. Xiao, Y. Wan, Y. Huo, Z. Xu, and M. R. Lyu, Interaction2code: How far are we from automatic interactive webpage generation? arXiv preprint arXiv:2411.03292, 2024. [27] B. Gou, R. Wang, B. Zheng, Y. Xie, C. Chang, Y. Shu, H. Sun, and Y. Su, Navigating the digital world as humans do: Universal visual grounding for gui agents, arXiv preprint arXiv:2410.05243, 2024. [28] OpenAI, Hello gpt-4o, https://openai.com/index/hello-gpt-4o/. [29] Y. Lu, J. Yang, Y. Shen, and A. Awadallah, Omniparser for pure vision based gui agent, 2024. [Online]. Available: https://arxiv.org/abs/2408.00203 [30] C. Crawl, Common crawl datasets, 2025, accessed: 2025-05-01. [Online]. Available: https://data.commoncrawl.org/ [31] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., Learning transferable visual models from natural language supervision, in International conference on machine learning. PmLR, 2021, pp. 87488763. [32] mdiff, https://github.com/m-matelski/mdiff. [33] S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang et al., Qwen2. 5-vl technical report, arXiv preprint arXiv:2502.13923, 2025. [34] Z. Chen, W. Wang, Y. Cao, Y. Liu, Z. Gao, E. Cui, J. Zhu, S. Ye, H. Tian, Z. Liu et al., Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling, arXiv preprint arXiv:2412.05271, 2024. [35] J. Zhu, W. Wang, Z. Chen, Z. Liu, S. Ye, L. Gu, Y. Duan, H. Tian, W. Su, J. Shao et al., Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models, arXiv preprint arXiv:2504.10479, 2025. [36] B. Li, Y. Zhang, D. Guo, R. Zhang, F. Li, H. Zhang, K. Zhang, Y. Li, Z. Liu, and C. Li, Llava-onevision: Easy visual task transfer, arXiv preprint arXiv:2408.03326, 2024. [37] Anthropic, Claude 3.7 sonnet and claude code, https://https://www.anthropic.com/news/ claude-3-7-sonnet. [38] G. Deepmind, Gemini 2.5 flash, https://deepmind.google/technologies/gemini/flash/. [39] OpenAI, Introducing openai o3 and o4-mini, https://openai.com/index/ introducing-o3-and-o4-mini/. [40] , Introducing gpt-4.1 in the api, https://openai.com/index/gpt-4-1/. [41] , Learning to reason with llms, https://openai.com/index/learning-to-reason-with-llms/. [42] G. Deepmind, Gemini 2.5: Our most intelligent ai model, https://blog.google/technology/ google-deepmind/gemini-model-thinking-updates-march-2025/. [43] K. Kampf and N. Brichtova, Experiment with gemini 2.0 flash native image generation, https://developers.googleblog.com/en/experiment-with-gemini-20-flash-native-imagegeneration/. 11 FullFront-mini FullFront-mini Dataset To facilitate rapid iterative evaluation of MLLMs and initial exploration of the benchmark by researchers, we constructed FullFront-mini dataset. This subset is condensed version of the full FullFront dataset, with the following specific composition. Webpage Perception QA: Includes 200 Real-world QA, 100 Synthetic QA, and 50 Multi-window QA data samples. Webpage Code Generation: Comprises 20 Image to Code, 10 Text to code, 20 Code Authoring (with 2 samples for each interaction type), and 10 Code Refinement data samples. Webpage Design: Consists of 10 Webpage Design task data samples."
        },
        {
            "title": "B Webpage Code Generation Details",
            "content": "B.1 Category-based utitization strategy for images Regarding images, instead of using simple placeholders, we employ category-based utilization strategy. We classify common real-world image content into 15 predefined categories: People, Animal, Food, Plant, Landscape, Icon, Logo, Architecture, Technology, Transportation, Map, Texture, Art, Movie, and Other (visualized in Figure 7). Each category is linked to fixed, non-copyrighted image URL following standardized format, such as https://fixed_part/{Category}.jpg. During the ground truth generation for code tasks, GPT-4o and Claude 3.7 Sonnet are instructed to select an appropriate category for any required image and use its corresponding URL. For evaluation, when an MLLM is tasked with generating webpage code, it must understand the image content from the provided screenshot, classify it into one of these 15 categories, and then generate HTML using the correct category-specific URL. Furthermore, because the intrinsic dimensions of these repository images are unknown, the MLLM is explicitly required to manually set the image sizes (width and height) and position within the HTML code to ensure the rendered output matches the layout depicted in the screenshot. This approach assesses MLLMs capabilities in image perception, categorization, and appropriate styling. It also ensures visual consistency for subsequent evaluation steps. Crucially, by deriving visual designs from real-world screenshots, our method generates webpages with greater diversity compared to from scratch techniques. This strategy ingeniously bypasses the laborious simplification of real-world code while still achieving simplifications primary goalssuch as removing sensitive information and external dependenciesand preserving as much original visual information as possible through categorized representation. The use of categoryspecific image URLs also facilitates straightforward dataset expansion with new image types in the future. B.2 Subtask Specifications Image to Code (200 samples) evaluates direct HTML generation from these HTML-v2 rendered screenshots: This task evaluates an MLLMs ability to generate HTML code that accurately replicates given webpage screenshot, which is rendered from HTML-v2. This is the most straightforward screenshot-to-code generation task. Text to Code (50 samples): This task aims to evaluate MLLMs ability to generate webpage code solely based on textual description. We randomly select 50 pages rendered from the HTML-v2 and use Claude 3.7 Sonnet to generate detailed textual descriptions of these pages. During testing, MLLMs only receives these textual descriptions as input, with the goal of generating HTML code that reproduces the original webpage. All textual descriptions undergo second round of manual review to ensure their accuracy and quality. Interaction Authoring (100 samples): Moving beyond static page generation, this task evaluates MLLMs ability to implement dynamic, interactive webpages. Inspired by Interaction2Code [26], we define ten common interaction types, categorized under click and hover events. For data construction, 100 samples derived from the static HTML-v1 (allowing models to focus primarily on interaction logic) are augmented with interaction code by Claude 3.7 Sonnet, followed by manual verification. During testing, MLLMs receive before and after interaction screenshots and must reproduce the static page while implementing the depicted interactive behavior. To facilitate automated interaction 12 Figure 7: The 15 predefined image categories used in FullFront for standardized image representation. detection, MLLMs are instructed to assign the ID #InteractionPart to the primary HTML element involved. The ten defined interaction types, with specific implementation requirements for each, are: 1. Click to Display Dropdown (Interaction_click_1): An element, when clicked, reveals dropdown menu whose content, position, and style are contextually adapted. Requires aria-expanded attribute toggling and specific dropdown selectors. 2. Click to Toggle Checkbox (Interaction_click_2): clickable checkbox that toggles its checked/unchecked state. Must use an <input type=\"checkbox\"> or an element with role=checkbox, displaying checked state after interaction. 3. Click to Change Background Color (Interaction_click_3): An element significantly changes its background color upon being clicked, with the new color being distinct and detectable. 4. Click to Display Modal/Dialog (Interaction_click_4): Clicking an element triggers modal window or dialog box with contextually generated content and styling. The modal must match specific selectors like .modal or [aria-modal=true]. 5. Click to Display Tooltip (Interaction_click_5): An element, when clicked, displays tooltip providing additional information. The tooltip must adhere to specified class names or attributes (e.g., .tooltip, [role=tooltip]). 6. Click to Display Text Input (Interaction_click_6): Clicking an element reveals text box or input area for user entry, appropriately sized and positioned. 7. Hover to Display Dropdown (Interaction_hover_1): dropdown menu appears when the mouse hovers over an element, with adaptive content and styling. 8. Hover to Bold Text (Interaction_hover_2): Text within an element becomes bold (fontWeight 600 or bold/bolder) upon mouse hover. 9. Hover to Underline Text (Interaction_hover_3): Text within an element gains an underline when hovered over, with the computed textDecoration including underline. 10. Hover to Display Tooltip (Interaction_hover_4): tooltip with additional information appears when the mouse hovers over an element, conforming to specific class or attribute requirements. 13 The models must determine the correct interaction type from the visual cues and implement it according to these detailed specifications, providing the complete HTML, CSS, and JavaScript in single file. Code Refinement (50 samples): In this task, the model receives webpage screenshot rendered from HTML-v2 and its HTML-v1 code. The goal is to refine the HTML-v1 code based on the screenshot to match the quality of HTML-v2, simulating code optimization and enhancement scenarios."
        },
        {
            "title": "C Evaluation Metrics Specifications",
            "content": "C.1 Gemini Visual Score: Criteria and Rubric To facilitate fine-grained and human-aligned visual assessment of MLLM-generated webpages, we employ the Gemini 2.5 Flash model as sophisticated visual evaluator. This model is tasked with comparing rendered webpage image (generated by an MLLM) against its corresponding ground-truth image. For each pair, the evaluator provides quantitative assessment across ten distinct visual dimensions. Each dimension is scored on scale of 0 to 10, where score of 10 signifies perfect identity between the two images in that specific aspect, and score of 0 indicates no discernible similarity. Scores between 1 and 9 represent varying degrees of partial similarity, with higher values denoting closer resemblance. The prompt provided to the Gemini 2.5 Flash model for this evaluation is as follows: Your task is to assess two webpage images and output score between 0 and 10 for each of the following 10 questions, reflecting the degree of similarity between the webpages. score of 10 indicates perfect similarity (identical in every aspect), while score of 0 indicates no similarity. For partial similarities, assign score between 1 and 9, where higher scores reflect greater similarity. Only output comma-separated list of 10 numbers enclosed in square brackets, e.g., [10,8,6,4,2,0,0,0,0,0]. Do not assign score of 10 unless the two images are identical in the respective category. The ten evaluation criteria, along with guiding examples for scoring, are detailed below. These criteria are designed to cover comprehensive range of visual attributes that contribute to the overall quality and fidelity of webpage design. 1. Element Reproduction (Score: 0-10): This criterion assesses whether all key visual elements present in the ground-truth design (e.g., textual content, images, buttons, icons, input fields) are fully reproduced in the generated webpage. It also considers if these reproduced elements are styled identically to the original in terms of appearance (e.g., color, shape, visual effects). Score 10: All key elements are present, correctly placed, and styled identically to the original. Score 5-7: Most key elements are present, but some may be missing, slightly altered in style (e.g., wrong button color, different icon), or have minor placement deviations. Score 1-4: Significant elements are missing, or many elements are present but styled very differently. Score 0: Elements are completely different or almost all key elements are absent. 2. Proportion and Size Consistency (Score: 0-10): This evaluates if the relative and absolute sizes and proportions of all elements (including text blocks, images, buttons, and containers) in the generated page match those in the ground-truth design, thereby maintaining the intended visual harmony and balance. Score 10: All elements maintain exact proportions and sizes relative to each other and the overall page, as in the original. Score 6-8: Minor, barely noticeable differences in element sizes or proportions. The overall visual balance is largely maintained. 14 Score 1-5: Noticeable discrepancies in the size or proportion of several elements, potentially disrupting visual harmony. Score 0: Significant, widespread discrepancies in element sizes and proportions, leading to substantially different visual feel. 3. Layout and Typography Fidelity (Score: 0-10): This focuses on the faithful replication of the overall page structure and typographic choices. It examines the placement and styling of major layout components such as headers, footers, navigation bars, sidebars, content grids, and columns, as well as the consistent application of typography (font families, weights) across these structural elements. Score 10: The overall layout structure and typography are identical to the original design. Score 5-7: The layout is structurally similar with correct identification of major sections, but there might be minor deviations in the exact placement, dimensions, or typographic details of these sections. Score 1-4: The layout significantly deviates from the original, or key structural components are missing or incorrectly implemented. Score 0: The layout is entirely different from the original design. 4. Alignment and Spacing Accuracy (Score: 0-10): This criterion measures the precision of element alignment (e.g., left, right, center, justified, relative to other elements) and the consistency of spacing (margins, padding, gutters) both within and between elements, compared to the ground-truth design. Score 10: All elements exhibit perfect alignment and spacing as per the original design. Score 6-8: Minor, subtle misalignments or inconsistent spacing that do not significantly impact readability or aesthetics. Score 1-5: Noticeable and frequent misalignments or spacing issues that detract from the designs polish. Score 0: Major, pervasive misalignments and spacing errors leading to disorganized appearance. 5. Visual Hierarchy Clarity (Score: 0-10): This assesses if the generated webpage successfully maintains the same visual hierarchy as the original design. This means that the relative prominence and order of importance of different elements (achieved through size, color, contrast, placement, etc.) should guide the users attention similarly, allowing for quick identification of key information and calls to action. Score 10: The visual hierarchy is identical, with elements carrying the same emphasis and importance as the original. Score 5-7: The overall hierarchy is preserved, but there might be slight alterations in the emphasis of certain elements, or minor confusion in the flow. Score 1-4: The visual hierarchy is noticeably different or unclear, making it difficult to identify key information. Score 0: The visual hierarchy is completely different or absent, leading to confusing user experience. 6. Color Consistency (Score: 0-10): This evaluates the match of the overall color scheme, including primary, secondary, and accent colors, as well as specific hues, saturation, and brightness levels used throughout the generated webpage, compared to the ground-truth. Score 10: All colors, including background, text, and element colors, are identical to the original design. Score 6-8: The color palette is very similar, with only minor, hard-to-detect variations in hue, saturation, or brightness. Score 1-5: Noticeable differences in key colors, or palette that is thematically similar but clearly distinguishable. Score 0: The color scheme is completely different from the original design. 7. Style Consistency (Score: 0-10): This criterion judges whether the overall aesthetic style of the generated webpage (e.g., modern, minimalistic, brutalist, skeuomorphic, playful) aligns with the intended style of the original design. This is more holistic assessment of the look and feel. Score 10: The overall aesthetic style is identical to the original. Score 4-7: The style is broadly similar (e.g., both are modern), but there are distinguishable differences in execution or specific stylistic choices that make it not an exact match. Score 1-3: The style is tangentially related or only shares very few common elements, but is mostly different. Score 0: The aesthetic style is entirely different from the original design. 8. Text Style Consistency (Score: 0-10): This focuses specifically on the typographic attributes of textual content, such as font family, size, weight, style (italic, bold), color, line height, letter spacing, paragraph spacing, and text alignment, ensuring they are consistent with the original design specifications. Score 10: All text attributes (font, size, spacing, color, alignment, etc.) are identical to the original. Score 5-7: Fonts are similar (e.g., correct family but slightly off weight or size), or there are minor inconsistencies in line/paragraph spacing or alignment. Score 1-4: Significant deviations in font choices, sizes, or other text styling attributes. Score 0: Text styles are completely different. 9. Text Content Accuracy (Score: 0-10): This evaluates if the primary textual content (headings, body text, labels, captions) displayed on the generated webpage accurately reproduces the text from the original design, without omissions, additions, or substantial alterations. Score 10: All main textual content is identical to the original. Score 5-7: Most text is identical, but there are minor typos, omissions of small phrases, or slight rephrasing that doesnt change the core meaning. Score 1-4: Significant portions of text are missing, incorrect, or substantially altered. Score 0: The textual content is entirely different or almost completely absent. 10. Overall Content Representation (Score: 0-10): This is holistic measure of whether the generated webpage effectively conveys the same core information, message, purpose, and user intent as the original design, considering all visual and textual elements collectively. Score 10: The generated page perfectly represents the same content, information, and intent as the original. Score 6-8: The core content and intent are conveyed, but some secondary information might be missing, presented less clearly, or slightly altered. Score 1-5: The generated page conveys significantly different or incomplete set of information or intent compared to the original. Score 0: The content representation is entirely different, conveying different message or purpose. The model is instructed to output these ten scores as comma-separated list enclosed in square brackets, for example: [10,8,6,4,2,0,0,0,0,0]. This structured output facilitates automated parsing and aggregation of the visual evaluation results. C.2 Code Score: Formulation and Components Our Code Score evaluates the similarity between an MLLM-generated HTML document (Hgen) and reference HTML document (Href ). The process involves parsing both documents into Document Object Model (DOM) trees, extracting associated CSS, and then performing weighted aggregation of several similarity aspects. 1. Structural Similarity Both Hgen and Href are parsed into DOM trees. We then extract sequences of HTML tags, Sgen and Sref respectively, representing the structural hierarchy of the documents (as implemented in extract_structure and structure_to_sequence). The structural similarity (Simstruct) is quantified by the ratio of the length of the Longest Common Subsequence (LCS) of these tag sequences to the length of the reference sequence, Sref . threshold (θmatch = 0.9 16 in our implementation) is used within the LCS calculation (lcs_length_with_threshold) to determine if two tags are considered similar enough to be part of common subsequence. Simstruct = LCS_Lengthθmatch (Sgen, Sref ) Length(Sref ) (1) If Length(Sref ) is zero, Simstruct is defined as 1.0. 2. Content-Type Similarity This assesses similarity for three key content types: text, images, and forms. For each type, corresponding elements are identified and compared. Element Matching For each content type {text, image, form}, we extract all elements of that type from Hgen (denoted Egen,c) and Href (denoted Eref,c). matching algorithm (match_elements) identifies optimal corresponding pairs (egen, eref ) between Egen,c and Eref,c based on type-specific similarity measures (detailed below) and matching threshold (θmatch = 0.9). This process yields set of matched pairs Mc. Implementation Rate For each content type c, an implementation rate (Ratec) is calculated. This reflects the proportion of reference elements found and successfully matched in the generated HTML: Ratec = Mc Eref,c (2) Eref,c If image_implementation_rate, and form_implementation_rate. is zero, Ratec is 1.0. The code tracks text_implementation_rate, Similarity Scores for Matched Elements For each matched pair (egen, eref ) Mc: Text Elements (c = text): Similarity is assessed based on: 1. Content Similarity (Simtext_content(egen, eref )): SequenceMatcher on the textual content extracted. Calculated using Pythons 2. Style Similarity (Simtext_style(egen, eref )): Computed by comparing key CSS properties (e.g., color, font-size, font-weight, background-color, etc.). Each property has weight wp. The style similarity is weighted average of individual property similarities. Numerical properties (e.g., sizes) are compared using ratio, while string properties use SequenceMatcher. The average Simtext_content and Simtext_style are calculated over all matched text elements. Image Elements (c = image): Similarity (Simimage(egen, eref )) is weighted combination of: 1. URL Similarity (0.6 weight): Based on comparing extracted category information from the image src attribute (e.g., Animal from .../Animal.jpg) or filenames if the category pattern doesnt match. 2. Style Similarity (0.3 weight): styles, using image-specific CSS properties (e.g., width, height, border-radius, as per self.style_weights[image]). 3. Alt Text Similarity (0.1 weight): Comparing the alt attributes using Calculated similarly to text SequenceMatcher. The average Simimage is calculated over all matched image elements. Form Elements (c = form): Similarity (Simf orm(egen, eref )) depends on the specific form element type (e.g., input, button). Its generally weighted combination of: 1. Attribute Similarity: Compares critical HTML attributes specific to the form element type (e.g., type, name, value, placeholder for input elements) using SequenceMatcher. 2. Style Similarity: Calculated using form-specific CSS properties (e.g., width, height, background-color). 3. Text Content Similarity (only for elements like button, label, option): Compares textual content using SequenceMatcher. The average Simf orm is calculated over all matched form elements. 17 Figure 8: Comparative Webpage Designs: Ground Truth (Label Webpage) vs. gemini-2.0-flashexp-image-generation and GPT-4o. The image displays (from left to right) the ground truth webpage, the design generated by gemini-2.0-flash-exp-image-generation, and the design generated by GPT-4o. Adjusted Similarity Scores The average similarity scores for each content aspect are then adjusted by their respective implementation rates to penalize incompleteness: Sim Sim text_content = Simtext_content Ratetext text_style = Simtext_style Ratetext Sim image = Simimage Rateimage Sim orm = Simf orm Ratef orm (3) (4) (5) (6) where Sim denotes the average similarity for matched elements of that type. 3. Final Code Score Aggregation The final Code Score (Scorecode) is weighted sum of the structural similarity and the adjusted content-type similarities: Scorecode = Wstruct Simstruct + Wtext_content Sim text_content + Wtext_style Sim image + Wf orm Sim text_style orm (7) + Wimage Sim This multi-faceted Code Score provides nuanced evaluation of the generated HTML, considering its structural integrity, content accuracy, stylistic fidelity, and overall completeness across different element types."
        },
        {
            "title": "D Qualitative Analysis and Case Studies",
            "content": "D.1 Webpage Design This section provides qualitative examples to supplement the quantitative results for the Webpage Design task. The Webpage Design task evaluates an MLLMs ability to generate visual webpage design based on textual description, assessing its capacity for conceptualization within the front-end workflow. Figure 8 illustrates the outputs from two evaluated text-to-image MLLMs, GPT-4o and gemini-2.0-flash-exp-image-generation, alongside the target Label Webpage (ground truth) for representative example. As observed in Figure 8, the design generated by GPT-4o (right) demonstrates notably closer resemblance to the Label Webpage (left) compared to the output from gemini-2.0-flash-exp-imagegeneration (middle). Specifically: Layout and Structure: GPT-4o more successfully replicates the overall page structure, including the header, hero section, Popular Categories grid, and footer arrangement. The placement and relative sizing of these major components are more aligned with the ground truth. In contrast, the gemini-2.0flash-exp-image-generation produces layout that, while containing some similar thematic elements 18 Figure 9: Qualitative Comparison of Webpage Code Generation by MLLMs. This figure illustrates the visual fidelity of webpages generated by various closed-source (Claude 3.7 Sonnet, O4-mini, Gemini 2.5 Flash, GPT-4o) and open-source (InternVL3-78B, LLaVA-Onevision) MLLMs against the ground truth (Label Image). (like search bar and category-like items), deviates more significantly in its structural organization and visual hierarchy. Element Completeness and Typography: GPT-4o tends to generate design with higher degree of element completeness. For example, the navigation links in the header, the search bar within the hero section, and the individual category cards appear more fully formed and are stylistically closer to the target. The typography choices in GPT-4os output also generally exhibit greater fidelity. Detail Discrepancies: Despite its superior overall performance, the GPT-4o design still exhibits discrepancies in fine-grained details. For instance, the footer section in the GPT-4o output uses light background, contrasting with the dark background of the Label Webpage footer. In summary, this qualitative example suggests that while text-to-image MLLMs like GPT-4o are capable of generating coherent webpage designs that capture the essence of textual description in terms of major layout and components, achieving precise, fine-grained control over all visual attributes (such as exact colors, specific text content, and minor element styling) remains an area with substantial opportunity for advancement. The models can successfully translate textual concepts into visual webpage structures, but their ability to adhere to nuanced, detailed specifications requires further improvement. D.2 Webpage Code Generation qualitative case study of the Webpage Code Generation task further highlights the performance disparities. As illustrated in Figure 9, closed-source models generally demonstrate superior capabilities in overall page layout and element reproduction compared to their open-source counterparts. For instance, models like Claude 3.7 Sonnet achieve remarkably high degree of visual similarity to the label image in terms of component placement and stylistic consistency. However, even leading proprietary models exhibit limitations in capturing fine-grained details. In the provided example, O4-mini, Gemini 2.5 Flash, and GPT-4o incorrectly render the main headline 19 Figure 10: Rendered outputs from various MLLMs for the NAVI PILLAY webpage section. Despite failing the perceptual QA task regarding the tags position relative to the title, all these MLLMs correctly implement the Human Rights Advocate tag above the main NAVI PILLAY title in their generated code. text with center alignment, deviating from the original left alignment. Furthermore, none of the evaluated models successfully replicated the circular search input field or the gradient background of the top banner. Header icons were also consistently omitted across all MLLM-generated outputs. These observations underscore that while current MLLMs can produce impressively structured and visually coherent webpages, there remains significant room for improvement in accurately perceiving and implementing nuanced design elements and precise details. This indicates gap in achieving pixel-perfect replication and fully comprehensive visual understanding, particularly for complex or non-standard UI components. D.3 Correct Code Implementation Despite Perceptual Errors This part provides visual illustration supporting the discussion in Section 5.2, which highlights an intriguing discrepancy between MLLM performance on perceptual QA tasks and their ability to generate visually accurate code. Specifically, as detailed in Figure 4 (b) of the main paper, all evaluated MLLMs incorrectly identify the positioning of the Human Rights Advocate tag relative to the main title (NAVI PILLAY) and subtitle in the Webpage Perception QA task. However, when these same MLLMs are tasked with generating the webpage code, they often demonstrate correct implementation of this very elements placement. Figure 10 presents the rendered outputs for the NAVI PILLAY webpage section from several MLLMs benchmarked in FullFront. These outputs are derived from the code generation tasks where models are asked to reproduce the webpage. As can be observed in Figure 10, despite their prior failure in the specific perceptual QA question regarding the tags position, all depicted MLLM outputs correctly place the Human Rights Advocate tag directly above the main NAVI PILLAY title. This placement is consistent with the ground-truth webpage."
        }
    ],
    "affiliations": [
        "Microsoft",
        "Sun Yat-sen University",
        "The Chinese University of Hong Kong",
        "Tongji University",
        "University of Washington"
    ]
}