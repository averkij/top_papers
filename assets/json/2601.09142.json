{
    "paper_title": "EvasionBench: Detecting Evasive Answers in Financial Q&A via Multi-Model Consensus and LLM-as-Judge",
    "authors": [
        "Shijian Ma",
        "Yan Lin",
        "Yi Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Detecting evasive answers in earnings calls is critical for financial transparency, yet progress is hindered by the lack of large-scale benchmarks. We introduce EvasionBench, comprising 30,000 training samples and 1,000 human-annotated test samples (Cohen's Kappa 0.835) across three evasion levels. Our key contribution is a multi-model annotation framework leveraging a core insight: disagreement between frontier LLMs signals hard examples most valuable for training. We mine boundary cases where two strong annotators conflict, using a judge to resolve labels. This approach outperforms single-model distillation by 2.4 percent, with judge-resolved samples improving generalization despite higher training loss (0.421 vs 0.393) - evidence that disagreement mining acts as implicit regularization. Our trained model Eva-4B (4B parameters) achieves 81.3 percent accuracy, outperforming its base by 25 percentage points and approaching frontier LLM performance at a fraction of inference cost."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 4 1 ] . [ 1 2 4 1 9 0 . 1 0 6 2 : r EvasionBench: Detecting Evasive Answers in Financial Q&A via Multi-Model Consensus and LLM-as-Judge Shijian MA1,, Yan LIN2,,, Yi YANG1 1Hong Kong University of Science and Technology 2University of Macau mas8069@foxmail.com, yanlin@um.edu.mo, imyiyang@ust.hk Equal contribution Corresponding author management and investors. However, management responses often exhibit varying degrees of evasivenessproviding tangential information, hedging, or outright refusing to address analyst inquiries (Nuaimi et al., 2025). Research has established strong correlations between evasive responses and negative future performance: companies with high evasiveness show 63% likelihood of underperformance within 180 days (Paragon Intel, 2024), and 40 percentage-point rise in response incoherence corresponds to 0.74% drop in 1-day stock returns. This motivates automated evasion detection systems for financial transparency analysis. Despite its practical importance, evasion detection faces critical bottleneck: high-quality supervision is scarce precisely where it matters most at the boundary between partially responsive and truly evasive answers. In practice, expert annotation is costly and inconsistent on ambiguous cases, while naïve large-scale pseudo-labeling tends to over-represent easy examples and to amplify the idiosyncrasies of single teacher. Consequently, single-model distillation (e.g., training on GPT-4 labels) can yield overconfident but biased labels, especially for subtle boundary cases, and scaling the same teacher does not address this failure mode. Multi-annotator crowdsourcing is natural remedy, but it is expensive and requires domain expertise to maintain consistency. Recent work has explored Constitutional AI (Bai et al., 2022) and LLM-as-Judge (Zheng et al., 2023) for evaluation. Separately, AI feedback has been used to scale alignment supervision (e.g., RLAIF (Lee et al., 2023) and UltraFeedback (Cui et al., 2024)). However, an end-to-end recipe for using multiple strong LLM annotators and judge to construct task labels for supervised learning remains underexplored: existing pipelines rarely provide (i) principled way to target boundary cases at scale and (ii) scalable mechanism to resolve label uncertainty without turning to humans. Figure 1: Overview of our multi-model annotation framework. Claude Opus 4.5 and Gemini-3-Flash independently annotate samples, with disagreements resolved by Claude Opus 4.5 as judge."
        },
        {
            "title": "Abstract",
            "content": "Detecting evasive answers in earnings calls is critical for financial transparency, yet progress is hindered by the lack of large-scale benchmarks. We introduce EvasionBench, comprising 30,000 training samples and 1,000 humanannotated test samples (Cohens Kappa 0.835) across three evasion levels. Our key contribution is multi-model annotation framework leveraging core insight: disagreement between frontier LLMs signals hard examples most valuable for training. We mine boundary cases where two strong annotators conflict, using judge to resolve labels. This approach outperforms single-model distillation by 2.4%, with judge-resolved samples improving generalization despite higher training loss (0.421 vs 0.393)evidence that disagreement mining acts as implicit regularization. Our trained model Eva-4B (4B parameters) achieves 81.3% accuracy, outperforming its base by 25 percentage points and approaching frontier LLM performance at fraction of inference cost."
        },
        {
            "title": "Introduction",
            "content": "Earnings call Q&A sessions represent critical information exchange channel between corporate 1 We introduce EvasionBench, comprehensive benchmark for financial Q&A evasion detection, alongside novel multi-model consensus annotation framework with LLM-as-Judge designed to address these two gaps. Our key insight is that disagreement between strong models is practical proxy for boundary ambiguity: if two frontier annotators conflict, the sample is likely hard and disproportionately informative for learning robust decision boundary. We therefore use disagreement to actively mine challenging examples and employ judge model to adjudicate them, producing training labels that are both scalable and less tied to any single models bias. Concretely, our annotation pipeline operates in four stages: (1) independently annotate samples with Claude Opus 4.5 and Gemini-3-Flash, (2) extract disagreements (approximately 17% of samples), (3) invoke judge to adjudicate conflicts, and (4) mix high-confidence consensus samples with judge-resolved hard samples for training. This approach yields 30,000 balanced training samples and 1,000 expert-validated test samples across three evasion levels (direct, intermediate, fully evasive) based on the Rasiah taxonomy (Rasiah, 2007). Contributions We make the following contributions: 1. EvasionBench Dataset: We present comprehensive benchmark featuring 30,000 balanced training samples constructed via multimodel consensus and 1,000 human-annotated test samples across three evasion levels. Expert validation with two independent annotators achieves Cohens Kappa of 0.835 (Almost Perfect by Landis and Koch (1977) standards), exceeding standard publication thresholds for annotation quality. 2. Multi-Model Annotation Framework: We propose methodology combining multimodel consensus with LLM-as-Judge for disagreement resolution, enabling systematic hard sample mining without expensive human annotation. While AI feedback has been explored for alignment and evaluation (Zheng et al., 2023; Lee et al., 2023; Cui et al., 2024), our work provides an end-to-end recipe for disagreement-driven task label construction in financial evasion detection. 3. Empirical Comparison to Opus-Only Baseline: We compare models trained on our multimodel + judge constructed data against strong Opus-only consensus baseline. Eva-4B achieves 81.3% accuracy versus 78.9% for the Opus-only baseline (+2.4% absolute). Notably, the Opus-only baseline exhibits lower training loss (0.393 vs 0.421), suggesting that disagreement mining improves generalization despite harder training supervision. 4. Comprehensive Evaluation: We benchmark 12 models (3 closed-source, 5 open-source, 4 specialized/baseline) on our human test set, establishing performance baselines. Our 4Bparameter model (Eva-4B: 81.3%) ranks 4th overall and outperforms its base model by 25.1 percentage points, demonstrating the effectiveness of judge-resolved training data."
        },
        {
            "title": "2.1 Evasion Detection and Financial NLP",
            "content": "Nuaimi et al. (2025) establish psychological discourse taxonomy for evasive answers in earnings calls, adopting the Rasiah framework (Rasiah, 2007) to classify responses as direct, intermediate, or fully evasive. They develop lightweight baselines using linguistic features including hedges, verbosity, and semantic alignment, but with limited training data and no systematic hard sample mining. Barcellos (2025) examine how investors distinguish deceptive evasion tactics, demonstrating that suspicious thinking helps overcome evasion and motivating automated detection systems. Industry applications have demonstrated the market impact of evasion: 40 percentage-point rise in response incoherence corresponds to 0.74% drop in 1-day stock returns (Paragon Intel, 2024). Domainspecific models like FinBERT (ProsusAI, 2019) have shown the value of financial pre-training for sentiment analysis. Our work extends financial NLP to evasion detection with large-scale benchmark resources and rigorous human validation."
        },
        {
            "title": "Annotation",
            "content": "Knowledge distillation (Hinton et al., 2015) transfers knowledge from large teacher models to smaller student models, typically using single teachers soft labels or hard predictions. Xie et al. (2020) extend this with self-training using noisy student models, showing that large-scale pseudolabeling can improve performance. However, these approaches rely on single teacher model: scaling 2 teacher can increase label volume, but it does not inherently (i) diversify annotation biases or (ii) target ambiguous boundary cases that dominate downstream errors. Our work differs in goal and mechanism. Instead of treating the teacher as gold labeler, we use multi-model consensus to identify highconfidence samples and disagreement mining to target boundary cases where supervision is most valuable. We then use LLM-as-Judge to resolve disagreements, producing scalable labels that are less tied to any single models idiosyncrasies. 2.3 LLM-as-Judge and Constitutional AI Recent work has explored using LLMs to evaluate other LLMs. Constitutional AI (Bai et al., 2022) uses AI feedback for harmlessness training. Wang et al. (2022) demonstrate that self-consistency improves reasoning by sampling multiple outputs and taking majority votes. Zheng et al. (2023) introduce MT-Bench for evaluating LLM-as-a-judge with strong correlation to human preferences, and identify position bias as key concern where judges systematically prefer responses in certain positions. In contrast, our focus is training data construction. Existing LLM-as-Judge pipelines typically stop at ranking model outputs; they do not provide an end-to-end recipe for creating large-scale, high-quality task labels that emphasize boundary cases. Meanwhile, AI feedback has been used to scale preference-style supervision for alignment (Lee et al., 2023; Cui et al., 2024). We extend the judge paradigm to disagreement resolution and hard sample mining: we first use multiple strong annotators to surface ambiguous samples, and then use judge to adjudicate conflicts for scalable supervised fine-tuning on classification labels. Our protocol addresses position-bias concerns through reasoning-based assessment rather than positional randomization, though we acknowledge this as limitation requiring future validation. closely related to query-by-committee (Seung et al., 1992): we use model disagreement to identify boundary samples. Unlike classical active learning (which queries humans) or committee-based semisupervision (e.g., tri-training (Zhou and Li, 2005)), we close the loop with an LLM judge to resolve labels at scale, enabling hard sample mining with quality control."
        },
        {
            "title": "3 Task Definition",
            "content": "Following Nuaimi et al. (2025), we adopt the Rasiah taxonomy (Rasiah, 2007) for classifying management responses in earnings call Q&A. Each question-answer pair is labeled with one of three categories: direct: Management directly addresses the core question with specific information (quantitative data, clear directional guidance, or concrete commitments). intermediate: Management provides related information but sidesteps the core question (partial answers, tangential discussion, or contextual information without addressing the specific inquiry). fully_evasive: Management does not address the question at all (explicit refusals, topic redirection, or complete non-responses). This three-class taxonomy captures the spectrum of evasion in corporate communication, from transparent disclosure to complete avoidance. The intermediate category represents boundary cases that are particularly challenging for both humans and models to classify consistently. Figure 2 illustrates representative examples from each evasion category."
        },
        {
            "title": "4.1 Data Collection and Source Data",
            "content": "Active learning (Settles, 2009) queries informative samples to reduce labeling costs, typically using uncertainty sampling or query-by-committee (Seung et al., 1992). Hard example mining (Shrivastava et al., 2016) focuses training on difficult cases in object detection. Curriculum learning (Bengio et al., 2009) schedules training from easy to hard examples. Our disagreement-driven approach is most"
        },
        {
            "title": "Preparation",
            "content": "We construct EvasionBench from earnings call transcripts in the S&P Capital IQ database, comprehensive financial dataset containing 1.38 million conference call transcripts with 74 million text components spanning multiple years. Our data extraction follows rigorous three-stage quality filtering pipeline adapted from best practices in financial NLP (Nuaimi et al., 2025): 3 Direct Example: Q: Any update on the fine related to the fish kill that you guys announced? A: No, theres been no update on that. dataset splits through transcript-level deduplication, preventing data leakage across training, validation, and test sets. Intermediate Example: Q: was hoping you could clarify the scope of the Berlin plans for building right now. A: We cant say too much about this, except that there will be local cell production that will serve the needs of the Berlin factory. Fully Evasive Example: Q: Would you provide more color behind this 35% to 40% decline by channel or by brand? A: We are not providing any color or more detailed color beyond this guidance. This is guidance policy only for sales for the next quarter. Figure 2: Task examples illustrating three evasion levels in earnings call Q&A. Stage 1: Q&A Pair Extraction We identify analyst questions (Type 3 components) and corresponding management answers (Type 4 components) from the raw transcript structure, filtering out operator instructions, conversational pleasantries (e.g., thank you, good morning), and non-substantive exchanges. Stage 2: Quality Filtering We retain only Q&A pairs meeting minimum quality criteria: questions must contain question mark, answers must exceed 30 characters, and samples must not contain truncation markers indicating incomplete transcription (e.g., [indiscernible], [audio gap], ...). Stage 3: Substantial Content Selection We restrict to pairs where the combined question and answer length exceeds 500 characters, ensuring sufficient linguistic context for nuanced evasion analysis beyond surface-level pattern matching. Source Corpus Construction Applying this pipeline to the Capital IQ database yields two complementary extraction batches: (1) 57K-sample corpus with natural label distribution skewed toward direct and intermediate responses (57.6% direct, 42.2% intermediate, 0.2% evasive), and (2) 30K-sample corpus exhibiting higher prevalence of evasive responses (74.7% evasive, 18.5% intermediate, 6.8% direct) due to different sampling strategies and temporal coverage. To construct our balanced 30,000-sample training set, we apply stratified sampling from these two sources to achieve exactly 10,000 samples per class. We ensure complete disjointness between all 4.2 Multi-Model Annotation Framework Traditional knowledge distillation relies on single teacher model to generate training labels, which may introduce model-specific biases and miss subtle boundary cases. We propose novel multi-model consensus framework with LLMas-Judge that leverages disagreement as signal for hard sample mining. Annotation Pipeline Our framework operates in four stages (illustrated in Figure 1): 1. Dual Annotation: Each sample is independently annotated by Claude Opus 4.5 and Gemini-3-Flash using identical prompts (see Appendix A). Both models generate structured outputs containing: (1) predicted label (direct, intermediate, or fully_evasive), and (2) supporting reasoning explaining the classification decision. 2. Agreement Detection: We identify samples where both models agree on the label (approximately 83% of cases). These represent highconfidence examples with clear evasion signals. 3. Judge Resolution: For disagreement cases (approximately 17% of samples), we invoke Claude Opus 4.5 as judge to adjudicate conflicts following the protocol detailed in Section 4.2. 4. Hard Sample Mining: Judge-resolved samples represent boundary cases where even strong models disagree. These provide valuable training signal for learning subtle distinctions between evasion levels. LLM-as-Judge Protocol To ensure unbiased and reproducible disagreement resolution, we implement rigorous judging protocol with four key design principles: 1. Blind Evaluation: The judge receives anonymized predictions labeled as Model 1 and Model 2 without knowledge of which corresponds to Claude Opus 4.5 or Gemini-3Flash. While we do not randomize position across samples (Model 1 consistently corresponds to Claude Opus 4.5 for implementation 4 consistency), the judge has no explicit knowledge of this mapping and must evaluate based on reasoning quality alone. 2. Reasoning-Based Assessment: The judge evaluates both the predicted label and the reasoning explanation from each model, not just the label alone. Each models reasoning typically consists of 1-2 sentences explaining why the response was classified as direct, intermediate, or fully evasive based on alignment with the question core. This ensures decisions are grounded in logical soundness and taxonomy adherence rather than arbitrary preference. 3. Evaluation Criteria: The judge prompt determine which explicitly instructs: prediction is more accurate and logically sound given the Q&A context. The judge must return structured JSON {\"winner\":\"model1\"\"model2\", output: \"reason\":\"<justification>\"}, where the justification is limited to 100 characters to enforce concise reasoning. 4. Inference Parameters: We use temperature=0.3 to balance consistency with diversity. This prevents purely deterministic outputs (which could lead to position bias) while maintaining reliable judgments across similar samples. Each disagreement receives exactly one judge evaluation; we do not employ sampling-based ensembling or majority voting. The complete judge prompt template is provided in Appendix A. This protocol addresses potential concerns about self-preference bias (using Claude Opus 4.5 to judge its own predictions) by requiring the judge to evaluate reasoning quality rather than simply choosing its own label. Empirically, we find that the judge selects Model 1 (Opus) in approximately 60% of disagreements and Model 2 (Gemini) in 40%, suggesting the mechanism does not exhibit extreme self-preference. Rationale for Disagreement Mining Our approach is motivated by two key insights: (1) model disagreement signals ambiguity or boundary cases that require careful judgment, and (2) training on diverse samples (both clear consensus and resolved edge cases) improves generalization compared to single-model labels that may encode specific biases. This framework enables us to construct 30,000sample balanced training set (10,000 per class) combining approximately 25,000 consensus samples (83.3%) with 5,000 judge-resolved hard samples (16.7%). The final label for each training sample represents either the agreed-upon label from dual-model consensus or the judges adjudicated decision, ensuring comprehensive quality control across the entire dataset. 4.3 Human Validation and Test Set Construction Test Set Sample Selection We construct 1,000sample test set by combining Q&A pairs from two complementary sources to ensure diverse coverage: (1) 597 samples (59.7%) from the earnings call evasion benchmark dataset released by Nuaimi et al. (2025), and (2) 403 samples (40.3%) newly extracted from Capital IQ transcripts following our 3-stage filtering pipeline. This dual-source composition provides evaluation on both external benchmark samples and domain-matched in-distribution data. Expert Annotation Protocol All 1,000 test samples are manually annotated by single domain expert with extensive experience in financial discourse analysis. The annotator receives comprehensive training on the Rasiah taxonomy and follows structured annotation protocol: Extract Question Core: Identify what the analyst specifically wants to know (quantitative, temporal, directional, binary, causal, or breakdown information). Evaluate Response Alignment: Assess whether managements response matches the question core (full match, partial match, or no match). Apply Classification: Assign label based on decision criteria for direct, intermediate, or fully_evasive categories. Inter-Annotator Agreement Validation To validate annotation quality, we conduct rigorous reliability assessment using second independent expert annotator. We randomly sample 100 Q&A pairs (10% of test set, stratified by class) for dual annotation. Results demonstrate strong interannotator reliability: Cohens Kappa: 0.835 (Almost Perfect by Landis and Koch (1977) standards) 5 Table 1: EvasionBench dataset statistics. Training uses balanced sampling; test reflects natural distribution. Split Total Direct Inter. Evasive Training Test (Human) 30,000 1,000 10,000 10,000 256 10,000 332 Raw Agreement: 86% overall We perform full-parameter fine-tuning rather than LoRA or other parameter-efficient methods, as our preliminary experiments showed that complete adaptation yields better performance for domainspecific classification. The model is trained to directly output one of three labels (direct, intermediate, fully_evasive) given question-answer pair as input. Per-Class Agreement: Direct (95.5%), Fully 5.2 Training Strategy Evasive (91.2%), Intermediate (74.3%) The lower agreement on intermediate cases (74.3%) reflects the inherent ambiguity of boundary examples, consistent with our hypothesis that these represent challenging cases requiring careful judgment. This validates our multi-model approach: even expert humans find intermediate classification difficult. The final 1,000-sample test set exhibits natural class distribution: 412 direct (41.2%), 256 intermediate (25.6%), 332 fully evasive (33.2%). This distribution reflects the natural prevalence of evasion tactics in earnings calls while ensuring sufficient representation of all categories for robust evaluation. We train on the 30,000-sample balanced dataset constructed via our multi-model annotation framework. Training is conducted on 2 NVIDIA B200 SXM6 GPUs (180GB VRAM each) using the MSSwift framework. Training configuration: Epochs: 2 (empirically determined to prevent overfitting) Learning Rate: 2e-5 with linear warmup (3% warmup ratio) Batch Size: 8 per GPU with gradient accumulation steps of 2, yielding effective batch size of 32 Precision: bfloat16 mixed precision training"
        },
        {
            "title": "4.4 Dataset Statistics",
            "content": "Max Sequence Length: 2048 tokens Table 1 summarizes EvasionBench dataset composition. The training set contains 30,000 balanced samples (10,000 per class) constructed via our multi-model annotation framework, combining both consensus and judge-resolved samples from the two source corpora (48.3% from the 57K extraction, 51.7% from the 30K extraction). The test set contains 1,000 samples with Q&A pairs from dual sources (59.7% external benchmark, 40.3% Capital IQ) and expert human annotations validated through inter-annotator agreement (Cohens Kappa 0.835 on 100-sample subset)."
        },
        {
            "title": "5.1 Model Architecture",
            "content": "We select Qwen3-4B-Instruct-2507 (Team, 2025) as our base model for evasion detection. This choice is motivated by three factors: (1) strong instruction-following capabilities essential for classification tasks, (2) manageable scale (4B parameters) enabling full-parameter fine-tuning with reasonable computational resources, and (3) demonstrated performance on reasoning and structured output tasks. Optimizer: AdamW Gradient Checkpointing: Enabled to reduce memory footprint The training set contains approximately 25,000 consensus samples (83.3%, where both models agree on the label) mixed with 5,000 judgeresolved samples (16.7%, disagreement cases adjudicated by Claude Opus 4.5). Critically, all labels represent final adjudicated decisions: consensus samples receive the agreed-upon label, while judgeresolved samples receive the label selected by the judge after evaluating both models reasoning. This ensures every training sample has undergone rigorous validation through either dual-model agreement or explicit judge arbitration, exposing the model to both clear examples with strong intermodel consensus and challenging boundary cases where even frontier LLMs initially disagreed."
        },
        {
            "title": "Construction",
            "content": "To contextualize the benefits of our multi-model + judge framework, we construct strict Opus-only 6 baseline where every training label is the Opus label (opus_label) with the same overall dataset size and class balance (30,000 total; 10,000 per class). The Opus-only training data is sampled from the same two source corpora and balanced by opus_label (50.1% from the 57K extraction, 49.9% from the 30K extraction), rather than systematically incorporating disagreement cases as hard samples. Matched scale and class balance: 30,000 samples with 10,000 per class source: Opus-only label Labels are opus_label for every sample, balanced across two source corpora (50.1% from 57K, 49.9% from 30K) Table 2: Model performance on 1,000-sample human test set. Eva-4B ranks 4th overall and 2nd among opensource models. Model Accuracy F1-Macro Closed-source LLMs Claude Opus 4.5 Gemini-3-Flash GPT-5.2 Open-source LLMs GLM-4.7 Eva-4B (Ours) Qwen3-Coder MiniMax-M2.1 Kimi-K2-0905 DeepSeek-V3.2 Specialized Baselines Opus-Only Qwen3-4B-Base 83.9 83.7 80.5 82.6 81.3 75.6 73.2 69.2 65.7 78.9 56. 0.838 0.833 0.805 0.809 0.807 0.759 0.716 0.665 0.665 0.785 0.535 Same training configuration: Same hyperparameters (2 epochs, lr=2e-5, effective batch size 32, bfloat16) This comparison isolates the impact of data construction and label generation strategy under matched training conditions. We note that the underlying Q&A pairs are not guaranteed to be identical across training sets due to differences in consensus filtering and supplementation; we therefore interpret results as comparison of two training data construction paradigms rather than strict ablation. Results are presented in Section 6.3."
        },
        {
            "title": "6.1 Experimental Setup",
            "content": "Evaluation Protocol All models are evaluated on our 1,000-sample human-annotated test set. We report accuracy (percentage of correct predictions) and F1-macro (unweighted average of per-class F1 scores). For our specialized models (Eva-4B and specialized baselines), we additionally report per-class F1 scores to analyze performance across evasion levels. Compared Models We evaluate 11 models spanning three categories: Closed-source LLMs: Claude Opus 4.5, Gemini-3-Flash, GPT-5.2 Open-source LLMs: GLM-4.7, Qwen3Coder, MiniMax-M2.1, Kimi-K2-0905, DeepSeek-V3.2 Specialized Models: Eva-4B (ours, Opus+Gemini+Judge), Opus-Only (Opusonly (base baseline), Qwen3-4B-Base model) All models perform single-pass classification on full question-answer pairs without decomposition or multi-step reasoning, ensuring fair comparison."
        },
        {
            "title": "6.2 Main Results",
            "content": "Table 2 presents evaluation results on the 1,000sample human test set. Claude Opus 4.5 achieves the highest accuracy (83.9%), followed closely by Gemini-3-Flash (83.7%) and open-source GLM4.7 (82.6%)."
        },
        {
            "title": "Key Findings",
            "content": "1. Eva-4B Competitive Performance: Our 4B-parameter model achieves 81.3% accuracy (F1-Macro: 0.807), ranking 4th overall and 2nd among open-source models (after GLM-4.7). Eva-4B outperforms the base model (Qwen3-4B: 56.2%) by 25.1 percentage points, demonstrating that targeted finetuning on judge-resolved data yields substantial improvements. 2. Frontier LLM Ceiling: Claude Opus 4.5 (83.9%) and Gemini-3-Flash (83.7%) establish the current performance ceiling. The 2.6-point gap between Eva-4B and top models suggests room for improvement via larger model architectures or enhanced training data quality. 7 Table 3: Comparison of training data construction strategies. Multi-model + judge training improves accuracy by 2.4 absolute points over strict Opus-label baseline under matched training conditions. Training Strategy Accuracy F1-Macro Opus-Only (Single-Teacher) Eva-4B (Multi-Model + Judge) Improvement 78.9% 81.3% +2.4% 0.785 0. +0.022 3. Fine-tuning Effectiveness: Eva-4B (81.3%) dramatically outperforms its base model Qwen3-4B (56.2%) and exceeds generalpurpose models (Qwen3-Coder: 75.6%, MiniMax-M2.1: 73.2%, DeepSeek-V3.2: 65.7%). The Opus-only baseline (78.9%) provides reference for traditional single-teacher distillation-style data construction."
        },
        {
            "title": "6.3 Comparison Study: Multi-Model + Judge",
            "content": "vs Opus-Only Baseline We compare Eva-4B (trained on Claude Opus 4.5 + Gemini-3-Flash consensus with judge-resolved samples) against Opus-Only (trained on Opus-only labels). Both models use the same base architecture (Qwen3-4B-Instruct-2507), the same training recipe, and matched training set size and class balance (30,000 total; 10,000 per class). However, the training Q&A pairs are not guaranteed to be identical, since Eva-4B explicitly includes disagreement cases resolved by judge, while the Opus-only baseline is constructed by sampling and balancing purely by opus_label. This comparison therefore evaluates whether our multi-model + judge data construction strategy yields better downstream generalization than strict Opus-label baseline under matched training conditions. Eva-4Bs training set includes approximately 5,000 samples (16.7%) where models initially disagreed and required explicit judge arbitration, while Opus-Only uses Opus labels for every training example without systematic hard example identification. Table 3 presents results. Training Loss Analysis Critically, the singlemodel baseline achieves lower training loss (0.393 final loss) compared to Eva-4B (0.421 final loss). Figure 3 illustrates this phenomenon: the singlemodel baseline converges faster and reaches lower training loss, yet performs worse on held-out test data."
        },
        {
            "title": "This supports our hypothesis that",
            "content": "judge8 Figure 3: Training loss curves for single-model baseline versus Eva-4B. Lower training loss does not guarantee better test performance, demonstrating that judgeresolved samples act as regularization. Table 4: Per-class F1 scores for Eva-4B, showing intermediate category as most challenging. Category F1 Score Error Pattern Direct Intermediate Fully Evasive 0.851 0.698 0.873 Confused with intermediate Boundary ambiguity Strong signal words resolved disagreement samples can improve generalization by mitigating overfitting. The Opus-only baseline achieves lower training loss, while Eva-4B learns more robust decision boundaries from disagreement-resolved samples, yielding higher test accuracy."
        },
        {
            "title": "6.4 Per-Class Performance Analysis",
            "content": "Table 4 analyzes Eva-4Bs performance across evasion categories. Fully evasive responses are easiest to detect (F1: 0.873), followed by direct responses (F1: 0.851). Intermediate cases prove most challenging (F1: 0.698), consistent with human interannotator agreement patterns (74.3% on intermediate vs 95.5% on direct). Error analysis reveals that 72% of errors involve confusion between direct and intermediate categories, reflecting the subtle distinction between partial answer with context versus sidestepping with related information. The intermediate category exhibits lower inter-annotator agreement (74.3%) compared to direct (95.5%) and fully evasive (91.2%), confirming this is genuinely ambiguous boundary even for expert human annotators."
        },
        {
            "title": "7 Conclusion",
            "content": "We introduced EvasionBench, comprehensive benchmark for detecting evasive answers in financial Q&A, featuring 30,000 balanced training samples and 1,000 expert-validated test samples with Cohens Kappa of 0.835. Our primary contribution is novel multi-model annotation framework with LLM-as-Judge that mines hard samples through disagreement detection, achieving 2.4% higher accuracy than single-model distillation (81.3% vs 78.9%). Critically, we demonstrate that judge-resolved samples improve generalization despite higher training loss, proving that disagreement mining acts as effective regularization. Our 4B-parameter model achieves 81.3% accuracy, outperforming the base model by 25.1 percentage points and ranking 2nd among open-source models, while being orders of magnitude more efficient than frontier LLMs for deployment. Future work includes cross-domain evaluation, finer-grained taxonomies for boundary cases, and scaling to larger model families."
        },
        {
            "title": "Limitations",
            "content": "Domain Specificity: EvasionBench focuses exclusively on earnings call Q&A. Generalization to other domains (political debates, customer service) requires empirical validation. English-Only Evaluation: Current dataset and models are limited to English transcripts. Multilingual evaluation would assess crosslingual transferability. LLM Annotation Costs: Multi-model annotation with judge resolution requires multiple API calls per sample (2 annotators + 1 judge for disagreements), increasing cost by approximately 2.2-2.3x compared to singlemodel distillation. However, the 2.4% accuracy improvement justifies this expense for high-stakes applications. Judge Position Bias: We do not randomize model positions in judge prompts (Model 1 always = Opus), which could introduce subtle position bias. While our 60-40 split suggests limited bias, future work should validate this through position-swapped re-evaluation on held-out samples. Self-Preference Concerns: Using Claude Opus 4.5 to judge its own predictions introduces potential self-preference bias. While our reasoning-based protocol and empirical 60-40 split suggest this is manageable, an alternative design using third-party judge model (e.g., GPT-5.2) would eliminate this concern entirely. Taxonomy Subjectivity: The intermediate category exhibits lower inter-annotator agreement (74.3%), reflecting genuine ambiguity in task definition. Finer-grained taxonomies may reduce subjectivity. Temporal Generalization: Training data spans 2005-2023, but language and disclosure norms evolve. Periodic dataset updates would maintain relevance."
        },
        {
            "title": "Ethics Statement",
            "content": "Data Privacy EvasionBench uses publicly available earnings call transcripts from S&P Capital IQ. All data is already in the public domain through regulatory filings (SEC) and company investor relations. No private or confidential information is included. Intended Use This research aims to improve financial transparency and investor protection by detecting evasive corporate communication. Intended applications include: Academic research on corporate disclosure quality Investor tools for analyzing management transparency Regulatory oversight for financial misconduct detection Potential Misuse We acknowledge potential risks: Market Manipulation: Automated evasion scores could be weaponized for short-selling campaigns or stock manipulation. We emphasize that evasion detection is one signal among many and should not drive investment decisions in isolation. False Positives: Models may misclassify legitimate confidentiality (trade secrets, competitive information) as evasion. Human judgment remains essential for high-stakes decisions. Gaming: Companies may learn to manipulate evasion detection systems through adversarial response engineering. Continuous model updates and human oversight mitigate this risk. Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. 2023. RLAIF: Scaling reinforcement learning from human feedback with AI feedback. arXiv preprint arXiv:2309.00267. Not Investment Advice EvasionBench and Eva4B are research artifacts, not financial advisory tools. Users must conduct independent due diligence and consult licensed financial professionals before making investment decisions. Khaled Al Nuaimi, Gautier Marti, Alexis Marchal, and Andreas Henschel. 2025. Detecting evasive answers in financial Q&A: psychological discourse taxonomy and lightweight baselines. In Proceedings of The 10th Workshop on Financial Technology and Natural Language Processing, pages 191196. Bias Considerations Financial NLP models may encode biases from training data (e.g., industryspecific communication norms, executive gender/race correlations). We encourage users to evaluate fairness metrics for their specific applications and report bias concerns to the research community. Reproducibility and Transparency We commit to full reproducibility by releasing all data, code, model weights, and annotation guidelines. This enables independent verification and responsible extension of our work."
        },
        {
            "title": "References",
            "content": "Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, and 32 others. 2022. Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073. Leonardo P. Barcellos. 2025. Overcoming deceptive evasions in earnings calls: The role of investor suspicion. Management Science. Forthcoming. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. 2009. Curriculum learning. Proceedings of the 26th Annual International Conference on Machine Learning (ICML), pages 4148. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan Ni, Guotong Xie, Ruobing Xie, Yankai Lin, Zhiyuan Liu, and Maosong Sun. 2024. Ultrafeedback: Boosting language modIn Proceedings of els with scaled AI feedback. the International Conference on Machine Learning (ICML). Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531. J. Richard Landis and Gary G. Koch. 1977. The measurement of observer agreement for categorical data. Biometrics, 33(1):159174. Paragon Intel. 2024. Earnings call evasion analysis: How to predict company underperformance. ProsusAI. 2019. FinBERT: Financial sentiment analysis with BERT. https://github.com/ProsusAI/ finBERT. Parameswary Rasiah. 2007. Evasion in Australias Parliamentary Question Time: The Case of the Iraq War. Ph.D. thesis, University of Western Australia. Burr Settles. 2009. Active Learning Literature Survey. University of Wisconsin-Madison Department of Computer Sciences. Computer Sciences Technical Report 1648. H. Sebastian Seung, Manfred Opper, and Haim Sompolinsky. 1992. Query by committee. In Proceedings of the Fifth Annual Workshop on Computational Learning Theory (COLT), pages 287294. Abhinav Shrivastava, Abhinav Gupta, and Ross Girshick. 2016. Training region-based object detectors with online hard example mining. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 761769. Qwen Team. 2025. Qwen3 technical report. Preprint, arXiv:2505.09388. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171. Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V. Le. 2020. Self-training with noisy student improves ImageNet classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068710698. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-judge with MT-bench and chatbot arena. In Advances in Neural Information Processing Systems (NeurIPS). Zhi-Hua Zhou and Ming Li. 2005. Tri-training: Exploiting unlabeled data using three classifiers. IEEE Transactions on Knowledge and Data Engineering, 17(11):15291541."
        },
        {
            "title": "Guidelines",
            "content": "This appendix includes the full prompt templates and human annotation guidelines used throughout the paper. We embed the appendix pages directly from single PDF to preserve formatting for arXiv distribution. 11 12 13 14 16"
        }
    ],
    "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Macau"
    ]
}