{
    "paper_title": "Predicting Time-Dependent Flow Over Complex Geometries Using Operator Networks",
    "authors": [
        "Ali Rabeh",
        "Suresh Murugaiyan",
        "Adarsh Krishnamurthy",
        "Baskar Ganapathysubramanian"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Fast, geometry-generalizing surrogates for unsteady flow remain challenging. We present a time-dependent, geometry-aware Deep Operator Network that predicts velocity fields for moderate-Re flows around parametric and non-parametric shapes. The model encodes geometry via a signed distance field (SDF) trunk and flow history via a CNN branch, trained on 841 high-fidelity simulations. On held-out shapes, it attains $\\sim 5\\%$ relative L2 single-step error and up to 1000X speedups over CFD. We provide physics-centric rollout diagnostics, including phase error at probes and divergence norms, to quantify long-horizon fidelity. These reveal accurate near-term transients but error accumulation in fine-scale wakes, most pronounced for sharp-cornered geometries. We analyze failure modes and outline practical mitigations. Code, splits, and scripts are openly released at: https://github.com/baskargroup/TimeDependent-DeepONet to support reproducibility and benchmarking."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] - . s [ 1 4 3 4 4 0 . 2 1 5 2 : r Predicting Time-Dependent Flow Over Complex Geometries Using Operator Networks Ali Rabeha, Suresh Murugaiyana, Adarsh Krishnamurthya, Baskar Ganapathysubramaniana, aIowa State University, Ames, Iowa, USA Abstract Fast, geometry-generalizing surrogates for unsteady flow remain challenging. We present time-dependent, geometryaware Deep Operator Network that predicts velocity fields for moderate-Re flows around parametric and non-parametric shapes. The model encodes geometry via signed distance field (SDF) trunk and flow history via CNN branch, trained on 841 high-fidelity simulations. On held-out shapes, it attains 5% relative L2 single-step error and up to 1000X speedups over CFD. We provide physics-centric rollout diagnostics, including phase error at probes and divergence norms, to quantify long-horizon fidelity. These reveal accurate near-term transients but error accumulation in fine-scale wakes, most pronounced for sharp-cornered geometries. We analyze failure modes and outline practical mitigations. Code, splits, and scripts are openly released here to support reproducibility and benchmarking. Keywords: Time-dependent Neural Operators, Periodic Flow Simulations, Complex Geometries, Signed Distance Field 1. Introduction Timedependent flow simulations underpin tasks in shape optimization and flow control across aerospace, automotive, civil, and energy systems [1, 2]. Yet highfidelity CFD remains computationally intensive, especially for large design spaces or long horizons, and the need to mesh complex geometries further increases cost and engineering effort [3, 4]. canonical stress test is unsteady, incompressible flow past immersed bodies: vortex formation, shedding frequency, and wake interactions must be captured accurately to avoid spurious loads and instabilities [5, 6]. In practice, unsteady wakes around wings and vehicle bodies can induce oscillatory forces and fatigue [7, 8]; vortex shedding behind buildings and bridge piers can trigger hazardous vibrations [9]; and wake interactions degrade turbine efficiency and lifetime [10, 11]. Resolving multiple shedding cycles at sufficiently fine spatialtemporal resolution often requires tens of thousands of time steps and hours of wallclock time on HPC systems [12, 13], making large-scale design sweeps or real-time inference infeasible. These challenges motivate fast, reliable scientific machine learning (SciML) surrogates that retain physical fidelity while offering orders-of-magnitude speedups [14]. Neural operators have emerged as promising class of surrogates for PDEs by learning mappings between function spaces rather than individual solution instances [15]. Among them, Deep Operator Networks (DeepONet) encode an input function (branch) and query location (trunk) to regress field values [16], and have been applied across steady and transient physics in 2D/3D [17, 18, 19, 20]. Alternative operator families include spectral models such as the Fourier Neural Operator (FNO) and transformer-based operators that capture longrange spatiotemporal dependencies [21, 22]. These approaches have demonstrated impressive speedups on canonical benchmarks (e.g., cylinder wakes) and even global weather surrogates [23, 24]. Despite rapid progress, stable longhorizon rollouts for unsteady flows over arbitrary shapes remain difficult. Autoregressive prediction can accumulate small step-wise errors into unphysical fields [25, 26]; moreover, fidelity depends critically on how geometry and flow history are represented. Recent geometry-aware models encode shapes via point clouds, meshes, or lowdimensional parametric descriptors [27, 28, 29, 30], while temporal models emphasize leveraging history to capture vortex memory effects [26]. Achieving robustness across diverse geometries and stability Corresponding Author across long rollouts therefore hinges on (i) an expressive, numerically convenient geometry encoding and (ii) an effective mechanism to exploit recent flow history. We extend the Geometric Deep Operator Network of He et al. [27] to unsteady 2D flows past complex shapes by (i) encoding geometry with signed distance field (SDF) in the trunk and (ii) encoding recent velocity history with lightweight CNN in the branch, inspired by history-aware surrogates [31]. We train and evaluate on three FlowBench [12] shape families: (1) smooth NURBS, (2) irregular sphericalharmonic blob shapes, and (3) nonparametric SkelNetOn contours [32]. We study both singlestep accuracy and autoregressive rollouts, emphasizing generalization across shape classes. Beyond standard error metrics, we adopt physics-based diagnostics tailored to unsteady wakes: (a) phase error at wake probes (timeand frequency-domain) to assess shedding frequency and lag, and (b) divergence norms to quantify incompressibility consistency over rollouts. These diagnostics, together with shape-conditioned analyses (e.g., sharp-cornered vs. smooth geometries), help illuminate failure modes and suggest practical remedies. Our contributions in this work include the following: time-dependent, geometryand history-aware DeepONet that couples SDF-based implicit geometry with CNN history encoder for unsteady flows over parametric and non-parametric shapes. systematic study on history length and shape variability for single-step and rollout accuracy across three FlowBench families. Physics-centric rollout diagnostics (probe phase error; divergence norms) that expose long-horizon drift and relate it to geometric sharpness. Practical guidance and ablations (e.g., SDF choices; history encoding) that inform robust surrogate design for geometry-rich unsteady CFD. The remainder is organized as follows. Section 2 reviews operator-learning methods for time-dependent dynamics. Section 3 details datasets and our time-dependent Geometric DeepONet. Section 4 reports quantitative/qualitative results and physics diagnostics. Section 5 summarizes findings, limitations, and future directions. 2. Related Work Neural-operator surrogates for unsteady CFD must (i) encode geometry in way that supports generalization across shapes and resolutions, and (ii) leverage temporal history to prevent error amplification in long rollouts. We review work along these two axes and then position our approach. Geometry Encoding: central question is how to represent complex shapes so that an operator can query the field at arbitrary locations while remaining robust across family of geometries. Point-cloud DeepONet and its geometric variants [27, 33] inject surface information (point clouds or meshes) into the trunk network, improving shape generalization for steady or quasi-steady settings. Geometry-Informed Neural Operator (GINO) [28] introduces graph-based kernels that propagate signals over geometric graphs, offering resolution-invariant conditioning on shape. These methods substantively advance geometry awareness, yet they typically do not couple the geometry encoding with an explicit mechanism for exploiting recent spatio-temporal evolution, which is critical for unsteady wakes. Temporal Modeling: Orthogonally, several operator designs target temporal coherence and long-horizon stability. The Temporal Neural Operator (TNO) [34] augments operator inputs with dedicated temporal branch that aggregates prior solution fields, yielding accuracy gains on time-dependent PDEs. PDE-Refiner [25] applies diffusion-style iterative denoiser to correct autoregressive predictions, improving rollout stability without altering the base predictor. Mixture operators [26] blend multiple temporal pathways to mitigate error accumulation, effectively learning complementary temporal dynamics. While these works address stability, they generally assume fixed grids or weak geometry conditioning, limiting performance on diverse, non-parametric shapes. Our approach unifies these threads by pairing an explicit geometry encoding, specifically signed distance field (SDF) fed to the trunk, with history encoder, specifically lightweight CNN over recent velocity frames within single Geometric DeepONet architecture. Relative to point-cloud/mesh conditionings [27, 33] and graph-kernel schemes [28], the SDF provides dense, resolution-agnostic implicit representation that is straightforward to mask and differentiate. Compared to purely temporal stabilizers [34, 25, 26], our design couples geometry and history explicitly, targeting the coupled source of rollout drift in unsteady wakes: sensitivity to both boundary shape and recent flow 2 evolution. This synthesis aims at robust generalization over diverse shapes and improved stability in autoregressive prediction, addressing gap in current neural-operator research. 3. CFD Dataset and Model Details (a) Snapshots of flow showing vortex shedding around different shapes. CNN encoder Branch MLP (ReLU) element-wise product Spatial Average Branch MLP (ReLU) (x,y,SDF) Trunk MLP (ReLU) Trunk MLP (Sine) dot product u, [B, 2 Nt, H, W] [B, c1, H/2, W/2] [B, c2, H/4, W/4] [B, c3, H/8, W/8] Input Conv 1 Conv 33 Conv 55 Pool 22 Pool 22 Pool 22 Conv 1 Conv 33 Conv 55 Pool 22 Pool 22 Pool 22 Conv 1 Conv 33 Conv 55 Pool 22 Pool 22 Pool 22 [B, 3 c3, H/8, W/8] [B, c1, H/16, W/16] [B, c2, H/32, W/32] Conv 11 Pool 22 Conv 11 Pool 2 Flattened (b) Time-dependent Geometric DeepONet architecture. Figure 1: (a) Representative snapshots from the FlowBench FPO dataset, illustrating vortex-shedding behind four representative shapes. (b) Time-dependent Geometric DeepONet surrogate model: the branch network in Stage-1 process Nt velocity frames through parallel convolutional streams (Inception-style CNN) fed into an MLP, while the trunk network encodes spatial (x, y, SDF) via another MLP. These are fused element-wise, passed through Stage-2 branch MLP (ReLU) and trunk MLP (sine), and finally contracted to predict the next-step velocity field. We use the flow around an object (FPO) dataset from the publicly available FlowBench dataset [12], hosted on Hugging Face. This AI-ready dataset comprises 1,103 high-fidelity 2D simulations of unsteady, incompressible flows past complex shapes on 1024 256 grid with 242 temporal snapshots per case. To balance accuracy and efficiency, we uniformly subsample every fourth frame yielding 60 timesteps per case, while still capturing key vortex-shedding dynamics. Simulation data are generated with rigorously validated NavierStokes solver using the shifted boundary method to enforce boundary conditions on complex geometries [35, 36]. Benchmark comparisons show velocity profiles, Strouhal numbers, drag CD and lift CL coefficients in good agreement with results in the literature Yang et al. [37]. Examples of flow around different geometries are shown in Figure 1a. The FPO dataset is provided as NumPy compressed (.npz) files. We provide two .npz files: one for the inputs, suffixed with the marker \"_X.npz\", and one for the outputs, suffixed with the marker \"_Y.npz\". Each of these .npz files contains 4D NumPy tensor structured as [number_of_channels][timesteps][resolution_x][resolution_y]. In our workflow, we omit the Reynolds channel at inference, letting the model infer flow conditions from the velocity history, and use only the SDF as input. Similarly, we predict only the velocity components (u, v). 3 We evaluate model performance by randomly splitting the 1,103 cases into 841 training and 262 test samples. We further divide the training set into an 80%/20% random shuffle to form training and validation subsets. The held-out 262-case test set is used exclusively for final evaluation. Figure 1b illustrates our time-dependent Geometric DeepONet, which consists of two parallel networks branch and trunk and two-stage fusion process. The branch network extracts multi-scale features from sequence of Nt past velocity fields (we denote this input sequence length by = Nt) by applying three parallel convolutional streams (1 1, 3 3, and 5 5 kernels), each followed by 2 2 max-pooling, to reduce the spatial resolution by factor of 32. These feature maps are concatenated, flattened, and fed into three-layer MLP (Stage 1) to produce global latent vector of dimension m. Simultaneously, the trunk network (Stage 1) processes each spatial query point by taking its coordinates (x, y) and corresponding SDF value through three-layer MLP, yielding local feature vector of dimension at each grid point. We fuse the branch and trunk outputs via an element-wise product, thereby combining temporal and geometric information. In Stage 2, the fused tensor is split into two paths. The branch path first computes spatial average over all grid points and processes the resulting vectors with three-layer MLP (Stage 2) using ReLU activations. The trunk path retains the full fused tensor (without averaging) and feeds it into another three-layer MLP (Stage 2) using sine activations, generating per-point outputs. final dot-product contraction along the latent modes between the branch and trunk outputs yields the predicted velocity components (u, v) at each spatial location. For concise summary of the entire data-flow, see Algorithm 1. Algorithm 1 Time-Dependent Geometric DeepONet Require: Past Nt velocity frames {utNt , . . . , ut1}, SDF grid 1: Branch encoding: 2: Stack past frames into [B, Cout Nt, H, W] Apply three parallel conv streams (kernels 1 1, 3 3, 5 5), each with 2 2 max-pool Fuse via 1 1 convs + pooling, then flatten For each query point (x, y), read SDF value (x, y, SDF) 3: 4: 5: MLP global latent vector Rm 6: Trunk encoding: 7: 8: MLP local feature vector Rm 9: Stage 1 fusion: element-wise product [B, P, m] 10: Stage 2 encoding: 11: 12: 13: Final fusion: dot-product over modes [B, P, Cout] 14: Loss computation: 15: = MSE(cid:0)(u, v), (ugt, vgt)(cid:1) over all points with SDF > Branch path: spatially average fused tensor, then MLP [B, Cout] Trunk path: apply MLP to each fused feature [B, P, Cout] We trained our model of 1.6 million parameters using the Adam optimizer with learning rate of 103, batch size of 16, for 1000 epochs on single A100 GPU (12 days). Hyperparameters learning rate, batch size, and network width and depth were carefully tuned via structured grid search over multiple candidate values, selecting the configuration that minimized validation loss. Full layer dimensions and hyperparameters are provided in Appendix .1. The training and validation losses for our Time-Dependent Geometric-DeepONet is reported in Figure .11 in Appendix .2, where we present the training and validation loss curves for 4 different input sequence lengths = 1 through = 16. 4. Results To quantify prediction accuracy we report two metrics over the test set at each timestep t: L2(t) = (cid:113)(cid:80)P (cid:0)ui i=1 (cid:113)(cid:80)P i= pred(t) ui (cid:0)ui gt(t)(cid:1)2 gt(t)(cid:1)2 , 4 and L(t) = max 1iP where is the number of spatial grid points and ui denotes the velocity component (either or v) at point i. (cid:12)(cid:12)(cid:12)ui pred(t) ui (cid:12)(cid:12)(cid:12), gt(t) 4.1. Effect of Sequence Length on Prediction Accuracy Figure 2 compares the time evolution of relative L2 and errors for both single step and autoregressive rollout predictions as the input sequence length is varied from 1 to 16. In the single-step setting (blue curves), all values of yield virtually identical, low error from = 0 onward, demonstrating that no additional past context beyond the immediately preceding field is affecting the short-term accuracy. Under rollouts (red), longer sequences yield slightly lower error for the first few timesteps, an expected benefit of having more initial ground truth frames. But beyond 20, the error trajectories for = 1, 4, 8, and 16 are similar. In other words, larger only delays error growth by handful of steps, without improving long-term fidelity. Because using > 1 requires that many more ground-truth inputs (increasing data loading and memory demands) yet offers no lasting accuracy advantage, we select = 1 for all subsequent experiments. This choice minimizes input requirements while preserving both single step and rollout performance. 4.2. Single Step prediction Single-Step Prediction Accuracy. As shown in Figure 3(a), the relative L2 error remains effectively constant at approximately 5% over all 60 timesteps, reflecting the models one step evaluation where ground-truth inputs are provided at each step. Correspondingly, the RMSE for both and components holds steady at about 0.035 (see Figure 3(b)), demonstrating that the network delivers uniform accuracy throughout the time sequence. This stable error profile confirms the models capacity to accurately predict the immediate future state when supplied with true past frames. Figure 3(c)-(n) shows comparison of ground truth and prediction velocity components fields (u, v) for single geometry at three timesteps (t = 30, = 45, = 59). The comparison shows good agreement, reflecting the models ability to accurately predict the immediate future state. Robust Geometric Generalization. Figure 4 compares predictions at = 30 for four markedly different geometries ranging from smooth, symmetric NURBS shapes to highly irregular harmonic perturbations and non-parametric skeleton outlines. Despite the pronounced variations in wake dynamics induced by sharp corners and symmetry of the geometries, the predicted and fields remain in excellent agreement with CFD ground truth across all cases. Also, the number and spacing of shed vortices in the wake match between ground truth and prediction, indicating accurate capture of vortex-shedding frequency. These results underscore the surrogates ability to adapt to complex boundary geometries and capture the corresponding flow patterns, which can vary dramatically depending on local curvature and feature sharpness. Pointwise Temporal Dynamics Near Boundaries. In Figure 5, we plot the time series of and at two downstream probes located at = 1D and = 2D (where is the characteristic diameter of each shape). These locations lie within the near-geometry region, where viscous boundary-layer effects dominate and flow transition depends sensitively on leading-edge shape and curvature. Despite the inherent difficulty of modeling highly transient, non-sinusoidal signals in this boundary-layer, the predicted time series closely follow the ground truth in both phase and amplitude. This agreement highlights the surrogate models ability to resolve fine-scale, geometry-driven unsteady phenomena at critical downstream positions. 4.3. Rollout prediction Rollout Prediction Accuracy. As shown in Figure 6(a), the overall relative L2 error begins at approximately 5% and grows to about 55% by = 60, indicating cumulative error accumulation when previous predictions are fed back into the model. This trend reflects the degradation of the models accuracy during rollouts, where each predicted field becomes the input for the next timestep. Correspondingly, the RMSE for both the and components increases monotonically (see Figure 6(b)), rising from roughly 0.02 and 0.05 at = 0 to 0.4 and 0.7 at = 60 for and v, respectively. This shows that the surrogate struggles to maintain accuracy over extended time horizons. Figure 6(c)(h) compare ground-truth and predicted velocity fields for single geometry at = 30, = 45, and = 59. At = 30, 5 r 2 i e r 2 i e r 2 i e r E 2 i e 102 100 80 60 20 0 0 single-step rollout 102 r 200 100 single-step rollout 10 30 time 40 50 60 0 10 20 40 50 60 30 time 100 80 60 40 20 100 80 60 40 20 100 80 60 40 20 (a) L2 error using input sequence s=1. 102 single-step rollout 10 20 30 time 40 50 60 (c) L2 error using input sequence s=4. 102 single-step rollout 10 20 30 40 50 time (e) L2 error using input sequence s=8. 102 single-step rollout 20 40 time 50 60 r 200 100 0 (b) error using input sequence s=1. 102 single-step rollout 10 20 30 time 40 50 (d) error using input sequence s=4. 102 300 r 200 0 300 200 100 0 r single-step rollout 10 20 30 40 60 time (f) error using input sequence s=8. 102 single-step rollout 30 40 time 50 60 (g) L2 error using input sequence s=16. (h) error using input sequence s=16. Figure 2: Time-evolution of single-step versus rollout prediction errors for varying input sequence lengths. Panels (a) and (b) plot the relative L2 and errors over time using an input sequence of length = 1. Panels (c) and (d) show the same metrics for = 4; panels (e) and (f) for = 8; and panels (g) and (h) for = 16. 6 r 2 i e 8 4 2 0 102 0 20 30 time 40 50 60 R 8 6 4 2 0 10 0 10 20 30 time 50 60 (a) Relative L2 error over time. (b) RMSE of and v. (c) GT, = 30 (d) GT, = (e) GT, = 59 (f) Pred, = 30 (g) Pred, = 45 (h) Pred, = 59 (i) GT, = 30 (j) GT, = (k) GT, = 59 (l) Pred, = 30 (m) Pred, = 45 (n) Pred, = 59 Figure 3: Single-step prediction of flow velocity for an example geometry. (a) Relative L2 error over time. (b) RMSE of and over time. (ce) Ground-truth at = 30, 45, 59. (fh) Predicted at = 30, 45, 59. (ik) Ground-truth at = 30, 45, 59. (ln) Predicted at = 30, 45, 59. Colorbars for are shown in (e) and (h), and for in (k) and (n). (a) GT (b) Pred (c) GT (d) Pred (e) GT (f) Pred (g) GT (h) Pred (i) GT (j) Pred (k) GT (l) Pred (m) GT (n) Pred (o) GT (p) Pred Figure 4: Single-step predictions for four example geometries at = 30. Each pair of rows corresponds to one shape: the top row shows the u-component and the bottom row the v-component. 0.5 0 0.5 0.8 0.6 0.4 0 0.1 0.8 0.6 0.4 0.2 0 = 1D = 1D = 2D = 2D 1 0 1 0.5 0.5 0.5 0 0.5 1 0 40 60 0 20 40 0 20 40 60 0 40 60 time time time time GT Pred GT Pred GT Pred GT Pred = 1D = 1D = 2D = 2D 1 0 1 0.9 0. 0.7 0.6 1 1 0 20 40 60 20 40 60 0 20 60 0 20 40 60 time time time time GT Pred GT Pred GT Pred GT Pred = 1D = 1D = 2D = 2D 0.5 0.5 0.5 0.4 0.3 0. 0 0.5 0 20 60 0 20 40 60 20 40 60 0 20 60 time time time time GT Pred GT Pred GT Pred GT Pred = 1D = 1D = 2D = 2D 1 0 1 0 20 60 0 20 40 60 1 0.8 0.6 0.4 0.2 1 0 1 20 40 60 0 20 60 time time time time GT Pred GT Pred GT Pred GT Pred Figure 5: Single step time-series of and at two points at downstream distance from geometry = 1D and = 2D, where is the geometry diameter. We show collection of 4 shapes where each row corresponds to single geometry. 9 the prediction closely matches the reference, with minor discrepancies confined near the solid boundary. By = 45, wake vortices exhibit slight phase shifts and reduced amplitude. At = 59, the predicted wake structure is noticeably different and vortex centers are displaced, highlighting the challenge of long-horizon rollouts. Robust Geometric Generalization During Rollouts. Figure 7 compares rollout predictions at = 30 for four markedly different geometries ranging from smooth, symmetric NURBS shapes to highly irregular harmonic perturbations and non-parametric skeleton outlines. The first two shapes (panels (a)(h)) show high deviation from the ground-truth data, particularly near sharp edges where the surrogate smooths peak velocities and displaces vortex cores. The last two shapes are in better agreement (panels (i)(p)), with wake vortices correctly positioned and velocity amplitudes closely matching the CFD reference. These results indicate that while rollout accuracy degrades for geometries with pronounced corners, the surrogate maintains robust prediction quality for smoother boundaries, preserving key flow structures across diverse shape complexities. Point Wise Temporal Dynamics During Rollouts. Figure 8 shows the rollout time series of and at two downstream probes located at = 1D and = 2D (where is the characteristic diameter of each shape) for four representative geometries. These probes lie within the neargeometry boundary layer region, where unsteady viscous effects and wake development are most pronounced. The predicted signals maintain accurate phase alignment and amplitude matching with ground truth for the first 2030 timesteps across all shapes but diverge gradually thereafter. The two shapes with sharp corners (rows one and two) exhibit larger deviations, characterized by phase lag and damped peak values, compared to the smoother geometries (row four), which demonstrate closer agreement through = 60. These results demonstrate the surrogates ability to capture essential unsteady boundary layer phenomena under feedback, while highlighting systematic error growth during rollouts, which scales with boundary complexity. Strouhal Number and Phase Lag. Figure 9 evaluates how well the surrogate preserves the periodic wake dynamics for single-timestep history (s = 1). For each test geometry, we record the vertical velocity v(x, t) on the wake centerline at four probes (x/D = 1, 2, 3, 4) and define the Strouhal number as the dominant non-dimensional frequency of v(x, t). The left column compares predicted versus ground-truth Strouhal numbers at all probes: the points form tight cloud around the = line with only few highand low-frequency outliers, and the number of these extremes decreases further downstream as the wake becomes less sensitive to local geometric details. To quantify phase coherence, we estimate at each probe phase offset ϕ(x) by finding the time delay τ(x) that maximizes the cross-correlation between predicted and ground-truth signals at the dominant shedding frequency, and then converting this delay into ϕ(x) = 2π fshed(x) τ(x) (right column). Phase lags are concentrated near zero, with fluctuations and no systematic tendency to lead or lag. In addition to these distributions, Table 1 summarizes error statistics for Strouhal number and phase lag across downstream probes and sequence lengths. For single-timestep input (s = 1), the relative L2 error in Strouhal number lies between 0.19 and 0.23 with < 0.59 at all locations. The mean phase lag is about 0.3 rad, with outliers approaching 3 rad that correspond to the most challenging, sharp-cornered geometries. As the history length increases (s = 4, 8, 16), both metrics generally improve: the best relative L2 error in Strouhal number decreases to 0.17 and drops below 0.52 at most probes, while the mean phase lag is reduced to < 0.15 rad for = 8 and below 0.1 rad at several probes for = 16. The maximum phase lag remains O(3) rad due to small number of difficult cases, but these outliers do not dominate the statistics. Overall, the model captures the dominant shedding frequency and its phase with minimal temporal context, and longer input sequences further improve both frequency and phase predictions. Error Amplification at Sharp Corners. We find that sharp corners are especially prone to error accumulation. First, the SDF at our 1024 256 grid only approximates sharp corners in pixelated way, smoothing out true corner geometry. Second, the CNN encoder downsamples spatial resolution by 32, which further blurs small-scale vortical structures that originate at those corners. During rollouts, these initial insufficient encodings at sharp edges propagate downstream and amplify, leading to the larger errors observed for high curvature shapes. Sample Level Variability in Prediction Accuracy. Figure 10 presents violin style density estimates of the relative L2 error across all test geometries at = 20 and = 50 for both single step and rollout evaluations. In the single step case (panel (a)), the error distribution at = 20 is tightly concentrated around low values (peak near 23%), and the error spread remains the same at = 50, indicating that minority of shapes particularly those with sharp features exhibit 10 high instantaneous error. The rollout distributions (panel (b)) show substantially greater broadening: at = 20, the median error is already higher than the single step case (peak near 1520%), and by = 50 the density extends to over 40% for most samples. The pronounced tails in both single step and rollout violins reveal that some geometries accumulate error much more rapidly, leading to bimodal appearance in the density. This reflects that smoother shapes cluster at low error throughout, whereas irregular and high-curvature geometries produce outliers with significantly degraded accuracy. Overall, these plots underscore that while the surrogate performs reliably on average, its worst case rollout performance can vary by an order of magnitude depending on sample shape. 102 100 r 2 i e 80 60 20 0 0 102 100 R 80 60 40 20 0 10 20 30 time (b) RMSE of and v. 50 60 10 20 30 time 50 60 (a) Relative L2 error over time. (c) GT, = 30 (d) GT, = 45 (e) GT, = (f) Pred, = 30 (g) Pred, = 45 (h) Pred, = 59 (i) GT, = 30 (j) GT, = 45 (k) GT, = (l) Pred, = 30 (m) Pred, = 45 (n) Pred, = 59 Figure 6: Rollout prediction of flow velocity for an example geometry. (a) Relative L2 error over time. (b) RMSE of and over time. (ce) Ground-truth at = 30, 45, 59. (fh) Predicted at = 30, 45, 59. (ik) Ground-truth at = 30, 45, 59. (ln) Predicted at = 30, 45, 59. Colorbars for are shown in (e) and (h), and for in (k) and (n). 11 (a) GT (b) Pred (c) GT (d) Pred (e) GT (f) Pred (g) GT (h) Pred (i) GT (j) Pred (k) GT (l) Pred (m) GT (n) Pred (o) GT (p) Pred Figure 7: Rollout predictions for four example geometries at = 30. Each pair of rows corresponds to one shape: the top row shows the u-component and the bottom row the v-component. 12 0. 0 0.5 1 0. 0 0.1 0 0.1 0. 0.8 0.6 0.4 0.2 0 = 1D = 1D = 2D = 2D 1 0 1 0.5 0 0.5 1 0 0 20 40 60 0 40 60 0 20 40 0 20 40 60 time time time time GT Pred GT Pred GT Pred GT Pred = 1D = 1D = 2D = 2D 1 0 1 0.5 0 0 1 0 20 40 0 20 40 60 0 40 60 0 20 40 time time time time GT Pred GT Pred GT Pred GT Pred = 1D = 1D = 2D = 2D 0.5 0 0.5 0.5 0.4 0. 0.5 0 0.5 0 40 60 0 20 40 0 20 40 60 0 40 60 time time time time GT Pred GT Pred GT Pred GT Pred = 1D = 1D = 2D = 2D 1 0 1 0 40 60 0 20 40 time time 1 0.8 0.6 0.4 0.2 0 1 0 20 40 0 20 40 60 time time GT Pred GT Pred GT Pred GT Pred Figure 8: Rollout time-series of and at two points at downstream distance from geometry = 1D and = 2D, where is the geometry diameter. We show collection of 4 shapes where each row correspond to single geometry. 13 = 1D"
        },
        {
            "title": "0.2\nGT S t\nx = 2D",
            "content": "0.2 GT = 3D 0.2 GT = 4D 0.4 0.3 0.2 0. . P 0 0 0. 0.4 0.3 0.2 0.1 . r 0 0 0.1 0.4 0. 0.2 0.1 . P 0 0.1 0.4 0.3 0.2 0.1 . P = 1D e P 0 2 0.3 0.4 0 200"
        },
        {
            "title": "100\nSample index\nx = 2D",
            "content": "g s 2 0 2 0. 0.4 0 50 200 250 100 Sample index = 3D e P 2 0 0.3 0.4 0 50 200 150 100 Sample index = 4D e P 2 2 0 0 0.1 0.2 GT 0. 0.4 0 50 150 100 Sample index 250 Figure 9: Strouhal number and phase-lag (in radians) for the case of = 1 at four downstream probe locations. Left column: predicted versus ground-truth Strouhal number with dashed = reference line. Right column: phase lag (prediction minus ground truth) for test samples. Each row corresponds to different streamwise position = 1D, = 2D, = 3D, and = 4D, where is the geometry diameter. 14 Table 1: Relative L2 and errors in the predicted Strouhal number, and mean / maximum phase lag between predicted and ground-truth wake signals, for different input sequence lengths and downstream probe locations x/D. Bold values indicate the best-performing sequence length for each metric across all probe locations (x/D). x/D 2 3 4 1 4 8 16 1 4 8 1 4 8 16 1 4 8 16 relative L2(Strouhal) L(Strouhal) mean phase lag [rad] max phase lag [rad] 0.233 0.228 0.210 0.190 0.209 0.210 0.195 0. 0.196 0.213 0.193 0.178 0.190 0.210 0.211 0.171 0.563 0.546 0.521 0.418 0.589 0.575 0.557 0.525 0.578 0.558 0.581 0.513 0.565 0.510 0.565 0. 0.337 0.353 0.128 0.168 0.267 0.365 0.140 0.033 0.274 0.314 0.160 0.049 0.241 0.326 0.184 0.091 3.004 2.876 2.886 2.666 2.807 2.832 2.399 2. 3.099 3.009 3.011 2.617 3.028 3.033 3.124 3.101 (a) Single-step relative L2 error at = 20 and = 50. (b) Rollout relative L2 error at = 20 and = 50. Figure 10: Violin-style density estimates of relative L2 error at = 20 and = 50 for (a) single-step predictions and (b) autoregressive rollouts. 5. Conclusions In this work, we introduce time dependent Geometric Deep Operator Network that integrates an SDF based geometry encoding with convolutional history encoder to predict unsteady, periodic flow around complex 2D shapes. Our extensive evaluation on the FlowBench flow past an object dataset demonstrates the following key findings: High Single-Step Accuracy: When provided with ground-truth inputs, the surrogate achieves an average relative L2 error of approximately 5% and stable RMSE values for both and components across 60 timesteps. Error Accumulation in Rollouts: Under autoregressive rollouts, prediction error grows monotonically to about 55% relative L2 by = 60, highlighting challenges in long horizon stability. Geometric Generalization: The model reliably captures vortex shedding patterns and wake structures across smooth shapes. Performance degrades the most for geometries with sharp corners, where rollouts exhibit 15 smoothed peaks and displaced vortices. Predictions for smoother shapes maintain closer agreement with the ground truth data. Point Wise Temporal Dynamics: Time series at downstream probes (x = 1D, 2D) reveal accurate prediction of the time series for the single step predictions. The rollout prediction shows phase alignment and amplitude matching for the first 2030 steps before error accumulates and degrades predictions. Sample Level Variability: Violin plot analysis shows pronounced tails and bimodality of the error distribution, indicating that complex geometries can incur an order-of-magnitude higher error than smoother counterparts. These results underscore both the promise and limitations of purely data-driven neural operators for unsteady flow: they offer dramatic speedups (1000) and strong short term accuracy but require further developments to sustain long term autoregressive rollout on complex boundaries. Future directions for this work span physics, generative refinement, temporal context, and generalization. First, one could incorporate explicit physical consistency during training by adding NavierStokes residual regularization term (PINN style), which penalizes violations of incompressibility and momentum balance. This term could feature spatial weighting near the embedded boundary and time-weighting across rollout steps, directly targeting long horizon drift. Second, to improve robustness during rollout without sacrificing fast inference, one should investigate diffusionbased decoders (in the spirit of PDERefiner approaches) that take the operators coarse prediction and iteratively denoise/refine it, correcting accumulated phase and amplitude errors and sharpening nearboundary features when needed. Third, rather than relying on fixed history length, we plan to explore adaptive history schemes that selectively incorporate longer temporal context only when the model detects increased uncertainty or onset of unstable dynamics, preserving efficiency in steady regimes while improving predictions during transients. Finally, it will be useful to rigorously evaluate outofdistribution generalization by testing on geometries unseen during training, including sharper corners, different aspect ratios, and altered curvature/topology, using not only pointwise errors but also physicscentric rollout diagnostics (e.g., divergence, probe phase/Strouhal consistency) to identify failure modes and guide targeted data augmentation and model design. Acknowledgements We gratefully acknowledge the NAIRR pilot program for enabling computational access, and we thank the ISU HPC cluster Nova and TACCs Frontera for additional computing support. This research was funded by the AI Research Institutes program through NSF and USDANIFA under the AI Institute for Resilient Agriculture (Award No. 2021-67021-35329), with further support from NSF grants CMMI-2053760 and DMREF-2323716. Data Availability This study utilizes the FlowBench Flow Past an Object (FPO) dataset, which is publicly accessible on HuggingFace at https://huggingface.co/datasets/BGLab/FlowBench/tree/main/FPO_NS_2D_1024x256. The dataset is licensed under CC-BY-NC-4.0 license and serves as benchmark for developing and evaluating scientific machine learning (SciML) models. The code used for training, to facilitate reproducibility or results, is available at https://github.com/baskargroup/TimeDependent-DeepONet. 16 References [1] Beichang He, Omar Ghattas, and James Antaki. Computational strategies for shape optimization of timedependent navier-stokes flows. Engineering Design Research Center, TR-CMU-CML-97-102, Carnegie Mellon Univ, 1997. [2] Bijan Mohammadi and Olivier Pironneau. Applied shape optimization for fluids. OUP Oxford, 2009. [3] Xueliang Li, Mingzhi Yang, Lin Bi, Renze Xu, Canyan Luo, Siqi Yuan, Xianxu Yuan, and Zhigong Tang. An efficient cartesian mesh generation strategy for complex geometries. Computer Methods in Applied Mechanics and Engineering, 418:116564, 2024. [4] Dazhao Gou and Yansong Shen. Gpu-accelerated cfd-dem modeling of gas-solid flow with complex geometry and an application to raceway dynamics in industry-scale blast furnaces. Chemical Engineering Science, 294: 120101, 2024. [5] Behzad Forouzi Feshalami, Shuisheng He, Fulvio Scarano, Lian Gan, and Chris Morton. review of experiments on stationary bluff body wakes. Physics of Fluids, 34(1), 2022. [6] Ying Wu, Zhi Cheng, Ryley McConkey, Fue-Sang Lien, and Eugene Yee. Modelling of flow-induced vibration of bluff bodies: comprehensive survey and future prospects. Energies, 15(22):8719, 2022. [7] Joshua Baden Fuller. The unsteady aerodynamics of static and oscillating simple automotive bodies. PhD thesis, Loughborough University Loughborough, 2012. [8] Lars Ericsson. Unsteady flow separation can endanger the structural integrity of aerospace launch vehicles. Journal of Spacecraft and Rockets, 38(2):168179, 2001. [9] Puja Haldar and Somnath Karmakar. State of the art review of aerodynamic effects on bridges. Journal of The Institution of Engineers (India): Series A, 103(3):943960, 2022. [10] Mujahid Badshah, Saeed Badshah, James VanZwieten, Sakhi Jan, Muhammad Amir, and Suheel Abdullah Malik. Coupled fluid-structure interaction modelling of loads variation and fatigue life of full-scale tidal turbine under the effect of velocity profile. Energies, 12(11):2217, 2019. [11] Sang Lee, Matthew Churchfield, Patrick Moriarty, Jason Jonkman, and John Michalakes. Atmospheric and wake turbulence impacts on wind turbine fatigue loadings. In 50th AIAA Aerospace Sciences Meeting including the New Horizons Forum and Aerospace Exposition, page 540, 2012. [12] Ronak Tali, Ali Rabeh, Cheng-Hau Yang, Mehdi Shadkhah, Samundra Karki, Abhisek Upadhyaya, Suriya Dhakshinamoorthy, Marjan Saadati, Soumik Sarkar, Adarsh Krishnamurthy, et al. Flowbench: large scale benchmark for flow simulation over complex geometries. DMLR, 2025. URL https://openreview.net/ forum?id=0xrOmmB2KQ. [13] Franke, Rodi, and Schönung. Numerical calculation of laminar vortex-shedding flow past cylinders. Journal of Wind Engineering and Industrial Aerodynamics, 35:237257, 1990. [14] Ali Rabeh, Ethan Herron, Aditya Balu, Soumik Sarkar, Chinmay Hegde, Adarsh Krishnamurthy, and Baskar Ganapathysubramanian. Benchmarking scientific machine-learning approaches for flow prediction around complex geometries. Communications Engineering, 4(1):182, 2025. [15] Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural operator: Learning maps between function spaces with applications to PDEs. Journal of Machine Learning Research, 24(89):197, 2023. [16] Lu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, and George Em Karniadakis. Learning nonlinear operators via deeponet based on the universal approximation theorem of operators. Nature Machine Intelligence, 3:218229, 2021. 17 [17] Ali Rabeh, Adarsh Krishnamurthy, and Baskar Ganapathysubramanian. 3d neural operator-based flow surrogates around 3d geometries: Signed distance functions and derivative constraints. arXiv preprint arXiv:2503.17289, 2025. [18] Sifan Wang, Hanwen Wang, and Paris Perdikaris. Learning the solution operator of parametric partial differential equations with physics-informed deeponets. Science advances, 7(40):eabi8605, 2021. [19] Mehdi Shadkhah, Ronak Tali, Ali Rabeh, Ethan Herron, Cheng-Hau Yang, Abhisek Upadhyaya, Adarsh Krishnamurthy, Chinmay Hegde, Aditya Balu, and Baskar Ganapathysubramanian. Mpfbench: large scale dataset for sciml of multi-phase-flows: Droplet and bubble dynamics. DMLR, 2025. URL https: //openreview.net/pdf?id=9gQnIFI4es. [20] Wei Li, Martin Bazant, and Juner Zhu. Phase-field deeponet: Physics-informed deep operator neural network for fast simulations of pattern formation governed by gradient flows of free-energy functionals. Computer Methods in Applied Mechanics and Engineering, 416:116299, 2023. [21] Zijie Li, Dule Shu, and Amir Barati Farimani. Scalable transformer for pde surrogate modeling. Advances in Neural Information Processing Systems, 36:2801028039, 2023. [22] Maximilian Herde, Bogdan Raonic, Tobias Rohner, Roger Käppeli, Roberto Molinaro, Emmanuel de Bézenac, and Siddhartha Mishra. Poseidon: Efficient foundation models for pdes, 2024. [23] Yuanjun Dai, Yiran An, Zhi Li, Jihua Zhang, and Chao Yu. Fourier neural operator with boundary conditions for efficient prediction of steady airfoil flows. Applied Mathematics and Mechanics, 44(11):20192038, 2023. [24] Jaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh Chattopadhyay, Morteza Mardani, Thorsten Kurth, David Hall, Zongyi Li, Kamyar Azizzadenesheli, et al. Fourcastnet: global datadriven high-resolution weather model using adaptive fourier neural operators. arXiv preprint arXiv:2202.11214, 2022. [25] Phillip Lippe, Bas Veeling, Paris Perdikaris, Richard Turner, and Johannes Brandstetter. PDE-refiner: Achieving accurate long rollouts with neural PDE solvers. Advances in Neural Information Processing Systems, 36: 6739867433, 2023. [26] Harris Abdul Majid and Francesco Tudisco. Mixture of neural operators: Incorporating historical information for longer rollouts. In ICLR 2024 Workshop on AI4DifferentialEquations In Science, 2024. [27] Junyan He, Seid Koric, Diab Abueidda, Ali Najafi, and Iwona Jasiuk. Geom-deeponet: point-cloud-based deep operator network for field predictions on 3d parameterized geometries. Computer Methods in Applied Mechanics and Engineering, 429:117130, September 2024. ISSN 0045-7825. doi: 10.1016/j.cma.2024.117130. URL http://dx.doi.org/10.1016/j.cma.2024.117130. [28] Zongyi Li, Nikola Kovachki, Chris Choy, Boyi Li, Jean Kossaifi, Shourya Otta, Mohammad Amin Nabian, Maximilian Stadler, Christian Hundt, Kamyar Azizzadenesheli, et al. Geometry-informed neural operator for large-scale 3d pdes. Advances in Neural Information Processing Systems, 36:3583635854, 2023. [29] Samundra Karki, Mehdi Shadkah, Cheng-Hau Yang, Aditya Balu, Guglielmo Scovazzi, Adarsh Krishnamurthy, and Baskar Ganapathysubramanian. Direct flow simulations with implicit neural representation of complex geometry. arXiv preprint arXiv:2503.08724, 2025. [30] Zongyi Li, Daniel Zhengyu Huang, Burigede Liu, and Anima Anandkumar. Fourier neural operator with learned deformations for pdes on general geometries. Journal of Machine Learning Research, 24(388):126, 2023. [31] Heming Bai, Zhicheng Wang, Xuesen Chu, Jian Deng, and Xin Bian. Data-driven modeling of unsteady flow based on deep operator network. Physics of Fluids, 36(6), 2024. 18 [32] Ilke Demir, Camilla Hahn, Kathryn Leonard, Geraldine Morin, Dana Rahbani, Athina Panotopoulou, Amelie Fondevilla, Elena Balashova, Bastien Durix, and Adam Kortylewski. Skelneton 2019: Dataset and challenge on deep learning for geometric shape understanding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 00, 2019. [33] Junyan He, Shashank Kushwaha, Jaewan Park, Seid Koric, Diab Abueidda, and Iwona Jasiuk. Sequential deep operator networks (s-deeponet) for predicting full-field solutions under time-dependent loads. Engineering Applications of Artificial Intelligence, 127:107258, 2024. [34] Waleed Diab and Mohammed Al-Kobaisi. Temporal neural operator for modeling time-dependent physical phenomena. arXiv preprint arXiv:2504.20249, 2025. [35] Alex Main and Guglielmo Scovazzi. The shifted boundary method for embedded domain computations. part I: Poisson and stokes problems. Journal of Computational Physics, 372:972995, 2018. [36] Cheng-Hau Yang, Kumar Saurabh, Guglielmo Scovazzi, Claudio Canuto, Adarsh Krishnamurthy, and Baskar Ganapathysubramanian. Optimal surrogate boundary selection and scalability studies for the shifted boundary method on octree meshes. Computer Methods in Applied Mechanics and Engineering, 419:116686, 2024. [37] Cheng-Hau Yang, Guglielmo Scovazzi, Adarsh Krishnamurthy, and Baskar Ganapathysubramanian. Simulating incompressible flows over complex geometries using the shifted boundary method with incomplete adaptive octree meshes. arXiv preprint arXiv:2411.00272, 2024. 19 Appendix .1. Model Architecture Details Table .2 gives layer-by-layer specification of our time-dependent Geometric DeepONet. Notation: batch size; = Nt number of input timesteps; H, spatial height and width; = total points; latent dimension; c3 CNN branch channels; c1, c2 fusion channels; Cout output channels. Component Configuration Branch CNN Input CNN Encoder Encoder Fusion Stage Encoder Fusion Stage 2 Branch MLP (Stage 1) Trunk Input Trunk MLP (Stage 1) Stage 1 Fusion Branch MLP (Stage 2) Trunk MLP (Stage 2) Final Fusion 32 [B, 2Nt, H, W] 3 Parallel conv streams (1 1, 3 3, 5 5), each with 2 2 max pooling, producing [B, c3, H/8, W/8]; concatenated to [B, 3c3, H/8, W/8]. 1 1 conv (3c3 c1), 2 2 pool [B, c1, H/16, W/16]. 1 1 conv ( c1 c2), 2 2 pool [B, c2, H/32, W/32]. [ c2 32 , 256, 128, ], ReLU [B, P, 3] with channels corresponding to (x, y, SDF) [3, 128, 128, m], ReLU Element-wise product of branch latent and trunk features [B, P, m]. [ m, 128, 128, Cout ], ReLU [ m, 128, 128, Cout ], sine Dot-product over modes [B, P, Cout] Table .2: Architecture of the time-dependent Geometric DeepONet. Appendix .2. Training and Validation Loss To further analyze training performance, we present the evolution of training and validation loss for our TimeDependent Geometric-DeepONet across four input sequence lengths: = 1, = 4, = 8, and = 16, as shown in Figure .11. Appendix .3. Strouhal Number and Phase Lag with Increased Input Sequence Lengths In the main text, we analyzed the Strouhal number and phase lag for single-timestep input history (s = 1); see Figure 9. Here, we repeat the same diagnostics for longer input sequences with = 4, = 8, and = 16. The corresponding results are shown in Figure .12, Figure .13, and Figure .14, respectively. For each sequence length, the left column plots predicted versus ground-truth Strouhal numbers at four downstream probes (x/D = 14), while the right column shows the distribution of phase lag (prediction minus ground truth) across test samples at the same locations. Across all values of s, the Strouhal scatter remains tightly clustered around the = line for every probe location, with small number of outliers. As summarized in Table 1, increasing the sequence length yields modest but consistent improvements in the Strouhal error: the relative L2 error decreases from 0.23 for = 1 to 0.17 for = 16. The phase-lag distributions are centered near zero for all s, with slightly reduced mean phase lag for = 8 and = 16. These diagnostics show that longer input histories provide small gains in both frequency and phase accuracy, while the overall temporal coherence of the wake is well captured with single-timestep input (s = 1). 20 Input sequence = 1 Input sequence = 4 100 101 L 102 0 100 101 L"
        },
        {
            "title": "Training\nValidation",
            "content": "200 400 600 800 1,000 Epoch Input sequence = Training Validation 200 400 600 800 1, Epoch 101 100 101 L 10 0 101 100 101 L"
        },
        {
            "title": "Training\nValidation",
            "content": "200 400 600 800 1,000 Epoch Input sequence = Training Validation 200 400 600 800 1, Epoch Figure .11: Training and validation loss in semi-log scale for Time-Dependent Geometric-DeepONet across 4 input sequence lengths = 1-s = 16. This figure presents the evolution of both training (blue) and validation (orange) losses over 1000 epochs for each sequence length. 21 = 1D"
        },
        {
            "title": "0.2\nGT S t\nx = 2D",
            "content": "0.2 GT = 3D 0.2 GT = 4D 0.4 0.3 0.2 0. . P 0 0 0. 0.4 0.3 0.2 0.1 . r 0 0 0.1 0.4 0. 0.2 0.1 . P 0 0.1 0.4 0.3 0.2 0.1 . P = 1D e P 0 2 0.3 0.4 0 200"
        },
        {
            "title": "100\nSample index\nx = 2D",
            "content": "g s 2 0 2 0. 0.4 0 50 200 250 100 Sample index = 3D e P 2 0 0.3 0.4 0 50 200 150 100 Sample index = 4D e P 2 2 0 0 0.1 0.2 GT 0. 0.4 0 50 150 100 Sample index 250 Figure .12: Strouhal number and phase-lag (in radians) for the case of = 4 at four downstream probe locations. Left column: predicted versus ground-truth Strouhal number with dashed = reference line. Right column: phase lag (prediction minus ground truth) for test samples. Each row corresponds to different streamwise position = 1D, = 2D, = 3D, and = 4D, where is the geometry diameter. 22 = 1D"
        },
        {
            "title": "0.2\nGT S t\nx = 2D",
            "content": "0.2 GT = 3D 0.2 GT = 4D 0.4 0.3 0.2 0. . P 0 0 0. 0.4 0.3 0.2 0.1 . r 0 0 0.1 0.4 0. 0.2 0.1 . P 0 0.1 0.4 0.3 0.2 0.1 . P = 1D e P 0 2 0.3 0.4 0 200"
        },
        {
            "title": "100\nSample index\nx = 2D",
            "content": "g s 2 0 2 0. 0.4 0 50 200 250 100 Sample index = 3D e P 2 0 0.3 0.4 0 50 200 150 100 Sample index = 4D e P 2 2 0 0 0.1 0.2 GT 0. 0.4 0 50 150 100 Sample index 250 Figure .13: Strouhal number and phase-lag (in radians) for the case of = 8 at four downstream probe locations. Left column: predicted versus ground-truth Strouhal number with dashed = reference line. Right column: phase lag (prediction minus ground truth) for test samples. Each row corresponds to different streamwise position = 1D, = 2D, = 3D, and = 4D, where is the geometry diameter. 23 = 1D"
        },
        {
            "title": "0.2\nGT S t\nx = 2D",
            "content": "0.2 GT = 3D 0.2 GT = 4D 0.4 0.3 0.2 0. . P 0 0 0. 0.4 0.3 0.2 0.1 . r 0 0 0.1 0.4 0. 0.2 0.1 . P 0 0.1 0.4 0.3 0.2 0.1 . P = 1D e P 0 2 0.3 0.4 0 200"
        },
        {
            "title": "100\nSample index\nx = 2D",
            "content": "g s 2 0 2 0. 0.4 0 50 200 250 100 Sample index = 3D e P 2 0 0.3 0.4 0 50 200 150 100 Sample index = 4D e P 2 2 0 0 0.1 0.2 GT 0. 0.4 0 50 150 100 Sample index 250 Figure .14: Strouhal number and phase-lag (in radians) for the case of = 16 at four downstream probe locations. Left column: predicted versus ground-truth Strouhal number with dashed = reference line. Right column: phase lag (prediction minus ground truth) for test samples. Each row corresponds to different streamwise position = 1D, = 2D, = 3D, and = 4D, where is the geometry diameter."
        }
    ],
    "affiliations": [
        "Iowa State University, Ames, Iowa, USA"
    ]
}