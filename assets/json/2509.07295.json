{
    "paper_title": "Reconstruction Alignment Improves Unified Multimodal Models",
    "authors": [
        "Ji Xie",
        "Trevor Darrell",
        "Luke Zettlemoyer",
        "XuDong Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Unified multimodal models (UMMs) unify visual understanding and generation within a single architecture. However, conventional training relies on image-text pairs (or sequences) whose captions are typically sparse and miss fine-grained visual details--even when they use hundreds of words to describe a simple image. We introduce Reconstruction Alignment (RecA), a resource-efficient post-training method that leverages visual understanding encoder embeddings as dense \"text prompts,\" providing rich supervision without captions. Concretely, RecA conditions a UMM on its own visual understanding embeddings and optimizes it to reconstruct the input image with a self-supervised reconstruction loss, thereby realigning understanding and generation. Despite its simplicity, RecA is broadly applicable: across autoregressive, masked-autoregressive, and diffusion-based UMMs, it consistently improves generation and editing fidelity. With only 27 GPU-hours, post-training with RecA substantially improves image generation performance on GenEval (0.73$\\rightarrow$0.90) and DPGBench (80.93$\\rightarrow$88.15), while also boosting editing benchmarks (ImgEdit 3.38$\\rightarrow$3.75, GEdit 6.94$\\rightarrow$7.25). Notably, RecA surpasses much larger open-source models and applies broadly across diverse UMM architectures, establishing it as an efficient and general post-training alignment strategy for UMMs"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 5 9 2 7 0 . 9 0 5 2 : r a"
        },
        {
            "title": "RECONSTRUCTION ALIGNMENT IMPROVES UNIFIED\nMULTIMODAL MODELS",
            "content": "Ji Xie1, Trevor Darrell1, Luke Zettlemoyer2, XuDong Wang1 1UC Berkeley, 2University of Washington Project Page: https://reconstruction-alignment.github.io/"
        },
        {
            "title": "ABSTRACT",
            "content": "Unified multimodal models (UMMs) unify visual understanding and generation within single architecture. However, conventional training relies on imagetext pairs (or sequences) whose captions are typically sparse and miss fine-grained visual detailseven when they use hundreds of words to describe simple image. We introduce Reconstruction Alignment (RecA), resource-efficient posttraining method that leverages visual understanding encoder embeddings as dense text prompts, providing rich supervision without captions. Concretely, RecA conditions UMM on its own visual understanding embeddings and optimizes it to reconstruct the input image with self-supervised reconstruction loss, thereby realigning understanding and generation. Despite its simplicity, RecA is broadly applicable: across autoregressive, masked-autoregressive, and diffusion-based UMMs, it consistently improves generation and editing fidelity. With only 27 GPU-hours, post-training with RecA substantially improves image generation performance on GenEval (0.730.90) and DPGBench (80.9388.15), while also boosting editing benchmarks (ImgEdit 3.383.75, GEdit 6.947.25). Notably, RecA surpasses much larger open-source models and applies broadly across diverse UMM architectures, establishing it as an efficient and general post-training alignment strategy for UMMs. Figure 1: Post-training UMMs with reconstruction alignment (i.e., RecA) substantially improves image generation and editing. Left: performance comparison on GenEval and DPGBench, where 1.5B-parameter model post-trained with RecA surpasses much larger models across multiple benchmarks (Table 1: GenEval, DPGBench and Wise; Table 3: ImgEdit and GEdit-Bench-EN); Middle: compared with GPT-4o, RecA follows generation instructions more faithfully, especially for color attributes and spatial positions; Right: for editing, RecA better preserves instance identity, overall layout, and object shapes of the original images, such as the girls lips."
        },
        {
            "title": "INTRODUCTION",
            "content": "Building on the success of large language models (LLMs) (Brown et al., 2020; Touvron et al., 2023; Yang et al., 2024a), researchers have developed Multimodal Large Language Models (MLLMs) (Liu et al., 2024b; Bai et al., 2023; 2025; Radford et al., 2021; Zhai et al., 2023; Chen et al., 2024; Zhu corresponding author"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Dense supervision from visual embeddings. a) Typical image generation models are trained on imagecaption pairs and/or sequences whose text is sparse representation of visual information. An image is worth far more than hundred of words and contains rich details that text alone cannot capture. As shown in the left three examples, even lengthy captions (500 words) miss key aspects such as textures, styles, layouts, shapes, and attributes, leading to imperfect generations relative to the original image. b) By contrast, embeddings from visual understanding encoders, e.g., CLIP, preserve richer and more faithful semantics. Can these imageembedding pairs provide the dense supervision needed to enhance image generation and editing? Surprisingly, the answer is yes: we find that imageembedding pairs can improve T2I and image editing in zero-shot manner. et al., 2025) with strong visual understanding performance. Recently, unified multimodal models (UMMs), or Omni Models, have been proposed to both understand and generate across modalitiesreading and writing visual content and text within single architecture (Team, 2024; Zhou et al., 2025; Tong et al., 2024; Ge et al., 2024; Wu et al., 2024b; Chen et al., 2025c; Pan et al., 2025a; OpenAI, 2024; Li et al., 2025). The academic community envisions that unified framework can inherit the reasoning and world knowledge of LLMs while extending them to content generation. conventional limiHowever, UMMs face fundamental tation: training relies on imagetext pairs, where captions provide supervision. Even captions spanning hundreds of words omit critical visual details such as spatial layout, geometry, or fine-grained attributes (Figure 2), introducing systematic biases into the learned representations. For instance, since captions rarely describe broccolis color, models tend to overfit to the rule broccoli green, often collapsing to green outputs or failing on atypical prompts like yellow broccoli (Figure 3). This misalignment motivates us to explore alternative forms of supervision. Rather than relying on captions, we leverage embeddings from visual understanding encoders (Radford et al., 2021; Zhai et al., 2023; Chen et al., 2024; Zhu et al., 2025), which map pixels into language-aligned semantic space interpretable by unified multimodal models. Crucially, embeddings from understanding encoders (e.g., CLIP, SigLIP) capture semantic structure far more effectively than those from generation encoders (e.g., VAE, VQ-GAN). These semantic embeddings provide dense, semantically grounded supervision without paired captions, raising central question: Can we improve the generation capabilities of UMMs by training them with semantic embeddings as maximally informative text prompts? Figure 3: UMMs can often correctly recognize an uncommon concept (yellow broccoli) but fail to generate it, revealing misalignment between understanding and generation. Building on these insights, we propose RecA, resource-efficient post-training strategy. The core idea is simple: condition UMMs on their own understanding vision encoder embeddingsdense visual prompts that encode layout, color, and attributes beyond what captions captureand train"
        },
        {
            "title": "Preprint",
            "content": "Figure 4: Overview of the semantic reconstruction alignment (RecA) pipeline. visual understanding encoder (e.g., CLIP or DINO) extracts semantic features from the input image, which are fused with template text embeddings and passed to Unified Multimodal Model (UMM) to regenerate the image. The UMM is optimized with self-supervised loss (diffusion or cross-entropy) between the original and reconstructed images or image latents. At inference time, RecA requires no additional inputs, operating as standard UMM. them to reconstruct the image. This semantic reconstruction provides richer supervision without additional labels. Despite its simplicity, RecA yields substantial improvements. With only 27 A100 GPU hours, 1.5B-parameter UMM post-trained with RecA surpasses GPT-4o and significantly larger open-source models, achieving state-of-the-art results on GenEval (0.86) and DPGBench (87.21). Importantly, these gains are obtained without any GPT-4o-Image distillation data or reinforcement learning (OpenAI, 2024; Chen et al., 2025a;b), in contrast to prior work. Moreover, when post-trained with GPT-4o data, RecA further improves to GenEval (0.90) and DPGBench (88.15), substantially surpassing previous methods that use much larger model sizes. RecA also boosts image editing quality, raising ImgEdit from 3.383.75 and GEdit from 6.947.25. Moreover, RecA applies broadly across UMM familiesincluding Show-o (Xie et al., 2025b) (AR), Harmon (Wu et al., 2025c) (AR+MAR), OpenUni (Wu et al., 2025b) (AR+Diffusion), and BAGEL (Deng et al., 2025) (AR+Diffusion)highlighting its generality. The key contributions can be summarized as follows: Method: We introduce RecA, semantic reconstructionbased post-training method that uses semantic visual embeddings as dense prompts, providing rich supervision without captions. Generality: We show that RecA consistently improves diverse UMM architectures, from autoregressive to hybrid frameworks. Performance: We demonstrate strong empirical gains, with 1.5B-parameter model surpassing GPT-4o and larger open-source models using only 27 A100 GPU hours, significantly outperforming prior state of the art without distillation or RL."
        },
        {
            "title": "2 RECONSTRUCTION ALIGNMENT",
            "content": "In this section, we present Reconstruction Alignment (RecA) as self-supervised image reconstruction objective. By training the model to reconstruct images from its visual understanding encoder embeddings, RecA provides dense supervision that captures fine-grained visual details often omitted by text captions. We show the overall pipeline at Figure 4."
        },
        {
            "title": "2.1 MOTIVATION AND SETUP",
            "content": "Since the visual understanding encoder projects image features into language-aligned semantic space, truly \"unified\" model of visual understanding and generation should be capable of leveraging dense, rich semantic information and generating the same image. To test this, we extract semantic embeddings from the visual understanding encoder, insert them into prompt template (e.g., Describe the image in detail.), and ask the UMM to regenerate the input image."
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Post-training with RecA restores visual details beyond the baseline. For each query image (left), we feed its visual understanding embeddings back into the UMM with the instruction Describe the image in detail. The baseline model (centre)s visual responses, i.e., images, preserve the main subject but distort layout, textures, and colors, while RecA markedly restores visual details like geometry, color, and overall fidelity. As shown in Figure 5, the results are revealing: current UMMs preserve the main subject but scramble spatial layout and composition. This suggests that UMMs trained under conventional paradigms fail to fully exploit the models fine-grained semantic, i.e., the latent spaces for understanding and generation remain only partially aligned."
        },
        {
            "title": "2.2 RE CA TRAINING PARADIGM",
            "content": "Training losses. Traditional UMMs are trained with combination of text-to-image (T2I) and image-to-text (I2T) objectives. Formally: Lt2i = L(fθ(tprompt), Igt), Li2t = L(fθ(concat(tquestion, hv)), tanswer) (1) where L(, ) denotes the training loss (e.g., cross-entropy for autoregressive models (Xie et al., 2025b; Chen et al., 2025d), diffusion loss for diffusion-based models (Zhou et al., 2025; Deng et al., 2025)), tprompt, tquestion, tanswer are text inputs/outputs, hv are embeddings extracted from the visual understanding encoder, and Igt is the ground-truth image (we omit the explicit VAE decoder notation for simplicity). fθ() represents the UMM parameters θ. Our key idea is to replace conventional T2I supervision with an image reconstruction loss. Instead of using text captions that are inherently sparse in visual information, we condition the UMM on its own visual understanding embeddings, which provide richer semantics. The reconstruction loss is: LRecA = L(fθ(concat(ttemplate, hv)), Igt) (2) where ttemplate is simple prompt template triggering image reconstruction. The overall training objective can be formulated as: Ltotal = λRecALRecA + λi2tLi2t + λt2iLt2i. (3) In our experiments, we set λRecA = 1 and λt2i = 0. For UMMs with shared parameters for both understanding and generation, we set λı2t = 1, thereby preserving image-to-text understanding. For the visual encoder input, the image is resized to the lowest resolution the encoder can accept. Model inference. At inference time, our post-trained UMM operates identically to standard UMM and requires no additional visual embeddings. For image generation, only text prompt is needed; for image editing, the inputs remain the text prompt and the original image. This preserves the models original usability while providing enhanced generation capabilities."
        },
        {
            "title": "2.3 DIFFERENCE BETWEEN RE CA AND PREVIOUS WORKS",
            "content": "RecA introduces semantic-level image reconstruction as native post-training objective for UMMs, directly leveraging visual understanding priors to improve both generation and editing, without auxiliary modules or text-image supervision. In contrast, previous works integrate reconstruction in different ways: (I) diffusion-supervised enhancement (e.g., DIVA, ViLex) (Wang et al., 2024b; 2025c; Ma et al., 2025a) leverages pretrained diffusion models to regularize vision encoders"
        },
        {
            "title": "Preprint",
            "content": "and improve understanding; (II) reconstruction from hidden states (e.g., ROSS (Wang et al., 2024a; 2025b)) adds lightweight decoders to regenerate input images from intermediate embeddings, thereby regularizes the model to preserve fine-grained details for VLMs; (III) representation alignment (e.g., REPA (Yu et al., 2024)) introduces additional alignment modules that map noisy hidden states from the VAE in denoising networks to clean image representations obtained from external, pretrained visual encoders; and (IV) reconstruction as prior (e.g., Lumos (Ma et al., 2025b)) adds additional DINO features (Caron et al., 2021) into the attention blocks of the diffusion model, which is further trained on large-scale text-to-image data. Our method is fundamentally different in terms of the methodology, architecture, motivation and task."
        },
        {
            "title": "2.4 DIFFERENCE BETWEEN RE CA AND CLASSIFIER-FREE GUIDANCE (CFG)",
            "content": "Classifier-free guidance (CFG (Ho & Salimans, 2022)) is typically used to improve fidelity. At each generation step, we compute conditional prediction ocond and an unconditional prediction ouncond, where denotes either the autoregressive heads logits (Chen et al., 2025d; Xie et al., 2025b) or the diffusion heads predicted noise (Zhou et al., 2025; Deng et al., 2025). The output is given by: = (1 + ω) ocond ω ouncond, with ω the guidance scale. RecA is conceptually orthogonal to CFG. Whereas CFG relies on contrast between conditional and null-text (or template) prompt, RecA leverages embeddings from the visual understanding encoder as dense prompts for reconstruction-based alignment. The two techniques are fully compatible and can be applied together. (4)"
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "We validate our RecA method across various unified multimodal models (UMMs), image datasets, and evaluation benchmarks. In particular, we investigate the following aspects: SOTA Results: RecA achieves state-of-the-art performance on both image generation and editing benchmarks. (Table 1, Table 3) Generality: RecA delivers consistent performance gains across diverse UMM frameworks, demonstrating its generalizability. (Table 2, Figure 6, Figure 7) Training Paradigm: RecA serves as post-training method applied after UMM pre-training, and is most effective when used at the final stage of model training. (Table 5, Table 6)"
        },
        {
            "title": "3.1 EXPERIMENT SETUP",
            "content": "Model architectures. We evaluate RecA across four open-source UMM architectures: Show-o (AR) (Xie et al., 2025b) employs autoregressive generation, using CLIP (Radford et al., 2021) / VQGAN (Esser et al., 2021) as the visual understanding / generation encoder, evaluated at 256256 and 512512 resolutions. Harmon (MAR) (Wu et al., 2025c) adopts masked autoregressive generation, with MAE (Li et al., 2024) / VAE (Kingma & Welling, 2013) as the visual understanding / generation encoder, evaluated at 0.5B and 1.5B parameter scales. OpenUni (AR+Diff) (Wu et al., 2025b) integrates autoregressive and diffusion generation, with InternVL3 (Zhu et al., 2025) / VAE (Kingma & Welling, 2013) as the visual understanding / generation encoder. It serves as an open-source counterpart of MetaQueries (Pan et al., 2025b); we evaluate both 1.6B and 3.6B variants at 512512 resolution without GPT-4o-Image distillation. BAGEL (AR+Diff) (Deng et al., 2025) also employs AR+Diff generation, with SigLIP (Zhai et al., 2023) / VAE (Kingma & Welling, 2013) as the visual understanding / generation encoder, supporting image editing at 10241024 resolution. Together, these models cover the main families of UMM architectures: autoregressive (AR), masked autoregressive (MAR), and autoregressive+diffusion (AR+Diff). Evaluation details. We evaluate text-to-image generation on GenEval (Ghosh et al., 2023) and DPGBench (Hu et al., 2024), and image editing on ImgEdit (Ye et al., 2025) and GEdit-BenchEN (Liu et al., 2025). Our baselines include both generation-only models (e.g., SDXL (Podell et al.,"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Results on GenEval and DPGBench. Scores marked with (*) are our reproduced results using 12 random seeds. We use Harmon-1.5B as our base model and post-train it with RecA. The gray-colored rows denote private models, and their results are cited from (Yan et al., 2025) and (Geng et al., 2025). All arrows () denote that higher is better. GenEval Benchmark DPGBench Model Params Single Obj. Two Obj. Counting Colors Position Color Attri. Overall Score 0.99 0.98 0.99 0.98 0.99 0.99 0.96 0.94 0.74 0.93 0.71 0.85 0.95 0.87 0.72 0.39 0.86 0.34 0.74 0.72 0.47 2B 2.6B 4.8B 8B 12B 24B Generation Only Models SD3-Medium SDXL SANA-1.5 Emu3-Gen FLUX-dev Playground-v3 DALL-E 3 Unified Multimodal Models 0.85 GPT-4o-Image 0.67 Show-o* 0.69 Harmon* 0.58 Show-o2 0.59 Janus-Pro 0.80 BAGEL* RecA 0.71 Unified Multimodal Models Trained with GPT-4o Data 0.90 Ovis-U1 0.64 OmniGen2 0.63 BLIP3-o* 0.79 UniWorld-V1 RecA 0.76 - 1.3B 1.5B 7B 7B 14B 1.5B 0.99 0.98 0.99 1.00 0.99 0.99 1.00 0.92 0.84 0.87 0.87 0.89 0.93 0.98 3.6B 7B 8B 20B 1.5B 0.98 0.95 0.92 0.93 0.97 0.98 1.00 1.00 0.99 1.00 0.89 0.85 0.84 0.81 0.79 0.82 0. 0.89 0.82 0.86 0.92 0.90 0.86 0.93 0.92 0.88 0.91 0.89 0.94 0.33 0.15 0.59 0.17 0.21 0.50 0.43 0.74 0.30 0.45 0.52 0.79 0.51 0.76 0.79 0.55 0.86 0.49 0.91 0.60 0.23 0.65 0.21 0.48 0.54 0. 0.71 0.52 0.51 0.62 0.66 0.63 0.77 0.75 0.76 0.67 0.70 0.83 0.74 0.55 0.81 0.54 0.68 0.76 0.67 0.84 0.69 0.73 0.76 0.80 0.79 0.86 0.89 0.80 0.83 0.80 0.90 84.08 74.65 84.70 80.60 84.00 87.06 83. 86.23 83.63 80.93 86.14 84.33 84.03 87.21 83.72 83.57 80.73 88.15 Table 2: RecA consistently improves image generation performance across unified multimodal models (AR, Masked AR, and AR+Diffusion architectures). All models are evaluated on GenEval, DPGBench, and WISE. We report results for the largest model/resolution variant of each architecture; results for smaller models and detailed WISE scores are provided in appendixs Sec. D. GenEval DPG WISE Model RecA Show-o OpenUni Harmon BAGEL Single 97. Two Count Color Position Attri. 27.3 Overall 66.2 99. 52.3 78.2 80.3 71.8 61.9 98.2 (+1.0) 90.6 (+10.3) 66.8 (+4.9) 84.1 (+5.9) 37.4 (+10.1) 56.8 (+4.5) 72.3 (+6.1) 84.94 (+2.73) 0.40 (0.00) 99.1 (0.0) 92.7 (+20.9) 52.3 (+0.4) 87.1 (+3.2) 43.8 (+20.5) 70.3 (+28.7) 74.1 (+12.2) 82.75 (+3.73) 0.54 (+0.11) 99.9 (+0.5) 97.7 (+10.4) 71.4 (+2.7) 92.6 (+6.2) 75.7 (+30.8) 76.6 (+25.5) 85.7 (+12.8) 87.21 (+6.28) 0.50 (+0.09) 99.3 (+0.2) 93.9 (+0.9) 80.3 (+0.4) 87.6 (+1.6) 60.8 (+9.5) 72.6 (+9.2) 82.4 (+3.6) 85.29 (+1.26) 0.52 (+0.02) 82.21 79.02 80.93 84.03 0.40 61. 0.43 72.9 0.41 78.8 0.50 51. 68.7 79.9 99.4 87.3 99.1 93. 44.9 83.9 23.3 41.6 86.4 51. 51.3 86.0 63.4 2023), DALL-E 3 (Betker et al.)) and unified multimodal models (e.g., Show-o (Xie et al., 2025b), Harmon (Wu et al., 2025c), BAGEL (Deng et al., 2025), GPT-4o-Image (OpenAI, 2024)). Training data. We post-train UMMs with high-quality open-source data, including MidjourneyV6 (CortexLM, 2024), LLaVA Mix-665K (Liu et al., 2024b), and 8,000 FLUX-generated samples (jackyhate, 2024). To ensure fair comparison, our main experiments exclude GPT-4o-Image distillation data, which can inflate benchmark scores due to GenEval Template Leakage. Further implementation, evaluation and dataset details are provided in appendixs Sec. B."
        },
        {
            "title": "3.2 RE CA ACHIEVES SOTA PERFORMANCE ON IMAGE GENERATION AND EDITING",
            "content": "SOTA performance without GPT-4o-Image distillation. As shown in Table 1, post-training Harmon-1.5B model with RecA achieves SOTA results across both generation-only and UMM architectures. Despite using significantly fewer parameters, RecA surpasses recent methods, e.g., Janus-Pro (Chen et al., 2025c), OmniGen2 (Wu et al., 2025a), and BAGEL (Deng et al., 2025)."
        },
        {
            "title": "Preprint",
            "content": "Table 3: Results on image editing benchmarks. We compare RecA with previous methods on ImgEdit and GEdit-Bench-EN, excluding models distilled from GPT-4o-Image. The scores are obtained from our local evaluation using the released checkpoints, while the gray-colored rows denote private models, and their results are cited from (Deng et al., 2025; Ye et al., 2025). Method Bg. Style Adj. Ext. Rem. Rep. Add Comp. Act. Ovr. SC PQ Overall ImgEdit GEdit-Bench-EN - - - - - Gemini 2.0 GPT-4o-Image 4.57 4.93 4.33 2.90 3.66 BAGEL-NHR 3.56 4.43 3.62 1.57 3.17 FLUX-Kontext 3.89 4.62 3.69 1.81 2.97 BAGEL 3.38 4.53 3.58 1.49 3.15 BAGEL-RecA 3.85 4.73 3.86 1.68 3.75 vs. baseline +0.60 +0.47 +0.20 +0.28 +0. - - 4.35 4.61 3.98 4.12 4.20 3.80 3.82 3.71 4.28 4.20 +0.44 +0.49 - 3.96 2.88 3.00 2.64 2.94 +0.30 - - 5.43 6.78 4.89 4.20 7.85 7.62 3.85 3.48 8.04 6.87 4.18 3.60 6.95 7.30 4.21 3.38 7.96 6.64 4.56 3.75 8.24 6.87 +0.35 +0.37 +0.28 +0.23 5.36 7.53 7.08 6.27 6.94 7.27 +0.37 Image editing results are also substantially improved, as shown in Table 3. RecA consistently outperforms existing baselines across all categories. On ImgEdit, RecA achieves an overall score of 3.75, surpassing FLUX-Kontext (3.60) and the baseline BAGEL (3.38). Notably, RecA beats the concurrent work BAGEL-NHR (Kuprashevich et al., 2025) (3.48), which employs supervised fine-tuning (SFT) on 300,000 high-quality image editing data. On GEdit-Bench-EN, RecA reaches 7.27, significant improvement over FLUX-Kontexts 6.27. Remarkably, these results are obtained using only 8,000 unlabeled images and 27 GPU hours."
        },
        {
            "title": "3.3 RE CA IS EFFECTIVE ACROSS DIFFERENT UMM FRAMEWORKS",
            "content": "Consistent performance gains across UMM architectures. As shown in Table 2, RecA demonstrates consistent and significant improvements across all evaluated UMM frameworks. We report results for the largest model variant of each architecture, with smaller model results provided in appendixs Sec. D. On GenEval, the most substantial improvement is achieved by Harmon-1.5B, which attains score of 85.7 (+12.8). On DPGBench, RecA achieves strong performance improvements across all model variants, with Harmon-1.5B reaching 87.21 (+6.28). Performance on WISE benchmark. WISE (Niu et al., 2025) evaluates text-to-image reasoning through 1,000 knowledge-puzzle prompts requiring implicit factual knowledge (e.g., Einsteins favorite instrument). As shown in Table 2, RecA yields consistent gains on Harmon and OpenUni, while BAGEL and Show-o show smaller improvements. These results highlight that RecA is particularly effective at strengthening semantic alignment, while deeper reasoning may depend more on advances in the underlying language model. We view WISE as complementary challenge and an exciting direction for future research (Ye et al., 2024; Tong et al., 2024). Additional results are provided in Appendixs Sec. D."
        },
        {
            "title": "3.4 MORE RESULTS",
            "content": "Qualitative results. Figure 6 and Figure 7 demonstrate consistent improvements after RecA. The baseline model fails on cases of multiple objects, complex attributions, and explicit spatial layouts, while RecA post-trained model consistently succeeds. For dense prompts, the post-trained model preserves fine-grained details (e.g., kitchen window, sunlight) that the baseline model blurs or omits. Comprehensive qualitative results for both image generation and editing tasks are provided in appendixs Sec. H. For image editing, we observe consistent performance improvements across diverse tasks including object addition, replacement, style transfer, and color modification. Additional qualitative editing results are presented in appendixs Sec. G. Visual understanding results. To verify that RecA preserves the visual understanding capabilities of UMMs, we evaluate our post-trained Harmon model on benchmarks including POPE (Li et al., 2023b), MME (Fu et al., 2023), GQA (Hudson & Manning, 2019), MMMU (Yue et al., 2024), and SEED (Li et al., 2023a). As shown in Table 4, performance remains stable with minimal variations within normal fine-tuning variance. This confirms that RecA maintains understanding capabilities while improving generation fidelity. For Table 4: Visual understanding performance. Model MME POPE (Acc) POPE (F1) GQA MMMU SEED 65.2 Harmon 1195 RecA 65.3 1223 58.8 58.4 83.9 83.2 34.7 35. 83.8 83."
        },
        {
            "title": "Preprint",
            "content": "Figure 6: Image generation results vs. baselines. We use Harmon-1.5B as baseline. Post-trained model better handles multiple objects, complex attributions, and spatial layouts, preserving fine details missed by the baseline. Figure 7: Image editing results vs. baselines. We use BAGEL as baseline. Our model consistently improves object addition, replacement, style transfer, and scene modification. architectures with frozen understanding components (OpenUni, BAGEL), visual understanding performance is naturally preserved."
        },
        {
            "title": "3.5 EMPIRICAL STUDIES OF RE CA AS A POST-TRAINING METHOD",
            "content": "SFT vs. RecA as post-training methods. We directly compare supervised fine-tuning (SFT) with RecA as alternative post-training strategies for UMMs. Unlike SFT, which depends on captionimage pairs, RecA is self-supervised and requires only unlabeled images, learning by reconstructing from visual embeddings. Our key questions are: (i) which method is more effective for post-training, and (ii) when and how should RecA be applied relative to SFT? RecA is more effective for post-training. As shown in Table 5, on MidjourneyV6, SFT yields modest improvements but quickly plateaus at 74.76 on GenEval. Adding denser captions via MidjourneyLLaVA (brivangl, 2024) does not helpGenEval remains nearly unchanged and DPGBench even drops to 80.67. In contrast, replacing SFT with RecA dramatically boosts performance under the same setup, achieving 85.69 GenEval (+10.93) and 87.21 DPGBench (+6.54). This demonstrates"
        },
        {
            "title": "Preprint",
            "content": "SFT RecA GenEval DPG Dataset 80.89 MidjourneyV6 80.67 MidjourneyV6 (dense) 87.21 MidjourneyV6 86.54 MidjourneyV6 85.19 BLIP3o-60k 86.50 BLIP3o-60k 85.96 BLIP3o-60k BLIP3o-60k (w/o GenEval) 85.24 86.37 BLIP3o-60k (w/o GenEval) 74.76 74.05 85.69 81.22 84.95 85.21 86.14 80.88 84.76 Comparison of SFT Table 5: and RecA across post-training datasets. Post-training with RecA consistently outperforms SFT, especially when benchmark-specific data (e.g., GenEval) is excluded. On high-quality images, RecA can even surpass SFT under dense prompts. All results are reported at 5k steps. Figure 8: Qualitative T2I results. The large images (10241024) are generated by the post-trained BAGEL, while the small images (512 512) are generated by the post-trained Harmon. Detailed captions are listed in appendixs Sec. F. that self-supervised semantic reconstruction is far more effective post-training strategy than supervised fine-tuning. Further analysis of SFT on BLIP3o-60k is provided in Appendix C. Table 6: Training recipe. SFT vs. RecA as sequential post-training stages. Dataset When to apply RecA? We also study training order to understand when to apply RecA relative to SFT. As shown in Table 5, mixed training (half SFT, half RecA per batch) is unstable: it slightly helps on BLIP3o-60k but fails on MidjourneyV6. 85.67 If we do SFT on BLIP3o-60k, sequential training 87.50 88.15 is more decisive: as shown in Table 6, applying RecA after SFT (SFTRecA) consistently delivers the best results (90.15/88.15 on MidjourneyV6). Reversing the order (RecA SFT) degrades performance (85.91/85.67). This asymmetry underscores that SFT provides broad languageimage alignment, while RecA serves best as refinement stage enhancing semantic grounding and visual faithfulness. Order RecA SFT BLIP3o-60k SFT RecA BLIP3o-60k MidjourneyV6 SFT RecA 85.91 89.00 90.15 GenEval DPG"
        },
        {
            "title": "Preprint",
            "content": "Figure 9: Qualitative image editing results. Edited outputs are generated by BAGEL post-trained with RecA. Left: original images; Right: edited images. Table 7: Using semantic embeddings from visual understanding encoders consistently outperforms generation encoder latents across benchmarks. Vision Encoder Visual understanding vs. visual generation encoder. Most UMMs employ two encoders: one for visual understanding (semantic features) and one for visual generation (pixel-level features). BAGEL follows this design, making it natural testbed for RecA. As shown in Table 7, conditioning on the generation encoder (VAE) yields only marginal or degraded gains, while embeddings from the understanding encoder (ViT) produce consistently better results across GenEval, DPGBench, ImgEdit, and GEdit. This indicates that RecA benefits most from semantic embeddings that capture high-level conceptual information, rather than raw visual details. Baseline VAE (generation) ViT (understanding) GenEval DPG ImgEdit GEdit 84.03 83.92 85. 78.8 78.5 82.4 3.38 3.63 3.75 6.94 7.08 7.27 Conclusion. Across data and strategies, RecA consistently proves to be stronger post-training method than SFT. The best post-training recipe is two-stage pipeline: first SFT on high-quality paired data for coarse alignment, followed by RecA for self-supervised fine-grained refinement."
        },
        {
            "title": "4.1 UNIFIED MULTIMODAL MODELS (UMMS)",
            "content": "Unified multimodal model (UMM) is single, end-to-end-trained backbone capable of multimodal understanding and generation within single model or system (Zhang et al., 2025b). Recent studies have explored broad design space for different UMMs: (I) AR. Chameleon and Janus tokenize images and predict them autoregressively in next-token prediction way (Team, 2024; Qu et al., 2024; Chen et al., 2025d;c; Wu et al., 2024a), and Show-o (Xie et al., 2025b) introduces discretediffusion schedule to improve the token prediction process. (II) AR + Diffusion. Models such as Transfusion attach diffusion (or flow-matching) head to the shared transformer (Zhou et al., 2025; Deng et al., 2025; Ma et al., 2024; Shi et al., 2024; Zhang et al., 2025a; Xie et al., 2025c). Some UMMs keep powerful pretrained MLLM frozen for reasoning and route its intermediate features with learnable queries (Pan et al., 2025b; Chen et al., 2025a; Wu et al., 2025b; Lin et al., 2025) to an external image generator. (III) AR + MAR. Masked-autoregressive (MAR) (Li et al., 2024) is novel autoregressive image generation paradigm without any vector quantization, which has been adopted by models like Harmon (Wu et al., 2025c; Fan et al., 2025; Yang et al., 2024b)."
        },
        {
            "title": "4.2 POST-TRAINING STRATEGIES FOR UMMS",
            "content": "Recent efforts explore different post-training techniques for enhancing the generation capability of UMMs. (I) Chain-of-Thought (CoT) or test-time verification let the model do reasoning before the generation or verify generated image step-by-step, but they depend on external models and do not improve the UMMs native generation capability. (Guo et al., 2025; Wang et al., 2025d; Fang et al., 2025; Tian et al., 2025). (II) Reinforcement learning. Methods such as DPO and GRPO optimize generation policies with human or automatic preference signals, but require curated"
        },
        {
            "title": "Preprint",
            "content": "paired data and carefully tuned advantage functions. (Guo et al., 2025; Mao et al., 2025; Tian et al., 2025; Jiang et al., 2025). (III) High-quality synthetic data. BLIP3o distils 60,000 GPT-4oImage generated text-image pair for GenEval, dense-prompt, pose, and landmark scenarios, while ShareGPT-4o-Image contributes rich editing data (OpenAI, 2024; Chen et al., 2025a;b; Wang et al., 2025e). Supervised fine-tuning (SFT) on these sets lifts benchmark scores appreciably, yet the approach needs heavy data-generation cost and risks distribution shift."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this work, we propose RecAa lightweight post-training paradigm that replaces sparse text-toimage supervision with dense features from the models own visual understanding encoder. RecA requires no extra caption data yet improves image generation and editing across architectures: our RecA-1.5B reaches 0.86 on GenEval and 87.21 on DPGBench, and sequential two-staged strategy pushes them to 0.90 and 88.15, setting new state-of-the-art. We discuss the broader implications, limitations and future directions of our work in Appendix C."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "We sincerely thank Ishan Misra, Xingyi Zhou, Haoqi Fan, Mannat Singh, Dewei Zhou, Haotian Tang, Yulun Wu, Baifeng Shi, Jiaxin Ge, Yuwei Niu, Xun Wang, Jiuhai Chen for their insightful discussions and valuable feedback on our paper. We especially thank Size Wu and Yang Ye for their assistance in reproducing the baseline results."
        },
        {
            "title": "REFERENCES",
            "content": "Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. CoRR, abs/2308.12966, 2023. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. James Betker, Gabriel Goh, Li Jing, TimBrooks, Jianfeng Wang, Linjie Li, LongOuyang, JuntangZhuang, JoyceLee, YufeiGuo, WesamManassra, PrafullaDhariwal, CaseyChu, YunxinJiao, and Aditya Ramesh. Improving image generation with better captions. BlackForest. Black forest labs; frontier ai lab, 2024. URL https://blackforestlabs.ai/. brivangl. brivangl/midjourney-v6-llava, 2024. URL https://huggingface.co/ datasets/brivangl/midjourney-v6-llava. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In NeurIPS, 2020. Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. Proceedings of the IEEE/CVF international conference on computer vision, pp. 96509660, 2021. Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, et al. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025a."
        },
        {
            "title": "Preprint",
            "content": "Junying Chen, Zhenyang Cai, Pengcheng Chen, Shunian Chen, Ke Ji, Xidong Wang, Yunjin Yang, and Benyou Wang. Sharegpt-4o-image: Aligning multimodal models with gpt-4o-level image generation. arXiv preprint arXiv:2506.18095, 2025b. Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025c. Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025d. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. CortexLM. Cortexlm/midjourney-v6, 2024. URL https://huggingface.co/datasets/ CortexLM/midjourney-v6. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, pp. 1287312883, 2021. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. Lijie Fan, Luming Tang, Siyang Qin, Tianhong Li, Xuan Yang, Siyuan Qiao, Andreas Steiner, Chen Sun, Yuanzhen Li, Tao Zhu, et al. Unified autoregressive visual generation and understanding with continuous tokens. arXiv preprint arXiv:2503.13436, 2025. Rongyao Fang, Chengqi Duan, Kun Wang, Linjiang Huang, Hao Li, Shilin Yan, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, et al. Got: Unleashing reasoning capability of multimodal large language model for visual generation and editing. arXiv:2503.10639, 2025. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji. MME: comprehensive evaluation benchmark for multimodal large language models. CoRR, abs/2306.13394, 2023. Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A. Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive, 2024. URL https://arxiv.org/abs/2404.12390. Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. Zigang Geng, Yibing Wang, Yeyao Ma, Chen Li, Yongming Rao, Shuyang Gu, Zhao Zhong, Qinglin Lu, Han Hu, Xiaosong Zhang, Linus, Di Wang, and Jie Jiang. X-omni: Reinforcement learning makes discrete autoregressive image generative models great again, 2025. URL https:// arxiv.org/abs/2507.22058. Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. In NeurIPS, 2023. Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Peng Gao, Hongsheng Li, and PhengAnn Heng. Can we generate images with cot? lets verify and reinforce image generation step by step. arXiv:2501.13926, 2025."
        },
        {
            "title": "Preprint",
            "content": "Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment, 2024. Drew A. Hudson and Christopher D. Manning. GQA: new dataset for real-world visual reasoning and compositional question answering. In CVPR, pp. 67006709. Computer Vision Foundation / IEEE, 2019. jackyhate. jackyhate/text-to-image-2m, 2024. URL https://huggingface.co/datasets/ jackyhate/text-to-image-2M. Jingjing Jiang, Chongjie Si, Jun Luo, Hanwang Zhang, and Chao Ma. Co-reinforcement learning for unified multimodal understanding and generation. arXiv preprint arXiv:2505.17534, 2025. Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. Maksim Kuprashevich, Grigorii Alekseenko, Irina Tolstykh, Georgii Fedorov, Bulat Suleimanov, Vladimir Dokholyan, and Aleksandr Gordeev. Nohumansrequired: Autonomous high-quality image editing triplet mining, 2025. URL https://arxiv.org/abs/2507.14119. Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, et al. Flux. 1 kontext: arXiv preprint Flow matching for in-context image generation and editing in latent space. arXiv:2506.15742, 2025. Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023a. Han Li, Xinyu Peng, Yaoming Wang, Zelin Peng, Xin Chen, Rongxiang Weng, Jingang Wang, Xunliang Cai, Wenrui Dai, and Hongkai Xiong. Onecat: Decoder-only auto-regressive model for unified understanding and generation, 2025. URL https://arxiv.org/abs/2509. 03498. Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems, 37: 5642456445, 2024. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In EMNLP, pp. 292305. Association for Computational Linguistics, 2023b. Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. Bingchen Liu, Ehsan Akhgari, Alexander Visheratin, Aleks Kamko, Linmiao Xu, Shivam Shrirao, Chase Lambert, Joao Souza, Suhail Doshi, and Daiqing Li. Playground v3: Improving textto-image alignment with deep-fusion large language models, 2024a. URL https://arxiv. org/abs/2409.10695. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 36, 2024b. Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. Shijie Ma, Yuying Ge, Teng Wang, Yuxin Guo, Yixiao Ge, and Ying Shan. Genhancer: Imperfect generative models are secretly strong vision-centric enhancers. arXiv preprint arXiv:2503.19480, 2025a."
        },
        {
            "title": "Preprint",
            "content": "Shuailei Ma, Kecheng Zheng, Ying Wei, Wei Wu, Fan Lu, Yifei Zhang, Chen-Wei Xie, Biao Gong, Jiapeng Zhu, and Yujun Shen. Learning visual generative priors without text. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 80518061, 2025b. Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Xingkai yu, Liang Zhao, Yisong Wang, Jiaying Liu, and Chong Ruan. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation, 2024. Weijia Mao, Zhenheng Yang, and Mike Zheng Shou. Unirl: Self-improving unified multimodal models via supervised and reinforcement learning. arXiv preprint arXiv:2505.23380, 2025. Yuwei Niu, Munan Ning, Mengren Zheng, Bin Lin, Peng Jin, Jiaqi Liao, Kunpeng Ning, Bin Zhu, and Li Yuan. Wise: world knowledge-informed semantic evaluation for text-to-image generation. arXiv preprint arXiv:2503.07265, 2025. OpenAI. Introducing gpt-4o with image generation capabilities, 2024. URL https://openai. com/index/introducing-4o-image-generation. Accessed: 2025-07-04. Kaihang Pan, Wang Lin, Zhongqi Yue, Tenglong Ao, Liyu Jia, Wei Zhao, Juncheng Li, Siliang Tang, and Hanwang Zhang. Generative multimodal pretraining with discrete diffusion timestep tokens. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2613626146, 2025a. Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, Ji Hou, and Saining Xie. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025b. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Liao Qu, Huichao Zhang, Yiheng Liu, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Daniel Du, Zehuan Yuan, and Xinglong Wu. Tokenflow: Unified image tokenizer for multimodal understanding and generation. arXiv preprint arXiv:2412.03069, 2024. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, pp. 87488763, 2021. Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, and Lili Yu. Lmfusion: Adapting pretrained language models for multimodal generation. arXiv preprint arXiv: 2412.15188, 2024. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Rui Tian, Mingfei Gao, Mingze Xu, Jiaming Hu, Jiasen Lu, Zuxuan Wu, Yinfei Yang, and Afshin Dehghan. Unigen: Enhanced training & test-time strategies for unified multimodal understanding and generation. arXiv preprint arXiv:2505.14682, 2025. Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023. Guo-Hua Wang, Shanshan Zhao, Xinjie Zhang, Liangfu Cao, Pengxin Zhan, Lunhao Duan, Shiyin Lu, Minghao Fu, Xiaohao Chen, Jianshan Zhao, Yang Li, and Qing-Guo Chen. Ovis-u1 technical report, 2025a. URL https://arxiv.org/abs/2506.23044."
        },
        {
            "title": "Preprint",
            "content": "Haochen Wang, Anlin Zheng, Yucheng Zhao, Tiancai Wang, Zheng Ge, Xiangyu Zhang, and Zhaoxiang Zhang. Reconstructive visual instruction tuning. arXiv preprint arXiv:2410.09575, 2024a. Haochen Wang, Yucheng Zhao, Tiancai Wang, Haoqiang Fan, Xiangyu Zhang, and Zhaoxiang Zhang. Ross3d: Reconstructive visual instruction tuning with 3d-awareness. arXiv preprint arXiv:2504.01901, 2025b. Wenxuan Wang, Quan Sun, Fan Zhang, Yepeng Tang, Jing Liu, and Xinlong Wang. Diffusion feedback helps clip see better. arXiv preprint arXiv:2407.20171, 2024b. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024c. XuDong Wang, Xingyi Zhou, Alireza Fathi, Trevor Darrell, and Cordelia Schmid. Visual lexicon: Rich image features in language space. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1973619747, 2025c. Yi Wang, Mushui Liu, Wanggui He, Longxiang Zhang, Ziwei Huang, Guanghao Zhang, Fangxun Shu, Zhong Tao, Dong She, Zhelun Yu, et al. Mint: Multi-modal chain of thought in unified generative models for enhanced image generation. arXiv:2503.01298, 2025d. Yuhan Wang, Siwei Yang, Bingchen Zhao, Letian Zhang, Qing Liu, Yuyin Zhou, and Cihang Xie. Gpt-image-edit-1.5m: million-scale, gpt-generated image dataset, 2025e. URL https:// arxiv.org/abs/2507.21033. Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, Ze Liu, Ziyi Xia, Chaofan Li, Haoge Deng, Jiahao Wang, Kun Luo, Bo Zhang, Defu Lian, Xinlong Wang, Zhongyuan Wang, Tiejun Huang, and Zheng Liu. Omnigen2: Exploration to advanced multimodal generation, 2025a. URL https://arxiv. org/abs/2506.18871. Junfeng Wu, Yi Jiang, Chuofan Ma, Yuliang Liu, Hengshuang Zhao, Zehuan Yuan, Song Bai, and Xiang Bai. Liquid: Language models are scalable multi-modal generators. arXiv preprint arXiv:2412.04332, 2024a. Size Wu, Zhonghua Wu, Zerui Gong, Qingyi Tao, Sheng Jin, Qinyue Li, Wei Li, and Chen Change Loy. Openuni: simple baseline for unified multimodal understanding and generation. arXiv preprint arXiv:2505.23661, 2025b. Size Wu, Wenwei Zhang, Lumin Xu, Sheng Jin, Zhonghua Wu, Qingyi Tao, Wentao Liu, Wei Li, and Chen Change Loy. Harmonizing visual representations for unified multimodal understanding and generation, 2025c. URL https://arxiv.org/abs/2503.21979. Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024b. Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Chengyue Wu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, et al. Sana 1.5: Efficient scaling of training-time and inferencetime compute in linear diffusion transformer. arXiv preprint arXiv:2501.18427, 2025a. Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. In ICLR, 2025b. Jinheng Xie, Zhenheng Yang, and Mike Zheng Shou. Show-o2: Improved native unified multimodal models. arXiv preprint arXiv:2506.15564, 2025c. Zhiyuan Yan, Junyan Ye, Weijia Li, Zilong Huang, Shenghai Yuan, Xiangyang He, Kaiqing Lin, Jun He, Conghui He, and Li Yuan. Gpt-imgeval: comprehensive benchmark for diagnosing gpt4o in image generation. arXiv preprint arXiv:2504.02782, 2025."
        },
        {
            "title": "Preprint",
            "content": "An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024a. Jian Yang, Dacheng Yin, Yizhou Zhou, Fengyun Rao, Wei Zhai, Yang Cao, and Zheng-Jun Zha. MMAR: towards lossless multi-modal auto-regressive probabilistic modeling. arXiv preprint arXiv:2410.10798, 2024b. Tian Ye, Zicheng Xu, Yuanzhi Li, and Zeyuan Allen-Zhu. Physics of language models: Part 2.1, grade-school math and the hidden reasoning process. arXiv preprint arXiv:2407.20311, 2024. Yang Ye, Xianyi He, Zongjian Li, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Bohan Hou, and Li Yuan. Imgedit: unified image editing dataset and benchmark. arXiv preprint arXiv:2505.20275, 2025. Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024. Xiang Yue, Yuansheng Ni, Tianyu Zheng, Kai Zhang, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. MMMU: massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI. In CVPR, pp. 95569567. IEEE, 2024. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training, 2023. Hong Zhang, Zhongjie Duan, Xingjun Wang, Yingda Chen, Yuze Zhao, and Yu Zhang. NexusarXiv preprint gen: unified model for image understanding, generation, and editing. arXiv:2504.21356, 2025a. Xinjie Zhang, Jintao Guo, Shanshan Zhao, Minghao Fu, Lunhao Duan, Jiakui Hu, Yong Xien Chng, Guo-Hua Wang, Qing-Guo Chen, Zhao Xu, Weihua Luo, and Kaifu Zhang. Unified multimodal understanding and generation models: Advances, challenges, and opportunities, 2025b. URL https://arxiv.org/abs/2505.02567. Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. In-context edit: Enabling instructional image editing with in-context generation in large scale diffusion transformer. arXiv preprint arXiv:2504.20690, 2025c. Chunting Zhou, LILI YU, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. In ICLR, 2025. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025."
        },
        {
            "title": "A PROMPT TEMPLATES FOR RE CA",
            "content": "For template diversification in RecA post-training, we utilize GPT-o3 to generate 360 distinct prompt templates based on the seed template \"Describe the image in detail.\" Below are representative examples from our template collection: \"Provide detailed description of the image.\" \"What can you observe in this image? Please describe it comprehensively.\""
        },
        {
            "title": "Preprint",
            "content": "\"Analyze and describe the visual elements present in this image.\" \"Give thorough description of what you see in the image.\" \"Examine the image carefully and provide detailed account.\" \"Describe the contents of this image in detail.\" \"What does this image show? Please provide comprehensive description.\" \"Offer detailed visual analysis of the image.\" \"Describe all the visual elements you can identify in this image.\" \"Provide an in-depth description of what is depicted in the image.\" The complete collection of 360 templates varies in linguistic structure, complexity, and prompting style while maintaining the core objective of triggering detailed image descriptions."
        },
        {
            "title": "B EXPERIMENTAL SETUP",
            "content": "Implementation Details. We post-train all models except BAGEL on single 80 GB NVIDIA A100 GPU. Table 8 presents the detailed hyperparameter configurations for all evaluated models, including Show-o (256²/512²), Harmon (0.5B/1.5B), OpenUni (1.6B/3.6B), and BAGEL (14B). The table includes optimization settings, trainable modules, and loss weights used during RecA posttraining. Hyper-parameters not listed in Table 8 follow the original open-source baselines. All λt2i is set to 0. Evaluation Details. Following previous work, we evaluate text-to-image generation capabilities using GenEval (Ghosh et al., 2023) and DPGBench (Hu et al., 2024). Our baselines include: (I) Generation-only models: SD3-Medium (Esser et al., 2024), SDXL (Podell et al., 2023), SANA-1.5 (Xie et al., 2025a), Emu3-Gen (Wang et al., 2024c), FLUX-dev (BlackForest, 2024), Playground-v3 (Liu et al., 2024a), and DALL-E 3 (Betker et al.). (II) Unified multimodal models: Show-o (Xie et al., 2025b), Harmon (Wu et al., 2025c), Janus-pro (Chen et al., 2025c), OmniGen2 (Wu et al., 2025a), BLIP3-o (Chen et al., 2025a), BAGEL (Deng et al., 2025), Showo2 (Xie et al., 2025c), UniWorld-V1 (Lin et al., 2025), Ovis-U1 (Wang et al., 2025a), and GPT4o-Image (OpenAI, 2024). For enhanced statistical reliability, we re-evaluate some models using 12 random seeds (tripling the standard 4 seeds). For image editing evaluation, we employ ImgEdit (Ye et al., 2025) and GEdit-Bench-EN (Liu et al., 2025) benchmarks. We exclude GPT-4o-Image and its distillation-trained models due to poor identity preservation. We select BAGEL (Deng et al., 2025), and FLUX-Kontext (Labs et al., 2025) as our primary baselines. Notably, we compare with the concurrent work BAGEL-NHR (Kuprashevich et al., 2025), which employs supervised fine-tuning (SFT) approach using high-quality image editing datasets. Due to changes in the GPT API version and benchmark maintenance issues, the leaderboard scores exhibit significant variance and are difficult to reproduce. Therefore, we report our own local evaluations for consistency. We use GPT-4.1 for ImgEdit evaluation and GPT-4o for GEdit-Bench-EN evaluation. Training Data. Due to limited availability of UMM training data, we use high-quality open-source MidjourneyV6 dataset (CortexLM, 2024) for RecA. To preserve visual understanding capabilities of UMMs with shared parameters for both understanding and generation (Show-o and Harmon), we also incorporate LLaVA Mix-665K (Liu et al., 2024b). For BAGEL, which shows sensitivity to image distribution, we use 8,000 1024 1024 FLUX-generated samples from Text-to-Image2M (jackyhate, 2024). GPT-4o-Image distillation data (e.g., BLIP3o-60k) can boost GenEval and DPGBench scores by 57% on average (Chen et al., 2025a). However, these datasets contain GenEval prompt templates, creating evaluation biasa phenomenon we term GenEval Template Leakage. In our primary experiments, we deliberately avoid GPT-4o-Image distillation data to ensure fair comparison."
        },
        {
            "title": "C DISCUSSION",
            "content": "BLIP3o-60k reveals template leakage issues. To ensure fair comparison with state-of-the-art approaches, we conduct experiments with both the full BLIP3o-60k dataset and cleaned version"
        },
        {
            "title": "Preprint",
            "content": "Optimization Optimizer Learning rate β weight decay warmup steps Training steps Grad. accumulation Per-GPU batch size Trainable modules Frozen parts Loss weights λi2t λRecA Table 8: Fine-tuning hyperparameter setup. Show-o Harmon OpenUni BAGEL 2562 5122 0.5B 1.5B 1.6B 3.6B 14B 5e-7 5e-7 1eAdamW AdamW AdamW AdamW AdamW AdamW AdamW 1e-5 (0.9, 0.999) (0.9, 0.999) (0.9, 0.95) (0.9, 0.95) (0.9, 0.95) (0.9, 0.95) (0.9, 0.95) 0.01 500 5K 2 48 0.01 1000 1K 8 (global) 0.01 1000 5K 5 2 0.01 1000 5K 5 4 0.01 500 3K 2 96 0.01 50 5K 16 0.01 50 5K 16 20 4e-5 1e-5 1e-5 CLIP 1.0 1. MLLM Und. Expert 1.0 1.0 1.0 1.0 0.0 1. 0.0 1.0 0.0 1.0 where 7,000 text-image pairs corresponding to GenEval prompt templates are removed. When models are trained with the full BLIP3o-60k set, traditional SFT attains an inflated 84.95 GenEval score. However, removing the 7k template-matching pairs immediately reduces this score to 80.88 (-4.07). Notably, the DPGBench performance remains unchanged (+0.05), indicating that the removed data primarily contributes to GenEval-specific improvements rather than general generation capabilities. This observation confirms that BLIP3o-60k represents benchmark-targeted instruction tuning approach rather than universal, robust one. RecA achieves comparable performance and superior robustness to high-quality distillation data. When using the full BLIP3o-60k dataset (including template leakage), we achieve 85.21 GenEval scoredemonstrating that RecA can match the performance of expensive GPT-4o-Image distillation data using only unlabeled images and self-supervision. More importantly, RecA exhibits superior performance on dense prompt tasks: outperforms SFT on DPGBench (86.50 vs 85.19), despite BLIP3o-60k containing much dense-prompt data that benefit traditional SFT training. RecA demonstrates superior robustness against benchmark-specific overfitting. When GenEval templates are removed, RecAs performance degrades by only 0.45, and its DPGBench score remains virtually unchanged, while traditional SFT suffers substantial 4.07 drop. This minimal degradation highlights RecAs resistance to benchmark-specific overfitting and universality of image-reconstruction objectives. MORE QUANTITATIVE RESULTS ON TEXT-TO-IMAGE GENERATION We provide comprehensive quantitative analysis of RecA across all evaluated unified multimodal models in this section. Table 9 shows RecAs consistent performance improvements across all evaluated architectures, with particularly notable gains in positional understanding and color attribution tasks. The detailed WISE benchmark results in Table 10 reveal that RecA primarily enhances semantic alignment capabilities while showing modest improvements in reasoning-intensive tasks. Why the improvements on Show-o is modest? (I) The CLIP vision encoder exhibits insufficient capacity for extracting high-level semantic information necessary for enhanced generation. (II) Show-os constrained codebook size of 4096 tokens fundamentally limits its representational capacity and generation flexibility. (III) Show-o demonstrates minimal inherent image reconstruction capabilities, producing reconstructions with significant noise artifacts. Consequently, RecA must simultaneously learn to mitigate noise generation while conditioning on CLIP features, presenting additional optimization challenges. Why the text-to-image generation improvements on BAGEL is modest? (I) The computational constraints associated with training the 14B parameter model prevent full exploitation of RecAs"
        },
        {
            "title": "Preprint",
            "content": "Table 9: RecA brings consistent performance gains to various UMM frameworks. We show the performance gains after applying RecA to different unified multimodal models. All models are evaluated on GenEval, DPGBench, and WISE benchmarks. Position Attri. 14.2 Model Ours GenEval Single Two Count Color DPG WISE Overall Show-o-256 Show-o-512 OpenUni-1.6B OpenUni-3.6B Harmon-0.5B Harmon-1.5B BAGEL 0.33 57.4 84.3 80.3 61. 52.1 63.3 44.3 83.5 66.2 27. 78.2 56.6 82.3 52.3 30.3 82. 70.65 97.4 97.4 (0.0) 73.6 (+10.3) 56.0 (+3.9) 83.8 (+1.5) 20.3 (+6.1) 40.2 (+9.9) 61.9 (+5.3) 75.70 (+5.05) 0.34 (+0.01) 97.2 98.2 (+1.0) 90.6 (+10.3) 66.8 (+4.9) 84.1 (+5.9) 37.4 (+10.1) 56.8 (+4.5) 72.3 (+6.1) 84.94 (+2.73) 0.40 (0.00) 97.1 97.1 (0.0) 84.3 (0.0) 57.4 (0.0) 83.5 (0.0) 44.3 (0.0) 56.0 (0.0) 70.4 (0.0) 80.45 (+4.16) 0.45 (+0.02) 99.1 99.1 (0.0) 92.7 (+20.9) 52.3 (+0.4) 87.1 (+3.2) 43.8 (+20.5) 70.3 (+28.7) 74.1 (+12.2) 82.75 (+3.73) 0.54 (+0.11) 99.7 99.9 (+0.2) 92.3 (+11.8) 59.4 (+1.6) 91.7 (+5.0) 58.5 (+26.3) 70.7 (+21.0) 78.7 (+11.1) 84.67 (+4.55) 0.40 (+0.07) 99.4 99.9 (+0.5) 97.7 (+10.4) 71.4 (+2.7) 92.6 (+6.2) 75.7 (+30.8) 76.6 (+25.5) 85.7 (+12.8) 87.21 (+6.28) 0.50 (+0.09) 99.1 99.3 (+0.2) 93.9 (+0.9) 80.3 (+0.4) 87.6 (+1.6) 60.8 (+9.5) 72.6 (+9.2) 82.4 (+3.6) 85.29 (+1.26) 0.52 (+0.02) 84.03 80.12 80.93 76. 79.02 63.4 86.0 51.3 78.8 93. 79.9 0.50 49.7 51.1 56.0 41. 86.7 32.2 67.6 86.4 44.9 72. 70.4 83.9 23.3 61.9 68.7 57. 87.3 71.8 51.9 80.5 0.33 0. 0.40 0.43 0.43 Table 10: WISE benchmark results. Performance on reasoning-based text-to-image generation. Harmon and OpenUni exhibit zero-shot improvement but Show-o and BAGEL do not. Model Ours Cultural Time Space Biology Physics Chemistry Overall Show-oShow-o-512 OpenUni-1.6B OpenUni-3.6B Harmon-0.5B Harmon-1.5B BAGEL 0.27 0.38 0.28 (+0.01) 0.40 (+0.02) 0.46 (+0.01) 0.27 (+0.01) 0.38 (+0.01) 0.31 0.56 (+0.01) 0.32 (+0.01) 0.50 (+0.02) 0.31 0.31 (0.00) 0.42 0.42 (0.00) 0.26 0.45 0.37 0.55 0.48 0.40 0.40 (0.00) 0.41 0.54 0.35 0.52 0.46 (+0.05) 0.56 (+0.02) 0.42 (+0.07) 0.54 (+0.02) 0.24 0.24 (0.00) 0.33 0.34 (+0.01) 0.32 0.30 (-0.02) 0.36 0.34 (-0.02) 0.40 0.40 (0.00) 0.43 0.45 (+0.02) 0.28 0.36 0.39 0.50 0.38 0. 0.43 0.52 (+0.16) 0.51 (+0.13) 0.69 (+0.10) 0.50 (+0.07) 0.64 (+0.14) 0.40 (+0.06) 0.54 (+0.11) 0.33 (+0.05) 0.43 (+0.04) 0.54 (+0.13) 0.38 (+0.07) 0.48 (+0.10) 0.24 (+0.02) 0.40 (+0.07) 0.44 (+0.06) 0.58 (+0.10) 0.57 (+0.05) 0.48 (+0.11) 0.58 (+0.14) 0.32 (+0.03) 0.50 (+0.09) 0.43 (+0.01) 0.51 (-0.02) 0.67 (+0.03) 0.46 (+0.04) 0.59 (+0.02) 0.46 (+0.03) 0.52 (+0.02) 0.31 0.37 0.42 0. 0.34 0.43 0.38 0.22 0.33 0. 0.48 0.44 0.29 0.41 0.64 0. 0.43 0.50 0.38 0.42 0.53 potential benefits. (II) We hypothesize that BAGELs pre-existing image editing training, which conditions on both SigLIP and VAE features, has already endowed the model with robust capabilities for SigLIP feature-based reconstruction. This pre-established competency constrains the potential improvement space for RecA in generation tasks, while correspondingly enabling substantial enhancements in image editing capabilities."
        },
        {
            "title": "E LIMITATIONS AND FUTURE WORK",
            "content": "Despite the effectiveness of RecA across various unified multimodal models (UMMs), our approach has several limitations that warrant discussion: The improvements on the counting task are minor. As shown in Table 2, the models gains on counting are not substantial. We attribute this to object number being mid-level visual feature that current MLLMs are not adept at extracting. BLINK (Fu et al., 2024) points out that the information required by high-level semantic tasks (e.g., distinguishing left from right, identifying color) can often be captured and leveraged in linguistic form, whereas counting resists language-only mediation in MLLMs. Consequently, RecA primarily enhances the models generative ability on tasks directly tied to high-level semantics, such as color attributes and positional relations. Improving counting"
        },
        {
            "title": "Preprint",
            "content": "may require the mixture training objective of high-quality counting datasets or the incorporation of reinforcement learning techniques. Architecture-specific constraints. RecA exhibits limited effectiveness across certain architecture: (I) Show-o with image tokenization. For models employing discrete image tokenizers as vision encoders for understanding tasks, we observe tendency toward learning trivial one-to-one mappings. During training, Show-o demonstrates abrupt convergence behavior with cross-entropy loss dropping to near-zero values at specific training timesteps, suggesting potential overfitting phenomena. Future research could address this through regularization strategies such as input perturbation (e.g., image blurring, noise injection). (II) BLIP-3o. As illustrated in Figure 10, BLIP-3o possesses inherent image reconstruction capabilities. Application of RecA fails to improve reconstruction capability and text-to-image generation capability. We hypothesize that BLIP-3os pre-training already incorporates reconstruction objectives, rendering additional reconstruction training counterproductive. Figure 10: BLIP-3o reconstruction results. We observe that BLIP-3o possesses inherent image reconstruction capabilities. Following RecA application, reconstruction quality exhibits degradation rather than improvement, suggesting that BLIP-3o incorporates reconstruction objectives during pretraining."
        },
        {
            "title": "F GENERATED CAPTIONS",
            "content": "F.1 CAPTIONS FOR FIGURE 2 10-word Caption: Kitten and puppy cuddle warmly under cozy yellow blanket. 100-word Caption: heartwarming scene of pure companionship unfolds on soft white bed where tiny tabby kitten and tricolor Corgi puppy nestle side by side. Wrapped gently under fluffy mustard-yellow blanket, they gaze curiously at the viewer with wide, innocent eyes. The kittens delicate stripes and the puppys soft fur blend harmoniously under the golden tones of the blanket, creating tender atmosphere filled with comfort and warmth. Their small heads rest close, touching lightly, as if theyve found solace in each others presence. The peaceful bedroom backdrop adds serene touch to this moment of affection. 500-word Caption: In warmly lit bedroom, the tranquil quiet of the morning is broken only by the soft breaths of two unlikely yet inseparable companions. Upon pristine white bed, layered with"
        },
        {
            "title": "Preprint",
            "content": "thick cotton sheets and plush pillows, lies tender tableau of comfort and innocence: tabby kitten and tricolor Corgi puppy, curled gently together beneath soft, fleece mustard-yellow blanket. Their eyes are wide and bright, reflecting mixture of curiosity and calm, as if theyve just woken up from shared dream. The kitten is small, delicate, with soft grayish-brown tabby pattern running across its fur like brushstrokes. Its tiny paws are tucked beneath its chest, body pressed closely to the puppys for warmth and security. Its ears point upward slightly, twitching at the soft sounds of the morning breeze filtering through the room. Its eyes, bright glimmering hazel, look out with an expression that is both alert and serene. The puppy, young Pembroke Welsh Corgi, has face full of characterexpressive brown eyes, upright ears edged with tan, and snout speckled with white. Its fur is beautiful tricolor blend of rich black, creamy tan, and snowy white, each hue carefully painted by nature to highlight its playful features. Though still small, its body is slightly larger than the kittens, and its front paw is placed protectively near its feline friend. There is no tension between themonly the shared comfort of closeness, the unspoken language of trust. The blanket they share adds golden warmth to the scene, its texture soft and inviting. It wraps around them like cocoon, hinting at chill in the air beyond the bed, but ensuring they remain snug in their haven. The folds of the blanket frame them like the petals of flower, drawing the eye inward to their peaceful gaze and gentle postures. In the background, the bedroom is softly lit by morning light diffused through window. The white and cream tones of the bedding and walls contribute to the serenity of the scene, allowing the vivid colors of the animals and their blanket to take center stage. Pillows are propped up neatly behind them, hinting that someone may have just stepped away, leaving behind these two nestled in perfect moment of quiet affection. This image tells silent storynot just of two animals coexisting, but of genuine bond. It evokes themes of comfort, friendship, and the uncomplicated joy of presence. It reminds us of the purity in companionship, of how safety and peace can be found not just in familiar places, but in the closeness of those we trust. The kitten and the puppy, young and full of life, are captured here in fleeting, gentle stillness that speaks to the heart. F.2 CAPTIONS FOR FIGURE 8 Figure 11: Additional ImgEdit benchmark results. Qualitative results on image editing tasks. In each pair, the left image is generated by the baseline model and the right by the RecA post-trained model 512 512 images (left to right, top to bottom):"
        },
        {
            "title": "Preprint",
            "content": "Two spiraling strands of rich, crimson-colored pasta rest elegantly on the surface of polished dark wooden table, the grain of the wood accentuating their vibrant hue. This rustic Italian kitchen is bathed in the warm, golden light of the late afternoon sun, which highlights the intricate texture of the pasta. The table, set amidst traditional décor and terracotta pots filled with fresh herbs, offers tranquil setting for this simple yet captivating culinary display. golden squirrel and bright magenta elephant in bright sunlight. During the warm glow of dwindling summer evening, particular fussy feline with distinctive calico markings is perched atop garden table. The cat, seemingly indifferent to its surroundings, sports pair of large, reflective aviator sunglasses that sit comically upon its small, furry face. Around the cat, there are scattered pots of blooming flowers, contributing to the charm of the scene, and in the background, hints of orange and pink skies are visible through the foliage. highly intricate and vibrant cityscape that reflects fusion of Moebiuss imaginative design and Makoto Shinkais detailed animation style. The streets are aglow with neon signs in kaleidoscope of colors, casting reflections on the glossy, rain-slicked pavements. Towering skyscrapers with glowing windows rise towards starless night sky, as the artwork garners significant attention and praise on ArtStation. deep red rose with plush petals sits elegantly coiled atop an ivory, intricately patterned lace napkin. The napkin rests on rustic wooden table that contributes to the charming garden setting. As the late evening sun casts warm golden hue over the area, the shadows of surrounding foliage dance gently around the rose, enhancing the romantic ambiance. Nearby, the green leaves of the garden plants provide fresh and verdant backdrop to the scene. close-up image capturing the intricate details of maple leaf, which is composed entirely of clear, sparkling water droplets. The leaf is set against smooth, dark background that accentuates its delicate water structure. The droplets glisten as they cling to the invisible veins of the leaf, creating natural yet surreal piece of art. detailed photograph captures the image of statue with the likeness of an ancient pharaoh, unexpectedly accessorized with pair of bronze steampunk goggles resting atop its head. The statue is dressed in an anachronistic fashion, featuring crisp white t-shirt and fitted black leather jacket that contrasts with its traditional headdress. The background is simple, solid color that accentuates the statues unconventional attire and the intricate details of the steampunk eyewear. On the soft, warm sand of the beach, fluffy white rabbit with rounded ears is caught in curious moment, gently placing its paw on the ribbed surface of pink scallop shell. The scallop, slightly open, reveals its smooth interior contrasting with its coarse outer texture, while hues of pink and orange from the setting sun reflect off its surface. Theres tranquil ocean backdrop with the gentle ebbing of the tide, and the fading daylight casts golden glow over the scene, highlighting the rabbits soft fur and the shells subtle color. 1024 1024 images (left to right, top to bottom): transparent glass cube on an endless desert, with burning candle inside casting shadows on the sand. drop of water containing miniature forest with colorful tiny flower inside. vibrant and traditional depiction of Tteokguk, Korean rice cake soup, served during the Chuseok festival. The image features steaming bowl of clear broth filled with soft, chewy rice cakes, slices of zucchini, carrots, and green onions, garnished with sprinkle of sesame seeds. Surrounding the bowl are traditional Korean elements, such as woven basket with red dates, small wooden spoon, and wooden table with warm, earthy tone. The atmosphere is cozy and festive, with soft, natural lighting and slightly blurred background to emphasize the dish. The scene captures the essence of Korean culture and the warmth of the Chuseok celebration, with focus on authenticity and detail. Photorealistic closeup image of two pirate ships battling each other as they sail inside cup of coffee. cute handmade felt doll of little girl standing on grassy patch. She wears an orange knitted hooded dress with blue buttons and matching boots. The girl is holding smiling sun-shaped balloon made of felt. Fluffy white clouds with button details float in the sky, and soft green felt trees surround the scene. The style is whimsical, cozy, and playful, with pastel colors and dreamy, handcrafted aesthetic. The word RECA is written on street surface, with the word STARTS written just below it, surrounded by colorful chalk drawings and playful doodles. Van Gogh style painting of cyberpunk city at sunrise."
        },
        {
            "title": "Preprint",
            "content": "Figure 12: Additional GEdit-Bench-EN results. Qualitative results on image editing tasks. In each pair, the left image is generated by the baseline BAGEL model and the right by the RecA post-trained model. An anime-style portrait of girl with big sparkling eyes, detailed hair highlights, soft gradient background, vibrant colors. dreamy composition of young woman with butterflies emerging from her skin, wings glowing in soft golden hues, surreal and enchanting. She is wearing wedding dress and has white angel wings, waving her hand. surreal split-face portrait: left side realistic woman with soft skin and vivid blue eye, right side robotic cyborg with exposed steel plates, fluorescent blue circuits, tiny gears, and blood-red mechanical eye, cinematic lighting, futuristic and striking."
        },
        {
            "title": "G MORE QUALITATIVE RESULTS ON IMAGE EDITING",
            "content": "We provide additional qualitative comparisons in this section. Figure 13 presents comparison between the RecA post-trained BAGEL and previous SOTA models, ICEdit (Zhang et al., 2025c), FLUX-Kontext (Labs et al., 2025), and GPT-4o-Image. Compared with ICEdit and FLUX-Kontext, our method demonstrates stronger instruction-following capability. In contrast to GPT-4o-Image, our method exhibits superior identity preservation and background fidelity. Figure 12 and 11 showcase consistent improvements in semantic consistency, instruction following, and visual quality preservation across various editing tasks including background modification, style transfer, and object manipulation."
        },
        {
            "title": "Preprint",
            "content": "Figure 13: Qualitative comparison of editing results across different methods. Our method achieves more faithful instruction following (e.g., rainbow dome, star addition), better identity preservation (e.g., pet and human faces), and stronger background consistency."
        },
        {
            "title": "Preprint",
            "content": "H MORE QUALITATIVE RESULTS ON TEXT-TO-IMAGE GENERATION Figure 14: Uncurated generation results from Harmon-1.5B (top) and post-trained model (bottom). Prompt: white banana and black banana. Figure 15: Uncurated generation results from Harmon-1.5B (top) and post-trained model (bottom). Prompt: photo of yellow broccoli. Figure 16: Uncurated generation results from Harmon-1.5B (top) and post-trained model (bottom). Prompt: photo of an orange snowboard and green cat."
        },
        {
            "title": "Preprint",
            "content": "Figure 17: Uncurated generation results from Harmon-1.5B (top) and post-trained model (bottom). Prompt: photo of skis right of zebra. Figure 18: Uncurated generation results from Harmon-1.5B (top) and post-trained model (bottom). Prompt: photo of microwave and truck. Figure 19: Uncurated generation results from Harmon-1.5B (top) and post-trained model (bottom). Prompt: diamond on the right, an emerald in the middle, ruby on the left."
        },
        {
            "title": "Preprint",
            "content": "Figure 20: Uncurated generation results from Harmon-1.5B (top) and post-trained model (bottom). Prompt: Cheerful and bright, vibrant lighting. shell and bright orange bear in bright setting. Figure 21: Uncurated generation results from Harmon-1.5B (top) and post-trained model (bottom). Prompt: During the warm glow of dwindling summer evening, particular fussy feline with distinctive calico markings is perched atop garden table. The cat, seemingly indifferent to its surroundings, sports pair of large, reflective aviator sunglasses that sit comically upon its small, furry face. Around the cat, there are scattered pots of blooming flowers, contributing to the charm of the scene, and in the background, hints of orange and pink skies are visible through the foliage. Figure 22: Uncurated generation results from Harmon-1.5B (top) and post-trained model (bottom). Prompt: vivid scene unfolds where several deep red, perfectly round tomatoes spill from woven brown basket onto rustic wooden tabletop. The basket lies on its side as the plump tomatoes scatter across the surface, some touching the dark green leaves of nearby herb plant. In the background, the blurred outline of an open kitchen window lets in soft, natural light, casting gentle shadows around the fallen produce."
        },
        {
            "title": "Preprint",
            "content": "Figure 23: Uncurated generation results from Harmon-1.5B (top) and post-trained model (bottom). Prompt: polished brown leather briefcase with visible stitching details rests on white tablecloth, displaying sense of organization amidst the surrounding environment. Beside the briefcase, vibrant red fedora hat provides striking contrast against the pristine table covering. The table, placed in room with light beige walls, gives an impression of professional setting with touch of personal style. Figure 24: Uncurated generation results from Harmon-1.5B (top) and post-trained model (bottom). Prompt: festive array of red and yellow balloons tied with curling ribbons, gently bobbing from the breeze of spinning ceiling fan. The fan has wooden blades and brass finish, which contrasts with the bright colors of the balloons. The balloons are clustered in joyful bunch, casting soft shadows on the ceiling above."
        }
    ],
    "affiliations": [
        "UC Berkeley",
        "University of Washington"
    ]
}