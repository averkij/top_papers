{
    "paper_title": "Understanding Embedding Scaling in Collaborative Filtering",
    "authors": [
        "Zhuangzhuang He",
        "Zhou Kaiyu",
        "Haoyue Bai",
        "Fengbin Zhu",
        "Yonghui Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Scaling recommendation models into large recommendation models has become one of the most widely discussed topics. Recent efforts focus on components beyond the scaling embedding dimension, as it is believed that scaling embedding may lead to performance degradation. Although there have been some initial observations on embedding, the root cause of their non-scalability remains unclear. Moreover, whether performance degradation occurs across different types of models and datasets is still an unexplored area. Regarding the effect of embedding dimensions on performance, we conduct large-scale experiments across 10 datasets with varying sparsity levels and scales, using 4 representative classical architectures. We surprisingly observe two novel phenomenon: double-peak and logarithmic. For the former, as the embedding dimension increases, performance first improves, then declines, rises again, and eventually drops. For the latter, it exhibits a perfect logarithmic curve. Our contributions are threefold. First, we discover two novel phenomena when scaling collaborative filtering models. Second, we gain an understanding of the underlying causes of the double-peak phenomenon. Lastly, we theoretically analyze the noise robustness of collaborative filtering models, with results matching empirical observations."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 9 0 7 5 1 . 9 0 5 2 : r a"
        },
        {
            "title": "Understanding Embedding Scaling in Collaborative\nFiltering",
            "content": "Zhuangzhuang He1, Kaiyu Zhou2, Haoyue Bai3, Fengbin Zhu4, Yonghui Yang4 1 UIUC, 2 NTU, 3 ASU, 4 NUS"
        },
        {
            "title": "Abstract",
            "content": "Scaling recommendation models into large recommendation models has become one of the most widely discussed topics. Recent efforts focus on components beyond the scaling embedding dimension, as it is believed that scaling embedding may lead to performance degradation. Although there have been some initial observations on embedding, the root cause of their non-scalability remains unclear. Moreover, whether performance degradation occurs across different types of models and datasets is still an unexplored area. Regarding the effect of embedding dimensions on performance, we conduct large-scale experiments across 10 datasets with varying sparsity levels and scales, using 4 representative classical architectures. We surprisingly observe two novel phenomenon: double-peak and logarithmic. For the former, as the embedding dimension increases, performance first improves, then declines, rises again, and eventually drops. For the latter, it exhibits perfect logarithmic curve. Our contributions are threefold. First, we discover two novel phenomena when scaling collaborative filtering models. Second, we gain an understanding of the underlying causes of the double-peak phenomenon. Lastly, we theoretically analyze the noise robustness of collaborative filtering models, with results matching empirical observations."
        },
        {
            "title": "Introduction",
            "content": "Scaling up Transformer [Vaswani, 2017] parameters to boost performance in NLP tasks is key reason behind the success of large language models. Therefore, natural question arises: can we replicate similar success in collaborative filtering? It is widely agreed that embedding dimension is one of the key controllable parameters in collaborative filtering. However, there is also consensus that scaling the embedding dimension may leads to performance degradation [Guo et al., 2024, Ardalani et al., 2022, Zhang et al., 2024, Ye et al., 2025, Xu et al., 2025, Peng et al., 2025]. For example, Guo et al. [2024] argues: Worsely, increasing the embedding dimension does not sufficiently improve the performance or even hurts the model, and Zhang et al. [2024] believes: . . . Merely expanding the sparse component of model does not enhance its ability to capture the complex interactions . . . . As result, recent works [Zhang et al., 2024, Guo et al., 2024] on scaling recommendation models bypass the seemingly unviable path of expanding embedding dimension. further question arises: why do scaling embedding dimensions fail to improve recommendation performance? Guo et al. [2024] observe that when the embedding dimensions are scaled, embedding will collapse. Specifically, they conduct spectral analysis based on singular value decomposition of the learned embedding matrices and find that most of the singular values are extremely small. Furthermore, several studies on efficient recommendation Liu et al. [2020], Qu et al. [2023], Wang et al. [2024], Luo et al. [2024], Liu et al. [2021] treat the embedding dimensionality as learnable parameter and optimize it through predefined objectives, thereby enhancing the models overall effectiveness. Because, they think that there exists an optimal dimensionality for embeddings. In other words, performance exhibits single-peak phenomenon as the embedding is scaled. Overall, Preprint. previous works observed that increasing the dimensionality not only worsens performance but also causes changes in the singular values of the embedding. In this paper, we are more interested in understanding why the performance deteriorates with scaling and what the underlying causes are. Would it follow the previously observed single-peak phenomenon? To enable comprehensive investigation, we conducted large-scale experiments across 10 datasets with varying sizes and sparsity levels  (Table 3)  , using 4 representative collaborative filtering models (reason at D.4). Surprisingly, we observe two novel phenomena: double-peak and logarithmic (Figure 1), apart from the common single-peak. Additionally, we have two observations, for the same model, some datasets exhibit double-peak trend, while others show logarithmic increase. More interestingly, we also find that even within the same dataset, the phenomena are not always consistent. For example, BPR [Rendle et al., 2009] exhibits double-peak pattern, while SGL [Wu et al., 2021] follows logarithmic trend. Figure 1: Double-peak: first rising, then falling, followed by another rise and decline; logarithmic, performance follows logarithmic increase. Clearly, we prefer the logarithmic scaling for its expected stable performance, while the emergence of the double-peak phenomenon indicates limitation in scalability. Since both phenomena appear across different datasets within the same model, we boldly speculate that this is due to certain negative factors in the interactions. Inspired by He et al. [2022], we concluded that the issue originates from the noise present in the data, and we conducted thorough analysis of the causes of noise at each stage. We also conducted an in-depth analysis of the noise resistance and robustness of various collaborative filtering models and the results are generally consistent with the empirical evidence. Our contributions are summarized as follows. To the best of our knowledge, our work is the first report of the double-peak and logarithmic phenomenon. In the realm of collaborative filtering, there has long been perplexing question: why does expanding embedding dimensions not always result in performance enhancements? Our research make further additions to it. Through in-depth analysis, we identified interaction noise, in addition to overfitting, as the key factors causing the double-peak phenomenon. To test this idea, we propose simple yet effective sample drop strategy that helps collaborative filtering models scale more effectively. For instance, the embedding dimensions can be expanded up to 32,768 without significant risk of collapse. We theoretically analyzed the noise robustness of BPR, NeuMF, LightGCN, and SGL, with results largely aligning with empirical observations, indirectly validating our analysis. For instance, BPR shows poor noise robustness and more frequently exhibits the double-peak phenomenon, while SGL is more robust and tends to follow logarithmic pattern. The implications for future research. Transformer-based LLMs have garnered significant attention. Most of researches [Zhang et al., 2024, Xu et al., 2024] aim to bring the success of the Transformer architecture into recommendation models to achieve superior scaling performance. However, in this paper, we explore different path: finding the Transformer within collaborative filtering models. We discovered that SGL possesses structural advantages in this regard. Therefore, future research directions may focus on better filtering clean interactions and further scaling up SGL for enhanced performance."
        },
        {
            "title": "2 Background: Collaborative Filtering",
            "content": "Notations. We use lowercase letters for scalars. Denote the set of real numbers by R. We use bolded lowercase letters to represent the entire embedding, and subscripts to refer to specific embedding pu. We use calligraphic letters e.g., to denote the value spaces (other notions in Appendix A). User and Item Embedding. In collaborative filtering, the core components involve representing users and items using embedding vectors. The user embedding contains the users preference infor2 mation, which can better capture the similarity between different users. Similarly, the item embedding contains the items feature information. Let us define: pu Rk as the embedding vector for user u, where = 1, 2, . . . , m. qi Rk as the embedding vector for item i, where = 1, 2, . . . , n. is the dimensionality of the embedding. Collaborative Filtering. To compute the preference score ˆrij of user for item i, many powerful and representative collaborative filtering models have been proposed, such as BPR [Rendle et al., 2009], NeuMF [He et al., 2017], LightGCN [He et al., 2020] and SGL [Wu et al., 2021]. ❶ BPR [Rendle et al., 2009] calculates the preference score as the inner product of user and item embeddings, or employs neural network to capture nonlinear interactions: ˆrij = u qi = (cid:88) l=1 uil vjl, ˆrij = fMLP(pu, qi), where uil and vjl are the l-th components of pu and qi, respectively. ❷ NeuMF [He et al., 2017] is representative deep learning-based collaborative filtering model with classic two-tower architecture. It fuses Generalized Matrix Factorization (GMF) and Multi-Layer Perceptron (MLP) to capture both linear and nonlinear useritem interactions: ij = ˆrGMF qi, ˆrMLP ij = fMLP([puqi]), where [] denotes vector concatenation. The final prediction ˆrij is computed by combining the outputs from GMF and MLP, typically through fully connected layer. ❸ LightGCN [He et al., 2020] simplifies GCN by removing feature transformation and nonlinear activation to better capture high-order connectivity in user-item graphs. It adopts symmetric normalization in the graph convolution process, aggregating embeddings from neighboring nodes using degree-aware weighting scheme: p(l+1) = (cid:88) iNu 1 (cid:112)NuNi q(l) , q(l+1) = (cid:88) uNi 1 (cid:112)NiNu p(l) . This symmetric normalization allows the model to more accurately propagate information and better capture user interests. ❹ SGL [Wu et al., 2021] introduces self-supervised learning by augmenting graph structure and node features, incorporating contrastive objectives to improve robustness and alleviate sparsity: = LBPR (cid:124) (cid:123)(cid:122) (cid:125) collaborative filtering +γ Lcont (cid:124)(cid:123)(cid:122)(cid:125) contrastive , (cid:1) and where LBPR is the Bayesian Personalized Ranking loss (cid:80) Lcontrastive is the contrastive loss encouraging consistency between augmented views. In addition, γ controls the contrastive tasks contribution. (u,i,j)D ln σ (cid:0)p qi qj"
        },
        {
            "title": "3 Analysis of Scaling: Scale it up! Scale it up again!",
            "content": "We focus on three points: First, what strategy we use to increase embedding size. Second, what phenomenon we observe in experiments. Third, why these phenomena happen. 3.1 Observation Settings Our experiments are based on the RecBole 1 [Xu et al., 2023]. We select four popular collaborative filtering models: BPR [Rendle et al., 2009], NeuMF [He et al., 2017], LightGCN [He et al., 2020] and SGL [Wu et al., 2021], along with ten classic datasets, covering different level of scale and sparsity. These datasets are described in the Appendix 3. To ensure thorough investigation, our experiments were conducted on setup equipped with 8*80G GPUs. Other settings regarding all model parameters, optimizer, learning rate, etc. are in the Appendix D.1. 1https://www.recbole.io/docs/index.html 3 3.2 Double-peak and Logarithmic Phenomenon In the past understanding, when the embedding dimension increases, the performance shows single-peak phenomenon of first increasing and then decreasing. However, we believe that this understanding lacks microscopic and extensive observations. Through large-scale experiments (Figure 2), we find that on the ML-100K dataset, when the embedding dimension increases, the overall performance of the BPR model is indeed consistent with previous studies, showing an overall trend of first increasing and then decreasing. However, we discovere more details for the first time: as the embedding dimension increases, the recommendation performance initially increases significantly, then decreases, then increases slightly, and finally decreases again. We call this new trend doublepeak phenomenon (as shown in Figure 2 (b, c, e, f)). Moreover, we noticed that the most of peak performance occurs at 29, which are different from the traditional settings of 32, 64, and 128. But unexpectedly, in LightGCN and SGL, this phenomenon is alleviated lot, and SGL alleviates it better than LightGCN (as shown in Figure 2 (k, h)). In addition, our experiments show that on the Modcloth dataset, BPR, NeuMF, LightGCN, and SGL all show continuous upward trend. For example, even when the dimension is set to 214, the performance of LightGCN is still rising. We call this ideal trend the logarithmic phenomenon (as shown in Figure 2 (a)), that is, as the embedding dimension increases, the performance will continue to rise, but the increase amplitude will gradually decrease. The logarithmic phenomenon completely exceeds the past understanding. Compared with the NDCG@20 at 128 dimensions, the performance has increased by 25.57%. On the contrary, if we only set the dimension to 128, we will lose 34.36% of the potential performance improvement. Currently, most research in the recommendation field, such as debiasing [Chen et al., 2021, 2023] and denoising [Wang et al., 2021, He et al., 2024], believes that 10% performance improvement is meaningful. However, in our observations, simply increasing the dimension can achieve performance improvement far exceeding that of elaborately designed methods. To ensure the generality of the phenomenon, we design series of experiments, conducting observations across other 7 datasets (E.1) and also analysis the results in detail. Empirical results confirm that the phenomenon is indeed widespread. Two Observations. ❶ The same model exhibits different phenomenons across different datasets. As shown in Figure 2 (a, d), BPR exhibits double-peak phenomenon on the ML-100K dataset, while showing logarithmic growth on the Modcloth dataset. This seems to be due to some negative factor in ML-100K that causes scaling to fail. ❷ The same dataset exhibits different phenomena across different models. As shown in Figure 2 (a, b, c), we observe that BPR exhibits double-peak phenomenon, while LightGCN and SGL show logarithmic growth. Moreover, we believe this might be because the structures of LightGCN and SGL incorporate mechanisms that mitigate negative factors. Moreover, the logarithmic growth of SGL appears to be more ideal. 3.3 Why Two Phenomenon Occur Based on the previous two observations, we believe this is due to the presence of certain negative factors in the dataset that BPR and NeuMF cannot resist, while LightGCN and SGL can. When these negative factors are mitigated, logarithmic trend emerges, whereas if influenced by these factors, double-peak pattern is observed. In addition, the difficult-to-understand aspect of the double-peak phenomenon lies in the third stage, where performance rises again. If this stage were absent, with performance simply rising and then falling, it could easily be explained as overfitting. However, the appearance of the third stage prevents us from attributing it to overfitting, and we believe this is the key to understanding this phenomenon. For logarithmic phenomenon, this is an ideal trend. If every model can exhibit this trend on every dataset, it would be wonderful. Upon close examination of Figures 2, 4, 5, we observe that SGL tends to exhibit such logarithmic trend. We attribute this behavior to the contrastive learning framework in SGL, which provides both regularization and robustness enhancement. In contrast, other methods lack such mechanism, which may explain their deviation from this desirable pattern. Noisy Interactions Induces the Double-peak Phenomenon. For the double-peak phenomenon, this is novel phenomenon. Our approach to analyzing this phenomenon was inspired by two phenomena in machine learning: the memorization effect on noisy labels [Zhang et al., 2017] and the sparse double descent [He et al., 2022] which found that the sparsity of model parameters and the level of label noise jointly affect the models performance (more about noisy interactions at B.3). 4 (a) ModCloth - BPR (b) Douban - BPR (c) ML-100k - BPR (d) ModCloth - NeuMF (e) Douban - NeuMF (f) ML-100k - NeuMF (g) ModCloth - LightGCN (h) Douban - LightGCN (i) ML-100k - LightGCN (j) ModCloth - SGL (k) Douban - SGL Figure 2: Scale the embedding dimension exponentially by factor of 2 across different collaborative filtering models and datasets. Each row corresponds to model (BPR, LightGCN, SGL, NeuMF), and each column represents dataset (Modcloth, Douban, ML-100k). (l) ML-100k - SGL [He et al., 2022] is phenomenon that occurs during Definition 1 (Sparse Double Descent) model pruning. It is characterized by the models test performance first decreasing due to overfitting as sparsity increases, then improving once overfitting is alleviated after reaching certain level of sparsity, and finally decreasing again as sparsity increases, due to the loss of useful information. We think that scaling the embedding dimension is related to the sparsification of neural networks in recommendation tasks, as increasing the dimension effectively reduces the sparsity of the neural network. In the first stage, the cleaning learning stage, the model initially learns from clean interactions. As the dimensionality of the embedding increases, the model has more space to accommodate user interests, leading to performance improvements. In the second stage, the noise memorization stage, the presence of interaction noise, combined with dimensionality large enough to accommodate noisy interactions, causes the model to begin learning from these noisy interactions, resulting in performance degradation. In the third stage, the sweet stage, the embedding space enlarges to adapt to interaction noise, thereby enhancing the models robustness. In the fourth stage, the overfitting stage, the continuous increase in embedding dimensions leads to higher fit to the training dataset, which subsequently reduces generalization performance. 5 (a) ML-100K (b) Douban Figure 3: Comparing the standard BPR with the denoising strategy-based BPR Drop in ML-100K and Douban, we can clearly observe that the performance degradation has significantly improved."
        },
        {
            "title": "4 One of the Keys to Scaling: Improve Robustness to Noisy Interactions",
            "content": "In this section, we conduct detailed analysis based on the models and explore the characteristics of each model, focusing on three main questions: Why are BPR and NeuMF easily affected by noise? Why does graph collaborative filtering exhibit noise resistance? Why does graph self-supervised learning perform better than graph collaborative filtering? 4.1 Analysis of the Resistance to Noise of BPR We first analyze why the BPR model is particularly sensitive to noisy interactions. Our analysis is based on the degradation of representation quality under noisy preference distributions. Definition 2 (Representation Quality) Let Θ = {p } denote the ideal latent representations learned from clean data D0, and Θ = {pu, qi} be any learned parameters. The representation quality is defined as: u, Q(Θ) := Θ Θ = (cid:88) pu u2 2 + qi 2 2. (cid:88) (1) Theorem 1 (Representation Perturbation Under Noisy Interactions) Let Θ0 be the minimizer of the clean empirical loss ExD0[ℓ(Θ; x)], and consider noisy interaction distribution Dδ := (1 δ)D0 + δN , where is noise distribution and δ [0, 1] is the noise ratio. Define the noise gradient shift as noise := ExN [Θℓ(Θ0; x)] ExD0 [Θℓ(Θ0; x)]. Assume the loss ℓ(Θ; x) is twice differentiable and the expected clean loss is locally strongly convex around Θ0. Then, for sufficiently small δ, the representation quality satisfies: where Θδ is the minimizer under Dδ, and := 2 Θ clean optimum. The complete proof can be found in C.1 Q(Θδ) := Θδ Θ02 δ2 1noise2 + o(δ2), (2) ExD0[ℓ(Θ; x)]Θ=Θ0 is the Hessian at the Remark 1 Theorem 1 reveals that the degradation in representation quality under noisy interactions is quadratically proportional to the noise ratio δ, and linearly controlled by noise, which is dictated by the model architecture. Lemma 1 (High Gradient Sensitivity in BPR) In BPR, (u, i, j) = is: the scoring function is defined as (qi qj), which is linear in its parameters. Therefore, the per-sample gradient Θℓ(Θ; x) (qj qi, pu, pu), (3) with unbounded norm when embeddings grow. As result, noise tends to be large, making BPR more vulnerable to noisy samples. Remark 2 This explains the double-peak phenomenon observed in BPR training, especially under large embedding dimensions, due to overfitting to noise. 6 Experimental Evidence. To validate the robustness of BPR, we adopted simple strategy: we applied denoising collaborative filtering methods to observe whether there would be an improvement in performance, particularly when the embedding dimensions are high (Code at D.2). (cid:0)Θt; {xi (xi) τ }(cid:1) . Θt+1 Θt ηℓBPR (4) This equation illustrates dynamic sample-dropping strategy [Wang et al., 2021] to enhance BPRs noise robustness. During gradient updates, only samples with losses below threshold τ are retained, filtering out high-loss interactions likely to be noise. Experiments show this approach effectively mitigates noise in high-dimensional embedding spaces while improving recommendation performance (as shown in Figure 3). The threshold generally depends on the dropout rate: higher dropout leads to lower threshold, and vice versa. We conducted extra experiments on Figure 7 with different dropout rates for further evidence. 4.2 Analysis of the Resistance to Noise of NeuMF We now analyze the noise sensitivity of NeuMF from Jacobian-based perspective. Despite the inclusion of nonlinear components, the gradient propagation through multiple nonlinear layers can amplify noise, depending on the Jacobian of the model output with respect to the parameters. Lemma 2 (Noise-Induced Gradient Amplification) Let (u, i) be the NeuMF prediction function, and let Θ Rd denote all model parameters. Define the Jacobian matrix of the scoring function with respect to parameters as Jx := Θf (u, i) R1d for input = (u, i). Then for noisy interaction , the norm of the loss gradient is bounded by: Θℓ(Θ; x) Lℓ Jx, (5) where Lℓ is the Lipschitz constant of the loss function. Hence, the gradient norm is directly governed by the spectral norm Jx of the Jacobian. Theorem 2 (Gradient Instability in NeuMF) Let (u, i) = MLP([pu; qi])+p qi be the NeuMF scoring function, where MLP has layers with weight matrices (1), . . . , (L), each with spectral norm (l)2. Then the Jacobian satisfies: Jx = (cid:13) (cid:13) (cid:13) (cid:13) (u, i) Θ (cid:13) (cid:13) (cid:13) (cid:13) (cid:32) (cid:89) l=1 (cid:33) (l)2 zℓ(f (u, i), y), (6) which grows multiplicatively with depth, implying NeuMF can have exponentially amplified gradient sensitivity under noisy interactions. The proof precess are at C.2. Corollary 1 (Comparative Noise Sensitivity: NeuMF vs. BPR) Although BPR has unbounded inner product gradients w.r.t. embeddings, its Jacobian norm is linear in embedding size, i.e., BPR pu + qi. In contrast, NeuMFs Jacobian grows with the product of layer specx tral norms, potentially leading to (7) in deep or poorly regularized MLPs. Hence, NeuMF may not exhibit superior robustness, and may in fact be more susceptible to gradient perturbations from noise. J NeuMF > BPR , 4.3 Analysis of the Resistance Noise of Graph Collaborative Filtering LightGCN exhibits relatively expected logarithmic growth pattern. We believe this is likely due to the inherent noise-resistant structure of LightGCN. We think that the process of aggregating information from users neighboring nodes in graph convolution [He et al., 2020] is highly consistent with Mixup [Zhang et al., 2018]. (u, v) = (cid:124) (cid:32) 1 Ni (cid:88) uk, vi (cid:33) ; kNi (cid:123)(cid:122) graph convolution (u, v) = (cid:32) (cid:88) i=1 (cid:124) (cid:88) λiui, i=1 (cid:123)(cid:122) ixup 7 (cid:125) (cid:33) λivi , s.t. (cid:125) (cid:88) i=1 λi = 1. (8) Specifically, first-order neighbors aggregate information about items, while second-order neighbors aggregate information from other users. Since Mixup is regarded as noise-resistant, we hypothesize that graph-based collaborative filtering possesses similar noise-resistant capabilities. Moreover, this idea aligns with findings in Han et al. [2024]. (cid:33) uk, vi 1 Ni (cid:88) kNi (cid:32) (cid:32) (u, v) = = (cid:88) 1 Ni uk, (cid:88) kNi 1 Ni vi kNi (cid:32) λi= 1 Ni = (cid:88) (cid:88) λiuk, λivi . (cid:33) (cid:33) (9) kNi kNi This makes it easy to understand why LightGCN is noise-resistant, as Mixup, being classic and popular method for domain generalization, is effective in mitigating noise disturbances. Next, we analyze from the spectral perspective. Definition 3 (Spectral Representation of Graph Convolution) Let the adjacency matrix of the user-item interaction graph be R(m+n)(m+n), and the degree matrix be D. The normalized adjacency matrix is = D1/2AD1/2. Performing eigendecomposition on A: = ΛU , Λ = diag(λ1, λ2, . . . , λm+n), (10) where λ1 λ2 λm+n, and is the orthogonal eigenvector matrix. Definition 4 (Low-Rank Property of Graph Convolution) The embedding propagation in LightGCN (Eq.2) can be represented as spectral filtering process. After layers of propagation, the final user/item embeddings are: pu = (cid:88) l=0 βl Alp(0) , qi = (cid:88) l=0 βl Alq(0) , (11) L+1 . In the spectral domain, this is equivalent to applying low-pass filter to the initial where βl = 1 embeddings: pu = (cid:33) βlΛl p(0) . (cid:32) (cid:88) l=0 (12) The filter coefficient matrix (cid:80)L l=0 βlΛl attenuates high-frequency components (small λk) while preserving low-frequency components (large λk), leading to low-rank embeddings. The complete proof can be found in C.4. 4.4 Analysis of the Resistance Noise of Self-supervised Graph Collaborative Filtering Self-supervised graph collaborative filtering (SGL) is subset of graph-based collaborative filtering. Naturally, we can also infer that self-supervised graph collaborative filtering inherently possesses the ability to resist interaction noise due to the graph convolution mechanism. Moreover, in most cases, SGL outperforms LightGCN. So, what additional module plays crucial role in this difference? SGL adopts self-supervised contrastive learning. SGL generates multi-view representations via stochastic graph perturbations and optimizes contrastive loss. For view generation, we create augmented graphs and by: edge dropout: Remove edges with probability ρ and feature masking: zero out embedding dimensions with probability ρ. For contrastive loss, we maximize consistency between views for the same node: Lcont = (cid:88) log exp (sim(z u, vU exp (sim(z u)/τ ) u, )/τ ) (cid:80) , (13) uU where sim() is cosine similarity, τ is temperature hyperparameter, and embeddings of u. u, are augmented 8 Definition 5 (Mutual Information Objective in Contrastive Learning) SGL maximizes the mutual information I(z via the contrastive loss Lcont. By the information bottleneck principle: u) between augmented views and u; where H() denotes entropy. Minimizing Lcont forces I(z u; u) = H(z u) H(z and uz (14) to share noise-invariant information. u), Theorem 3 (Embedding Space Constraint via Contrastive Learning) Let the noise perturbation δu satisfy δu S, where is the subspace of clean interaction signals. The contrastive loss in SGL implicitly constrains the embedding vectors zu to lie within S: (15) where ϵ is constant related to noise level, and is the orthogonal complement of S. The complete proof can be found in C.3. ProjS (zu)2 ϵ, u, Corollary 2 (Noise Filtering in SGL) As demonstrated in Theorem 3, SGL achieves noise resistance by employing low-pass filtering in graph convolution to attenuate high-frequency noise while retaining essential low-frequency components, and by imposing subspace constraint in contrastive learning to further reduce noise variance and improve noise robustness. Mathematically, SGL embeddings are: zu = ProjS (z u) (cid:124) (cid:125) (cid:123)(cid:122) low-rank signal + ProjS (δu) , (cid:125) (cid:123)(cid:122) noise (cid:124) (16) where ProjS (δu)2 ϵ, and ϵ decreases with contrastive learning strength γ."
        },
        {
            "title": "5 Discussion",
            "content": "The quality of the recommendation dataset is more important than we thought. The quality of the dataset, especially noisy interactions, yields substantial influence on the scaling recommendation performance. When the dataset contains less noise, the model can scale more smoothly, with algorithms adapting and expanding more efficiently to handle larger volumes of data. Conversely, high level of noise data muddles the underlying patterns, undermining the models ability to scale effectively and leading to suboptimal performance as the scale increases. Collaborative filtering models with noise-resistant structures have more advantages when it comes to scaling. We can clearly see that, compared to BPR, NeuMF and LightGCN, SGL demonstrates stronger scaling capabilities. Even when the embedding dimensions are significantly increased, SGL maintains stable performance or stays close to peak performance, unlike BPR, which experiences sharp decline. This advantage stems from the inherent noise resistance of SGLs structure. We believe that further exploration of this structure could potentially lead to the emergence of Transformer in the field of collaborative filtering."
        },
        {
            "title": "6 Conclusion and Limitations",
            "content": "Conclusion. Our contributions lie in the discovery of two new phenomena (double-peak and logarithmic), the interpretation of their causes, and the theoretical analysis of noise robustness in representative collaborative filtering models. Specifically, to investigate whether scaling embedding dimensions can improve collaborative filtering models, we conducted extensive observational experiments across 10 datasets and 4 representative models. Compared to the previously recognized single-peak phenomenon, we identified two novel phenomena: double-peak and logarithmic. We conducted detailed analysis of the underlying cause of the double-peak effect and found it to result from noise interactions. To empirically validate our hypothesis, we introduced denoising loss function, which supported our explanation. Furthermore, we provided theoretical analysis of the models robustness to noise with results aligned with our empirical observations. Limitations. Due to limitations in available computational resources, we were unable to conduct experiments on broader range of collaborative filtering models. Additionally, our observations were based primarily on the Top@K metric with set to 20. With sufficient resources, future work could explore additional values of (e.g., 5, 10) and incorporate more fine-grained evaluation metrics to further validate the generality of the observed phenomena."
        },
        {
            "title": "References",
            "content": "A Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. Xingzhuo Guo, Junwei Pan, Ximei Wang, Baixu Chen, Jie Jiang, and Mingsheng Long. On the embedding collapse when scaling up recommendation models. In Forty-first International Conference on Machine Learning, 2024. Newsha Ardalani, Carole-Jean Wu, Zeliang Chen, Bhargav Bhushanam, and Adnan Aziz. Understanding scaling laws for recommendation models. arXiv preprint arXiv:2208.08489, 2022. Buyun Zhang, Liang Luo, Yuxin Chen, Jade Nie, Xi Liu, Shen Li, Yanli Zhao, Yuchen Hao, Yantao Yao, Ellie Dingqiao Wen, et al. Wukong: Towards scaling law for large-scale recommendation. In Forty-first International Conference on Machine Learning, 2024. Yufei Ye, Wei Guo, Jin Yao Chin, Hao Wang, Hong Zhu, Xi Lin, Yuyang Ye, Yong Liu, Ruiming Tang, Defu Lian, et al. Fuxi-α: Scaling recommendation model with feature interaction enhanced transformer. arXiv preprint arXiv:2502.03036, 2025. Yi Xu, Zhiyuan Lu, Xiaochen Li, Jinxin Hu, Hong Wen, Zulong Chen, Yu Zhang, and Jing Zhang. Addressing information loss and interaction collapse: dual enhanced attention framework for feature interaction. arXiv preprint arXiv:2503.11233, 2025. Shaowen Peng, Kazunari Sugiyama, Xin Liu, and Tsunenori Mine. Balancing embedding spectrum for recommendation. ACM Transactions on Recommender Systems, 3(4):125, 2025. Haochen Liu, Xiangyu Zhao, Chong Wang, Xiaobing Liu, and Jiliang Tang. Automated embedding size search in deep recommender systems. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 23072316, 2020. Yunke Qu, Tong Chen, Xiangyu Zhao, Lizhen Cui, Kai Zheng, and Hongzhi Yin. Continuous input embedding size search for recommender systems. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 708717, 2023. Shuyao Wang, Yongduo Sui, Jiancan Wu, Zhi Zheng, and Hui Xiong. Dynamic sparse learning: novel paradigm for efficient recommendation. In Proceedings of the 17th ACM International Conference on Web Search and Data Mining, pages 740749, 2024. Qinyi Luo, Penghan Wang, Wei Zhang, Fan Lai, Jiachen Mao, Xiaohan Wei, Jun Song, Wei-Yu Tsai, Shuai Yang, Yuxi Hu, et al. Fine-grained embedding dimension optimization during training for recommender systems. arXiv preprint arXiv:2401.04408, 2024. Siyi Liu, Chen Gao, Yihong Chen, Depeng Jin, and Yong Li. Learnable embedding sizes for recommender systems. In International Conference on Learning Representations, 2021. Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. Bpr: Bayesian personalized ranking from implicit feedback. UAI 09, 2009. Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian, and Xing Xie. Selfsupervised graph learning for recommendation. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval, pages 726735, 2021. Zheng He, Zeke Xie, Quanzhi Zhu, and Zengchang Qin. Sparse double descent: Where network pruning aggravates overfitting. In International Conference on Machine Learning, pages 8635 8659. PMLR, 2022. Rengan Xu, Junjie Yang, Yifan Xu, Hong Li, Xing Liu, Devashish Shankar, Haoci Zhang, Meng Liu, Boyang Li, Yuxi Hu, et al. Enhancing performance and scalability of large-scale recommendation systems with jagged flash attention. arXiv preprint arXiv:2409.15373, 2024. Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. Neural collaborative filtering. In Proceedings of the 26th international conference on world wide web, pages 173182, 2017. 10 Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. Lightgcn: Simplifying and powering graph convolution network for recommendation. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, pages 639648, 2020. Lanling Xu, Zhen Tian, Gaowei Zhang, Junjie Zhang, Lei Wang, Bowen Zheng, Yifan Li, Jiakai Tang, Zeyu Zhang, Yupeng Hou, Xingyu Pan, Wayne Xin Zhao, Xu Chen, and Ji-Rong Wen. Towards more user-friendly and easy-to-use benchmark library for recommender systems. In SIGIR, pages 28372847. ACM, 2023. Jiawei Chen, Hande Dong, Yang Qiu, Xiangnan He, Xin Xin, Liang Chen, Guli Lin, and Keping Yang. Autodebias: Learning to debias for recommendation. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 2130, 2021. Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan He. Bias and debias in recommender system: survey and future directions. ACM Transactions on Information Systems, 41(3):139, 2023. Wenjie Wang, Fuli Feng, Xiangnan He, Liqiang Nie, and Tat-Seng Chua. Denoising implicit feedIn Proceedings of the 14th ACM international conference on web back for recommendation. search and data mining, pages 373381, 2021. Zhuangzhuang He, Yifan Wang, Yonghui Yang, Peijie Sun, Le Wu, Haoyue Bai, Jinqi Gong, Richang Hong, and Min Zhang. Double correction framework for denoising recommendation. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 10621072, 2024. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In International Conference on Learning Representations, 2017. Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. International Conference on Learning Representations, 2018. Xiaotian Han, Hanqing Zeng, Yu Chen, Shaoliang Nie, Jingzhou Liu, Kanika Narang, Zahra Shakeri, Karthik Abinav Sankararaman, Song Jiang, Madian Khabsa, Qifan Wang, and Xia Hu. On the equivalence of graph convolution and mixup. Transactions on Machine Learning Research, 2024. Gaowei Zhang, Yupeng Hou, Hongyu Lu, Yu Chen, Wayne Xin Zhao, and Ji-Rong Wen. Scaling law of large sequential recommendation models. arXiv preprint arXiv:2311.11351, 2023. Donald Loveland, Xinyi Wu, Tong Zhao, Danai Koutra, Neil Shah, and Mingxuan Ju. Understanding and scaling collaborative filtering optimization from the perspective of matrix rank. In Proceedings of the ACM on Web Conference 2025, pages 436449, 2025. Hung Vinh Tran, Tong Chen, Nguyen Quoc Viet Hung, Zi Huang, Lizhen Cui, and Hongzhi Yin. thorough performance benchmarking on lightweight embedding-based recommender systems. ACM Transactions on Information Systems, 2025. Antonio Ginart, Maxim Naumov, Dheevatsa Mudigere, Jiyan Yang, and James Zou. Mixed diIn 2021 mension embeddings with application to memory-efficient recommendation systems. IEEE International Symposium on Information Theory (ISIT), pages 27862791. IEEE, 2021. Weipu Chen, Zhuangzhuang He, and Fei Liu. When sparsemoe meets noisy interactions: An ensemble view on denoising recommendation. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2025. Yonghui Yang, Le Wu, Zihan Wang, Zhuangzhuang He, Richang Hong, and Meng Wang. Graph bottlenecked social recommendation. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 38533862, 2024. Yonghui Yang, Le Wu, Yuxin Liao, Zhuangzhuang He, Pengyang Shao, Richang Hong, and Meng Wang. Invariance matters: Empowering social recommendation via graph invariant learning. In Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 20382047, 2025. Weilin Lin, Xiangyu Zhao, Yejing Wang, Yuanshao Zhu, and Wanyu Wang. Autodenoise: Automatic data instance denoising for recommendations. In Proceedings of the ACM Web Conference 2023, pages 10031011, 2023. 12 Appendix of Understanding Embedding Scaling"
        },
        {
            "title": "Contents",
            "content": "A Notions Related Work B.1 Scaling Recommendation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Efficient Recommendation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Denoising Recommendation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Theoretical Discussions C.1 Proof of Theorem 1 . C.2 Proof of Theorem 2 . C.3 Proof of Theorem 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.4 Derivation of Definition 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Experimental Settings D.1 Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Pytorch Code of BPR Drop . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 Detailed description of the datasets . . . . . . . . . . . . . . . . . . . . . . . . . . D.4 Reason for Choosing Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Experiments E.1 All Observations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Effect of Varies Dropping Ratio . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 14 14 15 15 15 16 17 18 18 19 19 20 20"
        },
        {
            "title": "A Notions",
            "content": "Symbol Description Table 1: Symbol Definitions u, i, pu Rk qi Rk Θ = {pu, qi} Θ = {p u, } Θ0 Θη D0 Dη = D0 Dη η (0, 1) λ > 0 σ(z) = 1 L0(Θ) Lη(Θ) L(Θ) Q(Θ) 2 µ > 0 > 0 1+ez Indices representing users. Indices representing items. Latent representation vector for user u. Latent representation vector for item i. Set of all user and item latent representations. Ideal set of user and item latent representations satisfying true preference relations. Optimal representations by minimizing the loss function without noise. Optimal representations by minimizing the loss function with noise proportion η. Set of authentic (noise-free) interaction triples (u, i, j). Set of noisy interaction triples (u, j, i). Combined set of authentic and noisy interaction triples. Proportion of noisy interactions in the dataset. Regularization parameter in the loss function. Sigmoid function. Loss function without noise. Loss function component due to noisy interactions. Total loss function with noise = L0(Θ) + Lη(Θ). Representation quality metric. Frobenius norm. Euclidean (L2) norm. Strong convexity parameter. Lower bound constant related to noisy interactions."
        },
        {
            "title": "B Related Work",
            "content": "B.1 Scaling Recommendation. Large language models (LLM) have shone brightly in todays era, and one of the key reasons is that the performance of the foundational Transformer [Vaswani, 2017] model can scale with parameter expansion. Recently, few efforts [Ardalani et al., 2022, Zhang et al., 2023, 2024, Guo et al., 2024, Xu et al., 2024, Ye et al., 2025, Xu et al., 2025, Peng et al., 2025, Loveland et al., 2025] in the recommendation field have explored similar idea: whether the performance of recommendation models can increase with parameter scaling. [Ardalani et al., 2022] explores the scaling properties of recommendation models across data, compute, and parameters, revealing that parameter scaling has limited impact on performance compared to data scaling, and highlights the need for future advancements in model architecture and system design. [Guo et al., 2024] identifies the embedding collapse phenomenon as the inhibition of scalability, where the embedding matrix leans towards occupying low-dimensional subspace. It demonstrates two-sided effect of feature interaction in recommendation models, with collapsed embeddings restricting learning and worsening the collapse. B.2 Efficient Recommendation. How to set the dimension of embeddings is common question for almost everyone studying collaborative filtering. This is because one of the core aspects of collaborative filtering is representing users or items with embeddings, and the quality of these embeddings directly determines the performance of the recommendation system. Reflecting on the efforts of recent years, to determine the optimal embedding dimension, many researchers have learned the embedding dimension automatically from the perspectives of neural architecture search [Liu et al., 2020, Qu et al., 2023], weight pruning [Wang et al., 2024, Luo et al., 2024, Liu et al., 2021, Tran et al., 2025] or exploring mixed dimension [Ginart et al., 2021]. These methods seem to have yielded notable results. The implicit assumption of these methods is that there exists an optimal dimension, meaning the performance exhibits single-peak phenomenon. 14 B.3 Denoising Recommendation In recommendation systems based on implicit feedback, user behaviors (e.g., clicks, views) often contain noisy interactionssignals that do not truly reflect user preferences, such as clicks driven by item position or popularity rather than genuine interest Chen et al. [2025], Yang et al. [2024, 2025]. These biases degrade model generalization and make learning reliable user preferences challenging. One of the simplest and most practical approaches is the drop-based method, which identifies noisy samples based on their loss values: samples with higher loss are more likely to be noisy, while those with lower loss are considered clean. drop-based method filters out noisy interactions before training. For example, T-CE [Wang et al., 2021] finds that noisy samples tend to have higher loss values, which can be used as denoising signal. AutoDenoise [Lin et al., 2023] models denoising as search-and-decision process and applies reinforcement learning to automate it. DCF [He et al., 2024] further improves performance by distinguishing between noisy interactions and hard samples, and by relabeling the noisy ones."
        },
        {
            "title": "C Theoretical Discussions",
            "content": "C.1 Proof of Theorem 1 Proof. Since Θ0 minimizes the clean objective, we have the first-order optimality condition: ΘExD0 [ℓ(Θ0; x)] = 0. Let us denote the noisy objective as: Lδ(Θ) := ExDδ [ℓ(Θ; x)] = (1 δ)ExD0[ℓ(Θ; x)] + δExN [ℓ(Θ; x)]. Then the gradient of the noisy objective at the clean optimum Θ0 is: ΘLδ(Θ0) = (1 δ)ΘExD0 [ℓ(Θ0; x)] + δΘExN [ℓ(Θ0; x)] = δ noise. (17) (18) Assume the loss is twice continuously differentiable and the clean loss is locally µ-strongly convex. ExD0 [ℓ(Θ; x)]Θ=Θ0 0 be the positive definite Hessian. Then, using Taylor Let := 2 Θ expansion: ΘLδ(Θδ) ΘLδ(Θ0) + H(Θδ Θ0) + R, where O(Θδ Θ02). At the noisy optimum, the gradient vanishes: 0 = ΘLδ(Θδ) Θδ Θ0 1(δ noise). (19) (20) Now compute: Q(Θδ) = Θδ Θ02 δH 1noise2 = δ2 1noise2 + o(δ2). C.2 Proof of Theorem 2 Proof. Let (u, i) = MLP([pu; qi]) + qi, and define the loss ℓ(Θ; x) := ℓ(f (u, i), y) for sample = (u, i, y). The gradient of the loss with respect to model parameters Θ satisfies the chain rule: Θℓ(Θ; x) = zℓ(f (u, i), y) Θf (u, i), (21) where := (u, i) is the scalar output score. Let Jx := Θf (u, i) denote the Jacobian of the scoring function. Taking norms on both sides yields: Θℓ(Θ; x) zℓ(f (u, i), y) Jx. (22) 15 We now analyze Jx. The scoring function can be decomposed as: (u, i) = MLP([pu; qi]) + qi. (23) The Jacobian of the inner product term is linear in pu and qi, and contributes only bounded term proportional to pu + qi. We focus on the MLP term, which dominates in depth. Let the MLP be defined recursively as: h(0) = [pu; qi], h(l) = σ(l)(W (l)h(l1)), for = 1, . . . , L, where σ(l) is pointwise activation function (e.g., ReLU or sigmoid), and (l) Rdldl1 is the weight matrix at layer l. Then the gradient of the output with respect to parameters is: (u, i) Θ = MLP(h(0)) Θ + (p qi) Θ . Focusing on the MLP term, its Jacobian w.r.t. h(0) is: MLP(h(0)) h(0) = (cid:89) l=1 D(l)W (l), where D(l) := diag(σ(l)(W (l)h(l1))) is diagonal matrix of activation derivatives. Taking operator norms, we get: (cid:13) (cid:13) (cid:13) (cid:13) MLP(h(0)) h(0) (cid:13) (cid:13) (cid:13) (cid:13) (cid:89) l=1 D(l)2 (l)2. (24) (25) (26) Since D(l)2 maxx σ(l)(x), which is bounded for standard activations like ReLU (bounded by 1) or sigmoid (bounded by 1/4), we conclude: (cid:13) (cid:13) (cid:13) (cid:13) (u, i) Θ (cid:13) (cid:13) (cid:13) (cid:13) Cσ (cid:89) l=1 (l)2, for some constant Cσ > 0 depending on activation functions. Combining this with the earlier chain rule, we obtain: Θℓ(Θ; x) zℓ(f (u, i), y) Cσ (cid:89) l=1 (l)2. (27) (28) Hence, the gradient norm can grow multiplicatively with the spectral norms of the MLP layers, implying exponential sensitivity in depth. C.3 Proof of Theorem 3 Proof. Decompose the embedding as zu = is noise. The contrastive loss requires: + δu, where is the clean signal and δu cos(z u, u) = (z + δ + δ u)T (z uz + δ u) + δ 1. Expanding this approximation: 1 δ δ 2z u2 u2 1 = δ δ u2 0. (29) (30) Since δ and δ are i.i.d., the expected variance is: (31) Thus, Var(δu) ϵ2, constraining noise to low variance and ensuring embeddings primarily reside in S. u2] = 2E[δu2] 2E[δT u]] = 2Var(δu). δ E[δ δ 16 C.4 Derivation of Definition 4 Derivation. We begin by recursively applying the propagation rule described in Eq. (12). The embeddings of the l-th layer are given by: P(l) = AP(l1), (32) where P(0) represents the initial embeddings, and is the normalized adjacency matrix. This recurrence relation reflects the embedding propagation at each layer of the graph convolutional network. The final embeddings after layers of propagation are obtained by taking the weighted sum across all layers: (cid:88) = βl AlP(0), where βl = l=0 L+1 is the weight for the l-th layer. Substituting the spectral decomposition of into this expression, we get: = ΛU , (33) (34) where is the matrix of eigenvectors and Λ is the diagonal matrix of eigenvalues of A. Therefore, we can rewrite the embeddings as: = (cid:88) l=0 βl(U ΛlU )P(0). Using the associativity of matrix multiplication, this simplifies to: = (cid:33) βlΛl P(0). (cid:32) (cid:88) l=0 (35) (36) Now, let us analyze the behavior of the filter coefficients (cid:80)L each eigenvalue λk of A, the corresponding coefficient behaves as follows: For λk 1 (low-frequency components), we have: l=0 βlΛl in the spectral domain. For (cid:88) l=0 βlλl (cid:88) l=0 1 + 1 1l = 1. This means that low-frequency components are preserved. For λk 0 (high-frequency components), we have: (cid:88) l=0 βlλl (cid:88) l=0 1 + 1 0l = 0. (37) (38) This means that high-frequency components are attenuated, effectively suppressing noise. Thus, the filter (cid:80)L retains the low-frequency components of the embeddings. l=0 βlΛl acts as low-pass filter that suppresses high-frequency components and As result, the final embedding matrix is dominated by the low-frequency components, and the rank of is bounded by the number of significant eigenvalues (those corresponding to lowfrequency components). Specifically, if is the number of significant eigenvalues, we can conclude that the rank of is bounded by r: rank(P) r. (39) This establishes the low-rank property of the embeddings after layers of propagation in the LightGCN model."
        },
        {
            "title": "D Experimental Settings",
            "content": "D.1 Hyperparameters We pick the most commonly used Top-k metrics NDCG@k to measure performance, as well as for comprehensive view of the phenomenon, we set to be 20. We set the embedding dimensions as powers of 2, starting from 2 and going up to 216. We train the model for 300 epochs with batch size of 2,048 using the Adam optimizer. The learning rate is fixed at 0.001 without weight decay (L2 regularization), and we disable gradient norm clipping. For negative sampling during training, we adopt static strategy with 1 negative item per positive interaction, sampled uniformly from the item space. We evaluate the model after every training epoch and implement early stopping if validation performance does not improve for 10 consecutive epochs. All models operate in essentially the same way; apart from differences in architecture, aspects like training loss are nearly identical. D.2 Pytorch Code of BPR Drop class BPRdropLoss(nn.Module): \"\"\"BPRLoss, based on Bayesian Personalized Ranking"
        },
        {
            "title": "Code of BPR Drop",
            "content": "Args: - gamma(float): Small value to avoid division by zero Shape: - Pos_score: (N) - Neg_score: (N), same shape as the Pos_score - Output: scalar. Examples:: >>> loss = BPRLoss() >>> pos_score = torch.randn(3, requires_grad=True) >>> neg_score = torch.randn(3, requires_grad=True) >>> output = loss(pos_score, neg_score) >>> output.backward() \"\"\" def __init__(self, save_ratio, get_low, gamma=1e-10): super(BPRdropLoss, self).__init__() self.gamma = gamma self.save_ratio = save_ratio self.get_low = get_low def forward(self, pos_score, neg_score): # loss = -torch.log(self.gamma + torch.sigmoid(pos_score - neg_score)).mean() per_sample_losses = -torch.log(self.gamma + torch.sigmoid(pos_score - neg_score)) num_samples = per_sample_losses.size(0) = int(num_samples * self.save_ratio) = max(k, 1) if self.get_low == 1: _, indices = torch.topk(per_sample_losses, k, largest=False) else: _, indices = torch.topk(per_sample_losses, k, largest=True) selected_losses = per_sample_losses[indices].mean() return selected_losses 18 D.3 Detailed description of the datasets In our research, we utilized four distinct datasets to ensure the comprehensiveness and robustness of our analysis. Each dataset contributes unique characteristics to our study. Table 3 summarizes the statistics of the 10 datasets used in our experiments, demonstrating the comprehensiveness of our evaluation. The datasets span wide range of interaction scales, from Small (e.g., ML-100K) to X-Large (e.g., Amazon Books), and cover various sparsity levels, from relatively dense (e.g., ML-1M) to extremely sparse (e.g., Amazon Beauty). Notably, the majority of datasets fall into the Ultra-sparse category, reflecting the real-world challenge of data sparsity in recommendation. This diversity ensures thorough and robust evaluation of model performance across different data regimes. The color-coded annotations provide intuitive insights into dataset characteristics. D.4 Reason for Choosing Models BPR [Rendle et al., 2009] is classic collaborative filtering model known for its simplicity and ease of use. Many subsequent influential methods have been developed based on it. NeuMF [He et al., 2017], in contrast, is highly representative deep learning-based model that adopts classic twotower architecture, combining linear modeling via matrix factorization with nonlinear interaction modeling through neural networks. As research progressed, it became clear that the relationships between users and items can be naturally modeled as graph structures. This insight led to the emergence of numerous graph-based collaborative filtering approaches, such as LightGCN [He et al., 2020]. Building upon graph structures, graph self-supervised modelssuch as SGL [Wu et al., 2021]further enhance performance by leveraging both feature and structural information, attracting significant attention. These are the reasons why we chose to focus on these three categories of models. The more specific information is at Table 2. Table 2: collaborative filtering models and their publication details Model Paper Title Year Venue Citation BPR NeuMF LightGCN LightGCN: Simplifying and Powering Graph Convolution NetBPR: Bayesian Personalized Ranking from Implicit Feedback Neural Collaborative Filtering 2009 UAI 8,232 2017 WWW 8,052 4,358 SIGIR 2020 SGL work for Recommendation Self-supervised Graph Learning for Recommendation 2021 SIGIR 1,423 Table 3: Statistics of the 10 benchmark datasets used in our experiments, highlighting both the dataset scale and sparsity level. Scale is categorized based on the number of interactions: Small (<1M), Medium (1M5M), Large (5M10M), and X-Large (>10M). Sparsity Level is categorized based on the sparsity ratio: Dense (<0.6), Semi-sparse (0.60.95), Sparse (0.950.99), and Ultrasparse (0.99). Colored backgrounds provide intuitive cues for comparison across datasets. Dataset #Users #Items #Interactions Sparsity Scale Sparsity Level ML-100K Modcloth Amazon Baby ML-1M Douban Pinterest Amazon Beauty Yelp Gowalla Amazon Books 943 9,000 531,890 6,040 11,257 55,187 1,210,271 1,326,101 107,092 8,026,324 1,682 10,000 64,426 3,706 2,281 9,911 249,274 174,567 1,280,969 2,330,066 100,000 500,000 915,446 1,000,209 1,330,000 1,445,622 2,023,070 5,261,669 6,442,892 22,507,155 19 Semi-sparse Ultra-sparse Ultra-sparse Small 0.93695 Small 0.99945 0.99997 Small 0.97570 Medium Sparse 0.99945 Medium Ultra-sparse 0.99736 Medium Ultra-sparse 0.99999 Medium Ultra-sparse Ultra-sparse 0.99998 Ultra-sparse 0.99990 0.99990 X-Large Ultra-sparse Large Large"
        },
        {
            "title": "E Experiments",
            "content": "E.1 All Observations we present additional experimental results on various datasets. As shown in Figures 4, 5, and 6, SGL demonstrates slower performance degradation trend compared to LightGCN, which in turn outperforms NeuMF, while BPR performs similarly to SGL. These results are consistent with our theoretical analysis, confirming the validity of our investigation into the noise robustness of different models. Additionally, we observed double-peak phenomenon in other datasets as well (e.g., Figure 4 (b, e, h)), along with an logarithm phenomenon (e.g., Figure 4 (j, k, l)). Moreover, the single-peak phenomenon widely discussed in prior work is also clearly present in our experiments (e.g., Figure 5 (a, b, c, etc.)). We argue that SGLs ability to maintain or even improve performance as the embedding dimension increases is largely due to the regularization and robustness enhancement effects introduced by contrastive learning. In contrast, LightGCN lacks auxiliary training signals and regularization methods beyond supervised learning, making it more prone to overfitting or performance degradation in high-dimensional settings. Overall, these experimental results further validate the generality of the newly observed phenomena and confirm that the observed performance trends are well aligned with our theoretical analysis. E.2 Effect of Varies Dropping Ratio The Figures 7 show the NDCG@20 of BPR Drop with different save-ratios and standard BPR on ML - 100K and Douban datasets as embedding size varies. In Figure (a) for ML-100K, BPR Drop with save - ratios 0.95, 0.9, 0.85, 0.8 and BPR are compared as embedding size ranges from 29 to 215. BPR Drop exhibits distinct trends, indicating sensitivity to scaling. In Figure (b) for Douban, similar comparisons are made. BPR Drops performance curves show unique patterns with embedding size changes, suggesting better scalability than standard BPR. These plots offer insights into BPR Drops scaling behavior for optimization in different datasets. 20 (a) Amazon BeautyBPR (b) Amazon Baby - BPR (c) Amazon Books - BPR (d) Amazon Beauty - NeuMF (e) Amazon Baby - NeuMF (f) Amazon Books - NeuMF (g) Amazon Beauty - LightGCN (h) Amazon Baby - LightGCN (i) Amazon Books - LightGCN (j) Amazon Beauty - SGL (k) Amazon Baby - SGL Figure 4: Scale the embedding dimension exponentially by factor of 2 across different collaborative filtering models and datasets. Each row corresponds to model (BPR, LightGCN, SGL, NeuMF), and each column represents dataset (Amazon Beauty, Amazon Baby, Amazon Books). (l) Amazon Books - SGL (a) Yelp - BPR (b) Gowalla - BPR (c) Pinterest - BPR (d) Yelp - NeuMF (e) Gowalla - NeuMF (f) Pinterest - NeuMF (g) Yelp - LightGCN (h) Gowalla - LightGCN (i) Pinterest - LightGCN (j) Yelp - SGL (k) Gowalla - SGL Figure 5: Scale the embedding dimension exponentially by factor of 2 across different collaborative filtering models and datasets. Each row corresponds to model (BPR, LightGCN, SGL, NeuMF), and each column represents dataset (Yelp, Gowalla, Pinterest). (l) Pinterest - SGL (a) ML-1M - BPR (b) ML-1M - LightGCN (c) ML-1M - SGL Figure 6: Scale the embedding dimension exponentially by factor of 2 across different collaborative filtering models and datasets. 22 (a) ML-100K (b) Douban Figure 7: Comparing the standard BPR with the denoising strategy-based BPR Drop in ML-100K and Douban, we can clearly observe that the performance degradation has significantly improved."
        }
    ],
    "affiliations": [
        "ASU",
        "NTU",
        "NUS",
        "UIUC"
    ]
}