{
    "paper_title": "Vector Quantization using Gaussian Variational Autoencoder",
    "authors": [
        "Tongda Xu",
        "Wendi Zheng",
        "Jiajun He",
        "Jose Miguel Hernandez-Lobato",
        "Yan Wang",
        "Ya-Qin Zhang",
        "Jie Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vector quantized variational autoencoder (VQ-VAE) is a discrete auto-encoder that compresses images into discrete tokens. It is difficult to train due to discretization. In this paper, we propose a simple yet effective technique, dubbed Gaussian Quant (GQ), that converts a Gaussian VAE with certain constraint into a VQ-VAE without training. GQ generates random Gaussian noise as a codebook and finds the closest noise to the posterior mean. Theoretically, we prove that when the logarithm of the codebook size exceeds the bits-back coding rate of the Gaussian VAE, a small quantization error is guaranteed. Practically, we propose a heuristic to train Gaussian VAE for effective GQ, named target divergence constraint (TDC). Empirically, we show that GQ outperforms previous VQ-VAEs, such as VQGAN, FSQ, LFQ, and BSQ, on both UNet and ViT architectures. Furthermore, TDC also improves upon previous Gaussian VAE discretization methods, such as TokenBridge. The source code is provided in https://github.com/tongdaxu/VQ-VAE-from-Gaussian-VAE."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 9 0 6 6 0 . 2 1 5 2 : r a"
        },
        {
            "title": "VECTOR QUANTIZATION USING\nGAUSSIAN VARIATIONAL AUTOENCODER",
            "content": "Tongda Xu1, Wendi Zheng1,2, Jiajun He3, Jose Miguel Hernandez-Lobato3, 1Tsinghua University, 2Zhipu AI Yan Wang1, Ya-Qin Zhang1, Jie Tang1,2 3University of Cambridge"
        },
        {
            "title": "ABSTRACT",
            "content": "Vector quantized variational autoencoder (VQ-VAE) is discrete auto-encoder that compresses images into discrete tokens. It is difficult to train due to discretization. In this paper, we propose simple yet effective technique, dubbed Gaussian Quant (GQ), that converts Gaussian VAE with certain constraint into VQ-VAE without training. GQ generates random Gaussian noise as codebook and finds the closest noise to the posterior mean. Theoretically, we prove that when the logarithm of the codebook size exceeds the bits-back coding rate of the Gaussian VAE, small quantization error is guaranteed. Practically, we propose heuristic to train Gaussian VAE for effective GQ, named target divergence constraint (TDC). Empirically, we show that GQ outperforms previous VQ-VAEs, such as VQGAN, FSQ, LFQ, and BSQ, on both UNet and ViT architectures. Furthermore, TDC also improves upon previous Gaussian VAE discretization methods, such as TokenBridge. The source code is provided in https://github.com/tongdaxu/VQ-VAE-from-Gaussian-VAE."
        },
        {
            "title": "INTRODUCTION",
            "content": "Vector-quantized variational autoencoder (Van Den Oord et al., 2017) is an autoencoder that compresses images into discrete tokens. It is fundamental to autoregressive generative models (Esser et al., 2021; Chang et al., 2022; Yu et al., 2023; Sun et al., 2024b). However, VQ-VAE is difficult to train: the encoding process of VQ-VAE is not differentiable and challenges such as codebook collapse often emerge (Sønderby et al., 2017). Special techniques are required to ensure the convergence of VQ-VAE, such as commitment loss (Van Den Oord et al., 2017), expectation maximization (EM) (Roy et al., 2018), Gumbel-Softmax (Jang et al., 2016; Maddison et al., 2016; Sønderby et al., 2017), and entropy loss (Yu et al., 2023; Zhao et al., 2024). In this paper, we circumvent the challenge of training VQ-VAE by converting Gaussian VAE with certain constraint into VQ-VAE without any training. More specifically, we propose Gaussian Quant (GQ), simple yet effective method for training-free conversion. The core idea is to generate codebook of one-dimensional Gaussian noise and, for each dimension of the posterior, select the codebook entry that is closest to the posterior mean. Theoretically, we show that as the logarithm of the codebook size exceeds the bits-back coding bitrate (Hinton & Van Camp, 1993; Townsend et al., 2019) of the Gaussian VAE, the resulting quantization error is small. In other words, GQ and the Gaussian VAE exhibit similar rate-distortion performance. This result serves as the theoretical foundation of GQ and provides principled guideline for selecting codebook sizes. Practically, we introduce the target divergence constraint (TDC) to train Gaussian VAE for efficient conversion. TDC encourages the Gaussian VAE to achieve the same KullbackLeibler (KL) divergence for each dimension, corresponding to the bits-back coding bitrate. Empirically, we demonstrate that GQ with Gaussian VAE trained by TDC, outperforms previous VQ-VAEs such as VQGAN, FSQ, LFQ, and BSQ (Van Den Oord et al., 2017; Mentzer et al., 2023; Yu et al., 2023; To whom correspondence should be addressed."
        },
        {
            "title": "Preprint",
            "content": "Zhao et al., 2024) in terms of reconstruction quality, using both UNet and ViT backbones. Additionally, we show that TDC can improve previous Gaussian VAE discretization methods, such as TokenBridge (Wang et al., 2025). Figure 1: The rate-distortion performance on the ImageNet dataset demonstrates that GQ outperforms previous VQ-VAEs on both UNet and ViT architectures. Our contributions can be summarized as follows: (Section 3.1) We propose GQ, simple yet effective approach that converts pre-trained Gaussian VAE with certain constraint into VQ-VAE without training. (Section 3.2) Theoretically, we prove that when the codebook size of GQ is close to the bits-back coding bitrate of the Gaussian VAE, the conversion error remains small. (Section 3.3) Empirically, we introduce target divergence constraint (TDC) to implement GQ and show that GQ outperforms previous VQ-VAEs, such as VQGAN, FSQ, LFQ, and BSQ, on both UNet and ViT architectures. (Section 3.5) Furthermore, we show that TDC can be used to improve previous Gaussian VAE discretization approaches, such as TokenBridge."
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "We denote the original image as X, the latent variable as Z, the encoder as (), and the decoder as g(). We use log() to denote the natural logarithm (base e) and KL divergence as DKL(). Similarly, we use log2() to denote the logarithm with base 2, and KL divergence as DKL(2)(). 2.1 VECTOR QUANTIZED VARIATIONAL AUTOENCODER VQ-VAE (Van Den Oord et al., 2017) transforms source image into series of integer tokens, which can be decoded using codebook and decoder. To facilitate auto-regressive generation, it typically involves deterministic transformation and shared codebook across different tokens. More specifically, VQ-VAE maintains codebook c1:K with size and bitrate of log K. The encoding process of VQ-VAE involves finding the closest codeword cj in c1:K to the encoder output (x)i for each latent dimension i. Denote distortion as (, ), the optimization target of VQ-VAE is the rate-distortion function weighted by Lagrangian multiplier λ: + E[(X, g(ˆz))] (cid:123)(cid:122) (cid:125) distortion LV = λ log (cid:124) (cid:123)(cid:122) (cid:125) bitrate +LReg, (cid:124) ˆzi = arg min cj {c1:K } (x)i cj, where c1:K are learned codebook, (1) and LReg are regularization for VQ-VAE to converge, such as combination of commitment loss and codebook loss (Van Den Oord et al., 2017) and Gumbel Softmax loss (Sønderby et al., 2017). 2.2 GAUSSIAN VARIATIONAL AUTOENCODER AND BITS-BACK CODING The Gaussian VAE is special type of VAE (Kingma et al., 2013) with prior (0, I) and fully factorized Gaussian posterior q(ZX). The encoding process of Gaussian VAE simply involves"
        },
        {
            "title": "Preprint",
            "content": "sampling the latent variable zi q(ZiX) for each latent dimension i. Assuming log p(XZ = z) (1/λ)(X, g(z)), then the negative evidence lower bound (ELBO) of the Gaussian VAE is equivalent to rate-distortion function of bits-back coding bitrate term and distortion term: LV AE = λ DKL(q(ZX)N (0, 1)) (cid:125) (cid:123)(cid:122) bits-back coding bitrate (cid:124) zi q(ZiX = x) = (µi, σ2 (cid:124) + E[(X, g(z))] , (cid:123)(cid:122) (cid:125) distortion ), = 1 . . . d. (2) The bitrate of zi is the bits-back coding bitrate, defined as DKL(q(ZiX)N (0, I)) (Hinton & Van Camp, 1993; Townsend et al., 2019). This is because, when compressing losslessly, one can communicate zi using DKL(q(ZX)N (0, I)) nats for arbitrary precision."
        },
        {
            "title": "3.1 DIRECT QUANTIZATION OF GAUSSIAN VAE",
            "content": "We propose an extremely simple technique to obtain VQ-VAE from Gaussian VAE: we directly generate one-dimensional Gaussian noise as the codebook for VQ-VAE (Van Den Oord et al., 2017) and quantize the posterior mean µi of the Gaussian VAE independently for each dimension i. Because the codebook consists entirely of samples from Gaussian distribution, we refer to our approach as Gaussian Quant (GQ). Specifically, we randomly generate codebook values c1:K (0, 1), which is the same for each dimension. Then, for each dimension i, we select the cj that is closest to the posterior mean µi and denote the quantized value as ˆzi: ˆzi = arg min cj {c1:K } µi cj, where c1:K (0, 1). (3) 3.2 THEORETICAL RELATIONSHIP BETWEEN THE CODEBOOK SIZE AND QUANTIZATION ERROR Why GQ works and how to select are not straightforward questions. Theoretically, we show that GQ preserves the rate-distortion property of the Gaussian VAE: when the bitrate log matches the bits-back coding bitrate of the Gaussian VAE, the quantization error is small. More specifically, we show that the probability of large quantization error decays doubly exponentially as the codebook bitrate log exceeds the bits-back coding bitrate DKL(q(ZiX)N (0, 1)). Theorem 1. Denote the mean and standard deviation of q(ZiX = x) as µi and σi, respectively. Assuming that the product and sum satisfy µiσi c1 and µi + σi c2, the probability of quantization error ˆzi µi σi decays doubly exponentially with respect to the number of nats by which the codebook bitrate log exceeds the bits-back coding bitrate. i.e., when log = DKL(q(ZiX)N (0, 1)) + t, Pr{ˆzi µi σi} exp (et ec10.5 ). (4) (cid:114) 2 π (cid:124) (cid:123)(cid:122) constant (cid:125) Conversely, when the codebook bitrate log is smaller than the bits-back coding bitrate, the probability of large quantization error increases exponentially toward 1. More specifically, we show that the probability of large quantization error increases exponentially when the codebook bitrate log is lower than the bits-back bitrate DKL(q(ZiX)N (0, 1)). Theorem 2. The probability of quantization error ˆzi µi σi increases exponentially with respect to the number of nats by which the codebook bitrate log is lower than the bits-back coding bitrate, i.e., when log = DKL(q(ZiX)N (0, 1)) t, Pr{ˆzi µi σi} 1 et (cid:114) 2 π (cid:124) 3 20. . (5) e0.5c2 (cid:123)(cid:122) constant (cid:125)"
        },
        {
            "title": "Preprint",
            "content": "Theorems 1 and 2 provide principled guideline for choosing K, such that log should be close to the bits-back bitrate DKL(q(ZiX)N (0, 1)). In practice, setting log2 = (cid:6)DKL(2)(q(ZiX)N (0, 1))(cid:5) typically yields small enough reconstruction error, where denotes the rounding operator. Using larger does not provide additional benefits, while smaller increases the error significantly."
        },
        {
            "title": "3.3 PRACTICAL IMPLEMENTATION WITH TARGET DIVERGENCE CONSTRAINT",
            "content": "There are two challenges to make GQ practical. The first challenge is, if we want to construct VQ-VAE with specific codebook size K, how can we train Gaussian VAE with corresponding KL divergence? The second challenge is, for vanilla Gaussian VAE trained to minimize the loss in Eq.2, the values of DKL(2)(q(ZiX)N (0, 1)) vary significantly across dimensions i. How to train Gaussian VAE with KL divergence remains close to log across each dimension? To address these two challenges, we propose the Target Divergence Constraint (TDC). TDC is designed to ensure that DKL(2)(q(ZiX)N (0, 1)) is close to log2 for all dimensions = 1, . . . , d. Specifically, we set the target KL divergence as log2 K. For each dimension i, we impose greater penalty if the KL divergence exceeds log2 + α bits, and smaller penalty if it falls below log2 α bits by using different λs for each case, where α is the hyper-parameter controlling thresholding: LT DC = (cid:88) i=1 λiDKL(q(ZiX)N (0, 1)) + (X, g(z)), λmin, DKL(2)(q(ZiX)N (0, 1)) < log2 α bits, λmean, DKL(2)(q(ZiX)N (0, 1)) [log2 α, log2 + α] bits, λmax, DKL(2)(q(ZiX)N (0, 1)) > log2 + α bits. (6) where λi = To determine λmin, λmean, λmax, we extend the heuristic in MIRACLE (Havasi et al., 2018b) and HiFiC (Mentzer et al., 2020). More specifically, we initialize λmin = λmean = λmax = 1, and update them according to the following rule: λmin = λmin β if mini{DKL(2)(q(ZiX)N (0, 1))} > log2 α else λmin/β, λmean = λmean β if meani{DKL(2)(q(ZiX)N (0, 1))} > log2 else λmean/β, λmax = λmax β if maxi{DKL(2)(q(ZiX)N (0, 1))} > log2 + α else λmax/β, where β is the hyper-parameter controlling update speed. To avoid numerical issue, we further clip λmin, λmean, λmax into range of [103, 103] after each update. In practice, we use α = 0.5, β = 1.01. In Appendix B, we propose an alternative implementation of TDC using Lambert function, which is less effective for ViT models. (7)"
        },
        {
            "title": "3.4 GROUPING TO MULTIPLE DIMENSIONS",
            "content": "The vanilla VQ-VAE has three key parameters: codebook size, codebook dimension and number of token. Some previous alternative to VQ-VAE, such as LFQ and BSQ (Yu et al., 2023; Zhao et al., 2024) limit the codebook dimension to 1. To support codebook dimension greater than 1, we can group tokens into single large token with codebook size log2 = (cid:108)(cid:80)i+m . There are three grouping strategies to achieve this: postquantization (PQ), post-training (PT), and training-based (TR), with different trade-offs in flexibility and performance. We brief these strategies in main text and provide details in Appendix C.2. l=i DKL(q(ZlX)N (0, 1)) (cid:107) PQ is the most flexible grouping strategy and can be applied after GQ. For PQ, since the posterior of the Gaussian VAE q(ZX) is factorized Gaussian, the quantization is also independent across dimensions. This means that we can trivially combine tokens into larger one, by treating each token as an integer in 1/m-based number system and aggregate them, which is the same as other one dimensional VQ-VAEs (Chang et al., 2022; Mentzer et al., 2023; Zhao et al., 2024). PT is less flexible than PQ, as it can only be applied before GQ and after training the Gaussian VAE. For PT, we can view the one-dimensional GQ in Eq. 3 as the maximum likelihood estimator of one-dimensional Gaussian. This approach can be extended to an m-dimensional diagonal Gaussian"
        },
        {
            "title": "Preprint",
            "content": "distribution. Additionally, for low bitrate cases, we observe that m-dimensional PT leads to low codebook usage. This is because µi is bounded by 2DKL. When the DKL is small, some vector in the codebook that is far from 0 is never used. To address this, we introduce regularization term weighted by ω to improve codebook usage by encouraging the selection cj that is far from 0: (µi:i+m cj)/σi:i+m ωcj, where c1:K (0, Im). ˆzi:i+m = arg min (8) cj {c1:K } TR is the least flexible strategy and must be used during the training of the Gaussian VAE. Specifically, building on the PT quantization approach, we can relax the TDC training target by considering the relationship between (cid:80)i+m l=i DKL(q(ZlX)N (0, 1)) and log2 0.5 bits. In terms of performance, PQ does not affect reconstruction at all. PT provides slight improvements in reconstruction, while TR significantly enhances reconstruction performance (see Table 13). 3."
        },
        {
            "title": "IMPROVING TOKENBRIDGE WITH TARGET DIVERGENCE CONSTRAINT",
            "content": "TokenBridge (Wang et al., 2025) also convert pre-trained Gaussian VAE into VQ-VAE. It adopts the Post Training Quantization (PTQ) technique from model compression and proposes to treat latent as model parameters to discretize. It uses fixed codebook composed of 2K centroids of Gaussian distribution. It then quantizes the posterior sample by finding the closest centroid. However, TokenBridge directly quantizes vanilla Gaussian VAE without limiting the KL divergence of each dimension. This leads to suboptimal rate-distortion performance. In fact, we can also improve the performance of TokenBridge using TDC. Specifically, the quantization centers of TokenBridge are the equal-probability partition centers of the (0, 1) distribution, which can be seen as special case of GQ with an evenly distributed codebook c1:K. From this perspective, the number of PTQ bits should also match the bits-back coding bitrate of the Gaussian VAE, and TDC can therefore enhance the performance of TokenBridge. As we demonstrate in Table 3, TDC indeed improves TokenBridge performance by large margin. 3.6 RELATIONSHIP WITH REVERSE CHANNEL CODING GQ is closely related to reverse channel coding, which aims to simulate distribution using samples from distribution (Harsha et al., 2007; Li & El Gamal, 2018; Havasi et al., 2018b; Flamich et al., 2020; Theis & Yosri, 2021; Flamich et al., 2022; He et al., 2024). For example, Minimal Random Coding (MRC) (Havasi et al., 2018b), when applied to Gaussian VAE, samples from the categorical distribution with logits given by the likelihood difference: ˆzi ˆq(c1:K) elog q(Zi=cj X)log (cj 0,1). (9) The key difference between MRC and GQ is that MRC and its variants (Havasi et al., 2018b; Theis & Yosri, 2021; Flamich et al., 2022; He et al., 2024) simulate distribution through stochastic sampling, whereas VQ-VAE requires deterministic quantization. For one dimension quantization, the bias bound of MRC derived from Chatterjee & Diaconis (2015) can not be achieved as VQ-VAE does not allow stochastic encoding. On the other hand, our achievability and converse bound is compatible with deterministic quantization. In terms of quantization error, GQ outperforms MRC by definition (Eq. 3). Besides, GQ without grouping (m=1) can be implemented by bisect search with better asymptotic complexity (See Appendix D.11). Additionally, TDC is closely related to the MIRACLE heuristic and IsoKL parametrization of Gaussian VAE (Havasi et al., 2018a; Flamich et al., 2022; Lin et al., 2023). More specifically, MIRACLE also proposes adjusting λ during VAE training. However, MIRACLE only maintains single λ, making it less effective for controlling the mean of DKL but less effective for constraining its minimum and maximum values. On the other hand, IsoKL imposes strict control on DKL by directly solving for σ given µ using the Lambert function (Corless et al., 1996; Brezinski, 1996). However, IsoKL suffers from numerical issues and leads to suboptimal performance."
        },
        {
            "title": "4 EXPERIMENTAL RESULTS",
            "content": "4.1 EXPERIMENTAL SETUP Models and Baselines For image reconstruction, we select two representative autoencoder architectures: UNet from Stable Diffusion 3 (Esser et al., 2024), and ViT from the BSQ (Zhao et al., 2024)."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Qualitative results on ImageNet dataset and 0.25 bpp. Our GQ has most visually pleasing reconstruction result. For VQ-VAE baselines, we include vanilla VQGAN (Van Den Oord et al., 2017) and several representative variants, including FSQ (Mentzer et al., 2023), LFQ (Yu et al., 2023), and BSQ (Zhao et al., 2024). Besides, we compare our approach to pre-trained VQ-VAE of VQGAN-Taming (Esser et al., 2021), VQGAN-SD (Rombach et al., 2022) Llama-Gen (Sun et al., 2024b), FlowMo (Sargent et al., 2025), BSQ (Zhao et al., 2024) and other conversion approaches such as TokenBridge and ReVQ . Additionally, we demonstrate that our TDC technique can improve TokenBridge (Wang et al., 2025), previous training-free approach for converting Gaussian VAEs into VQ-VAE. For image generation, we employ the Llama transformer (Touvron et al., 2023; Shi et al., 2024a). Datasets, Bitrates, and Metrics For the datasets, we use the ImageNet (Deng et al., 2009) training split for training, and both the ImageNet and COCO (Lin et al., 2014) validation split for testing. For reconstruction and generation experiments, all images are resized to 256 256 and 128 128 respectively. In terms of bitrates, we evaluate the image reconstruction performance of all models using codebook size of 214 218 and token numbers of 1024, 2048, and 4096, which correspond to bpp (bits-per-pixel) values of 0.22 1.00. This extends the BSQ evaluation beyond 0.25 0.50 bpp. For metrics, we adopt Peak Signal-to-Noise Ratio (PSNR), Learned Perceptual Image Patch Similarity (LPIPS) (Zhang et al., 2018), Structural Similarity Index Measure (SSIM) (Wang et al., 2004), and reconstruction Frechet Inception Distance (rFID) (Heusel et al., 2017) for image reconstruction; generation Frechet Inception Distance (gFID) and Inception Score (IS) (Salimans et al., 2016) are used for image generation. For further details, see Appendix C. 4.2 MAIN RESULTS Image Reconstruction In Table 1 and Table 15, we compare our GQ method to other quantization approaches across the 0.251.00 bpp range. The results show that, in terms of reconstruction metrics such as PSNR, LPIPS, SSIM, and rFID, our GQ approach achieves state-of-the-art performance in most cases. The advantage of GQ is consistent across both UNet and ViT model architectures, as well as for both the ImageNet and COCO datasets. Visually, in Figure 2, it is shown that our GQ also produces pleasing reconstruction and preserves lot more details in the source image. Besides, in Table. 2, we show that our GQ achieves competitive performance compared with several pre-trained"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Quantitive results on ImageNet dataset. Our GQ outperforms other VQ-VAEs on both UNet and ViT architecture, across 0.25-1.00 bpp. Bold: best. Method VQGAN FSQ LFQ BSQ GQ (Ours) VQGAN FSQ LFQ BSQ GQ (Ours) VQGAN FSQ LFQ BSQ GQ (Ours) bpp (# of tokens) 0.25 (2161024) 0.50 (2162048) 1.00 (2164096) UNet based ViT based PSNR LPIPS SSIM rFID PSNR LPIPS SSIM rFID 26.51 26.34 24.74 25.62 27. 29.21 29.29 26.90 27.88 30.17 32.06 32.38 28.31 30.50 32.47 0.125 0.075 0.164 0.086 0.059 0.052 0.047 0.107 0.059 0.039 0.026 0.025 0.074 0.032 0.023 0.748 0.756 0.722 0.754 0. 0.831 0.845 0.800 0.836 0.875 0.896 0.905 0.840 0.900 0.907 5.714 1.125 16.337 1.080 0.529 1.600 0.871 8.035 0.788 0.492 0.580 0.636 3.617 0.346 0.322 25.39 25.87 24.81 26.52 27. 27.86 28.83 27.87 28.44 30.42 31.32 31.58 26.67 31.60 31.71 0.103 0.109 0.143 0.083 0.061 0.062 0.055 0.068 0.051 0.037 0.032 0.026 0.105 0.027 0.024 0.740 0.751 0.725 0.793 0. 0.823 0.842 0.829 0.852 0.882 0.899 0.905 0.790 0.914 0.903 3.518 3.856 15.716 1.649 0.932 1.228 1.067 2.444 0.700 0.592 0.716 0.544 8.288 0.379 0.349 Table 2: Quantitive results on ImageNet dataset. Our GQ outperforms previous pre-trained VQVAEs with less training. Bold: best, : from paper, -: not available. Method VQGAN-Taming VQGAN-SD Llama-Gen-32 FlowMo-Hi GQ (Ours) BSQ GQ (Ours) bpp (# of tokens) 0.22 (2141024) 0.28 (2181024) PSNR LPIPS SSIM rFID ImageNet Training Epochs Params(M) 23.38 - 24.44 24.93 25.31 27.78 27. - - 0.064 0.073 0.064 0.063 0.054 - - 0.768 0.785 0.762 0.817 0.804 1.190 1.140 0.590 0.560 0.491 0.990 0. (OpenImages) (OpenImages) 40 300 40 200 40 67 83 70 945 82 175 87 models such as FlowMo, with less training epochs. Additionally, in Table. 11, we show that our GQ outperforms previous methods to discretize Gaussian VAE, including TokenBridge and ReVQ. Improving TokenBridge In Table 3, we compare TokenBridge (Wang et al., 2025) applied to vanilla Gaussian VAE and TDC constrainted Gaussian VAE. The results show that the quantization error of TokenBridge is quite large when applied to vanilla Gaussian VAE. In contrast, TDC significantly reduces the quantization error. Image Generation In Table 4, we evaluate the performance of GQ in terms of image generation. It is shown that compared with VQGAN, FSQ, LFQ and BSQ, our GQ has higher codebook usage and codebook entropy. In terms of generation FID and IS, our GQ is comparable to FSQ and better than other methods. Additionally, we train DiT (Peebles & Xie, 2022) with same model architecture and same training setting, using the Gaussian VAE with and without TDC. It is shown that for limited computation, auto-regressive generation is more efficient than diffusion generation, in terms Table 3: Quantitive results of improving TokenBridge based on UNet architecture. Our TDC improves TokenBridge significantly. Method Gaussian VAE Gaussian VAE (w/ TDC) TokenBridge TokenBridge (w/ TDC) GQ (Ours) bpp (# of tokens) 1.00 (-) 1.00 (216 4096) ImageNet validation COCO validation PSNR LPIPS SSIM rFID PSNR LPIPS SSIM rFID 32.73 32.61 28.24 31.67 32. 0.022 0.023 0.045 0.025 0.022 0.910 0.906 0.869 0.903 0.908 0.490 0.460 0.823 0.385 0. 32.64 32.69 28.19 31.56 32.53 0.018 0.019 0.043 0.022 0.020 0.917 0.919 0.878 0.910 0. 2.380 2.717 4.167 2.171 2."
        },
        {
            "title": "Preprint",
            "content": "Table 4: Quantitive results on class conditional image generation of ImageNet dataset. Our GQ achieves best codebook usage and competitive generation performance. Bold: best. Method Codebook Usage Codebook Entropy generation FID IS Diffusion Gaussian VAE w/o TDC Gaussian VAE w/ TDC Auto-regressive VQGAN FSQ LFQ BSQ TokenBridge GQ (Ours) - - 16.4% 94.3% 24.9% 99.8% 94.6% 100.0% - - 4.36 14.74 9.65 14.93 14.94 15.17 8.35 8. 8.01 7.33 7.73 7.82 7.82 7.67 202.19 205.94 151.40 224.88 142.09 221.64 198.24 230.79 of both FID and IS. This result shows that the conversion from Gaussian VAE to VQ-VAE facilities auto-regressive generation, which improves the efficiency of image generation. Complexity Compared with Gaussian VAE, the overhead of GQ is negligible (See Appendix D.10). 4.3 ABLATION STUDIES Effectiveness of Pre-trained Gaussian VAE It is possible to train vanilla VQ-VAE (Van Den Oord et al., 2017) using the same codebook as GQ, which is equivalent to VQ-VAE with fixed, Gaussian noise codebook. It is also possible to directly train the Gaussian VAE neural network with the GQ target in Eq. 8 using Gumbel-Softmax (Jang et al., 2016; Maddison et al., 2016). However, as shown in Table 5, both methods do not converge well. Furthermore, fine-tuning GQ after initializing it with pre-trained Gaussian VAE also has only marginal effect on performance. These results indicate that GQs conversion from pre-trained Gaussian VAE is necessary and sufficient. Table 5: The effect of pre-trained Gaussian VAE. Converting GQ from pre-trained Gaussian VAE is better than training GQ from scratch. Method Training target bpp PSNR LPIPS SSIM rFID Vanilla VQ-VAE GQ (from scratch) GQ (from scratch) Gumbel-softmax Eq. 8 GQ (finetune from Gaussian VAE) Gumbel-softmax Eq. 8 GQ (convert from Gaussian VAE) no 1.00 8.50 29.65 32.45 32. 0.763 0.044 0.022 0.023 0.156 0.866 0.905 0.907 360.597 0.928 0.264 0.327 Effectiveness and Alternatives of TDC To demonstrate the necessity of TDC in Eq. 6, we train vanilla Gaussian VAE without TDC. As shown in Table 6, the mean DKL(2) of the vanilla Gaussian VAE is close to that of the Gaussian VAE with TDC (3.99 vs. 4.26 bits). However, the range of DKL(2) is much wider for the vanilla model (0.2627.29 vs. 2.935.63 bits). Although the reconstruction performance of the two Gaussian VAEs is very similar (PSNR: 32.73 vs. 32.61 dB, rFID: 0.490 vs. 0.460), GQ with TDC outperforms GQ without TDC by large margin (PSNR: 31.25 vs. 26.43 dB, rFID: 0.372 vs. 0.978). This demonstrates that TDC is necessary. Alternatives to TDC are the MIRACLE heuristic (Havasi et al., 2018a) and IsoKL (Flamich et al., 2022). MIRACLE is not that effective in terms of controlling the range is outperformed by TDC (PSNR 29.48 vs. 32.11 dB). On the other hand, IsoKL imposes stricter constraint by requiring that DKL is exactly the same across all dimensions. Iso-KL enforece the constraint well but has inferior performance (PSNR 30.45 vs. 32.11 dB). This is because IsoKL is not numerically stable and it discards the solution with σ2 > 1. In Appendix B, we propose numerically stable version of Mean-KL (Lin et al., 2023), which is IsoKL that supports grouping (m > 1). However, it does not work well for ViT based models. Alternatives to GQ There are several stochastic alternatives to GQ, such as MRC, ORC, and coding (Havasi et al., 2018b; Theis & Yosri, 2021; Flamich et al., 2022; He et al., 2024). In Table 7, we compare these methods in terms of reconstruction quality. When applied to TDC-constrained Gaussian VAE, GQ achieves the best PSNR, SSIM, and rFID. Besides, when grouping = 1, GQ"
        },
        {
            "title": "Preprint",
            "content": "Table 6: The effect of adding constraints to Gaussian VAE. GQ is effective only on Gaussian VAE trained with TDC constraint. Methods Constraint DKL(2) mean, min-max Gaussian VAE GQ no - Gaussian VAE MIRACLE GQ - Gaussian VAE GQ IsoKL - Gaussian VAE TDC (ours) GQ - 3.99, 0.26-27.29 - 4.34,0.91-26.98 - 4.34, 4.24-4.38 - 4.26, 2.93-5.63 - log2 bpp 1.00 1.00 - 4 - - 4 - 4 1.00 1.00 1.00 1.00 1.06 1.00 PSNR LPIPS SSIM rFID 32.73 26.43 32.82 29.48 30.54 30.45 32.61 32. 0.022 0.834 0.023 0.039 0.878 0.878 0.023 0.023 0.910 0.054 0.910 0. 0.027 0.030 0.906 0.906 0.490 0.978 0.436 0.439 0.400 0.468 0.460 0. can be implemented using bisect search. This means that GQ is asymptotically faster than MRC, ORC and coding (See Appendix D.11). Table 7: Comparison between MRC methods and GQ on ImageNet dataset. GQ has better reconstruction quality and can be implemented asymptotically faster when grouping = 1. Methods Encoding / Decoding Complexity PSNR LPIPS SSIM rFID Gaussian VAE (w/ TDC) O(1)/O(1) MRC (original) MRC (A coding) ORC GQ (Ours) O(2DKL(q(ZiX)N (0,1)))/O(1) O(D(q(ZiX)N (0, 1)))/O(D(q(ZiX)N (0, 1))) O(2DKL(q(ZiX)N (0,1)))/O(1) O(DKL(q(ZiX)N (0, 1)))/O(1) 32. 32.09 32.09 32.09 32.11 0.023 0.023 0.023 0.023 0.023 0.906 0.460 0.906 0.906 0.906 0. 0.425 0.425 0.419 0.414 The TDC Parameters and Grouping Strategies See Appendix D."
        },
        {
            "title": "5 RELATED WORKS",
            "content": "VQ-VAE (Van Den Oord et al., 2017) is an autoencoder that compresses images into discrete tokens. Due to the discretization, it is impossible to be train directly using gradient descent. Various techniques have been proposed to address this, such as commitment loss (Van Den Oord et al., 2017), expectation maximization (EM) (Roy et al., 2018), the straight-through estimator (STE) (Bengio et al., 2013), and Gumbel-softmax (Jang et al., 2016; Maddison et al., 2016; Sønderby et al., 2017; Shi et al., 2024b). In addition, VQ-VAE is prone to codebook collapse. To mitigate this, various methods have been proposed, such as reducing the code dimension (Yu et al., 2021a; Sun et al., 2024a), product quantization (Zheng et al., 2022), residual quantization (Lee et al., 2022), dynamic quantization (Huang et al., 2023), multi-level quantization (Razavi et al., 2019), feature projection (Zhu et al., 2024), rotation codebooks (Fifty et al., 2024) and etc. (Yu et al., 2021b; Chiu et al., 2022; Takida et al., 2022; Zhang et al., 2023; Huh et al., 2023; Gautam et al., 2023; Goswami et al., 2024). More related to our work, some variants of VQ-VAE with fixed codebook emerge, such as FSQ (Mentzer et al., 2023), LFQ (Yu et al., 2023), and BSQ (Zhao et al., 2024). However, training tricks such as the straight-through estimator (STE) is still required. Among all previous works, TokenBridge (Wang et al., 2025) and ReVQ (Zhang et al., 2025) are most aligned with our objective. TokenBridge and ReVQ also convert pre-trained Gaussian VAE into VQ-VAE. However, TokenBridge and ReVQ do not constraint the divergence of Gaussian VAE, leading to suboptimal performance. Besides, ReVQ requires some training while our approach is training-free. Reverse Channel Coding See Section. 3.6."
        },
        {
            "title": "6 CONCLUSION & DISCUSSION",
            "content": "In this paper, we propose Gaussian Quant (GQ), an extremely simple yet effective technique that converts pre-trained Gaussian VAE into VQ-VAE without any additional training. Theoretically, we show that when the logarithm of the GQ codebook size exceeds the bits-back coding bitrate of the"
        },
        {
            "title": "Preprint",
            "content": "Gaussian VAE, small quantization error is achieved. In addition, we propose target divergence constraint (TDC) to implement GQ effectively. Empirically, we demonstrate that GQ outperforms previous discrete VAEs, such as VQGAN, FSQ, LFQ, and BSQ (Van Den Oord et al., 2017; Mentzer et al., 2023; Yu et al., 2023; Zhao et al., 2024). Furthermore, our TDC also improves the performance of TokenBridge (Wang et al., 2025). We limit our evaluation of GQ to the standard SD3.5 UNet (Esser et al., 2024) and the BSQ-ViT architecture. Additionally, we restrict the bpp range to 0.221.00, which extends the BSQs original range of 0.250.50 bpp. We acknowledge that there are several highly competitive VQ-VAEs that adopt multi-scale or residual architectures (Tian et al., 2024; Han et al., 2024) or study the low bpp regime (bpp 0.2) (Yu et al., 2024; Sargent et al., 2025; Zhang et al., 2025). However, in this paper, we use standard architectures and typical bpp range to focus on the core aspects of the quantization method. Additionally, we focus on achieving good trade-off between bitrate and reconstruction quality, leaving the complex relationship between reconstruction and generation performance to future works (Wang et al., 2024; Xiong et al., 2025; Hansen-Estruch et al., 2025)."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "The approach proposed in this paper focus on reconstruction of existing images with limited bitrate. As the model is essentially not generative, the ethic concerns is not obvious. Nevertheless, the GAN module in decoder might has negative effects, including issues related to mis-representation and trustworthiness. REPRODUCIBILITY STATEMENT For theoretical results, the proofs for all theorems are presented in Appendix A. For the experiments, we use publicly accessible datasets such as ImageNet (Deng et al., 2009). Implementation details and hyper-parameters are provided in Appendix C. Besides, we include the source code for reproducing the experimental results in the supplementary material."
        },
        {
            "title": "REFERENCES",
            "content": "Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013. Claude Brezinski. Extrapolation algorithms and pade approximations: historical survey. Applied numerical mathematics, 20(3):299318, 1996. Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1131511325, 2022. Sourav Chatterjee and Persi Diaconis. The sample size required in importance sampling. arXiv: Probability, 2015. URL https://api.semanticscholar.org/CorpusID:9827126. Chung-Cheng Chiu, James Qin, Yu Zhang, Jiahui Yu, and Yonghui Wu. Self-supervised learning with random-projection quantizer for speech recognition. In International Conference on Machine Learning, pp. 39153924. PMLR, 2022. Robert Corless, Gaston Gonnet, David EG Hare, David Jeffrey, and Donald Knuth. On the lambert function. Advances in Computational mathematics, 5:329359, 1996. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, K. Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248255, 2009. URL https://api.semanticscholar.org/CorpusID:572463 10. Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1286812878, 2020. URL https://api.semanticscholar.org/CorpusID: 229297973."
        },
        {
            "title": "Preprint",
            "content": "Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1287312883, 2021. Patrick Esser, Sumith Kulal, A. Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. ArXiv, abs/2403.03206, 2024. URL https://api.sema nticscholar.org/CorpusID:268247980. Christopher Fifty, Ronald G. Junkins, Dennis Duan, Aniketh Iger, Jerry Liu, Ehsan Amid, Sebastian Thrun, and Christopher Re. Restructuring vector quantization with the rotation trick. ArXiv, abs/2410.06424, 2024. URL https://api.semanticscholar.org/CorpusID: 273229218. Gergely Flamich, Marton Havasi, and Jose Miguel Hernandez-Lobato. Compressing images by encoding their latent representations with relative entropy coding. ArXiv, abs/2010.01185, 2020. URL https://api.semanticscholar.org/CorpusID:222132826. Gergely Flamich, Stratis Markou, and Jose Miguel Hernandez-Lobato. Fast relative entropy coding with a* coding. ArXiv, abs/2201.12857, 2022. URL https://api.semanticscholar. org/CorpusID:246430918. Tanmay Gautam, Reid Pryzant, Ziyi Yang, Chenguang Zhu, and Somayeh Sojoudi. Soft conarXiv preprint vex quantization: Revisiting vector quantization with convex optimization. arXiv:2310.03004, 2023. Nabarun Goswami, Yusuke Mukuta, and Tatsuya Harada. Hypervq: Mlr-based vector quantization in hyperbolic space. arXiv preprint arXiv:2403.13015, 2024. Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis. ArXiv, abs/2412.04431, 2024. URL https://api.semanticscholar.org/CorpusID: 274515181. Philippe Hansen-Estruch, David Yan, Ching-Yao Chung, Orr Zohar, Jialiang Wang, Tingbo Hou, Tao Xu, Sriram Vishwanath, Peter Vajda, and Xinlei Chen. Learnings from scaling visual tokenizers for reconstruction and generation. arXiv preprint arXiv:2501.09755, 2025. Prahladh Harsha, Rahul Jain, David McAllester, and Jaikumar Radhakrishnan. The communicaIn Twenty-Second Annual IEEE Conference on Computational tion complexity of correlation. Complexity (CCC07), pp. 1023. IEEE, 2007. Marton Havasi, Robert Peharz, and Jose Miguel Hernandez-Lobato. Minimal random code learning: Getting bits back from compressed model parameters. ArXiv, abs/1810.00440, 2018a. URL https://api.semanticscholar.org/CorpusID:52901536. Marton Havasi, Robert Peharz, and Jose Miguel Hernandez-Lobato. Minimal random code learning: Getting bits back from compressed model parameters. arXiv preprint arXiv:1810.00440, 2018b. Jiajun He, Gergely Flamich, and Jose Miguel Hernandez-Lobato. Accelerating relative entropy coding with space partitioning. Advances in Neural Information Processing Systems, 37:75791 75828, 2024. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. In Neural Gans trained by two time-scale update rule converge to local nash equilibrium. Information Processing Systems, 2017. URL https://api.semanticscholar.org/Co rpusID:326772. Geoffrey Hinton and Drew Van Camp. Keeping the neural networks simple by minimizing the description length of the weights. In Proceedings of the sixth annual conference on Computational learning theory, pp. 513, 1993."
        },
        {
            "title": "Preprint",
            "content": "Mengqi Huang, Zhendong Mao, Zhuowei Chen, and Yongdong Zhang. Towards accurate image Improved autoregressive image generation with dynamic vector quantization. 2023 coding: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2259622605, 2023. URL https://api.semanticscholar.org/CorpusID:258823089. Minyoung Huh, Brian Cheung, Pulkit Agrawal, and Phillip Isola. Straightening out the straightthrough estimator: Overcoming optimization challenges in vector quantized networks. In International Conference on Machine Learning, pp. 1409614113. PMLR, 2023. Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016. Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization. CoRR, abs/1412.6980, 2014. URL https://api.semanticscholar.org/CorpusID: 6628106. Diederik Kingma, Max Welling, et al. Auto-encoding variational bayes, 2013. Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1151311522, 2022. URL https://api.semanticscho lar.org/CorpusID:247244535. Cheuk Ting Li and Abbas El Gamal. Strong functional representation lemma and applications to coding theorems. IEEE Transactions on Information Theory, 64(11):69676978, 2018. Jihao Andreas Lin, Gergely Flamich, and Jose Miguel Hernandez-Lobato. Minimal random code learning with mean-kl parameterization. ArXiv, abs/2307.07816, 2023. URL https://api. semanticscholar.org/CorpusID:259936972. Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr In European Dollar, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. Conference on Computer Vision, 2014. URL https://api.semanticscholar.org/Co rpusID:14113767. Chris Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016. Fabian Mentzer, George Toderici, Michael Tschannen, and Eirikur Agustsson. High-fidelity generative image compression. ArXiv, abs/2006.09965, 2020. URL https://api.semanticsc holar.org/CorpusID:219721015. Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen. Finite scalar quantization: Vq-vae made simple. arXiv preprint arXiv:2309.15505, 2023. William S. Peebles and Saining Xie. Scalable diffusion models with transformers. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 41724182, 2022. URL https: //api.semanticscholar.org/CorpusID:254854389. Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. In Neural Information Processing Systems, 2019. URL https://api.semantic scholar.org/CorpusID:173990382. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Aurko Roy, Ashish Vaswani, Arvind Neelakantan, and Niki Parmar. Theory and experiments on vector quantized autoencoders. arXiv preprint arXiv:1805.11063, 2018. Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. ArXiv, abs/1606.03498, 2016. URL https://api.se manticscholar.org/CorpusID:1687220."
        },
        {
            "title": "Preprint",
            "content": "Kyle Sargent, Kyle Hsu, Justin Johnson, Fei-Fei Li, and Jiajun Wu. Flow to the mode: Modeseeking diffusion autoencoders for state-of-the-art image tokenization. ArXiv, abs/2503.11056, 2025. URL https://api.semanticscholar.org/CorpusID:277043290. Fengyuan Shi, Zhuoyan Luo, Yixiao Ge, Yujiu Yang, Ying Shan, and Limin Wang. Scalable image tokenization with index backpropagation quantization. 2024a. URL https://api.semant icscholar.org/CorpusID:274445794. Fengyuan Shi, Zhuoyan Luo, Yixiao Ge, Yujiu Yang, Ying Shan, and Limin Wang. Taming scalable visual tokenizer for autoregressive image generation. ArXiv, abs/2412.02692, 2024b. URL http s://api.semanticscholar.org/CorpusID:277772880. Casper Kaae Sønderby, Ben Poole, and Andriy Mnih. Continuous relaxation training of discrete latent variable image models. In Beysian DeepLearning workshop, NIPS, volume 201, 2017. Yang Song, Jascha Narain Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. ArXiv, abs/2011.13456, 2020. URL https://api.semanticscholar.org/CorpusID: 227209335. Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. ArXiv, abs/2406.06525, 2024a. URL https://api.semanticscholar.org/CorpusID:270371603. Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024b. Yuhta Takida, Takashi Shibuya, WeiHsiang Liao, Chieh-Hsin Lai, Junki Ohmura, Toshimitsu Uesaka, Naoki Murata, Shusuke Takahashi, Toshiyuki Kumakura, and Yuki Mitsufuji. Sq-vae: Variational bayes on discrete representation with self-annealed stochastic quantization. arXiv preprint arXiv:2205.07547, 2022. Lucas Theis and Noureldin Yosri. Algorithms for the communication of samples. ArXiv, abs/2110.12805, 2021. URL https://api.semanticscholar.org/CorpusID: 239768821. Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. ArXiv, abs/2404.02905, 2024. URL https: //api.semanticscholar.org/CorpusID:268876071. Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cris tian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melissa Hall Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. ArXiv, abs/2307.09288, 2023. URL https://api.semanticscho lar.org/CorpusID:259950998. James Townsend, Tom Bird, and David Barber. Practical lossless compression with latent variables using bits back coding. arXiv preprint arXiv:1901.04866, 2019. Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017."
        },
        {
            "title": "Preprint",
            "content": "Laurens van der Maaten and Geoffrey E. Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9:25792605, 2008. URL https://api.semanticscholar.org/ CorpusID:5855042. Jeremy Vonderfecht and Feng Liu. Lossy compression with pretrained diffusion models. ArXiv, abs/2501.09815, 2025. URL https://api.semanticscholar.org/CorpusID: 275606601. Luting Wang, Yang Zhao, Zijian Zhang, Jiashi Feng, Si Liu, and Bingyi Kang. Image understanding makes for good tokenizer for image generation. Advances in Neural Information Processing Systems, 37:3101531035, 2024. Yuqing Wang, Zhijie Lin, Yao Teng, Yuanzhi Zhu, Shuhuai Ren, Jiashi Feng, and Xihui Liu. Bridging continuous and discrete tokens for autoregressive visual generation. arXiv preprint arXiv:2503.16430, 2025. Zhou Wang, Alan Conrad Bovik, Hamid R. Sheikh, and Eero P. Simoncelli. Image quality asIEEE Transactions on Image Processsessment: from error visibility to structural similarity. ing, 13:600612, 2004. URL https://api.semanticscholar.org/CorpusID: 207761262. Tianwei Xiong, Jun Hao Liew, Zilong Huang, Jiashi Feng, and Xihui Liu. Gigatok: Scaling visual tokenizers to 3 billion parameters for autoregressive image generation. arXiv preprint arXiv:2504.08736, 2025. Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. ArXiv, abs/2110.04627, 2021a. URL https://api.semanticscholar.org/Corpus ID:238582653. Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627, 2021b. Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusion tokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is worth 32 tokens for reconstruction and generation. ArXiv, abs/2406.07550, 2024. URL https://api.semanticscholar.org/CorpusID:270379986. Borui Zhang, Qihang Rao, Wenzhao Zheng, Jie Zhou, and Jiwen Lu. Quantize-then-rectify: Efficient vq-vae training, 2025. URL https://arxiv.org/abs/2507.10547. Jiahui Zhang, Fangneng Zhan, Christian Theobalt, and Shijian Lu. Regularized vector quantization for tokenized image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1846718476, 2023. Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 586595, 2018. URL https://api.semanticschola r.org/CorpusID:4766599. Yue Zhao, Yuanjun Xiong, and Philipp Krahenbuhl. Image and video tokenization with binary spherical quantization. arXiv preprint arXiv:2406.07548, 2024. Chuanxia Zheng, Long Tung Vuong, Jianfei Cai, and Dinh Q. Phung. Movq: Modulating quantized vectors for high-fidelity image generation. ArXiv, abs/2209.09002, 2022. URL https://ap i.semanticscholar.org/CorpusID:252367709. Lei Zhu, Fangyun Wei, Yanye Lu, and Dong Chen. Scaling the codebook size of vqgan to 100,000 with utilization rate of 99%. ArXiv, abs/2406.11837, 2024. URL https://api.semantic scholar.org/CorpusID:270560634."
        },
        {
            "title": "A PROOF OF MAIN RESULTS",
            "content": "Theorem 1. Denote the mean and standard deviation of q(ZiX = x) as µi, σi, and assuming µi, σi product and sum are bounded by µiσj c1, µi + σj c2, then the probability of quantization error ˆzi µi σi decays double exponentially to the amount of nats that codebook bitrate log exceeds the bits-back coding bitrate, i.e., when log = DKL(q(ZiX)N (0, 1)) + t, Pr{ˆzi µi σi} exp (et ec10.5 ). (6) (cid:114) 2 π (cid:124) (cid:123)(cid:122) constant (cid:125) Proof. Denote the cumulative distribution function (CDF) of (0, 1) as Φ, and probability density function (PDF) of (0, 1) as ϕ, then we need to consider the probability that no samples falls between [µi σi, µi + σi], which is Pr{ˆzi µi σi} = (1 (Φ(µi + σi) Φ(µi σi)))K. (10) Now we use the Bernoulli inequality, that R, 1+y ey. Let = (Φ(µi +σi)Φ(µi σi)), we have 1 (Φ(µi + σi) Φ(µi σi)) exp ((Φ(µi + σi) Φ(µi σi))). (11) Taking Eq. 11 into Eq. 10, we have Pr{ˆzi µi σi} exp ((Φ(µi + σi) Φ(µi σi)))K (12) = exp (K (Φ(µi + σi) Φ(µi σi))) = exp (K (cid:90) µi+σi µiσi ϕ(x)dx). By integral mean value theorem, [µi σi, µi + σi], such that (cid:90) µi+σi µiσi ϕ(x)dx = 2σiϕ(x). And then we have Pr{ˆzi µi σi} exp (K 2σiϕ(x)). (13) (14) Next, we consider three cases: µi σi 0, µi + σi 0, and µi σi 0 µi + σi. First, consider the case when µi σi 0. Obviously we have ϕ(µi + σi) ϕ(x), and we have Pr{ˆzi µi σi} exp (K 2σiϕ(µi + σi)) = exp (K = exp (K = exp (K (cid:114) 2 π (cid:114) 2 π (cid:114) 2 π σie 1 2 (µi+σi)2 ) 2 (µ2 +σ2 log σ21.0+1.0)µiσi) eDKL(q(ZiX)N (0,1))µiσi0.5). (15) Notice that as µi σi 0, σi > 0, we must have µiσi > 0, then Pr{ˆzi µi σi} exp (K (cid:114) 2 π eDKL(q(ZiX)N (0,1))µmaxσmax0.5). (16)"
        },
        {
            "title": "Preprint",
            "content": "Similarly, we can show similar result for µi + σi 0. Obviously we have ϕ(µi σi) ϕ(x), and we have Pr{ˆzi µi σi} exp (K 2σiϕ(µi σi)) = exp (K = exp (K = exp (K exp (K (cid:114) 2 π (cid:114) 2 π (cid:114) 2 π (cid:114) 2 π σie 1 2 (µi+σi)2 ) 1 2 (µ +σ2 log σ21.0+1.0)+µiσi ) eDKL(q(ZiX)N (0,1))+µiσi0.5) eDKL(q(ZiX)N (0,1))µmaxσmax0.5) (17) Now, consider the case when µi σi < 0 < µi + σi, obviously we must have either ϕ(µi σi) ϕ(x), or ϕ(µi + σi) ϕ(x). If ϕ(µi + σi) ϕ(x), then the result is the same as µ σi 0. If ϕ(µi σi) ϕ(x), then the result is the same as µ + σi 0. Therefore, for all µi, σi, we have Pr{ˆzi µi σi} exp (K Taking the value of in, we have the result Pr{ˆzi µi σi} exp (K = exp (et = exp (et (cid:114) 2 π (cid:114) 2 π (cid:114) 2 π (cid:114) 2 π eDKL(q(ZiX)N (0,1))µmaxσmax0.5). (18) eDKL(q(ZiX)N (0,1))µmaxσmax0.5) eµmaxσmax0.5) ec10.5). (19) Theorem 2. The probability of quantization error ˆzi µi σi increase exponentially to the amount of nats that codebook bitrate log lower than the bits-back coding bitrate, i.e., when log = DKL(q(ZiX)N (0, 1)) t, Pr{ˆzi µi σi} 1 et (cid:114) 2 π (cid:124) 20.5 . (7) e0.5c2 (cid:123)(cid:122) constant (cid:125) Proof. Similar to the proof of Theorem. 1, we have Pr(ˆzi µi σi) = (1 (Φ(µi + σi) Φ(µi σi)))K. (20) Now we use an inequality, that (0, 1), N, 1, (1 y)K 1 Ky. This is due to the fact that (1 y)K is convex in (0, 1), and 1 Ky is tangent line at = 0. With this inequality, we have (1 (Φ(µi + σi) Φ(µi σi)))K 1 K(Φ(µi + σi) Φ(µi σi)). (21)"
        },
        {
            "title": "Preprint",
            "content": "Again, we can use integral mean value theorem, and find out that when µi σi 0, Pr(ˆzi µi σi) 1 K(Φ(µi + σi) Φ(µi σi)) 2 (xiσi)2 σie 1 = 1 1 K2σiϕ(µi σi) (cid:114) 2 π (cid:114) 2 π (cid:114) 2 π = 1 = 1 1 2 (x2 +σ2 log σ 1.0)+µiσi0.5 eDKL(q(ZiX)N (0,1))+µiσi0.5 1 KeDKL(q(ZiX)N (0,1)) (cid:114) 2 π e0.5(µi+σi)20.5 (22) Similar results can be obtained for µi + σi 0. For the case that µi σi 0 µi + σi, we have Pr(ˆzi µi σi) 1 K(Φ(µi + σi) Φ(µi σi)) 1 K2σiϕ(0) = 1 = 1 = 1 (cid:114) 2 π (cid:114) 2 π (cid:114) 2 π σie 1 2 (0)2 1 2 (µ2 +σ2 log σ 1.0)0.5+0.5(µ2 +σ2 ) eDKL(q(ZiX)N (0,1))+0.5(µ2 +σ2 )+µiσi0.5 = 1 KeDKL(q(ZiX)N (0,1)) (cid:114) 2 π e0.5(µi+σi)20.5 (23) Taking the value of = eDKL(q(ZiN (0,1)))t , we have the result Pr(ˆzi µi σi) 1 eDKL(q(ZiN (0,1)))tDKL(q(ZiX)N (0,1)) 1 et (cid:114) 2 π e0.5c2 20.5. (cid:114) 2 π e0.5(µi+σi)20.5 (24) To better illustrate the significance of these bounds, we provide practical example. We evaluate the ImageNet validation dataset using pre-trained Gaussian VAE and compute that c1 = 8.12 and c2 = 1.50. We then visualize the upper bound and lower bound of Prˆz µi σi in Fig. 3. The results show that when the codebook bitrate exceeds the bits-back coding bitrate by approximately 10 nats, the probability of large quantization error diminishes to zero. Conversely, when the codebook bitrate is smaller than the bits-back coding bitrate, the probability of large quantization error increases. STABLE MEAN-KL PARAMETRIZATION We investigate an alternative to TDC, namely the Mean-KL parametrization (Lin et al., 2023), which is considered to be easier to train than TDC since it does not require the construction of an empirical R(λ) model. B.1 MEAN-KL PARAMETRIZATION The Mean-KL parametrization (Lin et al., 2023) supports grouping with > 1. Its neural network output consists of two m-dimensional tensors, ˆγi : + and τ : + m, which allocate the DKL target across the dimensions and determine the mean, respectively. More specifically, the"
        },
        {
            "title": "Preprint",
            "content": "Figure 3: visualization of large quantization error lowerbound and upperbound with ImageNet validation dataset. Mean-KL parametrization determines the mean µi:i+m and variance σ2 denotes the principal branch of the Lambert function: i:i+m as follows, where W() γi:i+m = Softmax(ˆγi:i+m), κi:i+m = γi:i+mK, µ1:m = (cid:112)2κi:i+mtanh(τi:i+m), i:i+m = W( exp(µ2 σ2 i:i+m 2κi:i+m 1.0)). (25) The Mean-KL parametrization is designed for model compression. When directly applied to Gaussian VAEs, two typical cases may arise, as shown in Table 8, both of which can result in not-anumber (NaN) error in floating-point computations. Table 8: Two typical types of NaN in Mean-KL parametrization. µi -2.7286 0.0013 σ2 κi 3.7227 NaN 9.1458107 NaN B.2 STABLE MEAN-KL PARAMETRIZATION It is evident that the two types of NaN errors are caused by excessively large values of µi and excessively small values of κi, respectively. To address this numerical issue, we propose the Stable Mean-KL parametrization, which introduces two regularization parameters, r1 = 0.1 and r2 = 0.01. The parameter r1 ensures that each κi r1/m, while r2 shrinks µi towards 0. κi:i+m = γi:i+m(K r1) + r1/m, µ1:m = (cid:112)2κi:i+mtanh(τi:i+m)(1 r2), (26) B.3 RESULTS OF STABLE MEAN-KL PARAMETRIZATION In Table 9, we present the results of the Stable Mean-KL parametrization. For UNet-based models, Stable Mean-KL achieves performance comparable to TDC. However, for ViT-based models, Stable Mean-KL performs significantly worse than TDC. Since Stable Mean-KL does not consistently outperform TDC, we choose to use TDC for the final model. Nonetheless, if only UNet-based models are required, Stable Mean-KL can be an effective alternative to TDC, as it does not require an empirical R(λ) model and is significantly simpler to train."
        },
        {
            "title": "Preprint",
            "content": "Table 9: Quantitive results on ImageNet validation dataset. Method GQ (Mean-KL) GQ (Stable Mean-KL) GQ (TDC) bpp (# of tokens) 1.00 (2164096) UNet based ViT based PSNR LPIPS SSIM rFID PSNR LPIPS SSIM rFID NaN 32.35 32.47 NaN 0.023 0.023 NaN 0.905 0.907 NaN 0.280 0.322 NaN 30.80 31. NaN 0.030 0.024 NaN 0.891 0.903 NaN 0.556 0."
        },
        {
            "title": "C IMPLEMENTATION DETAILS",
            "content": "C.1 DETAILS OF TRAINING AND DISTORTION OBJECTIVE We train all VQ-VAEs on the ImageNet validation dataset using 8 H100 GPUs for approximately 24 hours. For UNet models, we train each model for 30 epochs using the ADAM (Kingma & Ba, 2014) optimizer with batch size of 128 and learning rate of 1 104. For ViT models, we train each model for 40 epochs using the ADAM optimizer with batch size of 256 and learning rate of 4 107. All VQ-VAEs are trained using the following distortion objective, which corresponds to the classical VQ-GAN (Esser et al., 2020) objective employed in the Stable Diffusion VAE (Rombach et al., 2022). (X, g(z)) = LM SE(X, g(z)) + w1LLP IP S(X, g(z)) + w2LGAN (g(z)). Following the implementation of Stable Diffusion, we set w1 = 1.0 and w2 = 0.75 for UNet models. Consistent with the implementation of BSQ (Zhao et al., 2024), we set w1 = 0.1 and w2 = 0.1 for ViT models. (27) For the image generation model, we first train all VQ-VAEs using images of size 128 128, following the same settings as described above. Subsequently, we train the auto-regressive transformer for image generation using the implementation of IBQ (Shi et al., 2024b) with Llama-base transformer architecture. The transformer has vocabulary size of 216, 16 layers, 16 attention heads, and an embedding dimension of 1024. We train the transformer for 100 epochs using the ADAM optimizer with learning rate of 3 104 and batch size of 512. C.2 DETAILS OF GROUPING STRATEGIES We extend the notation from main text. We group tokens into one large token with codebook size K. For each quantization output ˆzi, we denote the corresponding index in the codebook as Ii. And we have cIi = ˆzi. Post Quantization (PQ) The Post Quantization (PQ) grouping strategy happens after GQ. We first train Gaussian VAE with codebook size for each dimension 1/m. Next, we obtain GQ tokens I1:d. Then, we group those tokens into d/m groups with group size of m. Denote the group index as = 0, . . . , d/m 1, then each group can be denoted as {Igm+l}, = 1, . . . , m. In that case, we have max{Igm+l} 1/m. Then, we can view each Igm+l as an integer in 1/m-base numerical system. Then, aggregating tokens {Igm+l} into one large token is as easy as concatenating tokens into larger integer in K-based numerical system: = (cid:80)m l=1 Igm+lK (li)/m K. Post Training (PT) The Post Training (PT) grouping strategy happens after the training of Gaussian VAE. We still start with Gaussian VAE with target codebook size for each dimension 1/m. Next, instead of performing GQ for each dimension, we consider the following dimension GQ for each group g: ˆzgm:gm+m = arg min cj {c1:K } (µgm:gm+m cj)/σgm:gm+m, where c1:K (0, Im). (28) In fact, we can view one dimension GQ as max likelihood: ˆzi = arg max cj {c1:K } log q(Zi = cjX), where c1:K (0, 1)."
        },
        {
            "title": "Preprint",
            "content": "Then we can extend the max likelihood to dimension, which is equivalent to the basic version of PT in Eq. 29. ˆzgm:gm+m = arg max cj {c1:K } log q(Zgm:gm+m = cjX), where c1:K (0, Im). When group size is large, we observe that vanilla PT in Eq. 29 leads to codebook collapse (See Table. 13) and decay in performance. Therefore, we include regularization term weighted by ω: ˆzgm:gm+m = arg min cj {c1:K } (µgm:gm+m cj)/σgm:gm+m ωcj, where c1:K (0, Im). (29) Training Aware (TA) The Training Aware (TR) grouping strategy happens before the training of Gaussian VAE. More specifically, we directly interoperate grouping with the TDC, and introduce the group TDC as follows: Lm DC = d/m (cid:88) λg (cid:88) DKL(q(Zgm+lX)N (0, 1)) + (X, g(z)), where λg = λmin, (cid:80)m λmean, (cid:80)m λmax, (cid:80)m l=1 g=0 l=1 DKL(2)(q(Zgm+lX)N (0, 1)) < log2 α bits, l=1 DKL(2)(q(Zgm+lX)N (0, 1)) [log2 0.5, log2 + 0.5] bits, l=1 DKL(2)(q(Zgm+lX)N (0, 1)) > log2 + α bits. (30) Besides, the λ update heuristic should also consider dimension as group: λmin = λmin β if ming{ (cid:88) l=1 λmean = λmean β if meang{ DKL(2)(q(Zgm+lX)N (0, 1))} > log2 α else λmin/β, (cid:88) l=1 DKL(2)(q(Zgm+lX)N (0, 1))} > log2 else λmean/β, λmax = λmax β if maxg{ (cid:88) l=1 DKL(2)(q(Zgm+lX)N (0, 1))} > log2 + α else λmax/β. (31) C.3 DETAILS OF HYPER-PARAMETERS Below, we describe the implementation details along with the definition of hyperparameters for each method. In Table 10, we list the values of these hyperparameters for different bits-per-pixel (bpp) settings. VQGAN (Van Den Oord et al., 2017) We adopt the factorized codebook VQGAN variant following (Zheng et al., 2022). For each codebook, we use codebook size of = 216 and group dimension of = 16. The number of codebooks varies depending on the bitrate. Additionally, we use codebook loss weight of λ = 1.0 and commitment loss weight of ζ = 0.25. FSQ (Mentzer et al., 2023) The only parameter of FSQ is the codebook list l, which represents the quantization level for each integer value. We set each unit value to 24 = 16, and populate with the appropriate number of 16s according to the desired bitrate. LFQ (Yu et al., 2023) For LFQ at 0.25 bpp, we follow the original paper and split large codebook of size 216 into = 2 smaller codebooks, each with = 28 and codebook dimension of = 8. We use an entropy loss weight of λ = 0.1 and commitment loss weight of ζ = 0.025. BSQ (Zhao et al., 2024) We fix the size of each BSQ codebook to 21, with group dimension of = 1, and vary the number of codebooks according to the desired bitrate. For the entropy penalization parameter, we set λ = 0.1, following the official implementation. GQ We use TR grouping with fixed codebook size of = 216. Each group has dimension m, and there are groups in total, such that = 16. The value of varies depending on the bitrate."
        },
        {
            "title": "Preprint",
            "content": "Table 10: Details of Hyper-parameter values. bpp 0.25 0.50 1.00 0.25 0.50 1.00 0.25 0.50 1.00 0.25 0.50 1. 0.25 0.50 1.00 VQ FSQ LFQ BSQ GQ Hyper-parameters = 216, = 1, = 16, λ = 1.0, ζ = 0.25 = 216, = 2, = 16, λ = 1.0, ζ = 0.25 = 216, = 4, = 16, λ = 1.0, ζ = 0.25 = {16, 16, 16, 16} = {16, 16, 16, 16, 16, 16, 16, 16} = {16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16} = 28, = 2, = 8, λ = 0.1, ζ = 0.025 = 28, = 4, = 8, λ = 0.1, ζ = 0.025 = 28, = 8, = 8, λ = 0.1, ζ = 0.025 = 21, = 16, = 1, λ = 0.1 = 21, = 32, = 1, λ = 0.1 = 21, = 64, = 1, λ = 0.1 = 216, = 1, = 16, ω = 2.0 = 216, = 2, = 8, ω = 2.0 = 216, = 4, = 4, ω = 0."
        },
        {
            "title": "D ADDITIONAL QUANTITIVE RESULTS",
            "content": "D.1 COMPARISON TO OTHER CONVERSION METHODS In Table. 11, we compare our GQ to TokenBridge (Wang et al., 2025) and ReVQ (Zhang et al., 2025), which also convert Gaussian VAE into VQ-VAE. Those results are from their original paper. It is shown that GQ has best reconstruction metrics. Besides, only GQ has theoretical guarantee. Table 11: Quantitive results on ImageNet dataset. Bold: best, : from paper, -: not available. Method Training free Theoretical guarantee OpenMagViT-V2 TokenBridge ReVQ-256T GQ (Ours) No Yes No Yes No No No Yes bpp (# of tokens) 0.07 (218 256) 0.375 (26 4096) 0.07 (218 256) 0.07 (218 256) PSNR LPIPS SSIM rFID 21.63 - 21.96 22.30 0.111 - 0.121 0.116 0.640 - 0.640 0.642 1.17 1.11 2.05 1.04 D.2 THE TDC PARAMETERS In Table 12, we show the effect of different TDC parameters. It is shown that β = 1.001, 1.01, 1.1 does not make significant different on result. However, setting α > 0.5 is harmful to performance. Table 12: Ablation study on TDC parameters. α 0.5 0.1 1.0 0.5 0.5 β PSNR LPIPS SSIM rFID 1.01 1.01 1.01 1.1 1.001 27.61 27.56 27.61 27.63 27.48 0.059 0.058 0.063 0.060 0.058 0.807 0.812 0.811 0.809 0. 0.529 0.551 0.701 0.534 0.510 D.3 EFFECTIVENESS OF GROUPING STRATEGIES In Table 13, we evaluate the effect of token grouping techniques. The scenario we consider involves grouping four 4-bit tokens into single 16-bit token, which is reasonable setting for autoregressive"
        },
        {
            "title": "Preprint",
            "content": "generation. The results show that PQ has no effect on reconstruction performance, while PT provides some improvements in PSNR and SSIM. In contrast, TR, which involves training the Gaussian VAE with grouping target, achieves the best reconstruction performance. Table 13: Effects of token grouping on ImageNet dataset. TR strategy has best reconstruction performance for grouping = 4. Grouping log2 DKL(2) mean, min-max Gaussian VAE (w/ TDC) GQ GQ GQ no (m=1) no (m=1) PQ (m=4) PT (m=4) Gaussian VAE (w/ TDC) TR (m=4) TR (m=4) GQ - 4 16 16 - 16 4.26, 2.93-5.63 - - - 15.99, 14.81-17.54 - bpp 1.06 1.00 1.00 1.00 1.00 1.00 PSNR LPIPS SSIM rFID 32.61 32.11 32.11 32. 32.62 32.47 0.023 0.023 0.023 0.023 0.023 0.023 0.906 0.906 0.906 0.907 0.909 0.907 0.460 0.414 0.414 0. 0.331 0.322 Additionally, in Table 14, we show the effect of the regularization parameter ω for PT and TR. For high bitrates, such as 1.00 bpp, regularization is not required; in other words, setting ω = 0.0 yields the good enough codebook usage and rFID. For lower bitrates, such as 0.50 bpp, ω = 0.0 leads to codebook collapse, while ω = 2.0 achieves the best codebook entropy and rFID. Table 14: Ablation Study on regularization ω. bpp 0.50 1. ω 0.0 1.0 2.0 4.0 0.0 1.0 2.0 4.0 Codebook Usage Codebook Entropy PSNR LPIPS SSIM rFID 99.3% 100.0% 100.0% 100.0% 100.0% 100.0% 100.0% 100.0% 14.96 15.14 15.22 14.81 15.05 15.05 15.06 15.07 30.00 30.35 30.17 28. 32.47 32.47 32.47 32.44 0.044 0.040 0.039 0.061 0.023 0.023 0.024 0.024 0.873 0.877 0.875 0.846 0.907 0.907 0.907 0.907 0.783 0.589 0.492 1. 0.322 0.327 0.332 0.343 Table 15: Quantitive results on COCO 2017 dataset. Bold: best. Method VQGAN FSQ LFQ BSQ GQ (Ours) VQGAN FSQ LFQ BSQ GQ (Ours) VQGAN FSQ LFQ BSQ GQ (Ours) bpp (# of tokens) 0.25 (2161024) 0.50 (2162048) 1.00 (2164096) UNet based ViT based PSNR LPIPS SSIM rFID PSNR LPIPS SSIM rFID 26.25 26.01 24.60 25.29 27.29 29.06 29.08 26.47 27.58 30.14 31.97 32.30 28.16 30.33 32.36 0.099 0.072 0.164 0.085 0.057 0.049 0.043 0.103 0.057 0.037 0.024 0.022 0.072 0.031 0. 0.756 0.767 0.722 0.763 0.816 0.839 0.855 0.805 0.844 0.877 0.901 0.917 0.845 0.906 0.915 14.110 5.451 32.789 5.803 3.797 6.616 4.008 17.508 4.465 3.116 3.455 2.797 11.121 2.638 1. 25.11 25.85 24.46 26.15 27.55 27.83 28.51 27.54 28.19 30.18 31.07 31.48 26.36 31.38 31.50 0.106 0.112 0.143 0.082 0.060 0.058 0.053 0.067 0.049 0.034 0.029 0.023 0.103 0.026 0. 0.747 0.765 0.729 0.798 0.830 0.832 0.851 0.833 0.858 0.887 0.904 0.911 0.794 0.918 0.908 11.231 11.213 29.975 7.034 5.305 5.461 5.390 8.700 4.587 3.616 3.494 3.045 20.381 2.835 2. D.4 THE QUANTIZATION ERROR IN PIXEL SPACE Previously we examine the quantization error in latent space. We can further discuss the quantization error in pixel space given the decoder is smooth. More specifically, we have:"
        },
        {
            "title": "Preprint",
            "content": "Table 16: The effect of quantization in pixel space. Latents µi = E[ZiX] (posterior mean) zi q(ZiX) (Gaussian sample) DKL(q(ZiX)N (0, 1)) = 4.26 bits ˆzi (GQ) bits per latent log2 = 4 bits 16 bits PSNR LPIPS SSIM rFID 32.92 32.61 32.11 0.020 0.021 0.023 0.913 0.911 0.906 0.46 0.46 0. Corollary 3. Following the setting in Theorem 1, and assuming the decoder g(.) satisfy g(x1) g(x2) c3x1 x2, we have: r{g(ˆzi) g(µi) c3σi} exp(et (cid:114) 2 π ec10.5). (32) Proof. As g(ˆzi) g(µi) c3ˆzi µi, we have r{g(ˆzi) g(µi) c3σi} r{c3 ˆzi µi c3σi} = r{ ˆzi µi σi}. We can see that theoretically, the quantization error can be magnified by the Lipschitz constant c3. However, we note that this is not significant issue in practice. As shown in the Table 16, the actual loss of quality caused by GQ remains reasonable. D.5 ROBUSTNESS OF CODEBOOK TO RANDOM SEED As the codebook is usually large enough (216) to compensate for the randomness. We provide additional in Table 17 showing that the random seed has little effect on reconstruction performance. We used three continuos random seeds without cherry-picking. It is clear that the performance of GQ is not affected by randomness. Table 17: The effect of codebook randomness on the performance of GQ. Random Seed PSNR LPIPS SSIM rFID 42 43 44 27.61 27.61 27.62 0.059 0.059 0.059 0.807 0.807 0. 0.529 0.523 0.526 D.6 THE EFFECT OF SIMPLY INCREASING CODEBOOK SIZE According to Theorem 1, increasing log too much over DKL will not significantly improve reconstruction but will lead to waste of bitrate (tokens). We provide an additional in Table 18, showing that GQ trained with 14 bits and quantized into 18 bits does not perform as well as GQ trained with 18 bits and quantized into 18 bits. The drawback of simply increasing the codebook size for GQ is that it is not as effective as increasing the target of TDC to that size and quantizing with proper log = DKL. Table 18: The effect of simply increase codebook size. Training Target Codebook Size DKL(2) = 14 DKL(2) = 14 DKL(2) = log2 = 14 log2 = 18 log2 = 18 bpp (num of tokens) 214 1024 218 1024 218 1024 PSNR LPIPS SSIM rFID 25.31 27.79 27. 0.064 0.059 0.054 0.762 0.808 0.804 0.491 0.513 0.424 D.7 QUANTIZED LATENT VISUALIZATION In Figure 4, we show the t-NSE (van der Maaten & Hinton, 2008) visualization of latent after GQ, using 5 subclass of ImageNet."
        },
        {
            "title": "Preprint",
            "content": "Figure 4: The t-NSE visualization of latent of GQ vs. unquantized Gaussian VAE. It is shown that the latent before and after quantization are quite similar. Table 19: The generation performance of GQ in different bitrate regime. Method TokenBridge GQ (Ours) TokenBridge GQ (Ours) bpp (num of tokens) 0.1875(216 256) 0.1875(216 256) 0.25(216 256) 0.25(216 256) gFID IS 8.29 7.74 7.82 7.67 188.05 229.53 198.24 230. D.8 MORE GENERATION RESULTS To better understand the generation performance, in Table 19, we present the auto-regressive generation result of GQ in different bitrate regime. And in Table 20, we present the auto-regressive generation result of GQ in FFHQ dataset. It is shown that the advantage of GQ is consistent in different bitrate regime and datasets. D.9 PRIOR-POSTERIOR MISMATCH Sometimes the Gaussian VAE might suffer from prior-posterior mismatch. However, in our case, such mismatch is not severe. To illustrate this, we estimate the prior posterior mismatch by considering the relationship between q(Z) and (0, I). More specifically, we have DKL(q(Z)N (0, 1)) 1 N (cid:88) (log q(zi) log (zi0, I)). (33) i=1 Additionally, we can estimate the optimal bitrate without effected by prior posterior mismatch with similar approximation: DKL(q(ZX)p(Z)) 1 N (cid:88) (log q(ziX) log q(zi)). i=1 (34) Table 20: The generation performance of GQ with FFHQ dataset. Method BSQ TokenBridge GQ (Ours) bpp (num of tokens) 0.25(216 256) 0.25(216 256) 0.25(216 256) gFID IS 5.48 7.15 5.09 - - -"
        },
        {
            "title": "Preprint",
            "content": "Table 21: The bitrate and prior-posterior mismatch."
        },
        {
            "title": "Divergence",
            "content": "bits per pixel bpp w.r.t. (0, I) (DKL(q(ZX)N (0, 1))) bpp w.r.t. q(Z) (DKL(q(ZX)q(Z))) prior posterior mismatch (DKL(q(Z)N (0, 1))) 0.25 0.25 0.000328 We train diffusion model to estimate log q(zi) by using PF-ODE and Skilling-Hutchinson trace estimator (See Appendix D.2 of Song et al. (2020)). We use the Gaussian VAE (w/ TDC) + DiT diffusion model and ImageNet validation dataset. The euler PF-ODE steps is set to 250 and the Skilling-Hutchinson number of sample is set to 1. The result is shown in Table 21. The results show that the prior-posterior mismatch is only 0.00033 bits per pixel, accounting for approximately 0.1% of the total bpp. Furthermore, the best bpp and the actual bpp show no significant difference on scale of 0.01. This indicates that the bitrate waste caused by the prior-posterior mismatch is negligible, and the mismatch itself is not significant. D.10 COMPLEXITY Table 22: The encoding and decoding overhead of GQ over Gaussian VAE. Method UNet based Encoding FPS Decoding FPS Gaussian VAE GQ (torch) GQ (CUDA) 104 12 79 64 61 As with FSQ and BSQ (Mentzer et al., 2023; Zhao et al., 2024), our codebook can be generated on the fly by maintaining the same random number generator seed on both the encoder and decoder sides. Therefore, our GQ model has the same parameter size as the vanilla Gaussian VAE. In Table 22, we compare the encoding and decoding frames per second (FPS) of the Gaussian VAE and GQ. We use 256 256 images with batch size of 1, and we report the wall clock time, meaning that the time required for loading data is included. The results show that the encoding FPS of GQ (implemented in PyTorch) is 12 on an H100 GPU, which is considerably slower than the 104 FPS achieved by the Gaussian VAE. On the other hand, GQ does not incur any decoding overhead. To reduce the computational complexity of GQ, we implement GQ using tailored CUDA kernel. Specifically, we follow the approach of Vonderfecht & Liu (2025), with key difference: we maintain the codebook, as our bottleneck is not codebook instantiation. Additionally, we avoid the creation of large buffer vectors by performing the summation over within the CUDA kernel instead of in PyTorch. With this approach, we achieve an encoding FPS of approximately 80, with negligible overhead compared to the Gaussian VAE. detailed comparison between the PyTorch implementation and the CUDA implementation of GQ is provided below as GQ torch and GQ CUDA, respectively. 3 4 5 7 8 9 10 11 1 def GQ_torch(mu, sigma, codebook, m, bs, K): # mu.shape = (bs, m) # sigma.shape = (bs, m) # codebook.shape = (K, m) # This step create (bs, m, K) vector, which is the performance bottleneck dist_m =((mu[:,None] - codebook[None])/sigma[:,None])**2 dist = torch.sum(dist_m, dim=1) # sum over dimension indices = torch.argmin(dist, dim=1) # argmax over dimension zhat = torch.index_select(codebook, 0, indices) # select quantized results return indices, zhat"
        },
        {
            "title": "Preprint",
            "content": "3 4 5 6 7 9 13 14 15 16 18 19 ) { 20 21 22 23 24 26 27 28 29 30 31 } 1 def GQ_CUDA(mu, sigma, codebook, m, bs, K): dist = torch.zeros([bs, K]) # need an extension wrapping and register the kernel into operator, we omit it in paper # see code appendix for details GQ_Kernel<<<bs * / 256,256>>>(mu, sigma, codebook, dist, m, bs, K) indices = torch.argmin(dist, dim=1) # argmax over dimension zhat = torch.index_select(codebook, 0, indices) # select quantized results return indices, zhat 10 11 __global__ void GQ_Kernel( 12 const float* mu, const float* sigma, const float* codebook, float* dist, int64_t m, int64_t bs, int64_t int idx = blockIdx.x * blockDim.x + threadIdx.x; if (idx >= * bs) return; int bi = idx / K; int ni = idx % K; float = 0.0f; for (int = 0; < m; i++) { float = (codebook[ni * + i] - mu[bi * dim + i]) / sigma[bi * dim + i]; += * b; } dist[idx] = a; return; D.11 ASYMPTOTIC COMPLEXITY It is noteworthy that GQ without grouping quantization, such as PT or TR, or GQ with group size of = 1, is asymptotically faster than reverse channel coding methods. This is because, for = 1, the GQ target in Eq.3 reduces to quadratic form. In this case, it suffices to sort the scalar codebook c1:K in advance. Despite the sorting takes Ω(DKL(q(ZiX)N (0, 1))), the sorting is only need to be done once and can be amortized across dimension and dataset. Subsequently, the minimization in Eq.3 can be performed in O(DKL(q(ZiX)N (0, 1))) time using binary search. The details is shown in Algorithm 2. On the other hand, most reverse channel coding methods require O(2DKL(q(ZiX)N (0,1))) computational complexity (Havasi et al., 2018b; Flamich et al., 2020; Theis & Yosri, 2021). coding (Flamich et al., 2022) can achieve O(D(q(ZiX)N (0, 1))) encoding complexity, albeit at the cost of increased decoding complexity. However, we note that this complexity advantage is not particularly meaningful in practice. This is because any auto-regressive generation model requires softmax operation over the entire codebook, which has complexity of O(2DKL(q(ZiX)N (0,1))). In practice, only tractable codebook sizes, such as 216 or 218, are used."
        },
        {
            "title": "E ADDITIONAL QUANTITATIVE RESULTS",
            "content": "E.1 ADDITIONAL QUALITATIVE RESULTS AND FAILURE CASES In Figure 5, we present additional qualitative results showing that GQ achieves superior visual quality. However, we also note that none of the approaches is successful in reconstructing the plate of the residential vehicle. The text content remains challenging for low bitrate VQ-VAEs. Algorithm 1: GQ (argmax) 1 input c1:K(sorted, such that cj cj+1), µi 2 = for = 1 to do 3 5 6 if cj µido = cj µi, = return cj , Algorithm 2: GQ (bisect) 1 input c1:K(sorted, such that cj cj+1), µi = 1, = while + 1 < do = (l + r)//2 if cm < µi do = else = ifcl µi < cr µido return cl, else return cr, 3 4 5 7 8 9 10"
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Qualitative results on ImageNet dataset and 0.25 bpp. None of those approaches correctly reconstruct the plate number."
        }
    ],
    "affiliations": [
        "Tsinghua University",
        "University of Cambridge",
        "Zhipu AI"
    ]
}