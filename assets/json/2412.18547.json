{
    "paper_title": "Token-Budget-Aware LLM Reasoning",
    "authors": [
        "Tingxu Han",
        "Chunrong Fang",
        "Shiyu Zhao",
        "Shiqing Ma",
        "Zhenyu Chen",
        "Zhenting Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reasoning is critical for large language models (LLMs) to excel in a wide range of tasks. While methods like Chain-of-Thought (CoT) reasoning enhance LLM performance by decomposing problems into intermediate steps, they also incur significant overhead in token usage, leading to increased costs. We find that the reasoning process of current LLMs is unnecessarily lengthy and it can be compressed by including a reasonable token budget in the prompt, but the choice of token budget plays a crucial role in the actual compression effectiveness. We then propose a token-budget-aware LLM reasoning framework, which dynamically estimates token budgets for different problems based on reasoning complexity and uses the estimated token budgets to guide the reasoning process. Experiments show that our method effectively reduces token costs in CoT reasoning with only a slight performance reduction, offering a practical solution to balance efficiency and accuracy in LLM reasoning. Code: https://github.com/GeniusHTX/TALE."
        },
        {
            "title": "Start",
            "content": "Token-Budget-Aware LLM Reasoning Tingxu Han1, Chunrong Fang*1, Shiyu Zhao2, Shiqing Ma3, Zhenyu Chen1, Zhenting Wang2 1Nanjing University 2Rutgers University 3UMass Amherst 4 2 0 2 4 2 ] . [ 1 7 4 5 8 1 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Reasoning is critical for large language models (LLMs) to excel in wide range of tasks. While methods like Chain-of-Thought (CoT) reasoning enhance LLM performance by decomposing problems into intermediate steps, they also incur significant overhead in token usage, leading to increased costs. We find that the reasoning process of current LLMs is unnecessarily lengthy and it can be compressed by including reasonable token budget in the prompt, but the choice of token budget plays crucial role in the actual compression effectiveness. We then propose token-budget-aware LLM reasoning framework, which dynamically estimates token budgets for different problems based on reasoning complexity and uses the estimated token budgets to guide the reasoning process. Experiments show that our method effectively reduces token costs in CoT reasoning with only slight performance reduction, offering practical solution to balance efficiency and accuracy in LLM reasoning. Code: https://github.com/GeniusHTX/TALE. It is not enough to have good mind; the main thing is to use it well. Ren√© Descartes"
        },
        {
            "title": "Introduction",
            "content": "Reasoning plays crucial role in enabling large language models (LLM) to perform effectively across wide range of tasks (Zhou et al., 2022; Hao et al., 2023, 2024a). variety of methods have been proposed to enhance the reasoning capabilities of large language models (Suzgun et al., 2022; Wang et al., 2023; Feng et al., 2023; Yao et al., 2024a; Xie et al., 2024). Among these, Chain-of-Thought (CoT) (Wei et al., 2022) is the most representative *Corresponding Author. Project Lead. Pre-print with preliminary results, work in progress. 1 and widely adopted approach. It enhances the reliability of the models answers by guiding large language models with the prompt \"Lets think step by step\", encouraging them to decompose the problem into intermediate steps and solve each before arriving at the final answer. Figure 1a and Figure 1b illustrate an intuitive example. Observe that without CoT, the LLM produces incorrect answers to the question. With CoT-enhanced prompt, the LLM systematically breaks the question into multiple steps and reasons through each step sequentially. By addressing each step incrementally, the LLM eventually arrives at the correct answer. Although reasoning enhancement approaches such as CoT impressively improve LLM performance, they produce substantial additional overhead, specifically in the form of the increased number of tokens produced (Wei et al., 2022; Feng et al., 2023; Yao et al., 2024a). As shown in Figure 1b, the answer to prompt with CoT has notably higher token costs due to the detailed intermediate reasoning steps included in the output. Such high token costs can lead to significant expenses, including increased computational resource usage and longer running times during the LLM inference phase, ultimately resulting in significant additional monetary and energy costs. This raises an important question: Is the reasoning process of current LLMs unnecessarily lengthy, and how can it be compressed? Nayab et al. (2024) demonstrate that LLM has the potential to follow length constraint in the prompt. Building on this, we find that including token budget (see Table 1) in the prompts is promising approach to compressing the CoT reasoning tokens. However, the choice of token budget plays crucial role in the actual compression effectiveness. For example, Figure 1d illustrates that including reasonable token budget (e.g., 50 tokens in this case) in the instructions reduces the token cost in the chain-ofthought (CoT) process from 258 output tokens to as many as with 50-token budget. In other words, when the token budget is relatively small, LLMs often fail to follow the given token budget. In such cases, the actual token usage significantly exceeds the given budgeteven much larger than the token costs with larger token budgets. We refer to this phenomenon as the Token Elasticity in the CoT process with token budgeting. To address this, the optimal token budget for specific LLM and particular question can be searched by gradually reducing the budget specified in the prompt, identifying the smallest token budget that achieves both the correct answer and the lowest actual token cost. Based on the above observations and analysis, we designed prototype for token-budget-aware reasoning in large language models (LLMs). Our approach leverages the token budget to guide the reasoning process, dynamically allocating different token budgets to problems based on an estimation of their reasoning complexity. We call our method TALE (Token-Budget-Aware LLM rEasoning). For given problem and specific LLM, TALE first estimates an appropriate token budget and then uses it to guide the reasoning process. We discuss different implementations of TALE in Section 5. Experiment results show that TALE significantly reduces token costs in LLM chain-of-thought (CoT) reasoning while largely maintaining the correctness of the answers. On average, TALE achieves 68.64% reduction in token usage while maintaining accuracy with less than 5% decrease."
        },
        {
            "title": "2 Related Work",
            "content": "LLM Reasoning. Reasoning in LLMs has seen substantial advancements through techniques that generate intermediate steps, enabling more accurate and effective performance across diverse domains (Wu et al., 2022; Yang et al., 2022; Zhou et al., 2022; Sun et al., 2024; OpenAI, 2024c). Various LLM reasoning techniques are proposed to improve the LLM performance. Chen et al. (2024) formulates reasoning as sampling from latent distribution and optimizing it via variational approaches. Ho et al. (2022) utilizes LLM as reasoning teachers, improving the reasoning abilities of smaller models through knowledge distillation. Among them, Chain-of-Thought (CoT) prompting has emerged as key technique for improving LLM reasoning by breaking problems into intermediate steps, enabling better performance on multiple (a) Direct answering (15 output tokens). (b) Vanilla CoT (258 output tokens). (c) CoT with an unreasonable budget (157 output tokens). (d) CoT with an reasonable budget (86 output tokens). Figure 1: Examples of different problem solving paradigms. The reasoning processes are highlighted. 86 output tokens, while still enabling the LLM to arrive at the correct answer. However, when the token budget is set to different smaller value (e.g., 10 tokens), the output token reduction is less effective, resulting in 157 output tokensnearly twice 2 tasks (Wei et al., 2022; Lyu et al., 2023; Li et al., 2023; Feng et al., 2024). Extensions of CoT include self-consistency, which aggregates multiple reasoning paths to improve robustness (Wang et al., 2022), and Tree-of-Thoughts, which explores reasoning steps in tree-like structure for more complex tasks (Yao et al., 2024b). Reflexion introduces iterative refinement, where the model critiques and updates its intermediate steps (Shinn et al., 2024). Token Cost of LLM. Although the above methods enhance reasoning accuracy, they often increase token usages, posing challenges to efficiency (Wang et al., 2024; Chiang and Lee, 2024; Bhargava et al., 2023). Consequently, it is important to mitigate token consumption while maintaining the model performance. To address this issue, Li et al. (2021) introduces multi-hop processing technique designed to filter out irrelevant reasoning. While effective, this approach is limited to traditional neural networks, such as PALM (Bi et al., 2020), and lacks adaptability to large language models (LLMs). Zheng et al. (2024) aims to improve LLM inference speed by predicting response lengths and applying scheduling algorithm to enhance efficiency. However, their method is constrained to and does not address the reduction of actual token costs. Hao et al. (2024b) reduces token usage by substituting decoded text tokens with continuous latent tokens. However, its application is currently restricted to small-scale, early language models like GPT-2 (Radford et al., 2019). Additionally, it significantly impacts reasoning accuracy, resulting in over 20% relative accuracy reduction on benchmarks such as GSM8K (Cobbe et al., 2021)."
        },
        {
            "title": "3 Token Redundancy in LLM Reasoning",
            "content": "Token Budget. Previous research Nayab et al. (2024) demonstrates that LLM has the potential to follow length constraint in the prompt. Table 1 shows the difference between the vanilla CoT and the CoT with token budget. For instance, by including token budget (50 tokens) within the prompt, as illustrated in Figure 1d, the LLM adjusts the length of its output (86 output tokens), trying to align with the specified budget. This indicates that LLMs have certain capability in following prompts with an explicit token budget. Token Redundancy Phenomenon. We find that providing reasonable token budget can significantly reduce the token cost during reasoning. As shown in Figure 1d, including token budget in Table 1: Illustrations of the vanilla CoT prompt and the token-budget-aware prompt."
        },
        {
            "title": "Vanilla CoT",
            "content": "Lets think step by step:"
        },
        {
            "title": "Example",
            "content": "Lets think step by step and use less than budget tokens: Lets think step by step and use less than 50 tokens: the instructions reduces the token cost in the chainof-thought (CoT) process by several times, but the LLM still gets the correct answer. Our results in Figure 2 and Table 2 also confirm there are large number of redundant tokens in the reasoning process of the state-of-the-art LLMs. Causes of Token Redundancy in LLM Reasoning. possible explanation for this token redundancy is that during the post-training phase, such as the RLHF process (Ouyang et al., 2022), annotators might favor more detailed responses from LLMs, marking them as preferred. As result, the model learns to associate longer, more detailed responses with alignment to human preferences and tends to produce such outputs during reasoning. However, in many scenarios, we primarily need LLMs to provide the correct answer and make accurate decisions, rather than elaborate extensively with detailed explanations. This motivates the need to eliminate redundant tokens in the LLM reasoning process in many cases."
        },
        {
            "title": "4 Searching Optimal Token Budget",
            "content": "As demonstrated in Figure 1, different token budgets have different effects. Therefore, it is natural to investigate the following question: How to search the optimal token budget for specific question and particular LLM? Vanilla Method for Optimal Budget Search. An intuitive method is finding the minimal needed tokens as the budget, ensuring that the LLM can still produce correct and accurate responses within this constraint. To find the minimal token budget required for each question, we utilize binary search-based minimal budget search algorithm. Algorithm 1 showcases the details. Before initiating the search process, we first apply the vanilla CoT to generate an answer for each question, as illustrated in Figure 1b. The number of tokens in the resulting answer is then calculated and designated as the right boundary for search, denoted by right. The 3 (a) GPT-4o-mini budget search. (b) GPT-4o-mini token cost. (c) Yi-lightning budget search. (d) Yi-lightning token cost. Figure 2: Token elasticity phenomenon. The x-axis denotes the budget search iteration. The y-axis denotes the searched budget (Figure 2a and Figure 2c) or the real token costs for each searched budget (Figure 2b and Figure 2d). Different colors denote different samples. The token cost is significantly lower in reasonable token budget range. When the token budget is smaller than the reasonable range, the token cost gradually increases. Algorithm 1 Budget Search Input: feasibility checking function isFeasible, large language model M, given question and the ground truth label Output: searched budget Œ≤ 1: function SEARCH(isFeasible,M, x, y) 2: right the actual token costs of with vanilla CoT prompt on Œ≤ (0 + right)/2 Œ≤0 right while True do if isFeasible(M, x, y, Œ≤0, Œ≤) then (cid:3) Update the searched budget Œ≤ (0 + right)/2 (cid:3) Record previous searched budget Œ≤0 right (cid:3) Update the search range right Œ≤ else break return Œ≤ 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: function isFeasible is used to determine the feasibility of budget. budget is considered feasible here if the CoT prompt with that budget preserves the correctness of the answer. Given the feasibility function, large language model M, question and label yas the input, Algorithm 1 first calculates the right boundary of search (line 2). With 0 as the left boundary, the current possible budget Œ≤ is computed as the midpoint of 0 and right (line 3). We use Œ≤0 to record the previously searched budget (line 4). While the current Œ≤ is feasible, the algorithm updates Œ≤ by recalculating the midpoint (line 7) and adjusts the search bounds accordingly to narrow the range (line 9). Once the loop ends, the final budget Œ≤ is returned as the searched result (line 12). Algorithm 1 is designed to find the minimal budget efficiently. However, we observe that the minimal Figure 3: The effects of optimal searched budget. CoT with our optimal searched budget reduces the token costs significantly without influencing the accuracy. budget required to produce correct answer is not necessarily the optimal budget. When the budget is unreasonably small, the actual token cost often exceeds that of cases where larger budget is used. Observation of Token Elasticity. During our minimal budget search process, we observe token elasticity phenomenon as we approach the minimal budget. Specifically, as Algorithm 1 progresses, we aim to identify the minimal budget that still ensures the answers correctness. However, we find that if the budget is reduced beyond certain range, the token cost increases, indicating that further reductions in the budget lead to increasing token consumption. Figure 2 showcases the evidence. The x-axis represents the iterations of the budget binary search, with the budget values decreasing progressively. The y-axis in Figure 2b and Figure 2d show the corresponding token costs at each budget search iteration. We can observe that when the searched budget crosses range for reasonable budgets, the token costs begin to gradually increase. Figure 1c also shows an example. As observed, when small token budget (e.g., 10 tokens) is used, the real token cost is significantly higher compared to scenarios where reasonable token budget is allocated (i.e., Figure 1d). Token Elasticity based Optimal Budget Search. The token elasticity observation shows that while 4 Algorithm 2 Greedy Feasibility Function Input: large language model M, question and the ground truth label y, previous and current budget: Œ≤0, Œ≤ Output: True if the budget satisfies the requirements, False otherwise 1: function isFeasible(M, x, y, Œ≤0, Œ≤) 2: t, t0 gets the actual token costs under budgets of Œ≤ and Œ≤0 if M(x, Œ≤) == and < t0 then return True return False 3: 4: 5: Figure 4: The workflow of TALE. Given question, TALE first estimates the token budget using budget estimator. It then crafts token-budget-aware prompt by combining the question with the estimated budget. Finally, the prompt is input to the LLM to generate the answer as the final output. minimal budget may keep the correctness of the answer, it does not necessarily minimize the token cost. Figure 1c and Figure 1d illustrate an intuitive example. To address this, we enhance Algorithm 1 by incorporating greedy search strategy aimed at finding the optimal budget that simultaneously minimizes token cost and preserves answer correctness. Specifically, we introduce an additional constraint to the isFeasible condition. Beyond ensuring correctness, the updated budget must result in lower token cost compared to the previously searched budget. Algorithm 2 outlines the feasibility function employed during the search process. Initially, the actual token cost is computed for both the current and previously evaluated budgets (line 2). Next, feasibility is assessed based on two criteria: the correctness of the answer and reduction in token cost following greedy strategy (line 3). The search process is terminated if either condition fails."
        },
        {
            "title": "5.1 Overview",
            "content": "Based on the above observations and analysis, we designed our method TALE for token-budgetaware reasoning in LLMs. Figure 4 provides an overview of TALEs workflow. TALE aims 5 to elaborate token-budget-aware prompt that achieves performance comparable to vanilla CoT while reducing token costs. To strike this balance, TALE follows two-phase approach: budget estimation and prompt construction. The observation of token elasticity, as discussed in Section 4, suggests that only an appropriate budget located in reasonable range can effectively reduce token costs and preserve LLM performance simultaneously. The optimal budget searched by Algorithm 1 and Algorithm 2 is located in such reasonable budget range and achieves satisfying trade-off between token costs and LLM performance. In this case, TALE first estimates reasonable token budget that is close to the searched optimal budget for the given question. Using the estimated token budget, TALE then crafts token-budget-aware prompt and then feeds it into LLM to generate the final answer. Figure 7 illustrates an example of TALE."
        },
        {
            "title": "5.2 Budget Estimation",
            "content": "To estimate an appropriate budget within the reasonable budget range, two possible solutions are taken into consideration: zero-shot-based mechanism and budget regression. We also discuss an approach that internalizes the budget awareness of LLM by fine-tuning it. All approaches focus on developing an estimator that effectively approximates the optimal budget. Zero-shot Estimator. For zero-shot mechanism to predict the reasonable budget, TALE leverages the reasoning LLM itself as an estimator. Before querying the LLM with question, TALE first prompts the LLM to estimate the number of output tokens needed to answer the question. Figure 5 illustrates the budget estimation prompt. The key intuition behind this is the human-like thinking paradigm. When presented with mathematical question, although it may take humans few minutes to calculate the answer, they can typically estimate the time or effort required to solve it after just briefly reviewing the question. For instance, when presented with question from primary school arithmetic and another from college-level calculus, human may not immediately provide the answers. Still, it is easy to infer that the former can be solved in seconds, while the latter requires significantly longer time, even with only brief glance. RQ2 in Section 6 showcases the performance of budget estimation. Observe that large amount of the estimated budgets are around the optimal searched budget and achieve competitive performance. Lets think step by step and use less than Œ≤ tokens: where Œ≤ is the searched optimal budget for the given question xi (see search process in Algorithm 1 and Algorithm 2). Figure 1d illustrates an example. The resulting LLM output, constrained by the token budget specified in the prompt, is taken as the crafted target output yi. In the LLM fine-tuning stage, we train the LLM M(Œ∏) using the crafted target outputs from the first stage. The instruction prompt for fine-tuning is standardized as follows: Lets think step by step: The training sample for fine-tuning consists of the given question xi paired with the crafted target output yi. The training objective aligns with the loss function described in Equation 2 where (x, yi) indicates the training sample and pi means the above instruction prompt. During fine-tuning, the LLM is encouraged to internalize the token budget constraint and prefer compact reasoning process, following the target outputs generated in the first stage. This two-stage process ensures that the LLM produces concise yet correct responses, effectively balancing reasoning quality with token efficiency during inference."
        },
        {
            "title": "6 Evaluation",
            "content": "In this section, we provide the preliminary results of the zero-shot estimator version of our method TALE. This project is ongoing and we will update more results soon. Three state-of-the-art LLMs (i.e., GPT-4o (OpenAI, 2024b), GPT-4omini (OpenAI, 2024a), Yi-lightning (Wake et al., 2024)) are involved in the experiments. Our evaluation is surrounding around the following research questions(RQs): RQ1. How effective is TALE in reducing token costs while maintaining LLM performance? RQ2. How effective is TALE to estimate the token budget given question? RQ3. How general is TALE across different stateof-the-art LLMs?"
        },
        {
            "title": "6.1 Experiment Setup",
            "content": "Metrics. The target of TALE is to balance the LLM performance and extra redundant token costs. Specifically, TALE seeks to minimize token consumption while maintaining comparable LLM performance. To evaluate the LLM performance, three most challenging mathematical datasets are taken Figure 5: The prompt for zero-shot estimator. )}N Regression Estimator. For regression-based estimator, we aim to train/fine-tune another LLM (Œ∏) to serve as the estimator, such that (Œ∏) estimates the optimal token budget given specific LLM and particular question. Given = {(p, xi, Œ≤ i=1, is the instruction prompt, xi is question, Œ≤ is our searched optimal budget (searched by Algorithm 1 and Algorithm 2) for and is the dataset size. We assign the instruction prompt as Estimate the token budget for the following question. Next, we initialize (Œ∏) using pre-trained LLM, such as LLaMA 3-8B (AI@Meta, 2024). Then, we craft the target output yi for xi using Œ≤ . For example, given Œ≤ 0 as 14, the corresponding target output y0 is The token budget is 14. We aim to make the model output as close as possible to the target output yi by minimizing loss function, which can be formalized as: Œ∏ = arg min Œ∏ (cid:88) i=1 (fŒ∏(pi, xi), yi) (1) where Œ∏ denotes the model parameters and Œ∏ the optimized parameters. The loss function is defined as follows: = (cid:88) i=1 log P(yi pi, xi; Œ∏) (2) Through Equation 2, model parameters Œ∏ are optimized to maximize the probability (yi pi, xi; Œ∏) of the target output yi given the input xi and instruction prompt across all training samples. Token-Budget Awareness Internalization. To obtain an LLM with token awareness, we fine-tune the LLM to internalize the budget estimation into the inference process and produce token-efficient reasoning responses. Specifically, we fine-tune the LLM M(Œ∏) so that it generates token-budgetaware answers. This process is divided into two key stages: target output generation and LLM finetuning. In the target output generation stage, we craft the target output yi by prompting M(Œ∏) with Chain-of-Thought (CoT) prompt that incorporates our searched optimal token budget. The prompt is formatted as follows: tently demonstrates significant improvements in efficiency while maintaining competitive accuracy. Directly answering achieves the lowest output tokens (14.57 on average) and expenses (25.37 on average) but has the lowest accuracy (52.31% on average). Vanilla CoT achieves the highest accuracy (83.75% on average) but incurs significant token cost (461.25 on average) and expenses (289.78 on average). TALE demonstrates balanced trade-off between performance and cost. It achieves competitive accuracy (81.03%) while reducing token costs (32% of Vanilla CoT) and expenses (41% of Vanilla CoT) significantly. For accuracy, notably, on GSM8K, TALE even improves accuracy to 84.46%, surpassing Vanilla CoT, demonstrating its ability to adapt well to complex reasoning tasks while remaining efficient. For output tokens on GSM8K-Zero, TALE achieves an impressive reduction in output token costs from 252.96 to 22.67 while maintaining high accuracy (98.72%), showcasing its capability to optimize reasoning efficiency in such tasks. For expenses, TALE demonstrates its cost-effectiveness, reducing expenses from 78.58 to 18.62 while achieving reasonable accuracy (73.67% vs 75.00%) on MathBench-Arithmetic. TALE demonstrates that incorporating token-budget awareness allows for significant reduction in token costs and monetary expenses without major compromise in accuracy. TALE reduces output token costs by 68.64% on average, making it more efficient solution for budget-constrained reasoning tasks while retaining competitive performance. These results highlight TALEs generalizability across tasks with varying complexity, demonstrating its potential to scale in real-world scenarios while managing computational and financial resources effectively."
        },
        {
            "title": "6.3 RQ2. Effectiveness of Budget Estimation.",
            "content": "In this RQ, we evaluate the effectiveness of the budget estimation performance. An ideal estimated budget should be located around the optimal searched budget and in the bottom area of Figure 2. We further define such an area as the ideal budget range and give the formalized definition in Section A.1. good budget should be located in the ideal budget range. Two metrics are taken into consideration: in-range accuracy and out-of-range distance. In-range accuracy determines whether the predicted budget ÀÜŒ≤ falls within the ideal budget Figure 6: The instruction prompt used to format the LLM output on multiple-choice questions. i=1 (cid:80)N into consideration: GSM8K (Cobbe et al., 2021), GSM8K-Zero (Chiang and Lee, 2024), and MathBench (Liu et al., 2024). GSM8K-Zero, derived from the GSM8K dataset, specifically targets the analysis of over-reasoning and redundancy in LLMgenerated outputs. In short, GSM8K-Zero is designed so that the answers are embedded within the questions themselves. LLMs can easily generate correct responses without complicated additional reasoning or redundant calculations. Accuracy (Acc). This metric is calculated as the folI{M(xi) = yi}, lowing: Accuracy = 1 where (xi, yi) . xi is the math question from dataset and yi the ground truth answer. M() returns the answer for given question. I{} represents an indicator function. This function evaluates whether the inside given condition holds. Specifically, it returns 1 if the condition is true and 0 if the condition is false. For better evaluation, we format the LLM output by crafting an elaborate instruction detailed in Figure 6. Number of Output Tokens. We evaluate the token costs by calculating the average output token consumption for each specific task. The token costs are measured as follows: output T(M(xi)), Number of Output Tokens = 1 where xi represents the given question, and is function that measures the number of tokens. Intuitively, the more output tokens, the higher the costs incurred by M. To evaluate costs more precisely, we calculate the average expense per sample for querying the LLM. The total token expense includes both input and output tokens used during the query process. (cid:80)N i="
        },
        {
            "title": "6.2 RQ1. Effectiveness of TALE.",
            "content": "Table 2 presents comparison of TALE and other prompt engineering methods, including Directly Answering and Vanilla CoT, in their effectiveness for seven different datasets. The results are evaluated regarding ACC, Number of Output Tokens (Output Tokens for short), and Expense. well-designed prompt engineering approach should induce the LLM to generate correct response with as few tokens as possible. TALE consis7 Table 2: Comparison of TALE (Zero-shot Estimator Version) and other prompt engineering methods. Directly Answering means prompting LLM without any reasoning process. Vanilla CoT means the vanilla CoT prompting with budget. The model used in our evaluation is GPT-4o-mini (OpenAI, 2024a). Observe that TALE achieves an average accuracy (ACC) of 80.22%, with an average output token cost of 138.53 and an average expense of 118.46. TALE reduces output token costs by 67%, lowers expenses by 59%, and maintains competitive performance compared to the vanilla CoT approach. ACC , Output Tokens , Expense (105$ / sample) . Dataset Directly Answering Vanilla CoT TALE (Ours) ACC Output Tokens Expense ACC Output Tokens Expense ACC Output Tokens Expense 28.29% GSM8K GSM8K-Zero 97.21% MathBench-Arithmetic 59.67% 33.33% MathBench-Middle 51.33% MathBench-High 44.00% MathBench-College Average 52.31% 12.46 18.85 41.10 5.00 5.00 5. 14.57 39.43 91.69 9.78 3.58 4.07 3.68 81.35% 99.50% 75.00% 84.67% 84.00% 78.00% 25.37 83.75% 318.10 252.96 313.51 553.93 653.24 675. 461.25 541.09 886.79 78.58 68.22 82.44 81.56 84.46% 98.72% 73.67% 79.33% 80.00% 70.00% 289.78 81.03% 77.26 22.67 39.60 238.14 254.82 259. 148.72 279.84 276.12 18.62 42.95 47.61 45.60 118.46 Table 3: The generalization of TALE (Zero-shot Estimator Version) across different LLMs. Yi-lightning (Wake et al., 2024), GPT-4o-mini (OpenAI, 2024a) and GPT-4o (OpenAI, 2024b) are taken into consideration. We conduct the evaluation on MathBench-College. ACC , Output Tokens , Expense (105$ / sample) . LLM Directly Answering Vanilla CoT TALE (Ours) ACC Output Tokens Expense ACC Output Tokens Expense ACC Output Tokens Expense 66.67% Yi-lightning GPT-4o-mini 44.00% 57.33% GPT-4o 80.01 5.00 5.00 3.09 3.68 61. 79.33% 78.00% 84.00% 998.10 675.78 602.29 21.55 81.56 1359.42 76.67% 70.00% 80.00% 373.52 259.85 181.61 17.25 45.60 759. range . Mathematically, it can be expressed as: I{ ÀÜŒ≤ } = (cid:40) if ÀÜŒ≤ , 1, 0, otherwise. Out-of-range distance quantifies the distance between ÀÜŒ≤ and if the predicted budget Œ≤ falls outside the ideal budget range . Let dist( ÀÜŒ≤, ) represent the distance, defined as: dist( ÀÜŒ≤, ) = 0, min ÀÜŒ≤W ÀÜŒ≤ Œ≤, if ÀÜŒ≤ , if ÀÜŒ≤ / . Intuitively, higher in-range accuracy and lower out-range distance indicate better estimated budget. During our evaluation, the in-range accuracy is 60.61%, and the out-of-range distance is 109.64. It indicates that more than two-thirds of estimated budgets are located in the ideal range. For those out-of-range samples, they have an offset of 109.64 tokens on average."
        },
        {
            "title": "6.4 RQ3. Generalization of TALE.",
            "content": "Table 3 demonstrates the generalization of TALE across Yi-lightning (Wake et al., 2024), GPT4o-mini (OpenAI, 2024a), and GPT-4o (OpenAI, 2024b) on MathBench-College, showing its ability to reduce output tokens and expenses while maintaining competitive accuracy significantly. TALE 8 achieves substantial token savings, reducing output tokens by 62.6% for Yi-lightning, 61.5% for GPT-4o-mini, and 69.8% for GPT-4o, compared to Vanilla CoT. Expense reductions are equally notable, with costs decreasing from 21.55 to 17.25 for Yi-lightning, 81.56 to 45.60 for GPT-4o-mini, and 1359.42 to 759.95 for GPT-4o. Despite these cost savings, TALE maintains strong accuracy, achieving 76.67% on Yi-lightning, 70.00% on GPT4o-mini, and 80.00% on GPT-4o, comparable to Vanilla CoT. These results highlight TALEs effectiveness in balancing cost efficiency and reasoning performance across diverse LLM architectures. The observed accuracy drop is most significant for GPT-4o-mini. This could be attributed to its smaller number of parameters, which makes it more challenging to answer correctly within limited response reasoning length. 7 Conclusion In this paper, we introduce TALE, framework that reduces token redundancy in Chain-ofThought (CoT) reasoning by incorporating token budget awareness. TALE dynamically estimates token budgets based on task complexity to guide reasoning, balancing efficiency and accuracy. Experiments show that TALE reduces token usage by 68.9% on average with less than 5% accuracy loss, outperforming Vanilla CoT in cost-effectiveness while generalizing well across various LLMs."
        },
        {
            "title": "References",
            "content": "AI@Meta. 2024. Llama 3 model card. Aman Bhargava, Cameron Witkowski, Shi-Zhuo Looi, and Matt Thomson. 2023. Whats the magic word? control theory of llm prompting. arXiv preprint arXiv:2310.04444. Bin Bi, Chenliang Li, Chen Wu, Ming Yan, Wei Wang, Songfang Huang, Fei Huang, and Luo Si. 2020. Palm: Pre-training an autoencoding&autoregressive language model for context-conditioned generation. arXiv preprint arXiv:2004.07159. Haolin Chen, Yihao Feng, Zuxin Liu, Weiran Yao, Akshara Prabhakar, Shelby Heinecke, Ricky Ho, Phil Mui, Silvio Savarese, Caiming Xiong, et al. 2024. Language models are hidden reasoners: Unlocking latent reasoning capabilities via self-rewarding. arXiv preprint arXiv:2411.04282. Cheng-Han Chiang and Hung-yi Lee. 2024. Overreasoning and redundant calculation of large language models. arXiv preprint arXiv:2401.11467. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, and Liwei Wang. 2024. Towards revealing the mystery behind chain of thought: theoretical perspective. Advances in Neural Information Processing Systems, 36. Xidong Feng, Ziyu Wan, Muning Wen, Ying Wen, Weinan Zhang, and Jun Wang. 2023. Alphazero-like tree-search can guide large language model decoding and training. In NeurIPS 2023 Foundation Models for Decision Making Workshop. Shibo Hao, Yi Gu, Haotian Luo, Tianyang Liu, Xiyan Shao, Xinyuan Wang, Shuhua Xie, Haodi Ma, Adithya Samavedhi, Qiyue Gao, et al. 2024a. Llm reasoners: New evaluation, library, and analysis of step-by-step reasoning with large language models. arXiv preprint arXiv:2404.05221. Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. 2023. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992. Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. 2024b. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769. Namgyu Ho, Laura Schmid, and Se-Young Yun. 2022. Large language models are reasoning teachers. arXiv preprint arXiv:2212.10071. Chenliang Li, Bin Bi, Ming Yan, Wei Wang, and Songfang Huang. 2021. Addressing semantic drift in generative question answering with auxiliary extraction. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 942947. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. 2023. Making language models better reasoners with step-aware In Proceedings of the 61st Annual Meetverifier. ing of the Association for Computational Linguistics (Volume 1: Long Papers), pages 53155333. Hongwei Liu, Zilong Zheng, Yuxuan Qiao, Haodong Duan, Zhiwei Fei, Fengzhe Zhou, Wenwei Zhang, Songyang Zhang, Dahua Lin, and Kai Chen. 2024. Mathbench: Evaluating the theory and application proficiency of llms with hierarchical mathematics benchmark. arXiv preprint arXiv:2405.12209. Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. 2023. Faithful chain-ofthought reasoning. arXiv preprint arXiv:2301.13379. Sania Nayab, Giulio Rossolini, Giorgio Buttazzo, Nicolamaria Manes, and Fabrizio Giacomelli. 2024. Concise thoughts: Impact of output length on llm reasoning and cost. arXiv preprint arXiv:2407.19825. OpenAI. 2024a. Gpt-4o mini: advancing cost-efficient intelligence. Technical report, OpenAI. Accessed: July 18, 2024. OpenAI. 2024b. Hello gpt-4o. Technical report, OpenAI. Accessed: May 13, 2024. OpenAI. 2024c. Learning to reason with llms. Technical report, OpenAI. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2024. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36. Guangyan Sun, Mingyu Jin, Zhenting Wang, ChengLong Wang, Siqi Ma, Qifan Wang, Ying Nian Wu, Yongfeng Zhang, and Dongfang Liu. 2024. Visual agents as fast and slow thinkers. arXiv preprint arXiv:2408.08862. 9 with large language models. Advances in Neural Information Processing Systems, 36. Zangwei Zheng, Xiaozhe Ren, Fuzhao Xue, Yang Luo, Xin Jiang, and Yang You. 2024. Response length perception and sequence scheduling: An llmempowered llm inference pipeline. Advances in Neural Information Processing Systems, 36. Denny Zhou, Nathanael Sch√§rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. 2022. Least-to-most prompting enables complex reasonarXiv preprint ing in large language models. arXiv:2205.10625. Mirac Suzgun, Nathan Scales, Nathanael Sch√§rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. 2022. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261. Alan Wake, Albert Wang, Bei Chen, CX Lv, Chao Li, Chengen Huang, Chenglin Cai, Chujie Zheng, Daniel Cooper, Ethan Dai, et al. 2024. Yi-lightning technical report. arXiv preprint arXiv:2412.01253. Junlin Wang, Siddhartha Jain, Dejiao Zhang, Baishakhi Ray, Varun Kumar, and Ben Athiwaratkun. 2024. Reasoning in token economies: Budget-aware evaluation of llm reasoning strategies. arXiv preprint arXiv:2406.06461. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023. Planand-solve prompting: Improving zero-shot chain-ofthought reasoning by large language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 26092634. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837. Tongshuang Wu, Michael Terry, and Carrie Jun Cai. 2022. Ai chains: Transparent and controllable human-ai interaction by chaining large language In Proceedings of the 2022 CHI model prompts. conference on human factors in computing systems, pages 122. Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy Lillicrap, Kenji Kawaguchi, and Michael Shieh. 2024. Monte carlo tree search boosts reasoning via iterative preference learning. arXiv preprint arXiv:2405.00451. Jingfeng Yang, Haoming Jiang, Qingyu Yin, Danqing Zhang, Bing Yin, and Diyi Yang. 2022. Seqzero: Few-shot compositional semantic parsing with sequential prompts and zero-shot models. arXiv preprint arXiv:2205.07381. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2024a. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2024b. Tree of thoughts: Deliberate problem solving"
        },
        {
            "title": "A Appendix",
            "content": "(a) Direct answering (10 output tokens). token consumption. Let Œ≤ = {Œ≤1, Œ≤2, ..., Œ≤N } denote all possible budgets that can maintain answer correctness. rolling window Œ≤ is applied iteratively over Œ≤. Let represent the range size, which is adaptively determined during our evaluation as 3 , where is the total number of possible budgets. budget range is defined as: Wk(i) = {Œ≤j + 1}, 1 Œ≤ + 1 The ideal budget range is defined as: = arg min (cid:88) Œ≤j Wk(i) T(Œ≤j) , (3) where denote the actual token consumption for given budget Œ≤ Œ≤. We aim to estimate budget located in the ideal budget ranges without any search process. In that case, TALE obtains the ideal budget within acceptable sacrifice. (b) Vanilla CoT (271 output tokens). (c) TALE (68 output tokens). Figure 7: An intuitive example to illustrate the workflow of TALE. A.1 Definition of Ideal Budget Range Ideal Budget Range. Based on the observation of token elasticity, token cost bottom range exists during searching for the optimal budget. In this range, the token costs approach the token cost lowest bound. Before or after the range, the token cost will increase. We define such bottom range as ideal budget range. Its worth noting that the budget continuously degrades during the search. Only the token cost rebounds. Thats why we refer to this observation as token elasticity. To summarize, ideal budget range is an range that minimizes actual"
        }
    ],
    "affiliations": [
        "Nanjing University",
        "Rutgers University",
        "UMass Amherst"
    ]
}