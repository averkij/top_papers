{
    "paper_title": "RefVTON: person-to-person Try on with Additional Unpaired Visual Reference",
    "authors": [
        "Liuzhuozheng Li",
        "Yue Gong",
        "Shanyuan Liu",
        "Bo Cheng",
        "Yuhang Ma",
        "Liebucha Wu",
        "Dengyang Jiang",
        "Zanyi Wang",
        "Dawei Leng",
        "Yuhui Yin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce RefTON, a flux-based person-to-person virtual try-on framework that enhances garment realism through unpaired visual references. Unlike conventional approaches that rely on complex auxiliary inputs such as body parsing and warped mask or require finely designed extract branches to process various input conditions, RefTON streamlines the process by directly generating try-on results from a source image and a target garment, without the need for structural guidance or auxiliary components to handle diverse inputs. Moreover, inspired by human clothing selection behavior, RefTON leverages additional reference images (the target garment worn on different individuals) to provide powerful guidance for refining texture alignment and maintaining the garment details. To enable this capability, we built a dataset containing unpaired reference images for training. Extensive experiments on public benchmarks demonstrate that RefTON achieves competitive or superior performance compared to state-of-the-art methods, while maintaining a simple and efficient person-to-person design."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 3 6 5 9 0 0 . 1 1 5 2 : r REFVTON: PERSON-TO-PERSON TRY ON WITH ADDITIONAL UNPAIRED VISUAL REFERENCE"
        },
        {
            "title": "A PREPRINT",
            "content": "Liuzhuozheng Li1,2 Yue Gong2 Shanyuan Liu2 Bo Cheng2 Yuhang Ma2 Liebucha Wu2 Dengyang Jiang3 Zanyi Wang4 Dawei Leng2 Yuhui Yin2 1The University of Tokyo 2360 AI Research 3 Hong Kong University of Science and Technology 4University of California San Diego. Code is available at: https://github.com/360CVGroup/RefVTON Figure 1: In-the-wild try-on results generated by our RefVTON model with p2p style, trained on person and garment images from our Virtual Fitting with Reference (VFR) dataset.The first row demonstrates our mask-free try-on capability, where the garment is transferred directly to the target person without masks or pose estimation. The second row shows our additional-reference try-on mode, in which extra visual references are incorporated to enhance structural accuracy, texture fidelity, and overall realism."
        },
        {
            "title": "ABSTRACT",
            "content": "We introduce RefTON, flux-based person-to-person virtual try-on framework that enhances garment realism through unpaired visual references. Unlike conventional approaches that rely on complex auxiliary inputs such as body parsing and warped mask or require finely designed extract branches to process various input conditions, RefTON streamlines the process by directly generating tryon results from source image and target garment, without the need for structural guidance or *Equal Contribution Corresponding Author RefVTON: person-to-person Try on with Additional Unpaired Visual Reference PREPRINT auxiliary components to handle diverse inputs. Moreover, inspired by human clothing selection behavior, RefTON leverages additional reference images (the target garment worn on different individuals) to provide powerful guidance for refining texture alignment and maintaining the garment details. To enable this capability, we built dataset containing unpaired reference images for training. Extensive experiments on public benchmarks demonstrate that RefTON achieves competitive or superior performance compared to state-of-the-art methods, while maintaining simple and efficient person-to-person design."
        },
        {
            "title": "Introduction",
            "content": "The Virtual Try-On (ViTON) model aims to generate photo-realistic images of person wearing target clothing, tool crucial for applications in online retail and personalized fashion systems. ViTON methods are broadly categorized into Generative Adversarial Networks (GANs) [1] and Diffusion Models [2, 3]. Early ViTON research relied on GANs [46], which typically employed warping modules to deform clothing for alignment with the human body, followed by fusion to achieve visual harmony. However, GAN-based approaches frequently generate unrealistic artifacts, particularly when dealing with complex clothing textures or challenging human poses. Recently, methods based on latent diffusion models (LDMs) [7, 8] have gained traction, significantly enhancing clothing warping and addressing structural arrangement and texture preservation during denoising [912]. Despite these advances, current diffusion-based ViTON technologies still generally rely on extensive auxiliary conditions, such as clothing region masks, garment masks, human poses, key points, or multi-modal inputs like text prompts [8, 1316]. Figure 2: The effect of using reference images for the virtual try-on task. From left to right in the three middle subfigures are: (i) results generated without using reference images during either training or inference; (ii) results generated by model trained and inferred with reference images. Incorporating reference images consistently improves the try-on quality and authenticity in both training and inference stages. Please zoom in for more details. Despite the remarkable progress of prior virtual tryon approaches, they are still constrained by two critical limitations that hinder the authenticity of the try-on results and broader applicability: First, these approaches rely on multiple external models and internal modules, such as pose estimators [1721], human parse models [22, 23], segmentation models [24, 25], to process different conditions, which compromises the practicality. To process diverse inputs, additional modules are integrated into the model, which consequently increases the overall framework complexity. Moreover, in practical applications, the quality of conditional inputssuch as the cloth maskhas substantial impact on the quality of the final try-on results; Second, many aspects of clothing, such as style, texture, and detailed design, cannot be fully perceived from the garment image alone; instead, it is more important to consider the overall appearance when model wears the garment. Therefore, in real-world try-on scenarios, such as online shopping, users are typically more interested in model images rather than the garment itself. They tend to see how the target garment looks when worn on real person, rather than relying solely on the isolated garment image as reference. For example, as illustrated in Fig. 2, at the garment in the first row, it is difficult to tell whether it is green translucent fabric or light green opaque one, whereas the reference image clearly reveals its green translucent material. We cannot accurately identify transparent materials or intricate designs, such as lace collars, solely from cloth images. In contrast, the reference images of human models wearing the garments reveal such details. However, existing virtual try-on methods do not support such references due to the lack of corresponding reference data in public datasets [26, 4, 2731]. Based on the above observation, we propose RefTON, flux-based person-to-person virtual try-on framework that achieves strong performance without relying on any external models or auxiliary components, while being further enhanced by additional reference images that offer more accurate and context-aware guidance for the try-on model. First, to ensure the best performance, we adopt the powerful image editing model flux-kontext as our base and apply adaptation on its position index for RoPE [32] to make it suitable for muti-conditonal virtual try-on tasks. Similar to [14], RefTON eliminates the need for auxiliary inputs such as segmentation masks using two-stage training strategy, allowing for simple inference with only source image and the target garment as inputs. Second, we introduce the 2 RefVTON: person-to-person Try on with Additional Unpaired Visual Reference PREPRINT use of images of different person clothed in the target garment as the visual references, like the model images in online shopping, which better reflect users real-world behavior when choosing clothes and enable the preservation of fine garment details that existing methods cannot achieve. To achieve these objectives, we propose reference data generation pipeline, by which construct dataset with supplementary reference images and use unpaired person-cloth sample to train our own model to utilize reference image as additional visual guidance. These improvements empower RefTON to achieve both simplified model structure and streamlined inference process, while simultaneously delivering superior generation results. To the best of our knowledge, RefTON is the first to support both mask-based and mask-free try-on within single model while incorporating reference images into the pipeline. In summary, the main contributions are as follows: We propose incorporating additional reference images into the virtual try-on pipeline. This significantly enhances the authenticity and visual quality of the try-on results, achieving State-of-the-Art performance in preserving fine garment design details. We designed an reference data generation framework to create the necessary reference images for both the clothing and target ground truth samples. Based on this pipeline, we built the VFR dataset upon existing benchmarks (e.g., VITON-HD, DressCode, ViViD), providing robust new resource to improve the practicality and evaluation of virtual try-on models. We present an adaptation of the Flux-Kontext I2I model with modified positional indexing mechanism for handling multi-conditional/resolution inputs in virtual try-on. Leveraging this architecture and two-stage training strategy, our model effectively supports both mask-based and person-to-person try-on tasks, achieving state-of-the-art results and strong generalization to in-the-wild personclothing images."
        },
        {
            "title": "2 Related Works",
            "content": "Generative modeling has advanced rapidly, with diffusion models (DMs) [33], score-based generative models (SGMs) [34], and flow-based methods emerging as leading paradigms. Flow-based methods [3537] have evolved to address inefficiencies in traditional continuous normalizing flows (CNFs)which require expensive backpropagation through ODE solvers during training [38]. breakthrough in this area is Flow Matching (FM) [39], which learns time-dependent vector field to deterministically transport simple prior distribution to the target data distribution. Unlike CNFs, FM uses simulation-free training objective that avoids numerical integration at training time, significantly reducing computational overhead. This approach not only achieves comparable or superior sample quality to DMs but does so with far fewer sampling steps, as it directly parameterizes the probability flow instead of learning stochastic reverse process [39]. The Flux-Kontext architecture offers powerful and flexible method for image editing. Input images are encoded into latent embeddings, which are then flattened into sequences and concatenated with Gaussian noise ϵ. Our model builds upon this architecture, leveraging its flexibility and ability to manipulate and edit images in controlled and simplified way. 2.1 Diffusion-based Virtual Tryon Diffusion models have advanced rapidly in recent years [2, 3], leading to wide range of diffusion-based approaches in the virtual try-on domain [9, 10, 4]. Stable Diffusion [40, 9], with its flexible inpainting and text-guided capabilities, has become widely adopted for virtual try-on tasks. DiffusionCLIP [41] further incorporates CLIP loss to refine the generated images. DCI-VTON [42] follows traditional two-stage pipeline, first warping clothing to align with the body and then fusing the warped garment with the persons image. Subsequently, IDM-VTON [4] introduces dedicated GarmentNet module that guides the garment structure and appearance, further improving visual realism. CatVTON [14], OmniTry [43], Any2AnyTryon [44], and OmniVTON [16] investigate p2p virtual try-on methods that enable mask-free try-on. However, they either require additional conditions, such as human pose, or cannot handle mask-based and mask-free tasks simultaneously. Despite these advances, diffusion-based virtual try-on methods still face notable limitations in practical application and technical completeness, leaving room for further research and optimization. In addition, p2p tryon methods such as TryOffDiff [45] and ViTON-GUN [46] rely on try-offthentry-on pipeline, which could easily lose garment details and accumulate errors. In contrast, our RefVTON skips the try-off stage, directly fitting the target garment onto the person. Guided by the reference image, this approach ensures superior preservation of both garment structure and material fidelity. In summary, existing diffusion-based virtual try-on methods either (1) rely on excessive auxiliary annotations or (2) lack support for reference images of clothed individuals. Our RefTON adapts the virtual try-on task to the Flux-Kontext model to address these limitations, combining person-to-person, reference-aided diffusion training while supporting both mask-based inpainting and mask-free editing for virtual try-on. 3 RefVTON: person-to-person Try on with Additional Unpaired Visual Reference PREPRINT Figure 3: The pipeline of our two-stage training strategy: (a) In the first stage, which follows similar paradigm to mask-based try-on approaches, the model is trained on masked person images to generate person images wearing random garments for the next stage training. (b) In the second stage, the synthesized person images produced in the first stage, along with the target garment and additional reference images (optional), are jointly used as inputs to train person-to-person virtual try-on model that directly fits the target cloth onto the persons body."
        },
        {
            "title": "3 Method",
            "content": "3.1 Preliminary RefTON is built upon DiT [47], scalable Transformer architecture for diffusion-based generation. Images are encoded into latent space via an autoencoder [48] and then patched into tokens [49]. The diffusion process [2] operates on these tokens, with the Transformer consuming noisy tokens and predicting their denoised results. We consider the problem of generating images conditioned on an embedding y, which may encode garment images, semantic maps, human pose, or other modality-specific control signals. Let denote the latent image representation obtained from VAE encoder. The goal of flux.1 [50, 51] is to approximate the conditional distribution p(x y) by learning time-dependent velocity field v(x, y, t) that transports sample from simple prior p0(x) = (0, I) at = 0 to the data distribution pdata(xy) at = 1. The dynamics of the conditional probability density p(x, ty) over time are governed by the continuity equation: p(xy, t) = (cid:0)v(x, y, t) p(xy, t)(cid:1), x0 p0, x1 pdata. (1) To estimate v(x, y, t), we train diffusion-transformer backbone to approximate the neural velocity field vθ using the conditional flow matching objective [39, 52]: Lθ = Et,xi,ϵ,yi (cid:104)(cid:13) (cid:13)vθ(x, yi, t) (xi ϵ)(cid:13) 2 (cid:13) 2 (cid:105) , = (1 t) xi + ϵ, (2) where U(0, 1), xi Xtrain, and ϵ (0, I). This training objective encourages the model to learn velocity field vθ(x, y, t) that consistently guides the noisy samples toward the data distribution conditioned on y, following the probability flow ODE starting from the Gaussian prior: dx = v(x, y, t)dt, (3) 4 RefVTON: person-to-person Try on with Additional Unpaired Visual Reference PREPRINT Figure 4: Adaptation of three-channel position index: the first channel encodes different conditional inputs, while the second and third channels provide spatial positional information for adapting the resolution of the target inputs. enabling controllable image synthesis at inference time. In the virtual try-on setting, let xi denote the image of person wearing the target cloth, and let yi represent collection of conditional inputs, including the cloth-agnostic image ai, the target cloth ci, the target person images pi, the visual references ri, and others. Formally, we write yi = [ai, ci, di, . . . ]. The objective is to progressively transform Gaussian noise sample ϵ into the target image xi guided by conditions yi. 3.2 Person To Person Virtual Try-on Model with Two Stage Training Unlike prior methods that first mask the target clothing region to create an agnostic image, our goal is to fit the target garment onto the person directly, without relying on additional conditions such as densepose [17] or segmentation masks. To achieve this, we train diffusion model on pairs of clothing images ci and person images wearing different garments pi,cj , as shown in Fig. 3(b). This requires constructing training set of unpaired images of the same person in different clothes, denoted as [ pi,cj , ci, pi,ci], where the subscript cj indicates garment other than ci. Unfortunately, such unpaired clothperson data pairs [ pi,cj , ci, pi,ci ] are not available in current open-source benchmarks and most of them only contain data pair [ci, pi,ci]. Therefore, we synthesize unpaired person images pi,cj using mask-based try-on model, which serves as the first stage of our RefTON training strategy. As illustrated in Fig. 3(a), we train virtual try-on model using agnostic person images ai, clothing images ci, densepose maps di, warp masks mi, and other auxiliary conditions as inputs. During inference, random unpaired garment cj is selected to generate the corresponding synthesized person image pi,cj . To ensure the quality of the synthesized person image pi,cj , the agnosticcloth pairs [ai, ci] used for training must belong to the same garment category (e.g., if ai is from the dresses subset, the selected garment cj should also come from dresses rather than upper body or lower body). Otherwise, the generated person image may appear unrealistic due to mismatches between the clothing mask region and the target garment (e.g., fitting skirt onto the upper body or shirt onto the lower body). 3.3 Multi-input Adaptation of RefTON After obtaining the unpaired person images, we train our person-to-person model by replacing the agnostic images ai with the corresponding person images pi, which the first stage model generates. To enable the model to handle both masked and unmasked inputs flexibly, we randomly feed agnostic and person images with equal probability (50%) during training. Instead of training try-on model from scratch, we adopt Flux-Kontext [51] as the foundation model, freeze the parameters of its encoder and decoder, and apply Low-Rank Adaptation (LoRA) [53] to the MM-DiT blocks for parameter-efficient fine-tuning. The training objective follows the same flow-matching loss as defined in Equation 2, where ai and pi are alternately used to ensure compatibility with both input types. To enable the model to extract visual information from the target garment worn by another person, an additional reference image ri is provided as input with certain probability (25%). Its latent embeddings are appended to the latent feature sequence [ϵi, ai, ci] after the image encoder, as illustrated in Fig. 4. 5 RefVTON: person-to-person Try on with Additional Unpaired Visual Reference PREPRINT Figure 5: The overall pipeline of generating the reference images. We first generate the appearance descriptions using Qwen2.5-VL [54], and then concatenate the appearance with the corresponding actions and outfits to construct the positive and negative prompts, as shown in (a). Subsequently, the images and the textual prompts are fed into the Editing Model, which generates photos of individuals wearing the same clothes. These results are reference images for each imagecloth pair, as shown in (b). Some prior work [44] leverages adaptive position embeddings for virtual try-on. Similarly, Flux-Kontext employs an index generator to produce position index of shape [L, 3] for the image embedding sequence, which is then transformed into Rotary position Embeddings (RoPE) to provide spatial cues. The first channel distinguishes Gaussian noise ϵi from conditional embeddings, while the second and third encode horizontal and vertical coordinates  (Fig. 4)  . Building on this design, our model converts conditional inputs into latent feature after Auto-Encoder and flatten into D-dimensional latent embeddings, denoted as [ai, ci, ri], and concatenates them with Gaussian noise embeddings ϵi along the sequence dimension. This formulation allows conditional inputs of multi-resolutions to be concatenatedunlike [44], which concatenates noisy and conditional inputs directly in pixel space, requiring them to share the same image size. Each embedding is assigned three-channel position index: the first channel is extended from binary flag to four discrete values to distinguish multiple conditions, and the second and third channels store integer spatial coordinates. To handle multi-resolution conditions, we further rescale these position indices by the resolution ratio between the target and conditional images, ensuring numerical consistency across resolutions. The modified DiT position index design and the overall RefTON architecture are illustrated in Fig. 4. 3.4 Virtual-Tryon Generation with Extra Visual Reference For humans, particularly in e-commerce shopping, we believe that relying solely on garment images and conditions extracted by external models, as done in previous methods, is insufficient to imagine the realistic visual effects of fitting target garment onto ones body. garments style, texture, and fine design elements are more faithfully represented when worn by another person, whereas viewing the garment in isolation fails to convey these nuances. The same applies to generative virtual try-on models, as illustrated in Fig. 2. Therefore, we argue that relying on cloth images and conditions extracted by external models, as in previous methods, is insufficient to capture the realistic visual effect of fitting target garment onto person. By incorporating reference person images, that the target clothes worn by different person ri, into the virtual try-on generation process, the model can similarly get more direct and intuitive visual guidance. To provide the model with reference person images ri, we construct pairs in the form of different persons wearing the target garment, denoted as [ci, ri]. Such reference images ri are not available in existing open-source virtual try-on datasets, similar to the person images pi. In this subsection, we introduce our reference data generation pipeline, which synthesizes the required reference images, enriching current virtual try-on datasets with supplementary references and enabling our model to be trained with additional visual guidance. Existing open-source datasets, including VITON-HD [4], DressCode [26], and IGPairs [29], lack unpaired reference images ri showing the different persons wearing the target garment, thereby constraining the models training. We employ generative models to synthesize such reference images to overcome this limitation. For the generated reference data ri to provide accurate and intuitive visual guidance, we argue that they should meet the following requirements: 6 RefVTON: person-to-person Try on with Additional Unpaired Visual Reference PREPRINT Figure 6: Qualitative comparison on the VITON dataset. and the model is trained following the pipeline in Fig. 3 (b). Ref denotes the additional reference ri image is used during the inference, while mask-free indicates that the image is generated using an unmasked person pi image instead of masked agnostic image ai. Preserve the target garment faithfully. The target clothings color, texture, and design must remain unchanged to ensure accurate reference. Introduce diversity in persons appearance. The person wearing the target cloth in the reference image should unlike the target person wearing the same garment. Otherwise, the model may learn shortcut and overfit to the target image. This diversity can be achieved by altering hairstyle, hair/skin color, body pose, or facial expressions. Vary the non-target garments to provide outfit diversity. While the target garment remains unchanged, other garments should be modified. For example, if the target garment is an upper-body item, the reference image should retain the same upper-body garment but alter the lower-body clothing, shoes, or accessories. The overall generation pipeline for reference data is illustrated in Figure 5. As shown in the pipeline, we employ Flux-Kontext [51] to generate reference images from the target descriptions, leveraging its strong capability to maintain consistency with the input image (1). To address requirement (2), we describe the target image and then intentionally provide an alternative description that ensures the reference image differs from the target input. For this purpose, we use Qwen2.5-VL [54] to generate detailed descriptions of the models appearance, and instruct it to produce variants that do not resemble the original model. Finally, to fulfill requirement (3), we curate list of garment descriptions representing diverse non-target clothing items. The outlook, action, and outfit descriptions are concatenated as the positive prompt. In contrast, the original image description is used as the negative prompt, and both are fed into the generative model to synthesize reference images. We supplement existing virtual try-on benchmarks including VITON-HD [5], DressCode [26], ViViD [27], FashionTryOn [28] and IGPairs [29] by generating corresponding reference pairs for each target garment image ci, forming data pairs [ci, ri] that are used for both model training and evaluation.Some open-source datasets, such as FashionTryOn [28] and IGPairs [29], contain numerous duplicated or low-quality samples. We compare the CLIP features of images to filter out redundant samples and employ Qwen2.5-VL to identify distorted or unclear images, as well as images where the person faces away from the camera, ensuring overall data quality before generating the final reference set via our data generation pipeline. Finally, we enrich open-source virtual try-on datasets with additional visual references ri and person images pi, and combine them to form our own dataset, named Virtual Fitting with RefVTON: person-to-person Try on with Additional Unpaired Visual Reference PREPRINT Table 1: Quantitative comparison across VITON-HD [4] and DressCode [26]. The best and second best results are shown in bold and underline. +R denotes the use of reference images, and MF indicates mask-free inputs. Missing values are shown as . Subscripts and represent the paired and unpaired test settings, respectively. Unless otherwise specified, the same notations carry the same meanings throughout the figures and tables in this paper. Method CAT-DM [59] IDM-VTON [12] OOTDiffusion [8] CatVTON [14] CatVT2ON [15] OmniVTON [16] PromptDresser PromptDresser [13] RefTON (Ours) RefTON+R (Ours) pose [13] CatVTON(MF) [14] Any2AnyTryon [44] TryOffDiff [45] RefTON/MF (Ours) RefTON+R/MF(Ours) Input VITON-HD DressCode Mask Pose LPIPSp SSIMp FIDp KIDp FIDu KIDu LPIPSp SSIMp FIDp KIDp FIDu KIDu 0.080 0.102 0.071 0.057 0.057 0.145 0.097 0.112 0.057 0.049 0.061 0.088 0.061 0.053 0.877 0.870 0.878 0.870 0.890 0.832 0.878 0.869 0.873 0.879 0.870 0.839 0.866 0.872 5.60 6.29 8.81 5.43 8.10 7.76 9.07 8.54 5.45 4.69 5.89 6.93 5.98 5. 0.83 0.82 0.41 2.25 1.16 0.67 0.82 0.68 8.93 9.02 11.22 9.62 8.58 8.43 Mask-Free setting 0.51 0.74 1.04 0.82 9.29 8.97 11.9 8.40 8.32 1.37 1.09 2.99 1.06 0. 1.17 0.98 2.60 0.81 0.78 0.062 0.045 0.046 0.037 0.119 0.037 0.031 0.045 0.041 0.035 0.920 0.927 0.892 0.922 0.865 0.912 0.918 0.902 0.901 0.906 8.64 4.20 3.99 5.72 5.34 3.48 2. 4.78 3.84 3.34 0.90 0.37 0.82 2.34 1.20 0.95 1.30 1.33 1.15 6.14 8.63 6.45 5.31 5.07 7.40 7.90 5.00 5.02 1.40 3.84 1.36 1. 2.62 2.70 1.17 1.28 Reference (VFR), for training our RefTON model. The detailed data collection, filtering procedures, and samples visualization are provided in the Appendix."
        },
        {
            "title": "4 Experiments",
            "content": "We evaluate the effectiveness of our proposed method on two public benchmarks, DressCode [26] and VITON-HD [4], both containing images with resolution of 1024 768. The VITON-HD dataset comprises 13,670 upper-body image pairs of women, split into 11,647 pairs for training and 2,032 pairs for testing. The DressCode dataset includes three subsetsupper body, lower body, and dresseswith 48,392 training and 5,400 testing pairs. Since DressCode does not provide wrapped cloth masks or agnostic images ai, we generate them using the mask generation tool from CatVTON [14]. During training, we fine-tune our model on the flux-kontext backbone using the Low-Rank Adaptation (LoRA) technique, with rank of 64 and α = 128, optimized by AdamW. For quantitative evaluation, all generated images are resized to 512 384 to ensure fair comparison with previous methods. In single-dataset experiment, the model is trained independently for 20,000 steps on VITON-HD and 48,000 on DressCode with batch size of 128, using 8 NVIDIA H100 GPUs. To further enhance generalization and robustness to in-the-wild inputs, we train an additional model on the mixed VFR dataset introduced in Sec. 3.4. We report cross-dataset evaluation results of our RefVTON model at resolution of 1024 768 on both VITON-HD and DressCode, and demonstrate its in-the-wild performance using images captured by ourselves and collected from the internet. 4.1 Quantative result We evaluate the numerical results of our virtual try-on model on the VITON and DressCode datasets, distinguishing between paired and unpaired try-on settings. For paired try-on settings with ground truth in test datasets, we utilize four widely used metrics to assess the similarity between the synthesized images and their corresponding authentic images: the Structural Similarity Index (SSIM) [55], Learned Perceptual Image Patch Similarity (LPIPS) [56], Fréchet Inception Distance (FID)[57], and Kernel Inception Distance (KID)[58]. For unpaired settings, where we measure the distributional similarity between the synthesized and real samples, we specifically rely on the Fréchet Inception Distance (FID) and Kernel Inception Distance (KID). As shown in Table4. Our method (RefVTON) consistently performs better than prior baselines, demonstrating higher try-on fidelity and strong alignment with the target persons pose. With the addition of reference images (+R), the quality and detail consistency of the try-on results is further improved compared with the results without reference images, establishing new state-of-the-art results across multiple metrics. Notably, even in the mask-free settingwithout agnostic masks or auxiliary inputsour method maintains garment style correctness and pose consistency, while reaching accuracy on par with or superior to baseline methods, highlighting its robustness and practicality. Table 4 and Table 2 summarizes the quantitative evaluation on the DressCode dataset. Our method (RefVTON) outperforms all baselines, delivering higher try-on quality and achieving strong consistency with the target persons RefVTON: person-to-person Try on with Additional Unpaired Visual Reference PREPRINT Table 2: Quantitative results on three subsets of the DressCode dataset [26]: upper body, lower body, and dresses. The best and second-best results are highlighted in bold and underline, respectively. The symbol * denotes results reported in prior work, while +R indicates results with reference image input, and MF refers to mask-free input images. The subscripts and denote specific evaluation metrics for precision and recall, respectively. Method Upper-body Lower-body Dresses FIDp KIDp FIDu KIDu FIDp KIDp FIDu KIDu FIDp KIDp FIDu KIDu CAT-DM [59] OOTDiffusion [8] PromptDresser [13] RefTON(Ours) RefTON+R(Ours) RefTON/MF(Ours) RefTON+R/MF(Ours) 9.85 11.03 11. 7.62 6.39 8.37 7.20 2.38 0.29 0.74 1.10 0.85 1.43 1.18 12.62 11.13 11.08 11.20 11.53 1.89 0.98 0.87 1.11 1.12 10.25 9.72 12. 7.60 6.61 8.79 7.85 1.81 0.64 1.46 1.38 1.05 1.51 1.21 14.83 13.07 12.56 12.50 12.74 2.82 2.11 1.67 1.83 2.05 10.71 10.65 11. 7.32 6.09 7.36 6.24 2.02 0.54 1.10 1.30 1.16 1.33 1.20 14.30 11.56 11.16 10.73 10.05 3.36 1.98 1.72 1.41 1.30 Figure 7: Qualitative comparison on the DressCode dataset., and the model is trained following the pipeline in Fig. 3 (b). Ref. denotes the additional reference ri image is used during the inference, while mask-free indicates that the image is generated using an unmasked person pi image instead of masked agnostic image ai. pose and body structure. Integrating reference images (+R) further enhances the results, establishing new state-ofthe-art. Importantly, even in the mask-free settingwithout agnostic masks or additional inputsour method correctly preserves garment styles (e.g., clothing length and design) and maintains high pose alignment, while achieving accuracy comparable to or surpassing prior baselines, demonstrating robustness and practicality. 4.2 QUALITATIVE COMPARISON Although the previous method can fit the garment onto human body, the generated results often fail to accurately capture the garments cut, style, details, and other characteristics that determine how it should be worn. In contrast, our method achieves superior visual fidelity compared to baselines. As shown in Fig. 6, on the VITON dataset, our RefVTON produces highly realistic rendering of challenging garment materials such as hollow or semi-transparent fabrics without relying on reference images. For example, in the case of garments with hollow lace structures, our model accurately preserves the delicate perforated patterns and material transparency. In contrast, the baselines fail to reconstruct these details, often generating incorrect solid textures or spurious dotted patterns. Moreover, our approach demonstrates the best preservation of garment patterns: the printed letters and logos on clothes remain clear and consistent with the input, highlighting its advantage in maintaining fine-grained appearance details. The generation quality is further enhanced with the addition of reference images, which intuitively verify the effectiveness of the proposed tryon method. Even without using agnostic masks and by directly transferring the clothing ci onto person images pi,cj , our method still outperforms most baseline approaches. Our model also performs well on benchmark comprising wider variety of clothing types. As shown in Fig. 7, we evaluate our method on the DressCode dataset, where garments are categorized into three types: upper body, lower 9 RefVTON: person-to-person Try on with Additional Unpaired Visual Reference PREPRINT Table 3: We compare our approach with [8] under cross-dataset evaluation, reporting results on both paired and unpaired settings for the VITON-HD and DressCode datasets. The best results are highlighted in bold. The * marker refers to the results reported in previous work. The symbol +R denotes results with reference image input, and MF indicates mask-free input images. Subscripts and represent the paired and unpaired test settings, respectively. Methods VITON-HD DressCode SSIM FIDp KIDp LPIPS FIDu KIDu SSIM FIDp KIDp LPIPS FIDu KIDu OOTDiffusion [8] 0.839 11.22 EVTAR(Ours) EVTAR+R(Ours) 0.851 0.859 6.23 5.13 EVTAR/MF(Ours) EVTAR+R/MF(Ours) 2.72 0.80 0.62 0. 0.072 0.060 9.11 8.59 8.88 8.39 0.915 11.96 1.08 0.87 0.82 0.65 0.896 0.903 3.70 3. 1.21 1.13 0.97 0. 0.045 0.038 5.22 5.03 5.03 4.87 1.20 1.11 1.23 1.10 body, and dresses. Our approach produces more faithful and natural try-on results compared to previous methods. In particular, it renders reflective materials such as leather and metallic fabrics with superior realism, avoiding the over-smoothing or distortion artifacts commonly observed in other methods. Moreover, even without agnostic masks, our model can still perform consistent try-on guided by garment style, accurately preserving the length, structure, and overall design without introducing mismatched or inconsistent clothing shapes. 4.3 Training on VRF Dataset and Evaluation To better evaluate the effectiveness of our proposed person-to-person virtual try-on framework and reference data construction pipeline, we train our RefVTON model on the VRF dataset collected using our data engine introduced in sec 3.4. The training hyperparameters are consistent with those used in the DressCode and VITON-HD experiments. For evaluation, we quantitatively test the generated try-on images on the DressCode and VITON-HD test sets, with quantitative metrics reported in Table 3. Our method (RefVTON) consistently achieves superior or comparable performance to OOTDiffusion [8] across both VITON-HD and DressCode benchmarks, in both paired and unpaired settings. Although RefVTON is not explicitly trained on DressCode or VITON-HD, the model trained on the mixed VRF dataset outperforms dataset-specific baselines across most metrics (e.g., FID and LPIPS), demonstrating strong generalization capability. These results highlight the effectiveness of our approach in learning transferable representations that maintain robustness across diverse datasets. 4.4 Ablation Study We conduct an ablation study to examine our model under four settings (w/&w/o mask, w/&w/o Ref.). As shown in Table 4, our model maintains consistently strong performance across all settings. Introducing reference image yields clear improvements in both mask-based and maskfree modes, while moving from mask-based to mask-free inputs causes only mild metric fluctuations, confirming the models stable robustness without masks. Fig. 8 further provides qualitative comparisons. The first two rows show that reference images help the model correctly infer garment structures and materials that are ambiguous in the flat images (e.g., hollow textures, semitransparency). The last two rows illustrate that mask quality heavily affects mask-dependent models: overly aggressive masks remove important items (e.g., handbags), while conservative masks retain unwanted regions (e.g., legs), leading to incorrect garment geometry. In contrast, our mask-free model consistently produces correct outputs regardless of mask or reference conditions, demonstrating that mask-free capability reduces reliance on mask quality and enables more flexible and stable try-on performance. Figure 8: Qualitative results of the ablation study across different settings. Ref. denotes that reference image is provided, while MF indicates mask-free inputs using the original person image instead of masked agnostic image. 10 RefVTON: person-to-person Try on with Additional Unpaired Visual Reference PREPRINT"
        },
        {
            "title": "5 Conclusion",
            "content": "This paper introduces RefTON, virtual try-on framework that supports both mask-based and mask-free inference and leverages reference images to guide the try-on process. We extend Flux-Kontext to handle multi-condition inputs of varying resolutions via modified positional index. To train RefTON, we propose reference data generation pipeline integrating Qwen2.5-VL and Flux-Kontext. This design allows RefTON to faithfully preserve translucent fabrics, intricate designs, and fine details, consistently outperforming existing methods both quantitatively and qualitatively, achieving state-of-the-art performance in all settings."
        },
        {
            "title": "References",
            "content": "[1] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. [2] Jonathan Ho, Ajay Jain, and P. Abbeel. Denoising diffusion probabilistic models. ArXiv, abs/2006.11239, 2020. URL https://api.semanticscholar.org/CorpusID:219955663. [3] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1067410685, 2021. URL https://api.semanticscholar.org/CorpusID:245335280. [4] Seunghwan Choi, Sunghyun Park, Minsoo Lee, and Jaegul Choo. Viton-hd: High-resolution virtual try-on via misalignment-aware normalization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1413114140, 2021. [5] Xintong Han, Zuxuan Wu, Zhe Wu, Ruichi Yu, and Larry Davis. Viton: An image-based virtual try-on network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 75437552, 2018. [6] Bochao Wang, Huabin Zheng, Xiaodan Liang, Yimin Chen, Liang Lin, and Meng Yang. Toward characteristicpreserving image-based virtual try-on network. In Proceedings of the European conference on computer vision (ECCV), pages 589604, 2018. [7] Mengting Chen, Xi Chen, Zhonghua Zhai, Chen Ju, Xuewen Hong, Jinsong Lan, and Shuai Xiao. Wear-any-way: Manipulable virtual try-on via sparse correspondence alignment. In European Conference on Computer Vision, pages 124142. Springer, 2024. [8] Yuhao Xu, Tao Gu, Weifeng Chen, and Chengcai Chen. Ootdiffusion: Outfitting fusion based latent diffusion In AAAI Conference on Artificial Intelligence, 2024. URL https://api. for controllable virtual try-on. semanticscholar.org/CorpusID:268247604. [9] Jeongho Kim, Guojung Gu, Minho Park, Sunghyun Park, and Jaegul Choo. Stableviton: Learning semantic correspondence with latent diffusion model for virtual try-on. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 81768185, 2024. [10] Luyang Zhu, Dawei Yang, Tyler Zhu, Fitsum Reda, William Chan, Chitwan Saharia, Mohammad Norouzi, and Ira Kemelmacher-Shlizerman. Tryondiffusion: tale of two unets. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 46064615, 2023. [11] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human image animation using diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14811490, 2024. [12] Yisol Choi, Sangkyung Kwak, Kyungmin Lee, Hyungwon Choi, and Jinwoo Shin. Improving diffusion models for authentic virtual try-on in the wild. In European Conference on Computer Vision, pages 206235. Springer, 2024. [13] Jeongho Kim, Hoiyeong Jin, Sunghyun Park, and Jaegul Choo. Promptdresser: Improving the quality and controllability of virtual try-on via generative textual prompt and prompt-aware mask. arXiv preprint arXiv:2412.16978, 2024. [14] Zheng Chong, Xiao Dong, Haoxiang Li, Shiyue Zhang, Wenqing Zhang, Xujie Zhang, Hanqing Zhao, Dongmei Jiang, and Xiaodan Liang. Catvton: Concatenation is all you need for virtual try-on with diffusion models. arXiv preprint arXiv:2407.15886, 2024. [15] Zheng Chong, Wenqing Zhang, Shiyue Zhang, Jun Zheng, Xiao Dong, Haoxiang Li, Yiling Wu, Dongmei Jiang, and Xiaodan Liang. Catv2ton: Taming diffusion transformers for vision-based virtual try-on with temporal concatenation. arXiv preprint arXiv:2501.11325, 2025. [16] Zhaotong Yang, Yuhui Li, Shengfeng He, Xinzhe Li, Yangyang Xu, Junyu Dong, and Yong Du. Omnivton: Training-free universal virtual try-on. arXiv preprint arXiv:2507.15037, 2025. 11 RefVTON: person-to-person Try on with Additional Unpaired Visual Reference PREPRINT [17] Rıza Alp Güler, Natalia Neverova, and Iasonas Kokkinos. Densepose: Dense human pose estimation in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 72977306, 2018. [18] Alexander Toshev and Christian Szegedy. Deeppose: Human pose estimation via deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 16531660, 2014. [19] Z. Cao, G. Hidalgo Martinez, T. Simon, S. Wei, and Y. A. Sheikh. Openpose: Realtime multi-person 2d pose estimation using part affinity fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2019. [20] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Realtime multi-person 2d pose estimation using part affinity fields. In CVPR, 2017. [21] Shih-En Wei, Varun Ramakrishna, Takeo Kanade, and Yaser Sheikh. Convolutional pose machines. In CVPR, 2016. [22] Peike Li, Yunqiu Xu, Yunchao Wei, and Yi Yang. Self-correction for human parsing. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020. doi:10.1109/TPAMI.2020.3048039. [23] Jian Dong, Qiang Chen, Xiaohui Shen, Jianchao Yang, and Shuicheng Yan. Towards unified human parsing and pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 843850, 2014. [24] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer In Proceedings of the IEEE/CVF Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. international conference on computer vision, pages 40154026, 2023. [25] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollár, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. URL https://arxiv.org/abs/2408.00714. [26] Davide Morelli, Matteo Fincato, Marcella Cornia, Federico Landi, Fabio Cesari, and Rita Cucchiara. Dress code: High-resolution multi-category virtual try-on. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 22312235, 2022. [27] Zixun Fang, Wei Zhai, Aimin Su, Hongliang Song, Kai Zhu, Mao Wang, Yu Chen, Zhiheng Liu, Yang Cao, and Zheng-Jun Zha. Vivid: Video virtual try-on using diffusion models. arXiv preprint arXiv:2405.11794, 2024. [28] Na Zheng, Xuemeng Song, Zhaozheng Chen, Linmei Hu, Da Cao, and Liqiang Nie. Virtually trying on new clothing with arbitrary poses. In Proceedings of the 27th ACM international conference on multimedia, pages 266274, 2019. [29] Fei Shen, Xin Jiang, Xin He, Hu Ye, Cong Wang, Xiaoyu Du, Zechao Li, and Jinghui Tang. Imagdressingv1: Customizable virtual dressing. In AAAI Conference on Artificial Intelligence, 2024. URL https://api. semanticscholar.org/CorpusID:271244829. [30] Zijian He, Peixin Chen, Guangrun Wang, Guanbin Li, Philip HS Torr, and Liang Lin. Wildvidfit: Video virtual try-on in the wild via image-based controlled diffusion models. arXiv preprint arXiv:2407.10625, 2024. [31] Ming Meng, Qi Dong, Jiajie Li, Zhe Zhu, Xingyu Wang, Zhaoxin Fan, Wei Zhao, and Wenjun Wu. Hf-vton: High-fidelity virtual try-on via consistent geometric and semantic alignment. arXiv preprint arXiv:2505.19638, 2025. [32] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [33] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 22562265. PMLR, 2015. [34] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Scorebased generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=PxTIG12RRHS. [35] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv preprint arXiv:1605.08803, 2016. [36] Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components estimation. arXiv preprint arXiv:1410.8516, 2014. [37] Durk Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. Advances in neural information processing systems, 31, 2018. RefVTON: person-to-person Try on with Additional Unpaired Visual Reference PREPRINT [38] Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David Kristjanson Duvenaud. Neural ordinary differential equations. In Neural Information Processing Systems, 2018. URL https://api.semanticscholar.org/ CorpusID:49310446. [39] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. ArXiv, abs/2210.02747, 2022. URL https://api.semanticscholar.org/CorpusID:252734897. [40] Davide Morelli, Alberto Baldrati, Giuseppe Cartella, Marcella Cornia, Marco Bertini, and Rita Cucchiara. Ladivton: Latent diffusion textual-inversion enhanced virtual try-on. In Proceedings of the 31st ACM international conference on multimedia, pages 85808589, 2023. [41] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Diffusionclip: Text-guided diffusion models for robust image manipulation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 24262435, 2022. [42] Junhong Gou, Siyu Sun, Jianfu Zhang, Jianlou Si, Chen Qian, and Liqing Zhang. Taming the power of diffusion models for high-quality virtual try-on with appearance flow. In Proceedings of the 31st ACM International Conference on Multimedia, pages 75997607, 2023. [43] Yutong Feng, Linlin Zhang, Hengyuan Cao, Yiming Chen, Xiaoduan Feng, Jian Cao, Yuxiong Wu, and Bin Wang. Omnitry: Virtual try-on anything without masks. arXiv preprint arXiv:2508.13632, 2025. [44] Hailong Guo, Bohan Zeng, Yiren Song, Wentao Zhang, Jiaming Liu, and Chuang Zhang. Any2anytryon: Leveraging adaptive position embeddings for versatile virtual clothing tasks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1908519096, 2025. [45] Riza Velioglu, Petra Bevandic, Robin Chan, and Barbara Hammer. Enhancing person-to-person virtual try-on with multi-garment virtual try-off. arXiv preprint arXiv:2504.13078, 2025. [46] Nannan Zhang, Zhenyu Xie, Zhengwentai Sun, Hairui Zhu, Zirong Jin, Nan Xiang, Xiaoguang Han, and Song Wu. Viton-gun: Person-to-person virtual try-on via garment unwrapping. IEEE Transactions on Visualization and Computer Graphics, 31(10):77407751, 2025. doi:10.1109/TVCG.2025.3550776. [47] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [48] Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. [49] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [50] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [51] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Müller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. URL https: //arxiv.org/abs/2506.15742. [52] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. [53] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [54] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [55] Jim Nilsson and Tomas Akenine-Möller. Understanding ssim. arXiv preprint arXiv:2006.13846, 2020. [56] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. [57] Maximilian Seitzer. pytorch-fid: FID Score for PyTorch. https://github.com/mseitzer/pytorch-fid, August 2020. Version 0.3.0. [58] Mikołaj Binkowski, Danica Sutherland, Michael Arbel, and Arthur Gretton. Demystifying mmd gans. arXiv preprint arXiv:1801.01401, 2018. [59] Jianhao Zeng, Dan Song, Weizhi Nie, Hongshuo Tian, Tongtong Wang, and An-An Liu. Cat-dm: Controllable accelerated virtual try-on with diffusion model. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83728382, 2024. 13 RefVTON: person-to-person Try on with Additional Unpaired Visual Reference PREPRINT"
        },
        {
            "title": "A Appendix",
            "content": "A.1 Detailed Reference Image Generation Process This section provides detailed description of the process for generating the reference image. Many virtual try-on datasets offer the garment image and the image of the target person wearing the target garment, but they do not include the reference image, which shows the visual effect of the target garment ci being worn by another person pcj . The generation of the reference image can be viewed as an image editing task on pcj . As discussed in Section 3.3, the reference image ri must satisfy three key requirements. Firstly, the reference image should faithfully preserve the details of the target garment ci. This requires the editing model to have strong consistency abilities. The Flux-Kontext model has robust ability to edit the target region corresponding to the text prompt while keeping unrelated regionssuch as the area not related to the garmentunchanged. Moreover, the Flux-Kontext model can perform precise local editing according to the text prompt, in this case, focusing on the person under the garment. Therefore, we choose the popular Flux-Kontext model [51] to edit the input image conditioned on the text prompt. Specifically, we add sentence such as keep the {target cloth} cloth unchanged in the text prompt. Secondly, the the person in the reference image should look significantly different from the person in the target image. During training, we observed that if the reference image is too similar to the target image, the model tends to rely on \"shortcut\"directly copying from the reference image and ignoring the agnostic/person image ai/pi and the cloth ci. To avoid this, we ensure that the person in the reference image differs from the target image to better showcase the visual effect of the clothing when worn, rather than focusing on the appearance of the person themselves. To achieve this, we utilize the Text-to-Image (T2I) capabilities of the Flux-Kontext model. We extract an accurate description of the persons appearance in the target image (e.g., \"The model has an East Asian appearance, with light skin, long black hair, and neutral expression...\") and pass it as the negative prompt to the T2I model. In contrast, we provide an opposite description (e.g., \"The model has an African appearance, with dark skin, short yellow hair, and cheerful expression...\") as the positive prompt to guide the editing of the target image, as shown in Figure 4(b). However, extracting descriptions for each image manually is labor-intensive. To address this, we use vision-language model (VLM) such as Qwen2.5-VL to automatically generate the description and its opposite. Specifically, we pass the target image pi,ci to the VLM and provide prompt like Start with Positive: describe only the models race, skin, hair, eyes, and expression, then give the opposite in one sentence with Negative: changing those traits without not or clothing. The generated description and its opposite are then fed into the negative and positive prompt encoders of the T2I model to edit the target image, as shown in Figure 4(a). In this way, we automate the generation of the reference image ri by editing the target image. Thirdly, after extracting the description and opposite description of the persons appearance, we introduce more diversity into the reference image by varying the non-target garments and actions of the human in the target image. This can be achieved through the image editing model by adding descriptions related to non-target garments and actions. We provide description bank containing candidate descriptions for outfits and actions across three scenarios: the person in the image is wearing dress, an upper-body garment, or lower-body garment. This ensures that the description of the editable garment differs from the target garment ci. Furthermore, the outfit descriptions also include accessories such as glasses, wristwatches, and bracelets to increase diversity. These descriptions, along with the actions and outfit details, are concatenated into positive prompts and passed to the T5 text encoder, as shown in Figure 4. Fig 9 illustrates selected text prompts from the prompt description bank used for reference image generation. Furthermore, Fig 10 exhibits selection of the resulting reference data samples. A.2 Further Quantitative Evaluation on High Resolution Here we provide more evaluation results of the generated image. To further evaluate our method under high-resolution settings, Table 4 reports quantitative results on both VITON-HD and DressCode at resolution of 1024. Across the paired and unpaired protocols, RefTON and RefTON+R consistently achieve high performance on nearly all metrics. Notably, the mask-free variants (RefTON/MF and RefTON+R/MF) deliver particularly strong results. These results demonstrate that our framework scales effectively to high-resolution synthesis and remains robust across diverse virtual try-on settings. A.3 Additional Qualitative Results In this section, we provide various visualizations to further demonstrate the robustness, generalization ability, and high-fidelity performance of our method across different datasets, clothing categories, and evaluation settings. RefVTON: person-to-person Try on with Additional Unpaired Visual Reference PREPRINT Figure 9: Text prompts from the Outfit and Action Description Bank. To ensure the model edits only the person while preserving the target clothing, we assign different outfits and action description categories to different clothing inputs. Table 4: Quantitative comparison across VITON-HD [4] and DressCode [26] at resolution of 1024. The best and second best results are shown in bold and underline. +R denotes the use of reference images, and MF indicates mask-free inputs. Subscripts and represent the paired and unpaired test settings, respectively. Unless otherwise specified, the same notations carry the same meanings throughout the figures and tables in this paper. Method LPIPSp SSIMp FIDp KIDp FIDu KIDu LPIPSp SSIMp FIDp KIDp FIDu KIDu VITON-HD DressCode RefTON (Ours) RefTON+R (Ours) 0.079 0. RefTON/MF (Ours) RefTON+R/MF (Ours) 0.068 0.067 0.870 0.873 0.880 0.875 5.96 5.25 5.02 4. Mask-based setting 1.05 0.97 0.85 0.71 8.91 9.10 1.15 1.41 Mask-Free setting 8.87 8.98 1.05 1.22 0.056 0.052 0.028 0.030 0.899 0.902 0.956 0. 3.28 2.84 1.03 1.15 0.76 0.65 0.19 0.25 4.84 4.73 4.24 4. 0.83 0.76 0.59 0.69 As shown in Fig. 11, 12, 13, 14, and 15, our approach consistently preserves garment details, structural correctness, and texture realism under both paired and unpaired scenarios, with or without mask-free (MF) inputs. Moreover, as illustrated in Fig.??, even in challenging in-the-wild conditions, our model exhibits strong robustnessmaintaining accurate body pose, preserving background integrity, and producing stable try-on results without introducing artifacts or unintended changes. Complex Patterns (VITON-HD). As shown in Fig. 11, our model faithfully reproduces complex and fine-grained clothing patterns. Even for garments with dense textures, irregular motifs, or high-frequency visual elements, the generated results retain clear, sharp, and recognizable patterns with minimal distortion. The strong pattern-preservation ability highlights the effectiveness of our approach in capturing both global appearance and subtle local details. Complex Structures (VITON-HD). Fig. 12 shows that our method handles garments with challenging structural designs, such as multi-layered regions, unique silhouettes, or uncommon shapes. The generated try-on results maintain correct garment geometry, coherent contours, and physically plausible spatial arrangements. This demonstrates that our framework models structural priors robustly, enabling accurate synthesis even under significant variations in shape. RefVTON: person-to-person Try on with Additional Unpaired Visual Reference PREPRINT Figure 10: Sample reference images generated by our reference data generation pipeline. The editing model takes the target persons image as input and synthesizes corresponding reference images, while preserving the garments appearance to match the cloth. DressCode Upper-body, Lower-body, and Dress Sub-sets. As shown in Figs. 13, 14, and 15, our approach performs consistently well across the three DressCode subsets. In the unpaired and mask-free settings, our model successfully preserves fabric materials, shading, and texture characteristics while achieving realistic alignment between the garment and human body. Across diverse clothing typesincluding tops, pants, skirts, and full-body dressesthe synthesized results maintain stable structure, smooth boundaries, and visually coherent integration, demonstrating strong generalization and robustness. 16 RefVTON: person-to-person Try on with Additional Unpaired Visual Reference PREPRINT Figure 11: Qualitative paired results in VITON-HD dataset with complex patterns on clothes. reference denotes that reference image is provided. 17 RefVTON: person-to-person Try on with Additional Unpaired Visual Reference PREPRINT Figure 12: Qualitative paired results in VITON-HD dataset with complex structure on clothes. reference denotes that reference image is provided. 18 RefVTON: person-to-person Try on with Additional Unpaired Visual Reference PREPRINT Figure 13: Qualitative results of upper-body sub-set in Dresscode dataset unpaired setting. reference denotes that reference image is provided, while MF indicates mask-free inputs using the original person image instead of masked agnostic image. 19 RefVTON: person-to-person Try on with Additional Unpaired Visual Reference PREPRINT Figure 14: Qualitative results of lower-body sub-set in Dresscode dataset unpaired setting. reference denotes that reference image is provided, while MF indicates mask-free inputs using the original person image instead of masked agnostic image. 20 RefVTON: person-to-person Try on with Additional Unpaired Visual Reference PREPRINT Figure 15: Qualitative results of dresses sub-set in Dresscode dataset unpaired setting. reference denotes that reference image is provided, while MF indicates mask-free inputs using the original person image instead of masked agnostic image."
        }
    ],
    "affiliations": [
        "360 AI Research",
        "Hong Kong University of Science and Technology",
        "The University of Tokyo",
        "University of California San Diego"
    ]
}