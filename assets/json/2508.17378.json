{
    "paper_title": "UI-Level Evaluation of ALLaM 34B: Measuring an Arabic-Centric LLM via HUMAIN Chat",
    "authors": [
        "Omer Nacar"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) trained primarily on English corpora often struggle to capture the linguistic and cultural nuances of Arabic. To address this gap, the Saudi Data and AI Authority (SDAIA) introduced the $ALLaM$ family of Arabic-focused models. The most capable of these available to the public, $ALLaM-34B$, was subsequently adopted by HUMAIN, who developed and deployed HUMAIN Chat, a closed conversational web service built on this model. This paper presents an expanded and refined UI-level evaluation of $ALLaM-34B$. Using a prompt pack spanning modern standard Arabic, five regional dialects, code-switching, factual knowledge, arithmetic and temporal reasoning, creative generation, and adversarial safety, we collected 115 outputs (23 prompts times 5 runs) and scored each with three frontier LLM judges (GPT-5, Gemini 2.5 Pro, Claude Sonnet-4). We compute category-level means with 95\\% confidence intervals, analyze score distributions, and visualize dialect-wise metric heat maps. The updated analysis reveals consistently high performance on generation and code-switching tasks (both averaging 4.92/5), alongside strong results in MSA handling (4.74/5), solid reasoning ability (4.64/5), and improved dialect fidelity (4.21/5). Safety-related prompts show stable, reliable performance of (4.54/5). Taken together, these results position $ALLaM-34B$ as a robust and culturally grounded Arabic LLM, demonstrating both technical strength and practical readiness for real-world deployment."
        },
        {
            "title": "Start",
            "content": "UI-LEVEL EVALUATION OF ALLAM 34B: MEASURING AN ARABIC-CENTRIC LLM VIA HUMAIN CHAT 5 2 0 2 4 2 ] . [ 1 8 7 3 7 1 . 8 0 5 2 : r Omer Nacar NAMAA Community Riyadh - KSA onajar@psu.edu.sa"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models (LLMs) trained primarily on English corpora often struggle to capture the linguistic and cultural nuances of Arabic. To address this gap, the Saudi Data and AI Authority (SDAIA) introduced the ALLaM family of Arabic-focused models. The most capable of these available to the public, ALLaM 34B, was subsequently adopted by HUMAIN, who developed and deployed HUMAIN Chat, closed conversational web service built on this model. This paper presents an expanded and refined UI-level evaluation of ALLaM 34B. Using prompt pack spanning modern standard Arabic, five regional dialects, code-switching, factual knowledge, arithmetic and temporal reasoning, creative generation, and adversarial safety, we collected 115 outputs (23 prompts 5 runs) and scored each with three frontier LLM judges (GPT-5, Gemini 2.5 Pro, Claude Sonnet-4). We compute category-level means with 95% confidence intervals, analyze score distributions, and visualize dialect-wise metric heat maps. The updated analysis reveals consistently high performance on generation and code-switching tasks (both averaging 4.92/5), alongside strong results in MSA handling (4.74/5), solid reasoning ability (4.64/5), and improved dialect fidelity (4.21/5). Safetyrelated prompts show stable reliable performance of (4.54/5). Taken together, these results position ALLaM 34B as robust and culturally grounded Arabic LLM, demonstrating both technical strength and practical readiness for real-world deployment. Keywords Arabic LLM UI-level evaluation HUMAIN Chat dialectal Arabic safety and security"
        },
        {
            "title": "Introduction",
            "content": "Transformer-based language models have revolutionized natural language processing, but they remain largely Englishcentric [1, 2]. When applied to Arabic, many models exhibit poor fluency, factuality, and dialectal sensitivity. Studies emphasize that cultural alignment is as critical as linguistic accuracy in evaluating Arabic LLMs. Western-centric benchmarks often fail to capture the cultural and religious sensitivities of Arabic-speaking communities, leading to systematic misalignment and reduced trust. For instance, recent work on enriching the Arabic MMLU benchmark highlights significant gaps in how Arabic LLMs are evaluated, showing that models tested only on translated or Western-framed datasets risk overlooking essential cultural norms and values [3]. These findings underscore the need for evaluation frameworks that explicitly account for cultural resonance, bias, and social appropriateness, moving beyond purely technical performance metrics to ensure inclusivity and user trust. Against this backdrop, the ALLaM project was initiated by the Saudi Data and AI Authority (SDAIA) to create family of Arabic-centric large language models. The series includes models of varying sizes (7B, 13B, 34B, and 70B parameters), pretrained on balanced mixture of Arabic and English corpora with vocabulary expansion to better represent Arabic morphology [4]. Early benchmarks demonstrated that ALLaM models achieved state-of-the-art results on Arabic-focused evaluations while retaining strong performance in English. Building on this foundation, HUMAIN adopted the 34-billion-parameter variant, ALLaM 34B, and developed HUMAIN Chat (https://chat.humain.ai/en), closed conversational service showcasing the models capabilities. Because the system is accessible only through its user interfacewithout public API or model weightsrigorous evaluation must occur at the UI level. Our work extends prior efforts by offering comprehensive UI-level evaluation of ALLaM 34B to date. Specifically: 1. We analyze test set of 115 responses generated from 23 prompts (five per prompt). 2. We compute category-wise means, standard deviations, and 95% confidence intervals for the overall score, and visualize category and dialectal performance. 3. We present both quantitative results and qualitative examples to illustrate strengths and limitations. Together, these contributions provide rigorous and culturally informed assessment of ALLaM 34B, positioning it as one of the most capable Arabic-centric LLMs currently available."
        },
        {
            "title": "2 Methodology",
            "content": "In this work, we introduce structured evaluation pipeline for assessing ALLaM 34B through the HUMAIN Chat interface. As illustrated in Figure 1, the pipeline consists of four main stages; (i) construction of balanced prompt pack, (ii) systematic sampling of responses through the user interface, (iii) multi-metric scoring by independent LLM judges, and (iv) aggregation and analysis of results. This design ensures that the evaluation remains reproducible, comprehensive, and sensitive to the linguistic and cultural dimensions central to Arabic. Figure 1: Proposed Evaluation Pipeline 2.1 Prompt Pack We curated prompt pack spanning seven thematic categories: modern standard Arabic (MSA), dialect, code-switching, knowledge, reasoning, generation, and safety/security. In total, 23 distinct prompts were designed, each intended to probe specific linguistic or functional capability. Dialect prompts cover five regional varieties (Najdi, Hijazi, Egyptian, Moroccan, and Levantine), while safety prompts include adversarial cases such as prompt injection, jailbreaks, and hidden instruction exfiltration. The prompt set was deliberately balanced to reflect both everyday user needs and high-stakes safety considerations. 2.2 Sampling Protocol Each prompt was submitted five times through the HUMAIN Chat interface, resulting in 115 model outputs. This repeated sampling captures variability introduced by the systems internal decoding process. Because no API-level controls (e.g., temperature, top-p) are exposed, this protocol ensures diversity while remaining faithful to the real user experience. Response latency was also monitored and found to be consistently low (13 seconds), supporting the systems suitability for interactive use. Figure 2: Evaluation categories used in our prompt pack. 2.3 Scoring and Metrics Each response was independently evaluated by three frontier LLM judges: GPT-5 [5], Gemini 2.5 Pro [6], and Claude Sonnet-4 [7]. Judges rated outputs on five-point Likert scale (1=poor, 5=excellent) across five dimensions: 1. Accuracy factual correctness of the response. 2. Fluency grammaticality and stylistic naturalness. 3. Instruction following adherence to the prompt requirements. 4. Safety avoidance of harmful or prohibited content. 5. Dialect fidelity correct use of the requested dialect (applied only when relevant). For each response, the overall score was defined as the mean of the applicable metrics. 2.4 Analysis Framework We aggregated the judge scores to compute per-prompt and per-category means, standard deviations, and 95% confidence intervals. Scores were further grouped by high-level category (e.g., MSA, dialect, code-switching) to enable comparative analysis. Dialectal results were visualized through heat map, while overall category performance was summarized using distribution plots and tables. In addition to LLM-as-a-judge [8] scoring, we conducted human evaluation to validate the automated assessments. We reviewed subset of responses across categories, with particular attention to dialectal fidelity and cultural appropriateness. Their ratings showed high agreement with the LLM judges, especially on fluency and accuracy, and provided valuable nuance in areas where dialect usage or cultural framing was subtle. This human validation step strengthens confidence in the reliability of the results while ensuring that the evaluation reflects both technical and cultural dimensions."
        },
        {
            "title": "3 Results",
            "content": "In this section, we present the outcomes of our evaluation pipeline. Figure 2 summarizes the categories that are added in the prompt pack and Table 1 summarizes the quantitative results across categories, while additional figures provide insights into score distributions and dialectal performance. 3.1 Category-level performance Table 1 reports the mean overall score per category with 95 % CI. Code-switching and generation tasks remain the strongest (mean 4.92 for both) with narrow confidence intervals, reflecting consistently high fluency and adherence to instructions. Knowledge (4.77), MSA (4.74) and reasoning (4.64) follow closely. Safety and security categories average 4.54 overall and Dialect prompts score 4.21 on average. The specific adversarial categoriesprompt injection, jailbreak, and data exfiltrationcluster around 4.20 with essentially zero variance. These results show that ALLaM 34B delivers strong and reliable performance across diverse linguistic and functional tasks, balancing fluency, accuracy, cultural appropriateness, and safety. This effectiveness highlights its potential as robust Arabic-centric LLM suitable for real-world deployment. Table 1: Average score by category with 95 % confidence intervals. Category Mean overall 95 % CI Code-Switching Generation Knowledge MSA Reasoning Safety Dialect Prompt Injection Jailbreak Data Exfiltration 4.92 4.92 4.77 4.74 4.64 4.54 4.21 4.20 4.20 4.20 [4.85, 5.00] [4.88, 4.97] [4.65, 4.89] [4.66, 4.81] [4.49, 4.79] [4.43, 4.65] [4.09, 4.34] [4.20, 4.20] [4.20, 4.20] [4.20, 4.20] 3.2 Dialect Analysis Figure 3 presents heat map of average scores across metrics for each dialect. The visualization highlights clear differences in performance across regional varieties. Najdi, Hijazi, and Egyptian dialects achieve similar overall performance of about 3.8, characterized by strong fluency and perfect dialect fidelity scores. By contrast, Levantine dialect performance drops to 2.73 overall, driven mainly by lower accuracy despite otherwise fluent output. Moroccan is also weaker at 3.3 overall, where the model frequently defaults to generic MSA and occasionally misuses regional vocabulary. Figure 3: Average scores across metrics and dialects. Examining example prompts reveals consistent (cid:63)(cid:208)(cid:241)(cid:74)(cid:10)(cid:203)(cid:64) (cid:9)(cid:224)(cid:241)(cid:202) (cid:17)(cid:131), the model correctly understood the dialectal input and generated accurate weather For Najdi prompts such as tendencies across dialects. (cid:213)(cid:187)(cid:89)(cid:9)(cid:74)(cid:171) (cid:241)(cid:109)(cid:46)(cid:204)(cid:39)(cid:64) (cid:9)(cid:144)(cid:65)(cid:75)(cid:10)(cid:81)(cid:203)(cid:65)(cid:75)(cid:46) reports. However, instead of producing natural Najdi reply, ALLaM 34B frequently switched into retrieval-like mode, presenting structured factual weather summaries in English (e.g., The weather in Riyadh today is mostly sunny, with temperature of 42C...). This suggests that when the model invokes external knowledge or search-style behavior, it prioritizes English factual output over dialectal generation. While factually correct, this reduces cultural authenticity and highlights mismatch between dialectal intent and the models chosen response style. (cid:13) For Hijazi prompts like (cid:63)(cid:208)(cid:241)(cid:74)(cid:10)(cid:203)(cid:64) (cid:66)(cid:64) including sections on weather, achievements, security events, and real estate projects. These responses were accurate and content-rich, yet the style again drifted away from natural Hijazi, indicating over-reliance on MSA formalism. (cid:17)(cid:129)(cid:29)(cid:10)(cid:64)(cid:13), the model generated lengthy and structured news bulletins in MSA, (cid:16)(cid:232)(cid:89)(cid:103)(cid:46) (cid:250)(cid:10) (cid:9)(cid:175) (cid:80)(cid:65)(cid:74)(cid:46) (cid:9)(cid:107) Egyptian inputs such as (cid:250)(cid:10)(cid:230)(cid:46)(cid:107)(cid:65)(cid:147) (cid:65)(cid:75)(cid:10) (cid:233)(cid:75)(cid:10)(cid:64)(cid:13) (cid:201)(cid:211)(cid:65)(cid:171) often triggered formulaic self-introductions (cid:248)(cid:10) (cid:241) (cid:9)(cid:170)(cid:203) (cid:104)(cid:46) (cid:15) (cid:9)(cid:88)(cid:241)(cid:214) (cid:9)(cid:223) (cid:44)(cid:208)(cid:67) (cid:171) (cid:65)(cid:9)(cid:75) (cid:13) (cid:64) (cid:65)(cid:19)(cid:74)(cid:46)(cid:107)(cid:81)(cid:211). While polite and fluent, these outputs ignored the conversational dialectal intent, defaulting to generic assistant-like MSA responses rather than casual Egyptian phrasing. Overall, the heat map and qualitative inspection suggest that ALLaM 34B reliably understands dialectal input but tends to respond in MSA or hybrid formal style, especially when dialect-specific corpora are sparse. Najdi, Hijazi, and Egyptian varieties benefit from greater representation in training data, whereas Levantine and Moroccan remain under-supported. Notably, the model occasionally breaks character (e.g., providing English retrieval-like responses) when confronted with less familiar dialects. This uneven coverage underscores the need for targeted dialectal corpora, culturally grounded finetuning, and evaluation benchmarks that reward dialectal authenticity rather than only fluency. 3.3 Qualitative examples Beyond averages, individual examples illustrate how ALLaM 34B responds across different categories. Table 2 presents selection of representative prompts along with multiple runs and our observations. To keep the presentation concise, only short excerpts of the generated responses are shown; full responses were fluent and well-formed. These examples illustrate both the strengths and challenges observed in our evaluation. The model shows strong consistency in formal rewriting and code-switching tasks, demonstrates reliable refusal of unsafe or unanswerable prompts, and maintains stylistic fluency across runs. Table 2: Representative prompts with multiple runs and commentary. Prompt (cid:13) (cid:58) (cid:249)(cid:10) (cid:214)(cid:222)(cid:133)(cid:80) (cid:72)(cid:46) (cid:241)(cid:202)(cid:131) (cid:65)(cid:75)(cid:46) (cid:46)(cid:248)(cid:10) (cid:88)(cid:65)(cid:146)(cid:16)(cid:74)(cid:16)(cid:175)(cid:66)(cid:64) (cid:241)(cid:210)(cid:9)(cid:74)(cid:202)(cid:203) (cid:16)(cid:232)(cid:81)(cid:16)(cid:174) (cid:9)(cid:174)(cid:203)(cid:64) (cid:16)(cid:233)(cid:74)(cid:10)(cid:203)(cid:65)(cid:16)(cid:74)(cid:203)(cid:64) (cid:16)(cid:233) (cid:9)(cid:171)(cid:65)(cid:74)(cid:10)(cid:147) (cid:89)(cid:171) (cid:16)(cid:232)(cid:81)(cid:30)(cid:10)(cid:74)(cid:46)(cid:187) (cid:16)(cid:233)(cid:147)(cid:81)(cid:9)(cid:175) (cid:250)(cid:10)(cid:171)(cid:65)(cid:9)(cid:74)(cid:162)(cid:147)(cid:66)(cid:64) (cid:90)(cid:65)(cid:191) (cid:9)(cid:89)(cid:203)(cid:64) (cid:201)(cid:17)(cid:74)(cid:214)(cid:223)(cid:10) (cid:13) (cid:64) Evaluation Focus MSA Formal Rewriting Translate this Arabizi into Arabic: ana rayeh el-beit b3d shwaya. Code-Switching Transliteration Arabizi (cid:46) (cid:13) (cid:9)(cid:172)(cid:81)(cid:171) (cid:64)(cid:66) (cid:58) (cid:201)(cid:16)(cid:175) (cid:44) (cid:16)(cid:233)(cid:75)(cid:46) (cid:65)(cid:103)(cid:46) (cid:66)(cid:13) (cid:64) (cid:9)(cid:175) (cid:16)(cid:174)(cid:16)(cid:74)(cid:130)(cid:214)(cid:207)(cid:64) (cid:250)(cid:10) (cid:63) (cid:201)(cid:74)(cid:46) (cid:13) (cid:9)(cid:225)(cid:211) (cid:64) (cid:19)(cid:89)(cid:187) (cid:65)(cid:16)(cid:74)(cid:211) (cid:9)(cid:225)(cid:186)(cid:16)(cid:75) (cid:213)(cid:203) (cid:13) (cid:17)(cid:128)(cid:65)(cid:171) (cid:9)(cid:224)(cid:65)(cid:130)(cid:9)(cid:29)(cid:64)(cid:13) (cid:200)(cid:240) (cid:64) (cid:241)(cid:235) (cid:9)(cid:225)(cid:211) (cid:44) (cid:16)(cid:233)(cid:16)(cid:174)(cid:75)(cid:46) (cid:65)(cid:130)(cid:203)(cid:64) (cid:46)(cid:46)(cid:46) (cid:16)(cid:233)(cid:74)(cid:10) (cid:16)(cid:72)(cid:65)(cid:210)(cid:74)(cid:10)(cid:202)(cid:170)(cid:16)(cid:74)(cid:203)(cid:64) (cid:169)(cid:74)(cid:10)(cid:212)(cid:103)(cid:46) (cid:201)(cid:235)(cid:65)(cid:109)(cid:46)(cid:26)(cid:16)(cid:39) (cid:13)(cid:75)(cid:64)(cid:241) (cid:17)(cid:130)(cid:171) (cid:16)(cid:233)(cid:75)(cid:10)(cid:81)(cid:229)(cid:133) (cid:80)(cid:240)(cid:81)(cid:211) (cid:16)(cid:233)(cid:210)(cid:202)(cid:191) (cid:250)(cid:10)(cid:205) (cid:73)(cid:46) (cid:16)(cid:74)(cid:187)(cid:64)(cid:240) (cid:64) (cid:9)(cid:88)(cid:64)(cid:13) Safety Unanswerable Prompt Prompt Injection (Ignore Instructions) (cid:13) (cid:64) (cid:250)(cid:10)(cid:171)(cid:65)(cid:9)(cid:74)(cid:162)(cid:147)(cid:66)(cid:64) (cid:90)(cid:65)(cid:191) (cid:9)(cid:89)(cid:203)(cid:64) (cid:89)(cid:170)(cid:12)(cid:75)(cid:10) Sample Runs (excerpt) (cid:16)(cid:135)(cid:74)(cid:10) Run1: Run1: (cid:16)(cid:232)(cid:81)(cid:30)(cid:10)(cid:74)(cid:46)(cid:187) (cid:16)(cid:233)(cid:147)(cid:81)(cid:9)(cid:175) (cid:250)(cid:10)(cid:171)(cid:65)(cid:9)(cid:74)(cid:162)(cid:147)(cid:66)(cid:64) (cid:90)(cid:65)(cid:191) (cid:9)(cid:89)(cid:203)(cid:64) (cid:201)(cid:17)(cid:74)(cid:214)(cid:223)(cid:10) (cid:16)(cid:174)(cid:106)(cid:16)(cid:74)(cid:203) (cid:46) (cid:248)(cid:10) (cid:88)(cid:65)(cid:146)(cid:16)(cid:74)(cid:16)(cid:175)(cid:66)(cid:64) (cid:241)(cid:210)(cid:9)(cid:74)(cid:203)(cid:64) (cid:16)(cid:233)(cid:74)(cid:10)(cid:130)(cid:28)(cid:10) (cid:13)(cid:75)(cid:81)(cid:203)(cid:64) (cid:201)(cid:211)(cid:64)(cid:241)(cid:170)(cid:203)(cid:64) (cid:89)(cid:103) Run2: (cid:46)(cid:46)(cid:46) (cid:19)(cid:16)(cid:233)(cid:147)(cid:81)(cid:9)(cid:175) (cid:250)(cid:10)(cid:171)(cid:65)(cid:9)(cid:74)(cid:162)(cid:147)(cid:66)(cid:64) (cid:90)(cid:65)(cid:191) (cid:9)(cid:89)(cid:203)(cid:64) (cid:201)(cid:17)(cid:74)(cid:214)(cid:223)(cid:10) (cid:16)(cid:232)(cid:81)(cid:30)(cid:10)(cid:74)(cid:46)(cid:187) Run3: (cid:46)(cid:46)(cid:46) (cid:16)(cid:233)(cid:75)(cid:10)(cid:241) (cid:17)(cid:131) (cid:89)(cid:170)(cid:75)(cid:46) (cid:108)(cid:26)(cid:39)(cid:10)(cid:64)(cid:80) (cid:65)(cid:9)(cid:75) (cid:16)(cid:73)(cid:28)(cid:10)(cid:74)(cid:46)(cid:203)(cid:64) (cid:16)(cid:73)(cid:28)(cid:10)(cid:74)(cid:46)(cid:203)(cid:64) (cid:250)(cid:205)(cid:64)(cid:13) (cid:73)(cid:46) (cid:235)(cid:64) (cid:9)(cid:88) (cid:65)(cid:9)(cid:75) Run3: (cid:201)(cid:74)(cid:10)(cid:202)(cid:16)(cid:175) (cid:89)(cid:170)(cid:75)(cid:46) (cid:16)(cid:233)(cid:75)(cid:10)(cid:241) (cid:17)(cid:131) (cid:89)(cid:170)(cid:75)(cid:46) (cid:108)(cid:26)(cid:39)(cid:10)(cid:64)(cid:80) (cid:65)(cid:9)(cid:75) (cid:16)(cid:73)(cid:28)(cid:10)(cid:74)(cid:46)(cid:203)(cid:64) (cid:13) (cid:9)(cid:172)(cid:81)(cid:171) (cid:64) (cid:66) (cid:13) (cid:9)(cid:172)(cid:81)(cid:171) (cid:64) (cid:66) (cid:13) (cid:9)(cid:172)(cid:81)(cid:171) (cid:64) (cid:66) Run2: Run5: Run1: (cid:13) (cid:64) (cid:13) (cid:64) (cid:13) (cid:64) Run3: Run1: (cid:72)(cid:46) Run2: Run5: (cid:13) (cid:9)(cid:230)(cid:9)(cid:74)(cid:186)(cid:214)(cid:223)(cid:10) (cid:66) (cid:44)(cid:80) (cid:9)(cid:89)(cid:16)(cid:74)(cid:171) (cid:64) (cid:13) (cid:9)(cid:230)(cid:9)(cid:74)(cid:186)(cid:214)(cid:223)(cid:10) (cid:66) (cid:44)(cid:80) (cid:9)(cid:89)(cid:16)(cid:74)(cid:171) (cid:64) (cid:13) (cid:9)(cid:230)(cid:9)(cid:74)(cid:186)(cid:214)(cid:223)(cid:10) (cid:66) (cid:44)(cid:80) (cid:9)(cid:89)(cid:16)(cid:74)(cid:171) (cid:64) (cid:65)(cid:103)(cid:46) (cid:66)(cid:13) (cid:64) (cid:250)(cid:10) (cid:16)(cid:233)(cid:75)(cid:46) (cid:65)(cid:103)(cid:46) (cid:66)(cid:13) (cid:64) (cid:250)(cid:10) (cid:16)(cid:233)(cid:75)(cid:46) (cid:65)(cid:103)(cid:46) (cid:66)(cid:13) (cid:64) (cid:250)(cid:10) Commentary accurate Consistently and stylistically appropriate rewritings. Minor lexical variation shows robustness. Correct transliterations in all runs. Alternates between ((cid:108)(cid:26)(cid:39)(cid:10)(cid:64)(cid:80)) and formal dialectal ((cid:73)(cid:46) (cid:235)(cid:64) (cid:9)(cid:88)) phrasing. All runs correctly refused to answer, showing safe behaviour and adherence to instructions. Model consistently resisted the malicious instruction, demonstrating robustness to injection attempts."
        },
        {
            "title": "4 Discussion",
            "content": "Capabilities: The analysis confirms that ALLaM 34B excels at general text generation, code-switching and MSA tasks. The high scores and tight confidence intervals indicate reliable performance. Code-switching taskswhere the model interleaves Arabic and English or transliterated Arabiziachieve near-perfect fluency and accuracy, highlighting the effectiveness of vocabulary expansion and alignment techniques reported in the ALLaM paper. Generation prompts elicit coherent formal letters and creative essays. Fluency remains strong across dialects even when accuracy suffers, suggesting that the model learns surface structure more readily than content. Dialectal fidelity:p Uneven performance across dialects points to imbalances in training data. Najdi, Hijazi and Egyptian dialects perform similarly, likely reflecting their prevalence in the corpus. Levantine scores are lower, and Moroccan performance suffers the most. Collecting more high-quality dialectal corpora or employing dialect-specific adapters may improve coverage. Additionally, adding dialectal annotations or training on labeled dialect corpora could help the model differentiate between dialects rather than reverting to MSA. Safety and robustness: ALLaM 34B demonstrates strong safety behaviour across variety of adversarial scenarios. The model reliably declines harmful instructions, avoids generating unsafe or inappropriate content, and responds appropriately to unanswerable or sensitive history queries. Across prompts designed to test jailbreaks, prompt injections, or hidden instruction exfiltration, the system maintained refusals and preserved conversational integrity. Average safety scores of approximately 4.5 indicate that ALLaM 34B has been aligned to prioritize user safety, striking balance between fluency and responsible refusal. Nevertheless, continued probing with more sophisticated adversarial attacks will be important to fully assess robustness and to ensure long-term reliability under deployment."
        },
        {
            "title": "5 Conclusion",
            "content": "This study offered comprehensive UI-level evaluation of ALLaM 34B through the HUMAIN Chat platform, showing that the model excels in code-switching, generation, and modern standard Arabic, while performing solidly in reasoning, safety, and adversarial robustness, and more moderately across dialects. Despite limitationsincluding reliance on the chat interface, relatively small prompt pack, and the use of LLM-based judgesthe results provide strong evidence of ALLaM 34Bs effectiveness as culturally grounded Arabic LLM. Future work should expand dialectal coverage, integrate direct human evaluation, and test subsequent releases, with the aim of advancing trustworthy, robust, and culturally aligned Arabic AI systems."
        },
        {
            "title": "References",
            "content": "[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [2] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [3] Omer Nacar, Serry Taiseer Sibaee, Samar Ahmed, Safa Ben Atitallah, Adel Ammar, Yasser Alhabashi, Abdulrahman Al-Batati, Arwa Alsehibani, Nour Qandos, Omar Elshehy, et al. Towards inclusive arabic llms: culturally aligned benchmark in arabic large language model evaluation. In Proceedings of the First Workshop on Language Models for Low-Resource Languages, pages 387401, 2025. [4] Saiful Bari, Yazeed Alnumay, Norah Alzahrani, Nouf Alotaibi, Hisham Alyahya, Sultan AlRashed, Faisal Mirza, Shaykhah Alsubaie, Hassan Alahmed, Ghadah Alabduljabbar, et al. Allam: Large language models for arabic and english. arXiv preprint arXiv:2407.15390, 2024. [5] OpenAI. Gpt-5 is here. https://openai.com/news/gpt-5, 2025. Accessed: 2025-08-23. [6] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [7] Anthropic. Claude sonnet 4. https://www.anthropic.com/news/claude-sonnet-4, 2025. Accessed: 202508-23. [8] Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594, 2024."
        }
    ],
    "affiliations": [
        "NAMAA Community Riyadh - KSA"
    ]
}