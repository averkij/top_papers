{
    "paper_title": "RHYTHM: Reasoning with Hierarchical Temporal Tokenization for Human Mobility",
    "authors": [
        "Haoyu He",
        "Haozheng Luo",
        "Yan Chen",
        "Qi R. Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Predicting human mobility is inherently challenging due to complex long-range dependencies and multi-scale periodic behaviors. To address this, we introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for Human Mobility), a unified framework that leverages large language models (LLMs) as general-purpose spatio-temporal predictors and trajectory reasoners. Methodologically, RHYTHM employs temporal tokenization to partition each trajectory into daily segments and encode them as discrete tokens with hierarchical attention that captures both daily and weekly dependencies, thereby significantly reducing the sequence length while preserving cyclical information. Additionally, we enrich token representations by adding pre-computed prompt embeddings for trajectory segments and prediction targets via a frozen LLM, and feeding these combined embeddings back into the LLM backbone to capture complex interdependencies. Computationally, RHYTHM freezes the pretrained LLM's backbone to reduce attention complexity and memory cost. We evaluate our model against state-of-the-art methods using three real-world datasets. Notably, RHYTHM achieves a 2.4% improvement in overall accuracy, a 5.0% increase on weekends, and a 24.6% reduction in training time. Code is publicly available at https://github.com/he-h/rhythm."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 5 1 1 3 2 . 9 0 5 2 : r RHYTHM: Reasoning with Hierarchical Temporal Tokenization for Human Mobility Haoyu He Haozheng Luo Yan Chen Qi R. Wang Northeastern University Northwestern University {he.haoyu1, q.wang}@northeastern.edu hluo@u.northwestern.edu, ychen@northwestern.edu"
        },
        {
            "title": "Abstract",
            "content": "Predicting human mobility is inherently challenging due to complex long-range dependencies and multi-scale periodic behaviors. To address this, we introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for Human Mobility), unified framework that leverages large language models (LLMs) as general-purpose spatio-temporal predictors and trajectory reasoners. Methodologically, RHYTHM employs temporal tokenization to partition each trajectory into daily segments and encode them as discrete tokens with hierarchical attention that captures both daily and weekly dependencies, thereby significantly reducing the sequence length while preserving cyclical information. Additionally, we enrich token representations by adding pre-computed prompt embeddings for trajectory segments and prediction targets via frozen LLM, and feeding these combined embeddings back into the LLM backbone to capture complex interdependencies. Computationally, RHYTHM freezes the pretrained LLMs backbone to reduce attention complexity and memory cost. We evaluate our model against state-of-the-art methods using three real-world datasets. Notably, RHYTHM achieves 2.4% improvement in overall accuracy, 5.0% increase on weekends, and 24.6% reduction in training time. Code is publicly available at https://github.com/he-h/rhythm."
        },
        {
            "title": "Introduction",
            "content": "Human mobility shapes transportation systems [6, 62], informs epidemic control strategies [63, 8], and guides sustainable city planning [4], making accurate movement prediction essential for optimizing infrastructure, managing disease spread, and building resilient communities [42]. Yet human trajectories exhibit long-range dependencies, spatial heterogeneity [78, 19], and dynamic influences such as weather anomalies or special events [7, 36], producing non-stationary, multi-scale spatio-temporal patterns. To address this challenge, we introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for Human Mobility), human mobility foundation model that rethinks mobility modeling via structured temporal abstraction. RHYTHM unites efficient multi-scale temporal tokenization with the deep reasoning capabilities of pretrained LLMs, delivering scalable, accessible framework for accurate trajectory prediction with substantially reduced computational overhead. The structure of the framework is inspired by inherent patterns of human mobility. Peoples movements are not random; they follow an underlying order marked by recurring patterns. Previous studies [12, 24, 18] have shown strong daily and weekly rhythms in movement patterns. Notably, Song et al. [58] quantify this regularity by showing that 93% of daily trajectories are predictable, underscoring the critical role of cyclical temporal context in mobility modeling. Capturing the complex interdependence between spatial locations and repeating time intervalswhere place choices and 39th Conference on Neural Information Processing Systems (NeurIPS 2025). timing influence each other in nontrivial, periodic waysremains central challenge. Yet existing approaches fall short: Markov and RNN-based methods either disregard long-term periodicity or suffer vanishing gradients over long sequences [76, 19, 22], while transformer-based methods treat time as static, failing to disentangle multi-scale temporal patterns [27, 72]. To bridge this gap, we decompose each trajectory into meaningful segments, tokenizing each into discrete representations that capture local patterns (e.g., morning commutes) through intra-segment attention. These segment tokens are then pooled into higher-level representations, enabling inter-segment attention to model long-range dependencies across days, as illustrated in Figure 1, thereby reducing sequence length and quadratically lowering attention cost. To enhance semantic richness, each token is augmented with pre-computed prompt embeddingderived from frozen LLM encompassing trajectory context and task descriptions-before being passed into the backbone for deep reasoning. Recent work demonstrates LLMs remarkable capabilities not only as sequential representation extractors to capture the spatiotemporal attention patterns but also as reasoning mod- [13, 9]. Prior works els [25, 59, 17, 21] demonstrate their reasoning capabilities through techniques such as few-shot prompting [9], chain-of-thought reasoning [50, 49, 66], and incontext learning [16]. However, mobility-specific models like PMT [71] and STMoE-BERT [26] lack the capability to leverage LLMs for modeling complex correlations in human flows, limiting their predictive performance. By integrating an LLM-based reasoning module, RHYTHM more effectively models these complex interdependencies. To maintain scalability, RHYTHM adopts parameter-efficient adaptation strategy by freezing the pretrained LLM and avoid extensive fine-tuning. This design captures fine-grained spatio-temporal dynamics, deep semantic context, and leverages LLM reasoningall while minimizing computational and memory overheadmaking RHYTHM ideally suited for deployment in resource-constrained, real-world environments. Figure 1: Motivation for RHYTHM. Instead of processing entire trajectories as continuous sequence, RHYTHM segments trajectories into tokens to better capture periodic patterns. Contributions. We propose RHYTHM, unified, computationally efficient framework that captures both temporal dynamics and cyclical patterns, as illustrated in Figure 2. Our contributions are as follows: We introduce temporal tokenization that encodes daily mobility patterns as discrete tokens, reducing the processed sequence length while capturing cyclical and multi-scale mobility dependencies through hierarchical attention mechanism. We design an efficient prompt-guided approach that integrates semantic trajectory information and task description with segment embeddings, enhancing RHYTHMs ability to interpret complex mobility patterns. We propose parameter-efficient adaptation strategy using frozen pretrained LLMs, reducing trainable parameters to 12.37% of the full model size and achieving 24.6% reduction in computational cost compared to other baselines. Empirically, we evaluate RHYTHM on three real-world mobility datasets, demonstrating superior performance compared to state-of-the-art models. RHYTHM achieves 2.4% improvement in prediction accuracy, with 5.0% increase on weekends."
        },
        {
            "title": "2 Related Work",
            "content": "In this section, we provide brief overview of related work, with detailed review in Section B. 2 Figure 2: The proposed architecture of RHYTHM. Our framework processes historical trajectories through spatio-temporal embedding and temporal tokenization (b), capturing local and global dependencies via hierarchical attention. Segment representations are enriched with semantic embeddings from trajectory information, while future timestamps incorporate task description context (a). This combined sequence passes through frozen LLM backbone with output projection to generate location predictions. Mobility Prediction. Human mobility prediction progresses from probabilistic approaches [75, 22] to deep learning architectures, as demonstrated in recent studies. Sequence models like LSTM [37] and attention mechanisms [19] improve temporal modeling, while graph-based methods [55, 14] integrate spatial relationships. Transformer architectures [65, 77, 45] further enhance long-range dependency modeling but struggle with the hierarchical structure of mobility patterns. Recent work with LLMs [20, 64] shows promise but typically treats mobility as generic sequences, ignoring the inherent periodicity of human movement. Cross-domain Adaptation of LLMs. LLMs emerge as powerful natural language processing systems and quickly evolve into general-purpose foundation models capable of reasoning and generation tasks [9, 1]. Their remarkable adaptability has enabled successful applications in computer vision [5, 53], speech [69, 46], biomedicine [82, 44, 57], time series forecasting [11, 81], and finance [31, 70]. While many adaptations rely on parameter-efficient fine-tuning methods like LoRA [29], recent approaches maintain frozen LLMs by utilizing them as sequential representation extractors, preserving their semantic capabilities while reducing computational costs [40, 32, 2]. To the best of our knowledge, RHYTHM is the first approach that adapts frozen LLMs to mobility prediction without compromising the models reasoning capabilities or requiring extensive fine-tuning."
        },
        {
            "title": "3 Method",
            "content": "In this section, we introduce RHYTHM, an LLM-based deep architecture tailored for prompt-guided representation learning of spatio-temporal patterns with its periodicity, as shown in Figure 2. In the following, we first define the problem and then introduce the model structure of RHYTHM, including its computational efficiency and theoretical guarantees. 3.1 Problem definition Let = {x1, x2, . . . , xT } denote users historical trajectory, where each xi = (ti, li) consists of timestamp ti and location li from finite set of locations L. Given sequence of future timestamps = {tT +1, tT +2, . . . , tT +H } with prediction horizon H, the goal is to predict the corresponding future locations = {lT +1, lT +2, . . . , lT +H }. Formally, we seek function which maps historical trajectories and future timestamps to the users future locations. : (X , ) (cid:55) Y, 3.2 Model structure Spatio-Temporal Feature Encoding. For each observation xi, we construct temporal embeddings to capture cyclical patterns in human movements:"
        },
        {
            "title": "Etemporal",
            "content": "i = EToD(ti)EDoW(ti), where indicates concatenation, EToD represents the time-of-day embedding (capturing 24-hour cycles), and EDoW represents the day-of-week embedding (capturing weekly patterns). These are learnable embeddings that map discrete temporal indices to continuous representations, with Etemporal RD with matching the backbone LLMs input dimension. i"
        },
        {
            "title": "Spatial embeddings Espatial",
            "content": "i RD for location li is defined as:"
        },
        {
            "title": "Espatial\ni",
            "content": "= ELoc(li)(Wcoord[lati, loni]T + bcoord), where ELoc denotes the categorical location embedding, and the second term projects the geographic coordinates (lati, loni) into the embedding space. Here, Wcoord Rdcoord2 is the projection matrix and dcoord denotes the projected dimension. The spatio-temporal embedding Ei RD is obtained by element-wise addition: Ei = Etemporal + Espatial . For future timestamps without known locations or missing historical records, the spatial component is set to zero, allowing the model to operate on temporal information alone while preserving dimensional consistency. Temporal Tokenization. Human mobility patterns exhibit inherent multi-scale temporal structures that span both short-term routines (e.g., daily activities) and long-term periodicities (e.g., weekly rhythms) [58, 24]. To effectively model these dynamics, RHYTHM employs temporal tokenization mechanism that effectively disentangles local patterns from global dependencies, inspired by Liu et al. [40]. Formally, we partition the embedded sequence into non-overlapping segments {s1, s2, . . . , sN }, each capturing meaningful temporal intervals (e.g., daily patterns): si = {E(i1)L+1, E(i1)L+2, . . . , EiL} for = 1, 2, . . . , N, where each segment si has length (number of time steps). Within each segment, we employ intra-segment attention to model local temporal dependencies: E(i) = Attention(si). Our attention mechanism follows pre-norm transformer architecture with gated feed-forward network as introduced by Dubey et al. [17], which enhances gradient flow during training and increases model expressivity. The implementation details are provided in Section C. To enable efficient modeling of cross-segment dependencies, we apply learnable pooling operation that condenses each segment into discrete token representation: SEi = Pool(cid:0) E(i)(cid:1). The resulting segment tokens { SE1, SE2, . . . , SEN } undergo inter-segment attention to capture broader temporal context and long-range dependencies: SE1:N = Attention(SE1:N ), yielding refined segment-level embedding SEi RD that integrates contextual information across multiple temporal scales. By reducing the effective sequence length from to while preserving both fine-grained temporal dynamics and long-term dependencies, our approach addresses the computational challenges of modeling extended mobility trajectories. Semantic Context Integration. Prior work such as FPT [81] employs LLMs as general-purpose sequential representation extractors. However, models like those in Wu et al. [67], Nie et al. [48] typically discard semantic embeddings and other critical information. For instance, timestamp attributes (e.g., day of the week, hour of the day) are essential for capturing chronological patterns in 4 human mobility, while spatial detailssuch as coordinatesprovide additional context for accurate prediction. While recent works [40, 68, 80] begin to incorporate semantic information in sequential data, they typically employ traditional embedding approaches that fail to holistically capture the rich contextual information inherent in mobility patterns. Our work addresses this limitation by developing mobilitybased semantic embedding method that leverages detailed trajectory information. key challenge in utilizing LLMs for mobility prediction is balancing information richness with computational efficiency. Unlike approaches such as LLM-Mob [64] that rely on extensive promptingwhich can lead to excessive context lengths and computational overhead, our approach breaks trajectory information into smaller, segment-specific pieces that retain essential mobility patterns while ensuring shorter prompts for each component. This decomposition significantly improves computational efficiency without sacrificing semantic richness. For each segment token, we provide informative trajectory descriptions, and for each future timestamp, we provide task descriptions and timestamp information, clarifying the expected output as shown in Section D. These prompts are then processed by pretrained LLMs to generate semantic embeddings. We adopt strategy that uses the special end-of-sequence (<EOS>) token for positional embedding, thereby integrating semantic information without extending the overall context length. Technically, we define the semantic embedding TEi RD for each token as follows: TEi = SelectLast(LLM(Prompt(x(i1)L+1:iL))). Similarly semantic embedding of task description for future timestamp is defined as TET = SelectLast(LLM(Prompt(T ))). Notably, TE is pre-computed using the LLM, so runtime forward pass through the language model is not required during training. Cross-representational Mobility Prediction. Since the latent space of the LLM encompasses both temporal tokens and semantic tokens, the semantic embedding can be aligned with the corresponding time span without extending the context length. Consequently, the combined embedding CEi for segment is obtained by elementwise adding the segment embedding SEi and semantic embedding SEi: CEi = SEi + TEi. Here, TEi serves role similar to positional embeddings [61], while avoiding the sequence length overhead incurred by prompt concatenation [32]. Similarly, the combined embedding for future timestep + is computed as CEN +j = EN +j + TET , following the combined embedding for segments. After obtaining the enriched embeddings CEi, we feed them into the backbone of RHYTHMa pretrained LLM. The LLM processes these embeddings through its deep layers, performing in-context reasoning over the aligned temporal and semantic information, and yields contextualized hidden representations hi from its last hidden layer. hi = LLM(CEi). Then we apply an output projection layer to map the LLMs final representations to set of logits corresponding to candidate locations. (lT +jX , ) = softmax(Wo hN +j + bo), where Wo RLD. These logits are then used to determine the most likely location predictions, thereby generating human mobility forecasting as defined in our problem statement. 3.3 Computational Efficiency RHYTHM achieves computational and parameter efficiency through several complementary design choices. Semantic embeddings are computed onceofflineusing the frozen LLM prior to model training, thereby eliminating any need for language-model inference at runtime. Simultaneously, our temporal tokenization mechanism significantly reduces sequence length from + to + H, thereby decreasing the quadratic attention complexity from O((T + H)2) to O((N + H)2), which is particularly valuable when processing extended mobility histories. Furthermore, we keep the LLM backbone parameters frozen during training, resulting in faster convergence and reduced memory requirements. The combination of these approaches enables RHYTHM to efficiently process long mobility trajectories while maintaining strong predictive performance (as shown in Figure 3), making it suitable for large-scale mobility prediction tasks where computational resources may be constrained. 5 3.4 Theoretical Guarantee We emphasize that our design choices provide strong theoretical guarantees. Employing an LLM as universal sequential representation extractor provides two key advantages: (1) it ensures the convergence of output values, as demonstrated in Zhou et al. [81, Theorem E.2], and (2) it guarantees uniform distribution of the feature space in the last hidden layer of the LLM, as outlined in Zhou et al. [81, Theorem E.3]. Together, these properties enable LLMs to enhance the learning capability of the final multi-layer perceptron layer. Additionally, since our model is transformer-based, Ramsauer et al. [54] demonstrates that the transformer architecture is special case of modern Hopfield networks. Our approach has guaranteed upper bound on memory retrieval error in LLMs [30, Lemma 3.2]. These theoretical benefits reinforce our method, and our results provide validation."
        },
        {
            "title": "4 Experiment",
            "content": "In this section, we conduct experiments to demonstrate the performance and efficiency of RHYTHM. We evaluate the performance of RHYTHM on three real-world mobility datasets and compare it with several state-of-the-art baselines. We also conduct series of ablation studies to investigate the effectiveness of the proposed strategies. Models. To evaluate the models performance on mobility prediction, we use multiple pretrained LLMs as the backbone of RHYTHM. These models are obtained from Hugging Face along with their pretrained weights. The specific LLM variants used are detailed in Section F.4. Evaluation Metrics. For mobility prediction, we employ Accuracy@k, where candidate locations are ranked based on model-predicted probabilities, and prediction is considered correct if the true location is among the top kand Mean Reciprocal Rank (MRR) to evaluate ranking performance. These metrics have been shown to correlate well with human mobility prediction tasks [23, 19]. We also utilize Dynamic Time Warping (DTW) [47] and BLEU [51] as real-world metrics to evaluate the performance. DTW quantifies their spatial alignment, while BLEU measures the n-gram similarity between predicted and ground-truth trajectories. Detailed descriptions about the evaluation metrics can be found in Section F.1. Datasets. We evaluate our approach on three real-world datasets collected from the cities of Kumamoto, Sapporo, and Hiroshima sourced from YJMob100K [74]. Each day is divided into 48 time slots (each representing 30 minutes), though not every slot contains an observation. Each dataset is divided into training, validation, and test sets based on days, with 70%, 20%, and 10% of the data allocated to each set, respectively. More details about the dataset can be found in Section E. Settings. The temporal resolution is 30 minutes. In our experiment, we use 7-day lookback window with 336 time slots and set the prediction horizon to 48 time slots (1 day). Also, we set the segment length as 48 time slots for our experiments. The model is trained using cross-entropy loss to maximize prediction accuracy across the target locations. 4.1 Overall Performance To assess the efficiency of RHYTHM on human mobility prediction, we compare RHYTHM with several state-of-the-art baselines. In this experiment, we evaluate the models on the test datasets using FP16 precision. We conduct each evaluation three times with different random seeds and present the average for each metric. Baselines. To evaluate the performance of RHYTHM, we compare to LSTM-based models, transformer-based models, and LLM-based models. For the transformer-based models, we conduct experiments with PatchTST [48], PMT [71], ST-MoE-BERT [26], CMHSA [27], iTransformer [39] and COLA [65]. Among these models, ST-MoE-BERT, PMT and COLA are the state-of-the-art models for human mobility prediction. PatchTST and iTransformer are two powerful transformerbased models for time series forecasting. We add the spatiotemporal embedding to the input of those transformer-based time-series models for fair comparison. For the LLM-based models, we conduct experiments with Time-LLM [32] and Mobility-LLM [23]. Time-LLM is state-of-the-art 6 model for time series forecasting using LLMs. We also add the spatiotemporal embedding to the input of Time-LLM for fair comparison. Additionally, in order to make fair comparison, we use the LLaMA-3.2 1B1 as the pretrained LLM model for fine-tuning Time-LLM. Mobility-LLM is versatile LLM-based framework designed for multiple mobility tasks. For the LSTM-based models, we conduct experiments with LSTM [34] and DeepMove [19]. Results. Table 1 show that RHYTHM outperforms the baselines across three datasets in most metrics. On the Sapporo and Hiroshima dataset, RHYTHM achieves the best performance in all evaluation metric. These findings underscore the effectiveness of RHYTHM in mobility prediction tasks. CMHSA and PMT may perform better in Accuracy@3 on the Kumamoto dataset due to their specialized attention mechanisms that effectively capture mid-range candidate locations in this region. Despite sharing an LLM-based architecture, Mobility-LLM underperforms compared to RHYTHM, likely because it was primarily designed for visiting intention tasks requiring rich semantic context. In contrast, RHYTHM leverages temporal tokenization and LLM to model multi-scale spatio-temporal dependencies, prioritizing precise location likelihood maximization. This design focus enables RHYTHM to excel in top-rank precision metrics. Overall, RHYTHM achieves 2.4% improvement in Accuracy@1 and 1.0% Accuracy@5 respectively compared to the best baseline model. Table 1: Performance of RHYTHM and baselines on the Kumamoto, Sapporo, and Hiroshima datasets. The evaluation metrics include Accuracy@k for different values of k. The reported results are averaged over three runs; variance values are omitted as all are 2%. The best results are highlighted in bold, and the second-best results are underlined. RHYTHM demonstrates superior performance compared to baselines across most configurations. Sapporo Kumamoto Hiroshima Model LSTM DeepMove PatchTST iTransformer Time-LLM CMHSA PMT COLA ST-MoE-BERT Mobility-LLM RHYTHM-LLaMA-1B RHYTHM-Gemma-2B RHYTHM-LLaMA-3B Acc@1 Acc@3 Acc@5 Acc@1 Acc@3 Acc@5 Acc@1 Acc@3 Acc@ 0.2652 0.2779 0.2751 0.2609 0.2712 0.2862 0.2697 0.2864 0.2862 0.2666 0.2929 0.2923 0.2941 0.4799 0.4986 0.5018 0.4724 0.4848 0.5182 0.4475 0.5186 0.5155 0.4793 0.5200 0.5191 0.5205 0.5472 0.5683 0.5716 0.5412 0.5535 0.5887 0.5187 0.5896 0.5871 0.5448 0.5835 0.5932 0. 0.2310 0.2825 0.2703 0.2696 0.2792 0.2890 0.2878 0.2847 0.2869 0.2838 0.2931 0.2943 0.2938 0.3940 0.4672 0.4582 0.4500 0.4746 0.4901 0.4896 0.4865 0.4856 0.4703 0.4876 0.4896 0.4875 0.4526 0.5264 0.5168 0.5070 0.5352 0.5525 0.5522 0.5497 0.5480 0.5288 0.5502 0.5545 0. 0.2129 0.2804 0.2752 0.2804 0.2698 0.2874 0.2850 0.2874 0.2839 0.2826 0.2913 0.2953 0.2929 0.3775 0.4810 0.4839 0.4857 0.4753 0.5001 0.4982 0.5013 0.4925 0.4856 0.5027 0.5074 0.5032 0.4415 0.5477 0.5522 0.5523 0.5426 0.5684 0.5668 0.5708 0.5601 0.5525 0.5753 0.5798 0. Geographical Evaluation. We evaluate RHYTHM against baseline models using BLEU and DTW, which respectively measure n-gram similarity and spatial alignment error between predicted and ground-truth trajectories. As shown in Table 2, RHYTHM scores the best DTW performance on Sapporo, demonstrating superior spatial alignment. While COLA leads in BLEU scores for all cities, RHYTHM ranks second in Kumamoto. This highlights key trade-off between exact sequence matching and minimizing spatial deviations. One potential explanation is that COLA employs post-hoc adjustment technique that recalibrates predictions to better align with the long-tail frequency distribution of locations, which may enhance mid-tier accuracy by mitigating overconfidence in dominant locations. Notably, RHYTHM significantly outperforms LSTM-based methods and transformer baselines by leveraging temporal tokenization and prompt-guided reasoning to enhance sequential coherence and spatial precision. This results in an optimal balance for real-world mobility tasks. For MRR, RHYTHM consistently outperforms all baselines, achieving 1.44% improvement over the best baseline and demonstrates its superior ranking capability across diverse mobility patterns. Additional experimental results and extended evaluations are reported in Section G. Transferability. To demonstrate that RHYTHM transfers well across pretrained LLMs, we vary the size of the pretrained backbone and train it on the mobility prediction datasets (see Table 1 for detailed results). In our experiments, we change the size of the pretrained model in RHYTHM 1https://huggingface.co/meta-llama/Llama-3.2-1B 7 Table 2: Performance comparison of RHYTHM with baselines using geographical metrics. The evaluation metrics include DTW (), BLEU (), and MRR (). The best results are highlighted in bold, and the second-best results are underlined. Model LSTM DeepMove PatchTST iTransformer Time-LLM CMHSA PMT COLA ST-MoE-BERT Mobility-LLM RHYTHM-LLaMA-1B RHYTHM-Gemma-2B RHYTHM-LLaMA-3B Kumamoto Sapporo Hiroshima DTW BLEU MRR DTW BLEU MRR DTW BLEU MRR 5014 4630 5251 6178 5984 4490 4536 4446 4691 5603 4478 4416 4470 0.1564 0.1746 0.1315 0.1275 0.1285 0.1810 0.1524 0.2064 0.1557 0.1649 0.1793 0.1928 0. 0.3860 0.4021 0.4021 0.3796 0.3912 0.4158 0.3720 0.4164 0.4151 0.3858 0.4216 0.4205 0.4220 4507 3818 4099 4074 3915 3786 3799 3793 3796 3911 3745 3995 4035 0.1716 0.1959 0.1784 0.1780 0.2145 0.2299 0.2017 0.2496 0.2102 0.1917 0.2496 0.2019 0. 0.3270 0.3887 0.3773 0.3730 0.3902 0.4034 0.4026 0.3996 0.4001 0.3902 0.4045 0.4065 0.4048 5908 4981 5021 5094 5126 4841 4851 4840 4889 4985 5059 4857 4935 0.1544 0.1933 0.1884 0.1789 0.1988 0.2289 0.2009 0.2445 0.2117 0.2056 0.2083 0.2109 0. 0.3113 0.3959 0.3945 0.3977 0.3872 0.4086 0.4065 0.4095 0.4031 0.3990 0.4069 0.4173 0.4140 and test them on the mobility prediction datasets. We use the LLaMA-3.2-1B, LLaMA-3.2-3B, and Gemma-2-2B model as the pretrained backbone of RHYTHM. The results indicate that the performance of RHYTHM improves as the model size increases. Notably, the LLaMA-3.2-3B and Gemma-2-2B model outperforms the LLaMA-3.2-1B model in most metrics. This result demonstrates the performance of RHYTHM scales with LLM size and suggests that larger models may achieve even greater performance improvements on larger datasets. Note that our models are pretrained with 30 epochs. Its plausible that the LLaMA-3.2-3B model requires more epochs to fully converge and realize its full performance potential compared with LLaMA-3.2-1B model. However, the LLaMA-3.2-3B model still achieves competitive performance compared to the LLaMA-3.2-1B model. Overall, the LLaMA-3.2-3B model demonstrated 0.40% improvement in Acc@1 compared to the LLaMA-3.2-1B model, highlighting the scalability of RHYTHM. Figure 3: Training Speed vs. performance of RHYTHM and baseline models on the Sapporo Dataset. Figure 4: Efficiency comparison of alternative LLMs, evaluated by the same configuration of Table 4. Training Speed. To evaluate the training speed of RHYTHM, we conduct experiments on the Sapporo dataset using the same training configuration. We run these experiments on single NVIDIA A100 GPU with 40GB of memory. The results are shown in Figure 3. RHYTHM reduces the number of trainable parameters to only 12.37% of the full model size, reflecting its parameter-efficient design. In terms of runtime, it achieves 24.6% reduction in training time compared with the best-performing baseline, while remaining faster than most other models, being 80.6% faster than LLM-based methods on average. Although RHYTHM is slower than lightweight models such as LSTM, DeepMove, PatchTST, and iTransformer, it substantially outperforms them in accuracy. Moreover, RHYTHM maintains computational efficiency comparable to PMT, COLA and ST-MoE-BERT, despite having significantly higher parameter counts, demonstrating its parameter-efficient design and scalable architecture. Furthermore, RHYTHMs training speed scales predictably with parameter count: LLaMA-3B is 2.2 times slower than LLaMA-1B model, while Gemma-2-2B shows 1.9 times 8 slowdown. detailed breakdown of preprocessing time, storage cost, and training time across datasets is provided in Section H. Daily and Weekly Trend Analysis. We analyze the periodic accuracy trends of RHYTHM and baselines on Sapporo, measuring performance fluctuations across daily and weekly intervals in Figure 5. RHYTHM demonstrates distinct performance characteristics: achieving 5.0% and 3.4% improvements during weekends and evening peak hours respectively, while showing comparable performance during highly regular periods like nighttime and standard weekday working hours. This pattern reveals fundamental insightRHYTHM excels precisely when mobility prediction becomes complex decision-making task rather than simple pattern matching. During regular hours, mobility is largely deterministic with fixed routines where traditional models pattern memorization suffices; however, weekends and transitional periods involve nuanced choices influenced by multiple contextual factors, aligning with findings from Barbosa et al. [3] on weekend variability. In these complex scenarios, RHYTHMs hierarchical attention captures both local daily context and global weekly patterns, while the LLM backbone provides reasoning capabilities to model non-routine decision points. This makes RHYTHM particularly valuable for real-world applications where handling irregular, unpredictable periods is crucial for system reliability, even if simpler models suffice for deterministic segments. Figure 5: Weekly (left) and daily (right) accuracy trends of RHYTHM and baseline models on Sapporo. These results illustrate that prediction performance fluctuates over both daily and weekly intervals. 4.2 Method Analysis In this section, we perform ablation studies to assess the effectiveness of the proposed strategies and test the scaling behavior of RHYTHM. Ablation study. All experiments utilize LLaMA-3.2-1B as the backbone model. To evaluate our key components, we conduct ablation studies across three datasets, as shown in Table 3. Removing temporal tokenization significantly degrades performance by 5.39%, while eliminating hierarchical attention (HA) results in 0.90% decrease, demonstrating that structured temporal encoding is the most critical element of our framework. Regarding semantic enhancement, our findings indicate that both trajectory information and task description prompts contribute substantially to RHYTHMs effectiveness, with their removal causing combined performance drop of 1.82%. Task descriptions yield marginally higher impact than trajectory information, with their removal causing an additional 0.10% performance decrease compared to omitting trajectory information. Further ablation studies examining the contribution of different design choices are included in Section I. Scaling Behavior. Scalability is critical factor in the success of large-scale models. We assess RHYTHMs scalability by analyzing its performance in different model sizes. We conduct experiments using pretrained LLMs of varying sizes, including OPT, LLaMA-3.2, DeepSeek-R1, Gemma-2, Phi-2, and Qwen 2.5 detailed in Table 6. As shown in Table 4, the predictors prediction performance generally improves as the number of LLM parameters increases. This observation is consistent with scaling law dynamics in large models [33]. This scaling behavior highlights the trade-off between predictive performance and adaptation cost. To capture this balance, we assess RHYTHMs scalability across three dimensions: model performance, parameter size, and training and inference speed (time per epoch), as shown in Figure 4. Our results indicate that the largest model, LLaMA-3.2-3B, achieves the best performance for human mobility prediction, while LLaMA-3.2-1B 9 Table 3: Ablation study on each module in RHYTHM. We evaluate each modules contribution to overall performance. The best results are highlighted in bold. Each module significantly influences RHYTHMs performance across all datasets. Kumamoto Sapporo Hiroshima Model Acc@1 Acc@3 Acc@5 Acc@1 Acc@3 Acc@5 Acc@1 Acc@3 Acc@5 RHYTHM w/o HA w/o token w/o Traj info. w/o Task desc. 0.2929 0.2917 0.2801 0.2914 0.2895 0.5200 0.5163 0.5049 0.5176 0. 0.5835 0.5881 0.5764 0.5891 0.5889 0.2938 0.2901 0.2768 0.2879 0.2883 0.4866 0.4856 0.4775 0.4842 0.4839 0.5502 0.5481 0.5409 0.5472 0.5463 0.2913 0.2895 0.2749 0.2858 0.2882 0.5027 0.4946 0.4812 0.4916 0. 0.5753 0.5657 0.5535 0.5633 0.5648 remains the most suitable choice in RHYTHM, providing an optimal balance between performance and computational cost. Table 4: Scalability Performance on RHYTHM. We conduct experiments to evaluate the scalability of RHYTHM on Sapporo using pretrained models of varying parameter sizes. The evaluation metrics include Accuracy@k, MRR, training time per epoch (in seconds), and inference time per epoch (in seconds). The best results are highlighted in bold, while the second-best results are underlined. In most configurations, the performance of RHYTHM improves as the model size increases. Backbone Training Time (s) Inference Time (s) Acc@1 Acc@3 Acc@5 MRR OPT-125M OPT-350M LLaMA-3.2-1B Qwen-2.5-1.5B DeepSeek-R1-1.5B Gemma-2-2B Phi-2 LLaMA-3.2-3B 787 986 5235 9241 7308 9928 10047 11566 107 224 359 336 335 559 693 762 0.2798 0.2837 0.2929 0.2897 0.2921 0.2923 0.2915 0.2941 0.4726 0.4789 0.5200 0.4873 0.5164 0.5191 0.5166 0.5205 0.5231 0.5343 0.5835 0.5521 0.5896 0.5932 0.5892 0. 0.3819 0.3923 0.4216 0.4049 0.4188 0.4205 0.4183 0."
        },
        {
            "title": "5 Conclusion",
            "content": "This paper proposes RHYTHM, an efficient and scalable framework for mobility prediction. RHYTHM leverages temporal tokenization with hierarchical attention mechanisms to model spatiotemporal dependencies while incorporating semantic embeddings to capture cyclical patterns. The integration of frozen pretrained LLMs as reasoning engines enables RHYTHM to interpret nuanced decision-making processes that influence mobility choices, particularly in scenarios with irregular or non-routine movement patterns, at reduced computational costs. Empirical results demonstrate that RHYTHM significantly outperforms state-of-the-art methods in accuracy. Moreover, its high scalability allows for the seamless integration of different pretrained LLMs in plug-and-play manner, offering flexible and efficient prediction framework. Limitations. It is worth noting that RHYTHM has certain limitations. Its performance depends heavily on the quality of pretrained LLMs, which were designed for language tasks rather than mobility prediction. If these models are resource-constrained, they may fail to capture user mobility patterns accurately. RHYTHM does not adopt an autoregressive prediction strategy; although widely studied in time-series modeling [40], we instead emphasize holistic sequence prediction to capture broader contextual dependencies. Future extensions may integrate autoregressive decoding to more closely mimic step-by-step human mobility decisions. In addition, while freezing pretrained LLMs improves efficiency, RHYTHMs training time remains high, limiting its practicality in some applications. Despite these challenges, RHYTHM provides novel framework for mobility prediction, advancing efficiency and accuracy. Future work will focus on refining fine-tuning and quantization methods [43, 15, 73] to improve scalability and reduce resource demands."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "The authors would like to thank Mingzhen for insightful discussions and the anonymous reviewers for their constructive comments. H.H. and Q.R.W.s work is supported by the National Science Foundation (NSF) under Grant Nos. 2125326, 2114197, 2228533, and 2402438, as well as by the Northeastern University iSUPER Impact Engine. H.L. is partially supported by the OpenAI Researcher Access Program. This research was supported in part through the computational resources and staff contributions provided by the Quest High Performance Computing facility at Northwestern University, which is jointly supported by the Office of the Provost, the Office for Research, and Northwestern University Information Technology. Any opinions, findings, conclusions, or recommendations expressed in the paper are those of the authors and do not necessarily reflect the views of the funding agencies."
        },
        {
            "title": "References",
            "content": "[1] Ibrahim Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. Revisiting neural scaling laws in language and vision. Advances in Neural Information Processing Systems, 35:22300 22312, 2022. [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35: 2371623736, 2022. [3] Hugo Barbosa, Marc Barthelemy, Gourab Ghoshal, Charlotte James, Maxime Lenormand, Thomas Louail, Ronaldo Menezes, José Ramasco, Filippo Simini, and Marcello Tomasini. Human mobility: Models and applications. Physics Reports, 734:174, 2018. [4] Michael Batty. The new science of cities. MIT press, 2013. [5] William Berrios, Gautam Mittal, Tristan Thrush, Douwe Kiela, and Amanpreet Singh. Towards language models that can see: Computer vision through the lens of natural language. arXiv preprint arXiv:2306.16410, 2023. [6] Luís MA Bettencourt, José Lobo, Dirk Helbing, Christian Kühnert, and Geoffrey West. Growth, innovation, scaling, and the pace of life in cities. Proceedings of the national academy of sciences, 104(17):73017306, 2007. [7] Sebastiano Bontorin, Simone Centellegher, Riccardo Gallotti, Luca Pappalardo, Bruno Lepri, and Massimiliano Luca. Mixing individual and collective behaviors to predict out-of-routine mobility. Proceedings of the National Academy of Sciences, 122(17):e2414848122, 2025. [8] Dirk Brockmann, Lars Hufnagel, and Theo Geisel. The scaling laws of human travel. Nature, 439(7075):462465, 2006. [9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [10] Oriol Cabanas-Tirapu, Lluís Danús, Esteban Moro, Marta Sales-Pardo, and Roger Guimerà. Human mobility is well described by closed-form gravity-like models learned automatically from data. Nature Communications, 16(1):1336, 2025. [11] Ching Chang, Wei-Yao Wang, Wen-Chih Peng, and Tien-Fu Chen. Llm4ts: Aligning pre-trained llms as data-efficient time-series forecasters. ACM Transactions on Intelligent Systems and Technology, 16(3):120, 2025. [12] Eunjoon Cho, Seth Myers, and Jure Leskovec. Friendship and mobility: user movement in location-based social networks. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 10821090, 2011. 11 [13] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240): 1113, 2023. [14] Weizhen Dang, Haibo Wang, Shirui Pan, Pei Zhang, Chuan Zhou, Xin Chen, and Jilong Wang. Predicting human mobility via graph convolutional dual-attentive networks. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining, pages 192200, 2022. [15] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems, 36, 2024. [16] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Baobao Chang, Xu Sun, Lei Li, and Zhifang Sui. survey on in-context learning. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 11071128, Miami, Florida, USA, November 2024. Association for Computational Linguistics. [17] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [18] Nathan Eagle and Alex Pentland. Reality mining: sensing complex social systems. Personal and ubiquitous computing, 10:255268, 2006. [19] Jie Feng, Yong Li, Chao Zhang, Funing Sun, Fanchao Meng, Ang Guo, and Depeng Jin. Deepmove: Predicting human mobility with attentional recurrent networks. In Proceedings of the 2018 world wide web conference, pages 14591468, 2018. [20] Jie Feng, Yuwei Du, Jie Zhao, and Yong Li. AgentMove: large language model based agentic framework for zero-shot next location prediction. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 13221338, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. ISBN 979-8-89176-189-6. [21] Luciano Floridi and Massimo Chiriatti. Gpt-3: Its nature, scope, limits, and consequences. Minds and Machines, 30:681694, 2020. [22] Sébastien Gambs, Marc-Olivier Killijian, and Miguel Núñez del Prado Cortez. Next place prediction using mobility markov chains. In Proceedings of the first workshop on measurement, privacy, and mobility, pages 16, 2012. [23] Letian Gong, Yan Lin, Xinyue Zhang, Yiwen Lu, Xuedi Han, Yichen Liu, Shengnan Guo, Youfang Lin, and Huaiyu Wan. Mobility-LLM: Learning visiting intentions and travel preference from human mobility data with large language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [24] Marta Gonzalez, Cesar Hidalgo, and Albert-Laszlo Barabasi. Understanding individual human mobility patterns. nature, 453(7196):779782, 2008. [25] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [26] Haoyu He, Haozheng Luo, and Qi Wang. St-moe-bert: spatial-temporal mixture-of-experts In Proceedings of the 2nd ACM framework for long-term cross-city mobility prediction. SIGSPATIAL International Workshop on Human Mobility Prediction Challenge, pages 1015, 2024. [27] Ye Hong, Yatao Zhang, Konrad Schindler, and Martin Raubal. Context-aware multi-head self-attentional neural network model for next location prediction. Transportation Research Part C: Emerging Technologies, 156:104315, 2023. 12 [28] Shang-Ling Hsu, Emmanuel Tung, John Krumm, Cyrus Shahabi, and Khurram Shafique. Trajgpt: Controlled synthetic trajectory generation using multitask transformer-based spatiotemporal model. In Proceedings of the 32nd ACM International Conference on Advances in Geographic Information Systems, pages 362371, 2024. [29] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. [30] Jerry Yao-Chieh Hu, Maojiang Su, En jui kuo, Zhao Song, and Han Liu. Computational limits of low-rank adaptation (loRA) fine-tuning for transformer models. In The Thirteenth International Conference on Learning Representations, 2025. [31] Allen Huang, Hui Wang, and Yi Yang. Finbert: large language model for extracting information from financial text. Contemporary Accounting Research, 40(2):806841, 2023. [32] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, and Qingsong Wen. Time-LLM: Time series forecasting by reprogramming large language models. In The Twelfth International Conference on Learning Representations, 2024. [33] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [34] Dejiang Kong and Fei Wu. Hst-lstm: hierarchical spatial-temporal long-short term memory network for location prediction. In Ijcai, volume 18, pages 23412347, 2018. [35] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. Biobert: pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):12341240, 2020. [36] Weiyu Li, Qi Wang, Yuanyuan Liu, Mario Small, and Jianxi Gao. spatiotemporal decay model of human mobility when facing large-scale crises. Proceedings of the National Academy of Sciences, 119(33):e2203042119, 2022. [37] Qiang Liu, Shu Wu, Liang Wang, and Tieniu Tan. Predicting the next location: recurrent model with spatial and temporal contexts. In Proceedings of the AAAI conference on artificial intelligence, volume 30, 2016. [38] Yifan Liu, Xishun Liao, Haoxuan Ma, Brian Yueshuai He, Chris Stanford, and Jiaqi Ma. Human mobility modeling with limited information via large language models. arXiv preprint arXiv:2409.17495, 2024. [39] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng itransformer: Inverted transformers are effective for time series forecasting. In The Long. Twelfth International Conference on Learning Representations, 2024. [40] Yong Liu, Guo Qin, Xiangdong Huang, Jianmin Wang, and Mingsheng Long. Autotimes: Autoregressive time series forecasters via large language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [41] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. [42] Massimiliano Luca, Gianni Barlacchi, Bruno Lepri, and Luca Pappalardo. survey on deep learning for human mobility. ACM Computing Surveys (CSUR), 55(1):144, 2021. [43] Haozheng Luo, Chenghao Qiu, Maojiang Su, Zhihan Zhou, Zoe Mehta, Guo Ye, Jerry YaoChieh Hu, and Han Liu. Fast and low-cost genomic foundation models via outlier removal. In Forty-second International Conference on Machine Learning, 2025. [44] Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. Biogpt: generative pre-trained transformer for biomedical text generation and mining. Briefings in bioinformatics, 23(6):bbac409, 2022. 13 [45] Yingtao Luo, Qiang Liu, and Zhaocheng Liu. Stan: Spatio-temporal attention network for next location recommendation. In Proceedings of the web conference 2021, pages 21772185, 2021. [46] Soumi Maiti, Yifan Peng, Shukjae Choi, Jee-weon Jung, Xuankai Chang, and Shinji Watanabe. Voxtlm: Unified decoder-only models for consolidating speech recognition, synthesis and speech, text continuation tasks. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1332613330. IEEE, 2024. [47] Meinard Müller. Dynamic time warping. Information retrieval for music and motion, pages 6984, 2007. [48] Yuqi Nie, Nam Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. time series is worth 64 words: Long-term forecasting with transformers. In The Eleventh International Conference on Learning Representations, 2023. [49] Zhenyu Pan, Haozheng Luo, Manling Li, and Han Liu. Conv-coa: Improving open-domain question answering in large language models via conversational chain-of-action. arXiv preprint arXiv:2405.17822, 2024. [50] Zhenyu Pan, Haozheng Luo, Manling Li, and Han Liu. Chain-of-action: Faithful and multimodal question answering through large language models. In The Thirteenth International Conference on Learning Representations, 2025. [51] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318, 2002. [52] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. [53] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. [54] Hubert Ramsauer, Bernhard Schäfl, Johannes Lehner, Philipp Seidl, Michael Widrich, Lukas Gruber, Markus Holzleitner, Thomas Adler, David Kreil, Michael Kopp, Günter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. Hopfield networks is all you need. In International Conference on Learning Representations, 2021. [55] Xuan Rao, Lisi Chen, Yong Liu, Shuo Shang, Bin Yao, and Peng Han. Graph-flashback network for next location recommendation. In Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data mining, pages 14631471, 2022. [56] Filippo Simini, Marta González, Amos Maritan, and Albert-László Barabási. universal model for mobility and migration patterns. Nature, 484(7392):96100, 2012. [57] Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. Nature, 620(7972):172180, 2023. [58] Chaoming Song, Zehui Qu, Nicholas Blumm, and Albert-László Barabási. Limits of predictability in human mobility. Science, 327(5968):10181021, 2010. [59] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. [60] Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multimodal few-shot learning with frozen language models. Advances in Neural Information Processing Systems, 34:200212, 2021. [61] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 14 [62] Geoff Vigar. The politics of mobility: transport, the environment, and public policy. Taylor & Francis, 2002. [63] Qi Wang, Nolan Edward Phillips, Mario Small, and Robert Sampson. Urban mobility and neighborhood isolation in americas 50 largest cities. Proceedings of the National Academy of Sciences, 115(30):77357740, 2018. [64] Xinglei Wang, Meng Fang, Zichao Zeng, and Tao Cheng. Where would go next? large language models as human mobility predictors. arXiv preprint arXiv:2308.15197, 2023. [65] Yu Wang, Tongya Zheng, Yuxuan Liang, Shunyu Liu, and Mingli Song. Cola: Cross-city mobility transformer for human trajectory simulation. In Proceedings of the ACM on Web Conference 2024, pages 35093520, 2024. [66] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [67] Dennis Wu, Jerry Yao-Chieh Hu, Weijian Li, Bo-Yu Chen, and Han Liu. STanhop: Sparse tandem hopfield model for memory-enhanced time series prediction. In The Twelfth International Conference on Learning Representations, 2024. [68] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting, 2022. [69] Shang Wu, Yen-Ju Lu, Haozheng Luo, Maojiang Su, Jerry Yao-Chieh Hu, Jiayi Wang, Jing Liu, Najim Dehak, Jesus Villalba, and Han Liu. SPARQ: Outlier-free speechLM with fast adaptation and robust quantization, 2025. [70] Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. Bloomberggpt: large language model for finance. arXiv preprint arXiv:2303.17564, 2023. [71] Xinhua Wu, Haoyu He, Yanchao Wang, and Qi Wang. Pretrained mobility transformer: foundation model for human mobility. arXiv preprint arXiv:2406.02578, 2024. [72] Yongji Wu, Defu Lian, Shuowei Jin, and Enhong Chen. Graph convolutional networks on user mobility heterogeneous graphs for social relationship inference. In IJCAI, pages 38983904, 2019. [73] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pages 3808738099. PMLR, 2023. [74] Takahiro Yabe, Kota Tsubouchi, Toru Shimizu, Yoshihide Sekimoto, Kaoru Sezaki, Esteban Moro, and Alex Pentland. Yjmob100k: City-scale and longitudinal dataset of anonymized human mobility trajectories. Scientific Data, 11(1):397, 2024. [75] Dingqi Yang, Daqing Zhang, Vincent Zheng, and Zhiyong Yu. Modeling user activity preference by leveraging user spatial temporal characteristics in lbsns. IEEE Transactions on Systems, Man, and Cybernetics: Systems, 45(1):129142, 2014. [76] Dingqi Yang, Benjamin Fankhauser, Paolo Rosso, and Philippe Cudre-Mauroux. Location prediction over sparse user mobility traces using rnns. In Proceedings of the twenty-ninth international joint conference on artificial intelligence, pages 21842190, 2020. [77] Song Yang, Jiamou Liu, and Kaiqi Zhao. Getnext: trajectory flow map enhanced transformer for next poi recommendation. In Proceedings of the 45th International ACM SIGIR Conference on research and development in information retrieval, pages 11441153, 2022. [78] Wei Zhai, Xueyin Bai, Yu Shi, Yu Han, Zhong-Ren Peng, and Chaolin Gu. Beyond word2vec: An approach for urban functional region extraction and identification by combining place2vec and pois. Computers, environment and urban systems, 74:112, 2019. 15 [79] Yunhao Zhang and Junchi Yan. Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting. In The eleventh international conference on learning representations, 2023. [80] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting, 2021. [81] Tian Zhou, Peisong Niu, Xue Wang, Liang Sun, and Rong Jin. One fits all: Power general time series analysis by pretrained LM. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [82] Zhihan Zhou, Weimin Wu, Jieke Wu, Lizhen Shi, Zhong Wang, and Han Liu. Genomeocean: Efficient foundation model for genome generation, 2025."
        },
        {
            "title": "NeurIPS Paper Checklist",
            "content": "1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the papers contributions and scope? Answer: [Yes] Justification: The abstract and introduction clearly state the main claims made in the paper. Guidelines: The answer NA means that the abstract and introduction do not include the claims made in the paper. The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. No or NA answer to this question will not be perceived well by the reviewers. The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations in Section 5. Guidelines: The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. The authors are encouraged to create separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on few datasets or with few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach. For example, facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, worse outcome might be that reviewers discover limitations that arent acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. Theory assumptions and proofs Question: For each theoretical result, does the paper provide the full set of assumptions and complete (and correct) proof? Answer: [Yes] 17 Justification: We provide theory guarantees of RHYTHM in Section 3.4. Guidelines: The answer NA means that the paper does not include theoretical results. All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. All assumptions should be clearly stated or referenced in the statement of any theorems. The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide short proof sketch to provide intuition. Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental result reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide all the details necessary to reproduce the main experimental results in the paper. The details of the experiments are included in both the main paper and the supplemental material. Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. If the contribution is dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is novel architecture, describing the architecture fully might suffice, or if the contribution is specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to hosted model (e.g., in the case of large language model), releasing of model checkpoint, or other means that are appropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is new model (e.g., large language model), then there should either be way to access this model for reproducing the results or way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code 18 Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The code and data, including detailed instructions to reproduce the main experimental results, are publicly available at https://github.com/he-h/rhythm. Guidelines: The answer NA means that paper does not include experiments requiring code. Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for new open-source benchmark). The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only subset of experiments are reproducible, they should state which ones are omitted from the script and why. At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental setting/details Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We introduce the experimental setting in main paper and provide additional details in the supplemental material. Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to level of detail that is necessary to appreciate the results and make sense of them. The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment statistical significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report the values in all tables and observed stable results with less than 2% variance. Guidelines: The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. 19 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). The method for calculating the error bars should be explained (closed form formula, call to library function, bootstrap, etc.) The assumptions made should be given (e.g., Normally distributed errors). It should be clear whether the error bar is the standard deviation or the standard error of the mean. It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report 2-sigma error bar than state that they have 96% CI, if the hypothesis of Normality of errors is not verified. For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments compute resources Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We introduce the compute resources in Section F.2. Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didnt make it into the paper). 9. Code of ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have carefully reviewed and followed the NeurIPS Code of Ethics throughout our research process. Guidelines: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. If the authors answer No, they should explain the special circumstances that require deviation from the Code of Ethics. The authors should make sure to preserve anonymity (e.g., if there is special consideration due to laws or regulations in their jurisdiction). 10. Broader impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss the broader impacts in Section A. Guidelines: The answer NA means that there is no societal impact of the work performed. 20 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our model does not have such risk of misuse. Guidelines: The answer NA means that the paper poses no such risks. Released models that have high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have properly credited all assets used in our research and respected their licenses and terms of use. Guidelines: The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include URL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from particular source (e.g., website), the copyright and terms of service of that source should be provided. If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of dataset. For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. If this information is not available online, the authors are encouraged to reach out to the assets creators. 13. New assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We do not release new assets in our work. Guidelines: The answer NA means that the paper does not release new assets. Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. The paper should discuss whether and how consent was obtained from people whose asset is used. At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and research with human subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: We do not include crowdsourcing experiments or research involving human subjects in this paper. Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional review board (IRB) approvals or equivalent for research with human subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: We do not include crowdsourcing experiments or research involving human subjects in this paper. Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. 22 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 16. Declaration of LLM usage Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required. Answer: [NA] Justification: We only use LLMs to assist with writing and formatting the paper. Guidelines: The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components. Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM) for what should or should not be described."
        },
        {
            "title": "Supplemental Material",
            "content": "A Broader Impact Extended Related Work Attention Implementation Details Prompt Design Examples Dataset Experiment Settings F.1 Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.2 Computational Resource . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.3 Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.4 LLM variants . . . . . . . . . . . . . Additional Experimental Results G.1 Autoregressive vs Non-autoregressive Strategy . . . . . . . . . . . . . . . . . . . . G.2 Deployment efficiency compared to LLM-based baselines . . . . . . . . . . . . . . Resource Requirements and Computational Cost H.1 Dataset Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . H.2 Training Resource Usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Ablation Studies Segment Length Sensitivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . I.1 I.2 Impact of Pretrained LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . I.3 Computational Cost of RHYTHM Components . . . . . . . . . . . . . . . . . . . 24 26 26 27 27 27 28 28 28 28 28 28 29 29 29 29"
        },
        {
            "title": "A Broader Impact",
            "content": "This paper introduces new foundation model for human mobility, aiming to improve the reliability and generalization of foundation model applications in spatio-temporal domains. While the work does not have immediate societal implications, it lays the groundwork for future applications in urban planning, public health, disaster response, and transportation. However, the model may inadvertently encode or amplify biases present in the training data, potentially leading to inequitable outcomes in mobility predictions."
        },
        {
            "title": "B Extended Related Work",
            "content": "Mobility Prediction. Human mobility prediction has evolved from foundational statistical models to advanced deep learning frameworks. Physics-inspired models such as the gravity model [10] and the radiation model [56] predict aggregate population flows using distance and opportunity metrics but lack individual-level detail. To address this, probabilistic approaches like Markov chains [22] and tensor factorization [75] emerge, modeling location transitions at the user level. While these methods improve personalization, they struggle with sparse trajectories and higher-order dependencies inherent in real-world mobility data. Deep learning introduces sequence-aware architectures like LSTM [37], which capture local temporal contexts, and attention-enhanced variants [19] that address vanishing gradients. However, these models often overlook cyclical patterns. Hybrid approaches like Graph-Flashback [55] and GCDAN [14] integrate graph structures to model spatial relationships, but their reliance on fixed-length sequences limits scalability for long-term forecasting. Transformers [61] revolutionize the field with self-attention mechanisms for long-term dependency modeling. Innovations like STAN [45] combine 24 spatial-temporal attention for next-POI recommendation, while COLA [65] extends this to cross-city dynamics. GETNext [77] further refines predictions by disentangling individual preferences from population flows. Despite these advances, Transformer-based methods remain timestamp-centric, incurring quadratic complexity for multi-day sequences and failing to explicitly model hierarchical periodicities (e.g., daily vs. weekly rhythms). While Transformer-based approaches improve long-term dependency modeling, they still rely on timestamp-centric encoding and struggle with hierarchical periodicities. Recent work explores large language models (LLMs) as an alternative, leveraging their strong generalization capabilities for mobility tasks [23, 38]. Studies like LLM-Mob [64] and AgentMove [20] leverage prompt engineering for next-location prediction and trajectory user linking, while TrajGPT [28] generates synthetic visits via autoregressive decoding. However, these approaches treat mobility sequences as generic token streams, neglecting structured periodic patterns and the modality gap between natural language and spatio-temporal data. Unlike existing LLM-based methods that treat mobility sequences as generic tokens, our approach uses temporal tokenization to explicitly model structured periodicity (daily/weekly cycles), thereby mitigating modality mismatches and capturing multi-scale dependencies for improved long-term mobility prediction. Time Series Foundation Models. Existing time series foundation models can be divided into two categories: transformer-based models and language-based models. For transformer-based time series models [67, 39, 48], prior studies focus on transformer architecture and self-attention mechanisms to capture temporal dependency in time series data. For instance, PatchTST [48] introduces patchbased self-attention mechanism to capture long-range dependencies in time series data. STanHop [67] and Crossformer [79] employ hierarchical self-attention to capture temporal dependencies and hierarchical structures in time series data. For language-based time series models [40, 32], prior studies adapt the LLMs to time series data and achieve state-of-the-art performance in time series forecasting tasks. For instance, AutoTime [40] introduces novel autoregressive structure to capture the temporal dependency in time series data. Time-LLM [32] employs large language model to capture the complex transitions of time series data. However, these models struggle to capture the inherent complexity of human mobilitywith its abrupt location shifts and temporal dynamicswhereas RHYTHM leverages novel spatio-temporal embedding paired with an autoregressive framework to effectively model these intricate transitions. Cross-domain Adaptation of LLMs. LLMs have evolved from specialized natural language processing systems into versatile foundation models capable of sophisticated reasoning across diverse tasks [1, 9]. Their transformer-based architecture and extensive pretraining have enabled remarkable transfer capabilities to domains beyond text. In computer vision, models like CLIP [53] align visual and textual representations for zero-shot recognition, while in time series analysis, approaches such as One-Fits-All [81] and LLM4TS [11] demonstrate competitive forecasting through tokenized numerical sequences. In biomedicine, BioBERT [35] and BioGPT [44] demonstrate significant gains on clinical NLP benchmarks, while instruction-tuned models like Med-PaLM approach expertlevel medical QA performance [57]. In finance, domain-specific LLMs such as FinBERT [31] and BloombergGPT [70] substantially outperform general-purpose models on sentiment analysis and information extraction. Rather than computationally expensive full fine-tuning, parameter-efficient adaptation strategies have gained prominence. Low-Rank Adaptation (LoRA) [29] introduces trainable low-rank matrices into attention layers, while more recent approaches keep LLMs entirely frozen by using lightweight adapters that project non-linguistic inputs into the models embedding space. In vision, prefix-tuning methods [2, 60] train small encoders to produce \"prompts\" for frozen LLMs, while time series approaches [40, 32] employ projection layers to convert numeric sequences into token embeddings. Applications of LLMs to human mobility modeling remain limited, with existing approaches relying primarily on parameter-intensive adaptation. Mobility-LLM [23] employs partial fine-tuning, while LLM-Mob [64] leverages in-context learning but lacks structured temporal modeling. In contrast, RHYTHM maintains fully frozen LLM backbone, preserving the models pre-trained knowledge while introducing specialized spatio-temporal framework that efficiently adapts to mobility data characteristics."
        },
        {
            "title": "C Attention Implementation Details",
            "content": "Our attention mechanism implements pre-norm transformer block to enhance training stability, with gated feed-forward network for improved expressivity. The mathematical formulation of our attention block is as follows: = LayerNorm(X) + Multi-Head Attention(LayerNorm(X)), = + GatedFFN(LayerNorm(Z)), where is the input sequence. The multi-head attention operation computes: Multi-Head(X) = [head1head2 . . . headh]Wout, (cid:19) (cid:18) XWq,i(XWk,i) headi = Softmax XWv,i, dk with attention heads, where Wq,i, Wk,i, Wv,i Rddk are the query, key, and value projection matrices for the i-th head, and Wout Rdd is the output projection matrix. The gated feed-forward network incorporates an adaptive gating mechanism: GatedFFN(Z) = FFN(Z) σ(WgateZ), FFN(Z) = W2 GELU(W1Z), where σ denotes the sigmoid function, represents element-wise multiplication, and Wgate Rdd is the learnable gating matrix. The feed-forward network expands the hidden dimension by factor of 4, with W1 R4dd and W2 Rd4d. Dropout is applied after both the attention and feed-forward operations to prevent overfitting."
        },
        {
            "title": "D Prompt Design Examples",
            "content": "Trajectory Information This is the trajectory of user <User_ID> of day <Day_ID> which is <Day_of_Week>. The trajectory consists of <N> records, each record of coordinate is as follows: 08:30: 09:00: 09:30: 10:00: 10:30: 11:00: 11:30: 12:00: 12:30: 13:00: (X=136, Y=42); (X=136, Y=42); (X=137, Y=41); (X=146, Y=37); (X=145, Y=38); (X=144, Y=38); (X=135, Y=41); (X=135, Y=42); (X=135, Y=42); (X=135, Y=42). Key transitions: Y=38) (X=135, Y=41). At 10:00: (X=137, Y=41) (X=146, Y=37); At 11:30: (X=144, Main stay locations: (X=136, Y=42) from 08:30 to 09:30 (0.5 hours); (X=145, Y=38) from 10:00 to 11:00 (0.5 hours); (X=135, Y=42) from 11:30 to 13:00 (1.5 hours). Task Description You are mobility prediction assistant that forecasts human movement patterns in urban environments. grid of cells, where each cell is identified by coordinates (X,Y). The The city is represented as 200 200 coordinate increases from left (0) to right (199), and the coordinate increases from top (0) to bottom (199). TASK: Based on User <User_ID>s historical movement patterns, predict their locations for Day <Day_ID> (<Day_of_Week>). The predictions should capture expected locations at 30-minute intervals throughout the day (48 time slots). The model should analyze patterns like frequent locations, typical daily routines, and time-dependent behaviors to generate accurate predictions of where this user is likely to be throughout the next day. The previous days trajectory data contains information about the users typical movement patterns, regular visited locations, transition times, and duration of stays. locations, morning and evening routines, lunch-time behaviors, weekend vs. weekday differences, and recurring visit patterns. Key patterns to consider include: home and work"
        },
        {
            "title": "E Dataset",
            "content": "We provide the detail of datasets used in this paper as shown in Table 5. Table 5: Dataset Statistics City Users Duration Spatial Resolution Places Kumamoto Sapporo Hiroshima 3k 17k 22k 75 days 75 days 75 days 500m 500m 500m 500m 500m 500m 40k 40k 40k"
        },
        {
            "title": "F Experiment Settings",
            "content": "F.1 Evaluation Metrics Accuracy@k measures proportion of correct predictions within top-k ranked locations: Accuracy@k = 1 (cid:88) i= 1(lT +i top-k(ˆpT +i)), where 1() is the indicator function and ˆpT +i is the predicted probability distribution. Mean Reciprocal Rank (MRR) evaluates quality of ranked predictions: MRR = 1 (cid:88) i= 1 rank(lT +i) , where rank(lT +i) is the rank position of the true location. Dynamic Time Warping (DTW) measures spatial similarity between trajectories: DTW(Y, ˆY) = min π (cid:88) (i,j)π d(lT +i, ˆlT +j), where π is valid warping path and d(, ) is the Euclidean distance. BLEU quantifies n-gram overlap between predicted and ground-truth sequences: BLEU = BP exp (cid:33) wn log pn , (cid:32) (cid:88) n=1 where pn is n-gram precision, wn is the weight for each n-gram level, and BP is brevity penalty. F.2 Computational Resource We perform all experiments using single NVIDIA A100 GPU with 40GB of memory and 24-core Intel(R) Xeon(R) Gold 6338 CPU operating at 2.00GHz. Our code is developed in PyTorch [52] and utilizes the Hugging Face Transformer Library2 for experimental execution. F.3 Hyperparameters We present the hyperparameters used in the training stage for each model. Embeddings for timeof-day and day-of-week, the categorical location embedding, and the coordinate projection all use hidden dimensions of 128, 128, 256, and 128, respectively. We use AdamW [41] as the optimizer. For model training, we conduct systematic hyperparameter search, exploring learning rates from the set {1 104, 3 104, 5 104} and weight decay values from {0, 0.001, 0.01}. Through extensive validation experiments, we determine the optimal configuration for each dataset. All models are trained with consistent batch size of 64 across all datasets for fair comparison. The final hyperparameter settings are selected based on performance on the validation set. F.4 LLM variants Our experiments deployed multiple foundation language models as text embedders and frozen backbones within RHYTHM to evaluate cross-scale performance. Table 6 presents the pre-trained models accessed through the Hugging Face Transformers library, ranging from 125M to 3B parameters. Table 6: Pre-trained language models employed as backbones in RHYTHM. Model Parameters HuggingFace Repository OPT-125M OPT-350M LLaMA-3.2-1B Qwen-2.5-1.5B DeepSeek-R1-1.5B Gemma-2-2B Phi-2 LLaMA-3.2-3B 125M 350M 1.24B 1.54B 1.78B 2.61B 2.78B 3.21B facebook/opt-125m facebook/opt-350m meta-llama/Llama-3.2-1B Qwen/Qwen2.5-1.5B deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B google/gemma-2-2b-it microsoft/phi-2 meta-llama/Llama-3.2-3B"
        },
        {
            "title": "G Additional Experimental Results",
            "content": "G.1 Autoregressive vs Non-autoregressive Strategy Table 7 compares RHYTHM under autoregressive and non-autoregressive strategies. The nonautoregressive approach delivers comparable accuracy while being over two orders of magnitude faster. Table 7: Comparison of RHYTHM with autoregressive and non-autoregressive prediction strategies. Results are reported using Acc@1, Acc@3, Acc@5, and computational time per iteration (s/iter). The best results are highlighted in bold. Model Time (s/iter) Acc@1 Acc@3 Acc@5 Non-autoregressive Autoregressive 0.96 39.80 0.2929 0.2884 0.5200 0.5247 0.5835 0.5801 G.2 Deployment efficiency compared to LLM-based baselines Table 8 reports deployment efficiency for RHYTHM compared with LLM-based baselines, evaluated under both GPU and CPU inference. On GPU, RHYTHM requires substantially less memory 2https://huggingface.co/docs/transformers 28 and achieves lower latency than TimeLLM, while remaining competitive with Mobility-LLM. On CPU, RHYTHM also demonstrates favorable latency and moderate RAM usage, underscoring its practicality for deployment in resource-constrained environments. Table 8: Deployment efficiency of RHYTHM, TimeLLM, and Mobility-LLM. Deployment efficiency of RHYTHM, TimeLLM, and Mobility-LLM. We report model size, GPU memory footprint (GRAM), inference latency on GPU and CPU, and RAM usage. GPU CPU Model Size (MB) GRAM (MB) Latency (ms) RAM (MB) Latency (ms) RHYTHM TimeLLM Mobility-LLM 5841.9 3762.7 4880.7 5741.4 11213.1 9872.8 261.6 0.8 392.7 9.5 192.1 5. 12497.9 18677.8 10552.2 39.5 0.9 64.2 2.5 32.3 1."
        },
        {
            "title": "H Resource Requirements and Computational Cost",
            "content": "In this section, we detail RHYTHMs resource requirements to provide practical perspective on the computational cost of deploying RHYTHM. H.1 Dataset Preprocessing Time and storage costs of semantic embedding generation are detailed in Table 9. Table 9: Preprocessing cost of RHYTHM across datasets. Storage Size (GB) Dataset Preprocessing Time (h) Kumamoto Sapporo Hiroshima 3.1 15.9 20.1 1.6 9.1 11.8 H.2 Training Resource Usage Training requirements with varying sequence lengths are summarized in Table 10, including GPU memory (GRAM) and runtime per epoch. The results show that memory consumption and training time do not increase linearly with sequence length, demonstrating that RHYTHM can handle long sequences at moderate additional cost due to its temporal tokenization design. Table 10: Training cost with varying sequence lengths. Sequence Length (T ) GRAM (MB) Time/Epoch (min) 48 168 336 7126.3 7469.5 8202.0 9826.4 23.1 24.6 26.5 30."
        },
        {
            "title": "I Additional Ablation Studies",
            "content": "I.1 Segment Length Sensitivity We analyze the impact of varying temporal segment length on prediction accuracy and efficiency in Table 11. Smaller segments (e.g., = 1, 30 minutes) capture fine-grained details but result in excessive fragmentation and substantially higher computation time. In contrast, very large segments (e.g., = 96, 2 days) reduce runtime but blur meaningful daily mobility boundaries, leading to 29 degraded accuracy. Daily segmentation (L = 48) achieves the best balance, yielding the highest predictive performance while keeping iteration time under 1 second. These findings validate our choice of = 48 as balanced temporal unit for mobility modeling. Table 11: Effect of varying segment length on prediction accuracy and runtime. The best results are highlighted in bold. Segment Length (L) Segments (N ) Acc@1 Acc@3 Acc@5 Time (s/iter) 1 (30 min) 24 (12 hr) 48 (1 day) 96 (2 days) 336 14 7 3 0.2801 0.2883 0.2929 0.2851 0.5049 0.5124 0.5200 0.5087 0.5764 0.5792 0.5835 0. 6.59 1.10 0.96 0.93 I.2 Impact of Pretrained LLMs RHYTHM benefits substantially from pretraining: the pretrained LLM variant achieves the strongest accuracy, while randomly initialized and LLM-free variants lag behind as shown in Table 12. Table 12: Comparison of RHYTHM with pretrained, randomly initialized, and no-LLM variants on Kumamoto. Pretraining provides the best accuracy across all metrics. RHYTHM Variant Acc@1 Acc@3 Acc@ w/ pretrained LLM (ours) w/ randomly initialized LLM (frozen) w/o LLM (only attention) 0.2929 0.2556 0.2749 0.5200 0.4842 0.4921 0.5835 0.5313 0.5623 I.3 Computational Cost of RHYTHM Components Table 13 shows how different architectural choices affect model size and training time. Temporal tokenization and hierarchical attention both contribute to reducing runtime without significantly increasing parameter count, highlighting their role in RHYTHMs efficient design. Table 13: Trainable parameters (absolute and relative) and training time per epoch for different architectural configurations of RHYTHM on Kumamoto. Configuration Trainable Params (MB) Share of Total (%) Time/Epoch (min) Full RHYTHM (frozen LLM) Unfrozen LLM w/ LoRA w/o Temporal Tokenization Only Attention Module (no LLM) w/o HA 152 200 148 152 12.37 16.27 12.00 12.37 3.25 31 102 53 16"
        }
    ],
    "affiliations": [
        "Northeastern University",
        "Northwestern University"
    ]
}