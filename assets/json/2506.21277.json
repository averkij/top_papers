{
    "paper_title": "HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context",
    "authors": [
        "Qize Yang",
        "Shimin Yao",
        "Weixuan Chen",
        "Shenghao Fu",
        "Detao Bai",
        "Jiaxing Zhao",
        "Boyuan Sun",
        "Bowen Yin",
        "Xihan Wei",
        "Jingren Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the rapid evolution of multimodal large language models, the capacity to deeply understand and interpret human intentions has emerged as a critical capability, which demands detailed and thoughtful reasoning. In recent studies, Reinforcement Learning (RL) has demonstrated potential in enhancing the reasoning capabilities of Large Language Models (LLMs). Nonetheless, the challenges associated with adapting RL to multimodal data and formats remain largely unaddressed. In this paper, we identify two issues in existing multimodal reasoning models: insufficient global context understanding and shortcut problems. Insufficient context understanding can happen when a model misinterprets multimodal context, resulting in incorrect answers. The shortcut problem occurs when the model overlooks crucial clues in multimodal inputs, directly addressing the query without considering the multimodal information. To tackle these issues, we emphasize the necessity for the model to reason with a clear understanding of the global context within multimodal inputs. This global context understanding can effectively prevent the model from overlooking key multimodal cues and ensure a thorough reasoning process. To ensure the accurate interpretation of multimodal context information, we implement a context reward judged by a large language model, alongside format and accuracy rewards. Additionally, to improve complex reasoning capability, we employ the LLM to assess the logical reward, determining whether the reasoning process successfully integrates multimodal information with logical methods. We also introduce a reasoning omni-modal benchmark, IntentBench, aimed at evaluating models in understanding complex human intentions and emotions. Our proposed method demonstrates advanced performance across multiple omni-modal benchmarks compared to other open-source omni-modal models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 7 7 2 1 2 . 6 0 5 2 : r HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context"
        },
        {
            "title": "Detao Bai",
            "content": "Qize Yang Shimin Yao Weixuan Chen Shenghao Fu Jiaxing Zhao Boyuan Sun Bowen Yin Xihan Wei"
        },
        {
            "title": "Jingren Zhou",
            "content": "Tongyi Lab, Alibaba Group qize.yqz@alibaba-inc.com https://github.com/HumanMLLM/HumanOmniV"
        },
        {
            "title": "Abstract",
            "content": "With the rapid evolution of multimodal large language models, the capacity to deeply understand and interpret human intentions has emerged as critical capability, which demands detailed and thoughtful reasoning. In recent studies, Reinforcement Learning (RL) has demonstrated potential in enhancing the reasoning capabilities of Large Language Models (LLMs). Nonetheless, the challenges associated with adapting RL to multimodal data and formats remain largely unaddressed. In this paper, we identify two issues in existing multimodal reasoning models: insufficient global context understanding and shortcut problems. Insufficient context understanding can happen when model misinterprets multimodal context, resulting in incorrect answers. The shortcut problem occurs when the model overlooks crucial clues in multimodal inputs, directly addressing the query without considering the multimodal information. To tackle these issues, we emphasize the necessity for the model to reason with clear understanding of the global context within multimodal inputs. This global context understanding can effectively prevent the model from overlooking key multimodal cues and ensure thorough reasoning process. To ensure the accurate interpretation of multimodal context information, we implement context reward judged by large language model, alongside format and accuracy rewards. Additionally, to improve complex reasoning capability, we employ the LLM to assess the logical reward, determining whether the reasoning process successfully integrates multimodal information with logical methods. Moreover, we develop reasoning training dataset that incorporates context information across tasks involving images, videos, and audio. We also introduce reasoning omni-modal benchmark, IntentBench, aimed at evaluating models in understanding complex human intentions and emotions. Our proposed method demonstrates advanced performance across multiple omni-modal benchmarks compared to other open-source omni-modal models."
        },
        {
            "title": "Introduction",
            "content": "As the applications of Multimodal Large Language Models (MLLMs) in human interaction rapidly increase, understanding and reasoning human intentions and thoughts in complex scenarios becomes increasingly important. Through recent improvements in pretraining and instruction fine-tuning, the capabilities of omni-modal models [1, 2, 3, 4] have significantly advanced. While these models are able to handle multiple types of inputs like text, video, and audio simultaneously in complex realworld environments, they often lack strong reasoning abilities. Inspired by DeepSeek-R1 [5], many *Equal contribution Project lead Preprint. Under review. Figure 1: Visualizations of the vanilla GRPO method applied in multimodal tasks. When the model is overconfident on questions, it tends to answer questions directly without considering the global context (left) or may overlook key multimodal inputs (right). reasoning methods [6, 7, 8, 9] for MLLMs adapt Group Relative Policy Optimization(GRPO) [10] to train models. Specifically, given multimodal input and question, these methods prompt the MLLM to generate reasoning chain that leads to an answer. The model is optimized using both accuracy reward and format reward. The accuracy reward assesses the correctness of the answer, while the format reward encourages following the reason-answer output format. By considering the question step-by-step, models achieve enhanced performance on various tasks, especially for multi-modal mathematical problems. However, models directly adapting vanilla GRPO rely heavily on text reasoning, ignoring the abundant multimodal cues and their comprehensive understanding. In this paper, we highlight two issues prevalent in current multimodal reasoning models: insufficient global context understanding and shortcut problems during reasoning. Without understanding the global context or ignoring some crucial details, models will misinterpret the multimodal inputs, resulting in incorrect answers. For example, in the left part of Figure 1, the model trained with vanilla GRPO only captures part of the evidence in the video and provides sub-optimal answer, failing to consider the entire video thoroughly. While in the right part, the model only observes the visual reaction of the face without considering the speech from the woman, leading to an incorrect answer. To address these two problems, we require the model to reason based on precise understanding of the global context within multimodal inputs. To achieve this, the model first outputs the context information within the <context> tag. The context understanding prevents the model from bypassing crucial multimodal inputs and offers comprehensive global context during the subsequent reasoning process. For example, when someone says no, only within full context can the model determine whether it is rejection, joke, or reverse request. To ensure the model accurately comprehends the multimodal context information, we introduce context reward in addition to the format reward and accuracy reward. This context reward guides the model to improve its understanding of context, assessed by LLM that compares the consistency between the reference context and the models output. Additionally, to encourage the model to develop complex reasoning abilities, we introduce logical reward by using the LLM to assess whether the reasoning process integrates multimodal information and incorporates logical analysis techniques such as reflection, deduction, and induction. The context reward is calculated based on the context part of the completions, and the logical reward is related to both the context and reasoning parts in the completions. Furthermore, training MLLMs to reason is extremely challenging, primarily due to the scarcity of large-scale human-annotated reasoning data [11, 12, 13]. For this purpose, we develop omni-modal reasoning training dataset, which incorporates context information and consists of understanding tasks 2 involving images, videos, and audios. Another challenge in developing omni-modal reasoning models is the lack of related benchmarks to evaluate their performance effectively. We present IntentBench, novel omni-modal benchmark designed to comprehend human activities and intentions in complex scenes. It includes 633 videos and 2,689 questions that are related to auditory and visual cues within the videos. This benchmark requires strong understanding and reasoning of global context, careful observation, and complex social relationships. Daily-Omni [14] and WorldSense [15] primarily focus on general perception scenarios. In these datasets, some questions are only related to either video or audio clues. In contrast, IntentBench is designed to evaluate the understanding and reasoning abilities of omni-modal models regarding the complex intentions and emotions of humans. Finally, we develop HumanOmniV2, which achieves the best performance among open-source omni-modal models, with scores of 58.47% on Daily-Omni, 47.1% on WorldSense, and 69.33% on IntentBench. Our contributions can be summarized as: We propose that models should summarize the context of multimodal inputs before engaging in the reasoning process. This approach aims to mitigate issues such as skipping crucial multimodal information and context understanding on multimodal inputs. Additionally, we employ context rewards and logical rewards to incentivize models to accurately summarize the context and facilitate complex reasoning. We provide an omni-modal reasoning training dataset, which includes summaries of multimodal input and reasoning paths for both cold start training and the reinforcement learning stage. Furthermore, we have curated human-centric benchmark, IntentBench, for omnimodal evaluation, which requires simultaneously understanding video and audio, the global context, complex social relationships, and careful observation. Our proposed HumanOmniV2 achieves the best performance across multiple omni-modal benchmarks compared to existing open-source omni-modal methods, including the newly introduced IntentBench, Daily-Omni, and WorldSense."
        },
        {
            "title": "2 Related Works",
            "content": "Omni-Modal Large Language Model and Benchmarks. Omni-modal large language models [1, 16, 17] usually include video and audio modalities, moving towards more comprehensive multimodal understanding. MiniCPM-o 2.6 [17] and Ocean-Omni-1.5 [2] enhance their visionlanguage foundation by adding audio processing capabilities, enabling operation across more modalities. Ola [3] excels with its progressive modality alignment strategy, which incrementally enhances the language models supporting modalities. VITA1.5 [4] focuses primarily on real-time, end-to-end speech interaction, while IXC2.5-OL [18] introduces memory mechanisms to manage long contexts in streaming videos. ViSpeak [19] proposes novel task named visual instruction feedback in which models should be aware of visual contents and learn to extract instructions from them. While most existing multimodal benchmarks [20, 21, 22, 23] focus solely on images or videos, only few validate both audio and video simultaneously. Although OmniBench[24] combines image and audio for evaluation, it primarily focuses on straightforward cognitive tasks and only provides images instead of original videos. Daily-Omni [14] and WorldSense [15] primarily focus on general scenarios where some challenges relate solely to video or audio clues. In contrast, our benchmark requires both audio and video understanding simultaneously to answer each question. Besides, unlike existing reasoning benchmarks that focus on the STEM (science, technology, engineering, and math) domain, our benchmark is designed to help models understand the complex intentions and emotions of humans. Reinforcement Learning for MultiModal Reasoning. Reinforcement learning has shown greater effectiveness than supervised fine-tuning (SFT) in building general reasoning abilities. It allows models to explore wider range of language and develop their own thinking processes. Recent studies [25, 26] combine RL with vision and language, enhancing these capacities. Insight-V [27] employs multi-agent system to refine models by selecting and learning from self-generated reasoning paths. R1-VL [26] introduces two novel rule-based reasoning rewards to improve reasoning accuracy and validity. Vision-R1 [25] proposes progressive thinking suppression training strategy, employing GRPO with hard formatting result reward function to gradually enhance the models reasoning skills on 10k multimodal math dataset. Video-R1 [7] introduces the T-GRPO algorithm, aimed at 3 Figure 2: (a)(b)(c) are examples from Social-IQ 2.0, MDPE, and EMER, respectively. (d) is the statistic of the curated testing set from Social-IQ 2.0 enhancing models ability to leverage temporal information within videos for improved reasoning. Two concurrent works, Visionary-R1 [28] and Observe-R1 [29], also involve the model observing the image or video first before reasoning. However, they focus exclusively on vision-related tasks and do not evaluate the context comprehensively, largely overlooking the potential of more comprehensive omni-modal integration. R1-Omni [30] primarily focuses on audio-visual referring segmentation, while EchoInk-R1 [31] explores applying vanilla GRPO on OmniBench, which limits their general applicability. In contrast, with accurate context understanding, our omni-modal reasoning model not only understands complex human intentions but also performs exceptionally well on general omni-modal benchmarks."
        },
        {
            "title": "IntentBench",
            "content": "Currently, multimodal reasoning evaluation datasets like MathVista [32], MMVU [33], and VideoMMMU [20] primarily focus on the STEM domain, where audio cues are typically unnecessary. We introduce new benchmark for evaluating omni-modal reasoning, namely IntentBench, which requires analysis on audio and vision clues. Our dataset focuses on understanding human social interactions in videos, including intention, emotion, and deception. In multi-turn-taking conversations and real-world interactions, interpreting glance, change in tone, or the varied meanings of the same words in different contexts is highly challenging task for multimodal large language models. Our benchmark is curating from Social-IQ 2.0 [34, 35], EMER [36], and MDPE [37]. Social-IQ 2.0 paves the way for explainable social intelligence. The dataset is meticulously curated, featuring validated videos, questions, and answers, alongside complexity annotations for each question and answer. Social-IQ 2.0 includes more than 1,000 videos, 6,000 questions, and 24,000 answers. While humans can understand social contexts with high accuracy, current advanced computational models still face difficulties with this task. EMER offers detailed explanations for its emotion, unlike traditional emotion recognition. It extracts more reliable open vocabulary emotion labels, as each label is grounded in specific basis. It includes 332 video samples from MER2023 for annotation. Human emotions are often subtle or mixed, requiring careful observation of facial expressions, body movements, and speech. This complexity requires deep analysis to interpret emotional states. 4 MDPE includes recordings of 193 individuals, with each person answering 24 questions. Out of these, 9 questions are randomly selected for lying, and the interviewer assesses whether the candidate is lying. After the interview, participants also complete the \"Subject Lie Confidence Scale,\" where they rate their confidence in successfully deceiving, ranging from 1 to 5, with 1 meaning successful deception and 5 meaning not deceived successfully. To effectively judge deception, the model must thoroughly analyze body language, microexpressions, tone, and speech content, which makes it complex task. For Social-IQ 2.0, we select 300 videos and 2,356 questions. We use GPT-4o [38] with only text modal for testing to identify challenging questions. We also replace easy options to increase the difficulty of the testing set. Finally, we conduct manual verification to ensure each question is relevant to multimodal information and cannot be answered directly through text alone. GPT-4o achieves 75.71% on the original testing set. After our modifications, the performance is 60.02%. The detailed distribution of the questions of this part of the benchmark is shown in Figure 2 (d). For EMER, we refine the emotion vocabulary within these videos and organize all descriptive vocabulary into hierarchical categories. For the open vocabulary emotion options of the person in the videos, in addition to the original emotion ground-truth labels, we randomly select emotion description terms from other groups to create multiple-choice question with multiple answers. We randomly select 133 videos and their corresponding questions as the testing set. We assess the performance of this part of data using the F1-score. For MDPE, we reformat it to QA format and select samples where the interviewees feel uncertain about successfully deceivinga total of 60 clips (i.e., the confidence rating is above 3). We also include 20 clips where they feel confident in their deception (i.e., the confidence rating is lower than 3, more challenging) and 120 no-deception clips, totaling 200 videos to form the deception data. Finally, our IntentBench comprises 2,689 questions and 633 videos. Figure 2 shows some examples of IntentBench. Each question requires an analysis of both visual and audio cues from the videos. In the appendix, we also provide more detailed comparison with the original dataset."
        },
        {
            "title": "4.1 Preliminary",
            "content": "Group Relative Policy Optimization (GRPO) [10] streamlines the reinforcement learning approach by eliminating the critic model. This is achieved by generating multiple responses for each sample and then calculating normalized reward within the group to determine the advantage value. We follow GRPO with standard practice, but introduce two key modifications based on recent work [39, 40]. First, we use the token-level loss to overcome the imbalance problem for long sample training. Second, we remove the question-level normalization term, which may result in varying weights in the objective for different questions, leading to difficulty bias in optimization. Besides, we apply the dynamic KL to avoid restricting exploration in the initial stage and diverging in the late stage, encouraging better exploration and improving training stability. With these changes, the GRPO objective is updated as follows: qP(q),{oi}G i=1πθold (q) 1 i=1 oi (cid:80)G (cid:88) oi (cid:88) (cid:16) min i=1 t=1 ri,t(θ) ˆAi,t, clip(ri,t(θ), 1 ε, 1 + ε) ˆAi,t (cid:17) ˆβDKL (πθπref) (θ) = where ri,t(θ) = πθ(oi,t q, oi,<t) πθold(oi,t q, oi,<t) , ˆAi,t = Ri mean({Ri}G i=1), DKL [πθπref ] = πref (oi q) πθ (oi q) log πref (oi q) πθ (oi q) 1. (1) (2) (3) where both ε and ˆβ are hyper-parameters. ε controls the clipping bound and limits the range of policy updates to avoid large changes that could destabilize training. ˆβ is the KL penalty coefficient 5 Figure 3: The reasoning path of our model on an example from Social-IQ 2.0. The model first clearly understands the context information of the video clip in the multi-person talking scenario; then it starts reasoning with the multimodal clues to precisely answer the question. that regularizes deviation from reference policy πref. In this work, we dynamically reduce the KL penalty in the first iterations. ˆβ = (cid:26)β1 + β2 (β2 β1) if 0 if > S. (4) Using large constraint β1 in the initial stage keeps the model within close range of the baseline model, ensuring stable training. Conversely, using small constraint β2 encourages more in-depth thinking and the generation of long, detailed reasoning, but may also lead to reward hacking."
        },
        {
            "title": "4.2 Shortcuts and Context in Multimodal Reasoning",
            "content": "When training with vanilla GRPO, one key issue is the lack of global context understanding, leading to inaccurate identification of key evidence and context during reasoning, as shown in Figure 1. Another problem is that the model tends to overlook multimodal inputs, relying more on textual patterns from questions to generate answers. However, vision and audio cues are vital in multimodal interactions, and the correct answer often depends on subtle multimodal information. To address this challenge, we propose two modifications. First, we require the model to explicitly interpret or summarize the context information from multimodal inputs before it starts reasoning, avoiding the shortcut problem. Second, to ensure more reliable reasoning based on multimodal context, we require the model to integrate multimodal information throughout the reasoning process. We encourage reasoning capabilities such as reflection, reviewing multimodal inputs, and logical thinking. Response format. We train the model to first generate its understanding of the multimodal context before proceeding with reasoning. Specifically, we prompt the model to generate detailed description, which is wrapped using <context>...</context> tag, capturing the context information from multimodal inputs. In the <think>...</think> tag, the model starts logically reasoning and is encouraged to reflect during reasoning, reviewing the multimodal content, as shown in Figure 3. The model puts the final answer to the question in <answer>...</answer> tag. The final format we request the model to follow is: <context>...</context> <think>...</think> <answer>...</answer> 6 Figure 4: Illustration of our method. We use Qwen2.5-Omni-Thinker[1] as our base model. For each training sample, we generate 8 completions and compute format and accuracy rewards with verifiable labels. Additionally, we assess reasoning-logical and context rewards by using LLM as the judge, applying these rewards only to corresponding seen tokens for different rewards. The output is evaluated using binary format reward rf {0, 1}, which verifies whether the generated response follows the context-think-answer format. The system prompt is specified in the appendix. To improve the context understanding and the reasoning process, we design specific rewards for both the context understanding and reasoning phases."
        },
        {
            "title": "4.3 Context Reward and Reasoning Logical Reward with Casual Mask",
            "content": "While the format enforces structure, it does not guarantee that the context is sufficiently detailed and precise to support reasoning. To address this issue, we introduce specialized context reward rc {0, 1} based on utilizing LLMs to evaluate the summary of context by offering clear scoring guidelines and prompts. Specifically, we feed the generated context into an LLM and ask it to answer the question based on the generated context and reference context for each labeled sample. Furthermore, to assess the logical reward rl {0, 1} of the reasoning process, we prompt the LLM to evaluate whether the reasoning incorporates multimodal information, reflection, confirmation, and logical deduction. Since these two rewards only evaluate certain intermediate tokens within the reasoning path, the context reward and logical reward are masked and applied solely to the corresponding seen tokens, as shown in Figure 4. To ensure balanced training, the context reward and logical reward are individually normalized. For accuracy rewards, we utilize LLM to obtain similarity scores for open-ended question answers, rather than relying on metrics such as BLEU [41] or ROUGE [42] scores. For multiple-choice questions with single answer, we use accuracy as the reward. For multiple-choice questions with multiple answers, we utilize the F1-score as the reward. For OCR and ASR tasks, the reward is calculated as 1 WER (Word Error Rate). In evaluating numerical answers, we check for equality between two numbers."
        },
        {
            "title": "4.4 Training Recipe",
            "content": "Cold start training. We select Qwen2.5-Omni-7B-thinker as our base model and modify the models system prompt to ensure it follows our specified output format. During the initial stage, we use long CoT (Chain-of-Thought) data, incorporating video and image reasoning data for cold start training, to stabilize the model in the reasoning phase. RL Training Stage 1. After completing the cold start training, we sample 8 times for each sample and retain those with an accuracy within (0, 0.75). At this stage, all RL training samples are annotated with context to facilitate rewarding relevant content during RL training. Thus, the reward would be more dense. For reasoning rewards, we use the context generated in each completion instead of the 7 Methods Table 1: Comparison with other methods on Daily-Omni [14]. Context Understanding AV Event Alignment Inference Reasoning Event Sequence Comparative LLM Size 30s Subset 60s Subset Avg Proprietary MLLMs Gemini 2.0 Flash Gemini 2.0 Flash Lite - - 62.18 55.04 73.28 64. 63.73 58.03 63.72 54.25 76.62 74.03 75.43 72.00 62.29 60.57 56.57 67.84 53.01 61. Open-Source Video-Audio MLLMs Unified-IO-2 [45] VideoLLaMA2 [46] Qwen2.5-Omni [1] Qwen2.5-Omni [1] Ola [3] MiniCPM-o [3] Ours 8B 7B 3B 7B 7B 7B 7B 25.63 35.71 38.66 44.12 40.33 40.33 46.63 31.30 35.88 48.09 51.15 60.30 61.06 67.93 26.42 35.75 33.68 38.86 39.89 49.22 51. 25.82 31.70 33.99 40.52 44.11 48.36 51.63 35.06 40.91 54.55 57.79 61.03 68.83 72.72 29.71 34.29 44.00 61.71 66.28 61.14 74.28 26.74 38.02 46.68 42.35 50.85 54.09 63.06 30.00 28.24 31.82 35.17 48.36 40.52 38.36 47.45 48.72 49.87 52.00 53.13 53.09 58.47 ground-truth context. During this phase, we employ LLM (e.g., Qwen2.5-72B) as the reward model. For detailed prompt information, please refer to the supplementary material. RL Training Stage 2. After completing the first stage of RL training, we aim to enhance the models general capabilities by utilizing general RL training data, including the data sourced from Video-R1[43] and OmniInstruct [24]. For these datasets, we also sample 8 times for each sample and retain those with an accuracy within the range of (0, 0.75). Since the previous phase already guides the model to learn more precise context and complex reasoning processes, in this phase, we only use the format reward and accuracy reward."
        },
        {
            "title": "5.1 Experimental Settings",
            "content": "Training data, details, and baselines. To obtain high-quality CoT data, we sample part of the data from Video-R1, the training set of Social-IQ2.0, and the remaining 200 entries from EMER. Then, we use Gemini-2.5-pro [44] to rewrite the reasoning processes, and for Social-IQ2.0, we carry out manual reviews to ensure the reasoning accuracy. For the sampled data from Video-R1, we also use Gemini-2.5-pro to rewrite the reasoning path. Finally, we collect 24K high-quality video-audio training data for cold-start training and RL training. In the cold start training stage, the learning rate is set to 1e5 and the batch size is 128. During the RL stages, the clip range for training is 0.2. The step for KL constraints is half of the total steps, with β1 and β2 set at 0.04 and 0.01, respectively. The learning rate is 1e6, and the maximum output length is 2048. To validate the effectiveness of our proposed method, we compare our full method against the model trained only with SFT using CoT data, our method without extra rewards, and the vanilla GRPO [10]. Evaluation and benchmarks. In testing, the video frame sampling rate is 2 FPS, with maximum of 32 frames. We evaluate our method on IntentBench, Daily-Omni [14], and WorldSense [15]. Most questions in these benchmarks require simultaneous understanding of both video and audio, making them well-suited for assessing omni-modal models. Daily-Omni [14] is an audio-visual questioning and answering benchmark that includes 684 videos capturing everyday life scenarios from various sources. These videos are rich in both audio and visual information, and the benchmark features 1,197 multiple-choice QA pairs spread across 6 major tasks. WorldSense [15] features strong coupling of audio and video, requiring models to effectively leverage the synergistic perception capabilities of omni-modal data. It includes 1,662 audio-visual synchronized videos, sorted into 8 main domains. Additionally, there are 3,172 multiple-choice QA pairs across 26 different tasks to enable thorough evaluation."
        },
        {
            "title": "5.2 Main Results",
            "content": "We compare our method with open-source omni-modal methods and proprietary MLLMs on DailyOmni and WorldSense, as shown in Table 1 and Table 2, and IntentBench in Table 3. The experimental results indicate that our model outperforms most open-source omni-modal models. Specifically, in out-of-domain testing, our method achieves the best results among open-source models. Particularly, 8 Methods Table 2: Comparison with other methods on WorldSense [15]. Daily Life Culture & Politics Tech & Science Film & TV Performance LLM Size Games Sports Music Claude 3.5 Sonnet [47] GPT 4o [48] Gemini 1.5 Pro [49] Unified-IO-2 XXL [45] VideoLLaMA2 [46] VITA-1.5 [4] Qwen2.5-Omni [1] Ours - - - 7B 7B 7B 7B 7B Proprietary MLLMs 31.7 44.0 47.2 30.6 38.3 50. 36.5 43.5 50.4 Open-Source Video-Audio MLLMs 31.7 25.4 35.9 49.8 51.7 23.9 21.8 34.3 43.6 47.6 23.7 24.5 39.8 43.8 44.8 43.7 48.0 53. 27.1 29.4 38.2 47.8 50.2 30.7 41.9 52.4 25.5 26.2 41.2 48.3 47.3 31.9 41.2 46.8 23.7 24.6 32.6 39.1 44.3 36.6 42.6 40. 25.7 25.5 34.7 43.5 45.2 33.9 42.7 42.0 27.3 27.1 39.9 47.3 44.2 Avg 34.8 42.6 48.0 25.9 25.4 36.9 45.4 47. Table 3: Comparison with other methods on our IntentBench. Since these models do not support audio input, we use the transcript of the audio instead. ER represents the context reward and logical reward. Methods LMM Why How What When Who/Which Other Social Emotion Deception Avg GPT-4o [38] GPT-o1(think) [50] Gemini-2.5-Pro (think) [44] MiniCPM-o [17] VITA-1.5 [4] Ola [3] Qwen2.5-Omni [1] Ours (full method) - SFT (CoT) - Ours w/o ER - Ours w/o context and ER - - - 8B 7B 7B 7B 7B 7B 7B 7B Proprietary MLLMs 61.46 68.19 68.57 55.69 65.82 67.41 60.00 66.04 65.12 35.71 57.14 57.14 76.00 76.00 64.00 Open-Source Video-Audio MLLMs 57.87 53.15 60.60 62.60 66.76 58.30 67.47 64.47 53.48 49.36 55.37 63.44 67.08 60.28 65.98 64.08 57.14 51.66 56.87 63.53 71.25 62.91 68.54 66. 57.14 71.42 64.28 57.14 Ours 50.00 64.28 50.00 50.00 68.00 64.00 76.00 76.00 84.00 76.00 76.00 80.00 63.31 68.83 70. 61.14 61.14 62.91 69.03 72.39 65.28 71.20 71.40 60.99 67.26 68.23 23.85 53.20 46.66 59.74 82.41 79.00 84.00 81.37 59.00 59.50 60. 49.50 59.50 44.50 63.50 64.00 57.00 62.00 61.50 59.98 66.69 67.15 54.51 54.17 57.41 64.20 69.33 62.03 68.44 66.72 it boosts performance from 61.71 to 74.28 in the \"reasoning\" task within Daily-Omni, demonstrating comparable performance to Gemini-2.0-Flash-Lite. We also observe that our method performs worse than Qwen2.5-Omni in the Performance and Music tasks of WorldSense. These two tasks are primarily related to visual perception or auditory perception. In in-domain testing, our method achieves the best results compared to other methods and baselines, as shown in Table 3. Our method effectively combines global context information with fine-grained audio-video clues during reasoning, leading to higher accuracy in complex social situations and understanding human intent. As shown in Figure 5, we present an example from our IntentBench. Within the <context> tag, our model summarizes the global context information, including who, where, and what is happening in the video. This global context information helps mitigate the shortcut problem in multimodal reasoning and provides basis for subsequent reasoning. Consequently, the model avoids focusing solely on specific details, ensuring that it does not overlook the broader context during reasoning."
        },
        {
            "title": "5.3 Ablation Study and Analyses",
            "content": "The effect of cold start. As shown in Table 3, the performance of the cold start long CoT training (i.e., SFT(CoT) row in Table 3) slightly decreases compared to Qwen2.5-Omni. We think the primary reason is that the CoT training data is largely smaller than Qwen2.5-Omni (1,200 billion tokens in total). Additionally, Qwen2.5-Omni already performs comparably well on original Social-IQ 2.0 and MDPE, possibly because the training sets of these datasets are already included in the training of the original Qwen2.5-Omni. The effect of context reward and logical reward. The context reward incentivizes the model to enhance context quality, while the logical reward promotes complex reasoning and integrates 9 Figure 5: Visualization result of our method on IntentBench. multimodal inputs into the reasoning process. Consequently, as shown in Table 3, the performance without these additional rewards is inferior to our full method. The effect of context. In Table 3, while our model without context and extra rewards performs better than Qwen2.5-Omni, its performance drops to 66.72 compared to our full method. This result highlights the importance of the global understanding of multimodal inputs. The context can effectively offer comprehensive understanding for the subsequent reasoning process."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we highlight two issues in multimodal reasoning: insufficient global context understanding and the tendency to overlook multimodal inputs. To address these challenges, we introduce HumanOmniV2, an omni-modal reasoning model that requires summarizing the global context of inputs. By basing reasoning on this comprehensive understanding, it reduces problems like overlooking essential multimodal information and enhances context comprehension of multimodal inputs. We also introduce an omni-modal reasoning benchmark that focuses on understanding human intentions and emotions in complex social interactions. Our proposed method surpasses other open-source omni-modal methods and vanilla GRPO across various benchmarks, demonstrating the effectiveness of summarizing the context of multimodal inputs before reasoning."
        },
        {
            "title": "7 Limitations",
            "content": "Our method reduces the probability of shortcuts and context errors during reasoning, but it does not completely solve the issues. This response format might also restrict the ability of the model to correct context information during the following thinking process. Moreover, our experiments are primarily based on 7B model, so consistent conclusions may not be guaranteed at larger scales. Additionally, shorter response length also restricts the models ability to reflect thoroughly on multimodal information. In future research, along with expanding context and pretraining scale, we aim to explore methods for multiple verification of multimodal information during the reasoning process to enhance accuracy."
        },
        {
            "title": "8.1 More visualizations of our method",
            "content": "From Figure 6 to 9, we present visualizations of the evaluation results for our full method and the compared method on IntentBench. Our model can effectively comprehend global context information and capture fine-grained video-audio clues from multimodal inputs during the reasoning process. Figure 6: Visualization result of our method and the compared method on IntentBench. 11 Figure 7: Visualization result of our method and the compared method on IntentBench. 12 Figure 8: Visualization result of our method and the compared method on IntentBench. 13 Figure 9: Visualization result of our method and the compared method on IntentBench."
        },
        {
            "title": "8.2 The prompts used in our method",
            "content": "As shown in Figure 10, we carefully design the system prompt to guide our model in thoroughly understanding the global context while encouraging self-reflection and verification, ensuring that the model focuses on and adheres to our specified output format. Figure 11 and Figure 12 are the prompts for assessing logical reward and context reward, respectively. We carefully design the user prompt to guide the LLM in precisely assessing the quality of the context summary and determining whether the reasoning incorporates multimodal information, reflection, confirmation, and logical deduction. You are helpful assistant. Your primary goal is to deeply analyze and interpret information from various available modalities (image, video, audio, text context) to answer questions with human-like depth and clear, traceable thought process. Begin by thoroughly understanding the image, video, audio, or other available context information, and then proceed with an in-depth analysis related to the question. In reasoning, it is encouraged to incorporate self-reflection and verification into your reasoning process. You are encouraged to review the image, video, audio, or other context information to ensure the answer accuracy. Provide your understanding of the image, video, and audio between the <context> </context> tags, detail the reasoning between the <think> </think> tags, and then give your final answer between the <answer> </answer> tags. Figure 10: The system prompt of our model. Please analyze whether the reasoning text is derived from the evidence and context text based on the following criteria and give score of 0-5: Grading criteria description (relevance and rationality): Integration of Clues (1 point): During the reasoning process, there is incorporation of clues from the video, image, or audio. Reflection and Confirmation (1 point): The reasoning involves reflection or second confirmation of choices or answers, including revisiting video, image, or audio evidence. Logical Reasoning (1 point): The thought process is clear, deriving conclusions through rigorous logical reasoning, analysis, or extension without additional assumptions or contradictions. Problem Analysis (1 point): The reasoning process includes thorough analysis in conjunction with the problem at hand. Overall Consistency (1 point): The reasoning text is based on visual or audio evidence and context information, presenting no extra assumptions or contradictions. Assign one point for each criterion that is met, for total possible score of five points. Verify that each criterion is addressed and reflect this in your scoring. context: reference reasoning path: hypothesis only return the score number: Figure 11: The prompt for assessing the logical reward. 15 You are assessing how well the hypothesis text covers the key information from the reference text. Differences in wording or extra details in the hypothesis are fine if the references main points are included: Score based on this coverage: 5 points : Hypothesis clearly and accurately reflects significant core themes or key aspects of the reference. It demonstrates good understanding of substantial part of the reference material. 4 points : Hypothesis reflects some important themes or aspects of the reference. The connection is evident, though perhaps not as comprehensive or central as 5. 2 points : Hypothesis shows recognizable connection to themes or aspects of the reference, but it might be more superficial, focus on less central points, or only partially grasp key aspect. 1 points : Hypothesis has tenuous or very limited connection to the reference. It might touch on peripheral detail or heavily reinterpreted aspect, but largely misses the main substance. 0 points : Hypothesis does not reflect any significant themes or key aspects of the reference, or is on completely different topic. Example analysis process: Identify main themes and key aspects in reference. Determine if hypothesis connects to or discusses any of these themes/aspects from reference. Judge the strength and relevance of this connection. reflected? Differences are expected; evaluate if the hypothesis still meaningfully reflects some key part of the reference. Is core part of the reference Assign score based on how well significant aspect is reflected. reference: reference hypothesis: hypothesis only return the score number: Figure 12: The prompt for assessing the context reward."
        },
        {
            "title": "8.3 The Differences between IntentBench and Its Original Datasets.",
            "content": "In Social-IQ 2.0 [35], which is developed based on Social IQ 1.0[34], some questions in the dataset are straightforward and can be answered directly using only the text. Additionally, we also notice that some models may have been trained on this dataset. Thus, we use GPT-4o [38] with only text modal for testing to identify challenging questions. We also replace easy options to increase the difficulty of the testing set. Finally, we conduct manual verification to ensure each question is relevant to multimodal information and cannot be answered directly through text alone. GPT-4o achieves 75.71% on the original testing set. After our modifications, the performance is 60.02%. Through these enhancements, the dataset becomes more challenging compared to its original version. Figure 2 shows some comparisons between the original options and our refined version. The original EMER [36] is not designed for open vocabulary evaluation. In the AffectGPT [51], the authors propose grouping the predicted results and ground truth emotions for each sample using large language model (LLM) and then calculating the F1 score. However, the need to frequently utilize LLMs for evaluation can lead to instability, causing significant challenges in both training and testing. Thus, we refine the emotion vocabulary within these videos and organize all descriptive vocabulary into hierarchical categories. For the open vocabulary emotion options of the person in the videos, in addition to the original emotion ground-truth labels, we randomly select emotion description terms from other groups to create multiple-choice questions with multiple answers. Figure 14 shows some comparisons between the original answer and our refined version. The MDPE dataset [37] is initially crafted for classification tasks. To streamline the evaluation of multimodal models, we reformat them into question-answer format and incorporate the original 16 Figure 13: Comparison of the original options in Social-IQ 2.0 and our refined options. Figure 14: Comparison of the original open-vocabulary answer and our designed multi-choice question with multiple answers in EMER. questions in each video as part of the query. Additionally, we refine the sample selection process by randomly selecting samples according to specific ratios based on difficulty levels. We select samples where the interviewees feel uncertain about successfully deceivinga total of 60 clips (i.e., the confidence rating is above 3). We also include 20 clips where they feel confident in their deception (i.e., the confidence rating is lower than 3, more challenging) and 120 no-deception clips, totaling 200 videos to form the deception data."
        },
        {
            "title": "References",
            "content": "[1] J. Xu, Z. Guo, J. He, H. Hu, T. He, S. Bai, K. Chen, J. Wang, Y. Fan, K. Dang et al., Qwen2. 5-omni technical report, arXiv preprint arXiv:2503.20215, 2025. [2] Y. Li, H. Sun, M. Lin, T. Li, G. Dong, T. Zhang, B. Ding, W. Song, Z. Cheng, Y. Huo et al., Ocean-omni: To understand the world with omni-modality, arXiv preprint arXiv:2410.08565, 2024. [3] Z. Liu, Y. Dong, J. Wang, Z. Liu, W. Hu, J. Lu, and Y. Rao, Ola: Pushing the frontiers of omni-modal language model with progressive modality alignment, arXiv preprint arXiv:2502.04328, 2025. [4] C. Fu, H. Lin, X. Wang, Y.-F. Zhang, Y. Shen, X. Liu, H. Cao, Z. Long, H. Gao, K. Li et al., Vita-1.5: Towards gpt-4o level real-time vision and speech interaction, arXiv preprint arXiv:2501.01957, 2025. [5] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi et al., Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, arXiv preprint arXiv:2501.12948, 2025. [6] F. Meng, L. Du, Z. Liu, Z. Zhou, Q. Lu, D. Fu, T. Han, B. Shi, W. Wang, J. He et al., Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning, arXiv preprint arXiv:2503.07365, 2025. [7] K. Feng, K. Gong, B. Li, Z. Guo, Y. Wang, T. Peng, B. Wang, and X. Yue, Video-r1: Reinforcing video reasoning in mllms, arXiv preprint arXiv:2503.21776, 2025. [8] Z. Liu, Z. Sun, Y. Zang, X. Dong, Y. Cao, H. Duan, D. Lin, and J. Wang, Visual-rft: Visual reinforcement fine-tuning, arXiv preprint arXiv:2503.01785, 2025. [9] H. Shen, P. Liu, J. Li, C. Fang, Y. Ma, J. Liao, Q. Shen, Z. Zhang, K. Zhao, Q. Zhang et al., Vlm-r1: stable and generalizable r1-style large vision-language model, arXiv preprint arXiv:2504.07615, 2025. [10] Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu et al., Deepseekmath: Pushing the limits of mathematical reasoning in open language models, arXiv preprint arXiv:2402.03300, 2024. [11] H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe, Lets verify step by step, in The Twelfth International Conference on Learning Representations, 2023. [12] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei, Deep reinforcement learning from human preferences, Advances in neural information processing systems, vol. 30, 2017. [13] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray et al., Training language models to follow instructions with human feedback, Advances in neural information processing systems, vol. 35, pp. 27 73027 744, 2022. [14] Z. Zhou, R. Wang, and Z. Wu, Daily-omni: Towards audio-visual reasoning with temporal alignment across modalities, arXiv preprint arXiv:2505.17862, 2025. [15] J. Hong, S. Yan, J. Cai, X. Jiang, Y. Hu, and W. Xie, Worldsense: Evaluating real-world omnimodal understanding for multimodal llms, arXiv preprint arXiv:2502.04326, 2025. [16] J. Zhao, Q. Yang, Y. Peng, D. Bai, S. Yao, B. Sun, X. Chen, S. Fu, X. Wei, L. Bo et al., Humanomni: large vision-speech language model for human-centric video understanding, arXiv preprint arXiv:2501.15111, 2025. [17] Y. Yao, T. Yu, A. Zhang, C. Wang, J. Cui, H. Zhu, T. Cai, H. Li, W. Zhao, Z. He et al., Minicpm-v: gpt-4v level mllm on your phone, arXiv preprint arXiv:2408.01800, 2024. [18] P. Zhang, X. Dong, Y. Cao, Y. Zang, R. Qian, X. Wei, L. Chen, Y. Li, J. Niu, S. Ding et al., Internlmxcomposer2. 5-omnilive: comprehensive multimodal system for long-term streaming video and audio interactions, arXiv preprint arXiv:2412.09596, 2024. [19] S. Fu, Q. Yang, Y.-M. Li, Y.-X. Peng, K.-Y. Lin, X. Wei, J.-F. Hu, X. Xie, and W.-S. Zheng, Vispeak: Visual instruction feedback in streaming videos, arXiv preprint arXiv:2503.12769, 2025. [20] K. Hu, P. Wu, F. Pu, W. Xiao, Y. Zhang, X. Yue, B. Li, and Z. Liu, Video-mmmu: Evaluating knowledge acquisition from multi-discipline professional videos, arXiv preprint arXiv:2501.13826, 2025. [21] C. Fu, Y. Dai, Y. Luo, L. Li, S. Ren, R. Zhang, Z. Wang, C. Zhou, Y. Shen, M. Zhang et al., Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 24 10824 118. [22] Y.-X. Peng, Q. Yang, Y.-M. Tang, S. Fu, K.-Y. Lin, X. Wei, and W.-S. Zheng, Actionart: Advancing multimodal large models for fine-grained human-centric video understanding, arXiv preprint arXiv:2504.18152, 2025. 18 [23] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun et al., Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 95569567. [24] Y. Li, G. Zhang, Y. Ma, R. Yuan, K. Zhu, H. Guo, Y. Liang, J. Liu, Z. Wang, J. Yang et al., Omnibench: Towards the future of universal omni-language models, arXiv preprint arXiv:2409.15272, 2024. [25] W. Huang, B. Jia, Z. Zhai, S. Cao, Z. Ye, F. Zhao, Z. Xu, Y. Hu, and S. Lin, Vision-r1: Incentivizing reasoning capability in multimodal large language models, arXiv preprint arXiv:2503.06749, 2025. [26] J. Zhang, J. Huang, H. Yao, S. Liu, X. Zhang, S. Lu, and D. Tao, R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization, arXiv preprint arXiv:2503.12937, 2025. [27] Y. Dong, Z. Liu, H.-L. Sun, J. Yang, W. Hu, Y. Rao, and Z. Liu, Insight-v: Exploring long-chain visual reasoning with multimodal large language models, arXiv preprint arXiv:2411.14432, 2024. [28] J. Xia, Y. Zang, P. Gao, Y. Li, and K. Zhou, Visionary-r1: Mitigating shortcuts in visual reasoning with reinforcement learning, arXiv preprint arXiv:2505.14677, 2025. [29] Z. Guo, M. Hong, and T. Jin, Observe-r1: Unlocking reasoning abilities of mllms with dynamic progressive reinforcement learning, arXiv preprint arXiv:2505.12432, 2025. [30] J. Zhao, X. Wei, and L. Bo, R1-omni: Explainable omni-multimodal emotion recognition with reinforcement learning, arXiv preprint arXiv:2503.05379, 2025. [31] Z. Xing, X. Hu, C.-W. Fu, W. Wang, J. Dai, and P.-A. Heng, Echoink-r1: Exploring audio-visual reasoning in multimodal llms via reinforcement learning, arXiv preprint arXiv:2505.04623, 2025. [32] P. Lu, H. Bansal, T. Xia, J. Liu, C. Li, H. Hajishirzi, H. Cheng, K.-W. Chang, M. Galley, and J. Gao, Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts, arXiv preprint arXiv:2310.02255, 2023. [33] Y. Zhao, L. Xie, H. Zhang, G. Gan, Y. Long, Z. Hu, T. Hu, W. Chen, C. Li, J. Song et al., Mmvu: Measuring expert-level multi-discipline video understanding, arXiv preprint arXiv:2501.12380, 2025. [34] A. Zadeh, M. Chan, P. P. Liang, E. Tong, and L.-P. Morency, Social-iq: question answering benchmark for artificial social intelligence, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 88078817. [35] A. Wilf, L. Mathur, S. Mathew, C. Ko, Y. Kebe, P. P. Liang, and L.-P. Morency, Social-iq 2.0 challenge: Benchmarking multimodal social understanding, https://github.com/abwilf/Social-IQ-2.0-Challenge, 2023. [36] Z. Lian, H. Sun, L. Sun, H. Gu, Z. Wen, S. Zhang, S. Chen, M. Xu, K. Xu, K. Chen et al., Explainable multimodal emotion recognition, arXiv preprint arXiv:2306.15401, 2023. [37] C. Cai, S. Liang, X. Liu, K. Zhu, Z. Wen, J. Tao, H. Xie, J. Cui, Y. Ma, Z. Cheng et al., Mdpe: multimodal deception dataset with personality and emotional characteristics, arXiv preprint arXiv:2407.12274, 2024. [38] A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark, A. Ostrow, A. Welihinda, A. Hayes, A. Radford et al., Gpt-4o system card, arXiv preprint arXiv:2410.21276, 2024. [39] Q. Yu, Z. Zhang, R. Zhu, Y. Yuan, X. Zuo, Y. Yue, T. Fan, G. Liu, L. Liu, X. Liu et al., Dapo: An open-source llm reinforcement learning system at scale, arXiv preprint arXiv:2503.14476, 2025. [40] Z. Liu, C. Chen, W. Li, P. Qi, T. Pang, C. Du, W. S. Lee, and M. Lin, Understanding r1-zero-like training: critical perspective, arXiv preprint arXiv:2503.20783, 2025. [41] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, Bleu: method for automatic evaluation of machine translation, in Proceedings of the 40th annual meeting of the Association for Computational Linguistics, 2002, pp. 311318. [42] C.-Y. Lin, Rouge: package for automatic evaluation of summaries, in Text summarization branches out, 2004, pp. 7481. [43] K. Feng, K. Gong, B. Li, Z. Guo, Y. Wang, T. Peng, B. Wang, and X. Yue, Video-r1: Reinforcing video reasoning in mllms, arXiv preprint arXiv:2503.21776, 2025. [44] Google, Gemini 2.5 pro, https://deepmind.google/technologies/gemini/pro/, 2025. [45] J. Lu, C. Clark, S. Lee, Z. Zhang, S. Khosla, R. Marten, D. Hoiem, and A. Kembhavi, Unified-io 2: Scaling autoregressive multimodal models with vision language audio and action, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 26 43926 455. [46] Z. Cheng, S. Leng, H. Zhang, Y. Xin, X. Li, G. Chen, Y. Zhu, W. Zhang, Z. Luo, D. Zhao et al., Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms, arXiv preprint arXiv:2406.07476, 2024. 19 [47] Anthropic, Introducing the next generation of Claude, https://www.anthropic.com/news/claude-3-family, 2024, accessed: 2024-10-22. [48] A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark, A. Ostrow, A. Welihinda, A. Hayes, A. Radford et al., Gpt-4o system card, arXiv preprint arXiv:2410.21276, 2024. [49] G. Team, P. Georgiev, V. I. Lei, R. Burnell, L. Bai, A. Gulati, G. Tanzer, D. Vincent, Z. Pan, S. Wang et al., Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, arXiv preprint arXiv:2403.05530, 2024. [50] A. Jaech, A. Kalai, A. Lerer, A. Richardson, A. El-Kishky, A. Low, A. Helyar, A. Madry, A. Beutel, A. Carney et al., Openai o1 system card, arXiv preprint arXiv:2412.16720, 2024. [51] Z. Lian, H. Sun, L. Sun, J. Yi, B. Liu, and J. Tao, Affectgpt: Dataset and framework for explainable multimodal emotion recognition, arXiv preprint arXiv:2407.07653, 2024."
        }
    ],
    "affiliations": [
        "Tongyi Lab, Alibaba Group"
    ]
}