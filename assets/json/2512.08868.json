{
    "paper_title": "EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce",
    "authors": [
        "Rui Min",
        "Zile Qiao",
        "Ze Xu",
        "Jiawen Zhai",
        "Wenyu Gao",
        "Xuanzhong Chen",
        "Haozhen Sun",
        "Zhen Zhang",
        "Xinyu Wang",
        "Hong Zhou",
        "Wenbiao Yin",
        "Xuan Zhou",
        "Yong Jiang",
        "Haicheng Liu",
        "Liang Ding",
        "Ling Zou",
        "Yi R.",
        "Fung",
        "Yalong Li",
        "Pengjun Xie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Foundation agents have rapidly advanced in their ability to reason and interact with real environments, making the evaluation of their core capabilities increasingly important. While many benchmarks have been developed to assess agent performance, most concentrate on academic settings or artificially designed scenarios while overlooking the challenges that arise in real applications. To address this issue, we focus on a highly practical real-world setting, the e-commerce domain, which involves a large volume of diverse user interactions, dynamic market conditions, and tasks directly tied to real decision-making processes. To this end, we introduce EcomBench, a holistic E-commerce Benchmark designed to evaluate agent performance in realistic e-commerce environments. EcomBench is built from genuine user demands embedded in leading global e-commerce ecosystems and is carefully curated and annotated through human experts to ensure clarity, accuracy, and domain relevance. It covers multiple task categories within e-commerce scenarios and defines three difficulty levels that evaluate agents on key capabilities such as deep information retrieval, multi-step reasoning, and cross-source knowledge integration. By grounding evaluation in real e-commerce contexts, EcomBench provides a rigorous and dynamic testbed for measuring the practical capabilities of agents in modern e-commerce."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 8 6 8 8 0 . 2 1 5 2 : r 2025-12EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce Rui Min, Zile Qiao*, Ze Xu, Jiawen Zhai, Wenyu Gao, Xuanzhong Chen, Haozhen Sun, Zhen Zhang, Xinyu Wang, Hong Zhou, Wenbiao Yin, Xuan Zhou, Yong Jiang, Haicheng Liu, Liang Ding, Ling Zou, Yi R. (May) Fung, Yalong Li, Pengjun Xie Tongyi Lab , Alibaba Group"
        },
        {
            "title": "Abstract",
            "content": "Foundation agents have rapidly advanced in their ability to reason and interact with real environments, making the evaluation of their core capabilities increasingly important. While many benchmarks have been developed to assess agent performance, most concentrate on academic settings or artificially designed scenarios while overlooking the challenges that arise in real applications. To address this issue, we focus on highly practical real-world setting, the e-commerce domain, which involves large volume of diverse user interactions, dynamic market conditions, and tasks directly tied to real decision-making processes. To this end, we introduce EcomBench, holistic E-commerce Benchmark designed to evaluate agent performance in realistic e-commerce environments. EcomBench is built from genuine user demands embedded in leading global e-commerce ecosystems and is carefully curated and annotated through human experts to ensure clarity, accuracy, and domain relevance. It covers multiple task categories within e-commerce scenarios and defines three difficulty levels that evaluate agents on key capabilities such as deep information retrieval, multi-step reasoning, and cross-source knowledge integration. By grounding evaluation in real e-commerce contexts, EcomBench provides rigorous and dynamic testbed for measuring the practical capabilities of agents in modern e-commerce."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) are rapidly advancing from passive knowledge retrievers to autonomous agents (Team et al., 2025; Qiu et al., 2025; Kimi, 2025; Zeng et al., 2025a; Li et al., 2025c) capable of reasoning, planning, and interacting with real-world environments. At its core, assessing the foundational capabilities of these advanced LLM agents has become essential, where wide range of benchmarks have been proposed (Mialon et al., 2023; Wei et al., 2025; Zhou et al., 2025; Xbench Team, 2025; Yao et al., 2022; Wong et al., 2025; Yao et al., 2024). These benchmarks play fundamental role in measuring the progress of agent development and advancing research on general agentic capabilities. Despite the rapid growth of agent evaluation research, most existing studies focus on agent evaluation through academic puzzle-style benchmarks, leaving real-world tasks largely unexplored. E-commerce, as one of the most complex and economically significant domains, involves tasks defined by constantly changing market trends and rich domain-specific knowledge. It poses unique challenges that require agents to integrate analytical reasoning, practical expertise, and adaptive tool usage. Therefore, evaluating Equal contribution. Corresponding author. 1 Figure 1: Comparison of model performance on the EcomBench. agents in this environment is crucial for understanding whether current models can effectively deliver relevant information and generate effective solutions within real-world e-commerce scenarios. To rigorously assess agent capabilities in realistic e-commerce settings, we present EcomBench, benchmark built from authentic e-commerce tasks. Unlike benchmarks that focus on environment-interaction settings (Zhang et al., 2025; Yao et al., 2022), EcomBench centers on composite question-answering tasks that reflect authentic users daily issues in e-commerce. The benchmark assesses not only an agents domain expertise but also its ability to use tools for precise information retrieval, and its capacity to capture authentic user intent and draw conclusions that genuinely benefit users. To construct EcomBench that faithfully reflects real user needs, we adopt human-in-the-loop framework. We first collect large volume of real-world user demands from leading global e-commerce ecosystems. From these collected data, we extract genuine user inquiries and refine them into well-defined tasks that require accurate and verifiable answers. This process yields our seed questions, which are subsequently reviewed and refined by our e-commerce experts to ensure cognitive depth, structural clarity, and answer verifiability. To comprehensively evaluate agents across different levels of task difficulty, we divide the benchmark into three difficulty levels and construct high-difficulty questions using Tool Hierarchy approach. Specifically, we employ an LLM with tools to identify hard questions: instead of relying solely on simple tools such as web search and browsing, we equip the judge model with more advanced, e-commerce-specific tools to identify questions that require multi-step, complex reasoning and cannot be solved in just few action steps. These challenging samples demand deeper reasoning and long-horizon planning, thereby enabling rigorous assessment of agents tool use, reasoning, and e-commerce domain expertise. Empirically, we evaluate variety of existing models on EcomBench (shown in Figure 1) and find that their performance demonstrates considerable potential for improvement in solving practical e-commerce tasks. We will regularly maintain and expand EcomBench to provide increasingly complex and realistic challenges that align with real-world e-commerce scenarios. In sum, our benchmark design follows four core principles: Authenticity: EcomBench is built on large-scale genuine user demands extracted from leading global e-commerce ecosystems. These data form the basis of EcomBench, ensuring that each question reflects real e-commerce scenarios and accurately captures users actual needs. Professionalism: All questions are first written and refined by experienced e-commerce experts, who incorporate real user needs and domain knowledge into the question design. They are then subjected to peer validation, which ensures the clarity and overall quality of the final benchmark. Comprehensiveness: Our benchmark covers wide range of e-commerce tasks, such as policy consulting, cost analysis, and marketing strategy. EcomBench also supports multiple question formats, combining multiple-choice and open-ended questions, with each question carefully calibrated for its difficulty level. Dynamism: As e-commerce continues to evolve rapidly, the benchmark is regularly updated to keep the tasks aligned with real-world market trends while reducing the potential risks of data contamination. The rest of this paper is organized as follows. Section 2 illustrates Authenticity by describing the curation process of EcomBench supported by our human-in-the-loop data engine. Section 3 provides detailed examination of the benchmark, presenting representative examples to demonstrate its quality under Professionalism and its fine-grained task categorization under Comprehensiveness. To demonstrate the utility of the benchmark, Section 4 evaluates existing models on EcomBench. Finally, Section 5 describes the dynamic update mechanism that preserves the Dynamism of EcomBench in the rapidly evolving e-commerce domain."
        },
        {
            "title": "2 Human-in-the-loop Data Curation",
            "content": "The following section describes how high-quality questions are curated in EcomBench using our humanin-the-loop data engine. In Section 2.1, we first introduce the process of seed question collection and refinement from human experts. We then describe our question selection strategy for controlling task difficulty in Section 2.2."
        },
        {
            "title": "2.1 Data Collection from Real-World User Demands",
            "content": "E-commerce has become deeply embedded in everyday life, with millions of users generating diverse demands such as searching for policy information, estimating costs, selecting products, and making business decisions. These frequent and diverse activities provide rich source of real-world data that captures genuine user intentions and operational needs. Building on this foundation, EcomBench leverages these authentic interactions as the basis for constructing tasks that represent real-world ecommerce scenarios. To ensure that the data reflects prevailing market trends, we derive it from real user demands embedded in leading global e-commerce ecosystems such as Amazon, while maintaining timeliness. With the large volume of collected user demands, we transform these raw contexts into seed questions with verifiable answers. To achieve this, we prompt an LLM to examine each collected user demand and filter out samples that lack concrete answers, such as requests involving subjective evaluation criteria. These seed questions, however, are not yet ready for direct labeling and evaluation, as some may contain vague instructions that yield no ground-truth answers or are overly simple for answer retrieval. To construct valid questions for benchmarking, we engage e-commerce experts to refine them using domainspecific knowledge, ensuring that each question is well-informed by domain expertise and precisely formulated. After refinement, we conduct peer validation for answer verification, where each question is independently labeled by at least three experts. As common practice, we discard questions with inconsistent answers among experts to ensure reliability. Avoiding Purely Synthetic Questions. After obtaining the seed questions, we primarily rely on human effort for question reconstruction and labeling, rather than using LLMs directly for question synthesis. Although this process introduces higher costs, the resulting questions more faithfully capture genuine human demands. Since e-commerce fundamentally revolves around human participation, the questions 3 in our dataset not only reflect real-world e-commerce behaviors but also incorporate human insight and expertise of the domain."
        },
        {
            "title": "2.2 Question Selection with Tool Hierarchy",
            "content": "To comprehensively evaluate agent capabilities across varying task difficulties, EcomBench supports three difficulty levels. To identify high-difficulty tasks, we adopt tool-hierarchy-based question selection approach. Specifically, we equip the LLM with specialized e-commerce tools such as product price retrieval and trend analysis, and then apply rejection sampling to retain questions that require complex reasoning chains and cannot be solved in just few action steps. We designate these as level-3 (high-difficulty) tasks in our dataset. Our rationale is that, for achieving the same objective, higher-level e-commerce tools can accomplish tasks in fewer steps than atomic tools such as web search or browsing, which typically require multiple actions. Consequently, these questions substantially increase the challenge for agents lacking advanced tools or e-commerce expertise, as solving them demands extensive action sequences, long-horizon reasoning, and adaptive tool usage. This filtering mechanism based on tool hierarchy provides scalable strategy for constructing high-difficulty questions. 3 In-depth Analysis of EcomBench Table 1: Task categories and descriptions in EcomBench."
        },
        {
            "title": "Inventory Control",
            "content": "Tasks involving platform rules, qualification submissions, and tax registration processes, commonly seen in queries about compliance and policy-related demands in daily operations. Tasks related to checking order profit, preparing quotes, and adjusting prices under different market or customer conditions, often raised when users assess profitability. Tasks covering shipping arrangements, handling returns and exchanges, and improving basic logistics routes, frequently asked about in day-to-day fulfillment issues. Tasks involving planning promotions, setting up ads, and finding ways to reach users, typically appearing in queries about boosting traffic or visibility. Tasks using trend signals and simple data insights to identify product categories with good potential, reflected in many questions about choosing the right products to sell. Tasks looking at data to spot early signs of new opportunities, often asked when users explore new directions for growth. Tasks involving safety-stock planning, restocking decisions, and clearance actions, commonly seen in questions about balancing stock availability and overstock risks."
        },
        {
            "title": "3.1 Fine-Grained Task Category",
            "content": "To ensure comprehensive evaluation across diverse user demands, EcomBench encompasses broad spectrum of task types, primarily covering seven categories commonly observed in real-world e4 commerce scenarios: Policy Consulting, Cost and Pricing, Fulfillment Execution, Marketing Strategy, Intelligent Product Selection, Opportunity Discovery, and Inventory Control. We provide detailed description in Table 1."
        },
        {
            "title": "3.2 Case Study on Difficulty Levels",
            "content": "In addition to the diversity of e-commerce task types, our benchmark includes tasks of different difficulty levels, which are designated as follows: We manually annotate and categorize each task into three levels. Level 1 (20%) includes relatively simple cases that evaluate an agents foundational expertise in e-commerce and its ability to perform basic tool operations. Level 2 (30%) contains moderately complex tasks that require the agent to decompose problems and reason across multiple action steps to reach solution. Level 3 (50%) consists of the most challenging tasks in our dataset, which are both manually verified and constructed through the tool-hierarchy process. These tasks involve cross-source knowledge integration, deep information retrieval, and long-horizon reasoning and planning capabilities. For clarity, we sample questions from the Policy Consulting and Cost and Pricing categories across three difficulty levels and present these examples in Figure 2. These examples illustrate that our benchmark is not limited to simple information retrieval but instead evaluates agents across multiple dimensions, requiring them to gather information from diverse sources, interpret domain-specific regulations, and make coherent decisions within realistic scenarios."
        },
        {
            "title": "4.1 Evaluation Settings",
            "content": "Backbones To comprehensively assess existing model performance on EcomBench, we evaluate diverse set of leading models, including Doubao DeepResearch (Doubao, 2025), Quark Agent, DeepSeekChat (DeepSeek Team, 2025), ChatGPT-5.1 (Team, 2025f), Gemini DeepResearch (Gemini Team, 2025), MiniMax Agent (Team, 2025e), SuperGrok Expert (xAI, 2025), Flowith Agent (Team, 2025b), Skywork General (Team, 2025g), Manus Agent (Team, 2025d), GenSpark Agent (Team, 2025c), and Coze Space Agent (Team, 2025a). Scoring Methodology While each question in EcomBench has uniquely verifiable ground-truth answer, model outputs may contain semantically equivalent responses expressed in different forms. To ensure fair and precise evaluation, we prompt an LLM to compare each response with its corresponding ground-truth answer. For each question, the judge assigns binary score of 1 if the response is correct and 0 otherwise, and we report the average correctness over all questions for each model. We also manually inspect subset of the evaluations to verify the consistency of the automatic scoring."
        },
        {
            "title": "4.2 Evaluation across Difficulty Levels",
            "content": "To validate our difficulty stratification, we evaluate the top ten models across three levels. As shown in Figure 3, we observe progressive performance decline as difficulty increases, validating the effectiveness of our design. Specifically, models exhibit high proficiency in Level 1, with scores consistently ranging between 80% and 95%. However, the performance declines noticeably in Level 2 and drops sharply in Level 3. Even the leading models, ChatGPT-5.1 and Gemini DeepResearch, which achieve over 90% accuracy in Level 1, see their scores fall to 46% in Level 3. For the remaining assessed models, the scores of Level 3 generally remain below 35%. This substantial gap confirms that while current flagship models can reliably handle basic tasks in daily e-commerce scenarios, high-difficulty requests involving complex constraints remain significant challenge. 5 Figure 2: Examples of questions with different difficulty levels from EcomBench. Level 1 (Policy Consulting): Our company has developed new laptop power adapter with nameplate output power of 48 watts, and we plan to sell it nationwide in the United States in 2025. U.S. energy regulations, what is the maximum no-load power consumption allowed for this adapter? Please round the result to two decimal places. To meet Answer: 0.1 Level 1 (Cost and Pricing): Given that the Spanish home decor e-commerce market is projected to have compound annual growth rate of 6.8% from 2025 to 2028, and that Mediterranean-style decor is expected to grow 2.2 percentage points faster than the overall home decor category, what is the cumulative growth rate of Mediterranean-style decor over these three years? Round the result to two decimal places. Answer: 29.50% Level 2 (Policy Consulting): According to Canadian toy safety regulations, batch of 10,000 plastic toys intended for children under three years old is inspected using an AQL 1.0 sampling standard. If 2% of the toys actually exceed the lead limit of 90 mg/kg, what is the probability (in percent) that the batch will be incorrectly accepted? Round to the nearest whole number. Answer: 79% Level 2 (Cost and Pricing): UK company sells customized product bundle to consumers in Germany, consisting of: electronics priced at 200 (standard VAT rate), physical books priced at 50 (reduced VAT rate), and digital course priced at 100 (subject to the digital services VAT rate). Given: exchange rate 1 = 1.18; German VAT rate for digital services is 9%; EU import rules state that if the total value of the goods exceeds 150, customs duties apply (5% for electronics, 0% for books); customs duties are calculated based on the value of the goods (digital services excluded); VAT is calculated based on the value of goods plus customs duties plus the value of digital services; and customized electronics require 15% configuration fee (applies only to the electronics). Calculate the total amount (in euros) the German consumer must pay, and round the result to two decimal places. Answer: 530.86 euros Level 3 (Policy Consulting): For laptop power adapter with nameplate output power of 48 watts that complies with the U.S. Department of Energy (DOE) Level VI efficiency standard, what is the minimum allowable input power, in watts, when the adapter delivers its full 48-watt output? Please round your result to four decimal places. Answer: 55.1724 Level 3 (Cost and Pricing): An intelligent doorbell operates in the 5.8 GHz band (5820 MHz) under the EU Radio Equipment Directive (RED). Its transmitter output power is 500 mW, using 4 dBi antenna, with cable loss of 1.5 dB. Calculate the equivalent isotropically radiated power (EIRP) in dBm. If the device uses an 800 kHz modulation bandwidth, determine the minimum required out-of-band emission attenuation according to EN 300 328. whether this configuration complies with Article 3(2) of the RED for short-range devices. Output the EIRP and attenuation as integers, and the compliance result as Yes or No. Round all results to the nearest integer. Finally, verify Answer: 30, 50, No 6 Figure 3: Performance comparison across different difficulty levels in EcomBench."
        },
        {
            "title": "4.3 Evaluation across Task Categories",
            "content": "To further investigate the performance of distinct models across various task categories, we organize the task categories into three domains for clearer presentation: Policy-Related (comprising Policy Consulting and Fulfillment Execution), Finance-Related (comprising Cost and Pricing and Inventory Control), and Strategy-Related (comprising Opportunity Discovery, Intelligent Product Selection, and Marketing Strategy). Our objective is to examine how models perform in specialized e-commerce contexts and identify potential variations in their domain expertise. As illustrated in Figure 4, which presents the top six models for each category, we observe clear performance disparities. Each model demonstrates distinct strengths; for instance, SuperGrok excels in Finance-Related tasks but falls short in Strategy-Related scenarios. Additionally, while ChatGPT-5.1 leads the overall leaderboard, it is surpassed by SuperGrok and Gemini DeepResearch in the Finance-Related and Strategy-Related categories, respectively, indicating that different models exhibit domain-specific strengths. This poses challenge for building more general e-commerce agent. Figure 4: Performance comparison across different task categories in EcomBench."
        },
        {
            "title": "5 Dynamic Maintenance and Updating of EcomBench",
            "content": "Quarterly Update Cycle EcomBench is updated on quarterly basis to ensure that it remains timely and coherent with real e-commerce scenarios. Each update serves two main purposes. First, as foundation agents continue to improve, many existing questions may no longer pose meaningful challenges. These overly simple items are replaced with new tasks that require more complex reasoning and more adaptive tool usage, thereby keeping the overall difficulty aligned with the progress of contemporary models. Second, the e-commerce domain continues to evolve, such as policy revisions, market fluctuations, and emerging product trends, causing some existing questions to become outdated. To address this, each quarterly release introduces updated questions that reflect the latest developments while removing questions that no longer match current practice. Through this continuous renewal, EcomBench remains 7 both challenging for advancing agents and consistent with real-world e-commerce dynamics. Ever-expanding E-commerce Tasks The initial release of EcomBench includes questions with verifiable and concise answers that cover common e-commerce scenarios. We plan to expand the benchmark with additional task types that capture wider range of real-world e-commerce challenges, such as market analysis and forecasting. This expansion aims to move beyond simple fact-based questions toward more analytical, decision-oriented, and predictive tasks. As the benchmark evolves, it will better capture the complexity of e-commerce practice and offer more discriminative evaluations of agent capabilities."
        },
        {
            "title": "6 Related Work",
            "content": "DeepResearch Agent for Complex Information Seeking The development of deep research agents stems from foundational advances in retrieval augmentation, agentic frameworks, and training methodologies. The effort to overcome the static knowledge limitations of LLMs began with Retrieval-Augmented Generation (RAG) (Lewis et al., 2020), which integrates external information. Early single-step methods evolved into more sophisticated, multi-step workflows featuring iterative retrieval (Shao et al., 2023), query planning (Ma et al., 2023), and self-critique mechanisms (Asai et al., 2024). However, these RAG pipelines often lack the dynamic, autonomous decision-making characteristic of true agentic systems. paradigm shift toward autonomy was marked by agentic frameworks like ReAct (Yao et al., 2023), which synergizes reasoning (thought) and acting (tool use) in an interleaved manner. This model forms the conceptual backbone for modern deep research agents, including WebThinker (Li et al., 2025c), WebDancer (Wu et al., 2025a), and Tongyi DeepResearch (Qiao et al., 2025; Team et al., 2025), which extend the agentic paradigm to handle long-horizon, complex information-seeking tasks through specialized tools and structured reasoning processes. The scarcity of high-quality training data for such complex tasks has driven focus on automated synthesis. While initial information-driven methods generated questions from pre-crawled web content (Wu et al., 2025a), they could suffer from structural inconsistencies. To address this, more structured, formalization-driven paradigms have emerged. These include using set-theoretic constructs to ensure consistency (Tao et al., 2025) or employing tool-augmented complexity escalation to systematically generate verifiable, super-human level datasets (Li et al., 2025b;a; Qiao et al., 2025; Team et al., 2025). Evaluation of Foundation Agents The evaluation of LLM agents has rapidly evolved from assessing static knowledge retrieval, established by RAG frameworks (Lewis et al., 2020) and early benchmarks like HotpotQA (Yang et al., 2018), to measuring dynamic, real-world interaction. pivotal shift was marked by GAIA (Mialon et al., 2023), which introduced multi-step, tool-dependent tasks with verifiable factual answers, establishing robust paradigm for evaluating generalist assistants. As model capabilities advanced, subsequent benchmarks have diverged to probe the frontiers of agent intelligence. One direction tests profound, \"Google-proof\" expert reasoning, exemplified by GPQA (Rein et al., 2024) and Humanitys Last Exam (Phan et al., 2025). Another direction examines procedural reliability in complex and noisy environments. This includes testing an agents ability for persistent traversal of entangled information (Wei et al., 2025), deep vertical exploration of websites (Wu et al., 2025b), reasoning under conflicting search results (Pham et al., 2025), and navigating the unique challenges of non-English ecosystems Zhou et al. (2025). More recently, the focus has expanded beyond technical performance to measure economic impact, trend pioneered by xbench (Xbench Team, 2025). Recently, the research community has shifted its focus toward evaluating agents in more realistic scenarios. For instance, FutureX (Zeng et al., 2025b) investigates agents ability to perform future prediction tasks; FinSearchComp (Hu et al., 2025) assesses their capabilities in financial information seeking, and StockBench Chen et al. (2025) evaluates agents within realistic stock trading environments. Along this line, ECom-Bench (Wang et al., 2025) evaluates multimodal agents through persona-based user simulations, while our EcomBench focuses on more general e-commerce expertise, covering broader 8 range of domains and decision-driven tasks grounded in practical e-commerce scenarios."
        },
        {
            "title": "7 Conclusion and Limitations",
            "content": "In this paper, we present EcomBench, holistic benchmark for evaluating agent capabilities in realistic and dynamic e-commerce scenarios. We focus on the e-commerce domain, which represents complex and rapidly evolving real-world environment characterized by diverse user interactions and high-stakes decision-making. EcomBench is built on four core principles: authenticity, derived from genuine user demands in global e-commerce ecosystems; professionalism, ensured through human-in-the-loop curation process involving domain experts; comprehensiveness, encompassing multiple task categories and three distinct difficulty levels; and dynamism, maintained through quarterly update cycle that reflects market trends and mitigates data contamination. These principles make EcomBench rigorous and realistic testbed for evaluating the practical capabilities of foundation agents. Limitations. EcomBench currently focuses on question-answering tasks and does not explicitly evaluate agents in environments with interactions. In addition, many real-world e-commerce problems are inherently predictive, such as product selection and market trend forecasting, which we will incorporate in future releases. As e-commerce is highly dynamic and human-centric domain, maintaining the overall quality of the benchmark requires substantial human effort. Consequently, long-term maintenance depends on regular problem design and verification, which inevitably increases the cost of dataset construction. Despite these limitations, EcomBench offers valuable testbed, and we view it as living benchmark that we will continuously refine and expand to enable more comprehensive evaluation in real e-commerce scenarios."
        },
        {
            "title": "References",
            "content": "Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve, generate, and critique through self-reflection. 2024. Yanxu Chen, Zijun Yao, Yantao Liu, Jin Ye, Jianing Yu, Lei Hou, and Juanzi Li. Stockbench: Can llm agents trade stocks profitably in real-world markets? arXiv preprint arXiv:2510.02209, 2025. DeepSeek Team. Introducing deepseek-v3.1: our first step toward the agent era!, 2025. URL https: //api-docs.deepseek.com/news/news250821. ByteDance Doubao. Doubao, 2025. URL http://www.doubao.com/. Gemini Team. Gemini deep research, 2025. URL https://gemini.google/overview/deep-research/. Liang Hu, Jianpeng Jiao, Jiashuo Liu, Yanle Ren, Zhoufutu Wen, Kaiyuan Zhang, Xuanliang Zhang, Xiang Gao, Tianci He, Fei Hu, et al. Finsearchcomp: Towards realistic, expert-level evaluation of financial search and reasoning. arXiv preprint arXiv:2509.13160, 2025. Kimi. Kimi-researcher: End-to-end rl training for emerging agentic, 2025. URL https://moonshotai.g ithub.io/Kimi-Researcher/. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:94599474, 2020. Kuan Li, Zhongwang Zhang, Huifeng Yin, Rui Ye, Yida Zhao, Liwen Zhang, Litu Ou, Dingchu Zhang, Xixi Wu, Jialong Wu, et al. Websailor-v2: Bridging the chasm to proprietary agents via synthetic data and scalable reinforcement learning. arXiv preprint arXiv:2509.13305, 2025a. Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, et al. Websailor: Navigating super-human reasoning for web agent. arXiv preprint arXiv:2507.02592, 2025b. Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng Dou. Webthinker: Empowering large reasoning models with deep research capability. CoRR, abs/2504.21776, 2025c. doi: 10.48550/ARXIV.2504.21776. URL https://doi.org/10.48550/a rXiv.2504.21776. Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. Query rewriting in retrievalaugmented large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 53035315, 2023. Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations, 2023. Thinh Pham, Nguyen Nguyen, Pratibha Zunjare, Weiyuan Chen, Yu-Min Tseng, and Tu Vu. Sealqa: Raising the bar for reasoning in search-augmented language models. arXiv preprint arXiv:2506.01062, 2025. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, et al. Humanitys last exam. arXiv preprint arXiv:2501.14249, 2025. Zile Qiao, Guoxin Chen, Xuanzhong Chen, Donglei Yu, Wenbiao Yin, Xinyu Wang, Zhen Zhang, Baixuan Li, Huifeng Yin, Kuan Li, et al. Webresearcher: Unleashing unbounded reasoning capability in long-horizon agents. arXiv preprint arXiv:2509.13309, 2025. 10 Jiahao Qiu, Xuan Qi, Tongcheng Zhang, Xinzhe Juan, Jiacheng Guo, Yifu Lu, Yimin Wang, Zixin Yao, Qihan Ren, Xun Jiang, et al. Alita: Generalist agent enabling scalable agentic reasoning with minimal predefinition and maximal self-evolution. arXiv preprint arXiv:2505.20286, 2025. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy. arXiv preprint arXiv:2305.15294, 2023. Zhengwei Tao, Jialong Wu, Wenbiao Yin, Junkai Zhang, Baixuan Li, Haiyang Shen, Kuan Li, Liwen Zhang, Xinyu Wang, Yong Jiang, et al. Webshaper: Agentically data synthesizing via information-seeking formalization. arXiv preprint arXiv:2507.15061, 2025. Coze Space Team. Coze agent, 2025a. URL https://www.coze.com. Flowith Team. Flowith agent, 2025b. URL https://flowith.io. Genspark Team. Genspark agent, 2025c. URL https://www.genspark.ai. Manus Team. Manus agent, 2025d. URL https://manus.im. Minimax Team. Minimax agent, 2025e. URL https://agent.minimaxi.com. OpenAI Team. Gpt-5, 2025f. URL https://chatgpt.com. SkyWork AI Team. Skywork ai, 2025g. URL https://skywork.ai/. Tongyi DeepResearch Team, Baixuan Li, Bo Zhang, Dingchu Zhang, Fei Huang, Guangyu Li, Guoxin Chen, Huifeng Yin, Jialong Wu, Jingren Zhou, et al. Tongyi deepresearch technical report. arXiv preprint arXiv:2510.24701, 2025. Haoxin Wang, Xianhan Peng, Huang Cheng, Yizhe Huang, Ming Gong, Chenghan Yang, Yang Liu, and Jiang Lin. Ecom-bench: Can llm agent resolve real-world e-commerce customer support issues? In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: Industry Track, pp. 276284, 2025. Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516, 2025. Ryan Wong, Jiawei Wang, Junjie Zhao, Li Chen, Yan Gao, Long Zhang, Xuan Zhou, Zuo Wang, Kai Xiang, Ge Zhang, et al. Widesearch: Benchmarking agentic broad info-seeking. arXiv preprint arXiv:2508.07999, 2025. Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Yong Jiang, Pengjun Xie, et al. Webdancer: Towards autonomous information seeking agency. arXiv preprint arXiv:2505.22648, 2025a. Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu Zhou, Pengjun Xie, et al. Webwalker: Benchmarking llms in web traversal. arXiv preprint arXiv:2501.07572, 2025b. xAI. Grok-4, 2025. URL https://x.ai/news/grok-4. Xbench Team. Xbench-deepsearch, 2025. URL https://xbench.org/agi/aisearch. 11 Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018. Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35: 2074420757, 2022. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. τ-bench: benchmark for toolagent-user interaction in real-world domains. arXiv preprint arXiv:2406.12045, 2024. Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, et al. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. arXiv preprint arXiv:2508.06471, 2025a. Zhiyuan Zeng, Jiashuo Liu, Siyuan Chen, Tianci He, Yali Liao, Yixiao Tian, Jinpeng Wang, Zaiyuan Wang, Yang Yang, Lingyue Yin, et al. Futurex: An advanced live benchmark for llm agents in future prediction. arXiv preprint arXiv:2508.11987, 2025b. Xianren Zhang, Shreyas Prasad, Di Wang, Qiuhai Zeng, Suhang Wang, Wenbo Yan, and Mat Hans. functionality-grounded benchmark for evaluating web agents in e-commerce domains. arXiv preprint arXiv:2508.15832, 2025. Peilin Zhou, Bruce Leon, Xiang Ying, Can Zhang, Yifan Shao, Qichen Ye, Dading Chong, Zhiling Jin, Chenxuan Xie, Meng Cao, et al. Browsecomp-zh: Benchmarking web browsing ability of large language models in chinese. arXiv preprint arXiv:2504.19314, 2025."
        }
    ],
    "affiliations": [
        "Tongyi Lab, Alibaba Group"
    ]
}