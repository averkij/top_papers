{
    "paper_title": "Yor-Sarc: A gold-standard dataset for sarcasm detection in a low-resource African language",
    "authors": [
        "Toheeb Aduramomi Jimoh",
        "Tabea De Wille",
        "Nikola S. Nikolov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Sarcasm detection poses a fundamental challenge in computational semantics, requiring models to resolve disparities between literal and intended meaning. The challenge is amplified in low-resource languages where annotated datasets are scarce or nonexistent. We present \\textbf{Yor-Sarc}, the first gold-standard dataset for sarcasm detection in Yorùbá, a tonal Niger-Congo language spoken by over $50$ million people. The dataset comprises 436 instances annotated by three native speakers from diverse dialectal backgrounds using an annotation protocol specifically designed for Yorùbá sarcasm by taking culture into account. This protocol incorporates context-sensitive interpretation and community-informed guidelines and is accompanied by a comprehensive analysis of inter-annotator agreement to support replication in other African languages. Substantial to almost perfect agreement was achieved (Fleiss' $κ= 0.7660$; pairwise Cohen's $κ= 0.6732$--$0.8743$), with $83.3\\%$ unanimous consensus. One annotator pair achieved almost perfect agreement ($κ= 0.8743$; $93.8\\%$ raw agreement), exceeding a number of reported benchmarks for English sarcasm research works. The remaining $16.7\\%$ majority-agreement cases are preserved as soft labels for uncertainty-aware modelling. Yor-Sarc\\footnote{https://github.com/toheebadura/yor-sarc} is expected to facilitate research on semantic interpretation and culturally informed NLP for low-resource African languages."
        },
        {
            "title": "Start",
            "content": "Yor-Sarc: gold-standard dataset for sarcasm detection in low-resource African language Toheeb A. Jimoh 1, Tabea De Wille 1, and Nikola S. Nikolov 1 1Department of Computer Science and Information Systems, University of Limerick, Castletroy, V94 T9PX, Limerick, Ireland Abstract Sarcasm detection poses fundamental challenge in computational semantics, requiring models to resolve disparities between literal and intended meaning. The challenge is amplified in lowresource languages where annotated datasets are scarce or nonexistent. We present Yor-Sarc, the first gold-standard dataset for sarcasm detection in Yor`uba, tonal Niger-Congo language spoken by over 50 million people. The dataset comprises 436 instances annotated by three native speakers from diverse dialectal backgrounds using an annotation protocol specifically designed for Yor`uba sarcasm by taking culture into account. This protocol incorporates context-sensitive interpretation and community-informed guidelines and is accompanied by comprehensive analysis of inter-annotator agreement to support replication in other African languages. Substantial to almost perfect agreement was achieved (Fleiss κ = 0.7660; pairwise Cohens κ = 0.67320.8743), with 83.3% unanimous consensus. One annotator pair achieved almost perfect agreement (κ = 0.8743; 93.8% raw agreement), exceeding number of reported benchmarks for English sarcasm research works. The remaining 16.7% majority-agreement cases are preserved as soft labels for uncertaintyaware modelling. Yor-Sarc1 is expected to facilitate research on semantic interpretation and culturally informed NLP for low-resource African languages. Keywords: Natural Language Processing (NLP), Yor`uba, African languages, Sarcasm detection, Low-resource language, Data annotation 6 2 0 2 1 2 ] . [ 1 4 6 9 8 1 . 2 0 6 2 : r Corresponding author: toheeb.jimoh@ul.ie 1https://github.com/toheebadura/yor-sarc"
        },
        {
            "title": "1 Introduction",
            "content": "Sarcasm detection has become an important sub-task in natural language processing (NLP) as sarcasm frequently masks surface sentiment, thereby degrading the reliability of sentiment and opinion mining systems deployed on social media and other user-generated content. Automatically identifying sarcastic utterances is, however, difficult problem given that it mostly relies on subtle pragmatic cues, literary knowledge, and cultural context, and is often misconstrued with figurative language phenomena such as irony and metaphor. Contemporary studies have explored sarcasm detection for English (Misra and Arora, 2023; Bamman and Smith, 2015) and handful of other high-resource languages using supervised and neural approaches, including transformer-based models, demonstrating that robust performance typically depends on carefully annotated corpora and task-specific benchmarks. In contrast, low-resource languages remain significantly underexplored in sarcasm research. While there are emerging efforts for languages such as Urdu (Khan et al., 2024), and Arabic (Farha and Magdy, 2020), among others, the global landscape remains heavily skewed toward English and small number of non-African languages. These low-resource language studies consistently highlight primary challenge: the scarcity of high-quality, manually annotated sarcasm datasets (Jimoh et al., 2025). For African languages, the resource gap is even more keen. Despite recent progress in sentiment analysis and general text classification, as exemplified by the AfriSenti benchmark for Twitter sentiment in 14 African languages (Muhammad et al., 2023) and Naijasenti (Muhammad et al., 2022), which explores sentiment analysis among Nigerian languages, there are, to the best of current knowledge, no publicly documented gold-standard corpora dedicated to sarcasm detection for any major West African language, save Nigerian Pidgin (Ladoja and Afape, 2024). Yor`uba, tonal and morphologically rich language of the Niger-Congo family spoken by over 50 million people in Nigeria and the diaspora (Fagbolu et al., 2016), suffers this fate. recent study of NLP for Yor`uba identifies significant advances in tasks such as diacritic restoration, part-of-speech tagging, machine translation, and sentiment analysis, but explicitly notes that figurative language phenomenaincluding sarcasmremain virtually unexplored (Jimoh et al., 2025). This lack of resources not only limits the development of sarcasm-aware sentiment systems for Yor`uba speakers but also excludes Yor`uba pragmatics and satirical connotations from the broader scientific understanding of sarcasm across languages. This research addresses this gap by introducing the first, to the best of our knowledge, manually annotated sarcasm dataset for the Yor`uba language. The corpus currently consists of 436 short texts collected from diverse sources, including (formerly Twitter), Facebook, Instagram, BBC News Yor`uba, YouTube video captions, and crowdsourced examples obtained via an ehtically approved online survey, all written in Standardised Yor`uba (SY) orthography with tone and diacritic marks. Each instance is independently labelled as sarcastic or non-sarcastic by three native Yor`uba speakers, and gold target label is derived via majority voting while preserving individual annotator decisions. This three-annotator scheme is inspired by recent low-resource sarcasm and sentiment datasets that emphasise multi-annotator truth sets and the importance of modelling agreement for pragmatically complex phenomena. The contributions of this work are threefold. First, it presents the first publicly available goldstandard sarcasm corpus for Yor`uba, filling critical gap in African NLP resources and complementing existing text classification benchmarks such as AfriSenti (Muhammad et al., 2023). Second, it documents culturally grounded annotation protocol and three-annotator scheme tailored to Yor`uba sarcasm, including comprehensive inter-annotator agreement analysis such that it would inform similar efforts in other African languages. Ultimately, it provides corpus-level analysis of the dataset, examining class distribution and source characteristics, among others. The rest of this paper is structured as follows: Section 2 reviews related work on sarcasm detection and dataset curation for this task. Section 3 describes the Yor`uba sarcasm dataset, annotation framework, and inter-annotator agreement analysis. Section 4 presents the inter-annotator agreement analysis (IAA). Section 5 presents the conclusion, discusses limitations, and outlines future directions."
        },
        {
            "title": "2 Related works",
            "content": "Early work on sarcasm detection focused primarily on English and treated the task as sentencelevel text classification, using rule-based approaches involving lexical, syntactic, and sentimentbased incongruity features (Joshi et al., 2017; Riloff et al., 2013). Over time, the field has moved from rulebased and traditional machinelearning models, including k-Nearest Neighbor and logistic regression, among others, to deep learning and transformerbased approaches that can better capture semantic and pragmatic cues (Davidov et al., 2010; Misra and Arora, 2023). Recent studies have been incorporating large language models and agentic systems (Lee et al., 2025; Zhang et al., 2025), indicating progress in the sarcasm detection task in NLP. Simultaneously, publicly available datasets for monolingual sarcasm detection in high-resource languages and few low-resource languages are being released to facilitate continuous research in the domain. Datasets such as SARC (Reddit) (Khodak et al., 2018) and MUStARD (multimodal dialogue) (Castro et al., 2019) have enabled research on sarcasm in multiturn dialogue settings, showing the competence of advanced methods of sarcasm detection on these benchmarks. More recently, multimodal sarcasm detection has attracted attention (Kamau and Abbas, 2025), with surveys documenting how visual, involving images and emojis, and acoustic signals complement text in social media and speechbased sarcasm recognition (Gao et al., 2024). These lines of work provide resource and methodological foundations, albeit they are almost exclusively focused on highresource, predominantly Western languages. In African NLP, progress has largely been driven by efforts to build foundational resources for core tasks such as language identification (Asubiaro et al., 2018), partofspeech tagging (Dione et al., 2023), named entity recognition (Adelani et al., 2022), and sentiment analysis (Muhammad et al., 2022). The AfriSenti benchmark (Muhammad et al., 2023) is major contribution, providing over 110, 000 annotated tweets for sentiment analysis across 14 African languages, including Yor`uba. It further illustrates how carefully designed, native-annotated corpora can accelerate research on African languages; however, it focuses on coarse-grained sentiment labels rather than fine-grained figurative phenomena such as sarcasm."
        },
        {
            "title": "3 Dataset, Multi-Annotator Framework and Inter-Annotator Agree-",
            "content": "ment We collected 436 Yor`uba instances from six diverse sources (Figure 1). BBC News Yor`uba comprises 65.4%(n = 285), providing contextually-grounded examples with professional editing. Social media platforms contribute 124 instances (28.5%) of informal, spontaneous language use: Instagram (21.8%), X/Twitter (3.9%), Facebook (2.8%), and YouTube (2.3%). Crowdsourced contributions (3.9%) fill gaps in naturally occurring data, particularly face-to-face contexts. This multi-source approach balances formal and informal records."
        },
        {
            "title": "3.1 Annotation Process",
            "content": "We employed rigorous multi-annotator framework to ensure the highest quality gold standard for Yor`uba sarcasm detection. Three native Yor`uba speakers, all with linguistic expertise and fluency in the language, independently annotated 436 instances of Yor`uba text for sarcasm presence or otherwise. Annotators were provided with comprehensive annotation guidelines developed through an iterative pilot study involving 20 training examples with subsequent discussion and refinement."
        },
        {
            "title": "3.2 Annotator Protocol",
            "content": "Given that our annotators are native speakers of the language and understand diverse dialectal backgrounds, such as Standard Yor`uba, If`e., `Ij`e.bu, among others, it ensures our gold standard captures 3 Figure 1: Distribution of instances across data sources varied interpretations while maintaining consistency through clear guidelines. For effectiveness, each annotator independently labeled all instances without consultation or access to other annotators decisions. Binary labels (sarcastic or non-sarcastic) were assigned based on whether the statement conveyed meaning contrary to its literal interpretation."
        },
        {
            "title": "3.3 Inter-Annotator Agreement Metrics",
            "content": "To assess the reliability and quality of our annotations, we employ comprehensive suite of interannotator agreement metrics. These metrics quantify the extent to which our three independent annotators (A1, A2, and A3) concur in their sarcasm judgments across all = 436 instances. We adopt multi-metric approach to provide robust annotation quality, following best practices in computational linguistics (Artstein and Poesio, 2008). Our agreement analysis framework comprises three complementary measurement approaches."
        },
        {
            "title": "3.4 Pairwise Agreement (Cohen’s Kappa):",
            "content": "Cohens kappa coefficient (κ) is used to measure the agreement between each pair of annotators, correcting for chance agreement. For each annotator pair (i, j), Cohens kappa (McHugh, 2012) measures agreement while correcting for chance: κij = Po Pe 1 Pe (1) where Po is the observed agreement proportion and Pe is the expected agreement by chance. For the binary labels li,k {0, 1}: 4 Po = Pe = 1 (cid:88) k= (cid:88) c{0,1} 1[li,k = lj,k] pi,c pj,c, where pi,c = 1 (cid:88) k=1 1[li,k = c] (2) (3) We report all three pairwise values (κ12, κ13, κ23) and their average κ = 1 3 bootstrap confidence intervals (Efron and Tibshirani, 1985). (cid:80) i<j κij, with 95% 3.4.1 Multi-Rater Agreement (Fleiss Kappa) Also, to assess overall agreement across all three annotators simultaneously, we compute Fleiss kappa (Fleiss, 1971): κF = Pe 1 Pe (4) For each instance k, let nk,c denote the number of raters assigning category c. The per-instance agreement is: Pk = 1 r(r 1) (cid:88) c=1 [n2 k,c nk,c] = k,0 + n2 n2 6 k,1 3 (5) where = 3 is the number of raters and = 2 is the number of categories. The mean observed agreement is: Furthermore, the expected agreement is computed from marginal distributions as follows: ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) k=1 Pk Pe = (cid:88) c= p2 c, where pc ="
        },
        {
            "title": "1\nN · r",
            "content": "N (cid:88) k=1 nk,c (6) (7)"
        },
        {
            "title": "3.4.2 Agreement Pattern Analysis",
            "content": "To determine the quantification of unanimous, majority, and disagreement patterns, including soft label derivation, we categorize each instance by agreement level: Unanimous (3 0): All annotators agree (l1,k = l2,k = l3,k) Majority (2 1): Two annotators agree, one dissents In addition, we report unanimity rate 1 sarcastic and non-sarcastic subsets. (cid:80)N k= 1[Unanimousk] and compute class-stratified rates for 5 3.4.3 Soft Label Derivation To preserve uncertainty information, we derive soft labels as the proportion of sarcastic votes as follows: sk = 1 r (cid:88) i=1 li,k {0.0, 0.333, 0.667, 1.0} Moreover, consensus labels for training are obtained via majority vote as follows: ˆlk = 1 (cid:34) (cid:88) i=1 (cid:35) li,k r/"
        },
        {
            "title": "3.5 Annotator Bias and Disagreement Analysis",
            "content": "3.5.1 Annotator Bias Quantification We measure systematic bias as the deviation from consensus: Biasi ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) k=1 (li,k ˆlk) (8) (9) (10) where Biasi > 0 indicates liberal (more sarcastic) labelling and Biasi < 0 indicates conservative Labelling. Cross-annotator consistency is measured by standard deviation σbias."
        },
        {
            "title": "3.5.2 Instance-Level Uncertainty",
            "content": "Per-instance uncertainty is quantified using binary entropy as follows: H(k) = sk log2(sk) (1 sk) log2(1 sk) (11) where H(k) = 0 for unanimous cases and H(k) 0.92 for 2-1 splits. High-entropy instances (H(k) > 0.5) identify hard cases for evaluation."
        },
        {
            "title": "3.5.3 Confusion Matrix Analysis",
            "content": "For each annotator pair, we construct normalized confusion matrices to assess disagreement symmetry: ij[a, b] = nab nab (cid:80) , nab = (cid:88) k=1 1[li,k = a, lj,k = b] (12) For each annotator pair (Ai, Aj), we construct row-normalized confusion matrices ij, where each entry ij[a, b] represents the conditional probability that annotator Aj assigns label given that annotator Ai assigns label a. Balanced off-diagonals indicate symmetric disagreement; imbalanced off-diagonals suggest systematic bias."
        },
        {
            "title": "3.6 Interpretation threshold",
            "content": "We adopt standard interpretation frameworks for interpretation as follows: 6 Table 1: Interpretation of Cohens and Fleiss Kappa Kappa Range (κ) κ < 0.40 0.40 κ < 0.60 0.60 κ < 0.80 κ 0.80 Interpretation Poor to Fair Moderate Substantial Almost Perfect (Cohen) / Excellent (Fleiss)"
        },
        {
            "title": "4 Inter-Annotator Agreement Analysis",
            "content": "The reliability of semantic annotation is fundamental to dataset validity, particularly for pragmatic phenomena like sarcasm, where intended meaning differs from literal interpretation. Consequently, conduct comprehensive inter-annotator agreement (IAA) analysis using both traditional reliability metrics and distributional analysis to assess annotation quality and characterize disagreement patterns."
        },
        {
            "title": "4.1 Overall Agreement Metrics",
            "content": "We measure inter-annotator agreement using Cohens kappa (κ) (McHugh, 2012) for pairwise reliability and Fleiss kappa (Fleiss, 1971) for overall multi-rater consistency. Both metrics correct for chance agreement, accounting for class distribution imbalances. Our dataset achieves substantial overall agreement (Fleiss κ = 0.7660), with pairwise Cohens κ ranging from 0.6732 to 0.8743  (Table 2)  . According to the widely-adopted interpretation scale of (Landis and Koch, 1977), this places our overall agreement firmly in the substantial range (κ [0.61, 0.80]), and the best annotator pair exceeding the threshold for almost perfect agreement. Table 2: Pairwise inter-annotator agreement metrics Cohens κ Agreement % Interpretation"
        },
        {
            "title": "Pair",
            "content": "A1A2 A1A3 A2A3 0.8743 0.7539 0."
        },
        {
            "title": "Average",
            "content": "0.7671 93.81% 88.53% 84.17% 88.84%"
        },
        {
            "title": "Substantial",
            "content": "Notably, the A1A2 pair achieved almost perfect agreement (κ = 0.8743), with 93.81% raw agreement across all 436 instances. This represents the highest reported inter-annotator agreement for sarcasm annotation to our knowledge, substantially exceeding published benchmarks for English (κ = 0.560.62; Gonzalez-Ibanez et al., 2011; Riloff et al., 2013) and code-mixed data (κ = 0.75; Swami et al., 2018). Achievement of an almost perfect agreement for subjective semantic task in tonal language demonstrates that systematic annotation protocols using native speaker cultural expertise can overcome linguistic complexity. The average pairwise κ of 0.7671 (standard deviation σ = 0.104) indicates consistent annotation quality across all pair combinations. The relatively low variance suggests that the high agreement of the A1A2 pair is not an outlier but rather represents the upper bound of consistently strong performance."
        },
        {
            "title": "4.2 Agreement Distribution and Consensus Patterns",
            "content": "Beyond aggregate reliability metrics, we analyze the distribution of agreement levels across individual instances to understand consensus patterns  (Table 3)  . This instance-level analysis reveals where annotators achieve unanimous consensus versus where genuine interpretive variation occurs. 7 Table 3: Distribution of agreement levels across instances Agreement Level Count Percentage Description Unanimous (30) Majority (21) 363 83.26% 16.74% All annotators agree Two annotators agree substantial majority of instances (83.26%, = 363) received unanimous agreement from all three annotators, indicating that sarcasm markers are clear and interpretable in most cases. This high unanimity rate substantially exceeds typical rates for subjective annotation tasks (Artstein and Poesio, 2008) and suggests effective annotation guidelines that successfully establish the abstract concept of sarcasm for Yor`uba context. Moreover, with binary labels and three annotators, complete three-way disagreement is mathematically impossible; thus, no disagreement is reasonably recorded. The remaining 16.74% of instances (n = 73) exhibited 2 1 majority agreement patterns, where two annotators agreed while one dissented. Undoubtedly, shared cultural and linguistic knowledge enabled at least two annotators to reach consensus even on ambiguous cases, demonstrating the value of native speaker expertise for semantic annotation. Figure 2 visualizes the pairwise agreement structure as heatmap, revealing the systematic pattern where A1 shows strong agreement with both A2 (κ = 0.8743) and A3 (κ = 0.7539), while A2 and A3 exhibit somewhat lower but still substantial agreement (κ = 0.6732). Darker shading indicates stronger agreement. The A1A2 pair achieves almost perfect agreement (κ = 0.8743), while all other pairs demonstrate substantial agreement. Values represent Cohens κ with white boxes for enhanced visibility. Figure 2: Pairwise inter-annotator agreement heatmap"
        },
        {
            "title": "4.3 Soft Labels and Uncertainty Preservation",
            "content": "Following recent work on learning from disagreement (Uma et al., 2021; Plank, 2022), we preserve annotator uncertainty through soft labels rather than forcing artificial consensus. For each instance, from Equation 8, we compute the soft label as the mean of the three binary annotations: sk = 1 3 3 (cid:88) i=1 ℓi,k {0.0, 0.333, 0.667, 1.0} (13) where ℓi,k {0, 1} represents annotator is binary label for instance k. Figure 3 shows the resulting soft label distribution. The strongly bimodal distribution, with 83.9% of instances at the extremes (0.0 or 1.0), reflects the high unanimity rate. The remaining 16.1% of instances with intermediate values (0.333 or 0.667) represent cases where annotators disagreed, capturing genuine uncertainty in the semantic interpretation of sarcasm. Figure 3: Distribution of soft labels (left) and cumulative distribution (right). This soft label approach enables uncertainty-aware model training, where models can learn to assign lower confidence to instances where human annotators disagreed. Rather than treating disagreement as noise to be eliminated through adjudication, we preserve it as valuable signal about inherent semantic ambiguity (Plank, 2022)."
        },
        {
            "title": "4.4 Annotator Behavior and Pairwise Patterns",
            "content": "To understand the source of pairwise agreement variation, we analyze individual annotator behavior  (Table 4)  . The three annotators show moderate variation in sarcastic label assignment rates: A1 marked 41.06% of instances as sarcastic, A2 marked 45.87%, and A3 marked 30.96%a range of 14.91 percentage points. Table 4: Individual annotator label distribution and bias analysis Annotator Sarcastic Non-Sarcastic Sarcastic %"
        },
        {
            "title": "Bias",
            "content": "A1 A2 A3 179 200 135 257 236 301 9 41.06% 45.87% 30.96%"
        },
        {
            "title": "Conservative\nBalanced\nConservative",
            "content": "This variation reflects interpretive threshold differences rather than inconsistent application of annotation guidelines. Post-annotation debriefing revealed that all three annotators recognized the same sarcasm indicators, but differed in their inferential thresholds: A1 being Conservative with 41.06% sarcasm labels could indicate requirements for clear linguistic markers before assigning labels; A2 being Balanced with 45.87% could indicate weighting linguistic and contextual evidence approximately equally; A3 is Highly conservative with 30.96%, connoting the application of the strictest suggestive criterion. Importantly, these different perspectives explain the pairwise agreement patterns observed in Table 2. The high A1A2 agreement (κ = 0.8743) reflects their relatively similar thresholds (41.06% vs. 45.87%, difference = 4.81 percentage points). The lower A2A3 agreement (κ = 0.6732) corresponds to their larger threshold difference (45.87% vs. 30.96%, difference = 14.91 percentage points). Figure 4 presents confusion matrices for all three annotator pairs, revealing the error patterns underlying these agreement levels. Numbers indicate instance counts with percentages in parentheses. The A1A2 matrix shows highly symmetric errors (13 vs. 14), while A2A3 exhibits asymmetry (42 vs. 27) consistent with their threshold difference (A2 more liberal). Figure 4: Confusion matrices for all annotator pairs. The A1A2 confusion matrix reveals nearly symmetric disagreement patterns: when they disagree, A1 marks sarcastic while A2 does not in 13 cases, compared to 14 cases in the reverse direction. This symmetry indicates that their disagreements are essentially random rather than reflecting systematic bias. In contrast, the A2A3 matrix shows pronounced asymmetry: A2 marks instances as sarcastic that A3 does not in 42 cases, compared to only 27 cases in the reverse direction. This asymmetry directly reflects A2s more liberal threshold (15 percentage points higher sarcastic rate), resulting in more false positives from A3s perspective. Critically, this threshold variation represents genuine interpretive differences in semantic judgment rather than annotation errors. All three annotators applied the guidelines consistently; they simply differed in how much evidence they required to conclude that an utterance conveyed sarcastic rather than literal meaning."
        },
        {
            "title": "4.5 Benchmark Comparison and Quality Assessment",
            "content": "To contextualize our agreement metrics within the broader landscape of sarcasm annotation research, we compare Yor-Sarc with published benchmarks  (Table 5)  . Our average pairwise κ of 0.7671 substantially exceeds all prior English sarcasm annotation studies of which we are aware. Yor-Sarc achieves higher agreement than all prior work, with the best pair exceeding all benchmarks by substantial margins. 2Subset with high annotator confidence 10 Table 5: Comparison with published sarcasm annotation benchmarks Study Language κ vs. Ours Gonzalez-Ibanez et al. (2011) English English Riloff et al. (2013) English Bamman and Smith (2015) Hindi-English Swami et al. (2018) 0.560.62 0.67 0.812 0.75 900 +27%41% 3,000 9,128 5, +14% +8% +2% Yor-Sarc (average) Yor-Sarc (best pair) Yor`uba Yor`uba 0.7671 0.8743 436 436 Gonzalez-Ibanez et al. (2011) reported κ = 0.560.62 for English Twitter sarcasm annotation, which we exceed by 27% 41%. (Riloff et al., 2013) achieved κ = 0.67 for English sarcasm detection, which we surpass by 14%. Even (Swami et al., 2018), who reported κ = 0.75 for English-Hindi codemixed sarcasm, which is the previous best agreement, we exceed by 2% on average and 16% for our best pair. Only (Bamman and Smith, 2015) reports comparable agreement (κ = 0.81), but this value represents high-confidence subset rather than the full dataset. Our A1A2 pair achieves κ = 0.8743 across all instances without subsetting. This achievement is considered notable given that Yor`uba is tonal language, low-resource language, and our dataset includes naturally occurring examples from diverse sources rather than single domain."
        },
        {
            "title": "5 Conclusion and Future Directions",
            "content": "This study presents the initial efforts to accelerate the growth of sarcasm detection in low-resource African languages by introducing Yor-Sac, gold-standard manually annotated sarcasm detection dataset in Yor`uba language. This language is spoken by over 50 million people globally, which includes major parts of West Africa and notable southern American countries. Sarcasm annotation is particularly challenging, particularly because it requires pragmatic inference beyond literal meaning, alongside cultural and contextual background (Lee et al., 2025). Within this context, κ = 0.7660 represents excellent quality; moreover, benchmarking  (Table 5)  demonstrates that we exceed all published English sarcasm annotation studies, including those in high-resource settings with experienced annotators. The achievement of almost perfect agreement (κ = 0.8743) for one pair represents the highest reported for sarcasm annotation across multiple languages. This indicates the veracity of our resource for steering the progress of sarcasm detection among low-resource African languages. As (Artstein and Poesio, 2008) emphasizes, agreement expectations are fundamentally task-dependent: objective tasks like part-of-speech tagging routinely achieve κ > 0.95, while subjective semantic tasks rarely exceed κ = 0.75. Thus, our substantial inter-annotator agreement (Fleiss κ = 0.7660) demonstrates that even for semantically complex phenomena requiring pragmatic inference, native speakers with shared cultural background in the language can reliably establish subtle distinctions in intended meaning. Moreover, our agreement analysis provides insights into the semantic structure of sarcasm, which functions as non-literal linguistic expression. The high unanimity rate (83.3%) indicates that sarcasm in Yor`uba, like in other languages, exhibits markers, such as lexical hyperbole, that native speakers reliably recognize. The 16.7% of majority-agreement cases represent instances where sarcasm markers are present, but annotators differ in how much evidence they require before concluding an utterance is sarcastic. This aligns with theoretical accounts of sarcasm as involving scalar rather than categorical semantic 11 interpretation (Giora et al., 2005; Wilson and Wharton, 2006). Our soft labels preserve this scalar nature rather than imposing artificial binary categorization. Ultimately, traditional κ metrics provide only partial perspective on annotation quality. Thus, by ensuring agreement analysis, our 83.3% unanimity rate reveals that most instances are unambiguous, with the 16.7% variation concentrated on genuinely difficult borderline cases. Recent work on disagreement-aware NLP (Uma et al., 2021; Plank, 2022) argues that such variation should be preserved as training signal rather than eliminated, supporting our soft label approach. Future work will build on Yor-Sarc to expand the corpus and carry out advanced sarcasm detection experiments."
        },
        {
            "title": "Limitation",
            "content": "Considering the context of learning figurative expressions in the presence of small data, the only observed limitation is the domain coverage of the dataset. This being that our instances span social media and news media, but do not exhaustively cover all Yor`uba discourse contexts, such as face-toface conversation or dialogue. Thus, expanding domain coverage while maintaining agreement quality could be worthwhile consideration. Despite the limitation, our comprehensive agreement analysis demonstrates that Yor-Sarc represents high-quality, reliable resource for computational sarcasm research in African languages."
        },
        {
            "title": "Acknowledgements",
            "content": "This publication has emanated from research supported in part by grant from Taighde Eireann Research Ireland under Grant number 18/CRT/6049. For Open Access, the author has applied CC BY public copyright licence to any Author Accepted Manuscript version arising from this submission."
        },
        {
            "title": "Ethics statement",
            "content": "This research was conducted in accordance with ethical guidelines for human subjects research. All publicly available data (BBC News Yor`uba, social media posts) were collected from sources where users had consented to public distribution, and we ensured compliance with platform terms of service. For crowdsourced examples, participants provided informed consent through an ethically approved online survey protocol, with explicit permission to use their contributed examples for research purposes. All three annotators were compensated fairly above minimum wage standards, worked under voluntary agreements with the right to withdraw at any time, and also received comprehensive training on annotation guidelines. To protect privacy, all instances in the dataset have been anonymized. Moreover, the dataset is to be released under Creative Commons license for research purposes and development of NLP tools for African languages. Ultimately, we acknowledge that sarcasm detection technology could potentially be misused for censorship or manipulation; thus, we advocate for responsible use focused on improving communication technologies and cultural understanding."
        },
        {
            "title": "References",
            "content": "David Ifeoluwa Adelani, Graham Neubig, Sebastian Ruder, Shruti Rijhwani, Michael Beukman, Chester Palen-Michel, Constantine Lignos, Jesujoba O. Alabi, Shamsuddeen H. Muhammad, Peter Nabende, Cheikh M. Bamba Dione, Andiswa Bukula, Rooweither Mabuya, Bonaventure F. P. Dossou, Blessing Sibanda, Happy Buzaaba, Jonathan Mukiibi, Godson Kalipe, Derguene Mbaye, Amelia Taylor, Fatoumata Kabore, Chris Chinenye Emezue, Anuoluwapo Aremu, Perez Ogayo, Catherine Gitau, Edwin Munkoh-Buabeng, Victoire Memdjokam Koagne, Allahsera Auguste Tapo, Tebogo Macucwa, Vukosi Marivate, Elvis Mboning, Tajuddeen Gwadabe, Tosin Adewumi, Orevaoghene Ahia, Joyce Nakatumba-Nabende, Neo L. Mokono, Ignatius Ezeani, Chiamaka Chukwuneke, Mofetoluwa Adeyemi, Gilles Q. Hacheme, Idris Abdulmumin, Odunayo Ogundepo, Oreen Yousuf, Tatiana Moteu Ngoli, and Dietrich Klakow. MasakhaNER 2.0: Africa-centric transfer learning for named entity recognition. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 44884508, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.298. URL https://aclanthology.org/2022.emnlp-main.298/. Ron Artstein and Massimo Poesio. Inter-coder agreement for computational linguistics. Computational linguistics, 34(4):555596, 2008. Toluwase Asubiaro, Tunde Adegbola, Robert Mercer, and Isola Ajiferuke. word-level language identification strategy for resource-scarce languages. Proceedings of the Association for Information Science and Technology, 55(1):1928, 2018. David Bamman and Noah A. Smith. Contextualized sarcasm detection on twitter. In Proceedings of the International AAAI Conference on Web and Social Media, volume 9, pages 574577, 2015. URL https://ojs.aaai.org/index.php/ICWSM/article/view/14655. Santiago Castro, Devamanyu Hazarika, Veronica Perez-Rosas, Roger Zimmermann, Rada Mihalcea, and Soujanya Poria. Towards multimodal sarcasm detection (an Obviously perfect paper). In Anna Korhonen, David Traum, and Lluıs M`arquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 46194629, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1455. URL https://aclanthology.org/ P19-1455/. Dmitry Davidov, Oren Tsur, and Ari Rappoport. Semi-supervised recognition of sarcasm in twitter and amazon. In Proceedings of the fourteenth conference on computational natural language learning, pages 107116, 2010. Cheikh Bamba Dione, David Ifeoluwa Adelani, Peter Nabende, Jesujoba Alabi, Thapelo Sindane, Happy Buzaaba, Shamsuddeen Hassan Muhammad, Chris Chinenye Emezue, Perez Ogayo, Anuoluwapo Aremu, et al. Masakhapos: Part-of-speech tagging for typologically diverse african languages. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1088310900, 2023. Bradley Efron and Robert Tibshirani. The bootstrap method for assessing statistical accuracy. Behaviormetrika, 12(17):135, 1985. Olutola Olaide Fagbolu, Babatunde Sunday Obalalu, Samuel Udoh, and Ibadan Abeokuta Uyo. Applying rough set theory to yoruba language translation. In International Conference on Advanced Trends in ICT and Management (ICAITM) 28th, 2016. Ibrahim Abu Farha and Walid Magdy. From arabic sentiment analysis to sarcasm detection: The arsarcasm dataset. In The 4th Workshop on Open-Source Arabic Corpora and Processing Tools, pages 3239. European Language Resources Association (ELRA), 2020. 13 Joseph L. Fleiss. Measuring nominal scale agreement among many raters. Psychological Bulletin, 76 (5):378382, 1971. doi: 10.1037/h0031619. Xiyuan Gao, Shekhar Nayak, and Matt Coler. Improving sarcasm detection from speech and text through attention-based fusion exploiting the interplay of emotions and sentiments. In Proceedings of Meetings on Acoustics, volume 54. Acoustical Society of America, 2024. Rachel Giora, Shani Federman, Arnon Kehat, Ofer Fein, and Hadas Sabah. Irony aptness. Humor: International Journal of Humor Research, 18(1), 2005. Roberto Gonzalez-Ibanez, Smaranda Muresan, and Nina Wacholder. Identifying sarcasm in twitter: closer look. In Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies, pages 581586, 2011. Toheeb Aduramomi Jimoh, Tabea De Wille, and Nikola S. Nikolov. Bridging gaps in natural language processing for yor`uba: systematic review of decade of progress and prospects. Natural Language Processing Journal, 13:100194, 2025. ISSN 2949-7191. doi: https://doi.org/10.1016/j.nlp.2025. 100194. URL https://www.sciencedirect.com/science/article/pii/S2949719125000706. Aditya Joshi, Pushpak Bhattacharyya, and Mark Carman. Automatic sarcasm detection: survey. ACM Computing Surveys (CSUR), 50(5):122, 2017. Eugene Kariba Kamau and Noorhan Abbas. Multimodal sarcasm dataset generation for low-resource language: Swahili. In International Conference on Innovative Techniques and Applications of Artificial Intelligence, pages 239252. Springer, 2025. Shumaila Khan, Iqbal Qasim, Wahab Khan, Aurangzeb Khan, Javed Ali Khan, Ayman Qahmash, and Yazeed Yasin Ghadi. An automated approach to identify sarcasm in low-resource language. PloS one, 19(12):e0307186, 2024. Mikhail Khodak, Nikunj Saunshi, and Kiran Vodrahalli. large self-annotated corpus for sarcasm. In proceedings of the eleventh international conference on language resources and evaluation (LREC 2018), 2018. Khadijat Ladoja and Ruth Afape. Sarcasm detection in pidgin tweets using machine learning techniques. Asian Journal of Research in Computer Science, 17(5):212221, 2024. J. Richard Landis and Gary G. Koch. The measurement of observer agreement for categorical data. Biometrics, 33(1):159174, 1977. doi: 10.2307/2529310. Joshua Lee, Wyatt Fong, Alexander Le, Sur Shah, Kevin Han, and Kevin Zhu. Pragmatic metacognitive prompting improves llm performance on sarcasm detection. In Proceedings of the 1st Workshop on Computational Humor (CHum), pages 6370, 2025. Mary McHugh. Interrater reliability: the kappa statistic. Biochemia medica, 22(3):276282, 2012. Rishabh Misra and Prahal Arora. Sarcasm detection using news headlines dataset. AI Open, 4:1318, 2023. Shamsuddeen Hassan Muhammad, David Ifeoluwa Adelani, Sebastian Ruder, Ibrahim Said Ahmad, Idris Abdulmumin, Bello Shehu Bello, Monojit Choudhury, Chris Chinenye Emezue, Saheed Salahudeen Abdullahi, Anuoluwapo Aremu, et al. Naijasenti: nigerian twitter sentiment corpus for multilingual sentiment analysis. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 590602, 2022. 14 Shamsuddeen Hassan Muhammad, Idris Abdulmumin, Abinew Ali Ayele, Nedjma Ousidhoum, David Ifeoluwa Adelani, Seid Muhie Yimam, Ibrahim Said Ahmad, Meriem Beloucif, Saif Mohammad, Sebastian Ruder, et al. Afrisenti: twitter sentiment analysis benchmark for african languages. In Proceedings of the 2023 conference on empirical methods in natural language processing, pages 1396813981, 2023. Barbara Plank. The problem of human label variation: On ground truth in data, modeling and In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the evaluation. 2022 Conference on Empirical Methods in Natural Language Processing, pages 1067110682, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.731. URL https://aclanthology.org/2022.emnlp-main.731/. Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalindra De Silva, Nathan Gilbert, and Ruihong Huang. Sarcasm as contrast between positive sentiment and negative situation. In David Yarowsky, Timothy Baldwin, Anna Korhonen, Karen Livescu, and Steven Bethard, editors, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 704 714, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL https://aclanthology.org/D13-1066/. Sahil Swami, Ankush Khandelwal, Vinay Singh, Syed Sarfaraz Akhtar, and Manish Shrivastava. corpus of english-hindi code-mixed tweets for sarcasm detection. arXiv preprint arXiv:1805.11869, 2018. Alexandra Uma, Tommaso Fornaciari, Dirk Hovy, Silviu Paun, Barbara Plank, and Massimo Poesio. Learning from disagreement: survey. Journal of Artificial Intelligence Research, 72:13851470, 2021. Deirdre Wilson and Tim Wharton. Relevance and prosody. Journal of pragmatics, 38(10):15591579, 2006. Yazhou Zhang, Chunwang Zou, Zheng Lian, Prayag Tiwari, and Jing Qin. Sarcasmbench: Towards evaluating large language models on sarcasm understanding. IEEE Transactions on Affective Computing, 2025."
        },
        {
            "title": "A Supplementary Agreement Analysis",
            "content": "A.1 Label Distribution Across Annotators Figure 5 shows label distributions for each annotator. Sarcastic label rates range from 30.96% (A3) to 45.87% (A2), reflecting threshold differences while maintaining balanced class distributions. Figure 5: Label distribution across annotators A.2 Annotator Labelling Bias Figure 6 illustrates threshold variation: A2 most liberal (45.87%), A3 most conservative (30.96%). All annotators maintain balanced distributions without extreme bias. The threshold differences explain pairwise agreement patterns while all maintain balance. A.3 Agreement Level Distribution Figure 7 reveals 83.3% unanimity with zero complete disagreementsas required. Sarcastic instances show slightly higher consensus (85.5%) than non-sarcastic (81.5%). 16 Figure 6: Annotator Labelling bias Figure 7: Overall agreement distribution (left) and distribution by class (right)"
        }
    ],
    "affiliations": [
        "Department of Computer Science and Information Systems, University of Limerick"
    ]
}