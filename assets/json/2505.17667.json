{
    "paper_title": "QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning",
    "authors": [
        "Fanqi Wan",
        "Weizhou Shen",
        "Shengyi Liao",
        "Yingcheng Shi",
        "Chenliang Li",
        "Ziyi Yang",
        "Ji Zhang",
        "Fei Huang",
        "Jingren Zhou",
        "Ming Yan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent large reasoning models (LRMs) have demonstrated strong reasoning capabilities through reinforcement learning (RL). These improvements have primarily been observed within the short-context reasoning tasks. In contrast, extending LRMs to effectively process and reason on long-context inputs via RL remains a critical unsolved challenge. To bridge this gap, we first formalize the paradigm of long-context reasoning RL, and identify key challenges in suboptimal training efficiency and unstable optimization process. To address these issues, we propose QwenLong-L1, a framework that adapts short-context LRMs to long-context scenarios via progressive context scaling. Specifically, we utilize a warm-up supervised fine-tuning (SFT) stage to establish a robust initial policy, followed by a curriculum-guided phased RL technique to stabilize the policy evolution, and enhanced with a difficulty-aware retrospective sampling strategy to incentivize the policy exploration. Experiments on seven long-context document question-answering benchmarks demonstrate that QwenLong-L1-32B outperforms flagship LRMs like OpenAI-o3-mini and Qwen3-235B-A22B, achieving performance on par with Claude-3.7-Sonnet-Thinking, demonstrating leading performance among state-of-the-art LRMs. This work advances the development of practical long-context LRMs capable of robust reasoning across information-intensive environments."
        },
        {
            "title": "Start",
            "content": "QWENLONG-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning Fanqi Wan, Weizhou Shen, Shengyi Liao, Yingcheng Shi, Chenliang Li, Ziyi Yang, Ji Zhang, Fei Huang, Jingren Zhou, Ming Yan Qwen-Doc Team, Alibaba Group https://github.com/Tongyi-Zhiwen/QwenLong-L1 https://huggingface.co/Tongyi-Zhiwen/QwenLong-L1-32B https://modelscope.cn/models/iic/QwenLong-L1-32B"
        },
        {
            "title": "Abstract",
            "content": "Recent large reasoning models (LRMs) have demonstrated strong reasoning capabilities through reinforcement learning (RL). These improvements have primarily been observed within the short-context reasoning tasks. In contrast, extending LRMs to effectively process and reason on long-context inputs via RL remains critical unsolved challenge. To bridge this gap, we first formalize the paradigm of long-context reasoning RL, and identify key challenges in suboptimal training efficiency and unstable optimization process. To address these issues, we propose QWENLONG-L1, framework that adapts short-context LRMs to long-context scenarios via progressive context scaling. Specifically, we utilize warm-up supervised fine-tuning (SFT) stage to establish robust initial policy, followed by curriculum-guided phased RL technique to stabilize the policy evolution, and enhanced with difficulty-aware retrospective sampling strategy to incentivize the policy exploration. Experiments on seven long-context document question-answering benchmarks demonstrate that QWENLONG-L1-32B outperforms flagship LRMs like OpenAI-o3-mini and Qwen3-235B-A22B, achieving performance on par with Claude-3.7-Sonnet-Thinking, demonstrating leading performance among stateof-the-art LRMs. This work advances the development of practical long-context LRMs capable of robust reasoning across information-intensive environments. 5 2 0 2 3 2 ] . [ 1 7 6 6 7 1 . 5 0 5 2 : r Figure 1: Overall results of QWENLONG-L1 across seven long-context reasoning benchmarks. Starting from R1-Distill-Qwen-32B, QWENLONG-L1-32B achieves an average gain of 5.1 points, surpassing OpenAI-o3-mini, Qwen3-235B-A22B, and comparable to Claude-3.7-Sonnet-Thinking. Corresponding author. Preprint. Work in progress. Figure 2: Comparison of training dynamics between short-context and long-context reasoning RL. The long-context reasoning RL demonstrates two key challenges: suboptimal training efficiency, with slower improvements in reward score caused by more reduction in entropy, and unstable optimization process, with more fluctuations in KL divergence introduced from greater variance in longer output."
        },
        {
            "title": "Introduction",
            "content": "Recent breakthroughs in large reasoning models (LRMs) have showcased significant improvements in reasoning capabilities, achieving performance comparable to human experts in complex problemsolving scenarios [49]. These advancements, exemplified by OpenAI-o1 [26, 15], DeepSeek-R1 [6, 11], and Qwen-QwQ [41, 43], have sparked extensive research efforts to explore and enhance broad spectrum of reasoning tasks through reinforcement learning (RL), ranging from foundational logical reasoning [29, 47] to advanced challenges in programming [8, 7] and mathematics [24, 39], with innovations in RL algorithms driving progress in reasoning quality enhancements [3, 55, 23, 54]. Following RL fine-tuning, LRMs exhibit phenomenon analogous to human slow thinking [4], characterized by the emergence of sophisticated problem-solving strategies such as divide-and-conquer and backtracking mechanisms in their extended chain-of-thought (CoT) reasoning outputs [46]. While this process enhances reasoning performance on short context tasks (e.g., 4K tokens) [45, 22], its scalability to long-context scenarios (e.g., 120K tokens), which requires robust contextual grounding and multi-step reasoning, remains unexplored. This limitation poses significant barrier to practical applications requiring interaction with external knowledge, such as deep research [38, 27, 40], where LRMs must collect and process information from knowledge-intensive environments. To shed light on this topic, we first introduce the concept of long-context reasoning RL. Different from short-context reasoning RL, which primarily relies on internal knowledge stored within model parameters, long-context reasoning RL instead necessitates that LRMs perform retrieval and grounding of relevant information from long-context inputs, followed by generation of reasoning chains based on the incorporated information [12, 30, 52]. To illustrate the differences between short-context and long-context reasoning RL, we conduct preliminary experiment to compare the training dynamics in Figure 2. Our results reveal that long-context reasoning RL exhibits suboptimal training efficiency compared to the short-context counterpart with (a) delayed reward convergence. This discrepancy stems from (b) marked reduction in output entropy when processing long-context inputs, which restricts exploratory behavior during policy optimization. Furthermore, we identify unstable optimization process, evidenced by (c) intermittent spikes in KL divergence. These instabilities are introduced by the inherent variance amplification due to (d) longer output length with heterogeneous input length distributions, leading to greater variability during policy updating. To address these challenges, we propose QWENLONG-L1, novel RL framework designed to facilitate the transition of LRMs from short-context proficiency to robust long-context generalization, as shown in Figure 3. Inspired by recent studies on context extension during pretraining [9, 48, 10], our framework enhances short-context LRMs through progressive context scaling during RL training. The framework comprises three core components: warm-up supervised fine-tuning (SFT) phase to initialize robust policy, curriculum-guided RL phase that facilitates stable adaptation from short to long contexts, and difficulty-aware retrospective sampling mechanism that adjusts training complexity across stages to incentivize policy exploration. Leveraging recent RL algorithms, including GRPO [34] and DAPO [54], our framework integrates hybrid reward functions combining rule-based and model-based binary outcome rewards to balance precision and recall. Through strategic utilization of group relative advantages during policy optimization, it guides LRMs to 2 Figure 3: Overview of QWENLONG-L1, which is novel long-context reasoning RL training framework. The proposed framework integrates group-relative RL algorithms, hybrid reward mechanisms, and progressive context scaling strategies to enable stable adaptation from short-context to longcontext LRMs with robust contextual grounding and multi-step reasoning capabilities. learn effective reasoning patterns essential for long-context reasoning scenarios, resulting in robust long-context grounding and superior reasoning capabilities. In our experiments, we focus on document question answering (DocQA) [51, 17, 13] as representative real-world long-context reasoning task. Specifically, we introduce DOCQA-RL-1.6K, specialized RL training dataset comprising 1.6K DocQA problems spanning mathematical, logical, and multi-hop reasoning domains. Experimental results across seven long-context DocQA benchmarks demonstrate the superiority of QWENLONG-L1 compared to various proprietary and open-source LRMs. Notably, QWENLONG-L1-14B achieves superior performance over Gemini2.0-Flash-Thinking and Qwen3-32B, while QWENLONG-L1-32B outperforms OpenAI-o3-mini, Qwen3-235B-A22B, and even matches Claude-3.7-Sonnet-Thinking. Our analysis further identifies several critical insights in long-context reasoning RL optimization: (1) progressive context scaling promotes higher entropy and stabilizes KL divergence, enhancing training efficiency; (2) SFT proves to be an economical way for performance enhancement, whereas RL unlocks the potential to achieve optimal performance; (3) RL naturally fosters specialized long-context reasoning behaviors that boost final performance, but imitating these behaviors do not translate into gains when applied to SFT. Our key contributions are summarized as follows: We conceptualize the paradigm of long-context reasoning RL and identify its unique challenges, making further step towards developing practical long-context LRMs capable of grounding and integrating information for complex, real-world reasoning scenarios. We present QWENLONG-L1, the first RL framework designed for long-context LRMs. Through progressive context scaling, QWENLONG-L1 enables stable short-to-long context adaptation via group-relative RL optimization and hybrid reward mechanisms. We showcase the effectiveness of QWENLONG-L1 through comprehensive experiments across seven long-context document question answering benchmarks. Our results reveal that QWENLONGL1 achieves substantial performance gains compared to cutting-edge LRMs, offering fundamental recipe and practice for long-context reasoning optimization."
        },
        {
            "title": "2 Long-Context Reasoning Reinforcement Learning",
            "content": "While existing works have explored reinforcement learning (RL) for enhancing short-context reasoning tasks [14, 56, 16], the extension of long-context reasoning RL remains an unresolved challenge. To this end, we introduce QWENLONG-L1, the first RL framework specifically designed for long-context reasoning. In this section, we first describe the preliminaries, and then detail the implementation, including the RL algorithms for long-context reasoning, the progressive context scaling strategies for stable optimization, and the hybrid reward mechanisms to balance evaluation precision and recall. 2.1 Preliminaries The standard RL objective for language models optimizes KL-regularized expected reward [32]: max πθ ExD,yπθ(x) [rϕ(x, y)] βDKL [πθ(y x) πref(y x)] (1) where rϕ(x, y) denotes the reward for output given input from the policy model πθ, and πref represents the reference model for DKL regularization. 3 Unlike prior works wherein the input typically is short question, requiring the policy model πθ to generate output based on its parametric knowledge, we extend this formulation to incorporate an additional long-context c. Therefore, the policy model πθ needs to first ground relevant information from c, and then provide reasoning chains to solve the question x: Ex,cD,yπθ(x,c) [rϕ(x, c, y)] βDKL [πθ(y x, c) πref(y x, c)] The context and the question are concatenated by the following prompt template: max πθ (2) Table 1: Prompt template to concatenate the input context and question. Please read the following text and answer the question below. <text> context </text> question Format your response as follows: \"Therefore, the answer is (insert answer here)\". 2.2 RL Algorithms Proxy Policy Optimization (PPO) To optimize the policy model πθ using the above objective, the PPO [33] algorithm proposes to maximize the clipped surrogate objective: JPPO(θ) = Ex,cD,yπθold (x,c) (cid:34) 1 y (cid:88) t=1 (cid:32) min πθ(yt x, c, y<t) πθold(yt x, c, y<t) At, clip (cid:32) πθ(yt x, c, y<t) πθold(yt x, c, y<t) (cid:33) , 1 ε, 1 + ε At (cid:33)(cid:35) (3) where πθ is the current policy model, πθold is the old policy model before updating, ε is the clipping hyperparameter, and At is the advantage estimator of the t-th token. For given input context and question x, πθold first generate sequential output y, and then At is calculated to update πθ using the generalized advantage estimation (GAE) [31] with learned value function Vϕ. For long-context inputs, the quadratic complexity of attention mechanisms renders value network training computationally prohibitive. Therefore, we opt for the group-relative RL algorithms, including GRPO [34] and DAPO [54], to estimate the advantage through group-normalized rewards instead of an additional value network. Group Relative Policy Optimization (GRPO) In GRPO, given the context and the question x, the old policy model πθold generates group of outputs {yi}G i=1, with rewards {ri}G i=1 calculated by the reward function. The optimization objective for the policy model πθ becomes: JGRPO(θ) = x,cD,{yi}G i=1πθold (x,c) (cid:34) 1 (cid:88) i=1 1 yi (cid:32) yi (cid:88) t=1 min (cid:16) πθ(yi,tx, c, yi,<t) πθold(yi,tx, c, yi,<t) (cid:33)(cid:35) Ai,t, clip (cid:16) πθ(yi,tx, c, yi,<t) πθold(yi,tx, c, yi,<t) , 1 ε, 1 + ε (cid:17) (cid:17) Ai,t βDKL(πθπref) where the advantage for token yi,t is estimated by normalizing the group-level rewards: Ai,t = ri mean({ri}G std({ri}G i=1) i=1) In our implementation, we remove the KL term in the GRPO objective to encourage the exploration capacity of the policy model, following the common suggestions in recent works [14, 23, 54]. Decoupled Clip and Dynamic Sampling Policy Optimization (DAPO) DAPO integrates several techniques to ensure more stable and efficient RL process: (1) higher clip threshold to avoid entropy collapse, (2) dynamic sampling strategy to remove examples with zero reward variance, (3) token-level loss to mitigate the length bias, and (4) an overlong reward shaping to avoid excessively long outputs. The optimization objective for the policy model πθ is: JDAPO(θ) = (x,c)D,{yi}G i=1πθold (x,c) (cid:34) 1 i=1 yi (cid:80)G (cid:88) yi (cid:88) i=1 t=1 min (cid:16) πθ(yt x, c, y<t) πθold(yt x, c, y<t) Ai,t, clip (cid:16) πθ(yt x, c, y<t) πθold(yt x, c, y<t) , 1 εlow, 1 + εhigh (cid:35) (cid:17) (cid:17) Ai,t 4 (6) (4) (5) where εlow and εhigh are the low and high clip thresholds, respectively. The dynamic sampling strategy ensures that the examples have non-zero reward variance, and the token-level loss sets an equal weight for loss calculation for each token in the outputs. The overlong reward shaping set soft length-award punishment as follows: rfinali = ri, ri + (LmaxLcache)yi ri 1, Lcache yi Lmax Lcache , Lmax Lcache < yi Lmax (7) Lmax < yi where Lmax is the maximum sequence length and Lcache is buffer zone for gradual length penalization. Different from GRPO, the final advantage for yi,t in DAPO becomes: Ai,t = 2.3 Progressive Context Scaling rfinali mean({rfinali}G std({rfinali}G i=1) i=1) (8) Training LRMs for long-context reasoning presents unstable optimization dynamics. To address these issues, we propose progressive context scaling framework, including curriculum-guided RL strategy to stabilize short-to-long context optimization, difficulty-aware retrospective sampling mechanism to prioritize exploration of complex instances, and warm-up supervised fine-tuning (SFT) phase to provide robust initialization before RL training. Curriculum-Guided Phased Reinforcement Learning The RL process is divided into discrete phases, with the target context lengths of Lk for each phase. Starting from an initial input length L1, each subsequent phase increases the input length until reaching the maximum target length LK. During phase k, the policy model πθ is trained exclusively on examples that satisfies: Lk1 < + Lk, L0 = 0 (9) where and denote the question length and supporting context length, respectively. Difficulty-Aware Retrospective Sampling Building on the crucial efficacy of instance difficulty in previous data selection studies [59, 19], we adopt difficulty-aware retrospective sampling method to strategically incorporate instances from preceding phases. Specifically, we implement importance sampling weighted by difficulty scores to curate retrospective instances: diff(x, c) = 1 mean({ri}G i=1) (10) where diff() denotes the difficulty function, quantified as the inverse mean reward {ri}G i=1 from group of outputs generated by the base model. Lower mean rewards correspond to higher difficulty scores, prioritizing challenging instances during retrospective sampling. Warm-Up Supervised Fine-Tuning Prior to initiating RL training, we employ warm-up supervised fine-tuning (SFT) stage to establish robust initial policy model capable of grounding information from the long-context inputs. This critical preparatory stage ensures the policy model develops fundamental capabilities in context comprehension, reasoning chain generation, and answer extraction before exposure to the instability of RL optimization. The SFT process utilizes high-quality demonstrations DSFT distilled from teacher LRM, where each example contains question x, supporting context c, and gold-standard reasoning path with verified correctness. To align with the progressive scaling curriculum, we construct DSFT within the initial input length L1 in curriculum-guided RL. The model is trained to minimize the standard negative log-likelihood objective: LSFT(θ) = E(x,c,y)DSFT 1 y (cid:88) t=1 log πθ(y x, c, <t) (11) The resulting SFT model serves as the initial policy πθ for RL training, providing stable starting parameters. In Section 4.2, we demonstrate the effectiveness of the proposed three strategies for stable short-to-long context scaling in reasoning RL. 5 2.4 Hybrid Reward Mechanisms Prior works on short-context reasoning tasks in mathematics, coding, and logical reasoning [24, 7, 47] typically utilize rule-based reward functions that prioritize precision through strict answer matching and format verification to mitigate reward hacking risks [35]. However, long-context reasoning tasks such as open-domain question answering present unique challenges due to their inherent answer diversity. Overly restrictive rule-based rewards in such contexts risk constraining valid answer variations, potentially compromising overall performance. To address these limitations, we propose hybrid reward mechanism that combines rule-based verification [11] with LLM-as-a-judge [58], thereby balancing precision and recall through complementary evaluation. Rule-Based Verification The rule-based component rrule ensures precision by verifying strict adherence to task-specific correctness criteria. For question answering tasks, we first extract the final answer yans from model generations using regular expressions aligned with the structured prompt template in Table 1, and then perform exact string matching against the gold answer ygold: rrule(y) = I(yans = ygold) (12) where represents the indicator function. Notably, we intentionally omit the format reward for answer extraction, as the base model demonstrates sufficient inherent format compliance capabilities; excessive format rewards could oversimplify the learning objective, potentially hindering the models ability for reasoning chain exploration [56, 16]. LLM-as-a-Judge To complement the precision-oriented rule-based component and address potential false negatives in string matching, we introduce an LLM-based evaluator rLLM that assesses semantic equivalence between generated and gold answers: where the LLM judge produces binary correctness score based on the evaluation template as follows: rLLM(x, y) = LLM(x, yans, ygold) (13) Table 2: Prompt template for LLM-as-a-judge to compare the predicted answer and the gold answer given the question quesiton. You are an expert in verifying if two answers are the same. Your input is problem and two answers, Answer 1 and Answer 2. You need to check if they are equivalent. Your task is to determine if two answers are equivalent, without attempting to solve the original problem. Compare the answers to verify they represent identical values or meaning, even when written in different forms or notations. Your output must follow the following format: 1) Provide an explanation for why the answers are equivalent or not. 2) Then provide your final answer in the form of: [[YES]] or [[NO]] Problem: question Answer 1: predicted answer Answer 2: gold answer Combined Reward Formulation The integrated reward function combines both rule-based verification and LLM-as-a-judge through maximum selection: rϕ(x, y) = max(rrule(y), rLLM(x, y)) (14) Given the relative simplicity of answer comparison tasks, we employ small model, e.g., Qwen2.51.5B-Instruct [50], with temperature of zero for deterministic scoring. This configuration enables efficient reward computation during online RL training while maintaining evaluation reliability."
        },
        {
            "title": "3 Experimental Setup",
            "content": "In our experiments, we employ document question answering (DocQA) as our primary evaluation task for long-context reasoning capabilities, as it inherently requires both contextual grounding and multi-step reasoning. This section details our experimental setup for training and evaluation. 3.1 Training Datasets RL Dataset To construct challenging RL dataset for verifiable long-context reasoning, we develop DOCQA-RL-1.6K, which comprises 1.6K DocQA problems across three reasoning domains: (1) 6 Table 3: Detailed statistics of our train and test datasets. Length is calculated by the Qwen tokenizer. Statistics Train Dataset RL SFT DocMath Frames 2Wiki HQA Musi NarQA Qasp Test Dataset # Examples Avg. Length Max. Length 5,305 13,064 20,003 1,591 11,437 59,559 200 17,645 176,285 824 15,756 117,131 200 7,530 17,035 200 13,431 17, 200 16,327 17,883 200 29,887 65,357 200 5,074 21,927 Mathematical Reasoning: We use 600 problems from the DocMath [57] dataset, requiring numerical reasoning across long and specialized documents such as financial reports2; (2) Logical Reasoning: We employ DeepSeek-R1 [11] to synthesize 600 multi-choice questions requiring logic analysis of real-world documents spanning legal, financial, insurance, and production domains from our curated collection; (3) Multi-Hop Reasoning: We sample 200 examples from MultiHopRAG [36] and 200 examples from Musique [44], emphasizing cross-document reasoning. SFT Dataset To establish robust starting point for RL optimization, we distill 5.3K high-quality question-document-answer triplets through DeepSeek-R1 [11]. Aligned with recent data curation methods for LRMs [25, 53], we clean and filter questions based on quality, complexity, and diversity. Additionally, we control the quality and length of the documents to ensure precise contextual information. In Table 3, we provide the statistics of our RL and SFT datasets. 3.2 Training Details Base Model In our experiments, we initialize our base model with R1-Distill-Qwen-14B and R1Distill-Qwen-32B [11], subsequently implementing SFT and RL optimization phases3. RL Training As depicted in Section 2.3, we propose progressive context scaling mechanism for long-context reasoning RL optimization. Specifically, the training process follows two-phase curriculum context scaling, with 20K input length L1 in phase I, and 60K input length L2 in phase II. We incorporate difficulty-aware retrospective sampling to maintain the most difficult samples with an average accuracy of zero from phase to II. The training is conducted on 32xA100-80G GPUs, with train batch size of 128, mini batch size of 32, rollout number of 8, and learning rate of 2e-6. We set temperature to 0.7 and top-p to 0.95 with maximum output length of 10K for sampling. SFT Training The input length in the SFT stage is set to 20K. The training is conducted on 32xA10080G GPUs for 3 epochs, with train batch size of 128, and learning rate of 5e-6. 3.3 Evaluation Details Benchmarks We conduct evaluation on seven long-context DocQA benchmarks, including multi-hop reasoning benchmarks4 such as 2WikiMultihopQA [13], HotpotQA [51], Musique [44], NarrativeQA [17], Qasper [5], and Frames [18] as well as mathematical reasoning benchmarks like DocMath [57]. We report the maximum of exact match and LLM-judged accuracy as the final score, aligned with the reward function in Section 2.4. We use DeepSeek-V3 [21] as the judge model with temperature of 0.0 to provide reliable evaluation. The benchmark statistics are shown in Table 3. Configurations We evaluate our long-context LRMs with maximum input length of 120K and output length of 10K. For the proprietary LRMs with limited context length, we set the maximum input length to 50K. We conduct zero-shot evaluation with temperature of 0.7 and top-p of 0.95. 3.4 Baselines We compare QWENLONG-L1 against the following state-of-the-art LRMs. Proprietary LRMs OpenAI-o1-preview [15], Claude-3.7-Sonnet-Thinking [1], OpenAI-o3mini [28], Qwen3-Plus [42], QwQ-Plus [43], and Gemini-2.0-Flash-Thinking [37]. Open-Source LRMs DeepSeek-R1 [11], Qwen3-235B-A22B [42], Qwen3-32B [42], QwQ-32B [43], R1-Distill-Qwen-32B [11], and R1-Distill-Qwen-14B [11]. 2For DocMath, we sample 75% items from each subset from its valid split for training and 25% for evaluation. 3We exclude 7B/1.5B variants due to their mathematical reasoning feature inherent from Qwen2.5-Math [50]. 4We use the data from LongBench [2] for 2WikimultihopQA, HotpotQA, Musique, NarrativeQA, and Qasper. Table 4: Main results across seven long-context DocQA benchmarks. We highlight the top-1 and top-3 performance. indicates the performance gains and declines compared to the base models. Avg. 2Wiki HQA Musi NarQA Qasp DocMath Frames Models OpenAI-o1-preview Claude-3.7-Sonnet-Thinking OpenAI-o3-mini Qwen3-Plus QwQ-Plus Gemini-2.0-Flash-Thinking DeepSeek-R1 Qwen3-235B-A22B QwQ-32B Qwen3-32B R1-Distill-Qwen-32B R1-Distill-Qwen-14B R1-Distill-Qwen-14B-SFT to R1-Distill-Qwen-14B QWENLONG-L1-14B-GRPO to R1-Distill-Qwen-14B QWENLONG-L1-14B-DAPO to R1-Distill-Qwen-14B R1-Distill-Qwen-32B-SFT to R1-Distill-Qwen-32B QWENLONG-L1-32B-GRPO to R1-Distill-Qwen-32B QWENLONG-L1-32B-DAPO to R1-Distill-Qwen-32B Proprietary LRMs 80.8 70.9 75.5 73.6 73.5 69. 87.5 86.5 86.5 90.5 89.0 82.9 Open-Source LRMs 79.6 74.6 72.9 70.0 67.0 64.2 89.9 91.5 90.5 87.0 84.0 87.0 Ours Methods 83.5 84.4 83.5 82.4 81.0 79. 82.5 84.4 78.5 83.4 80.5 77.5 69.0 68.3 66.5 69.8 66.5 62.5 74.5 63.3 66.0 62.8 61.0 58.0 65.7 (+1.5) 68.7 (+4.5) 67.4 (+3.2) 71.6 (+4.6) 72.2 (+5.2) 70.1 (+3.1) 88.5 (+1.5) 88.5 (+1.5) 89.0 (+2.0) 87.0 (+3.0) 87.0 (+3.0) 90.5 (+6.5) 80.5 (+3.0) 86.5 (+9.0) 84.0 (+6.5) 80.5 (+0.0) 82.0 (+1.5) 83.0 (+2.5) 60.0 (+2.0) 63.5 (+5.5) 63.0 (+5.0) 65.5 (+4.5) 66.0 (+5.0) 69.0 (+8.0) 64.5 67.5 66.5 66.0 64.5 63. 66.0 67.5 59.5 58.0 62.5 61.0 60.0 (-1.0) 65.0 (+4.0) 65.5 (+4.5) 65.0 (+2.5) 68.0 (+5.5) 67.5 (+5.0) 68.0 61.5 59.0 57.5 62.0 57.0 59.5 60.0 58.0 57.5 54.0 51.0 52.0 (+1.0) 53.5 (+2.5) 57.0 (+6.0) 57.5 (+3.5) 61.0 (+7.0) 56.0 (+2.0) 57.0 56.0 55.0 52.5 53.5 45.5 53.0 53.0 57.5 56.0 50.0 51.0 72.9 70.7 70.4 70.3 70.0 65.7 72.1 70.6 69.0 67.8 65.6 64.2 48.5 (-2.5) 51.5 (+0.5) 52.5 (+1.5) 54.0 (+4.0) 56.0 (+6.0) 58.5 (+8.5) 65.0 (+0.8) 68.2 (+4.0) 68.3 (+4.1) 68.7 (+3.2) 70.3 (+4.7) 70.7 (+5.1) Figure 4: Pass@K rates of QWENLONG-L1-14B with different sample numbers across all benchmarks. We show that QWENLONG-L1-14B surpasses DeepSeek-R1 with small sampling number."
        },
        {
            "title": "4 Experimental Results",
            "content": "4.1 Main Results Table 4 presents the overall performance of QWENLONG-L1 across seven long-context document question answering (DocQA) benchmarks. The key findings are as follows: 8 Figure 5: Ablation studies of progressive context scaling strategy, where Baseline refers to the base or SFT model before RL training, RL refers to the naive single-stage RL, and Phased RL refers to the curriculum-guided phased RL. RS refers to the difficulty-aware retrospective sampling. Limited Efficacy of SFT for Long-Context Reasoning. Since the base model, R1-Distill-Qwen, is primarily optimized for short-context reasoning tasks in mathematics, coding, and scientific domains, we conduct SFT to adapt it for long-context reasoning before RL training, as outlined in Section 2.3. Despite this intervention, the SFT model only shows an average gain of 0.8 points on 14B and 3.2 points on 32B. Furthermore, the improvements exhibit significant variability across benchmarks, suggesting limited generalizability of the SFT approach to long-context reasoning scenarios. Significant Improvements via RL Integration. Through the integration of RL, QWENLONG-L1 exhibits remarkable advancements in long-context reasoning performance. Notably, QWENLONGL1-14B achieves an average improvement of 4.1 and 4.0 points over the base model with DAPO and GRPO, surpassing the 0.4 points improvement of the SFT baseline by significant margin. Furthermore, when scaling to 32B base model, QWENLONG-L1-32B even demonstrates 5.1 and 4.7 points performance increase with DAPO and GRPO. These results highlight the advanced capacity of RL approaches in refining the output distribution to address intricate, context-dependent reasoning problems through group-relative advantage estimation and incentives for on-policy sampled outputs. Leading Performance among Flagship LRMs. Our evaluation demonstrates that QWENLONG-L1 achieves superior performance compared to leading proprietary and open-source LRMs. Specifically, QWENLONG-L1-14B achieves an average score of 68.3, surpassing Gemini-2.0-Flash-Thinking, R1-Distill-Qwen-32B, and Qwen3-32B, while mathcing the performance of QwQ-32B. Moreover, QWENLONG-L1-32B achieves an average score of 70.7, exceeding the performance of QwQ-Plus, Qwen3-Plus, Qwen3-235B-A22B, and OpenAI-o3-mini, even comparable to Claude-3.7-SonnetThinking, demonstrating leading performance among state-of-the-art flagship LRMs. Additional Enhancements by Test-Time Scaling. We further conduct experiments to analyze the test-time scaling performance of QWENLONG-L1. Following established works [11, 45], we generate 16 candidate outputs per input question and evaluate Pass@K to quantify exploratory capability across all benchmarks. As illustrated in Figure 4, QWENLONG-L1-14B exhibits consistent performance enhancements with increased sampling scales. Notably, QWENLONG-L1-14B demonstrates remarkable gains, even surpassing DeepSeek-R1 and OpenAI-o1-preview with small sample size. Specifically, it achieves an average Pass@2 rate of 73.7 across all benchmarks, outperforming both DeepSeek-R1s 72.1 and OpenAI-o1-previewss 72.9, highlighting the efficacy of test-time scaling. Moreover, the significant gap between Pass@K and Pass@1 metrics indicates further potential for RL training to better bridge the transition from diverse exploration to optimal exploitation. 4.2 Ablation Studies In this section, we conduct ablation studies to investigate the key components in QWENLONG-L1 that enable successful progressive context scaling for long-context reasoning RL, including warm9 up supervised fine-tuning, curriculum-guided phased reinforcement learning, and difficulty-aware retrospective sampling, with the experimental results shown in Figure 5. Warm-up Supervised Fine-tuning. To illustrate the influence of warm-up SFT, we first evaluate the overall performance of models trained with and without this preparatory phase across seven benchmarks, using various RL algorithms and training strategies. As illustrated in Figure 5 (a), integration warm-up SFT yields significant performance improvements in all experimental setups. To further explore the mechanism of warm-up SFT in RL dynamics, Figure 5 (b) tracks the reward scores and gradient norm during training. The results reveal that warm-up SFT not only accelerates reward improvements but also sustains lower gradient norm during different RL phases, validating its capacity to prioritize performance gains over format alignment when transitioning models from short-context to long-context reasoning tasks. These findings highlight the necessity of integrating SFT as precursor to providing robust and efficient initialization for RL training. Curriculum-Guided Phased Reinforcement Learning. As shown in Figure 5 (a), we conduct comparative analysis between naive single-stage RL and the proposed curriculum-guided phased RL, with different training configurations: GRPO, DAPO, SFT + GRPO, and SFT + DAPO. The results demonstrate that our phased RL methodology achieves substantial performance improvements. We also note that this improvement is less pronounced when models are initialized with SFT, suggesting that warm-up training partially compensates for curriculum design. Further analysis in Figure 5 (c) reveals that single-stage RL exhibits heightened instability, as demonstrated by fluctuating KL divergence and entropy collapse. These results confirm the pivotal role of curriculum-guided phased training in the stable policy evolution from short-context to long-context reasoning RL. Difficulty-Aware Retrospective Sampling. To maintain wild exploration of hard examples, we introduce difficulty-aware retrospective sampling strategy to integrate subset of hard samples from prior training phases into the current training data. As illustrated in 5 (a), this strategy yields further performance enhancements with phased RL. Notably, in Figure 5 (d), despite undergoing phase RL training, these retained hard examples also lead to significantly lower reward and higher policy entropy, which incentivize the policy model to augment the exploration process. 4.3 Additional Analysis In this section, we investigate the questions pertaining to the development of long-context LRMs, focusing on the trade-off between SFT and RL in optimizing long-context reasoning capabilities, and the emergence and dynamics of long-context reasoning behaviors during training. Trade-off between SFT and RL in Optimization. As discussed in Section 4.2, SFT offers robust initialization for RL training. However, given that the initial SFT phase in our experiments relied on short-context training data, critical question arises regarding the role of long-context SFT and its impact on RL. To this end, we train longcontext SFT model using 10K context-question-answer triplets distilled from DeepSeek-R1, maintaining the same data distribution as the short-context SFT phase. This longcontext SFT model serves as the starting point for singlestage RL training, without progressive context scaling due to its inherent long-context capability. Figure 6: Comparison between different models before and after RL, where Baseline denotes the base model, Short SFT denotes the short-context SFT model, and Long SFT denotes the long-context SFT model. As shown in Figure 6, the long-context SFT model surpasses both the base model by 2.6 points and the short-context SFT model by 2.1 points. Despite the requirement for more training data, SFT offers distinct practical advantages, including reduced computational complexity, minimal infrastructure demands, and diminished reliance on specialized technical expertise, thereby positioning it as an economical strategy for performance enhancement [11]. However, further RL applied to the longcontext SFT model yields marginal improvements, with only 0.3 points gains and 67.4 final score, significantly underperforming the 3.2 points improvements and 68.2 final score achieved when RL is applied to the short-context SFT model. These results highlight two insights for long-context LRM development: (1) SFT and RL exhibit distinct yet complementary purposesSFT achieves acceptable performance with less effort, whereas RL is indispensable for attaining optimal results; 10 Figure 7: The change in reasoning behavior over training steps. We focus on four core reasoning behaviors, including long-context specific grounding and three general reasoning strategies: subgoal setting, backtracking, and verification. During RL training, we show that all the behaviors increase progressively with the corresponding performance gains. However, despite SFT leading to significantly increased reasoning behaviors, these efforts fail to improve the final performance. (2) Maximizing performance necessitates prioritizing RL over SFT, as excessive focus on SFT risks trapping models in local optima, thereby constraining opportunities for RL improvements. Emergence and Dynamics of Long-Context Reasoning Behaviors. The reasoning behaviors critically shape LRMs reasoning trajectories and rewards [11, 14]. To investigate these dynamics, we follow recent studies [56, 23] to analyze the evolution of reasoning behaviors during SFT and RL training. Specifically, we use DeepSeek-V3 [21] to extract and track the shifts of the average count of four core reasoning behaviors over training steps, including long-context specific grounding and three general reasoning behaviors: subgoal setting, backtracking, and verification: Grounding: The model recalls related information in the long context to support subsequent reasoning, e.g., Let me look through the provided text to find.... Subgoal Setting: The model decomposes complex questions into multiple manageable subgoals to solve them step-by-step, e.g., To solve this, we first need to.... Backtracking: The model identifies errors in generations and go back to revise its approach iteratively, e.g., This approach wont work because.... Verification: The model validates the predicted answers systematically to ensure solution correctness with self-reflection, e.g., Lets verify this result by.... The results in Figure 7 reveal three insights: (1) All LRMs exhibit marked reasoning behaviors, with long-context grounding occurring most frequently, underscoring its effectiveness in managing contextual dependencies during reasoning. (2) RL training amplifies these behaviors progressively, correlating with significant performance gains, suggesting RLs efficacy in refining the output space to prioritize reasoning patterns conducive to accurate solutions. (3) In contrast, while SFT models demonstrate increased reasoning behaviors, these adjustments fail to transform into performance improvements, likely due to SFTs inherent reliance on imitation learning, which prioritizes superficial pattern alignment over substantive reasoning skill development [59, 20]."
        },
        {
            "title": "5 Conclusion and Future Work",
            "content": "In this study, we explore the development of long-context LRMs with robust contextual grounding and reasoning capabilities through reinforcement learning (RL). We first propose the paradigm of longcontext reasoning RL and identify suboptimal training efficiency and unstable optimization process. To address these challenges, we present QWENLONG-L1, progressive context scaling RL framework designed to bridge the gap between short-context proficiency and long-context generalization. Specifically, the training process begins with warm-up SFT, followed by curriculum-guided phased RL, with difficulty-aware retrospective sampling strategy. Experiments across seven long-context document question-answering benchmarks demonstrate that QWENLONG-L1 achieves leading performance among state-of-the-art proprietary LRMs. Specifically, QWENLONG-L1-14B outperforms Gemini-2.0-Flash-Thinking and Qwen3-32B, while QWENLONG-L1-32B further surpasses OpenAIo3-mini, Qwen3-235B-A22B, and even matches Claude-3.7-Sonnet-Thinking. Our analysis yields three key insights for long-context reasoning RL, including the pivotal role of progressive context scaling in enabling stable adaptation, the necessity of prioritizing RL for optimal performance, and the increase of long-context reasoning behaviors during RL training for performance improvements. 11 Future work should prioritize three key avenues to advance long-context LRMs. First, scaling real-world tasks, like automated scientific research and long video analysis, will provide appropriate environments to enhance long-context comprehension and decision-making capabilities. Second, developing advanced architectures is essential, including optimized attention mechanisms, e.g., linear and sparse attention, and efficient infrastructures, e.g., asynchronous actor rollout and parameter updating. Third, rethinking long-context RL paradigms, such as transitioning from token-level to turnlevel markov decision process (MDP), might enable the breakdown of long-context into sequential interactions and optimizing them iteratively, paving the way for infinite-context RL systems."
        },
        {
            "title": "6 Case Study",
            "content": "To demonstrate QWENLONG-L1s capabilities, we show two illustrative examples from our analysis. Case complong-testmini-183: When evaluating QWENLONG-L1-14B against R1-Distill-Qwen14B and R1-Distill-Qwen-14B-SFT, distinct behavioral differences emerge. In this instance, R1Distill-Qwen-14B is misled by the detailed Interest is payable semiannually beginning October 15, 2011. It grounds irrelevant temporal and financial information, and consequently miscalculates the first-year interest payment. Although R1-Distill-Qwen-14B-SFT is fine-tuned on high-quality SFT triplets distilled from DeepSeek-R1, it still fails to resolve this issue. Instead, it demonstrates persistent limitations by entering loop of over-analysis on unrelated documents. The uncertainty and self-doubting behavior eventually causes it to exhaust the maximum generation limit (10,000 tokens) without providing the final answer. In contrast, while QWENLONG-L1-14B initially exhibits similar distractions, it quickly engages in effective self-reflection. Through timely verification and backtracking, it successfully filters out the irrelevant details and arrives at the correct answer. Case complong-testmini-265: This example illustrates how long-context reasoning behaviors (e.g. grounding, backtracking, verification, subgoal setting) emerge and interplay in practice, complementing our behavior evolution analysis in Figure 7. This task requires the model to infer the total interest accrued over two years based on dense financial filings. Here, the model correctly computes that the total interest is $980,000 based on principal of $4,900,000 at 10% annual interest over two years. This success is attributed to the emergence of multiple reasoning behaviors, showcasing how long-context reasoning RL enhances not only improves answer accuracy but also fosters the development of internal cognitive behaviors during reasoning. DocMath complong-testmini-183 Ground Truth: 32.4 Prompt: Please read the following text and answer the question below. <text> ...127Docs... 128: On March 22, 2011, we issued $300.0 million in aggregate principal amount of 8.0% senior unsecured notes due 2019 (the \"Notes\") at an offering price equal to 100% of par. The Notes were sold in private placement to eligible purchasers in transaction exempt from the registration requirements of the Securities Act of 1933, as amended. 129: We received net proceeds of $291.6 million, after deducting the discount of $7.5 million and offering expenses of $0.9 million. We used the net proceeds to repay indebtedness under our existing credit facility. The discount and the offering expenses are being amortized over the life of the Notes. The amortization is included in \"Interest expense\" on our unaudited condensed consolidated statements of operations. 130: The Notes were issued under an indenture dated March 22, 2011, (the \"Indenture\"), mature April 15, 2019, and bear interest at 8.0%. Interest is payable semiannually beginning October 15, 2011. The Notes are general unsecured obligations and are effectively junior in right of payment to any of our secured indebtedness to the extent of the value of the collateral securing such indebtedness. ...271Docs... </text> What is the total capital cost, combining both the offering cost of the senior notes and interest payment in the first year, in million dollars? Format your response as follows: \"Therefore, the answer is (insert answer here)\". 12 R1-Distill-Qwen-14B: Response (2166 tokens): <think> Alright, so need to figure out the total capital cost for EV Energy Partners, L.P., combining both the offering cost of the senior notes and the interest payment in the first year. Let me go through the provided financial statements step by step to find all the relevant information. First, Ill look for the senior notes offering costs. [...] The offering expenses were $0.9 million, and the discount was $7.5 million. [...] So the total offering costs (discount and expenses) are $7.5 + $0.9 = $8.4 million. Next, need to find the interest payment for the first year. The senior notes have an 8% annual interest rate and are due in 2019. They were issued in March 2011, so the first interest period would be from March to September 2011, but the interest is actually payable semi-annually starting in October 2011. [...] Since the notes were outstanding for six months, the semi-annual interest would be 8% / 2 = 4% of the principal. Therefore, the interest for the first year would be 4% of $300 million, which is $12 million. [...] The interest is semi-annual, so the first payment would be in October 2011, and the second in April 2012. So for the first year (2011), they would have paid $12 million in interest on the senior notes. [...] So combining the offering cost ($8.4 million) and the interest payment for the first year ($12 million), the total capital cost is $8.4 + $12 = $20.4 million. [...] </think> The total capital cost includes the offering costs and the interest payment for the first year. The offering costs for the senior notes were $8.4 million, and the interest expense for the first year (2011) was $12 million. Therefore, the total capital cost is $8.4 million + $12 million = $20.4 million. Therefore, the answer is $20.4 million. Grounding Behavior: Instances of Grounding Behavior: 1. Explicit Grounding: \"Let me go through the provided financial statements step by step to find all the relevant information.\" 2. Explicit Grounding: \"In the Notes section, under NOTE 5: FAIR VALUE MEASUREMENTS, see that on March 22, 2011, they issued $300 million in senior notes.\" 3. Explicit Grounding: \"Wait, but the offering cost is mentioned in NOTE 9: OWNERS EQUITY.\" 4. Explicit Grounding: \"Looking at the balance sheet, under June 30, 2011, the long-term debt is $480,183 thousand, which is $480.183 million.\" 5. Explicit Grounding: \"Wait, in NOTE 7: LONG-TERM DEBT, under the credit facility, it says that borrowings bear interest at floating rate based on LIBOR.\" 6. Grounding Verification: \"Wait, the financial statements show interest expense for the three months ended June 30, 2011 as $8,124 thousand, which is $8.124 million.\" 7. Grounding Verification: \"Wait, but in the condensed statements, the interest expense for the three months is $8.124 million, and for six months, its $13,283 thousand.\" 8. Grounding Verification: \"Wait, the notes were issued in March 2011, so from March 2011 to March 2012 is the first year.\" 9. Grounding Verification: \"Wait, in the financial statements, the interest expense for the six months ended June 30, 2011 is $13.283 million, which includes the senior notes interest for three months and the credit facilitys interest for the entire six months.\" 10. Grounding Verification: \"Wait, the senior notes were issued in March 2011, so the first interest period is from March 22, 2011 to September 22, 2011, which is about six months, and the interest would be 4% of $300 million, which is $12 million.\" 11. Grounding Verification: \"Wait, but the offering cost is one-time expense, and the interest is an ongoing expense.\" 13 12. Grounding Verification: \"Wait, but the question is about the total capital cost, which usually includes both the initial costs and the interest expense over the period.\" Each of these instances involves either explicit references to the document or verification steps that lead back to the document, thus counting as grounding behavior. R1-Distill-Qwen-14B-SFT: Response (10000 tokens): <think> Okay, lets tackle this question. [...] First, need to find where the offering costs for the senior notes are mentioned. [...] So the total offering costs here are $7.5 million (discount) plus $0.9 million (expenses), which adds up to $8.4 million. Next, the interest payment for the first year. The senior notes were issued in March 2011 with 8.0% annual interest rate. The total principal amount is $300 million. [...] However, the notes mention that interest is payable semi-annually starting October 15, 2011. [...] Wait, the notes were issued on March 22, 2011, and the first interest payment is on October 15, 2011. [...] The interest expense for the three months ended June 30, 2011, under \"Interest expense\" is $8,124 thousand. [...] In the Condensed Consolidated Statements of Operations for the six months ended June 30, 2011, the interest expense is $13,283 thousand. [...] In Note 7, under the senior notes, it says: \"The discount and the offering expenses are being amortized over the life of the Notes. The amortization is included in Interest expense on our unaudited condensed consolidated statements of operations.\" So the total initial costs for the senior notes are $7.5 million + $0.9 million = $8.4 million. These are amortized over the life of the notes, so each year, $8.4 million / 8 = $1.05 million per year. For the first year (2011), that would be $1.05 million. The question is about the interest payment in the first year. [...] The senior notes [...] bear 8% interest, payable semi-annually. The first payment is on October 15, 2011 [...]. So in the first year (2011), theres only one payment in October. [...] The coupon rate is 8%, so each payment is 4% of the principal. The principal is $300 million, so each payment is $12 million. The first payment is in October 2011, so in 2011, only one payment of $12 million would be made. [...] So total capital cost would be $8.4 million + $12 million = $20.4 million. [...] Alternatively, [...] the amortization of the offering costs is $8.4 million over 8 years, so for the first year, thats $1.05 million. Therefore, [...] the interest paid would be $12 million (coupon) + $1.05 million (amortization) = $13.05 million. The offering costs are $8.4 million. So total capital cost would be $8.4 million + $13.05 million = $21.45 million. Alternatively, [...] the offering costs are $8.4 million. The interest expense for the first year (2011) would be the interest paid on the senior notes [...] (Reach maximum token limits) Grounding Behavior: Explanation of Grounding Behavior: 1. \"Looking through the financial statements, in Note 7 under Long-Term Debt, theres section about the 8.0% Senior Notes due 2019.\" Explicit Grounding: The model directly references the financial statements and Note 7 to locate information about the senior notes. 2. \"Under Proceeds from debt offering, it says they received net proceeds of $291.6 million after deducting discount of $7.5 million and offering expenses of $0.9 million.\" Explicit Grounding: The model extracts specific details from the document about the net proceeds, discount, and offering expenses. 14 3. \"The interest expense for the six months ended June 30, 2011, is listed as $8,124 thousand, but thats semi-annual.\" Explicit Grounding: The model references the financial statements to find the interest expense for the six months ended June 30, 2011. 4. \"Wait, the notes mention that interest is payable semi-annually starting October 15, 2011.\" Explicit Grounding: The model refers back to the notes to verify the timing of interest payments. 5. \"The interest expense for the three months ended June 30, 2011, under Interest expense is $8,124 thousand.\" Explicit Grounding: The model again references the financial statements to confirm the interest expense for the three months ended June 30, 2011. 6. \"Wait, the notes were issued on March 22, 2011, and the first interest payment is on October 15, 2011.\" Explicit Grounding: The model returns to the document to verify the issuance date and the first interest payment date. 7. \"Lets check the Notes again. In Note 7, under the senior notes due 2019, it says the net proceeds were $291.6 million, after deducting discount of $7.5 million and offering expenses of $0.9 million.\" Explicit Grounding: The model revisits Note 7 to confirm the details about the net proceeds, discount, and offering expenses. 8. \"Lets look at the interest expense line in the Statements of Operations.\" Explicit Grounding: The model refers to the Statements of Operations to verify the interest expense. 9. \"In the Condensed Consolidated Statements of Operations for the six months ended June 30, 2011, the interest expense is $13,283 thousand.\" Explicit Grounding: The model extracts the interest expense figure from the financial statements. 10. \"In Note 7, under Interest expense, it mentions that the amortization of the discount and deferred loan costs is included in interest expense.\" Explicit Grounding: The model refers back to Note 7 to confirm how the amortization of discount and expenses is treated. 11. \"Lets check the Notes again. In Note 7, the senior notes discount and expenses are amortized over the life of the notes.\" Explicit Grounding: The model revisits Note 7 to verify the amortization period for the discount and expenses. 12. \"Lets check the interest expense in the Statements of Operations. For the six months ended June 30, 2011, the interest expense is $13,283 thousand.\" Explicit Grounding: The model refers to the Statements of Operations to confirm the interest expense figure. Each of these steps involves explicit grounding behavior, where the model directly references or returns to the original document to verify or extract information. QWENLONG-L1-14B: Response (5304 tokens): <think> Alright, so need to figure out the total capital cost for EV Energy Partners, L.P. in the first year, combining both the offering cost of the senior notes and the interest payment. Let me go through the provided financial statements step by step to find the necessary information. First, Ill look at the Notes to the Condensed Consolidated Financial Statements. In Note 7, under \"8.0% Senior Notes due 2019,\" [...] The discount was $7.5 million, and the offering expenses were $0.9 million. So, the total offering costs here would be $7.5 million (discount) + $0.9 million (expenses) = $8.4 million. Next, need to find the interest payment for the first year. 15 [...] The notes were issued in March 2011, so the full years interest would be 8% of $300 million, which is $24 million. But since the interest is semi-annual and the first payment was in October 2011, the interest for the first year (2011) might only be half of that, so $12 million. Wait, no, thats not correct because the interest is calculated on the outstanding principal for each period. Let me think again. [...] The senior notes have fixed rate of 8.0%. The interest expense for the senior notes would be 8% of $300 million, which is $24 million annually [...]. think we should consider the full years interest, which would be $24 million. [...] Im going to go with the full years interest, which is $24 million, plus the offering costs of $8.4 million, totaling $32.4 million. </think> The total capital cost includes both the offering costs and the interest payment for the first year. The offering costs for the senior notes were $8.4 million (discount of $7.5 million and expenses of $0.9 million). The interest payment for the first year (2011) is the full annual interest of 8% on the $300 million senior notes, which is $24 million. Adding these together gives total capital cost of $32.4 million. Therefore, the answer is $32.4 million. Grounding Behavior: Explanation of Grounding Behavior: 1. Explicit Grounding: The model repeatedly references specific sections of the document, such as \"Note 7,\" \"Condensed Consolidated Financial Statements,\" and \"Condensed Consolidated Statements of Operations,\" to extract information about the senior notes, offering costs, and interest expenses. For example: \"First, Ill look at the Notes to the Condensed Consolidated Financial Statements. In Note 7, under 8.0% Senior Notes due 2019, it mentions that on March 22, 2011, they issued $300 million in aggregate principal amount of senior notes.\" \"Looking at the financial statements, in the Condensed Consolidated Statements of Operations, under Interest expense, for the six months ended June 30, 2011, its $13,283 thousand.\" 2. Grounding Verification: The model frequently double-checks its understanding by returning to the document to verify details, such as the timing of interest payments, the breakdown of interest expenses, and the calculation of offering costs. For example: \"Wait, in the financial statements, for the six months ended June 30, 2011, the interest expense is $13,283 thousand. Let me check if that includes the senior notes.\" \"Wait, in Note 7, it says that the net proceeds from the senior notes were $291.6 million, after deducting the discount of $7.5 million and offering expenses of $0.9 million.\" 3. Repeated Verifications: The model revisits the same sections multiple times to confirm calculations or clarify ambiguities, such as the interest payment for the first year and the breakdown of offering costs. For example: \"Wait, let me go back to the question: What is the total capital cost, combining both the offering cost of the senior notes and interest payment in the first year, in million dollars?\" \"Wait, in the financial statements, the interest expense for the six months ended June 30, 2011, is $13,283 thousand. Let me check if that includes the senior notes.\" DocMath complong-testminiGround Truth: 980000.0 Prompt: Please read the following text and answer the question below. <text> ...329Docs... 330: In connection with our July 2021 acquisition of 100% of the equity of TopPop, on July 26, 2021, we issued to the sellers promissory notes in the aggregate principal amount of $4,900,000 (the \"TopPop Notes\"). The TopPop Notes bear interest at the rate of 10% per annum, matured on July 26, 2022 and are secured by all of the outstanding membership interest in TopPop. Under the terms of the TopPop Notes, we have five-day grace period to July 31, 2022 before an event of default under the TopPop 16 Notes occurs. Upon an event of default under the TopPop Notes, the holders of such TopPop Notes may exercise all rights and remedies available under the terms of the TopPop Notes or applicable laws, including to foreclose on certain collateral consisting of the membership interests of TopPop. On July 26, 2022, the total principal amount outstanding under the TopPop Notes was $4,900,000, exclusive of accrued and unpaid interest. 331: We are currently in discussions with holders of the TopPop Notes regarding possible solutions for the payment of the TopPop Notes, including the possible extension of the maturity date of the TopPop Notes for an additional year. There can be no assurance that our discussions will be successful and if we are not successful in finding an acceptable resolution to the existing default or the impending event of default, the noteholders will be able to seek judgement for the full amount due and may seek to foreclose on our assets. If this occurs, any such remedy will have material adverse effect on our business, results of operations and financial condition and is likely to negatively impact the price of our common stock. Holders of approximately $3.55 million of these notes have agreed to extend the term for 30 days and have indicated that they will not seek cash settlement prior to August 2023. ...30Docs... </text> How much total interest will the company have to pay under the new agreement until August 2023 in dollars if the company managed to extend the \"TopPop Notes\" maturity date for one more year? Format your response as follows: \"Therefore, the answer is (insert answer here)\". QWENLONG-L1-14B: Response (2192 tokens): <think> Okay, so need to figure out how much total interest the company will have to pay under the new agreement until August 2023 if they extend the \"TopPop Notes\" maturity date for one more year. Let me start by understanding the problem. From the text, see that the TopPop Notes were issued on July 26, 2021, with principal amount of $4,900,000. They bear an interest rate of 10% per annum. The original maturity was on July 26, 2022, but theres five-day grace period until July 31, 2022, before default occurs. However, the company is in discussions to extend the maturity date by an additional year, so the new maturity would be around August 2023. need to calculate the interest for this extended period. The original term was from July 26, 2021, to July 26, 2022, which is one year. If they extend it by one more year, the new maturity would be July 26, 2023, but the user mentioned until August 2023, so Ill consider the period up to August 2023. First, should determine the exact period for which the interest is to be calculated. The original notes were from July 26, 2021, to July 26, 2022. If extended for another year, the new maturity is July 26, 2023, but the user specifies until August 2023, which is bit beyond that. However, think the interest is calculated up to the maturity date, so maybe just until July 26, 2023, unless the extension is until August, in which case need to adjust. But the user says the company managed to extend the maturity date for one more year, so the total period would be two years from the original issue date. Let me check the details again. Wait, the original notes matured on July 26, 2022, and the company is discussing an extension for an additional year. So the new maturity would be July 26, 2023, making the total term two years. But the user mentions until August 2023, so perhaps the extension is until August 31, 2023, or just an extra year beyond the original 2022 date. think the key is to calculate the interest for the two-year period from July 26, 2021, to July 26, 2023, and then maybe add bit more if the extension is beyond that. But lets focus on the period until August 2023. The interest rate is 10% per annum, and the notes are simple interest, believe, unless stated otherwise. The interest is calculated on the principal amount, which is $4,900,000. So, the interest for one year would be $4,900,000 * 10% = $490,000. If they extend it for another year, thats another $490,000, making the total interest $980,000. But wait, the original notes had maturity in 2022, and the extension is for one more year, so from 2022 to 2023. Let me check if the interest is only for the extended year or for the entire two years. Looking at the text, it says the TopPop Notes bear interest at 10% per annum and mature on July 26, 2022. The extension is for an additional year, so the interest would be for the period from July 26, 2022, to July 26, 2023, which is one year. Therefore, the interest for the extended period would be $490,000. But the user is asking for the total interest until August 2023. So if the extension is until August 2023, thats bit more than year. However, typically, interest is calculated on an annual basis, and if the extension is for full year, the interest would be $490,000. If the extension is only until August 2023, 17 thats about 13 months, so the interest would be $4,900,000 * 10% * (13/12) $508,333.33, but Im not sure if the extension is for full year or just until August. Wait, in the text under Note 7, it says: \"The Company has agreed to keep this agreement in place and for the three months ended March 31, 2022, the Company has recorded consulting expense of $45,000.\" Thats about the leasehold improvements, not the TopPop Notes. Looking again at Note 14, which is under the leasehold improvements section, dont see any specific info on the TopPop Notes beyond their principal and interest rate. So, Ill assume that the extension is for an additional year beyond the original 2022 maturity, making the total term two years. Therefore, the total interest would be $490,000 for the first year and another $490,000 for the second year, totaling $980,000. Wait, but in Note 7, under the leasehold improvements, it says: \"The Company has agreed to keep this agreement in place and for the three months ended March 31, 2022, the Company has recorded consulting expense of $45,000.\" No, thats different note. Im getting confused. Wait, let me go back to the relevant sections. In Note 7 (which is actually under Item 7, line 15), it says: \"The Promissory Notes bear interest at the rate of 10% per annum and mature on July 26, 2022.\" Then, in the Recent Developments section (Item 2, line 25), it mentions that the TopPop Notes have maturity on July 26, 2022, and that the company is in discussions to possibly extend the maturity date for an additional year. So, if the maturity is extended to August 2023, thats an additional year beyond 2022. Therefore, the interest for the original year (2021-2022) was $490,000. Now, with the extension, they would have to pay interest for another year, so another $490,000, totaling $980,000. But wait, in the text under Item 2, line 25, it says: \"the Company is currently in discussions with holders regarding possible solutions for the payment of the Promissory Notes, including the possible extension for an additional year.\" So, the extension is for an additional year beyond the original 2022 maturity, making the new maturity July 26, 2023. So, the interest would be for two years: from July 26, 2021, to July 26, 2023. Therefore, total interest would be $4,900,000 * 10% * 2 = $980,000. But let me double-check. The original maturity was July 26, 2022, and the extension is for an additional year, so the notes would now mature on July 26, 2023. Therefore, the interest would be for the period from July 26, 2021, to July 26, 2023, which is exactly two years. So, the total interest would be $4,900,000 * 10% * 2 = $980,000. Wait, but in the text under Item 15, line 15, it says: \"the Company was late in filing its Annual Report on Form 10-K for the year ended December 31, 2021, which was filed on June 15, 2022. During the first quarter ended March 31, 2022, there was no business activity or transactions.\" That doesnt affect the interest calculation. Another point: the notes are secured by the membership interests in TopPop, but the interest calculation is straightforward. Its simple interest unless compounded, which isnt mentioned here. The text doesnt specify compounding, so Ill assume its simple interest. So, for each year, the interest is 10% of the principal. Since the principal remains $4,900,000, and the period is two years, the total interest is $490,000 per year * 2 = $980,000. Wait, but in the recent developments (Item 2, line 25), it mentions that the TopPop Notes have maturity on July 26, 2022, and the company is discussing an extension for an additional year. So, the extension would be until July 26, 2023, making it two-year period. Therefore, the total interest the company would have to pay until August 2023 under the new agreement is $980,000. </think> The TopPop Notes have principal amount of $4,900,000 and an annual interest rate of 10%. If the maturity date is extended for one additional year until August 2023, the total interest for the two-year period would be $490,000 per year, totaling $980,000. Therefore, the answer is $980, 000 . Grounding Behavior: Explanation of Grounding Behavior: 1. \"From the text, see that the TopPop Notes were issued on July 26, 2021, with principal amount of $4,900,000.\" 2. \"Looking at the text, it says the TopPop Notes bear interest at 10% per annum and mature on July 26, 2022.\" 3. \"Wait, in the text under Note 7, it says: The Company has agreed to keep this agreement in place and for the three months ended March 31, 2022, the Company has recorded consulting expense of $45,000.\" 4. \"Looking again at Note 14, which is under the leasehold improvements section, dont see any specific info on the TopPop Notes beyond their principal and interest rate.\" 5. \"Wait, let me go back to the relevant sections. In Note 7 (which is actually under Item 7, line 15), it says: The Promissory Notes bear interest at the rate of 10% per annum and mature on July 26, 2022.\" Backtracking Behavior: 1. First Backtracking: The model initially considers the period up to August 2023 but then questions whether the interest is calculated up to the maturity date (July 26, 2023) or beyond. It revises its approach to focus on the two-year period from July 26, 2021, to July 26, 2023. 2. Second Backtracking: The model initially calculates the interest for two years but then questions whether the interest is only for the extended year (2022-2023) or for the entire two years. It revises its approach to consider the interest for the extended year only. 3. Third Backtracking: The model considers the possibility of the extension being until August 2023 (13 months) and calculates the interest accordingly. However, it then questions whether the extension is for full year or just until August and revises its approach to assume full year. 4. Fourth Backtracking: The model mistakenly references Note 7 (leasehold improvements) instead of the relevant section for the TopPop Notes. It corrects itself by going back to the relevant sections (Note 14 and Item 2). 5. Fifth Backtracking: The model initially calculates the total interest as $980,000 but then doublechecks the period and confirms that the interest is for two years (July 26, 2021, to July 26, 2023). 6. Sixth Backtracking: The model considers the possibility of compounded interest but revises its approach to assume simple interest since compounding is not mentioned in the text. Verification Behavior: The chain-of-reasoning provided contains several instances where the model checks and verifies intermediate results or reasoning steps. Here are the key instances of verification behavior: 1. Verification of the period for interest calculation: The model checks the exact period for interest calculation, considering both original and extended maturity dates. Example statements: \"Let me check the details again\" \"Wait, the original notes matured on July 26, 2022, and the company is discussing an extension for an additional year.\" 2. Verification of the interest calculation: The model repeatedly verifies the interest calculation for the extended period. Example statements: \"So, the interest for one year would be $4,900,000 * 10% = $490,000. If they extend it for another year, thats another $490,000, making the total interest $980,000.\" \"Wait, but in the text under Item 2, line 25, it says: the Company is currently in discussions with holders regarding possible solutions for the payment of the Promissory Notes, including the possible extension for an additional year.\" 3. Verification of the extension period: The model examines whether the extension is for full year or partial period. Example statement: \"But the user is asking for the total interest until August 2023. So if the extension is until August 2023, thats bit more than year.\" 4. Verification of the interest type: 19 The model confirms whether simple or compound interest applies. Example statement: \"The text doesnt specify compounding, so Ill assume its simple interest.\" 5. Final verification of the total interest: The model double-checks the complete interest calculation. Example statements: \"So, the total interest would be $4,900,000 * 10% * 2 = $980,000.\" \"Wait, but in the recent developments (Item 2, line 25), it mentions that the TopPop Notes have maturity on July 26, 2022, and the company is discussing an extension for an additional year.\" Subgoal Setting Behavior: 1. Understand the problem: The model starts by identifying the need to calculate the total interest under the new agreement. 2. Determine the period for interest calculation: The model breaks down the task by figuring out the exact period for which the interest needs to be calculated, considering the original and extended maturity dates. 3. Clarify the extension details: The model attempts to clarify whether the extension is for full year or until August 2023, which affects the interest calculation. 4. Calculate interest for the extended period: The model calculates the interest for the extended period, considering whether it is full year or partial year. 5. Verify assumptions: The model checks the text to confirm assumptions about the interest rate, compounding, and the exact period of the extension. 6. Finalize the total interest: The model concludes by calculating the total interest for the two-year period based on the verified assumptions."
        },
        {
            "title": "References",
            "content": "[1] Anthropic. Claude 3.7 sonnet system card, Feburary 2025. [2] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. LongBench: bilingual, multitask benchmark for long context understanding. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 31193137, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.172. [3] Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. [4] Kahneman Daniel. Thinking, fast and slow. 2017. [5] Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah Smith, and Matt Gardner. dataset of information-seeking questions and answers anchored in research papers. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 45994610, 2021. [6] DeepSeek-AI. Deepseek-r1-lite-preview is now live: unleashing supercharged reasoning power!, November 2024. [7] Ahmed El-Kishky, Alexander Wei, Andre Saraiva, Borys Minaev, Daniel Selsam, David Dohan, Francis Song, Hunter Lightman, Ignasi Clavera, Jakub Pachocki, et al. Competitive programming with large reasoning models. arXiv preprint arXiv:2502.06807, 2025. [8] Hugging Face. Open r1: fully open reproduction of deepseek-r1, January 2025. 20 [9] Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng. Data engineering for scaling language models to 128k context. In International Conference on Machine Learning, 2024. [10] Tianyu Gao, Alexander Wettig, Howard Yen, and Danqi Chen. How to train long-context language models (effectively). arXiv preprint arXiv:2410.02660, 2024. [11] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [12] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model pre-training. In International conference on machine learning, pages 39293938. PMLR, 2020. [13] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing multi-hop qa dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th International Conference on Computational Linguistics, pages 66096625, 2020. [14] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, and Heung-Yeung Shum Xiangyu Zhang. Open-reasoner-zero: An open source approach to scaling reinforcement learning on the base model, 2025. [15] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [16] Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. [17] Tomáš Koˇcisk`y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. The narrativeqa reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317328, 2018. [18] Satyapriya Krishna, Kalpesh Krishna, Anhad Mohananey, Steven Schwarcz, Adam Stambler, Shyam Upadhyay, and Manaal Faruqui. Fact, fetch, and reason: unified evaluation of retrieval-augmented generation. arXiv preprint arXiv:2409.12941, 2024. [19] Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao. From quantity to quality: Boosting llm performance with selfguided data selection for instruction tuning. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 75957628, 2024. [20] Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Chandu, Chandra Bhagavatula, and Yejin Choi. The unlocking spell on base llms: Rethinking In The Twelfth International Conference on Learning alignment via in-context learning. Representations, 2023. [21] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [22] Jiaheng Liu, Dawei Zhu, Zhiqi Bai, Yancheng He, Huanxuan Liao, Haoran Que, Zekun Wang, Chenchen Zhang, Ge Zhang, Jiebin Zhang, et al. comprehensive survey on long context language modeling. arXiv preprint arXiv:2503.17407, 2025. [23] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025. 21 [24] Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl, 2025. [25] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. [26] OpenAI. Learning to reason with llms, September 2024. [27] OpenAI. Introducing deep research, February 2025. [28] OpenAI. Openai o3-mini system card, January 2025. [29] Jiayi Pan, Junjie Zhang, Xingyao Wang, Lifan Yuan, Hao Peng, and Alane Suhr. Tinyzero: Clean, minimal, accessible reproduction of deepseek r1-zero, Janurary 2025. [30] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, In-context retrieval-augmented language models. Transactions of the and Yoav Shoham. Association for Computational Linguistics, 11:13161331, 2023. [31] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015. [32] John Schulman, Xi Chen, and Pieter Abbeel. Equivalence between policy gradients and soft q-learning. arXiv preprint arXiv:1704.06440, 2017. [33] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [34] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [35] Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward gaming. Advances in Neural Information Processing Systems, 35:9460 9471, 2022. [36] Yixuan Tang and Yi Yang. Multihop-rag: Benchmarking retrieval-augmented generation for multi-hop queries. In First Conference on Language Modeling, 2024. [37] Gemini Team. Gemini 2.0 flash thinking, December 2024. [38] Gemini Team. Try deep research and our new experimental model in gemini, your ai assistant, December 2024. [39] NovaSky Team. Unlocking the potential of reinforcement learning in improving reasoning models, Feburary 2025. [40] Perplexity Team. Introducing perplexity deep research, February 2025. [41] Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown, November 2024. [42] Qwen Team. Qwen3: Think deeper, act faster, April 2025. [43] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. [44] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539554, 2022. [45] Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, Haosheng Zou, Yongchao Deng, Shousheng Jia, and Xiangzheng Zhang. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond. arXiv preprint arXiv:2503.10460, 2025. 22 [46] Siwei Wu, Zhongyuan Peng, Xinrun Du, Tuney Zheng, Minghao Liu, Jialong Wu, Jiachen Ma, Yizhi Li, Jian Yang, Wangchunshu Zhou, et al. comparative study on reasoning patterns of openais o1 model. arXiv preprint arXiv:2410.13639, 2024. [47] Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768, 2025. [48] Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective long-context scaling of foundation models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 46434663, 2024. [49] Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, et al. Towards large reasoning models: survey of reinforced reasoning with large language models. arXiv preprint arXiv:2501.09686, 2025. [50] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. [51] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23692380, 2018. [52] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations, 2023. [53] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387, 2025. [54] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [55] Yufeng Yuan, Yu Yue, Ruofei Zhu, Tiantian Fan, and Lin Yan. Whats behind ppos collapse in long-cot? value optimization holds the secret. arXiv preprint arXiv:2503.01491, 2025. [56] Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025. [57] Yilun Zhao, Yitao Long, Hongjun Liu, Ryo Kamoi, Linyong Nan, Lyuhao Chen, Yixin Liu, Xiangru Tang, Rui Zhang, and Arman Cohan. Docmath-eval: Evaluating math reasoning capabilities of llms in understanding long and specialized documents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1610316120, 2024. [58] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. [59] Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36:5500655021, 2023."
        }
    ],
    "affiliations": [
        "Qwen-Doc Team, Alibaba Group"
    ]
}