{
    "paper_title": "HiGS: History-Guided Sampling for Plug-and-Play Enhancement of Diffusion Models",
    "authors": [
        "Seyedmorteza Sadat",
        "Farnood Salehi",
        "Romann M. Weber"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While diffusion models have made remarkable progress in image generation, their outputs can still appear unrealistic and lack fine details, especially when using fewer number of neural function evaluations (NFEs) or lower guidance scales. To address this issue, we propose a novel momentum-based sampling technique, termed history-guided sampling (HiGS), which enhances quality and efficiency of diffusion sampling by integrating recent model predictions into each inference step. Specifically, HiGS leverages the difference between the current prediction and a weighted average of past predictions to steer the sampling process toward more realistic outputs with better details and structure. Our approach introduces practically no additional computation and integrates seamlessly into existing diffusion frameworks, requiring neither extra training nor fine-tuning. Extensive experiments show that HiGS consistently improves image quality across diverse models and architectures and under varying sampling budgets and guidance scales. Moreover, using a pretrained SiT model, HiGS achieves a new state-of-the-art FID of 1.61 for unguided ImageNet generation at 256$\\times$256 with only 30 sampling steps (instead of the standard 250). We thus present HiGS as a plug-and-play enhancement to standard diffusion sampling that enables faster generation with higher fidelity."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 0 0 3 2 2 . 9 0 5 2 : r HIGS: HISTORY-GUIDED SAMPLING FOR PLUG-ANDPLAY ENHANCEMENT OF DIFFUSION MODELS Seyedmorteza Sadat1, Farnood Salehi2, Romann M. Weber2 1ETH Zürich, 2DisneyResearchStudios {seyedmorteza.sadat}@inf.ethz.ch {farnood.salehi, romann.weber}@disneyresearch.com"
        },
        {
            "title": "ABSTRACT",
            "content": "While diffusion models have made remarkable progress in image generation, their outputs can still appear unrealistic and lack fine details, especially when using fewer number of neural function evaluations (NFEs) or lower guidance scales. To address this issue, we propose novel momentum-based sampling technique, termed history-guided sampling (HiGS), which enhances quality and efficiency of diffusion sampling by integrating recent model predictions into each inference step. Specifically, HiGS leverages the difference between the current prediction and weighted average of past predictions to steer the sampling process toward more realistic outputs with better details and structure. Our approach introduces practically no additional computation and integrates seamlessly into existing diffusion frameworks, requiring neither extra training nor fine-tuning. Extensive experiments show that HiGS consistently improves image quality across diverse models and architectures and under varying sampling budgets and guidance scales. Moreover, using pretrained SiT model, HiGS achieves new state-of-the-art FID of 1.61 for unguided ImageNet generation at 256256 with only 30 sampling steps (instead of the standard 250). We thus present HiGS as plug-and-play enhancement to standard diffusion sampling that enables faster generation with higher fidelity. CFG + HiGS (Ours) Portrait photo of female with red hair Figure 1: Sampling with diffusion models using fewer steps or lower guidance scales often results in blurry images with artifacts. We propose HiGS, novel sampling method that enhances quality and details in generated images under various sampling budgets and guidance scales by leveraging weighted average of the diffusion models past predictions at each inference step. The results shown are generated with Stable Diffusion 3.5 (Esser et al., 2024) using 10 steps and guidance scale of 1.2."
        },
        {
            "title": "INTRODUCTION",
            "content": "Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2021b) are class of generative models that learn the data distribution by reversing forward noising process that gradually transforms data points into Gaussian noise. Although in theory, the reverse process should yield high-quality samples from the target distribution, diffusion models can produce outputs that are blurry or lack details due to optimization errors and inaccurate estimations of the data distribution at intermediate time steps. This issue becomes more pronounced when using fewer sampling steps or lower classifier-free guidance scales (Karras et al., 2022). The generation process in diffusion models typically involves multiple neural function evaluations (NFEs), which are computationally expensive, especially for large-scale diffusion models containing billions of parameters (Esser et al., 2024). Reducing the number of NFEs reduces the sampling cost but leads to outputs that lack clarity, detail, and coherent global structures, as seen in Figure 1. Although various sampling methods have been proposed to reduce the required sampling steps (Lu et al., 2022; Karras et al., 2022), existing samplers still rely on relatively high step counts (e.g., 50) to achieve satisfactory results (Podell et al., 2023). Finding training-free methods that enable generating high-quality samples with fewer NFEs remains an open research question. In addition to iterative sampling, modern diffusion models use high guidance scales to enhance sample quality and prompt alignment (Podell et al., 2023; Nichol et al., 2022). Classifier-free guidance (CFG) (Ho et al., 2021) has proven essential for reducing outliers and improving generation quality (Karras et al., 2024). However, CFG doubles the network forward passes per sampling step, thereby increasing the computational cost of inference, and higher guidance scales often lead to oversaturation and reduced diversity (Sadat et al., 2025a; 2024). Consequently, further research is needed to enhance generation quality when using lower guidance scales combined with fewer NFEs. In this paper, We investigate the sampling process in diffusion models and propose training-free, momentum-based modification of inference that consistently improves global structure, detail, and sharpness across different sampling budgets and guidance scales. Inspired by momentum-based variance reduction in stochastic optimization (Cutkosky & Orabona, 2019), we introduce historyguided sampling (HiGS), novel method for improving the quality of diffusion models by leveraging the history of predictions made by the network. We demonstrate that this prediction history defines an effective guidance direction for steering the sampling trajectory toward higher-quality regions of the data distribution, especially under fewer NFEs or lower guidance scales. HiGS enables higher quality outputs and more efficient sampling from pretrained models while also improving the generation quality at lower guidance scales to avoid the drawbacks of high CFG scales. HiGS introduces no additional overhead to the sampling process and can be seamlessly integrated into existing diffusion models and samplers without extra training. Through extensive experiments across range of models and setups (including distilled diffusion models), we show that HiGS consistently improves sample quality, particularly in scenarios involving fewer sampling steps or lower guidance scales. Our results indicate that HiGS achieves higher quality metrics compared to standard sampling methods, establishing it as universal enhancement for pretrained diffusion models under various sampling budgets and guidance scales. Furthermore, using pretrained SiT model, HiGS achieves state-of-the-art FID of 1.61 for unguided (i.e., without CFG) ImageNet generation at 256256 while using only 30 sampling steps instead of the standard 250."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Score-based diffusion models (Song & Ermon, 2019; Song et al., 2021b; Sohl-Dickstein et al., 2015; Ho et al., 2020) learn complex data distributions by inverting forward process that gradually adds Gaussian noise to the data. These models have rapidly advanced generative modeling, surpassing prior approaches in both fidelity and diversity (Nichol & Dhariwal, 2021; Dhariwal & Nichol, 2021). They have demonstrated state-of-the-art performance across wide range of tasks, including unconditional and conditional image synthesis (Dhariwal & Nichol, 2021; Karras et al., 2022; Yu et al., 2025b; Karras et al., 2024), text-to-image generation (Podell et al., 2023; Esser et al., 2024; Labs, 2024; Qin et al., 2025), video generation (Blattmann et al., 2023b;a; Bar-Tal et al., 2024; Wan et al., 2025), image-to-image translation (Saharia et al., 2022a; Liu et al., 2023a; Xia et al., 2023), and audio generation (Chen et al., 2021; Huang et al., 2023; Liu et al., 2023b; Tian et al., 2025). 2 Since the introduction of DDPM (Ho et al., 2020), the field has seen substantial progress, with improvements spanning network architectures (Hoogeboom et al., 2023; Karras et al., 2023; Peebles & Xie, 2022; Dhariwal & Nichol, 2021), sampling techniques (Song et al., 2021a; Karras et al., 2022; Liu et al., 2022; Lu et al., 2022; Salimans & Ho, 2022), and training strategies (Nichol & Dhariwal, 2021; Karras et al., 2022; Song et al., 2021b; Salimans & Ho, 2022; Rombach et al., 2022). Despite these advancements, sampling from diffusion models still require relatively high step counts, and various guidance mechanismssuch as classifier guidance (Dhariwal & Nichol, 2021) and classifier-free guidance (Ho & Salimans, 2022)remain critical for enhancing image quality and ensuring strong prompt alignment (Nichol et al., 2022). recent line of work has focused on developing better ODE solvers for the diffusion sampling process, often combined with improved training techniques to make the sampling ODE more linear (Lu et al., 2022; Karras et al., 2022; Esser et al., 2024). Despite these advances, most state-of-the-art samplers still require relatively high number of sampling steps (e.g., 50 steps for Stable Diffusion XL (Podell et al., 2023)). Another direction of research explores distilling the diffusion model into student network capable of sampling with fewer steps (Salimans & Ho, 2022; Song et al., 2023; Sauer et al., 2024). However, step distillation remains computationally expensive, requiring long training on advanced hardware. We show that HiGS serves as training-free method to improve generation quality across various sampling budgets and networks (including distilled diffusion models). Modern diffusion models often rely on high guidance scales to achieve strong image quality and prompt alignment. However, CFG doubles inference cost, and excessive CFG scales reduce diversity and cause oversaturation (Sadat et al., 2024; 2025a). On the other hand, sampling with lower CFG scales and NFEs avoid these issues but typically yield blurry images lacking fine detail and coherent structure. While several methods mitigate the drawbacks of high CFG scales (Kynkäänniemi et al., 2024; Sadat et al., 2025a; Wang et al., 2024), little attention has been given to improving quality under low CFG scales and limited sampling steps. We show that HiGS enhances generation across both low and high CFG regimes, offering benefits under varying CFG scales and sampling budgets. In summary, sampling from diffusion models is an expensive iterative process that might produce unrealistic outputs under certain configurations. We propose training-free method that improves generation quality across different sampling budgets and guidance scales, particularly in low-NFE and low-CFG scenarios. Thus, we present HiGS as plug-and-play enhancement for diffusion sampling."
        },
        {
            "title": "3 BACKGROUND",
            "content": "This section provides brief overview of diffusion models. Let pdata(x) denote data sample, and let [0, ] represent continuous time. The forward diffusion process gradually corrupts data by adding Gaussian noise: zt = + σ(t)ϵ, where ϵ (0, III) and σ(t) defines monotonically increasing noise schedule, satisfying σ(0) = 0 and σ(T ) = σmax σdata. As shown by Karras et al. (2022), this forward process can be described by the following ordinary differential equation (ODE): dz = σ(t)σ(t) zt log pt(zt)dt = zt log pt(zt)dt, where we choose σ(t) = t, and pt(zt) is the distribution of noisy data at time t, with p0 = pdata and pT = (0, σ2 maxIII). If the time-dependent score function zt log pt(zt) is known, one can sample from pdata by integrating the ODE (or its stochastic counterpart) in reverse, from = to = 0. (1) In practice, the score function is approximated using neural denoiser Dθ(zt, t), trained to recover the clean data from the noisy input zt. Conditional generation is supported by extending the denoiser to take an additional input (e.g., class labels or text), resulting in Dθ(zt, t, y). Classifier-free guidance (CFG) Classifier-free guidance (CFG) is technique for enhancing sample quality during inference by interpolating between unconditional and conditional model predictions (Ho & Salimans, 2022). Given an unconditional prediction Dθ(zt, t), the guided denoiser output at each sampling step is computed as: DCFG(zt, t, y) = wCFGDθ(zt, t, y) (wCFG 1)Dθ(zt, t), (2) where wCFG = 1 corresponds to no guidance, and larger values increase conditioning strength. The unconditional model is typically trained by randomly dropping the condition with some probability [0.1, 0.2] during training. Alternatively, separate network or the conditional model itself can 3 be used to estimate the unconditional score (Karras et al., 2023; Sadat et al., 2025b). While CFG is known to improve perceptual quality, it often comes at the cost of increased oversaturation and reduced sample diversity (Sadat et al., 2025a; 2024)."
        },
        {
            "title": "4 SAMPLING WITH PREDICTION HISTORY",
            "content": "Let t0 > t1 > . . . > tM be the sampling time grid with +1 steps. The model prediction at time step tk is denoted by Dθ(ztk , tk, y). Given window of size 1, we define the history Hk at step as the set of past predictions prior to tk, i.e., Hk := {Dθ(zti , ti, y)}iIk for Ik := {max(0, ), . . . , 1}. We next show how leveraging this history enhances sampling quality. For simplicity, . . we define Dc(ztk ) = DCFG(ztk , tk, y) = Dθ(ztk , tk), and DCFG(ztk ) to represent the conditional, unconditional, and CFG outputs, respectively. . = Dθ(ztk , tk, y), Du(ztk )"
        },
        {
            "title": "4.1 MOTIVATION",
            "content": "We first claim that the Euler sampler for diffusion models can be interpreted as performing stochastic gradient descent (SGD) on time-varying energy function. To show this, consider the ODE in Equation (1) and discretization {t0, t1, . . . , tM }. single Euler step at time tk can be written as ztk+1 = ztk + tk(tk tk+1) ztk = ztk tk(tk tk+1) ztk where the energy function Et(zt) is defined via pt(zt) = 1 exp(Et(zt)) for some normalization constant Z. This shows that each Euler step in diffusion sampling corresponds to step of gradient descent on the time-dependent energy Et(zt) with learning rate tk(tk tk+1). log ptk (ztk ) Etk (ztk ), (3) (4) Motivated by this new perspective, we argue that we can enhance this gradient estimate to improve the efficiency and quality of the sampling process in diffusion models. One promising approach is to augment the gradient with an additional momentum-based term similar to STORM (Cutkosky & Orabona, 2019), variance-reduction method for non-convex optimization. Specifically, given differentiable function , the momentum term in STORM is defined as (ztk ) (ztk1 ), which incorporates information from consecutive steps for more stable updates. Applying this to Equation (3), we obtain Etk (ztk ) Etk1 (ztk1 ) as the momentum term. Equivalently, this can be seen as incorporating the residual of past score terms or model outputs, since the score function corresponds to the energy gradient. In the following, we further generalize this idea to incorporate multiple past predictions rather than only the previous step (similar to multistep ODE solvers (Atkinson et al., 2009)). An alternative perspective is given in Appendix A, where we connect classifier-free guidance to gradient ascent on specific objective. In Appendix B, we show that the history terms reduce the Euler solvers local truncation error from O(h2 k), where hk = tk tk+1, thereby improving the global error from O(h) to O(h2) with = maxk hk. k) to O(h"
        },
        {
            "title": "4.2 HISTORY-GUIDED SAMPLING",
            "content": "Building on the insights discussed above, we propose novel sampling method for diffusion models, termed history-guided sampling (HiGS), which integrates past predictions into the current sampling step. We leverage the set of past predictions Hk at each sampling step tk to improve generation quality such as sharpness, details and global structure. We detail each step of HiGS below. Buffer input When the sampling is done with CFG, we can choose to keep track of the CFG-guided predictions DCFG(ztk ) or the conditional outputs Dc(ztk ) as the history. We found that the CFGguided predictions are more effective in improving the quality of sampling. Thus, we use DCFG(ztk ) as the inputs to Hk when CFG is enabled, leading to Hk := {DCFG(zti )}iIk . Incorporating the history The next design choice is how to use Hk during sampling. Specifically, we want function that determines the influence of past predictions on the current step. By generalizing the momentum update rule in STORM, we adopt an EMA-style weighted average that emphasizes recent predictions: g(Hk) = (cid:88) iIk α(1 α)k1iDCFG(zti ), (5) 4 where α (0, 1) is hyperparameter. This formulation integrates information from history while prioritizing recent steps for more informative guidance. We later show that several alternative definitions of g, such as simple averaging, are viable options (see Appendix for more details). Let Dtk = DCFG(ztk ) g(Hk) denote the guidance term in HiGS. straightforward strategy for using Dtk at inference is to combine this update with the current output, analogous to CFG, via DCFG(ztk )+wHiGSDtk with scale wHiGS. In the following, we introduce several improvements over this naive baseline that we found crucial to substantially boost performance. Appendix presents extensive ablation studies to support various design choices of HiGS. Scheduling the guidance weight In our experiments, the benefits of HiGS were most evident during the early and middle sampling steps. We noticed that as sampling progresses, the update term introduces diminishing improvements and may even cause noisy artifacts. To address this, we use weight schedule wHiGS(tk) that adapts the scale of the guidance term according to the time step tk. We employ square-root schedule given by wHiGS(t) = 0 wHiGS 0 (cid:113) ttmin tmaxtmin tmin, tmin < tmax, > tmax. (6) Optional orthogonal projection Additionally, we found that it is sometimes beneficial to project the update vector Dtk on DCFG(ztk ) and downweight its parallel component to prevent oversaturation and color artifacts (especially at higher values of wHiGS) (Sadat et al., 2025a). The parallel component in Dtk can be computed via Dtk = Dtk , DCFG(ztk ) DCFG(ztk ), DCFG(ztk ) DCFG(ztk ), (7) and the orthogonal component is computed via = DCFG(ztk ) Dtk . We use Dtk (η) = tk + ηDtk as the projected update direction, where η [0, 1] is hyperparameter. As tk demonstrated in Figure 11 (appendix), the orthogonal projection step can mitigate oversaturation and color artifacts in the generated outputs. Frequency-domain filtering We further observed that using Dtk (η) as the guidance term generally leads to unrealistic color compositions in generations (see Figure 15 in the appendix). To solve this, we employ frequency-based high-pass filtering using the discrete cosine transform (DCT). Since color composition corresponds to low-frequency contents of an image, we apply DCT to the update term Dtk (η) and attenuate its low-frequency signals with sigmoid high-pass filter: H(R) = Sigmoid(λ(R Rc)), (8) (cid:113) + 2 2 is the radial frequency at coordinates (x, y), Rc is the cutoff threshold, and λ where = controls the sharpness of the transition. This procedure effectively removes the color shifts, leading to more realistic and visually consistent outputs. Accordingly, the final update rule for HiGS is DHiGS(ztk ) = DCFG(ztk ) + wHiGS(tk) iDCT(cid:0)H(R) DCT(Dtk (η))(cid:1). (9)"
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "We now provide extensive qualitative and quantitative comparisons between standard sampling and sampling with HiGS to show that HiGS enhances the performance of diffusion models under various setups. Further experiments, ablations, and implementation details are provided in the appendix. Setup We evaluate our method primarily on text-to-image generation using Stable Diffusion models (Rombach et al., 2022; Podell et al., 2023; Esser et al., 2024), and on class-conditional generation with ImageNet (Russakovsky et al., 2015) using DiT-XL/2 (Peebles & Xie, 2022), and SiT-XL + REPA (Yu et al., 2025a; Leng et al., 2025). For each model, we employ the default sampling algorithms (e.g., the Euler solver for Stable Diffusion XL) and rely on the official pretrained checkpoints and publicly released codebases to ensure alignment with the original implementations. Additional details regarding the experimental configurations and hyperparameters are provided in Appendix F."
        },
        {
            "title": "CFG",
            "content": "+ HiGS (Ours)"
        },
        {
            "title": "CFG",
            "content": "+ HiGS (Ours) blue car ballet dancer next to waterfall"
        },
        {
            "title": "CFG",
            "content": "+ HiGS (Ours)"
        },
        {
            "title": "CFG",
            "content": "+ HiGS (Ours) pizza modern glass skyscraper at sunset Figure 2: Effect of HiGS on generated samples using standard sampling setups. Compared with CFG outputs, HiGS produces sharper images with improved structure and fewer artifacts. Evaluation metrics For text-to-image generation, we primarily use HPSv2 (Wu et al., 2023) as our main quality and prompt alignment metric, as we found it to be the most aligned with human judgment. We also report ImageReward (Xu et al., 2023) and the HPSv2 win rate as additional preference scores, along with the CLIP Score (Hessel et al., 2021) for completeness. For class-conditional models, we use the Fréchet Inception Distance (FID) (Heusel et al., 2017) as the primary metric to assess both image fidelity and diversity, given its strong alignment with human visual perception in this setting. To ensure fair comparisons, all FID evaluations are conducted under standardized setup to minimize sensitivity to implementation differences. We also report Inception Score (IS) (Salimans et al., 2016), Precision (Kynkäänniemi et al., 2019), and Recall (Kynkäänniemi et al., 2019) as complementary metrics to separately evaluate sample quality and diversity. 5.1 MAIN RESULTS Qualitative comparisons We first qualitatively evaluate the impact of HiGS on generation quality under three different regimes: the normal setup using practical CFG scales with high NFEs (Figure 2), sampling with fewer number of NFEs (Figure 3), and sampling under low CFG scales (Figure 4). We note that HiGS is able to improve generation quality under all these settings, showing that its benefits cover wide range of CFG scales and sampling budgets. HiGS vs CFG scale Next, we quantitatively evaluate the effect of adding HiGS to the sampling process on generation quality across different CFG scales wCFG. Figure 5a shows that HiGS is beneficial across all guidance scales, maintaining significant margin in terms of HPS win rate. HiGS vs the number of sampling steps Similarly, Figure 5b shows that HiGS outperforms the CFG baseline in generation quality across all NFEs. This demonstrates that HiGS is beneficial for all sampling budgets, and its advantages are not limited to specific number of sampling steps. Comparison with different models Furthermore, we evaluate the impact of HiGS on output quality across different models and metrics in Tables 1 and 2, using fixed guidance scale and sampling steps per baseline for both sampling with and without HiGS to ensure fair comparison. As before, HiGS consistently improves generation quality across diverse setups and metrics, indicating that it serves as universal enhancement to standard diffusion sampling."
        },
        {
            "title": "CFG",
            "content": "+ HiGS (Ours)"
        },
        {
            "title": "CFG",
            "content": "+ HiGS (Ours) pink dog basket of macaroons"
        },
        {
            "title": "CFG",
            "content": "+ HiGS (Ours)"
        },
        {
            "title": "CFG",
            "content": "+ HiGS (Ours) Close-up photo of cupcake Detailed photo of lion Figure 3: Comparison of standard sampling and HiGS under fewer number NFEs. HiGS outputs exhibit significantly better global structure, sharpness, and details than the baseline. CFG + HiGS (Ours) CFG + HiGS (Ours) Glossy red teapot on reflective surface Photoreal marble bust, studio render lighting CFG + HiGS (Ours) CFG + HiGS (Ours) Cafe latte in round red cup and saucer stack of buttermilk pancakes Figure 4: Illustration of the effect of HiGS on generated outputs under lower guidance scales. HiGS leads to noticeable improvements in image quality and details compared to the baseline. HPSv2 Win Rate HPSv2 Win Rate 0.29 0. 0.27 0.26 0.25 0.24 0.8 0. 0.4 0.2 0.28 0.27 0.26 0. 0.24 0.23 0.8 0.6 0.4 0. CFG HiGS 1.5 2 2.5 3 3.5 wCFG 4.5 5 1.5 2 2.5 3 3.5 wCFG 4 4.5 5 10 15 20 # Steps 25 30 10 15 20 # Steps 30 (a) Changing the guidance scale (b) Changing the number of sampling steps Figure 5: Effect of HiGS across guidance scales and sampling budgets using Stable Diffusion 3 (Esser et al., 2024). HiGS consistently outperforms standard sampling with CFG in all settings, highlighting its effectiveness in improving generation quality under varying guidance scales and sampling budgets. Table 1: Quantitative evaluation of the effect of HiGS on sampling. HiGS consistently improves all metrics across different models, demonstrating higher generation quality than the CFG baseline. For fairness, sampling with and without HiGS is performed using the same NFE and CFG scale. Model SiT-XL + REPA (Yu et al., 2025a) DiT-XL/2 (Peebles & Xie, 2022) Stable Diffusion XL (Podell et al., 2023) Stable Diffusion 3 (Esser et al., 2024) Guidance CFG HiGS (Ours) CFG HiGS (Ours) CFG +HiGS (Ours) CFG +HiGS (Ours) FID 12.08 4.86 8.73 7. 28.49 26.18 27.19 26.84 IS Precision Recall 187.11 277.20 173.21 180. 35.07 36.22 40.11 40.94 0.68 0.80 0.72 0.75 0.56 0.59 0.73 0. 0.73 0.70 0.68 0.71 0.54 0.57 0.41 0.42 Table 2: Evaluation of using HiGS with various Stable Diffusion models. HiGS improves all metrics reflecting human preference for image quality and prompt alignment across multiple benchmarks. All comparisons are conducted with the same NFE and CFG scale for fairness. Benchmark Model Guidance ImageReward HPSv2 Win Rate DrawBench (Saharia et al., 2022b) Stable Diffusion 3 Stable Diffusion XL Stable Diffusion 3.5 Stable Diffusion XL Parti Prompts (Yu et al., 2022) Stable Diffusion 3 Stable Diffusion 3.5 Stable Diffusion XL HPS Prompts (Wu et al., 2023) Stable Diffusion 3 Stable Diffusion 3.5 CFG +HiGS (Ours) CFG +HiGS (Ours) CFG +HiGS (Ours) CFG +HiGS (Ours) CFG +HiGS (Ours) CFG +HiGS (Ours) CFG +HiGS (Ours) CFG +HiGS (Ours) CFG +HiGS (Ours) -0.091 0.148 0.491 0.621 0.621 0.702 0.191 0.360 0.843 0.919 0.879 0. 0.327 0.515 0.820 0.901 0.821 0.889 0.224 0.249 0.257 0.272 0.258 0. 0.239 0.261 0.273 0.285 0.270 0.282 0.245 0.275 0.279 0.291 0.274 0. 0.07 0.93 0.18 0.82 0.21 0.79 0.08 0.92 0.19 0.81 0.18 0. 0.04 0.96 0.21 0.79 0.18 0.82 5.2 IMAGENET BENCHMARK Table 3 shows that HiGS improves the performance of recent state-of-the-art models on conditional ImageNet generation at 256256 resolution. For unguided generation (i.e., without CFG), HiGS 8 Table 3: Effect of adding HiGS to recent state-of-the-art methods for conditional ImageNet generation at 256256. HiGS significantly improves sampling speed, matching the FID of the original models in just 3040 steps. Moreover, HiGS achieves new state-of-the-art FID of 1.61 for conditional generation without CFG using the SiT-XL + REPA-E model (Leng et al., 2025). Model Guidance # Steps FID IS Precision Recall REPA-E (Leng et al., 2025) REPA (Yu et al., 2025a) Unguided +HiGS (Ours) CFG +HiGS (Ours) Unguided +HiGS (Ours) CFG +HiGS (Ours) 250 30 250 40 250 40 250 1.83 1.61 1.26 1.32 5.90 5.43 1.42 1.44 217.30 240.75 314.90 306. 157.80 165.91 305.70 306.80 0.77 0.81 0.79 0.80 0.70 0.71 0.80 0. 0.66 0.62 0.66 0.65 0.69 0.68 0.65 0.64 Table 4: Effect of adding HiGS to the sampling process of distilled models. HiGS improves the quality of these models, showing that its effects are complementary to diffusion distillation. Model Guidance ImageReward HPSv2 Win Rate CLIP Score SDXL-Flash (SD-Community) SDXL-Lightning (Lin et al., 2024) CFG +HiGS (Ours) CFG +HiGS (Ours) 0.774 0.864 0.63 0.66 0.273 0.298 0.277 0.285 0.03 0.97 0.18 0. 0.332 0.333 0.318 0.317 reduces the state-of-the-art FID from 1.83 to 1.61 using only 30 sampling steps, without any retraining of the base model. Moreover, by improving sample quality at lower NFEs, HiGS provides trainingfree way to accelerate sampling from existing models. For guided generation with CFG, HiGS matches the performance of the base models in just 40 sampling steps, compared to 250 steps with the default sampler. Finally, this experiment shows that HiGS is compatible with guidance interval (Kynkäänniemi et al., 2024), as both CFG baselines incorporate this technique in their samplers. 5.3 COMPATIBILITY WITH DISTILLED MODELS Finally, we also demonstrate that HiGS is compatible with distilled diffusion models that use fewer sampling steps. Diffusion distillation reduces sampling steps by training student model to replicate the performance of the base model. As shown in Table 4, HiGS enhances the sampling quality of these distilled models, achieving significant win rate according to the HPS score. This shows that the benefits of HiGS are complementary to diffusion distillation, and that HiGS can be applied as training-free method to further enhance the quality of distilled diffusion models."
        },
        {
            "title": "6 DISCUSSION AND CONCLUSION",
            "content": "In this work, we presented history-guided sampling (HiGS), simple, training-free modification of diffusion sampling that leverages the history of model predictions to steer the reverse process toward higher quality and more coherent images. By applying history-informed correction that emphasizes the deviation between the current prediction and weighted average of past predictions, HiGS mitigates blur and artifacts that often arise during sampling, particularly with fewer number of NFEs or lower CFG scales. HiGS requires no additional network evaluations, integrates seamlessly with existing samplers and architectures, and is fully plug-and-play. Our experiments demonstrated that HiGS consistently improves perceptual quality and fidelity across diverse models and sampling budgets, with especially strong benefits in low-NFE and low-CFG regimes. While highly effective, HiGS still inherits, albeit to lesser extent, the biases and some limitations of the underlying diffusion models, and addressing these challenges remains promising direction for future work."
        },
        {
            "title": "BROADER IMPACT STATEMENT",
            "content": "Our method has the potential to improve the realism and quality of outputs produced by diffusion models without requiring expensive retraining, making it practically valuable for applications in visual content creation. However, as generative modeling technologies continue to evolve, the ease of producing and distributing synthetic or misleading content also increases. While such advances can significantly boost creativity and productivity, they also raise important ethical and societal concerns. It is therefore essential to raise awareness about the potential misuse of generative models and to carefully consider the broader societal implications of their deployment. For an in-depth discussion on ethics and creativity in generative modeling, we refer readers to Lin & Losavio (2025)."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "Our work builds upon the official implementations of the pretrained models cited in the main text. Algorithm 1 gives the algorithm for applying HiGS at inference, and the pseudocode for HiGS is presented in Algorithms 2 and 3. Additional implementation details, including the hyperparameters used in the main experiments, are given in Appendix F."
        },
        {
            "title": "REFERENCES",
            "content": "Kendall Atkinson, Weimin Han, and David Stewart. Numerical solution of ordinary differential equations. John Wiley & Sons, 2009. Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, et al. Lumiere: space-time diffusion model for video generation. In SIGGRAPH Asia 2024 Conference Papers, pp. 111, 2024. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets. CoRR, abs/2311.15127, 2023a. doi: 10.48550/ARXIV.2311.15127. URL https://doi.org/10.48550/arXiv. 2311.15127. Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2256322575, 2023b. Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad Norouzi, and William Chan. Wavegrad: Estimating gradients for waveform generation. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=NsMLjcFaO8O. Ashok Cutkosky and Francesco Orabona. Momentum-based variance reduction in non-convex sgd. Advances in neural information processing systems, 32, 2019. Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat gans on image synthesis. In MarcAurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (eds.), Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 87808794, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/ 49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. arXiv preprint arXiv:2403.03206, 2024. E. Hairer, S. P. Nørsett, and G. Wanner. Solving ordinary differential equations (2nd revised. ed.): nonstiff problems. Springer-Verlag, Berlin, Heidelberg, 1993. ISBN 0387566708. 10 Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: referencefree evaluation metric for image captioning. CoRR, abs/2104.08718, 2021. URL https://arxiv.org/ abs/2104.08718. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 66266637, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ 8a1d694707eb0fefe65871369074926d-Abstract.html. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. CoRR, abs/2207.12598, 2022. doi: 10.48550/arXiv.2207.12598. URL https://doi.org/10.48550/arXiv.2207.12598. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Hugo Larochelle, MarcAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https: //proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html. Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. CoRR, abs/2106.15282, 2021. URL https://arxiv.org/abs/2106.15282. Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution images. CoRR, abs/2301.11093, 2023. doi: 10.48550/arXiv.2301.11093. URL https://doi.org/10.48550/arXiv.2301.11093. Qingqing Huang, Daniel S. Park, Tao Wang, Timo I. Denk, Andy Ly, Nanxin Chen, Zhengdong Zhang, Zhishuai Zhang, Jiahui Yu, Christian Havnø Frank, Jesse H. Engel, Quoc V. Le, William Chan, and Wei Han. Noise2music: Text-conditioned music generation with diffusion models. CoRR, abs/2302.03917, 2023. doi: 10.48550/arXiv.2302.03917. URL https://doi.org/10.48550/ arXiv.2302.03917. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models. 2022. URL https://openreview.net/forum?id=k7FuTOWMOc7. Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the training dynamics of diffusion models, 2023. Tero Karras, Miika Aittala, Tuomas Kynkäänniemi, Jaakko Lehtinen, Timo Aila, and Samuli Laine. Guiding diffusion model with bad version of itself. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=bg6fVPVs3s. Tuomas Kynkäänniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence dAlché-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 39293938, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/ 0234c510bc6d908b28c70ff313743079-Abstract.html. Tuomas Kynkäänniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, and Jaakko Lehtinen. Applying guidance in limited interval improves sample and distribution quality in diffusion models. CoRR, abs/2404.07724, 2024. doi: 10.48550/ARXIV.2404.07724. URL https://doi.org/ 10.48550/arXiv.2404.07724. Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. Xingjian Leng, Jaskirat Singh, Yunzhong Hou, Zhenchang Xing, Saining Xie, and Liang Zheng. Repa-e: Unlocking vae for end-to-end tuning with latent diffusion transformers. arXiv preprint arXiv:2504.10483, 2025. 11 Shanchuan Lin, Anran Wang, and Xiao Yang. Sdxl-lightning: Progressive adversarial diffusion distillation, 2024. Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. In David J. Fleet, Tomás Pajdla, Bernt Schiele, and Tinne Tuytelaars (eds.), Computer Vision - ECCV 2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V, volume 8693 of Lecture Notes in Computer Science, pp. 740755. Springer, 2014. doi: 10.1007/ 978-3-319-10602-1_48. URL https://doi.org/10.1007/978-3-319-10602-1_48. Xiaojian Lin and Michael Losavio. comprehensive survey on bias and fairness in generative ai: Legal, ethical, and technical responses. Ethical, and Technical Responses (March 04, 2025), 2025. Guan-Horng Liu, Arash Vahdat, De-An Huang, Evangelos A. Theodorou, Weili Nie, and Anima Anandkumar. I2sb: Image-to-image schrödinger bridge. CoRR, abs/2302.05872, 2023a. doi: 10.48550/arXiv.2302.05872. URL https://doi.org/10.48550/arXiv.2302.05872. Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark Plumbley. Audioldm: Text-to-audio generation with latent diffusion models. arXiv preprint arXiv:2301.12503, 2023b. Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id= PlKWVd2yBkY. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. solver: fast ODE solver 10 steps. 260a14acce2a89dad36adc8eefe7c59e-Abstract-Conference.html. In NeurIPS, 2022. for diffusion probabilistic model Dpmsampling in around URL http://papers.nips.cc/paper_files/paper/2022/hash/ Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 81628171. PMLR, 2021. URL http://proceedings.mlr.press/ v139/nichol21a.html. Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: towards photorealistic image generation and editing with text-guided diffusion models. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato (eds.), International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 1678416804. PMLR, 2022. URL https://proceedings.mlr.press/ v162/nichol22a.html. William Peebles and Saining Xie. Scalable diffusion models with transformers. CoRR, abs/2212.09748, 2022. doi: 10.48550/arXiv.2212.09748. URL https://doi.org/10.48550/arXiv. 2212.09748. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. SDXL: improving latent diffusion models for high-resolution image synthesis. CoRR, abs/2307.01952, 2023. doi: 10.48550/ARXIV.2307.01952. URL https: //doi.org/10.48550/arXiv.2307.01952. Qi Qin, Le Zhuo, Yi Xin, Ruoyi Du, Zhen Li, Bin Fu, Yiting Lu, Jiakang Yuan, Xinyue Li, Dongyang Liu, et al. Lumina-image 2.0: unified and efficient image generative framework. arXiv preprint arXiv:2503.21758, 2025. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp. 1067410685. IEEE, 2022. doi: 10.1109/CVPR52688.2022.01042. URL https://doi.org/10.1109/ CVPR52688.2022.01042. 12 Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. Int. J. Comput. Vis., 115(3):211252, 2015. doi: 10.1007/s11263-015-0816-y. URL https://doi.org/10.1007/s11263-015-0816-y. Seyedmorteza Sadat, Jakob Buhmann, Derek Bradley, Otmar Hilliges, and Romann M. Weber. CADS: Unleashing the diversity of diffusion models through condition-annealed sampling. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=zMoNrajk2X. Seyedmorteza Sadat, Otmar Hilliges, and Romann M. Weber. Eliminating oversaturation and artifacts In The Thirteenth International Conference on of high guidance scales in diffusion models. Learning Representations, 2025a. URL https://openreview.net/forum?id=e2ONKX6qzJ. Seyedmorteza Sadat, Manuel Kansy, Otmar Hilliges, and Romann M. Weber. No training, no problem: Rethinking classifier-free guidance for diffusion models. In The Thirteenth International Conference on Learning Representations, 2025b. URL https://openreview.net/forum?id=b3CzCCCILJ. Chitwan Saharia, William Chan, Huiwen Chang, Chris A. Lee, Jonathan Ho, Tim Salimans, David J. Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In Munkhtsetseg Nandigjav, Niloy J. Mitra, and Aaron Hertzmann (eds.), SIGGRAPH 22: Special Interest Group on Computer Graphics and Interactive Techniques Conference, Vancouver, BC, Canada, August 7 - 11, 2022, pp. 15:115:10. ACM, 2022a. doi: 10.1145/3528233.3530757. URL https://doi.org/10. 1145/3528233.3530757. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. 2022b. URL http://papers.nips.cc/paper_files/paper/2022/hash/ ec795aeadae0b7d230fa35cbaf04c041-Abstract-Conference.html. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=TIdIXIpzhoI. Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pp. 22262234, 2016. URL https://proceedings.neurips.cc/paper/2016/hash/ 8a3363abe792db2d8761d6403605aeb7-Abstract.html. Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast high-resolution image synthesis with latent adversarial diffusion distillation. In Takeo Igarashi, Ariel Shamir, and Hao (Richard) Zhang (eds.), SIGGRAPH Asia 2024 Conference Papers, SA 2024, Tokyo, Japan, December 3-6, 2024, pp. 106:1106:11. ACM, 2024. doi: 10.1145/3680528.3687625. URL https://doi.org/10.1145/3680528.3687625. SD-Community. Sdxl flash in collaboration with project fluently. https://huggingface.co/ sd-community/sdxl-flash. Accessed: 2024-09-08. Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. 37:22562265, 2015. URL http://proceedings. mlr.press/v37/sohl-dickstein15.html. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021a. URL https://openreview.net/forum?id=St1giarCHLP. Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence dAlché-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual 13 Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 1189511907, 2019. URL https://proceedings.neurips.cc/paper/2019/ hash/3001ef257407d5a371a96dcd947c7d93-Abstract.html. Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021b. URL https://openreview.net/forum?id=PxTIG12RRHS. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 32211 32252. PMLR, 2023. URL https://proceedings.mlr.press/v202/song23a.html. Zeyue Tian, Yizhu Jin, Zhaoyang Liu, Ruibin Yuan, Xu Tan, Qifeng Chen, Wei Xue, and Yike Guo. Audiox: Diffusion transformer for anything-to-audio generation, 2025. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Xi Wang, Nicolas Dufour, Nefeli Andreou, Marie-Paule Cani, Victoria Fernández Abrevaya, David Picard, and Vicky Kalogeiton. Analysis of classifier-free guidance weight schedulers. Trans. Mach. Learn. Res., 2024, 2024. URL https://openreview.net/forum?id=SUMtDJqicd. Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. Bin Xia, Yulun Zhang, Shiyin Wang, Yitong Wang, Xinglong Wu, Yapeng Tian, Wenming Yang, Radu Timotfe, and Luc Van Gool. Diffi2i: Efficient diffusion model for image-to-image translation, 2023. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:1590315935, 2023. Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation. Trans. Mach. Learn. Res., 2022, 2022. URL https://openreview.net/ forum?id=AFDcYJKhND. Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. In International Conference on Learning Representations, 2025a. Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. 2025b. Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and Jiwen Lu. Unipc: unified predictorcorrector framework for fast sampling of diffusion models. CoRR, abs/2302.04867, 2023. doi: 10.48550/arXiv.2302.04867. URL https://doi.org/10.48550/arXiv.2302.04867."
        },
        {
            "title": "A CFG AS GRADIENT ASCENT",
            "content": "This section provides an alternative intuition behind HiGS. Recall that CFG update at time step tk can be written in the alternate form DCFG(ztk ) = Dc(ztk ) + (wCFG 1) (Dc(ztk ) Du(ztk )), (10) which can be interpreted as single gradient ascent step on the objective fCFG(Dc(ztk ), Du(ztk )) = 1 2 w.r.t. the model output Dc(ztk ) (Sadat et al., 2025a). Here, sg[] denotes the stop-gradient operation. This suggests that we can augment the gradient update in CFG with history-based momentum term similar to STORM (Cutkosky & Orabona, 2019). For differentiable function , STORM uses the gradient difference (ztk ) (ztk1 ) to incorporate information from consecutive steps. Applying this to the CFG objective from Equation (11), we have Dc(ztk ) sg[Du(ztk )]2 (11) fCFG(ztk ) = Dc(ztk ) Du(ztk ), fCFG(ztk1 ) = Dc(ztk1 ) Du(ztk ), (12) (13) so the STORM momentum reduces to fCFG(ztk ) fCFG(ztk1 ) = Dc(ztk ) Dc(ztk1 ). This suggests enhancing the guidance direction with history term along Dc(ztk ) Dc(ztk1 ). In Section 4, we generalized this idea to incorporate multiple past predictions rather than only Dc(ztk1 ) and introduced several refinements that further improve generation quality. (14)"
        },
        {
            "title": "B ERROR ANALYSIS OF HIGS",
            "content": "We now show that adding HiGS to the Euler solver for diffusion models can improve the convergence rate of its local truncation error, and hence leading to better global estimates with fewer sampling steps. Let u(zt, t) = zt log pt(zt). The sampling ODE for diffusion models is then equal to dz = u(zt, t)dt. The Euler step for this ODE at time step tk is equal to (15) ˆztk+1 = ˆztk + (tk tk+1)u( ˆztk , tk) = ˆztk + hku( ˆztk , tk), where hk = tk tk+1 is the step size, and ˆzt0 (cid:0)000, σ2 we follow an update similar to HiGS based on the previous update u(ztk1 , tk1), i.e., we use max (16) (cid:1). Now, assume that instead of u(ztk , tk), u(ztk , tk) = u(ztk , tk) + wk(u(ztk , tk) u(ztk1 , tk1)) (17) for some time dependent weight schedule wk = w(tk). Theorem B.1. Let u(z, t) be sufficiently smooth in both arguments with bounded derivatives, and let hk = tk tk+1 denote the variable step size for time grid t0 > t1 > . . . > tM with + 1 steps. Then, at each time step tk, the Euler update in Equation (15) has local truncation error O(h2 k). In contrast, there exists weight wk such that the modified update rule of Equation (17) used in HiGS, yields local truncation error O(h3 k). Consequently, the global error improves from O(h) for Euler to O(h2) with HiGS, where = maxk(tk tk+1). Proof. Let z(t) denote the ground truth solution to the sampling ODE (derived by integration). For simplicity, we define zk = z(tk). Using Taylor expansion, we get zk+1 = zk + dz dt (tk)(tk+1 tk) + 1 2 d2z dt2 (ζ1)(tk+1 tk)2 for some ζ1 [tk+1, tk]. Since we have dz zk+1 = zk (tk+1 tk)u(zk, tk) + dt = u(z, t), we get d2z dt2 (ζ1)(tk+1 tk) 1 2 = zk + hku(zk, tk) (z(ζ1), ζ1). h2 2 du dt (18) (19) (20) Accordingly, the local truncation error Lk of the euler method is given by Lk = zk+1 (zk + hku(zk, tk)) = h2 (cid:13) (cid:13) (cid:13) (cid:13) du dt (cid:13) (cid:13) (cid:13) (cid:13) (z(ζ1), ζ1) Ch2 (21) for some constant C. This shows that the local truncation error for the Euler solver is equal to O(h2 k). We next show that using the history-based update in Equation (17) improves this rate to O(h3 k). If we use the Taylor expansion of u(zk1, tk1), we get (22) (23) (24) (25) (26) u(zk1, tk1) = u(zk, tk) + hk1 du dt (zk, tk) + h2 k1 2 d2u dt2 (z(ζ2), ζ2) for some ζ2 [tk, tk1]. This leads to u(zk, tk) = u(zk, tk) wkhk1 du dt (zk, tk) wkh2 k1 2 d2u dt2 (z(ζ2)ζ2). Using another Taylor expansion for z(t) (this time up to third order), we get zk+1 = zk + dz dt (tk)(tk+1 tk) + 1 2 du dt for some ζ3 [tk+1, tk]. Accordingly, the new truncation error Lk is given by = zk + hku(zk, tk) (zk, tk) + h2 2 d2z dt2 (tk)(tk+1 tk)2 + d2u dt2 (z(ζ3), ζ3) h3 6 1 6 d3z dt3 (ζ3)(tk+1 tk) Lk = zk+1 (zk + hk u(zk, tk)) (cid:13) (cid:13) (cid:13) (cid:13) = hk(wkhk1 hk 2 Now, if we set wk = hk 2hk ) h3 6 du dt (zk, tk) + d2u dt2 (z(ζ3), ζ3) + , the first term vanishes, and we get (cid:13) (cid:13) (cid:13) (cid:13) d2u dt2 (z(ζ3), ζ3) + h2 khk1 4 h3 6 Lk = wkhkh2 2 k1 d2u dt2 (z(ζ2), ζ2) (cid:13) (cid:13) (cid:13) (cid:13) . (27) d2u dt2 (z(ζ2), ζ2) (cid:13) (cid:13) . (cid:13) (cid:13) (28) Since the step sizes are bounded, there is constant such that hk1 Ahk. We therefore have (cid:13) d2u (cid:13) dt2 (z(ζ3)ζ3) (cid:13) (cid:13) Assuming that u(z, t) is sufficiently smooth, its derivatives should be bounded on [tM , t0]. Thus, d2u dt2 (z(ζ2), ζ2) Ah3 4 Lk h3 6 (cid:13) (cid:13) . (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) + (29) (30) for some constant . Therefore, Theorem B.2 implies that for = maxk(tk tk+1), the global error is O(h) for the Euler update and O(h2) for the history-based update in HiGS. Lk h3 = O(h3 k) Theorem B.2. Let u(z, t) be sufficiently smooth in both arguments, and consider decreasing time grid t0 > t1 > > tM with variable step sizes hk = tk tk+1. Suppose numerical ODE solver produces updates with local truncation error Lk of order O(hp k), for some 1. Then the final global error of the solver Et0 satisfies Et0 = O(hp) for = maxk(tk tk+1). Proof. general proof is given in Chapter II.3, Theorem 3.4 of Hairer et al. (1993)."
        },
        {
            "title": "C RELATION TO AUTOGUIDANCE",
            "content": "Autoguidance (Karras et al., 2024) leverages weaker diffusion model (a smaller or less trained variant of the base model) to improve generation quality. However, since it requires training an additional model, it cannot be applied out of the box to enhance pretrained models without extra training. In contrast, the history term introduced by HiGS can be interpreted as an explicit negative signal derived from the previous predictions of the diffusion model. Specifically, the prediction Dc(ztk ) at each step corresponds to denoised version of the current input ztk , which initially tends to be blurry and of lower quality. As sampling progresses, successive predictions become sharper, and thus the update term Dc(ztk ) g(Hk) roughly captures the difference between the base models prediction and that of worse version. Because the negative signal g(Hk) is defined entirely from past predictions, HiGS can be applied to pretrained models without any additional training. Moreover, HiGS can be combined with autoguidance by replacing the CFG prediction DCFG(ztk ) in HiGS with the corresponding prediction from autoguidance."
        },
        {
            "title": "APG",
            "content": "+ HiGS (Ours)"
        },
        {
            "title": "APG",
            "content": "+ HiGS (Ours)"
        },
        {
            "title": "APG",
            "content": "+ HiGS (Ours)"
        },
        {
            "title": "APG",
            "content": "+ HiGS (Ours) Figure 6: Compatibility of HiGS with APG (Sadat et al., 2025a) on Stable Diffusion 3 (Esser et al., 2024). HiGS enhances the quality and detail of APG, indicating that its benefits are complementary to various guidance methods."
        },
        {
            "title": "D ADDITIONAL EXPERIMENTS",
            "content": "Sampling time Since HiGS does not require an additional forward pass, its sampling time is effectively identical to the CFG baseline. To verify this, we measured inference performance on Stable Diffusion 3 using the same GPU, with and without HiGS. In both cases, sampling achieved about 6.50 iterations per second with identical memory usage, confirming that HiGS has negligible impact on runtime or memory. Compatibility with adaptive projected guidance We next demonstrate that HiGS is compatible with other CFG variants such as adaptive projected guidance (APG) (Sadat et al., 2025a). Figure 6 shows that HiGS also improves the quality and details of the APG samples. This indicates that our approach is complementary to various CFG modifications. CLIP scores While HPSv2 and ImageReward both account for image quality and prompt alignment, we also report CLIP scores in this section for completeness. Table 5 shows that HiGS achieves the same CLIP score as the baseline, indicating that it preserves the prompt alignment of CFG. We note, however, that CLIP scores may not fully reflect human judgment, as they often show limited variation across models (Podell et al., 2023). Therefore, we primarily relied on HPSv2 as our main metric for evaluating both quality and prompt alignment. Compatibility with different samplers The main results of this paper examined the effect of adding HiGS to various models with their default samplers. In this experiment, we explicitly demonstrate that HiGS is compatible with different diffusion sampling techniques using DiT-XL/2 as the base model. Table 6 shows that HiGS improves the quality across all samplers (including multistep solvers such as DPM++), indicating that its benefits are complementary to changing the sampling method. Avoiding the issues of high CFG scales While increasing CFG generally improves image quality at various NFEs, high CFG scales often result in oversaturation and reduced diversity. In contrast, HiGS enhances image quality at lower CFG scales while inherently avoiding these drawbacks. Figures 7 and 8 demonstrate that applying HiGS at lower CFG scales yields more diverse and realistic generations compared to sampling with high CFG. Furthermore, as shown in the main paper, HiGS consistently outperforms CFG across wide range of scales. Table 5: Comparison of CLIP scores across datasets and models. HiGS consistently improves generation quality (see Table 2) while maintaining virtually identical CLIP scores. This demonstrates that the quality gains are achieved without sacrificing prompt alignment."
        },
        {
            "title": "Guidance",
            "content": "CLIP Score DrawBench (Saharia et al., 2022b) Stable Diffusion"
        },
        {
            "title": "Stable Diffusion XL",
            "content": "Stable Diffusion 3."
        },
        {
            "title": "Stable Diffusion XL",
            "content": "Parti Prompts (Yu et al., 2022) Stable Diffusion 3 Stable Diffusion 3.5 Stable Diffusion XL HPS Prompts (Wu et al., 2023) Stable Diffusion Stable Diffusion 3.5 CFG +HiGS (Ours) CFG +HiGS (Ours) CFG +HiGS (Ours) CFG +HiGS (Ours) CFG +HiGS (Ours) CFG +HiGS (Ours) CFG +HiGS (Ours) CFG +HiGS (Ours) CFG +HiGS (Ours) 0.308 0.309 0.329 0. 0.334 0.330 0.317 0.318 0.327 0.325 0.330 0.329 0.333 0.336 0.332 0. 0.337 0.336 Table 6: Effect of adding HiGS to various popular diffusion samplers using the DiT-XL/2 model with 15 steps and wCFG = 1.25. Note that HiGS improves the performance of all samplers (including multistep solvers such as DPM++), and hence its effect is complementary to changing the sampler. Sampler DDIM (Song et al., 2021a) DPM++ (Lu et al., 2022) DDPM (Ho et al., 2020) PLMS (Liu et al., 2022) UniPC (Zhao et al., 2023) Guidance CFG +HiGS (Ours) CFG +HiGS (Ours) CFG +HiGS (Ours) CFG +HiGS (Ours) CFG +HiGS (Ours) FID 11.87 8.73 7.15 6.66 24.65 17. 6.75 6.13 6.79 6.60 IS Precision Recall 151.40 173.21 180.05 191. 107.25 128.38 183.11 191.44 185.86 192.81 0.69 0.73 0.75 0.76 0.58 0. 0.74 0.75 0.75 0.76 0.70 0.69 0.71 0.70 0.67 0.64 0.72 0. 0.71 0.69 Changing the scale in HiGS Figure 9 shows the performance of HiGS as the scale wHiGS varies. We observe that performance improves as wHiGS increases, but degrades when the scale becomes too large, while all settings still outperform the CFG baseline. Overall, we find that wHiGS 3 provides consistently strong performance across models."
        },
        {
            "title": "E ABLATION STUDIES",
            "content": "The choice for buffer input Table 7 shows that both the conditional prediction Dc(zt) and the guided prediction DCFG(zt) are viable inputs to HiGS, but using DCFG(zt) as the history leads to better performance. Accordingly, we used DCFG(zt) in our experiments whenever possible."
        },
        {
            "title": "High CFG",
            "content": "Low CFG + HiGS (Ours) High CFG Low CFG + HiGS (Ours) Figure 7: High CFG scales improve overall structure of the image but lead to lack of diversity, oversaturation, and unrealistic effects. HiGS significantly improves the quality of lower CFG scales, leading to more realistic and diverse generations. Samples are generated with Flux (Labs, 2024). Frequency filtering Figure 10 shows that applying frequency filtering is essential for avoiding color artifacts in our method. Without this step (both with and without projection), the images often exhibit unrealistic patterns and unnatural color compositions. These issues are effectively mitigated through our DCT-based filtering, and we later show that performance remains fairly robust to different choices of hyperparameters used in the DCT filter. Projection We next demonstrate that projection can also reduce color artifacts in the generated images in some situations. Figure 11 shows that after projection, the generations appear more realistic with fewer oversaturated regions. Thus, incorporating projection into HiGS may lead to more realistic outputs when oversaturation exists."
        },
        {
            "title": "High CFG",
            "content": "Low CFG + HiGS (Ours) High CFG Low CFG + HiGS (Ours) Figure 8: High CFG scales improve overall structure of the image but lead to lack of diversity, oversaturation, and unrealistic effects. HiGS significantly improves the quality of lower CFG scales, leading to more realistic and diverse generations. Samples are generated with Flux (Labs, 2024). Effect of the thresholds in the weight schedule We next analyze the impact of tmin and tmax in the weight scheduler. Figure 12 shows that setting tmin too low or too high leads to either excessive or insufficient guidance, resulting in suboptimal performance. Similarly, Figure 13 indicates that reducing tmax generally causes insufficient guidance and degraded results. Overall, we found that setting tmin [0.3, 0.5] and tmax [0.9, 1.0] yields consistently good performance across models. Effect of EMA value Figure 14 shows that HiGS performs well for wide range of EMA values α. We have found that setting α = 0.5 or α = 0.75 gives good results across all models and architectures. 20 HPSv2 ImageReward CLIP Score 0.26 0. 0.25 0.25 0.24 0.24 0.23 0. 0.35 0.3 0.25 0.2 0.15 CFG CFG 0.5 1 1.5 wHiGS 2.5 3 0.5 1 2 2. 3 1.5 wHiGS 0.32 0.32 0. 0.32 0.32 0.32 CFG 0.5 2 2.5 3 1.5 wHiGS Figure 9: Effect of varying wHiGS on different quality metrics with Stable Diffusion XL (Podell et al., 2023). HiGS consistently outperforms the CFG baseline across wide range of wHiGS scales. In practice, we found that setting wHiGS 3 generally leads to good performance across models. Table 7: Comparison of using the conditional model prediction vs the CFG prediction as input to HiGS. While both options outperform baseline sampling without HiGS, using the CFG-guided prediction (when available) leads to better results. Config HPSv2 Image Reward CLIP Score Baseline (with CFG) +HiGS (Conditional) +HiGS (CFG) 0.238 0.249 0.255 0.174 0.234 0.371 0.317 0.315 0. w/o DCT filtering w/ DCT filtering w/o DCT filtering w/ DCT filtering Figure 10: Effect of frequency filtering on generation quality. Without DCT filtering, the results show unrealistic color compositions, which are corrected after applying the filtering operation. w/o projection w/ projection w/o projection w/ projection Figure 11: Effect of using projection in HiGS. Without projection, the generations might produce oversaturated results, which can be mitigated by reducing the strength of the parallel component. 21 HPSv ImageReward CLIP Score 0.26 0.26 0.25 0. 0.24 0.3 0.25 0.2 0.32 0. 0.32 0.32 0 0.2 0.4 0. 0.8 1 0 0.2 0.4 0. 0.8 1 0 0.2 0.4 0. 0.8 1 tmin tmin tmin Figure 12: Effect of varying tmin on different quality metrics for Stable Diffusion XL. The results show that performance degrades when tmin is set too low or too high. We find that tmin [0.3, 0.5] yields good results across all models. HPSv2 ImageReward CLIP Score 0.26 0.25 0. 0.24 0.35 0.3 0.25 0.2 0. 0.32 0.32 0.32 0.32 0.5 0. 0.7 0.8 0.9 1 0.5 0. 0.7 0.8 0.9 1 0.5 0. 0.7 0.8 0.9 1 tmax tmax tmax Figure 13: Effect of varying tmax on different quality metrics for Stable Diffusion XL. The results show that reducing tmax often leads to insufficient guidance and degraded quality. We recommend setting tmax [0.9, 1] for all models. Effect of the DCT threshold We show in this section that HiGS is relatively robust to the choice of the DCT threshold Rc as long as it is sufficiently low. Figure 15 shows that the metrics are more or less the same for lower Rc values. We set Rc 0.05 for all models. Effect of weight scheduling We next show that alternative choices for the weight scheduler are possible. Specifically, we test constant function for wHiGS(t) applied over an interval, i.e., wHiGS(t) = 0 wHiGS 0 tmin, tmin < tmax, > tmax, as well as linear schedule given by wHiGS(t) = 0 wHiGS 0 ttmin tmaxtmin tmin, tmin < tmax, > tmax. (31) (32) Table 8 shows that all three options are viable. We chose the square-root function in Section 4, as we empirically found that it produces more visually appealing results. Various options for the history function In Table 9, we show that different choices of yield similar quality metrics, indicating that HiGS is robust to this design decision. We adopt the EMA option, as it produced slightly more realistic outputs in our visual evaluations. Moreover, it enables computing the average on the fly without storing all past predictions in memory (see Algorithm 2). 22 HPSv2 ImageReward CLIP Score 0.3 0.28 0.26 0.24 0.22 0. 0.18 0.16 0.2 0.4 0.6 0. EMA α 1 0.5 0 0.5 0.34 0.32 0.3 0.28 0.26 0. 0.4 0.6 0.8 0.2 0.4 0. 0.8 EMA α EMA α Figure 14: Effect of varying the EMA parameter α on different quality metrics for Stable Diffusion XL. The results indicate that performance remains consistent across choices of α. HPSv2 ImageReward CLIP Score 0.3 0.28 0.26 0.24 0. 0.2 0.18 0.16 0 5 102 0.1 DCT threshold 0.15 1 0.5 0 0.5 0. 1 0 0.34 0.32 0.3 0. 0.26 0.2 0 5 102 0.1 DCT threshold 0. 0.2 5 102 0.1 DCT threshold 0.15 Figure 15: Effect of varying the DCT threshold on different quality metrics for Stable Diffusion XL. The results indicate that performance remains consistent across different threshold choices. Table 8: Effect of different weight schedulers on HiGS. Weight Scheduler wHiGS HPSv2 Constant Square-root Linear 0.261 0.261 0.260 1.75 2.50 3.25 Image Reward CLIP Score 0.36 0.39 0.37 0.319 0.319 0. Table 9: Effect of different averaging functions g(Hk) on HiGS. g(Hk) HPSv2 Image Reward CLIP Score Random Average Weighted average EMA average 0.261 0.261 0.260 0. 0.362 0.349 0.370 0.371 0.318 0.319 0.320 0."
        },
        {
            "title": "F IMPLEMENTATION DETAILS",
            "content": "The detailed algorithmic procedure for HiGS is provided in Algorithm 1, with pseudocode given in Algorithms 2 and 3. Since HiGS reuses past predictions without requiring additional forward passes, its computational cost is equivalent to standard CFG. Other operations, such as DCT, add negligible overhead. Thus, HiGS improves quality without increasing the overall sampling cost or memory as can be seen in the code and our experiments. We always scale the time step such that [0, 1]. For the frequency filter, we found that λ = 50 and Rc 0.05 work well across all models. Similarly, setting tmin [0.3, 0.5] and tmax [0.9, 1.0] lead 23 Table 10: Guidance parameters used for Table 1."
        },
        {
            "title": "Model",
            "content": "SiT-XL + REPA DiT-XL/2 Stable Diffusion XL Stable Diffusion 3 # Steps wCFG wHiGS 1.5 1.25 2.5 2.5 1 2 1.75 1.75 30 15 20 20 η 1 0 0 tmin 0.3 0.3 0.4 0.4 tmax α 1 1 1 1 0.75 0.75 0.75 0.75 Rc 0.05 0.05 0.05 0.05 Table 11: Guidance parameters used for Table 2."
        },
        {
            "title": "Model",
            "content": "Stable Diffusion XL Stable Diffusion 3 Stable Diffusion 3.5 # Steps wCFG wHiGS 1.75 2.5 1.75 2.5 1.75 2.5 20 20 20 η 1 1 1 tmin 0.4 0.4 0. tmax α 1 1 1 0.5 0.5 0.5 Rc 0.05 0.05 0.05 Table 12: Guidance parameters used for Table 3. Model SiT-XL + REPA (Unguided) SiT-XL + REPA (with CFG) SiT-XL + REPA-E (Unguided) SiT-XL + REPA-E (with CFG) # Steps wCFG wHiGS 1 1.8 1 2.5 1 1 1.25 0.75 40 40 30 40 η 1 1 1 tmin 0.4 0.35 0.35 0.3 tmax α 1 1 1 1 0.75 0.75 0.75 0.75 Rc 0.05 0.05 0.05 0.05 to consistently strong results in our tests. For EMA values, we observed that α = 0.5 or α = 0.75 worked well across all experiments. We used all past predictions as the history length , which allows us to compute the EMA average on the fly without buffering all previous predictions in memory. The hyperparameters used for each experiment are given in Tables 10 to 12. For quantitative evaluation, FID scores for class-conditional models in Table 1 are reported using 10,000 generated samples and the full ImageNet training set. The ImageNet results in Table 3 are based on 50,000 generated samples to ensure fair comparison with prior work. For text-to-image models, we used the entire validation set of the COCO 2017 dataset (Lin et al., 2014) as ground truth text-image pairs. We followed the ADM evaluation framework (Dhariwal & Nichol, 2021) for computing FID, IS, Precision, and Recall to maintain consistency across all evaluations."
        },
        {
            "title": "G ADDITIONAL VISUAL EXAMPLES",
            "content": "We provide additional visual examples in this section to demonstrate the effectiveness of HiGS in enhancing the quality of various models across wide range of guidance scales and sampling setups. Figures 16 to 19 show further text-to-image generation results using Stable Diffusion models (Podell et al., 2023; Esser et al., 2024). In addition, Figure 20 presents visual results for applying HiGS to Flux (Labs, 2024). Finally, Figure 21 provides class-conditional generation with SiT-XL + REPA (Yu et al., 2025a). In all cases, HiGS consistently improves quality over the baseline. 24 Compute conditional and unconditional predictions: Dc(ztk ), Du(ztk ) Apply CFG: DCFG(ztk ) = wCFGDc(ztk ) (wCFG 1)Du(ztk ) Compute the history signal g(Hk) = (cid:80) Form the guidance direction: Dtk = DCFG(ztk ) g(Hk) if projection enabled (i.e., η < 1) then Algorithm 1 Sampling with HiGS Require: Diffusion model Dθ, input condition Require: CFG scale wCFG, HiGS schedule wHiGS(t), history length Require: EMA parameter α, projection weight η, DCT parameters (Rc, λ) 1: Initialize latent zT (0, III), history buffer 2: for tk {t0, t1, . . . , tT } do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: end for 16: return z0 end if Apply DCT-based high-pass filter: Dtk iDCT(H(R) DCT(Dtk )) Apply HiGS: DHiGS(ztk ) = DCFG(ztk ) + wHiGS(tk)Dtk Update history: Hk+1 Hk {DCFG(ztk )}, truncate oldest if Hk+1 > Apply one sampling step: zt1 = SAMPLINGSTEP(DHiGS(zt), zt, t) Project Dtk into orthogonal and parallel components w.r.t. DCFG(ztk ) Dtk tk α(1 α)t1iDCFG(zti) + ηDtk iIk 25 Algorithm 2 Utility functions used in the implementation of HiGS. import torch import torch_dct as dct def project( v0: torch.Tensor, # [B, C, H, W] v1: torch.Tensor, # [B, C, H, W] ): dtype = v0.dtype v0, v1 = v0.double(), v1.double() v1 = torch.nn.functional.normalize(v1, dim=[-1, -2, -3]) v0_parallel = (v0 * v1).sum(dim=[-1, -2, -3], keepdim=True) * v1 v0_orthogonal = v0 - v0_parallel return v0_parallel.to(dtype), v0_orthogonal.to(dtype) def square_root_schedule(t, start=0, end=1): if > end or <= start: return 0.0 return ((t - start) / (end - start)) ** 0.5 def dct2(x): return dct.dct_2d(x, norm=\"ortho\") def idct2(x): return dct.idct_2d(x, norm=\"ortho\") def apply_high_freq_dct_mask(diff, threshold=0.05, sharpness=50): B, C, H, = diff.shape device = diff.device = dct2(diff) = torch.arange(H, device=device).view(H, 1) / = torch.arange(W, device=device).view(1, W) / = torch.sqrt(u**2 + v**2) # normalized distance from top-left (DC) mask = torch.sigmoid((d - threshold) * sharpness) X_filtered = * mask # broadcast over (B, C) diff_filtered = idct2(X_filtered).to(diff.dtype) return diff_filtered class HistoryBuffer: def __init__(self, ema_alpha=0.75): self.ema = None self.ema_alpha = ema_alpha def add(self, current_pred): if self.ema is None: self.ema = torch.zeros_like(current_pred) self.ema = self.ema_alpha * current_pred + (1 - self.ema_alpha) * self.ema 26 Algorithm 3 PyTorch implementation of HiGS. class HiGSGuidance: def __init__( self, w_higs, t_min=0.4, t_max=1.0, eta=1.0, ema_alpha=0.75, dct_threshold=0.05, ): self.history = HistoryBuffer(ema_alpha=ema_alpha) self.weight = w_higs self.min_t = t_min self.max_t = t_max self.parallel_weight = eta self.dct_threshold = dct_threshold def step(self, current_pred, timestep=None): \"\"\" Compute the HiGS guidance step. \"\"\" # current_pred can be either CFG-guided or conditional predictions if self.history.ema is None: self.history.add(current_pred) return torch.zeros_like(current_pred) diff = current_pred - self.history.ema # compute the projection of the difference diff_par, diff_orth = project(diff, current_pred) diff = diff_orth + diff_par * self.parallel_weight # Compute the scaling factor based on the current timestep gamma = square_root_schedule(timestep, self.min_t, self.max_t) scale = self.weight * gamma # Update the history with the current prediction self.history.add(current_pred) # Apply the high-frequency DCT mask to the difference if self.dct_threshold >= 0: diff = apply_high_freq_dct_mask(diff, threshold=self.dct_threshold) # Return the scaled difference return scale * diff"
        },
        {
            "title": "CFG",
            "content": "+ HiGS (Ours)"
        },
        {
            "title": "CFG",
            "content": "+ HiGS (Ours)"
        },
        {
            "title": "CFG",
            "content": "+ HiGS (Ours)"
        },
        {
            "title": "CFG",
            "content": "+ HiGS (Ours) Figure 16: Generated samples using Stable Diffusion XL with 20 steps and wCFG = 3. CFG + HiGS (Ours) CFG + HiGS (Ours) CFG + HiGS (Ours) CFG + HiGS (Ours) Figure 17: Generated samples using Stable Diffusion 3 with 16 steps and wCFG = 2."
        },
        {
            "title": "CFG",
            "content": "+ HiGS (Ours)"
        },
        {
            "title": "CFG",
            "content": "+ HiGS (Ours)"
        },
        {
            "title": "CFG",
            "content": "+ HiGS (Ours)"
        },
        {
            "title": "CFG",
            "content": "+ HiGS (Ours) Figure 18: Generated samples using Stable Diffusion 3 with 28 steps and wCFG = 4.5. CFG + HiGS (Ours) CFG + HiGS (Ours) CFG + HiGS (Ours) CFG + HiGS (Ours) Figure 19: Generated samples using Stable Diffusion 3 with 28 steps and wCFG = 2."
        },
        {
            "title": "CFG",
            "content": "+ HiGS (Ours)"
        },
        {
            "title": "CFG",
            "content": "+ HiGS (Ours) CFG + HiGS (Ours) CFG + HiGS (Ours) CFG + HiGS (Ours) CFG + HiGS (Ours) Figure 20: Generated samples using Flux with 15 steps and wCFG = 1.25."
        },
        {
            "title": "CFG",
            "content": "+ HiGS (Ours)"
        },
        {
            "title": "CFG",
            "content": "+ HiGS (Ours) CFG + HiGS (Ours) CFG + HiGS (Ours) Figure 21: Class-conditional generation with SiT-XL + REPA using 30 steps and wCFG = 1.8."
        }
    ],
    "affiliations": [
        "Disney Research Studios",
        "ETH Zürich"
    ]
}