{
    "paper_title": "Llama-Embed-Nemotron-8B: A Universal Text Embedding Model for Multilingual and Cross-Lingual Tasks",
    "authors": [
        "Yauhen Babakhin",
        "Radek Osmulski",
        "Ronay Ak",
        "Gabriel Moreira",
        "Mengyao Xu",
        "Benedikt Schifferer",
        "Bo Liu",
        "Even Oldridge"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce llama-embed-nemotron-8b, an open-weights text embedding model that achieves state-of-the-art performance on the Multilingual Massive Text Embedding Benchmark (MMTEB) leaderboard as of October 21, 2025. While recent models show strong performance, their training data or methodologies are often not fully disclosed. We aim to address this by developing a fully open-source model, publicly releasing its weights and detailed ablation studies, and planning to share the curated training datasets. Our model demonstrates superior performance across all major embedding tasks -- including retrieval, classification and semantic textual similarity (STS) -- and excels in challenging multilingual scenarios, such as low-resource languages and cross-lingual setups. This state-of-the-art performance is driven by a novel data mix of 16.1 million query-document pairs, split between 7.7 million samples from public datasets and 8.4 million synthetically generated examples from various open-weight LLMs. One of our key contributions is a detailed ablation study analyzing core design choices, including a comparison of contrastive loss implementations, an evaluation of synthetic data generation (SDG) strategies, and the impact of model merging. The llama-embed-nemotron-8b is an instruction-aware model, supporting user-defined instructions to enhance performance for specific use-cases. This combination of top-tier performance, broad applicability, and user-driven flexibility enables it to serve as a universal text embedding solution."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 5 2 0 7 0 . 1 1 5 2 : r Llama-Embed-Nemotron-8B: Universal Text Embedding Model for Multilingual and Cross-Lingual Tasks Yauhen Babakhin, Radek Osmulski, Ronay Ak, Gabriel Moreira, Mengyao Xu, Benedikt Schifferer, Bo Liu, Even Oldridge NVIDIA"
        },
        {
            "title": "Abstract",
            "content": "We introduce llama-embed-nemotron-8b, an open-weights text embedding model2 that achieves state-of-the-art performance on the Multilingual Massive Text Embedding Benchmark (MMTEB) leaderboard as of October 21, 2025. While recent models show strong performance, their training data or methodologies are often not fully disclosed. We aim to address this by developing fully open-source model, publicly releasing its weights and detailed ablation studies, and planning to share the curated training datasets. Our model demonstrates superior performance across all major embedding tasks including retrieval, classiﬁcation and semantic textual similarity (STS) and excels in challenging multilingual scenarios, such as low-resource languages and cross-lingual setups. This state-of-the-art performance is driven by novel data mix of 16.1 million query-document pairs, split between 7.7 million samples from public datasets and 8.4 million synthetically generated examples from various openweight LLMs. One of our key contributions is detailed ablation study analyzing core design choices, including comparison of contrastive loss implementations, an evaluation of synthetic data generation (SDG) strategies, and the impact of model merging. The llama-embednemotron-8b is an instruction-aware model, supporting user-deﬁned instructions to enhance performance for speciﬁc use-cases. This combination of top-tier performance, broad applicability, and user-driven ﬂexibility enables it to serve as universal text embedding solution."
        },
        {
            "title": "1\nDense text embedding models are a fundamental component of modern information retrieval. They\nare critical for a wide range of applications, including web search, question answering, semantic\ntextual similarity, and recommendation engines. Their importance has been further ampliﬁed by the\nwidespread adoption of Retrieval-Augmented Generation (RAG), which grounds Large Language\nModels (LLMs) in external context. Recent notable text embedding models include NV-Embed [1],\nNV-Retriever [2], Qwen3-Embedding [3], and Gemini Embedding [4]. These models achieve\nstrong results on benchmarks like the Massive Text Embedding Benchmark (MTEB) [5, 6], which\ncomprehensively evaluate models across a broad range of tasks.",
            "content": "In parallel, the ﬁeld has seen signiﬁcant shift towards multi-modal [7, 8, 9] and omni-modal [10] embedding models. This trend is driven by the need to handle real-world documents, such as PDFs or slides, which contain mix of text, tables, and charts, as well as other rich modalities like audio and video. While multi-modal models address this trend, high-performance, text-only embedding models remain relevant and efﬁcient solution for wide range of text-centric use cases. This includes large volume of inherently text-native data, such as news articles, support tickets, and 1Correspondence to Yauhen Babakhin (ybabakhin@nvidia.com) 2We released the model at https://huggingface.co/nvidia/llama-embed-nemotron-8b 1 Table 1: Aggregated results for the MTEB(Multilingual, v2) split of the MTEB Leaderboard (as of October 21, 2025). Ranking on the Leaderboard is performed based on the Borda rank. Each task is treated as preference voter, which gives votes to the models based on their relative performance on the task. The model with the highest number of votes across all tasks obtains the best rank."
        },
        {
            "title": "Borda Rank Borda Votes",
            "content": "Mean (Task) 131 tasks Mean (Task Type) 9 task types llama-embed-nemotron-8b gemini-embedding-001 Qwen3-Embedding-8B Qwen3-Embedding-4B Qwen3-Embedding-0.6B gte-Qwen2-7B-instruct Linq-Embed-Mistral 1. 2. 3. 4. 5. 6. 7. 39,573 39,368 39,364 39,099 37,419 37,167 37,149 69.46 68.37 70.58 69.45 64.34 62.51 61.47 61.09 59.59 61.69 60.86 56.01 55.93 54. legal documents, as well as popular pipelines where documents like scanned PDFs or invoices are ﬁrst converted to text via Optical Character Recognition (OCR). The central challenge in this domain remains the development of truly \"universal\" text model that performs robustly across diverse tasks, domains, and, critically, multiple languages. To address this challenge, we introduce llama-embed-nemotron-8b, new universal text embedding model. Our model establishes new state-of-the-art, achieving the 1st place rank on the comprehensive Multilingual Massive Text Embedding Benchmark (MMTEB) leaderboard [6] (as of October 21, 2025). See aggregated results in Table 1. This paper details the architecture, training methodology, and data mixing strategies that enable llama-embed-nemotron-8b to effectively unify text representation across wide spectrum of languages and text embedding tasks."
        },
        {
            "title": "2 Model\nOur model, llama-embed-nemotron-8b, is a universal, instruction-tuned text embedding model de-\nsigned to generate specialized embeddings for a wide range of tasks, including retrieval, classiﬁca-\ntion, and STS. It has the ability to adapt its embedding outputs based on a task-speciﬁc instructional\npreﬁx.",
            "content": "We initialize the model using the weights and architecture of the Llama-3.1-8B model [11]. The base Llama-3.1 model is decoder-only transformer that employs causal attention mask, where each token can only attend to itself and previous tokens. We replace the causal attention mask in all transformer layers with standard bi-directional attention (i.e., no masking). This allows every token in the input sequence to freely attend to all other tokens in the sequence, effectively converting the model into bi-directional encoder. We unfreeze all the model weights and ﬁne-tune Llama-3.1-8B end-to-end. To produce single, ﬁxed-size embedding, the model processes the tokenized input sequence and produces sequence of hidden states RLdmodel from its ﬁnal transformer layer, where is the sequence length and dmodel is the models hidden dimension (4096). We then apply global average pooling over the sequence dimension of these ﬁnal hidden states to obtain the ﬁnal embedding vector v. The models specialization for diverse task types is guided by textual instruction provided in the input. All inputs are formatted using speciﬁc template: Input = f\"Instruct: {task_instruction}nQuery: {T}\" where task_instruction is string that tells the model to produce an embedding suitable for the target task or use-case. While the core encoder model is shared, its application architecture varies depending on the task family: For Retrieval Tasks: We employ bi-encoder architecture. The query and corpus (documents) are processed independently by the shared encoder. This projects them into common embedding space. Query is using the appropriate instruction template, while no speciﬁc formatting is required for documents. At inference time, relevance is computed using cosine similarity, enabling efﬁcient and scalable search across large corpora. For STS & Classiﬁcation Tasks: The model functions as uni-encoder. Each text is passed through the model with the appropriate instruction to generate its embedding. These embeddings are then directly used for the task, such as computing cosine similarity for STS or serving as features for classiﬁer. This ﬂexible, instruction-driven approach allows single model to effectively handle the varied demands of all the MMTEB task types."
        },
        {
            "title": "3.1 Training Objective\nWe leverage contrastive learning to train the model, mapping inputs to a shared embedding space.\nThe core objective is to maximize the similarity between related items and minimize it for unrelated\nones. While the speciﬁc deﬁnition of the training triplet – an anchor query (q), a positive document\n(d+), and a set of negative documents (DN ) – is adapted depending on the speciﬁc problem type,\nthe training is governed by the InfoNCE contrastive loss [12]. The formal objective is:",
            "content": "L(q, d+, DN ) = log exp(sim(q, d+)/τ ) (cid:80) di{d+}DN exp(sim(q, di)/τ ) (1) where is the embedding of an anchor, d+ is the embedding of the positive item, and DN denotes the set of negative items. sim() represents the cosine similarity function, and τ is the temperature hyperparameter. While Equation 1 deﬁnes the core objective, the composition of the training triplet (q, d+, DN ) is adapted for the different task types: For Retrieval Tasks: The inputs directly map to <query, positive document, negative documents> triplet. The query is formatted with the retrieval instruction (as described in Section 2), while documents (d+ and DN ) do not require any preﬁx instructions. Some of the popular components which are frequently added to the DN set are in-batch negatives [4] and same-tower negatives [3, 13]. For our model training, we do not utilize any extra negatives in DN , apart from the mined hard negatives. Our hard negative mining process is described in Section 4.4. For Classiﬁcation Tasks: The input text serves as the anchor q. The positive d+ is the text of the correct label name. The negatives DN are set of random incorrect label names from the given classiﬁcation task. The anchor is formatted with the speciﬁc classiﬁcation instructions, while label names are processed without any modiﬁcations. For STS Tasks: These tasks are treated as symmetric. Given positive pair of texts (TA, TB), we create training instance: = TA, d+ = TB. In this case, DN contains hard negative examples which are mined from the datasets corpus. All texts (query, positive, and negatives) are processed using the same instruction preﬁx: \"Retrieve semantically similar text.\"."
        },
        {
            "title": "3.2 Training Stages\nWe start training from Llama-3.1-8B foundation model weights [11]. Llama-3.1-8B model is al-\nready pre-trained on a corpus of about 15T multilingual tokens. This makes it a strong base model\nfor training multi-lingual text embedding models. We train the model in two stages described\nbelow. Detailed hyperparameters for each stage are provided in Appendix A.",
            "content": "Stage 1: Retrieval Pretraining. The goal of the ﬁrst stage is to adapt Llama-3.1-8B LLM to both bi-directional attention and embedding model setup. In this stage we use only retrieval data where queries and documents are based on the Web corpus. We use only single hard-negative for each <query, document> pair which is mined from the same Web corpus. This stage constitutes about 70% of the overall data mix. In the second stage, we ﬁne-tune the model using high-quality datasets Stage 2: Fine-Tuning. for various problem types: retrieval, classiﬁcation, STS, and bitext mining. The goal of this stage is to train well-rounded model that works across different tasks. This stage constitutes the other 30% of the overall data mix."
        },
        {
            "title": "3.3 Model Merging\nWe train multiple models using the two-stage approach above, and then apply model merging\nacross resulting checkpoints. Model merging involves combining the parameters of multiple mod-\nels, and is a well-established technique [14, 15]. Recently, this approach has been popularized for\nimproving the robustness and generalization of embedding models, such as Qwen3-Embedding [3],\nGemini Embedding [4], and EmbeddingGemma [16].",
            "content": "The core idea of this method is to average the parameters obtained from the individual model runs. There are different strategies for selecting the individual checkpoints. These include averaging checkpoints from different steps within the same training run [14], or combining models from multiple, distinct training runs that may use different hyperparameters or intentional data variations [15]. Our ﬁnal model is an average of six diverse individual checkpoints. We achieved this diversity by varying the data mixes and model hyperparameters across training runs. This ﬁnal model produces the best evaluation results compared to individual checkpoints, with no inference time increase. We provide more details about the results in the ablation studies (Section 6.4)."
        },
        {
            "title": "4.1 Pretraining Data Mix\nFor pretraining data, we relied on the NVIDIA’s Nemotron-CC-v2 dataset [17]. We employed two\nstrategies to create our ﬁnal pretraining set, which consists of approximately 11.8M <query, docu-\nment> pairs.",
            "content": "3Nemotron RAG collection: https://huggingface.co/collections/nvidia/nemotron-rag 4 Table 2: Overview of the training data mix, detailing the number of <query, document> pairs for pretraining and ﬁne-tuning, segmented by non-synthetic and synthetic data sources. Training Stage Non-Synthetic Data"
        },
        {
            "title": "Synthetic Data",
            "content": "Pretraining Fine-tuning"
        },
        {
            "title": "Total",
            "content": "5.0M 2.7M 7.7M 6.8M 1.6M 8.4M In this strategy, we utilized the Diverse-QA Utilize existing questions from Nemotron-CC-v2. split of the Nemotron-CC-v2 dataset. We extracted the existing questions and their corresponding positive documents from this split. We then mined hard negatives from pool of 1M document chunks sampled from the same Diverse-QA corpus. This approach yielded 5.0M training pairs. Generate new questions for Nemotron-CC-v2 corpus. For our second strategy, we generated new synthetic queries. We took the existing documents from the Diverse-QA split and generated our own synthetic questions for them, creating new set of <query, positive document> pairs. Subsequently, we mined hard negatives for these new pairs in the same manner as the ﬁrst strategy. This approach contributed the remaining 6.8M training pairs."
        },
        {
            "title": "4.2 Fine-tuning Data Mix\nFor the ﬁne-tuning data mix, we started with a mix introduced in NV-Embed [1]. The MTEB\nLeaderboard [6] reports a zero-shot percentage for each model, which indicates whether any of the\nbenchmark’s train, validation, or test splits were used during a model’s training phase. This metric\nis designed to ensure that the MTEB evaluation datasets remain out-of-domain for the models being\ntested. Therefore, to preserve the integrity of our zero-shot evaluation, we removed the majority\nof data originating from both MTEB(Multilingual, v2) and MTEB(eng, v2) splits of the MTEB.",
            "content": "As reported in Table 2, our ﬁne-tuning mix consists of two parts: non-synthetic and synthetic data. For non-synthetic part, we utilized well-known public datasets, like MIRACL [18], HotpotQA [19], MS MARCO[20], Natural Questions [21], SQuAD [22], and more. The next section describes the synthetic part. The full list of ﬁne-tuning datasets, together with number of samples is presented in Appendix B."
        },
        {
            "title": "4.3 Synthetic Data Generation\nTo enhance the diversity of our datasets, we employed a comprehensive Synthetic Data Generation\n(SDG) strategy, with a speciﬁc focus on multi-lingual and cross-lingual data. We applied SDG for\ncreating datasets across primary task types: retrieval, classiﬁcation, STS, and bitext mining.",
            "content": "Our methodology relied on two main strategies, inspired by the recent state-of-the-art embedding models. The ﬁrst strategy, similar to [23] approach, involved the end-to-end generation of complete <query, positive, negatives> text triplets from scratch. The second strategy, inspired by [3] and [4], leveraged seed corpus. We ﬁrst sampled positive document from the corpus, generated corresponding query, and subsequently mined hard-negatives from the same corpus. Additionally, we expanded our multi-lingual data by translating several existing high-quality datasets into various target languages. For all SDG and translation tasks, we utilized diverse mix of powerful, open-weights LLMs. This list includes: gpt-oss-20b and gpt-oss-120b [24], Mixtral-8x22B-Instruct-v0.1 [25], Llama-3.3-70B-Instruct [11], Llama-4-Scout-17B-16E-Instruct and Llama-4-Maverick-17B-128E-Instruct [26]. We further explore the quality of synthetic datasets produced by different LLMs in our ablation studies (Section 6.2)."
        },
        {
            "title": "5 Results\nWe evaluate our model on the Multilingual split of the MTEB benchmark [5], which was introduced\nin Massive Multilingual Text Embedding Benchmark (MMTEB) [6]. This is the most extensive\nbenchmark for multilingual and cross-lingual text embedding models, comprising 131 diverse tasks\nacross 9 task types and 250+ languages (both high- and low-resource). The task types include\nBitext Mining, Classiﬁcation, Clustering, Instruction Reranking, Multilabel Classiﬁcation, Pair\nClassiﬁcation, Reranking, Retrieval, and STS.",
            "content": "Our model is instruction-aware, supporting custom instructions to optimize performance for speciﬁc use cases. For the MMTEB evaluations, we ﬁrstly took task-speciﬁc instructions directly from the MMTEB evaluation datasets, and adapted instructions from the Qwen3-Embedding evaluation repository 4 for tasks without default instructions. We present detailed comparison of our model against the other top-10 models on the MMTEB Leaderboard (as of October 21, 2025) in Table 3. Other models in the comparison include geminiembedding-001 [4], Qwen3-Embedding family of models [3], gte-Qwen2-7B-instruct [27], LinqEmbed-Mistral [28], multilingual-e5-large-instruct [29], embeddinggemma-300m [16] and SFREmbedding-Mistral [30]. Our model achieves state-of-the-art performance, securing the Rank 1 position with 39,573 Borda votes. This represents signiﬁcant lead of over 200 votes compared to the 2nd place (geminiembedding-001) and 3rd place (Qwen3-Embedding-8B) models. Ranking on the ofﬁcial MMTEB Leaderboard is determined by the Borda count method [6]. Each task is treated as preference voter, which gives votes to the models based on their relative performance on the task. The best model obtains the highest number of votes. The model with the highest number of votes across all tasks obtains the highest rank. The Borda count method has been shown to be more robust for comparing NLP systems [31]. While Qwen3-Embedding-8B, achieves higher \"Mean (Task)\" score (70.58 vs. our 69.46), this mean metric can be sensitive to outlier performance on small subset of benchmarks. High scores on few tasks can inﬂate the overall average without necessarily indicating consistent generalization. In contrast, the Borda rank is designed to reward broad and consistent generalization across the entire spectrum of 131 tasks, rather than strong performance in limited number of areas."
        },
        {
            "title": "6 Ablation Study\nIn this section, we ablate design choices that helped the development of the llama-embed-nemotron-\n8b model. Due to the computational cost of ablating at the 8B scale, all studies presented below\n(unless mentioned otherwise) were conducted on a 1B model ﬁne-tuned from Llama-3.2-1B [11]\non our ﬁne-tuning data mix. These smaller-scale experiments allowed us to validate our training\ndecisions before scaling up the experiments.",
            "content": "4Qwen3-Embedding GitHub repository: https://github.com/QwenLM/Qwen3-Embedding 6 Table 3: Evaluation results of top leaderboard models on MTEB(Multilingual, v2) Leaderboard (as of October 21, 2025). Ranking on the ofﬁcial Leaderboard is determined by the Borda rank. Mean (Task) column is the average of scores across 131 individual tasks, while Mean (Type) is the average across 9 problem types. Model Borda Rank Borda Votes Mean (Task) Mean (Type) Bitext Mining Class. Clust. Instr. Rerank. Multi. Class. Pair Class. Rerank. Retrieval STS llama-embed-nemotron-8b gemini-embedding-001 Qwen3-Embedding-8B Qwen3-Embedding-4B Qwen3-Embedding-0.6B gte-Qwen2-7B-instruct Linq-Embed-Mistral multilingual-e5-large-instruct embeddinggemma-300m SFR-Embedding-Mistral 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. 39,573 39,368 39,364 39,100 37,419 37,167 37,149 36,921 36,728 36,579 69. 61.09 68.37 70.58 69.45 64.34 62.51 61.47 63.22 61.15 60.90 59.59 61.69 60.86 56.01 55.93 54.14 55.08 54.31 53.92 81.72 79.28 80.89 79.36 72.23 73.92 70.34 80.13 64.40 70.00 73. 54.35 10.82 71.82 74.00 72.33 66.83 61.55 62.24 64.94 60.90 60.02 54.59 57.65 57.15 52.33 52.77 50.60 50.75 51.17 51.84 5.18 10.06 11.56 5.09 4.94 0.94 -0.40 5.61 0.16 29. 29.16 28.66 26.77 24.59 25.48 24.77 22.91 24.82 24.55 83.97 83.63 86.40 85.05 80.83 85.13 80.43 80.86 81.40 80.29 67.78 65.58 65.63 65.08 61.41 65.55 64.37 62.61 63.25 64.19 68. 79.41 67.71 70.88 69.60 64.65 60.08 58.69 57.12 62.49 59.44 79.40 81.08 80.86 76.17 73.98 74.86 76.81 74.73 74.79 Table 4: Comparison of different InfoNCE loss implementations on the MMTEB Leaderboard. Loss Gecko Qwen3-Embedding Gemini Embedding Ours (HNs Only) Borda Votes 37,903 36,835 38,135 38,225 Mean (Task) 63.45 62.14 63.83 64.03 Mean (Type) Bitext Mining Class. Clust. 55.86 55.49 55.90 56.04 72.58 73.50 73.13 72.94 65.12 60.60 66.28 66.99 52.29 54.82 53.04 52. Instr. Rerank. 5.28 4.97 4.76 5.69 Multi. Class. 25.46 25.22 24.70 24.60 Pair Class. 80.68 80.63 80.74 80. Rerank. Retrieval STS 64.21 64.11 64.26 64.44 61.20 60.12 60.28 60.66 75.87 75.41 75.96 75."
        },
        {
            "title": "6.1 Contrastive Loss Formulations\nIn this analysis, we compare our InfoNCE loss implementation to other prominent formulations.\nThese approaches primarily differ in the composition of negative samples used in the loss denom-\ninator.",
            "content": "Gecko Model [13] implementation contrasts <query, positive passage> pair against comprehensive set of negatives: (1) single hard negative, (2) other positive passages from different queries in the batch, and (3) other queries in the batch. This third category is termed \"same-tower negatives\" [32], which are noted as being beneﬁcial for symmetric text embedding tasks (e.g., semantic similarity). Qwen3-Embedding family of models [3] uses same-tower negatives not only for the queries, but also for in-batch positive and negative documents. Gemini Embedding [4] explicitly omits the same-tower negatives from the loss to avoid potential false negatives. The loss denominator is thus limited to only single hard negative and other positive passages in the batch. Ours: In contrast, our approach simpliﬁes the loss denominator to include only hard negative documents (HNs) (one in pretraining, four in ﬁne-tuning). This formulation omits all the inbatch negatives and same-tower negatives. To have fair comparison, we have ﬁxed all the hyperaparameters, and only tuned learning rate for each loss separately. As shown in Table 4, all approaches achieve similar performance. This suggests that the inclusion of in-batch negatives or same-tower negatives provides minimalto-no signiﬁcant beneﬁt over our simpler approach. Our implementation, which relies only on hard negatives, achieves the highest number of Borda votes (38,225) and wins the most individual task types."
        },
        {
            "title": "6.2 Choice of LLM for Synthetic Data Generation\nFor training llama-embed-nemotron-8b we relied on multiple open-weights LLMs to generate syn-\nthetic data for various problem types: retrieval, classiﬁcation, STS, and bitext mining. This analysis\nfocuses on evaluating LLMs for the task of generating classiﬁcation datasets. Following [1, 23],\nwe created synthetic examples in 2 steps:",
            "content": "Step 1. Prompt LLM to generate list of potential classiﬁcation tasks. These tasks are also used as instructions in our instruction-aware training. Step 2. Given the classiﬁcation task, prompt LLM to generate (a) text sample, (b) the correct label, and (c) list of plausible but incorrect labels (misleading labels). We use the correct label name as positive and the misleading label names as negatives. Our baseline is model that was trained without any synthetic classiﬁcation datasets. We compare it against generating 100k synthetic samples using each of the LLMs, and training separate embedding models with extra 100k examples in the data mix. For each LLM we follow Step 1 and Step 2 described above, and generate data only in English language. LLMs being evaluated include gpt-oss-20b and gpt-oss-120b [24], Mixtral-8x22B-Instruct-v0.1 [25], Llama-3.3-70BInstruct [11], Llama-4-Scout-17B-16E-Instruct and Llama-4-Maverick-17B-128E-Instruct [26]. By comparing performance on individual MMTEB evaluation datasets, we observed that different LLMs for SDG excel in different domains/languages. Therefore, we also compare the performance of each individual LLM with another mix of 100k synthetic samples, which mixes examples from all the LLMs with equal weights (i.e., 16.7k samples from each of the 6 models). See our full results in Table 5. We compare results on multiple task types including classiﬁcation, multilabel classiﬁcation and clustering, which is also closely related to the classiﬁcation. Results suggest that the largest LLM is not necessarily the best model for SDG, as gpt-oss-20b performs very well, while being the smallest model. But the best results are achieved by using data mix compiled from all the LLMs. These ﬁndings suggest that diversity of synthetic data is more important than single-model quality. One of the reasons for such behavior might be that Mix approach has more diverse tasks list from the Step 1 compared to individual LLMs. This mixing approach for SDG is used in both our pretraining and ﬁne-tuning datasets, and across all the problem types. We also extend this principle by using cross-model SDG, where Step 1 classiﬁcation tasks are generated with one model, while Step 2 actual samples are generated with another model. To conclude this analysis, we compare baseline without synthetic classiﬁcation data to the best SDG approach. We can see how only 100k synthetic classiﬁcation examples show an improvement of +464 Borda votes (37,812 vs 37,348) and +0.94 in Mean points (62.89 vs 61.95). This demonstrates the efﬁcacy of synthetic data. The next ablation explores how good synthetic data is, compared to in-domain classiﬁcation datasets. Impact of Synthetic vs. In-Domain Data"
        },
        {
            "title": "6.3\nThe MTEB leaderboard [6] tracks whether models use in-domain data in their data mixes (e.g., the\ntraining split of an evaluation dataset). This \"zero-shot\" percentage is tracked because in-domain\ndata can signiﬁcantly inﬂate performance on a speciﬁc evaluation dataset, making comparisons\ndifﬁcult.",
            "content": "This ablation study quantiﬁes the gap between our synthetic data and in-domain data. The goal is to measure the effectiveness of our synthetic mix in closing the performance gap to 8 Table 5: Comparison of different LLMs for generating synthetic classiﬁcation datasets."
        },
        {
            "title": "Borda\nVotes",
            "content": "Mean (Task) Class. Clust. Multi. Class. - 37, 61.95 62.16 49.94 22.40 gpt-oss-20b gpt-oss-120b Mixtral-8x22B-Instruct-v0.1 Llama-3.3-70B-Instruct Llama-4-Scout-17B-16E-Instruct Llama-4-Maverick-17B-128E-Instruct Mix from all models 21B (3.6B active) 117B (5.1B active) 141B (39B active) 70B 109B (17B active) 400B (17B active) - 37,732 37,594 37,797 37,623 37,595 37,643 37,812 62.54 62.38 62.64 62.49 62.21 62.36 62.89 63.71 63.12 63.67 63.15 63.02 63.20 64.39 50.45 50.77 50.89 50.80 50.57 50.51 50.95 23.21 22.47 22.81 22.85 22.70 22.41 23.37 Table 6: Comparison of synthetic datasets against in-domain data."
        },
        {
            "title": "Amazon Czech Greek Estonian TweetTopic",
            "content": "Baseline (no synthetic data) 73.06 64.68 38.21 +1M synthetic samples +75k in-domain samples 82.64 90. 66.58 71.73 42.60 60.51 38.80 49.36 57.20 72.85 78.00 80. model trained on in-domain data. For this analysis, we randomly selected ﬁve classiﬁcation evaluation datasets from MTEB(Multilingual, v2), namely AmazonCounterfactualClassiﬁcation [33], CzechProductReviewSentimentClassiﬁcation [34], GreekLegalCodeClassiﬁcation [35], EstonianValenceClassiﬁcation [36], and TweetTopicSingleClassiﬁcation [37]. The baseline model is model trained without any synthetic classiﬁcation datasets. It is compared to two other models. Crucially, the model trained on in-domain data described below was prepared solely for this ablation study to serve as comparative benchmark. This in-domain data was not used in our ﬁnal llama-embed-nemotron-8b model submitted to the MTEB leaderboard. First model is trained on about 1M synthetic classiﬁcation samples generated with the approach described in Section 6.2. Second model is trained on train splits of all the ﬁve datasets being evaluated: AmazonCounterfactualClassiﬁcation (17.7k observations), CzechProductReviewSentimentClassiﬁcation (24.0k), GreekLegalCodeClassiﬁcation (28.5k), EstonianValenceClassiﬁcation (3.3k) and TweetTopicSingleClassiﬁcation (1.5k). Results are presented in Table 6. The model trained on our synthetic classiﬁcation data consistently outperforms baseline model across all ﬁve tasks. However, model trained on in-domain data achieves the highest scores, substantially outperforming the synthetic data mix. Notably, even very small amount of in-domain data provides powerful signal, with 1.5k train samples from TweetTopicSingleClassiﬁcation dataset surpassing about 1M synthetic samples. This shows that while our synthetic data allows to improve general performance on classiﬁcation benchmarks, it is not complete substitute for acquiring even small amounts of high-quality, in-domain data."
        },
        {
            "title": "6.4 Model Merging\nIn Section 3.3, we discussed a model merging technique that enhances model’s generalizability at\nno additional inference costs. This ablation quantiﬁes the impact of this technique by comparing",
            "content": "9 Table 7: Evaluation results of individual checkpoints and the ﬁnal llama-embed-nemotron-8b model on the MTEB(Multilingual, v2) Leaderboard. llama-embed-nemotron-8b is an average of six individual models listed in the table. Model Individual model 1 Individual model 2 Individual model 3 Individual model 4 Individual model 5 Individual model 6 Borda Votes 39,167 39,265 39,336 39,401 39,435 39, Mean (Task) Mean (Type) Bitext Mining Class. Clust. 67.27 67.99 68.00 68.36 68.38 68. 59.36 59.78 59.71 60.18 60.05 60.37 78.45 78.60 79.33 79.52 79.37 79.56 70.17 71.83 71.78 71.89 71.87 72.37 llama-embed-nemotron-8b 39,573 69. 61.09 81.72 73.21 Instr. Rerank. 10.28 10.42 9.14 10.32 9.48 10.80 Multi. Class. 27.86 28.59 28.64 29.20 28.72 29.71 Pair Class. 83.25 83.86 83.76 83.82 83.91 83.60 Rerank. Retrieval STS 66.84 66.85 66.83 67.04 66.85 66.91 64.99 65.77 64.86 66.15 67.33 66.99 78.27 78.33 78.82 78.76 78.83 79.08 10.82 29.86 83. 67.78 68.69 79.41 54.11 53.79 54.27 54.91 54.12 54.34 54.35 the merged model against its individual component checkpoints. Our ﬁnal llama-embed-nemotron8b model is an average of six diverse individual models, weighted equally. This diversity stems from varying data mixes and hyperparameter sets used during training. In Table 7, we present the MTEB(Multilingual, v2) results for the individual checkpoints and the ﬁnal merged model. Notably, our best individual model (\"Model 6\") would already achieve SOTA performance on the MMTEB Leaderboard, securing 39,454 Borda votes (as of October 21, 2025). However, model merging yields signiﬁcantly stronger model, improving the score by +119 Borda votes (39,573 vs 39,454) and mean by +0.84 (69.46 vs 68.62) over \"Model 6\". key observation is that the individual models specialize in different task types. For instance, \"Model 4\" specializes in clustering and reranking; \"Model 5\" in pair classiﬁcation and retrieval; \"Model 6\" in classiﬁcation and STS. Merging all six checkpoints together creates robust cumulative ensemble that aggregates these complementary strengths. This results in the strongest overall model, which achieves the top score in almost all problem types (apart from clustering)."
        },
        {
            "title": "7 Conclusion\nIn this work, we introduced llama-embed-nemotron-8b, a new open-weights universal text em-\nbedding model. Our model achieves state-of-the-art performance, securing the #1 position on the\nMMTEB leaderboard as of October 21, 2025. It demonstrates superior generalization according\nto the Borda count method, outperforming other top models across a diverse set of 131 tasks and\n250+ languages.",
            "content": "This result is driven by combination of strong Llama-3.1-8B foundational model converted to bi-directional encoder, novel 16M-pair training data mix, and robust training methodology. Our ablation studies revealed that using mix of diverse open-weights LLMs for synthetic data generation yields more robust results than using any single LLM, emphasizing the importance of data diversity. By releasing the weights of the llama-embed-nemotron-8b model, we provide powerful, instruction-aware tool for wide range of applications, including retrieval, classiﬁcation, and STS. We also plan to release our curated data mix to facilitate future research and development in robust, multilingual text embeddings. References [1] Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nv-embed: Improved techniques for training llms as generalist embedding models, 2025. 10 [2] Gabriel de Souza Moreira, Radek Osmulski, Mengyao Xu, Ronay Ak, Benedikt Schifferer, and Even Oldridge. Nv-retriever: Improving text embedding models with effective hardnegative mining. arXiv preprint arXiv:2407.15831, 2024. [3] Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, Fei Huang, and Jingren Zhou. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176, 2025. [4] Jinhyuk Lee, Feiyang Chen, Sahil Dua, Daniel Cer, Madhuri Shanbhogue, Iftekhar Naim, Gustavo Hernández Ábrego, Zhe Li, Kaifeng Chen, Henrique Schechter Vera, Xiaoqi Ren, Shanfeng Zhang, Daniel Salz, Michael Boratko, Jay Han, Blair Chen, Shuo Huang, Vikram Rao, Paul Suganthan, Feng Han, Andreas Doumanoglou, Nithi Gupta, Fedor Moiseev, Cathy Yip, Aashi Jain, Simon Baumgartner, Shahrokh Shahi, Frank Palma Gomez, Sandeep Mariserla, Min Choi, Parashar Shah, Sonam Goenka, Ke Chen, Ye Xia, Koert Chen, Sai Meher Karthik Duddu, Yichang Chen, Trevor Walker, Wenlei Zhou, Rakesh Ghiya, Zach Gleicher, Karan Gill, Zhe Dong, Mojtaba Seyedhosseini, Yunhsuan Sung, Raphael Hoffmann, and Tom Duerig. Gemini embedding: Generalizable embeddings from gemini, 2025. [5] Niklas Muennighoff, Nouamane Tazi, Loïc Magne, and Nils Reimers. Mteb: Massive text embedding benchmark. arXiv preprint arXiv:2210.07316, 2022. [6] Kenneth Enevoldsen, Isaac Chung, Imene Kerboua, Márton Kardos, Ashwin Mathur, David Stap, Jay Gala, Wissam Siblini, Dominik Krzemiński, Genta Indra Winata, Saba Sturua, Saiteja Utpala, Mathieu Ciancone, Marion Schaeffer, Gabriel Sequeira, Diganta Misra, Shreeya Dhakal, Jonathan Rystrøm, Roman Solomatin, Ömer Çağatan, Akash Kundu, Martin Bernstorff, Shitao Xiao, Akshita Sukhlecha, Bhavish Pahwa, Rafał Poświata, Kranthi Kiran GV, Shawon Ashraf, Daniel Auras, Björn Plüster, Jan Philipp Harries, Loïc Magne, Isabelle Mohr, Mariya Hendriksen, Dawei Zhu, Hippolyte Gisserot-Boukhlef, Tom Aarsen, Jan Kostkan, Konrad Wojtasik, Taemin Lee, Marek Šuppa, Crystina Zhang, Roberta Rocca, Mohammed Hamdy, Andrianos Michail, John Yang, Manuel Faysse, Aleksei Vatolin, Nandan Thakur, Manan Dey, Dipam Vasani, Pranjal Chitale, Simone Tedeschi, Nguyen Tai, Artem Snegirev, Michael Günther, Mengzhou Xia, Weijia Shi, Xing Han Lù, Jordan Clive, Gayatri Krishnakumar, Anna Maksimova, Silvan Wehrli, Maria Tikhonova, Henil Panchal, Aleksandr Abramov, Malte Ostendorff, Zheng Liu, Simon Clematide, Lester James Miranda, Alena Fenogenova, Guangyu Song, Ruqiya Bin Saﬁ, Wen-Ding Li, Alessia Borghini, Federico Cassano, Hongjin Su, Jimmy Lin, Howard Yen, Lasse Hansen, Sara Hooker, Chenghao Xiao, Vaibhav Adlakha, Orion Weller, Siva Reddy, and Niklas Muennighoff. Mmteb: Massive multilingual text embedding benchmark, 2025. [7] Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, Céline Hudelot, and Pierre Colombo. Colpali: Efﬁcient document retrieval with vision language models, 2024. [8] Mengyao Xu, Gabriel Moreira, Ronay Ak, Radek Osmulski, Yauhen Babakhin, Zhiding Yu, Benedikt Schifferer, and Even Oldridge. Llama nemoretriever colembed: Top-performing text-image retrieval model. arXiv preprint arXiv:2507.05513, 2025. [9] Michael Günther, Saba Sturua, Mohammad Kalim Akram, Isabelle Mohr, Andrei Ungureanu, Bo Wang, Sedigheh Eslami, Scott Martens, Maximilian Werk, Nan Wang, and Han Xiao. jina-embeddings-v4: Universal embeddings for multimodal multilingual retrieval, 2025. 11 [10] Mengyao Xu, Wenfei Zhou, Yauhen Babakhin, Gabriel Moreira, Ronay Ak, Radek Osmulski, Bo Liu, Even Oldridge, and Benedikt Schifferer. Omni-embed-nemotron: uniﬁed multimodal retrieval model for text, image, audio, and video, 2025. [11] Aaron Grattaﬁori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heaﬁeld, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Raﬁ Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterﬁeld, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, 13 Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models, 2024. [12] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 15971607. PmLR, 2020. [13] Jinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Jeremy R. Cole, Kai Hui, Michael Boratko, Rajvi Kapadia, Wen Ding, Yi Luan, Sai Meher Karthik Duddu, Gustavo Hernandez Abrego, Weiqiang Shi, Nithi Gupta, Aditya Kusupati, Prateek Jain, Siddhartha Reddy Jonnalagadda, Ming-Wei Chang, and Iftekhar Naim. Gecko: Versatile text embeddings distilled from large language models, 2024. [14] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization, 2019. [15] Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. Model soups: averaging weights of multiple ﬁne-tuned models improves accuracy without increasing inference time, 2022. [16] Henrique Schechter Vera, Sahil Dua, Biao Zhang, Daniel Salz, Ryan Mullins, Sindhu Raghuram Panyam, Sara Smoot, Iftekhar Naim, Joe Zou, Feiyang Chen, Daniel Cer, Alice Lisak, Min Choi, Lucas Gonzalez, Omar Sanseviero, Glenn Cameron, Ian Ballantyne, Kat Black, Kaifeng Chen, Weiyi Wang, Zhe Li, Gus Martins, Jinhyuk Lee, Mark Sherwood, Juyeong Ji, Renjie Wu, Jingxiao Zheng, Jyotinder Singh, Abheesht Sharma, Divyashree Sreepathihalli, Aashi Jain, Adham Elarabawy, AJ Co, Andreas Doumanoglou, Babak Samari, Ben Hora, Brian Potetz, Dahun Kim, Enrique Alfonseca, Fedor Moiseev, Feng Han, Frank Palma Gomez, Gustavo Hernández Ábrego, Hesen Zhang, Hui Hui, Jay Han, Karan Gill, Ke Chen, Koert Chen, Madhuri Shanbhogue, Michael Boratko, Paul Suganthan, Sai Meher Karthik Duddu, Sandeep Mariserla, Setareh Ariafar, Shanfeng Zhang, Shijie Zhang, Simon Baumgartner, Sonam Goenka, Steve Qiu, Tanmaya Dabral, Trevor Walker, Vikram Rao, Waleed Khawaja, Wenlei Zhou, Xiaoqi Ren, Ye Xia, Yichang Chen, Yi-Ting Chen, Zhe Dong, Zhongli Ding, Francesco Visin, Gaël Liu, Jiageng Zhang, Kathleen Kenealy, Michelle Casbon, Ravin Kumar, Thomas Mesnard, Zach Gleicher, Cormac Brick, Olivier Lacombe, Adam Roberts, Qin Yin, Yunhsuan Sung, Raphael Hoffmann, Tris Warkentin, Armand Joulin, Tom Duerig, and Mojtaba Seyedhosseini. Embeddinggemma: Powerful and lightweight text representations, 2025. [17] Aarti Basant, Abhijit Khairnar, Abhijit Paithankar, Abhinav Khattar, Adithya Renduchintala, Aditya Malte, Akhiad Bercovich, Akshay Hazare, Alejandra Rico, Aleksander Ficek, Alex Kondratenko, Alex Shaposhnikov, Alexander Bukharin, Ali Taghibakhshi, Amelia Barton, Ameya Sunil Mahabaleshwarkar, Amy Shen, Andrew Tao, Ann Guan, Anna Shors, Anubhav Mandarwal, Arham Mehta, Arun Venkatesan, Ashton Sharabiani, Ashwath Aithal, Ashwin Poojary, Ayush Dattagupta, Balaram Buddharaju, Banghua Zhu, Barnaby Simkin, Bilal Kartal, Bita Darvish Rouhani, Bobby Chen, Boris Ginsburg, Brandon Norick, Brian Yu, Bryan Catanzaro, Charles Wang, Charlie Truong, Chetan Mungekar, Chintan Patel, Chris Alexiuk, Christian Munley, Christopher Parisien, Dan Su, Daniel Afrimi, Daniel Korzekwa, Daniel Rohrer, Daria Gitman, David Mosallanezhad, Deepak Narayanan, Dima Rekesh, Dina Yared, Dmytro Pykhtar, Dong Ahn, Duncan Riach, Eileen Long, Elliott Ning, Eric Chung, 14 Erick Galinkin, Evelina Bakhturina, Gargi Prasad, Gerald Shen, Haifeng Qian, Haim Elisha, Harsh Sharma, Hayley Ross, Helen Ngo, Herman Sahota, Hexin Wang, Hoo Chang Shin, Hua Huang, Iain Cunningham, Igor Gitman, Ivan Moshkov, Jaehun Jung, Jan Kautz, Jane Polak Scowcroft, Jared Casper, Jian Zhang, Jiaqi Zeng, Jimmy Zhang, Jinze Xue, Jocelyn Huang, Joey Conway, John Kamalu, Jonathan Cohen, Joseph Jennings, Julien Veron Vialard, Junkeun Yi, Jupinder Parmar, Kari Briski, Katherine Cheung, Katherine Luna, Keith Wyss, Keshav Santhanam, Kezhi Kong, Krzysztof Pawelec, Kumar Anik, Kunlun Li, Kushan Ahmadian, Lawrence McAfee, Laya Sleiman, Leon Derczynski, Luis Vega, Maer Rodrigues de Melo, Makesh Narsimhan Sreedhar, Marcin Chochowski, Mark Cai, Markus Kliegl, Marta Stepniewska-Dziubinska, Matvei Novikov, Mehrzad Samadi, Meredith Price, Meriem Boubdir, Michael Boone, Michael Evans, Michal Bien, Michal Zawalski, Miguel Martinez, Mike Chrzanowski, Mohammad Shoeybi, Mostofa Patwary, Namit Dhameja, Nave Assaf, Negar Habibi, Nidhi Bhatia, Nikki Pope, Nima Tajbakhsh, Nirmal Kumar Juluru, Oleg Rybakov, Oleksii Hrinchuk, Oleksii Kuchaiev, Oluwatobi Olabiyi, Pablo Ribalta, Padmavathy Subramanian, Parth Chadha, Pavlo Molchanov, Peter Dykas, Peter Jin, Piotr Bialecki, Piotr Januszewski, Pradeep Thalasta, Prashant Gaikwad, Prasoon Varshney, Pritam Gundecha, Przemek Tredak, Rabeeh Karimi Mahabadi, Rajen Patel, Ran El-Yaniv, Ranjit Rajan, Ria Cheruvu, Rima Shahbazyan, Ritika Borkar, Ritu Gala, Roger Waleffe, Ruoxi Zhang, Russell J. Hewett, Ryan Prenger, Sahil Jain, Samuel Kriman, Sanjeev Satheesh, Saori Kaji, Sarah Yurick, Saurav Muralidharan, Sean Narenthiran, Seonmyeong Bak, Sepehr Sameni, Seungju Han, Shanmugam Ramasamy, Shaona Ghosh, Sharath Turuvekere Sreenivas, Shelby Thomas, Shizhe Diao, Shreya Gopal, Shrimai Prabhumoye, Shubham Toshniwal, Shuoyang Ding, Siddharth Singh, Siddhartha Jain, Somshubra Majumdar, Soumye Singhal, Stefania Alborghetti, Syeda Nahida Akter, Terry Kong, Tim Moon, Tomasz Hliwiak, Tomer Asida, Tony Wang, Tugrul Konuk, Twinkle Vashishth, Tyler Poon, Udi Karpas, Vahid Noroozi, Venkat Srinivasan, Vijay Korthikanti, Vikram Fugro, Vineeth Kalluru, Vitaly Kurin, Vitaly Lavrukhin, Wasi Uddin Ahmad, Wei Du, Wonmin Byeon, Ximing Lu, Xin Dong, Yashaswi Karnati, Yejin Choi, Yian Zhang, Ying Lin, Yonggan Fu, Yoshi Suhara, Zhen Dong, Zhiyu Li, Zhongbo Zhu, and Zijia Chen. Nvidia nemotron nano 2: An accurate and efﬁcient hybrid mamba-transformer reasoning model, 2025. [18] Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kamalloo, David AlfonsoHermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin. Miracl: multilingual retrieval dataset covering 18 diverse languages. Transactions of the Association for Computational Linguistics, 11:11141131, 2023. [19] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018. [20] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et al. Ms marco: human generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268, 2016. [21] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466, 2019. [22] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016. 15 [23] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Improving text embeddings with large language models, 2024. [24] OpenAI. gpt-oss-120b & gpt-oss-20b model card, 2025. [25] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mixtral of experts, 2024. [26] Meta AI. The llama 4 herd: The beginning of new era of natively multimodal intelligence. https://ai.meta.com/blog/llama-4-multimodal-intelligence/, 2025. [27] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. arXiv preprint Towards general text embeddings with multi-stage contrastive learning. arXiv:2308.03281, 2023. [28] Junseong Kim, Seolhwa Lee, Jihoon Kwon, Sangmo Gu, Minkyung Cho Yejin Kim, Jy yong Sohn, and Chanyeol Choi. Linq-embed-mistral:elevating text retrieval with improved gpt data through task-speciﬁc control and quality reﬁnement. Linq AI Research Blog, 2024. [29] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Multilingual e5 text embeddings: technical report. arXiv preprint arXiv:2402.05672, 2024. [30] Rui Meng, Ye Liu, Shaﬁq Rayhan Joty, Caiming Xiong, Yingbo Zhou, and Semih Yavuz. Sfr-embedding-mistral:enhance text retrieval with transfer learning. Salesforce AI Research Blog, 2024. [31] Pierre Colombo, Nathan Noiry, Ekhine Irurozki, and Stephan Clemencon. What are the best systems? new perspectives on nlp benchmarking, 2022. [32] Fedor Moiseev, Gustavo Hernandez Abrego, Peter Dornbach, Imed Zitouni, Enrique Alfonseca, and Zhe Dong. Samtone: Improving contrastive loss for dual encoder retrieval models with same tower negatives, 2023. [33] James ONeill, Polina Rozenshtein, Ryuichi Kiryo, Motoko Kubota, and Danushka Bollegala. wish would have loved this one, but didnt multilingual dataset for counterfactual detection in product review. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 70927108, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. [34] Ivan Habernal, Tomáš Ptáček, and Josef Steinberger. Sentiment analysis in Czech social media using supervised machine learning. In Alexandra Balahur, Erik van der Goot, and Andres Montoyo, editors, Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 6574, Atlanta, Georgia, June 2013. Association for Computational Linguistics. [35] Christos Papaloukas, Ilias Chalkidis, Konstantinos Athinaios, Despina-Athanasia Pantazi, and Manolis Koubarakis. Multi-granular legal topic classiﬁcation on greek legislation. In Proceedings of the Natural Legal Language Processing Workshop 2021, pages 6375, Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. 16 [36] Hille Pajupuu, Jaan Pajupuu, Rene Altrov, and Kairi Tamuri. Estonian Valence Corpus / Eesti valentsikorpus. 11 2023. [37] Dimosthenis Antypas, Asahi Ushio, Jose Camacho-Collados, Leonardo Neves, Vitor Silva, and Francesco Barbieri. Twitter Topic Classiﬁcation. In Proceedings of the 29th International Conference on Computational Linguistics, Gyeongju, Republic of Korea, October 2022. International Committee on Computational Linguistics. [38] Julian McAuley and Jure Leskovec. Hidden factors and hidden topics: understanding rating dimensions with review text. In Proceedings of the 7th ACM Conference on Recommender Systems, RecSys 13, page 165172, New York, NY, USA, 2013. Association for Computing Machinery. [39] George Tsatsaronis, Georgios Balikas, Prodromos Malakasiotis, Ioannis Partalas, Matthias Zschunke, Michael Alvers, Dirk Weissenborn, Anastasia Krithara, Sergios Petridis, Dimitris Polychronopoulos, et al. An overview of the bioasq large-scale biomedical semantic indexing and question answering competition. BMC bioinformatics, 16:128, 2015. [40] Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. Arnetminer: extraction and mining of academic social networks. In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 08, page 990998, New York, NY, USA, 2008. Association for Computing Machinery. [41] Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang, Junlin Wu, and Yi-Shin Chen. CARER: Contextualized affect representations for emotion recognition. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 36873697, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. [42] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: large-scale dataset for fact extraction and veriﬁcation. arXiv preprint arXiv:1803.05355, 2018. [43] Daniel Khashabi, Amos Ng, Tushar Khot, Ashish Sabharwal, Hannaneh Hajishirzi, and Chris Callison-Burch. Gooaq: Open question answering with diverse answer types, 2021. [44] Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Maneesh Singh, and Mohit Bansal. Hover: dataset for many-hop fact extraction and claim veriﬁcation. arXiv preprint arXiv:2011.03088, 2020. [45] Yuchen Zhuang, Aaron Trinh, Rushi Qiang, Haotian Sun, Chao Zhang, Hanjun Dai, and Bo Dai. Towards better instruction following retrieval models, 2025. [46] Xiang Yue, Tuney Zheng, Ge Zhang, and Wenhu Chen. Mammoth2: Scaling instructions from the web. Advances in Neural Information Processing Systems, 2024. [47] Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge m3embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation, 2024. [48] Xinyu Zhang, Xueguang Ma, Peng Shi, and Jimmy Lin. Mr. tydi: multi-lingual benchmark for dense retrieval. arXiv preprint arXiv:2108.08787, 2021. [49] Adina Williams, Nikita Nangia, and Samuel R. Bowman. broad-coverage challenge corpus for sentence understanding through inference, 2018. 17 [50] Vera Boteva, Demian Gholipour, Artem Sokolov, and Stefan Riezler. full-text learning to rank dataset for medical information retrieval. 2016. [51] Patrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale Minervini, Heinrich Küttler, Aleksandra Piktus, Pontus Stenetorp, and Sebastian Riedel. Paq: 65 million probably-asked questions and what you can do with them. Transactions of the Association for Computational Linguistics, 9:10981115, 2021. [52] DataCanary, hilﬁalkaff, Lili Jiang, Meg Risdal, Nikhil Dandekar, and tomtung. Quora question pairs, 2017. [53] Gregor Geigle, Nils Reimers, Andreas Rücklé, and Iryna Gurevych. Tweac: Transformer with extendable qa agent classiﬁers. arXiv preprint, abs/2104.07081, 2021. [54] David Wadden, Kyle Lo, Bailey Kuehl, Arman Cohan, Iz Beltagy, Lucy Lu Wang, and Hannaneh Hajishirzi. Scifact-open: Towards open-domain scientiﬁc claim veriﬁcation. arXiv preprint arXiv:2210.13777, 2022. [55] Stack-Exchange-Community. Stack exchange data dump, 2023. [56] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Superglue: stickier benchmark for generalpurpose language understanding systems, 2020. [57] Bo Pang and Lillian Lee. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the ACL, 2005. [58] cjadams, Jeffrey Sorensen, Julia Elliott, Lucas Dixon, Mark McDonald, nithum, and Will Cukierski. https://kaggle.com/ competitions/jigsaw-toxic-comment-classification-challenge, 2017. Kaggle. Toxic comment classiﬁcation challenge. [59] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. triviaqa: Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. arXiv e-prints, page arXiv:1705.03551, 2017. [60] Silly-Machine. Tupy-dataset (revision de6b18c), 2023. 18 Implementation Details This section details the hyperparameters and implementation speciﬁcs for our llama-embed-nemotron8b model. Pretraining was conducted for 25.0 hours, and ﬁne-tuning for 21.5 hours. Both stages utilized cluster of 64 NVIDIA A100 80GB GPUs. The hyperparameters for both training stages are summarized in Table 8. Table 8: Main hyperparameters for llama-embed-nemotron-8b training."
        },
        {
            "title": "Pretraining",
            "content": "Fine-tuning Peak learning rate Batch size Number of steps Scheduler Warm-up steps Optimizer Weight decay Number of hard negatives Temperature Query max length Document max length 1e-5 2,048 5,773 2e-6 128 33,"
        },
        {
            "title": "100\nAdamW\n0.01\n4\n0.02\n512\n512",
            "content": "B Fine-Tuning Data Mix for llama-embed-nemotron-8b This section details the high-quality, curated data mix used in the ﬁne-tuning stage of the llamaembed-nemotron-8b training. The complete dataset consists of 4.3 million samples, sourced from diverse range of corpora, including multi-lingual and cross-lingual data. The mix is composed of approximately 2.7 million non-synthetic samples from public sources and 1.6 million synthetic samples generated to target speciﬁc model capabilities. detailed breakdown of the component datasets and their respective sample sizes is provided in Table 9. 19 Table 9: Component datasets and sample counts for the llama-embed-nemotron-8b ﬁne-tuning data mix."
        },
        {
            "title": "Number of samples",
            "content": "AmazonReviews [38] BioASQ [39] DBLP-Citation-network V17 [40] EmotionClassiﬁcation [41] FEVER [42] GooAQ [43] HotpotQA [19] HoVer [44] InF-IR [45] MAmmoTH2 stackexchange [46] MIRACL [18] MLDR [47] Mr.TyDi [48] MS MARCO[20] MultiNLI [49] Natural Questions [21] NFCorpus [50] PAQ [51] Quora question pairs [52] RedditClustering [53] SciFact [54] SQuAD [22] Stack Exchange [55] SuperGLUE Textual Entailment [56] Synthetic bitext mining data [57] Synthetic classiﬁcation data Synthetic retrieval data Synthetic STS data Toxic Comment Classiﬁcation [58] TriviaQA [59] TuPy [60]"
        },
        {
            "title": "Total",
            "content": "60,000 2,495 25,000 13,046 140,085 100,000 170,000 29,721 77,518 317,180 79,648 9,500 12,610 500,000 75,505 100,231 3,685 500,000 101,762 90,000 919 87,599 80,001 3,094 169,534 1,044,212 182,814 239,997 16,800 73,346 3,200 4,309,"
        }
    ],
    "affiliations": [
        "NVIDIA"
    ]
}