{
    "paper_title": "Detecting Overflow in Compressed Token Representations for Retrieval-Augmented Generation",
    "authors": [
        "Julia Belikova",
        "Danila Rozhevskii",
        "Dennis Svirin",
        "Konstantin Polev",
        "Alexander Panchenko"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Efficient long-context processing remains a crucial challenge for contemporary large language models (LLMs), especially in resource-constrained environments. Soft compression architectures promise to extend effective context length by replacing long token sequences with smaller sets of learned compressed tokens. Yet, the limits of compressibility -- and when compression begins to erase task-relevant content -- remain underexplored. In this paper, we define token overflow as a regime in which compressed representations no longer contain sufficient information to answer a given query, and propose a methodology to characterize and detect it. In the xRAG soft-compression setting, we find that query-agnostic saturation statistics reliably separate compressed from uncompressed token representations, providing a practical tool for identifying compressed tokens but showing limited overflow detection capability. Lightweight probing classifiers over both query and context xRAG representations detect overflow with 0.72 AUC-ROC on average on HotpotQA, SQuADv2, and TriviaQA datasets, demonstrating that incorporating query information improves detection performance. These results advance from query-independent diagnostics to query-aware detectors, enabling low-cost pre-LLM gating to mitigate compression-induced errors."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 3 1 ] . [ 2 5 3 2 2 1 . 2 0 6 2 : r Detecting Overflow in Compressed Token Representations for Retrieval-Augmented Generation Julia Belikova1,2, Danila Rozhevskii1, Dennis Svirin1,4, Konstantin Polev2, and Alexander Panchenko1,3 1Skoltech, 2Sber AI Lab, 3AIRI 4Institute for Information Transmission Problems of the Russian Academy of Sciences Correspondence: {julia.belikova, a.panchenko}@skol.tech"
        },
        {
            "title": "Abstract",
            "content": "Efficient long-context processing remains crucial challenge for contemporary large language models (LLMs), especially in resourceconstrained environments. Soft compression architectures promise to extend effective context length by replacing long token sequences with smaller sets of learned compressed tokens. Yet, the limits of compressibility and when compression begins to erase task-relevant content remain underexplored. In this paper, we define token overflow as regime in which compressed representations no longer contain sufficient information to answer given query, and propose methodology to characterize and detect it. In the xRAG soft-compression setting, we find that query-agnostic saturation statistics reliably separate compressed from uncompressed token representations, providing practical tool for identifying compressed tokens but showing limited overflow detection capability. Lightweight probing classifiers over both query and context xRAG representations detect overflow with 0.72 AUC-ROC on average on HotpotQA, SQuADv2, and TriviaQA datasets, demonstrating that incorporating query information improves detection performance. These results advance from query-independent diagnostics to query-aware detectors, enabling lowcost pre-LLM gating to mitigate compressioninduced errors."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) remain computationally constrained when processing long contexts, even as architectural advances and extended context windows become widely available (Vaswani et al., 2017; Liu et al., 2024). In retrievalaugmented generation (RAG), this limitation is particularly acute: retrieved evidence must be aggressively compressed or truncated, creating tension between efficiency and faithfulness (Lewis et al., 2020; Aushev et al., 2025). Soft compression architectures address this by mapping long contexts into dense vectors that can be directly consumed by the model, dramatically reducing token count while preserving global semantics (Liao et al., 2025). However, the same mechanism that enables extreme compression also introduces critical failure mode. As more information is packed into fixed-dimensional compressed token, its representation can enter token overflow: it no longer carries sufficient task-relevant signal for the query and effectively behaves like noise, silently degrading downstream performance. Recent work on trainable tokens shows that individual embeddings have substantial theoretical capacity, but also that practical limits depend strongly on architecture, training, and input complexity (Kuratov et al., 2025). Yet, current compression systems are typically evaluated only via end-task metrics, offering little insight into when single compressed token crosses from informative to overflowed states. This paper investigates token overflow in soft compression architectures. We ask: (RQ1) How can we characterize overflow in compressed representations? (RQ2) Can overflow be detected efficiently, without full LLM inference, using lightweight diagnostics? (RQ3) Is overflow detectable from compressed tokens alone, or does it require modeling query-context interactions? To address these questions, we: formalize token overflow and propose methodology advancing from queryindependent to query-aware detection approaches; demonstrate that saturation statistics reliably distinguish compressed tokens from standard tokens, providing practical tool for identifying compressed representations, but show limited overflow detection capability; show that attention patterns during generation provide moderate overflow signal but require LLM forward passes; develop learned probing classifiers operating on joint query-context representations that achieve strong overflow detection without LLM inference  (Table 1)  , showing that incorporating query information improves detection performance; Although our experiments focus on the xRAG architecture, the methodology is general and we expect it to yield similar results when applied to other setups. The source code is publicly available online1."
        },
        {
            "title": "2 Related Work",
            "content": "Long-context modeling and compression Efficient long-context processing has been tackled through architectural changes (Beltagy et al., 2020; Zaheer et al., 2020; Dai et al., 2019) and explicit compression. Context compression is systematized into hard, soft, and hybrid paradigms (Liao et al., 2025): hard compression selects token subsets with strict information bottlenecks; soft compression maps contexts into dense vectors accessible via attention; hybrid methods combine both approaches. Our work analyzes the failure modes of soft compression, asking when compressed vectors fail to carry useful task information. Soft compression in RAG Retrieval-augmented generation (RAG) frameworks extend LLMs with external corpora (Lewis et al., 2020), motivating compression of retrieved passages. Several soft compression methods have been proposed: AutoCompressors (Chevalier et al., 2023) learn summary vectors by training the model to reconstruct compressed context through attention; ICAE (Ge et al., 2024) employs in-context autoencoding to compress sequences into memory slots with combined reconstruction and language modeling objectives (4 compression, 1% parameters). We focus our experiments on xRAG (Cheng et al., 2024), utilizing it not merely as baseline, but as representative projector-based compression paradigm. Unlike autoencoder-based methods that compress context via complex recurrence or reconstruction objectives, xRAG treats dense retrieval embeddings as distinct modality. It employs lightweight projector to map these embeddings directly into the LLMs input space. This architectural choice 1https://github.com/s-nlp/overflow-detection isolates the compression mechanism from the complexities of extensive parameter fine-tuning (<0.1% parameters), allowing us to study the interactions between the projector and the frozen LLM in controlled setting. By decoupling the retrieval representation from the generative process, xRAG provides clean access to preand post-projection states, making it an ideal testbed for analyzing signal degradation and token saturation without the confounders of end-to-end model adaptation. Motivation for overflow detection Detecting information overflow where input complexity exceeds compressed token capacity is critical for optimizing RAG pipelines. It enables adaptive chunking, allowing systems to dynamically resize input segments based on semantic density rather than arbitrary fixed lengths. Furthermore, early overflow detection facilitates computational pruning: identifying and discarding saturated representations immediately after projection prevents wasteful LLM inference on degraded context. Despite progress across methods, most evaluations treat compressed vectors as black boxes and focus on downstream metrics (Ge et al., 2024; Cheng et al., 2024). Recent work shows single vectors can theoretically encode thousands of tokens, yet practical capacity depends on architecture and complexity (Kuratov et al., 2025). In contrast, we operationalize capacity limits through overflow detection in xRAG, advancing from query-independent saturation statistics to query-aware learned probing."
        },
        {
            "title": "3 Methodology",
            "content": "Our goal is to characterize and detect token overflow in soft compression architectures across tasks and context regimes. We focus on xRAG-style compressors attached to frozen LLM backbones, studying how compressed token properties change as context complexity increases and downstream quality degrades. We employ spectrum of detection approaches with increasing query-awareness: from query-agnostic saturation statistics, through query-conditioned attention patterns, to fully queryaware learned probing classifiers. Our methodology is motivated by the observation that the same compressed representation may be sufficient for one query but overflowed for another. This motivates our approach, advancing from query-independent to query-aware detection: 1. Context complexity and saturation statistics (query-agnostic): Measure intrinsic properties of compressed representations independent of any query useful for identifying and characterizing compressed tokens. where ϵ is task-dependent. Exploring such threshold-based criteria with alternative evaluation functions is promising direction for future work. 2. Attention features (query-conditioned): Analyze how the LLM utilizes compressed tokens during generation for specific query captures behavioral signals but requires LLM forward passes. 3.2 Context Complexity Measures For each input context xi (or aggregated retrieved context), we compute set of context complexity measures intended to approximate how hard the context is to compress: 3. Learned probing (query-aware): Train classifiers on joint query-context representations to detect overflow in embedding space achieves strong detection without LLM inference. Context length Nctx: the number of tokens in the original, uncompressed context before any truncation. This is the simplest proxy for potential compression pressure and directly correlates with computational cost. This approach allows us to evaluate whether overflow detection improves as query information is incorporated, while identifying the most efficient deployment strategy."
        },
        {
            "title": "3.1 Problem Setup",
            "content": "Let be frozen LLM and soft compression module (e.g., xRAGs modality-fusion compressor) that maps an input sequence of tokens with embeddings Rnd to compressed tokens = C(X) Rkd. The compressed tokens are then injected into (e.g., as extra prefix tokens or interleaved context) and used to solve downstream task such as extractive QA. Given an input instance with original context xi, question qi, and gold output yi, we define task performance under compression, Ti(Ci), as scalar metric (e.g., F1, EM, or ROUGE), indicating whether the generated answer is judged correct. We compare it to reference performance ref obtained from either (i) an uncompressed baseline (full context within the models window), or (ii) lightly compressed setting where degradation is empirically negligible. We define an overflow state for instance as: Oi = 1(cid:0)T ref = 1 Ti(Ci) = 0(cid:1) . (1) Our objective is to (a) understand how compressed representations differ between overflow and nonoverflow regimes, and (b) learn detectors that can predict overflow from representations alone, without recomputing Ti. More generally, this formulation can be extended by defining overflow via degradation threshold, ref Ti(Ci) ϵ, (2) Language-model perplexity PPLi: the average per-token negative log-likelihood under the base LLM (without compression), which captures how predictable the context is given the models training distribution. Higher perplexity indicates linguistically or semantically atypical content. Statistical compressibility Ri: the compression ratio achieved by standard lossless compressor (e.g., gzip or LZMA) on the raw text. We define Ri = xibytes , where larger valzip(xi)bytes ues indicate more redundancy and thus higher statistical compressibility. These metrics allow us to analyze how overflow correlates with raw length, lexical predictability, and sequence-level redundancy (compressibility)."
        },
        {
            "title": "3.3 Token Saturation Statistics",
            "content": "We quantify saturation at the level of compressed tokens and their propagated hidden states. For each compressed token vector Rd and its corresponding hidden states h(ℓ) at layer ℓ, we compute the following statistics. Hoyers sparsity Hoyers index (Hoyer, 2004) measures how concentrated vectors energy is in few dimensions: H(v) = . (3) v1 v2 1 It ranges from 0 (all components equal) to 1 (only one non-zero component). Informative compressed tokens are hypothesized to exhibit higher sparsity (structured, selective activations), while overflowed tokens tend towards low sparsity (flat, noise-like patterns). Spectral entropy We apply discrete cosine transform (DCT) to and treat the normalized squared magnitudes as an energy distribution over frequency components. The spectral entropy is defined as folowing: (cid:88) S(v) = pi log pi, pi = i=1 DCT(v)i2 DCT(v)2 2 . (4) Low entropy corresponds to concentrated energy (structured signals), whereas near-maximum entropy indicates white-noiselike spectra. Kurtosis We compute the excess kurtosis of the entries of v: K(v) = E[(vj µ)4] σ4 3, (5) where µ and σ are the mean and standard deviation across dimensions. Heavy-tailed distributions (positive kurtosis) suggest few large, informative coordinates, while overflowed tokens are expected to approach Gaussian-like behavior (K 0)."
        },
        {
            "title": "Overflow Signals",
            "content": "While saturation statistics measure intrinsic token properties, they ignore how the LLM actually uses compressed tokens during generation. To capture this behavioral dimension, we extract attentionbased features that quantify the models reliance on xRAG tokens when processing specific query. For each instance, we perform forward pass through the LLM with both the query and compressed context, extracting attention weights RLHT across all layers L, heads H, and sequence positions . We compute: Mean attention to xRAG tokens For each layer ℓ and head h, we measure the average attention mass directed to compressed token positions: a(ℓ,h) xRAG = 1 Tq (cid:88) (cid:88) iTq jTxRAG A(ℓ,h) i,j , (6) where Tq denotes query token positions and TxRAG denotes xRAG token positions. We aggregate across layers and heads to obtain instance-level statistics: mean, max, min, and standard deviation of attention to xRAG tokens. Attention ratios To contextualize xRAG attention, we compute ratios comparing attention to compressed versus uncompressed tokens: rxRAG/non-xRAG = axRAG anon-xRAG . (7) This ratio isolates whether the model preferentially attends to compressed representations or relies more heavily on other context. Attention entropy For each query position i, we compute the entropy of its attention distribution over all positions: Enti = (cid:88) j=1 Ai,j log Ai,j. (8) High entropy indicates diffuse attention (potentially signaling uncertainty or lack of relevant information), while low entropy indicates focused attention to specific tokens (Rykov et al., 2025)."
        },
        {
            "title": "3.5 Overflow Detection Methods",
            "content": "We evaluate overflow detection through two complementary approaches that span spectrum from interpretable to representational methods. First, we test feature-based classification using explicit, hand-crafted features to determine whether overflow manifests in interpretable, low-dimensional signals. Second, we develop learned probing classifiers that operate directly on high-dimensional query and context embedding vectors. Feature-based classification We aggregate the hand-crafted features described in 3.23.4 (context complexity, saturation statistics, attention patterns) and train logistic regression classifier implemented in scikit-learn2. All hyperparameters are detailed in Appendix D. Learned probing on vector representations While feature-based methods offer interpretability, they may fail to capture complex interactions between query and context that manifest in the geometry of representation spaces. We therefore develop learned probing classifiers that operate directly on joint query-context representations. Our hypothesis is that overflow detection requires modeling alignment patterns in shared representation space. 2https://scikit-learn.org Representation extraction For each instance with query qi and context xi, we extract embeddings at multiple stages: explicitly structures the representation space by encouraging same-class instances to cluster while pushing apart opposite-class instances. Query representations: qpreproj triever embedding), qpostproj projection), qmid from intermediate and final LLM layers). Rdret (reRdLLM (after RdLLM (hidden states , qlast Context representations: xpreproj Rdret (rei triever embedding), xpostproj RdLLM (compressed token after projection), xmid RdLLM (hidden states from intermediate and final layers). , xlast We construct joint feature vectors by concatenating query and context representations at matched or complementary stages: For the SCL probe, we minimize the combined objective = LBCE + λ LSCL, (10) where LBCE provides direct classification supervision, while LSCL imposes metric constraints by maximizing cosine similarity between same-label pairs and minimizing it between different-label pairs in the learned representation space."
        },
        {
            "title": "4 Results",
            "content": "ϕi = [ x(sc) ; q(sq) ], (9)"
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "where sc, sq {preproj, postproj, mid, last} denote the extraction stage. Our primary experiments use projection-stage representations (preproj, postproj) which are available immediately after encoding without requiring LLM forward passes. Following prior work demonstrating that intermediate transformer layers encode complementary information useful for interpretation tasks (CH-Wang et al., 2024; Belikova et al., 2025), we additionally evaluate multi-layer representations (including mid, last) to assess the efficiency-accuracy trade-off. Classifier architectures To systematically assess the role of model capacity and training objectives in overflow detection, we evaluate three neural probe architectures: Linear Probe: single linear transformation applied to the joint feature vector ϕi. This minimal architecture tests whether overflow is linearly separable in the concatenated representation space. MLP Probe: two-layer feedforward network with one hidden layer, introducing nonlinear feature interactions while maintaining computational efficiency. MLP Probe with Supervised Contrastive Learning (SCL): An MLP trained with hybrid objective that combines standard binary cross-entropy with supervised contrastive term (Khosla et al., 2020). This architecture As preliminary study, we use the xRAG-7B model3 as the base LLM and SFR-EmbeddingMistral4 as the retriever embedding model for all experiments. We focus on three extractive question answering datasets: SQuADv2 (Rajpurkar et al., 2018), context-based QA benchmark over Wikipedia passages; TriviaQA (Joshi et al., 2017), large-scale reading comprehension dataset with independently collected evidence documents; and HotpotQA (Yang et al., 2018), multi-hop reasoning dataset requiring information synthesis across multiple paragraphs. All three datasets were part of the xRAG compression modules training data, providing realistic testbed for studying overflow in deployed systems. We use test set examples that were correctly answered with uncompressed context, filtering out instances where the model fails regardless of compression. This ensures overflow detection focuses on compression-induced failures rather than inherent task difficulty. Evaluation protocol Answer correctness is evaluated using GPT-4o-mini for SQuADv2, which assesses semantic equivalence between generated and ground-truth answers. For TriviaQA and HotpotQA, we apply substring-based exact-match criterion: predictions are marked correct if they contain any reference answer as substring. All classifiers are evaluated using 5-fold stratified crossvalidation; training and hyperparameter details are provided in 3.5 and Appendix D. Stage Features TriviaQA SQuADv2 HotpotQA Pre-compression Context 0.589 0.019 0.605 0.025 0.541 0.017 Pre-inference Post-inference Representation Representation-joint Saturation Attention Representation Representation-joint Saturation Saturation-joint 0.687 0.015 0.725 0.021 0.568 0.022 0.627 0.012 0.684 0.016 0.719 0.015 0.583 0.019 0.600 0.014 0.662 0.015 0.703 0.019 0.529 0.024 0.608 0.020 0.665 0.016 0.713 0.016 0.546 0.011 0.585 0. 0.653 0.023 0.720 0.007 0.533 0.010 0.623 0.013 0.655 0.024 0.733 0.011 0.549 0.008 0.633 0.015 Table 1: Overflow prediction performance (ROC-AUC) across different pipeline stages. Pre-compression: priori context features. Pre-inference: pre-LLM inference combining preprojection and postprojection features. Post-inference: post-LLM inference combining features from middle and last layers. Representation-joint combines query and context representations. Bold: best performance per dataset; underlined: second-best. 4.2 Main Results We organize our findings around the three research questions posed in 1, advancing from characterizing overflow (RQ1) through efficient detection methods (RQ2) to understanding the role of querycontext interactions (RQ3). Table 1 presents our main results across three datasets, comparing detection performance at different pipeline stages. We distinguish between two detection stages: preinference (before LLM processing) uses concatenated preprojection and postprojection representations for probing, while post-inference (requiring full LLM forward pass) uses concatenated middle and last layer hidden states. For saturation statistics, the same stage distinction applies, with features extracted from corresponding layer representations. Table 3 (Appendix B) provides detailed ablation study examining feature extraction at different architectural layers."
        },
        {
            "title": "4.2.1 RQ1: Characterizing overflow in\ncompressed representations",
            "content": "To understand the nature of overflow, we first examined whether compressed tokens exhibit distinctive geometric properties that might correlate with information loss. Saturation statistics distinguish token types but not overflow We compared the defined saturation statistics across xRAG and non-xRAG tokens at multiple LLM layers across all three datasets. To avoid positional bias and control for contextual confounds, we compare xRAG token statistics against four baselines: (i) mean of all non-xRAG tokens 3https://hf.co/Hannibal046/xrag-7b 4https://hf.co/Salesforce/ SFR-Embedding-Mistral (when compression is applied), capturing the aggregate behavior of standard tokens in compressed sequences; (ii) mean of original context tokens, representing uncompressed context behavior; (iii) first original context token, isolating position-specific effects; and (iv) first token in no-context scenarios, establishing baseline without any context information. Tables 2, 4, and 5 (Appendix C) present percentage differences across these baselines. The results reveal consistent patterns across datasets: xRAG tokens show lower sparsity and kurtosis, and dramatically higher spectral entropy across all layers (all < 0.001). Spectral entropy shows the largest differences (87% across all datasets and baselines), while excess kurtosis shows substantial differences ranging from 2998% depending on layer and baseline. Hoyers sparsity demonstrates more modest but consistent differences of 733%. Crucially, these patterns remain remarkably stable across datasets and all four baselines, validating that observed properties reflect genuine characteristics of xRAG tokens rather than measurement artifacts or positional biases. The differences persist from middle to final layers, suggesting that compression effects propagate through the network without being normalized away. To verify that these representational differences enable tokentype identification, we tested linear separability between xRAG and non-xRAG tokens across all baseline configurations. Linear classifiers achieve near-perfect separation (> 0.95 AUC-ROC for all variants), confirming that saturation statistics reliably distinguish compressed from uncompressed representations in the models activation space. Notably, more complex classifier architectures (MLP, MLP-SCL) provide no improvement over linear models for this token-type classification task (see Appendix A), further confirming that compressed and uncompressed tokens occupy distinctly separable regions. However, while these metrics successfully characterize compressed tokens, they fail to predict overflow. Despite the substantial magnitude of differences and near-perfect linear separability of token types, saturation statistics achieve only nearrandom predictive performance for overflow detection across datasets  (Table 1)  . Even when combined with query information (Saturation-joint), performance remains limited (0.550.63 AUCROC). Context complexity provides minimal signal Context-level features (shown in Table 1) also achieve near-random performance, only marginally exceeding saturation statistics. This indicates that overflow is not strongly predicted by general context properties alone (perplexity, length, statistical compressibility) in our experimental setting. While our current datasets involve relatively short passages compressed into single tokens, we suggest that context complexity features may become more informative in settings with substantially longer contexts or more extreme compression ratios. Summary for RQ1: Saturation statistics provide reliable method to separate compressed tokens from uncompressed tokens, achieving nearperfect linear separability and revealing distinct activation-space statistics with 787% relative differences across multiple metrics, layers, and tokens. However, these query-agnostic properties do not predict task-relevant information loss, indicating that while compressed tokens are distinct in representation space, overflow detection requires modeling query-context interactions beyond intrinsic token characteristics."
        },
        {
            "title": "4.2.2 RQ2: Efficient overflow detection\nwithout full LLM inference",
            "content": "While saturation statistics and context complexity features show limited predictive power, we investigated whether learned classifiers can effectively detect overflow, and critically, at which stage in the compression pipeline degradation becomes detectable. Tables 1 and 3 compare detection performance at two stages: pre-inference (projection stage, before LLM processing) and post-inference (LLM hidden states, after forward pass). Overflow is detectable immediately after compression Learned probing classifiers achieve 0.72 AUC-ROC on average at the post-projection stage  (Table 1)  , substantially outperforming context-only models and query-agnostic baselines. Crucially, compression-induced information loss manifests in the representation space immediately after projection, before any LLM processing. The overflow signal is already present in query-context alignment patterns, revealing that degradation is determined by the compression step itself rather than emerging during generation. LLM processing provides no additional signal Post-inference detection using middle-layer hidden states achieves identical performance, confirming that overflow established at compression time merely propagates through the network without amplification or masking  (Table 3)  . Attention patterns (0.62 AUC-ROC on average) and saturation statistics (even when query-conditioned) provide no meaningful improvement over projectionstage features. Last-layer features show slightly degraded performance, suggesting earlier layers better preserve overflow-relevant signals. Summary for RQ2: Overflow detection without LLM inference matches post-inference performance. This reveals that compression degradation manifests immediately after projection and is determined during compression rather than during generation, enabling both efficient detection and deeper understanding of compression capacity limits."
        },
        {
            "title": "4.2.3 RQ3: The necessity of modeling\nquery-context interactions",
            "content": "Our final question addresses whether overflow can be detected from compressed tokens alone or whether incorporating query information improves detection performance. Joint representations substantially outperform single-source models Tables 1 and 3 compare detection using context-only representations (Representation), joint query-context representations (Representation-joint), and query-agnostic saturation statistics. Given the poor performance of saturation statistics alone, we explored whether incorporating contextual information could improve detection by aggregating statistics (Hoyers sparsity, spectral entropy, and excess kurtosis) from all non-xRAG tokens in the compressed sequence, computing their mean, maximum, minimum, and Stage Statistic Excess Kurtosis Middle Layer Hoyers index Last Layer Spectral Entropy Excess Kurtosis Hoyers index Spectral Entropy Non-xRAG Context (first) Context (mean) No Context 92.0 24.4 0.1 98.5 31.4 0.1 29.1 23.0 87.1 98.8 32.1 87.1 29.6 19.4 87.1 94.1 19.2 87.1 -23.1 20.1 87.1 80.4 7.2 87. Table 2: Relative differences (%) in saturation statistics between xRAG and baseline tokens on SQuADv2, computed as baselinexRAG baseline 100%. Middle Layer: LLM intermediate layer features. Last Layer: LLM final layer features. Positive values indicate xRAG tokens have lower saturation (more structured representations). Large differences ( 50%) in Excess Kurtosis and Spectral Entropy demonstrate consistent xRAG-specific properties across multiple baselines. The reveal results standard deviation (Saturation-joint). clear hierarchy: representation-joint models achieve 0.700.73 AUC-ROC across datasets and stages, substantially outperforming context-only models (0.640.69 AUC-ROC). Saturation-joint yields only modest improvements over saturation-only features (0.580.63 vs. 0.520.58 AUC-ROC), remaining substantially below representation-based methods. This confirms that token-level activation statistics alone are insufficient for overflow detection, regardless of aggregation strategy. This reveals that overflow is not an intrinsic property of compressed representations but emerges from the mismatch between what information the compressed token contains and what the query requires. Joint representation models capture this alignment directly in representation space, enabling accurate overflow prediction. Notably, saturation statistics maintain consistent low performance across all pipeline stages, confirming their utility for identifying compressed tokens but not for predicting query-specific overflow. Similar to tokentype classification (RQ1), linear classifiers prove sufficient for overflow detection, with more complex architectures providing minimal improvement (Appendix A), suggesting that overflow manifests as relatively simple (approximately linearly separable) structure in joint representation space. Summary for RQ3: Overflow detection fundamentally requires modeling query-context interactions. Joint representation models yield the strongest performance, outperforming context-only models by 58 percentage points  (Table 1)  ."
        },
        {
            "title": "5 Conclusion",
            "content": "We investigated token overflow in soft compression architectures and proposed methodology advancing from query-independent to query-aware detection. Our findings show that saturation statistics reliably separate compressed from uncompressed tokens (787% relative differences), while learned probing on joint query-context representations achieves efficient pre-inference overflow detection (0.72 AUC-ROC on average) without LLM forward passes. Post-inference detection achieves comparable performance, confirming that overflow can be detected efficiently before expensive LLM processing. These results enable safer deployment of compression modules through low-cost pre-LLM gating and adaptive chunking strategies."
        },
        {
            "title": "Limitations",
            "content": "This work focuses on the xRAG architecture as an initial controlled study. Future work should extend the methodology to longer contexts, diverse tasks (summarization, multi-hop reasoning), and other compression architectures to validate generalizability. Exploring richer overflow definitions beyond task performance degradation could capture subtle information loss patterns. Our detection performance establishes strong baseline, with promising directions including multi-task learning across different compression ratios, incorporating architectural features of the compressor, and developing adaptive systems that dynamically adjust compression based on predicted overflow risk. The methodologys architecture-agnostic design facilitates such extensions to emerging compression techniques."
        },
        {
            "title": "Ethical Considerations",
            "content": "Generation of text with LLMs using compressed and overflown tokens can lead to hallucinations and untrustworthy output. This way, the created technology may be considered helpful for minimizing such effects. At the same time, as the absolute accuracy numbers of the developed classifier are relatively low, and eventual false positive predictions could lead to overconfidence in trustworthiness of generated texts. Therefore, we suggest that more research is needed to raise the absolute values of the developed classifiers to ensure their safe use in various text generation workflows and applications."
        },
        {
            "title": "References",
            "content": "Islam Aushev, Egor Kratkov, Evgenii Nikolaev, Andrei Glinskii, Vasilii Krikunov, Alexander Panchenko, Vasily Konovalov, and Julia Belikova. 2025. RAGulator: Effective RAG for regulatory question answering. In Proceedings of the 1st Regulatory NLP Workshop (RegNLP 2025), pages 114120, Abu Dhabi, UAE. Association for Computational Linguistics. Julia Belikova, Konstantin Polev, Rauf Parchiev, and Dmitry Simakov. 2025. Data-efficient meta-models for evaluation of context-based questions and anIn Proceedings of the 48th Interswers in llms. national ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2025, Padua, Italy, July 13-18, 2025, pages 43854389. ACM. Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. CoRR, abs/2004.05150. Sky CH-Wang, Benjamin Van Durme, Jason Eisner, and Chris Kedzie. 2024. Do androids know theyre In Findings of only dreaming of electric sheep? the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 44014420. Association for Computational Linguistics. Xin Cheng, Xun Wang, Xingxing Zhang, Tao Ge, SiQing Chen, Furu Wei, Huishuai Zhang, and Dongyan Zhao. 2024. xrag: extreme context compression for retrieval-augmented generation with one token. In Proceedings of the 38th International Conference on Neural Information Processing Systems, NIPS 24, Red Hook, NY, USA. Curran Associates Inc. Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. 2023. Adapting language models to compress contexts. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 610, 2023, pages 38293846. Association for Computational Linguistics. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc V. Le, and Ruslan Salakhutdinov. 2019. Transformer-xl: Attentive language models beyond fixed-length context. CoRR, abs/1901.02860. Tao Ge, Jing Hu, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. 2024. In-context autoencoder for context compression in large language model. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Patrik O. Hoyer. 2004. Non-negative matrix factorization with sparseness constraints. J. Mach. Learn. Res., 5:14571469. Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. Preprint, arXiv:1705.03551. Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. 2020. Supervised contrastive learning. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Yuri Kuratov, Mikhail Arkhipov, Aydar Bulatov, and Mikhail Burtsev. 2025. Cramming 1568 tokens into single vector and back again: Exploring the limIn Proceedings its of embedding space capacity. of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pages 1932319339. Association for Computational Linguistics. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledgeIn Advances in Neural Inintensive NLP tasks. formation Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Huanxuan Liao, Wen Hu, Yao Xu, Shizhu He, Jun Zhao, and Kang Liu. 2025. Beyond hard and soft: Hybrid context compression for balancing local and global information retention. Preprint, arXiv:2505.15774. Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024. Lost in the middle: How language models use long contexts. Trans. Assoc. Comput. Linguistics, 12:157173. Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you dont know: Unanswerable questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784789, Melbourne, Australia. Association for Computational Linguistics. Elisei Rykov, Valerii Olisov, Maksim Savkin, Artem Vazhentsev, Kseniia Titova, Alexander Panchenko, Vasily Konovalov, and Julia Belikova. 2025. SmurfCat at SemEval-2025 task 3: Bridging external knowledge and model uncertainty for enhanced halIn Proceedings of the 19th lucination detection. International Workshop on Semantic Evaluation (SemEval-2025), pages 10341045, Vienna, Austria. Association for Computational Linguistics. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 59986008. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 23692380. Association for Computational Linguistics. Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. 2020. Big bird: Transformers for longer sequences. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual."
        },
        {
            "title": "A Classifiers Ablation Study",
            "content": "Figure 1: Comparison of classifier architectures (Linear scikit-learn, Linear PyTorch, MLP, MLP with SCL) across datasets and feature combinations. All architectures achieve comparable performance, with differences typically <1 percentage point, demonstrating that overflow is largely linearly separable in joint representation space."
        },
        {
            "title": "B Features Ablation Study",
            "content": "Stage Features TriviaQA SQuADv2 HotpotQA Pre-projection Representation Representation-joint Saturation Post-projection Representation Middle layer Last layer Representation-joint Saturation Attention Representation Representation-joint Saturation Saturation-joint Attention Representation Representation-joint Saturation Saturation-joint 0.679 0.019 0.703 0.018 0.534 0.016 0.675 0.016 0.718 0.021 0.553 0.017 0.620 0.012 0.675 0.016 0.713 0.015 0.553 0.017 0.557 0.021 0.584 0.011 0.675 0.016 0.698 0.013 0.554 0.020 0.582 0.015 0.641 0.013 0.687 0.014 0.526 0. 0.660 0.016 0.699 0.021 0.530 0.011 0.581 0.008 0.660 0.016 0.710 0.015 0.530 0.011 0.560 0.026 0.584 0.027 0.661 0.016 0.695 0.018 0.523 0.015 0.541 0.011 0.635 0.017 0.701 0.010 0.522 0.009 0.652 0.025 0.715 0.006 0.518 0.011 0.603 0.014 0.652 0.025 0.727 0.009 0.518 0.011 0.597 0. 0.615 0.014 0.653 0.024 0.707 0.015 0.530 0.010 0.618 0.011 Table 3: Ablation study examining feature extraction at different architectural stages (ROC-AUC). Pre-projection: retriever embeddings before projection. Post-projection: representations after projection. Middle layer: intermediate LLM hidden states. Last layer: final LLM hidden states. Representation-joint combines query and context representations. Bold: best performance per dataset; underlined: second-best."
        },
        {
            "title": "C Saturation Statistics",
            "content": "Stage Statistic Excess Kurtosis Middle Layer Hoyers index Last Layer Spectral Entropy Excess Kurtosis Hoyers index Spectral Entropy Non-xRAG Context (first) Context (mean) No Context 92.4 24.4 0.1 98.3 28.4 0.0 41.3 26.1 87.1 98.4 28.1 87.1 40.5 20.6 87.1 93.1 16.0 87.1 -4.4 21.1 87.1 81.4 7.4 87.0 Table 4: Relative differences (%) in saturation statistics between xRAG and baseline tokens on TriviaQA, computed as baselinexRAG baseline 100%. Middle Layer: LLM intermediate layer features. Last Layer: LLM final layer features. Positive values indicate xRAG tokens have lower saturation (more structured representations). Large differences ( 50%) in Excess Kurtosis and Spectral Entropy demonstrate consistent xRAG-specific properties across multiple baselines. Stage Statistic Excess Kurtosis Middle Layer Hoyers index Last Layer Spectral Entropy Excess Kurtosis Hoyers index Spectral Entropy Non-xRAG Context (first) Context (mean) No Context 91.6 24.4 0.1 98.1 28.3 0. 39.7 25.1 87.1 98.9 33.5 87.1 46.0 22.0 87.1 92.3 14.7 87.1 -10.6 20.8 87.1 80.8 7.0 87.0 Table 5: Relative differences (%) in saturation statistics between xRAG and baseline tokens on HotpotQA, computed as baselinexRAG baseline 100%. Middle Layer: LLM intermediate layer features. Last Layer: LLM final layer features. Positive values indicate xRAG tokens have lower saturation (more structured representations). Large differences ( 50%) in Excess Kurtosis and Spectral Entropy demonstrate consistent xRAG-specific properties across multiple baselines."
        },
        {
            "title": "D Hyperparameters",
            "content": "Table 6 summarizes all hyperparameters used in our experiments. All hyperparameters were tuned on the SQuADv2 validation set and then fixed across TriviaQA and HotpotQA datasets. Method Parameter Value Feature-based Classification (Logistic Regression) Solver Regularization Max iterations Preprocessing lbfgs L2, = 105 1000 StandardScaler Common Settings (All Neural Probes) Cross-validation Batch size Optimizer Preprocessing Regularization Early stopping 5-fold stratified, 80%/20% train/val 256 Adam, learning rate = 104 StandardScaler Lreg = λ2 2N θ2 θ1 with (λ2, λ1) = (500, 100) Patience = 20 epochs 2 + λ Linear Probe MLP Probe Architecture Max epochs Single linear layer 150 Architecture Hidden dimension Activation Max epochs Two-layer feedforward 1024 ReLU MLP-SCL Probe Architecture Hidden dimension Activation Dropout Normalization Contrastive weight Temperature Max epochs Two-layer feedforward 1024 SiLU 0.1 (before and after hidden layer) BatchNorm1d after hidden layer λ = 0.3 τ = 0.07 50 Table 6: Hyperparameters for all overflow detection methods. The regularization term for neural probes combines L2 and L1 penalties scaled by the number of model parameters (excluding biases)."
        }
    ],
    "affiliations": [
        "AIRI",
        "Institute for Information Transmission Problems of the Russian Academy of Sciences",
        "Sber AI Lab",
        "Skoltech"
    ]
}