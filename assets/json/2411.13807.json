{
    "paper_title": "MagicDriveDiT: High-Resolution Long Video Generation for Autonomous Driving with Adaptive Control",
    "authors": [
        "Ruiyuan Gao",
        "Kai Chen",
        "Bo Xiao",
        "Lanqing Hong",
        "Zhenguo Li",
        "Qiang Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid advancement of diffusion models has greatly improved video synthesis, especially in controllable video generation, which is essential for applications like autonomous driving. However, existing methods are limited by scalability and how control conditions are integrated, failing to meet the needs for high-resolution and long videos for autonomous driving applications. In this paper, we introduce MagicDriveDiT, a novel approach based on the DiT architecture, and tackle these challenges. Our method enhances scalability through flow matching and employs a progressive training strategy to manage complex scenarios. By incorporating spatial-temporal conditional encoding, MagicDriveDiT achieves precise control over spatial-temporal latents. Comprehensive experiments show its superior performance in generating realistic street scene videos with higher resolution and more frames. MagicDriveDiT significantly improves video generation quality and spatial-temporal controls, expanding its potential applications across various tasks in autonomous driving."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 2 ] . [ 1 7 0 8 3 1 . 1 1 4 2 : r MagicDriveDiT: High-Resolution Long Video Generation for Autonomous Driving with Adaptive Control Ruiyuan Gao1, Kai Chen2, Bo Xiao3, Lanqing Hong4, Zhenguo Li4, Qiang Xu1 1CUHK 2HKUST 3Huawei Cloud 4Huawei Noahs Ark Lab {rygao,qxu}@cse.cuhk.edu.hk, kai.chen@connect.ust.hk, {xiaobo15,honglanqing,li.zhenguo}@huawei.com Project Website: https://flymin.github.io/magicdrivedit/"
        },
        {
            "title": "Abstract",
            "content": "The rapid advancement of diffusion models has greatly improved video synthesis, especially in controllable video generation, which is essential for applications like autonomous driving. However, existing methods are limited by scalability and how control conditions are integrated, failing to meet the needs for high-resolution and long videos for autonomous driving applications. In this paper, we introduce MagicDriveDiT, novel approach based on the DiT architecture, and tackle these challenges. Our method enhances scalability through flow matching and employs progressive training strategy to manage complex scenarios. By incorporating spatial-temporal conditional encoding, MagicDriveDiT achieves precise control over spatialtemporal latents. Comprehensive experiments show its superior performance in generating realistic street scene videos with higher resolution and more frames. MagicDriveDiT significantly improves video generation quality and spatial-temporal controls, expanding its potential applications across various tasks in autonomous driving. 1. Introduction With the rapid development of diffusion models, video synthesis [13, 14] has seen significant advancements. Controllable video synthesis [16, 20] greatly facilitates the use of synthetic data in numerous downstream applications [37]. Especially in the field of autonomous driving, technologies like MagicDrive [11] and DriveDreamer [35] exemplify how controllable multi-view video synthesis, capable of generating high-quality street-view videos, effectively aid downstream tasks, and enhance the performance and reliability of autonomous systems. High-resolution and long video synthesis is focal point of research in the content generation domain [5, 27], and is highly demanded by autonomous driving technology. Specifically, autonomous driving perception models require higher-resolution inputs to discern details and distant obFigure 1. MagicDriveDiT generates high-resolution and long videos with multi-view and control supports, significantly exceeding the limitation of previous works [11, 12, 15, 35]. jects within the data [22]. The application of video synthesis in 3D reconstruction also necessitates high resolution to capture intricate details [10, 45]. On the other hand, long videos provide more content for more comprehensively evaluating autonomous driving algorithms [42]. From technical perspective, high resolution is straightforward method to enhance generation quality [5], while long videos encompass more dynamic content [40], aiding models in effectively learning the laws of the world [3]. However, synthesizing high-resolution long videos presents two major challenges. Firstly, the increase in data volume require the generative models high scalability to support high-quality synthesis effectively. Secondly, geometry control in video generation poses significant challenges. Previous controllable techniques are primarily designed for image generation [11, 18, 27, 35, 38, 39], focusing on spatial latent spaces. Achieving high-resolution long video synthesis necessitates 3D compressed VAE that integrates spatial and temporal information [3, 19, 43, 47]. Utilizing spatial-temporal latents renders previous control"
        },
        {
            "title": "Type Method",
            "content": "Total Res."
        },
        {
            "title": "Front\nView",
            "content": "GAIA-1[15] DriveDreamer [35] Vista [12] 2885121 1281921 57610241 Multiview MagicDrive [11] Drive-WM [38] Panacea [39] DriveDreamer2 [46] Delphi [27] DiVE [18]"
        },
        {
            "title": "MagicDriveDiT",
            "content": "2244006 1923846 2565126 2564486 5125126 480p6 84816006 4248006 26 32 25 60 8 8 8 10 16 129 241 Table 1. Comparison of Resolution and Frame Count. We only consider single inference, since rollout notably degrades quality. Only support text & image(s) conditions. See note in Appendix. Our model generalizes well from image to video generation through mixed-resolution and duration training, with extrapolation capabilities, significantly exceeding the resolution and frame number in previous works. 2. Related Work Video Generation in Autonomous Driving. Video generation is crucial for autonomous driving, with application in training perception models [11], testing [42], and scene reconstruction [10, 45]. It demands extensive fields of view and dynamic object motion handling, requiring generation model controllability [11] and high-resolution video production [22] with more frames [10] and multiple camera perspectives [45]. Diffusion models have improved controllable multi-view video generation, yet existing models [18, 27, 38, 39, 46] lack adequate resolution and frame count for data engine applications [10, 11, 45] and policy testing [17]. As compared in Table 1, our work, MagicDriveDiT, achieves unprecedented resolution and frame count in video generation for autonomous driving. Diffusion Models and DiT Architectures. Diffusion models [13, 32, 48] generate data by learning the denoising steps from Gaussian noises to samples, widely used in image [8, 31, 37] and video generation [14]. From the modelings perspective, Flow Matching [9, 23] simplifies the modeling of diffusion models, enhancing the efficiency of training and inference. From the implementations perspective, the architecture of diffusion models shifts from UNet [13] to DiT, due to better scalability [6, 30], especially for high-resolution tasks [5]. Our MagicDriveDiT also leverages Flow Matching and DiT for scaling to highresolution and long video generation. Conditional Generation. Conditional generation is crucial for various applications utilizing generative models. Crossattention layers from LDM [31] and ControlNets [44] adFigure 2. Different from spatial latent [11, 18, 27, 38, 39], spatialtemporal latent (ours) requires spatial-temporal condition injection for geometry controls (we omit the text condition here). methods inadequate, as demonstrated in Figure 2. This necessitates novel approaches to effectively manage elements in synthetic videos, ensuring both spatial and temporal coherence with the given conditions. In this paper, we introduce MagicDriveDiT, novel approach based on the DiT architecture, designed to address the above challenges for high-resolution and long streetview video synthesis with precise control. We adopt the flow matching [9] formulation for diffusion models, implementing it with DiT-based architecture [30, 47], to enhance scalability. This improves the models capacity to efficiently handle complex data across diverse scenarios. To enhance controllability, we use specially designed spatialtemporal conditional encoding for spatial-temporal latents from CogVAE [43], enabling precise management of representations. Besides, we adopt progressive bootstrapping strategy for model training, transitioning from short to long videos, allowing the model to capture intricate details and generalize to complex scenarios. Additionally, we utilize videos of various resolutions and durations to strengthen the models generalization ability, enabling the model to synthesize longer videos than those for training. Our MagicDriveDiT excels in generating highly realistic videos that align with road maps, 3D bounding boxes, and different camera perspectives, achieving higher resolution and more frames than previous works [11, 12, 18, 27]. Comprehensive experiments and comparisons demonstrate the effectiveness of our training and control methods, significantly improving controllable street view video synthesis. MagicDriveDiTs flexibility in handling various resolutions, frames, and control signals enables the creation of novel street views suitable for simulations, expanding its potential applications across diverse domains. In summary, the main contributions of this paper include: We design an efficient framework, MagicDriveDiT, leveraging progressive bootstrapping to achieve high-quality high-resolution long video generation. We develop novel spatiotemporal control for object positions, road semantics, and camera trajectories while maintaining multi-frame, multi-view consistency. 2 Figure 3. Architecture Overview of MagicDriveDiT. To incorporate different conditions for video generation, MagicDriveDiT adopts two-branch architecture as in [44] with basic STDiT3 blocks from [6, 47]. We also propose the MVDiT block for multi-view consistency and Spatial-Temporal (Box/Traj.) Encoder to inject condition into the Spatial-Temporal (SP) Latent. ditive encoding for grid-shaped control signals are leading methods in controllable diffusion-based generation. In street view generation, MagicDrive [11] and MagicDrive3D [10] integrate 3D bounding boxes, BEV maps, ego trajectories, and camera poses for multi-view street scene synthesis. However, these methods are limited to spatial encoding and are not directly applicable to spatialtemporal VAE latents [43], as shown in Figure 2. MagicDriveDiT presents novel control paradigm for spatiotemporally compressed VAE latents, enabling the controllable generation of high-resolution long videos. 3. Preliminary Problem Formulation. This paper addresses controllable high-resolution, long video generation for street views. Given sequence of frame descriptions {St}, {0, . . . , }, the goal is to generate corresponding streetview videos from latent variables (0, I), i.e., {Ic,t} = G({St}, z), where {0, . . . , C} denotes the camera views. Here, high resolution indicates that has high resolution, and long video implies that is large. To describe street-view video, we adopt the conditions outlined in [10, 11]. Specifically, frame descriptions St = {C, Mt, Bt, L, Tr0 } include the camera poses {Cc} = [Rc, tc]1, road map Mt {0, 1}whc representing meter road area in BEV with semantic classes, 3D bounding boxes Bt = {(ci, bi)}N i=1 where each j=1 R83 object is described by box bi = {(xj, yj, zj)}8 and class ci C, text adding information for the whole video (e.g., weather and time of day), and the ego vehicle trajectory Tr0 which describes the transformation from the LiDAR coordinate of each frame to the first frame, i.e., Tr0 ]. All geometric information, except for Tr0 , = [R0 , t0 1Typically, cameras are fixed throughout the video. is parameterized according to the local LiDAR coordinate of the ego car. LDMs and Flow Matching. For high-resolution image generation, Rombach et al. [31] propose Latent Diffusion Models (LDMs), using pre-trained VAE for image downsampling and diffusion models for latent generation. This manner is widely adopted in both image generation [6, 9] and video generation [1, 3, 47]. MagicDriveDiT is also based on the VAE+diffusion formulation. With the latest advancement on diffusion models, Esser et al. [9] propose to train large scale diffusion models through simulation-free rectified flow [24, 25] and v-prediction loss [9]: zt = (1 t)x0 + tϵ LCF = EϵN (0,I)vΘ(zt, t) 1 1 (z ϵ)2 2, (1) (2) where lognorm(0, 1) is timestep and vΘ is the model. 4. Methods 4.1. Overview of MagicDriveDiT For model architecture, depicted in Figure 3, MagicDriveDiT introduces novel DiT-based diffusion model for controllable street-view generation, utilizing STDiT-3 blocks as per Zheng et al. [47]. These blocks feature separate spatial and temporal modules to enhance information processing. The architecture design incorporates two significant modifications. First, to facilitate multi-view generation, the Multi-View DiT (MVDiT) block integrates crossview attention layer [11], as shown on the left of Figure 3. Second, given the need for handling multiple control elements, MagicDriveDiT employs cross-attention [31] for text, boxes, camera views, and trajectories, alongside an addictive branch [44] for maps to inject control signals. Nevertheless, the spatial encoding of control signals [11, 27, 38, 39, 46] is incompatible with spatial-temporal latents, as illustrated in Figure 2. Consequently, the encoder for each control signal is reformulated, exemplified by the spatial-temporal box encoder on the right of Figure 3. Further details are provided in Section 4.2. Furthermore, MagicDriveDiT employs progressive bootstrap training strategy (Section 4.3), transitioning from images to low-resolution and short videos, and ultimately to high-resolution and long videos, to enhance diffusion model convergence. The final training stage incorporates variable length and resolution videos, allowing the model to generate images and videos at diverse resolutions and extrapolate to longer frame counts beyond the training settings. Section 4.4 provides further details. 4.2. Design for High-Resolution Long Video DiT and 3D VAE. Training diffusion models for highresolution and long video generation is computationally intensive and requires substantial GPU memory. DiT and 3D VAE are pivotal in scaling diffusion models in these aspects. As noted by Peebles and Xie [30], there is strong negative correlation between model Gflops and FID, making DiT preferable to architectures like UNet [11] for computational efficiency. For GPU memory, 3D VAE offers temporal downsampling. typical 2D VAE [31] compresses images by factor of 8 in both height and width, achieving 64 compression ratio. In contrast, 3D VAE [43] compresses temporal information by 4, resulting in 256 compression ratio2, significantly reducing the sequence length of pachified latent and memory consumption, particularly beneficial for transformers [7]. Additionally, the trend of architectural unification [34] allows DiT to utilize advanced parallelization methods, e.g., sequence parallelism [47], to overcome single GPU memory constraints, facilitating higher resolution and longer video generation. major challenge with 3D VAE adoption is geometric control. As illustrated in Figure 2, geometric control manages per-frame content spatially. With 2D VAE, frames are encoded into latents. Using geometric descriptors {St}, {1, ..., }, video geometric control degrades to spatial control on images [28], as control signals and latents are temporally aligned. However, 3D VAE yields /f latents (where is the temporal compression ratio), misaligning control signals with latents and rendering previous control techniques [11, 27, 39] ineffective. Spatial-Temporal Conditioning Techniques. For geometric control in MagicDriveDiT, we introduce spatialtemporal encoding to align control signals with spatialtemporal latents. This involves re-aligning the maps (Mt), boxes (Bt), and trajectory (Tr0 ) within the scene descriptor 2We ignore possible discrepancy in the dimension of latents through our discussion. Figure 4. Spatial-Temporal Encoder for Maps (a) and Boxes (b). Our spatial encoding module follows [11], and temporal encoding integrates the downsampling strategy in our 3D VAE [43], resulting in temporally aligned embedding between the control signals and video latents. (St). Maps, represented as grid data, are straightforward to manage. By extending ControlNets [44] design, we utilize temporal downsampling modules with new trainable parameters in 3D VAE to align features between control and base blocks, as depicted in Figure 4(a). For 3D boxes, padding is applied to invisible boxes to maintain consistent box sequence length across views and frames, illustrated in Figure 3 right. We employ downsampling module with temporal transformer and RoPE [33] to capture temporal correlation, creating spatial-temporal embeddings aligned with video latents, as in Figure 4(b). The spatial-temporal encoder for boxes can also be adapted for ego trajectory (Tr0 ), by replacing the MLP for boxes with an MLP for camera poses [11]. All the downsampling ratios are aligned with the adopted 3D VAE [43], i.e., with 8n or 8n + 1 as inputs, and 2n or 2n + 1 as outputs, respectively. 4.3. Progressive Bootstrap Training To expedite model convergence during training, we schedule the training data based on the duration of single training iterations. Specifically, we adopt three-stage training approach: initially using low-resolution images for bootstrap training, transitioning to low-resolution short videos, and ultimately employing high-resolution long video training. This training strategy is grounded in two observations. First, in the controllable generation, we note that the model initially optimizes for higher content quality before learning controllability, pattern also observed by Gao et al. [12]. Training the model from scratch requires numerous iterations for convergence. Our progressive transition approach enables the model to acquire controllability more rapidly. Second, during the stage transition, we find that the model adapts more quickly to high resolution [5] compared to long-video controllability. Therefore, we train the controllability from the first stage and focus on optimizing more iterations with (short) videos rather than images."
        },
        {
            "title": "Method",
            "content": "FVD mAP mIoU MagicDrive [11] (16f) MagicDrive [11] (60f) MagicDrive3D [10]"
        },
        {
            "title": "MagicDriveDiT",
            "content": "218.12 217.94 210.40 94.84 11.86 11.49 12.05 18.17 18.34 18.27 18.27 20. Table 2. Comparison with Baselines for Controllable Video Generation. Videos are generated according to conditions from the nuScenes validation set. Only first 16 frames are kept for evaluation, as in [29]. / indicates that higher/lower value is better. mAP with BEVFusion [26] and road mIoU with CVT [49] for controllability assessment. BEVFusion and CVT are image-based models. Our MagicDriveDiT is trained solely on the training set of the nuScenes dataset, and we generate corresponding videos/images for the validation set annotations, applying the aforementioned metrics with perception models pre-trained on real data for evaluation. Model Setup. In accordance with Section 4.2, we adopt the 3D VAE framework from CogVideoX [43] and train diffusion models for street-view video generation from scratch3. Initially, the model comprises only spatial blocks for base and control blocks, focusing on image generation. Temporal blocks are incorporated in the second stage, forming the full architecture of MagicDriveDiT. Sequence parallelism is employed during the final stage to manage GPU memory limitations, detailed in Appendix A. Further training details can be found in Appendix D. 5.2. Results and Analysis Generation Quality. MagicDriveDiT excels in both video and image generation tasks. In video tasks, it significantly reduces FVD compared to MagicDrive  (Table 2)  , due to the DiT architecture enhancing inter-frame consistency and spatial-temporal condition encoding for precise control of object motion and positioning. Further illustrated in Figure 6, MagicDriveDiT generates high-resolution videos that not only improve quality but also incorporate more intricate details, closely resembling footage captured by real cameras. This enhancement is achieved through our advanced training on variable lengths and resolutions, allowing for more realistic and detailed outputs. Also beneficial from the mixing training approach, MagicDriveDiT is capable of image generation. As shown in Table 3, MagicDriveDiT matches baseline performance in multi-view street-view tasks and surpasses baselines in vehicle segmentation mIoU and object detection mAP. This demonstrates the strong generalization capabilities of our spatial-temporal condition encoding. Controllability. The quantitative results presented in Table 2 and 3 demonstrate that the images and videos gen3For more discussion on this choice, see Section 5.3 and Appendix F. Figure 5. Progressive Bootstrap Training in MagicDriveDiT. For high-resolution long video generation, we train the model to progressively scale up from both resolution and frame count. 4.4. Variable Length and Resolution Adaptation As stated in Section 4.3, we employ three-stage training process. In the last two stages, we incorporate videos of varying resolutions and lengths for training. Specifically, in the second stage, we train with mixture of videos up to 65 frames in length and maximum resolution of 424800. In the third stage, we mix videos up to 241 frames (the maximum frame count of the dataset) and resolution of up to 8481600 (the maximum resolution of dataset). Details of the mixing method are provided in the Appendix B. Compared to training with single resolution and length, this mixed approach allows the model to rapidly comprehend information across the dimensions of resolution and frame count (Section 5.3 holds more ablation study for this). Consequently, our final model supports the generation of various resolutions (224400, 424800, 8481600) and frame counts (1-241 frames). It also enables extrapolation across both dimensions, allowing sampling beyond the training configurations, such as 8481600 at 129 frames or 424800 at 241 frames (as shown in Section 6). 5. Experiments 5.1. Experimental Setups Dataset and Baselines. We employ the nuScenes dataset [4], prominent dataset for street-view generation [11, 27, 41], to evaluate MagicDriveDiT. We adhere to the official splits, using 700 multi-view videos for training and 150 for validation. Our primary baseline is MagicDrive [11], comprising three models: MagicDrive (16f) for 16-frame video generation; MagicDrive (60f), which extends MagicDrive (16f) to 60 frames, both from Gao et al. [11]; and MagicDrive3D, 16-frame model from Gao et al. [10]. Further details are provided in Appendix C. Evaluation Metrics. We evaluate both realism and controllability in street-view generation for videos and images. For video generation, we follow benchmarks from [29], utilizing FVD for video quality; mAP from 3D object detection and mIoU from BEV segmentation for controllability, using BEVFormer [21] for both tasks, which is video-based perception model. For image generation, we adopt the metrics from Gao et al. [11], employing FID for image quality; 5 Figure 6. Qualitative Comparison between MagicDriveDiT and MagicDrive [11]. Frames are extracted from the generated videos. To conserve space, we only present the 3 (out of 6) perspectives that include the front view. Two crops from the generated views are magnified and shown on the right. By generating 4 resolution of MagicDrive, the synthesized street view of our model contains finer details. Method FID Road mIoU Vehicle mIoU mAP BEVControl [41] 24.85 MagicDrive [11] (Img) 16.20 60.80 61.05 26.80 N/A 12.30 27.01 MagicDriveDiT 20.91 59.79 32.73 17.65 Table 3. Comparison with Baselines for Controllable Image Generation. All the annotations & camera views from the nuScenes validation set are used for evaluation. / indicates that higher/lower value is better. Figure 7. Reconstruction Visualization from Different VAEs. CogVAE [43] maintains most details compared with others and exhibits better performance for high-resolution contents. erated by MagicDriveDiT effectively reflect the control conditions. Additionally, Figure 8 provides visual results, showing that multiple control conditions can independently influence the generated content. For instance, weather can be altered via text input (from sunny to rainy) while maintaining road structure, and the trajectories of other vehicles and the ego vehicle. By varying the combinations of conditions, MagicDriveDiT is capable of producing diverse range of high-quality street-view videos. 6 5.3. Ablation Studies VAE Comparison for Street Views. Before training the diffusion models, we evaluate the performance of opensource 3D VAEs (i.e., CogVAE [43] and Open-Sora [47]) on street views, in comparison with the 2D SD VAE [31]. As illustrated in Figure 7, CogVAE consistently outperforms its counterparts on reconstruction ability. Besides, as presented in Appendix E, CogVAE demonstrates minimal performance degradation over longer video sequences, making it particularly well-suited for long video generation tasks. Additionally, we found that all VAEs exhibit improved reconstruction capabilities with increasing resolution. This insight is beneficial for advancing our models ability to generate high-quality images and videos by focusing on higher-resolution outputs. More detailed quantitative results and discussion see Appendix E. Spatial-Temporal Conditioning. We demonstrate the spatial-temporal encoders effectiveness by the validation loss in over-fitting experiments (Figure 9) and visualization comparison (Figure 10). We compare two baselines: global temporal dimension reduction (Reduce) and temporal dimension interpolation (Interp.) In over-fitting training with 16 samples, the 4 downsampling technique (4 down, ours) accelerates convergence and achieves the lowest final validation loss [9], as shown in Figure 9. Furthermore, Figure 10 illustrates that, unlike the global reduction baseline, 4 down reduces artifacts and maintains accurate motion trajectories. These results confirm the spatial-temporal encoders capability to enhance data encoding and improve video generation performance using spatial-temporal latents. Progressive Bootstrap Training. The three-stage progressive training approach markedly improves model training for alignment. Figure 8. MagicDriveDiT generates high-resolution (e.g., 424800) street-view videos for 241 frames (i.e., the full length of nuScenes videos, approximately 20 seconds at 12 FPS) with multiple controls (i.e., road map, object boxes, ego trajectory, and text). Notably, the 241-frame length at 424800 are unseen during training, demonstrating our methods generalization capability to video length. We annotate the ego-vehicle trajectory and selected objects to aid localization, with same-color boxes denoting the same object. Due to space constraints, the rainy example includes only two frames; additional examples can be found in the Appendix H. Stages Sec./Iter. Iter. for 4 days stage 1 stage 2 stage 3 4.32 39.84 264.96 80k 8.7k 1.3k Table 4. Speed for Each Training Stage of MagicDriveDiT, measured on NVIDA A800 GPUs. Over 4-day period (for example), Stage 1 training yields nearly 60 times more iterations than Stage 3, and Stage 2 offers about 7 times more. This value is calculated by multiplication with sequence parallel (SP) size (in practice, we use SP size of 4 for the stage 3, with 66.24s/it). as discussed in Section 4.3. The progressive strategy enables the rapid acquisition of high-quality video generation capabilities, utilizing faster iterations in the early stages to enhance convergence and expedite learning. Variable Length and Resolution Training. As presented in Section 4.4, MagicDriveDiT adopts training strategy by mixing different length and resolution videos. Our ablation study demonstrates the effectiveness of this strategy. As shown in Table 5, the limitations of VAE (as discussed Figure 9. Validation Loss through Training with Different SP Encodings. 4down (our methods in MagicDriveDiT) can help the model converge, performing the best among all the encodings. efficiency relative to direct Stage 3 training. Table 4 indicates that, over 4 days, for example, Stage 1 executes approximately 60 times more iterations than Stage 3, and Stage 2 achieves about 7 times more iterations. The progressive training is vital for controlled generative models, which require extensive iterations for effective convergence, 7 Figure 10. Visual Effect of Spatial-Temporal Encoding for Boxes. For videos with spatial-temporal latent encoding, simple global reduce baseline can cause artifacts and trailing effects in object trajectories across viewpoints (we highlight them with red boxes). Our spatial-temporal encoding effectively resolves this, maintaining object clarity and accurate motion trajectories. Training Data FVD mAP mIoU 17224400 (1-65)-224400 17(224440 - 424800) 97.21 100.73 96.34 1-65(224440 - 424800) 99. 10.17 10.51 14.91 15.44 12.42 12.74 17.53 18.26 Table 5. Comparison between Different Training Configurations. To test adaptation ability for higher resolution and longer videos, all the models load pre-trained weight for short videos (9424800) and are trained with the same GPU hours. in Section 5.2) are evident when using only low-resolution videos (17224400), indicated by worse FVD and lower mAP and mIoU scores compared to other settings. Incorporating longer videos improves the models controllability (both mAP and mIoU are higher), while incorporating highresolution videos enhances overall quality (all three metrics are notably improved) Although mixing different frame lengths slightly degrades FVD, it is crucial for enabling the model to generate videos of various lengths and extrapolate to unseen lengths (see Section 6). Thus, we combine both resolutions and frame lengths, effectively balancing the trade-offs between video quality, controllability, and model functionality. 6. Applications Extrapolation for Longer Video Generation. Through variable length and resolution training (Section 4.4), MagicDriveDiT effectively generates videos exceeding the training setups length. Though trained on videos up to 129424800 and 241224400, MagicDriveDiT successfully produces 241424800 videos (full length of nuScenes), which are double the length of the training samples, as demonstrated in Figure 8. To further validate this capability, we compare the first-16-frame FVD for short First-16Avg. of Per-16-Frame Resolution Frame 2 424800 530.65 562.99 3 / 4 / 8481600 559.70 573.46 583. 585.89 Table 6. Generation Quality for Videos Longer than Training. We randomly sample 10 sequence from the nuScenes validation set and report FVD (the lower the better). n: times of maximum training frame number, i.e., 129 frames for 424800 and 33 for 8481600. /: exceed the maximum frame of dataset. videos (17 frames) and per-16-frame FVD for longer videos (65 frames and above). As shown in Table 6, the 16-frame FVDs remain consistent across both seen and extrapolated configurations, extending to 12984816006 generation (this is not the upper limit; please see note in the Appendix). This affirms the models robust generalization capability. 7. Conclusion and Discussion In this paper, we presented MagicDriveDiT, an innovative framework for high-resolution and long video synthesis with precise control, specifically tailored for applications like autonomous driving. By leveraging the DiT architecture and Flow Matching, MagicDriveDiT effectively addresses the challenges of scalability and geometry control in video generation. Our approach introduces novel spatialtemporal conditional encoding and progressive bootstrapping strategy with variable length and resolution adaption, enhancing the models generalization ability. Extensive experiments demonstrate that MagicDriveDiT generates realistic videos that maintain spatial and temporal coherence, significantly surpassing previous methods in resolution and frame count. This work advances video synthesis and opens new possibilities for simulations and other applications in autonomous driving."
        },
        {
            "title": "References",
            "content": "[1] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 1, 3 [2] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In CVPR, pages 2256322575, 2023. [3] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. 1, 3 [4] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: multimodal dataset for autonomous driving. In CVPR, 2020. 5 [5] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-sigma: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In ECCV, 2024. 1, 2, 4 [6] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-alpha: Fast training of diffusion In transformer for photorealistic text-to-image synthesis. ICLR, 2024. 2, 3 [7] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In NeurIPS, 2022. 4 [8] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In NeurIPS, 2021. 2 [9] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. 2, 3, [10] Ruiyuan Gao, Kai Chen, Zhihao Li, Lanqing Hong, Zhenguo Li, and Qiang Xu. Magicdrive3d: Controllable 3d generation for any-view rendering in street scenes. arXiv preprint arXiv:2405.14475, 2024. 1, 2, 3, 5 [11] Ruiyuan Gao, Kai Chen, Enze Xie, HONG Lanqing, Zhenguo Li, Dit-Yan Yeung, and Qiang Xu. Magicdrive: Street view generation with diverse 3d geometry control. In ICLR, 2024. 1, 2, 3, 4, 5, 6 [12] Shenyuan Gao, Jiazhi Yang, Li Chen, Kashyap Chitta, Yihang Qiu, Andreas Geiger, Jun Zhang, and Hongyang Li. Vista: generalizable driving world model with arXiv preprint high fidelity and versatile controllability. arXiv:2405.17398, 2024. 1, 2, 4, 3 [13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 2 [14] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. In NeurIPS, 2022. 1, [15] Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado. Gaia-1: generative world model for autonomous driving. arXiv preprint arXiv:2309.17080, 2023. 1, 2 [16] Li Hu. Animate anyone: Consistent and controllable imageto-video synthesis for character animation. In CVPR, 2024. 1 [17] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, Lewei Lu, Xiaosong Jia, Qiang Liu, Jifeng Dai, Yu Qiao, and Hongyang Li. Planning-oriented autonomous driving. In CVPR, 2023. 2 [18] Junpeng Jiang, Gangyi Hong, Lijun Zhou, Enhui Ma, Hengtong Hu, Xia Zhou, Jie Xiang, Fan Liu, Kaicheng Yu, Haiyang Sun, et al. Dive: Dit-based video generation with enhanced control. arXiv preprint arXiv:2409.01595, 2024. 1, 2 [19] PKU-Yuan Lab and Tuzhan AI etc. Open-sora-plan, 2024. 1 [20] Pengxiang Li, Kai Chen, Zhili Liu, Ruiyuan Gao, Lanqing Hong, Guo Zhou, Hua Yao, Dit-Yan Yeung, Huchuan Lu, and Xu Jia. Trackdiffusion: Tracklet-conditioned video generation via diffusion models. In WACV, 2025. 1 [21] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Yu Qiao, and Jifeng Dai. Bevformer: Learning birds-eye-view representation from multi-camera images via spatiotemporal transformers. In ECCV, 2022. 5 [22] Tingting Liang, Hongwei Xie, Kaicheng Yu, Zhongyu Xia, Zhiwei Lin, Yongtao Wang, Tao Tang, Bing Wang, and Zhi Tang. BEVFusion: Simple and Robust LiDAR-Camera Fusion Framework. In NeurIPS, 2022. 1, [23] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In ICLR, 2023. 2 [24] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In ICLR, 2023. 3 [25] Xingchao Liu, Chengyue Gong, et al. Flow straight and fast: Learning to generate and transfer data with rectified flow. In ICLR, 2023. 3 [26] Zhijian Liu, Haotian Tang, Alexander Amini, Xingyu Yang, Huizi Mao, Daniela Rus, and Song Han. Bevfusion: Multitask multi-sensor fusion with unified birds-eye view representation. In ICRA, 2023. 5 [27] Enhui Ma, Lijun Zhou, Tao Tang, Zhan Zhang, Dong Han, Junpeng Jiang, Kun Zhan, Peng Jia, Xianpeng Lang, Haiyang Sun, et al. Unleashing generalization of end-to-end autonomous driving with controllable long video generation. arXiv preprint arXiv:2406.01349, 2024. 1, 2, 4, 5 [28] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In ICLR, 2022. [29] W-CODA2024 Organizers. Track 2: Corner case scene generation - multimodal perception and comprehension of corner cases in autonomous driving, 2024. 5 [30] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. 2, 4 9 [44] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, 2023. 2, 3, 4 [45] Guosheng Zhao, Chaojun Ni, Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, Boyuan Wang, Youyi Zhang, Wenjun Mei, and Xingang Wang. Drivedreamer4d: World models are effective data machines for 4d driving scene representation. arXiv preprint arXiv:2410.13571, 2024. 1, 2 [46] Guosheng Zhao, Xiaofeng Wang, Zheng Zhu, Xinze Chen, Guan Huang, Xiaoyi Bao, and Xingang Wang. Drivedreamer-2: Llm-enhanced world models for diverse driving video generation. arXiv preprint arXiv:2403.06845, 2024. 2, [47] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, 2024. 1, 2, 3, 4, 6 [48] Ziyang Zheng, Ruiyuan Gao, and Qiang Xu. Non-cross diffusion for semantic consistency. In WACV, 2025. 2 [49] Brady Zhou and Philipp Krahenbuhl. Cross-view transformers for real-time map-view semantic segmentation. In CVPR, 2022. 5 [31] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 2, 3, 4, 6 [32] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2020. [33] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 4 [34] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 4 [35] Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, Jiagang Zhu, and Jiwen Lu. Drivedreamer: Towards real-worlddriven world models for autonomous driving. arXiv preprint arXiv:2309.09777, 2023. 1, 2 [36] Xiaofeng Wang, Zheng Zhu, Yunpeng Zhang, Guan Huang, Yun Ye, Wenbo Xu, Ziwei Chen, and Xingang Wang. Are we ready for vision-centric driving streaming perception? the asap benchmark. In CVPR, 2023. 2 [37] Yibo Wang, Ruiyuan Gao, Kai Chen, Kaiqiang Zhou, Yingjie Cai, Lanqing Hong, Zhenguo Li, Lihui Jiang, DitYan Yeung, Qiang Xu, et al. Detdiffusion: Synergizing generative and perceptive models for enhanced data generation and perception. In CVPR, 2024. 1, 2 [38] Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, and Zhaoxiang Zhang. Driving into the future: Multiview visual forecasting and planning with world model for autonomous driving. In CVPR, 2024. 1, 2, [39] Yuqing Wen, Yucheng Zhao, Yingfei Liu, Fan Jia, Yanhui Wang, Chong Luo, Chi Zhang, Tiancai Wang, Xiaoyan Sun, and Xiangyu Zhang. Panacea: Panoramic and controllable video generation for autonomous driving. In CVPR, 2024. 1, 2, 4 [40] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Xintao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicrafter: Animating open-domain images with video diffusion priors. In ECCV, 2024. 1 [41] Kairui Yang, Enhui Ma, Jibin Peng, Qing Guo, Di Lin, and Kaicheng Yu. Bevcontrol: Accurately controlling streetview elements with multi-perspective consistency via bev sketch layout. arXiv preprint arXiv:2308.01661, 2023. 5, 6 [42] Xuemeng Yang, Licheng Wen, Yukai Ma, Jianbiao Mei, Xin Li, Tiantian Wei, Wenjie Lei, Daocheng Fu, Pinlong Cai, Min Dou, et al. Drivearena: closed-loop generative simarXiv preprint ulation platform for autonomous driving. arXiv:2408.00415, 2024. 1, 2 [43] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 1, 2, 3, 4, 5, 6 MagicDriveDiT: High-Resolution Long Video Generation for Autonomous Driving with Adaptive Control Supplementary Material (Appendix) Please find the videos on our project website: https://flymin.github.io/magicdrivedit/ Note: Our model is capable of generating videos at resolution of 8481600 for 241 frames, which is the highest resolution and frame count in the nuScenes dataset. However, the inference cost is currently substantial. Therefore, the primary numerical results in our paper do not utilize this maximum setting, nor do we emphasize this capability in our main paper. We have included some generated results in Appendix for reference. Future work may focus on further reducing the inference cost. A. Sequence Parallel Training Inspired by Zheng et al. [47], we employ sequence parallelism to train DiT models with large sequence lengths. As illustrated in Figure I, we partition each input across the spatial dimension onto different GPUs. Most operations can be executed within single GPU; however, the attention blocks necessitate communication. On the right side of Figure I, we demonstrate the communication process, where the full sequence is gathered, but the attention heads are distributed across different GPUs. This approach allows for peer-to-peer communication between GPUs while maintaining roughly equal load. Figure I. Diagram for Sequence Parallel. Left: We split the spatial dimension before the first block and gather them after the last block. Right: For each attention module, we use all-to-all communication, changing the splitting dimension to attention heads. B: batch; T: temporal dimension; S: spatial dimension; D: latent dimension; HD: number of heads; CH: per-head dimension; SP: sequence parallel size. Additionally, for VAE encoding and decoding, we partition based on batch size and the number of camera views, leveraging multiple GPUs to accelerate processing. B. More Details for Mixed Resolution and Frames Training MagicDriveDiT is trained through progressive bootstrap approach (see Section 4.3) with variable length and resolution data configurations (see Section 4.4). Consequently, our method of data mixing corresponds to the three training stages, as detailed in Table I. Inspired by [47], to maximize the utilization of GPU resources, we employed bucket-like approach to adjust the data composition. Specifically, each GPU process (or sequence parallel communication group) loads only one type of data to ensure alignment of the batch dimension. Using the training time of the video format with the longest iteration time at batch size = 1 as benchmark, we adjusted the batch sizes of other data formats so that each type runs at approximately the same speed. Notably, during stage 3 training, due to the limited number of full video clips, we repeat this type of data within an epoch. This ensures that different types of data have similar magnitude of batch numbers within an epoch."
        },
        {
            "title": "Resolution",
            "content": "Frame(s)"
        },
        {
            "title": "Sequence Parallel Training Step",
            "content": "Stage 1"
        },
        {
            "title": "Img",
            "content": "Stage 2 Stage 3 224400 424800 224400 424800 8481600 Img, 9, 17, 33, 65 Img, 9, 17, 33 Img, 17, full Img, 17, 33, 65, 129 Img, 9, 17, - - 4 80000 40000 Table I. Configuration for Variable Length and Resolution Training. The mixing configuration aligns with our progressive bootstrap training with 3 stages, from low-resolution images to high-resolution long videos. C. More Experimental Details The nuScenes dataset includes 12Hz unannotated data and 2Hz annotated data. According to our experiments, high-framerate videos are more beneficial for generative model learning. Therefore, we follow [11] and interpolated the 2Hz annotations to 12Hz annotations with ASAP [36]. Although the interpolation results are not entirely accurate, they do not affect the training for video generation. Semantic Classes for Generation. We follow [11] in data setup. Specifically, for objects, ten categories include car, bus, truck, trailer, motorcycle, bicycle, construction vehicle, pedestrian, barrier, and traffic cone. For the road map, eight categories include drivable area, pedestrian crossing, walkway, stop line, car parking area, road divider, lane divider, and roadblock. D. More Training Details Optimization. We train our diffusion models using Adam optimizer and constant learning rate at 8e5 with 3000-step linear warm-up in the last two stages. We primarily use 32 NVIDA A800 GPUs (80G) for training. Our model can also trained with Ascend 910B (64G). The batch size for each stage is set according to the iteration speed, following the bucket strategy as [47]. For example, in stage 2, we set the batch size for 33424800 to 1, which takes about 30s/it. Then we set the batch size to other video types to achieve about 30s/it. This strategy can ensure the load balance between different GPU processes. Inference. By default, images/videos are sampled using Rectified Flow [9] with 30 steps and the classifier-free-guidance (CFG) scale at 2.0. To support CFG, we randomly drop different conditions at rate of 15%, including embeddings for text, camera, ego trajectory, and boxes. We follow Gao et al. [11] to use {0} as the null condition for maps in CFG inference. When inferring high-resolution long videos, we also use sequence parallel (Appendix A) to fit in the limited memory of single GPU. E. More Comparison among VAEs To quantitatively compare the performance of the VAE, we randomly selected two 6-view videos from the nuScenes dataset and used the PSNR metric to evaluate the VAEs reconstruction ability. Table II presents the results, averaged across the six views. From these results, we observe that CogVAE [43] demonstrates the best reconstruction ability, even surpassing the 2D VAE [31]. Comparing the results from different settings, we find that the current 3D VAEs exhibit good generalization ability for long videos, primarily due to the window-based downsampling techniques [43, 47]. Additionally, we observe that high-resolution content retains relatively high PSNR after VAE reconstruction, indicating that the current VAEs are more favorable for high-resolution data. This observation also supports our motivation for high-resolution generation. F. Reason for Using CogVAE without the Pre-trained Diffusion Model Informed by the work of MagicDrive [11] and others [12], fine-tuning from well-performing diffusion model can effectively accelerate model convergence. Consequently, in the initial implementation of the DiT architecture, we experimented with Open-Sora 1.2 [47]s VAE and diffusion models. However, the results were suboptimal, with image generation and video controllability falling short of MagicDrives performance. We attribute this primarily to the limited generalization capability of text-to-video diffusion and, more critically, to the inadequate reconstruction ability of the VAE."
        },
        {
            "title": "Image",
            "content": "17 fr. 33/34 fr. 224400 424800 8481600 CogVAE Open-Sora SD VAE CogVAE Open-Sora SD VAE CogVAE Open-Sora SD VAE 34.4261 30.4127 27.7131 38.4786 33.6114 30.9704 41.5023 37.0590 37.0504 31.0900 27.9238 27. 33.5852 30.2779 31.0789 36.0011 33.2856 33.2846 30.5986 27.5245 27.9404 32.9202 29.8426 31.3408 35.1049 32.8690 32.8680 Table II. VAE Comparison for Street Views. CogVAE [43] and Open-Sora [47] (1.2) are 3D VAEs; SD VAE [31] is 2D VAE, which is also widely adopted by previous street view generation (e.g., [11]). Results are PSNRs calculated through videos from the nuScenes validation set. MagicDriveDiT adopts CogVAE. We conducted comparative analysis of VAEs, as detailed in Section 5.3 and Appendix E, and found CogVAE [43] to perform well. Given that the VAE determines the upper limit of generation quality, we opted to use CogVAE for video encoding. Notably, CogVideoX [43] employs novel DiT structure, where each layers latent space integrates both video and text condition information. This approach may complicate the design of geometry-related conditions. Furthermore, CogVideoX was not trained in driving scenario. To eliminate these potential confounding factors, we decided to train the diffusion model from scratch using CogVAE. This strategy allows us to move beyond the constraints of pre-trained models, enabling more flexible modifications to the model architecture to achieve multi-view consistency and spatiotemporal encoding of geometry conditions. Our experience directly demonstrates that high-resolution, long street-view video generation does not necessarily require pre-trained image-text or video-text models. Even so, this is beyond the primary focus of our paper, and we leave related questions for future work. G. Single Inference v.s. Rollout Inference (a) Generation from Vista. It takes the first frame as input and generates the following (only support the front view). (b) Generation from MagicDriveDiT. We take conditions as inputs and generate the full video (only show the first 9s for comparison). Figure II. Comparison between Rollout for Long Videos (Vista [12]) and Single Inference (our MagicDriveDiT). Although rollout can handle long videos, the quality is significantly degraded. In contrast, our extrapolation maintains high quality in long video generation. 3 To achieve long video generation, previous work typically employs method of future frame prediction combined with rollout. This involves, after the n-th inference, taking the last frames from this inference as the first frames for the + 1-th inference, thus enabling long video generation. However, since the model does not directly capture long-term dependencies and accumulates errors with each inference, such rollouts often fail to support sufficiently long videos. Among rollout methods, Vista [12] currently achieves relatively good results. We compared 9-second video generated by performing 4 rollouts with Vista (the paper claims it can support 6 rollouts) to 9-second segment produced by our method, MagicDriveDiT. It is evident that our method maintains consistent video quality over long sequences, whereas Vistas results show noticeable decline. Therefore, we believe that the hybrid training and length extrapolation approach adopted by MagicDriveDiT can achieve higher quality in long video generation. H. More Visualization As said in the Note, MagicDriveDiT is capable of generating 6 848 1600 241 videos (20s at 12 fps). We include more generated samples in Figure III-IV. Please see the videos on our project page. 4 Figure III. We show some frames from the generated 6 848 1600 241 videos with the same scene configuration (i.e., boxes, maps, cameras, and ego trajectory) but under different weather conditions. Conditions are from the nuScenes validation set. 5 Figure IV. We show some frames from the generated 6 848 1600 241 videos with the same scene configuration (i.e., boxes, maps, cameras, and ego trajectory) but under different time-of-day conditions. Conditions are from the nuScenes validation set."
        }
    ],
    "affiliations": [
        "CUHK",
        "HKUST",
        "Huawei Cloud",
        "Huawei Noahs Ark Lab"
    ]
}