{
    "paper_title": "When to Speak, When to Abstain: Contrastive Decoding with Abstention",
    "authors": [
        "Hyuhng Joon Kim",
        "Youna Kim",
        "Sang-goo Lee",
        "Taeuk Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) demonstrate exceptional performance across diverse tasks by leveraging both pre-trained knowledge (i.e., parametric knowledge) and external knowledge (i.e., contextual knowledge). While substantial efforts have been made to leverage both forms of knowledge, scenarios in which the model lacks any relevant knowledge remain underexplored. Such limitations can result in issues like hallucination, causing reduced reliability and potential risks in high-stakes applications. To address such limitations, this paper extends the task scope to encompass cases where the user's request cannot be fulfilled due to the lack of relevant knowledge. To this end, we introduce Contrastive Decoding with Abstention (CDA), a training-free decoding method that empowers LLMs to generate responses when relevant knowledge is available and to abstain otherwise. CDA evaluates the relevance of each knowledge for a given query, adaptively determining which knowledge to prioritize or which to completely ignore. Extensive experiments with four LLMs on three question-answering datasets demonstrate that CDA can effectively perform accurate generation and abstention simultaneously. These findings highlight CDA's potential to broaden the applicability of LLMs, enhancing reliability and preserving user trust."
        },
        {
            "title": "Start",
            "content": "When to Speak, When to Abstain: Contrastive Decoding with Abstention Hyuhng Joon Kim1, Youna Kim1, Sang-goo Lee1 2, Taeuk Kim3 * 1Seoul National University, 2IntelliSys, Korea, 3Hanyang University {heyjoonkim, anna9812, sglee}@europa.snu.ac.kr kimtaeuk@hanyang.ac.kr 4 2 0 2 7 1 ] . [ 1 7 2 5 2 1 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) demonstrate exceptional performance across diverse tasks by leveraging both pre-trained knowledge (i.e., parametric knowledge) and external knowledge (i.e., contextual knowledge). While substantial efforts have been made to leverage both forms of knowledge, scenarios in which the model lacks any relevant knowledge remain underexplored. Such limitations can result in issues like hallucination, causing reduced reliability and potential risks in high-stakes applications. To address such limitations, this paper extends the task scope to encompass cases where the users request cannot be fulfilled due to the lack of relevant knowledge. To this end, we introduce Contrastive Decoding with Abstention (CDA), training-free decoding method that empowers LLMs to generate responses when relevant knowledge is available and to abstain otherwise. CDA evaluates the relevance of each knowledge for given query, adaptively determining which knowledge to prioritize or which to completely ignore. Extensive experiments with four LLMs on three question-answering datasets demonstrate that CDA can effectively perform accurate generation and abstention simultaneously. These findings highlight CDAs potential to broaden the applicability of LLMs, enhancing reliability and preserving user trust."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) (Team et al., 2023; Achiam et al., 2023; Dubey et al., 2024) acquire extensive parametric knowledge during pretraining, demonstrating remarkable performance across diverse range of downstream tasks. However, despite their strong capabilities, LLMs exhibit notable limitations in domains underrepresented or inaccessible during pre-training, such as domainspecific (Kandpal et al., 2023; Raja et al., 2024; *Corresponding author. Figure 1: This study considers two primary scenarios. First, when relevant knowledge is present within the model, it is expected to generate response grounded in that knowledge. Second, when no relevant knowledge is available, the model should abstain from producing potentially inaccurate or misleading outputs. Feng et al., 2024a) or outdated data (Lazaridou et al., 2024; Kasai et al., 2023; Zhao et al., 2024a). To overcome these challenges, approaches have emerged that integrate previously unseen information during inference (Buttcher et al., 2016; Yin et al., 2016; Karpukhin et al., 2020). They provide external information to the model as contextual knowledge, extending its capabilities beyond the limitations of pre-training and enabling more accurate responses in overlooked or evolving domains. To maximize the capabilities of LLMs, it is essential to identify and leverage the knowledge that most closely aligns with the users intent. One widely adopted method is context-aware contrastive decoding (CCD) (Zhao et al., 2024b; Kim et al., 2024b; Qiu et al., 2024), which enables the model to selectively attend to parametric or contextual knowledge most relevant to the users query during each decoding step. CCD not only enhances the quality of generated outputs but is also practical, as it can be seamlessly integrated into off-the-shelf LLMs without additional training."
        },
        {
            "title": "2 Related Work",
            "content": "While various efforts have been made to effectively utilize existing knowledge, practical scenarios where neither parametric nor contextual knowledge is available for the user query remain significantly unexplored. Leveraging irrelevant knowledge in such cases can lead to critical issues such as hallucination (Shi et al., 2023; Xiao and Wang, 2021; Niu et al., 2024), resulting in imprecise generation, reduced reliability, and potential risks in high-stakes applications. As illustrated in Figure 1, it is imperative for the model to (1) generate accurate responses when relevant knowledge is available, (2) abstain from responding (Varshney et al., 2024; Zhang et al., 2024a; Wen et al., 2024) when such knowledge is unavailable. However, such behavior requires not only the precise assessment of the models knowledge but also incorporating such assessment into the generation process, both of which are inherently challenging. To this end, we propose Contrastive Decoding with Abstention (CDA), novel, training-free decoding method that enables the model not only to leverage relevant parametric or contextual knowledge in generation but also to abstain when no appropriate knowledge is available. During the decoding process, CDA assesses the relevance of each knowledge, adaptively determining the extent to which each source of knowledge contributes to the generation. Furthermore, CDA steers the model towards abstention if no relevant knowledge is available. The relevancy is estimated based on the level of uncertainty associated with the knowledge in response to specific query. Extensive experiments with four LLMs on three question-answering (QA) datasets (Zhang et al., 2023; Etezadi and Shamsfard, 2023) demonstrate that CDA effectively enables LLMs to abstain from generating responses in the absence of relevant knowledge while maintaining existing capabilities without additional training. Further validation in retrieval-augmented generation (RAG) settings also highlights the effectiveness of CDA, proving its robustness across practical scenarios. These findings underscore the potential of CDA to broaden the applicability of LLMs, ensuring both reliability and precision in real-world tasks."
        },
        {
            "title": "2.1 Contrastive Decoding",
            "content": "Contrastive decoding (CD) controls text generation by contrasting different output distributions and steers the model in the desired direction. DExperts (Liu et al., 2021) employs an ensemble of an expert and an anti-expert model for tasks such as detoxification. Li et al. (2023) contrasts the output distributions of large LM and small LM for open-ended text generation. CD is also proven effective in domains such as reasoning (OBrien and Lewis, 2023) and machine translation (Waldendorf et al., 2024). Recently, there has been growing interest in context-aware contrastive decoding (CCD) (Zhao et al., 2024b; Kim et al., 2024b; Qiu et al., 2024), which enables the model to leverage both parametric and contextual knowledge during decoding. Shi et al. (2024b) proposes CAD, addressing knowledge conflicts (Longpre et al., 2021; Chen et al., 2022; Zhou et al., 2023) between the parametric knowledge and the given context. Despite its effectiveness, these approaches face limitations when the model lacks relevant knowledge for the given input. In this work, we address this limitation by enhancing CCD with abstention capability for more reliable generation."
        },
        {
            "title": "2.2 Abstention in LLMs",
            "content": "LLMs often generate unintended or undesirable responses, such as hallucinations (Maynez et al., 2020; Ji et al., 2023; Jiang et al., 2024), biases (Sap et al., 2020; Feng et al., 2023), and harmful or unsafe outputs (Anwar et al., 2024; Ye et al., 2024; Zhang et al., 2024b). In such instances, it is appropriate for the model to abstain (Kamath et al., 2020; Feng et al., 2024b; Srinivasan et al., 2024) from generating unintended content. Abstention can be employed for unanswerable (Sulem et al., 2022; Amayuelas et al., 2024) or ambiguous (Min et al., 2020; Kim et al., 2024a) queries. Furthermore, models may abstain when relevant parametric knowledge is absent (Ahdritz et al., 2024; Kim and Thorne, 2024). Abstention can be facilitated by utilizing confidence scores of generations (Sun et al., 2022; Kuhn et al., 2023; Duan et al., 2024) or training the model to provide appropriate abstention abilities (Zhang et al., 2024a; Xu et al., 2024; Sun et al., 2024). Unlike previous approaches, this work proposes decoding method enabling the model to abstain, further ensuring reliable outputs. 3."
        },
        {
            "title": "Initial Dataset Construction",
            "content": "For the experiments, we utilize three datasets from the MRQA benchmark (Fisch et al., 2019): Natural Questions (NQ) (Kwiatkowski et al., 2019), HotpotQA (Yang et al., 2018), and TriviaQA (Joshi et al., 2017). Each dataset consists of query xi, an answer yi, and pre-defined context ci.2 We split ci into 100-word spans containing yi to avoid excessively long contexts. Through preprocessing, we construct the dataset Dorg = {(xi, ci, yi)}Norg i=1."
        },
        {
            "title": "3.3 Parametric Knowledge Estimation",
            "content": "To estimate the models parametric knowledge, we assess the generation consistency (Wang et al., 2023)3 for given query xi Dorg. We prompt the model with Tp(xi), leveraging only the parametric knowledge and sample responses. We then compute the consistency rate = , where represents the number of correct responses. If = 0, we assume the model lacks relevant parametric knowledge for xi. These samples are collected as DP=0 = {(xi, ci, yi, pi = 0)}NP=0 i=1 . For pre-defined threshold η, the model is considered to have relevant parametric knowledge if > η, given its consistent accuracy for xi. These samples are grouped into DP=1 = {(xi, ci, yi, pi = 1)}NP=1 i=1 . The resulting dataset is defined as DP = DP=0 + DP=1."
        },
        {
            "title": "3.4 Contextual Knowledge Estimation",
            "content": "In this stage, we select relevant and irrelevant contexts for given query. Relevant Context Selection We provide the model with Tc(xi, ci) where (xi, ci) DP. By leveraging the information from ci, we generate samples to further verify whether the given context span contains appropriate information. The relevant context c+ with consistency rate > η is selected to form DC=1 = {(xi, c+ , yi, pi)}NC=1 i=1 . Irrelevant Context Selection We select context ctrain from the training set with the highest SBERT (Reimers and Gurevych, 2019) embedding similarity with c+ , which prevents selecting unnecessarily 2This work is grounded on the assumption that the given context contains factual information. While incorporating the contexts factuality would reflect more practical scenario, we consider it orthogonal to the scope of this study. 3Estimating parametric knowledge is inherently challenging (Shi et al., 2024a), as various factors influence the estimation. Since considering all potential factors is often infeasible, we employ fixed inference setting for all the experiments. We contend that this approach can provide the closest approximation of the knowledge within controlled setting. Figure 2: Four possible scenarios according to the existence of the models parametric and contextual knowledge. The model is expected to generate reliable responses based on its possessed knowledge."
        },
        {
            "title": "Analysis",
            "content": "For systematic analysis of the model across varying conditions of parametric and contextual knowledge, we design controlled experimental setting, simulating the scenarios in Figure 2. This section first formulates the problem and describes the experimental environment setup process in detail. Additional information, including inference templates, is provided in Appendix A."
        },
        {
            "title": "3.1 Problem Formulation",
            "content": "This paper focuses on handling QA tasks by leveraging the models parametric and contextual knowledge. Parametric knowledge (P) is defined as the knowledge the model acquires during pre-training, and contextual knowledge (C) refers to the external knowledge provided within the input context at inference time. The knowledge is deemed relevant if it contains information capable of generating an accurate response to the users query. For given query and context c, the objective is to produce the ground-truth answer when relevant knowledge is available or to abstain otherwise. Figure 2 illustrates the scenarios addressed in this work. reliable model should properly generate correct answer or abstain grounded on the possessed knowledge. Inputs are defined answerable if one or more relevant knowledge is present (P=1 or C=1). With relevant parametric knowledge (P=1), the model is expected to generate the correct answer irrespective of c. On the other hand, given relevant contextual knowledge (C=1), the model should generate grounded on c. When no relevant knowledge is available (P=0 and C=0), the query is considered unanswerable, and the model should abstain from generating incorrect responses. unrelated contexts. Then we provide the model with Tc(xi, ctrain ) where xi DC=1 to measure the consistency rate r. By selecting irrelevant context with = 0 ensures that does not provide any relevant information about xi. The resulting dataset is defined as DC=0 = {(xi, c+ , yi, pi)}NC=0 i=1 . , c"
        },
        {
            "title": "3.5 Final Dataset Construction",
            "content": "For the final dataset, we balance the number of samples with pi = 0 and pi = 1 and construct = {(xi, c+ i=1. Note that the final number of samples may vary across models due to differences in their possessed knowledge. , yi, pi)}N , c"
        },
        {
            "title": "4 Analysis of Model Uncertainty",
            "content": "Entropy (Malinin and Gales, 2021; Abdar et al., 2021) is widely used to assess the models uncertainty for its knowledge for given query (Kuhn et al., 2023; Duan et al., 2024), particularly prevalent in CCD (Kim et al., 2024b; Qiu et al., 2024). In this section, we probe the models behavior in entropy for each conducted scenario. Experimental Setting The entropy for an output distribution is measured as: = (cid:88) i=1 di log di, (1) where di it the ith token of the vocabulary V. The parametric uncertainty is quantified as the last token entropy when prompted with Tp(x), only given the x. Similarly, contextual uncertainty is estimated by prompting the model with Tc(x, c), further providing c. In this section, we analyze the entropy distribution of LLAMA3 8B INSTRUCT (Dubey et al., 2024) on the NQ dataset conducted in Section 3. Analysis Figure 3 (a) is frequency histogram of the parametric and contextual uncertainty in each scenario. In cases where either knowledge is present (P=1 or C=1), the relative comparison of entropy is enough to distinguish the relevant knowledge. For instance, when P=0 and C=1, the contextual knowledge displays greater confidence, enabling the model to prioritize relatively certain knowledge. However, this approach fails to distinguish the cases where both knowledge are absent (P=0 and C=0), as the distribution closely resembles the cases where P=1 and C=1. We also examined the behaviors of explicit abstention prompt (a) Parametric and contextual uncertainty distribution (b) Explicit abstention uncertainty distribution Figure 3: Analysis of LLAMA3 8B INSTRUCT in the NQ dataset. (a) The frequency distributions of parametric and contextual uncertainty. (b) The frequency distributions when utilizing explicit abstention prompt. Both methods fall short of distinguishing all scenarios. Ta(x, c), depicted in Figure 3 (b). These distributions are also insufficient to distinguish all scenarios, underscoring the ineffectiveness (Cheng et al., 2024; Kim et al., 2024a). The results demonstrate the inherent challenge of differentiating the possible scenarios with naïve measures. In this paper, we utilize an effectively calibrated measure to identify all possible scenarios, enabling the determination of when to provide an answer or abstain."
        },
        {
            "title": "5 Contrastive Decoding with Abstention",
            "content": "Contrastive Decoding with Abstention (CDA) is novel decoding method integrating abstention within the CCD process. This section provides detailed description of the overall process."
        },
        {
            "title": "5.1 Preliminary",
            "content": "Given model θ at decoding step t, the parametric knowledge distribution dp and the contextual knowledge distribution dc is defined as follows: dp = logitθ(yt Tp(x, y<t)), dc = logitθ(yt Tc(x, c, y<t)). (2)"
        },
        {
            "title": "CCD measures the final output distribution do\nan ensemble of dp",
            "content": "t as and dc + wc as: (dc = dp do dp ). (3) wc should precisely quantify the relevance of c, ensuring higher weight when is deemed relevant. 5."
        },
        {
            "title": "Incorporating Abstention",
            "content": "To facilitate CDAs abstention capability, we incorporate the abstention distribution da . da = logitθ(yt Ta(x, c, y<t)) (4) for the final is deCDA expands Eq. 3 by applying da output distribution, where the weight for da = 1 wp fined as wa = dp (dc + wc do wa = (1 wc dp = wp + wc dc Intuitively, wa decreases when the model is confident of possessed knowledge, whereas it increases with both knowledge significantly uncertain. wc . dp (da ) + wa ) dp dc + wc + (1 wp dp ) da + wa ) da wc (5) Figure 4: Illustration of all possible results. The model should generate correct answers for answerable queries (N1) and abstain for unanswerable queries (N5). Any other responses (N2, N3, N4) are classified as incorrect. and wc by normalizing rp Finally, we obtain wp and rc . These values represent the relevance of each knowledge and are employed as weights, as specified in Eq. 5. = rp wp rp +rc rp , wc = rc rp +rc rc (7) When relevant knowledge is present, the corresponding knowledge is assigned significantly higher weight, whereas the absence of any knowledge leads to notably reduced weights."
        },
        {
            "title": "5.4 CDA with Momentum (CDA-M)",
            "content": "For our approach to function as intended, wp and wc must effectively quantify the relevance of each corresponding knowledge to x. To do so, we assess the relevance as the uncertainty of each knowledge regarding x. Specifically, we utilize the entropy from Eq. 1, where the parametric uncertainty Hp and contextual uncertainty Hc are derived from their respective distributions dp and dc . and Hc Nonetheless, as illustrated in Figure 3, directly comparing Hp is imprecise since they are conditioned on distinct inputs and prompts. To ensure fair comparison, we calibrate (Zhao et al., 2021; Holtzman et al., 2021; He et al., 2024) the uncertainty measures by accounting for the models intrinsic bias. Specifically, we estimate the bias by providing content-free null prompt by replacing xi and ci with placeholder tokens and c, respectively, removing any specific semantic information. By providing Tp(x) and Tc(x, c), we obtain the parametric null distribution dp and the contextual null distribution dc , along with their corresponding entropy values Hp and Hc . The confidence for each knowledge is quantified as the additional information provided by given input compared to the null prompt, measured as follows. Hp = max(Hp rp Hp , 0) , rc Hc = max(Hc Hc , 0) (6) During each decoding step, previously generated content may unintentionally steer the model toward incorrect knowledge. To mitigate this, we apply momentum, where the weight from the previous step wt1 influences the current step wt as follows. wt α wt1 + (1 α) wt (8) Here, α determines the degree of influence."
        },
        {
            "title": "6.1 Experimental Setting",
            "content": "The experiments utilize the three datasets from Section 3 and four instruction-tuned models including LLAMA3 8B INSTRUCT (Dubey et al., 2024), LLAMA2 7B & 13B CHAT (Touvron et al., 2023), and MISTRAL 7B INSTRUCT (Jiang et al., 2023). Further details are stipulated in Appendix C."
        },
        {
            "title": "6.2 Evaluation Metric",
            "content": "Given our focus on handling two distinct tasks simultaneously, single metric is insufficient to capture the overall performance. Hence, we utilize the following metrics for comprehensive evaluation from various perspectives based on the five possible results illustrated in Figure 4. Method F1ans F1abs LLAMA3 8B INSTRUCT 57.31 CONTEXT 54.05 CAD ACD 71.51 ABSTAIN 60.72 SELF-ASK 57.73 69.96 63. FSB ACD-A CDA CDA-M 71.88 72.84 LLAMA2 7B CHAT 57.19 CONTEXT 54.77 CAD 62.60 ACD ABSTAIN 27.73 SELF-ASK 57.81 49.97 42.58 FSB ACD-A CDA CDA-M 63.32 68.16 LLAMA2 13B CHAT 57.17 CONTEXT 54.20 CAD 66.35 ACD ABSTAIN 52.52 SELF-ASK 59.30 63.57 58.85 FSB ACD-A CDA CDA-M 67.73 69.87 2.83 1.02 1.19 52.24 48.02 55.74 52.47 55.58 55.63 0.27 0.27 0.27 42.99 10.58 46.54 44.13 49.27 49.33 0.00 0.37 0.19 47.08 19.72 47.52 46. 45.27 46.23 MISTRAL 7B INSTRUCT 56.96 CONTEXT 54.96 CAD 69.26 ACD ABSTAIN 72.12 SELF-ASK 62.09 62.27 62.54 FSB ACD-A CDA CDA-M 71.38 73. 0.00 1.04 0.00 51.49 45.14 53.91 52.53 55.53 55.24 NQ RS 50.32 47.35 62.66 49.27 44.29 60.94 54.51 62.93 63. 50.03 47.91 54.79 17.11 51.04 37.64 30.96 55.74 59.28 50.00 47.44 58.07 40.35 52.84 56.05 51.19 60.31 61.95 49.80 48.17 60.60 64.81 56.52 53.53 57.27 63.68 64. Acc. Cov. F1ans F1abs RS Acc. Cov. F1ans F1abs RS Acc. Cov. HotpotQA TriviaQA 49.96 47.22 62.51 39.24 35.44 50.15 44.27 52.25 52.93 50.00 47.88 54.75 12.93 49.59 29.45 23.80 46.31 49. 50.00 47.40 58.05 31.79 49.93 47.02 42.21 52.07 53.49 49.80 48.04 60.60 55.43 48.49 43.00 47.58 53.21 54.32 50.32 47.35 62.66 57.72 53.98 65.93 60.31 67.43 68. 50.03 47.91 54.79 35.88 51.09 48.56 43.26 59.54 63.10 50.00 47.44 58.07 50.37 53.05 59.40 55.33 62.53 64.33 49.80 48.17 60.60 67.34 58.38 59.69 60.14 67.34 68. 57.18 55.63 74.60 67.40 51.22 75.67 66.41 77.35 78.17 57.13 55.06 69.68 39.63 58.77 63.03 54.65 72.93 73.52 57.10 55.07 69.32 56.35 58.17 68.56 62.61 70.16 70. 56.94 53.78 72.88 57.48 61.11 73.10 61.33 74.38 74.91 0.22 0.88 0.33 56.25 48.48 58.42 54.69 61.83 61.85 0.00 0.00 0.00 46.56 20.55 53.88 51.10 44.68 44. 0.00 0.00 0.00 52.99 15.89 56.80 55.25 35.31 37.56 0.00 0.36 0.00 54.95 45.12 57.82 57.39 57.12 58.11 50.03 48.71 65.27 56.93 34.25 67.33 58.59 68.96 69. 49.98 48.18 60.97 25.50 52.73 51.20 42.98 66.51 66.92 49.96 48.18 60.65 41.97 52.06 60.41 54.94 64.10 64.40 49.82 47.09 63.77 47.73 56.54 66.82 57.50 67.20 67. 50.00 48.60 65.23 45.95 27.17 56.29 48.05 57.22 57.80 49.88 48.18 60.97 19.74 49.76 40.85 33.55 59.15 59.66 49.96 48.18 60.65 32.80 49.87 49.44 44.18 58.81 58. 49.82 47.05 63.77 37.18 48.86 56.32 47.00 56.68 56.86 50.03 48.71 65.27 63.85 49.93 71.25 63.17 73.31 73.92 49.98 48.18 60.97 43.10 52.91 59.81 53.28 67.59 67. 49.96 48.18 60.65 54.95 52.14 65.28 60.45 64.52 64.89 49.82 47.09 63.77 56.59 57.91 69.59 60.41 70.27 70.86 57.19 56.39 76.34 66.20 52.12 76.14 65.82 78.11 78. 57.16 54.93 73.19 62.28 57.84 71.11 62.23 72.71 73.57 57.16 55.98 72.01 44.88 57.94 65.08 57.48 70.28 74.29 57.11 54.75 74.59 61.54 61.94 76.87 59.99 75.58 77. 1.47 1.00 1.99 57.54 48.56 61.03 57.88 64.87 64.86 0.09 0.09 0.20 52.78 7.47 53.72 52.40 52.79 54.25 0.41 0.16 0.08 45.90 10.46 53.01 49.10 49.54 52. 0.17 0.59 0.04 50.90 49.61 34.49 43.16 48.70 49.76 50.11 49.39 66.92 54.75 36.72 67.37 58.01 69.08 69.56 50.01 48.06 64.03 50.04 50.84 63.39 55.37 66.26 66. 50.04 48.98 63.00 30.86 51.33 54.61 47.61 62.89 66.55 49.97 47.93 65.24 55.83 57.31 69.60 55.87 69.01 70.25 49.93 49.26 66.67 43.61 28.93 55.69 46.77 56.59 57. 50.00 48.04 64.00 39.96 49.84 53.31 45.31 56.91 57.21 49.98 48.95 62.99 24.02 49.92 44.26 38.08 53.82 56.91 49.95 47.86 65.24 46.33 48.54 64.43 48.81 60.84 61. 50.11 49.39 66.92 63.24 50.54 72.08 63.52 74.46 74.94 50.01 48.06 64.03 58.88 50.87 66.76 59.55 68.38 69.27 50.04 48.98 63.00 45.35 51.37 61.22 44.88 65.42 69. 49.97 47.93 65.24 58.91 59.27 70.01 56.86 70.34 71.71 Table 1: Experimental results on three different datasets. For each dataset, the best method is highlighted in bold, and the second-best method is underlined. CDA and CDA-M outperform all the baselines across different metrics. Answerable Prediction F1 (F1ans) (Kim et al., 2024a) F1ans measures the harmonic mean of precision ( ) for answerable queries. N1 N1+N2+N4 N1 N1+N2+N ) and recall ( Abstention F1 (F1abs) (Kim et al., 2024a) The model is expected to abstain from generating incorrect responses for unanswerable queries but also minimize over-abstention. F1abs measures both precision ( N5 ) and recall ( N5 ). N3+N5 N4+N Reliability Score (RS) weighted sum of accuracy (Acc., N1+N3+N5 and coverage (Cov., N1+N2+N3+N4+N5 (Xu et al., 2024) RS is the ) N1 N1+N2+N3+N4+N5 ). RS(α) = α Cov. + (1 α) Acc. (9) Here, α represents the models answer rate (1 N3+N5 ). RS prioritizes accuracy at lower N1+N2+N3+N4+N5 answer rate. In contrast, RS focuses on avoiding errors with higher coverage at high answer rate. We also report the models accuracy and coverage for more thorough analysis."
        },
        {
            "title": "6.3 Baselines",
            "content": "To evaluate the effectiveness of our approach, we compare different inference methods as baselines. includes contextual promptDirect Prompting ing (CONTEXT) employing Tc(x, c) as the input and abstention prompting (ABSTAIN) utilizing Ta(x, c), with an explicit instruction for abstention. SELF-ASK SELF-ASK prompts the model with Tc(x, c) and further verifies the generation (Kadavath et al., 2022). Predictions verified as unknown are abstained. Context-aware Decoding (CAD) CAD amplifies contextual influence by gauging do + ) with fixed wc wc = dc (Shi et al., 2024b) . dp (dc Adaptive Contrastive Decoding (ACD) ACD follows Eq. 3 where wc at each decoding step (Kim et al., 2024b). = 1 Hc Hp +Hc ACD with Abstention (ACD-A) ACD-A expands ACD to perform abstention where wc = 1 followHp ing Eq. 5. Hc +Ha +Hc Ha +Ha +Hc = 1 and wa Hp First Step Branching (FSB) FSB selects the prompt to use from Tp(), Tc(), and Ta() at the first decoding step by comparing Hp 1, and Ha 1. 1, Hc"
        },
        {
            "title": "6.4 Main Results",
            "content": "The main results are presented in Table 1. Methods not accounting for abstention fail to handle unanswerable queries. CONTEXT, CAD, and ACD exhibit near-zero F1abs, indicating their inability to handle unanswerable queries. The negligible gap between accuracy and coverage also validates their incapability to abstain. Incorporating abstention enhances the handling of unanswerable queries. ABSTAIN and SELFASK exhibit biased abstentions, demonstrating low accuracy and high coverage, resulting in low RS overall. ACD-A exhibits poor performance despite incorporating abstention. FSB emerges as the strongest among the baseline, effectively addressing (un)answerable queries. Overall, the baselines demonstrate the ability to perform abstention, but they fail to effectively balance the trade-off between generating accurate answers and appropriately abstaining when necessary. CDA and CDA-M exhibit superior performance across all datasets. CDA and CDAM achieve strong performance in both F1ans and F1abs, highlighting their effectiveness in handling (un)answerable queries. Notably, their accuracy is second only to ACD, underscoring their abilities to adequately incorporate relevant knowledge. Figure 5 displays the average performance across all scenarios for each metric, providing clear overview Figure 5: Average performance compared with the strongest baselines (ACD, FSB). The area covered by each method is indicated in parentheses. CDA-M covers the largest area, indicating superior performance across all metrics. Individual results are provided in Figure 11. Figure 6: The proportion of correct prediction and abstention for each scenario. CDA effectively balances between correct predictions and abstentions, especially in the presence of irrelevant contexts. of the results. This observation reaffirms that CDAM consistently outperforms the baselines in multiple perspectives."
        },
        {
            "title": "7 Ablation Study",
            "content": "This section conducts series of ablation studies for deeper analysis of CDA. Unless otherwise specified, all experiments are conducted on LLAMA3 8B INSTRUCT with the setting from Section 3. CDA is compared with baselines capable of abstention, including ABSTAIN, SELF-ASK, FSB, and ACD-A. Further details are described in Appendix D."
        },
        {
            "title": "7.1 Analysis of Different Scenarios",
            "content": "Figure 6 illustrates the proportion of correct predictions (in blue) and abstentions (in gray) for each Figure 8: Average RS in RAG settings across all datasets. CDA and CDA-M outperform all the baselines."
        },
        {
            "title": "7.3 Effect of Calibration",
            "content": "CDA quantifies knowledge relevance with calibrated uncertainty measures. To evaluate the effect of calibration, we modify Eq. 5 to use noncalibrated measures, setting rp = Hc . The results are presented in Table 2. We can observe significant degradation in performance across all metrics compared to CDA, highlighting the importance of calibration. = Hp and rc"
        },
        {
            "title": "7.4 Evaluation on RAG Setting",
            "content": "To evaluate CDA in real-world scenarios, we conduct evaluations within the RAG setting. Experimental Setting We utilize CONTRIEVERMSMARCO (Izacard et al., 2022) as retriever, and the top-1 context is retrieved from the Wikipedia contexts.4 Unlike the controlled setting, where each knowledge is precisely estimated, the prior knowledge of the given query is unknown in the practical RAG setting. Since it is difficult to confirm the querys answerability, we evaluate the performance solely based on the Reliability Score (RS). Experimental Results Figure 8 presents the average results across all datasets and models. Similar to the main experiment, methods without abstention capabilities demonstrate poor performance, while FSB exhibits the best performance among the baselines. Overall, CDA and CDA-M outperform the baselines, highlighting the effectiveness even in the practical RAG setting. Full results in each scenario can be found in Table 8."
        },
        {
            "title": "8 Conclusion",
            "content": "In this work, we present Contrastive Decoding with Abstention (CDA), novel, training-free decoding method that incorporates abstention in cases 4Wikipedia dump from Dec. 2018. Figure 7: Changes in F1ans and F1abs according to α. Applying momentum significantly improves F1ans."
        },
        {
            "title": "Method",
            "content": "F1ans F1abs RS Acc. Cov. NQ"
        },
        {
            "title": "TriviaQA",
            "content": "CDA Calibration CDA Calibration CDA Calibration 71.88 61.00 77.35 67.41 78.11 66. 55.58 52.36 61.83 56.13 64.87 57.62 62.93 49.70 68.96 57.10 69.08 55. 52.25 39.63 57.22 46.14 56.59 44.02 67.43 57.98 73.31 63.85 74.46 63. Table 2: The effects of applying calibration. We can observe significant degradation without calibration. scenario. Note that when relevant knowledge is present (P=1 or C=1), the model should generate correct answers and abstain otherwise (P=0 and C=0). Most baselines exhibit strong abstention bias, especially with irrelevant context, without making full use of the parametric knowledge. Overall, CDA adequately leverages relevant knowledge while maintaining balanced abstention capability."
        },
        {
            "title": "7.2 Ablation on Momentum Weight",
            "content": "In this section, we evaluate the impact of momentum weight α on CDA-M by varying α from 0.0 to 1.0. Figure 7 displays F1ans and F1abs of CDAM cross different α alongside the best-performing baselines. We can observe that F1abs remains consistent regardless of α, while F1ans improves as momentum is incorporated. In general, applying momentum mitigates hallucinations while preserving the abstention capabilities. of insufficient relevant knowledge. CDA quantifies the relevance of parametric and contextual knowledge for given query and dynamically attends to relevant knowledge. When both types of knowledge are deemed irrelevant, CDA guides the model to abstain, thereby preventing it from generating invalid responses. Through extensive experiments on multiple question-answering datasets, CDA demonstrates its ability to generate accurate responses when relevant knowledge is available and abstain otherwise, significantly mitigating the risks of hallucination. The findings highlight CDAs potential to increase reliability and safety in critical applications."
        },
        {
            "title": "Limitations",
            "content": "Our study acknowledges few limitations that present opportunities for future research. Inference Cost Similar to other contrastive decoding approaches, CDA requires additional inferences, which increase the cost compared to greedy decoding. Specifically, CDA requires total of five inferences, including the use of null prompts. However, we consider this reasonable trade-off for building reliable and safe model. Limitations in Task Scope Our study primarily focuses on single-context scenarios, providing relatively clear distinction between the presence and absence of knowledge, facilitating precise analysis. However, extending the approach to multicontext scenarios would be an important direction for future work. Additionally, while we evaluate short-form QA tasks, expanding the method to include reasoning-intensive, long-form generation tasks would be meaningful advancement. Advanced Abstention The current work mainly focuses on the models ability to simply express abstention, which lacks user-friendliness. Future research could explore incorporating reasoning capabilities to explain the rationale behind abstention decisions."
        },
        {
            "title": "References",
            "content": "Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U. Rajendra Acharya, Vladimir Makarenkov, and Saeid Nahavandi. 2021. review of uncertainty quantification in deep learning: Techniques, applications and challenges. Information Fusion, 76:243297. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Gustaf Ahdritz, Tian Qin, Nikhil Vyas, Boaz Barak, and Benjamin Edelman. 2024. Distinguishing the knowable from the unknowable with language models. arXiv preprint arXiv:2402.03563. Alfonso Amayuelas, Kyle Wong, Liangming Pan, Wenhu Chen, and William Yang Wang. 2024. Knowledge of knowledge: Exploring known-unknowns uncertainty with large language models. In Findings of the Association for Computational Linguistics: ACL 2024, pages 64166432, Bangkok, Thailand. Association for Computational Linguistics. Usman Anwar, Abulhair Saparov, Javier Rando, Daniel Paleka, Miles Turpin, Peter Hase, Ekdeep Singh Lubana, Erik Jenner, Stephen Casper, Oliver Sourbut, Benjamin L. Edelman, Zhaowei Zhang, Mario Günther, Anton Korinek, Jose Hernandez-Orallo, Lewis Hammond, Eric Bigelow, Alexander Pan, Lauro Langosco, Tomasz Korbak, Heidi Chenyu Zhang, Ruiqi Zhong, Sean hEigeartaigh, Gabriel Recchia, Giulio Corsi, Alan Chan, Markus Anderljung, Lilian Edwards, Aleksandar Petrov, Christian Schroeder de Witt, Sumeet Ramesh Motwani, Yoshua Bengio, Danqi Chen, Philip Torr, Samuel Albanie, Tegan Maharaj, Jakob Nicolaus Foerster, Florian Tramèr, He He, Atoosa Kasirzadeh, Yejin Choi, and David Krueger. 2024. Foundational challenges in assuring alignment and safety of large language models. Transactions on Machine Learning Research. Survey Certification, Expert Certification. Stefan Buttcher, Charles LA Clarke, and Gordon CorInformation retrieval: Implementing mack. 2016. and evaluating search engines. Mit Press. Hung-Ting Chen, Michael Zhang, and Eunsol Choi. 2022. Rich knowledge sources bring complex knowledge conflicts: Recalibrating models to reflect conflicting evidence. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 22922307, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Qinyuan Cheng, Tianxiang Sun, Xiangyang Liu, Wenwei Zhang, Zhangyue Yin, Shimin Li, Linyang Li, Zhengfu He, Kai Chen, and Xipeng Qiu. 2024. Can AI assistants know what they dont know? In Fortyfirst International Conference on Machine Learning. Jinhao Duan, Hao Cheng, Shiqi Wang, Alex Zavalny, Chenan Wang, Renjing Xu, Bhavya Kailkhura, and Kaidi Xu. 2024. Shifting attention to relevance: Towards the predictive uncertainty quantification of In Proceedings free-form large language models. of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 50505063, Bangkok, Thailand. Association for Computational Linguistics. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Romina Etezadi and Mehrnoush Shamsfard. 2023. The state of the art in open domain complex question answering: survey. Applied Intelligence, 53(4):4124 4144. Ruitao Feng, Xudong Hong, Mayank Jobanputra, Mattes Warning, and Vera Demberg. 2024a. Retrievalaugmented modular prompt tuning for low-resource data-to-text generation. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 1405314062, Torino, Italia. ELRA and ICCL. Shangbin Feng, Chan Young Park, Yuhan Liu, and Yulia Tsvetkov. 2023. From pretraining data to language models to downstream tasks: Tracking the trails of In political biases leading to unfair NLP models. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1173711762, Toronto, Canada. Association for Computational Linguistics. Shangbin Feng, Weijia Shi, Yike Wang, Wenxuan Ding, Vidhisha Balachandran, and Yulia Tsvetkov. 2024b. Dont hallucinate, abstain: Identifying LLM knowledge gaps via multi-LLM collaboration. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1466414690, Bangkok, Thailand. Association for Computational Linguistics. Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi Chen. 2019. MRQA 2019 shared task: Evaluating generalization in reading comprehension. In Proceedings of the 2nd Workshop on Machine Reading for Question Answering, pages 113, Hong Kong, China. Association for Computational Linguistics. Kang He, Yinghan Long, and Kaushik Roy. 2024. Prompt-based bias calibration for better zero/fewshot learning of language models. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1267312691, Miami, Florida, USA. Association for Computational Linguistics. Ari Holtzman, Peter West, Vered Shwartz, Yejin Choi, and Luke Zettlemoyer. 2021. Surface form competition: Why the highest probability answer isnt always In Proceedings of the 2021 Conference on right. Empirical Methods in Natural Language Processing, pages 70387051, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022. Unsupervised dense information retrieval with contrastive learning. Preprint, arXiv:2112.09118. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM Comput. Surv., 55(12). Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Che Jiang, Biqing Qi, Xiangyu Hong, Dayuan Fu, Yang Cheng, Fandong Meng, Mo Yu, Bowen Zhou, and Jie Zhou. 2024. On large language models hallucination with regard to known facts. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 10411053, Mexico City, Mexico. Association for Computational Linguistics. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16011611, Vancouver, Canada. Association for Computational Linguistics. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Language models Tran-Johnson, et al. 2022. arXiv preprint (mostly) know what they know. arXiv:2207.05221. Amita Kamath, Robin Jia, and Percy Liang. 2020. Selective question answering under domain shift. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5684 5696, Online. Association for Computational Linguistics. Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 2023. Large language models struggle to learn long-tail knowledge. In Proceedings of the 40th International Conference on Machine Learning, ICML23. JMLR.org. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for opendomain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 67696781, Online. Association for Computational Linguistics. Jungo Kasai, Keisuke Sakaguchi, yoichi takahashi, Ronan Le Bras, Akari Asai, Xinyan Velocity Yu, Dragomir Radev, Noah A. Smith, Yejin Choi, and Kentaro Inui. 2023. Realtime QA: Whats the answer right now? In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Hyuhng Joon Kim, Youna Kim, Cheonbok Park, Junyeob Kim, Choonghyun Park, Kang Min Yoo, Sanggoo Lee, and Taeuk Kim. 2024a. Aligning language models to explicitly handle ambiguity. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 19892007, Miami, Florida, USA. Association for Computational Linguistics. Minsu Kim and James Thorne. 2024. Epistemology of language models: Do language models have holistic knowledge? In Findings of the Association for Computational Linguistics: ACL 2024, pages 12644 12669, Bangkok, Thailand. Association for Computational Linguistics. Youna Kim, Hyuhng Joon Kim, Cheonbok Park, Choonghyun Park, Hyunsoo Cho, Junyeob Kim, Kang Min Yoo, Sang-goo Lee, and Taeuk Kim. 2024b. Adaptive contrastive decoding in retrievalaugmented generation for handling noisy contexts. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 24212431, Miami, Florida, USA. Association for Computational Linguistics. Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. In The Eleventh International Conference on Learning Representations. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452466. Angeliki Lazaridou, Adhiguna Kuncoro, Elena Gribovskaya, Devang Agrawal, Adam Liška, Tayfun Terzi, Mai Gimenez, Cyprien de Masson dAutume, Tomas Kocisky, Sebastian Ruder, Dani Yogatama, Kris Cao, Susannah Young, and Phil Blunsom. 2024. Mind the gap: assessing temporal generalization in neural language models. In Proceedings of the 35th International Conference on Neural Information Processing Systems, NIPS 21, Red Hook, NY, USA. Curran Associates Inc. Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. 2023. Contrastive decoding: Open-ended text generation as optimization. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1228612312, Toronto, Canada. Association for Computational Linguistics. Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A. Smith, and Yejin Choi. 2021. DExperts: Decoding-time controlled text generation with experts and anti-experts. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 66916706, Online. Association for Computational Linguistics. Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. 2021. Entity-based knowledge conflicts in question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 70527063, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Andrey Malinin and Mark Gales. 2021. Uncertainty estimation in autoregressive structured prediction. In International Conference on Learning Representations. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 98029822, Toronto, Canada. Association for Computational Linguistics. Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 19061919, Online. Association for Computational Linguistics. Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2020. AmbigQA: Answering ambiguous open-domain questions. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5783 5797, Online. Association for Computational Linguistics. Cheng Niu, Yuanhao Wu, Juno Zhu, Siliang Xu, KaShun Shum, Randy Zhong, Juntong Song, and Tong Zhang. 2024. RAGTruth: hallucination corpus for developing trustworthy retrieval-augmented language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1086210878, Bangkok, Thailand. Association for Computational Linguistics. Sean OBrien and Mike Lewis. 2023. Contrastive decoding improves reasoning in large language models. arXiv preprint arXiv:2309.09117. Zexuan Qiu, Zijing Ou, Bin Wu, Jingjing Li, Aiwei Liu, and Irwin King. 2024. Entropy-based decoding for retrieval-augmented large language models. arXiv preprint arXiv:2406.17519. Mahimai Raja, Yuvaraajan, et al. 2024. rag-based medical assistant especially for infectious diseases. In 2024 International Conference on Inventive Computation Technologies (ICICT), pages 11281133. IEEE. questions. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 10751085, Seattle, United States. Association for Computational Linguistics. Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 39823992, Hong Kong, China. Association for Computational Linguistics. Maarten Sap, Saadia Gabriel, Lianhui Qin, Dan Jurafsky, Noah A. Smith, and Yejin Choi. 2020. Social bias frames: Reasoning about social and power implications of language. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 54775490, Online. Association for Computational Linguistics. Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves In Advances in Neural Information to use tools. Processing Systems, volume 36, pages 6853968551. Curran Associates, Inc. Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and Luke Zettlemoyer. 2024a. Detecting pretraining data from large language models. In The Twelfth International Conference on Learning Representations. Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke Zettlemoyer, and Wen-tau Yih. 2024b. Trusting your evidence: Hallucinate less with contextaware decoding. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers), pages 783791, Mexico City, Mexico. Association for Computational Linguistics. Xiao Shi, Zhengyuan Zhu, Zeyu Zhang, and Chengkai Li. 2023. Hallucination mitigation in natural language generation from large-scale open-domain knowledge graphs. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1250612521, Singapore. Association for Computational Linguistics. Tejas Srinivasan, Jack Hessel, Tanmay Gupta, Bill Yuchen Lin, Yejin Choi, Jesse Thomason, and Khyathi Chandu. 2024. Selective selective prediction: Reducing unnecessary abstention in In Findings of the vision-language reasoning. Association for Computational Linguistics: ACL 2024, pages 1293512948, Bangkok, Thailand. Association for Computational Linguistics. Meiqi Sun, Wilson Yan, Pieter Abbeel, and Igor Mordatch. 2022. Quantifying uncertainty in foundation models via ensembles. In NeurIPS 2022 Workshop on Robustness in Sequence Modeling. Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liangyan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, and Trevor Darrell. 2024. Aligning large multimodal models with factually augmented RLHF. In Findings of the Association for Computational Linguistics: ACL 2024, pages 1308813110, Bangkok, Thailand. Association for Computational Linguistics. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. 2023. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Neeraj Varshney, Pavel Dolin, Agastya Seth, and Chitta Baral. 2024. The art of defending: systematic evaluation and analysis of LLM defense strategies on safety and over-defensiveness. In Findings of the Association for Computational Linguistics: ACL 2024, pages 1311113128, Bangkok, Thailand. Association for Computational Linguistics. Jonas Waldendorf, Barry Haddow, and Alexandra Birch. 2024. Contrastive decoding reduces hallucinations in large multilingual machine translation models. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 25262539, St. Julians, Malta. Association for Computational Linguistics. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations. Bingbing Wen, Jihan Yao, Shangbin Feng, Chenjun Xu, Yulia Tsvetkov, Bill Howe, and Lucy Lu Wang. 2024. Know your limits: survey of abstention in large language models. Preprint, arXiv:2407.18418. Elior Sulem, Jamaal Hay, and Dan Roth. 2022. Yes, no or IDK: The challenge of unanswerable yes/no Yijun Xiao and William Yang Wang. 2021. On hallucination and predictive uncertainty in conditional language generation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 27342744, Online. Association for Computational Linguistics. Hongshen Xu, Zichen Zhu, Situo Zhang, Da Ma, Shuai Fan, Lu Chen, and Kai Yu. 2024. Rejection improves reliability: Training LLMs to refuse unknown questions using RL from knowledge feedback. In First Conference on Language Modeling. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23692380, Brussels, Belgium. Association for Computational Linguistics. Junjie Ye, Sixian Li, Guanyu Li, Caishuang Huang, Songyang Gao, Yilong Wu, Qi Zhang, Tao Gui, and Xuanjing Huang. 2024. ToolSword: Unveiling safety issues of large language models in tool learning across three stages. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2181 2211, Bangkok, Thailand. Association for Computational Linguistics. Dawei Yin, Yuening Hu, Jiliang Tang, Tim Daly, Mianwei Zhou, Hua Ouyang, Jianhui Chen, Changsung Kang, Hongbo Deng, Chikashi Nobata, JeanMarc Langlois, and Yi Chang. 2016. Ranking relevance in yahoo search. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 16, page 323332, New York, NY, USA. Association for Computing Machinery. Hanning Zhang, Shizhe Diao, Yong Lin, Yi Fung, Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji, and Tong Zhang. 2024a. R-tuning: Instructing large language models to say dont know. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 71137139, Mexico City, Mexico. Association for Computational Linguistics. Lingxi Zhang, Jing Zhang, Xirui Ke, Haoyang Li, Xinmei Huang, Zhonghui Shao, Shulin Cao, and Xin Lv. 2023. survey on complex factual question answering. AI Open, 4:112. Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong Long, Xiao Liu, Xuanyu Lei, Jie Tang, and Minlie Huang. 2024b. SafetyBench: Evaluating the safety of large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1553715553, Bangkok, Thailand. Association for Computational Linguistics. Bowen Zhao, Zander Brumbaugh, Yizhong Wang, Hannaneh Hajishirzi, and Noah Smith. 2024a. Set the clock: Temporal alignment of pretrained language models. In Findings of the Association for Computational Linguistics: ACL 2024, pages 1501515040, Bangkok, Thailand. Association for Computational Linguistics. Tony Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning. Zheng Zhao, Emilio Monti, Jens Lehmann, and Haytham Assem. 2024b. Enhancing contextual understanding in large language models through contrastive decoding. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 42254237, Mexico City, Mexico. Association for Computational Linguistics. Wenxuan Zhou, Sheng Zhang, Hoifung Poon, and Muhao Chen. 2023. Context-faithful prompting for large language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1454414556, Singapore. Association for Computational Linguistics."
        },
        {
            "title": "Setting",
            "content": "In this section, we provide the details of the controlled experimental setup. A.1 Dataset Details Natural Questions (NQ) (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), HotpotQA (Yang et al., 2018) are an open-domain question answering datasets, structured to include question, short form answer, and pre-defined context. The answer span, including the ground-truth answer, can be found within the context. Specifically, NQ is composed of information-seeking queries from the Google search engine, and the contexts are Wikipedia pages retrieved by Crowdworkers. HotpotQA is multi-hop reasoning dataset consisting of two entity-linked paragraphs from Wikipedia and questions gathered from crowdworkers. Unlike its original setting, which includes distractor paragraphs, we use the processed version from MRQA (Fisch et al., 2019), where the distractors have been removed. TriviaQA utilizes question-and-answer pairs collected from trivia and quiz-league websites. We use the web version of TriviaQA from MRQA. A.2 Data Preprocessing Answer the following question. The dataset comprises query xi, ground-truth answer yi, and pre-defined context ci. To ensure the input and the output remain concise, we limit the length of the xi and the yi to 50 and 10 words, respectively. When xi appears multiple times within ci, we construct corresponding number of spans (c1 ) for the same query xi. The resulting triplets {(xi, c1 , yi)} are all included in the dataset Dorg. , yi), ..., (xi, ck , ..., ck A.3 Knowledge Estimation Inference For parametric knowledge estimation, we utilize the template Tp(x) from Table 3, which only provides the input query x. The model can solely leverage its intrinsic parametric knowledge to generate an answer for x. When estimating contextual knowledge, we employ the template Tc(x, c) from Table 4, where additional context is available for inference. The model can utilize information provided in to generate response. Relevant Knowledge Estimation Even though the pre-defined context always contains the answer span, additional validation is necessary since it is split into 100-word spans and thus may not contain valid information to answer the query. Thus, we measure the sampling consistency by applying temperature of 1.0 and generating = 10 samples for single query. We compare each generation with the ground-truth answer and measure the correctness. η used as threshold value for the consistency rate, is set to 0.7. In other words, the model is deemed to have relevant knowledge for the input if at least eight samples are correct out of ten generations. Irrelevant Knowledge Estimation For irrelevant context selection, we first exclude contexts from the training set that contains the answer span as they may provide unintended information for generation. Then, we utilize SBERT (Reimers and Gurevych, 2019) embedding to measure the cosine similarity between the training set contexts and the relevant context. Finally, we select contexts with consistency rate of = 0 to ensure that they do not provide unintended information or hints. Final Dataset Construction Finally, we balance the number of parametric relevant (pi = 1) and irrelevant (pi = 0) samples by random sampling to match the size of the smaller set. The final <few-shot demonstrations> Question: <question> Answer: Table 3: Template utilizing parametric knowledge. Answer the following question based on the given context. <few-shot demonstrations> Context: <context> Question: <question> Answer: Table 4: Template employing contextual knowledge. number of selected samples for each model is presented in Table 5. Note that the number of selected data points varies for each model, as the possessed knowledge within each model differs."
        },
        {
            "title": "B Details of Model Uncertainty Analysis",
            "content": "For the preliminary experiment, the parametric uncertainty is measured by providing with parametric template Tp() from Table 3. Similarly, the contextual uncertainty is quantified by presenting and with the contextual template Tc() from Table 4. The explicit abstention prompt Ta() is demonstrated in Table 6, also utilizing and as the inputs. The entropy is measured following Eq. 1. Figure 3 displays the frequency histogram of the entropy values with different inference templates and scenarios. Specifically, the entropy values ranging from 0.0 to 1.0 are divided into 40 bins. Each samples entropy value is assigned to corresponding bin, and the frequency of samples within each bin is visualized. We can observe intuitive results for the answerable scenarios (P=0 or C=0). When P=1 and C=1, both knowledge display highly certain distribution (low entropy). When P=1 and C=0, the parametric knowledge demonstrates more certain distribution than contextual knowledge. In cases where P=0 and C=1, the contextual distribution exhibits relatively confident distribution. However, when we consider the cases of P=0 and C=0, it becomes challenging to differentiate all the scenarios. Specifically, they display similar distribution since both knowledge is uncertain in such scenario. The relative comparison between the"
        },
        {
            "title": "Model",
            "content": "NQ"
        },
        {
            "title": "HotpotQA TriviaQA",
            "content": "LLAMA3 8B INSTRUCT LLAMA2 7B CHAT LLAMA2 13B CHAT MISTRAL 7B INSTRUCT 2,338 1,462 2,150 764 3,614 2,748 3,508 1,110 7,524 13,702 9,806 9,364 Table 5: Number of samples for each dataset constructed for the controlled experimental setting. Answer the following question based on the given context. If you are unsure or dont know the correct answer, simply respond with \"Unknown.\" <few-shot demonstrations> Context: <context> Question: <question> Answer: Table 6: Template with explicit instruction for abstention two knowledge makes distinguishing from the scenario where P=1 and C=1 difficult. Furthermore, the abstention prompts also demonstrate similar distributions across all the scenarios. This aligns with the previous findings (Cheng et al., 2024; Kim et al., 2024a) that abstention prompts often display biased abstention regardless of the input. The findings highlight the importance of properly calibrated measures that can distinguish each scenario."
        },
        {
            "title": "C Experiential Details",
            "content": "Details of CDA and CDA-M Identical to the experimental setting from Section 4, Tp(), Tc(), and Ta() for CDA utilize the template from Table 3, Table 4, and Table 6, respectively. For the calibration, the query placeholder token is set to [QUESTION] and the context placeholder token utilizes the token [CONTEXT]. The output distribution dp are measured as follows. and dp dp = logitθ(yt Tp(x, y<t)), dc = logitθ(yt Tc(x, c, y<t)), (10) where y<t are the previously generated tokens. The momentum for CDA-M is applied for each weight as follows. wc α wc α wp wp α wa wa t1 + (1 α) wc , t1 + (1 α) wp , t1 + (1 α) wa , (11) where the momentum weight α for is set to 0.7. the following the Answer context, question, and the answer, is the question known or unknown? Answer only known or unknown. question."
        },
        {
            "title": "Given",
            "content": "Context: <context> Question: <question> Answer: <initial generation> Is the question known or unknown? Answer only known or unknown. Known or Unknown: Table 7: Verification template for SELF-ASK. C.1 Evaluation Details , , yi, pi)}N For evaluation, we utilize the dataset = {(xi, c+ i=1 constructed in Section 3. Specifically, for xi D, we inference with both relevant (xi, c+ ) contexts. Only the input (xi, ) where pi = 0 is deemed unanswerable. All other inputs are considered answerable by leveraging either parametric or contextual knowledge. ) and irrelevant (xi, The model is provided with 2-shot demonstration from the training set and evaluated on greedy generation with temperature 0. Following previous works (Mallen et al., 2023; Schick et al., 2023), the prediction is correct for answerable queries if it contains the ground-truth answer. For unanswerable queries, the model is expected to properly abstain. Since there are various ways to abstain, we use the following phrases to detect the abstention. The presence of pre-defined abstention phrases in the models output is treated as successful abstention. The pre-defined phrases are the following: [unknown answer, answer is unknown, unable to answer, no answer, cannot answer, dont know, do not know] C.2 Baselines In this section, we describe the implementation details of the baselines. The parametric (Tp()), contextual (Tc()), abstention (Ta()) prompts utilize the template from Table 3, Table 4, and Table 6, respectively. SELF-ASK uses Tc(x, c) from Table 4 for the initial generation. The initial generation is verified using the template from Table 7. If the verification result is unknown, the model abstains from providing the initial prediction. CAD utilizes fixed weight wc . Following the original work (Shi et al., 2024b), we report the best result between cases with wc set to 0.5 and 1. and dp The output distribution dc are derived by leveraging templates Tc() and Tp(), respectively. ACD measures the contextual uncertainty Hc and parametric uncertainty Hp by utilizing templates Tc() and Tp(), respectively. ACD-A incorporates abstention distribution to ACD. We measure the abstention distribution with template Ta(). 1, Hc 1, and Ha FSB utilizes Tp(), Tc(), and Ta() to measure Hp 1, respectively. We compare the entropy values at the first decoding step and select the prompting method with the lowest entropy (highest confidence). We process the decoding with the selected prompting method."
        },
        {
            "title": "D Details on Ablation Experiments",
            "content": "D.1 Ablation on Momentum Weight The results presented in Section 7.2 indicate that incorporating momentum enhances the performance of F1ans. In this section, we conduct case study to analyze this finding. Figure 9 (a) demonstrates an example where an irrelevant context is provided. Figure 9 (b) displays the generation result of CDA and the weights measured for each knowledge at every decoding step. CDA initially assigns more weight to relevant parametric knowledge, generating the correct span up to International Bank. However, the models attention shifts toward contextual knowledge, incorporating incorrect information from International Bank for Reconstruction and Development. We presume that the phrase International Bank causes this shift in weight, which also appears in the irrelevant context. In contrast, Figure 9 (b) demonstrates that CDA-M, leveraging momentum, mitigates abrupt shifts in attention toward irrelevant knowledge, resulting in accurate generation. Notably, CDA-M does not persistently focus on single knowledge source. case where CDAM fully utilizes both knowledge appropriately is shown in Figure 10. Figure 10 displays case where relevant context is provided to the model. CDA-M initially focuses on parametric knowledge, and over time, it transitions to incorporate contextual knowledge, producing richer and more nuanced answer. Figure 9: Example of an incorrect case of CDA and the correct case of CDA-M. (a) The query and an irrelevant context are provided to the model. (b) The generation result of CDA and the weights measures at every decoding step. The weight shifts to the irrelevant context, resulting in incorrect generation. (c) The generation result of CDA-M. The momentum mitigates abrupt shifts in attention, resulting in correct generation. D.2 Evaluation on RAG Setting Table 8 presents the full results of every dataset and backbone. CDA and CDA-M consistently outperform the baselines across all the datasets, underscoring their effectiveness in practical scenarios."
        },
        {
            "title": "Method",
            "content": "NQ"
        },
        {
            "title": "HotpotQA TriviaQA",
            "content": "LLAMA3 8B INSTRUCT CONTEXT ABSTAIN CAD ACD SELF-ASK FSB ACD-A CDA CDA-M 33.35 50.74 28.86 38.40 45.67 53.66 51.80 54.33 54.32 LLAMA2 7B CHAT CONTEXT ABSTAIN CAD ACD SELF-ASK FSB ACD-A CDA CDA-M 30.11 22.06 27.20 33.02 31.72 37.36 33.56 47.09 46.40 LLAMA2 13B CHAT CONTEXT ABSTAIN CAD ACD SELF-ASK FSB ACD-A CDA CDA-M 31.15 41.54 28.40 35.65 34.15 49.88 48.30 51.42 51.95 MISTRAL 7B INSTRUCT CONTEXT ABSTAIN CAD ACD SELF-ASK FSB ACD-A CDA CDA-M 30.68 49.11 28.60 32.42 39.34 49.05 48.42 49.36 49.51 31.00 50.71 28.24 34.05 39.61 51.07 50.35 51.77 51.80 28.28 20.02 25.86 30.04 33.99 39.58 35.05 45.07 45. 30.57 24.54 27.59 32.73 33.68 45.28 41.62 44.06 44.73 29.00 41.99 26.08 28.91 38.78 47.32 47.14 46.94 47.13 65.30 76.66 59.32 73.91 64.91 79.14 75.58 80.62 80. 61.01 70.70 54.44 67.35 63.12 73.13 70.55 73.18 73.90 66.81 47.11 61.80 72.09 67.73 72.79 65.78 77.89 78.74 62.40 73.00 56.09 66.24 67.94 70.26 70.39 72.86 73. Table 8: Results of Reliability Score (RS) across all the considered scenarios in the RAG setting. CDA and CDA-M consistently outperform all the baselines. Figure 10: Example generation of CDA-M. (a) The query and relevant context. (b) The generation result of CDA-M. CDA-M initially focuses on parametric knowledge, and the attention transitions to incorporate contextual knowledge, generating richer output. (a) LLAMA3 8B INSTRUCT NQ (b) LLAMA3 8B INSTRUCT HotpotQA (c) LLAMA3 8B INSTRUCT TriviaQA (d) LLAMA2 7B CHAT NQ (e) LLAMA2 7B CHAT HotpotQA (f) LLAMA2 7B CHAT TriviaQA (g) LLAMA2 13B CHAT NQ (h) LLAMA2 13B CHAT HotpotQA (i) LLAMA2 13B CHAT TriviaQA (j) MISTRAL 7B INSTRUCT NQ (k) MISTRAL 7B INSTRUCT HotpotQA (l) MISTRAL 7B INSTRUCT TriviaQA Figure 11: Individual results for all datasets and backbones compared with the strongest baselines (ACD, FSB) The area covered by each method is indicated in parentheses. Overall, CDA-M covers the largest area, indicating superior performance across all metrics."
        }
    ],
    "affiliations": [
        "Hanyang University",
        "IntelliSys, Korea",
        "Seoul National University"
    ]
}