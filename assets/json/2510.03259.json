{
    "paper_title": "Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement Learning",
    "authors": [
        "Yoonjeon Kim",
        "Doohyuk Jang",
        "Eunho Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent studies on reasoning models explore the meta-awareness of language models, the ability to know how to think by itself. We argue that large reasoning models lack this meta-awareness property by proving severe misalignment between true rollouts and predicted meta information. We posit that aligning meta-prediction with true rollouts will lead to significant performance gains. To verify this hypothesis, we design a training pipeline that boosts Meta-Awareness via Self-Alignment (MASA), and prove that enhanced meta-awareness directly translates to improved accuracy. Unlike existing meta-cognitive reasoning models, our method does not require external training sources but leverages self-generated signals to train meta-awareness. Moreover, our method enables efficient training by i) filtering out zero-variance prompts that are either trivial or unsolvable and ii) cutting off lengthy rollouts when they are unlikely to lead to correct answers. The results are inspiring: our strategy yields significant improvements in both accuracy and training efficiency on in-domain tasks and shows strong generalization to out-of-domain benchmarks. More specifically, our method can speed up GRPO training by over 1.28x to reach the same performance, and achieve a 19.3% gain in accuracy on AIME25, and a 6.2 % average gain over six mathematics benchmarks. Training with meta-cognitive guidance enhances out-of-domain generalization, giving a 3.87 % boost on GPQA-Diamond and a 2.08 % overall accuracy gain across 13 benchmarks spanning logical, scientific, and coding domains."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 9 5 2 3 0 . 0 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "META-AWARENESS ENHANCES REASONING MODELS: SELF-ALIGNMENT REINFORCEMENT LEARNING Yoonjeon Kim1 Doohyuk Jang1 Eunho Yang1,2 1KAIST 2AITRICS"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent studies on reasoning models explore the meta-awareness of language models, the ability to know how to think by itself. We argue that large reasoning models lack this meta-awareness property by proving severe misalignment between true rollouts and predicted meta information. We posit that aligning metaprediction with true rollouts will lead to significant performance gains. To verify this hypothesis, we design training pipeline that boosts Meta-Awareness via Self-Alignment (MASA), and prove that enhanced meta-awareness directly translates to improved accuracy. Unlike existing meta-cognitive reasoning models, our method does not require external training sources but leverages self-generated signals to train meta-awareness. Moreover, our method enables efficient training by i) filtering out zero-variance prompts that are either trivial or unsolvable and ii) cutting off lengthy rollouts when they are unlikely to lead to correct answers. The results are inspiring: our strategy yields significant improvements in both accuracy and training efficiency on in-domain tasks and shows strong generalization to out-of-domain benchmarks. More specifically, our method can speed up GRPO to reach the same performance, and achieve 19.3% gain training by over 1.28 in accuracy on AIME25, and 6.2% average gain over six mathematics benchmarks. Training with meta-cognitive guidance enhances out-of-domain generalization, giving 3.87 % boost on GPQA-Diamond and 2.08 % overall accuracy gain across 13 benchmarks spanning logical, scientific, and coding domains. The code is available at https://github.com/akatigre/MASA-RL. (a) Poor Meta-Awareness of GRPO Model (c) Train Step Contribution (b) Enhanced Meta-Awareness of MASA (d) Meta-Awareness Contribution Figure 1: (a) Existing large reasoning models lack meta-awareness. (b) MASA significantly improves meta-awareness, as shown by the alignment between meta-predictions and the actual rollout statistics (difficulty and length). (c) Training step has limited impact on accuracy. (d) Metaawareness directly translates to increased accuracy."
        },
        {
            "title": "Preprint",
            "content": ""
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent studies have confirmed that applying RL-based post-training to large language models (LLMs) (Brown et al., 2020; Yang et al., 2025a; Touvron et al., 2023) can significantly enhance their reasoning ability. In particular, methods such as GRPO (Shao et al., 2024), which efficiently train large reasoning models (LRMs) (Guo et al., 2025a; Chen et al., 2025b) without an explicit critic model, have recently attracted considerable attention. By directly incentivizing behaviors aligned with task-desirable outcomes, this training paradigm has gained prominence as an effective mechanism for attaining state-of-the-art performance on reasoning-intensive tasks such as mathematics and code generation. Beyond the success of LRMs, the paradigm of meta-awareness, which is the ability to recognize its own knowledge and ignorance, has drawn increasing attention from the research community (Sui et al., 2025; Ha et al., 2025; De Sabbata et al., 2024; Chen et al., 2025a; Liu et al., 2025b; Zhang et al., 2025a; Shen et al., 2025; Tu et al., 2025; Shi et al., 2025; Qu et al., 2025). However, existing approaches remain constrained by their reliance on external model, curated dataset and human-designed reasoning pipelines where meta-cognitive actions are only conditionally rewarded based on the success of the solution trajectory. To this end, we propose novel RL framework, Meta-Awareness via Self-Alignment (MASA), that strengthens the meta-awareness of reasoning models by rewarding the alignment within selfgenerated signals, eliminating the need for external sources. Our method further introduces parallel rollouts for meta-predictions and solution paths, separating them into distinct reward pipelines. We show that MASA improves reasoning performance by leveraging meta-awareness of solution length, problem difficulty, and underlying mathematical concepts, outperforming even the gains achieved by simply increasing training steps (Figure 1c, Figure 1d). To strengthen the alignment between actual rollout statistics and meta-predictions, we introduce supervised fine-tuning on dynamically collected expert meta-trajectories, following DAgger-style imitation learning approach (Ross et al., 2011). The improved meta-predictions make training more efficient through predictive gating, which identifies and filters out zero-variance prompts that are either trivial or unsolvable, and early cutoff, which terminates long rollouts that are predicted to be incorrect. In addition, the meta-predictions enrich prompts with auxiliary hints that facilitate reasoning. Building on this foundation, we evaluate the effectiveness of our approach by combining with GRPO and DAPO (Yu et al., 2025; Shao et al., 2024), showing that our method is not dependent on specific policy gradient algorithm. Remarkably, MASA achieves substantial improvements in in-domain mathematical benchmarks showing average accuracy gains of 6.2%. Furthermore, boosting metaawareness also enhances generalization, as evidenced by improvements across logical, coding, and scientific reasoning benchmarks. These results demonstrate that equipping reasoning models with meta-awareness not only strengthens in-domain performance but also broadens general reasoning capabilities. Finally, predictive gating and early cutoff deliver significant efficiency gains, attaining baseline performance 1.28 times faster than the GRPO training. The contributions of this paper can be summarized as follows: We demonstrate that enhancing meta-awareness directly translates into measurable performance gains on complex reasoning tasks. We demonstrate that incentivizing meta-awareness improves both in-domain and out-of-domain generalization across logical, scientific, and coding benchmarks. We show the efficacy of meta-prediction based post-training via predictive gating and early cutoff, speeding up the time to reach baseline performance by 1. ."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Meta-Cognitive Learning Meta-cognition is viewed as prerequisite for self-improving LLMs (Liu & van der Schaar, 2025). Existing methods rely on extrinsic mechanisms with fixed action loops, limiting adaptability. Self-improving agents that plan, regulate, and reflect (Dong et al., 2025; Didolkar et al., 2025) or refine prompts via past reasoning (Qiu et al., 2025; Liu et al., 2025b)"
        },
        {
            "title": "Preprint",
            "content": "(a) Self-Alignment Reward (section 3.2) (b) Meta-Aware Gating & Hinting & Cutoff (section 3.3) Figure 2: Overall Framework of MASA (a) Parallel rollout of meta prediction path and solution path. Meta predictions are rewarded by self-alignment from statistics collected from solution rollouts. (b) Meta-based predictive gating, early cutoff and notion hinting from meta-predictions. entangle control with reasoning, often causing interference. In contrast, our approach disentangles the meta and solution path separately for stable training on meta-awareness. Other works require curated datasets (Ha et al., 2025), or delegate control to external verifiers (Ma et al., 2025; He et al., 2025) or multi-agent systems (Wan et al., 2025; Yang & Thomason, 2025; Bilal et al., 2025; Khandelwal et al., 2025), reducing scalability of meta-cognitive training. Training-free heuristics such as confidence-based stopping (Yang et al., 2025b; Qiao et al., 2025; Lu et al., 2025) or correctness checks (Ma et al., 2025) offer efficiency but lack genuine language-level meta-cognition. In contrast, our approach do not rely on human-curated reasoning pipelines, external verifiers/PRMs, or specialized datasets targeting meta-cognitive ability, but rather leverage the self-generated signals to encourage alignment between the meta-prediction and primary thinking process. Self-Control for Efficient Training Another direction that leverages meta-cognition is to regulate reasoning efficiency by allocating budgets via difficulty assessment (Chen et al., 2025a; Tu et al., 2025; Shi et al., 2025; Qu et al., 2025; Huang et al., 2025; Ji et al., 2025; Di & JoyJiaoW, 2025; Han et al., 2024b; Fang et al., 2025; Yang et al., 2025c; Zhang et al., 2025b; Wang et al., 2025; Zhang et al., 2025a; Shen et al., 2025), constraining output length with penalties or fixed limits (Aggarwal & Welleck, 2025; Li et al., 2025; Xiang et al., 2025; Zhang & Zuo, 2025), and adaptively stopping, continuing, or reflecting for compact reasoning (Ha et al., 2025; Zhang et al., 2025c; Dai et al., 2025). While these methods improve inference-time efficiency, they focus on making reasoning shorter or faster at inference time, often at the expense of reasoning performance drop. In contrast, we target post-training efficiency, achieving both efficiency and improved performance during model training rather than the inference."
        },
        {
            "title": "3 MASA: META-AWARENESS VIA SELF-ALIGNMENT AND MASA-efficient",
            "content": "We first provide background on group relative policy optimization (GRPO) variants (Section 3.1). Then we show our method: (i) MASA, which endows the LLM with the capability to perform accurate meta-predictions (Section 3.2); and (ii) MASA-efficient, an efficiency-enhanced version that accelerates MASA through predictive gating, early cutoff, and prompt hinting (Section 3.3). 3.1 PRELIMINARIES We present an overview on GRPO, which is popular RL algorithm for post-training reasoning , the policy model πθold produces group of models. Given task drawn from the distribution , oG} responses, which are referred to as rollouts, . Each response is assigned with reward based on the match between the ground truth answer and the extracted answer from r1, { the response. This is formalized as , rG} o1, { qQ, {oi}G i=1πθold (.q) LRL(θ) = (cid:34) 1 (cid:88) oi (cid:88) 1 oi (cid:110) min (cid:104) Γi,t(θ) ˆAi,t , clip1+ϵ 1ϵ(Γi,t(θ)) ˆAi,t (cid:105) βDKL(πθ (cid:111) πref ) (cid:35) ,"
        },
        {
            "title": "Preprint",
            "content": "where the importance sampling ratio between the current policy πθ and the old policy πθold is defined q, oi,<t), and clip( ) restricts the importance sampling raas Γi,t(θ) = πθ(oi,t ϵ, 1+ϵ]. Advantage calculation is formulated as ˆAi,t = rimean({ri}G tio between [1 . Following i=1) the practice of recent RL algorithms proposed in recent GRPO variants (Liu et al., 2025a; Zhang & Zuo, 2025; Zheng et al., 2025; Yu et al., 2025), we set β = 0 to ignore the KL divergence term. q, oi,<t)/πθold(oi,t std({ri}G i=1)"
        },
        {
            "title": "3.2 MASA: META-AWARENESS VIA SELF-ALIGNMENT",
            "content": "The policy model πθ is prompted with the task with two variants of instruction templates, meta1. The policy model outputs metaprediction template and solution template, creating qmeta and qsol osol prediction rollouts i=1 given qsol in parallel. The } solution rollouts are equivalent to the rollouts in regular GRPO algorithm explained in Section 3.1, while meta rollouts are structured responses that consist of predicted length, predicted difficulty, and the list of mathematical notions. i=1 given qmeta and solution rollouts ometa { } { The rollout and reward assignment for solution rollouts and meta-predictions are separated as described in Figure 2(a). For solution, the reward is assessed by the agreement between models i=1. For meta-prediction rollouts, solution and the ground truth solution, which we denote as we rely on three rewarding criteria: self-alignment of length, pass-rate, and math notions, averaged into rmeta = (rlength + rdifficulty + rnotion) /3. rsol } { Length Reward. The length alignment reward assigns 1 if the prediction belongs in the range of rollout lengths of correct solution paths. More formally, we define the length reward as rlength = 1(cid:2) min(lcorrect) lpred max(lcorrect)(cid:3), (1) = 0), then the reward assigned becomes 0. where lcorrect is list of correct response lengths from solution rollouts and lpred is the predicted length from meta rollout. In cases where correct responses do not exist for the task lcorrect ( Difficulty Reward. The difficulty alignment reward is computed as exponentially decaying reward by the factor of difference between the predicted pass-rate dpred and the true pass-rate dsol as { } osol rdifficulty = bdpreddsol (2) dsol where < 1. We choose an exponentially decaying reward to ensure that the reward becomes 1 if = 0 and rapidly approach to 0 as the difficulty difference becomes larger. dpred Notion Reward. The notion reward is defined for the list of notions, npred = [n1, , np], which are mathematical concepts that are predicted to be used in solution rollout that yields correct answer. We count the ratio of notions that appear more frequently in correct solution rollouts than in incorrect ones. Formally we define notion reward as rnotion = 1 npred (cid:88) 1(cid:2)fcount(n, 1) nnpred fcount(n, 0) > 0(cid:3), (3) where fcount is function that counts the number of notion appearance in correct or incorrect solution rollouts. The counting function is defined as follows, fcount (cid:0)n, t(cid:1) = (cid:12) (cid:12) (cid:8) 1, . . . , : } { osol , rsol = t(cid:9)(cid:12) (cid:12) , 0, 1 , } { (4) to reward notion that is more frequently included in correct solutions (t = 1) than in incorrect ones (t = 0). In detail, the notions included in the problem itself is excluded in the counting process to avoid reward hacking and the predicted notions are lemmatized to properly find inclusion in the solution rollouts via exact matching. 1The average token length of meta-predictions are 36% of average solution rollout length. The metaprediction template is deferred to Section A."
        },
        {
            "title": "Preprint",
            "content": "Algorithm 1 MASA-efficient: Efficient Meta-Aware Training with SFT on Expert Trajectories. Require: Task distribution Q, expert dataset buffer Dexpert, initial policy parameters θ, efficient start step Ensure: Optimized policy parameters θ θold θ for step = 1, . . . , do Sample task prompt Sample meta-trajectories {ometa if step > then }M i=1 πθold ( qmeta) Efficient sampling with predictive gating, early cutoff, and notion hinting or minibatch else Sample reasoning trajectories {osol }G i=1 πθold ( qsol) end if θ θ αθLRL(θ) Extract expert trajectory {oexpert} from {osol Dexpert Dexpert {oexpert} if Dexpert Nexpert then }G i=1 and {ometa }M i=1 Equation (5) θ θ βθLBC(θ, Dexpert) Dexpert end if θold θ end for 3.3 MASA-efficient: META-BASED ACTIVE CONTROL FOR EFFICIENT POST-TRAINING MASA-efficient is variant of MASA that can further boost training efficiency by leveraging the length and difficulty predictions from meta-predictions. From the observation that early step metapredictions are unstable, we encourage the behavior cloning of the policy model on the ideal metaprediction trajectories that are gathered throughout each RL step, inspired by behavior cloning (BC) (Mendonca et al., 2019; Silver et al., 2017; Schick et al., 2023). We denote these ideal metapredictions as expert dataset, Dexpert, which are meta-predictions that scored high notion score and the predictions on pass-rate and length are substituted by the true statistics gathered from the solution rollouts. Once the expert dataset size reaches Nexpert, we minimize cross-entropy loss on Dexpert on the current policy model as θ LBC (θ min α θLRL(θ), Dexpert) , (5) Dexpert) = EoDexpert where α is the learning rate for RL training. Formally, the behavior cloning loss is defined as LBC(θ, Note that we gather samples from the current policy model prediction and accumulate up to batch size of Nexpert, as outdated trajectories do not reflect the current policy model behavior, and the outdated expert meta trajectories are evicted from t=1 log πθ (ot (cid:105) o<t) (cid:104) Dexpert following DAgger (Ross et al., 2011). (cid:80)o . Non-Parallel Efficient Training with Gating and Cutoff As MASA-efficient is the efficient variant of MASA. To encourage meta-awareness before accelerating the training phase, we first perform self-alignment based policy updates for the early steps following MASA pipeline2, until the policy model shows stable meta-prediction alignment with the true solution rollouts. From this point, we alter into non-parallel pipeline that executes meta-predictions first, for predictive gating, followed by solution rollouts, applying early length cutoff. We also utilize the predicted notions to provide additional hint for the model in solving the questions as illustrated in Figure 2(b). Predictive gating filters out zero-variance tasks, that exceeds or under-reaches the models current capacity. Unlike DAPO that performs pruning after doing lengthy and inefficient solution rollouts, our method saves computation by using short meta-predictions as gate on whether to rollout the lengthy solution beforehand3. In detail, the predictive gating is activated only if the standard deviation over predicted pass-rates is below 0.1 to ensure confident meta-prediction. The length of the prediction is used as early cutoff threshold to stop the rollout that exceeds more than 2 2The selection of training step is explained in Figure 3. 3The length difference between the meta and solution rollouts is analyzed in Table 3."
        },
        {
            "title": "Preprint",
            "content": "Table 1: Performance of GRPO and MASA across In-domain Math benchmarks. Benchmark GRPO GRPO w/ MASA Pass@1 Pass@ Pass@1 Pass@32 AIME24 AIME25 AMC23 MATH500 Minerva Olympiad Average AIME24 AIME25 AMC23 MATH500 Minerva Olympiad Average 38.54 27.91 81.56 88.61 44.84 58.65 56.69 28.54 22.18 73.67 85.75 42.46 53.61 51. Qwen3-14B Base Model 70.00 56.67 97.50 97.60 71.32 77.74 78.47 40.10 (+ 4.04%) 29.90 (+ 7.13%) 84.61 (+ 3.74%) 88.54 (- 0.08%) 45.37 (+ 1.18%) 59.94 (+ 2.20%) 58.08 (+ 2.45%) Qwen3-8B Base Model 66.67 46.67 97.50 96.80 69.85 76.11 75.60 33.75 (+ 18.26%) 26.46 (+ 19.30%) 76.88 (+ 4.36%) 87.36 (+ 1.88%) 45.35 (+ 6.81%) 55.41 (+ 3.36%) 54.20 (+ 6.20%) 73.33 (+ 4.76%) 56.67 ( - ) 97.50 ( - ) 97.80 (+ 0.20%) 74.63 (+ 4.64%) 77.15 (- 0.76%) 79.51 (+ 1.33%) 70.00 (+ 5.00%) 50.00 (+ 7.14%) 100.00 (+ 2.56%) 96.80 ( - ) 72.06 (+ 3.16%) 78.48 (+ 3.11%) 77.89 (+ 3.03%) predicted length, as such lengths are highly likely to lead to incorrect rollout due to notion reward design. The precision and F1 score of predictive gating and early length cutoff in predicting the true zero-variance and incorrect rollouts are analyzed in Figure 3."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "Implementation Details. We use VeRL with the DeepScalerR (Luo et al., 2025) dataset, batch size 128, learning rate 1e-6, 10% weight decay, maximum response length 8K, and GRPO without KL. Training runs for one epoch (314 steps) using AdamW (Loshchilov & Hutter) with 20 warmup steps, gradient clipping 1.0, and clipping range [ϵlow = 0.2, ϵhigh = 0.28]. The rollouts use temperature 1.0 and top-p value of 1.0. Both actual (G) and meta-prediction (M ) rollouts are 16. Expert SFT uses 5 gradient updates per outer RL loop. The difficulty-reward base is = 0.01, and gating/cutoff begins at = 120 and the batch size for expert dataset is also set as 128. Evaluation Configuration. We use the provided math scoring function in VeRL to measure the accuracy of the predicted answer and ground truth answer sampling 32 responses, 16k maximum response length and temperature 0.6. Baselines. The baseline of our method is GRPO and DAPO. Throughout the experiment section, MASA refers to the model that is trained with our Meta-Awareness via Self-Alignment. MASAefficient indicates the version of model that includes the gating & cutoff applied from MASA at step 120. 4.1 OBSERVATIONS We analyze the performance of MASA through validation on mathematical benchmarks and generalized reasoning benchmarks. MASA Excels in In-Domain Mathematical Benchmarks MASA excels the baseline in six math benchmarks, AIME24, AIME25, AMC23, MATH500 (Hendrycks et al.), Minerva, and OlympiadBench (He et al., 2024)  (Table 1)  . Across all mathematical datasets, our method MASA shows great improvement over the baseline GRPO performance, showing an average of 6.2% improvement in Qwen3-8B model, and an average of 2.45% in 14B model."
        },
        {
            "title": "Preprint",
            "content": "Table 2: Performance of GRPO and MASA in Out-of-Domain benchmarks. Results are reported as pass@1 score. Logical Reasoning Scientific Reasoning Coding Benchmark ProntoQA ProofWriter FOLIO Logi. Deduct AR-LSAT Avg. GRPO w/ MASA Benchmark 90.56 72.27 69.16 80.81 37.00 69. GPQA Diamond R-Bench ARC-Challenge SciBench 93.74 73.23 69.24 81.03 38.00 71.05 Avg. GRPO w/ MASA Benchmark 51.72 60.69 93.10 28.33 EvalPlus CRUX-O MBPP LiveCodeBench 53.72 61.68 93.13 29. GRPO w/ MASA 77.32 72.72 71.84 31.49 77.66 73.39 72.97 31.61 58.46 59.54 Avg. 63. 63.91 MASA Generalizes to Out-of-Domain Reasoning Benchmarks The meta-awareness also benefits generalization ability of the reasoning model in out-of-domain logical & scientific & coding benchmarks as shown in Table 2. For logical reasoning domain, we follow the setup of (Pan et al., 2023) and test on ProntoQA (Saparov & He), ProofWriter (Tafjord et al., 2021), FOLIO (Han et al., 2024a), LogicalDeduction (Srivastava et al.), and AR-LSAT (Zhong et al., 2022). For scientific reasoning, we use GPQA Diamond (Rein et al., 2024), R-Bench (Guo et al., 2025b), ARC-Challenge (Clark et al., 2018), and SciBench (Wang et al., 2024). For coding, we evaluate on EvalPlus (Liu et al., 2023), CRUX-O (Gu et al., 2024), MBPP (Austin et al., 2021), and LiveCodeBench (Jain et al., 2025). Although MASA is not explicitly trained for generalization, strengthening meta-awareness consistently enhances out-of-domain performance. 4.2 ANALYSIS ON COMPONENT Implicit Meta-Awareness Reward Explicitly Changes the Model Output. How does the parallel rollout of meta-predictions influence the solution rollouts? We classify notions that appear more often in correct responses as positive notions and those that appear more often in incorrect responses as negative notions. After reward-based gradient updates, positive notions should become more common in correct solution rollouts, whereas negative notions should be suppressed. As shown in Figure 3a, positive notions from earlier steps consistently increase in correct rollouts (notion score > 0 indicates higher frequency in correct compared to incorrect), whereas negative notions are reduced in correct rollouts but amplified in incorrect ones (notion score < 0). Expert Trajectories Increases Meta-Awareness in Early Train Steps. Predictive gating aims to identify zero-variance prompts before rollout, while early cutoff predicts rollouts that will yield incorrect answer despite excessive token length. Adding expert trajectory supervised finetuning to MASA improves the precision of both mechanisms, as shown in Figure 3b and Figure 3c. Without expert SFT, MASA (green) shows unstable precision that drops sharply around step 80 and score F1 score of 0.411 and 0.732 in predictive gating and early cutoff, respectively. In contrast, with expert trajectories stabilizes the improvement, yielding final F1 score of 0.485 and 0.836 at training step 120. Based on this analysis, we begin to apply gating and cutoff only after step 120, once the predictions are stable in terms of both precision and F1 score. MASA-efficient reaches higher performance faster with faster train time. Table 3 shows the effectiveness of MASA-efficient in reducing the train time compared to MASA. The train time drastically reduces by 34.5%, while closely retaining the performance of MASA in intermediate level of math reasoning tasks such as AMC23 and MATH500. On the other hand, MASA-efficient shows at most 3.9% of performance drop in AIME, which consists of Olympiad level math problems, proving the need for less efficient but stronger MASA for complex reasoning tasks. We observe efficiency in terms of number of training tasks, number of generated tokens, and train time in Figure 4. MASA-efficient reaches the performance of the baseline model GRPO with notably smaller number of tasks, total generated tokens, and train time. As shown in the figure, the accuracy consistently outperforms the baseline under same budget condition, proving that MASA-efficient is highly effective in reducing the train time and compute resource. It is important to note that though our method MASA requires doubled rollouts for solution and meta-prediction paths, the average"
        },
        {
            "title": "Preprint",
            "content": "(a) Notion Dynamics (b) Predictive Gating (c) Early Cutoff Figure 3: (a) Notion score of positive / negative notions from earlier train step. (b) Precision Score of Predictive Gating on true zero variance prompts. (c) Precision Score of Early Cutoff on true incorrect roll-outs. Precisions are smoothed by moving average over 5 steps. Table 3: Analysis on MASA-efficient performance and average token length of two trajectories with MASA. MASA MASA-efficient Perf. Gap AIME25 AIME24 AMC23 MATH500 Avg Train Time (hrs) 33.75 26.46 76.88 87.36 56.11 52.50 32.71 25.42 76.88 87.68 55.67 34.93 -3.1% -3.9% - +0.4% -0.7% -34.5% (a) Performance and efficiency comparison. Solution Path Meta-Pred Path Token Length 6251 (b) Average token length. 2293 meta length of 2293 is 2.73 times smaller than the average solution path of average length 6251. By adding predictive gating and length cutoff, the total train time becomes much shorter since the gating happens before the lengthy solution path. Figure 5 shows the average proportion of prompts filtered by gating. On average, about 37% of prompts are removed before the model begins its full solution rollout, with the gating rate typically staying between 2040%. Early in the process, up to 80% of prompts remain after gating, but this quickly drops to stable, lower level. Although the baseline and MASA process the same number of tasks up to step 120, only 56% of tasks remain after filtering when using MASA-efficient after step 120 until step 314, compared to GRPO. Finally, note that we cannot measure the exact amount of rollout length saved by early cutoffs, since truncated rollouts do not reach an EOS token and thus their full length is unknown. Figure 5: Analysis on Gating. (a) Seen Tasks and Acc. (b) Generated Tokens and Acc. (c) Train time and Acc. Figure 4: Comparison of MASA-efficient and GRPO on same train budgets: number of seen train tasks, total generation tokens, and train time. Accuracy is calculated as the average of AIME24, AIME25, and AMC23. All accuracy curves are smoothed with 3-window moving average."
        },
        {
            "title": "4.3 ABLATION STUDIES",
            "content": "Ablation on RL Algorithm We test the applicability of our method MASA on DAPO in Table 4, which is variant of GRPO that introduces several technical changes in the optimization process. DAPO uses dynamic sampling to filter out tasks that yield zero-variance prompts to stabilize the gradient update and assigns penalty on overlong responses. We observed that applying the overlong penalty adversely affected accuracy under the 8k maximum response length setting. Accordingly, we adopted DAPO without the overlong penalty as the baseline. For DAPO, we conducted training for one epoch, consistent with the GRPO setup, and report the performance of the final model. Combined with DAPO, our method MASA outperforms all six mathematical benchmarks, reaching 18.61% of gain on Pass@1 in AIME24. Table 4: Performance comparison of MASA with DAPO, trained with Qwen3-8B base model. DAPO DAPO + MASA Benchmark AIME24 AIME25 AMC23 MATH500 Minerva Olympiad Average Pass@1 23.54 18.75 67.11 81.67 35.53 49.30 45. Pass@32 63.33 46.67 97.50 96.80 68.01 75.07 74.56 Pass@1 27.92 (+ 18.61%) 20.63 (+ 10.03%) 69.22 (+ 3.14%) 82.99 (+ 1.62%) 39.66 (+ 11.62%) 50.93 (+ 3.31%) 48.56 (+ 5.61%) Pass@32 70.00 (+ 10.53%) 60.00 (+ 28.56%) 97.50(-) 96.20 (- 0.62%) 71.69 (+ 5.41%) 76.71 (+ 2.18%) 78.68 (+ 5.53%) Meta-Component Contribution. Here we analyze which component among the three metapredictions contribute the most to the performance increase. The contribution of length, difficulty, and notion prediction for meta-awareness is shown in In specific, we analyze the Shapley R2 Figure 6. share4 of each feature - the three meta components (notion-aware, difficulty-aware, length-aware), and the train step - on the contribution to the increase in model performance. The results show that notionawareness is by far the most dominant factor, explaining over two-thirds of the variance in performance increase. Difficulty-awareness and lengthawareness plays smaller role while the effect of training step is almost negligible."
        },
        {
            "title": "5 CONCLUSION",
            "content": "Figure 6: Analysis on Meta-Components. We present MASA, meta-aware reinforcement learning framework that fosters meta-cognitive ability by self-alignment. By incorporating expert meta-thinking trajectories into training, our method enables stable and efficient optimization by integrating predictive gating and early cutoff. Empirically, MASA accelerates RL-based post-training while improving both in-domain and out-ofdomain performance, demonstrating notable gains in accuracy and generalization. These results highlight the promise of meta prediction as principled avenue for enhancing reasoning models."
        },
        {
            "title": "LIMITATION",
            "content": "While our approach to meta prediction can, in principle, be extended to broader range of metathinking strategies, in this work we focus on length, difficulty, and notion. The gating and cutoff hyper-parameters are set offline based on the analysis, but it would be beneficial to search hyperparameters online during train time. 4Calculated by LMG (LindemanMerendaGold) method."
        },
        {
            "title": "REFERENCES",
            "content": "Pranjal Aggarwal and Sean Welleck. L1: Controlling how long reasoning model thinks with reinforcement learning. 2025. doi: 10.48550/arXiv.2503.04697. URL https://arxiv.org/ abs/2503.04697. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Ahsan Bilal, Muhammad Ahmed Mohsin, Muhammad Umer, Muhammad Awais Khan Bangash, and Muhammad Ali Jamshed. Meta-thinking in llms via multi-agent reinforcement learning: survey. arXiv preprint arXiv:2504.14520, 2025. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Qiguang Chen, Dengyun Peng, Jinhao Liu, HuiKang Su, Jiannan Guan, Libo Qin, and Wanxiang Che. Aware first, think less: Dynamic boundary self-awareness drives extreme reasoning efficiency in large language models. arXiv preprint arXiv:2508.11582, 2025a. Yang Chen, Zhuolin Yang, Zihan Liu, Chankyu Lee, Peng Xu, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Acereason-nemotron: Advancing math and code reasoning through reinforcement learning. 2025b. doi: 10.48550/arXiv.2505.16400. URL https://arxiv.org/ abs/2505.16400. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. URL https://arxiv.org/abs/1803.05457. Use the ARC-Challenge split for ARC-C results. Muzhi Dai, Chenxu Yang, and Qingyi Si. S-grpo: Early exit via reinforcement learning in reasoning models. arXiv preprint arXiv:2505.07686, 2025. Nicol`o De Sabbata, Theodore Sumers, Badr AlKhamissi, Antoine Bosselut, and Thomas Griffiths. Rational metareasoning for large language models. arXiv preprint arXiv:2410.05563, 2024. Xinhan Di and JoyJiaoW. Enhancing math reasoning in small-sized llms via preview difficultyaware intervention. 2025. doi: 10.48550/arXiv.2508.01604. URL https://arxiv.org/ abs/2508.01604. Aniket Didolkar, Nicolas Balla, Sanjeev Arora, and Anirudh Goyal. Metacognitive reuse: Turning recurring llm reasoning into concise behaviors. arXiv preprint arXiv:2509.13237, 2025. Haonan Dong, Haoran Ye, Wenhao Zhu, Kehan Jiang, and Guojie Song. Meta-r1: Empowering large reasoning models with metacognition. arXiv preprint arXiv:2508.17291, 2025. Gongfan Fang, Xinyin Ma, and Xinchao Wang. Thinkless: Llm learns when to think. arXiv preprint arXiv:2505.13379, 2025. Alex Gu, Baptiste Roziere, Hugh James Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida Wang. CRUXEval: benchmark for code reasoning, understanding and execution. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 1656816621. PMLR, 2127 Jul 2024. URL https://proceedings.mlr.press/v235/gu24c.html. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025a."
        },
        {
            "title": "Preprint",
            "content": "Meng-Hao Guo, Jiajun Xu, Yi Zhang, Jiaxi Song, Haoyang Peng, Yi-Xuan Deng, Xinzhi Dong, Kiyohiro Nakayama, Zhengyang Geng, Chen Wang, Bolin Ni, Guo-Wei Yang, Yongming Rao, Houwen Peng, Han Hu, Gordon Wetzstein, and Shi-min Hu. R-bench: Graduate-level multi-disciplinary benchmarks for llm & mllm complex reasoning evaluation. arXiv preprint arXiv:2505.02018, 2025b. URL https://arxiv.org/abs/2505.02018. Rui Ha, Chaozhuo Li, Rui Pu, and Sen Su. From aha moments to controllable thinking: Toward meta-cognitive reasoning in large reasoning models via decoupled reasoning and control. 2025. doi: 10.48550/arXiv.2508.04460. URL https://arxiv.org/abs/2508.04460. Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Wenfei Zhou, James Coady, David Peng, Yujie Qiao, Luke Benson, et al. Folio: Natural language reasoning with firstorder logic. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 2201722031, 2024a. Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyu Zhao, Shiqing Ma, and Zhenyu Chen. Tokenbudget-aware llm reasoning. arXiv preprint arXiv:2412.18547, 2024b. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting In Proceedings of the 62nd agi with olympiad-level bilingual multimodal scientific problems. Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 38283850, 2024. Tao He, Rongchuan Mu, Lizi Liao, Yixin Cao, Ming Liu, and Bing Qin. Good learners think their thinking: Generative prm makes large reasoning model more efficient math learner. arXiv preprint arXiv:2507.23317, 2025. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). Shijue Huang, Hongru Wang, Wanjun Zhong, Zhaochen Su, Jiazhan Feng, Bowen Cao, and Yi R. Fung. Adactrl: Towards adaptive and controllable reasoning via difficulty-aware budgeting. 2025. doi: 10.48550/arXiv.2505.18822. URL https://arxiv.org/abs/2505.18822. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id= chfJJYC3iL. Yunjie Ji, Sitong Zhao, Xiaoyu Tian, Haotian Wang, Shuaiting Chen, Yiping Peng, Han Zhao, and Xiangang Li. How difficulty-aware staged reinforcement learning enhances llms reasoning capabilities: preliminary experimental study. 2025. doi: 10.48550/arXiv.2504.00829. URL https://arxiv.org/abs/2504.00829. Vedant Khandelwal, Francesca Rossi, Keerthiram Murugesan, Erik Miehling, Murray Campbell, Karthikeyan Natesan Ramamurthy, and Lior Horesh. Language models coupled with metacognition can outperform reasoning models. arXiv preprint arXiv:2508.17959, 2025. Gengxu Li, Tingyu Xia, Yi Chang, and Yuan Wu. Length-controlled margin-based preference optimization without reference model. 2025. doi: 10.48550/arXiv.2502.14643. URL https: //arxiv.org/abs/2502.14643. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36:2155821572, 2023. Tennison Liu and Mihaela van der Schaar. Position: Truly self-improving agents require intrinsic metacognitive learning. In Forty-second International Conference on Machine Learning Position Paper Track, 2025. URL https://openreview.net/forum?id=4KhDd0Ozqe."
        },
        {
            "title": "Preprint",
            "content": "Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. 2025a. doi: 10.48550/arXiv. 2503.20783. URL https://arxiv.org/abs/2503.20783. Ziru Liu, Cheng Gong, Xinyu Fu, Yaofang Liu, Ran Chen, Shoubo Hu, Suiyun Zhang, Rui Liu, Qingfu Zhang, and Dandan Tu. Ghpo: Adaptive guidance for stable and efficient llm reinforcement learning. arXiv preprint arXiv:2507.10628, 2025b. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations. Jinghui Lu, Haiyang Yu, Siliang Xu, Shiwei Ran, Guozhi Tang, Siqi Wang, Bin Shan, Teng Fu, Hao Feng, Jingqun Tang, et al. Prolonged reasoning is not all you need: Certainty-based adaptive routing for efficient llm/mllm reasoning. arXiv preprint arXiv:2505.15154, 2025. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl, 2025. Notion Blog. Ziyang Ma, Qingyue Yuan, Zhenglin Wang, and Deyu Zhou. Large language models have intrinsic meta-cognition, but need good lens. arXiv preprint arXiv:2506.08410, 2025. Russell Mendonca, Abhishek Gupta, Rosen Kralev, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Guided meta-policy search. Advances in Neural Information Processing Systems, 32, 2019. Liangming Pan, Alon Albalak, Xinyi Wang, and William Wang. Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 38063824, 2023. Ziqing Qiao, Yongheng Deng, Jiali Zeng, Dong Wang, Lai Wei, Fandong Meng, Jie Zhou, Ju Ren, and Yaoxue Zhang. Concise: Confidence-guided compression in step-by-step efficient reasoning. arXiv preprint arXiv:2505.04881, 2025. Zishang Qiu, Xinan Chen, Long Chen, and Ruibin Bai. Mela: metacognitive llm-driven architecture for automatic heuristic design. arXiv preprint arXiv:2507.20541, 2025. Yuxiao Qu, Matthew YR Yang, Amrith Setlur, Lewis Tunstall, Edward Emanuel Beeching, Ruslan Salakhutdinov, and Aviral Kumar. Optimizing test-time compute via meta reinforcement finetuning. arXiv preprint arXiv:2503.07572, 2025. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. URL https://openreview. net/forum?id=Ti67584b98. Stephane Ross, Geoffrey Gordon, and Drew Bagnell. reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 627635. JMLR Workshop and Conference Proceedings, 2011. Abulhair Saparov and He He. Language models are greedy reasoners: systematic formal analysis of chain-of-thought. In The Eleventh International Conference on Learning Representations. Timo Schick, Jane Dwivedi-Yu, Roberto Dess`ı, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:68539 68551, 2023. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. 2024. doi: 10.48550/arXiv.2402.03300. URL https://arxiv.org/abs/2402.03300."
        },
        {
            "title": "Preprint",
            "content": "Yi Shen, Jian Zhang, Jieyun Huang, Shuming Shi, Wenjing Zhang, Jiangze Yan, Ning Wang, Kai Wang, Zhaoxiang Liu, and Shiguo Lian. Dast: Difficulty-adaptive slow-thinking for large reasoning models. arXiv preprint arXiv:2503.04472, 2025. Taiwei Shi, Yiyang Wu, Linxin Song, Tianyi Zhou, and Jieyu Zhao. Efficient reinforcement finetuning via adaptive curriculum learning. arXiv preprint arXiv:2504.05520, 2025. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi by self-play with general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815, 2017. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam Brown, Adam Santoro, Aditya Gupta, Adri`a Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research. Yuan Sui, Yufei He, Tri Cao, Simeng Han, Yulin Chen, and Bryan Hooi. Meta-reasoner: Dynamic guidance for optimized inference-time reasoning in large language models. arXiv preprint arXiv:2502.19918, 2025. Oyvind Tafjord, Bhavana Dalvi, and Peter Clark. Proofwriter: Generating implications, proofs, and abductive statements over natural language. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 36213634, 2021. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Songjun Tu, Jiahao Lin, Qichao Zhang, Xiangyu Tian, Linjing Li, Xiangyuan Lan, and Dongbin Zhao. Learning when to think: Shaping adaptive reasoning in r1-style models via multi-stage rl. arXiv preprint arXiv:2505.10832, 2025. Ziyu Wan, Yunxiang Li, Xiaoyu Wen, Yan Song, Hanjing Wang, Linyi Yang, Mark Schmidt, Jun Wang, Weinan Zhang, Shuyue Hu, et al. Rema: Learning to meta-think for llms with multi-agent reinforcement learning. arXiv preprint arXiv:2503.09501, 2025. Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. Scibench: Evaluating college-level scientific problem-solving abilities of large language models. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=bq1JEgioLr. Yunhao Wang, Yuhao Zhang, Tinghao Yu, Can Xu, Feng Zhang, and Fengzong Lian. Adaptive deep reasoning: Triggering deep thinking when needed. arXiv preprint arXiv:2505.20101, 2025. Violet Xiang, Chase Blagden, Rafael Rafailov, Nathan Lile, Sang Truong, Chelsea Finn, and Nick Haber. Just enough thinking: Efficient reasoning with adaptive length penalties reinforcement learning. 2025. doi: 10.48550/arXiv.2506.05256. URL https://arxiv.org/abs/2506. 05256. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025a. Chenxu Yang, Qingyi Si, Yongjie Duan, Zheliang Zhu, Chenyu Zhu, Qiaowei Li, Zheng Lin, Li Cao, and Weiping Wang. Dynamic early exit in reasoning models. arXiv preprint arXiv:2504.15895, 2025b. Junjie Yang, Ke Lin, and Xing Yu. Think when you need: Self-adaptive chain-of-thought learning. arXiv preprint arXiv:2504.03234, 2025c. Wei Yang and Jesse Thomason. Learning to deliberate: Meta-policy collaboration for agentic llms with multi-agent reinforcement learning. arXiv preprint arXiv:2509.03817, 2025."
        },
        {
            "title": "Preprint",
            "content": "Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Jiajie Zhang, Nianyi Lin, Lei Hou, Ling Feng, and Juanzi Li. Adaptthink: Reasoning models can learn when to think. arXiv preprint arXiv:2505.13417, 2025a. Jixiao Zhang and Chunsheng Zuo. Grpo-lead: difficulty-aware reinforcement learning approach for concise mathematical reasoning in language models. 2025. doi: 10.48550/arXiv.2504.09696. URL https://arxiv.org/abs/2504.09696. Xingjian Zhang, Siwei Wen, Wenjun Wu, and Lei Huang. Edge-grpo: Entropy-driven grpo with guided error correction for advantage diversity. arXiv preprint arXiv:2507.21848, 2025b. Zijing Zhang, Ziyang Chen, Mingxiao Li, Zhaopeng Tu, and Xiaolong Li. Rlvmr: Reinforcement learning with verifiable meta-reasoning rewards for robust long-horizon agents. 2025c. doi: 10.48550/arXiv.2507.22844. URL https://arxiv.org/abs/2507.22844. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. Wanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu, Daya Guo, Yining Chen, Jiahai Wang, Jian Yin, Ming Zhou, and Nan Duan. Analytical reasoning of text. In Marine Carpuat, MarieCatherine de Marneffe, and Ivan Vladimir Meza Ruiz (eds.), Findings of the Association for Computational Linguistics: NAACL 2022, pp. 23062319, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-naacl.177. URL https://aclanthology.org/2022.findings-naacl.177/."
        },
        {
            "title": "Preprint",
            "content": "A DEFAULT META-PREDICTION PROMPT FOR MASA Prompt [System]: You are helpful assistant. [User]: Think step-by-step between <meta> and </meta>, ensuring comprehensive and detailed reasoning especially for determining the pass rate and solution length values. For each component (math notion, pass rate, solution length), provide comprehensive illustration or example during your reasoning in the <meta> section to clarify how each value is decided. When determining math notion, ensure that the notions listed do not directly include the notions already written in the problem statement. After </meta>, return JSON object with three keys: - math notion (list[str]) - pass rate (integer from 0 to 8) - solution length (integer from 128 to max response length ) { } Problem: problem } { EFFECT OF NOTION FEED-IN FOR MASA INFERENCE During training of MASA-efficient, we provided hints to the model through meta-predictions, whereas evaluation was conducted using only the actual rollout. Nevertheless, it is also possible to incorporate the notions predicted by the meta-prediction rollout into the prompt during inference, mirroring the training procedure. To examine the impact of such notion feed-in on performance, we performed the following experiment. We extended the training pipeline of MASA with Expert SFT by appending the notions predicted by meta-prediction to the original prompt as additional context. We then compared the final models performance with and without notion feed-in at inference time. The results are presented in table 5. As shown in the table, although the improvements are modest, incorporating notion feed-in consistently yields slightly higher Pass@1 scores on most benchmarks. This finding suggests that the predicted notions can serve as useful cues for problem solving and may enable further performance gains when leveraged during inference. Table 5: Performance of GRPO and MASA on Qwen3-8B across In-domain Math benchmarks. All metrics are Pass@1. NF denotes Notion-FeedIn. Benchmark GRPO Pass@ GRPO w/ MASA Pass@1 MASA + Expert (No NF) Pass@1 MASA + Expert (NF) Pass@1 Qwen3-8B Base Model AIME24 AIME25 AMC23 MATH500 Minerva Olympiad Average 28.54 22.18 73.67 85.75 42.46 53.61 51. 33.75 26.46 76.88 87.36 45.35 55.41 54.20 32.92 26.04 76.64 87.65 46.43 55.07 54.13 33.85 26.46 78.98 87.72 45.86 55.59 54."
        },
        {
            "title": "Preprint",
            "content": "C META-PREDICTION DYNAMICS DURING MASA TRAINING (a) Actual and meta-predicted accuracy over global training steps. (b) Actual and meta-predicted output global training steps. length over Figure 7: Actual vs. meta-predicted statistics across training To analyze the training dynamics of meta-prediction, we tracked how the models meta-predictions and actual performance changed over the course of training. As shown in fig. 7, the meta-predictions initially differed greatly from the actual values, but the gap gradually narrowed as training progressed. We also observed an interesting pattern in accuracy meta-prediction. Early in training, the model tended to predict excessively high pass rates for most problems, which created large discrepancy with the true accuracy, as shown in fig. 7a. This mismatch resulted in low rewards and sharp drop in the predicted values. Around step 80, the model began to distinguish between easy and hard problems, and MASAs performance improved improved rapidly. As we can see in fig. 7b, similar trend appeared in the difficulty metric. At first, the model failed to accurately estimate the token length of correct solutions, but after about step 80 it began to match the actual lengths more closely. Notably, this timing coincided with the point at which MASA began to outperform the baseline, supporting our hypothesis that meta-awareness contributes to performance gains."
        }
    ],
    "affiliations": [
        "AITRICS",
        "KAIST"
    ]
}