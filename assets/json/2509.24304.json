{
    "paper_title": "FrameThinker: Learning to Think with Long Videos via Multi-Turn Frame Spotlighting",
    "authors": [
        "Zefeng He",
        "Xiaoye Qu",
        "Yafu Li",
        "Siyuan Huang",
        "Daizong Liu",
        "Yu Cheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While Large Vision-Language Models (LVLMs) have achieved substantial progress in video understanding, their application to long video reasoning is hindered by uniform frame sampling and static textual reasoning, which are inefficient and struggle to handle visually intensive video tasks. To overcome these challenges, in this paper, we introduce the concept of thinking with long videos and propose a novel framework FrameThinker. Within this framework, LVLMs are able to iteratively interrogate video content. Developing such video reasoning capabilities in LVLMs presents notable challenges, particularly in adapting the model to new video actions (e.g. select frame), and designing reward functions to guide LVLMs to adopt the newly introduced action. To solve these challenges, we propose a two-phase training strategy, first employing Supervised Fine-Tuning (SFT) to instill fundamental action capabilities, followed by Reinforcement Learning (RL) to optimize a strategic decision-making policy. Notably, in this RL phase, we conduct an in-depth and comprehensive exploration of the reward design for each action and format reward. Extensive experiments on reasoning benchmarks like Video-Holmes, LongVideo-Reason, and long-video understanding benchmarks such as LongVideoBench, MLVU, VideoMME, and LVBench, demonstrate that FrameThinker achieves a significant average improvement of +10.4% over baselines while drastically reducing the number of processed frames. Most notably, our 7B model, FrameThinker establishes a new state-of-the-art on LongVideo-Reason, achieving 76.1% accuracy using an average of only 20.6 frames. This not only outperforms the competitive LongVILA-R1 (72.0%) but does so with over 20x fewer frames (vs. 512), demonstrating unparalleled efficiency and effectiveness."
        },
        {
            "title": "Start",
            "content": "FRAMETHINKER: LEARNING TO THINK WITH LONG VIDEOS VIA MULTI-TURN FRAME SPOTLIGHTING Zefeng He12 Xiaoye Qu1 Yafu Li3 1Shanghai AI Laboratory 4Shanghai Jiao Tong University 2Nanjing University 5Peking University Siyuan Huang4 Daizong Liu5 3The Chinese University of Hong Kong Yu Cheng3 5 2 0 2 0 3 ] . [ 2 4 0 3 4 2 . 9 0 5 2 : r Project Page: https://github.com/lcqysl/FrameThinker-RL"
        },
        {
            "title": "ABSTRACT",
            "content": "While Large Vision-Language Models (LVLMs) have achieved substantial progress in video understanding, their application to long video reasoning is hindered by uniform frame sampling and static textual reasoning, which are inefficient and struggle to handle visually intensive video tasks. To overcome these challenges, in this paper, we introduce the concept of thinking with long videos and propose novel framework FrameThinker. Within this framework, LVLMs are able to iteratively interrogate video content. Developing such video reasoning capabilities in LVLMs presents notable challenges, particularly in adapting the model to new video actions (e.g. select frame), and designing reward functions to guide LVLMs to adopt the newly introduced action. To solve these challenges, we propose two-phase training strategy, first employing Supervised Fine-Tuning (SFT) to instill fundamental action capabilities, followed by Reinforcement Learning (RL) to optimize strategic decision-making policy. Notably, in this RL phase, we conduct an in-depth and comprehensive exploration of the reward design for each action and format reward. Extensive experiments on reasoning benchmarks like Video-Holmes, LongVideo-Reason, and long-video understanding benchmarks such as LongVideoBench, MLVU, VideoMME, and LVBench, demonstrate that FrameThinker achieves significant average improvement of +10.4% over baselines while drastically reducing the number of processed frames. Most notably, our 7B model, FrameThinker establishes new state-of-theart on LongVideo-Reason, achieving 76.1% accuracy using an average of only 20.6 frames. This not only outperforms the competitive LongVILA-R1 (72.0%) but does so with over 20x fewer frames (vs. 512), demonstrating unparalleled efficiency and effectiveness."
        },
        {
            "title": "INTRODUCTION",
            "content": "In recent years, the remarkable advancements of Large Vision-Language Models (LVLMs) have been extended into the challenging domain of video understanding. State-of-the-art closed-source models such as Gemini-2.5 (Comanici et al., 2025), GPT-4o (OpenAI, 2024) and GPT-5 (OpenAI, 2025a), alongside powerful open-source counterparts like Qwen2.5-VL (Bai et al., 2025), have established new performance benchmarks on suite of video understanding benchmarks, such as Video-MME (Fu et al., 2025a) and LVBench (Wang et al., 2024a). However, fundamental limitation still persists in the operational paradigm of existing methods (Bai et al., 2025; Chen et al., 2024c; 2025b). This limitation lies in their reliance on processing large, pre-selected set of frames obtained through uniform sampling, as illustrated in the top panel of Figure 1. This strategy suffers from inefficiency, especially for long video reasoning, as it processes large number of irrelevant frames. Meanwhile, such long and noisy context, cluttered with irrelevant frames, degrades the reasoning performance (Qu et al., 2025a). Furthermore, these methods conduct their reasoning process exclusively through text tokens, which restricts their multimodal perceptual abilities after the initial input. Concurrently, while Video Agents (Fan et al., 2024; Wang et al., 2024b; 2025; Zhang et al., 2025b) can interact with videos using tools, they often depend on Work was done during Zefeng Hes internship at Shanghai AI Laboratory. Corresponding authors."
        },
        {
            "title": "Preprint",
            "content": "predefined workflows or external models, which limits their autonomy and flexibility in dynamically exploring video content based on reasoning needs. Furthermore, most of these methods are not end-to-end learnable from data, which restricts their potential for improvement. To address these limitations, we introduce FrameThinker, novel framework that empowers the model to perform an active and iterative analysis of the video content. As clearly illustrated in the bottom panel of Figure 1, our model first conducts sparse scan (Turn 1) to establish broad and general understanding. Guided by this initial assessment, it then identifies promising temporal segments and executes targeted zoom-in by selecting specific sequence of frames (Turn 2) to retrieve more fine-grained and detailed visual information. This iterative refinement process can be repeatedly applied as needed, fully guided by the models own reasoning, thereby allowing the model to dynamically gather the necessary evidence while processing only fraction of the total frames. In this way, our FrameThinker constructs multimodal chain of thought that interleaves textual reasoning with visual frames, ultimately empowering the model to think with long videos. Figure 1: (Top) Traditional Uniform Sparse Sampling is inefficient and may miss the key frame in long videos. (Bottom) Our method starts with sparse scan for an overview (Turn 1), then dynamically chooses frames on promising segments (Turn 2), enabling multi-turn analysis to efficiently focus on key frames in long video. Our training methodology facilitates this capability in two stages: we first employ Supervised FineTuning (SFT) to acquaint the model with the core mechanics of action execution (e.g. select frame), followed by Reinforcement Learning (RL) to cultivate an optimal policy for strategic action. Furthermore, we conduct an extensive exploration of the reward function design space for multi-turn video analysis. Among our key findings, we identify that unconditional action rewards are highly prone to mode collapse. While shifting to simple conditional reward structure mitigates this issue, it does not prevent the model from learning to output illogical actions. To address this deeper challenge, we further propose Cognitive Consistency Verification (CCV) module to suppress illogical executions and ensure the rationality and interpretability of the models actions. To validate the effectiveness of FrameThinker, we evaluate it on complex reasoning benchmarks like Video-Holmes (Cheng et al., 2025) and LongVideo-Reason (Chen et al., 2025b), as well as on longvideo comprehension benchmarks including LongVideoBench (Wu et al., 2024b), MLVU (Zhou et al., 2024), VideoMME (Fu et al., 2025a), and LVBench (Wang et al., 2024a). Across all tested benchmarks, our model consistently achieves superior results, outperforming the baseline by an average of 10.4% in accuracy while using significantly fewer frames. Notably, on the LongVideoReason benchmark, FrameThinker achieves an accuracy of 76.1% using merely 20.6 frames on average, surpassing the competitive LongVILA-R1 (Chen et al., 2025b) (72.0%) which requires 20 times more frames (512), and establishing new state-of-the-art. Furthermore, on the reasoning-intensive Video-Holmes benchmark, our method achieves an accuracy of 46.8%, which also establishes new state-of-the-art while using remarkably fewer frames. To summarize, our main contributions are threefold: We introduce FrameThinker for long video reasoning, novel framework that empowers LVLMs to actively and dynamically focus on reasoning-intensive video frames, shifting the paradigm from passive video processing to active, multi-turn iterative reasoning. We conduct comprehensive investigation of the reward design space for multi-turn video reasoning. Our deep dive into reward conditioning reveals that even conditional rewards can reinforce illogical thought-action pairs. This motivated our proposal of the Cognitive Consistency Verification (CCV) module to enforce logical consistency and interpretability. We demonstrate through extensive experiments that FrameThinker achieves superior accuracy while using significantly fewer frames than previous methods, establishing new state-of-the-art performance on challenging reasoning benchmarks."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Video Understanding. Large Vision-Language Models (LVLMs) have significantly advanced video understanding tasks (Liu et al., 2021; 2020; Qu et al., 2020). Following the release of DeepSeek-R1 (Guo et al., 2025), new wave of models based on Reinforcement Learning with Verifiable Reward (RLVR), including Video-R1 (Feng et al., 2025), VideoChat-R1 (Li et al., 2025), and LongVILA-R1 (Chen et al., 2025b), has demonstrated substantial performance gains, even surpassing proprietary models on certain benchmarks (Chen et al., 2025b). However, these methods predominantly rely on passive, uniform frame sampling strategy, which limits their efficiency and effectiveness on long videos. In this paper, we challenge this paradigm by introducing FrameThinker, framework that empowers the model to actively and iteratively interrogate the video. Video Agents. Video agents (Wang et al., 2024b; Fan et al., 2024; Wang et al., 2025; Zhang et al., 2025b; Liu et al., 2025) typically rely on external tools to perform complex tasks. With improvements in both these tools and LVLMs, video agents now achieve strong performance on multiple benchmarks. Despite their success, most methods rely on fixed workflows, with decision policies that cannot be learned from data, which limits their flexibility. In contrast, our approach learns an autonomous, end-to-end policy directly from data, enabling the model to decide when and how to interact with the video while leveraging its reasoning to guide visual exploration. Thinking with Image. Traditional Large Vision-Language Models (LVLMs) (Bai et al., 2025; Chen et al., 2024c; 2025b) typically process visual information as static starting condition, after which all subsequent reasoning unfolds purely in the textual domain. Recently, the emerging paradigm of Thinking with Images (Su et al., 2025a;b;c; OpenAI, 2025b; Zheng et al., 2025) has challenged this approach by enabling models to iteratively consult and leverage visual data as an integral part of their thought process, leading to improved performance. Inspired by this evolution, we propose FrameThinker, which empowers the model to dynamically query video for relevant frames based on its evolving cognitive state, thereby enabling the model to think with long videos."
        },
        {
            "title": "3 METHOD",
            "content": "3.1 FRAMEWORK OVERVIEW As illustrated in Figure 2, instead of processing the video in single, passive pass, our FrameThinker framework engages in multi-turn reasoning loop, actively and strategically seeking information from the video for its decision-making. At each step, the model first generates textual thought and then selects specific action from predefined action space. This explicit separation of reasoning and action is articulated through structured output format: <think>...</think><action>...</action>. Formally, for given input query i, the model produces sequence of thought-action-observation triplets, namely trajectory τ : (cid:16) τ (i) = (t(i) 1 , a(i) 1 , o(i) 1 ), . . . , (t(i) n(i), a(i) n(i), o(i) (cid:17) n(i)) , (1) where for the i-th example at step t, t(i) represents the textual thought (i.e <think> tag) generated by the model. This component captures the models explicit reasoning for its subsequent action. a(i) is the observation returned by the environment after executing action a(i) could be set of frames, frame number, or terminal signal. n(i) is the total number of steps in the trajectory. is the action (i.e <action> tag) selected by the model. o(i) . For instance, o(i) More complex and diverse examples of the models reasoning process are provided in Appendix C. In this paper, we explore the below three actions for long video reasoning: choose frames between START FRAME and END FRAME: This is the primary action for visual exploration. It allows the model to retrieve sequence of frames (e.g., 8 frames) from specific temporal segment of the video, enabling zoom-in capability. get frame number at time MM:SS: This auxiliary action enhances the models temporal awareness. It translates human-readable timestamp into precise frame index, which can then be used in subsequent choose frames actions."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: (a) An illustration of the iterative reasoning process of our proposed FrameThiner. The model first performs sparse scan, then uses thought-action steps to progressively gather evidence. The CCV module ensures this process is logically consistent and interpretable. (b) Our three-stage training pipeline, consisting of Data Preparation, Supervised Fine-Tuning (SFT) to learn action syntax, and Reinforcement Learning (RL) to optimize the policy. output answer: This is terminal action that concludes the reasoning process. The model executes this action when it has gathered sufficient evidence to confidently provide the final answer to the users query. 3.2 SUPERVISED FINE-TUNING To equip our FrameThinker model with the correct syntax for the defined actions, we first conduct Supervised Fine-Tuning (SFT) phase. This phase utilizes small, curated dataset of only 2,392 examples, designed specifically to instill the basic action grammar. The composition of this data is illustrated in the left panel of Figure 3, with detailed breakdown provided in Appendix F. The more complex task of learning strategic policy is deferred to the subsequent Reinforcement Learning (RL) phase. With this curated SFT dataset, we fine-tune the model using standard autoregressive language modeling objective. The model is trained to predict the next token in the ground-truth trajectories, which are composed of both <think> and <action> sequences. Specifically, we compute standard cross-entropy loss, but only on the tokens generated by the model (i.e., the content within the <think> and <action> tags). The input query and the observations from action execution are treated as context and are excluded from the loss calculation. This phase ingrains the fundamental mechanics of action execution, preparing the model for the more strategic learning in the subsequent RL phase. Figure 3: The distribution of data sources for the SFT (Left) and RL training phases (Right). 3.3 REINFORCEMENT LEARNING (RL) While the SFT phase successfully equips the model with the basic syntax of action execution, its supervised nature has inherent limitations. Reliance on small, fixed dataset can create tendency for"
        },
        {
            "title": "Preprint",
            "content": "the model to memorize specific solution paths, rather than truly generalizing the underlying reasoning strategy (Chu et al., 2025; Zhang et al., 2025a; Qu et al., 2025b; Yan et al., 2025). This approach leads to fragile policies that suffer from limited generalization when faced with novel scenarios. To transition the model from memorization to true generalization, we introduce the Reinforcement Learning (RL) phase. Crucially, this phase utilizes much larger and more diverse dataset of 28k examples, which discourages overfitting to specific patterns and compels the model to learn more robust, generalizable policy. The composition of this dataset is illustrated in the right panel of Figure 3, with further details available in Appendix F. We adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024; Guo et al., 2025) to optimize the policy during the RL phase. For each query q, we sample trajectories {τi}G i=1 from the old policy πθold and normalize their outcome rewards to construct the advantages. We omit the KL divergence term to improve efficiency and avoid prematurely constraining the policys search for an optimum. (Yu et al., 2025). The objective is defined as: JGRPO(θ) = Eτiπθold (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 min (riAi, clip(ri, 1 ϵ, 1 + ϵ) Ai) (cid:35) where ri = πθ(τi q) πθold(τi q) , Ai = Ri mean({R1, ..., RG}) std({R1, ..., RG}) + δ (2) (3) Here, Ri is the reward for trajectory τi, ϵ is the clipping hyperparameter, and δ is small constant for numerical stability. Given that the interactive, multi-turn nature of our framework introduces unique context for video understanding, it raises new questions for reinforcement learning, particularly regarding how to design an effective reward function. Therefore, in this paper, we make the first attempt to comprehensively explore the design space of the reward function in detail in the following paragraphs. 3.3.1 SHOULD WE USE FORMAT REWARD? Reinforcement Learning with Verifiable Reward (RLVR) frameworks (Guo et al., 2025) often include format reward. We initially explored this reward but observed critical issue: the model quickly learned to suppress action usage, discouraging exploration, as shown in our ablation study (Section 4.3). The cause is that large format reward (e.g., comparable to accuracy reward) creates perverse incentive. Early in training, the model discovers that outputting final answer without reasoning, even random guess, provides low-risk way to stably earn the format reward. Attempting actions, in contrast, risks format error and zero reward. Based on this finding, we deliberately omit the format reward. This choice is further justified by our frameworks execution engine, which inherently penalizes malformed actions: an invalid action triggers an execution error, prematurely terminating the trajectory and forfeiting any chance of receiving the final accuracy reward. 3.3.2 SHOULD ACTION REWARDS BE UNCONDITIONAL OR CONDITIONAL? We initially experimented with an unconditional action bonus, where the model receives reward simply for executing an action, regardless of the final outcome. However, we observed that this approach often leads to model collapse. For instance, the model might learn to repeatedly call the same simple action in loop or even attempt to execute multiple actions within single turn, with its reasoning collapsing into meaningless, repetitive text. detailed analysis of these failure modes is provided in Appendix D.1. Therefore, we shifted to conditional action bonus, where rewards are granted only when an action is part of trajectory that leads to correct final answer (Racc = 1). Our total reward is formulated as sum of this outcome-based accuracy reward and process-based action bonus: where the conditional action bonus, Raction, is defined as weighted sum over the actions taken: Raction = λcf I(cf τ ) + λgfn I(gfn τ ) Rtotal = Racc + Raction (4) (5)"
        },
        {
            "title": "Preprint",
            "content": "where λcf and λgfn are the reward weights for choose frames and get frame number. While this conditional reward structure resolves the issue of mode collapse, we found that more subtle challenge still remains: the model might learn to execute spurious or illogical actions that, by chance, are part of successful trajectory. For instance, the model might select different frame interval in its action than what it decided in the thought process, or it might request the frame number for specific timestamp but then choose frames from completely different interval (examples can be seen in Appendix D.3). This highlights the need for mechanism to enforce consistency between the models reasoning and its subsequent actions. To prevent the reinforcement of such flawed reasoning paths, we introduce the Cognitive Consistency Verification (CCV) module. During training, this rule-based module acts as filter after rollout. It is applied to every trajectory τ to validate its logical consistency by checking for redundancy, logical flow, and fidelity between thought and action (detailed in Appendix D.3). Any trajectory that fails these checks is immediately terminated. The final reward, Rfinal, is then calculated as: Rfinal = Rtotal VCCV(τ ) (6) where VCCV(τ ) is verification function that returns 1 if the trajectory τ passes the Cognitive Consistency Verification, and 0 otherwise. This mechanism effectively suppresses the models attempts to cheat for rewards by assigning zero reward to illogical trajectories. During the inference phase, the CCV module serves as runtime safeguard. If an illogical action is detected, the current attempt is terminated, allowing for retry or fallback. The CCV module ensures that the generated chain of thought serves as verifiable trace of the decision-making process, which is crucial for interpretability. It also yields additional performance gains, as detailed in Section 4.3. 3.3.3 WHICH ACTION SHOULD THE REWARD PRIORITIZE? With the reward structure defined, the next design question is how to set the weights, λcf and λgfn, to encourage the most effective behavior. We deliberately set λgfn λcf because supervising the choose frames action is difficult; even if an action is logically consistent and part of successful trajectory, it does not guarantee the action itself was meaningful. The model could have retrieved irrelevant frames and still reached the correct answer using information from the initial query or previous turns. In contrast, for time-specific tasks, the get frame number action is objectively correct and highly informative, providing precise, verifiable information that directly supports subsequent decision-making. This, in turn, provides indirect supervision for the choose frames action, as the bonus is only awarded if the model subsequently uses the obtained frame number correctly within its selected interval, condition enforced by the CCV module. 3.3.4 SHOULD WE ENCOURAGE THE MODEL TO PERFORM AS MANY TURNS AS POSSIBLE? In our multi-turn interactive setting, natural question arises: should we explicitly encourage the model to take more turns? To investigate this, we replace the previous action reward with newly designed function that directly promotes additional turns: Raction = (T 1) (7) where is the total number of turns in the trajectory and is small positive constant. The subtraction of 1 accounts for the final action to output answer, which is not an analysis step. We conducted experiments with both unconditional and conditional rewards and observed that, although the models average number of turns initially increased, training quickly became unstable and subsequently collapsed, with the models reasoning degrading into meaningless statements. (Detailed training curves and case studies are provided in Appendix D.2). We conclude that this reward setup makes the training unstable: the model, in its pursuit of higher cumulative rewards, learns to prioritize increasing the number of turns over maintaining coherent reasoning and achieving the actual task objective. Therefore, we did not adopt this configuration in our final design."
        },
        {
            "title": "Preprint",
            "content": "Table 1: Performance on Video-Holmes. The table breaks down the overall accuracy into seven sub-tasks: Social Reasoning (SR), Intention & Motive Chaining (IMC), Temporal Causal Inference (TCI), Timeline Analysis (TA), Multimodal Hint Reasoning (MHR), Physical Anomaly Reasoning (PAR), and Core Theme Inference (CTI). *denotes model trained on Video-Holmes. Model Frames SR IMC TCI TA MHR PAR CTI Overall GPT-4o (OpenAI, 2024) Gemini-1.5-Pro (Team et al., 2023) Gemini-2.5-Pro (Comanici et al., 2025) 32 - - 50.0 52.1 46.6 49.6 48.2 49.3 38.8 34.4 46.9 30.0 26.0 53.0 Closed Source Models GRPO-CARE (Chen et al., 2025a) GRPO-CARE* (Chen et al., 2025a) Video-R1 (Feng et al., 2025) VideoChat-R1 (Li et al., 2025) Qwen2.5-VL-7B (Bai et al., 2025) FrameThinker (Ours) Open Source Models - - 32 32 32 10.2 42.8 46.2 48.6 42.1 38. 58.9 35.1 44.9 41.7 38.8 34.8 58.7 25.6 31.5 28.9 24.5 17.6 49.1 40.5 49.5 34.5 39.5 30. 60.0 44.0 39.2 40.1 29.2 39.2 31.0 29.5 27.1 56.0 39.2 46.4 44.3 29.9 37.1 33.5 27.8 18. 53.6 37.0 38.9 37.4 32.6 37.4 35.9 29.3 25.2 49.1 42.0 41.2 45.0 33.5 40.7 36.5 33.0 27. 56."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "Our empirical evaluation is conducted in two main stages: small-scale and large-scale experiments. Both stages share an identical Supervised Fine-Tuning (SFT) phase, but they differ in the Reinforcement Learning (RL) phase. Due to resource constraints, the small-scale experiments serve as our primary platform for conducting detailed ablation studies. These studies are crucial for validating our experimental setup and examining key aspects of the training methodology, such as the design of the reward function. Specifically, for the small-scale experiments, we perform RL training exclusively on the training set of Video-Holmes (Cheng et al., 2025). We choose this benchmark because its emphasis on active clue-seeking for reasoning aligns perfectly with the philosophy of our framework. Conversely, the large-scale experiments are focused on training our final model on our complete RL dataset to achieve the best possible performance and enhance its ability to generalize across diverse problem distributions. The resulting model is then used to report our final results against competing methods on these benchmarks, validating the effectiveness of our approach. 4.1 BENCHMARKS AND BASELINES We evaluate our method on comprehensive suite of six benchmarks: Video-Holmes (Cheng et al., 2025), LongVideo-Reason (Chen et al., 2025b), LongVideoBench (Wu et al., 2024b), MLVU (Zhou et al., 2024), VideoMME-Long(w/o sub) (Fu et al., 2025a), and LVBench (Wang et al., 2024a). These benchmarks are strategically chosen to assess distinct capabilities. The first two benchmarks, Video-Holmes and LongVideo-Reason, are designed to evaluate the models advanced reasoning abilities. The subsequent four focus on long-video comprehension across spectrum of increasing durations, among which VideoMME-Long and LVBench specifically challenge performance in exceptionally long scenarios. Further details on each benchmark are provided in Appendix F.1. Our baseline models are chosen to provide comprehensive comparison against leading models. We select Qwen2.5-VL-7B (Bai et al., 2025) as our primary baseline for direct comparison, and we also include Video-R1 (Feng et al., 2025) and VideoChat-R1 (Li et al., 2025), as they are also based on Qwen2.5-VL-7B. Furthermore, for broader context, we also report the performance of other models as cited on the official benchmark leaderboards. 4.2 IMPLEMENTATION DETAILS Our implementation is based on the Qwen2.5-VL-7B-Instruct model. In Reinforcement Learning (RL), the number of frames sampled in each turn was adaptively set: 8 frames for most videos, and 12 frames for longer videos (> 300 s) to better handle the extended temporal context. The primary reward is based on the final accuracy (Racc = 1), supplemented by action reward, with no format reward. The small-scale experiments used learning rate of 1.0e-6 and reward bonuses λgfn = 0.2,"
        },
        {
            "title": "Preprint",
            "content": "(a) Ablation on the effect of training stages. (b) Ablation on the effect of format rewards. (c) Ablation on the effect of the CCV module. (d) Ablation on the effect of reward configurations. Figure 4: Comprehensive ablation studies on key components of our training methodology. (a) Direct comparison against fine-tuned Qwen2.5-VL-7B baseline. (b) Impact of including format reward. (c) Ablation on the CCV module during training and inference. (d) Performance under different action reward configurations. λcf = 0, while the large-scale training used learning rate of 5.0e-7 and reward bonuses λgfn = 0.5, λcf = 0.02. Further implementation details are provided in Section A. 4.3 SMALL-SCALE EXPERIMENTS Main Results. When trained on Video-Holmes, our model achieves an accuracy of 56.1%, establishing new state-of-the-art, as shown in Table 1. Our method not only surpasses existing models in accuracy but also use significantly fewer frames on average. Ablation Studies. We conducted series of ablation studies to quantify the advantages of our framework, with the results visualized in Figure 4. First, we conducted direct comparison by fine-tuning the baseline Qwen2.5-VL-7B model using our identical SFT or RL setups. The results, presented in Table 4a, demonstrate that our full approach achieves clear performance advantage, confirming the efficacy of FrameThinkers architecture. Next, we present the impact of format reward. As shown in Figure 4b, the version trained with format reward exhibits rapid decline in actions during the early stages of training, indicating that such reward can inadvertently suppress the models incentive to explore and utilize actions. Furthermore, we validated the effectiveness of our Cognitive Consistency Verification (CCV) module. The results in Figure 4c show its impact when applied during the training and inference phases, underscoring that the CCV modules contribution extends beyond just enhancing interpretability; it also leads to tangible improvement in performance. Finally, we investigated the impact of different configurations for action reward, with the results presented in Figure 4d. Our findings indicate that assigning larger reward bonus to the get frame number action than to the choose frames action (in this case, λgfn = 0.2 and λcf = 0) yields the most significant performance benefit. We attribute this to the more accurate and reliable information provided by the get frame number action. 4.4 LARGE-SCALE EXPERIMENTS In our large-scale experiments, we conducted comprehensive evaluation of FrameThinker across six challenging benchmarks, demonstrating its consistent superiority in both accuracy and frame efficiency. The models strength is particularly evident on benchmarks designed to test complex reasoning abilities  (Table 2)  . For instance, on LongVideo-Reason, our model establishes new state-of-the-art result. It achieves 76.1% accuracy using merely 20.6 frames on average, surpassing the strong LongVILA-R1 baseline which requires 512 frames. Table 2: Performance on reasoning benchmarks. *denotes indicates results evaluated by us. model trained on Video-Holmes. Model Video-Holmes LongVideo-Reason Frame Acc Frame Acc Closed Source Models GPT-4o (OpenAI, 2024) Gemini-1.5-Pro (Team et al., 2023) 32 32 Open Source Models LongVILA (Chen et al., 2024c) LongVILA-R1 (Chen et al., 2025b) GRPO-CARE (Chen et al., 2025a) GRPO-CARE* (Chen et al., 2025a) Video-R1 (Feng et al., 2025) VideoChat-R1 (Li et al., 2025) - - - - 32 32 42.0 41. - - 33.5 40.7 36.5 33.0 - - - 512 - - - 32 Qwen2.5-VL-7B (Bai et al., 2025) FrameThinker (Ours) 27.8 32 15.9 46.8 -50% +19.0 32 20.6 -36% - 69.3 62.7 72.0 - - 68.1 67.2 64.1 76.1 +12."
        },
        {
            "title": "Preprint",
            "content": "(a) Performance on Video-Holmes. (b) Overall performance. (c) Average accuracy comparison. Figure 5: (a) Accuracy and the number of frames processed on Video-Holmes. (b) radar chart comparing overall performance across six benchmarks, where results are normalized and scaled for visual comparison. (c) Average accuracy across six benchmarks. Our FrameThinker achieves the best average performance. Table 3: Performance on long-video benchmarks. indicates results evaluated by us. Model LongVideoBench MLVU VideoMME-Long LVBench Frame Acc Frame Acc Frame Acc Frame Acc Closed Source Models GPT-4o (OpenAI, 2024) Gemini-1.5-Pro (Team et al., 2023) PLLaVA (Xu et al., 2024) ShareGPT4Video (Chen et al., 2024b) LongVA (Zhang et al., 2024) VITA-1.5-7B (Fu et al., 2025b) Video-R1 (Feng et al., 2025) VideoChat-R1 (Li et al., 2025) 32 32 16 16 - - 32 32 Qwen2.5-VL-7B (Bai et al., 2025) FrameThinker (Ours) 32 21.1 -34% 58.5 55.2 0.5fps - 64.6 - 384 0.5fps 65.3 67. 60 3600 48.9 33.1 Open Source Models 40.2 39.7 - - 52.7 49.1 43.2 52.9 +9.7 - 16 256 - 32 32 - 46.4 56.3 - 60.2 54.3 48.4 32 23.2 59.1 -28% +10. - 16 128 128 32 32 32 24.1 -25% - 35.0 46.2 46.2 48.2 46.2 41.9 47.6 +5.7 16 - - - 32 32 32 23.9 -25% 26.1 - - - 35.3 34.3 31.6 36.6 +5. This trend of high performance and exceptional frame efficiency continues across the four long-video comprehension benchmarks  (Table 3)  . When viewed holistically, FrameThinkers well-rounded capabilities become clear. The radar chart in Figure 5b provides visual summary, illustrating its advantage over competing methods in various benchmarks. Quantitatively, this superiority is best captured by its average performance, as shown in Figure 5c. FrameThinker achieves an average accuracy of 53.2%, representing significant improvement of +10.4% over the baseline Qwen2.5-VL-7B. (Qwen2.5-VL-7B and its derivatives were evaluated using Chain-of-Thought prompting strategy, with the specific prompt detailed in Appendix B.3). For detailed training curves, see Appendix E."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we introduced FrameThinker, novel framework that advances video understanding by shifting the paradigm from passive, uniform sampling to active, iterative analysis. Our model is trained to dynamically interrogate video content, intelligently selecting and reasoning over minimal number of frames to find the answer. Our training pipeline consists of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). Within this framework, we further explore the reward design space for multi-turn video analysis, and introduce Cognitive Consistency Verification (CCV) module to ensure that the models actions remain logically grounded, interpretable, and aligned with its reasoning. Our extensive experiments show that FrameThinker achieves state-of-the-art performance on challenging reasoning benchmarks while drastically reducing the required visual context."
        },
        {
            "title": "Preprint",
            "content": "This work demonstrates significant advance in video understanding, creating powerful models that can achieve more comprehensive reasoning with drastically less visual context."
        },
        {
            "title": "REFERENCES",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, Junyang Lin, et al. Qwen2.5-VL Technical Report, 2025. Guo Chen, Yicheng Liu, Yifei Huang, Yuping He, Baoqi Pei, Jilan Xu, Yali Wang, Tong Lu, and Limin Wang. Cg-bench: Clue-grounded question answering benchmark for long video understanding, 2024a. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. In European Conference on Computer Vision, pp. 370387. Springer, 2024b. Yi Chen, Yuying Ge, Rui Wang, Yixiao Ge, Junhao Cheng, Ying Shan, and Xihui Liu. GrpoarXiv preprint care: Consistency-aware reinforcement learning for multimodal reasoning. arXiv:2506.16141, 2025a. Yukang Chen, Fuzhao Xue, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, et al. Longvila: Scaling long-context visual language models for long videos. arXiv preprint arXiv:2408.10188, 2024c. Yukang Chen, Wei Huang, Baifeng Shi, Qinghao Hu, Hanrong Ye, Ligeng Zhu, Zhijian Liu, Pavlo Molchanov, Jan Kautz, Xiaojuan Qi, et al. Scaling rl to long videos. arXiv preprint arXiv:2507.07966, 2025b. Junhao Cheng, Yuying Ge, Teng Wang, Yixiao Ge, Jing Liao, and Ying Shan. Video-holmes: Can mllm think like holmes for complex video reasoning? arXiv preprint arXiv:2505.21374, 2025. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Yue Fan, Xiaojian Ma, Rujie Wu, Yuntao Du, Jiaqi Li, Zhi Gao, and Qing Li. Videoagent: memory-augmented multimodal agent for video understanding. In European Conference on Computer Vision, pp. 7592. Springer, 2024. Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2410824118, 2025a. Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Haoyu Cao, Zuwei Long, Heting Gao, Ke Li, et al. Vita-1.5: Towards gpt-4o level real-time vision and speech interaction. arXiv preprint arXiv:2501.01957, 2025b. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025."
        },
        {
            "title": "Preprint",
            "content": "Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, and Limin Wang. Videochat-r1: Enhancing spatio-temporal perception via reinforcement fine-tuning. arXiv preprint arXiv:2504.06958, 2025. Daizong Liu, Xiaoye Qu, Xiao-Yang Liu, Jianfeng Dong, Pan Zhou, and Zichuan Xu. Jointly crossand self-modal graph attention network for query-based moment localization. In Proceedings of the 28th ACM International Conference on Multimedia, pp. 40704078, 2020. Daizong Liu, Xiaoye Qu, Jianfeng Dong, Pan Zhou, Yu Cheng, Wei Wei, Zichuan Xu, and Yulai Xie. Context-aware biaffine localizing network for temporal sentence grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1123511244, 2021. Ye Liu, Kevin Qinghong Lin, Chang Wen Chen, and Mike Zheng Shou. Videomind: chain-of-lora agent for long video reasoning. arXiv preprint arXiv:2503.13444, 2025. OpenAI. Hello GPT-4o. OpenAI Blog, May 2024. URL https://openai.com/index/ hello-gpt-4o/. Accessed: 2024-05-14. OpenAI. Chatgpt, 2025a. URL https://chat.openai.com. OpenAI. Thinking with images, 2025b. URL https://openai.com/index/ thinking-with-images/. Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Mateusz Malinowski, Yi Yang, Carl Doersch, et al. Perception test: diagnostic benchmark for multimodal video models. Advances in Neural Information Processing Systems, 36:4274842761, 2023. Tianyuan Qu, Longxiang Tang, Bohao Peng, Senqiao Yang, Bei Yu, and Jiaya Jia. Does your visionlanguage model get lost in the long video sampling dilemma? arXiv preprint arXiv:2503.12496, 2025a. Xiaoye Qu, Pengwei Tang, Zhikang Zou, Yu Cheng, Jianfeng Dong, Pan Zhou, and Zichuan Xu. Fine-grained iterative attention network for temporal language localization in videos. In Proceedings of the 28th ACM International Conference on Multimedia, pp. 42804288, 2020. Xiaoye Qu, Yafu Li, Zhaochen Su, Weigao Sun, Jianhao Yan, Dongrui Liu, Ganqu Cui, Daizong Liu, Shuxian Liang, Junxian He, et al. survey of efficient reasoning for large reasoning models: Language, multimodality, and beyond. arXiv preprint arXiv:2503.21614, 2025b. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Alex Su, Haozhe Wang, Weiming Ren, Fangzhen Lin, and Wenhu Chen. Pixel reasoner: Incentivizing pixel-space reasoning with curiosity-driven reinforcement learning. arXiv preprint arXiv:2505.15966, 2025a. Zhaochen Su, Linjie Li, Mingyang Song, Yunzhuo Hao, Zhengyuan Yang, Jun Zhang, Guanjie Chen, Jiawei Gu, Juntao Li, Xiaoye Qu, et al. Openthinkimg: Learning to think with images via visual tool reinforcement learning. arXiv preprint arXiv:2505.08617, 2025b. Zhaochen Su, Peng Xia, Hangyu Guo, Zhenhua Liu, Yan Ma, Xiaoye Qu, Jiaqi Liu, Yanshu Li, Kaide Zeng, Zhengyuan Yang, et al. Thinking with images for multimodal reasoning: Foundations, methods, and future frontiers. arXiv preprint arXiv:2506.23918, 2025c. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023."
        },
        {
            "title": "Preprint",
            "content": "Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Xiaotao Gu, Shiyu Huang, Bin Xu, Yuxiao Dong, et al. Lvbench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035, 2024a. Xiaohan Wang, Yuhui Zhang, Orr Zohar, and Serena Yeung-Levy. Videoagent: Long-form video understanding with large language model as agent. In European Conference on Computer Vision, pp. 5876. Springer, 2024b. Ziyang Wang, Shoubin Yu, Elias Stengel-Eskin, Jaehong Yoon, Feng Cheng, Gedas Bertasius, and Mohit Bansal. Videotree: Adaptive tree-based video representation for llm reasoning on long videos. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 3272 3283, 2025. Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua Tenenbaum, and Chuang Gan. Star: benchmark for situated reasoning in real-world videos. arXiv preprint arXiv:2405.09711, 2024a. Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. Advances in Neural Information Processing Systems, 37:2882828857, 2024b. Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of questionanswering to explaining temporal actions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 97779786, 2021. Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. Pllava: Parameter-free llava extension from images to videos for video dense captioning. arXiv preprint arXiv:2404.16994, 2024. Jianhao Yan, Yafu Li, Zican Hu, Zhi Wang, Ganqu Cui, Xiaoye Qu, Yu Cheng, and Yue Zhang. Learning to reason under off-policy guidance. arXiv preprint arXiv:2504.14945, 2025. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, et al. survey of reinforcement learning for large reasoning models. arXiv preprint arXiv:2509.08827, 2025a. Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. Xiaoyi Zhang, Zhaoyang Jia, Zongyu Guo, Jiahao Li, Bin Li, Houqiang Li, and Yan Lu. Deep video discovery: Agentic search with tool use for long-form video understanding. arXiv preprint arXiv:2505.18079, 2025b. Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing thinking with images via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025. Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv e-prints, pp. arXiv2406, 2024."
        },
        {
            "title": "APPENDIX CONTENTS",
            "content": "A More Implementation Details Prompts B.1 System Prompt for FrameThinker . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Fallback Prompt for CCV Failures . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Chain-of-Thought Prompt for Baselines . . . . . . . . . . . . . . . . . . . . . . . Additional Examples Bad Case D.1 Mode Collapse from Unconditional Rewards . . . . . . . . . . . . . . . . . . . . D.2 Mode Collapse from Multi-Turn Rewards . . . . . . . . . . . . . . . . . . . . . . D.3 Trajectories Failing Cognitive Consistency Verification (CCV) . . . . . . . . . . . Training Dynamics Datasets and Benchmarks F.1 Details of Datasets and Benchmarks Used . . . . . . . . . . . . . . . . . . . . . . F.2 Details of SFT Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 13 13 14 14 18 18 19 24 26"
        },
        {
            "title": "A MORE IMPLEMENTATION DETAILS",
            "content": "Additional Training Details. In the original setup, Supervised Fine-Tuning (SFT) was performed using parameter-efficient LoRA (Hu et al., 2022) with rank of 8 on 8 H800 GPUs. per-device batch size of 1 with 4 gradient accumulation steps was used, resulting in an effective batch size of 32. The initial learning rate was set to 1.0e-4, and cosine learning rate scheduler with warmup ratio of 0.1 was applied. Reinforcement Learning (RL) training was conducted on 8 H800 GPUs with batch size of 32 and rollout number of 8."
        },
        {
            "title": "B PROMPTS",
            "content": "B.1 SYSTEM PROMPT FOR FRAMETHINKER The detailed system prompt used to guide FrameThinker during our experiments is shown in Figure 6. This prompt defines its available actions, and the required format for its thought and action process. B.2 FALLBACK PROMPT FOR CCV FAILURES During the inference phase, the CCV module acts as runtime safeguard. If it detects an illogical action (i.e., failure in the Logical Flow or Fidelity Check), the ongoing iterative process is terminated. To prevent the model from getting stuck or failing the entire task, fallback mechanism is triggered. The model is then presented with new, simplified system prompt, as shown in Figure 7. This prompt instructs the model to provide direct, final answer directly, which ensures robust response."
        },
        {
            "title": "Preprint",
            "content": "Figure 6: The system prompt for FrameThinker. the specific syntax for action usage (get frame number and choose frames), and the structured <think>...<action> output format. It outlines its core task, Figure 7: The fallback system prompt triggered upon CCV failure during inference. It directs the model to formulate final answer directly. B.3 CHAIN-OF-THOUGHT PROMPT FOR BASELINES Additionally, we provide the Chain-of-Thought (CoT) prompt used for evaluating baseline models, such as Qwen2.5-VL-7B and its derivatives, illustrated in Figure 8. To ensure fair comparison, we adopted the same evaluation prompt as used in the original Video-Holmes benchmark (Cheng et al., 2025). Figure 8: The Chain-of-Thought prompt used for evaluating baseline models."
        },
        {
            "title": "C ADDITIONAL EXAMPLES",
            "content": "To further illustrate the capabilities of our framework, we present additional qualitative examples. In the following cases, only small subset of the frames retrieved and processed is visualized. Figure 9 demonstrates how the model handles precise, timestamp-specific query by first using an auxiliary action to locate the event."
        },
        {
            "title": "Preprint",
            "content": "Figure 9: An example of our framework solving reasoning task. Given question about specific timestamp (0:13), the model first formulates thought and executes the get frame number action. Based on the returned frame number, it initiates second reasoning turn, using the choose frames action to zoom in on the relevant scene. This multi-turn, iterative process allows the model to gather the necessary visual evidence (the state of the light switchs reflection) to arrive at the correct answer."
        },
        {
            "title": "Preprint",
            "content": "Figure 10 highlights the models ability to solve ambiguous, non-timestamp-specific questions through its iterative exploration strategy. Figure 10: An example of resolving an ambiguous query without specific timestamp. Given only sparse initial frames that suggest conflict, the models first thought is to hypothesize crucial It then executes temporal segment between the peaceful beginning and the depicted struggle. choose frames action to zoom in on this interval. The newly retrieved, denser frames provide unequivocal evidence of chase and attack, allowing the model to confidently deduce the Hunter and Fugitive relationship and select the correct answer. This case demonstrates the effectiveness of the models iterative refinement process for complex narrative understanding."
        },
        {
            "title": "Preprint",
            "content": "Figure 11 presents more complex reasoning trajectory, showcasing the models ability to perform hierarchical, multi-step search. Figure 11: An example of multi-step reasoning process that showcases hierarchical search. The model begins with broad search (choose frames between 16407 and 32814) based on the sparse initial frames. After this first action reveals the mask as key object, the model formulates new thought and performs second, more targeted search (choose frames between 25782 and 28125) to confirm its function. This iterative refinement strategy, moving from coarse to fine-grained analysis, demonstrates the models advanced capability to solve complex problems by efficiently focusing on the most relevant visual evidence."
        },
        {
            "title": "Preprint",
            "content": "In contrast to the multi-turn examples that require exploration, Figure 12 illustrates scenario of optimal efficiency where the model performs Direct Answer. Figure 12: An example of Direct Answering. The user asks about the very beginning of the video. The model correctly identifies that the provided Frame 0 already contains the necessary information. In its thought process, it recognizes the sufficiency of the initial evidence and confidently proceeds directly to the output answer action without invoking any actions. This case demonstrates the models strategic capability to not only explore when necessary but also to conclude efficiently when the answer is readily available, avoiding redundant actions."
        },
        {
            "title": "D BAD CASE",
            "content": "D.1 MODE COLLAPSE FROM UNCONDITIONAL REWARDS In our initial explorations of reward design for reinforcement learning, we observed that improper reward mechanisms could lead the model to learn invalid, repetitive strategiesa phenomenon known as mode collapse. This issue was particularly pronounced when providing unconditional rewards for specific actions, as the model would learn to exploit the reward function rather than solve the task. Unconditional reward for the get frame number action. We first experimented with reward function where fixed, positive bonus, specifically set to 0.2, was granted if the model executed the get frame number action at least once within trajectory, regardless of the final answers correctness. This unconditional incentive led to severe mode collapse, as illustrated in Figure 13. The models behavior degraded in two significant ways. First, its reasoning process became incoherent and circular; the content within the <think> tags devolved into nonsensical, repetitive phrases like need to first without any substantive analysis of the video content. Second, the model learned to repeatedly execute the exact same action (get frame number at time 00:22) without any intention of using the resulting information, demonstrating clear failure to engage in meaningful, goal-oriented exploration. The underlying cause of this collapse is that the model discovered simple, low-effort strategy to maximize its reward. The unconditional action bonus provided high-certainty, immediate reward, whereas the reward for correct final answer (Racc) was uncertain and required complex, multistep reasoning. Consequently, the model learned degenerate policy: ignore the task and simply perform the rewarded action to get the bonus. This strategy effectively circumvents the intended learning objective, discouraging any genuine attempt to understand the video and solve the problem. This critical observation underscored the necessity of making action rewards conditional on the final task success, principle we adopted in our final reward design. Unconditional reward for the choose frames action. Similarly, we observed distinct but related form of mode collapse when providing an unconditional reward of 0.2 solely for the choose frames action. In this scenario, the model is rewarded for executing this action, again, without any"
        },
        {
            "title": "Preprint",
            "content": "Figure 13: An example of mode collapse induced by unconditional reward for the get frame number action. dependency on the final outcome. As shown in Figure 14, the models response degenerates into single, elongated turn. Within this turn, the models reasoning is again replaced by nonsensical, tautological statements about options and choices. Crucially, it proceeds to execute the choose frames action multiple times in quick succession. These actions are often repetitive or illogical, lacking any strategic motivation derived from coherent thought process. The cause of this behavior is analogous to the previous case. The model identifies that repeatedly outputting the rewarded action is an efficient strategy for accumulating rewards. Instead of engaging in multi-turn dialogue to incrementally gather evidence, it learns to stuff single turn with as many rewarded actions as possible before concluding with likely random guess. This instance of mode collapse further reinforces our conclusion that unconditional rewards are fundamentally flawed for training strategic model. They incentivize the model to master the exploitation of the reward function itself, rather than to learn the complex, causal reasoning required to solve the actual task. This led us to develop conditional reward system, which we detail in the main paper. D.2 MODE COLLAPSE FROM MULTI-TURN REWARDS Reward for More Turns. Also, we investigated an alternative reward designed to encourage more extensive reasoning. This was implemented using the step-based reward function, shown in Equation 7. For this experiment, we set the reward for each turn to 0.2 (corresponding to = 0.2 in the"
        },
        {
            "title": "Preprint",
            "content": "Figure 14: An example of mode collapse induced by unconditional reward for the choose frames action. equation), with the total bonus capped at 0.6. We explored both an unconditional and conditional version of this reward. First, in the unconditional setting where the bonus is granted regardless of the final outcome, the training process proved highly unstable, as illustrated in the top row of Figure 15. Initially, the strategy appears successful: the average number of interaction turns per trajectory steadily increases, peaking at over three. However, around step 270, the training abruptly collapses, with the average number of turns plummeting to degenerate state. This policy collapse is mirrored in the average response length, which simultaneously destabilizes and drops. As shown in Figure 16, the models thought process is entirely eliminated, with the <think> tag simply mirroring the <action>. It then terminates the process in the subsequent turn with random answer, completely failing the task. In addition, we investigated conditional version, where the bonus was granted only upon correct final answer. However, we observed similar pattern of training instability, as shown in the bottom row of Figure 15. Although the policy collapse occurred at different point in training (around step 170), the overall dynamic was similar: the average number of turns initially rose and then catastrophically dropped. This demonstrates that directly incentivizing the model to take more turns, even when the reward is conditional, remains an unstable objective that leads to policy collapse. The reason for this collapse is rooted in the inherent instability of multi-turn reinforcement learning, which this reward design drastically exacerbates. Training policy for long-horizon, multi-turn interactions is intrinsically challenging due to issues like high variance and difficult credit assignment. The model must learn to maintain coherent reasoning state across many steps. The reward function in Equation 7, however, creates perverse incentive that steers the model away from this difficult task. As the graphs vividly show, the model learns that the path of least resistance is not to solve the problem, but to prioritize extending the trajectory. It abandons the complex goal of coherent reasoning in favor of simpler policy: generate just enough valid syntax to prompt the next turn. This leads to the observed collapse, forcefully demonstrating that the quality and purpose of interactions, rather than their sheer quantity, are what must be rewarded."
        },
        {
            "title": "Preprint",
            "content": "Figure 15: Training dynamics. (Top Row) Metrics from training with an unconditional reward. (Bottom Row) Metrics from training with conditional reward. The final output answer action is not included in the action counts. Figure 16: An example of mode collapse induced by reward that unconditionally encourages higher number of turns. D.3 TRAJECTORIES FAILING COGNITIVE CONSISTENCY VERIFICATION (CCV) The Cognitive Consistency Verification (CCV) module is designed to filter out and penalize trajectories that exhibit illogical or inconsistent reasoning patterns. This ensures that the model is rewarded for coherent reasoning rather than for finding loopholes in the reward system, and enhances the models interpretability. Below, we detail the three primary failure conditions that the module checks for."
        },
        {
            "title": "Preprint",
            "content": "Failure to Pass the Redundancy Check. First, we posit that coherent reasoning process should be progressive and avoid redundancy. Therefore, the CCV module checks if the model executes actions with identical content. Specifically, it flags any instance where an action with the exact same parameters is executed more than once within single trajectory. For example, repeatedly calling get frame number at time 00:22 to get the same frame number, or repeatedly executing choose frames between 100 and 200 to retrieve the same set of frames, would be penalized. Such repetitions indicate that the model is either stuck in logical loop or is not effectively using the information from its actions to advance its reasoning. We observe that this type of redundancy is primarily present during the early stages of training or in cases of mode collapse, as exemplified in Figure 13 where the model repeatedly requests the same timestamp. Such behavior becomes rare once the training process stabilizes. Failure to Pass the Logical Flow Check. Second, the CCV module enforces logical sequence for actions that are causally linked. This rule primarily addresses how the model utilizes information from the get frame number action. When this action retrieves specific frame number, subsequent choose frames action is expected to make meaningful use of this new information. failure to do so is flagged as logical inconsistency, as it demonstrates the model is not properly connecting its reasoning steps. An example of this logical breakdown is presented in Figure 17. The model correctly identifies the need to investigate timestamp (0:34) and successfully executes get frame number, which returns frame 815. This is critical piece of information obtained through an explicit action. However, in its very next step, the model requests frames from the interval 565-645. This interval does not contain frame 815, rendering the previous action useless and indicating disjointed reasoning process. The CCV module identifies this failure to utilize causally linked information and terminates the trajectory."
        },
        {
            "title": "Preprint",
            "content": "Figure 17: An example of Logical Flow Check failure. The model first successfully retrieves frame 815 corresponding to specific timestamp. However, its subsequent choose frames action targets an interval (565-645) that does not contain the retrieved frame, breaking the logical chain of actions."
        },
        {
            "title": "Preprint",
            "content": "Failure to Pass the Fidelity Check. Finally, the CCV module ensures that the models executed action is faithful to its preceding thought process. The module parses numerical arguments from both the <think> tag and the subsequent <action> tag to check for contradictions. This check is crucial for preventing the model from learning to generate plausible-sounding reasoning that is detached from its actual behavior, thereby suppressing the reinforcement of flawed or hallucinated reasoning paths. Figure 18 presents clear example of trajectory that fails this fidelity check. In the <think> block, the models reasoning identifies that the key event is located near frame 4974. However, the executed action, choose frames between 1400 and 1500, completely disregards this analysis and targets an entirely unrelated video segment. By identifying this logical inconsistency, the CCV module terminates the trajectory and assigns zero reward, preventing such flawed reasoning-action pairs from being positively reinforced. Figure 18: An example of Fidelity Check failure. The models reasoning in the <think> block identifies the relevant area around frame 4974, but the executed <action> targets an entirely unrelated interval (1400-1500). The CCV module flags this trajectory as inconsistent."
        },
        {
            "title": "E TRAINING DYNAMICS",
            "content": "The dynamics of the large-scale Reinforcement Learning (RL) training phase are illustrated in Figure 19. These metrics provide comprehensive overview of how the models policy and behavior evolve as it learns to interact with the video environment to solve tasks. As shown in Figure 19a, the primary objectiveaverage accuracyexhibits consistent upward trend, demonstrating that the policy is successfully learning. The learning curve begins to converge around step 1100 as the models performance on the task distribution approaches its peak. The average action reward (Figure 19b) displays more complex, two-phase pattern. It rises initially and then stabilizes, before resuming significant upward trajectory after approximately 850 steps, point which corresponds to roughly one full epoch over the training data. We interpret this as"
        },
        {
            "title": "Preprint",
            "content": "the model shifting its optimization focus as it begins its second pass through the dataset. In the first epoch (up to step 850), the model learns the fundamental use of actions to solve the more straightforward cases, establishing solid baseline accuracy. Once this primary accuracy reward begins to plateau, the policy must refine its strategy to earn rewards on more challenging problems. As it re-encounters the data in the second epoch, it learns to employ actions with greater precision and skill. This mastery of more nuanced action use is necessary to solve these harder instances, resulting in higher average action reward and allowing the model to continue increasing its total reward even as accuracy gains become marginal. Figure 19c reveals the evolution of the models action utilization strategy. The training begins with noticeable drop in the average number of actions. We attribute this to the model entering the RL phase with an incomplete mastery of the action space, as it was trained on only small SFT dataset and had not yet fully learned the correct format and usage of actions. The RL process quickly penalizes erroneous or superfluous actions, leading to this sharp corrective decline. Subsequently, as the model accurately learns the utility of beneficial actions, their usage rate recovers and fluctuates before settling into stable range, indicating convergence to more purposeful and efficient policy. Finally, the average response length (Figure 19d) shows general downward trend. Our SFT data is synthesized using Gemini-2.5-pro (Comanici et al., 2025), powerful, closed-source model that tends to generate longer, more detailed chains of thought, especially in such multi-turn scenarios. The Qwen2.5-VL-7B base model initially mimics this verbose style. However, during the RL phase, without an explicit reward for length, the policy naturally gravitates towards shorter, more concise outputs. This reflects an optimization towards efficiency, where the model retains the core reasoning necessary for task success while shedding the stylistic verbosity of the teacher model, behavior more aligned with its own inherent capabilities. (a) Average Accuracy (b) Average Action Reward (c) Average Actions per Trajectory (d) Average Response Length Figure 19: Key metrics from the Reinforcement Learning training phase. (a) The models accuracy on tasks steadily improves. (b) The reward attributed to taking actions increases, showing more effective action use. (c) The number of actions (excluding the output answer) per task stabilizes after an initial exploration phase. (d) The models generated response (thought + action) becomes more concise over time."
        },
        {
            "title": "F DATASETS AND BENCHMARKS",
            "content": "F.1 DETAILS OF DATASETS AND BENCHMARKS USED Video-Holmes (Cheng et al., 2025) features short suspense films with rich plots and implicit cues. Its core characteristic is requiring the model to act like detective, actively searching for and connecting multiple scattered visual and audio clues across different video segments. This process facilitates deep reasoning about causality, motivation, and themes, far exceeding the simple identification of isolated clues found in traditional benchmarks. We incorporate its training set into our training data and evaluate our final model on the official test set. LongVideo-Reason (Chen et al., 2025b) is designed to comprehensively enhance and evaluate the deep reasoning capabilities of vision-language models on long videos. It includes questionanswering content across various complex reasoning types, such as temporal, object-related, spatial, and plot-based inquiries. Similarly, we utilize its training set for our training process and conduct our evaluation on the test set. LSDBench (Qu et al., 2025a) is characterized by its high Necessary Sampling Density (NSD) questions. These questions specifically target very brief yet action-dense segments within long videos (e.g., one hour). This design rigorously tests models ability to locate and parse critical information amidst backdrop of redundant data. CG-Bench (Chen et al., 2024a) is novel, clue-driven question-answering benchmark for long video understanding. Its main feature is the emphasis on requiring the model to answer questions based on specific clues provided within the video context. NExT-QA (Xiao et al., 2021) pushes video understanding from simple action description to deep action explanation. It focuses on reasoning about the causal relationships (why something happened) and temporal relationships (what happened before/after an event) between actions in videos. STAR (Wu et al., 2024a) is benchmark designed to evaluate machines capability for situated reasoning in real-world videos. key feature is its abstraction of complex dynamic scenes into structured hypergraph representations. It programmatically generates questions centered on four reasoning typesinteraction, sequence, prediction, and feasibilityto conduct an in-depth diagnosis of models visual perception, contextual understanding, and logical reasoning skills. PerceptionTest (Patraucean et al., 2023) is novel multimodal video benchmark created to diagnostically evaluate models perceptual and reasoning abilities. The dataset contains videos specifically recorded to showcase interesting perceptual scenarios. LongVideoBench (Wu et al., 2024b) is novel benchmark specifically designed to evaluate the long-context video understanding capabilities of Large Multimodal Models (LMMs). Its core feature is the inclusion of diverse range of videos with durations of up to one hour. MLVU (Zhou et al., 2024) is benchmark for evaluating the long video understanding capabilities of Multimodal Large Language Models (MLLMs). It is distinguished by its significant extension of video lengths, ranging from 3 minutes to 2 hours, and its inclusion of diverse video genres such as movies, surveillance footage, and gameplay. VideoMME (Fu et al., 2025a) is the first comprehensive evaluation benchmark for Multimodal Large Language Models in video analysis. Its key characteristic is its all-encompassing design, which covers wide variety of video types (6 domains, 30 sub-categories) and broad temporal span, including short, medium, and long videos ranging from 11 seconds to 1 hour. LVBench (Wang et al., 2024a) is characterized by its extremely long video durations, averaging over one hour, which significantly surpasses previous benchmarks of its kind. We primarily evaluate our method on comprehensive suite of six benchmarks: Video-Holmes, LongVideo-Reason, LongVideoBench, MLVU, VideoMME-Long (w/o sub), and LVBench. As illustrated in Figure 20, these benchmarks are strategically chosen not only to assess distinct capabilities but also to cover progressively increasing range of video durations. The first two, VideoHolmes and LongVideo-Reason, are specifically selected to evaluate the models advanced reasoning abilities on complex narratives. Among them, VideoMME-Long and LVBench specifically test the model on exceptionally long video scenarios, evaluating the robustness of our approach."
        },
        {
            "title": "Preprint",
            "content": "Figure 20: Average video length (in minutes) across the six primary evaluation benchmarks. F.2 DETAILS OF SFT DATA The Supervised Fine-Tuning (SFT) dataset consists of two subsets, as detailed below. Process-Supervised Data. This subset is constructed from five reasoning templates: A. Direct Answering: Answers the query directly without any action. B. Single-Pass Frame Selection: Performs single choose frames action before answering. C. Timestamp-Grounded Selection: Uses get frame number for timestamp, then choose frames to inspect the relevant segment. D. Hierarchical Frame Selection: Performs second choose frames action that refines previously selected interval. E. Multi-Interval Exploration: Performs choose frames on two distinct intervals. Outcome-Supervised Data. For this subset, supervision is applied only to the correctness of the final answer, not the reasoning path. The distribution of these reasoning templates, alongside the number of actions per trajectory, is illustrated in Figure 21. The trajectories for both subsets are synthesized using Gemini-2.5-Pro (Comanici et al., 2025). To ensure high-quality dataset, we first retain only trajectories that lead to the correct answer and filter out instances with illogical reasoning paths using Gemini-2.5-Flash (Comanici et al., 2025). This curation process ensures that the SFT phase focuses on learning meaningful and effective action execution strategies. Figure 21: Distribution of reasoning templates (Left) and actions per trajectory (Right)."
        }
    ],
    "affiliations": [
        "Nanjing University",
        "Peking University",
        "Shanghai AI Laboratory",
        "Shanghai Jiao Tong University",
        "The Chinese University of Hong Kong"
    ]
}