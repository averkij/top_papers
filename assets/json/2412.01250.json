{
    "paper_title": "Collaborative Instance Navigation: Leveraging Agent Self-Dialogue to Minimize User Input",
    "authors": [
        "Francesco Taioli",
        "Edoardo Zorzi",
        "Gianni Franchi",
        "Alberto Castellini",
        "Alessandro Farinelli",
        "Marco Cristani",
        "Yiming Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing embodied instance goal navigation tasks, driven by natural language, assume human users to provide complete and nuanced instance descriptions prior to the navigation, which can be impractical in the real world as human instructions might be brief and ambiguous. To bridge this gap, we propose a new task, Collaborative Instance Navigation (CoIN), with dynamic agent-human interaction during navigation to actively resolve uncertainties about the target instance in natural, template-free, open-ended dialogues. To address CoIN, we propose a novel method, Agent-user Interaction with UncerTainty Awareness (AIUTA), leveraging the perception capability of Vision Language Models (VLMs) and the capability of Large Language Models (LLMs). First, upon object detection, a Self-Questioner model initiates a self-dialogue to obtain a complete and accurate observation description, while a novel uncertainty estimation technique mitigates inaccurate VLM perception. Then, an Interaction Trigger module determines whether to ask a question to the user, continue or halt navigation, minimizing user input. For evaluation, we introduce CoIN-Bench, a benchmark supporting both real and simulated humans. AIUTA achieves competitive performance in instance navigation against state-of-the-art methods, demonstrating great flexibility in handling user inputs."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 ] . [ 1 0 5 2 1 0 . 2 1 4 2 : r Collaborative Instance Navigation: Leveraging Agent Self-Dialogue to Minimize User Input Francesco Taioli1,2, Edoardo Zorzi2, Gianni Franchi3, Alberto Castellini2, Alessandro Farinelli2, Marco Cristani2, Yiming Wang4 1Polytechnic of Turin, 2University of Verona, 3U2IS, ENSTA Paris, Institut Polytechnique de Paris, 4 Fondazione Bruno Kessler francesco.taioli@polito.it, {name.surname}@univr.it, gianni.franchi@ensta-paris.fr, ywang@fbk.eu https://intelligolabs.github.io/CoIN Figure 1. sketched episode of the proposed Collaborative Instance Navigation (CoIN) task. The human user (bottom left) provides request (Find the picture) in natural language. The agent has to locate the object within completely unknown environment, interacting with the user only when needed via template-free, open-ended natural-language dialogue. Our method, Agent-user Interaction with UncerTainty Awareness (AIUTA), addresses this challenging task, minimizing user interactions by equipping the agent with two modules: Self-Questioner and an Interaction Trigger, whose output is shown in the blue boxes along the agents path (① to ⑤), and whose inner working is shown on the right. The Self-Questioner leverages Large Language Model (LLM) and Vision Language Model (VLM) in self-dialogue to initially describe the agents observation, and then extract additional relevant details, with novel entropy-based technique to reduce hallucinations and inaccuracies, producing refined detection description. The Interaction Trigger uses this refined description to decide whether to pose question to the user (①,③,④), continue the navigation (②) or halt the exploration (⑤)."
        },
        {
            "title": "Abstract",
            "content": "Existing embodied instance goal navigation tasks, driven by natural language, assume human users to provide complete and nuanced instance descriptions prior to the navigation, which can be impractical in the real world as human instructions might be brief and ambiguous. To bridge this gap, we propose new task, Collaborative Instance Navigation (CoIN), with dynamic agent-human interaction during navigation to actively resolve uncertainties about the target instance in natural, template-free, open-ended dialogues. To address CoIN, we propose novel method, Agent-user Interaction with UncerTainty Awareness (AIUTA), leveraging the perception capability of Vision Language Mod1 els (VLMs) and the capability of Large Language Models (LLMs). First, upon object detection, Self-Questioner model initiates self-dialogue to obtain complete and accurate observation description, while novel uncertainty estimation technique mitigates inaccurate VLM perception. Then, an Interaction Trigger module determines whether to ask question to the user, continue or halt navigation, minimizing user input. For evaluation, we introduce CoINBench, benchmark supporting both real and simulated humans. AIUTA achieves competitive performance in instance navigation against state-of-the-art methods, demonstrating great flexibility in handling user inputs. 1. Introduction Object-Goal navigation (ObjectNav) [1, 2] aims to locate any instance of category (i.e., Find picture) within an unknown 3D scene. Initially restricted to scenarios with only few predefined categories (6 21) to find [2, 42], ObjectNav has advanced towards more challenging tasks, e.g., locating specific instances [11, 14] in an openvocabulary [45] and multi-modal [10] setting. This evolution has been driven by breakthroughs in Large Language Models (LLMs) and Vision-Language Models (VLMs). In this work, we focus on the language-guided Instance Navigation task [10, 14] where agents rely on natural language description of both intrinsic (e.g., color, material) and extrinsic attributes (e.g., context, spatial relations) of the target object, such as the black-and-white picture depicting shirtless person, located near bed. This task better reflects real-world agent-user interaction, where users typically cannot provide images or visual references of the target. Despite recent advances, current methods assume that full instructions are provided before navigation begins [10, 35], which might be inconvenient in practice, as the human user may be unable or unwilling to provide all the nuanced details upfront. However, as the agent navigates the environment, specific details become crucial for accurately identifying the target, particularly in settings with multiple instances co-existing in the same scene that are visually similar. To address this, we introduce more realistic setting, Collaborative Instance Navigation (CoIN), that allows the agent to ask questions to the user during the navigation via template-free, open-ended natural-language dialogues. CoIN enables users to initiate the instance navigation task without providing extensive instance description. For example, the user might only specify the instance category, e.g., Find the picture, challenging minimal-guidance scenario. Notably, the agent-user interactions in CoIN differ significantly from prior scenarios [4, 7, 22]. In CoIN, we allow template-free, open-ended interactions in natural language, based on the agents understanding acquired during navigation, while prior works either perform imageonly interaction for users for target verification [22], or use templated question-answer pairs [7], relying on annotated ground-truth data for instance differentiation [4], or simpler sub-goals to reach the target [20, 23, 24, 26]. Within CoIN, two key research questions arise: 1) When and 2) How should agent-user interaction occur? In the first case, the agent must develop an internal model of its perceived environment to determine when to seek assistance from an external user to resolve ambiguities effectively. In the second, the agent should formulate the most informative questions to maximize its chances of locating the target."
        },
        {
            "title": "We introduce a novel",
            "content": "training-free approach called Interaction with UncerTainty Awareness Agent-user (AIUTA). This method incorporates Self-Questioner module, which leverages VLM and an LLM, forming self-dialogues within the agent to inquire and verify details of potential target detections. As shown in Fig. 1, the LLM first prompts the VLM to obtain an initial detection description which can be incomplete and inaccurate. To enrich the relevant details, the LLM further generates questions for the VLM, whose responses complement the initial description. However, since VLMs cannot guarantee accurate responses grounded in the visual counterpart [18, 27, 40], we further prompt the LLM to generate sanity-check questions about all relevant details (e.g., Is the person shirtless?). We force the VLMs response to be either Yes, No or dont know, proposing Normalized-Entropy-based technique to quantify the VLM uncertainty. By filtering out uncertain details, we obtain more accurate refined description. Finally, the Interaction Trigger predicts an alignment score between the known target objects attributes (i.e., facts we gathered from previous agent-human interactions, if any) and the refined detection description: based on this score, the module decides whether to continue navigation, terminate it, or ask clarifying question to the user. To facilitate the study of CoIN, we propose the first benchmark, CoIN-Bench, including curated dataset, new evaluation protocol supporting both simulated and real human users, and novel performance metric that accounts for the agent-user interactions. The dataset is created on top of GOAT-Bench [10], only considering episodes involving multiple instances in scene. During the navigation, agents can query the user for details regarding the target instance. Our benchmark supports on-line evaluation with real humans. However, as real-human evaluation struggles with scaling-up, and cannot be replicated, our benchmark also supports large-scale experiments by simulating the user responses via VLM with access to the target instance image. When evaluated on CoIN-Bench, with both real and VLM-simulated humans, AIUTA, while being trainingfree, outperforms state-of-the-art navigation methods which are trained on the dataset, in terms of success rate and path efficiency. AIUTA is also flexible with arbitrary user request inputs as demonstrated by the real human evaluation. Furthermore, we prove with an ablation study that the Normalized-Entropy-based technique for estimating VLM uncertainty in the Self-Questioner module can effectively improve navigation performance and reduce agent-user interactions, as it contributes to more accurate detection description. We also introduce novel dataset, Dont Know Visual Question Answering (IDKVQA), to facilitate fair comparison among various techniques for VLM uncertainty estimation, proving that our proposal results in the highest reliability among recent competitors. Paper Contributions are summarized as follows: 2 We introduce Collaborative Instance Navigation (CoIN), new realistic task with agent-user interactions during navigation, minimizing user inputs. We propose Agent-user Interaction with UncerTainty Awareness (AIUTA), the first method to address CoIN, using self-dialogues between VLM and LLM, to reduce perception uncertainty and agent-user interactions. We introduce CoIN-Bench, new benchmark featured for CoIN, supporting evaluation with both real-human users and VLM-simulated users for scalable experiments. We propose novel Normalized-Entropy-based technique to quantify the VLM perception uncertainty, as well as dedicated IDKVQA dataset to verify its reliability. 2. Related Works Object-Goal Navigation. ObjectNav policies are typically divided into two categories: training-based [10, 21, 30, 35] and zero-shot policies [6, 12, 37, 44, 46, 48, 50]. Trained policies rely exclusively on reinforcement learning [10, 21, 35] or in conjunction with behavioral cloning [30], both of which are computationally demanding and often struggle to generalize to unseen object categories. Vision-languagealigned embeddings offer promising alternative by enabling policies to incorporate detailed natural language descriptions as input. For instance, GOAT-Bench [10] employ CLIP embeddings as the goal modality, while methods like [21, 35] train on image-goal navigation [51] episodes and evaluate on the object-goal navigation task. Among zero-shot policies, several methods extend the seminal frontier-based exploration [43], by incorporating LLM reasoning [12, 46, 48, 50], CLIP-based localization [6] or vision-language value maps for frontier selections [44]. Interactive Embodied AI. Common approaches for Human-agent interaction involve agents asking users for assistance, with responses typically consisting of shortestpath actions for reaching target objects [3, 34] or simpler sub-goals expressed in natural language to guide navigation [20, 23, 24, 26, 38, 39]. In [31], authors proposed framework to measure the uncertainty of an LLM-based planner, enabling the agent to determine the next action or ask for help. Alex Arena [7] is platform designed for user-centric research, which includes Dialog-guided Task Completion benchmark, using human-annotated templated question-answer pairs collected via Amazon Mechanical Turk. FindThis [22] requires locating specific object instance through dialogue with the user. However, the agent only responds with images of candidate objects, lacking the ability to ask questions or engage in full natural language interactions, limiting its interactivity. In [4], the Zero-Shot Interactive Personalized Object Navigation is proposed, where agents must navigate to personalized objects (e.g., Find Alices computer). However, personalized goals are manually annotated, and the user, simulated by an LLM, can only respond with this ground-truth data. Both [4, 22] rely on pre-built top-down semantic/occupancy map to locate the objects of interest [36]; in contrast, our agent locates candidate objects only through openended, template-free, natural language dialog with the user. Vision-Language Models Uncertainty. Hallucinations, i.e., biases, reasoning failures and the generation of unfaithful text by LLMs are well-known issues [9]. Research by [25] shows that truthful information tends to concentrate on specific tokens, which can be leveraged to enhance error detection performance. However, these error detectors fail to generalize across datasets. Similarly, recent studies highlight systematic limitations in the visual capabilities of large vision-language models [18, 40], leading them to respond to unanswerable or misleading questions with hallucinated or inaccurate content [27]. To this end, PAI [18] proposes adjusting and amplifying attention weights assigned to image tokens, encouraging the model to prioritize visual information. In [49], linear probing on the logits distribution of the first tokens determines whether visual questions are answerable/unanswerable. 3. Collaborative Instance Navigation Collaborative Instance Navigation (CoIN) introduces novel scenario for the Instance Navigation task, where an agent navigates in an unknown environment to locate target instance in collaboration with human user via template-free and open-ended natural language interaction. The agent decides whether an interaction is needed to gather necessary target information from the user during the navigation. The objective of CoIN is to successfully locate the target instance with minimal user guidance, reducing the effort for the user in providing detailed description. Initially, the agent is positioned randomly in an unknown 3D environment [29]. The navigation starts upon receiving user request in natural language, which can be as minimal as by only specifying an open-set category c, e.g., Find the <category>. We assume that the user: (i) knows the instance target, and can provide any detailed description about it, and (ii) is collaborative to provide the true response when being asked by the agent. At each time step t, the agent perceives visual observation Ot of the scene, allowing it to guide policy π to pick an action at = {Forward 0.25m, Turn Right 15, Turn Left 15, Stop, Ask}, where Ask is the novel action that comes with our CoIN task. When invoked, the agent asks the user template-free open-ended question qau in natural language to gather more information about the target. With the human response rua, the agent updates the set of facts (set of attributes and characteristics) Ft, representing information derived exclusively from the interaction. Formally, the updated set of facts is represented as Ft = Ft1 rua. The navigation termi3 Figure 2. Graphical depiction of AIUTA: left shows its interaction cycle with the user, and right provides an exploded view of our method. ① The agent receives an initial instruction I: Find =<object category>. ② At each timestep t, zero-shot policy π [44], comprising frozen object detection module [17], selects the optimal action at. ③ Upon detection, the agent performs the proposed AIUTA. Specifically, ④ the agent first obtains an initial scene description of observation Ot from VLM. Then, Self-Questioner module leverages an LLM to automatically generate attribute-specific questions to the VLM, acquiring more information and refining the scene description with reduced attribute-level uncertainty, producing Sref ined. ⑤ The Interaction Trigger module then evaluates Sref ined against the facts related to the target, to determine whether to terminate the navigation (if the agent believes it has located the target object ⑥), or to pose template-free, natural-language questions to human ⑦, updating the facts based on the response ⑧. nates when certain criteria are satisfied, e.g., instance has likely been found, or maximum actions exceeded. CoIN is particularly challenging in scenarios where there exist multiple visually ambiguous instances; in that case, the agent should possess the capability of accurately perceiving the nuanced details that are critical in differentiating the target from others. Note that the agent can move anywhere in the continuous environment [32]. 4. Proposed Method Our proposed Agent-user Interaction with UncerTainty Awareness (AIUTA), module that enriches the agent, is illustrated in Fig. 2. Upon receiving an initial user request with minimal guidance that only specifies the category, e.g., Find the picture (① in Fig. 2), AIUTA updates the known facts regarding the target instance, i.e., Ft=0 = {I}. Then, it activates zero-shot navigation method, VLFM [44], perceiving the scene observation Ot and providing the navigation policy (② in Fig. 2). VLFM constructs an occupancy map to identify frontiers in the explored space, and value map that quantifies the semantic relevance of these frontiers for target object localization using the pre-trained BLIP2 [13] model. Object detection is performed by GroundingDINO, an open-set object detector [17]. More details about VLFM [44] in the Supp. Mat. (Sec. B.1). AIUTA is triggered upon the detection of an object belonging to the target class (③ in Fig. 2), executing two key components sequentially. First, the Self-Questioner (Sec. 4.1) leverages Vision Language Model (VLM) and Large Language Model (LLM) to obtain an accurate and detailed understanding of the observed object via selfquestioning, enabling reliable verification of the detection against the target (④ in Fig. 2). Next, the Interaction Trigger (Sec. 4.2), determines whether an agent-user interaction is necessary (in such case, triggering the action Ask), based on the observed object and known target facts Ft, and whether the agent should halt (i.e., Stop) or proceed with the navigation (⑤ in Fig. 2). In the case of Ask (⑦ in Fig. 2), AIUTA updates the target facts Ft with the response from the user (⑧ in Fig. 2). The agent terminates the navigation task once the target instance is deemed to be found. The complete algorithm can be found in Supp. Mat. (Sec. E). In the following, Self-Questioner and Interaction Trigger are fully detailed. 4.1. Self-Questioner Upon detection, the Self-Questioner component aims to obtain thorough and accurate description of the detected object. As suggested by previous studies [18, 27, 40], generative VLMs may produce descriptions that are not fully grounded on the visual content, leading to inaccuracy or hallucination. To mitigate this issue, we leverage an LLM to automatically generate attribute-specific questions for the VLM. In particular, we propose novel technique for estimating uncertainty in VLM perception, enabling the refinement of detection descriptions. The technique has three steps: (i) generating an initial detection description with detailed information relevant to target identification; (ii) estimating VLM perception uncertainty to validate object deaa tection; and (iii) refining the detection description by filtering out uncertain attributes. Each step is detailed below. Generation of the initial detection description. The agent initially prompts the VLM for an initial description Sinit of the observation Ot by providing the prompt Pinit = Describe the <target object> in the provided image. Formally, Sinit = VLM(Ot, Pinit). The description Sinit returned by the VLM could miss essential details for locating the specific instance, e.g., when looking for picture, the content of the picture itself may not be specified in the description. To mitigate this issue, we prompt the LLM to create list of questions Qdetails aa = {qj} given Sinit and Ft (the symbol Qaa is used to represent the self-dialogue performed by the agent). Formally, Qdetails = LLM(Pdetails, Sinit, Ft), where Pdetails is aa the prompt guiding the question generation to obtain more details (Supp. Mat. Sec. D.2). The questions of Qdetails are subsequently answered by the VLM. Specifically, it answers each question qj Qdetails aa with response rj = VLM(Ot, qj) given the observation Ot. Finally, we concatenate all responses to the initial detection Sinit, obtaining an enriched detection description Senriched. Perception uncertainty estimation. VLMs can generate hallucinated or inaccurate content [18, 27, 40], impacting the performance of AIUTA. To address this, we propose novel technique for estimating their perception uncertainty. Direct evaluation of this aspect is challenging and often requires architectural modifications. Instead, we employ prompt-guided Shannon entropy-based method for effective assessment. Our goal is to measure the uncertainty [0, 1] of the VLM in perceiving specific aspects of given image through visual question answering: the VLM answers to specific question with response and an associated uncertainty estimation u, i.e., (r, u) = VLM(Ot, q). Following the notation from [18], we consider an auto-regressive VLM, where XI is the image representation (i.e., image tokens), XP is the prompt representation (i.e., prompt text tokens), and XH is the history representation (token generated at previous time-steps). During inference, the VLM generates conditional probability distribution over the vocabulary Rw at each time step, expressed as: pVLM(y XI , XV , XH ), softmax (logitVLM(y XI , XV , XH )) . (1) Estimating the uncertainty of the VLM response is nontrivial as the VLM has an unbounded output space and its output probability distribution is over (large) vocabulary of size w. To address this issue, we leverage the standard instruction-tuning [15] procedures for VLMs, utilizing predefined set of templated answers to restrict the vocabulary size to fixed, small w. In particular, during inference, we use the following prompt: <Question>? You must answer with Yes, No, or ?=I dont know. In this way, we: (i) bound the auto-regressive nature to be essentially one-step prediction, thus avoiding length-normalization; (ii) bound the vocabulary size, i.e., = 3. We then compute the Shannon entropy [33] of probability distribution over vocabulary size w: H(pVLM) = (cid:88) i=1 p(yi) log p(yi). (2) The VLM uncertainty is then obtained by normalizing the entropy within the range [0, 1] as = , where Hmax Hmax = log(w) is the maximum entropy (i.e., maximum uncertainty) over vocabulary of size w. Given threshold τ , we can indicate if the answer is Certain or Uncertain, namely: C(u, τ ) = (cid:40) Certain, τ Uncertain, > τ (3) To reduce false positives, we use the prompt Pcheck = Does the image contain <target object>? Answer with Yes, No or ?=I dont know. (see Supp. Mat. Sec. D.3). This allows us to confirm the presence of the object, which we formally express as (rcheck, ucheck) = VLM(Ot, Pcheck). Following Eq. 3, we continue the AIUTA pipeline if response rcheck = Yes and uncertainty ucheck = Certain; otherwise, we continue exploring. aa = {qj}J To remove uncertain attributes, we prompt the LLM to extract set of attributes and values Kt = {(kj, vj)} from the detection description Senriched, where each attribute kj is associated to value vj, e.g., (frame, black); (content, RGB image of family), etc. For each attribute kj, we then prompt the LLM to generate list of questions, Qattribute j=1 to be answered by the agent itself. aa Formally, we extract attributes list and self-questions in one prompt, Qattribute = LLM(Pself questions, F, Senriched), where Pself question is the prompt for the LLM (Supp. Mat. Sec. D.4). For each question qj, we access both the response rj and the associated uncertainty uj by evaluating (rj, uj) = VLM(Ot, qj). This process allows us to confirm or refine the attributes based on the VLMs responses, obtaining final detailed description Sref ined. Detection description refinement. To obtain the final detailed description Sref ined, we let the LLM filter out uncertain attributes, given the enriched description Senriched and the set of questions, responses, and uncertainties {qj, rj, uj}. More formally, Sref ined = LLM(Pref ined, {qj, rj, uj}, Senriched), where Pref ined is the prompt for the LLM (see Supp. Mat. Sec. D.5). 4.2. Interaction Trigger Using the accurate and detailed description Sref ined of the detected object, the Interaction Trigger prompts the LLM 5 to decide whether to pose question to the human user or continue the navigation. Specifically, we prompt the LLM to estimate similarity score between scene description Sref ined and target object facts Ft. We instruct the LLM to estimate the similarity score based on the alignment between the detection description and the known facts. Formally, = LLM(Pscore, Sref ined, Ft), where Pscore is prompt instructing the LLM to produce the similarity score (see Supp. Mat. Sec. D.6). Based on the LLM-estimated similarity score, the agent takes corresponding action based on the following intuition: (i) if τstop, the navigation terminates as the agent deems the instance has been found; (ii) if < τskip, the agent deems the detected object is significantly different from the known target facts, thus skipping the agentuser interaction to reduce the user efforts in providing input. The agent will continue with the environment exploration; and (iii) if τskip < τstop, the description and facts are somewhat aligned, suggesting that posing question to the user can effectively reduce uncertainty. When taking the action Ask, we further leverage the capability of LLM to compose an effective question to the user, qau, aimed at maximizing information gain about the target instance, conditioned on the know target object facts and the refined observation description Sref ined. To minimize the number of LLM calls, we incorporate such question retrieval inside the Pscore prompt. After receiving the corresponding response from the human, rua, we update the target object facts Ft with new information, maximizing the effectiveness of later agent-human interactions. 5. CoIN-Bench To facilitate the evaluation of CoIN, we introduce CoINBench, which includes novel evaluation protocol with both simulated and real human users, along with new performance metric that accounts for agent-user interactions. Dataset. Our dataset is based on GOAT-Bench [10], where targets are specified either by the category name, language d, or by an image, description in natural in an open vocabulary manner. GOAT-Bench uses the HM3DSem [29] scene datasets and the Habitat simulator [32]. To adapt GOAT-Bench for agent-user interactions, we select episodes from multiple GOAT-Bench splits, including Train, Val Seen, Val Seen Synonyms and Val Unseen, organized by scene. To ensure the presence of multiple target instances of the same category within scene (i.e., distractors), we apply filtering procedure discarding episodes with less than dmin = 2 distractors objects. Moreover, we exclude episodes that do not contain an associated language description d. After filtering, the simulator [32] sets random starting position for the agent. To diversify the navigation difficulty, we set the geodesic distance between the target position and the starting position within [5m, 20m]. Since the images are renderings of reconstructed 3D scenes, their visual quality may be affected by poor scene reconstruction. We, therefore, manually verify the image quality, excluding any that contain the target with insufficient resolution, limited visual coverage, or that is indistinguishable among distractors. Finally, we ensure episodes are navigable without crossing floors, following [10, 42]. Our dataset contains 152 episodes for the Train split, 82 for the Val Seen, 47 for the Val Unseen, and 36 for the Val Seen Synonyms. We retain the original episode split as in GoatBench [10]. More details in the Supp. Mat. (Sec. A). Evaluation protocol. CoIN-Bench supports evaluation with both real humans (to assess the potential and limitation of genuine agent-human interactions) and VLM-based user, simulated user generated through novel VLMbased setup, enabling extensive, large-scale comparisons and scalability in experiments. Simulating human user is challenging, as the agent can ask open-ended, template-free questions about any attribute of the target object, making it impractical to provide dataset with all possible questionanswer pairs. To address this, we simulate human user for each episode by pairing the VLM with high-resolution image of the target object (10241024). This setup ensures the VLM has comprehensive visual coverage to answer the agents questions effectively. Notably, our approach is the first to simulate human user by giving VLM access to the target image itself, rather than relying solely on simple ground-truth descriptions [4], which may lack essential details for responding to agent queries. An episode is considered successful if the agent terminates by calling Stop action within 0.25m of the target goal viewpoint. If the target is not located, the agent terminates after maximum of 500 actions, as standard in ObjectGoal Navigation tasks [42]. Evaluation Metrics. Following [1, 42], we evaluate using two standard ObjectNav metrics: Success Rate (SR), our primary metric (highlighted in gray), and Success rate weighted by Path Length (SPL). Additionally, we introduce the average Number of Questions asked (NQ), which is applied to each successful episode. This metric measure the amount of user guidance, which should be minimal as required by our CoIN task. 6. Experiments We first benchmark AIUTA, in comparison with state-ofthe-art (SOTA) methods [10, 35, 44] on CoIN-Bench. Our evaluation with VLM-simulated users allows for quantitative comparisons at large scale, proving that CoIN is challenging and complex task to address. Moreover, our evaluation with real humans, despite using subset of CoIN-Bench, demonstrates that AIUTA can flexibly process arbitrary user request inputs. Finally, ablation studies support our AIUTA design choices, also showcasing the effectiveness of the Normalized-Entropy-based technique for estimating VLM uncertainty compared to recent baselines [19, 49] on our IDKVQA dataset. More experimental details are available in the Supp. Mat. Implementation Details. We employ LlaVA-NeXT [16] as VLM (LlaVA 1.6, Mistral 7B), and Llama 3.1 [5](70B) as the LLM, leveraging the free Groq API service (thank you!). The user interaction is limited to maximum of 4 rounds. We empirically set τ = 0.75 (Eq. 3), τstop = 7 and τskip = 5 as they yield the best result. Results with VLM-simulated users. Since the user request to AIUTA ranges from category to an instance description, we compare it against both SOTA Instance Navigation and ObjectNav methods: the SenseAct-NN Monolithic Policy (Monolithic) from GOAT-Bench [10], PSL [35] and the zero-shot, training-free VLFM policy [44]. Notably, both Monolithic policy (Supp. Mat. Sec B.2) and PSL (Supp. Mat. Sec. B.3) take fully detailed description of the target instance as input, while VLFM (Supp. Mat. Sec. B.1) takes category as input. Additionally, VLFM operates in zero-shot, training-free manner. In contrast, PSL is trained on the ImageNav task and transferred on the language-driven Instance navigation task, while Monolithic is trained on the original GOATBench [10] dataset. Tab. 1 summarizes the input types and training conditions. The evaluations have been conducted, as in GOAT-Bench [10], on the four partitions Val Seen, Val Seen Synonym, Val Unseen and Train. As written in Sec. 5, the primary metric is Success Rate (SR, highlighted in gray), followed by Success rate weighted by Path Length (SPL) and number of questions asked per successful episode (NQ). As shown in Tab. 1, the closest competitor VLFM [44] achieves nearly 0% SR across all splits when only the instance category is known to the agent. This is expected, as the presence of distractor objects (Sec. 5) poses significant challenges for ObjectNav methods, as they lack instancelevel discrimination capability. Instead, AIUTA can effectively gather additional information from the user when it finds an instance of the target object, with very moderate need for agent-user interactions (NQ< 2 for all splits). Compared to Monolithic and PSL, our AIUTA performs better in three out of four splits. After manual inspection, we found that the high prevalence of the cabinet object category in the Val Seen split makes object detection particularly challenging. Notably, AIUTA achieves SR that is 2 higher than Monolithic on the Train split, and nearly 3 higher than PSL on the Val Unseen split. This is surprising, as the competitors are training-based and operate with detailed instance descriptions as input. One possible explanation is that CLIP-based approaches, i.e., Monolithic and PSL, face limitations in fine-grained instance recognition, as discussed in [6, 10]. Results with real human users. total of 15 users volunteered for the user study. The evaluation protocol, user statistics, as well as demonstration videos are detailed in the Supp. Mat. To limit the required time and cognitive effort, we randomly chose subset of 16 episodes that contain detectable target instances across the splits. The episode list is available. Alongside specifying the category c, human users provide initial requests with varying levels of detail, e.g., Find the single bed with no headboard or Find the dark table. Tab. 2, shows that more specific instructions allow AIUTA to maintain high SR with fewer agent-user interactions, as the need for additional target-specific information decreases. As reference, we evaluate AIUTA with VLM-simulated users on the same selected episodes, observing lower performance than with the one obtained with real users, as VLMs may be subject to inaccurate responses. Yet, the performance gap is moderate, confirming that VLM-based user simulation can be viable alternative to scale up such human-in-the-loop evaluation. Ablation I: Analysis of key AIUTA components. Tab. 3 demonstrates the critical roles of both the Self-Questioner and Skip-Question (inside the Interaction Trigger) in boosting the performance of AIUTA. Without both SelfQuestioner and Skip-Question (Row 1), our model achieves SR of 9.21%, requiring, on average, high number of questions NQ. Disabling only the Self-Questioner (Row 2) decreases the SR, reducing NQ, as expected. Activating only the Self-Questioner (Row 3) improves SR to 9.87%, but with high number of questions NQ. When both components are enabled (Row 4), SR peak at 14.47%, with NQ minimized to 1.68. This demonstrates that combining the Self-Questioner with the Skip-Question mechanism substantially enhances efficiency and performance. Ablation II: VLM uncertainty estimation on IDKVQA. VLM uncertainty estimation is critical the SelfQuestioner module, enabling the agent to handle hallucinations and inaccuracies. To this end, we introduce IDKVQA, VQA dataset with 502 questions and 102 images from Goat-Bench [10], designed to compare our NormalizedEntropy-based technique against alternative techniques. Each question is answered by three annotators that can pick from the set {Yes, No, Dont Know}, allowing the agent to abstain when information is insufficient. We compare our Normalized-Entropy against three recent techniques on IDKVQA: MaxProb selects the answer with the highest predicted probability; an energy score-based framework for out-of-distribution detection [19]; and LP [49], recently proposed Logistic Regression model trained as linear probe on the logits distribution of the first generated token. Tab. 4 reports the performance using the Effective Reliability metric Φc proposed in [41]. Our proposed technique achieves the best Φc=1 score of 21.12, demonstrating for Method Model Condition Val Seen Val Seen Synonym Val Unseen Train Input Training-free SR SPL NQ SR SPL NQ SR SPL NQ SR SPL NQ Monolithic [10] (CVPR-24) PSL [35] (ECCV-24) VFLM [44] (ICRA-24) AIUTA c 4.88 10.98 0 8.64 2.93 5.71 0 3.2 - - - 1.57 8.33 8.33 0 11.11 1.54 3.06 0 3.32 - - - 1.50 0 4.26 0 12.77 0 2.67 0 9 - - - 1.66 6.58 4.61 0.66 14.47 4.08 1.39 0.61 7.22 - - - 1.68 Table 1. Results of the proposed AIUTA compared with baselines on the CoIN-Bench benchmark. We analyzed the SR (main metric, in gray), SPL, and the number of questions NQ. For each model, we indicate whether it operates in training-free manner. Additionally, we specify the input type: denotes models that utilize only the object category as input, while models that use its associated description. User type Input CoIN-Bench subset VLM Real Human VLM Real Human SR SPL NQ 68.75 87. 77.08 81.25 32.35 43.96 40.52 41.84 1.25 1.38 0.12 0.06 Table 2. Results on subset of CoIN-Bench between real human vs our simulated VLM, using as input either category or instruction containing arbitrary details regarding the target instance. Self-Questioner Skip-Question Train SR SPL NQ 9.21 8.55 9.87 14. 5.86 4.84 6.5 7.22 3.57 2.69 4.6 1.68 Table 3. Ablation of components in AIUTA on the Train split. VLM Model LLaVA llava-v1.6-mistral-7b-hf Selection Function MaxProb LP [49] Energy Score [19] Φc=1 15.94 14.01 20.45 Normalized Entropy (ours) 21.12 Table 4. Results of different selection functions and their corresponding Effective Reliability rate Φc=1 on the IDKVQA dataset. its effectiveness in uncertainty handling. Further details in the Supp. Mat. (Sec. C). Ablation III: Threshold τ sensitivity. We analyze the sensitivity of the threshold (τ in Eq. 3) for our NormalizedEntropy-based technique and second-best performing Energy Score [19]. Specifically, we subsample the datasets into 50%, 70%, and 100% of the original dataset size. For each subsampled dataset, we find the optimal threshold τ and evaluate its sensitivity by testing Φc=1 on 30 alternative thresholds around τ , normalizing it between 0 and 1. As shown in Fig. 3, our technique has smaller interquartile range and tighter distribution of Φc=1, while the Energy Score exhibits greater degradation from τ , which worsens as the dataset size decreases. This proves that our techFigure 3. τ sensitivity results. For each method, 30 new τ values are sampled symmetrically around the optimal threshold τ . The x-axis shows the set size as percentage of the original IDKVQA dataset size, while the y-axis displays the normalized ER Φc=1. nique is more robust in data-scarce situations, and is less sensitive to small variations in τ . Moreover, Energy Score depends on logits, thus being unbounded. On the contrary, our uncertainty is normalized, i.e. [0, 1], making optimal τ selection more efficient. 7. Conclusion We introduced the CoIN task, where an agent collaborates with the user during navigation to resolve uncertainties regarding the target instance. We presented AIUTA, the first method addressing CoIN with novel self-dialogue between an LLM and VLM, demonstrating state-of-theart navigation performance and effective user-agent interactions on our benchmark CoIN-Bench with VLM-simulated users. Our experiment with real humans showed that AIUTA has great flexibility in handling arbitrary length user input. Finally, ablation studies confirm the effectiveness of key components, including our Normalized-Entropy based technique for VLM uncertainty estimation. Limitations & Future work. AIUTA is dependent on the capabilities of current LLMs. Often larger models lead to more satisfactory performances due to better prompt handling. However, the high computational cost prohibits onboard processing, thus limiting real-world deployment as it requires cloud-based services and raises privacy concerns. Future work could focus on optimizing LLMs/VLMs with smaller, efficient models suitable for onboard operations."
        },
        {
            "title": "References",
            "content": "[1] Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, et al. On Evaluation of Embodied Navigation Agents. arXiv preprint arXiv:1807.06757, 2018. 2, 6 [2] Dhruv Batra, Aaron Gokaslan, Aniruddha Kembhavi, Oleksandr Maksymets, Roozbeh Mottaghi, Manolis Savva, Alexander Toshev, and Erik Wijmans. ObjectNav Revisited: On Evaluation of Embodied Agents Navigating to Objects. arXiv preprint arXiv:2006.13171, 2020. 2 [3] Ta-Chung Chi, Minmin Shen, Mihail Eric, Seokhwan Kim, and Dilek Hakkani-tur. Just Ask: An Interactive Learning Framework for Vision and Language Navigation. Proceedings of the AAAI Conference on Artificial Intelligence, 34 (03):24592466, 2020. 3 [4] Yinpei Dai, Run Peng, Sikai Li, and Joyce Chai. Think, Act, and Ask: Open-World Interactive Personalized Robot Navigation. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 32963303, 2024. 2, 3, 6 [5] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The Llama 3 Herd of Models. arXiv preprint arXiv:2407.21783, 2024. 7 [6] Samir Yitzhak Gadre, Mitchell Wortsman, Gabriel Ilharco, Ludwig Schmidt, and Shuran Song. CoWs on Pasture: Baselines and Benchmarks for Language-Driven Zero-Shot ObIn 2023 IEEE/CVF Conference on Comject Navigation. puter Vision and Pattern Recognition (CVPR). IEEE, 2023. 3, 7 [7] Qiaozi Gao, Govind Thattai, Suhaila Shakiah, Xiaofeng Gao, Shreyas Pansare, Vasu Sharma, Gaurav Sukhatme, Hangjie Shi, Bofei Yang, Desheng Zhang, Lucy Hu, Karthika Arumugam, Shui Hu, Matthew Wen, Dinakar Guthy, Shunan Chung, Rohan Khanna, Osman Ipek, Leslie Ball, Kate Bland, Heather Rocker, Michael Johnston, Reza Ghanadan, Dilek Hakkani-Tur, and Prem Natarajan. Alexa Arena: User-Centric Interactive Platform for Embodied AI. In Advances in Neural Information Processing Systems, pages 1917019194. Curran Associates, Inc., 2023. 2, [8] Danna Gurari, Qing Li, Abigale J. Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P. Bigham. VizWiz Grand Challenge: Answering Visual Questions from Blind People. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE, 2018. 5 [9] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of Hallucination in Natural Language Generation. ACM Comput. Surv., 55(12), 2023. 3 [10] Mukul Khanna, Ram Ramrakhya, Gunjan Chhablani, Sriram Yenamandra, Theophile Gervet, Matthew Chang, Zsolt Kira, Devendra Singh Chaplot, Dhruv Batra, and Roozbeh Mottaghi. GOAT-Bench: Benchmark for Multi-Modal Lifelong Navigation. In CVPR, page 1637316383. IEEE, 2024. 2, 3, 6, 7, 8 [11] Jacob Krantz, Stefan Lee, Jitendra Malik, Dhruv Batra, and Devendra Singh Chaplot. Instance-Specific Image Goal Navigation: Training Embodied Agents to Find Object Instances. arXiv preprint arXiv:2211.15876, 2022. 2 [12] Yuxuan Kuang, Hai Lin, and Meng Jiang. OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via VisionIn Findings of the AssociLanguage Foundation Models. ation for Computational Linguistics: NAACL 2024, pages 338351, Mexico City, Mexico, 2024. Association for Computational Linguistics. 3 [13] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. In Proceedings of the 40th International Conference on Machine Learning. JMLR.org, 2023. 4, 1, [14] Weijie Li, Xinhang Song, Yubing Bai, Sixian Zhang, and Shuqiang Jiang. ION: Instance-level Object Navigation. In ACM MM, pages 43434352. ACM, 2021. 2 [15] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual Instruction Tuning. In Advances in Neural Information Processing Systems, pages 3489234916. Curran Associates, Inc., 2023. 5 [16] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. LLaVA-NeXT: Improved reasoning, OCR, and world knowledge, 2024. 7 [17] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection. arXiv preprint arXiv:2303.05499, 2023. 4, 2 [18] Shi Liu, Kecheng Zheng, and Wei Chen. Paying More Attention to Image: Training-Free Method for Alleviating Hallucination in LVLMs. In Computer Vision - ECCV 2024. Springer Nature Switzerland, 2025. 2, 3, 4, 5 [19] Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. In Advances Energy-based Out-of-distribution Detection. in Neural Information Processing Systems, pages 21464 21475. Curran Associates, Inc., 2020. 7, 8, 5, 6 [20] Xiulong Liu, Sudipta Paul, Moitreya Chatterjee, and Anoop Cherian. CAVEN: An Embodied Conversational Agent for Efficient Audio-Visual Navigation in Noisy Environments. Proceedings of the AAAI Conference on Artificial Intelligence, 38(4):37653773, 2024. 2, [21] Arjun Majumdar, Gunjan Aggarwal, Bhavika Devnani, Judy Hoffman, and Dhruv Batra. ZSON: Zero-Shot Object-Goal In AdNavigation using Multimodal Goal Embeddings. vances in Neural Information Processing Systems, pages 3234032352. Curran Associates, Inc., 2022. 3, 4 [22] Arjun Majumdar, Fei Xia, Brian Ichter, Dhruv Batra, and Leonidas Guibas. FindThis: Language-Driven Object DisIn Proceedings of ambiguation in Indoor Environments. The 7th Conference on Robot Learning, pages 13351347. PMLR, 2023. 2, 3 [23] Khanh Nguyen and Hal Daume III. Help, Anna! Visual Navigation with Natural Multimodal Assistance via Retrospective Curiosity-Encouraging Imitation Learning. In Proceedings of the 2019 Conference on Empirical Methods 9 in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP). Association for Computational Linguistics, 2019. 2, 3 [24] Khanh Nguyen, Debadeepta Dey, and Bill Brockett, Chrnd Dolan. Vision-Based Navigation With LanguageBased Assistance via Imitation Learning With Indirect Intervention. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2019. 2, 3 [25] Hadas Orgad, Michael Toker, Zorik Gekhman, Roi Reichart, Idan Szpektor, Hadas Kotek, and Yonatan Belinkov. LLMs Know More Than They Show: On the Intrinsic arXiv preprint Representation of LLM Hallucinations. arXiv:2410.02707, 2024. 3 [26] Sudipta Paul, Amit Roy-Chowdhury, and Anoop Cherian. AVLEN: Audio-Visual-Language Embodied Navigation in 3D Environments. In Advances in Neural Information Processing Systems, pages 62366249. Curran Associates, Inc., 2022. 2, 3 [27] Yusu Qian, Haotian Zhang, Yinfei Yang, and Zhe Gan. How Easy is It to Fool Your Multimodal LLMs? An EmarXiv preprint pirical Analysis on Deceptive Prompts. arXiv:2402.13220, 2024. 2, 3, 4, 5 [28] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Transferable Visual Models From Natural Language Supervision. In Proceedings of the 38th International Conference on Machine Learning, pages 87488763. PMLR, 2021. 2 [29] Santhosh Kumar Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alexander Clegg, John Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel Chang, Manolis Savva, Yili Zhao, and Dhruv Batra. Habitat-Matterport 3D Dataset (HM3D): 1000 Large-scale 3D Environments for Embodied AI. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, 2021. 3, 6, [30] Ram Ramrakhya, Dhruv Batra, Erik Wijmans, and Abhishek Das. PIRLNav: Pretraining with Imitation and RL FineIn 2023 IEEE/CVF Conference tuning for OBJECTNAV. on Computer Vision and Pattern Recognition (CVPR). IEEE, 2023. 3 [31] Allen Z. Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Varley, Zhenjia Xu, Dorsa Sadigh, Andy Zeng, and Anirudha Majumdar. Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners. In 7th Annual Conference on Robot Learning, 2023. 3 [32] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv Batra. Habitat: Platform for Embodied AI Research. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019. 4, 6, 1 [33] Claude Elwood Shannon. mathematical theory of communication. The Bell system technical journal, 27(3):379423, 1948. 5 [34] Kunal Pratap Singh, Luca Weihs, Alvaro Herrasti, Jonghyun Choi, Aniruddha Kembhavi, and Roozbeh Mottaghi. Ask4Help: Learning to Leverage an Expert for Embodied Tasks. In Advances in Neural Information Processing Systems, pages 1622116232. Curran Associates, Inc., 2022. 3 [35] Xander Sun, Louis Lau, Hoyard Zhi, Ronghe Qiu, and Junwei Liang. Prioritized Semantic Learning for Zero-shot In Computer Vision - ECCV 2024. Instance Navigation. Springer Nature Switzerland, 2025. 2, 3, 6, 7, 8 [36] Francesco Taioli, Federico Cunico, Federico Girella, Riccardo Bologna, Alessandro Farinelli, and Marco Cristani. Language-enhanced RNR-Map: Querying Renderable NeuIn 2023 ral Radiance Field maps with natural language. IEEE/CVF International Conference on Computer Vision Workshops (ICCVW), page 46714676. IEEE, 2023. 3 [37] Francesco Taioli, Francesco Giuliari, Yiming Wang, Riccardo Berra, Alberto Castellini, Alessio Del Bue, Alessandro Farinelli, Marco Cristani, and Francesco Setti. Unsupervised Active Visual Search With Monte Carlo Planning Under Uncertain Detections. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46(12):1104711058, 2024. 3 [38] Francesco Taioli, Stefano Rosa, Alberto Castellini, Lorenzo Natale, Alessio Del Bue, Alessandro Farinelli, Marco Cristani, and Yiming Wang. Mind the Error! Detection and Localization of Instruction Errors in Vision-and-Language In IEEE/RSJ International Conference on InNavigation. telligent Robots and Systems (IROS), 2024. [39] Francesco Taioli, Stefano Rosa, Alberto Castellini, Lorenzo Natale, Alessio Del Bue, Alessandro Farinelli, Marco Cristani, and Yiming Wang. I2EDL: Interactive Instruction Error Detection and Localization. In 2024 33rd IEEE International Conference on Robot and Human Interactive Communication (ROMAN), page 18721877. IEEE, 2024. 3 [40] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), page 95689578. IEEE, 2024. 2, 3, 4, 5 [41] Spencer Whitehead, Suzanne Petryk, Vedaad Shakib, Joseph Gonzalez, Trevor Darrell, Anna Rohrbach, and Marcus Rohrbach. Reliable Visual Question Answering: Abstain In Computer Vision Rather Than Answer Incorrectly. ECCV 2022, page 148166. Springer Nature Switzerland, 2022. 7, 5 [42] Karmesh Yadav, Jacob Krantz, Ram Ramrakhya, Santhosh Kumar Ramakrishnan, Jimmy Yang, Austin Wang, John Turner, Aaron Gokaslan, Vincent-Pierre Berges, Roozbeh Mootaghi, Oleksandr Maksymets, Angel Chang, Manolis Savva, Alexander Clegg, Devendra Singh Chaplot, and Dhruv Batra. Habitat Challenge 2023, 2023. 2, 6 [43] B. Yamauchi. frontier-based approach for autonomous exploration. In Proceedings 1997 IEEE International Symposium on Computational Intelligence in Robotics and Automation CIRA97. Towards New Computational Principles for Robotics and Automation, pages 146151, 1997. 3 [44] Naoki Yokoyama, Sehoon Ha, Dhruv Batra, Jiuguang Wang, and Bernadette Bucher. VLFM: Vision-Language Frontier 10 Maps for Zero-Shot Semantic Navigation. In 2024 IEEE International Conference on Robotics and Automation (ICRA), page 4248. IEEE, 2024. 3, 4, 6, 7, 8, 1 [45] Naoki Yokoyama, Ram Ramrakhya, Abhishek Das, Dhruv Batra, and Sehoon Ha. HM3D-OVON: Dataset and Benchmark for Open-Vocabulary Object Goal Navigation. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2024. [46] Bangguo Yu, Hamidreza Kasaei, and Ming Cao. L3MVN: Leveraging Large Language Models for Visual Target Navigation. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2023. 3 [47] Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung-Ho Bae, Seungkyu Lee, and Choong Seon Hong. Faster Segment Anything: Towards Lightweight SAM for arXiv preprint arXiv:2306.14289, Mobile Applications. 2023. 2 [48] Lingfeng Zhang, Qiang Zhang, Hao Wang, Erjia Xiao, Zixuan Jiang, Honglei Chen, and Renjing Xu. TriHelper: ZeroShot Object Navigation with Dynamic Assistance. arXiv preprint arXiv:2403.15223, 2024. 3 [49] Qinyu Zhao, Ming Xu, Kartik Gupta, Akshay Asthana, Liang Zheng, and Stephen Gould. The First to Know: How Token Distributions Reveal Hidden Knowledge in Large In Computer Vision - ECCV Vision-Language Models? 2024. Springer Nature Switzerland, 2025. 3, 7, 8, 5 [50] Kaiwen Zhou, Kaizhi Zheng, Connor Pryor, Yilin Shen, Hongxia Jin, Lise Getoor, and Xin Eric Wang. ESC: Exploration with Soft Commonsense Constraints for ZeroIn Proceedings of the 40th Intershot Object Navigation. national Conference on Machine Learning, pages 42829 42842. PMLR, 2023. 3 [51] Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J. Lim, Abhinav Gupta, Li Fei-Fei, and Ali Farhadi. Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning. In 2017 IEEE International Conference on Robotics and Automation (ICRA), page 33573364. IEEE, 2017. 11 Collaborative Instance Navigation: Leveraging Agent Self-Dialogue to Minimize User Input"
        },
        {
            "title": "Supplementary Material",
            "content": "In this supplementary material, we first provide additional details regarding the CoIN-Bench dataset (Sec. A), including an overview of the GOAT-Bench dataset on which CoIN-Bench is based, as well as the statistics and examples of the target instance within CoIN-Bench. Next, we elaborate on the implementation details for baseline comparisons (Sec. B), describing the methods evaluated using VLM-simulated users and the procedures for evaluating AIUTA with real human users. Then, we provide further information regarding the evaluation conducted on IDKVQA (Sec. C), including the dataset creation, the evaluation metric, and the specifics of the state-of-the-art baselines used for comparison. Finally, we provide all the prompts in Sec. and the full algorithm of AIUTA in Sec. E. For demonstration of AIUTA in action, engaging with real human through natural language dialogues to collaboratively localize target instance, please refer to the accompanying video (aiuta demo.mp4) provided in the supplementary material. A.2. CoIN-Bench Instance examples. The CoIN-Bench benchmark poses significant challenge, especially when the agent is provided only with the target instance category. To illustrate this, Fig. 1 provides examples where the target instance is highlighted with red borders, while distractor objects in the same scene are marked with blue borders. As demonstrated, agent-user collaboration is crucial to gather the necessary specifics for identifying the target instance among other visually similar objects of the same category, such as the armchair or the plant. Dataset statistics. We provide additional statistics for the CoIN-Bench dataset. Fig. 2 illustrates the distribution of instance categories across different splits. These splits are ordered by dataset size, from the largest at the top (Train) to the smallest at the bottom (Val Seen Synonyms). The number of distinct categories decreases as the dataset size reduces. The Train split, being the largest, also contains the highest number of distinct categories, with bed, cabinet and table being the top 3 common categories. Val Seen Synonyms, being the smallest, only contains 3 categories. A. Additional details of CoIN-Bench B. Baselines A.1. GOAT-Bench Dataset. GOAT-Bench provides agents with sequence of targets specified either by category name c, language description d, or image in an open vocabulary fashion, using the HM3DSem [29] scene datasets and Habitat simulator [32]. Language descriptions are created with an automatic pipeline by leveraging ground truth semantic and spatial information from simulator [32] along with capabilities of VLM and LLM. Specifically, for each object-goal instance, viewpoint image is sampled to maximize frame coverage. From this sampled image, the names and 2D bounding box coordinates of visible objects are extracted. Then, spatial information is extracted with the BLIP-2 [13] model, while ChatGPT-3.5 is prompted to output the final language description. Splits. GOAT-Bench baselines are trained on Train split, and evaluated on validations splits. Notably, the evaluation splits are divided into Val Seen (i.e., object categories seen during training), Val Seen Synonyms (i.e., object categories that are synonyms to those seen during training) and Val Unseen (i.e., novel object categories). In this section, we provide description of the different baselines for Instance Navigation and Object Navigation used throughout the paper: VLFM (Sec. B.1), Monolithic (Sec. B.2) and PSL (Sec. B.3). B.1. VLFM VLFM [44] is zero-shot state-of-the-art object-goal navigation policy that does not require model training, pre-built maps, or prior knowledge about the environment. The core of the approach involves two maps: frontier map (see in Fig. 3 (a)) and value map (see Fig. 3 (b)). Frontier map. The frontier map is top-down 2D map built from depth and odometry information. The explored area within the map is updated based on the robots location, heading, and obstacles by reconstructing the environment into point cloud with the depth images, and then projecting them onto 2D grid. The role of the frontier map is to identify each boundary separating the explored and unexplored areas, thus identifying the frontiers (see the blue dots in Fig. 3 (a)). Value map. The value map is 2D map similar to the frontier map. For each point within the explored area, value 1 Figure 1. CoIN-Bench can be very challenging when only given the instance category to the agent. We highlight the target instance with red borders, while the distractor instances that exist in the same scene are marked with blue borders. is assigned by quantifying its relevance in locating the target object (see Fig. 3 (b)). At each timestep, frontiers are extracted from the frontier map, and the frontier with the highest value on the value map is selected as the next goal for exploration. To efficiently guide the navigation, VLFM projects the cosine similarity between the current visual observation and textual prompt (e.g., Find the picture) onto the value map. This similarity is computed using the BLIP-2 model [13], which achieves state-of-the-art performance in image-to-text retrieval. To verify whether target instance is present in the current observation, VLFM employs Grounding-DINO [17], an open-vocabulary object detector. Once candidate target is detected, MobileSAM [47] refines the detection by segmenting the objects contour within the bounding box. The segmented contour is paired with depth information to determine the closest point on the object relative to the agents position. This point serves as waypoint for the agent to navigate toward the object. At each timestep, the action at is selected using PointGoal navigation (PointNav) policy [1], which can navigate to either frontier or waypoint, depending on the context. B.2. Monolithic The Monolithic (SenseAct-NN Monolithic Policy) is single, end-to-end reinforcement learning (RL) policy designed for multimodal tasks, leveraging implicit memory and goal encoding proposed in [10]. RGB observations are encoded using frozen CLIP [28] ResNet50 encoder. Figure 2. We show the distribution of categories, categorized for each evaluation split. Figure 3. (a) Frontier map and (b) value map constructed by VLFM [44]. The blue dots in (a) (as well as the red dots in (b)) are the identified frontiers. modal inputs, 1024-dimensional goal embedding is derived using frozen CLIP image or CLIP text encoder, depending on the subtask modality (object, image, or language). All input featuresimage, location, orientation, and goal embeddingare concatenated into an observation embedding, which is processed through two-layer, 512dimensional GRU. At each timestep, the GRU predicts distribution over set of actions based on the current observation and the hidden state. The policy is trained using 4A40 GPUs for approximately 500 million steps. Additionally, the agent integrates GPS and compass inputs, representing location (x, y, z) and orientation (θ). These inputs are embedded into 32-dimensional vectors using encoder with fully connected layers. To model multiB.3. PSL PSL [35] is zero-shot policy for instance navigation, which is pre-trained on the ImageNav task and transferred 3 to achieve object goal navigation without using object annotations for training. Built on top of ZSON [21], observations are processed by learned ResNet50 encoder and frozen CLIP encoder obtaining, respectively, observation embeddings and semantic-level embeddings. To encode the goal modality, an additional frozen CLIP encoder is used, obtaining goal embedding. The goal and the semantic-level embeddings are additionally processed by semantic perception module, which reduces dimension condensing critical information, emphasizing the reasoning of the semantics differences in the goal and observation. Based on condensed embeddings and observation embeddings, the authors trained navigation policy using reinforcement learning. Specifically, the PSL agent is trained for 1G steps following ZSON [21], on 16 Nvidia RTX-3090 GPUs. B.4. User Study Human user evaluation. Our evaluation involves realhuman users to demonstrate the effectiveness of AIUTA when handling various initial user inputs. total of 15 volunteers participated in the study (9 males and 6 females), with ages ranging from 20 to 40 years. All participants have backgrounds in electronic engineering, computer science, or other relevant fields, minimizing expertise barriers to conducting the experiments. At the start of each episode, participants are given an image depicting the final target instance, which remains accessible throughout the experiment. This setup simulates real-world scenario where human has reference image in mind, enabling them to answer questions effectively. The human user then initiates the navigation by sending the initial instruction to the agent by typing initial user input via chatlike User Interface (UI) that we have developed for the evaluation (as demonstrated in the supplementary video, aiuta demo.mp4). Next, the human user is encouraged to respond to the questions posed by AIUTA in natural language and to truthfully reflect the facts about the target instance. For real-human evaluation, we have selected 16 episodes across CoIN-Bench dataset, with each episode evaluated twice using different initial user inputs. Specifically, when the initial user inputs only contain instance category, the human users initiate the navigation via using the fixed template Find the <category>. For the second evaluation, where initial user inputs can contain arbitrary levels of details of the target instance, i.e., , participants are encouraged to compose the input as they prefer after viewing the image of the target instance. The evaluation workload is randomly distributed among participants, with each conducting approximately two evaluations on average. For reproducibility, we provide the list of the 16 episodes and their corresponding initial arbitrary user inputs below. List of episodes and corresponding . We report the episode id for each evaluation split and the corresponding initial instruction . 1 dict( 2 # key as episode id, value as I* 3 val_seen={ 4 5 6 \"79\": \"the bed with soccer themed blanket\", \"30\": \"the blue chair\", \"118\": \"the single white bed, with an orange pillow on it\", \"40\": \"the blue dress with gold ornaments \", }, 8 9 train={ 10 11 12 13 15 \"517\": \"the single bed with no headboard\" , \"821\": \"the bed with dark headboard, multiple pillows on the comforter\", \"738\": \"the dark table\", \"30\": \"the black leather made sofa chair\" , \"134\": \"the white table made of marble\", \"19\": \"the bed with blue blanket on top of it\", }, 16 17 val_unseen={ 18 20 21 \"1\": \"the colored photograph above fireplace\", \"104\": \"the round mirror\", \"2\": \"the black and white photograph of person\", \"69\": \"the black photograph containing mauring quina\", }, 22 23 val_seen_synonyms={ \"45\": \"the brown armchair with floral pattern\", \"43\": \"the purple armchair\" }, 24 26 27 ) C. IDKVQA dataset C.1. Dataset An essential feature of the Self-Questioner is its ability to generate self-questions aimed at extracting additional attributes from the observation Ot and assessing the uncertainty of the VLM. However, there exists no dataset in the such context for us to understand if how reliable technique is for the VLM uncertainty estimation. For this purpose, we introduce IDKVQA, dataset specifically designed and annotated for visual question answering using the agents observations during navigation, where the answer includes not only Yes and No, but also dont know. Specifically, we sample 102 images from the training split of GOAT-Bench, ensuring there is no overlap with the images used in the CoIN-Bench Train evaluation split. Then, for each image, we leverage the SelfQuestioner pipeline to generate set of questions. Each image and is the question. The function g(x) is equal to 1 if the model is answering and 0 if it abstains. The parameter denotes the cost for an incorrect answer, and the VQA accuracy Acc is: Acc(f (x, y)) = min (cid:18) # annotations that match (x) 3 (cid:19) , 1 where the function : output response for each input pair x. Baselines. We evaluate our proposed Normalized Entropy against three baseline methods: (i) MaxProb, which selects the response with the highest predicted probability from the VLM, given image and question q. Formally, = VLM(i, q). It does not incorporate uncertainty estimation. (ii) LP [49], recently proposed Logistic Regression model trained as linear probe on the logits distribution of the first generated token. The model is trained on the Answerable/Unanswerable classification task using the VizWiz VQA dataset [8], which includes 23, 954 images for training. When applied to IDKVQA, the logistic regression model first predicts whether the question is Answerable or Unanswerable. If the question is deemed answerable, the response with the highest probability is selected among {Yes, No}; otherwise, the response dont know is returned. (iii) Energy score, an energy-based framework for outof-distribution (OOD) detection [19]. Following the implementation in [19], an energy score is computed to identify whether the given question-image pair is OOD. If the pair is classified as OOD, the response dont know is returned; otherwise, the response with the highest probability is selected among {Yes, No}. Finally, for our proposed Normalized Entropy estimation, we link the abstention function g(x) (i.e., determining whether the model abstains from answering) to Eq. 3 in the main paper. Specifically, g(x) = 1 if the Normalized Entropy classifies the model as certain, and g(x) = 0 otherwise. Then, if the model is deemed certain, we return the most probable answer {Yes, No}; otherwise, the response dont know is selected. C.3. Sensitivity analysis of the threshold τ This section provides additional details about how small variations of the threshold parameter τ affect both our Normalized Entropy technique (Eq. 3) and the Energy Score [19], with respect to the target metric Φc=1. To conduct this analysis, we perform an ablation study on datasets of varying sizes, obtained by randomly subsampling CoIN-Bench. Specifically, we create five sets containing 50% of the question-answer pairs from CoINBench, five sets comprising 70% of the question-answer Figure 4. Examples from IDKVQA, showing images and the questions generated by the LLM. question is annotated by three annotators, that can pick one answer from the set {Yes, No, dont know}. Fig. 4 illustrates sample images and their questions generated by the Self-Questioner module. C.2. VLM uncertainty estimation on IDKVQA. In this section, we present detailed analysis of VLM uncertainty estimation on IDKVQA, focusing on the evaluation metric and baseline methods. Metric. We evaluate the performance using the Effective Reliability metric Φc proposed in [41]. This metric captures the trade-off between risk and coverage in VQA model by assigning reward to questions that are answered correctly, penalty to questions answered incorrectly, and zero reward to the model abstaining. Formally: Φc(x) = Acc(x), 0, g(x) = 1 and Acc(x) > 0 g(x) = 1 and Acc(x) = 0 g(x) = 0 Here, = (i, q) is the input pair where is the pairs, and also use the full dataset (100%) for total of 11 datasets. For each dataset, we identify the optimal threshold τ for each method through an exhaustive search over predefined ranges, resulting in 22 optimal thresholds (11 per method) Around each τ , we define neighborhood τ comprising 30 new thresholds τ sampled symmetrically around it. Our goal is to analyze how Φc=1 changes across these neighif the values are spread out, it means that the borhoods: method is very sensitive to small changes of τ near the optimal value, whereas if they are more tightly distributed it means that it is more robust. Therefore, for each method and related neighborhood τ , we compute 30 Φc values, one for each τ τ , and normalize them to the range[0, 1] by dividing each value by the best Φc=1 found in τ . We do so to measure only the distribution of the Φc=1 values, not their absolute values, and to help the comparison across datasets of the same size (otherwise, due to chance, they could have distributions of different values). Finally, we aggregate all these normalized Φc=1 scores across dataset size, resulting in Fig.3 (main paper). From the figure, we can see that our technique has smaller interquartile ranges and tighter distributions of Φc=1, while the Energy Score [19] exhibits larger tails, indicating more variance. Moreover, our method shows distributions more biased toward higher values (which would indicate smaller degradation w.r.t. the best Φc=1) than those of the Energy Score, and this gap increases as the dataset size decreases. This shows that our technique is generally more robust, especially in data-scarce situations, and less sensitive to small variations in τ . D. Prompts D.1. Pinit - Initial Description 1 P_init = \"\"\"Describe the {target_object} in the provided image.\"\"\" D.2. Pdetails - Gather Additional Information 10 <START_TARGET_PICTURE_FACTS> 11 {facts_about_the_target_picture} 12 <END_TARGET_PICTURE_FACTS> 13 14 Your task is to: 15 - ask more question to the VQA model on the detected {target_object} to maximize information gain. 16 17 Ensure your output follows the following format: 18 19 YAML_START # must be present to get the information back 20 attributes_of_the_image: <attribute name>: \"<attribute value>\" # summarize all the known attributes from the description, enclosed in \" \" 22 questions: 23 24 YAML_END # must be present to get the information <question_number>: \"<question content>\" back 25 26 Provide your reasoning step-by-step, after the YAML_END tag.\"\"\" D.3. Pcheck - Check detection with LVML 1 P_check = \"\"\"Does the image contain { target_object}? Answer with Yes, No or ?=I dont know.\"\"\" D.4. Pself question - Extract attributes and generate Self-Questions 1 P_ATTRIBUTES_AND_SELF_QUESTIONS = \"\"\" 2 You are an intelligent embodied agent equipped with an RGB sensor, an object detector, and Visual Question Answering (VQA) model. Your task is to explore an indoor environment to find specific target {target_object}. 3 The detector has identified {target_object}. The VQA model has provided the following description of the scene: 4 5 <START_OF_DESCRIPTION> 6 {distractor_object_description} 7 <END_OF_DESCRIPTION> 8 9 Based on your past interactions with the user, 1 P_details = \"\"\"You are an intelligent embodied you know the following facts about the target agent equipped with an RGB sensor, an object detector, and Visual Question Answering ( VQA) model. 2 Your task is to explore an indoor environment to find specific target {target_object}. 3 The detector has identified {target_object}. The VQA model has provided the following description of the scene: 4 5 <START_OF_DESCRIPTION> 6 {distractor_object_description} 7 <END_OF_DESCRIPTION> 8 9 Based on your past interactions with the user, you know the following facts about the target picture: picture: <START_TARGET_PICTURE_FACTS> { facts_about_the_target_picture} < END_TARGET_PICTURE_FACTS> 10 11 Assume that the detected image description contains hallucinations. Your goal is to verify every attribute of the detected { target_object} description through questions. Formally: 12 - Detect possible hallucinations in the VQA model description 13 - Get more information about the detected object. 14 Every question should be in this format: \"< question content>? You must answer only with Yes, No, or ?=I dont know.\" This allows us to assess the likelihood of the answers. 6 15 back 16 17 Ensure your output follows the following format: 18 YAML_START # must be present to get the 28 29 Provide your reasoning step-by-step, after the YAML_END tag.\"\"\" information back 19 attributes_of_the_image: <attribute name>: \"<attribute value>\" # summarize all the known attributes from the description, enclosed in \" \" 21 22 questions_for_detected_object: # question for the detected object, if any 23 <Question number>: answer only with Yes, No, or ?=I dont know.\" \"<question>? You must 24 reasoning_for_detected_object: <Question number>: <reasoning> 25 26 YAML_END # must be present to get the information back 27 28 Provide your reasoning step-by-step, after the YAML_END tag.\"\"\" D.5. Pref ined - Refined image description 1 P_refined = \"\"\" 2 You are an intelligent embodied agent equipped with an RGB sensor, an object detector, and Visual Question Answering (VQA) model. 3 Your task is to refine an image description based on certainty estimates and user interactions . 4 5 Scenario: 6 The detector has identified scene with { target_object}. The VQA model provided this initial scene description: 7 8 <START_OF_DESCRIPTION> 9 {distractor_object_description} 10 <END_OF_DESCRIPTION> 11 12 13 Questions asked and responses: 14 <START_QUESTION_AND_RESPONSES> 15 {list_questions_answers_uncertainty_labels} 16 <END_QUESTION_AND_RESPONSES> 17 18 Task: 19 Using the questions/answer pairs with uncertainty labels, refine the image description. 20 Since we have to find {target_object}, put enphasis on it. Do not include in the description information that is labeled as uncertain. 21 22 Ensure your response follows the format below: 23 YAML_START # must be present to get the information back 24 attributes_of_the_image: 25 <attribute name>: \"<attribute value>\" # summarize all the known attributes from the description, enclosed in \" \" 26 image_description_refined: <insert refined description> not contain newline (n) after the tag image_description_refined: # Ensure that the string does D.6. Pscore - Alignment score 1 P_score = \"\"\" 2 You are an intelligent agent equipped with an RGB sensor, object detector, and Visual Question Answering (VQA) model. 3 Your goal is to identify target {target_object} based on scene description and prior knowledge of the target. 4 5 Scenario: 6 The object detector has identified scene containing {target_object}, and the VQA model has provided the following description: 7 8 <START_OF_DESCRIPTION> 9 {distractor_object_description} 10 <END_OF_DESCRIPTION> 11 12 Target object information: 13 Based on previous interactions, you know the target picture has the following characteristics: 14 <START_TARGET_PICTURE_FACTS> 15 {facts_about_the_target_picture} 16 <END_TARGET_PICTURE_FACTS> 17 18 Task: 19 1. Similarity analysis. 20 Analyze how closely the detected scene description aligns with the known facts about the target {target_object}. Provide similarity score between 0 and 10, where: 21 - 0 = The detected {target_object} is not the target object. 22 - 10 = The detected {target_object} is definitely the target object. 23 - If no information about the target is available , the score should be -1. 24 25 2. Question Generation: 26 - The question is for the target object, not the detected one. 27 - Ask exactly one specific, relevant, and humananswerable question related to the target object that maximizes information gain for identifying the target {target_object}. 28 - Do not ask speculative or irrelevant questions 29 - The question should be grounded in observable or known details from the scene, focusing on key characteristics that can help confirm or refute the identity of the target object. 30 31 Ensure your response follows the format below: 32 YAML_START # must be present to get the information back 33 similarity_score: <similarity score> 34 questions: <question_number>: <question_content> 35 36 YAML_END # must be present to get the information back 27 YAML_END # must be present to get the information 37 7 38 Provide your reasoning step-by-step for the similarity score and questions, after the YAML_END tag.\"\"\" E.2. Self Questioner Algorithm 2 Self Questioner Module Require: Target facts UncerObservation Ot, τ , tainty Pinit, Pdetails, Pcheck, Pself questions, Pref ined object Threshold , 1: Step 1: Detailed Detection Description, from Sinit to"
        },
        {
            "title": "Senriched",
            "content": "2: Initial scene description: Sinit VLM(Ot, Pinit) 3: Self-generate questions to enrich description"
        },
        {
            "title": "Qdetails",
            "content": "aa LLM(Pdetails, Sinit, ) aa do 4: for each question qj in Qdetails raa VLM(Ot, qj) 5: Sinit concatenate(Sinit, raa) 6: 7: Senriched Sinit 8: Step 2: Perception Uncertainty Estimation 9: (rcheck, ucheck) VLM(Ot, Pcheck) Check detection Updated scene description Get answers with uncertainty 10: if NOT (rcheck = Yes AND ucheck = Certain) then return empty string, thus continue exploring 11: 12: Qattribute aa LLM(Pself questions, F, Senriched) Generate self-questions to verify attributes 13: Container {} Store question, answer, uncertainty 14: for each question qj in Qattribute 15: (rj, uj) VLM(Ot, qj) Get answers and uncertainties Container concatenate(Container, {qj, rj, uj}) aa do 16: 17: Step 3: Detection Description Refinement 18: Srefined LLM(Prefined, Container, Senriched) Filter out uncertain attributes 19: return Srefined E. Algorithm the complete AIUTAs algorithm in We first present Sec. E.1. As outlined in the main paper (Sec. 4), AIUTA enriches the zero-shot training policy VLFM [44]. Specifically, we detail the input/output structure of AIUTA regarding VLFM policy π, as well as AIUTAs main component, i.e., the Self Questioner (see Sec. E.2) and the Interaction Trigger (Sec. E.3). E.1. AIUTA Algorithm Algorithm 1 outlines the complete AIUTA pipeline. Upon detecting candidate object, AIUTA first invokes the Self Questioner module (shown in Sec. E.2) to obtain an accurate and detailed understanding of the observed object and to reduce inaccuracies and hallucinations, obtaining refined observation description Sref ined. Then, with the known facts about the target instance and the refined description, AIUTA invokes the Interaction Trigger module (Sec. E.3) for up to 4 iterations rounds (i.e., Max Iteration Number = 4), as specified in Sec. 6 under Implementation Details. Within each interaction round, if AIUTA returns the STOP action, then the policy π terminates the navigation since the target instance is found; otherwise, the policy π continues the navigation process. Algorithm 1 AIUTA Require: Target object facts , Observation Ot, policy π, Candidate Object Detection, Max Iteration number Upon candidate object detection 1: Sref ined Self Questioner(F, Ot) enrich details and reduce inaccuracy, obtain refined description 2: if Sref ined = then 3: π(CONTINUE EXPLORING) VQA detection failed, Signal to policy π to continue exploration 4: for each iteration in Max Iteration Number do 5: 6: 7: aiuta action Interaction Trigger(F, Sref ined) if aiuta action = STOP then π(STOP) Signal to policy π that the object is found! Terminate exploration 8: 9: else π(CONTINUE EXPLORING) Signal to policy π to continue exploration 8 E.3. Interaction Trigger Algorithm 3 Interaction Trigger Require: Target object facts , Refined observation description Srefined, Pscore, τstop and τskip 1: (s, qau) LLM(Pscore, Srefined, ) get alignment score s, and question for the human qau target found, stop navigation. 2: if τstop then return STOP 3: 4: else if < τskip then 5: return CONTINUE EXPLORING skip the question and continue exploring 6: else 7: 8: rua Ask Human(qau) posing clarifying question qau from the agent to the human. Update Facts(F, rua) update target object facts"
        }
    ],
    "affiliations": [
        "Fondazione Bruno Kessler",
        "Polytechnic of Turin",
        "U2IS, ENSTA Paris, Institut Polytechnique de Paris",
        "University of Verona"
    ]
}