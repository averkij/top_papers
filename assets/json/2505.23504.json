{
    "paper_title": "VAU-R1: Advancing Video Anomaly Understanding via Reinforcement Fine-Tuning",
    "authors": [
        "Liyun Zhu",
        "Qixiang Chen",
        "Xi Shen",
        "Xiaodong Cun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video Anomaly Understanding (VAU) is essential for applications such as smart cities, security surveillance, and disaster alert systems, yet remains challenging due to its demand for fine-grained spatio-temporal perception and robust reasoning under ambiguity. Despite advances in anomaly detection, existing methods often lack interpretability and struggle to capture the causal and contextual aspects of abnormal events. This limitation is further compounded by the absence of comprehensive benchmarks for evaluating reasoning ability in anomaly scenarios. To address both challenges, we introduce VAU-R1, a data-efficient framework built upon Multimodal Large Language Models (MLLMs), which enhances anomaly reasoning through Reinforcement Fine-Tuning (RFT). Besides, we propose VAU-Bench, the first Chain-of-Thought benchmark tailored for video anomaly reasoning, featuring multiple-choice QA, detailed rationales, temporal annotations, and descriptive captions. Empirical results show that VAU-R1 significantly improves question answering accuracy, temporal grounding, and reasoning coherence across diverse contexts. Together, our method and benchmark establish a strong foundation for interpretable and reasoning-aware video anomaly understanding. Our code is available at https://github.com/GVCLab/VAU-R1."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 4 0 5 3 2 . 5 0 5 2 : r VAU-R1: Advancing Video Anomaly Understanding via Reinforcement Fine-Tuning Liyun Zhu1,2, Qixiang Chen1 Xi Shen3 Xiaodong Cun2, 2 GVC Lab, Great Bay University 1 Australian National University 3 Intellindust AI Lab {liyun.zhu, u7227010}@anu.edu.au, shenxiluc@gmail.com, cun@gbu.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Video Anomaly Understanding (VAU) is essential for applications such as smart cities, security surveillance, and disaster alert systems, yet remains challenging due to its demand for fine-grained spatio-temporal perception and robust reasoning under ambiguity. Despite advances in anomaly detection, existing methods often lack interpretability and struggle to capture the causal and contextual aspects of abnormal events. This limitation is further compounded by the absence of comprehensive benchmarks for evaluating reasoning ability in anomaly scenarios. To address both challenges, we introduce VAU-R1, data-efficient framework built upon Multimodal Large Language Models (MLLMs), which enhances anomaly reasoning through Reinforcement Fine-Tuning (RFT). Besides, we propose VAUBench, the first Chain-of-Thought benchmark tailored for video anomaly reasoning, featuring multiple-choice QA, detailed rationales, temporal annotations, and descriptive captions. Empirical results show that VAU-R1 significantly improves question answering accuracy, temporal grounding, and reasoning coherence across diverse contexts. Together, our method and benchmark establish strong foundation for interpretable and reasoning-aware video anomaly understanding. Our code is available at https://github.com/GVCLab/VAU-R1. Figure 1: Effectiveness of Reinforcement Fine-Tuning. We compare QA accuracy and temporal anomaly grounding performance across different models. VAU-R1, trained via Reinforcement Fine-Tuning (RFT), consistently outperforms its Supervised Fine-Tuning (SFT) counterpart. This demonstrates that RFT enhances both reasoning and temporal localization capabilities in VAU tasks."
        },
        {
            "title": "Introduction",
            "content": "Anomalies are events or behaviors that deviate from regular patterns or expected activities in In surveillance settings, these may include incidents such as fighting, theft, or given context. Work done while the author was visiting student at GVC Lab, Great Bay University. Corresponding Author Preprint. Under review. Figure 2: Overview of VAU-R1. VAU-R1 leverages Reinforcement Fine-Tuning to enhance the reasoning ability of MLLMs for video anomaly understanding. Specifically, we adopt Group Relative Policy Optimization (GRPO) to optimize the model with task-specific rewards, such as answer format, accuracy, and temporal Intersection-over-Union (IoU). We decompose the VAU task into four complementary tasks to facilitate comprehensive reasoning: multiple-choice QA, temporal anomaly grounding, anomaly reasoning, and anomaly classification. traffic violations, etc. Video Anomaly Understanding (VAU) aims to detect and interpret such irregular events in unstructured, real-world video streams [22]. The task is challenging due to scene complexity, context dependence, varying camera viewpoints, and diverse anomaly types [27, 44, 56]. Early approaches only focuses on detecting anomalies, which typically framed the task as binary classification, assigning normal or abnormal labels to individual frames and identifying the temporal boundaries of anomalous events [5, 6, 11, 16, 21, 33, 36, 47, 55]. While effective for localization, these methods offer limited interpretability and provide little insight into the underlying causes of anomalies [7, 8, 49]. Recent advances in Multi-modal Large Language Models (MLLMs) have introduced the ability to generate textual descriptions of anomalous events [9, 48, 51, 52, 53], improving model transparency to some extent. However, current methods still face three key limitations: (i) they lack the ability to generate coherent, multi-step reasoning chains; (ii) no comprehensive benchmark provides rich annotations to support detailed causal reasoning; and (iii) evaluation protocols for reasoning quality remain underdeveloped. To move beyond shallow classification and toward deeper understanding, we decompose VAU into four progressive stages: (i) Perception identifying the scene and relevant objects, either through free-text descriptions or guided multiple-choice questions; (ii) Grounding localizing the precise temporal segment where the anomaly occurs; (iii) Reasoning explaining the event by analyzing causal factors, temporal dynamics, and contextual cues; and (iv) Conclusion summarizing the event with final decision, such as assigning it to specific category (e.g., fighting vs. robbery). This structured formulation enables models to progressively build semantic understanding and supports more interpretable and task-aligned evaluation. To implement this four-stage formulation, we introduce VAU-R1, Reinforcement Fine-Tuning (RFT) framework designed to improve the reasoning capabilities of MLLMs on the VAU task. Our method builds on Group Relative Policy Optimization (GRPO) [31], incorporating task-specific reward signals based on answer format correctness, question-answer accuracy, and temporal grounding alignment. The framework is data-efficient and can be applied in low-resource settings, making it practical for real-world deployments. To support training and evaluation, we also construct VAUBench, new benchmark that spans diverse scenarios and provides rich annotations across the four reasoning stages, including multiple-choice QA pairs, detailed event descriptions, temporal groundings, and step-by-step rationales. Finally, we propose set of evaluation metricsQA 2 accuracy, temporal Intersection-over-Union (IoU), GPT-based reasoning score, and classification accuracyto quantitatively assess model performance across perception, grounding, reasoning, and conclusion. Together, VAU-R1 and VAU-Bench offer scalable and unified framework for advancing structured video anomaly understanding. Our contribution can be summarized as follows: We propose VAU-R1, data-efficient Reinforcement Fine-Tuning framework that improves the reasoning ability of MLLMs for video anomaly understanding. It outperforms standard supervised fine-tuning on reasoning-intensive tasks. We present VAU-Bench, the first large-scale benchmark with Chain-of-Thought annotations designed for video anomaly reasoning. It contains diverse collection of videos, QA pairs, temporal labels, and detailed rationales spanning wide range of real-world scenarios. We design unified evaluation protocol that measures model performance across four reasoning stages, jointly considering reasoning quality, answer correctness, and temporal localization to capture both interpretability and detection precision."
        },
        {
            "title": "2 Related Works",
            "content": "From Detection to Understanding. Early efforts in Video Anomaly Detection (VAD) can be broadly categorized into self-supervised and weakly-supervised paradigms. Self-supervised methods rely solely on normal video samples, learning the distribution of normal behavior and flagging deviations as anomalies [11, 21, 25]. In contrast, weakly-supervised methods are trained with both normal and anomalous videos using coarse video-level labels rather than fine-grained frame-level annotations [5, 16, 33, 36, 45, 47, 55]. These approaches typically adopt top-k selection strategy to identify the most likely anomalous segments. While effective for localizing anomaly boundaries, they often rely heavily on motion cues [56], operating under the assumption that rapid or irregular motion is indicative of anomalous behavior. However, this assumption does not hold for subtle or semantically complex anomalies, leading to poor interpretability. To address these limitations, recent work has turned to video anomaly understanding, leveraging MLLMs to provide more semantically grounded and interpretable reasoning [26]. Prompt-Based vs. Learning-Based Approaches for VAU. Building on the shift toward semantic understanding, recent approaches to VAU fall into two main categories: prompt-based and learningbased methods. Prompt-based methods typically use MLLMs as anomaly scorers [30, 51], or as reasoning agents via rule-based few-shot prompting [48] or learned question templates [49]. While these methods avoid computationally expensive training, their generalization ability is often limited due to the absence of task-specific adaptation. On the other hand, pretraining [8] and finetuning [52, 53] approaches aim to learn anomaly-aware representations by incorporating video captions and causal reasoning signals (e.g., cause and effect). Despite this progress, existing methods remain constrained to improving anomaly description and fail to capture the full logical chain of an anomaly. To overcome these limitations, we leverage reinforcement fine-tuning to enhance the models reasoning ability, enabling end-to-end identification of both when and why anomalies occur. Reinforcement Learning in MLLMs. With the rise of powerful models such as OpenAI-o1 [15] and DeepSeek-R1 [12], reinforcement learning has been increasingly adopted in the post-training stage of MLLMs to enhance their reasoning capabilities [3, 10, 14, 42, 54]. While effective, this process often demands substantial computational resources and large-scale datasets, making it less practical for targeted downstream tasks [34]. To address these challenges, Visual-RFT [23] introduces Reinforcement Fine-Tuning (RFT) for visual tasks, demonstrating improved data efficiency and stronger performance compared to Supervised Fine-Tuning (SFT). Building on this idea, VideoChatR1 [17] extends RFT to video domains, achieving promising results in tasks such as question answering, temporal grounding, and object tracking. Yet, these tasks remain fragmented and have not been unified under the video anomaly understanding setting. To bridge this gap, we propose framework that jointly addresses multiple tasks, aiming to advance comprehensive and interpretable anomaly reasoning."
        },
        {
            "title": "3 Methodology",
            "content": "3.1 Preliminary: Reinforcement Learning via Group Relative Policy Optimization Group Relative Policy Optimization (GRPO) [31] is reinforcement learning framework that optimizes policy πθ using preference-based feedback and multi-aspect reward signals. Given question x, GRPO generates candidate outputs = {o1, o2, . . . , oM } from the old policy πθold, each output oj assigned reward Rj computed as weighted sum of task-specific components: Rj = (cid:88) k=1 λkR(k) , (1) where R(k) is the k-th task-specific reward (e.g., accuracy, IoU, format compliance) and λk is its weight. To measure the relative quality of the j-th output, we calculate the normalized reward Rj for each output oj with the mean µR and standard deviation σR across candidates: GRPO maximises the following objective while keeping the update close to the original MLLM parameters πref through KL penalty term DKL( ): Rj = Rj µR σR . (2) max πθ EOπθold (x) (cid:88) j=1 Rj β DKL (πθ πref) , πθ(oj) πθold(oj) (3) where β is regularization coefficient. This formulation allows GRPO to incorporate diverse reward signals while retaining training stability through KL regularization. 3.2 VAU-R1 As shown in Figure 2, VAU-R1 is data-efficient reinforcement fine-tuning framework designed for the four VAU tasks, including Multi-choice QA, Temporal Anomaly Grounding, Anomaly Reasoning, and Anomaly Classification. Given videos and task-specific questions, we fine-tune pre-trained MLLM to improve its multi-step reasoning ability across different tasks. The model generates multiple candidate responses for each input, which are then scored using task-specific reward functions (e.g., accuracy, temporal IoU, or format compliance). We employ Group Relative Policy Optimization (GRPO) to optimize the model, which maximizes reward-weighted likelihood while constraining divergence from the reference model via KL regularization. Our reinforcement-based approach outperforms supervised fine-tuning (SFT) in both reasoning capability and generalization to unseen scenarios. The design of task-specific reward functions is further detailed in Section 3.3. 3.3 Reward Rules We adopt the general idea of GRPO-based RFT to optimize the VAU model by designing task-specific reward functions for different VAU components. Below, we detail each reward definition. Format Reward. For multiple-choice QA and anomaly classification tasks, we instruct the model to enclose its reasoning within <think>...</think> tags and the answer within <answer>...</answer> tags. For the temporal anomaly grounding task, we additionally require <glue>...</glue> tags to indicate the predicted time span in seconds. The reward is defined as: Rformat = (cid:26)1, if the output format is correct, 0, otherwise. (4) We apply format reward to VAU tasks to enforce structured outputs and discourage format violations. Accuracy Reward. We also define an accuracy reward Racc to measure the correctness of the models answer. In our experiments, this reward is given by: Racc = (cid:26)1, if output = ground truth, 0, otherwise. (5) 4 (a) (b) (c) Figure 3: Statistics of our VAU-Bench. (a) Distribution of main anomaly types. (b) Distribution of video durations (top) and the proportion of anomalous segments within each video (bottom). (c) The evaluation criteria for four VAU tasks. This simple accuracy reward encourages the model to choose the right answer during training. Temporal IoU Reward. To encourage precise temporal grounding, we introduce temporal Intersection-over-Union (IoU) reward RtIoU, which measures the alignment between the predicted and ground truth anomaly intervals. The reward is defined as: RtIoU = 1, IoU([s1, s2], [s 0, 1, 2]), if the model correctly classifies normal video; if the model correctly detects an anomaly and predicts [s1, s2]; otherwise. (6) Here, [s1, s2] denotes the predicted temporal span of the anomaly, while [s 2] is the ground truth interval. The temporal IoU quantifies the degree of overlap between these intervals, and serves as fine-grained reward signal to guide the model toward more accurate temporal localization. 1, Task-specific Reward Formulations. We apply task-specific combinations of the reward components mentioned above. For the multiple-choice QA task, we use combination of format and accuracy rewards: RQA = Rformat + Racc. For temporal anomaly grounding, we further include temporal IoU term to evaluate localization quality: RTAG = Rformat + Racc + RtIoU. For anomaly classification, we adopt similar reward design as QA: RCLS = Rformat + Racc. 3.4 VAU-Bench Task Definition. We decompose the VAU task into four stages: perception, grounding, reasoning, and conclusion. These stages address four core questions respectively: \"What happens in this video?\", \"When does the anomaly occur?\", \"Why does the anomaly happen?\", and \"What is our overall judgment of the anomaly?\". Corresponding to these stages, we define four VAU tasks: Multiple-Choice QA: Targets event perception by answering questions about videos. Temporal Grounding: Localizes anomalous segments in the video timeline. Anomaly Reasoning: Explores causal relationships to explain why an anomaly arises. 5 Table 1: Comparison of performance on MSAD and UCF-Crime datasets on multiple-choice QA task and anomaly reasoning task. Accw/o think and Accw/ think refer to the multiple-choice question accuracy without and with thinking, respectively. For the anomaly reasoning task, CLS, KM, FLU, INF, and FAC represent VAU-Eval scores generated by DeepSeek-V3, measuring classification accuracy, key concept alignment, linguistic fluency, informativeness, and factual consistency, respectively. Each dimension is scored on 10-point scale. Total denotes the aggregated score over five dimensions. Dataset MSAD QA Accuracy VAU-Eval Model Accw/o think InternVL2.5-2B 76.67 84.58 Qwen2.5-VL-7B InternVL2.5-8B-MPO 82.50 Accw/ think 72.08 83.33 84. CLS KM FLU INF FAC Total 6.84 6.75 6.83 6.23 6.41 6. 8.55 9.27 8.32 6.64 7.74 6.37 6.64 6.92 6.86 34.90 37.08 34.72 Qwen2-VL-2B +SFT +RFT Qwen2.5-VL-3B +SFT +RFT 5.43 77.08 82.92 5.43 82.92 (5.84) 83.75 (11.25) 6.05() 5.49() 8.89() 6.50() 6.05() 32.98() 32.25 32.84 72.50 85.83 5.94 6.04 6.29 6.55 8.77 8. 5.90 5.93 85.83 86.25 88.33 (2.50) 87.08 (4.58) 82.50 84.58 5.77 2.89 5.97() 5.49() 9.05() 6.84() 6.03() 33.38() 32.47 15.96 6.74 3. 9.02 4.89 5.24 2.22 5.70 2.44 84.86 InternVL2.5-2B Qwen2.5-VL-7B 92.03 InternVL 2.5 8B-MPO 89.64 68.13 89.64 90.44 4.40 4.80 3. 3.08 3.73 3.20 8.09 8.95 8.23 5.69 7.05 5.77 3.47 4.25 3.48 24.74 28.78 24.47 UCF-Crime Qwen2-VL-2B +SFT +RFT Qwen2.5-VL-3B +SFT +RFT 87.25 83.67 88.45 (1.20) 88.05 (4.38) 83.67 86.06 91.63 90.84 92.03 (0.40) 91.63 (8.36) 83.27 90. 3.47 3.61 4.04() 2.75() 7.72() 4.89() 3.11() 22.52() 21.02 20.66 7.75 7.30 4.49 4.79 2.48 2.26 2.82 2. 4.31 1.80 4.42() 2.98() 8.71() 5.98() 3.39() 25.49() 25.10 10.89 5.95 2.82 8.70 4.15 2.88 1.01 3.27 1. Anomaly Classification: Assigns the anomaly to its corresponding category. This structured decomposition provides clear framework for systematically addressing different perspectives of VAU, with each task rigorously evaluated using domain-specific metrics. Dataset Construction and Annotation. Existing video anomaly datasets typically provide only frame-level labels [1, 33, 56] or sparse descriptions [8, 9, 50], limiting their usefulness for reasoningbased tasks. To address this, we construct VAU-Bench, unified benchmark built from MSAD [56], UCF-Crime [33], and ECVA [8], enriched with Chain-of-Thought (CoT) annotations, including: (i) video descriptions, (ii) temporal boundaries, (iii) multiple-choice QA, and (iv) reasoning rationales. We apply cleaning pipeline to remove corrupted or overly long videos and merge overlapping anomaly types. For UCF-Crime and ECVA, we use DeepSeek-V3 [18] to generate video-level summaries, QA pairs, and reasoning chains. For MSAD, CoT annotations are produced through two-stage pipeline: we first apply InternVL-8B-MPO [42] to generate initial captions and analyses, which are then verified and refined using DeepSeek-V3 to obtain more accurate QA pairs and coherent reasoning rationales. We also give further construction and annotation details in the Appendix. Dataset Statistics. Figure 3 presents an overview of VAU-Bench, the first VAU benchmark designed for Chain-of-Thought reasoning. Our dataset contains 4,602 videos covering 19 major anomaly types, with total duration of 169.1 hours. It includes over 1.5 million words of fine-grained textual annotations, averaging 337 words per video, encompassing detailed descriptions, reasoning rationales, and multiple-choice questions. The dataset is split into 2,939 training, 734 validation, and 929 test videos. Additionally, we provide 3,700 temporal annotations to support the anomaly grounding task. Figure 3a shows the distribution of the main anomaly categories, while Figure 3b illustrates the diversity in video duration and anomaly sparsity. The evaluation protocols and metrics used for different tasks are summarized in Figure 3c, and we give more dataset statistics in the Appendix. Reasoning Evaluation Metric: VAU-Eval. For VAU tasks, prior work has adopted BLEU and ROUGE [8, 35, 53] to evaluate semantic content. However, such n-gram-based metrics often fall short in capturing reasoning quality and deeper relational understanding. To better assess anomaly reasoning, we propose VAU-Eval, GPT-based metric that compares model-generated descriptions and analyses with ground truth annotations. As illustrated in Figure 3c, we evaluate each response along five dimensions using DeepSeek-V3 [18] as the judge: classification accuracy, key concept Table 2: Comparison of temporal anomaly grounding performance on the three datasets. For each dataset, we present results for the base models, followed by SFT and RFT variants. w/o think and w/ think refer to the inference prompt without and with thinking, respectively. Rows highlighted in light yellow denote the results on the UCF-Crime dataset, serving as an out-of-distribution test for cross-dataset evaluation. Dataset Model w/o think w/ think mIoU R@0.3 R@0.5 R@0.7 mIoU R@0.3 R@0.5 R@0.7 MSAD ECVA UCF-Crime Qwen2-VL-2B 0.00 Qwen2.5-VL-7B 45. 0.00 70.83 30.00 Qwen2.5-VL-3B 21.27 30.65 + SFT 47.50 35.77 (14.50) 53.33 + RFT Qwen2-VL-2B 0.00 Qwen2.5-VL-7B 19.85 0.00 25.87 17.16 Qwen2.5-VL-3B 14.21 45.30 + SFT 66.67 35.09 (20.88) 49.00 + RFT Qwen2-VL-2B 2.74 Qwen2.5-VL-7B 22. Qwen2.5-VL-3B 10.91 4.98 + SFT 16.80 (5.89) + RFT 4.84 33.87 15.32 3.23 23.39 0.00 45.83 10.83 30.00 34.17 0.00 15. 6.47 49.75 28.86 0.00 16.13 6.45 0.81 8.06 0.00 0.00 21.67 17.57 0.00 26.67 13.00 35. 16.67 4.17 9.17 50.83 15.83 30.70 (17.70) 48.33 0.00 9.70 0.17 5.71 0.30 7.96 7.21 3.23 6.35 24.13 45.96 65.67 19.40 33.25 (26.90) 48.51 0.00 8. 3.23 0.00 4.03 0.12 4.89 7.68 5.76 9.21 (1.53) 0.00 8.06 10.48 5.65 9.68 0.00 11. 5.83 34.17 29.17 0.00 4.73 1.99 51.00 30.60 0.00 1.61 4.84 0.81 4.03 0.00 3. 1.67 15.00 12.50 0.00 2.99 0.50 26.12 18.41 0.00 0.00 1.61 0.81 1.61 alignment, fluency, informativeness, and factual consistency. Each dimension is scored on 10-point scale to provide fine-grained assessment of reasoning quality."
        },
        {
            "title": "4 Experiment",
            "content": "Implementation Details. Our main experiments are conducted using the Qwen2-VL-2B-Instruct [41] and Qwen2.5-VL-3B-Instruct [2] models. We apply full-parameter fine-tuning without adapters or LoRA, using 2 NVIDIA H20 GPUs for training. During the RFT training process, we adopt structured prompting strategy that guides the model to generate intermediate reasoning and final answers in standardized format. Specifically, each prompt instructs the model to enclose its reasoning process within <think>...</think> tags and its final answer within <answer>...</answer> tags. This format ensures consistency across different tasks. During inference, for Qwen-VL models, we sample frames at 1 FPS. For InternVL models, we uniformly sample 16 frames per video. 4.1 Evaluation of VAU-R1 Evaluation Protocol. We report results separately on the MSAD, ECVA, and UCF-Crime datasets rather than using single aggregated benchmark, as these datasets differ substantially in anomaly types, video durations, and scene contexts. All evaluation metrics for our four tasks are summarized in Figure 3c. For the QA task, we report multiple-choice accuracy. Temporal anomaly grounding is evaluated using temporal mean Intersection over Union (mIoU), as well as recall at different IoU thresholds: R@0.3, R@0.5, and R@0.7. For anomaly reasoning, we adopt the GPT-based VAU-Eval introduced in Section 3.4. Finally, binary and multi-class classification accuracy are used for evaluating the anomaly classification task. Evaluation on QA-Guided Reasoning. As shown in Table 1, we evaluate the reasoning capabilities of VAU-R1 on MSAD and UCF-Crime using multiple-choice QA accuracy and GPT-based VAU-Eval scores. We highlight two key observations. First, base models often perform worse when generating answers with reasoning (Accw/think) compared to without (Accw/o think) reasoning, indicating that naive Chain-of-Thought generation may introduce hallucination. In contrast, reinforcement fine-tuning (RFT) improves both QA accuracy with reasoning (e.g., +11.25 on MSAD) and overall reasoning quality. Second, RFT leads to consistent gains across five VAU-Eval dimensionsclassification, demonstrating its ability to strengthen structured reasoning. For instance, on MSAD, Qwen2.5-VL3B+RFT achieves the highest total VAU-Eval score (33.38), showing substantial improvement over 7 Table 3: Ablation study on task co-training for anomaly classification. Bin. Acc. denotes binary classification accuracy (normal vs. abnormal), and Multi Acc. denotes multi-class accuracy over 19 anomaly types plus the normal class. Results are reported with and without think prompting. Model w/o think w/ think Bin. Acc. Multi Acc. Bin. Acc. Multi Acc. Baseline (Qwen2.5-VL-3B-Instruct) +SFT w/ CLS +RFT w/ CLS +RFT w/ QA +RFT w/ TAG +RFT w/ QA-TAG +RFT w/ QA-TAG-CLS 62.77 81.12 60.30 59.01 67.81 65.77 64. 47.96 29.08 46.14 46.14 49.46 47.53 48.61 59.33 83.37 59.01 58.91 74.14 67.60 65.02 39.06 32.19 42.27 41.95 46.14 45.06 45.60 its SFT counterpart. These results confirm that RFT not only enhances answer correctness but also fosters robust and generalizable multimodal reasoning under the VAU setting. Evaluation on Temporal Anomaly Grounding. As shown in Table 2, we evaluate the temporal anomaly grounding performance across three datasets. Note that all models are trained only on MSAD and ECVA, while UCF-Crime serves as an out of distribution test set. We observe several key findings. First, across both inference settings (w/ and w/o think), RFT consistently outperforms the corresponding base models, demonstrating its effectiveness in improving temporal localization. Notably, the RFT-finetuned 3B model achieves higher mIoU than the larger 7B base model on ECVA. Second, similar to our observations in QA-guided reasoning, Chain-of-Thought prompting does not necessarily enhance grounding performance. In some cases, adding reasoning leads to degraded localization accuracy. Third, RFT shows significantly better generalization compared to SFT. In cross-dataset evaluation (e.g., UCF-Crime as an out-of-distribution test), SFT demonstrates limited generalization, whereas RFT maintains strong performance across unseen scenarios. While SFT occasionally outperforms RFT in isolated cases, we observe that its direct predictions are opaque and lack interpretability, often yielding repetitive, non-discriminative outputs across videos (see Figure 4). These results highlight the advantages of RFT for enhancing generalization in VAU tasks. Ablation Study. For VAU, the core objective is to make accurate high-level judgments about anomaly categories (e.g., distinguishing fight from robbery). To explore effective task formulations, we train models with different combinations of VAU tasksmultiple-choice QA, temporal anomaly grounding (TAG), and multi-class classification (CLS)to assess their impact on reasoning. As shown in Table 3, RFT models trained with TAG alone achieve the highest binary accuracy (74.14) and strong multi-class performance (46.14) under the think setting, highlighting the benefit of temporal grounding for perception and category discrimination. Combining QA and TAG also improves performance but is slightly less effective than TAG alone. In contrast, SFT tends to over-predict anomalies, yielding high binary accuracy but poor multi-class results, suggesting overfitting. Overall, grounding-based tasks are more effective for anomaly classification, and jointly optimizing tasks via reinforcement learning yields complementary gains in both accuracy and reasoning. Case Study. Figure 4 illustrates two representative examples from the QA and TAG tasks, comparing SFT and our VAU-R1 under the same Chain-of-Thought (CoT) prompt. In the QA example, SFT incorrectly selects normal explanation based on surface cues, while VAU-R1 correctly infers people-falling anomaly by identifying posture and behavioral irregularities. In the TAG example, SFT outputs coarse anomaly span without rationale, whereas VAU-R1 localizes the anomaly more precisely (0.013.6s) and provides an interpretable causal chain. These cases highlight VAU-R1s superior reasoning and interpretability in both classification and localization settings. More qualitative case studies are provided in the Appendix. 4.2 Discussion RFT Enhances Generalization and Interpretability. Our experiments demonstrate that RFT consistently outperforms SFT across multiple VAU tasks, offering improved interpretability  (Table 1)  and better generalization  (Table 2)  . In contrast, SFT tends to memorize task-specific patterns and suffers from poor generalization to unseen scenarios. This suggests that SFT-trained models are more prone to overfitting, especially when trained on limited or narrowly defined tasks. Is Chain-of-Thought Reasoning Necessary for VAU? Our findings suggest that Chain-of-Thought (CoT) reasoning does not always lead to better performance in visual understanding tasks. However, 8 Figure 4: Qualitative case of the QA (top) and TAG (bottom) task. All ground-truths and correct answers are highlighted in orange. Both SFT and RFT perform inference using the same CoT prompt. RFTs explicit chain-of-thought yields precise, interpretable QA choice and anomaly interval, whereas SFTs output is less informative and tends to produce inaccurate responses. it significantly enhances interpretability by providing structured justifications. Unlike mathematical or logical tasks, where reasoning is more deterministic, visual understanding involves inherently diverse reasoning paths. Therefore, designing simpler sub-tasks with well-defined reward signals to guide reasoning effectively remains underexplored. Directly applying complex tasks (e.g., multi-class anomaly classification) without task co-training often leads to suboptimal results  (Table 3)  . Rethinking Anomaly Understanding in Multimodal Contexts. VAU calls for constructing coherent reasoning chain that bridges spatial-temporal localization and causal inference. Yet, leveraging diverse cues such as keyframes, salient objects, and even additional modalities (e.g., audio) to support unified reasoning remains underexplored. We envision that future work could benefit from integrating these multimodal signals into structured reasoning framework, enabling more robust and interpretable anomaly understanding. Our method and benchmark take step in this direction by proposing unified evaluation protocol across perception, localization, and reasoning dimensions, ultimately guiding models toward accurate and justifiable anomaly judgments."
        },
        {
            "title": "5 Conclusion",
            "content": "We present VAU-R1, an advanced and unified Video Anomaly Understanding framework focusing on four VAU tasks: multi-choice QA, temporal grounding, anomaly reasoning, and classification. VAU-R1 leverages multimodal large language model (MLLM) and, notably, employs reinforcement fine-tuning to enhance anomaly reasoning and explainability via carefully designed GRPO reward functions for each task. To facilitate the training and evaluation of this framework, we also introduce VAU-Bench, the first chain-of-thought benchmark designed to train and evaluate VAU tasks at the 9 reasoning level. The experiments on different tasks prove the strong performance of the proposed method than baselines."
        },
        {
            "title": "References",
            "content": "[1] A. Acsintoae, A. Florescu, M.-I. Georgescu, T. Mare, P. Sumedrea, R. T. Ionescu, F. S. Khan, and M. Shah. Ubnormal: New benchmark for supervised open-set video anomaly detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2014320153, 2022. [2] S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, H. Zhong, Y. Zhu, M. Yang, Z. Li, J. Wan, P. Wang, W. Ding, Z. Fu, Y. Xu, J. Ye, X. Zhang, T. Xie, Z. Cheng, H. Zhang, Z. Yang, H. Xu, and J. Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [3] J. Bi, S. Liang, X. Zhou, P. Liu, J. Guo, Y. Tang, L. Song, C. Huang, G. Sun, J. He, et al. Why reasoning matters? survey of advancements in multimodal reasoning (v1). arXiv preprint arXiv:2504.03151, 2025. [4] C. Cao, Y. Lu, P. Wang, and Y. Zhang. new comprehensive benchmark for semi-supervised video anomaly detection and anticipation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2039220401, 2023. [5] Y. Chen, Z. Liu, B. Zhang, W. Fok, X. Qi, and Y.-C. Wu. Mgfn: Magnitude-contrastive glanceand-focus network for weakly-supervised video anomaly detection. In Proceedings of the AAAI conference on artificial intelligence, volume 37, pages 387395, 2023. [6] D. Ding, L. Wang, L. Zhu, T. Gedeon, and P. Koniusz. Lego: Learnable expansion of graph operators for multi-modal feature fusion. arXiv preprint arXiv:2410.01506, 2024. [7] K. Doshi and Y. Yilmaz. Towards interpretable video anomaly detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 26552664, 2023. [8] H. Du, G. Nan, J. Qian, W. Wu, W. Deng, H. Mu, Z. Chen, P. Mao, X. Tao, and J. Liu. Exploring what why and how: multifaceted benchmark for causation understanding of video anomaly. arXiv preprint arXiv:2412.07183, 2024. [9] H. Du, S. Zhang, B. Xie, G. Nan, J. Zhang, J. Xu, H. Liu, S. Leng, J. Liu, H. Fan, et al. Uncovering what why and how: comprehensive benchmark for causation understanding of video anomaly. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1879318803, 2024. [10] K. Feng, K. Gong, B. Li, Z. Guo, Y. Wang, T. Peng, B. Wang, and X. Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. [11] D. Gong, L. Liu, V. Le, B. Saha, M. R. Mansour, S. Venkatesh, and A. v. d. Hengel. Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 17051714, 2019. [12] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [13] K. Hara, H. Kataoka, and Y. Satoh. Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet? In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 65466555, 2018. [14] W. Huang, B. Jia, Z. Zhai, S. Cao, Z. Ye, F. Zhao, Z. Xu, Y. Hu, and S. Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. [15] A. Jaech, A. Kalai, A. Lerer, A. Richardson, A. El-Kishky, A. Low, A. Helyar, A. Madry, A. Beutel, A. Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. 10 [16] J. Leng, Z. Wu, M. Tan, Y. Liu, J. Gan, H. Chen, and X. Gao. Beyond euclidean: Dual-space representation learning for weakly supervised video violence detection. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [17] X. Li, Z. Yan, D. Meng, L. Dong, X. Zeng, Y. He, Y. Wang, Y. Qiao, Y. Wang, and L. Wang. Videochat-r1: Enhancing spatio-temporal perception via reinforcement fine-tuning. arXiv preprint arXiv:2504.06958, 2025. [18] A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [19] K. Liu, W. Liu, C. Gan, M. Tan, and H. Ma. T-c3d: Temporal convolutional 3d network for real-time action recognition. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. [20] K. Liu and H. Ma. Exploring background-bias for anomaly detection in surveillance videos. In Proceedings of the 27th ACM International Conference on Multimedia, pages 14901499, 2019. [21] W. Liu, W. Luo, D. Lian, and S. Gao. Future frame prediction for anomaly detectiona new baseline. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 65366545, 2018. [22] Y. Liu, D. Yang, Y. Wang, J. Liu, J. Liu, A. Boukerche, P. Sun, and L. Song. Generalized video anomaly event detection: Systematic taxonomy and comparison of deep models. ACM Computing Surveys, 56(7):138, 2024. [23] Z. Liu, Z. Sun, Y. Zang, X. Dong, Y. Cao, H. Duan, D. Lin, and J. Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025. [24] C. Lu, J. Shi, and J. Jia. Abnormal event detection at 150 fps in matlab. In Proceedings of the IEEE international conference on computer vision, pages 27202727, 2013. [25] Y. Lu, F. Yu, M. K. K. Reddy, and Y. Wang. Few-shot scene-adaptive anomaly detection. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part 16, pages 125141. Springer, 2020. [26] H. Lv and Q. Sun. Video anomaly detection and explanation via large language models. arXiv preprint arXiv:2401.05702, 2024. [27] G. Pang, C. Shen, L. Cao, and A. V. D. Hengel. Deep learning for anomaly detection: review. ACM computing surveys (CSUR), 54(2):138, 2021. [28] B. Ramachandra and M. Jones. Street scene: new dataset and evaluation protocol for video anomaly detection. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 25692578, 2020. [29] R. Rodrigues, N. Bhargava, R. Velmurugan, and S. Chaudhuri. Multi-timescale trajectory prediction for abnormal human activity detection. In The IEEE Winter Conference on Applications of Computer Vision (WACV), March 2020. [30] Y. Shao, H. He, S. Li, S. Chen, X. Long, F. Zeng, Y. Fan, M. Zhang, Z. Yan, A. Ma, et al. Eventvad: Training-free event-aware video anomaly detection. arXiv preprint arXiv:2504.13092, 2025. [31] Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [32] K. Simonyan and A. Zisserman. Two-stream convolutional networks for action recognition in videos. Advances in neural information processing systems, 27, 2014. [33] W. Sultani, C. Chen, and M. Shah. Real-world anomaly detection in surveillance videos. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 64796488, 2018. 11 [34] H. Tan, Y. Ji, X. Hao, M. Lin, P. Wang, Z. Wang, and S. Zhang. Reason-rft: Reinforcement fine-tuning for visual reasoning. arXiv preprint arXiv:2503.20752, 2025. [35] J. Tang, H. Lu, R. Wu, X. Xu, K. Ma, C. Fang, B. Guo, J. Lu, Q. Chen, and Y. Chen. Hawk: Learning to understand open-world video anomalies. Advances in Neural Information Processing Systems, 37:139751139785, 2024. [36] Y. Tian, G. Pang, Y. Chen, R. Singh, J. W. Verjans, and G. Carneiro. Weakly-supervised video anomaly detection with robust temporal feature magnitude learning. In Proceedings of the IEEE/CVF international conference on computer vision, pages 49754986, 2021. [37] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri. Learning spatiotemporal features In Proceedings of the IEEE international conference on with 3d convolutional networks. computer vision, pages 44894497, 2015. [38] M. Vijay, W.-X. LI, B. Viral, and V. Nuno. Anomaly detection in crowded scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19751981, 2010. [39] L. Wang, W. Li, W. Li, and L. Van Gool. Appearance-and-relation networks for video classification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 14301439, 2018. [40] L. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and L. Van Gool. Temporal segment networks: Towards good practices for deep action recognition. In European conference on computer vision, pages 2036. Springer, 2016. [41] P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K. Chen, X. Liu, J. Wang, W. Ge, Y. Fan, K. Dang, M. Du, X. Ren, R. Men, D. Liu, C. Zhou, J. Zhou, and J. Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [42] W. Wang, Z. Chen, W. Wang, Y. Cao, Y. Liu, Z. Gao, J. Zhu, X. Zhu, L. Lu, Y. Qiao, et al. Enhancing the reasoning ability of multimodal large language models via mixed preference optimization. arXiv preprint arXiv:2411.10442, 2024. [43] X. Wang, R. Girshick, A. Gupta, and K. He. Non-local neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 77947803, 2018. [44] P. Wu, C. Pan, Y. Yan, G. Pang, P. Wang, and Y. Zhang. Deep learning for video anomaly detection: review. arXiv preprint arXiv:2409.05383, 2024. [45] P. Wu, X. Zhou, G. Pang, Y. Sun, J. Liu, P. Wang, and Y. Zhang. Open-vocabulary video anomaly detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1829718307, 2024. [46] P. Wu, X. Zhou, G. Pang, Z. Yang, Q. Yan, P. Wang, and Y. Zhang. Weakly supervised video anomaly detection and localization with spatio-temporal prompts. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 93019310, 2024. [47] P. Wu, X. Zhou, G. Pang, L. Zhou, Q. Yan, P. Wang, and Y. Zhang. Vadclip: Adapting visionlanguage models for weakly supervised video anomaly detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 60746082, 2024. [48] Y. Yang, K. Lee, B. Dariush, Y. Cao, and S.-Y. Lo. Follow the rules: reasoning for video anomaly detection with large language models. In European Conference on Computer Vision, pages 304322. Springer, 2024. [49] M. Ye, W. Liu, and P. He. Vera: Explainable video anomaly detection via verbalized learning of vision-language models. arXiv preprint arXiv:2412.01095, 2024. [50] T. Yuan, X. Zhang, K. Liu, B. Liu, C. Chen, J. Jin, and Z. Jiao. Towards surveillance videoand-language understanding: New dataset baselines and challenges. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2205222061, 2024. [51] L. Zanella, W. Menapace, M. Mancini, Y. Wang, and E. Ricci. Harnessing large language models for training-free video anomaly detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1852718536, 2024. [52] H. Zhang, X. Xu, X. Wang, J. Zuo, C. Han, X. Huang, C. Gao, Y. Wang, and N. Sang. Holmesvad: Towards unbiased and explainable video anomaly detection via multi-modal llm. arXiv preprint arXiv:2406.12235, 2024. [53] H. Zhang, X. Xu, X. Wang, J. Zuo, X. Huang, C. Gao, S. Zhang, L. Yu, and N. Sang. Holmesvau: Towards long-term video anomaly understanding at any granularity. arXiv preprint arXiv:2412.06171, 2024. [54] H. Zhou, X. Li, R. Wang, M. Cheng, T. Zhou, and C.-J. Hsieh. R1-zeros\" aha moment\" in visual reasoning on 2b non-sft model. arXiv preprint arXiv:2503.05132, 2025. [55] H. Zhou, J. Yu, and W. Yang. Dual memory units with uncertainty regulation for weakly supervised video anomaly detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 37693777, 2023. [56] L. Zhu, L. Wang, A. Raj, T. Gedeon, and C. Chen. Advancing video anomaly detection: concise review and new dataset. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024."
        },
        {
            "title": "A Further Dataset Details",
            "content": "(a) (b) Figure 5: More dataset statistics of our VAU-Bench. (a) Distribution of training, validation, and test splits across the four tasks included in VAU-Bench. (b) Word cloud visualization of frequent terms appearing in the multiple-choice questions and choices. Dataset Annotation. VAU-Bench is constructed from three datasets: UCF-Crime, ECVA, and MSAD. While UCF-Crime [33] and ECVA [8] provide basic scene-level descriptions, they lack the structured annotations necessary for fine-grained reasoning. To address this, we leverage DeepSeekV3 [18], powerful large language model, to enrich the existing annotations from HIVAU-70K (which includes UCF-Crime) [53] and ECVA [8]. We use prompt-based instruction to guide the model in extracting key events, causal relationships, and anomalous behaviors, thereby producing reasoning-oriented annotations suitable for causal understanding. The detailed prompt design is provided in the blue-colored box below. Video Understanding Prompt. You are an expert in video understanding and reasoning. will give you structured metadata for surveillance or behavior-related video. Your task is twofold: Please analyze the entire video description, including anomaly labels, events, and all textual summaries. Based on this, generate comprehensive summary of what happens in the video in the following JSON structure: { \"judgement\": \"Does this video depict an anomaly? If yes, what is it called?\", \"description\": \"Chronological and factual summary of what happened in the video\", \"analysis\": { \"Specific Anomaly Type\": \"Select from the [Anomaly Type]\", \"Location\": \"Where the event occurs: indoor/outdoor/specific\", \"Key Evidence\": \"Key actions or objects that support classification\", \"Detailed Explanation\": \"Why these events are normal/anomalous\", \"Cause and Effect\": \"What led to the event and its outcome\", \"Conclusion\": \"Wrap-up reasoning with final conclusion about the event\" } } Generating QA Pair Prompt. You are an expert in reasoning-focused QA generation for surveillance analysis videos. You will be given structured video summary, including: (i) judgement (whether the video is anomalous or normal). (ii) chronological description of what happens in the video. (iii) multi-part analysis that breaks down the events anomaly type, location, key evidence, explanation, causes, and conclusion. Please generate single multiple-choice question-answer pair in JSON format. For the MSAD [56] dataset, which lacks textual annotations, we design structured Chain-of-Thought (CoT) annotation pipeline. We first use InternVL2.5-8B-MPO [42] as the Vision-Language Model (VLM) to generate initial annotations that include detailed descriptions, step-by-step reasoning, and anomaly classification. To further improve the quality of these annotations, we apply DeepSeek-V3 in secondary refinement stage, which enhances the coherence and clarity of the generated descriptions, QA pairs, and reasoning chains. The overall annotation pipeline consists of the following stages: Task Definition: The VLM is instructed to act as an anomaly detector. Video Description: The VLM generates detailed description of the video content. Step-by-Step Reasoning: The VLM performs multi-step reasoning to analyze the presence and nature of anomalies. Verification: Given the ground-truth anomaly type, the VLM verifies whether its prediction aligns with it. If not, it regenerates both the description and reasoning. Key Object Summarization: The VLM identifies key visual objects or cues relevant to the anomaly, expressed in 13 words. QA Generation: The VLM constructs multiple-choice questions by generating and shuffling plausible anomaly-related answer options. Quality Enhancement: We use DeepSeek-V3 to validate and refine the generated QA pairs, descriptions, and reasoning chains. After completing the CoT annotation for the entire VAU-Bench, we perform manual review to ensure the accuracy and consistency of all generated annotations. More Dataset Statistics. Table 4 presents detailed comparison of our VAU-Bench and existing video anomaly datasets. Compared to previous datasets, our benchmark offers longer total video duration, more diverse set of primary anomaly types (with similar categories merged), diverse multi-choice QA pairs, and richer Chain-of-Thought reasoning annotations. Figure 5a shows the dataset splits across four tasks. Each task contains balanced number of training, validation, and test samples, supporting robust evaluation. Figure 5b presents word cloud of frequent phrases extracted from the multiple-choice questions and answers in VAU-Bench. Notably, the presence of phrases such as \"best describes\", \"plausible explanation\", and \"behavioral clue\" highlights the variety of question formulations, encouraging models to engage in fine-grained interpretation. In addition, keywords such as robbery, man action, and scene indicate that our questions are intentionally crafted to guide models toward recognizing specific objects and anomaly types in complex real-world scenarios. Dataset Examples. We present representative examples from our VAU-Bench, each annotated to support four core tasks of video anomaly understanding. As illustrated in Figure 7, each example is richly labeled with question-answer pair, key visual evidence, anomaly type, temporal annotation, and multi-part reasoning chain that includes location, cause and effect, and high-level conclusion. This annotation format enables models not only to detect and classify anomalies, but also to explain them in structured, interpretable manner. Figure 7 and Figure 8 show challenging anomaly scenarios, while Figure 9 depicts normal scene, included to test model robustness and reduce false positives. These examples demonstrate the breadth and depth of our annotations, enabling holistic evaluation across perception and reasoning dimensions."
        },
        {
            "title": "B Experiment Details",
            "content": "Training Details. We use the Adam optimizer with learning rate of 2 105. The supervised fine-tuning (SFT) stage runs for less steps (e.g. 200) to avoid overfitting, while the Reinforcement Fine-Tuning (RFT) stage takes approximately 15 hours for 1.5k steps. We set the hyperparameter β in the KL divergence term of the GRPO to 0.04, using = 4 candidate outputs per prompt. The maximum response length is capped at 1024 tokens. 15 Table 4: Comparison of video anomaly detection benchmarks. We compare VAU-Bench with existing datasets in terms of size, annotation granularity, and reasoning capabilities. VAU-Bench is the first benchmark to support structured reasoning via multiple-choice questions and Chain-of-Thought (CoT) annotations. Columns indicate whether each dataset provides QA pairs, free-text descriptions (Descrip.), anomaly judgement (Judge.), reasoning (Reason.), and full CoT rationales. Dataset Year #Videos Total Len. #Type Annotation QA Pairs Descrip. Judge. Reason. CoT 2010 UCSD Ped1 [38] UCSD Ped2 [38] 2010 CUHK Avenue [24] 2013 2017 ShanghaiTech [21] 2018 UCF-Crime [33] 2020 Street Scene [28] 2020 IITB Corridor [29] 2022 UBNormal [1] 2023 NWPU [4] 2024 MSAD [56] UCA [50] CUVA [9] ECVA [8] HIVAU-70K [53] 2024 2024 2024 2025 70 28 35 437 1900 81 358 543 547 720 1854 1000 2240 5443 0.1h 0.1h 0.5h 3.5h 128.0h 3.8h 2.0h 2.2h 16.3h 4.1h 121.9h 32.5h 88.2h NA 5 Bounding-box 5 Bounding-box 5 Bounding-box 13 Bounding-box 13 Frame 17 Bounding-box Frame 10 Frame 22 Frame 43 Frame 11 13 Time Duration 11 Time Duration 21 Time Duration NA Time Duration VAUBench (Ours) 2025 4596 169.1h 19 Time Duration (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:33) (cid:33) (cid:33) (cid:33) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:33) (cid:37) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:33) (cid:37) (cid:33) (cid:37) (cid:37) (cid:37) (cid:33) (cid:33) (cid:33) (cid:33) VAU-Eval Prompt. Below is ground-truth description and analysis, followed by model-generated description and analysis. Please evaluate the models outputs from the following aspects: 1. Classification Correctness (10 pts) 2. Key Object and Action Matching (10 pts) 3. Fluency and Coherence (10 pts) 4. Informativeness and Domain Awareness (10 pts) 5. Factual Consistency (10 pts) Evaluation Details for Anomaly Reasoning. To evaluate the alignment between model-generated outputs and our annotated ground truth in video anomaly understanding, we introduce VAU-Eval, GPT-based evaluation protocol. The evaluation is structured as multi-turn interaction, where the model first generates description of the video and then performs reasoning to determine whether the video contains an anomaly. We then use DeepSeek-V3 [18] to assess the similarity between the predicted answers and the ground truth across five aspects: classification correctness, key object and action matching, fluency and coherence, informativeness and domain awareness, and factual consistency. Each aspect is scored out of 10 points, yielding total of 50 points per sample. To better reflect the models actual reasoning capabilities, we do not fine-tune the model on any reasoning-style description or analysis. Instead, we directly test models that are trained solely on the multiple-choice QA task, thus ensuring that their descriptive reasoning is not memorized but inferred. The detailed evaluation prompt used in this process is shown in the blue box above."
        },
        {
            "title": "C Further Evaluations",
            "content": "More Evaluations. As shown in Table 5, we conduct experiments on the ECVA dataset. Compared to UCF-Crime and MSAD, ECVA poses greater challenges across both recognition and reasoning tasks. All models consistently achieve lower VAU-Eval reasoning scores on ECVA, indicating that its longer videos, more camera movements, viewpoint shifts and richer anomaly diversity make fine-grained understanding more difficult. While our RFT-enhanced models achieve consistent improvements in multiple-choice QA accuracy, their VAU-Eval reasoning scores does not always improve. This suggests that while RFT helps models better predict the final answer, it does not necessarily enhance the reasoning process. These findings highlight the need for more fine-grained reward signals to guide the generation of high-quality rationales in complex scenarios. 16 Table 5: Comparison of performance on ECVA datasets on multiple-choice QA task and anomaly reasoning task. Accw/o think and Accw/ think refer to the multiple-choice question accuracy without and with thinking, respectively. For the anomaly reasoning task, CLS, KM, FLU, INF, and FAC represent VAU-Eval scores generated by DeepSeek-V3, measuring classification accuracy, key concept alignment, linguistic fluency, informativeness, and factual consistency, respectively. Each dimension is scored on 10-point scale. Total denotes the aggregated score over five dimensions. QA Accuracy VAU-Eval Dataset Model Accw/o think 78.84 InternVL2.5-2B Qwen2.5-VL-7B 83.02 InternVL2.5-8B-MPO 90.00 Accw/ think 58.84 86.98 83.72 ECVA Qwen2-VL-2B +SFT +RFT Qwen2.5-VL-3B +SFT +RFT 86.98 84.88 90.23 (3.25) 84.42 (0.47) 83.95 84.65 2.21 75.81 85.58 89.30 1.50 86.98 89.53 (3.95) 86.51 (10.70) 1.45 CLS KM FLU INF FAC Total 2.86 3.70 3.4 2.41 2.20 2.26 2.78 3.67 3. 2.36 2.12 2.28 2.58 1.22 2.24 7.57 8.64 7.87 7.81 7.37 7.52 8.33 4.37 8.05 4.62 6.40 4. 3.81 3.99 3.70 5.02 2.66 4.32 3.03 4.04 3.47 2.57 2.22 2.40 2.75 1.24 2.39 20.86 26.45 22. 18.96 17.90 18.16 20.89 10.99 18.45 Table 6: Performance of HolmesVAU 2B [53] and our VAU-R1 2B on multiple-choice QA and anomaly reasoning task. QA Accuracy VAU-Eval CLS KM FLU INF FAC Total Model HolmesVAU 2B VAU-R1 2B Dataset Accw/o think MSAD 85.00 UCF-Crime 86.45 70.47 ECVA Accw/ think 86.25 85.66 70. 3.73 3.05 2.54 82.92 (2.08) MSAD UCF-Crime 88.45 (2.00) ECVA 6.05 4.04 90.23 (19.76) 84.42 (13.72) 2.26 83.75 (2.50) 88.05 (2.39) 2.72 1.97 1.71 5.49 2.75 2. 6.82 6.30 6.26 8.89 7.72 7.52 3.55 3.08 2.78 6.50 4.89 3.70 3.33 2.39 2.30 6.05 3.11 2. 20.15 16.79 15.59 32.98 22.52 18.16 Comparison with Prior Work. As shown in Table 6, we evaluate HolmesVAU 2B [53], recently released baseline for VAU, on our benchmark to assess its reasoning capability in complex scenarios. While HolmesVAU 2B achieves reasonable performance across all datasets, it consistently underperforms compared to our Qwen-based models, particularly on the challenging ECVA dataset. This performance gap is evident in both multiple-choice QA accuracy and VAU-Eval reasoning scores, indicating limitations in HolmesVAU 2Bs ability to generalize to diverse and complex scenarios. In contrast, VAU-R1 demonstrates stronger alignment with human-annotated reasoning chains and greater robustness across datasets. Classification Results. Table 7 presents the binary and multi-class anomaly classification accuracy on three datasets: MSAD, UCF-Crime, and ECVA. We directly apply the RFT strategy to train multi-class anomaly classification task, which includes 19 different anomaly types as well as the normal class. However, directly training the complex multi-class task with RFT degrades performance, suggesting it is more effective to decompose the task into simpler sub-tasks with structured rewards to better guide learning. We compare multiple models under two settings: w/o think and w/ think. We observe that, for the relatively challenging multi-class anomaly task, incorporating an explicit think reasoning step improves the models classification accuracy. Temporal Localization Performance. Table 8 summarizes the temporal localization (mIoU) performance of representative methods, categorized into traditional models, multi-modal approaches, and MLLMs. As expected, early appearance-based methods (e.g., Two-stream [32], TSN [40], C3D [37]) achieve limited performance. Incorporating spatio-temporal modeling via 3D convolutions (T-C3D [19], ARTNet [39], 3DResNet [13]) brings moderate improvements, with Liu et al. [20] reaching mIoU of 16.40. More recent multi-modal approaches, such as VADClip [47] and STPrompt [46], achieve significantly better performance, with STPrompt reaching 23.90 mIoU. Our MLLM-based methods show promising yet limited temporal grounding capabilities. While Qwen2.5-VL-3B achieves only 10.91 mIoU, reinforcement tuning (+RFT) boosts performance to 16.80, indicating that structured reward learning helps align model outputs with temporal structures. Table 7: Comparison of anomaly classification accuracy on three datasets. Bin. Acc. denotes binary classification accuracy (normal vs. abnormal), and Multi Acc. denotes multi-class accuracy over 19 anomaly types and the normal class. Results are reported with and without think prompting. Dataset Model MSAD UCF-Crime ECVA Qwen2-VL-2B-Instruct Qwen2.5-VL-7B-Instruct Qwen2.5-VL-3B-Instruct + SFT + RFT Qwen2-VL-2B-Instruct Qwen2.5-VL-7B-Instruct Qwen2.5-VL-3B-Instruct + SFT + RFT Qwen2-VL-2B-Instruct Qwen2.5-VL-7B-Instruct Qwen2.5-VL-3B-Instruct + SFT + RFT w/o think w/ think Bin. Acc. Multi Acc. Bin. Acc. Multi Acc. 75.00 90.00 79.17 70. 62.50 70.00 69.58 28.75 60.42 75.00 73.33 74.58 52.92 66.67 56.67 33. 82.08 (2.91) 71.25 (1.67) 74.58 (1.25) 60.83 (4.16) 60.56 86.85 64.54 64.14 53.78 62.15 58.57 28.69 60.16 70. 62.55 69.32 51.79 61.35 52.19 37.05 62.55 (1.99) 57.77 (0.80) 62.15 (0.40) 56.97 (4.78) 41.95 64.85 52.83 96. 24.72 32.88 30.16 29.48 32.88 43.54 49.89 96.15 19.05 23.81 22.00 28. 49.66 (3.17) 30.61 (0.45) 55.78 (5.89) 31.07 (9.07) Table 8: Comparison of temporal localization performance (mIoU) across different methods on UCF-Crime dataset. Category Method Feature mIoU Two-stream [32] TSN [40] C3D [37] T-C3D [19] ARTNet [39] 3DResNet [13] NLN [43] Liu et al. [20] VADClip [47] STPrompt [46] Two-Stream TSN C3D C3D ARTNets I3D-ResNet I3D-ResNet I3D-ResNet CLIP CLIP Qwen2.5-VL-3B ViT Qwen2.5-VL-3B + RFT ViT ViT Qwen2.5-VL-7B 2.20 2.60 7.20 10.20 11.40 10.30 12.20 16. 22.05 23.90 10.91 16.80 22.72 Traditional Multi-modal MLLMs However, even with RFT, MLLMs still underperform compared to specialized temporal models, suggesting that current architectures may lack explicit temporal reasoning modules required for fine-grained localization. Case Study on Anomaly Reasoning. Figure 6 presents qualitative comparison between outputs generated by SFT and our proposed VAU-R1 model on anomaly reasoning task. Both models are evaluated using the same Chain-of-Thought (CoT) prompt and scored based on five criteria: classification correctness (CLS), key object matching (KM), fluency (FLU), informativeness (INF), and factual consistency (FAC). The SFT output incorrectly identifies the anomaly as political argument, which does not match the core issue (an escalator malfunction). It also fails to mention any key visual evidence or relevant location. In contrast, VAU-R1 produces more contextually appropriate response, identifying an emergency situation at subway station involving injured individuals and emergency vehicles. While the response focuses on surface-level emergency context rather than the root cause, it demonstrates greater fluency and relevance. The evaluation assigns higher total score of 22, with solid performance across all dimensions, particularly in fluency and informativeness."
        },
        {
            "title": "D Limitation and Future Work",
            "content": "One limitation of this work is its focus on constrained set of tasks, namely multiple-choice question answering, temporal grounding, anomaly reasoning, and anomaly classification. While these tasks form strong foundation for video anomaly understanding, there remains substantial room for extension. Future work could incorporate additional tasks such as spatial localization of key objects, which would enable more fine-grained event understanding. Moreover, introducing additional modalities (e.g., audio) may provide complementary cues that enhance both the robustness and contextual depth of anomaly reasoning."
        },
        {
            "title": "E Potential Societal Impact",
            "content": "We propose new method and benchmark for video anomaly understanding. Accurate and interpretable anomaly understanding systems can contribute to wide range of safety-critical applications, such as disaster early warning, fire prevention, fall detection, and public safety monitoring. By enabling models to reason about abnormal events, our approach can assist first responders in identifying urgent situations earlier and more reliably. However, this research inevitably involves scenarios that depict violent or chaotic abnormal behaviors. We strictly follow established ethical guidelines throughout our study. The datasets used in this study are publicly available and have been processed in accordance with the guidelines provided by their original publishers. We strictly adhere to these terms of use and employ the data solely for academic research purposes. To ensure privacy protection, the datasets include safeguards such as reduced video resolution and facial blurring, effectively preventing the identification of individuals. Looking ahead, we plan to explore anomaly understanding methods that incorporate privacy preservation as core design principle. 19 Figure 6: Qualitative case of the Anomaly Reasoning task. All correct description and analysis are highlighted in orange. The evaluation results are presented on the right of the answer respectively. Both SFT and VAU-R1 perform inference using the same CoT prompt. VAU-R1s output correctly identifies the anomaly with high fluency but lacks reasoning for the core event, whereas SFTs output is inaccurate and tends to produce repetitive responses. 20 Figure 7: Example of VAU-Bench. An explosion case in an outdoor backyard, highlighting complex anomaly detection and dynamic scene understanding, labeled with question-answer pair, key visual evidence, anomaly type, and multi-part reasoning chain that includes location, cause and effect, and high-level conclusion. 21 Figure 8: Example of VAU-Bench. stealing incident, demonstrating capabilities in human activity recognition and intent analysis. 22 Figure 9: Example of VAU-Bench. normal scene, used to evaluate model robustness against false positives and to enhance dataset diversity."
        }
    ],
    "affiliations": [
        "Australian National University",
        "GVC Lab, Great Bay University",
        "Intellindust AI Lab"
    ]
}