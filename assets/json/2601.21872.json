{
    "paper_title": "WebArbiter: A Principle-Guided Reasoning Process Reward Model for Web Agents",
    "authors": [
        "Yao Zhang",
        "Shijie Tang",
        "Zeyu Li",
        "Zhen Han",
        "Volker Tresp"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Web agents hold great potential for automating complex computer tasks, yet their interactions involve long-horizon, sequential decision-making with irreversible actions. In such settings, outcome-based supervision is sparse and delayed, often rewarding incorrect trajectories and failing to support inference-time scaling. This motivates the use of Process Reward Models (WebPRMs) for web navigation, but existing approaches remain limited: scalar WebPRMs collapse progress into coarse, weakly grounded signals, while checklist-based WebPRMs rely on brittle template matching that fails under layout or semantic changes and often mislabels superficially correct actions as successful, providing little insight or interpretability. To address these challenges, we introduce WebArbiter, a reasoning-first, principle-inducing WebPRM that formulates reward modeling as text generation, producing structured justifications that conclude with a preference verdict and identify the action most conducive to task completion under the current context. Training follows a two-stage pipeline: reasoning distillation equips the model with coherent principle-guided reasoning, and reinforcement learning corrects teacher biases by directly aligning verdicts with correctness, enabling stronger generalization. To support systematic evaluation, we release WebPRMBench, a comprehensive benchmark spanning four diverse web environments with rich tasks and high-quality preference annotations. On WebPRMBench, WebArbiter-7B outperforms the strongest baseline, GPT-5, by 9.1 points. In reward-guided trajectory search on WebArena-Lite, it surpasses the best prior WebPRM by up to 7.2 points, underscoring its robustness and practical value in real-world complex web tasks."
        },
        {
            "title": "Start",
            "content": "WebArbiter WEBARBITER: PRINCIPLE-GUIDED REASONING PROCESS REWARD MODEL FOR WEB AGENTS Yao Zhang1,3*, 1LMU Munich Shijie Tang1, Zeyu Li2, Zhen Han1*, Volker Tresp1,3 2Technical University of Munich 3Munich Center for Machine Learning (MCML) Project Page: WebArbiter"
        },
        {
            "title": "ABSTRACT",
            "content": "Web agents hold great potential for automating complex computer tasks, yet their interactions involve long-horizon, sequential decision-making with irreversible actions. In such settings, outcome-based supervision is sparse and delayed, often rewarding incorrect trajectories and failing to support inference-time scaling. This motivates the use of Process Reward Models (WebPRMs) for web navigation, but existing approaches remain limited: scalar WebPRMs collapse progress into coarse, weakly grounded signals, while checklist-based WebPRMs rely on brittle template matching that fails under layout or semantic changes and often mislabels superficially correct actions as successful, providing little insight or interpretability. To address these challenges, we introduce WebArbiter, reasoning-first, principle-inducing WebPRM that formulates reward modeling as text generation, producing structured justifications that conclude with preference verdict and identify the action most conducive to task completion under the current context. Training follows two-stage pipeline: reasoning distillation equips the model with coherent principle-guided reasoning, and reinforcement learning corrects teacher biases by directly aligning verdicts with correctness, enabling stronger generalization. To support systematic evaluation, we release WEBPRMBENCH, comprehensive benchmark spanning four diverse web environments with rich tasks and high-quality preference annotations. On WEBPRMBENCH, WebArbiter-7B outperforms the strongest baseline, GPT-5, by 9.1 points. In reward-guided trajectory search on WebArena-Lite, it surpasses the best prior WebPRM by up to 7.2 points, underscoring its robustness and practical value in real-world complex web tasks. 6 2 0 2 9 2 ] . [ 1 2 7 8 1 2 . 1 0 6 2 : r Figure 1: Performance comparison on WEBPRMBENCH. Left: Average Best-of-N Acc vs. model size, showing superior efficiency despite smaller scale. Right: Domain-wise Avg BoN Acc, where WebArbiter achieves the best results across all environments, confirming robustness and scalability. 1 WebArbiter"
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Language Models (LLMs) (Achiam et al., 2023; Guo et al., 2025a) have demonstrated impressive capabilities in planning (Huang et al., 2024; Zhang et al., 2025a), decision-making (Li et al., 2024), and complex task execution (Xi et al., 2024; Zhang et al., 2025b). Extending these abilities with browser access enables LLM agents to perform complex web tasks similar to humans (OpenAI, 2025a; Anthropic, 2024a; Adept, 2022). However, web interactions involve long horizons, multi-step decisions, and actions that can be irreversible. For example, submitting an incorrect form may not be recoverable. This requires agents to make reliable decisions throughout the interaction process, rather than relying solely on final outcomes. Traditional Outcome Reward Models (ORMs) are ill-suited: they provide only sparse and delayed feedback, may misclassify incorrect trajectories as successes, and cannot guide inference-time strategies, such as reward-guided search. Recent studies on web agents (Zhang et al., 2025b; Koh et al., 2025) have introduced step-level rewards using LLM-as-judge. While such supervision can be useful, LLM-as-judge suffers from high cost, limited scalability, and susceptibility to hallucination, often rewarding fluent but incorrect actions. This motivates the development of dedicated Process Reward Models (WebPRMs) for web tasks. Existing WebPRMs largely fall into two categories: scalar WebPRM (Miao et al., 2025), which collapse progress into coarse scores with little interpretability or weak grounding; and generative WebPRM (Chae et al., 2025), which rely on checklists that are brittle under dynamic layouts and state-dependent action semantics. Moreover, lacking explicit reasoning, generative WebPRMs remain vulnerable to surface correlations and sensitive to page changes. These limitations highlight the need for reasoning-first WebPRM that can verify progress, resist superficial biases, and provide interpretable chains for diagnosing errors. To this end, we propose WebArbiter, reasoning-first, principle-inducing WebPRM. It formulates process reward modeling as text generation: given task context and candidate actions with their reasoning traces, the model produces structured justification that concludes with preference verdict, identifying the action most conducive to task completion. Unlike scalar scores or checklist-based methods tied to fixed templates, WebArbiter dynamically derives principles from user intent and the current state, incorporates them into reasoning chains that verify whether an action advances task completion. Training follows two-stage pipeline: reasoning distillation equips the model with coherent principle-guided reasoning, and reinforcement learning (RL) corrects teacher biases and aligns verdicts with correctness. This design transforms reward signals from shallow correlations into auditable analyses, making judgments robust to environment and page variations, resistant to spurious cues, and accurate in credit assignment. To advance the evaluation of WebPRMs, we introduce WEBPRMBENCH, the first comprehensive evaluation benchmark spanning diverse environments dedicated to WebPRMs. It provides 1,150 step-level preference instances, each consisting of one correct action and four rejected alternatives, collected across 4 web environments: AssistantBench (Yoran et al., 2024), Mind2Web (Deng et al., 2023), WorkArena (Drouin et al., 2024; Boisvert et al., 2025), and WebArena (Zhou et al., 2023). The tasks span everyday activities such as online shopping and forum posting, as well as enterprise scenarios like updating schedules in IT management platforms. By combining scale, diversity, and fine-grained supervision, WEBPRMBENCH establishes unified standard for systematic evaluation of WebPRMs, with Pairwise and Best-of-N (BoN) Accuracy as the primary metrics. As shown in Fig. 1, experiments on WEBPRMBENCH show that WebArbiter achieves the highest Avg. BoN Acc among all evaluated models, outperforming the strongest proprietary LLM baseline, GPT-5, by 9.1 points, and consistently surpassing the previous SOTA WebPRM, WebShepherd, across all environments. Beyond static evaluation, WebArbiter also proves effective in practice: in reward-guided trajectory search on WebArena-Lite (Liu et al., 2024b), it delivers substantial gains, surpassing WebShepherd by up to 7.2 points, further demonstrating robustness in realistic interaction settings. The key contributions of this work are: 1. We propose WebArbiter, reasoning-first, principle-inducing PRM trained with reasoning distillation and RL, providing auditable reasoning chains and correctness-aligned signals. *Corresponding authors. 2 WebArbiter 2. We release WEBPRMBENCH, the first comprehensive evaluation benchmark to provide systematic WebPRM evaluation across 4 web environments, using Pairwise and Best-of-N (BoN) Accuracy as standard metrics. 3. We show that WebArbiter achieves SOTA performance on WEBPRMBENCH, surpassing both proprietary LLMs and the previous SOTA WebPRM. WebArbiter delivers up to 7.2% gains in reward-guided trajectory search on WebArena-Lite. 4. We analyze the effects of different training components through systematic ablations, showing that cold-start RL alone is unstable across environments, whereas reasoning distillation and explicit principles are essential for stable and transferable progress-aware judgments, with RL primarily acting as an amplifier."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 LLM-BASED AUTONOMOUS WEB AGENTS LLM advances have enabled browser-operating agents (Kim et al., 2024; Sun et al., 2024; Prasad et al., 2023; Fu et al., 2024; Ma et al., 2023; Zheng et al., 2023; Tao et al., 2023). One line distills environment-specific stateaction pairs from demonstrations, strong on seen states yet brittle on novel ones, with SteP as leading example on WebArena (Sodhi et al., 2024; Zhou et al., 2023). second line pursues open-ended exploration via reflexive evaluation and search (Pan et al., 2024; Shinn et al., 2024; Koh et al., 2024; Zhang et al., 2025b). third direction applies RL (Qi et al., 2025; Wei et al., 2025), yet real sites provide sparse and delayed signals, which makes value learning unstable without dense step feedback. Therefore, WebAgents require process-level judge that assesses progress step by step and supplies auditable signals for search and planning. 2.2 REWARD MODELS IN REASONING AND WEB TASKS RMs fall into two families. Scalar RMs attach single numeric score to response and use either absolute or discriminative schemes for evaluation (Uesato et al., 2022; Ouyang et al., 2022; Liu et al., 2024a; 2025; Park et al., 2024; Wang et al., 2024a; 2023b; 2024b). Generative RMs instead produce naturallanguage feedback from which rewards are extracted, aligning with LLM-as-Judge and supporting both single-instance evaluation and multi-response comparison; they show promising scalability but raise reliability concerns due to bias and hallucination (Lightman et al., 2023; Wang et al., 2023a; Zhang et al., 2025d; Wu et al., 2024; Ye et al., 2025; Zhang et al., 2024; 2025c). Building on these, Reasoning RMs cast judging as deliberate process: they first generate an explicit, context-grounded chain of principle and analysis, then issue single preference verdict, yielding adaptive test-time compute, stronger grounding, and interpretable feedback (Chen et al., 2025; Guo et al., 2025b; Mahan et al., 2024). In web agents, action rewards have been derived by the following methods: LLM-as-Judge (Zhang et al., 2025b; Koh et al., 2025), slow and unstable during search; scalar scoring (Miao et al., 2025), which collapses progress into coarse values with little interpretability and weak grounding; and checklist-driven generative feedback (Chae et al., 2025), whose external templates are brittle under layout and semantic drift and prone to surface correlations. These limitations motivate reasoning-first approach that turns rewards from shallow correlations into auditable analyses. WebArbiter produces structured justifications with single preference verdict, induces principles from the current instruction and state, and is trained by reasoning distillation followed by RL, so that judgments remain robust to environment variations, resist spurious cues, and provide accurate credit assignment while supporting inference-time scaling."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "In this section, we present the design of WebArbiter. Fig. 2 provides an overview of the WebArbiter framework, including the two-stage training pipeline and the inference-time principle-guided decision process. We begin by framing web navigation as Partially Observable Markov Decision Process (POMDP) in 3.1, then describe how we construct pairwise-preference dataset for training in 3.2. We introduce the training pipeline of WebArbiter model in 3.3. For clarity, we summarize all notations in Appendix A. 3 WebArbiter Figure 2: Overview of WebArbiter. Given an instruction I, current observation op, and history (a<p, c<p), the model compares candidate actions (a1 In Stage 1, principleguided reasoning traces are distilled from stronger teacher LLM. In Stage 2, WebArbiter is trained with RL using verifiable rewards {1, +1}, producing structured justifications and final verdict. During inference, the model induces principles (e.g., clarity, correctness, progress) from (I, op, a<p, c<p, (a1 p)), applies them to candidate actions, and outputs an auditable judgment identifying the action that best advances task completion. p) and (a2 p), (a p, c1 p, c2 p, c1 p, c2 p). 3.1 BACKGROUND We formalize web navigation as POMDP. The environment is defined by state space S, an action space A, and an observation space O. : denotes the state transition function. At step p, the agent receives partial observation op O, executes ap A, and transitions to sp+1 = (sp, ap) with new observation op+1. Following WebArena (Zhou et al., 2024), we represent observations using accessibility trees, i.e., text-only encodings of visible interactive elements and their attributes. Given task instruction and the initial state s0 S, the agent aims to generate trajectory τ = (a1, . . . , aP ) that completes the task. Here is the trajectory length and ap denotes the action at step p. The task evaluator determines whether the task is completed based on the final state. 3.2 TRAINING DATASET CONSTRUCTION We build on the WEBPRM COLLECTION (Chae et al., 2025) for training WebArbiter. Each instance consists of an instruction I, sequence of observations = (o1, . . . , oP ), and expert-annotated trajectories. Specifically, the dataset provides set of positive actions A+ = (a+ 1 , . . . , a+ ) taken from expert demonstrations and negative actions = (a ) obtained from rejected trajectories. We convert these into pairwise preference samples where each candidate action is paired with its reasoning trace, yielding the preference dataset DTrain used for WebArbiter training. 1 , . . . , 3.3 WEBARBITER: PRINCIPLE-INDUCING REASONING PROCESS REWARD MODEL is paired with reasoning trace cq WebArbiter is built on Transformer-decoder backbone and formulates process reward modeling as text generation task. At each state, it evaluates candidate actions {(aq q=1, where each action aq explaining why the agent generated this action. Given task instruction I, observation op, and history (a<p, c<p), the model autoregressively generates structured justification = (j1, . . . , jL) of length that concludes with preference verdict ˆy selecting the most appropriate action among the candidates. The historical traces are c<p = {c1, . . . , cp1}, i.e., the per-action reasoning traces for previously executed actions. concrete training example is provided in Appendix B. While our experiments instantiate this framework in the standard pairwise preference setting, the design is general and extends naturally to multi-candidate settings. p)}Q p, cq 4 WebArbiter Unlike the scalar WebPRM (Miao et al., 2025) that collapses progress into opaque scores or the checklist-based WebPRM (Chae et al., 2025), WebArbiter is reasoning-first, principle-inducing WebPRM: it dynamically derives principles from user intent and the current state, integrates them into reasoning chains that explicitly assess whether each candidate action truly advances task completion. This design moves reward signals beyond shallow correlations toward auditable analyses, yielding judgments that are robust to environment changes, resistant to spurious cues, and precise in credit assignment. Formally, the preference dataset is defined as <p, c(i) DTrain = {(I (i), o(i) , a(i) <p, (a1(i) , c1(i) ), (a2(i) , c2(i) ), y(i))}M i=1, where {a1 p, p} denotes the preferred action. For notational simplicity, let = (I, op, a<p, c<p, (a1 p, c1 p), (a2 p, c2 p)). WebArbiter πθ is parameterized by θ and models the justification autoregressively: πθ(j x) = (cid:89) l=1 πθ(jl x, j<l). 3.3.1 TRAINING OVERVIEW (1) (2) (3) The overall training objective is to maximize the likelihood that the predicted preference matches the ground truth: max πθ E(x,y)DTrain, ˆyπθ(jx) [1(ˆy = y)] . (4) Training proceeds in two stages. First, reasoning distillation, described in 3.3.2, equips the model to generate coherent, principle-guided justifications, promoting judgments grounded in explicit reasoning rather than surface correlations, property later validated by ablation studies in 5.1.3. Concretely, we sample examples from DTrain to construct DSFT for supervised distillation, while the remaining data form DRL for RL. Second, RL, detailed in 3.3.3, aligns the models verdicts with correctness signals and yields interpretable step-level rewards for long-horizon decision-making. Together, these stages enable WebArbiter to provide robust, interpretable, and scalable supervision for web agents. 3.3.2 STAGE 1: REASONING DISTILLATION Directly prompting an instruction-tuned LLM as reward model often yields superficial, inconsistent chains that do not justify why an action advances the task. We therefore distill principle-guided reasoning from stronger teacher. Concretely, o3 synthesizes structured justifications that first derive task-specific principles from the instruction and state, then ground these principles in the page, compare candidate actions against them, and finally output the preferred action. This equips WebArbiter with principles rather than surface heuristics. Ablations in 5.1.3 show that removing explicit principles and relying solely on reasoning-based justifications notably degrades performance, highlighting the role of principle induction in stabilizing step-level judgments. Given (x(i), y(i)) DSFT, the teacher generates justification ˆj(i) = (ˆj(i) ). The distillation dataset is then: DSFT = {x(i), ˆj(i))}K Objective. Reasoning distillation adjusts θ to maximize the likelihood of generating the teacher justification ˆj that concludes with the preferred action given x. We minimize the standard negative log-likelihood: 1 , . . . , ˆj(i) Li i=1. LSFT(θ) = 1 (cid:88) Li(cid:88) i=1 l= log πθ (cid:16)ˆj(i) x(i), ˆj(i) <l (cid:17) . (5) 3.3.3 STAGE 2: REINFORCEMENT LEARNING While distillation provides initial reasoning ability, it inherits teacher biases and may overfit to superficial patterns, limiting generalization to unseen environments. To further enhance judgment 5 WebArbiter Table 1: Data distribution of WEBPRMBENCH, the first comprehensive evaluation benchmark spanning diverse environments for WebPRMs. Models Mind2Web Cross-Task Cross-Website Cross-Domain WebArena AssistantBench WorkArena Total Count 148 417 201 30 212 accuracy, stability, and generalization, we introduce RL stage. WebArbiter πθ is treated as judgment policy that outputs justification that concludes with final verdict ˆy. During rollout, πθ generates the full justification and verdict, after which correctness reward R(x, ˆy) {1, 1} is assigned solely based on whether ˆy matches the ground-truth preference y. The distilled model from 3.3.2 serves as the reference policy πref, ensuring stable optimization. Objective. RL adjusts θ to maximize the expected reward while stabilizing reasoning style via KL regularization. The optimization objective is defined as: (cid:104) R(x, ˆy) β DKL(πθ πref) . E(x,y)DRL, ˆyπθ(jx) LRL(θ) = max (6) (cid:105) πθ In practice, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024) to optimize this objective, which enables stable updates under binary verifiable rewards. Through this RL stage, WebArbiter directly aligns its verdicts with correctness signals and converts structured justifications into reliable, interpretable step-level reward signals."
        },
        {
            "title": "4 WEBPRMBENCH",
            "content": "This section introduces WEBPRMBENCH, comprehensive multi-environment benchmark for evaluating WebPRMs. 4.1 BENCHMARK CONSTRUCTION WEBPRMBENCH is constructed from sucessful trajectories in AGENTREWARDBENCH (Lù et al., 2025), expanding beyond WEBREWARDBENCH (Chae et al., 2025), which only provides Mind2Web (Deng et al., 2023) and limited WebArena data (Zhou et al., 2023). We enrich WebArena with additional trajectories and incorporate AssistantBench (Yoran et al., 2024) and WorkArena (Drouin et al., 2024; Boisvert et al., 2025), resulting in broader coverage of real-world tasks across four environments. Mind2Web emphasizes cross-task generalization across heterogeneous websites. WebArena provides controlled environments such as shopping, CMS, Reddit, and GitLab. AssistantBench introduces open-world tasks on real websites. WorkArena focuses on enterprise workflows, including IT and HR. This diversity enables systematic evaluation across both consumer-facing and enterprise scenarios, covering broad range of task complexities. For each state, the action from the successful trajectory is retained as the positive label, and four rejected alternatives with associated reasoning traces are synthesized to form preference pairs. To ensure data quality, we sample negatives from diverse policy models to broaden coverage, apply rulebased filters to remove invalid or mismatched actions, discard inconsistent cases, and conduct expert verification to further ensure reliability. We also conduct targeted auditing to eliminate potential false negatives. To avoid positional bias, the positive action is not fixed to specific side and may appear on either side of the preference pair. Reasoning traces are truncated to fixed length to minimize formatting noise. The resulting benchmark spans 1,150 step-level preference instances across four environments, as shown in Tab. 1. Full construction details and benchmark statistics are provided in Appendix E. 4.2 EVALUATION PROTOCOL Evaluating WebPRMs requires metrics that capture both local preference fidelity and global decision reliability under realistic multi-candidate settings. Inspired by RMB (Zhou et al., 2025), we adopt two complementary metrics: Pairwise Accuracy, which measures correctness on individual preference 6 WebArbiter Table 2: Results on WEBPRMBENCH with Pairwise and BoN Acc. denotes our models. Bold numbers indicate the best results, while underlined numbers denote the second best. Our WebArbiter7B achieves the highest Avg BoN Acc, outperforming the second-best baseline, i.e., GPT-5, by 9.1. Models Mind2Web WebArena AssistantBench WorkArena Avg. Pairwise BoN Pairwise BoN Pairwise BoN Pairwise BoN Pairwise BoN LLM-as-judge, Proprietary Language Models GPT-4o-mini GPT-4o GPT-5 Claude-3.7-Sonnet Gemini-2.5-Flash DeepSeek-R1 81.74 79.99 80.86 80.20 81.30 81.62 50.92 52.62 62.39 57.90 57.01 57.37 78.23 84.58 84.83 82.80 82.71 82. LLM-as-judge, Open-source Language Models Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Llama-3-70B-Instruct 76.46 77.79 80.55 36.93 39.18 49.36 60.32 74.88 77.36 56.72 66.67 71.64 64.10 62.19 60. 15.42 42.79 50.75 89.17 85.83 81.67 81.50 80.00 78.49 75.83 84.17 85.83 73.33 66.67 63.33 61.30 63.33 56.18 33.33 53.33 70.00 81.43 84.33 81.14 82.10 83.30 84. 64.45 77.58 79.08 46.70 55.19 64.62 60.60 56.13 63.89 19.34 35.85 40.09 82.64 83.68 82.13 81.65 81.83 81.57 69.27 77.61 80.71 56.92 60.29 65.50 60.98 59.67 59. 26.76 42.78 52.55 WebPRMs (3B) WebShepherd-3B WebArbiter-3B WebPRMs (7B+) WebShepherd-8B WebArbiter-7B 87.50 93.32 65.21 78. 68.16 81.97 41.29 56.22 66.67 78.33 46.67 46.67 50.00 81.01 21.23 54. 68.08 83.65 43.60 59.06 86.66 97.07 73.69 89.53 68.33 88.43 43.88 68. 55.92 89.17 30.00 70.00 54.56 82.09 25.53 70.19 64.34 89.19 43.28 74. pairs, and Best-of-N (BoN) Accuracy, which evaluates robustness when ranking among multiple distractors. Compared with Pairwise Acc, BoN Acc applies stricter criterion by requiring the correct action to outrank all distractors simultaneously, providing stronger discriminative power and better alignment with downstream agent performance. deeper analysis of BoN vs. Pairwise Acc is in Appendix F. Pairwise Acc. Given preference pair (a+, a), where a+ is the correct action and is rejected one, the WebPRM is correct if it assigns higher preference to a+. Formally: 1 DBench 1(cid:2)πθ(a+) πθ(a)(cid:3). AccPairwise = (cid:88) (7) (a+,a)DBench BoN Acc. For each instance (a+, a1, . . . , aQ) DBench, the WebPRM is considered correct only when a+ is consistently ranked above all distractors, with = 4 in our benchmark. BoN Acc is: AccBoN = 1 DBench DBench (cid:88) (cid:89) i= q="
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "1[πθ(a+ ) πθ(aq )]. (8) We conduct comprehensive experiments to evaluate WebArbiter on the reward modeling benchmark WEBPRMBENCH in 5.1 and on practical applications in 5.2. 5.1 WEBPRMBENCH 5.1.1 EXPERIMENTAL SETUP Baselines. We compare WebArbiter against three categories of baselines. (1) Proprietary LLM-asjudge models, including GPT-4o-mini (OpenAI, 2024a), GPT-4o (OpenAI, 2024b), GPT-5 (OpenAI, 2025b), Claude-3.7-Sonnet (Anthropic, 2025), Gemini-2.5-Flash (Pichai & Hassabis, 2025), and DeepSeek-R1 (Guo et al., 2025a), which are prompted to act as judges by selecting the preferred action given task context. (2) Open-source LLM-as-judge models, represented by Qwen2.5-3BInstruct and Qwen2.5-7B-Instruct (Qwen et al., 2025), and Llama-3-70B-Instruct (Meta, 2024), providing accessible yet competitive instruction-tuned baselines. (3) WebPRMs, where we include WebShepherd (Chae et al., 2025). 7 WebArbiter Table 3: Ablation results on WEBPRMBENCH with Qwen2.5-7B-Instruct as backbone. We report Pairwise and BoN Acc across web environments. WebArbiter, combining principle-guided reasoning distillation with RL, achieves the highest overall performance. Method Mind2Web WebArena AssistantBench WorkArena Avg. Pairwise BoN Pairwise BoN Pairwise BoN Pairwise BoN Pairwise BoN Instruct (Original) Instruct + Cold Start RL Instruct + Cold Start RL + Principles Instruct + SFTw/o Principles + RL WebArbiter-7B 77.79 96.18 96.18 98. 97.07 39.18 86.00 88.00 94.34 89.53 74.88 71.10 77.80 74.60 88.43 42.79 35.80 46.30 41. 68.66 84.17 72.40 80.10 77.20 89.17 53.33 33.60 48.90 40.20 70.00 77.58 74.90 82.40 79. 82.09 35.85 37.90 51.80 44.60 70.19 77.61 78.15 84.12 82.35 89.19 42.78 48.33 58.75 55. 74.60 Implementation Details. We train WebArbiter on WEBPRM Collection (Chae et al., 2025), which comprises 30k step-level preference pairs drawn from the Mind2Web environment. We use 10k pairs for stage-1 reasoning distillation and the remainder for stage-2 RL. Models are initialized from Qwen2.5-3B-Instruct and Qwen2.5-7B-Instruct (Qwen et al., 2025) and fine-tuned with LoRA (Hu et al., 2022). Further implementation details are provided in the Appendix C, and all prompts are provided in Appendix D. Evaluation Metrics. We report results using two complementary metrics: Pairwise Accuracy, which measures correctness on individual preference pairs, and Best-of-N (BoN) Accuracy, which evaluates robustness under multi-candidate settings. Detailed definitions are provided in 4.2. 5.1.2 MAIN RESULTS WebArbiter Significantly Outperforms Baselines. As shown in Tab. 2, WebArbiter achieves the highest Avg. Pairwise Acc and Avg. BoN Acc, surpassing both proprietary and open-source LLMs. While LLM-as-judge methods often maintain moderate Pairwise Acc, their performance drops sharply on BoN Acc, revealing poor robustness to hard negatives. In contrast, WebArbiter sustains strong results on both metrics, establishing its reliability under realistic multi-candidate settings. We further analyze inference-time scaling behavior in Appendix G. Advantage over the SOTA WebPRM. WebShepherd (Chae et al., 2025) represents the previous SOTA WebPRMs. Trained on the same WEBPRM Collection, which was drawn from the Mind2Web environment, WebArbiter-7B achieves an Avg. BoN Acc of 74.60%, surpassing WebShepherd-8B by an absolute gain of 31%. Unlike WebShepherd, which relies on fragile checklists, WebArbiter employs principle-guided reasoning, yielding judgments robust to environment and page variations. Case studies illustrating these differences are provided in Appendix H. Robust Generalization Across Environments. As shown in Tab. 2, it attains SOTA BoN Acc on Mind2Web and WorkArena, and remains competitive with strong proprietary LLMs on WebArena and AssistantBench. These results indicate that principle-guided reasoning enables both effective indomain learning and robust performance in heterogeneous, noisy, and enterprise-scale environments. 5.1.3 ANALYSIS OF TRAINING DESIGN Training Recipes We compare four training variants to disentangle the effects of RL, principle guidance, and justification style. Instruct (Original) denotes purely instruction-tuned model without additional optimization. Instruct + Cold Start RL directly applies RL on top of the instruction model. Instruct + Cold Start RL + Principles augments RL with principle prompting during training, enabling explicit principle induction before judgment. Instruct + SFTw/o Principles + RL performs reasoning distillation without principles, followed by RL, thereby testing whether narrative-style justifications alone are sufficient. As shown in Tab. 3, WebArbiter achieves the best performance. Explicit principles anchor judgments to progress, producing stable supervision under multi-candidate web settings. RL Alone is Unstable Across Web Environments. Cold Start RL performs well on in-domain Mind2Web but collapses on out-of-domain benchmarks. This highlights that reward optimization without reasoning distillation struggles in noisy and complex environments. 8 WebArbiter Table 4: Results on WEBPRMBENCH under full-data and limited-data (10K) training regimes. We report Pairwise and BoN Acc across web environments. Reasoning distillation improves over answeronly SFT, while WebArbiter, i.e., reasoning distillation + RL, achieves the best overall performance. Method Train on Full Data Mind2Web WebArena AssistantBench WorkArena Avg. Pairwise BoN Pairwise BoN Pairwise BoN Pairwise BoN Pairwise BoN Instruct + SFT Instruct + Distilled + SFT WebArbiter-7B (Instruct + Distilled + RL) Train on 10K (Stage-1 Reasoning Distillation) Data 85.14 87.42 97.07 60.91 61.18 89.53 80.85 81.59 88.43 52.73 52.73 68.66 82.50 83.33 89. 56.67 63.33 70.00 79.57 81.13 82.09 52.88 56.73 70.19 82.02 83.37 89.19 55.80 58.49 74.60 Instruct + SFT Instruct + Distilled 84.53 85.20 60.82 63.40 82.21 83.10 58.71 61.80 82.50 83.00 56.67 60. 80.58 81.40 39.62 55.60 82.46 83.18 53.96 60.25 Principles Enable Cross-Environment Generalization. Augmenting RL with principles improves both Avg. Pairwise and BoN Acc, especially on AssistantBench and WorkArena, where real-world tasks require contextand state-dependent judgments beyond surface layout cues. AssistantBench features open-world websites with high structural variability, while WorkArena involves enterprise workflows governed by state-dependent constraints. Principle-guided reasoning provides transferable criteria for assessing true task progress in both cases, improving robustness and generalization. Reasoning Without Principles is Insufficient. SFTw/o Principles + RL, which relies solely on narrativestyle justifications, improves linguistic fluency and coherence of the generated explanations but consistently underperforms principle-aware settings. Without explicit principles to anchor judgment, the model tends to rationalize actions post hoc based on surface plausibility, making it vulnerable to spurious correlations and context-specific cues. As result, narrative reasoning alone is insufficient to reliably track genuine task progress in complex, long-horizon real-world web navigation. Reasoning Supervision We analyze the role of reasoning supervision by comparing answeronly SFT, distilled reasoning, and RL under both full-data and limited-data settings. Instruct + SFT optimizes the instruction-tuned model to directly output the final preference decision, without exposing any intermediate reasoning or justification during training. Instruct + Distilled + SFT runs an answer-only SFT stage on top of the distilled checkpoint, fine-tuning the model directly toward the final decision and serving as controlled comparison to RL-based training. WebArbiter (Instruct + Distilled + RL) further builds upon distilled reasoning by applying RL with verifiable rewards, encouraging principle-guided judgments that better reflect true task progress. Results on WEBPRMBENCH are reported in Tab. 4. Reasoning Distillation Improves Judgment Stability, with RL as an Amplifier. Comparing Instruct + Distilled + SFT with Instruct + SFT, we find that reasoning supervision leads to more reliable reward judgments, particularly in multi-candidate settings measured by BoN Acc. Under the full-data setting, applying answer-only SFT after distillation yields environment-dependent gains, as final-answer optimization can reintroduce shortcut correlations specific to individual web environments. Nevertheless, reasoning distillation induces more stable discrimination among competing trajectories by grounding judgments in true task progress rather than surface-level cues. Building upon this reasoning distillation phase, WebArbiter further applies RL to enlarge the margin between truly progress-making and spurious trajectories, achieving the highest overall performance. Reasoning Supervision Is Especially Effective Under Limited Data. Under the 10K (Stage-1 Reasoning Distillation) setting, Instruct + Distilled consistently outperforms Instruct + SFT across all environments, yielding clear improvements in both Pairwise and BoN Acc. Since both models are trained with identical data budgets, these gains cannot be attributed to data scale, but instead reflect training objective that explicitly biases the model toward progress-aware reward judgments. 5.2 REWARD-GUIDED TRAJECTORY SEARCH 5.2.1 EXPERIMENTAL SETUP AND IMPLEMENTATIONS Reward-guided trajectory search represents one of the most practical applications of PRMs, as it directly leverages fine-grained step-level supervision to improve decision quality during agent 9 WebArbiter Table 5: Success Rates (%) of trajectory search with GPT-4o-mini and GPT-4o as policy on WebArenaLite. * Results reported from the WebShepherd (Chae et al., 2025). is relative to the w/o Trajectory Search baseline. Our WebArbiter consistently achieves the highest gains across both policy models. Policy WebPRM Shopping CMS Reddit GitLab Avg. GPT-4o-mini GPT-4o w/o Trajectory Search* GPT-4o-mini WebShepherd-8B* WebArbiter-7B w/o Trajectory Search* GPT-4o-mini WebShepherd-8B* WebArbiter-7B 21. 24.44 26.09 37.78 23.91 26.67 30.43 44.44 22.86 22.86 45.71 42.86 31. 37.14 42.86 42.86 19.05 26.32 23.81 36.84 28.57 42.11 47.62 52.63 34. 33.33 40.62 46.67 56.25 40.00 46.88 56.67 24.51 26.74 34.06 41.04 35. 36.48 41.95 49.15 +2.23 +9.55 +19.13 +1.44 +6.91 +14. execution. To evaluate WebArbiter in this setting, we conduct experiments on WebArena-Lite1 (Liu et al., 2024b), which contains diverse, long-horizon tasks such as online shopping and content management, closely reflecting real-world web activities. Performance is measured with Success Rate. Following WebShepherd (Chae et al., 2025), we adopt Best-of-N sampling strategy: the policy model generates = 5 candidate actions for each step, and WebArbiter selects the most promising one through Knockout Tournament mechanism (Guo et al., 2025b). We evaluate two policies, GPT-4o-mini (OpenAI, 2024a) and GPT-4o (OpenAI, 2024b). 5.2.2 DOWNSTREAM ANALYSIS ACROSS DOMAINS As shown in Tab. 5, WebArbiter achieves substantial average improvements under both policy models, significantly outperforming all baselines. These gains stem from two main factors. First, reasoning mitigates spurious correlations that often mislead WebPRMs in domains such as Shopping and Reddit. The improvements on Shopping are particularly pronounced, as these tasks require dense semantic retrieval and inference: stronger policies can propose more promising candidate actions, and WebArbiters structured reward modeling further amplifies these advantages. Second, in GitLab, tasks frequently admit multiple equivalent paths. WebShepherd is brittle under such variability, whereas WebArbiter reasons over historical trajectories and the current state to evaluate action validity, enabling stronger generalization in dynamic workflows. We provide qualitative case studies in Appendix to further illustrate these failure modes of checklist-based supervision. By contrast, CMS exhibits more template-driven structure, where actions closely follow standardized patterns. In such settings, checklist-based supervision remains comparatively effective, which narrows the relative performance gap. Overall, WebArbiters reasoning-first design consistently provides robust, interpretable, and scalable supervision across diverse domains."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We presented WebArbiter, reasoning-first, principle-inducing process reward model that frames reward modeling as structured text generation and produces auditable step-level judgments with rationales. Through reasoning distillation and RL, WebArbiter converts superficial correlations into robust, progress-aware signals that verify genuine task advancement, yield consistent steplevel judgments across trajectories, and generalize across dynamic web environments. To support systematic evaluation, we released WEBPRMBENCH, the first comprehensive evaluation benchmark spanning diverse environments for WebPRMs in web navigation, covering four domains with diverse tasks and step-level preference annotations. Extensive experiments demonstrate SOTA performance on WEBPRMBENCH and substantial improvements in reward-guided trajectory search on WebArenaLite, establishing principle-guided reasoning WebPRMs as robust and interpretable foundation for scalable web agents. 1We did not have access to the MAP domain during this work and therefore excluded it from our experiments. 10 WebArbiter"
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Adept. Act-1: Transformer for actions. adept.ai/blog/act-1/ , 2022. Anthropic. Introducing computer use, new claude 3.5 sonnet, and claude 3.5 haiku. anthropic.com/news/3-5-models-and-computer-use, 2024a. Anthropic. Introducing computer use, new claude 3.5 sonnet, and claude 3.5 haiku. https: //www.anthropic.com/news/3-5-models-and-computer-use, October 2024b. Anthropic. Claude 3.7 sonnet and claude code. anthropic.com/news/claude-3-7-sonnet, 2025. Léo Boisvert, Megh Thakkar, Maxime Gasse, Massimo Caccia, Thibault Le Sellier De Chezelles, Quentin Cappart, Nicolas Chapados, Alexandre Lacoste, and Alexandre Drouin. Workarena++: Towards compositional planning and reasoning-based common knowledge work tasks, 2025. URL https://arxiv.org/abs/2407.05291. Hyungjoo Chae, Sunghwan Kim, Junhee Cho, Seungone Kim, Seungjun Moon, Gyeom Hwangbo, Dongha Lim, Minjin Kim, Yeonjun Hwang, Minju Gwak, Dongwook Choi, Minseok Kang, Gwanhoon Im, ByeongUng Cho, Hyojun Kim, Jun Hee Han, Taeyoon Kwon, Minju Kim, Beong woo Kwak, Dongjin Kang, and Jinyoung Yeo. Web-shepherd: Advancing prms for reinforcing web agents, 2025. URL https://arxiv.org/abs/2505.15277. Xiusi Chen, Gaotang Li, Ziqi Wang, Bowen Jin, Cheng Qian, Yu Wang, Hongru Wang, Yu Zhang, Denghui Zhang, Tong Zhang, Hanghang Tong, and Heng Ji. Rm-r1: Reward modeling as reasoning, 2025. URL https://arxiv.org/abs/2505.02387. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. Advances in Neural Information Processing Systems, 36:2809128114, 2023. Alexandre Drouin, Maxime Gasse, Massimo Caccia, Issam H. Laradji, Manuel Del Verme, Tom Marty, Léo Boisvert, Megh Thakkar, Quentin Cappart, David Vazquez, Nicolas Chapados, and Alexandre Lacoste. Workarena: How capable are web agents at solving common knowledge work tasks?, 2024. URL https://arxiv.org/abs/2403.07718. Yao Fu, Dong-Ki Kim, Jaekyeom Kim, Sungryull Sohn, Lajanugen Logeswaran, Kyunghoon Bae, and Honglak Lee. Autoguide: Automated generation and selection of state-aware guidelines for large language model agents. arXiv preprint arXiv:2403.08978, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025a. Jiaxin Guo, Zewen Chi, Li Dong, Qingxiu Dong, Xun Wu, Shaohan Huang, and Furu Wei. Reward reasoning model, 2025b. URL https://arxiv.org/abs/2505.14674. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. Understanding the planning of llm agents: survey, 2024. URL https://arxiv.org/abs/2402.02716. 11 WebArbiter Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks. Advances in Neural Information Processing Systems, 36, 2024. Jing Yu Koh, Stephen McAleer, Daniel Fried, and Ruslan Salakhutdinov. Tree search for language model agents. arXiv preprint arXiv:2407.01476, 2024. Jing Yu Koh, Stephen McAleer, Daniel Fried, and Ruslan Salakhutdinov. Tree search for language model agents, 2025. URL https://arxiv.org/abs/2407.01476. Manling Li, Shiyu Zhao, Qineng Wang, Kangrui Wang, Yu Zhou, Sanjana Srivastava, Cem Gokmen, Tony Lee, Erran Li Li, Ruohan Zhang, et al. Embodied agent interface: Benchmarking llms for embodied decision making. Advances in Neural Information Processing Systems, 37:100428 100534, 2024. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Chris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Jujie He, Chaojie Wang, Shuicheng Yan, Yang Liu, and Yahui Zhou. Skywork-reward: Bag of tricks for reward modeling in llms, 2024a. URL https://arxiv.org/abs/2410.18451. Xiao Liu, Tianjie Zhang, Yu Gu, Iat Long Iong, Yifan Xu, Xixuan Song, Shudan Zhang, Hanyu Lai, Xinyi Liu, Hanlin Zhao, Jiadai Sun, Xinyue Yang, Yu Yang, Zehan Qi, Shuntian Yao, Xueqiao Sun, Siyi Cheng, Qinkai Zheng, Hao Yu, Hanchen Zhang, Wenyi Hong, Ming Ding, Lihang Pan, Xiaotao Gu, Aohan Zeng, Zhengxiao Du, Chan Hee Song, Yu Su, Yuxiao Dong, and Jie Tang. Visualagentbench: Towards large multimodal models as visual foundation agents, 2024b. URL https://arxiv.org/abs/2408.06327. Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, and Juanzi Li. Pairjudge rm: Perform best-of-n sampling with knockout tournament, 2025. URL https://arxiv.org/abs/2501.13007. Xing Han Lù, Amirhossein Kazemnejad, Nicholas Meade, Arkil Patel, Dongchan Shin, Alejandra Zambrano, Karolina Stanczak, Peter Shaw, Christopher J. Pal, and Siva Reddy. Agentrewardbench: Evaluating automatic evaluations of web agent trajectories, 2025. URL https://arxiv.org/ abs/2504.08942. Kaixin Ma, Hongming Zhang, Hongwei Wang, Xiaoman Pan, and Dong Yu. Laser: Llm agent with state-space exploration for web navigation. arXiv preprint arXiv:2309.08172, 2023. Dakota Mahan, Duy Van Phung, Rafael Rafailov, Chase Blagden, Nathan Lile, Louis Castricato, Jan-Philipp Fränken, Chelsea Finn, and Alon Albalak. Generative reward models, 2024. URL https://arxiv.org/abs/2410.12832. Meta. Introducing meta llama 3: The most capable openly available llm to date. ai.meta.com/blog/meta-llama-3/, 2024. Bingchen Miao, Yang Wu, Minghe Gao, Qifan Yu, Wendong Bu, Wenqiao Zhang, Yunfei Li, Siliang Tang, Tat-Seng Chua, and Juncheng Li. Boosting virtual agent learning and reasoning: step-wise, multi-dimensional, and generalist reward model with benchmark, 2025. URL https://arxiv.org/abs/2503.18665. OpenAI. Gpt-4o mini: advancing cost-efficient intelligence. openai.com/gpt-4o-mini, 2024a. OpenAI. Gpt-4o. platform.openai.com/gpt-4o, 2024b. OpenAI. Introducing operator. openai.com/introducing-operator, 2025a. OpenAI. Gpt-5 is here. openai.com/gpt-5, 2025b. WebArbiter Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. URL https://arxiv.org/abs/2203.02155. Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, and Alane Suhr. Autonomous evaluation and refinement of digital agents. arXiv preprint arXiv:2404.06474, 2024. Junsoo Park, Seungyeon Jwa, Meiying Ren, Daeyoung Kim, and Sanghyuk Choi. Offsetbias: Leveraging debiased data for tuning evaluators, 2024. URL https://arxiv.org/abs/ 2407.06551. Sundar Pichai and Demis Hassabis. deepmind.google/models/gemini/flash, 2025. Gemini 2.5 flash. Archiki Prasad, Alexander Koller, Mareike Hartmann, Peter Clark, Ashish Sabharwal, Mohit Bansal, and Tushar Khot. Adapt: As-needed decomposition and planning with language models. arXiv preprint arXiv:2311.05772, 2023. Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Wenyi Zhao, Yu Yang, Xinyue Yang, Jiadai Sun, Shuntian Yao, Tianjie Zhang, Wei Xu, Jie Tang, and Yuxiao Dong. Webrl: Training llm web agents via self-evolving online curriculum reinforcement learning, 2025. URL https://arxiv.org/abs/2411.02337. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/ 2402.03300. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024. Paloma Sodhi, S. R. K. Branavan, Yoav Artzi, and Ryan McDonald. Step: Stacked llm policies for web actions, 2024. Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang. Adaplanner: Adaptive planning from feedback with language models. Advances in Neural Information Processing Systems, 36, 2024. Heyi Tao, Sethuraman TV, Michal Shlapentokh-Rothman, Derek Hoiem, and Heng Ji. Webwise: Web interface control and sequential exploration with large language models. arXiv preprint arXiv:2310.16042, 2023. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. arXiv preprint arXiv:2312.08935, 2023a. 13 WebArbiter Tianlu Wang, Ilia Kulikov, Olga Golovneva, Ping Yu, Weizhe Yuan, Jane Dwivedi-Yu, Richard Yuanzhe Pang, Maryam Fazel-Zarandi, Jason Weston, and Xian Li. Self-taught evaluators, 2024a. URL https://arxiv.org/abs/2408.02666. Zhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams, Makesh Narsimhan Sreedhar, Daniel Egert, Olivier Delalleau, Jane Polak Scowcroft, Neel Kant, Aidan Swope, and Oleksii Kuchaiev. Helpsteer: Multi-attribute helpfulness dataset for steerlm, 2023b. URL https://arxiv.org/abs/ 2311.09528. Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy J. Zhang, Makesh Narsimhan Sreedhar, and Oleksii Kuchaiev. Helpsteer2: Open-source dataset for training top-performing reward models, 2024b. URL https://arxiv.org/abs/2406.08673. Zhepei Wei, Wenlin Yao, Yao Liu, Weizhi Zhang, Qin Lu, Liang Qiu, Changlong Yu, Puyang Xu, Chao Zhang, Bing Yin, Hyokun Yun, and Lihong Li. Webagent-r1: Training web agents via end-to-end multi-turn reinforcement learning, 2025. URL https://arxiv.org/abs/2505. 16421. Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason Weston, and Sainbayar Sukhbaatar. Meta-rewarding language models: Self-improving alignment with llm-as-a-meta-judge, 2024. URL https://arxiv.org/abs/2407.19594. Zhiheng Xi, Yiwen Ding, Wenxiang Chen, Boyang Hong, Honglin Guo, Junzhe Wang, Dingwen Yang, Chenyang Liao, Xin Guo, Wei He, et al. Agentgym: Evolving large language model-based agents across diverse environments. arXiv preprint arXiv:2406.04151, 2024. Ziyi Ye, Xiangsheng Li, Qiuchi Li, Qingyao Ai, Yujia Zhou, Wei Shen, Dong Yan, and Yiqun Liu. Learning llm-as-a-judge for preference alignment. In The Thirteenth International Conference on Learning Representations, 2025. Ori Yoran, Samuel Joseph Amouyal, Chaitanya Malaviya, Ben Bogin, Ofir Press, and Jonathan Berant. Assistantbench: Can web agents solve realistic and time-consuming tasks?, 2024. URL https://arxiv.org/abs/2407.15711. Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240, 2024. Yao Zhang, Chenyang Lin, Shijie Tang, Haokun Chen, Shijie Zhou, Yunpu Ma, and Volker Tresp. Swarmagentic: Towards fully automated agentic system generation via swarm intelligence. arXiv preprint arXiv:2506.15672, 2025a. Yao Zhang, Zijian Ma, Yunpu Ma, Zhen Han, Yu Wu, and Volker Tresp. Webpilot: versatile and autonomous multi-agent system for web task execution with strategic exploration. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 2337823386, 2025b. Yao Zhang, Yu Wu, Haowei Zhang, Weiguo Li, Haokun Chen, Jingpei Wu, Guohao Li, Zhen Han, and Volker Tresp. Groundedprm: Tree-guided and fidelity-aware process reward modeling for step-level reasoning, 2025c. URL https://arxiv.org/abs/2510.14942. Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. The lessons of developing process reward models in mathematical reasoning. arXiv preprint arXiv:2501.07301, 2025d. Longtao Zheng, Rundong Wang, Xinrun Wang, and Bo An. Synapse: Trajectory-as-exemplar In The Twelfth International Conference on prompting with memory for computer control. Learning Representations, 2023. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. URL http://arxiv.org/abs/2403.13372. 14 WebArbiter Enyu Zhou, Guodong Zheng, Binghai Wang, Zhiheng Xi, Shihan Dou, Rong Bao, Wei Shen, Limao Xiong, Jessica Fan, Yurong Mou, Rui Zheng, Tao Gui, Qi Zhang, and Xuanjing Huang. Rmb: Comprehensively benchmarking reward models in llm alignment, 2025. URL https: //arxiv.org/abs/2410.09893. Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023. Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: realistic web environment for building autonomous agents, 2024. URL https://arxiv.org/abs/ 2307.13854."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Related Work 2.1 LLM-Based Autonomous Web Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Reward Models in Reasoning and Web Tasks 3 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1 Background . 3.2 Training Dataset Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 WebArbiter: Principle-Inducing Reasoning Process Reward Model 3.3.1 Training Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Stage 1: Reasoning Distillation . . . . . . . . . . . . . . . . . . . . . . . 3.3.2 Stage 2: Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . 3.3.3 4 WEBPRMBENCH 4.1 Benchmark Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Evaluation Protocol . . . 5 Experiments . . . 5.1 WEBPRMBENCH . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1.1 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1.2 Main Results . 5.1.3 Analysis of Training Design . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Reward-Guided Trajectory Search . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2.1 Experimental Setup and Implementations . . . . . . . . . . . . . . . . . . 5.2.2 Downstream Analysis across Domains . . . . . . . . . . . . . . . . . . . . . . 6 Conclusion Notation Summary Example of Preference Dataset Training Details Prompt Repository Benchmark Construction E.1 Preference Pair Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Dataset Composition and Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . Analysis of BoN Acc vs. Pairwise Acc Evaluation Inference-Time Scaling Case Study: WebArbiter vs. WebShepherd H.1 Milestone Creation under Multiple Equivalent Paths . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . H.2 Merge Request Identification under Ambiguous Context 16 3 3 3 3 4 4 4 5 5 5 6 6 6 7 7 7 8 8 9 9 10 10 17 18 18 19 19 20 21 22 22 23 WebArbiter"
        },
        {
            "title": "A NOTATION SUMMARY",
            "content": "For clarity, we summarize the main notations used throughout this paper: E: web environment, defined by state space S, action space A, and observation space O. : state transition function : S. I: task instruction. sp, op, ap: state, observation, and action at step p. cp: reasoning trace associated with action ap. c<p: reasoning traces of all previously executed actions. τ = (a1, . . . , aP ): trajectory of length . = (j1, . . . , jL): structured justification of length L, consisting of explicit reasoning and final verdict. πθ: WebArbiter model parameterized by θ. ˆy: predicted preference verdict. DTrain, DSFT, DRL: training datasets for supervised distillation and RL. DBench: evaluation dataset for WEBARBITER-BENCH. a+: action from successful trajectory (positive label). or aq: rejected action (q = 1, . . . , Q). Q: number of distractors in WEBARBITER-BENCH (set to 4). : dataset size (training or evaluation). R(x, ˆy): verifiable reward signal."
        },
        {
            "title": "B EXAMPLE OF PREFERENCE DATASET",
            "content": "To illustrate the construction of preference pair, consider task instruction: Instruction I: Find the 2026 conference submission page on the ICLR website. Current observation op: [356] banner header, role=banner [359] link Home [380] button Select Year (2026) [386] button Dates [391] button Calls [396] button Guides [401] button Organization [403] heading ICLR 2026 History (a<p, c<p): a1: Search \"ICLR\", c1: need to find the official ICLR website first. Let me search for ICLR to locate it. a2: Click link \"ICLR homepage\", c2: can see the ICLR official website link in the search results. Clicking on it will take me to the ICLR homepage where can find the conference submission information. Candidate actions and reasoning traces: (a1 p, c1 p): Click link \"Call for Papers\" ; c1 p: can see Call for Papers link on the ICLR homepage. This link would likely lead to the submission details page, which should contain information about the 2026 conference submission process that Im looking for. (a2 p, p): Click \"About\" link; c2 p: can see an About link on the ICLR homepage. Since need to find the 2026 conference submission page, the About section might contain conference overview information including links to submission details or important dates for the 2026 conference. Label y: a1 is preferred. This example is represented in the dataset as: (I, op, a<p, c<p, (a p, c1 p), (a2 p, c2 p), = a1 p). WebArbiter"
        },
        {
            "title": "C TRAINING DETAILS",
            "content": "All training is conducted on 8 NVIDIA A100-80GB GPUs with fixed random seeds. Our training framework is bead on LLama-Factory (Zheng et al., 2024) and VERL (Sheng et al., 2024) Distillation Stage. We train the model for 5 epochs with learning rate of 8e-4, using LoRA with rank of 128. We apply cosine learning rate scheduler with warmup ratio of 0.1. We set the batch size to 256 and the maximum sequence length to 8,192 tokens. RLVR Stage. We employ the VERL framework for GRPO training. The learning rate is set to 7e-6 for the 7B model, and 9e-6 for the 3B variant. The training uses fixed batch size of 512 with mini-batch size of 128, and adopts Fully Sharded Data Parallel (FSDP) for enhanced memory efficiency. For rollout generation, we deploy vLLM with tensor parallelism of 4 and GPU memory utilization limited to 0.4. Response sampling uses standard parameters (temperature=1.0, top-p=1.0), generating 7 candidate responses per prompt. We apply KL regularization with coefficient of 1.0 103 and clip ratio of 0.2. The maximum input sequence length is 8,192 tokens, and the maximum response length is 4,096 tokens."
        },
        {
            "title": "D PROMPT REPOSITORY",
            "content": "WebArbiter You are skilled expert at evaluating assistant responses. You should evaluate given responses based on the given judging criteria.n Given the context of the conversation and two responses from the Assistant, you need to determine the better response. Provide an overall comprehensive comparison upon them. #### Intent #### {intent} #### AXTREE #### Note: [bid] is the unique alpha-numeric identifier at the beginning of lines for each element in the AXTree. Always use bid to refer to elements in your actions. {observation} #### Trajectory #### Note: The trajectory contains the sequence of previous actions and their corresponding thoughts. Each entry reflects the agents internal reasoning (thought) and the concrete operation it performed (action). {trajectory} #### start url #### {start_url} #### current url #### The URL provides clues about the users position in the application flow. Use both the path and query parameters to infer page type (e.g., homepage, search results, product detail, cart, checkout). {current_url} #### Assistant Responses #### [The Begin of Response 1] THOUGHT: {thought1} ACTION: {action1} [The End of Response 1] [The Begin of Response 2] THOUGHT: {thought2} ACTION: {action2} 18 WebArbiter [The End of Response 2] ### Output Instructions ### Format your output strictly using the following XML-style tags: <State>Summarize the current state based on the URL, AXTree, and previous actions. Include what page the user is currently on, and what relevant UI elements or information are visible.</State> <Criteria>Other potential criteria specific to the query and the context, and the weights of each criteria.</Criteria> <Analysis>Compare Response 1 and Response 2 in detail according to the <State> and <Criteria>.</Analysis> <Answer>Response 1 or Response 2</Answer> Rules for <Answer>: - If Response 1 is better, output exactly: <Answer>Response 1</Answer> - If Response 2 is better, output exactly: <Answer>Response 2</Answer> Important Notes: - Be objective and base your evaluation strictly on the content of the responses. - Do not let the response order, length bias your judgment."
        },
        {
            "title": "E BENCHMARK CONSTRUCTION",
            "content": "E.1 PREFERENCE PAIR CONSTRUCTION Positive samples. We construct WEBPRMBENCH using the successful trajectories from AGENTREWARDBENCH, human-verified evaluation suite that aggregates over thousand trajectories generated by multiple LLM-based web agents across diverse real-world environments. Each trajectory in AGENTREWARDBENCH is annotated for success and execution quality by expert annotators, providing reliable source of environment-grounded optimal behavior. From this dataset, we select only those trajectories that complete each task with the minimum number of steps. Each trajectory is independently reviewed by annotators to ensure monotonic progress and to verify that no redundant or detour actions are present. When deviations are identified, annotators revise the trajectory to recover the shortest valid execution path consistent with successful task completion. For consistency, missing reasoning traces are completed to ensure that every stateaction pair is paired with coherent rationale. The resulting actions from these validated minimal-step trajectories serve as positive labels, reflecting actions empirically verified to succeed in the real web environment. Negative samples. For each state, we sample four alternative actions and their associated reasoning from diverse ensemble of policy models, covering both open-source and proprietary LLMs. The pool includes high-capacity instruction-tuned models such as Qwen2.5-7B / 72B-Instruct (Qwen et al., 2025), Llama-3.3-8B / 70B-Instruct (Meta, 2024), as well as frontier commercial models including GPT-4o / 4o-mini (OpenAI, 2024a;b), Claude-3.5-Haiku / Claude-3.7-Sonnet (Anthropic, 2024b; 2025), and Gemini-2.5-Flash / Gemini-2.5-Pro (Comanici et al., 2025). This ensures that alternative actions exhibit broad stylistic and policy diversity rather than reflecting any single models reasoning behavior. Since alternative actions may still succeed under certain web interfaces, we apply rule-based filtering procedure to remove actions that remain potentially valid. We retain only actions that are clearly invalid or non-progressing, ensuring that negative samples correspond to failures under the actual environment dynamics rather than differences in reasoning style. To ensure consistency and avoid false negatives, the filtered actions are manually reviewed, and any remaining actions that appear potentially valid are discarded. If more than four valid rejected actions remain after filtering, we randomly sample subset to maintain consistent number of action pairs per instance. All rationales are truncated to fixed length to reduce formatting noise while preserving semantic content. 19 WebArbiter Table 6: WEBPRMBENCH Website Visit Counts Domain # Domain # Domain service-now.com wa-gitlab-xl-1 wa-openstreetmap-xl-2 wa-forum-xl-1 delta.com redbox.monster wa-shopping-xl-1 spothero.com exploretock.com foxsports.com marriott.com travelzoo.com gamestop.com tesla.com extremeweatherwatch.com 212 wa-openstreetmap-xl-1 23 wa-shopping-admin-xl-1 17 wa-shopping-admin-xl-2 12 wa-shopping-xl-2 duckduckgo.com target.com kohls.com yellowpages.com qatarairways.com ikea.com rentalcars.com yelp.com koa.com cabelas.com 9 7 7 6 5 4 4 4 3 3 1 48 wa-forum-xl-2 21 16 11 google.com ryanair.com last.fm 8 wa-gitlab-xl-2 united.com 7 soundcloud.com 6 amctheatres.com 6 aa.com 5 kayak.com 4 sixflags.com 4 discogs.com 4 3 mta.info 2 rottentomatoes.com # 38 17 12 10 8 7 6 5 4 4 4 3 3 2 Figure 3: Action-type distribution in WEBPRMBENCH. E.2 DATASET COMPOSITION AND STATISTICS The final benchmark consists of 1,150 step-level preference instances across four environments, each containing one environment-verified positive action and four negative alternatives. Website distribution. Tab. 6 summarizes the distribution of visited websites in WEBPRMBENCH, highlighting the diversity and long-tailed nature of real-world web environments covered by the benchmark. Action-type distribution. Fig. 3 reports the action-type distributions of environment-verified positive actions and negative actions in WEBPRMBENCH. In both sets, click and fill constitute the majority of actions, consistent with common interaction primitives in real-world web navigation. The distribution of rejected actions closely mirrors that of chosen actions, with only minor shifts in relative proportions, indicating that negative actions are not dominated by rare or structurally distinct types but arise from the same high-frequency operations as successful actions. As result, action-type identity alone provides no reliable signal for correctness. Effective discrimination, therefore, requires 20 WebArbiter Table 7: Standard deviation of model scores under BoN and Pairwise evaluation across web environments on WEBPRMBENCH. Std. deviation Mind2Web-BoN Mind2Web-pairwise WebArena-BoN WebArena-pairwise AssistantBench-BoN AssistantBench-pairwise WorkArena-BoN WorkArena-pairwise 0.149 0.060 0.153 0.081 0.139 0.093 0.173 0. Figure 4: Correlation between BoN and Pairwise Acc across web benchmarks. Each scatter point corresponds to PRM. We report the correlation coefficient for each environment. While the two metrics are strongly correlated across all environments, BoN exhibits higher variance and provides finer-grained discrimination among models, particularly in complex web environments. assessing whether an action advances task progress under the current state, rather than relying on action-type assumptions or global frequency-based heuristics. ANALYSIS OF BoN Acc VS. Pairwise Acc EVALUATION We analyze how BoN Acc and Pairwise Acc behave as evaluation metrics for WebPRMs on WEBPRMBENCH. This comparison is practically important because WebPRMs are commonly used to rank multiple candidate actions during agent execution, whereas Pairwise Acc only measures correctness on isolated preference pairs. In our benchmark, BoN Acc imposes stricter evaluation criterion by requiring the correct action to outperform all distractors simultaneously, making it more representative of realistic multi-candidate decision-making scenarios. BoN Acc Provides Stronger Discriminative Power Across Environments. Tab. 7 reports the standard deviation of model scores under BoN and Pairwise Acc. Across all four environments, BoN WebArbiter (a) Pairwise Accuracy (b) BoN Accuracy Figure 5: Inference-time scaling of WebArbiter. Left: Pairwise and Right: BoN Acc as the number of sampled reward evaluations increases. Acc consistently exhibits higher variance than Pairwise Acc, indicating substantially less score compression and larger separation among models. This effect is particularly pronounced in WorkArena, where complex interaction dynamics and harder distractors amplify small weaknesses into measurable performance gaps. These results confirm that BoN Acc offers finer-grained discrimination among WebPRMs, especially in settings where robust multi-candidate judgment is required. BoN Acc and Pairwise Acc Are Consistent but Not Equivalent. Fig. 4 shows that BoN Acc and Pairwise Acc are strongly positively correlated across all environments. This indicates that the two metrics capture broadly aligned notions of WebPRM quality and induce similar overall ordering of models. However, the correlation strength varies across environments, reflecting differences in interaction structure and distractor difficulty. INFERENCE-TIME SCALING We further analyze how WebArbiter benefits from increased inference-time compute by varying the number of sampled reward evaluations. As shown in Fig. 5, both Pairwise and BoN Acc improve consistently as the sampling budget increases for WebArbiter-3B and WebArbiter-7B, confirming that the proposed reasoning-based WebPRM supports inference-time scaling. The improvements are moderate for Pairwise Acc but substantially more pronounced under the stricter BoN Acc, highlighting the advantage of additional inference computation in multi-distractor ranking scenarios. CASE STUDY: WEBARBITER VS. WEBSHEPHERD This section presents two GitLab-based case studies that concretely illustrate the failure modes of checklist-driven WebPRMs. These cases highlight how checklist-style supervision can become brittle under structural variability, and how WebArbiters reasoning-based evaluation yields more reliable action preferences. H.1 MILESTONE CREATION UNDER MULTIPLE EQUIVALENT PATHS The task is to create milestone for an upcoming merge operation. At the current step, the agent is on the GitLab project homepage, where the left navigation menu exposes an Issues entry that directly supports milestone management, alongside other entries such as Project information that lead to alternative but non-essential paths. Two candidate actions are considered: navigating through Project information or directly entering Issues, as shown in Fig. 6. WebShepherd evaluates these candidates using checklist-style criteria that emphasize procedurally typical navigation patterns. In GitLab, however, multiple interface paths may lead to the same functionality, and conventionally expected steps are not always necessary in the current context. 22 WebArbiter As result, WebShepherd may favor navigating through Project information despite the fact that milestone creation is already accessible via Issues, introducing an avoidable detour. In contrast, WebArbiter reasons over the current state and task objective to assess whether an action directly contributes to task progress. Observing that the required functionality is already available, it assigns higher preference to entering Issues and deprioritizes redundant navigation steps. This example reflects common characteristic of GitLab workflows: path multiplicity with varying informational value, under which checklist-driven supervision struggles to generalize consistently. H.2 MERGE REQUEST IDENTIFICATION UNDER AMBIGUOUS CONTEXT The second task requires locating specific merge request referenced by 404 link, checking for reply, and responding accordingly. The agent is initially presented with merge request overview page listing multiple candidates, none of which are explicitly linked to the given URL, while global search function is available to resolve this ambiguity, as shown in Fig. 7. The agent can either open one of the visible merge requests or initiate search to identify the correct target. Checklist-based supervision tends to favor actions that satisfy immediate procedural milestones, such as entering merge request page, without explicitly verifying whether the selected entity matches the task specification. Consequently, opening an arbitrary merge request may be preferred even though the tasks referent has not yet been identified. WebArbiter, by contrast, evaluates action validity by reasoning about task preconditions and required evidence. Since identifying the correct merge request is prerequisite for any subsequent review or response, actions that do not support disambiguation are penalized. WebArbiter therefore prefers initiating search and defers content-level interaction until the task context is correctly grounded. This case further illustrates how checklist-based rewards can conflate interaction progress with task progress in dynamic settings, whereas reasoning-based evaluation maintains alignment between actions and task intent. 23 WebArbiter Figure 6: Milestone creation under multiple equivalent paths in GitLab. Checklist-based WebShepherd prefers procedurally typical but non-essential navigation step under path multiplicity, while WebArbiter reasons over the current state and correctly selects the action that directly advances milestone creation. 24 WebArbiter Figure 7: Merge request identification under an ambiguous context. When the target merge request is not yet identified, WebShepherd prematurely commits to an arbitrary request, whereas WebArbiter reasons about task preconditions and prioritizes disambiguation via search."
        }
    ],
    "affiliations": [
        "LMU Munich",
        "Munich Center for Machine Learning (MCML)",
        "Technical University of Munich"
    ]
}