{
    "paper_title": "LaSeR: Reinforcement Learning with Last-Token Self-Rewarding",
    "authors": [
        "Wenkai Yang",
        "Weijie Liu",
        "Ruobing Xie",
        "Yiju Guo",
        "Lulu Wu",
        "Saiyong Yang",
        "Yankai Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a core paradigm for enhancing the reasoning capabilities of Large Language Models (LLMs). To address the lack of verification signals at test time, prior studies incorporate the training of model's self-verification capability into the standard RLVR process, thereby unifying reasoning and verification capabilities within a single LLM. However, previous practice requires the LLM to sequentially generate solutions and self-verifications using two separate prompt templates, which significantly reduces efficiency. In this work, we theoretically reveal that the closed-form solution to the RL objective of self-verification can be reduced to a remarkably simple form: the true reasoning reward of a solution is equal to its last-token self-rewarding score, which is computed as the difference between the policy model's next-token log-probability assigned to any pre-specified token at the solution's last token and a pre-calculated constant, scaled by the KL coefficient. Based on this insight, we propose LaSeR (Reinforcement Learning with Last-Token Self-Rewarding), an algorithm that simply augments the original RLVR loss with a MSE loss that aligns the last-token self-rewarding scores with verifier-based reasoning rewards, jointly optimizing the reasoning and self-rewarding capabilities of LLMs. The optimized self-rewarding scores can be utilized in both training and testing to enhance model performance. Notably, our algorithm derives these scores from the predicted next-token probability distribution of the last token immediately after generation, incurring only the minimal extra cost of one additional token inference. Experiments show that our method not only improves the model's reasoning performance but also equips it with remarkable self-rewarding capability, thereby boosting its inference-time scaling performance."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 3 4 9 4 1 . 0 1 5 2 : r LaSeR: Reinforcement Learning with Last-Token Self-Rewarding 2025-10Wenkai Yang1,, Weijie Liu2, Ruobing Xie2, Yiju Guo1, Lulu Wu2, Saiyong Yang2, Yankai Lin1, 1Gaoling School of Artificial Intelligence, Renmin University of China 2LLM Department, Tencent (cid:66) {wenkaiyang,yankailin}@ruc.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as core paradigm for enhancing the reasoning capabilities of Large Language Models (LLMs). To address the lack of verification signals at test time, prior studies incorporate the training of models selfverification capability into the standard RLVR process, thereby unifying reasoning and verification capabilities within single LLM. However, previous practice requires the LLM to sequentially generate solutions and self-verifications using two separate prompt templates, which significantly reduces efficiency. In this work, we theoretically reveal that the closed-form solution to the RL objective of self-verification can be reduced to remarkably simple form: the true reasoning reward of solution is equal to its last-token self-rewarding score, which is computed as the difference between the policy models next-token log-probability assigned to any pre-specified token at the solutions last token and pre-calculated constant, scaled by the KL coefficient. Based on this insight, we propose LaSeR (Reinforcement Learning with Last-Token Self-Rewarding), an algorithm that simply augments the original RLVR loss with MSE loss that aligns the last-token self-rewarding scores with verifier-based reasoning rewards, jointly optimizing the reasoning and self-rewarding capabilities of LLMs. The optimized self-rewarding scores can be utilized in both training and testing to enhance model performance. Notably, our algorithm derives these scores from the predicted next-token probability distribution of the last token immediately after generation, incurring only the minimal extra cost of one additional token inference. Experiments show that our method not only improves the models reasoning performance but also equips it with remarkable self-rewarding capability, thereby boosting its inference-time scaling performance. Code and models are available at https://github.com/RUCBM/LaSeR."
        },
        {
            "title": "Introduction",
            "content": "In the past few years, Large Language Models (LLMs) (Achiam et al., 2023; MetaAI, 2024a; Qwen Team, 2024; Liu et al., 2024a) have advanced significantly, excelling in various domains (Li et al., 2023; Wang et al., 2024b). However, they still face limitations in complex reasoning tasks (AI-MO, 2024a; OpenCompass, 2025; Rein et al., 2024; Jain et al., 2025). Recently, Reinforcement Learning with Verifiable Rewards (RLVR) has shown great promise in enhancing the complex reasoning abilities of LLMs, as demonstrated by OpenAI o1 (Jaech et al., 2024) and DeepSeek-R1 (Guo et al., 2025). By rewarding reasoning paths based on the consistency between final outcomes and ground-truth answers through deterministic verifier, RLVR incentivizes LLMs to produce more deliberate reasoning chains while effectively mitigating the risk of reward hacking (Gao et al., 2023). Despite its effectiveness, limitation of standard RLVR is its inability to continue providing verification signals for model outputs in scenarios where ground truth answers are unavailable, such as during test-time inference (Zuo et al., 2025). To address this, existing works either train an external verifier (Lightman et al., 2023; Snell et al., 2024; Zhang et al., 2024; Gao et al., 2024; Yang et al., 2025b) for evaluating candidate solutions or jointly optimize the self-verification and reasoning capabilities of the same policy model during RLVR (Sareen et al., 2025; Liu et al., 2025a; Zha et al., 2025; Jiang et al., 2025). However, we argue that these methods have major issue of inefficiency: the external verifier requires additional training on separate LLM during or after reinforcement learning (RL); while joint optimization involves generating both solutions and self-verifications sequentially under two separate prompt templates, which doubles the per-sample inference cost and reduces generation efficiency. In this work, we propose LaSeR (Reinforcement Learning with Last-Token Self-Rewarding), lightweight and highly effective algorithm that achieves this goal, jointly optimizing reasoning and self-verification capabilities at nearly zero additional cost. Our core insight is that models assessment in its own solution can be captured in the last tokens predicted probability distribution. We first show theoretically that the RL objective of selfverification has closed-form solution, where the true reasoning reward from the verifier is equal to the next-token log-probability ratio between the policy and reference models for pre-specified special token (an unused token Work done during an internship at Tencent. Corresponding author. 1 Figure 1: The full illustration of our method LaSeR. During training, our approach augments the standard RLVR process with an additional MSE loss between the verifier-based rewards (rv) and the last-token self-rewarding scores (rs), where rs is the difference between the policy models next-token log-probabilities of pre-specified special token at the final response token and pre-calculated constant cre , scaled by the KL coefficient βv. The optimized self-rewarding scores can serve as auxiliary reward signals in both training and testing stages to enhance model performance. like <vision start> that serves as the pre-defined ground truth for verifications on correct candidate solutions) at the last response token, scaled by the KL coefficient. We refer to this scaled log-probability ratio as the last-token self-rewarding score. Furthermore, we observe that for randomly chosen special token, its predicted log-probability under the reference model is practically constant, small value across all problems and solutions (see Figure 5). This enables us to simplify the self-rewarding score into remarkably simple form that depends only on the policy models outputs and pre-calculated constant, making it exceptionally efficient to compute. Building on above analysis, we replace the explicit RL optimization for self-verification with simple Mean Squared Error (MSE) loss. As illustrated in Figure 1, we train the model to align its last-token self-rewarding score with the true reasoning reward from the verifier. In specific, after the policy model generates the solution for each problem, we calculate the last-token self-rewarding score based on its last tokens next-token log-probability for the pre-specified special token, and construct the corresponding MSE loss. This MSE objective is added directly to the standard RLVR loss, allowing for seamless joint optimization for both the reasoning and self-rewarding capabilities of the policy model. At both training and testing time, our method generates each candidate solution and computes the self-rewarding score in single forward pass, incurring the cost of at most one additional token inference with no extra generation required. This is significantly more efficient than prior approaches, which require separate inference step. The optimized self-rewarding scores can not only complement the original reasoning rewards during RLVR to further enhance training performance, but also be used at test time to rank and weight solutions for more accurate answer aggregation. We conduct experiments on both LLaMA (MetaAI, 2024b) and Qwen (Qwen Team, 2024) architectures, including pre-trained, mid-trained and reinforced variants, to demonstrate the effectiveness of our method in broader math reasoning tasks. Experimental results show that our methods not only effectively improve the reasoning performance of the policy model, but also allows its self-rewarding accuracy to reach high level, thereby equipping the model with better confidence calibration of its own outputs and improving its inference-time scaling performance."
        },
        {
            "title": "2 Related Work",
            "content": "RLVR for LLM Reasoning Reinforcement Learning with Verifiable Rewards (RLVR), which sorely calculates binary rewards based on the final answers, has been shown to be highly effective in enhancing the reasoning capabilities of LLMs (Jaech et al., 2024; Guo et al., 2025; Team et al., 2025b). Current studies can be categorized into several directions, including but not limited to (1) designing more efficient and effective RLVR algorithms (Schulman et al., 2017; Shao et al., 2024; Yu et al., 2025a; Yue et al., 2025b; Liu et al., 2025b; Zheng et al., 2025), (2) extending RLVR to general reasoning domain (Ma et al., 2025; Zhou et al., 2025; Yu et al., 2025b; Li et al., 2025) and agent 2 scenarios (Wang et al., 2025b; Team et al., 2025a; Dong et al., 2025), (3) collecting diverse verifiable datasets (Hu et al., 2025; He et al., 2025; Liu & Zhang, 2025; Ma et al., 2025; Fan et al., 2025), and (4) analyzing the mechanisms of RLVR (Mukherjee et al., 2025; Yue et al., 2025a; Wen et al., 2025; Huan et al., 2025). External Verifiers for LLM Reasoning Training external verifiers to identify the correctness of the LLM-generated solutions is an effective way to enhance the reasoning performance of LLMs in the inference time. External verifiers usually fall into two categories: (1) Scalar Reward Models: Outcome-supervised Reward Models (ORMs) (Cobbe et al., 2021; Yang et al., 2024) and Process-supervised Reward Models (PRMs) (Lightman et al., 2023; Wang et al., 2024a; Skywork-o1, 2024; Yuan et al., 2024) are two representative approaches. ORMs provide supervision by evaluating the final answer, while PRMs offer more fine-grained feedback by assessing the intermediate reasoning steps. (2) Generative Verifiers: Recent studies have explored the potential of training LLMs to perform natural language critiques of reasoning solutions generated by the LLM generators, and then to judge their final outcomes (Zhang et al., 2024; Gao et al., 2024; Yang et al., 2025b; Zhao et al., 2025). This paradigm has demonstrated stronger verification performance than scalar reward models, as it enables the LLM verifier to conduct deliberate chain-of-thought reasoning before arriving at the final judgment. Self-Verification for LLM Reasoning Several recent studies (Sareen et al., 2025; Liu et al., 2025a; Zha et al., 2025; Jiang et al., 2025; Lu et al., 2025) aim to unify the roles of generator and verifier by equipping single policy model with self-verification capability. The trained self-verification capability can be used in both the RL training and inference-time scaling stages to enhance the model performance. However, these approaches require generating solutions and self-verifications sequentially during training and inference. In contrast, our method derives the self-rewarding signal directly from the next-token probability distribution of the final token of the generated sequence, achieving more efficient and effective unification of generation and self-verification."
        },
        {
            "title": "3 Methodology",
            "content": "3.1 Preliminaries RL Objective We denote πθ as the target policy model to be optimized, and πref as the reference model from which πθ is initialized. is the query set, is an input and is the generated response to x. The standard optimization objective of RL is formalized as Oπθ = max πθ xD,yπθ(x) (cid:104) r(x, y) βDKL(πθπre ) (cid:105) , (1) where r(x, y) represents reward function to score the response given x, DKL is the KullbackLeibler (KL) divergence loss regularizing the distance between two model distributions. RLVR Recently, RLVR (Guo et al., 2025; Hu et al., 2025) has emerged as widely adopted and effective paradigm In RLVR, the reward function is typically chosen as for enhancing the reasoning capabilities of LLMs. deterministic verifier rv, such as rule-based verifier, to evaluate whether the final extracted answer matches the ground-truth answer a, and to produce binary feedback (e.g., {0,1}). That is, rv(x, y) = 1 {aa} = (cid:26)1 0 if is semantically equivalent to a, otherwise. (2) Policy Gradient Method Policy Gradient (Sutton et al., 1998) is widely adopted algorithm to optimize the objective of Eq. (1), which updates the policy model via the estimated gradient xD,yπθ(x) θOπθ = (cid:34) t=1 where At is the advantage function measuring the relative value of the action at (i.e., token yt) compared to the baseline value under state st (i.e., sequence (x, y<t)). In practice, At can be estimated in various ways (Schulman et al., 2017; Ahmadian et al., 2024). For example, Group Relative Policy Optimization (GRPO) (Shao et al., 2024) estimates the baseline value as the average reward within sampled group {y1, , yK} for the same problem, and in sequence yi as computes the relative advantage for each token yi Ai ))/std(r1 Atθ log πθ(ytx, y<t) = rv(x, yi). ri mean(r v, , rK v, , rK = (ri ), (4) (3) (cid:35) , Implicit Reward Previous studies (Rafailov et al., 2023; Peters & Schaal, 2007) have identified that the optimal solution to the objective Eq. (1) satisfies that rv(x, y) = β log[πθ(yx)/πre (yx)] + β log Z(x), β rv(x, y)) is partition function. β log πθ(yx) where Z(x) = πre (yx) exp( 1 πre (yx) is termed as the implicit reward, which has been used in prior works (Mitchell et al., 2024; Liu et al., 2024b) to analyze the behavioral shift induced by the alignment process. (5) 3.2 LaSeR: Reinforcement Learning with Last-Token Self-Rewarding 3.2.1 Formal Formulation In training, ground-truth answers can be reliably used to determine the correctness of solutions. At test time, however, when ground-truth answers are unavailable, the use of verifiers becomes crucial for evaluating solution quality and providing feedback signals. To address this problem, in this work, we explore the promising paradigm of jointly optimizing the self-verification and reasoning capabilities of LLMs within the RLVR framework, thereby enabling them not only to produce high-quality reasoning paths but also to evaluate their own outputs at test time. According to Eq. (5), as Z(x) remains the same for all y, straight-forward idea is to utilize the implicit reward β log πθ(yx) πre (yx) as the indicator to rank different generations at test time. However, this approach has critical drawback: the absolute value of the implicit reward is length-biased, since the absolute value of β log πθ(yx) πre (yx) = β log πθ(yix,y<i) πre (yix,,y<i) increases proportionally with the response length. In reasoning tasks, the incorrect solutions are usually longer than the correct solutions (Hassid et al., 2025), making the implicit reward unreliable in evaluating solution correctness (see Appendix A). Furthermore, disregarding Z(x) and directly aligning the implicit reward with the true reasoning reward during training degrades the policy models generation ability (Cui et al., 2025), since fundamental gap (i.e., β log Z(x)) exists between the solution to RLVR and that to reward modeling. In this work, we begin by formulating our approach from the RL objective of verification. Given problem x, and candidate solution y, the model is required to produce verification to identify the correctness of the solution: πθ(x, y). Thus, the RL objective of verification can be written as Vπθ = max πθ xD,yπg(x),zπθ(x,y) (cid:104) ˆr(x, y, z) βvDKL(πθπre ) (cid:105) , (6) where πg is the generator to solve the problem (can also be the target model πθ itself in the self-verification setting), ˆr(x, y, z) is the verification reward that measures the consistency between the true correctness of and the verification result of z: ˆr(x, y, z) = (cid:26)1 0 if verification result of matches the true correctness of y, otherwise. (7) In practice, can be either single tokenfor instance, Yes or No to directly indicate whether the solution is verified as correct or incorrector sequence that includes both chain of thought and the final judgment. In this work, we focus on the former setting and simplify the ground-truth label space to two single tokens zc (e.g., Yes) and zi (e.g., No). That is, the verification reward is simplified to ˆr(x, y, z) = (cid:26)1 0 (z = zc and rv(x, y) = 1) or (z = zi and rv(x, y) = 0) otherwise. (8) Similarly, following from Eq. (5), the close-form solution to Eq. (6) can be written as ˆr(x, y, z) = βv log πθ(zx, y) πre (zx, y) + βv log Z(x, y), Z(x, y) = πre (zx, y) exp( 1 βv ˆr(x, y, z)). (9) Now, lets take closer look at Z(x, y). First, for {zc, zi}, πre (zx, y) is extremely small positive value for any problem-solution pair (x, y), i.e., πre (zx, y) 0, for {zc, zi}. The reason is that the model is not specifically optimized for predicting the next token once it completes the generation and produces the final token (typically the <EOS> token). We present numerical analysis to validate this claim in Figure 5, and we can see the value of πre (zx, y) is less than e9 for common tokens and even less than e20 for unused special tokens. Then, we can get that Z(x, y) = πre (zx, y) exp( 1 βv ˆr(x, y, z)) = πre (zx, y) exp( z/{zc,zi} 1 βv ˆr(x, y, z)) + πre (zcx, y) exp( 1 βv ˆr(x, y, zc)) + πre (zix, y) exp( 1 βv ˆr(x, y, zi)) = (1 πre (zcx, y) πre (zix, y)) exp(0) + (πre (zcx, y) + πre (zix, y)) exp( (10) 1 βv ) 1 1 + 0 exp( 1 βv ) = 1 = log Z(x, y) 0. The above analysis reveals that, under our formulation, the partition function that cannot be ignored by previous studies (Cui et al., 2025) can be now naturally discarded. Consequently, the optimal solution to Eq. (6) can be approximately reduced to: ˆr(x, y, z) = βv log[πθ(zx, y)/πre (zx, y)]. (11) 4 In particular, the true verification reward when the model verifies solution as correct is: ˆr(x, y, zc) = rv(x, y) = βv log[πθ(zcx, y)/πre (zcx, y)]. (12) The first equation is derived from the definition in Eq. (8). The second equation reveals that the true reasoning reward is equal to log-probability ratio of the policy model to the reference model at zc, scaled by the KL coefficient. Thus, to optimize the models verification capability, we do not need to explicitly perform RLVR procedure. Instead, we can directly optimize the following MSE loss: = xD,yπg(x) (cid:16) βv log[πθ(zcx, y)/πre (zcx, y)] rv(x, y) (cid:17)2 . (13) Thus, in the self-verification setting where πg = πθ, we can directly adds the above loss into the original RLVR loss to jointly optimize the reasoning and self-verification capabilities of the policy model: Sπθ = max πθ xD,yπθ(x) rv(x, y) βDKL(πθ(yx)πre (yx)) α βv log (cid:34) πθ(zcx, y) πre (zcx, y) rv(x, y) (cid:35)2 , (14) where α is loss balancing coefficient. We refer the term rs = βv log πθ(zcx,y) score, since it depends on the log-probability distributions of the last token in y. πre (zcx,y) to the last-token self-rewarding 3.3 Other Techniques Here, we discuss several practical techniques to further simplify and improve the efficiency and effectiveness of the self-rewarding MSE loss introduced above. Simplification of the Log-Probability in the Reference Model As shown in Figure 5, the quantity log πre (zcx, y) remains almost constant, exhibiting only negligible standard deviation across all and y. Therefore, we can regard it as pre-calculated constant cre in calculating the last-token self-rewarding score during both training and inference. This eliminates the need for forwarding through the reference model and thus further enhances efficiency. In specific, cre is the mean value of log πre (zcx, y) on small set of pre-generated set of (x, y). Furthermore, based on the findings in Figure 5, we select an unused special token as zc to make πre (zcx, y) closer to 0 and to further minimize its impact on the approximation of Z(x, y) = 1 and the stability of training. Self-Rewarding Loss Re-Weighting During training, the numbers of correct and incorrect solutions are imbalanced, and their ratio dynamically changes. To prevent the last-token self-rewarding score from being biased toward the class with more samples, we apply class-level loss re-weighting strategy within each optimization step. In each step, we calculate the total numbers of correct and incorrect solutions (identified by the deterministic verifier) for all problems in the current batch as Nc and Ni. Then, we apply the loss re-weighting as = 1 Nc + Ni (cid:104) wc1 {rv(x,y)=1} + wi {rv(x,y)=0} (cid:105) (cid:104) βv log πθ(zcx, y) βvcre rv(x, y) (cid:105)2 , (15) where wc = Nc+Ni are re-weighting factors. This practice achieves more balanced self2Nc verification capability. We provide empirical validations on this in Appendix E. Future work can explore more effective ways to address the issue of imbalanced distribution of solutions. and wi = Nc+Ni 2Ni Integration of Verifier-based and Self-Rewarding-based Advantages The last-token self-rewarding scores can not only be used at test time, but also facilitate the training process through the integration of verifier-based and self-rewarding-based advantages. We believe such practice can help mitigate the issue of misjudgments by rule-based verifiers, which often occur when the format of ground-truth answer is overly complex, and produce more fine-grained rewards. For example, in GRPO, the final advantage can be calculated as: ˆAi = (1 τ) where ri ri mean(r1 std(r1 = rv(x, yi) and ri v, , rK , , rK ) ) v, , rK , , rK ) ) = βv log πθ(zcx, yi) βvcre . ri mean(r1 std(r1 + τ , (16) To stabilize training, we adopt filtering strategy that sets τ = 0 for any group whenever the standard deviation std(r1 ) within this group falls below threshold T, which is set to 0.1. , , rK Separate Warm-Up of Reasoning and Self-Rewarding Capabilities During the initial phase of training, we optimize only the last-token self-rewarding score, without integrating self-rewarding-based advantages into the learning process. After certain steps when the last-token self-rewarding loss is sufficiently small, we proceed to integrate verifier-based and self-rewarding-based advantages. Moreover, when training from base (i.e., pre-trained) models, we first perform standard RLVR without incorporating the last-token self-rewarding loss in order to warm 5 Algorithm 1: LaSeR: Reinforcement Learning with Last-Token Self-Rewarding Input: Initial policy model πθ, prompts D, verifier rv, warm-up hyper-parameters wr and wsr, coefficient βv, pre-specified token zc, pre-calculated cre = (x,y)[log πre (zcx, y)] for Step = 1, , do 1. Set πold πθ; 2. Sample batch prompts Ds from D; 3. Generate solutions {yi}K 4. Calculate verifier-based rewards and advantages (e.g., Eq. (4)), calculate RL loss; 5. If wr, calculate last-token self-rewarding loss based on Eq. (15) and add it to RL loss; 6. If wsr, calculate self-rewarding-based advantages and perform advantage integration based on Eq. (16); 7. Update the policy model πθ using any RL algorithm with integrated loss and advantages; i=1 for each Ds; end Output: πθ up the models reasoning capability, followed by warm-up phase for the self-rewarding capability before the complete integration of verifier-based and self-rewarding-based advantages. By combining all the aforementioned techniques, our full algorithm Reinforcement Learning with Last-Token Self-Rewarding (LaSeR), is summarized in Algorithm 1 and illustrated in Figure 1. During the testing phase, once the model generates solution, we compute the last-token self-rewarding score based on rs = βv log πθ(zcx, y) βvcre . The comparison between this score and 0.5 determines the self-verification outcome of the solution, or the score itself can be further used to perform weighted majority voting. 3.4 Brief Discussion Comparison Between LaSeR and Prior Approaches Compared with previous methods (Sareen et al., 2025; Liu et al., 2025a; Zha et al., 2025) that requires the policy model to perform separate generations for solutions and verifications, our method directly derives the self-rewarding result from the next-token log-probability of the final solution token. In the RL process, the computation of token log-probabilities is typically carried out after all the generations are completed (Sheng et al., 2024). Therefore, we can directly replace the token id of the first padding token with the token id of the pre-specified token before computing the log-probabilities of the sequences, thereby incurring no additional computation cost during training. During inference, our method requires only one more token inference after the solution is completed, which substantially reduces the computational cost compared to previous methods. We also discuss the potential way to further reduce the self-rewarding cost by avoiding any extra token inference in Section 5.3, which can be an interesting future work. Difference Between Last-Token Self-Rewarding Loss and Supervised Fine-Tuning Loss An alternative to train the self-verification capability is to optimize the following supervised fine-tuning (SFT) loss by maximizing the next-token probability of the token zc or zi based on the context (x, y): LSFT = xD,yπg(x) [rv(x, y) log πθ(zcx, y) + (1 rv(x, y)) log πθ(zix, y)] . (17) The major difference between SFT loss and our last-token self-rewarding loss in Eq. (13) is that the SFT loss drives πθ(zcx, y) to fit 1 when rv(x, y) = 1, which may lead to strong interference with the optimization of reasoning capability. In contrast, our loss drives πθ(zcx, y) toward exp(1/βv) πref(zcx, y) for rv(x, y) = 1.0. When βv is relatively large, πθ(zcx, y) remains still very small, thereby exerting only negligible influence on the original RLVR optimization (e.g., πθ(zcx, y) = e13 when πref(zcx, y) = e23 and βv = 0.1). We provide the empirical comparison in Appendix F."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Settings Base Models and Baselines We primarily conduct empirical validations on both LLaMA3.2 MetaAI (2024b) and Qwen2.5 (Qwen Team, 2024) architectures, including three base models: OctoThinker-3B-Short-Base (Wang et al., 2025a) (mid-trained version of LLaMA3.2-3B-Base), Qwen2.5-7B-Base (Qwen Team, 2024) (pre-trained model) and Open-Reasoner-Zero-7B (Hu et al., 2025) (reinforced version of Qwen2.5-7B-Base). In principle, our method can be seamlessly integrated into any RLVR framework, as it only introduces an additional MSE loss term. In this work, we adopt the widely used GRPO (Shao et al., 2024) as the base algorithm and primarily investigate the effectiveness of applying our method within GRPO, while leaving the exploration on other RL algorithms in the future work. 6 Table 1: Reasoning and self-verification performance of each model on five mathematical reasoning benchmarks. We do not report the results of OctoThinker-based models on AIME24-25, as the number of correct solutions is quite insufficient for reliable evaluation. Reasoning Accuracy Self-Verification F1 Score Method MATH500 AMC23 AIME24 AIME25 Olym.- Bench Avg. MATH500 AMC23 AIME24 AIME25 Olym.- Bench Avg. OctoThinker-3B-Short-Base Base GRPO LaSeR - SWA 3.7 49.8 53.1 52.9 Qwen2.5-7B-Base Base GRPO LaSeR - SWA 35.8 79.9 80.2 78.0 1.3 25.3 27.0 26.1 20.6 55.9 58.1 58.3 Open-Reasoner-Zero-7B Base GRPO LaSeR - SWA 81.9 83.1 82.8 83. 60.3 61.9 62.7 62.6 - - - - 3.5 16.2 15.4 15.4 15.6 18.1 19.1 19.0 - - - - 1.6 13.8 15.7 12. 15.1 15.0 15.1 14.5 1.0 17.3 18.2 18.2 12.3 43.3 44.1 41.7 46.9 47.1 47.8 47.6 2.0 30.8 32.8 32.4 14.8 41.8 42.7 41. 44.0 45.0 45.5 45.4 22.3 56.9 73.6 80.4 36.4 54.6 83.2 79.7 26.7 57.1 87.2 87.5 11.2 47.3 70.2 70.9 30.8 59.7 82.5 80. 51.3 44.8 79.7 77.7 - - - - 27.6 36.6 79.6 81.3 45.9 14.6 64.6 63.3 - - - - 32.9 41.5 74.3 74. 55.2 28.1 77.7 77.3 13.7 48.8 73.6 66.0 36.9 53.5 78.3 83.3 37.5 49.5 78.7 77.9 15.7 51.0 72.5 72.4 32.9 49.2 79.6 79. 43.3 38.8 77.6 76.7 Training and Evaluation Datasets We adopt DeepMath-103K (He et al., 2025), large-scale and high-quality mathematical reasoning dataset, for our RL training data. In testing, we evaluate both the reasoning and self-verification performance of each model on five typical math reasoning benchmarks: MATH500 (Hendrycks et al., 2021), AMC23 (AI-MO, 2024b), AIME24 (AI-MO, 2024a), AIME25 (OpenCompass, 2025), and OlympiadBench (He et al., 2024). We also explore the effectiveness of our method in general reasoning tasks beyond math reasoning in Section 5.2. Training Settings The detailed training hyper-parameters of GRPO are put in Appendix C. The prompt template for each model is in Appendix I. When applying our method, we set the hyper-parameters (βv, α, τ) = (0.1, 0.1, 0.1), which are empirically determined based on the observations in Appendix D. zc is selected as <vision start> for Qwen2.5-7B-Base and Open-Reasoner-Zero-7B, and <reserved special token 0> for OctoThinker3B-Short-Base. The simplified constant of the reference log-probability, cref, is 23.0 for Qwen2.5-7B-Base and Open-Reasoner-Zero-7B, and 25.0 for OctoThinker-3B-Short-Base, as estimated from the results in Figure 5. The number of reasoning warm-up steps is set to 200 for both Qwen2.5-7B-Base and OctoThinker-3B-Short-Base, and the number of self-rewarding warm-up steps is 200 across all models. Evaluation Settings During generation, we set both the temperature and top to 1.0 for all models. The max generation len is 8192. On MATH500 and OlympiadBench, we sample 2 solutions for each problem; whereas on AMC23, AIME24, and AIME25, we sample 32 solutions per problem. We then report the average Pass@1 accuracy of each model on each benchmark. We also evaluate the self-verification performance of each model by computing the self-verification F1 score, defined as the harmonic mean of self-verification accuracy on self-generated correct and incorrect solutions. Baselines perform self-verification based on the prompt in Appendix I. Any solution without final answer is automatically treated as incorrect and excluded from the verification accuracy calculation. Detailed self-verification accuracy results are provided in Appendix G. 4.2 Main Results and Analysis We put the main results in Table 1. The key conclusion is that, across different model variants, our method not only yields better reasoning performance for the policy model compared with the baseline, but also substantially enhances its self-verification capability by enabling the self-rewarding scores to achieve remarkably high F1 scores. Regarding reasoning performance, applying our algorithm leads to higher accuracy in most settings and consistently yields higher average accuracy on the three base models. We think there are two main reasons for this improvement: (1) First, our method encourages the model to encode its assessment of the overall solution in the final response token, which leads to better confidence calibration. Improved calibration itself can have positive impact on the models learning. (2) Second, by integrating self-rewarding-based advantages into verifier-based advantages, our approach enables more fine-grained advantage estimation, which in turn facilitates more effective learning. For comparison, we report the results without self-rewarding-based advantages (-SWA) in Table 1. Regarding self-rewarding performance, applying simple last-token self-rewarding MSE loss substantially enhances 7 Table 2: Comparison of verification F1 scores between LaSeR (self-rewarding) and external reward models (Qwen2.5-Math-7B-PRM800K, Qwen2.5-Math-PRM-7B, and Qwen2.5-Math-RM-72B) on responses generated by different policy models. Method MATH500 AMC23 AIME24 AIME25 Olym. Avg. Generator: OctoThinker-3B-Short-LaSeR Qwen2.5-Math-7B-PRM800K (7B RM) Qwen2.5-Math-PRM-7B (7B RM) Qwen2.5-Math-RM-72B (72B RM) LaSeR (3B Self-Rewarding) Generator: Qwen2.5-7B-Laser Qwen2.5-Math-7B-PRM800K (7B RM) Qwen2.5-Math-PRM-7B (7B RM) Qwen2.5-Math-RM-72B (72B RM) LaSeR (7B Self-Rewarding) 77.0 80.9 89.2 73.6 59.4 82.5 87.8 83. Generator: Open-Reasoner-Zero-7B-LaSeR Qwen2.5-Math-7B-PRM800K (7B RM) Qwen2.5-Math-PRM-7B (7B RM) Qwen2.5-Math-RM-72B (72B RM) LaSeR (7B Self-Rewarding) 56.3 86.0 86.8 87.2 68.9 63.5 71.7 70.2 52.7 79.2 80.7 82.5 42.5 79.6 79.4 79. - - - - 58.8 75.1 81.3 79.6 51.4 70.8 71.0 64.6 - - - - 53.8 72.3 74.8 74.3 50.8 67.3 71.4 77. 68.5 64.1 72.9 73.6 52.0 77.8 75.4 78.3 38.5 76.0 75.5 78.7 71.5 69.5 77.9 72.5 55.3 77.4 80.0 79.6 47.9 75.9 76.8 77. Table 3: Comparison of reasoning and self-verification performance with and without reference log-probability simplification in our method. Based model is Open-Reasoner-Zero-7B. Reasoning Accuracy Self-Verification F1 Score Method MATH500 AMCAIME24 AIME25 Olym.- Bench w/ Simpl. w/o Simpl. 82.5 81.0 61.6 61. 18.8 17.3 16.2 17.3 46.5 48.3 Avg. 45.1 45.0 MATHAMC23 AIME24 AIME25 Olym.- Bench 82.3 81.8 79.3 79. 77.9 79.0 79.2 78.9 78.4 77.5 Avg. 79.4 79.3 the self-rewarding capability of the models, achieving around 80% self-verification F1 scores. This demonstrates strong self-verification accuracy on both correct and incorrect solutions. To further highlight the self-rewarding capabilities, we display the comparison results of verification F1 scores between LaSeR and three advanced external reward models (Qwen2.5-Math-7B-PRM800K (Zhang et al., 2025), Qwen2.5-Math-PRM-7B (Zhang et al., 2025), and Qwen2.5-Math-RM-72B (Yang et al., 2024)) on evaluating the solutions generated by the different reinforced models, i.e., OctoThinker-3B-Short-LaSeR, Qwen2.5-7B-LaSeR, and Open-Reasoner-Zero-7B-LaSeR. The results in Table 2 show that LaSeR outperforms equally sized state-of-the-art external verifiers in assessing the models own solutions, and even matches the verification performance of 72B reward model, demonstrating its non-trivial effectiveness in enhancing self-rewarding capability. Also, our method requires one additional token inference only to compute the self-rewarding scores for enabling the policy model to function simultaneously as both the generator and reward model, which is highly efficient and practical. 4.3 Inference-Time Scaling Results Here, we explore the effectiveness of self-rewarding in the inference-time scaling via weighted majority voting. We compare majority voting results with (RM@K) and without (Maj@K) weighting by the last-token self-rewarding scores, on MATH500 and OlympiadBench. The results are shown in Figure 2. We denote the three base models by OT-3B, Qwen2.5-7B, and ORZ-7B. The suffixes -GRPO and -LaSeR indicate the variants trained with GRPO and our method LaSeR, respectively. The results show that the optimized self-rewarding capability of the model is highly effective on improving its own inference-time scaling performance."
        },
        {
            "title": "5 Analysis and Discussion",
            "content": "5.1 The Impact of Simplifying The Reference Log-Probabilities to Constant As discussed in Section 3.3, we approximate the log-probability of the pre-specified token under the reference model, log πre (zcx, y), by its mean computed over small sample set when calculating the last-token self-rewarding scores. Here, we empirically validate this practice by conducting comparison experiments on Open-ReasonerZero-7B, with and without reference log-probability simplification in our method. We evaluate the checkpoint after 200 optimization steps in each setting (corresponding to the last checkpoint before advantage integration). The results are reported in Table 3. As shown, the simplification does not affect the optimization of reasoning 8 (a) Results of OT-3B-based models on MATH500 (b) Results of Qwen2.5-7B-based models on MATH500 (c) Results of ORZ-7B-based models on MATH (d) Results of OT-3B-based models on OlympiadBench (e) Results of Qwen2.5-7B-based models on OlympiadBench (f) Results of ORZ-7B-based models on OlympiadBench Figure 2: The majority voting (Maj@K) and weighted majority voting (RM@K) results on MATH500 and OlympiadBench. (a) Evaluation accuracy on MMLU-Pro and GPQA-Diamond (b) Self-rewarding score distribution on MMLU-Pro (c) Self-rewarding score distribution on GPQA-Diamond Figure 3: The generalizability of LaSeR on general reasoning tasks. and self-rewarding capabilities, since the performance under the two settings remains comparable. However, it effectively reduces the computational cost of calculating the last-token self-rewarding value by half. 5.2 The Generalizability of LaSeR to General Reasoning Domain We conduct additional experiments to explore the generalizability of our method to general reasoning domain. We use filtered version (Yu et al., 2025b) of WebInstruct-verified dataset (Ma et al., 2025), and conduct RL experiments on Qwen3-4B-Base (Yang et al., 2025a). We use the general-verifier-1.5B model from Ma et al. (2025) as the model-based verifier and adopt GRPO as the RL algorithm. For our method, we do not perform the advantage integration strategy here. The reason is that we observe the self-verification F1 score of our method during training is relatively low in the general reasoning setting (only between 65% and 70%, and the self-rewarding score distributions in the test sets shown in Figure 3(b) and Figure 3(c) also reveal this phenomenon). This leads to large noise in the self-rewarding-based advantage estimation, and consequently, the integration of self-rewarding-based advantages results in performance degradation. After training, we conduct evaluations on two general reasoning benchmarks: MMLU-Pro (Wang et al., 2024b) and GPQA-Diamond (Rein et al., 2024). We sample 4 solutions per problem on each dataset for each model, and calculate both the average accuracy and the (weighted) majority voting accuracy. Detailed training and evaluation settings are in Appendix H. We display the evaluation accuracy in Figure 3(a), and additionally, we display the self-rewarding score distributions on two datasets in Figure 3(b) and Figure 3(c) for reference. First, we observe that jointly optimizing the selfrewarding capability does not impact the models general reasoning ability, allowing the policy model to achieve comparable average reasoning accuracy to the baseline. However, as mentioned above, the optimized self-rewarding score on general reasoning tasks does not achieve the high accuracy seen in math reasoning tasks. We can see that the self-rewarding score distributions for correct and incorrect solutions on MMLU-Pro exhibit certain overlap, and the distinction further diminishes on the more challenging benchmark GPQA-Diamond. We speculate that two factors may contribute to this: (1) The models general reasoning ability is inherently weaker than its math reasoning ability, which limits the upper bound of its self-rewarding capabilities in the general reasoning domain. (2) The model-based verifier used in the experiment (general-verifier-1.5B) has limited verification ability, resulting in high noise in the reasoning rewards, which in turn affects the optimization of the self-rewarding capability. promising direction for future work is to further explore and unlock the full potential of our method in the general reasoning domain. Though not perfect, the optimized self-rewarding scores can still provide useful signals during inference time, leading to better weighted majority voting results. 5.3 Further Reduction or Increase of Self-Rewarding Cost In this section, we discuss two additional variants of LaSeR for future work. In the current method, we calculate the last-token self-rewarding score based on the next-token log-probability distribution of the <EOS> token, requiring one additional token inference compared with standard inference. One potential way to further reduce the inference cost of LaSeR is to derive the last-token self-rewarding score directly from the predicted log-probability of pre-specified token zc at the <EOS> token position. Specifically, let yT denote the <EOS> token in y. Then, the reduced last-token self-rewarding score can be defined as rs = βv log πθ(zcx, y<T) βvcre , as we have observed that πref(zcx, y<T) remains nearly constant across (x, y) (e.g., approximately e28 for Qwen2.5-7B-Base). In this case, we can achieve ideally zero additional inference cost for self-rewarding compared with standard generation by directly calculating the self-rewarding score from the log-probability distribution at the <EOS> token position, without requiring any extra token inference. In theory, this works because setting relatively large βv still yields small value of πθ(zcx, y<T) (e.g., πθ(zcx, y<T) = e18 when βv = 0.1 and cref = 28), thereby allowing πθ(<EOS>x, y<T) to still dominate the probability mass. However, although the probability is very low, we observe that the generator may still select zc at the end of the sequence in few cases during training, which can adversely affect training stability as the generator continues to generate after zc. One straight-forward solution may be to set the sampling hyper-parameter top to value less than 1.0. Future work can further investigate advanced strategies to make the above adjustment more principled and robust. While reducing the self-rewarding cost improves efficiency, an alternative is to increase the inference cost in exchange for stronger self-rewarding capability. That is, instead of computing the self-rewarding score based on the log-probability distribution of single token only, we can increase the number of additional inference tokens by calculating it over tokens as rs = βv )) Mβvcre . It is promising direction m=1 log πθ(zcx, y, zc, , zc (cid:125) (cid:123)(cid:122) (cid:124) m1 times for future research to explore whether increasing the number of additional inference tokens can yield positive inference-time scaling effect for latent self-rewarding capability."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we propose LaSeR, lightweight and effective algorithm that jointly optimizes the reasoning and self-rewarding capabilities of LLMs. By deriving the closed-form solution to the RL objective of verification, we uncover concise yet intriguing formula: the true reasoning reward provided by the verifier is equal to the last-token self-rewarding score produced by the model. This self-rewarding score depends on the models next-token log-probability for pre-specified token at the final response token, pre-calculated constant, and the KL coefficient. Based on this insight, our method simply adds MSE loss between the verifier-based reasoning rewards and the corresponding last-token self-rewarding scores into the standard RLVR process. The optimized self-rewarding scores can not only be incorporated back into the RL process to further enhance training, but also achieve high verification accuracy at test time, thereby improving solution ranking and selection."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Arash Ahmadian, Chris Cremer, Matthias Galle, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Ustun, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740, 2024. 10 AI-MO. Aime 2024. https://huggingface.co/datasets/AI-MO/aimo-validation-aime, 2024a. AI-MO. Amc 2023. https://huggingface.co/datasets/AI-MO/aimo-validation-amc, 2024b. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, et al. Agentic reinforced policy optimization. arXiv preprint arXiv:2507.19849, 2025. Run-Ze Fan, Zengzhi Wang, and Pengfei Liu. Megascience: Pushing the frontiers of post-training datasets for science reasoning. arXiv preprint arXiv:2507.16812, 2025. Bofei Gao, Zefan Cai, Runxin Xu, Peiyi Wang, Ce Zheng, Runji Lin, Keming Lu, Dayiheng Liu, Chang Zhou, Wen Xiao, et al. Llm critics help catch bugs in mathematics: Towards better mathematical verifier with natural language feedback. arXiv preprint arXiv:2406.14024, 2024. Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pp. 1083510866. PMLR, 2023. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Michael Hassid, Gabriel Synnaeve, Yossi Adi, and Roy Schwartz. Dont overthink it. preferring shorter thinking chains for improved llm reasoning. arXiv preprint arXiv:2505.17813, 2025. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 38283850, 2024. Zhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian Yu, Zhenwen Liang, Wenxuan Wang, et al. Deepmath-103k: large-scale, challenging, decontaminated, and verifiable mathematical dataset for advancing reasoning. arXiv preprint arXiv:2504.11456, 2025. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob In Thirty-fifth Conference Steinhardt. Measuring mathematical problem solving with the MATH dataset. on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL https: //openreview.net/forum?id=7Bywt2mQsCe. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasonerzero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025. Maggie Huan, Yuetai Li, Tuney Zheng, Xiaoyu Xu, Seungone Kim, Minxin Du, Radha Poovendran, Graham Neubig, and Xiang Yue. Does math reasoning improve general llm capabilities? understanding transferability of llm reasoning. arXiv preprint arXiv:2507.00432, 2025. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language In The Thirteenth International Conference on Learning Representations, 2025. URL models for code. https://openreview.net/forum?id=chfJJYC3iL. Yuhua Jiang, Yuwen Xiong, Yufeng Yuan, Chao Xin, Wenyuan Xu, Yu Yue, Qianchuan Zhao, and Lin Yan. Pag: Multi-turn reinforced llm self-correction with policy as generative verifier. arXiv preprint arXiv:2506.10406, 2025. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https:// github.com/tatsu-lab/alpaca_eval, 5 2023. 11 Zongzhao Li, Zongyang Ma, Mingze Li, Songyou Li, Yu Rong, Tingyang Xu, Ziqi Zhang, Deli Zhao, and Wenbing Huang. Star-r1: Spatial transformation reasoning by reinforcing multimodal llms. arXiv preprint arXiv:2505.15804, 2025. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024a. Alisa Liu, Xiaochuang Han, Yizhong Wang, Yulia Tsvetkov, Yejin Choi, and Noah A. Smith. Tuning language models by proxy. In First Conference on Language Modeling, 2024b. URL https://openreview.net/ forum?id=dribhnhm1i. Jiawei Liu and Lingming Zhang. Code-r1: Reproducing r1 for code with reliable rewards. 2025. Xiaoyuan Liu, Tian Liang, Zhiwei He, Jiahao Xu, Wenxuan Wang, Pinjia He, Zhaopeng Tu, Haitao Mi, and Dong Yu. Trust, but verify: self-verification approach to reinforcement learning with verifiable rewards. arXiv preprint arXiv:2505.13445, 2025a. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025b. Songshuo Lu, Hua Wang, Zhi Chen, and Yaohua Tang. Urpo: unified reward & policy optimization framework for large language models. arXiv preprint arXiv:2507.17515, 2025. Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, and Wenhu Chen. General-reasoner: Advancing llm reasoning across all domains. arXiv preprint arXiv:2505.14652, 2025. MetaAI. Introducing llama 3.1: Our most capable models to date. https://ai.meta.com/blog/ meta-llama-3-1/, 2024a. MetaAI. Llama 3.2: Revolutionizing edge ai and vision with open, customizable models. https://ai.meta. com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/, 2024b. Eric Mitchell, Rafael Rafailov, Archit Sharma, Chelsea Finn, and Christopher Manning. An emulator for fine-tuning large language models using small language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=Eo7kv0sllr. Sagnik Mukherjee, Lifan Yuan, Dilek Hakkani-Tur, and Hao Peng. Reinforcement learning finetunes small subnetworks in large language models. arXiv preprint arXiv:2505.11711, 2025. OpenCompass. Aime 2025. https://huggingface.co/datasets/opencompass/AIME2025, 2025. Jan Peters and Stefan Schaal. Reinforcement learning by reward-weighted regression for operational space control. In Proceedings of the 24th international conference on Machine learning, pp. 745750, 2007. Qwen Team. Qwen2. 5: party of foundation models. Qwen (Sept. 2024). url: https://qwenlm. github. io/blog/qwen2, 5, 2024. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. Kusha Sareen, Morgane Moss, Alessandro Sordoni, Rishabh Agarwal, and Arian Hosseini. Putting the value back in rl: Better test-time scaling by unifying llm reasoners with verifiers. arXiv preprint arXiv:2505.04842, 2025. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Skywork-o1. Skywork-o1 open series. https://huggingface.co/Skywork, November 2024. URL https: //huggingface.co/Skywork. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Richard Sutton, Andrew Barto, et al. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025a. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025b. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Mathshepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 94269439, 2024a. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574, 2024b. Zengzhi Wang, Fan Zhou, Xuefeng Li, and Pengfei Liu. Octothinker: Mid-training incentivizes reinforcement learning scaling. arXiv preprint arXiv:2506.20512, 2025a. Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Xing Jin, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, et al. Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning. arXiv preprint arXiv:2504.20073, 2025b. Xumeng Wen, Zihan Liu, Shun Zheng, Zhijian Xu, Shengyu Ye, Zhirong Wu, Xiao Liang, Yang Wang, Junjie Li, Ziming Miao, et al. Reinforcement learning with verifiable rewards implicitly incentivizes correct reasoning in base llms. arXiv preprint arXiv:2506.14245, 2025. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a. Wenkai Yang, Jingwen Chen, Yankai Lin, and Ji-Rong Wen. Deepcritic: Deliberate critique with large language models. arXiv preprint arXiv:2505.00662, 2025b. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025a. Tianyu Yu, Bo Ji, Shouli Wang, Shu Yao, Zefan Wang, Ganqu Cui, Lifan Yuan, Ning Ding, Yuan Yao, Zhiyuan Liu, et al. Rlpr: Extrapolating rlvr to general domains without verifiers. arXiv preprint arXiv:2506.18254, 2025b. Lifan Yuan, Wendi Li, Huayu Chen, Ganqu Cui, Ning Ding, Kaiyan Zhang, Bowen Zhou, Zhiyuan Liu, and Hao Peng. Free process rewards without process labels. arXiv preprint arXiv:2412.01981, 2024. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025a. Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, et al. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025b. Kaiwen Zha, Zhengqi Gao, Maohao Shen, Zhang-Wei Hong, Duane Boning, and Dina Katabi. Rl tango: Reinforcing generator and verifier together for language reasoning. arXiv preprint arXiv:2505.15034, 2025. Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240, 2024. Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. The lessons of developing process reward models in mathematical reasoning. arXiv preprint arXiv:2501.07301, 2025. Jian Zhao, Runze Liu, Kaiyan Zhang, Zhimu Zhou, Junqi Gao, Dong Li, Jiafei Lyu, Zhouyi Qian, Biqing Qi, Xiu Li, et al. Genprm: Scaling test-time compute of process reward models via generative reasoning. arXiv preprint arXiv:2504.00891, 2025. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. Xiangxin Zhou, Zichen Liu, Anya Sims, Haonan Wang, Tianyu Pang, Chongxuan Li, Liang Wang, Min Lin, and Chao Du. Reinforcing general reasoning without verifiers. arXiv preprint arXiv:2505.21493, 2025. Yuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen Zhang, Xinwei Long, Ermo Hua, et al. Ttrl: Test-time reinforcement learning. arXiv preprint arXiv:2504.16084, 2025. 14 Figure 4: Cumulative implicit reward values across 32 reasoning trajectories sampled from Open-ReasonerZero-7B on an AIME2024 problem. Red lines correspond to wrong solutions and green lines correspond to correct solutions. Figure 5: The mean and standard deviation of log πre (zcx, y) under different combinations of reference model πre and pre-specified token zc over 300 input-output pairs."
        },
        {
            "title": "A The Length Bias in Implicit Reward",
            "content": "Here, we present the trend of the cumulative implicit reward values (log πθ(y<ix) πre (y<ix) where πre is Qwen2.5-7B-Base) across 32 reasoning trajectories sampled from Open-Reasoner-Zero-7B on an AIME2024 problem, showing how they vary with the increasing trajectory lengths. As illustrated in Figure 4, the curves of all samples exhibit positive correlation between the implicit reward and the number of tokens, and longer trajectories tend to yield higher final implicit reward scores, indicating strong length bias in implicit reward. Since incorrect solutions are generally longer than correct ones in reasoning tasks (Hassid et al., 2025), implicit reward is therefore not reliable indicator of the relative quality of reasoning paths at test time. Statistics of log πre (zcx, y) We present the mean and standard deviation of log πre (zcx, y) computed over 300 input-output pairs. The reference model πre is chosen as either Qwen2.5-7B-Base or OctoThinker-3B-Short-Base, and the evaluation is performed under two different choices of zc for each reference model (one common token and one unused special token): Yes and <vision start> for Qwen2.5-7B-Base, Yes and <reserved special token 0> for OctoThinker-3B-Short-Base. The results in Figure 5 indicates that log πre (zcx, y) remains nearly constant and extremely small, with only low standard deviation across different and y. Thus, we can consider log πre (zcx, y) as constant when calculating the last-token self-rewarding scores, which effectively reduces the computational cost by half."
        },
        {
            "title": "C Detailed Training Settings",
            "content": "We use verl (Sheng et al., 2024) as our RL training framework. The basic training hyper-parameters in both GRPO training and LaSeR training for each model are put in Table 4, and the newly introduced training hyper-parameters for LaSeR are put in Table 5. The number of optimization steps is 1000 for Qwen2.5-7B-Base and OctoThinker3B-Short-Base, and 500 for Open-Reasoner-Zero-7B. In RL, reasoning reward of 1.0 is given if the final answer and the answer format are both correct; otherwise, it is 0.0. In our method, the reasoning warm-up is performed for Qwen2.5-7B-Base and OctoThinker-3B-Short-Base only, and the self-rewarding warm-up is performed for all models. Ablation Studies on Self-Rewarding Hyper-Parameters Here, we display the curves (with Exponential Moving Average (EMA) smoothing) of training rewards and training self-verification F1 scores of our method under different choices of coefficient βv and self-rewarding MSE loss weight α. The experiments are conducted on Open-Reasoner-Zero-7B, which help to skip the reasoning warm-up phase compared with using Qwen2.5-7B-Base and OctoThinker-3B-Short-Base, while the results are similar in other 15 Table 4: Basic training hyper-parameters of both GRPO and LaSeR. Table 5: Unique training hyper-parameters of LaSeR. Hyper-parameter"
        },
        {
            "title": "Value",
            "content": "128 128 8 2048 8192 1.0 1.0 1 106 0.0 Hyper-parameter Coefficient βv Loss Weight α Self-Rewarding Adv. Weight τ Reasoning Warm-Up Steps Self-Rewarding Warm-Up Steps"
        },
        {
            "title": "Value",
            "content": "0.1 0.1 0.1 200 200 two base models after reasoning warm-up. The dynamics of training rewards and training self-verification F1 scores are displayed in Figure 6. As we can see, assigning larger weight α to the last-token self-rewarding loss has more detrimental impact on the models reasoning capabilities. On the other hand, the coefficient βv has little impact on optimizing the self-rewarding scores, as long as it remains within reasonable range (0.1 0.5). However, much smaller values of βv can impair the models reasoning capability, as indicated by the analysis in the end of Section 3.4. For example, when βv = 0.05, we should have πθ(zcx, y) = e3 0.05 under πref(zcx, y) = e23 and rv(x, y) = 1, then the large value of πθ(zcx, y) causes large interference with the optimization of reasoning capability. In our main experiments, we choose (βv, α) = (0.1, 0.1). The Effect of Class-Level Re-Weighting on The Balanced Self-Verification Capability We present the training dynamics of our method on Open-Reasoner-Zero-7B, with and without class-level loss re-weighting, in Figure 7 for comparison. As shown, applying loss re-weighting leads to more balanced selfverification performance by mitigating the bias toward the majority class with larger sample size, while still maintaining high reasoning accuracy. Comparison between Last-Token Self-Rewarding Loss and Supervised Fine-Tuning"
        },
        {
            "title": "Loss",
            "content": "Following the discussion in Section 3.4, we compare the training performance of our introduced last-token selfrewarding loss with the supervised fine-tuning (SFT) loss on Open-Reasoner-Zero-7B. The training dynamics are shown in Figure 8. As observed, applying the SFT loss to optimize the self-rewarding capability causes substantial interference with the optimization of reasoning capability, leading to marked degradation in training rewards. Moreover, the SFT loss degrades extremely slowly, indicating that directly driving πθ(zcx, y) from 0 to 1 for rv(x, y) = 1 is inherently difficult. However, our method only requires fitting πθ(zcx, y) to exp(1/βv) πref(zcx, y) for rv(x, y) = 1, which is considerably easier and introduces much less interference. Detailed Self-Verification Results We report the detailed self-verification results of each model on self-generated solutions across all benchmarks in Table 6, including both overall accuracy and F1 score. Our method consistently yields significant improvements in models self-rewarding and self-verification capabilities, while incurring only minimal additional computational cost."
        },
        {
            "title": "H Training and Evaluation Settings in General Reasoning Experiments",
            "content": "The basic training and testing hyper-parameters for experiments on WebInstruct-verified are the same as those in Table 4 and Table 5, while the number of optimization steps here is 800. The simplified constant of the reference log-probability cref is 23.0. We do not employ the advantage integration strategy here, as we find that the optimized self-rewarding capability of Qwen3-4B-LaSeR on general reasoning tasks is limited, and introducing self-rewarding-based advantage integration leads to performance degradation."
        },
        {
            "title": "I Prompt Templates",
            "content": "We show the training, evaluation and self-verification prompt templates used in our experiments in the end. 16 (a) Training rewards with EMA smoothing (b) Training self-verification F1 scores with EMA smoothing Figure 6: The curves of training rewards and training self-verification F1 scores under different combinations of hyper-parameters with EMA smoothing (EMA coef.=0.9). (a) Training rewards with EMA smoothing (b) Training self-verification F1 scores with EMA smoothing Figure 7: The curves of training rewards and training self-verification F1 scores of our method with and without class-level loss re-weighting practice (EMA coef.=0.9). 18 (a) Training rewards with EMA smoothing (b) Self-rewarding/SFT loss curve on log scale Figure 8: The comparison of the training dynamics between the last-token self-rewarding loss and the SFT loss. 19 Table 6: Detailed self-verification results. Method MATH500 AMC AIME24 AIME25 Olym. Acc. F1 Acc. F1 Acc. F1 Acc. F1 Acc. F1 OctoThinker-3B-Short-Base Base GRPO LaSeR - SWA 60.2 58.2 77.0 81.0 22.3 56.9 73.6 80.4 Qwen2.5-7B-Base Base GRPO LaSeR - SWA 45.0 76.5 88.0 87.8 36.4 54.6 83.2 79.7 52.3 66.7 77.3 84.1 30.7 61.1 81.5 79.6 Open-Reasoner-Zero-7B Base GRPO LaSeR - SWA 79.6 52.9 90.1 89.0 26.7 57.1 87.2 87.5 66.6 50.9 77.7 76.2 11.2 47.3 70.2 70.9 30.8 59.7 82.5 80. 51.3 44.8 79.7 77.7 - - - - 24.5 60.4 92.2 94.3 39.6 66.9 87.2 87.7 - - - - 27.6 36.6 79.6 81. 45.9 14.6 64.6 63.3 - - - - 28.2 72.5 90.5 92.2 47.6 78.9 92.8 93.6 - - - - 32.9 41.5 74.3 74. 55.2 28.1 77.7 77.3 62.0 66.4 80.3 83.5 33.8 54.6 79.5 83.9 55.2 54.7 80.1 80.2 13.7 48.8 73.6 66.0 36.9 53.5 78.3 83. 37.5 49.5 78.7 77.9 20 Training and Evaluation Prompt Template for OctoThinker-3B-Short-Base <bos token> conversation between User and Assistant. The user asks question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. User: You must put your answer inside boxed{} and Your final answer will be extracted automatically by the boxed{} tag. {question} Assistant: Training Prompt Template for Qwen2.5-7B-Base <bos token> conversation between User and Assistant. The User asks question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. User: You must put your answer inside <answer> </answer> tags, i.e., <answer> answer here </answer>. And your final answer will be extracted automatically by the boxed{} tag. This is the problem: {question} Assistant: <think> Zero-Shot Evaluation Prompt Template for Qwen2.5-7B-Base < im start >system You are helpful assistant.< im end > < im start >user {question} Please reason step by step, and put your final answer within boxed{}.< im end > < im start >assistant Training and Evaluation Prompt Template for Open-Reasoner-Zero-7B conversation between User and Assistant. The User asks question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. User: You must put your answer inside <answer> </answer> tags, i.e., <answer> answer here </answer>. And your final answer will be extracted automatically by the boxed{} tag. {question} Assistant: <think> Training and Evaluation Prompt Template for Qwen3-4B-Base < im start >user {question} Please reason step by step, and put your final answer within boxed{}.< im end > < im start >assistant 21 Prompt Template for Self-Verification (Modified from Liu et al. (2025a)) Below you are presented with question and tentative response. Your task is to evaluate the response and assign rating to the response based on the following clear criteria: Rating Criteria: 1. Missing final answer, or incorrect response with the wrong final answer: assign boxed{0}. 2. Correct response with the correct final answer: assign boxed{1}. ### Question Begin ### {question} ### Question End ### ### Response Begin ### {response} ### Response End ### First provide your evaluation process, then clearly state your final rating value enclosed in boxed{} at the end."
        }
    ],
    "affiliations": [
        "Gaoling School of Artificial Intelligence, Renmin University of China",
        "LLM Department, Tencent"
    ]
}