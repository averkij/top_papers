{
    "paper_title": "4Real-Video: Learning Generalizable Photo-Realistic 4D Video Diffusion",
    "authors": [
        "Chaoyang Wang",
        "Peiye Zhuang",
        "Tuan Duc Ngo",
        "Willi Menapace",
        "Aliaksandr Siarohin",
        "Michael Vasilkovsky",
        "Ivan Skorokhodov",
        "Sergey Tulyakov",
        "Peter Wonka",
        "Hsin-Ying Lee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose 4Real-Video, a novel framework for generating 4D videos, organized as a grid of video frames with both time and viewpoint axes. In this grid, each row contains frames sharing the same timestep, while each column contains frames from the same viewpoint. We propose a novel two-stream architecture. One stream performs viewpoint updates on columns, and the other stream performs temporal updates on rows. After each diffusion transformer layer, a synchronization layer exchanges information between the two token streams. We propose two implementations of the synchronization layer, using either hard or soft synchronization. This feedforward architecture improves upon previous work in three ways: higher inference speed, enhanced visual quality (measured by FVD, CLIP, and VideoScore), and improved temporal and viewpoint consistency (measured by VideoScore and Dust3R-Confidence)."
        },
        {
            "title": "Start",
            "content": "4Real-Video: Learning Generalizable Photo-Realistic 4D Video Diffusion Chaoyang Wang1,* Michael Vasilkovsky1 Peiye Zhuang1,* Tuan Duc Ngo1,2 Willi Menapace1 Ivan Skorokhodov1 Sergey Tulyakov1 Peter Wonka1, Aliaksandr Siarohin1 Hsin-Ying Lee1 4 2 0 2 5 ] . [ 1 2 6 4 4 0 . 2 1 4 2 : r 1Snap Inc 2Umass Amherst 3KAUST https://snap-research.github.io/4Real-Video/ Figure 1. 4Real-Video is 4D generation framework that (top-left) takes fixed-view video and freeze-time video as input and generates grid of consistent video frames. One axis of the grid varies in time, and the other axis varies the viewpoint. The input videos can be real videos or videos generated by video model. Note that our method can generate grids larger than 8 8 videos. Here, we present subsets of frames as an example. (top-right) 4D videos generated from generated videos. (bottom) We can also capture real-world scene, and generate 4D video given different prompts."
        },
        {
            "title": "Abstract",
            "content": "We propose 4Real-Video, novel framework for generating 4D videos, organized as grid of video frames with both time and viewpoint axes. In this grid, each row contains frames sharing the same timestep, while each column contains frames from the same viewpoint. We propose novel two-stream architecture. One stream performs viewpoint updates on columns, and the other stream performs tempo- * main contributor, project lead. ral updates on rows. After each diffusion transformer layer, synchronization layer exchanges information between the two token streams. We propose two implementations of the synchronization layer, using either hard or soft synchronization. This feedforward architecture improves upon previous work in three ways: higher inference speed, enhanced visual quality (measured by FVD, CLIP, and VideoScore), and improved temporal and viewpoint consistency (measured by VideoScore and Dust3R-Confidence). 1. Introduction With the recent rise of video diffusion models [24, 27, 47], 4D video generation has emerged as an important exten4D video generation has numerous potential apsion. plications, including creating dynamic scenes and objects through post-processing and enabling immersive experiences via image-based rendering techniques. To position our work, we define 4D video as follows: 4D video is grid of video frames with time and view-point axis. In our arrangement, all frames in row share timestamp, and all in column share viewpoint (see Fig. 1, left for an example). Our definition contrasts with recent work that also uses the term 4D video\", to describe video generation with camera and motion control [52]. To clarify this distinction, we will refer to such approaches as camera-aware. While both paradigms share similar applications, we believe that 4D video has two important advantages compared to cameraaware video: (a) complete space-time grid can provide full 4D experiences and enable easier dynamic reconstruction, yet it is non-trivial for camera-aware methods to complete such grid, and (b) videos generated by camera-aware methods tend to have inferior multi-view consistency [16]. As 4D video generation is very recent topic, there are only few competing approaches. Some works [17, 44, 50] propose training the 4D models directly using the limited available 4D data, such as synthetic animated 3D assets from Objaverse [7] or human-specific dataset [32]. These models can generate space-time grid, yet they cannot generalize beyond the limited training data distribution. Furthermore, the architecture designs, which sequentially interleave temporal and view attention, often fail to account for their interdependence, leading to artifacts or reduced generalization. To address the challenges of generating 4D videos, we introduce novel multi-view video generation model leveraging two-stream architecture to enhance multi-view and temporal consistency. Our approach extends existing transformer-based video diffusion models by splitting video tokens into two streams: one dedicated to capturing temporal updates across fixed viewpoints and the other focused on view updates across freeze-time frames. These streams are processed independently using pre-trained transformer layers to reuse existing models efficiently. To ensure coherence between the streams, we introduce synchronization layer that dynamically exchanges information between the temporal and view tokens. Inspired by the optimization literature, we propose two types of synchronization layers that perform either hard or soft synchronization updates, with the latter providing greater flexibility by learning adaptive weights to modulate token interactions across layers. This design avoids the distributional shifts observed in sequential model designs, preserves the consistency of the original video model, and enables high-quality 4D generation. The proposed architecture design can generate diverse, dynamic multi-view videos in approximately 1 minute (88 frames at resolution of 288 512), as opposed to hours required by previous SDS-based approaches [2, 19, 29, 49]. Beyond its speed advantage, the model generalizes well with limited 4D training data. This is achieved by initially training on 2D transformed videos to simulate synchronized camera motion, followed by fine-tuning on small amount of animated Objaverse data [7]. Moreover, our model does not rely on explicit camera conditioning modules. Instead, it takes real or generated freeze-time video and fixedview video as conditional inputs, automatically inferring the viewpoints and motion to be generated. This effectively decomposes the problem, allowing us to leverage recent advancements in camera-controlled video generation as conditional input. It also simplifies the process of animating existing freeze-time videos by removing the requirement for users to provide camera poses explicitly. In summary, we make the following contributions: We propose two-stream architecture for 4D video generation that independently handles temporal and view updates, synchronizing streams to ensure consistency. We propose flexible synchronization mechanisms that enable efficient and adaptive token interactions, preserving the generation quality of pre-trained video layers. Our model is data-efficient and can produce highresolution 4D videos in fraction of the time required by prior methods. We obtain state-of-the-art results in terms of video quality and multi-view consistency. 2. Related Work Optimization-Based 4D Generation. Score Distillation Sampling (SDS) [6, 18, 28, 39, 41, 54] has been used to generate 3D content by obtaining gradients from pre-trained models like text-to-image [30, 31] and text-to-multiview models [20, 34]. Extending this approach, branch of 4D generation methods [2, 14, 19, 29, 35, 48, 51] leverages additional text-to-video supervision [5, 8, 12] to generate dynamic content. However, these methods require timeconsuming optimization processes, often requiring hours to produce 4D output. Furthermore, most methods derive 3D priors from multi-view diffusion models [20, 34] trained on an object-centric and synthetic dataset [7], resulting in bias toward object-centric, non-photorealistic outputs. Camera-aware video generation Text-to-video models [22, 24, 27, 47] have shown promising results in generating coherent and photorealistic video content. To enable more controllable and interactive content creation process, camera control in video generation has gained attention. These approaches [3, 9, 42, 43, 46] propose adding camera control by injecting camera pose information into the temporal layers. Additionally, some approaches [45, 52] collect and annotate videos with camera poses to fine-tune Figure 2. Overview of 4Real-Video. Left: we initialize the grid of frames with (generated or real) fixed-viewpoint video in the first row and freeze-time video in the first column. Middle: our architecture consists of two parallel token streams. The top part processes xv with viewpoint updates and the bottom part processes xt with temporal updates. Subsequently, synchronization layer computes the new tokens xv l+1 xt l+1 for the next layer in the diffusion transformer architecture. Right: we propose two implementations of the synchronization layer: hard and soft synchronization. video models. However, despite being visually consistent, the content generated by camera-aware methods tends to have multi-view inconsistencies. Furthermore, it is not trivial for camera-aware video models to generate complete space-time grid of 4D videos, limiting their applicability in fully 4D scenarios. 4D video generation In this work, we define 4D video as video grid organized along both temporal and viewpoint axes. Some work [17, 44, 50] trains 4D models using existing 4D data, typically subsets of Objaverse [7]. Although these models are conceptually capable of moving beyond object-centric content, their outputs remain constrained by the limited diversity of available 4D datasets in practice. To reduce reliance on synthetic data, alternatives like 4Real [49] relies solely on video model to generate consistent dynamic and freeze-time videos, followed by an optimization-based 4D reconstruction to obtain underlying 4D contents. However, the dynamic and freeze-time videos alone cannot guarantee consistency within the 4D grid, and the optimization is computationally expensive. CVD [16] tackles this limitation by fine-tuning video models to simultaneously generate structurally consistent video pairs, using pseudo-paired datasets curated from monocular video datasets [4, 37]. Although CVD proposes strategies to extend generation to multiple views, its consistency and efficiency remains suboptimal for multi-view 4D generation. 3. Method point, and all frames in column share timestep. In other words, each row is fixed-view video, and each column is freeze-time video. The inputs to our method are the first row I1 and the first column I1 of the frames. These inputs are from either real-world videos or synthetic outputs from existing video generation models. The task is to synthesize the remaining frames while ensuring both temporal and multi-view consistency (see Fig. 2 left). 3.1. Base video model training Freeze-time and dynamic video generation. Inspired by 4Real [49], we train base video model to support two distinct generation modes: freeze-time video that depicts static scenes with changes in viewpoint, and dynamic video that captures object motion. We group datasets into two categories: (1) videos with arbitrary camera and scene motions, and (2) videos of static scenes. Each group is associated with unique context embedding that controls the generation process to align with the respective distributions. Masked training. To handle flexible input configurations, the model is trained using random masking strategy. This enables the model to predict unseen frames based on any subset of input frames. The design (1) allows for autoregressive generation of long videos by progressively synthesizing frames, and (2) provides essential flexibility for the 4D video model to condition on various input video frames. 3.2. Multi-view video model 3.2.1. Two-stream architecture Problem setup. We aim to generate structured grid of video frames {Iij}, where all frames in row share viewCurrent state-of-the-art video diffusion models [24, 27, 47] mostly leverage transformer-based architecture such as DiT [26], which forwards video tokens through series of spatial-temporal transformer blocks with skip connections. Specifically, each DiT transformer block φl produces an update xl to the current video tokens xl at the l-th layer with condition c: xl = φl(xl; c), xl+1 = xl + xl. (1) In our setting, we need to extend to set of tokens describing all frames in the grid. We use xl to denote the set of all tokens at layer and xl,i,j to describe the set of tokens for frame at layer with time stamp and viewpoint j. As our goal is to reuse pre-trained high-quality video models as much as possible, we can utilize pre-trained DiT transformer layers to either update row for view-point (Eq. 2) or column for timestep (Eq. 3) of our frame grid, φv ({xl,i,1, ..., xl,i,T }; c) l({xl,1,j, ..., xl,V,j}; c) φt for 1 V, for 1 Y. (2) (3) Since we have total of timesteps and viewpoints, we can process the complete grid by either performing row updates or column updates in parallel when reusing existing DiT transformer blocks. To avoid overly complex notation, we write φv (xl, c) to denote the update of single row or parallel update of all rows jointly (and use analogous notation for column updates with φt l(xl, c)). Our first important design idea is variable (token) splitting to create two separate sets of tokens to encode the complete frame grid, xt for temporal and xv for view updates. The set xv will be processed using parallel row updates and the set xt will be processed using parallel column updates. Updates are computed independently and in parallel: = xv yv + φv (xv ; cv); yt = xt + φt l(xt l; ct). (4) l+1, xt l+1) = (yv We propose synchronization layer after each DiT transformer layer l, which exchanges information between the two token streams. The synchronization layer computes function (xv l) in order to obtain the input tokens for the next layer. This architecture is shown in Fig. 2. Several model designs have been proposed to extend pre-trained video models for 4D video generation. Next, we will review and analyze designs in previous works (Sec. 3.2.2). Then we introduce our design of the synchronization layer (Sec. 3.2.3-3.2.4). , yt 3.2.2. Sequential interleaving competing design choice would be to compute alternating updates for temporal and multi-view consistency: yl = xl + φv (xl; cv), xl+1 = yl + φt l(yl; ct). (5) and φv where φv denote applying the transformer layer φl across the view and the time axes, respectively. Most prior works [16, 17, 50] that sequentially interleave cross-view attention and cross-time attention, could be interpreted as performing the above steps. While conceptually simple, this approach has limitations: (i) It does not fully account for the interdependence between temporal and view consistency, treating them as independent objectives during each update. (ii) Outputs from the view update yl may be out-of-distribution for the temporal update, leading to artifacts or reduced generalization. Prior works either train an additional network to adapt yl to become in-distribution [17], or fine-tune the cross-view attention or the cross-time attention to compensate for the discrepancy [16, 50]. However, due to the limited available 4D data, fine-tuning the attention layers could degrade the quality of the video model, limiting its generalization capability to domains outside the 4D training set. 3.2.3. Synchronization in Optimization One can interpret the DiT blocks of the video model as functions performing fixed number of iterative variable updates to optimize an implicit cost function C(x; c) [1, 13]. The implicit cost function can be thought of as an abstract measure of \"closeness\" to realistic videos given context c. Under certain restricted assumptions, Ahn et al. [1] prove that for transformer with layers, it learns to perform iterations of preconditioned gradient descent to reach certain critical points of the training loss. The intuition that the transformer architecture can be seen as an iterative optimization solver motivates us to create link to the optimization literature to explain our synchronization layer design choices. Using the optimization analogy, our video model solves combined optimization problem for 4D generation: min Cv(x) + Ct(x). (6) where Cv ensures that each row of the grid is fixed-view video and Ct ensures that each column is freeze-time video. Using the idea of variable splitting, this problem can be transformed into the equivalent problem: min (xv,xt) Cv(xv) + Ct(xt) s.t. xv = xt. (7) An optimization problem with this structure can be tackled by algorithms like projected gradient descent, which performs projection on the constraint manifold at every iteration. This leads to the design of hard synchronization between the two token streams. Alternatively, one can employ quadratic regularization or an algorithm like ADMM [25] that does not strictly enforce the constraint at every iteration but makes the token streams more similar. This leads to the design of soft synchronization between the two token streams. 3.2.4. Synchronization layer design The synchronization layer maintains consistency between the two token streams, as defined in E.q. 4. Following this, we explore two synchronization strategies: Hard synchronization. Hard synchronization strictly enforces the constraint xt = xv at every iteration. straightforward approach to hard synchronization is to compute an update by averaging tokens. However, in contrast to traditional optimization, we can generalize this step to compute weighted combination with learned weights: xl+1 = Wv yv + Wt lyt l, (8) where Wv , Wt are linear weights for merging each token with initial values, i.e., 1 2 I. The weights are modulated by the diffusion time σ to make them adaptive to different stages of the diffusion process. Empirically, the 4D model with hard sync can indeed generate temporally consistent 4D videos. However, it tends to produce less desirable frames when the viewpoint differs significantly from the input fixed-view video. Common artifacts include objects appearing stretched in the direction of camera movement or unintended object motion when the time stamp is intended to be frozen (Refer to visual examples in Fig. 7). We hypothesize that the limitation of hard sync is that the merged video tokens are aggregated from both the freeze-time and fixed-view videos, causing discrepancy in the learned distribution of the base video model. ,xt Soft synchronization. The above observation motivates an alternative soft synchronization strategy the video tokens xv are kept in two separate streams instead of merging them into single copy as in Eq. (8). soft update is used to make the streams more similar. This design gives additional flexibility for the model to adaptively adjust the strength of synchronization at different layers. Again, we can design more general solution as would be available in traditional optimization and use modulated linear layer to predict asymmetrical token updates: , yt l; σ). , yt (9) (yv l) = Mod_Linear(yv Then, the tokens are updated separately: l+1 = yt l+1 = yv xv (10) + yv + yt l. , xt Soft synchronization offers more flexibility, adapting the strength of synchronization across layers. Empirically, this results in better consistency and fewer artifacts in challenging scenarios, such as large viewpoint changes. We visualize the update strength and token similarity in Fig. 3. We observe that the update strength increases for layers deeper in the network. The token similarity is initially drifting between the two token streams before they are made more similar by the increased update strength in later layers. 3.3. Implementation Training. The model is trained with the velocity matching loss of rectified flow [21], leveraging two data sources: (1) 2D transformed videos: we apply sequence of continuous 2D affine transformations to video frames to mimic (a) Relative magnitude of yv , yt in Eq. (10) at each layer. (b) Similarity between xv each layer. and xt at Figure 3. The dynamics of soft synchronization during inference. camera motion. This provides large-scale pseudo 4D data to train the model to generate synchronized motions. However, models only trained with this source tend to generate flattened foreground objects that are noticeable when changing viewpoints. (2) Animated Objaverse dataset: We render around 15,000 multi-view videos using animated 3D assets from Objaverse [7], positioning the rendering cameras on circular trajectory around each asset. Finetuning with this small-scale, synthetic, object-centric 4D dataset quickly equips the model with the ability to maintain both temporal and multi-view consistency, even in complex scenes containing multiple objects and intricate environments. Extending to wider view and longer time. The model is trained to generate an 8 8 frame grid in each step. For input fixed-view videos or freeze-time videos with extended durations, we generate frames autoregressively, advancing along the time and view axes in sliding window fashion. 4. Experiments 4.1. Implementation details Base video model. The base video model consists of 600M parameters, with 24 DiT blocks of 1024 channel size. We found that pixel-based diffusion models train faster and produce more coherent motion compared to latent-based models of similar model size. Thus, we opt to train the base model to directly output pixel values, given limited accessible GPU resources. The model is progressively trained from resolution of 3664 to 72128 using 24 A100 GPUs for 12 days. We then train diffusion-based upsampler to upsample the video to the target 288512 resolution. 4D model. The 4D video model is trained progressively from low to high resolution. It is first trained using pseudo 4D videos for 20k iterations, followed by fine-tuning for 3k iterations on the Animated Objaverse dataset [7]. Notably, longer training on the Objaverse data led to slight decrease in quality when applied to real-world scenes. Note that finetuning only affects the weights of the synchronization layers to avoid shifting the video distribution away from real Method FID CLIP FVD FVD-Test Time View Time View Visual Quality Temporal Consist. Factual Consist. Time View View View Time Time SV4D [44] MotionCtrl [42] Sequential Soft w/o Obj Hard Sync Soft Sync 204.81 87.10 96.64 80.17 79.92 78.36 19.46 20.20 28.16 28.11 28.16 28.22 1053.10 1556.36 1662.54 1392.48 972.87 906. 1245.42 1509.76 1797.15 1720.47 1045.35 1036.00 814.50 1170.04 897.08 318.18 316.14 308.15 323.99 302.18 597.19 302.18 251.44 261.02 2.26 2.36 2.30 2.41 2.42 2.43 2.02 2.30 2.28 2.39 2.40 2.42 2.03 2.38 2.21 2.37 2.40 2. 1.68 2.25 2.15 2.31 2.33 2.36 2.12 2.38 2.23 2.35 2.37 2.38 1.99 2.33 2.20 2.33 2.34 2.36 Table 1. Quantitative ablation. We evaluate the visual quality, temporal consistency, and text-video alignment using various metrics. that outputs scores assessing visual quality, temporal consistency, text-video alignment, motion degree, and factual consistency. In our case, we drop text-video alignment and motion degree scores since these scores are more related to the input conditional videos instead of generated frames. FVD [38] evaluates the Frechet Distance between the generated video distribution and the data distribution. We (1) against reported two versions of the FVD score: large dataset of real videos, where the score is relatively high due to the distribution mismatch caused by the out-ofdistribution content of our test cases, and (2) against statistics computed from the input test set videos to provide more relevant comparison. CLIP Score [11] evaluates the similarity between generated images against the text prompt. It also reflects the visual quality of the generated frames. GIM-Confidence. We utilize GIM [33], state-ofthe-art 2D image matching method, to measure the consistency of appearance across views. Specifically, we report the proportion of matching pixels across views under different confidence thresholds. Note the GIM focuses on 2D image matching and cannot reflect 3D consistency well. Dust3R-Confidence. To further evaluate 3D multiview consistency, we use Dust3R [40], state-of-the-art 3D reconstruction network to analyze generated freeze-time videos. Dust3R provides pixel-wise confidence scores reflecting 3D multi-view consistency, and we report the proportion of pixels above different confidence thresholds. We evaluated VideoScore and FVD for both videos playing either along the time axes as fixed-view videos, or along the view axes as freeze-time videos, in order to evaluate both temporal and multi-view consistency of the generated frame grid. GIM and Dust3R-Confidence are used only in ablations with fixed camera trajectories, where the confidence scores are comparable. Comparison against 4D video generation baselines. There is currently no prior method that functions exactly like ours, so we establish two baselines: (1) we use MotionCtrl [42], state-of-the-art camera control video generation method, to generate freeze-time videos for each frame of the input fixed-view videos. The videos are generated with Round-RI_90 camera trajectory and speed parameter set to 4.0 to encourage larger camera motions. (2) We Figure 4. Visual Comparisons. We show two viewpoints for fixed time for each method. Our method produces high-quality images, even under significant camera motion. In contrast, frames generated by 4Real and SV4D tend to appear more blurred, with objects notably distorted in SV4D. MotionCtrl struggles to generate frames under substantial camera motion. We use red bounding boxes to highlight regions with distortions and flickering, which become particularly noticeable when viewed as video. GIM-Confidence τ = 0. τ = 0.7 τ = 0.1 Dust3R-Confidence τ = 2.5 τ = 2.0 τ = 3.0 Sequential Soft w/o Obj Hard Sync Soft Sync 65.0 80.8 78.7 79.6 28.6 49.6 44.8 47.1 18.5 36.1 31.4 33.8 33.5 39.1 39.3 41.0 24.6 31.4 31.5 33.4 16.6 24.0 23.8 25. Table 2. Multi-view consistency is measured using an imagematching method and 3D reconstruction method. videos. 4.2. Evaluation Test sets. We use the Snap Video Model [22] to generate pairs of freeze-time and fixed-view videos, given diverse set of text prompts. Each video is 2 seconds long, consisting of 16 frames. In total, we collected 200 pairs to serve as testing inputs. Some samples of our results on the test set are shown in Fig. 5. Evaluation metrics. Evaluating 4D video generation is challenging without ground truth data. We employ the following metrics to assess generation quality: VideoScore [10] is video quality evaluation network Figure 5. Results from 4Real-Video. We can generate diverse and high-quality dynamic content. Figure 6. Deformable 3D Gaussian Splatting Reconstruction from the generated 4D videos demonstrate the spatial and temporal consistency of the proposed method. run SV4D [44], state-of-the-art 4D video model trained specifically for animated 3D objects. MotionCtrl fails to generate temporally coherent videos because freeze-time videos are generated independently, igIt also tends to generate noring temporal dependencies. very small camera motion despite being given large input speed. On the other hand, SV4D fails to create meaningful results when applied to realistic-style videos, which are out of its training domain. In comparison, our method generates realistic and coherent frame grids and achieves higher scores across different metrics, as shown in Table 1. Comparison against optimization-based 4D generation baselines. We also compare against recent 4D generation methods [2, 19, 49, 53] that rely on computationally expensive score distillation sampling [28]. Due to the limited number of samples we can acquire from these methods, and the fact that these samples were generated using different settings (object-centric v.s. scene-level), we conducted user study instead. The study involves 10 evaluators per video pair. In each session, evaluators were presented with two anonymized videos. Each video depicted dynamic object or scene, with the camera moving along circular trajectory and stopping at 2-4 poses to highlight object motions. We obtained 16 videos for 4Dfy [2], 14 videos for Dream-in-4D [53], 14 videos for AYG [19] and 36 videos for 4Real [49] from their respective project web pages. Evaluators were tasked with selecting their preferences based on seven criteria: motion realism, foreground/background quality, shape realism, general quality, motion quality, and video-text alignment. As shown in Figure 8, our method outperformed the competition in every category by large margin. More details Figure 7. Ablation comparisons. We visually compare the video quality and consistency among different design choices. 4.4. Deformable 3D Gaussian Splatting reconstruction from generated 4D videos To further validate the effectiveness of our method in generating explicit 3D representations, we fit deformable 3D Gaussian Splatting (3DGS) to the generated 4D videos. Fig. 6 qualitatively shows reconstructed 3DGS at different times and viewpoints. More details of the reconstruction pipeline are included in the supplementary. 5. Conclusion We propose 4Real-Video, novel framework for 4D video generation. The core idea of our framework is to process grid of frames using two separate token streams that are processed in parallel, with synchronization layer coordinating between the two streams. Remarkably, our model can generate diverse photorealistic 4D videos without requiring access to such dataset. Despite its strengths, our current implementation has several limitations that we aim to address in future work. First, the base video models small size constrains its capability, limiting the visual quality and resolution of the generated videos. This can be improved by incorporating more advanced and larger-scale video models. Second, our framework currently lacks support for 360 video generation. Enhancing this capability will involve improving the training of the base video model and incorporating camera pose conditioning. Third, generating freezetime videos remains significant challenge, particularly for dynamic elements such as running horses or fires, where robustness is limited. Finally, our approach requires postprocessing steps to construct explicit 3D representations of the generated dynamic scenes. In the future, it would be exciting to explore the possibility of single feedforward model for 4D generation and further advancing the field. Figure 8. User study against optimization-based 4D generation methods across different rating criteria. of the user study are provided in the supplementary. 4.3. Ablations We analyze our method by comparing it against the following variations: (1) sequential interleaved architecture (see Eq. (5)); (2) training only with pseudo-4D video dataset without Objaverse; (3) using hard synchronization; and (4) our full method with soft synchronization. The results are shown in Table 1, Table 2 and visualized in Fig. 7. Further details of each ablated design are provided in the supplementary material. We make the following observations: First, the proposed parallel architecture achieves better performance compared to the sequential architecture. Second, training our model without any 4D data can still produce competitive results compared to baselines, showing the robustness of our approach. It obtains higher GIMConfidence as the metric favors only image matching instead of real 3D consistency. Finally, soft synchronization improves quality over hard synchronization, leading to more coherent and visually appealing outputs."
        },
        {
            "title": "References",
            "content": "[1] Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement preconditioned gradient descent for in-context learning. NeurIPS, 2023. 4 [2] Sherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon Wetzstein, Leonidas Guibas, Peter Wonka, Sergey Tulyakov, Jeong Joon Park, Andrea Tagliasacchi, and David Lindell. 4d-fy: Text-to-4d generation using hybrid score distillation sampling. In CVPR, 2024. 2, 7 [3] Sherwin Bahmani, Ivan Skorokhodov, Aliaksandr Siarohin, Willi Menapace, Guocheng Qian, Michael Vasilkovsky, Hsin-Ying Lee, Chaoyang Wang, Jiaxu Zou, Andrea Tagliasacchi, et al. Vd3d: Taming large video diffuarXiv preprint sion transformers for 3d camera control. arXiv:2407.12781, 2024. 2 [4] Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In ICCV, 2021. 3 [5] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. 2 [6] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for highquality text-to-3d content creation. In ICCV, 2023. 2 [7] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: In Proceedings of universe of annotated 3d objects. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1314213153, 2023. 2, 3, 5 [8] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. [9] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. 2 [10] Xuan He, Dongfu Jiang, Ge Zhang, Max Ku, Achint Soni, Sherman Siu, Haonan Chen, Abhranil Chandra, Ziyan Jiang, Aaran Arulraj, et al. Mantisscore: Building automatic metrics to simulate fine-grained human feedback for video generation. arXiv preprint arXiv:2406.15252, 2024. 6 [11] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. CoRR, abs/2104.08718, 2021. 6 [12] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 2 [13] Stanisław Jastrzebski, Devansh Arpit, Nicolas Ballas, Vikas Verma, Tong Che, and Yoshua Bengio. Residual connections encourage iterative inference. arXiv:1710.04773, 2017. arXiv preprint [14] Yanqin Jiang, Li Zhang, Jin Gao, Weimin Hu, and Yao Yao. Consistent4d: Consistent 360 {deg} dynamic obarXiv preprint ject generation from monocular video. arXiv:2311.02848, 2023. 2 [15] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 1 [16] Zhengfei Kuang, Shengqu Cai, Hao He, Yinghao Xu, Hongsheng Li, Leonidas Guibas, and Gordon. Wetzstein. Collaborative video diffusion: Consistent multi-video generation with camera control. In arXiv, 2024. 2, 3, 4 [17] Bing Li, Cheng Zheng, Wenxuan Zhu, Jinjie Mai, Biao Zhang, Peter Wonka, and Bernard Ghanem. Vivid-zoo: Multi-view video generation with diffusion model, 2024. 2, 3, 4 [18] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In CVPR, 2023. [19] Huan Ling, Seung Wook Kim, Antonio Torralba, Sanja Fidler, and Karsten Kreis. Align your gaussians: Text-to-4d with dynamic 3d gaussians and composed diffusion models. In CVPR, 2024. 2, 7 [20] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In ICCV, 2023. 2 [21] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 5 [22] Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Ekaterina Deyneka, Tsai-Shien Chen, Anil Kag, Yuwei Fang, Aleksei Stoliar, Elisa Ricci, Jian Ren, et al. Snap video: Scaled spatiotemporal transformers for text-to-video synthesis. In CVPR, 2024. 2, 6 [23] Tuan Duc Ngo, Peiye Zhuang, Chuang Gan, Evangelos Kalogerakis, Sergey Tulyakov, Hsin-Ying Lee, and Chaoyang Wang. Delta: Dense efficient long-range 3d tracking for any video, 2024. 1 [24] OpenAI. Video generation models as world simulators, 2024. Accessed: 2024-11-08. 2, [25] Neal Parikh, Stephen Boyd, et al. Proximal algorithms. Foundations and trends in Optimization, 2014. 4 [26] William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. 4 [27] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat Singh, Mary Williamson, Matt Le, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Simran Motwani, Tao Xu, Tianhe Li, Tingbo Hou, Wei-Ning Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu, Arun Mallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh Wood, Ce Liu, Cen Peng, Dimitry Vengertsev, Edgar Schonfeld, Elliot Blanchard, Felix Juefei-Xu, Fraylie Nord, Jeff Liang, John Hoffman, Jonas Kohler, Kaolin Fire, Karthik Sivakumar, Lawrence Chen, Licheng Yu, Luya Gao, Markos Georgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li, Simone Parmeggiani, Steve Fine, Tara Fowler, Vladan Petrovic, and Yuming Du. Movie gen: cast of media foundation models, 2024. 2, 3 [28] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In ICLR, 2023. 2, [29] Jiawei Ren, Liang Pan, Jiaxiang Tang, Chi Zhang, Ang Cao, Gang Zeng, and Ziwei Liu. Dreamgaussian4d: Generative 4d gaussian splatting. arXiv preprint arXiv:2312.17142, 2023. 2 [30] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 2 [31] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. NeurIPS, 2022. 2 [32] Ruizhi Shao, Youxin Pang, Zerong Zheng, Jingxiang Sun, and Yebin Liu. Human4dit: Free-view human video generation with 4d diffusion transformer. arXiv preprint arXiv:2405.17405, 2024. 2 [33] Xuelun Shen, Zhipeng Cai, Wei Yin, Matthias Müller, Zijun Li, Kaixuan Wang, Xiaozhi Chen, and Cheng Wang. Gim: Learning generalizable image matcher from internet videos. arXiv preprint arXiv:2402.11095, 2024. 6 [34] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. In ICLR, 2024. 2 [35] Uriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual, Iurii Makarov, Filippos Kokkinos, Naman Goyal, Andrea Vedaldi, Devi Parikh, Justin Johnson, et al. Text-to-4d dynamic scene generation. arXiv preprint arXiv:2301.11280, 2023. [36] Brandon Smart, Chuanxia Zheng, Iro Laina, and Victor Adrian Prisacariu. Splatt3r: Zero-shot gaussian splatting from uncalibrated image pairs. 2024. 1 [37] Richard Tucker and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. In ACM TOG, 2018. 3 [38] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphaël Marinier, Marcin Michalski, and Sylvain Gelly. Fvd: new metric for video generation. arXiv preprint arXiv:1812.01717, 2018. 6 [39] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond Yeh, and Greg Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. In CVPR, 2023. [40] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In CVPR, 2024. 6, 1 [41] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. In NeurIPS, 2023. 2 [42] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH, 2024. 2, 6 [43] Daniel Watson, Saurabh Saxena, Lala Li, Andrea Tagliasacchi, and David Fleet. Controlling space and time with diffusion models. arXiv preprint arXiv:2407.07860, 2024. 2 [44] Yiming Xie, Chun-Han Yao, Vikram Voleti, Huaizu Jiang, and Varun Jampani. Sv4d: Dynamic 3d content generation with multi-frame and multi-view consistency. arXiv preprint arXiv:2407.17470, 2024. 2, 3, 6, 7 [45] Dejia Xu, Weili Nie, Chao Liu, Sifei Liu, Jan Kautz, Zhangyang Wang, and Arash Vahdat. Camco: Cameracontrollable 3d-consistent image-to-video generation. arXiv preprint arXiv:2406.02509, 2024. [46] Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, and Jing Liao. Direct-a-video: Customized video generation with userdirected camera movement and object motion. In ACM SIGGRAPH, 2024. 2 [47] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2, 3 [48] Yuyang Yin, Dejia Xu, Zhangyang Wang, Yao Zhao, and Yunchao Wei. 4dgen: Grounded 4d content generation with spatial-temporal consistency. arXiv preprint arXiv:2312.17225, 2023. 2 [49] Heng Yu, Chaoyang Wang, Peiye Zhuang, Willi Menapace, Aliaksandr Siarohin, Junli Cao, Laszlo Jeni, Sergey Tulyakov, and Hsin-Ying Lee. 4real: Towards photorealistic 4d scene generation via video diffusion models. In NeurIPS, 2024. 2, 3, 7, 1 [50] Haiyu Zhang, Xinyuan Chen, Yaohui Wang, Xihui Liu, 4diffusion: Multi-view arXiv preprint Yunhong Wang, and Yu Qiao. video diffusion model for 4d generation. arXiv:2405.20674, 2024. 2, 3, [51] Yuyang Zhao, Zhiwen Yan, Enze Xie, Lanqing Hong, Zhenguo Li, and Gim Hee Lee. Animate124: Animating one image to 4d dynamic scene. arXiv preprint arXiv:2311.14603, 2023. 2 [52] Yuyang Zhao, Chung-Ching Lin, Kevin Lin, Zhiwen Yan, Linjie Li, Zhengyuan Yang, Jianfeng Wang, Gim Hee Lee, and Lijuan Wang. Genxd: Generating any 3d and 4d scenes. arXiv preprint arXiv:2411.02319, 2024. 2 [53] Yufeng Zheng, Xueting Li, Koki Nagano, Sifei Liu, Otmar Hilliges, and Shalini De Mello. unified approach for textand image-guided 4d scene generation. In CVPR, 2024. 7 [54] Joseph Zhu and Peiye Zhuang. Hifa: High-fidelity text-to-3d with advanced diffusion guidance. In ICLR, 2023. 2 4Real-Video: Learning Generalizable Photo-Realistic 4D Video Diffusion"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Deformable 3D GS reconstruction details Using generated 4D videos with multi-view frame grids, we apply reconstruction method to produce an explicit 3D representation, i.e., deformable 3D geometric structures (GS). Canonical 3D representation. We use 3D Gaussian Splats [15] to represent the canonical shape of the dynamic scene. This representation consists of set of 3D Gaussian points defined by their 3D position, orientation, scale, opacity, and RGB color. The 3D Gaussian Splats are rendered by projecting the Gaussian points onto the image plane and aggregating pixel values using NeRF-like volumetric rendering equation. In our implementation, we find that constraining the Gaussians to be isotropic effectively reduces artifacts when viewing the 3D representation from viewpoints distinct from the training perspectives. Deformation field. To model 4D scene, we use deformation field to represent the offsets of the 3DGS. This deformation field is implemented as an MLP, which takes the 3D position of point and time as input and outputs 3D displacement offset. Initialize canonical 3D GS with 3D dense tracking. While the input freeze-time video may appear visually plausible, it is not truly geometrically accurate, particularly in the background regions. Directly optimizing 3D GS using these frames as ground truth results in significant artifacts. The most noticeable issue is the noisy reconstruction of background regions, which fail to separate cleanly In our preliminary exploration, we from the foreground. tested state-of-the-art feedforward reconstruction methods, including Dust3R [40] and Splatt3R [36]. However, in most cases, only the foreground regions could be reliably reconstructed, while the background remained noisy and entangled with the foreground. We attribute this limitation to the quality of the video model used to generate the inputs. In the long term, this issue could potentially be addressed by employing higher-quality video model. At this stage, we instead use recent 3D dense tracking method [23], which performs pixel-wise tracking to aggregate 3D points from various keyframes of the freeze-time video. These points are aligned towards central frame, whose coordinates are treated as the canonical frame. The advantage of switching to 3D tracking is that it does not require the scene to be static, allowing it to handle multi-view inconsistencies in the generated videos by treating them as non-rigid deformations. Furthermore, 3D tracking leverages monocular depth estimation as input, preserving the clean foreground/background separation provided by the estimated depth map. This results in visually more coherent and appealing outcome. Removing boundary floaters. 3D tracking often produces outlier points along depth boundaries, common artifact in monocular depth estimation. To eliminate these floaters, we apply rendering loss to optimize the opacity of each point, effectively pruning points that cause visual artifacts. Specifically, given set of aggregated 3D points from dense tracking, we know each points 3D position in the frame coordinates of each frame of the input freeze-time video. This allows us to use the differentiable 3DGS renderer to re-render each input frame and compute the loss. Furthermore, since the points are modeled as isotropic Gaussians without orientation and are already in the frame coordinate system, we avoid the need to estimate camera extrinsics at this stage. This approach enhances robustness against multi-view inconsistencies in the input video. Temporal deformation with view-dependent compensation. The next step involves fitting temporal deformation field to animate the canonical 3DGS to follow the motion in the input 4D video. However, due to imperfections in the multi-view consistency of the 4D videoan issue inherited from the input freeze-time videodirectly optimizing the temporal deformation field would lead to noisy reconstructions, mirroring the challenges previously discussed. To address this issue, we augment the temporal deformation with additional view-dependent deformation to compensate for inconsistencies in the generated frames across different views. Specifically, to re-render point on the input frame Iij of the input frame grid, where and represent the indices of view and time respectively, the deformation offset pij for each point in canonical space is now computed as: pij = pv + pt j, (11) where pt represents the temporal deformation computed by an MLP, and pv is the view-dependent deformation estimated via dense 3D tracking during the canonical 3DGS reconstruction stage. It is worth noting that view-dependent deformation has also been employed in 4Real [49]; however, in our approach, the view-dependent deformation is predicted from dense 3D tracking rather than optimized using rendering loss, making it more robust. 7. Implementation details of ablation study We compared different baseline variants to analyze our approach. Below, we provide details for each method corresponding to the columns in Fig. 7. 1. Which video has more realistic motion? Take into consideration the magnitude, smoothness, and consistency of the motion. Pay close attention to the deformed limbs of humans and animals and unnatural deformations. 2. Which video has the highest quality foreground? 3. Which video has the highest quality background? 4. Which video has an object of better, more realist shape? That is the video in which the main object has the most natural shape, again paying attention to deformed limbs of humans and animals and unnatural deformations. 5. In general which video looks higher quality? 6. Which video is most dynamic? The video that contains the most motion. Please keep in mind that these is several views of the same dynamic video, played one after the other, so ignore all camera movement and focus solely on object movement. Please exclude from consideration any random limb deformations. 7. Which video is better following the text description? That is which video reflects all the aspects included in the text description Finally, if there is no significant difference in your opinion send the video to junk. Sequential w/o training. We sequentially interleave crossview and cross-time attention, as described in Equation (5). All parameters in the attention layers are directly inherited from the base video model without additional training. We observe that this variant produces noisy outputs lacking meaningful structure. Parallel w/o training, hard sync. We perform inference using the proposed architecture without training the synchronization layers. For hard synchronization, we average the token updates, i.e. xl+1 = 1 2 (yv + yt l). (12) This version generates some content with elements from the input video, but it remains highly noisy. Parallel w/o training, soft sync. The soft synchronization is implemented as weighted averaging, xv l+1 = (1 wl)yv l+1 = (1 wl)yt xt + wlyt + wlyv (13) Here, wl represents the weight, which gradually increases with the layer depth, specifically defined as wl = 0.1 + 0.4. This approach produces results with more discernible content compared to hard synchronization. Sequential trained. The sequential architecture is trained following the same procedure as our proposed approach. We experimented with two variants: finetuning only the cross-time attention and finetuning only the temporal attention. Our findings indicate that finetuning temporal attention results in more stable outcomes. Therefore, for brevity, we report results only for the version where cross-time attention is finetuned. Parallel hard sync. The variant of our proposed method employing hard synchronization. Parallel soft sync w/o Objaverse. variant of our proposed method with soft synchronization, without finetuning on animated 4D Objaverse data. 8. User study details The user study shown in Fig. 8 is conducted with 10 evaluators per video pair. During each session, evaluators were presented with two anonymized videos with an interface as shown in Fig. 9. The evaluators were given the following instructions: You are shown description of video and two different 3D videos generated by AI based on this description. Your task is to answer 7 questions regarding the quality of these videos. Please pay close attention to instructions and answer as thoughtfully as you can. The video shows several consecutive views of the same dynamic object. Figure 9. screenshot of the interface for user study."
        }
    ],
    "affiliations": [
        "KAUST",
        "Snap Inc",
        "Umass Amherst"
    ]
}