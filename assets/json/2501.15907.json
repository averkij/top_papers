{
    "paper_title": "Emilia: A Large-Scale, Extensive, Multilingual, and Diverse Dataset for Speech Generation",
    "authors": [
        "Haorui He",
        "Zengqiang Shang",
        "Chaoren Wang",
        "Xuyuan Li",
        "Yicheng Gu",
        "Hua Hua",
        "Liwei Liu",
        "Chen Yang",
        "Jiaqi Li",
        "Peiyang Shi",
        "Yuancheng Wang",
        "Kai Chen",
        "Pengyuan Zhang",
        "Zhizheng Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in speech generation have been driven by the large-scale training datasets. However, current models fall short of capturing the spontaneity and variability inherent in real-world human speech, due to their reliance on audiobook datasets limited to formal read-aloud speech styles. To bridge this gap, we introduce Emilia-Pipe, an open-source preprocessing pipeline to extract high-quality training data from valuable yet underexplored in-the-wild data that capture spontaneous human speech in real-world contexts. By leveraging Emilia-Pipe, we construct Emilia, the first multilingual speech generation dataset derived from in-the-wild speech data. This dataset comprises over 101k hours of speech across six languages: English, Chinese, German, French, Japanese, and Korean. Besides, we expand Emilia to Emilia-Large, a dataset exceeding 216k hours, making it the largest open-source speech generation dataset available. Extensive experiments demonstrate that Emilia significantly outperforms traditional audiobook datasets in generating spontaneous and human-like speech, showcasing superior performance in capturing diverse speaker timbre and speaking styles of real-world human speech. Furthermore, this work underscores the importance of scaling dataset size to advance speech generation research and validates the effectiveness of Emilia for both multilingual and crosslingual speech generation."
        },
        {
            "title": "Start",
            "content": "Emilia: Large-Scale, Extensive, Multilingual, and Diverse Dataset for Speech Generation Haorui He, Zengqiang Shang, Chaoren Wang, Xuyuan Li, Yicheng Gu, Hua Hua, Liwei Liu, Chen Yang, Jiaqi Li, Peiyang Shi, Yuancheng Wang, Kai Chen, Pengyuan Zhang, Zhizheng Wu, Senior Member, IEEE 1 5 2 0 2 7 ] . [ 1 7 0 9 5 1 . 1 0 5 2 : r AbstractRecent advancements in speech generation have been driven by the large-scale training datasets. However, current models fall short of capturing the spontaneity and variability inherent in real-world human speech, due to their reliance on audiobook datasets limited to formal read-aloud speech styles. To bridge this gap, we introduce Emilia-Pipe, an open-source preprocessing pipeline to extract high-quality training data from valuable yet underexplored in-the-wild data that capture spontaneous human speech in real-world contexts. By leveraging Emilia-Pipe, we construct Emilia, the first multilingual speech generation dataset derived from in-the-wild speech data. This dataset comprises over 101k hours of speech across six languages: English, Chinese, German, French, Japanese, and Korean. Besides, we expand Emilia to Emilia-Large, dataset exceeding 216k hours, making it the largest open-source speech generation dataset available. Extensive experiments demonstrate that Emilia significantly outperforms traditional audiobook datasets in generating spontaneous and human-like speech, showcasing superior performance in capturing diverse speaker timbre and speaking styles of real-world human speech. Furthermore, this work underscores the importance of scaling dataset size to advance speech generation research and validates the effectiveness of Emilia for both multilingual and crosslingual speech generation. Index TermsSpeech Generation, TTS, Dataset, Multilingual, In-the-wild data I. INTRODUCTION recent years, (zero-shot) speech generation research has witnessed significant advancements [1], [2], [3], [4], [5], [6], [7], with diverse models utilizing large-scale training datasets. These advancements lead to improved voice quality, timbre similarity, and naturalness [8]. Nevertheless, generated speech still falls short of replicating the spontaneity and variability characteristic of real-world human speech [2], [9]. primary factor for this limitation is the reliance of current speech generation models on datasets derived from audiobooks [10], [11]. Such datasets predominantly feature formal, read-aloud speech styles, which starkly contrast with the diverse and spontaneous nature of human speech in casual or conversational settings. Real-world speech is characterized Haorui He, Chaoren Wang, Yicheng Gu, Liwei Liu, Jiaqi Li, Yuancheng Wang, and Zhizheng Wu are with the Chinese University of Hong Kong, Shenzhen, China Zengqiang Shang, Xuyuan Li, Hua Hua, Chen Yang, Peiyang Shi, and Pengyuan Zhang are with the Laboratory of Speech and Intelligent Information Processing, Institute of Acoustics, CAS, Beijing, China. Xuyuan Li, Hua Hua, Chen Yang, and Pengyuan Zhang are also with the University of Chinese Academy of Sciences, Beijing, China. Kai Chen is with Shanghai AI Laboratory, Shanghai, China. : Equal contribution, and the names are listed in random order. by wide range of phenomena, including breathing sounds, pauses, stuttering, repetitions, variations in speaking rate and emotions. Therefore, there is significant research gap for new dataset that captures broader spectrum of speech styles to advance the field toward generating more spontaneous and human-like speech. However, directly utilizing in-the-wild speech data presents significant challenges due to variations in quality, typically manifested as frequent background noise or music, reverberation, overlapping speakers within single sample, inconsistent audio lengths, and the absence of essential annotations such as text transcriptions [12], [13]. Training speech generation models on such unprocessed raw data can lead to degraded performance [12], [14], [15], [16]. Although prior studies [12], [13] have proposed automatic preprocessing pipelines to address challenges associated with in-the-wild speech data, these pipelines heavily rely on proprietary models, significantly limiting their accessibility to the broader research community. Additionally, these pipelines are restricted to monolingual (i.e., Chinese-only) speech data, rendering them unsuitable for processing the vast multilingual speech data available in the wild. Furthermore, the computational efficiency of these pipelines remains undocumented, raising concerns about their practicality for scaling speech generation datasets. There is an urgent need for an effective open-source preprocessing pipeline capable of handling multilingual in-the-wild speech data efficiently and facilitating substantial dataset expansion. In response, we introduce Emilia-Pipe, the first opensource preprocessing pipeline designed to harness valuable yet underexplored in-the-wild multilingual speech data for building high-quality training datasets of spontaneous and human-like speech generation models. The Emilia-Pipe is designed to comprise six core preprocessing steps: standardization, source separation, speaker diarization, fine-grained segmentation via voice activity detection (VAD), automated speech recognition (ASR), and filtering. Furthermore, Emilia-Pipe incorporates extensive engineering optimizations to enhance both robustness and efficiency. These designs position Emilia-Pipe as versatile and scalable tool for constructing extensive, multilingual, and diverse speech datasets, thereby addressing the critical gap in speech generation. Leveraging Emilia-Pipe, we construct Emilia, the first multilingual speech generation dataset derived from in-the-wild speech data, which comprises over 101k hours of speech data at 24 kHz and spans six languages: English (En), Chinese (Zh), German (De), French (Fr), Japanese (Ja), and Korean (Ko). Additionally, we further TABLE I: comparison of Emilia and Emilia-Large datasets with existing datasets for speech generation."
        },
        {
            "title": "Data Source",
            "content": "Total Duration (hours)"
        },
        {
            "title": "Language",
            "content": "Samp. Rate (Hz)"
        },
        {
            "title": "Dynamic",
            "content": "LJSpeech [17] AutoPrepWild [12] VCTK [18] AISHELL-3 [19] LibriTTS [20] GigaSpeech [21] WenetSpeech4TTS [13] Libri-Heavy [22] MLS [11] Libri-Light [10] Audiobook In-the-wild Studio Recording Studio Recording Audiobook In-the-wild In-the-wild Audiobook Audiobook Audiobook Emilia Emilia-Large In-the-wild In-the-wild 24 39 44 85 585 10k 12k 50k 51k 60k 101k 216k En Zh En Zh En En Zh En En/Fr/De/Nl/Es/It/Pt/Pl En En/Zh/De/Fr/Ja/Ko En/Zh/De/Fr/Ja/Ko 22.05k 24k/44.1k 48k 44.1k 24k 16k 16k 16k 16k 16k 24k 24k (Proprietary) (Proprietary) expand the Emilia dataset to an even larger scale of 216K hours, introducing Emilia-Large. Table compares Emilia and Emilia-Large with several existing speech generation datasets. The key advantages of the Emilia and Emilia-Large datasets are summarized as follows. Extensive: The Emilia dataset contains over 101k hours of speech data at 24 kHz, and the Emilia-Large variant expands the size to 216k hours. To our best, Emilia-Large is the largest open-source speech generation dataset. Multilingual: The Emilia and Emilia-Large datasets covers six languages, supporting the training of multilingual and crosslingual speech generation models. Diverse: The most significant feature of Emilia and EmiliaLarge, compared to previous datasets, is its composition of mostly spontaneous, in-the-wild speech data, encompassing wide range of speaking styles, which is crucial for training the next generation of speech generation models capable of producing spontaneous and human-like speech. Dynamic: The Emilia and Emilia-Large dataset uniquely features an automatic and efficient processing pipeline, i.e., Emilia-Pipe, enabling seamless expansion in total size and language coverage, significantly facilitating rapid and scalable dataset creation. This work extends upon our previous research presented at IEEE Spoken Language Technology Workshop 2024 [23], introducing the following four key enhancements: Larger-Scale Dataset: We expanded the initial Emilia dataset to create Emilia-Large, dataset more than twice the size of its predecessor, standing as the largest opensource speech generation dataset available to date. Comparative Analysis of Audiobook and In-the-Wild Datasets: We present more details for the experiments that compare the speech generation performance of audiobook datasets and the in-the-wild dataset Emilia. Exploration of Data Scaling Laws in Speech Generation: Leveraging the large-scale nature of Emilia, we conducted experiments to investigate the impact of dataset size on speech generation performance. These experiments provide valuable insights into the importance of scaling dataset size for advancing the field of speech generation. Multilingual and Crosslingual Effectiveness Analysis: We performed more comprehensive set of experiments to validate the multilingual and crosslingual capabilities of the Emilia dataset, further demonstrating its versatility and applicability across diverse linguistic contexts. Our code for the Emilia-Pipe1 and the Emilia and EmiliaLarge datasets2 has been made publicly available to facilitate further research and reproducibility. Additionally, during the course of this work, significant advancements in speech generation research have emerged using the initial Emilia dataset. These include, but are not limited to, MaskGCT [24], F5-TTS [25], and Vevo [26], all of which provide open-source pre-trained models or code, serving as valuable contributions to the research community. The remainder of this paper is structured as follows. Section II reviews related work, followed by comprehensive description of the proposed Emilia-Pipe in Section III. Section IV details and analyzes the construction of the Emilia dataset. Implementation specifics and experimental results are presented in Section V, accompanied by an in-depth discussion. Finally, Section VI concludes the paper. II. RELATED WORK A. Speech Generation Datasets The size of datasets for speech generation has increased substantially over the years. Early datasets typically comprised tens of hours of speech. For example, LJSpeech [17] contains 24 hours of speech data from single speaker. The VCTK dataset [18] includes 44 hours of speech data from 109 speakers, while AISHELL-3 [19] comprises approximately 85 hours of recordings from 218 speakers. Subsequently, larger-scale datasets have emerged to enable research in zero-shot speech generation. For instance, LibriTTS [20] contains 585.8 hours of speech data from audiobooks, while GigaSpeech offers 10k hours of unprocessed speech data from diverse sources in the wild. Later, the research community scaled up speech generation datasets to over 50k hours. Emerging datasets such as MLS [11] (51k hours), Libri-Light [10] (60k hours), and Libri-Heavy [22] (50k hours) significantly enhance the performance for zero-shot speech generation models. However, these datasets are either rooted in audiobooks, limited to formal read-aloud speech styles due to their reliance on audiobooks as data sources, which lack the naturalness and variability of 1https://github.com/open-mmlab/Amphion/tree/main/preprocessors/Emilia 2https://huggingface.co/datasets/amphion/Emilia-Dataset 3 Fig. 1: An overview of the Emilia-Pipe processing pipeline. It consists of six steps, namely, standardization, source separation, speaker diarization, fine-grained segmentation by voice activity detection (VAD), automated speech recognition (ASR), and filtering. spontaneous speech [9], [27], or they directly utilize raw in-thewild speech data that suffers from uncertain quality [12], [14], [15], [16]. Both scenarios lead to sub-optimal performance in speech generation tasks. Notably, two previous works [13], [12] propose similar automated preprocessing pipelines for building speech generation datasets from in-the-wild data. However, as discussed earlier, these pipelines heavily rely on proprietary models and are restricted to Chinese-only speech data, with unknown computational efficiency. To bridge this gap, we design and open-source an efficient pipeline, Emilia-Pipe, which can rapidly process large-scale raw multilingual speech data to facilitate substantial dataset creation and expansion. B. Speech Generation Models Traditional speech generation models, such as Tacotron [28], FastSpeech [29], [30], and NaturalSpeech [27], are limited by smaller datasets comprising tens of hours of speech data and limited number of speakers. Recent advancements in speech generation, driven by large-scale speech datasets, have significantly improved voice quality, timbre similarity, and naturalness in zero-shot speech generation using only short reference speech sample of few seconds [1], [2], [3], [4], [5], [6]. For example, VALLE [1], VoiceBox [3], and SoundStorm [4] use more than 50k hours of speech data for training. Later models, such as NaturalSpeech3 [2], BaseTTS [5], and Seed-TTS [6], further expanded dataset sizes to over 100k hours. Notably, BaseTTS reported the emergent abilities of TTS models as dataset sizes scaled, enabling the rendering of complex prosody patterns such as emotions based on textual cues without explicit labels. An large-scale open-source dataset is needed for large-scale speech generation research. This work addresses this specific need. III. THE EMILIA-PIPE PROCESSING PIPELINE As discussed above, raw speech data in the wild need to be processed to be leveraged for training speech generation models. Therefore, we design an automatic preprocessing pipeline, Emilia-Pipe, for transforming in-the-wild multilingual speech data into high-quality training datasets. As illustrated in Fig. 1, Emilia-Pipe includes six steps: standardization, source separation, speaker diarization, fine-grained segmentation by voice activity detection (VAD), automated speech recognition (ASR), and filtering. This section details the six steps of EmiliaPipe and evaluates its performance. a) Standardization: The raw speech data in the wild vary in encoding formats, sampling rates, and other characteristics. To standardize the collected data, we convert all samples to WAV files, set them to mono channel, and resample to 24 kHz. We set the sample width to 16 bits and adjust the target decibels relative to full scale to -20 dBFS. The actual gain is constrained within -3 to 3 dB to ensure appropriate volume without distortion. Finally, we normalize the waveform by dividing each sample by the maximum amplitude, ensuring values range between -1 and 1. This step ensures consistent data format for further processing. b) Source Separation: The raw speech data in the wild often contain background music and noise, which negatively impact the performance of speech generation models [3], [14], [15]. To address this issue, we employ source separation techniques to extract clean human vocals. Specifically, we utilize the open-source pre-trained Ultimate Vocal Remover model [31], UVR-MDX-Net Inst 33. This model achieves high signal-to-distortion ratio (SDR) of 11.15 for vocal separation on the Synth MVSep dataset [32]. Using this model, we effectively separate human vocals for further processing. c) Speaker Diarization: After extracting clean human vocals from the raw speech data, we apply speaker diarization techniques to partition long-form speech data into multiple utterances based on the speaker. This process generates series of speech segments, with each segment ideally containing only one speaker, ensuring compatibility with existing datasets for 3https://github.com/TRvlvr/model repo/releases/tag/all public uvr models 4 speech generation. To achieve this, we leverage the PyAnnote speaker diarization 3.1 pipeline.4, which includes three core components: speaker segmentation, speaker embedding, and clustering, achieving state-of-the-art (SOTA) speaker diarization performance [33], [34]. The output is list of temporal annotations indicating the start and end times of the singlespeaker segments. d) Fine-grained Segmentation by VAD: Although the speaker diarization pipeline provides coarse segmentation for the raw speech data, the resulting utterances may still be too long to fit into memory. To address this, we use VAD model to further segment the utterances into smaller segments ranging from 3 to 30 seconds. This is achieved by concatenating consecutive chunks containing voice activity from the same speaker. We leverage the open-source library Silero-VAD5. The pre-trained model provided in Silero-VAD achieves ROCAUC score of 0.99 on the LibriParty dataset, ensuring accurate detection of voice activity.6 e) ASR: The absence of text transcriptions impedes the direct use of in-the-wild dataset for TTS. Therefore, we use ASR techniques to transcribe the segmented speech data. Considering the trade-off among speed, robustness, and accuracy, we employ the medium version of the Whisper model [35], SOTA multilingual ASR model capable of robust speech translation and language identification. To further enhance efficiency, we leverage WhisperX [36], which builds on the faster-whisper backend7 and the CTranslate2 engine.8 This setup is up to four times faster than the official Whisper implementation while maintaining the comparable accuracy and using less memory. Additionally, we omit the original models inherent VAD component by using the outputs in the last step to avoid redundant processing and develop batched inference to transcribe the speech data in parallel. These improvements allow our ASR step to achieve accurate text transcriptions for the speech data with high efficiency. f) Filtering: In real-world scenarios, some noise may not be effectively handled by source separation, the ASR step may introduce errors, and some raw speech data may be of low quality [12]. Therefore, to ensure the quality of the resulting dataset, we apply the following filtering criteria.9 Firstly, we utilize the language identification results from the Whisper model in the ASR step. We discard speech data that are not predicted to belong to our target languages (En, Zh, De, Fr, Ja, Ko) or have language identification confidence lower than 80%. Secondly, we use the DNSMOS P.835 OVRL score [37] (hereafter referred to as DNSMOS score for brevity) to estimate the overall speech quality, preserving only those speech data with score higher than 3.0. Finally, for each raw speech data, we compute the average character duration over its corresponding segments. We consider segments with an average phone duration outside 1.5 times the interquartile 4https://github.com/pyannote/pyannote-audio 5https://github.com/snakers4/silero-vad 6https://github.com/speechbrain/speechbrain/tree/develop/recipes/ LibriParty/generate dataset 7https://github.com/SYSTRAN/faster-whisper 8https://github.com/OpenNMT/CTranslate2 9Please note that the filtering criteria can be adjusted to fit the specific needs of different use cases. range (IQR) above the third quartile or below the first quartile as outliers and discard the speech data for these segments. After filtering, we obtain the resulting high-quality dataset suitable for training the speech generation model. g) Performance Evaluation: To validate the effectiveness of Emilia-Pipe, we randomly sample subset of raw speech data, approximately 600 hours, and use Emilia-Pipe to process this subset to evaluate its effectiveness and efficiency. The evaluation is conducted on an independent server with eight NVIDIA RTX 4090 GPUs. The entire processing time takes about 3.99 hours.10 Table II shows the processing results of Emilia-Pipe on this subset. The raw data has wide range of audio durations from 9.22 to 18,056.98 seconds, with an average of 1,572.53 seconds and high variability. The DNSMOS scores range from 1.08 to 3.51, with an average of 2.50, indicating varied overall quality. After filtering, the total duration for the resulting data is further reduced to 176.22 hours, retaining 29.43% of the raw speech data, and the average DNSMOS score significantly improves to 3.26 with minimal variability, indicating that Emilia-Pipe can effectively transform in-the-wild speech data into high-quality training data for speech generation. Additionally, for processing this subset, Emilia-Pipe processes about 2.50 hours of data every minute, demonstrating our Emilia-Pipe significantly exceeds real-time standards, making it ideal for preprocessing extensive speech data and scaling up the training dataset. IV. THE EMILIA AND EMILIA-LARGE DATASET Leveraging Emilia-Pipe, we are able to construct speech generation datasets derived from in-the-wild speech data. In this section, we describe our constructed Emilia dataset and the extended Emilia-Large dataset. These datasets contain inthe-wild speech data in six languages (En, Zh, De, Fr, Ja, Ko), processed by Emilia-Pipe. Duration statistics for each language in the datasets are provided in Fig. 2. A. The Emilia Dataset 1) Overview: Using Emilia-Pipe, we construct the Emilia dataset from in-the-wild speech data sourced from vast collection of video and podcast platforms on the Internet. This data covers various content categories such as talk shows, interviews, debates, sports commentary, and audiobooks, thereby capturing wide array of real human speaking styles. After processing, the Emilia dataset includes total of 101,654 hours of multilingual speech data across six different languages. 2) Dataset Analysis: To validate the quality and diversity of the Emilia dataset, we conduct respective analyses. a) Quality: To evaluate the quality, we compared Emilia with several existing datasets using DNSMOS scores. This nonintrusive speech quality metric reflects the overall quality of the speech data and is highly correlated with human ratings [37]. Table III presents the speech quality comparison between Emilia and several existing datasets. Emilia achieves DNSMOS score of 3.26, ranking third among all datasets. The results indicate 10Please note that the processing speed may be subject to hardware environments and data characteristics. The reported figure is for reference. TABLE II: Statistics of 600 hours of in-the-wild raw speech data processed by Emilia-Pipe."
        },
        {
            "title": "Dataset",
            "content": "Raw Processed w/o Filtering Processed min 9.22 1.00 3.00 Duration (s)"
        },
        {
            "title": "DNSMOS",
            "content": "max avg std min max avg std Total Duration (hours) 18,056.98 30.00 30. 1,572.53 1,966.66 7.18 5.06 8.98 4.99 1.08 0.91 3.00 3.51 3.67 3.67 2.50 0.62 2.86 0.51 3.26 0.14 598.87 (100.00%) 340.54 (56.86%) 176.22 (29.43%) 46.77% 49.87% 1.72%0.22% 1.38% 1.59% En: 46.8k Zh: 49.9k De: 1.6k Fr: 1.4k Ja: 1.7k Ko: 0.2k (a) Emilia 61.95% 26.49% 3.44% 1.17% 3.80% 3.15% En: 134.1k (2.9x) Zh: 57.3k (1.1x) De: 6.8k (4.3x) Fr: 8.2k (6.0x) Ja: 2.5k (1.5x) Ko: 7.4k (34.3x) (b) Emilia-Large Fig. 2: Duration statistics (in hours) of the speech data in Emilia and Emilia-Large by language. The numbers in parentheses indicate the scaling factor (multiples) of the speech data in Emilia-Large compared to the original Emilia dataset. that, despite being sourced from raw speech data in the wild, after preprocessing, the speech quality of the Emilia dataset is comparable to existing datasets sourced from studio recordings or audiobooks and outperforms the existing datasets sourced from unprocessed in-the-wild speech data. b) Diversity: The Emilia dataset comprises collection of speech data from wide range of video and podcast platforms, capturing diverse speaking styles of real human speech. To quantify this diversity, we conducted analyses on both the acoustic and semantic feature spaces, comparing it with the MLS dataset, which is derived from audiobooks and widely used for training speech generation models. Specifically, we randomly selected 5k samples each from the English subset of MLS and Emilia, denoted as Emilia English and MLS English respectively in Fig. 3. TABLE III: Quality comparison between Emilia and nine existing datasets. The scores for LJSpeech, AutoPrepWild, AISHELL-3, and LibriTTS are derived from [12]. The score for Libri-Light is computed from its official small subset, and the score for WenetSpeech4TTS is computed from its official basic subset. The scores for MLS and Emilia are computed from randomly sampled 600-hour subset. Dataset LJSpeech [17] AutoPrepWild [12] VCTK [18] AISHELL-3 [19] LibriTTS [20] GigaSpeech [21] WenetSpeech4TTS [13] MLS [11] Libri-Light [10] Emilia DNSMOS 3.30 0.17 3.24 0.21 3.20 0.18 3.15 0.17 3.25 0.19 2.52 0.54 3.18 0.22 3.33 0.19 3.25 0.26 3.26 0.14 To analyze the diversity of acoustic features, we leveraged pre-trained WavLM model [38] to extract acoustic representations, capturing various characteristics such as speaker identity, emotion, and prosody. We then applied Principal Component Analysis (PCA) [39] to reduce the dimensionality of these representations to two dimensions. As shown in Fig. 3(a), the Emilia dataset exhibits broader dispersion compared to MLS, which shows more compact clustering. The more scattered pattern highlights that the Emilia dataset encompasses richer coverage of acoustic characteristics compared to the MLS dataset derived from audiobooks. For the semantic diversity analysis, we employed pretrained Sentence-BERT model [40] to generate text representations for the transcripts of each speech sample. Consequently, each speech sample is represented as 768-dimensional vector based on its textual content, providing comprehensive approximation of its semantics. Similar to the acoustic analysis, we reduced the dimensionality of the semantic features to two dimensions. As shown in Fig. 3(b), the scatter of textual features indicates that the Emilia dataset covers wide range of textual content, validating the significant diversity in Emilias semantic coverage. B. The Emilia-Large Dataset Given the significant advancements in large-scale speech generation [23], we are motivated to further scale up the volume of the training dataset for speech generation models to investigate the data scaling laws in speech generation and potentially enhance model performance. In this work, we expand the initial Emilia dataset to an even larger scale, 6 has been expanded several-fold (4.3 times for De, 6.0 times for Fr, and 34.3 times for Ko, respectively). This enhancement addresses the relatively limited data volume of these languages in Emilia, improving support for multilingual and crosslingual speech generation tasks. Emilia-Large (216k hours) Emilia (CC-BY-NC-4.0, 101k hours) Emilia-YODAS (CC-BY-4.0, 114k hours) Other small sources Fig. 4: The relationship between Emilia and Emilia-Large: Emilia-Large extends Emilia with additional data, EmiliaYODAS, primarily processed from YODAS2. V. EXPERIMENTS In this section, we conduct experiments to address the following evaluation questions (EQs), which are designed to validate the strengths of the Emilia dataset in terms of diversity (EQ1), extensiveness (EQ2), and multilingual utility (EQ3): EQ1: How does the performance of the in-the-wild dataset, Emilia, compare to that of existing audiobook datasets in training speech generation models? EQ2: What are the data scaling laws in speech generation, i.e., the impact of dataset size on speech generation performance with fixed number of model parameters? EQ3: How effective is the Emilia dataset for training multilingual and crosslingual speech generation models? A. Comparison of Audiobook and In-the-Wild Datasets (EQ1) In this subsection, we conduct experiments to compare audiobook datasets with in-the-wild datasets for text-to-speech (TTS) generation. a) Baselines: We implement two SOTA TTS models as baselines: (1) AR+SoundStorm [24]: two-stage model where llama-style autoregressive (AR) generative transformer first predicts semantic tokens using text and prompt semantic tokens as input. This is followed by SoundStorm-based semantic-to-acoustic model [4] that generates acoustic tokens conditioned on the predicted semantic tokens. (2) VoiceBox [3]: non-autoregressive (NAR) speech generation model that leverages flow matching and in-context learning to predict melspectrograms. For comprehensive details on these models, we refer readers to their respective publications. b) Training Sets: We evaluate the performance of TTS models trained on two English datasets: the English subset of the Emilia dataset (hereafter referred to as Emilia-En for brevity) and the MLS dataset, high-quality corpus derived from audiobooks. The Emilia-En dataset consists of approximately 46k hours of English speech, while the MLS dataset contains 45k hours, rendering their sizes comparable. (a) Acoustic diversity (b) Semantic diversity Fig. 3: comparison of acoustic and semantic diversities between Emilia and MLS datasets. introducing Emilia-Large, which contains total of 216,313 hours of spontaneous and human-like speech data. Emilia-Large builds upon the same construction methodology as Emilia using our proposed Emilia-Pipe but doubles the total dataset size, making it the largest open-source speech generation dataset to date. The expansion from Emilia to Emilia-Large primarily uses YODAS2 [41] as the raw data source, termed EmiliaYODAS. The relationship between Emilia and Emilia-Large is illustrated in Fig. 4. YODAS2 is large-scale (500k hours) real-world speech collection from YouTube videos with CCBY-3.0 licenses in more than 100 languages. We selectively downloaded and processed data from YODAS2 and other smaller sources in the six languages of the original Emilia dataset. 11 We process the raw speech data using our proposed Emilia-Pipe, with only one alteration: changing the DNSMOS filtering threshold for the incremental data to 2.4 to align with [12] and preserve more data. We make the Emilia-YODAS publicly available under the CC-BY-4.0 license. Fig. 2(b) demonstrates the duration statistics for each language in Emilia-Large. It is observed that the key distinction of Emilia-Large compared to Emilia is its significantly improved inclusion of low-resource languages, especially for German (De), French (Fr), and Korean (Ko). Specifically, compared to the original Emilia dataset, the data for these three languages 11Unused data of other languages in YODAS2 can also be seamlessly processed by Emilia-Pipe for speech generation training. Their effectiveness may depend on the ASR models performance in each language. TABLE IV: Objective and subjective evaluation results of TTS models trained with Emilia-En and MLS on LibriSpeech-Test and Emilia-Test evaluation sets. The best results for each model are highlighted in bold. Model Training Set LibriSpeech-Test Emilia-Test WER S-SIM FSD CMOS SMOS WER S-SIM FSD CMOS SMOS AR+SoundStorm VoiceBox MLS Emilia-En MLS Emilia-En 8.9% 8.4% 6.1% 7.2% 0.612 0.577 0.625 0.585 49.11 24.73 16.83 23. -0.36 -0.19 0.36 0.42 3.13 3.28 3.62 3.77 7.7% 6.6% 8.2% 7.4% 0.587 0.618 0.528 0.601 20.76 12.73 15.94 14.07 0.09 0.19 0.28 0. 3.71 3.73 3.61 3.76 c) Evaluation Sets: To ensure comprehensive evaluation, we employ two distinct evaluation sets: (1) LibriSpeech-Test: This set includes 1,200 speech samples in formal reading styles similar to those in the MLS dataset. (2) Emilia-Test: This set consists of 600 speech samples in spontaneous, human-like speaking styles akin to those in the Emilia dataset. Both sets are unseen by the baseline models during training. d) Metrics: We conduct both objective and subjective evaluations to assess the performance of the baseline models. For objective evaluation, we focus on the following aspects. (1) Intelligibility: Measured by the Word Error Rate (WER) of the generated speechs transcription compared to the target text. For the LibriSpeech-Test, we utilize fine-tuned HuBERTLarge ASR model.12 For Emilia-Test, we employ the more robust Whisper-Medium [35] for ASR. (2) Similarity: Assessed by Speaker Similarity Score (S-SIM) between the generated speech and the reference speech using the WavLM-TDCNN speaker embedding model.13 We report the similarity to the original reference speech. (3) Naturalness: Evaluated using the Frechet Speech Distance (FSD), which measures the distance between the distributions of generated and real samples in feature space [3]. We compute this metric using emotion2vec features [42] to evaluate the emotional similarity of the speech. For the subjective evaluation, we randomly select eight samples each from the LibriSpeech-Test and the Emilia-Test evaluation set. Twelve proficient English speakers serve as evaluators. The subjective evaluation includes: (1) Speaker Similarity: We employ the Similarity Mean Opinion Score (SMOS) to assess the speaker similarity of the generated speech to the reference speech. The SMOS scale ranges from 1 to 5, with steps of 0.5. (2) Comparative Naturalness: We use the Comparative Mean Opinion Score (CMOS) to evaluate the comparative naturalness of the generated speech against the reference speech. The CMOS scale ranges from -3 (indicating the generated speech is much worse than the reference speech) to 3 (indicating the generated speech is much better than the reference speech), with steps of 1. e) Results and Discussions: Table IV summarizes the objective and subjective evaluation results of TTS models trained with the Emilia-En and MLS datasets on the LibriSpeechTest and Emilia-Test evaluation sets. For the LibriSpeechTest dataset, the AR+SoundStorm model trained on Emilia-En achieved lower WER (8.4%) and FSD (24.73) compared to its MLS-trained counterpart, while the VoiceBox model trained on 12https://huggingface.co/facebook/hubert-large-ls960-ft 13https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker verification MLS achieved the best WER (6.1%), S-SIM (0.625), and FSD (16.83). On the Emilia-Test dataset, the AR+SoundStorm model trained on Emilia-En outperformed the MLS-trained model across all metrics, including WER (6.6%), S-SIM (0.618), FSD (12.73), CMOS (0.19), and SMOS (3.73). Similarly, the VoiceBox model trained on Emilia-En achieved superior results in WER (7.4%), S-SIM (0.601), and SMOS (3.76) compared to the MLS-trained version. The result validate that, on the audiobook-style LibriSpeechTest dataset, models trained on both the Emilia-En and MLS datasets achieve comparable levels of intelligibility, speaker similarity and naturalness. This suggests that the Emilia dataset, despite being derived from raw speech data in the wild, is as effective as high-quality datasets like MLS after processing with our proposed Emilia-Pipe. However, on the Emilia-Test dataset, which includes more spontaneous and human-like speech, training with in-the-wild datasets like Emilia significantly enhances models speech generation performance. f) Summary (Answer to EQ1): The comparison between the in-the-wild Emilia dataset and audiobook datasets for speech generation tasks reveals that while both types of datasets yield comparable performance in formal, audiobookstyle speech, the Emilia dataset significantly outperforms audiobook datasets in generating more spontaneous and humanlike speech, showcasing significantly superior performance in cloning diverse speaker timbre and speaking styles. B. Data Scaling Law in Speech Generation (EQ2) Given the extensive nature of the Emilia dataset, we conduct experiments to investigate the impact of dataset size on speech generation performance with fixed number of model parameters, a.k.a, the data scaling law in speech generation. This section describes our experimental setups and results. a) Experimental Setups: We leverage the TTS baselines, evaluation sets, and objective metrics described in Sec. V-A, progressively increasing the training set size to the following volumes: 5k, 10k, 46k (the total duration of English speech in Emilia-En), 100k, and 134k (the total duration of English speech in Emilia-Large) hours, while recording the corresponding performance changes. Fig. 5 illustrates the WER trends for AR+SoundStorm and VoiceBox across training set sizes. For AR+SoundStorm evaluated on LibriSpeech (L), WER decreases from 5.2% at 5k hours to 4.2% at 134k hours, while VoiceBox (L) shows reduction from 7.2% to 5.6% over the same range. On the Emilia test set (E), AR+SoundStorm (E) improves from 5.7% to 4.9% WER, and VoiceBox (E) declines from 7.1% to 6.4%. AR+SoundStorm (L) VoiceBox (L) AR+SoundStorm (E) VoiceBox (E) 6.5 5.9 5.2 4. 6.3 5.7 4.9 4.4 6.7 5.7 4.9 4. 6.4 5.6 4.9 4.2 7.2 7.1 5.7 5. 8 6 4 5k 10k 46k 100k 134k Fig. 5: WER (%) v.s. train set size. (L) denotes results on LibriSpeech-Test, and (E) denotes results on Emilia-Test. 0.8 0.7 0. 0.59 0.59 0.6 0.59 AR+SoundStorm (L) VoiceBox (L) AR+SoundStorm (E) VoiceBox (E) 0.61 0.61 0.6 0.59 0.64 0.63 0.6 0.58 0.64 0.62 0.61 0. 0.5 0.4 0.42 0.42 5k 0.46 0.44 10k 46k 100k 134k Fig. 6: S-SIM v.s. LibriSpeech-Test, and (E) denotes results on Emilia-Test. train set size. (L) denotes results on 22 21.33 22 21.97 21.23 AR+SoundStorm (L) VoiceBox (L) AR+SoundStorm (E) VoiceBox (E) 20.58 19.93 20.4 19.85 20.71 19. 16.39 14.82 15.27 15.13 15.62 14.65 15.43 14.78 15.31 14. 5k 10k 46k 100k 134k 18 16 14 Fig. 7: FSD v.s. LibriSpeech-Test, and (E) denotes results on Emilia-Test. train set size. (L) denotes results on Fig. 6 demonstrates S-SIM scores, with AR+SoundStorm (L) rising from 0.587 to 0.620 and VoiceBox (L) increasing from 0.418 to 0.606 as data scales to 134k hours. For Emilia tests, AR+SoundStorm (E) improves from 0.587 to 0.636, while VoiceBox (E) progresses from 0.422 to 0.612. Fig. 7 reveals FSD trends: AR+SoundStorm (L) decreases from 22.00 to 20.40, VoiceBox (L) from 21.33 to 19.85, AR+SoundStorm (E) from 14.82 to 15.31, and VoiceBox (E) from 16.39 to 14.48. The results reveal consistent scalability pattern across all metrics: both models demonstrate steady improvements as the 8 dataset size increases, with only one exception of FSD for AR+SoundStorm (E). For smaller datasets (5k to 10k hours), performance gains are more pronounced, indicating that even modest increases in training data yield significant improvements. As the dataset size exceeds 46k hours, the rate of improvement slows but maintains positive trend, eventually converging at approximately 100k hours. b) Summary (Answer to EQ2): Our experimental results reveal the data scaling law in speech generation: performance improves as the dataset size increases, but with diminishing returns. Initially, significant improvements are observed when increasing the dataset size to 46k hours. Beyond 46k hours, performance continues to improve, though become less pronounced and at slower rate, and tends to converge at around 100k hours. These findings suggest that while larger datasets contribute to better performance, there is point of diminishing returns where additional data provides only marginal benefits, guiding future research in balancing dataset size and computational resources for optimal speech generation performance. For current TTS models, which typically contain around 0.51 parameters14, dataset of approximately 100k hours for each specific language represents the most cost-effective choice. C. Multilingual and Crosslingual Speech Generation (EQ3) In this subsection, we conduct experiments to evaluate the effectiveness of the Emilia dataset for training multilingual and crosslingual speech generation models. a) Experimental Setup: For this experiment, we utilize the complete Emilia-Large dataset, which encompasses six languages: English (En), Chinese (Zh), German (De), French (Fr), Japanese (Ja), and Korean (Ko), to train speech generation models. For evaluation, the English (En) evaluation set is derived from Emilia-Test. The Chinese (Zh) evaluation set is randomly sampled from the AISHELL-3 dataset [19], while the evaluation sets for German (De), French (Fr), Japanese (Ja), and Korean (Ko) are sourced from Common Voice [43]. Each evaluation set contains minimum of 500 reference speech samples. In multilingual experiments, these reference speech samples are directly utilized to synthesize target texts in the same language. For crosslingual experiments, the reference speech samples and the target texts are sampled from evaluation sets of different languages. b) Results and Discussions: The experimental results in Table demonstrate the effectiveness of the Emilia-Large dataset for multilingual and crosslingual speech generation. In multilingual generation, where the reference and target languages are identical, both AR+SoundStorm and VoiceBox achieve strong performance across all six languages. Spcifically, AR+SoundStorm exhibits WER ranging from 3.6% (Zh-Zh) to 6.3% (Ko-Ko), with S-SIM between 0.511 (Zh-Zh) and 0.673 (Ko-Ko), and FSD as low as 24.99 (En-En). VoiceBox also shows comparable performance, with WER ranging from 5.2% (De-De) to 6.5% (En-En), S-SIM from 0.557 (Zh-Zh) to 0.683 (De-De), and FSD from 24.34 to 44.71 (Ja-Ja). In crosslingual generation, where the reference and target languages differ, performance remains robust but experiences 14The size of our baselines also falls within this range. 9 TABLE V: Experimental results of AR+SoundStorm and VoiceBox for multilingual and crosslingual speech generation. The models were trained on the Emilia-Large dataset. Results for multilingual speech generation are highlighted in grey. Reference Target Metric En Zh Fr De Ja Ko WER S-SIM FSD WER S-SIM FSD WER S-SIM FSD WER S-SIM FSD WER S-SIM FSD WER S-SIM FSD AR+SoundStorm VoiceBox En Zh Fr De Ja Ko En Zh Fr De Ja Ko 5.9% 0.568 24. 5.3% 0.507 56.15 5.3% 0.596 39.89 4.5% 0.619 39.96 4.6% 0.622 49.42 6.2% 0.657 36.71 5.8% 0.431 99. 3.6% 0.511 40.09 5.3% 0.527 66.21 4.5% 0.545 57.82 4.4% 0.557 68.70 4.1% 0.593 58.85 6.4% 0.452 82. 5.2% 0.509 56.75 5.3% 0.596 39.88 4.7% 0.603 44.86 4.7% 0.626 44.67 6.1% 0.665 32.27 5.9% 0.529 26. 5.4% 0.504 57.10 5.2% 0.596 38.48 4.2% 0.639 33.16 4.5% 0.618 50.47 6.2% 0.656 37.20 6.3% 0.446 89. 4.9% 0.516 56.71 5.8% 0.572 51.13 4.8% 0.596 53.38 4.8% 0.641 44.28 6.2% 0.673 31.95 8.3% 0.443 98. 5.7% 0.523 52.60 8.1% 0.557 54.41 6.8% 0.591 55.12 6.6% 0.633 52.19 6.3% 0.673 30.27 6.5% 0.588 24. 8.6% 0.524 109.67 7.0% 0.565 91.08 5.2% 0.639 83.37 7.4% 0.556 103.68 8.0% 0.589 86.57 7.9% 0.386 91. 5.6% 0.557 40.04 6.3% 0.485 80.76 7.4% 0.519 72.18 5.5% 0.525 76.65 5.6% 0.567 63.49 8.8% 0.458 78. 6.7% 0.524 58.47 5.6% 0.589 42.38 6.8% 0.577 54.77 6.9% 0.521 63.55 7.8% 0.545 53.75 8.6% 0.490 68. 5.9% 0.522 72.47 6.9% 0.582 58.16 5.2% 0.683 34.41 6.7% 0.557 72.41 8.3% 0.597 57.19 8.3% 10.2% 0.442 0.425 89.49 92. 6.4% 0.543 64.73 7.5% 0.547 63.36 6.9% 0.538 67.89 6.2% 0.584 44.71 5.6% 0.595 52.85 7.0% 0.591 57. 9.3% 0.556 61.51 8.9% 0.586 67.46 6.6% 0.596 56.34 6.0% 0.648 38.82 moderate degradation. For instance, the WER increases, such as from 6.3% (Ko-Ko) to 8.3% (En-Ko) for AR+SoundStorm. Similarly, the S-SIM decreases, as seen in the comparison between En-En (0.588) and (En-Zh) 0.386 for VoiceBox. The FSD also rises, for example, the FSD for En-Zh in AR+SoundStorm is 99.40 compared to 24.99 for Zh-Zh. These results highlight two key findings. First, the EmiliaLarge dataset facilitates strong multilingual speech generation for both models. Second, crosslingual generation introduces notable challenges: while models retain functionality, performance gaps emerge, particularly for VoiceBox, indicating that the NAR-based architectures may struggle more with cross-language acoustic transfer. Furthermore, when comparing multilingual training with English-only training  (Fig. 57) , multilingual models exhibit slight trade-off in English performance. For instance, AR+SoundStorm trained on the multilingual 256k-hour Emilia-Large achieves WER of 4.9%, an S-SIM of 0.636, and an FSD of 15.31 on the English samples in the evaluation sets. These metrics are slightly worse than those of its monolingual counterpart trained on 134k-hour English-only dataset (WER=4.5%, S-SIM=0.65, FSD=14.8, as reported in prior results). This suggests that while multilingual data enables crosslingual capabilities, it minimally compromises language-specific performance. This trade-off is an important consideration for practical applications. Future work could explore strategies to narrow these performance gaps. c) Summary (Answer to EQ3): The Emilia-Large dataset proves highly effective for training multilingual and crosslingual speech generation models. However, we find that crosslingual generation introduces moderate performance degradation, highlighting challenges in cross-language acoustic transfer. These findings underscore the value of the Emilia-Large dataset as critical resource for advancing multilingual and crosslingual speech generation. Future work could focus on enhancing model adaptability to address crosslingual challenges. VI. CONCLUSION AND LIMITATION In conclusion, this work introduces Emilia-Pipe, an effective preprocessing pipeline designed to transform raw in-the-wild speech data into high-quality training datasets for spontaneous and human-like speech generation. Leveraging Emilia-Pipe, we construct Emilia, the largest open-source multilingual speech generation dataset, spanning over 101k hours across six languages, and its extended version, Emilia-Large, encompassing 216k hours. Comparative analyses demonstrate that Emilia significantly outperforms traditional audiobook datasets in generating spontaneous and human-like speech. Our experiments also investigate the relationship between dataset size and speech generation performance, revealing consistent improvements with data scaling, though the trend becomes less pronounced as the dataset size exceeds 100k hours. Finally, we validate that the proposed Emilia dataset effectively supports multilingual and crosslingual speech generation, paving the way for future advancements in this field. Future work may include training effective spoof detection models to mitigate potential safety concerns arising from the use of highly spontaneous and human-like speech generation models trained with Emilia, such as the risk of synthetic spoken misinformation [44]. Additionally, further extending the Emilia dataset to the singing domain may improve singing voice generation [45]. Despite the advancements, we point out few limitations. First, our used speaker diarization system, while SOTA, is not perfect and results in small proportion of speech segments containing more than one speaker or overlaps, subsequently affecting speech generation performance. Integrating more advanced speaker diarization and separation techniques in the future may alleviate this issue. Second, Emilia-Pipe segments speech samples into intervals of 3 to 30 seconds. Is is observed that generating speech outside this range may sometimes lead to unexpected outcome. Adjustments in the hyper-parameters of Emilia-Pipe may be needed for specific use cases. 10 [24] Y. Wang, H. Zhan, L. Liu, R. Zeng, H. Guo, J. Zheng, Q. Zhang, X. Zhang, S. Zhang, and Z. Wu, Maskgct: Zero-shot text-to-speech with masked generative codec transformer, in Proc. of ICLR, 2025. [25] Y. Chen, Z. Niu, Z. Ma, K. Deng, C. Wang, J. Zhao, K. Yu, and X. Chen, F5-tts: fairytaler that fakes fluent and faithful speech with flow matching, arXiv preprint arXiv:2410.06885, 2024. [26] X. Zhang, X. Zhang, K. Peng, Z. Tang, V. Manohar, Y. Liu, J. Hwang, D. Li, Y. Wang, J. Chan, Y. Huang, Z. Wu, and M. Ma, Vevo: Controllable zero-shot voice imitation with self-supervised disentanglement, in Proc. of ICLR, 2025. [27] X. Tan, J. Chen, H. Liu, J. Cong, C. Zhang, Y. Liu, X. Wang, Y. Leng, Y. Yi, L. He, S. Zhao, T. Qin, F. Soong, and T.-Y. Liu, Naturalspeech: End-to-end text-to-speech synthesis with human-level quality, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 46, no. 6, pp. 42344245, 2024. [28] Y. Wang, R. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss, N. Jaitly, Z. Yang, Y. Xiao, Z. Chen, S. Bengio et al., Tacotron: Towards end-toend speech synthesis, arXiv preprint arXiv:1703.10135, 2017. [29] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu, Fastspeech: Fast, robust and controllable text to speech, in Proc. of NeurIPS, 2019. [30] Y. Ren, C. Hu, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu, Fastspeech 2: Fast and high-quality end-to-end text to speech, in Proc. of ICLR, 2021. [31] M. Kim, W. Choi, J. Chung, D. Lee, and S. Jung, Kuielab-mdx-net: two-stream neural network for music demixing, in Proc. of ISMIR MDX Workshop, 2021. [32] R. Solovyev, A. Stempkovskiy, and T. Habruseva, Benchmarks and leaderboards for sound demixing tasks, arXiv preprint arXiv:2305.07489, 2023. [33] H. Bredin, Pyannote.audio 2.1 speaker diarization pipeline: Principle, benchmark, and recipe, in Proc. of INTERSPEECH, 2023. [34] A. Plaquet and H. Bredin, Powerset multi-class cross entropy loss for neural speaker diarization, in Proc. of INTERSPEECH, 2023. [35] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, Robust speech recognition via large-scale weak supervision, in Proc. of ICML, 2023. [36] M. Bain, J. Huh, T. Han, and A. Zisserman, Whisperx: Time-accurate speech transcription of long-form audio, in Proc. of INTERSPEECH, 2023. [37] C. K. Reddy, V. Gopal, and R. Cutler, Dnsmos p.835: non-intrusive perceptual objective speech quality metric to evaluate noise suppressors, in Proc. of IEEE ICASSP, 2022. [38] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao et al., Wavlm: Large-scale self-supervised pretraining for full stack speech processing, IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 15051518, 2022. [39] K. P. F.R.S., Liii. on lines and planes of closest fit to systems of points in space, The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, vol. 2, no. 11, pp. 559572, 1901. [40] N. Reimers and I. Gurevych, Sentence-bert: Sentence embeddings using siamese bert-networks, in Proc. of EMNLP, 2019. [41] X. Li, S. Takamichi, T. Saeki, W. Chen, S. Shiota, and S. Watanabe, Yodas: Youtube-oriented dataset for audio and speech, in Proc. of IEEE ASRU, 2023. [42] Z. Ma, Z. Zheng, J. Ye, J. Li, Z. Gao, S. Zhang, and X. Chen, Emotion2vec: Self-supervised pre-training for speech emotion representation, Proc. ACL Findings, 2024. [43] R. Ardila, M. Branson, K. Davis, M. Kohler, J. Meyer, M. Henretty, R. Morais, L. Saunders, F. Tyers, and G. Weber, Common voice: massively-multilingual speech corpus, in Proc. of LREC, 2020. [44] P. Liu, L. Wang, R. He, H. He, L. Wang, H. Zheng, J. Shi, T. Xiao, and Z. Wu, Spmis: An investigation of synthetic spoken misinformation detection, in Proc. of IEEE SLT, 2024. [45] Y. Gu, C. Wang, J. Zhang, X. Zhang, Z. Fang, H. He, and Z. Wu, Singnet: Towards large-scale, diverse, and in-the-wild singing voice dataset, OpenReview, 2024."
        },
        {
            "title": "REFERENCES",
            "content": "[1] C. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen, Y. Liu, H. Wang, J. Li et al., Neural codec language models are zero-shot text to speech synthesizers, arXiv preprint arXiv:2301.02111, 2023. [2] Z. Ju, Y. Wang, K. Shen, X. Tan, D. Xin, D. Yang, Y. Liu, Y. Leng, K. Song, S. Tang et al., Naturalspeech 3: Zero-shot speech synthesis with factorized codec and diffusion models, in Proc. of ICML, 2024. [3] M. Le, A. Vyas, B. Shi, B. Karrer, L. Sari, R. Moritz, M. Williamson, V. Manohar, Y. Adi, J. Mahadeokar et al., Voicebox: Text-guided multilingual universal speech generation at scale, in Proc. of NeurIPS, 2024. [4] Z. Borsos, M. Sharifi, D. Vincent, E. Kharitonov, N. Zeghidour, and M. Tagliasacchi, Soundstorm: Efficient parallel audio generation, arXiv preprint arXiv:2305.09636, 2023. [5] M. ajszczak, G. Cambara, Y. Li, F. Beyhan, A. van Korlaar, F. Yang, A. Joly, A. Martn-Cortinas, A. Abbas, A. Michalski et al., Base tts: Lessons from building billion-parameter text-to-speech model on 100k hours of data, arXiv preprint arXiv:2402.08093, 2024. [6] ByteDance, Seed-tts: family of high-quality versatile speech generation models, arXiv preprint arXiv:2406.02430, 2024. [7] Y. Huang, Y. Wang, J. Li, H. Guo, H. He, S. Zhang, and Z. Wu, Debatts: Zero-shot debating text-to-speech synthesis, arXiv preprint arXiv:2411.06540, 2024. [8] X. Zhang, L. Xue, Y. Gu, Y. Wang, J. Li, H. He, C. Wang, T. Song, X. Chen, Z. Fang, H. Chen, J. Zhang, T. Y. Tang, L. Zou, M. Wang, J. Han, K. Chen, H. Li, and Z. Wu, Amphion: An open-source audio, music and speech generation toolkit, in Proc. of IEEE SLT, 2024. [9] X. Tan, T. Qin, F. Soong, and T.-Y. Liu, survey on neural speech synthesis, arXiv preprint arXiv:2106.15561, 2021. [10] J. Kahn, M. Rivi`ere, W. Zheng, E. Kharitonov, Q. Xu, P.-E. Mazare, J. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen, T. Likhomanenko, G. Synnaeve, A. Joulin, A. Mohamed, and E. Dupoux, Libri-light: benchmark for asr with limited or no supervision, in Proc. of IEEE ICASSP, 2020. [11] V. Pratap, Q. Xu, A. Sriram, G. Synnaeve, and R. Collobert, Mls: large-scale multilingual dataset for speech research, in Proc. of INTERSPEECH, 2020. [12] J. Yu, H. Chen, Y. Bian, X. Li, Y. Luo, J. Tian, M. Liu, J. Jiang, and S. Wang, Autoprep: An automatic preprocessing framework for in-the-wild speech data, in Proc. of IEEE ICASSP, 2024. [13] L. Ma, D. Guo, K. Song, Y. Jiang, S. Wang, L. Xue, W. Xu, H. Zhao, B. Zhang, and L. Xie, Wenetspeech4tts: 12,800-hour mandarin tts corpus for large speech generation model benchmark, in Proc. of INTERSPEECH, 2024. [14] T.-h. Huang, J.-h. Lin, and H.-y. Lee, How far are we from robust voice conversion: survey, in Proc. of IEEE SLT, 2021. [15] H. He, Y. Song, Y. Wang, H. Li, X. Zhang, L. Wang, G. Huang, E. S. Chng, and Z. Wu, Noro: noise-robust one-shot voice conversion system with hidden speaker representation capabilities, arXiv preprint arXiv:2411.19770, 2024. [16] X. Li, Z. Shang, H. Hua, P. Shi, C. Yang, L. Wang, and P. Zhang, Sf-speech: Straightened flow for zero-shot voice clone on small-scale dataset, arXiv preprint arXiv:2410.12399, 2024. [17] K. Ito and L. Johnson, The lj speech dataset, keithito.com/ LJ-Speech-Dataset/, 2017. [18] J. Yamagishi, C. Veaux, and K. MacDonald, Cstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit (version 0.92), datashare.ed.ac.uk/handle/10283/3443, 2019. [19] S. Yao, H. Bu, X. Xu, S. Zhang, and M. Li, Aishell-3: multi-speaker mandarin tts corpus, in Proc. of INTERSPEECH, 2021. [20] H. Zen, V. Dang, R. Clark, Y. Zhang, R. J. Weiss, Y. Jia, Z. Chen, and Y. Wu, Libritts: corpus derived from librispeech for text-to-speech, in Proc. of INTERSPEECH, 2019. [21] G. Chen, S. Chai, G. Wang, J. Du, W.-Q. Zhang, C. Weng, D. Su, D. Povey, J. Trmal, J. Zhang, M. Jin, S. Khudanpur, S. Watanabe, S. Zhao, W. Zou, X. Li, X. Yao, Y. Wang, Y. Wang, Z. You, and Z. Yan, Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio, in Proc. of INTERSPEECH, 2021. [22] W. Kang, X. Yang, Z. Yao, F. Kuang, Y. Yang, L. Guo, L. Lin, and D. Povey, Libriheavy: 50,000 hours asr corpus with punctuation casing and context, in Proc. of IEEE ICASSP, 2024. [23] H. He, Z. Shang, C. Wang, X. Li, Y. Gu, H. Hua, L. Liu, C. Yang, J. Li, P. Shi, Y. Wang, K. Chen, P. Zhang, and Z. Wu, Emilia: An extensive, multilingual, and diverse speech dataset for large-scale speech generation, in Proc. of IEEE SLT, 2024."
        }
    ],
    "affiliations": [
        "Chinese University of Hong Kong, Shenzhen, China",
        "Laboratory of Speech and Intelligent Information Processing, Institute of Acoustics, CAS, Beijing, China",
        "Shanghai AI Laboratory, Shanghai, China",
        "University of Chinese Academy of Sciences, Beijing, China"
    ]
}