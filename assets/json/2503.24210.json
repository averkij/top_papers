{
    "paper_title": "DiET-GS: Diffusion Prior and Event Stream-Assisted Motion Deblurring 3D Gaussian Splatting",
    "authors": [
        "Seungjun Lee",
        "Gim Hee Lee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reconstructing sharp 3D representations from blurry multi-view images are long-standing problem in computer vision. Recent works attempt to enhance high-quality novel view synthesis from the motion blur by leveraging event-based cameras, benefiting from high dynamic range and microsecond temporal resolution. However, they often reach sub-optimal visual quality in either restoring inaccurate color or losing fine-grained details. In this paper, we present DiET-GS, a diffusion prior and event stream-assisted motion deblurring 3DGS. Our framework effectively leverages both blur-free event streams and diffusion prior in a two-stage training strategy. Specifically, we introduce the novel framework to constraint 3DGS with event double integral, achieving both accurate color and well-defined details. Additionally, we propose a simple technique to leverage diffusion prior to further enhance the edge details. Qualitative and quantitative results on both synthetic and real-world data demonstrate that our DiET-GS is capable of producing significantly better quality of novel views compared to the existing baselines. Our project page is https://diet-gs.github.io"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 3 ] . [ 1 0 1 2 4 2 . 3 0 5 2 : r DiET-GS: Diffusion Prior and Event Stream-Assisted Motion Deblurring 3D Gaussian Splatting"
        },
        {
            "title": "Gim Hee Lee",
            "content": "Department of Computer Science, National University of Singapore seungjun.lee@u.nus.edu, gimhee.lee@nus.edu.sg Project page: DiET-GS.github.io Figure 1. Given set of blurry images and corresponding event streams, we propose novel framework to construct deblurring 3DGS by jointly leveraging the EDI [35] formulation and pretrained diffusion model as prior. Our DiET-GS++ enables high quality novel-view synthesis with recovering precise color and well-defined details from the blurry multi-view images."
        },
        {
            "title": "Abstract",
            "content": "Reconstructing sharp 3D representations from blurry multi-view images is long-standing problem in computer vision. Recent works attempt to enhance high-quality novel view synthesis from the motion blur by leveraging eventbased cameras, benefiting from high dynamic range and they often microsecond temporal resolution. However, reach sub-optimal visual quality in either restoring inaccurate color or losing fine-grained details. In this paper, we present DiET-GS, diffusion prior and event streamassisted motion deblurring 3DGS. Our framework effectively leverages blur-free event streams and diffusion prior in two-stage training strategy. Specifically, we introduce the novel framework to constrain 3DGS with event double integral, achieving both accurate color and well-defined details. Additionally, we propose simple technique to leverage diffusion prior to further enhance the edge details. Qualitative and quantitative results on both synthetic and real-world data demonstrate that our DiET-GS is capable of producing better quality of novel views compared to the existing baselines. 1. Introduction Novel view synthesis plays an important role in various vision applications such as scene understanding [19, 29, 60], virtual reality [20, 62], image processing [11, 31, 34], etc. To this end, Neural Radiance Fields (NeRF) [33] achieves notable success in generating high-quality novel views by reconstructing implicit 3D representations with deep neural network. However, it falls short of real-time rendering and efficient training, limiting its application in real-world scenarios. Recently, 3D Gaussian Splatting (3DGS) [17] has emerged as an efficient alternative to NeRF by representing scenes with explicit 3D Gaussian primitives. The explicit representation of 3DGS serves as lightweight replacement of the implicit neural representations used in NeRF, achieving better training and rendering efficiency as well as visual quality of novel views. To enable high-quality 3D reconstructions, both NeRF and 3DGS rely on multi-view images that are perfectly captured and free from any artifact. However, this preliminary condition is often unavailable in the real world. For example, camera needs prolonged exposure time in low light environment to allow enough light to reach the sensor for image formation. The camera has to remain absolutely still during this lengthy exposure time. Any camera motion during the capture leads to undesired motion blur. To circumvent this issue, line of works [21, 31, 36, 56, 69] has attempted to recover sharp 3D representations from blurry multi-view images. Despite the promising potential, it is non-trivial to restore fine-grained details from blurry images alone and thus often leading to sub-optimal visual quality. Several recent works have shown the efficacy of eventbased cameras, significantly improving motion deblurring in images captured from standard frame-based cameras. Event sensors enable blur-free measurements of brightness changes, benefiting from higher dynamic range and temporal resolution compared to standard cameras. Motivated by this distinct competency, several recent works have explored the potential of recovering sharp 3D representations from event streams. Earlier works [1, 13, 44] focus on utilizing solely event-based data, lacking the capacity to preserve color information. E-NeRF [18] combines blurry images into the framework as direct color supervision for 3D NeRF. Nonetheless, the estimated color exhibits blur around the edges since it does not account for the blur formation. E2NeRF and following works [3, 25, 39, 63] explicitly model the blur formation process to further enhance the color and edge details. However, most of these existing works still rely on blurry images alone to recover accurate color, often resulting in unwanted color artifacts. To supplement color guidance, Ev-DeblurNeRF [3] proposes to exploit the explicit relationship of Event Double Integral (EDI) between blurry images and event streams. In this paper, we propose DiET-GS, Diffusion prior and EvenT stream-assisted motion deblurring 3DGS. As illustrated in Fig. 1, our framework comprises two main stages: DiET-GS and DiET-GS++. Our Stage 1: DiET-GS first optimizes the deblurring 3DGS by jointly leveraging the real-captured event streams and the prior knowledge of pretrained diffusion model. To restore both accurate color and well-defined details, we introduce novel framework that uses the EDI prior to achieve 1) fine-grained details, 2) accurate color, and 3) regularization. Specifically, in addition to the EDI color guidance proposed by [3], we propose further constraints to recover fine-grained details by modeling EDI in the brightness domain through learnable camera response function. This learnable approach naturally considers the potential variation between RGB values and pixel intensity, leading to better real-world adaptation and thus effectively recovering intricate details. The EDI constraints from both RGB space [3] and brightness domain enable mutual compensation between color fidelity and finegrained details, resulting in optimal visual quality. Additionally, we derive regularization term from the EDI prior to further facilitate the optimization by ensuring the cycle consistency among the objective terms. To achieve more natural image refinement, we further incorporate diffusion prior in DiET-GS using the Renoised Score Distillation (RSD) proposed by [22]. Nonetheless, we empirically find that jointly leveraging both priors from real-captured data and pretrained diffusion model often weakens the full effect of diffusion prior. Consequently, our Stage 2: DiET-GS++ is further introduced to maximize the effect of diffusion guidance by adding extra learnable parameters to each 3D Gaussian in the pretrained DiETGS. Unlike [22], DiET-GS++ directly renders latent residual from the 3D Gaussians, resulting in simpler framework to leverage RSD optimization. Finally, resulting images from DiET-GS are further refined by solely relying on diffusion prior while edge details are effectively enhanced. Our main contributions are summarized as follows: novel framework to construct deblurring 3DGS by jointly leveraging event streams and the prior knowledge of pretrained diffusion model. two-stage train38ing strategy to effectively utilize realcaptured data and diffusion prior together. Once optimized, our method is capable of recovering well-defined details with accurate color from the input blurry images. Qualitative and quantitative results show that our framework significantly surpasses the existing baselines, achieving the best visual quality. 2. Related Works Event-based image deblurring. Recently, event-based cameras have gained significant popularity due to their high dynamic range and microseconds temporal resolution. Several methods have tried to leverage these distinct features to tackle image deblurring. Most notably, event-based double integral (EDI) [35] explicitly models the relationship between event streams and blurry image jointly captured during the fixed exposure time. Subsequent works follow EDI by fusing the events and RGB frames [9, 50, 51, 61, 67] or using learning-based approaches [14, 52] to further improve visual quality. Another line of works exploits event cameras to recover sharp 3D representations such as NeRF and 3DGS from blurry multi-view images. Earlier works [13, 18, 30, 44] utilize an event generation model [8] to enable sharp novel view synthesis from fast-moving camera while E-NeRF [18] combines RGB frames to further refine the color. Recent works [3, 25, 38, 39, 63] such as E2NeRF [38] and Ev-DeblurNeRF [3] combine blurry image formation [31] to model the camera motion during the exposure time while Ev-DeblurNeRF [3] further incorporates the EDI prior to provide additional color constraint to 3D NeRF. However, this color constraint often yields over-smoothed details by treating each RGB channel as brightness, which deviates from the real-world setting. In this work, we effectively restore intricate details by introducing learnable brightness estimation function to the EDI formulation that better adapts to real-world settings. Diffusion-based image restoration. Diffusion models have been successfully repurposed for image restoration tasks such as super-resolution (SR) [10, 24, 46, 55, 59, 64], deblurring [6, 41, 58] and JPEG restoration [15, 45]. comprehensive summary of diffusion-based restoration methods can be found in this recent survey [26]. DiffIR [59] is two-stage training approach that generates prior representation from diffusion model to restore an image. DvSR [58] surpasses regression-based methods by leveraging conditional diffusion models. More recently, diffusion prior has begun to be used for restoration of 3D representations such as NeRF. DiSR-NeRF [22] first attempts to leverage pretrained diffusion model to construct superresolution (SR) NeRF by proposing renoised variant of score distillation sampling [37], referred to as RSD optimization. In our approach, we propose simpler framework to adopt RSD optimization compared to [22], further enhancing the edge details of the rendered image. 3. Preliminaries Event Camera Model in Motion Deblurring. Event is triggered when the absolute difference of logarithmic brightness between time tj and tj1 exceeds Θpj at the same pixel location, where predefined threshold Θpj R+ controls the sensitivity to brightness change. It follows that: log(I(u, tj)) log(I(u, tj1)) = pj Θpj , (1) where I(u, t) denotes instantaneous intensity at pixel on given time t. Polarity pj {1, 1} specifies that either an increase or decrease in brightness. By denoting log(I(u, t)) as L(t), Eq. 1 can be generalized to any time interval [t, + t] by accumulating event signals as follows: L(t + t) L(t) = Θ E(t) = Θ (cid:90) t+t pδ(τ ) dτ, (2) where δ(τ ) is impulse function with unit integral and polarity pj on threshold Θ is omitted for brevity. By applying the exp() function to both sides of Eq. 2, we can rewrite I(t + t) = I(t) exp(Θ E(t)) to give the relationship between two instantaneous brightness observed at different time steps. We note that blurry image taken from conventional frame-based camera can be represented as averaging sequence of sharp images acquired during the fixed exposure time τ as follows: IB(u, t) = 1 τ (cid:90) t+τ /2 I(u, h) dh, (3) tτ /2 where IB is the blurry image captured during the time interval = [tτ /2, t+τ /2]. By combining Eq. 2 and Eq. 3, the connection between the blurry image IB and the latent sharp image at the same timestep can be constructed as: recovering sharp rendered images with better fine-grained details and colors. Diffusion Prior in 3D. Score-based diffusion models [49, 53] are popular type of generative model that learns score function that represents the gradient of the log probability density with respect to the data. Several works on Text-to-3D generation [5, 12, 27, 28, 32, 37, 53, 57, 70] that leverage the score function of pretrained diffusion model as diffusion prior have shown remarkable results. Notably, Score Distillation Sampling (SDS) [37] uses score function of diffusion model as an optimization target, providing the gradient from the score function of pretrained diffusion model to guide differentiable image parametrization = g(θ) without retraining the diffusion model from In 3D, θ can be any learnable 3D representascratch. tion [17, 33] with volume rendering function g(). Recently, variant of SDS has been introduced for image restoration task. Specifically, [22] proposes renoised variant of SDS called Renoised Score Distillation (RSD) to use diffusion prior for super-resolution in 3D neural representation field. Given data sample z0, the forward process obtains the noisy latent zt by adding Gaussian noise sample ϵ 1 αtϵ, where (0, I) at timestep t: zt = αt is timestep-dependent noising coefficient. In the DDPM reverse process, diffusion U-net is trained to predict the noise ϵ(z, y, t) to denoise zt into zt1 as follows: zt1 = 1 ϵ(zt, y, t)) + σtϵ, where is the conditioning input and σt is the standard deviation of Gaussian noise samples. Given the predicted denoised latent ˆzt1 from zt and the current noised latent zt1 at timestep 1, the objective of RSD is formulated as Lrsd = zt1 ˆzt1. In this paper, we propose simpler framework to leverage RSD optimization compared to [22]. (zt 1αt 1 αt αtz0 + αt 4. Our Method Fig. 2 shows an illustration of our DiET-GS framework which consists of two stages. Stage 1 (cf . Sec. 4.1) constructs deblurring 3DGS by leveraging an EDI constraint derived from real-captured data and the prior knowledge of pretrained diffusion model. Stage 2 (cf . Sec. 4.2) further refines the resulting images from Stage 1 by solely relying on diffusion prior to further enhance the edge details. 4.1. Stage 1: DiET-GS IB(u, t) = I(u, t) τ (cid:90) t+τ /2 tτ /2 exp(ΘE(h)) dh. (4) This relationship between the sharp image, blurry image, and event stream is known as Event-based Double Integral (EDI) [35]. Finally, we can remove motion blur from IB by solving for I(u, t) in Eq. 4 under the guidance of the event streams. In this paper, we propose novel framework to leverage the EDI prior to constrain 3D Gaussian Splatting in The goal of Stage 1 is to construct set of 3D Gaussians from 3D Gaussian Splatting (3DGS) [17] to render sharp images from blurry images and event streams. We name the 3D Gaussians trained in this stage as DiET-GS. Initialization. The construction of 3DGS requires point cloud initialization and camera calibrations with structurefrom-motion (SfM) [47], which often fail with blurry images. We mitigate this issue by leveraging EDI from Eq. 4 Figure 2. Overall framework of our DiET-GS. Stage 1 (DiET-GS) optimizes the deblurring 3DGS with the event streams and diffusion prior. To preserve accurate color and clean details, we exploit the EDI prior in multiple ways, including color supervision C, guidance for fine-grained details I, and additional regularization with EDI simulation. Stage 2 (DiET-GS++) is then employed to maximize the effect of diffusion prior by introducing extra learnable parameters fg. DiET-GS++ further refines the rendered images from DiET-GS, effectively enhancing rich edge features. More details are explained in Sec. 4.1 and Sec. 4.2. to recover an initial set of sharp images suitable for SfM. Specifically, given an RGB blurry image CB and an event stream captured during an exposure time τ , we reconstruct sharp latent image from the grayscale of CB with EDI. This sharp image can then be warped to any timestep within the exposure period [t τ /2, + τ /2] following I(t + t) = I(t) exp(Θ E(t)) as shown in Eq. 2. We obtain set of latent images for each training view by warping the recovered latent image to each of the timesteps uniformly sampled from the exposure time. The recovered sharp latent images are subsequently fed into SfM for the estimation of the camera poses and point cloud. Blur Reconstruction Loss. Let us denote the estimated camera poses along the approximated camera trajectory of as Pi = {pij}n1 blurry image CB j=0 . Given the camera poses Pi, we simulate the blurry image formation by discretizing Eq. 3 into: ˆCB = 1 n1 (cid:88) j=0 gθ(pij), (5) where gθ() is the 3DGS with rendering function and ˆCB is the estimated motion-blurred image. We can now use the real captured blurry images {CB}k1 i=0 to supervise the simulated blurry images { ˆCB}k1 i=0 , where is the number of training views. Following the original 3DGS, we thus formulate blur reconstruction loss to minimize the photometric error LP as Lblur = Lp(CB, ˆCB) = (1 λ1)L1 + λ1LDSSIM. Event Reconstruction Loss. Since an event stream provides blur-free microsecond-level measurements, it can be exploited to further aid fine-grained deblurring. To this end, we formulate an event reconstruction loss by leveraging the relation between brightness and generated events in Eq. 2. Specifically, we synthesize the left-hand side of Eq. 2 by defining the simulated log brightness ˆL(ti) at time ti as: ˆL(ti) = log(h(CRF( ˆCi))) (6) where ˆCi = gθ(pi) is the sharp image rendered from the 3DGS at camera pose pi corresponding to time ti, and h() is luma conversion function implemented by following the BT.601 [2] standard. Following [3], learnable tone mapping function CRF() is adopted to handle possible variations between the RGB and events response function. To simulate the brightness change in Eq. 2, we randomly sample two timesteps tα and tβ = tα + along the camera trajectory and approximate the camera poses corresponding to the sampled timesteps via spherical linear interpolation [48] of the known camera poses {Pi}k1 i=0 . Given the approximated camera poses at tα and tβ, we can finally synthesize the brightness change ˆL(tα, tβ) = ˆL(tβ) ˆL(tα) between time tα and tβ by estimating the log brightness from Eq. 6. Considering that the right-hand side of Eq. 2 can serve as ground-truth supervision, we thus formulate the event construction loss Lev as follows: Lev = ˆL(tα, tβ) L(tα, tβ)2 2, where L(tα, tβ) is the brightness difference observed from the event camera. Event Double Integration (EDI) Loss. Although Lev is capable of producing sharp details to certain degree, it lacks supervision in areas where events are not triggered. Furthermore, it is not trivial to blindly recover color details from an event response since the only color supervision comes from Lblur [3]. To this end, we propose novel optimization problem that leverages EDI prior to further constrain the 3DGS in terms of 1) fine-grained details, 2) precise color and 3) regularizing the optimization. Since EDI is defined in the monochrome brightness domain, we first model the EDI based on pixel intensity values. Specifically, we further exploit the composite function of the learnable camera response function CRF() folFigure 3. Cycle consistency among the objective terms. Ledi simul follows the formulation of Ledi gray except for substituting CB to simulated blurry image ˆCB derived from Lblur. It completes the cycle among the objective terms, further regularizing the fine-grained deblurring as shown in Fig. 6. lowed by h() in Eq. 6 to estimate the brightness of given color images. Given the ground truth blurry image CB, the brightness of CB, denoted as IB, is obtained by IB = h(CRF(CB)). We then recover the mid-exposure pose of image from IB using Eq. 4. Based on the latent image I, sharp latent image Ii at randomly sampled timestep ti can be recovered by warping to timestep ti as stated in the initialization step. Given Ii as image-level supervision, we synthesize the brightness of color image ˆCi rendered at the same timestep. Finally, the EDI loss in the monochrome pixel domain is formulated with the photometric error as follows: Ledi gray := Lp(Ii, ˆIi) = Lp(Ii, h(CRF( ˆCi))), where ˆIi is the estimated brightness of rendered color ˆCi. The learnable bright response function CRF() allows Ledi gray to effectively restore fine-grained details for the overall image by naturally filling the gap between the rendered color space and the brightness change captured by the event sensor. Although effective in restoring fine-grained details, we empirically find that CRF() often distorts color due to the lack of sufficient color supervision as shown in Fig. 5. Inspired by [3], we produce sharp RGB color by treating each RGB channel of CB as blurry brightness IB in Eq. 4 and applying channel-wise deblurring with EDI. Finally, sharp color Ci at randomly sampled timestep ti is obtained by warping to the corresponding timestep. Given Ci as color supervision, the EDI loss in the RGB space can be formulated as Ledi color = Lp(Ci, ˆCi), where ˆCi = gθ(pi) is the color image rendered at the same timestep as Ci. In addition to the EDI loss described above for imagelevel supervision, we further leverage EDI for additional regularization. Specifically, we synthesize both sides of Eq. 4 by simply replacing IB with the simulated blurry brightness ˆIB = h(CRF( ˆCB)), which is the estimated brightness of ˆCB. The EDI simulation loss can then be formulated as Ledi simul = Lp( Ii, ˆIi), where Ii is the sharp supervision obtained from the simulated blurry brightness via EDI processing. As illustrated in Fig. 3, Ledi simul ensures cycle consistency among the objective terms, further facilitating the optimization as regularization term. Finally, the EDI loss is given by combining all of the EDI-based objectives as: Ledi = Ledi gray + Ledi color + Ledi simul. Leveraging Diffusion Prior. Although event streams can provide blur-free details, they are susceptible to unnatural artifacts (Fig. 4b-4d) due to the unknown threshold Θ in Eq. 2-4 and noise accumulated from the event [30]. Since pretrained diffusion model [42] has already learned the distribution of natural images from large amounts of diverse datasets, it is intuitive to leverage this data-driven prior in addition to our model-based losses (Lblur, Lev and Ledi) to further refine the output image more natural. Specifically, we adopt the RSD optimization strategy proposed in [22] to our framework. However, unlike [22], our setting lacks the clean images which are necessary to guide noise prediction of diffusion model as conditional input. straightforward solution is to utilize the sharp latent image from EDI processing as surrogate for the clean image. Unfortunately, we empirically find that the EDI-processed images contain lot of artifacts that are detrimental to the noise inference of the UNet in the diffusion model. We circumvent this issue by using the ground truth blurry images as an alternative. We first render blurry image ˆCB from Eq. 3, and encode it to latent z0 = E( ˆCB) via pretrained VAE encoder E. Subsequently, we apply the forward process of the diffusion model by introducing noise at timesteps and 1 based on predetermined noising schedule to get two noised latents zt and zt1. The UNet backbone [43] of the pretrained diffusion model takes zt as input and the ground truth blurry image CB as condition to predict the noise residual ϵ(zt, y, t). Given the predicted noise and zt, we then obtain the predicted denoised latent ˆzt1 via the DDPM reverse process. Finally, the RSD loss Lrsd is formulated as an L1 error between zt1 and ˆzt1. Since the input of the diffusion model ˆCB is obtained by averaging set of rendered sharp images { ˆC}n1 i=0 along the camera trajectory, the supervision from the RSD loss is transferred to each rendered image, making them more natural. Training objective. The final objective Ls1 in Stage 1 is formulated as: Ls1 = λblurLblur + λevLev + λediLedi + λrsdLrsd, (7) with the weight λ for each objective term. 4.2. Stage 2: DiET-GS++ Although the RSD optimization in Stage 1 allows DiETGS to produce more natural and precise color as shown in the 2nd row of Fig. 6, we empirically find that the performance improvement gained falls short of our expectation (cf . Tab. 2). We postulate that this is due to the joint optimization of the event-based loss which are Lev and Ledi, and the RSD loss. Event-based supervision is derived from real-captured event streams, which tend to optimize 3DGS for specific training scene. In contrast, the RSD loss regularizes rendered images according to the distribution of diverse natural images modeled by pretrained diffusion Dataset Metric EvDeblur -blender EvDeblur -CDAVIS PSNR SSIM LPIPS MUSIQ CLIP-IQA PSNR SSIM LPIPS MUSIQ CLIP-IQA MPRNet+GS [65] 18.76 0.5912 0.3545 24.12 0.2413 27.51 0.7514 0.2013 25.12 0.2134 EDI+GS [35] 23.69 0.7694 0.1375 55.13 0.2751 32.95 0.8922 0.0790 40.06 0.2008 EFNet+GS [50] 21.03 0.6413 0.3214 35.13 0.2314 30.97 0.8503 0.1142 38.23 0.1934 BAD-NeRF [56] 19.78 0.6381 0.2490 23.63 0.1888 28.47 0.7981 0.2526 19.96 0.1791 BAD-GS [69] 22.23 0.7213 0.2012 32.43 0.1993 29.12 0.8129 0.2012 22.12 0. E2NeRF [38] 24.54 0.7993 0.1624 47.31 0.2129 31.54 0.8687 0.1059 38.82 0.2235 Ev-DeblurNeRF [3] 24.76 0.8038 0.1788 42.38 0.2300 32.30 0.8827 0.0571 41.32 0.2211 DiET-GS (Ours) 26.69 0.8607 0.1064 57.67 0.2769 34.22 0.9223 0.0496 45.80 0.2072 DiET-GS++ (Ours) 26.23 0.8478 0.1052 59.91 0.2960 33.16 0.9039 0.0502 50.44 0.2415 Table 1. Quantitative comparisons on both synthetic and real-world dataset. The results are the average of every scenes within the dataset. The best results are in bold while the second best results are underscored. model. Jointly optimizing these two constraints reaches an equilibrium between scene-specific details guided by the event-based loss and the prior knowledge of the pretrained diffusion model. Consequently, it is likely to weaken the full effectiveness of Lrsd. To this end, we incorporate an additional training step to fully leverage the diffusion prior by introducing extra learnable parameters to DiET-GS. We name the final model trained on Stage 2 as DiET-GS++. Specifically, we attach zero-initialized Gaussian feature fg RD for each 3D Gaussian in the pretrained DiET-GS, where is the feature dimension. Given camera pose p, we use DiET-GS to render deblurred image ˆC and 2D feature map f2D by accumulating color and fg, respectively. After encoding ˆC to the latent z0 = E( ˆC), we subsequently obtain refined latent 0 = z0 + f2D by combining z0 and f2D. Gaussian noise samples at timesteps and 1 are then introduced to 0 to get noised latents t1. Finally, given ˆC as conditional input, the UNet and backbone of pretrained diffusion model predicts the noise residual of to derive the denoised latent ˆz t1. The RSD loss between t1 is given as the optimization objective of Stage 2. We note that only the Gaussian features fg are trained during Stage 2, while the other parameters of DiET-GS remain fixed to preserve the prior of event streams derived in Stage 1. After optimization, our model can render latent residual f2D which contains rich edge details directly guided by the pretrained diffusion model. In the inference, the final sharp image is obtained by decoding the re0) = D(f2D + E( ˆC)), 0. Specifically, = D(z fined latent where is pretrained VAE decoder (cf . Fig. 1). t1 and ˆz Discussion. Although Stage 2 of our framework uses RSD loss, we differ from [22] as follows: 1) We exploit the rendering capability of 3DGS to obtain the learnable latent residual f2D. In contrast, [22] manually creates f2D for all training poses and thus requires further synchronization of NeRF with the trained latent features after the RSD optimization. This synchronization stage is not necessary in our design since DiET-GS++ can directly render the learned latent residual from the Gaussian features fg, thus resulting in simpler framework. Furthermore, our Stage 2 only takes 20 minutes of training time. 2) Our setting is more challenging than [22] due to the lack of ground truth clean images to condition the diffusion UNet. In this regard, we utilize the image rendered from DiET-GS as conditional input on the diffusion model to further enhance the edges. 5. Experiments 5.1. Implementation Details We build our framework based on the official code of 3DGS [17] and DiSR-NeRF [22]. Throughout Stage 1, we set the loss weights λblur = λedi = λrsd = 1.0 and λev = 0.1, and execute 100,000 iterations of training. During Stage 2, training spans 2,000 iterations with linearly decreasing time schedule of ancestral sampling. The number of poses estimated for each blurry image in the initialization of 3DGS is set to 9. All experiments are conducted using single NVIDIA RTX 6000 GPU. 5.2. Datasets. We evaluate our DiET-GS and DiET-GS++ on both synthetic and real-world datasets proposed by [3]. EvDeblurBlender Dataset is synthetic dataset that consists of four synthetic scenes derived from DeblurNeRF [31]: factory, pool, tanabata and trolley. Blurry images are provided along with the corresponding synthetic event streams simulated by [40]. Motion blur is produced during 40ms exposure time with single fast continuous motion by averaging set of images rendered at 1000 FPS in linear RGB space. We set Θ = 0.2 for the event threshold during training in synthetic scenes. The EvDeblur-CDAVIS Dataset contains five real-world scenes, each with 11 to 18 blurry training images paired with corresponding event streams. Color-DAVIS346 [23] is employed to capture both color events and standard frames at 346 260 pixel resolution using an RGBG Bayer pattern. 1000ms exposure time is given to produce motion blur. The event threshold is set to Θ = 0.25 for both negative and positive events during training in real-world scenes. Both synthetic and real-world datasets include five ground-truth (GT) sharp images captured from both seen and unseen viewpoints for each scene. (a) Blur Image (b) EDI+GS [35] (c) E2NeRF [38] (d) Ev-DeblurNeRF [3] (e) DiET-GS (Ours) (f) DiET-GS++ (Ours) (g) GT Figure 4. Qualitative comparisons on both synthetic (1st-2nd rows) and real-world (3rd-4th rows) datasets. DiET-GS shows cleaner texture with more accurate details compared to the event-based baselines while DiET-GS++ further enhances these features with sharper definition, achieving the best visual quality. 5.3. Experiment Settings Baseline. Our baselines are divided into three categories. The first category is the naive combination of image deblurring methods and 3DGS, where images are initially deblurred and subsequently used as training views for 3DGS. Specifically, an image deblurring method MPRNet [65], and event-based deblurring methods EDI [35] and EFNet [50] are adopted as baselines. The second category is the frame-only deblurring rendering methods, where only the RGB frames are utilized during training. We select BAD-NeRF [56] and BAD-GS [69] for this category. The third category is event-based deblurring rendering methods, where the RGB frames and event streams are jointly leveraged. We select E2NeRF [38] and Ev-DeblurNeRF [3] as the most recent works in this category with publicly available code. To fairly compare with our methods, we utilize camera poses estimated from COLMAP for all baselines instead of using GT poses provided by the dataset. Evaluation Metrics. We employ three standard metrics: Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM), and VGG-based Learned Perceptual Image Patch Similarity (LPIPS) [68] to evaluate the quality of the predicted views on the target views. Since our framework produces rich edge details under the guidance of generative model, different predicted views can be valid for the same blurry image. We thus follow [64] to additionally adopt MUSIQ [16] and CLIP-IQA [54] as NoReference Image Quality Assessment (NR-IQA) metrics to further evaluate the effectiveness of our framework. (a) Ledi gray (b) Ledi color (c) Both (d) GT Figure 5. Ablation study on Ledi gray and Ledi color 5.4. Quantitative Comparisons We show the quantitative results in Tab. 1. Our DiETGS largely outperforms all baselines in PSNR, SSIM, and LPIPS on both synthetic and real-world datasets, showing the effectiveness of our framework to leverage EDI prior. Furthermore, our DiET-GS++ shows significant improvement in MUSIQ and CLIP-IQA metrics, achieving the best results but showing slight drop in PSNR and SSIM metrics. As already discussed in [22, 64], since DiET-GS++ is solely guided by pretrained generative model, the resulting images may contain more variation with respect to GT samples compared to DiET-GS which is supervised by realcaptured data. Nonetheless, DiET-GS++ still substantially improves the visual quality as shown in NR-IQA metrics. Qualitative comparisons in Sec. 5.5 further validate the effectiveness of our DiET-GS++. We also present the user study in the Supplementary material, thoroughly examining the efficacy of leveraging diffusion prior in Stage 2. Lev Ledi gray Ledi color Ledi simul Lrsd (S1) Lrsd (S2) Lblur PSNR 29.73 32.74 33.91 34.48 34.92 35.04 34.89 SSIM 0.7797 0.8460 0.8761 0.8915 0.9033 0.9068 0. LPIPS MUSIQ 0.2160 0.1173 0.0752 0.0826 0.0624 0.0587 0.0600 24.77 39.69 44.25 40.79 43.79 45.04 45.37 CLIP-IQA 0.2031 0.2776 0.2684 0.2450 0.2468 0.2490 0.2584 33.86 0. 0.0634 51.71 0.2955 Table 2. Ablation study on DiET-GS and DiET-GS++ 5.5. Qualitative Comparisons Qualitative comparisons on both Ev-DeblurBlender and EvDeblurCDAVIS datasets are shown in Fig. 4, making the 1) Relying solely on the EDIfollowing observations. preprocessed images to optimize the 3DGS yields unsatisfactory results, introducing numerous artifacts as shown in the 2nd column. While effective at recovering sharp edges, EDI often produces inaccurate details with relatively low visual quality, hindering effective optimization of the 3DGS. This highlights the need for additional objective terms proposed in our framework to further constrain the 3DGS. 2) We find that our DiET-GS is capable of restoring cleaner textures and clearer edges compared to other baselines. For example, in the 3rd row, other baselines wrongly produce grid patterns on the wall stretched from the left object, while our DiET-GS restores more accurate texture on the same part. Similarly, in the 4th row, color jittering from E2NeRF and artifacts from EDI+GS and EvDeblurNeRF are observed, while DiET-GS exhibits cleaner details. The same observation is made for the text elements shown in the 1st and 2nd rows. 3) Our DiET-GS++ further refines the textures and edge details generated from DiETGS, demonstrating the efficacy of fully leveraging diffusion prior in Stage 2. For instance, grid patterns on the wall in the 3rd row are further mitigated in DiET-GS++, while sharper edges are also observed in both text elements (1st-2nd rows) and non-text elements (4th row) compared to DiET-GS. Overall, our final model DiET-GS++ shows marked improvement over the other baselines, consistently achieving more precise textures and well-defined details. 5.6. Ablations In this section, we present various ablation studies to validate the contributions of each component proposed in our DiET-GS++. All the quantitative evaluations are conducted on real-world scene, namely, Figures sample. Event-based Supervision. As shown in the 1st-2nd rows of Tab. 2, simulating brightness change in Lev improves PSNR by +3.01dB. Investigation of the EDI prior is conducted on the 3rd-6th rows of Tab. 2 along with qualitative results on Fig. 5 and the 1st row of Fig. 6. As shown in the 1st row of Fig. 5, using only Ledi gray yields sharper details, e.g. tiling on the background, and improves LPIPS (a) wo Ledi simul (b) Ledi simul (c) GT (d) wo Lrsd (S1) (e) Lrsd (S1) (f) GT Figure 6. Ablation on Ledi simul (1st row) and Lrsd (S1) (2nd row). scores compared to relying solely on Ledi color. However, color artifacts are also introduced due to the lack of sufIn contrast, Ledi color generates ficient color supervision. accurate color with higher PSNR and SSIM scores, though some details are over-smoothed. Leveraging both Ledi gray and Ledi color yields the best performance on all the PSNR, SSIM, and LPIPS metrics among the 3rd to 5th rows of Tab. 2, achieving both precise color and clear details. The 2nd row of Fig. 5 further shows the synergistic effect between the two constraints, where more well-defined details are produced when they are jointly utilized. Similarly, adding EDI simulation Ledi simul further aids fine-grained deblurring as shown in the red circles of Fig. 6, showing +0.12dB increase in PSNR. Diffusion prior. The following observations are made on the inclusion of diffusion prior. 1) The RSD optimization term in Stage 1 denoted as Lrsd (S1) shows slight improvement in MUSIQ and CLIP-IQA by +0.33 and +0.0094, respectively. 2) Lrsd (S1) can also serve as additional color guidance since the real-captured blurry image with groundtruth color information is given to the diffusion model as conditional input during RSD optimization, further constraining the color of DiET-GS. As shown in the 2nd row of Fig. 6, our DiET-GS++ guided by Lrsd (S1) (6e) generates better color compared to the version that omits RSD optimization in Stage 1 (6d). 4) Lrsd (S1) yields marginal performance improvement in NR-IQA metrics compared to the Lrsd (S2). As discussed in Sec. 4.2, joint optimization of event-based objectives and RSD loss tends to weaken the full effect of diffusion prior. However, considering the first and second findings, we still decide to exploit diffusion prior in Stage 1 and further adopt Stage 2 with additional parameters to fully leverage the guidance from the pretrained diffusion model. 6. Conclusion. We present DiET-GS, novel framework to jointly use event streams and pretrained score function. Our twostage strategy allows 3DGS to recover clean and sharp images from motion blur. Our novel EDI constraints achieve both accurate color and fine-grained details, and the diffusion prior effectively enhances edge details with simpler architecture. We believe our deblurring approach DiET-GS have potentials for significant practical applications especially in low-light environment or fast-moving camera. Acknowledgement. This research / project is supported by the National Research Foundation (NRF) Singapore, under its NRF-Investigatorship Programme (Award ID. NRFNRFI09-0008)."
        },
        {
            "title": "References",
            "content": "[1] Anish Bhattacharya, Ratnesh Madaan, Fernando Cladera, Sai Vemprala, Rogerio Bonatti, Kostas Daniilidis, Ashish Kapoor, Vijay Kumar, Nikolai Matni, and Jayesh Gupta. Evdnerf: Reconstructing event data with dynamic neural radiance fields. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 5846 5855, 2024. 2 [2] RIR BT et al. Studio encoding parameters of digital television for standard 4: 3 and wide-screen 16: 9 aspect ratios. International Radio Consultative Committee International Telecommunication Union, Switzerland, CCIR Rep, 2011. 4 [3] Marco Cannici and Davide Scaramuzza. Mitigating motion blur in neural radiance fields with events and frames. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 92869296, 2024. 2, 4, 5, 6, 7, 13, 14, 15, 16, 18, 19 [4] Liangyu Chen, Xiaojie Chu, Xiangyu Zhang, and Jian Sun. Simple baselines for image restoration. In European conference on computer vision, pages 1733. Springer, 2022. 14, 17 [5] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for highIn Proceedings of the quality text-to-3d content creation. IEEE/CVF international conference on computer vision, pages 2224622256, 2023. 3 [6] Zheng Chen, Yulun Zhang, Ding Liu, Jinjin Gu, Linghe Kong, Xin Yuan, et al. Hierarchical integration diffusion model for realistic image deblurring. Advances in neural information processing systems, 36, 2024. 2 [7] Jooyoung Choi, Jungbeom Lee, Chaehun Shin, Sungwon Kim, Hyunwoo Kim, and Sungroh Yoon. Perception priIn Proceedings of oritized training of diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1147211481, 2022. [8] Guillermo Gallego, Tobi Delbruck, Garrick Orchard, Chiara Bartolozzi, Brian Taba, Andrea Censi, Stefan Leutenegger, Andrew Davison, Jorg Conradt, Kostas Daniilidis, et al. Event-based vision: survey. IEEE transactions on pattern analysis and machine intelligence, 44(1):154180, 2020. 2 [9] Chen Haoyu, Teng Minggui, Shi Boxin, Wang YIzhou, and Huang Tiejun. Learning to deblur and generate high arXiv preprint frame rate video with an event camera. arXiv:2003.00847, 2020. 2 [10] Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research, 23(47):133, 2022. 2 [11] Xin Huang, Qi Zhang, Ying Feng, Hongdong Li, Xuan Wang, and Qing Wang. Hdr-nerf: High dynamic range neuIn Proceedings of the IEEE/CVF Conral radiance fields. ference on Computer Vision and Pattern Recognition, pages 1839818408, 2022. 1 [12] Yukun Huang, Jianan Wang, Yukai Shi, Xianbiao Qi, ZhengJun Zha, and Lei Zhang. Dreamtime: An improved optimization strategy for text-to-3d content creation. arXiv preprint arXiv:2306.12422, 2023. 3 [13] Inwoo Hwang, Junho Kim, and Young Min Kim. Ev-nerf: In Proceedings of the Event based neural radiance field. IEEE/CVF Winter Conference on Applications of Computer Vision, pages 837847, 2023. [14] Zhe Jiang, Yu Zhang, Dongqing Zou, Jimmy Ren, Jiancheng Lv, and Yebin Liu. Learning event-based motion deblurring. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 33203329, 2020. 2 [15] Bahjat Kawar, Jiaming Song, Stefano Ermon, and Michael Elad. Jpeg artifact correction using denoising diffusion restoration models. arXiv preprint arXiv:2209.11888, 2022. 2 [16] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 51485157, 2021. 7, 14 [17] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 1, 3, 6 [18] Simon Klenk, Lukas Koestler, Davide Scaramuzza, and Daniel Cremers. E-nerf: Neural radiance fields from moving event camera. IEEE Robotics and Automation Letters, 8 (3):15871594, 2023. 2 [19] Abhijit Kundu, Kyle Genova, Xiaoqi Yin, Alireza Fathi, Caroline Pantofaru, Leonidas Guibas, Andrea Tagliasacchi, Frank Dellaert, and Thomas Funkhouser. Panoptic neural fields: semantic object-aware neural scene representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1287112881, 2022. [20] Joseph LaViola Jr. Bringing vr and spatial 3d interaction to the masses through video games. IEEE Computer Graphics and Applications, 28(5):1015, 2008. 1 [21] Dogyoon Lee, Minhyeok Lee, Chajin Shin, and Sangyoun Lee. Dp-nerf: Deblurred neural radiance field with physical scene priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12386 12396, 2023. 1 [22] Jie Long Lee, Chen Li, and Gim Hee Lee. Disr-nerf: Diffusion-guided view-consistent super-resolution nerf. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2056120570, 2024. 2, 3, 5, 6, 7, 13 [23] Chenghan Li, Christian Brandli, Raphael Berner, Hongjie Liu, Minhao Yang, Shih-Chii Liu, and Tobi Delbruck. Design of an rgbw color vga rolling and global shutter dynamic and active-pixel vision sensor. In 2015 IEEE International Symposium on Circuits and Systems (ISCAS), pages 718 721. IEEE, 2015. 6, 13 [24] Haoying Li, Yifan Yang, Meng Chang, Shiqi Chen, Huajun Feng, Zhihai Xu, Qi Li, and Yueting Chen. Srdiff: Single image super-resolution with diffusion probabilistic models. Neurocomputing, 479:4759, 2022. [25] Wenpu Li, Pian Wan, Peng Wang, Jinghang Li, Yi Zhou, and Peidong Liu. Benerf: neural radiance fields from single blurry image and event stream. In European Conference on Computer Vision, pages 416434. Springer, 2025. 2, 14, 17 [26] Xin Li, Yulin Ren, Xin Jin, Cuiling Lan, Xingrui Wang, Wenjun Zeng, Xinchao Wang, and Zhibo Chen. Diffusion models for image restoration and enhancementa comprehensive survey. arXiv preprint arXiv:2308.09388, 2023. 2 [27] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 300309, 2023. 3 [28] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-toIn Proceedings of 3: Zero-shot one image to 3d object. the IEEE/CVF international conference on computer vision, pages 92989309, 2023. 3 [29] Zhizheng Liu, Francesco Milano, Jonas Frey, Roland Siegwart, Hermann Blum, and Cesar Cadena. Unsupervised continual semantic adaptation through neural rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 30313040, 2023. 1 [30] Weng Fei Low and Gim Hee Lee. Robust e-nerf: Nerf from sparse & noisy events under non-uniform motion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1833518346, 2023. 2, 5, 15 [31] Li Ma, Xiaoyu Li, Jing Liao, Qi Zhang, Xuan Wang, Jue Wang, and Pedro Sander. Deblur-nerf: Neural radiance fields from blurry images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1286112870, 2022. 1, 2, 6 [32] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-nerf for shape-guided generation of 3d shapes and textures. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1266312673, 2023. [33] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 1, 3 [34] Ben Mildenhall, Peter Hedman, Ricardo Martin-Brualla, Pratul Srinivasan, and Jonathan Barron. Nerf in the dark: High dynamic range view synthesis from noisy raw images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1619016199, 2022. 1 [35] Liyuan Pan, Cedric Scheerlinck, Xin Yu, Richard Hartley, Miaomiao Liu, and Yuchao Dai. Bringing blurry frame alive at high frame-rate with an event camera. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 68206829, 2019. 1, 2, 3, 6, 7, 14, 16, 17, 18, 19 [36] Cheng Peng and Rama Chellappa. Pdrf: Progressively deblurring radiance field for fast and robust scene reconstruction from blurry images. arXiv preprint arXiv:2208.08049, 2022. 1 [37] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. [38] Yunshan Qi, Lin Zhu, Yu Zhang, and Jia Li. E2nerf: Event In enhanced neural radiance fields from blurry images. Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1325413264, 2023. 2, 6, 7, 13, 14, 15, 16, 18, 19 [39] Yunshan Qi, Lin Zhu, Yifan Zhao, Nan Bao, and Jia Li. Deblurring neural radiance fields with event-driven bundle adjustment. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 92629270, 2024. 2 [40] Henri Rebecq, Daniel Gehrig, and Davide Scaramuzza. In Conference on Esim: an open event camera simulator. robot learning, pages 969982. PMLR, 2018. 6 [41] Mengwei Ren, Mauricio Delbracio, Hossein Talebi, Guido Gerig, and Peyman Milanfar. Multiscale structure guided the diffusion for image deblurring. IEEE/CVF International Conference on Computer Vision, pages 1072110733, 2023. 2 In Proceedings of [42] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 5, [43] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. 5 [44] Viktor Rudnev, Mohamed Elgharib, Christian Theobalt, and Vladislav Golyanik. Eventnerf: Neural radiance fields from single colour event camera. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 49925002, 2023. 2 [45] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad In Norouzi. Palette: Image-to-image diffusion models. ACM SIGGRAPH 2022 conference proceedings, pages 110, 2022. 2 [46] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David Fleet, and Mohammad Norouzi. Image superIEEE transactions on resolution via iterative refinement. pattern analysis and machine intelligence, 45(4):47134726, 2022. [47] Johannes Schonberger and Jan-Michael Frahm. StructureIn Proceedings of the IEEE confrom-motion revisited. ference on computer vision and pattern recognition, pages 41044113, 2016. 3 [48] Ken Shoemake. Animating rotation with quaternion curves. In Proceedings of the 12th annual conference on Computer graphics and interactive techniques, pages 245254, 1985. 4 [49] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 3 [50] Lei Sun, Christos Sakaridis, Jingyun Liang, Qi Jiang, Kailun Yang, Peng Sun, Yaozu Ye, Kaiwei Wang, and Luc Van Gool. Event-based fusion for motion deblurring with crossmodal attention. In European conference on computer vision, pages 412428. Springer, 2022. 2, 6, 7, 14, 16 [51] Lei Sun, Christos Sakaridis, Jingyun Liang, Peng Sun, Jiezhang Cao, Kai Zhang, Qi Jiang, Kaiwei Wang, and Luc Van Gool. Event-based frame interpolation with ad-hoc deIn Proceedings of the IEEE/CVF Conference on blurring. Computer Vision and Pattern Recognition, pages 18043 18052, 2023. 2 [52] Bishan Wang, Jingwei He, Lei Yu, Gui-Song Xia, and Wen Yang. Event enhanced high-quality image recovery. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XIII 16, pages 155171. Springer, 2020. 2 [53] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond Yeh, and Greg Shakhnarovich. Score jacobian chaining: Lifting In Propretrained 2d diffusion models for 3d generation. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1261912629, 2023. 3 [54] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Exploring clip for assessing the look and feel of images. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 25552563, 2023. 7, [55] Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin C.K. Chan, and Chen Change Loy. Exploiting diffusion prior for real-world image super-resolution. 2024. 2, 13 [56] Peng Wang, Lingzhe Zhao, Ruijie Ma, and Peidong Liu. Bad-nerf: Bundle adjusted deblur neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 41704179, 2023. 1, 6, 7, 16 [57] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in Neural Information Processing Systems, 36, 2024. 3 [58] Jay Whang, Mauricio Delbracio, Hossein Talebi, Chitwan Saharia, Alexandros Dimakis, and Peyman Milanfar. DeIn Proceedings of the blurring via stochastic refinement. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1629316303, 2022. 2, 3 [59] Bin Xia, Yulun Zhang, Shiyin Wang, Yitong Wang, Xinglong Wu, Yapeng Tian, Wenming Yang, and Luc Van Gool. Diffir: Efficient diffusion model for image restoration. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1309513105, 2023. 2 [60] Christopher Xie, Keunhong Park, Ricardo Martin-Brualla, and Matthew Brown. Fig-nerf: Figure-ground neural radiIn 2021 Inance fields for 3d object category modelling. ternational Conference on 3D Vision (3DV), pages 962971. IEEE, 2021. [61] Fang Xu, Lei Yu, Bishan Wang, Wen Yang, Gui-Song Xia, Xu Jia, Zhendong Qiao, and Jianzhuang Liu. Motion deblurring with real events. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 25832592, 2021. 2 [62] Linning Xu, Vasu Agrawal, William Laney, Tony Garcia, Aayush Bansal, Changil Kim, Samuel Rota Bul`o, Lorenzo Porzi, Peter Kontschieder, Aljaˇz Boˇziˇc, et al. Vr-nerf: HighIn SIGGRAPH Asia fidelity virtualized walkable spaces. 2023 Conference Papers, pages 112, 2023. 1 [63] Wangbo Yu, Chaoran Feng, Jiye Tang, Xu Jia, Li Yuan, and Yonghong Tian. Evagaussians: Event stream assisted arXiv preprint gaussian splatting from blurry images. arXiv:2405.20224, 2024. 2 [64] Zongsheng Yue, Jianyi Wang, and Chen Change Loy. Resshift: image superresolution by residual shifting. Advances in Neural Information Processing Systems, 36, 2024. 2, 7 Efficient diffusion model for [65] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling In ProShao. Multi-stage progressive image restoration. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1482114831, 2021. 6, 7, 14, 16 [66] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Restormer: Efficient transformer for high-resolution image restoration. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 57285739, 2022. 14 [67] Limeng Zhang, Hongguang Zhang, Jihua Chen, and Lei Wang. Hybrid deblur net: Deep non-uniform deblurring with event camera. IEEE Access, 8:148075148083, 2020. 2 [68] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 7 [69] Lingzhe Zhao, Peng Wang, and Peidong Liu. Bad-gaussians: Bundle adjusted deblur gaussian splatting. In European Conference on Computer Vision, pages 233250. Springer, 2025. 1, 6, 7, 16 [70] Joseph Zhu and Peiye Zhuang. Hifa: High-fidelity texttoarXiv preprint 3d with advanced diffusion guidance. arXiv:2305.18766, 3(5):7, 2023. DiET-GS: Diffusion Prior and Event Stream-Assisted Motion Deblurring 3D Gaussian Splatting"
        },
        {
            "title": "Supplementary Material",
            "content": "In this supplementary material, we provide more implementation details, introduce additional evaluations on various tasks, and conduct further ablation studies with more qualitative and quantitative analysis. More implementation details are provided in Sec. A. We conduct the user study in Sec. to further evaluate the visual quality of our method. Additional evaluations on single image deblurring task is introduced in Sec. C. Further ablation studies for our DiET-GS++ are presented in Sec. to validate our design choice. More qualitative and quantitative results on novel-view synthesis are provided in Sec. E. (a) (b) (c) Figure 7. User study. DiET-GS++ is compared to E2NeRF, EvDeblurNeRF (denoted as EDNeRF) and DiET-GS by 60 evaluators for each pair. DiET-GS++ gains significantly higher votes against the baselines, showing at least 37.96% difference. tively as follows: A. Implementation Details = D(f2D + E( ˆC)), ˆC = g(p), (8) Training. During training, we follow the configuration of the original 3DGS. The learnable camera response function CRF() is introduced after 1,500-iteration warm-up. Similarly, the regularization term Ledi simul is employed after 7,000-iteration warm-up, since DiET-GS should be able to simulate the blurry images properly. Following [3], we leverage the color events in Lev during training on realworld scenes, where the color events record color intensity changes following Bayer pattern [23]. In this case, the luma conversion h() is dropped from Eq. 6 and Lev is directly applied to the color channel responsible for triggering events. Furthermore, since green pixels appear twice as often in an RGBG Bayer pattern, we weigh the events contributions by 0.4, 0.2, and 0.4 for each of the RGB channels. Leveraging Diffusion Prior. We use Stable Diffusion 4 Upscaler (SD4) [42] as pretrained diffusion model to provide diffusion prior. SD4 is originally designed to upscale the image while recovering high-resolution details, with the low-resolution image as conditional input to the diffusion UNet. However, we find that SD4 is also effective at enhancing edge details at the same resolution. During the RSD optimization, we sample uniform random crops of 128128 resolution in latent space for fast optimization speed, following [22]. constant learning rate of 1e2 is employed for all learnable parameters fg in Stage 2. Color Correction. As also noted in [7, 55], we empirically find that leveraging diffusion prior alone in Stage 2 can exhibit color shifts. To address this issue, we adopt wavelet-based color correction proposed in [55] as postprocessing step. Specifically, let us denote the two images ˆC and rendered from DiET-GS and DiET-GS++ respecwhere g() is the pretrained 3D Gaussians from DiET-GS with rendering function and is the given camera pose. We assume that ˆC is capable of preserving the accurate color due to the color guidance from ground-truth blurry images and EDI prior in Stage 1. In contrast, from DiETGS++ tends to show color shift since it solely relies on diffusion prior while the edge details are effectively enhanced. To combine the accurate color information from ˆC and sharp edge details of C, we first decompose both images into high-frequency and low-frequency components via the wavelet decomposition. Considering that color information belongs to the low-frequency components while fine-grained details are mostly high-frequency components, we simply incorporate the low-frequency parts of ˆC and high-frequency parts of to obtain the final output. More details about wavelet-based color correction can be found in [55]. B. User Study To evaluate the visual quality in terms of human perception, we conduct user study with 60 evaluators. Specifically, we collect 30 pairs of samples from the test views of both synthetic and real-world datasets, where each pair consists of two images rendered from identical poses using different methods. During the user study, evaluators were asked to select the image with better quality between the two presented options for every pair. Baselines. We compare our DiET-GS++ to event-based deblurring rendering methods, including E2NeRF [38] and Furthermore, DiET-GS++ is also Ev-DeblurNeRF [3]. Methods MPRNet [65] NAFNet [4] Restormer [66] EDI [35] EFNet [50] BeNeRF [25] DiET-GS++ Batteries Powersupplies Labequipment Figures Drones Average MUSIQ CLIP-IQA MUSIQ CLIP-IQA MUSIQ CLIP-IQA MUSIQ CLIP-IQA MUSIQ CLIP-IQA MUSIQ CLIP-IQA 23.50 34.43 28.71 38.80 35.32 47.31 51.23 0.1074 0.2363 0.1210 0.2013 0.1503 0.1704 0.2654 0.1082 0.2023 0.1252 0.1338 0.1435 0.1789 0.2034 Table 3. Quantitative comparisons on single image deblurring with real-world datasets. 0.1034 0.1543 0.0790 0.1991 0.1762 0.2054 0.2078 0.1284 0.2045 0.1154 0.2260 0.1934 0.2445 0.2598 0.1326 0.2264 0.1631 0.2675 0.2234 0.2653 0.3012 34.01 49.25 42.32 49.63 45.23 56.06 56.32 30.97 41.64 36.37 41.75 39.12 48.71 50.34 23.34 35.47 32.13 33.53 30.13 44.25 45. 24.10 28.87 30.99 41.77 38.45 46.97 52.43 27.18 37.93 34.10 41.10 37.65 48.66 51.09 0.1160 0.2047 0.1207 0.2055 0.1773 0.2129 0.2475 Methods MUSIQ Training time (hr) Stage Stage 1 Total Rendering time (s) E2NeRF [38] Ev-DeblurNeRF [3] DiET-GS DiET-GS++ DiET-GS++-light 39.47 39. 45.31 51.71 50.23 24.3 3.4 9.8 9.8 1.1 - - - 0.17 0.17 24.3 3. 9.8 10.0 1.3 2.4139 0.8861 0.0014 1.8703 1.8703 Table 4. Comparison on training time and rendering time. compared with DiET-GS trained from Stage 1 to demonstrate the efficacy of leveraging diffusion prior in Stage 2. Results. As shown in Fig. 7, our DiET-GS++ gains at least 68.98% of the votes in each comparison, further validating the effectiveness of our framework. It also shows the clear gap of 37.96% over DiET-GS (cf . Fig. 7c), highlighting the efficacy of enhancing the edge details with diffusion prior in Stage 2. C. Single Image Deblurring We also conduct experiments on the single image deblurring task using the real-world Ev-DeblurCDAVIS dataset [3]. For evaluation, we randomly select 5 blurry images per scene and compare our DiET-GS++ against various single image deblurring baselines on these sampled images. Baselines. We classify the baselines into three categories. The first category is frame-based single image deblurring methods that rely solely on RGB frames to recover clean image. MPRNet [65], NAFNet [4], and Restormer [66] are selected for this category. The second category is eventenhanced deblurring methods that utilize additional event data to improve visual quality, consisting of EDI [35] and EFNet [50]. The third category combines NeRF and events to tackle single image deblurring, where BeNeRF [25] is chosen for this category. BeNeRF reconstructs the 3D scenes by learning the camera trajectory from single blurry image and corresponding event stream to deblur the given single view. Once we have trained BeNeRF, the deblurred image is produced by rendering the mid-exposure pose of the image along the estimated camera trajectory. Evaluation metrics. Since real-world dataset lacks the ground-truth images for the mid-exposure poses of blurry views, we employ the No Reference Image Quality Assessment (NR-IQA) metrics: MUSIQ [16] and CLIP-IQA [54] for the evaluation. Results. We present the quantitative comparisons in Tab. 3. Our DiET-GS++ consistently outperforms all baselines in every 5 real-world scenes. Specifically, compared to BeNeRF, performance is improved by an average of 2.43 and 0.0346 in MUSIQ and CLIP-IQA scores, respectively. Furthermore, we also present qualitative comparisons in Fig. 10. As shown in 2nd column, frame-based image deblurring method NAFNet often produces inaccurate details since it solely relies on blurry images to recover finegrained details. EDI and BeNeRF recover more precise details, benefiting from the event-based cameras while severe artifacts are still exhibited. Our DiET-GS++ shows the best visual quality with cleaner and well-defined details by leveraging EDI and pretrained diffusion model as prior. D. Ablation Study We present additional ablation studies to thoroughly investigate each component of DiET-GS++. All the experiments are conducted on real-world scene, namely, Figures sample. D.1. Training and Rendering Efficiency. We compare the optimization and rendering efficiency of our method to event-enhanced rendering methods, including E2NeRF [38] and Ev-DeblurNeRF [3] in Tab. 4. We present the training time of Stage 1 and Stage 2 separately, while the training time of Stage 2 remains blank if the corresponding method employs single-stage training. We observe from Tab. 4: 1) DiET-GS and DiET-GS++ require longer training time compared to Ev-DeblurNeRF. We find that RSD optimization in Stage 1 is the main factor of prolonged training time, since the gradient from the RSD loss flows to the 3D Gaussians through the pretrained VAE encoder, which introduces significant computational overhead. We thus propose the light variant of our DiET-GS++ in the 5th row by simply excluding the RSD loss in Stage 1, which we refer to as DiET-GS++-light. Despite slight performance drop in MUSIQ scores, our variant DiET-GS++- light shows the fastest optimization speed with 2.6 speedup in convergence compared to Ev-DeblurNeRF. 2) Training time for Stage 2 in DiET-GS++ only requires 0.17 hours, while showing significant improvement in MUSIQ scores compared to DiET-GS. In contrast to RSD optimization in Stage 1, the learnable latent residual is directly renCRF() PSNR SSIM LPIPS MUSIQ CLIP-IQA Conditional input PSNR SSIM LPIPS MUSIQ CLIP-IQA 32.93 34.89 0.8703 0.9049 0.1123 0.0600 38.93 45. 0.2000 0.2471 Table 5. Ablation on camera response function CRF(). EDI-processed image ground-truth blurry image 34.25 34.89 0.8964 0.9049 0.0754 0. 41.47 45.31 0.2233 0.2471 Table 6. Ablation on conditional input of RSD optiimzation in Stage 1. pensation between accurate color and well-defined details, achieving the best visual quality as shown in the Fig. 5c. D.3. Conditional Input of Diffusion UNet In Tab. 6, we explore the various options for conditional input of the diffusion UNet during the RSD optimization in Stage 1. The 1st row of Tab. 6 exploits the EDI-processed image as conditional input, while the sharp image rendered from 3DGS is given as the input to the diffusion process. However, this choice leads to inferior performance compared to leveraging the ground-truth blurry image as conditional input (2nd row). We postulate that this is because the unnatural artifacts introduced by EDI are often detrimental to the noise inference of the diffusion UNet. Despite the motion blur in the image, the ground-truth blurry image provides more natural guidance to noise prediction, such as accurate color prior, since it is real-captured from the frame-based camera. D.4. Wavelet-based Color Correction Fig. 9 presents the effectiveness of wavelet-based color correction. It effectively mitigates the color shift introduced from diffusion prior, achieving better color. E. More Results on Novel-View Synthesis Quantitative Results. In Tab. 7 and Tab. 8, we present the additional quantitative results on novel-view synthesis for each scene in both real-world and synthetic datasets. In most cases, our DiET-GS and DiET-GS++ significantly outperform existing baselines across all five evaluation metrics, showing the effectiveness of our framework. Qualitative Results. In Fig. 11 and Fig. 12, we present more qualitative comparisons on novel-view synthesis in both real-world and synthetic datasets. Our DiET-GS++ is capable of restoring: 1) accurate color, 2) fine-grained details and 3) clean texture, thus achieving the best visual quality compared to the existing baselines. F. Limitation Following previous works [3, 38], we structure DiET-GS assuming uniform-speed camera motion and dense, low-noise events. While real-world scenarios may not always meet these ideal conditions, advanced techniques like [30] could extend our methods applicability. (a) wo CRF() (b) CRF() Figure 8. Qualitative analysis on camera response function. (a) wo color correction (b) color correction (c) GT Figure 9. Ablation on wavelet-based color correction. dered from DiET-GS without exploiting the VAE encoder, which thus leads to faster gradient computation. 3) DiETGS enables real-time rendering, benefiting from the explicit representations of 3DGS. However, our DiET-GS++ exhibits longer rendering time compared to Ev-DeblurNeRF, since the rendered image is further refined through the VAE encoder and decoder. Nonetheless, leveraging diffusion prior is reasonable given the performance improvement of 12.0 MUSIQ scores compared to Ev-DeblurNeRF. D.2. Camera Response Function Tab. 5 shows the effectiveness of leveraging the learnable camera response function CRF(), showing the performance improvement in all 5 metrics. Furthermore, Fig. 8 demonstrates that the CRF() function is capable of restoring more well-defined details. As noted in [3], employing the learnable camera response function naturally fills the gap between the RGB space and the brightness change perceived by the event camera, thus effectively restoring the intricate details. Discussion. Although we adopt the similar strategy with [3] by leveraging learnable camera response function, we differ from [3] as follows: We further combine the CRF() function into the EDI formulation, proposing the novel EDI constraint for enhancing fine-grained details. As shown in the Fig. 5b, EDI color guidance Ledi color proposed by [3] often yields over-smoothed details since it treats each RGB channel as brightness which deviates from the real-world setting. To compensate the well-defined details, we propose Ledi gray by modeling the EDI in the brightness domain with exploiting learnable CRF() function. Using the Ledi color and Ledi gray together enables the mutual comScene Metric batteries figures drones powersupplies labequipment PSNR SSIM LPIPS MUSIQ CLIP-IQA PSNR SSIM LPIPS MUSIQ CLIP-IQA PSNR SSIM LPIPS MUSIQ CLIP-IQA PSNR SSIM LPIPS MUSIQ CLIP-IQA PSNR SSIM LPIPS MUSIQ CLIP-IQA MRPNet+GS [65] 28.42 0.7518 0.1948 22.13 0.2338 28.18 0.7311 0.2146 23.45 0.2418 27.13 0.7634 0.2012 28.38 0.1718 26.37 0.7513 0.1824 31.48 0.2477 27.47 0.7598 0.2138 20.18 0.1722 EDI+GS [35] 33.11 0.8994 0.0613 37.90 0.2182 33.51 0.8723 0.0977 38.48 0.2384 33.02 0.9025 0.0832 42.35 0.1633 32.10 0.8955 0.0657 46.04 0.2307 33.00 0.8911 0.0871 35.54 0.1534 EFNet+GS [50] 31.30 0.8556 0.0804 35.51 0.2293 31.28 0.8317 0.1324 37.13 0.2218 31.18 0.8617 0.1293 41.18 0.1526 30.92 0.8516 0.1029 44.15 0.2219 30.18 0.8512 0.1262 33.18 0.1418 BAD-NeRF [56] 28.29 0.8086 0.2245 17.71 0.1887 29.31 0.7703 0.2935 19.50 0.1836 28.51 0.8123 0.2122 19.05 0.1723 27.35 0.7953 0.2756 24.68 0.1762 28.89 0.8042 0.2563 18.84 0.1749 BAD-GS [69] 28.73 0.8217 0.1651 20.20 0.1918 30.12 0.7767 0.2438 22.13 0.1898 29.19 0.8317 0.1687 22.20 0.1743 28.38 0.8071 0.2247 24.90 0.1701 29.19 0.8276 0.2037 21.19 0. E2NeRF [38] 31.49 0.8715 0.0932 37.48 0.2445 32.59 0.8543 0.1108 39.47 0.2624 31.03 0.8780 0.1075 39.00 0.1877 31.06 0.8820 0.0826 45.17 0.2373 31.51 0.8578 0.1355 32.95 0.1854 Ev-DeblurNeRF [3] 32.63 0.8938 0.0443 42.99 0.2292 32.82 0.8577 0.0687 39.70 0.2441 31.62 0.8866 0.0538 41.81 0.1773 32.05 0.8980 0.0492 47.97 0.2501 32.36 0.8772 0.0696 34.14 0.2048 DiET-GS (Ours) 34.52 0.9304 0.0435 45.66 0.2327 34.89 0.9049 0.0600 45.37 0.2584 34.08 0.9339 0.0387 47.58 0.1778 33.54 0.9271 0.0460 50.25 0.2078 34.06 0.9150 0.0599 40.21 0.1708 DiET-GS++ (Ours) 33.51 0.9118 0.0444 49.89 0.2603 33.86 0.8846 0.0634 51.71 0.2955 32.92 0.9152 0.0396 50.17 0.2028 32.37 0.9108 0.0459 55.83 0.2531 33.13 0.8971 0.0575 44.60 0.1958 Table 7. Quantitative comparisons on novel-view synthesis in 5 real-world scenes Scene Metric factory pool tanabata trolley PSNR SSIM LPIPS MUSIQ CLIP-IQA PSNR SSIM LPIPS MUSIQ CLIP-IQA PSNR SSIM LPIPS MUSIQ CLIP-IQA PSNR SSIM LPIPS MUSIQ CLIP-IQA MRPNet+GS [65] 17.44 0.5918 0.3817 26.18 0.2211 19.49 0.4718 0.3219 15.19 0.1729 18.54 0.6203 0.3645 28.71 0.2818 19.58 0.6811 0.3499 26.39 0.2894 EDI+GS [35] 22.46 0.7629 0.1448 56.74 0.2177 24.83 0.6496 0.1897 47.12 0.2106 23.02 0.8088 0.1232 59.13 0.3288 24.43 0.8563 0.0923 57.56 0.3434 EFNet+GS [50] 19.74 0.6415 0.3319 37.12 0.2118 21.79 0.5238 0.3718 26.14 0.2037 20.80 0.6817 0.3128 39.28 0.2518 21.79 0.7182 0.2691 37.98 0.2583 BAD-NeRF [56] 18.81 0.6038 0.2822 27.43 0.1668 25.58 0.6888 0.2601 30.81 0.1860 16.91 0.6483 0.2175 17.56 0.2018 17.81 0.6114 0.2362 18.71 0.2007 BAD-GS [69] 21.35 0.6709 0.2391 36.19 0.1718 26.18 0.7418 0.2118 39.14 0.1911 20.18 0.7661 0.1608 27.19 0.2167 21.20 0.7064 0.1931 27.19 0.2176 E2NeRF [38] 22.28 0.7822 0.1838 45.88 0.2014 27.63 0.7488 0.1995 47.68 0.2473 23.43 0.8156 0.1505 47.81 0.1914 24.83 0.8505 0.1157 47.87 0. Ev-DeblurNeRF [3] 23.33 0.8189 0.1858 41.58 0.1947 27.26 0.7440 0.2230 44.63 0.2635 23.74 0.8059 0.1727 41.53 0.2572 24.70 0.8465 0.1335 41.80 0.2047 DiET-GS (Ours) 26.54 0.8856 0.0898 54.24 0.2215 27.40 0.7512 0.1895 51.01 0.2126 26.18 0.8965 0.0754 63.94 0.3115 26.67 0.9094 0.0708 61.48 0.3618 DiET-GS++ (Ours) 26.00 0.8707 0.0962 57.62 0.2270 26.5880 0.7283 0.1827 53.03 0.2384 25.90 0.8896 0.0712 65.54 0.3504 26.43 0.9026 0.0708 63.43 0.3683 Table 8. Quantitative comparisons on novel-view synthesis in 4 synthetic scenes (a) Blur Image (b) NAFNet [4] (c) EDI [35] (d) BeNeRF [25] (e) DiET-GS++ (Ours) Figure 10. Qualitative comparisons on single image deblurring with real-world datasets. (a) EDI+GS [35] (b) E2NeRF [38] (c) Ev-DeblurNeRF [3] (d) DiET-GS++ (Ours) (e) GT Figure 11. More qualitative comparisons on novel-view synthesis in real-world datasets. (a) EDI+GS [35] (b) E2NeRF [38] (c) Ev-DeblurNeRF [3] (d) DiET-GS++ (Ours) (e) GT Figure 12. More qualitative comparisons on novel-view synthesis in synthetic datasets."
        }
    ],
    "affiliations": [
        "Department of Computer Science, National University of Singapore"
    ]
}