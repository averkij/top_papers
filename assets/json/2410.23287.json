{
    "paper_title": "ReferEverything: Towards Segmenting Everything We Can Speak of in Videos",
    "authors": [
        "Anurag Bagchi",
        "Zhipeng Bao",
        "Yu-Xiong Wang",
        "Pavel Tokmakov",
        "Martial Hebert"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present REM, a framework for segmenting a wide range of concepts in video that can be described through natural language. Our method capitalizes on visual-language representations learned by video diffusion models on Internet-scale datasets. A key insight of our approach is preserving as much of the generative model's original representation as possible, while fine-tuning it on narrow-domain Referral Object Segmentation datasets. As a result, our framework can accurately segment and track rare and unseen objects, despite being trained on object masks from a limited set of categories. Additionally, it can generalize to non-object dynamic concepts, such as waves crashing in the ocean, as demonstrated in our newly introduced benchmark for Referral Video Process Segmentation (Ref-VPS). Our experiments show that REM performs on par with state-of-the-art approaches on in-domain datasets, like Ref-DAVIS, while outperforming them by up to twelve points in terms of region similarity on out-of-domain data, leveraging the power of Internet-scale pre-training."
        },
        {
            "title": "Start",
            "content": "REFEREVERYTHING: TOWARDS SEGMENTING EVERYTHING WE CAN SPEAK OF IN VIDEOS Anurag Bagchi1 Zhipeng Bao1 Yu-Xiong Wang2 1Carnegie Mellon University 2University of Illinois at Urbana-Champaign Pavel Tokmakov 3 Martial Hebert 1 3Toyota Research Institute 4 2 0 O 0 3 ] . [ 1 7 8 2 3 2 . 0 1 4 2 : r {anuragba,zbao,hebert}@cs.cmu.edu yxw@illinois.edu pavel.tokmakov@tri.global"
        },
        {
            "title": "ABSTRACT",
            "content": "We present REM, framework for segmenting wide range of concepts in video that can be described through natural language. Our method capitalizes on visual-language representations learned by video diffusion models on Internetscale datasets. key insight of our approach is preserving as much of the generative models original representation as possible, while fine-tuning it on narrowdomain Referral Object Segmentation datasets. As result, our framework can accurately segment and track rare and unseen objects, despite being trained on object masks from limited set of categories. Additionally, it can generalize to nonobject dynamic concepts, such as waves crashing in the ocean, as demonstrated in our newly introduced benchmark for Referral Video Process Segmentation (RefVPS). Our experiments show that REM performs on par with state-of-the-art approaches on in-domain datasets, like Ref-DAVIS, while outperforming them by up to 12 points in terms of region similarity on out-of-domain data, leveraging the power of Internet-scale pre-training. We include all of the video visualizations at https://miccooper9.github.io/projects/ReferEverything/."
        },
        {
            "title": "INTRODUCTION",
            "content": "One of the most remarkable features of natural language is its ability to describe human visual experience in all of its richness and complexity. Whether capturing fleeting moments, like raindrops rolling down the window, or smoke dissipating from cigarette (see row 2 in Figure 1), or describing dynamic processes, such as glass shattering or whirlpool forming in the water (row 1 in Figure 1), if we can utter them, we can also accurately localize them in space and time. This universal mapping between the discrete, symbolic realm of language and the continuous, ever-changing visual world is developed through lifetime of visual-linguistic interaction (Barsalou, 1999; Popham et al., 2021). The corresponding problem in computer vision - Referral Video Segmentation (RVS) (Gavrilyuk et al., 2018; Hu et al., 2016), is defined as the task of segmenting specific region in video based on natural language description. However, virtually all existing benchmarks and methods focus on specific subset of RVS - Referral Video Object Segmentation (RVOS) (Seo et al., 2020; Wu et al., 2022a), where the goal is to track and segment the object referenced by given expression. Why has the field concentrated so narrowly on this task? Although multiple factors contribute, we argue that the primary reason lies in the data. Historically, RVOS datasets have been developed by adding referral expression annotations to existing object tracking benchmarks (Pont-Tuset et al., 2017; Xu et al., 2018), which are inherently object-centric and limited in scale. At the same time, recent advances in Internet-scale datasets with billions of paired imageand videolanguage samples (Schuhmann et al., 2022; Bain et al., 2021) have opened new possibilities. These datasets have been used to train powerful denoising diffusion models (Rombach et al., 2022; Wang et al., 2023), and provide excellent representations of the natural visual-language space. In the image domain, numerous studies have shown that re-purposing diffusion models can yield highly generalizable representations of object shapes (Zhao et al., 2023; Ozguroglu et al., 2024). Very recently, Zhu et al. (2024) explored the application of video diffusion models for referral segmentation, but their approach exhibited limited generalization capabilities. Equal advising. 1 Figure 1: We present REM, framework for segmenting wide range of concepts in video that can be described through natural language by capitalizing on powerful visual-language representations learned by video diffusion models. REM generalizes with ease to challenging, dynamic concepts, such as raindrops or shattering glass, shown above. Video visualizations are available here. In this work, we introduce novel approach to RVS that leverages large-scale video-language representations learned by diffusion models. Our method, shown in Figure 3, enables spatio-temporal localization of wide range of concepts in video that can be described through natural language, beyond conventional object tracking. key factor behind our approachs success is preserving the rich representations learned by the generative model (see Figure 2). To this end, we retain the original model architecture and fine-tune it on existing referral imageand video-segmentation datasets, adjusting the output to generate object masks instead of Gaussian noise. As shown in Section 5.1, our model demonstrates competitive performance against state-of-the-art models on RVOS benchmarks. More significantly, it exhibits much stronger generalization. 2 Figure 2: Through Internet-scale pre-training, video diffusion models can generate realistic videos capturing the entire diversity of the dynamic visual world (generated samples shown above). We leverage their powerful visual-language representation for open-world referral video segmentation . To quantify this effect, we report results on the open-world object tracking benchmark - BURST (Athar et al., 2023), and the non-object Stuff segmentation dataset - VSPW (Miao et al., 2021), as well as collect new benchmark that focuses on dynamic process in Section 4. We define the latter as temporally evolving events, where the subjects undergo continuous changes in state, shape, or appearance (see examples in Figure 1). Our new benchmark, which we call Ref-VPS for Referral Video Process Segmentation, consists of 141 videos that are labeled with referral expressions and masks at 24 fps and span 39 unique concepts. Our experiments in Section 5.2 demonstrate that existing approaches fail to generalize to outside of the narrow training distribution, whereas our method effortlessly segments wide spectrum of targets as shown in Figures 1 and 4. Crucially, our approach exhibits relative improvement of up to 32% compared to the very recent method of Zhu et al. (2024), which is also based on video-diffusion representation. We investigate this in Section 5.3 and demonstrate that preserving as much of the representation learned during generative pre-training as possible is key to achieving the highest degree of generalization in referral video segmentation. We will release the code, models, and data for reproducing our results."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Referring Video Segmentation (RVS) involves segmenting specific regions in video based on natural language description (Gavrilyuk et al., 2018; Khoreva et al., 2019; Seo et al., 2020). Most benchmarks for this task were developed by adding referral expression annotations to existing Video Object Segmentation (VOS) datasets, such as DAVIS17 (Pont-Tuset et al., 2017) or YouTube-VOS (Xu et al., 2018). Consequently, the role of language in these benchmarks is limited to providing an interface for user-initialized object tracking (Wu et al., 2013; Perazzi et al., 2016). While this specific task Referral Video Object Segmentation (RVOS) is valuable, it addresses only narrow subset of the possible interactions between language and the space-time continuum of videos. Equally important is the ability of RVS methods to segment video concepts beyond common object categories. To address this gap, we introduce new benchmark focused on segmenting dynamic processes, which we term Referral Video Process Segmentation (Ref-VPS). Earlier RVOS approaches (Bellver et al., 2020; Ning et al., 2020; Hui et al., 2021) generally employed bottom-up strategy: first, image-level methods (Rother et al., 2004; Ye et al., 2019; Carion et al., 2020; Plummer et al., 2015) were applied to obtain frame-level masks, followed by spatiotemporal reasoning, such as mask propagation (Seo et al., 2020), to refine the segmentation across frames. More recently, with the success of cross-attention-based methods (Vaswani, 2017; Meinhardt et al., 2022; Zeng et al., 2022) in object segmentation and tracking, query-based architectures have been introduced to RVOS, leading to significant improvements, with ReferFormer (Wu et al., 2022a) and MUTR (Yan et al., 2024) being notable examples. The limited scale of paired videolanguage data with segmentation annotations has always been major limitation in RVOS, causing most methods to train jointly on video and image samples (Kazemzadeh et al., 2014; Jhuang et al., 2013). The latest approaches go even further and unify all object localization datasets and tasks in single framework to maximize the amount of training data (Yan et al., 2023; Wu et al., 2024; Cheng et al., 2023). However, while these models excel in object tracking, they struggle to generalize to more dynamic concepts. In contrast, we demonstrate that generative video-language pre-training on Internet-scale data (Schuhmann et al., 2022; Bain et al., 2021) results in universal mapping between the space of language and the ever-changing visual world. Diffusion Models have become the de-fact standard for generative learning in computer vision (Sohl-Dickstein et al., 2015; Ho et al., 2020) and beyond (Chi et al., 2023). Among them, the Denoising Diffusion Probabilistic Model (DDPM) (Ho et al., 2020) leverages neural network 3 components to model the denoising process and builds weighted variational bound for optimization. Stable Diffusion (SD) (Rombach et al., 2022) shifts the denoising process into the latent space of pre-trained autoencoder (Kingma & Welling, 2013), allowing for model scaling. Expanding from images to videos, diffusion models have seen success in text-to-video (T2V) generation (Wang et al., 2023; Chen et al., 2023; 2024; Zheng et al., 2024; Blattmann et al., 2023). In addition to the capacity to generate high-fidelity images based on text prompts, the T2V diffusion models implicitly learn the mapping from linguistic descriptions to video regions, providing an opportunity to repurpose them for RVOS. Among current T2V methods, ModelScope (Wang et al., 2023) and VideoCrafter (Chen et al., 2023; 2024) stand out for their open-source implementations, forming the backbone of our research. Visual-language Pre-training for Perception: in addition to being highly effective in image and video generation, diffusion models have been shown to learn strong representation of the natural image manifold. Several works have demonstrated that these representations can be re-purposed for classical computer vision problems, including semantic segmentation (Xu et al., 2023; Zhao et al., 2023; Zhang et al., 2023) and pixel-level correspondence (Tang et al., 2023), achieving an impressive degree of generalization. Others have shown that image diffusion models learn powerful representations of objects, enabling open-world novel view synthesis (Liu et al., 2023) and amodal segmentation (Ozguroglu et al., 2024). Most recently, Zhu et al. (2024) also leverages pretrained T2V models for RVOS, however, our analysis shows that their approach fails to fully capitalize on the universal visual-language mapping learned in generative pre-training. In this work, we explore the application of video diffusion models to RVS, demonstrating how to maintain high-level generalizability during fine-tuning. In separate line of work, visual-language representations learned with contrastive objectives (Bao et al., 2022; Radford et al., 2021) have been adapted for referring image (Lai et al., 2024; Rasheed et al., 2024; You et al., 2023; Xu et al., 2024) and video segmentation (Zhou et al., 2024). However, their performance remains limited, compared to both generative models, as well as classical referral segmentation approaches."
        },
        {
            "title": "3 METHOD",
            "content": "3.1 LEARNING THE VISUAL-LANGUAGE MANIFOLD VIA VIDEO DENOISING Text-to-Video (T2V) diffusion models (Wang et al., 2023; Chen et al., 2024; Zheng et al., 2024) generate videos that align with given language description, starting from Gaussian noise. The process can be formalized as: ˆx = fvdm(xT , c, ), (1) where ˆx is the generated video, denotes the maximum timestep specified by the video diffusion model fvdm, xT is sample drawn from Gaussian distribution (µT , σ2 ) predefined by the video diffusion model, and is the conditioning prompt. To reduce computational complexity, these models often perform denoising in the latent space (Rombach et al., 2022). Specifically, pretrained Variational Autoencoder (VAE) (Kingma & Welling, 2013) is employed to map the video from pixel space into latent space, denoted as E(x) = z, while decoder reconstructs it from the latent space, D(z) x. Thus, the generation process becomes: ˆx = (fvdm(zT , c, )) . (2) During training, rather than denoising from pure Gaussian latents, T2V models denoise from partially noisy video latents and optimize the following latent diffusion objective: EzE(x),t,ϵN (0,1) ϵ ϵθ(zt, ec, t)2 2 , min θ (3) where ϵ is the Gaussian noise added to the clean video latent, zt represents noisy video latent at timestep derived by the diffusion forward pass (Ho et al., 2020; Rombach et al., 2022), and ec is the conditional embedding generated from using text encoder, such as CLIP (Radford et al., 2021). The denoising network ϵθ(zt, ec, t), typically U-Net (Ronneberger et al., 2015), is tasked with predicting the noise ϵ. In this network, the conditional embedding ec interacts with the latent representations through cross-attention mechanisms, guiding the model to generate diverse, semantically accurate videos based on the provided text descriptions. 4 Figure 3: The model architecture of Refer Everything with Diffusion Models (REM). Like video diffusion model it is based on, our approach takes video frames with added noise and language expression as input. Our key insight is preserving as much of the diffusion representation intact as possible by supervising segmentation masks in the latent space of the frozen VAE. 3.2 FROM LANGUAGE-CONDITIONED DENOISING TO REFERRAL VIDEO SEGMENTATION Referral Video Segmentation (RVS) involves segmenting an entity in video across spatial and temporal dimensions, guided by natural language description. Formally, the task is defined as: ˆm = fRVS(x, c), (4) where fRVS is the RVS model, represents video sequence, denotes referral text prompt, and ˆm corresponds to the binary masks produced as output. This task aligns naturally with T2V models, which establish robust mapping between the entities described in the text and the corresponding spatial-temporal regions in the video by optimizing the denoising objective in Equation 3. Several prior works have explored the alignment of diffusion models with referral segmentation (Zhao et al., 2023; Xu et al., 2023; Zhu et al., 2024), typically employing these models as feature extractors. Specifically, they adjust the input format of referral segmentation model to match that of the denoising network ϵθ, and pass the resulting features to task-specific decoder fdec (e.g., convolutional network) to predict the target masks: ˆm = fdec(ϵ(n) (zt, ec, t)), θ where zt is the noisy latent representation of the input images at timestep t, ec is feature embedding of the referral expression c, and ϵ(n) denotes the intermediate feature at the nth layer. In practice, is usually set to small value (e.g., 50), and is set to the last layer indexes to obtain the optimal performance. The entire model is then trained in conventional discriminative learning setup. However, replacing parts of the generative model with newly initialized layers can disrupt alignment between the models representation from pre-training and the new features learned on narrow-domain datasets, leading to substantial loss of generalization capabilities. θ (5) In our approach, shown in Figure 3, we propose to instead preserve the architecture (and thus the representation) of the video diffusion model in its entirety. Specifically, rather than using intermediate features ϵ(n) , REM repurposes the whole denoising network ϵθ (together with the VAE) by shifting its objective from predicting noise to predicting mask latents (shown on the right in Figure 3): θ ˆm = D(ϵθ(zt, ec, t)), (6) where denotes the (frozen) VAE decoder used to produce the actual binary segmentation masks from the predicted latents. That is, instead of learning the decoder network fdec from scratch, we reuse the VAE from the video diffusion model. This subtle yet powerful modification allows the 5 model to better preserve its representation learned on Internet-scale data during generative pretraining while adapting to the task of RVS. Training and Optimization. During training, to encode the ground-truth segmentation masks with the VAE, we broadcast the single-channel mask into three channels by simply duplicating it (shown in the top right of Figure 3). For simplicity, we still denote this three-channel mask representation as m. The pretrained VAE can then map the mask sequence into the latent space via E(m) = zm and decode the masks back from predicted latents via D(zm) m. For the noisy latent zt and timestep t, we prioritize using latents that remain as clean as possible. Therefore, we always set the timestep to its minimum value, = 0. To train the model, we supervise the predicted mask latents using an L2 loss (shown in the center-right of Figure 3) by minimizing: EzmE(m),t=0 zm ϵθ(zt, ec, t)2 2 . min θ (7) Model Inference. During inference, we follow Equation 6, with = 0, to decode the predicted mask latent and generate three-channel mask predictions. We then compute the single-channel masks by averaging the pixel values of the three channels and applying constant threshold of 0.5 to binarize the result (as shown in the bottom right of Figure 3)."
        },
        {
            "title": "4 BENCHMARK DESIGN AND COLLECTION",
            "content": "Existing datasets only allow to quantify the generalization of RVS models to rare object categories (Athar et al., 2023) or static Stuff (Miao et al., 2021). To present more comprehensive generalization evaluation, we now discuss our approach to collecting new benchmark. As covering the entire spectrum of concepts that can be spoken of in videos would be extremely costly, we seek to identify the most salient subset of the problem that requires joint modeling of language and temporal dynamics. Specifically, we target dynamic processes, defined as temporally evolving events, where the subjects undergo continuous changes in state, shape, or appearance. Crucially, the subjects in this context are not limited to objects, but include all concepts that are spatio-temporally localizable in videos, such as light or fire. The key steps for collecting this new benchmark, which we call Referral Video Process Segmentation (Ref-VPS), include selecting representative videos and annotating them with referring expressions and segmentation masks. 4.1 VIDEO SELECTION To source the videos for our benchmark we require large, public, and diverse database that is queriable with natural language and allows re-distribution of content for research purposes. Based on these requirements, we choose the TikTok social media platform which has over 1 billion active users across the world and receives tens of millions of video uploads daily, capturing wide range of dynamic visual content. TikToks policies generally allow for free redistribution of content, with individual users having the option to opt-out. To search for videos that capture dynamic processes, as defined above, we first identify nonexhaustive list of six broad and possibly overlapping concepts (e.g., object transformations, or entities with dynamic boundaries, the full list together with definitions provided in Section in the appendix). Then, for each concept we ask ChatGPT (OpenAI, 2023) to provide list of concrete examples together with multiple text queries for search on TikTok (e.g., wax candle melting for object transformations), resulting in 120 individual concepts. We retrieved over 1,000 samples based on these queries, however, majority of the queries did not yield suitable videos because of the physical nature of the event or ambiguity of the search query not lending itself to being accurately captured on TikTok. After removing irrelevant videos, the retrieved set is reduced to 342 samples. We then manually filter these videos based on the following criteria: (1) videos that do not feature significant dynamic changes of the subject (e.g., mostly stationary clouds in the sky); (2) dynamic processes that occur too rapidly to allow for the labeling of sufficient number of non-empty frames (e.g., flashes of lightning); (3) video with frequent shot changes, which make it impossible to extract an interrupted clip capturing the event of interest. Additionally, for videos that represent compilations of similar events, we split them into individual clips and treat each one independently. The resulting dataset contains 141 video clips representing 39 dynamic process concepts. The entire dataset is intended for zero-shot evaluation, so we do not define any additional splits. Representative samples of the videos are shown in Figure 1."
        },
        {
            "title": "4.2 ANNOTATION COLLECTION AND EVALUATION",
            "content": "To label the videos selected above, we begin by adjusting the temporal boundaries of each clip to focus on the event of interest and avoid shot changes. We also make sure that the event is captured in its entirety whenever possible, including some context before and after it. The clips are then exported at 24 FPS as frames. If video contains irrelevant frames, such as the TikTok logo at the end, we crop the frames accordingly to remove the padding. To collect referral expressions, we first manually identify the entity of interest in each clip. The selected entity is then labeled with referral expressions by two independent annotators. Each annotator provides two expressions for the target, resulting in total of four expressions per clip, capturing different ways to describe the same phenomenon. Following the standard protocol (Khoreva et al., 2019; Seo et al., 2020), models are evaluated on all queries and the results are averaged. Finally, we densely label the targets identified above with segmentation masks at 24 FPS. To this end, we employ semi-automatic pipeline, capitalizing on the recently introduced SAM2 (Ravi et al., 2024) foundational model for interactive video segmentation. In particular, we provide positive and negative click annotation in the middle frame of video first to ensure accurate boundary segmentation. SAM2 then automatically segments the entity of interest in the frame, as well as propagates the mask across the entire clip. We interactively improve segmentation quality by providing additional clicks as needed. In the end, we manually refine the masks in frames where SAM2 fails. We report the statistics of our benchmark in the appendix. For evaluation, we follow Tokmakov et al. (2023) and only report region similarity as contour accuracy is often not well defined for the entities like smoke or light which are frequent in Ref-VPS."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "Datasets and Evaluation. We evaluate our method on five benchmarks in total. Ref-YTB (Seo et al., 2020) and Ref-DAVIS (Khoreva et al., 2019) are standard RVOS benchmarks for evaluating our models performance on object tracking. The evaluation on Ref-YTB is done on the official challenge server, and Ref-DAVIS is evaluated using the official evaluation code. For evaluating generalization to rare objects and Stuff categories, we use the BURST (Athar et al., 2023) and VSPW (Miao et al., 2021) datasets respectively. Finally, we evaluate REM and the strongest baselines on our newly introduced Ref-VPS benchmark that focuses on dynamic process segmentation (detailed in Section 4). All these datasets except Ref-YTB are only used for evaluation (i.e., the results are zero-shot). For Ref-YTB (Seo et al., 2020) and Ref-DAVIS (Khoreva et al., 2019) we use the standard evaluation metrics - Region Similarity (J ), Contour accuracy (F) and their mean (J &F). For all other evaluations we use the Region Similarity (J ) metric. Implementation details. We build our method upon ModelScopeT2V (Wang et al., 2023). We adopt two-stage training strategy following prior work (Zhu et al., 2024). In the first stage, we fine-tune only the spatial weights using image-text samples from Ref-COCO (Yu et al., 2016) for 1 epoch and then fine-tune all weights for 40 epoch using Ref-YTB (Seo et al., 2020) video-text training samples and 12k samples from Ref-COCO jointly. In the second stage, the image samples from Ref-COCO are converted to pseudo videos through augmentations following Wu et al. (2022b). We freeze the CLIP encoder and VAE during training. More details about the dataset, evaluation details on BURST and VSPW, and implementation are included in Section in the appendix. 5.1 REFERRAL VIDEO OBJECT SEGMENTATION RESULTS In this section, we compare REM to the state of the art on the standard RVOS benchmarks. We report results on the validation set of Ref-DAVIS (Khoreva et al., 2019) and the test set of Ref-YTB (Seo et al., 2020) in Table 1. Our method outperforms state of the art in terms of on Ref-DAVIS and 7 Method Pretraining Data Mask/Box Supervision Referformer (Wu et al., 2022a) MUTR (Yan et al., 2024) VLMO-L (Zhou et al., 2024) UNINEXT (Yan et al., 2023) VD-IT (Zhu et al., 2024) REM (Ours) ImageNet + Kinetics + SSv2 Ref-COCO/+/g + Ref-YTB ImageNet + Kinetics + SSv2 Unknown Object365 LAION5B + WebVid LAION5B + WebVid Ref-YTB + AVS Ref-COCO/+/g + Ref-YTB 10+ Image/Video datasets Ref-COCO/+/g + Ref-YTB Ref-COCO/+/g + Ref-YTB Ref-DAVIS 58.1 64.8 66.3 68.2 66.2 69.9 &F 61.1 68.0 70.2 72.5 69.4 72.6 64.1 71.3 74.1 76.8 72.6 75.29 Ref-YTB 61.3 66.4 65.3 67.6 64.4 67.05 &F 62.9 68.4 67.6 70.1 66.5 68. 64.6 70.4 69.8 72.7 68.5 69.73 Table 1: Comparison to the state of the art on the validation set of the Ref-DAVIS and the test set of Ref-YTB benchmarks using the standard metrics. Our method performs on par with the strong UNINEXT approach, despite not being specifically designed for object localization and having access to only fraction of the localization labels used by that method. Benchmark MUTR UNINEXT VD-IT REM (Ours) 10.1 VSPW BURST 30.2 10.5 27.9 12.7 30.9 15.2 40. Benchmark Ref-VPS (J ) MUTR UNINEXT VD-IT REM (Ours) 26.22 37.58 28.36 49.56 Table 2: Comparison to state of the art on the Stuff categories in the eval set of VSPW and on the joint val and test sets of BURST. Our approach demonstrates much stronger generalization, notably, outperforming VD-IT which is based on the same diffusion backbone. Table 3: Comparison to the state of the art on our new Ref-VPS benchmark. REM shows much stronger generalization to challenging, dynamic concepts in this dataset compared to the baselines by effectively capitalizing on Internet-scale visual-language pre-training. is only second to UNINEXT (Yan et al., 2023) on Ref-YTB. Note that this approach is specifically designed for object segmentation and utilizes more than 10 datasets with localization annotations like bounding boxes and masks for training. In contrast, REM adopts an architecture of video generation model and is only fine-tuned on one imageand one video-segmentation dataset. Despite this, our method is competitive with UNINEXT on standard RVOS benchmarks, and we will show next, outperforms it by up to 21 points out-of-domain in terms of . Another notable observation is that REM also outperforms VD-IT (Zhu et al., 2024), which is built on top of the same video diffusion backbone of Wang et al. (2023), on both datasets. This result demonstrates the effectiveness of our approach to preserving the visual-language representations learned on the Internet data, which will become even more evident in out-of-domain evaluation. 5.2 OUT-OF-DOMAIN GENERALIZATION We begin by performing generalization study on existing open-world tracking BURST dataset (Athar et al., 2023) as well as on the Stuff categories (Caesar et al., 2018) from VSPW (Miao et al., 2021) in Table 2. BURST is an open-world video object segmentation benchmark, whereas VSPW tests the ability to generalize to non-object categories. We report zero-shot evaluation results on the validation set of VSPW and combined validation and test sets of BURST and compare to the top performing methods from Table 1 that have public models. Firstly, we observe that on both out-of-domain challenges our method outperforms all the baselines by significant margins. The improvements are especially noticeable on BURST, demonstrating that REM successfully preserves the strong object representation learned by Internet-scale pre-training of the diffusion backbone. In contrast, VD-IT loses this generalization capacity during fine-tuning and only performs on par with UNINEXT. On the Stuff categories all the methods do relatively poorly, reflecting the challenge of generalizing to more amorphous Stuff. Here VD-IT maintains lead over entirely object-centric UNINEXT but REM still outperforms both baselines. Finally, we compare REM to the top-performing RVS baselines on our Ref-VPS benchmark in Table 3. Here the differences between the methods are more pronounced, highlighting the value of Ref-VPS in assessing video-language understanding capabilities of neural representations. Our approach outperforms all baselines by up to 12 points in Region Similarity (32% relative improvement), and notably surpasses the top RVOS method, UNINEXT, by 21.2 points (74.8% relative improvement). While generative pre-training enhances VD-ITs generalization ability compared to UNINEXT, it struggles to preserve its representations as effectively as REM. qualitative comparison of REM with VD-IT and UNINEXT is provided in Figure 4. Firstly, we can see that our approach can successfully track the sponge (which was never seen in training) in challenging sequence from BURST, whereas other methods focus on foreground objects. In 8 Figure 4: Qualitative results of REM and state of the art baselines on BURST, VSPW and Ref-VPS benchmarks. Our method demonstrates both superior coverage of rare, dynamic concepts and higher segmentation precision. Video comparisons are available here. the second sequence from VSPW REM successfully generalizes to the non-object wall category, whereas UNINEXT focuses on nearby object and VD-IT fails entirely. The following examples from our Ref-VPS illustrate that both baselines exhibit object-centric bias, as in the examples with Backbone Decoder Stable Diffusion 2.1 VideoCrafter-1 VideoCrafter-2 ModelScope T2V ModelScope T2V Frozen VAE CNN Ref-YTB Ref-VPS (J &F) 59.38 59.10 65.00 64.57 59. 30.69 29.22 37.28 39.05 29.93 Table 4: Analysis of the effects of generative pre-training and discriminate finetuning strategies on Ref-YTB and RefVPS. The key to the success of REM is capitalizing on Internet-scale image and video pre-training and preserving as much of this representation as possible. the lizard skin in row 3 and blue smoke in row 6. While VD-IT shows better generalization, it often simply segments the dominant region in the video (see rows 4 and 7 in Figure 4). In contrast, REM demonstrates both good coverage of rare concepts and high precision with respect to the language prompt. See more examples of highly dynamic sequences in Section B.1 in the appendix."
        },
        {
            "title": "5.3 ABLATION ANALYSIS",
            "content": "In this section, we analyze our proposed approach of transferring generative representations to the task of RVS. We report results on one representative RVOS benchmark (Ref-YTB) and on our new Ref-VPS. Note that for efficiency we fine-tune all the models on subset of image and video data (12000 samples) so the numbers are lower than those reported in the previous section. Generative pre-training. We begin by evaluating the effect of the generative pre-training strategy in Table 4. Firstly, we design frame-level baseline that fine-tunes StableDiffusion (Blattmann et al., 2023) on every frame individually (row 1 in the table). While this variant has no temporal modeling capacity, its architecture is similar to UNINEXT (Yan et al., 2023) - the state-of-the-art approach for RVOS. Interestingly, it strongly under-performs compared to our best video-based variant not only on our Ref-VPS but also on the object-centric Ref-YTB benchmark. These results demonstrate that, despite the fact that images are the dominant data source in generative pre-training, fine-tuning StableDiffsuion for video generation is crucial for learning an effective representation for tracking. Next, we compare several strategies for learning video diffusion models. We begin by studying two variants of the VideoCrafter model (Chen et al., 2023; 2024) (denoted as VideoCrafter-1 and VideoCrafter-2 in Table 4). They are both trained on 600M images from LAION (Schuhmann et al., 2022) and 10-20M Internet videos. However, VideoCrafter-2 is further tuned to increase the quality of the generated samples. Our findings indicate that this fine-tuning step leads to significant performance gains across both benchmarks. This suggests that improving the quality of video generation models can directly translate to enhanced performance in our video segmentation framework. Finally, we evaluate ModelScope (Wang et al., 2023), which is trained on larger LAION 2B and comparable amount of video samples (last row in Table 4). This model delivers performance comparable to the best version of VideoCrafter on the Ref-YTB benchmark, while demonstrating superior generalization to more challenging concepts in Ref-VPS. These results further highlight that both large-scale pre-training on image data as well as learning to model video-language interactions are crucial components for robust RVS representation learning. Fine-tuning strategy. We now ablate the effectiveness of our design decision to re-use frozen VAE decoder for mask prediction, rather than replacing it with dedicated mask prediction module, as was done in some of the prior work (Zhao et al., 2023; Zhu et al., 2024). To this end, we replace the VAE with CNN mask decoder adopted from (Zhao et al., 2023) and train it jointly with the rest of the model (last row in Table 4). Removing the pre-trained VAE decoder has moderate negative effect on performance on Ref-YTB, but, notably, destroys the models ability to generalize our challenging Ref-VPS benchmark. This result underscores the main message of our paper - preserving as much of the representation learned during generative pre-training is key for achieving generalization in referral video segmentation."
        },
        {
            "title": "6 DISCUSSION",
            "content": "In this paper, we proposed REM, framework that capitalizes on Internet-scale video-language representations learned by diffusion models to segment wide range of concepts in video that can be described through natural language. Our key insight is that changing as little as possible in the rep10 resentation is key to preserving its universal mapping between language and visual concepts during fine-tuning. We have evaluated the generalization capabilities of our approach on existing datasets for open-world object and stuff segmentation, and also collected Ref-VPS - new benchmark for referral segmentation of dynamic processes in videos. Our extensive experimental evaluation demonstrates that, despite only being trained on object masks, REM successfully generalizes to unseen object and non-object concepts, outperforming all prior work by up to 12 points in terms of region similarity. Despite REMs impressive generalization abilities, the problem of RVS is far from being solved. In Figure in the Appendix we visualize few failure cases of our method. REM still exhibits some object-centric bias and struggles with extremely fast changes. Exploring ways to preserve even more of the representation learned during generative pre-training, e.g., via low-rank adaptation (Hu et al., 2022) of the visual backbone, is very promising direction to address some of these issues. In addition, note that REM is generic framework where the backbone of Wang et al. (2023) can be replaced with more advanced representation, tracing the progress of language-conditioned video generative models."
        },
        {
            "title": "REFERENCES",
            "content": "Ali Athar, Jonathon Luiten, Paul Voigtlaender, Tarasha Khurana, Achal Dave, Bastian Leibe, and Deva Ramanan. BURST: benchmark for unifying object recognition, segmentation and tracking in video. In WACV, 2023. Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In ICCV, 2021. Zhipeng Bao, Martial Hebert, and Yu-Xiong Wang. Generative modeling for multi-task visual learning. In ICML, 2022. Lawrence Barsalou. Perceptual symbol systems. Behavioral and brain sciences, 22(4):577660, 1999. Bellver, Ventura, Silberer, Kazakos, Torres, and Giro-i Nieto. Refvos: closer look at referring expressions for video object segmentation, 2020. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. COCO-Stuff: Thing and stuff classes in context. In CVPR, 2018. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020. Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for highquality video generation. arXiv preprint arXiv:2310.19512, 2023. Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. arXiv preprint arXiv:2401.09047, 2024. Ho Kei Cheng, Seoung Wug Oh, Brian Price, Alexander Schwing, and Joon-Young Lee. Tracking anything with decoupled video segmentation. In CVPR, 2023. Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. In RSS, 2023. Kirill Gavrilyuk, Amir Ghodrati, Zhenyang Li, and Cees GM Snoek. Actor and action video segmentation from sentence. In CVPR, 2018. 11 Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 2020. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. 2022. Ronghang Hu, Marcus Rohrbach, and Trevor Darrell. Segmentation from natural language expressions. In ECCV, 2016. Tianrui Hui, Shaofei Huang, Si Liu, Zihan Ding, Guanbin Li, Wenguan Wang, Jizhong Han, and Fei Wang. Collaborative spatial-temporal modeling for language-queried video actor segmentation. In CVPR, 2021. Hueihan Jhuang, Juergen Gall, Silvia Zuffi, Cordelia Schmid, and Michael Black. Towards understanding action recognition. In ICCV, 2013. Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In EMNLP, 2014. Anna Khoreva, Anna Rohrbach, and Bernt Schiele. Video object segmentation with language referring expressions. In ACCV, 2019. Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In CVPR, 2024. Ruoshi Liu, Rundi We, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In ICCV, 2023. Ilya Loshchilov, Frank Hutter, et al. Decoupled weight decay regularization. In ICLR, 2019. Tim Meinhardt, Alexander Kirillov, Laura Leal-Taixe, and Christoph Feichtenhofer. Trackformer: Multi-object tracking with transformers. In CVPR, 2022. Jiaxu Miao, Yunchao Wei, Yu Wu, Chen Liang, Guangrui Li, and Yi Yang. VSPW: large-scale dataset for video scene parsing in the wild. In CVPR, 2021. Ke Ning, Lingxi Xie, Fei Wu, and Qi Tian. Polar relative positional encoding for video-language segmentation. In IJCAI, 2020. OpenAI. Chatgpt, 2023. URL https://chat.openai.com. Ege Ozguroglu, Ruoshi Liu, Dıdac Surıs, Dian Chen, Achal Dave, Pavel Tokmakov, and Carl Vondrick. pix2gestalt: Amodal segmentation by synthesizing wholes. In CVPR, 2024. Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and Alexander Sorkine-Hornung. benchmark dataset and evaluation methodology for video object segmentation. In CVPR, 2016. Bryan Plummer, Liwei Wang, Chris Cervantes, Juan Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models. In ICCV, 2015. Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbelaez, Alexander Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv:1704.00675, 2017. Sara Popham, Alexander Huth, Natalia Bilenko, Fatma Deniz, James Gao, Anwar NunezElizalde, and Jack Gallant. Visual and linguistic semantic representations are aligned at the border of human visual cortex. Nature neuroscience, 24(11):16281636, 2021. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 12 Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad Khan. Glamm: Pixel grounding large multimodal model. In CVPR, 2024. Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. SAM 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In CVPR, 2022. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In MICCAI, 2015. Carsten Rother, Vladimir Kolmogorov, and Andrew Blake. grabcut interactive foreground extraction using iterated graph cuts. ACM TOG, 2004. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. NeurIPS, 2022. Seonguk Seo, Joon-Young Lee, and Bohyung Han. Urvos: Unified referring video object segmentation network with large-scale benchmark. In ECCV, 2020. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML, 2015. Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. Emergent correspondence from image diffusion. NeurIPS, 2023. Pavel Tokmakov, Jie Li, and Adrien Gaidon. Breaking the object in video object segmentation. In CVPR, 2023. Vaswani. Attention is all you need. NeurIPS, 2017. Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. Jiannan Wu, Yi Jiang, Peize Sun, Zehuan Yuan, and Ping Luo. Language as queries for referring video object segmentation. In CVPR, 2022a. Jiannan Wu, Yi Jiang, Peize Sun, Zehuan Yuan, and Ping Luo. Language as queries for referring video object segmentation, 2022b. URL https://arxiv.org/abs/2201.00487. Junfeng Wu, Yi Jiang, Qihao Liu, Zehuan Yuan, Xiang Bai, and Song Bai. General object foundation model for images and videos at scale. In CVPR, 2024. Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Online object tracking: benchmark. In CVPR, 2013. Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. ODISE: Open-vocabulary panoptic segmentation with text-to-image diffusion models. In CVPR, 2023. Jiarui Xu, Xingyi Zhou, Shen Yan, Xiuye Gu, Anurag Arnab, Chen Sun, Xiaolong Wang, and Cordelia Schmid. Pixel-aligned language model. In CVPR, 2024. Ning Xu, Linjie Yang, Yuchen Fan, Jianchao Yang, Dingcheng Yue, Yuchen Liang, Brian Price, Scott Cohen, and Thomas Huang. YouTube-VOS: Sequence-to-sequence video object segmentation. In ECCV, 2018. Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Ping Luo, Zehuan Yuan, and Huchuan Lu. Universal instance perception as object discovery and retrieval. In CVPR, 2023. 13 Shilin Yan, Renrui Zhang, Ziyu Guo, Wenchao Chen, Wei Zhang, Hongyang Li, Yu Qiao, Hao Dong, Zhongjiang He, and Peng Gao. Referred by multi-modality: unified temporal transformer for video object segmentation. In AAAI, 2024. Linwei Ye, Mrigank Rochan, Zhi Liu, and Yang Wang. Cross-modal self-attention network for referring image segmentation. In CVPR, 2019. Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. arXiv preprint arXiv:2310.07704, 2023. Licheng Yu, Patrick Poirson, Shan Yang, Alexander C. Berg, and Tamara L. Berg. Modeling context in referring expressions, 2016. URL https://arxiv.org/abs/1608.00272. Fangao Zeng, Bin Dong, Yuang Zhang, Tiancai Wang, Xiangyu Zhang, and Yichen Wei. Motr: End-to-end multiple-object tracking with transformer. In ECCV, 2022. Junyi Zhang, Charles Herrmann, Junhwa Hur, Luisa Polania Cabrera, Varun Jampani, Deqing Sun, and Ming-Hsuan Yang. tale of two features: Stable diffusion complements dino for zero-shot semantic correspondence. NeurIPS, 2023. Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu, Jie Zhou, and Jiwen Lu. Unleashing textto-image diffusion models for visual perception. In ICCV, 2023. Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, 2024. URL https://github.com/hpcaitech/Open-Sora. Zikun Zhou, Wentao Xiong, Li Zhou, Xin Li, Zhenyu He, and Yaowei Wang. Driving referring video object segmentation with vision-language pre-trained models. arXiv preprint arXiv:2405.10610, 2024. Zixin Zhu, Xuelu Feng, Dongdong Chen, Junsong Yuan, Chunming Qiao, and Gang Hua. Exploring In ECCV, pre-trained text-to-video diffusion models for referring video object segmentation. 2024. 14 In this appendix, we first include the dataset details for your Ref-VPS dataset in Section A. Next, we include additional experimental evaluations, including comparisons with state-of-the-art methods on more challenging fighting scenes, failure case analysis, and concept coverage comparisons in Section B. Finally, in Section C, we show the full implementation details."
        },
        {
            "title": "A DATASET DETAILS",
            "content": "During our dataset collection, we first identify non-exhaustive list of six broad and possibly overlapping concepts. The list of the concepts and their definitions are included below: Temporal object changes: Concepts involving changes over time (e.g., object deformation, melting) Motion Patterns: Concepts involving movement and displacement of non-object regions (e.g., water ripples, flickering flames) Dynamic environmental changes: Changes in the environment that affect spatial regions over time (e.g., clouds moving across the sky, waves rising ) Interaction Sequences: Concepts involving interactions between objects (e.g., bullet hitting glass, object collisions) Pattern evolution: Concepts where patterns or textures evolve or change dynamically (e.g., changing patterns of smoke dispersion, fluctuating light levels) The final dataset contains 141 video clips representing 39 dynamic process concepts. We report comprehensive list of key statistics in Table A. Most of our samples are around 2.5 to 5 secs in length but can go up to more than 20 seconds. The distribution of our sample lengths is reported in Figure A."
        },
        {
            "title": "B ADDITIONAL EXPERIMENTAL EVALUATIONS",
            "content": "B.1 EVALUATION ON CHALLENGING FIGHT SCENES Fight sequences in movies, television, and animated shows pose unique set of challenges. Typically fight scenes are characterized by objects/characters undergoing severe and frequent occlusions and leaving the frame entirely, coupled with frequent pose changes of the camera. This leads to drastic changes in the appearance of the object and requires high levels of temporal and semantic consistency to accurately track, re-identify, and segment the referred entity. Our diffusion finetuning method excels in this domain of extremely challenging samples as illustrated in Figure B. It is clear to find that UNINEXT and VD-IT both fail whenever there is large occlusion causing the referred entity to become invisible. Even though VD-IT uses frozen Video diffusion features, their method is unable to leverage the temporal consistency learned during Video Diffusion pre-training as well as our method. B.2 FAILURE CASES few representative failure cases of REM on Ref-VPS are shown in Figure C. Our method suffers from object-centric bias in the most challenging scenarios and struggles with extremely fast processes. B.3 CONCEPT COVERAGE PLOT ON BURST DATASET We additionally show the concept coverage on BURST (Athar et al., 2023) dataset among VDIT (Zhu et al., 2024), MUTR (Yan et al., 2024), UNINEXT (Yan et al., 2023), and ours in Figure D. In general, our model has significantly better coverage of different object concepts with performance improvement of at least 9.5 %. Moreover, our REM is more robust in the long-tail regions, indicating the promising generalization capacity of our method by repurposing video diffusion models. The performance improvement compared to VD-IT, which uses frozen video diffusion features, indicates that simply freezing the diffusion model as backbone does not guarantee the transfer of the diffusion knowledge to downstream tasks and verifies the design of our approach. 15 Clips FPS Frames Concepts Avg length (s) Annotation FPS Min-resolution Max-resolution 141 24 22831 39 6.75 24 712 576 1024 576 Table A: Statistics of our Ref-VPS benchmark. Our dataset contains 141 video clips covering 39 concepts for dynamic processes. Figure A: Distribution of sample lengths in Ref-VPS. Type Benchmark Image Ref-COCO (Yu et al., 2016) Ref-YTB (Seo et al., 2020) Video Ref-DAVIS (Khoreva et al., 2019) Video Video BURST (Athar et al., 2023) Video VSPW (Miao et al., 2021)"
        },
        {
            "title": "Training Samples Testing Samples",
            "content": "320K 12,913 - - - - 2,096 90 2,049 343 Table B: Details about the benchmarks we used for training and evaluation."
        },
        {
            "title": "C IMPLEMENTATION DETAILS",
            "content": "C.1 DATASET DETAILS We report the details about the benchmarks we used in Table B. C.2 STUFF CATEGORY EVALUATION Neither BURST (Athar et al., 2023) nor VSPW (Miao et al., 2021) contains referral text for the segmented entities. For them, we automatically generate referral expressions using only the category information of the mask entity as the <class> (e.g., the hat). For VSPW we conduct our evaluation on the validation set which has 66 different stuff categories. In the case of BURST, we evaluate the combined validation and test set which contains 454 classes and total of 2049 sequences. C.3 BASELINE MODELS In addition to Table 1 in the main paper, we report the comprehensive list of bounding/mask supervision used by all methods in Table C. We quote the results of all the baseline models on Ref-YTB and Ref-DAVIS from their original paper. For the evaluation of BURST, VSPW, and our dataset, we report the numbers by running the official checkpoints of MUTR1, UNINEXT2, and VD-IT3. C.4 TRAINING DETAILS We build our method upon ModelScopeT2V (Wang et al., 2023). We adopt two-stage training strategy following prior work (Yan et al., 2024; Wu et al., 2022a). In the first stage, we fine-tune only the spatial weights using image-text samples from Ref-COCO (Yu et al., 2016) for 1 epoch and then fine-tune all weights for 40 epoch using Ref-YTB (Seo et al., 2020) video-text samples and 12k samples from Ref-COCO jointly. In the second stage, the image samples from Ref-COCO are 1https://github.com/OpenGVLab/MUTR 2https://github.com/MasterBin-IIAU/UNINEXT 3https://github.com/buxiangzhiren/VD-IT 16 Figure B: Qualitative comparison of REM with state-of-the-art baselines on dynamic and challenging fight scenes. The incorrectly labeled frames are outlined in red. Our method outperforms the other methods in handling frequent occlusions and POV changes. For better illustration of the differences, please watch the full videos here. converted to pseudo videos through augmentations following Wu et al. (2022b). We resize all the training data to 512512 for training and evaluate under the original resolution. We freeze the CLIP encoder and VAE during training. We use AdamW (Loshchilov et al., 2019) for optimization with constant learning rate of 1e-6 in both stages. The training batch size is 4, and for each sample, we randomly load 8-frame video clip. We train our model using 4 NVIDIA 80GB A100 GPUs, and it takes about 1 week to finish the whole training process. 17 Figure C: Failure cases of REM on Ref-VPS. Our model still exhibits some object-centric bias and struggles with extremely dynamic entities such as lightning. Figure D: Class-wise scores (mIoU) concept coverage on BURST. As indicated by the arrows, Our method is robust on the long-tail region compared to other methods. 18 Method Mask/Box annotations Referformer MUTR VLMO-L UNINEXT VD-IT REM (Ours) RefCOCO/g/+, Ref-Youtube-VOS Ref-Youtube-VOS, AVS RefCOCO/g/+, Ref-Youtube-VOS Objects365, COCO, RefCOCO/g/+, GOT10K, LaSOT, TrackingNet, Youtube-VOS, BDD100K, VIS19, OVIS, Ref-YoutubeVOS RefCOCO/g/+, Ref-Youtube-VOS RefCOCO/g/+, Ref-Youtube-VOS Ref-Davis (J &F) 61.1 68.0 70.2 72.5 Ref-YTB (J &F) 62.9 68.4 67.6 70.1 69.4 72. 66.5 68.4 Table C: Comprehensive list of bounding/mask supervision used by all methods."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Toyota Research Institute",
        "University of Illinois at Urbana-Champaign"
    ]
}