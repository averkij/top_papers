{
    "paper_title": "Grounding Language in Multi-Perspective Referential Communication",
    "authors": [
        "Zineng Tang",
        "Lingjun Mao",
        "Alane Suhr"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce a task and dataset for referring expression generation and comprehension in multi-agent embodied environments. In this task, two agents in a shared scene must take into account one another's visual perspective, which may be different from their own, to both produce and understand references to objects in a scene and the spatial relations between them. We collect a dataset of 2,970 human-written referring expressions, each paired with human comprehension judgments, and evaluate the performance of automated models as speakers and listeners paired with human partners, finding that model performance in both reference generation and comprehension lags behind that of pairs of human agents. Finally, we experiment training an open-weight speaker model with evidence of communicative success when paired with a listener, resulting in an improvement from 58.9 to 69.3% in communicative success and even outperforming the strongest proprietary model."
        },
        {
            "title": "Start",
            "content": "Grounding Language in Multi-Perspective Referential Communication"
        },
        {
            "title": "Zineng Tang",
            "content": "Lingjun Mao University of California, Berkeley {terran, lingjun, suhr}@berkeley.edu"
        },
        {
            "title": "Alane Suhr",
            "content": "4 2 0 2 4 ] . [ 1 9 5 9 3 0 . 0 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We introduce task and dataset for referring expression generation and comprehension in multi-agent embodied environments. In this task, two agents in shared scene must take into account one anothers visual perspective, which may be different from their own, to both produce and understand references to objects in scene and the spatial relations between them. We collect dataset of 2,970 humanwritten referring expressions, each paired with human comprehension judgments, and evaluate the performance of automated models as speakers and listeners paired with human partners, finding that model performance in both reference generation and comprehension lags behind that of pairs of human agents. Finally, we experiment training an open-weight speaker model with evidence of communicative success when paired with listener, resulting in an improvement from 58.9 to 69.3% in communicative success and even outperforming the strongest proprietary model."
        },
        {
            "title": "Introduction",
            "content": "Language agents embodied in situated interactions alongside human users must be able to reason jointly about the space they occupy, the language they encounter, and their human partners perception. For example, consider home assistant robot that is assisting its human user in finding their lost keys. This system must take into account its previous and current observations of the space, as well as estimate what the users current perspective is like in the shared environment. If the system generates description of the keys location that the user clearly and unambiguously understands, they have achieved communicative success. Figure 1 shows an example of such communicative task, where one person describes the location of an object to another person, whose view differs from their own. To correctly resolve and generate references to the surrounding environment, both the Figure 1: Example scene from our environment and dataset. The center image shows the speaker on the left and the listener on the right with their respective fields of view (FOV). The speaker refers to the target object, distinguished by its blue color, and the listener selects the candidate referent they believe is described by the speakers description, without access to its distinct color. speaker and listener must take into account the physical relationship between objects, their own view of the environment, and an estimate of the other persons perspective in the environment. We study human-human and human-agent referential communication in photorealistic 3D environments, introducing platform that supports generating task instances with varying levels of difficulty. In contrast to most prior work on referring expression generation and comprehension, we focus on the setting where both agents are physically embodied in scene but with different perspectives of the scene. We collect dataset of 2,970 human-written referring expressions grounded in 1,485 generated scenes. We evaluate several recent vision-and-language models on the tasks of referring expression generation and comprehension, including general instruction-tuned visionlanguage models, models designed for fine-grained vision-language processing, and modular visionand-language reasoning system. When interpreting human-written referring expressions, the finegrained Ferret model (You et al., 2023) performs the best, successfully identifying 69.2% of intended referents. Using human listeners, we find that the proprietary GPT-4o produces referring expressions that correctly identify the intended target referent for 64.9% of scenes, while the open-weight LLaVA1.5 (Liu et al., 2024) is only successful for 55.7% of scenes. Compared to the average human-human success rate of 87.6%, all models lag far behind humans when both generating and comprehending referring expressions. Analyzing the language used by both automated and human speakers reveals significant differences in referential strategies; for example, human speakers use themselves or the listener agent as reference points much more frequently than automated models, which mostly rely on other objects in the scene. Our scene-generation platform supports controlling two levels of task difficulty. First, it supports modifying the relative orientation of the agents. Second, we train referent placement policy to minimize communicative success between two automated agents. For scenes generated using this policy, we see significant decrease in communicative success across nearly all agent combinations. Finally, we fine-tune our weaker speaker model, LLaVA-1.5 using data collected during deployment with both human and automated listeners. During learning, we first sample referring expressions from the speaker model, convert empirical observations of language interpretation by listener into training examples (Kojima et al., 2021), then apply proximal policy optimization to update model parameters on this data. We compare our fine-tuned models with GPT-4o, LLaVA-1.5, and human speakers. With single round of training and fewer than 200 sampled referring expressions, we see significant improvements in LLaVA-1.5s ability to generate accurate referring expressions, with rates of communicative success with human listener improving from 58.9 to up to 69.3, outperforming even the originally-stronger GPT-4o speaker. This demonstrates the strengths of learning from interaction to improve communicative success in multi-perspective referential communication. Our contributions are as follows: 1. platform for generating 3D scenes that encompass two-player referential communication game, enabling the study of multi-perspective referring expression generation and comprehension (Section 2). This platform supports controlling task difficulty through the placement of agents and referents. 2. new dataset of comprising 27,504 sampled scenes, and 2,970 human-written referring expressions grounded in 1,485 generated scenes (Section 3.1). 3. comprehensive analysis of humanand model-written referring expressions, and benchmarking and analysis of different vision and language models on their communicative success (Sections 3.2 and 4). 4. An approach for improving an open-source vision-language model on reference generation by learning from communicative success in interaction with human listener agents (Section 5). Our code, models, and dataset are released under an open-source license upon publication at the following URL: https: //github.com/zinengtang/MulAgentRef."
        },
        {
            "title": "2 Task and Environment",
            "content": "We study the task of embodied referential communication, where two agents coordinate their attention in shared scene using referring expressions. To this end, we design platform that for generating photorealistic 3D scenes that support this task at varying levels of difficulty."
        },
        {
            "title": "2.1 Embodied Referential Communication",
            "content": "We use reference game (Clark and Wilkes-Gibbs, 1986), where speaker describes target referent, and listener attempts to identify the target using the speakers description. In our task, two agents are physically embodied in the same shared 3D scene, but with different perspectives, and thus different observations of the scene. Each scene includes candidate referent objects, one of which is target object that the speaker needs to communicate to the listener. Communicative success is achieved if the listener is able to identify the speakers intended target. Formally, let be the set of possible agent observations, each represented as 2D image; be the set of candidate referents in an scene, and be the set of possible referring expressions. speaker model ps : RN {1 . . . } maps from an observation of the shared scene, set of referents, and the index of the target referent rt to distribution over possible referring expressions. listener model pl : RN {1...N } Figure 2: Example scenes generated with different relative orientations ( 180 on left, 0 on right) and with randomly- (top) or adversarially- (bottom) placed referents. Adversarially-generated referent configurations often space referents more evenly, with the target referent not easily uniquely identifiable. maps from its observation of the scene, the set of all candidate referents, and the referring expression generated by the speaker to distribution over possible referent indices. Given scene with speaker observation os O, listener observation ol O, set of candidate referents R, and target referent index t, communicative success is achieved when the listener selects the intended target: = arg max xX ˆt = arg max 1iN ps(x os, R, t) pl(i ol, R, x) Success(ps, pl, os, ol, R, t) = 1 t=ˆt ."
        },
        {
            "title": "2.2 Scene Generation",
            "content": "Formally, we denote scene = (e, ρs, ρl, R, t) as an environment populated with two agents ρs and ρl and referents R, as well as the index of the target referent rt. To generate scene, we first sample base environment, then place the two agents, then the candidate referents. Finally, we render each agents observation of the scene.1 1Appendix A.1 contains additional details about scene generation, including object placement and observation rendering. Base environments. We load indoor 3D environments from ScanNet++ (Yeshwanth et al., 2023) as 3D meshes into habitat-sim (Savva et al., 2019), which supports basic object physics and ray casting for identifying fields of view visible to each agent. Agent placement. Both the speaker and listener agents are associated with camera pose ρ = (x, y, z, θ, ϕ, ψ), where x, y, denote the position in 3D space and θ, ϕ, ψ represent the pitch, roll, and yaw angles respectively. To ensure observations are reasonable, we sample the camera height from range of typical adult human height, and fix pitch θ and roll ϕ at 0. We enforce maximum distance between the agent cameras, and non-empty overlap of their respective fields of view. We randomly assign speaker and listener roles to the two cameras, except in the case that only one agents camera is in the others field of view, but not vice versa. In this case, the former camera represents the speaker. Candidate referent placement. Each scene contains set of = 3 candidate referents = {r1, . . . , rN }, where ri = xi, yi, zi denotes the location of each referent. target index 1 denotes the referent that the speaker aims to communicate to the listener. For each referent, we first sample position from the set of all empty coordinates in the scene. We use gravitational physics simulation to drop the each referent from this position until it comes to rest on solid horizontal surface. We use rejection sampling to ensure all referents are visible to both agents, and referents are not too close together. Agent observations. Each agents observation is represented as 2D image R3HW rendered from its camera pose ρ. The speakers observation os = projs(e, R, t, ρs) is projection of the speakers view of the environment, and ol = projl(e, R, ρl) is projection of the listeners view. The camera field of view is 90 both vertically and horizontally. While projl renders each referent with the same color (red), projs renders the target rt in different color (blue) from the distractor objects, allowing the speaker to easily distinguish the target when writing their referring expression. Both projections also render the other agents camera as 3D model of human, which are sampled from 2K2K (Han et al., 2023)."
        },
        {
            "title": "2.3 Controlled Difficulty",
            "content": "We implement two ways to control the difficulty of referential communication via scene generation: by manipulating the relative orientation of speaker and listener, and by adversarially placing referents. Figure 2 shows examples of four scenes generated from different relative orientations, and with and without adversarial referent placement. Speaker-listener orientation. The relative orientation of the speaker ρs and listener ρl is the absolute difference ψ = min(ψs ψl, 360 ψs ψl) of their horizontal rotations (yaw). We experiment with the influence of ψ on interaction dynamics. When ψ is close to 0, the two agents are facing the same direction, and their observations are likely to be similar to one another. When ψ is close to 180, the agents are facing each other and thus have completely different views of the same scene. Following Schober (1993), we hypothesize that differences in relative angles of speakers and listeners may influence language use. Our environment supports uniformly sampling agent placements with fixed relative orientation. Adversarial placement of referents. We design referent placement policy model : Os PsPl RN {1...N }, which takes as input set of empty coordinates C, the speakers observation prior to referent placement, and both agent poses. It generates distribution over referent locations prior to the physics simulation, and over referent indices representing the target. The policy model is implemented as vision transformer (Dosovitskiy et al., 2020), and is trained to maximize the communicative failure rate between two fixed agent models, ˆps and ˆpl, by optimizing E(R,t)R() (cid:2)1 Success(ˆps, ˆpl, os, ol, R, t)(cid:3) , max where os and ol are the agents observations after referents are placed. During scene generation, we use the trained policy to sample initial positions of referents, then apply gravitational physics to find the resting position of each referent."
        },
        {
            "title": "3 Experimental Setup",
            "content": "We use our scene generation platform to evaluate embodied, multi-perspective referential communication with pairs of agents including humans and automated models."
        },
        {
            "title": "3.1 Data",
            "content": "We generate set of 27,504 scenes for training and evaluating automated agents. We recruit crowdworkers to participate in the task both as listeners and speakers, collecting dataset of 2,970 humanwritten referring expressions paired with human listener selections in 1,485 of these scenes. Scene generation. We use ScanNet++ (Yeshwanth et al., 2023) (non-commercial license), which contains 450 high-quality 3D indoor environments, as the basis of our task instances. We generate scenes using both forms of controlled difficulty (Section 2.3). First, we train our adversarial referent placement policy, implemented as ViT-s/16 (Dosovitskiy et al., 2020), using GPT-4o as both speaker and listener in 27,600 generated scenes comprising 60 samples per base environment.2 To generate our final dataset of scenes, we first sample 300 agent placements for each relative angle in {0, . . . , 180} distributed uniformly across the 450 base environments. For each of these agent placements, we sample two referent placements, resulting in two complete scenes: one where referent locations are randomly sampled, and another where referents are placed using the adversarial referent placement policy. 2Appendix A.2 contains more details on the adversary. We use GPT-4o to perform rejection sampling on low-quality scenes. Our scene rejection process targets scenes where communication tasks become impossible or highly unrealistic. This includes scenes where referents are invisible to both parties, the image fidelity is extremely low, or referents defy physics by floating or clipping through the environment. We do not reject scenes that are simply difficult, e.g., due to object placement. The final dataset includes 27,504 scenes, which we split into train (24,644 scenes), validation (1,485) and test (1,375) splits. The split is by scene instances. The validation split is used for ablating different dataset components or models, and the test split is to be used for testing final model performance. Base environments may appear in multiple splits. Crowdsourcing. We recruit 194 crowdworkers on Prolific3. Qualified workers are fluent English speakers, reside in the United States, and pass qualification task by writing referring expressions for 15 scenes, with successful listener selection from two or more of three other workers for at least 10 of these referring expressions. On average, we pay $18 USD per hour.4 Speaker task. Speakers are presented with prompt that asks them to describe the location of the blue ball to another person who is always visible to them in the scene, and who cannot distinguish the colors of the balls. We make the listener always visible to the speaker to allow them to take into account the listeners perspective of the scene when writing referring expression. Speakers first click button that reveals their view of the scene. They write referring expression, then submit their work. We record both the referring expression and the time taken between revealing the scene and submitting the task. Listener task. Listeners first click button that reveals their view of the scene and referring expression. They click on the referent they believe to be the target in the image, then submit their work. We record both the click position and the time taken between revealing the view and submitting the task. listeners selection is the sphere which is rendered closest to their click position. Dataset statistics. For randomly-sampled subset of 1,485 scenes from the validation set, we collect referring expression from at least one worker, resulting in total of 2,970 referring expressions, paired with judgments from three separate listeners. Each referring expression is labeled with the majority-class referent selection. The median time spent per speaker and listener task are 33.0s and 10.5s respectively. For all scenes, the speaker can see the listener; for 26% of scenes, the listener can see the speaker."
        },
        {
            "title": "3.2 Evaluated Models",
            "content": "We experiment with four instruction-tuned visionlanguage models.5 Two of these models are designed for more general use: GPT-4o6, proprietary model developed by OpenAI that supports real-time joint processing of audio, vision, and text; and LLaVA-1.5 (Liu et al., 2024), large open-weight instruction-tuned multimodal model. We also experiment with two instruction-tuned open-weight models designed specifically to refer to regions of and ground references in images at any granularity: Ferret (You et al., 2023) and Groma (Ma et al., 2024). Ferret employs hybrid region representation that combines discrete coordinates and continuous features to represent regions in an image, while Groma utilizes localized visual tokenization mechanism, where an image is decomposed into regions of interest and encoded into region tokens. We use these models as listeners only as preliminary experiments showed poor performance on reference generation. We also experiment with modular visionlanguage reasoning systems, which decompose the problems of language understanding and perception by first mapping language to some executable code, which is then executed on an image (Subramanian et al., 2023; Gupta and Kembhavi, 2023). In this work, we use ViperGPT (Surís et al., 2023), using GPT-4 to generate intermediate Python programs. We use ViperGPT as listener agent only. For both speaker models, we provide as input the speakers observation os and prompt to describe the location of the blue sphere. For listeners, we provide as input referring expression and the listeners observation ol, as well as list of each candidate referents bounding box, and prompt the model to select the bounding box corresponding to the described target. We sample from all models using temperature of 0. 3https://www.prolific.com 4Appendix A.3 contains details on on data collection. 5Additional details, including prompts, are available in Appendix B.1. 6https://openai.com/index/hello-gpt-4o/ Human Ran. Adv. GPT-4o LLaVA-1.5 Ran. Adv. Ran. Adv. Ran. Adv. Ferret Groma ViperGPT Ran. Adv. Ran. Adv. Listeners Speakers Human GPT-4o LLaVA-1.5 90.2 67.8 55.2 84.9 62.0 56.1 67.6 61.1 50. 66.0 57.2 49.8 63.3 60.4 44.7 63.2 57.8 42.2 70.1 67.8 59.1 68.2 62.1 52.8 64.3 66.5 61. 65.7 64.8 55.4 57.8 55.6 48.9 56.0 53.3 48.7 Table 1: Rates of communicative success for all four combinations of human and automated speakers and listeners, across 1,485 scenes, split by scenes with random (Ran.) and adversarial (Adv.) referent placement. Results for human-human pairs are bolded and in blue; results for human speakers and automated listeners are in orange; results for human listeners and automated speakers are in green; and results for fully-automated pairs are in black."
        },
        {
            "title": "3.3 Evaluation and Analysis",
            "content": "We evaluate models both as speakers and listeners, partnered both with human and automated agents. Our main metric is communicative success: for each scene, did the pair of agents successfully coordinate on the target referent? Pairing automated listeners with human speakers evaluates models ability to comprehend human-written referring expression, and pairing automated speakers with human listeners evaluates models ability to precisely refer to region of the scene. Both sides of this communicative task require understanding spatial language and taking into account the other agents perspective of the shared scene. For each setting, we analyze the influence of task difficulty on communicative success."
        },
        {
            "title": "4 Results",
            "content": "We experiment with four configurations of agent dyads, combining humans and automated speakers and listeners. Table 1 includes results for the 1,485 validation scenes we use for collecting humanhuman data, split across scenes with random and adversarial referent placement. Human speakers and listeners. Using the referring expressions collected in Section 3.1, we find that human-human pairs achieve an average communicative success rate of 87.6.7 Human speakers, automated listeners. We evaluate model performance in comprehending humanwritten referring expressions. For each humanwritten referring expression in our collected dataset, we select the most likely referent according to the model. We observe substantially lower accuracy in referent selection compared to human listeners. Ferret, which was designed for fine-grained visionand-language processing, outperforms the other 7For fair comparison to settings where only one referring expression is produced per scene, we report the macro-average over scenes. The micro-average over all referring expressions in this experiment is 88.4. models at an average selection accuracy of 69.2, but still lags far behind human performance. Automated speakers, human listeners. We acquire single referring expression from each instruction-tuned model for each evaluation scene. For each referring expression, we acquire three human listener selections and compare the majority class referent to the intended target. Both GPT-4o and LLaVA-1.5 are significantly less successful in describing target referents when compared to human speakers; GPT-4os references lead to correct human listener selection in 64.9% of scenes, while the LLaVA-1.5 speaker is successful for 55.7%. Automated speakers and listeners. We evaluate settings where both agents are automated systems. Using the referring expressions acquired from both speaker agents, we use all five listener models to perform referent selection. In nearly all cases, performance with pairs of automated listeners is lower than dyads containing at least one human. However, both Ferret and Groma perform on par with human listeners on referring expressions generated by both GPT-4o and LLaVA-1.5, for both random and adversarial referent configurations. In fact, both models actually outperform human listeners for referring expressions generated by LLaVA-1.5 for random referent configurations."
        },
        {
            "title": "4.1 Adversarial Referent Placement",
            "content": "Our adversarial referent placement policy was trained to minimize communicative success between GPT-4o speaker and listener. Table 1 shows that scenes generated with this policy indeed reduce rates of communicative success in this setting by 3.9%, statistically significant difference confirmed by paired t-test (p < 0.05). The learned policy also reduces the success rate for nearly all other combinations of agents, including for humanhuman pairs, where we see rates of communicative success drops from 91.6 to 85.1 when adversarially placing candidate referents. Figure 3: Analysis of referential strategies with respect to speaker agent type (top) and ranges of overlap in field of view (bottom). For each speaker agent or range of overlap, we plot the distribution over four referential strategies across all validation scenes. Within each referential strategy, we also report the proportion of generated references that guide human listener to successfully select the target reference."
        },
        {
            "title": "4.2 Language Analysis",
            "content": "We manually annotate 200 randomly-sampled referring expressions written by crowdworkers and GPT-4o with respect to referential strategies used by the speaker. Then, to scale to all validation data, we use GPT-4o to categorize referential strategy given in-context examples selected from these 200 examples. We consider four core referential strategies: reference to other candidate referents (e.g., in front of the other two red balls), reference to fixed objects in the scene (in front of the kitchen entryway), and reference to the listener (on your left) or speakers perspective (closest to me). Figure 3 (top) shows the prevalence of each referential strategy for both human and automated speakers in the validation set. Overall, our analysis shows that, compared to humans, automated models are more likely to refer to the targets relative position among objects in the scene, and much less frequently refer to its position with respect to the listeners view. This policy is detrimental to model performance: LLaVA especially fails to accurately refer to other objects in the scene when describing the target, with only 61.2% of such references resulting in communicative success. We also analyze the influence of view similarity between both agents on referential strategies and communicative success (Figure 3, bottom). We compute field of view overlap8 as proxy for view 8Field of view overlap is computed as the intersection over similarity. As the speakers observations become increasingly similar to the listeners, they tend to describe the target with respect to other candidate referents. As their views become dissimilar, speakers shift strategies to refer to targets with respect to other objects in the scene, and with respect to their own perspective (Schober, 1993)."
        },
        {
            "title": "5 Learning from Communicative Success",
            "content": "We propose to further train our speaker model from learning signals acquired during referential communication. The basic premise that motivates this approach is that empirical observations of language interpretation provides evidence of utterance meaning, regardless of speaker intent (Kojima et al., 2021). For instance, if the listener selects different referent than the intended target, this indicates the speakers referring expression describes (or at the very least, better describes) the chosen referent, even if the generated expression fails to describe the intended referent. In contrast to prior work that proposes methods that learn from communicative success (or failure) (Kojima et al., 2021; Liu et al., 2023), we additionally explore the use of preference-based learning signals that explicitly pair the intended and chosen targets in case of communicative failure. union of both agents view on each candidate referents surface. For example, if the speaker sees the front of sphere and the listener is positioned to see the back of it, the overlap will be very low. Overlap is averaged over all candidate referents. Learning. During training, we collect dataset of examples = (cid:8)(S (i), x(i), ˆt(i))(cid:9)M i=1, each consisting of generated scene (including the target referent index t), referring expression ps(os, R, t; θ) sampled from pre-trained speaker and the referent ˆt pl(ol, R, x; ϕ) selected by listener. We fine-tune speaker parameters θ using our collected dataset of examples D. We experiment with four methods for using the collected data: (a) contrastive learning (Radford et al., 2021), (b) learning from successes only (LSO), (c) creating positive examples from every example (Pos. Only), and (d) pairwise preference learning (PPL). In contrastive learning, for examples where = ˆt, we apply contrastive objective to jointly maximize the probability of given the chosen referent ˆt and minimize the probability of given the intended referent t. For all other methods, we use offline proximal policy optimization (PPO; Schulman et al., 2017), adjusting only the reward function. When learning from successes only, examples receive reward of +1 when = ˆt and 0 otherwise. To create positive examples from every example, we assign positive reward of +1 to each utterance paired with the listeners selection ˆt, which may or may not be equivalent to t. In pairwise preference learning, we take advantage of the fact that, especially in light of communicative failure, we can assume that the referring expression better describes the listeners guess than the speakers intended referent. We formalize this with reward function that maximizes the difference between the likelihoods of the speakers referring expression describing the listeners chosen target ˆt versus the intended target t: ps(x os, R, ˆt; θ) ps(x os, R, t; θ) . In cases where = ˆt, the assigned reward is +1. Finally, we also experiment with imitation learning, where we acquire human-written references. For each reference, we acquire three human listener selections. For each selection, we directly finetune the speaker model parameters to maximize the probability of the human reference conditioned on the scene and listener selection. Experimental setup. We use the initial speaker model, pre-trained LLaVA-1.5 (Liu et al., 2024), to generate referring expressions for 200 scenes sampled from the training split. We experiment with learning from both human and automated listener agents. We hypothesize that human listeners will provide higher-quality feedback in the form Speaker Listener Accuracy Avg. Ref. Val. Length Test Pre-trained θ + Contrastive (Da) + Contrastive (Dh) + LSO (Da) + LSO (Dh) + Pos. Only (Da) + Pos. Only (Dh) + PPL (Da) + PPL (Dh) + Imitation Learning Human GPT-4o 59.7 60.9 62.1 61.5 65.6 62.1 66.0 66.7 69.2 67. 91.3 66.3 58.9 69.3 68.2 90.6 67.1 61.1 45.8 55.7 41.7 54.6 46.7 57.2 19.8 15.6 16.8 15.8 78.9 Table 2: Performance of the LLaVA-1.5 speaker before and after training on data collected in 200 scenes with human and automated listeners, as well as performance of human and GPT-4o speakers on the same set of scenes. We also report the average reference length for each speaker. of referent selections than the automated listener model, given human listeners superior languageunderstanding capability. However, using an automated listener is less costly, as it requires collecting no additional human data. For our automated listener, we also use pre-trained LLaVA-1.5. We collect single guess per referring expression from our automated listener, and three human listener guesses. This results in two datasets: Da containing 200 examples of automated listener selections, and Dh containing 600 examples of human selections. Both datasets contain the same 200 sampled speaker references. Training results in eight models trained on model-generated references: for each of the training objectives (Contrastive, LSO, Pos. Only, and PPL), we learn from automated and human listener selections (Da and Dh). For the same 200 scenes, we also acquire one human-written referring expression and 3 listener selections for imitation learning. For evaluation, we acquire three human listener selections for generated referring expressions in randomly-sampled but representative subset 195 scenes from the validation set. For the bestperforming and baseline models, we also evaluate on our test set of 1,375 scenes. Results. Table 2 shows that learning from communicative success significantly improves the quality of an initially-weak speaker agent. Overall, learning from human listeners (Dh) is significantly more effective than learning from an automated listener, though this is still beneficial. We also find that preference learning (PPL) significantly9 improves over training only on examples exhibiting correct target selection. After fine-tuning on only 200 sampled referring expressions with human judgments and preference-based reward, LLaVA-1.5 actually outperforms GPT-4o as speaker, with communicative success rate of 69.3 when paired with human listeners. This approach also performs comparatively to imitation learning, which is more costly due to requiring human-written references. Manual analysis on the validation examples reveals that after training, the model generates fewer genuinely ambiguous descriptions (43.6 to 36.0% of analyzed descriptions), and shifts from referential strategy that increasingly refers to the listener (3.2 to 20.6%) or speaker (8.5 to 21.3%) perspectives. We also analyze how training influences sentence length: prior to training, LLaVA-1.5 produces lengthy descriptions at an average length of 61.1 tokens. For all training objectives, the fine-tuned model generates shorter expressions than the initial model. However, only after applying PPL-based learning does the sentence length decrease close to lengths of human references, without training on any human references."
        },
        {
            "title": "6 Related Work",
            "content": "The meanings of relative spatial terms are highly dependent on the situated environment: the items participating in the relation and their intrinsic parts and affordances (Clark, 1973; Landau, 2018); the relative perspectives of participants in an embodied scene (Taylor and Tversky, 1996; Goschler et al., 2008); and within-interaction conventions formed during multi-turn embodied dialogue (Schober, 1993), among other factors. In this work, we focus on the influence of relative perspective between multiple on the use of spatial language. Production and comprehension of referring expressions has been studied in human-human dialogue (Clark and Wilkes-Gibbs, 1986; Taylor and Tversky, 1996; van der Sluis and Luz, 2011; Udagawa et al., 2020, inter alia), and in interactions between human and automated language users (Janarthanam and Lemon, 2010; Fang et al., 2014, 2015; Huang et al., 2020, inter alia). However, most work has focused on disembodied referential communication, where agents tasked with communicating about sets of stimuli (Hawkins et al., 2017; 9Using paired t-test, we find < 0.05 when comparing LSO and PPL for both fine-tuning dataset and < 0.05 when comparing Pos. Only and PPL. Haber et al., 2019), or where agents are not physically situated within an environment (Kazemzadeh et al., 2014; Achlioptas et al., 2020). The problem of situated language grounding in multi-agent settings reflects an increasingly popular real-world scenario of embodied agents. In studies where interaction participants are both embodied with different visual perspectives on the same scene, they must either be literally physically embodied in single scene (Schober, 1993; Taylor and Tversky, 1996), or are placed in synthetic environments (Udagawa and Aizawa, 2019). small number of existing works have trained language-generation models using evidence of communicative success in interaction with another agent. For example, Kojima et al. (2021) train an instruction-generating agent by observing humans follow generated instructions, and Liu et al. (2023) use signals from reference games with automated listeners to improve speaker model. Our work takes inspiration from the latter to improve our speaker model using referent selections from an automated listener; however, we explore preferencebased objective that explicitly pairs the intended and empirically chosen referents."
        },
        {
            "title": "7 Conclusion",
            "content": "We study multi-agent referential communication in situated interactions. In this setting, speaker and listener are both embodied in shared scene, but are placed in different locations, with different views of the scene. We design platform that supports generation of photorealistic 3D scenes, with control for difficulty of the referential task. We evaluate both humans and automated agents as speakers and listeners in this task. While human-human dyads are successful at coordinating on referent around 88.4% of the time, automated models fall far behind when used both as speakers and as listeners. However, we can substantially improve the performance of an open-weight speaker model by training it with evidence of communicative success in referential communication with both automated and human listeners. Our findings suggest that despite the increasing relevance of multi-agent situated interactions between humans and automated agents, there is significant headroom for applying models that jointly process language and visual perception in this setting. However, they also show the promise of training such agents in interaction with people."
        },
        {
            "title": "Limitations",
            "content": "Our task currently focuses on single-shot reference, where speaker creates single referring expression, and the listener cannot ask for clarification or engage in interactive reference resolution (Clark and Wilkes-Gibbs, 1986; Udagawa and Aizawa, 2019). Evaluating how models participate in an interactive version of our task is compelling direction for future work. Additionally, while our experiments are currently conducted exclusively in English, the language of space and motion has enormous variation across language communities (Levinson and Wilkins, 2006). Core spatial concepts studied in English, like on or in, do not have universally uniform meanings, with different languages dividing the conceptual space of spatial language in vastly different ways (Landau, 2017). Future work should explore how spatial concepts and referential strategies vary across movement and non-static environment, multi-turn conversations, language features, and more complex scenarios. Finally, our experiments on learning from communicative success perform only single round of speaker deployment and training. Future work could perform further rounds of speaker deployment and listener judgments (i.e., as in Kojima et al., 2021; Suhr and Artzi, 2023), and analyze dynamics of language change in continual learning setting."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported by Young Investigator Grant from the Allen Institute for AI. We thank the Berkeley NLP group and the anonymous reviews for their advice and suggestions on our work."
        },
        {
            "title": "References",
            "content": "Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, and Leonidas Guibas. 2020. Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes. In Computer Vision ECCV 2020, pages 422440, Cham. Springer International Publishing. Herbert H. Clark. 1973. Space, time semantics and the child. In Timothy E. Moore, editor, Cognitive Development and Acquisition of Language, pages 2763. Academic Press, San Diego. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers arXiv preprint for image recognition at scale. arXiv:2010.11929. Rui Fang, Malcolm Doering, and Joyce Chai. 2014. Collaborative models for referring expression generation in situated dialogue. AAAI. Rui Fang, Malcolm Doering, and Joyce Y. Chai. 2015. Embodied collaborative referring expression generation in situated human-robot interaction. In HRI. Juliana Goschler, Elena Andonova, and Robert J. Ross. 2008. Perspective use and perspective shift in spatial dialogue. In Spatial Cognition VI. Learning, Reasoning, and Talking about Space, pages 250265, Berlin, Heidelberg. Springer Berlin Heidelberg. Tanmay Gupta and Aniruddha Kembhavi. 2023. Visual programming: Compositional visual reasoning without training. In CVPR. Janosch Haber, Tim Baumgärtner, Ece Takmaz, Lieke Gelderloos, Elia Bruni, and Raquel Fernández. 2019. The PhotoBook dataset: Building common ground through visually-grounded dialogue. In ACL. Sang-Hun Han, Min-Gyu Park, Ju Hong Yoon, Ju-Mi Kang, Young-Jae Park, and Hae-Gon Jeon. 2023. High-fidelity 3d human digitization from single 2k resolution images. In CVPR. Robert D. Hawkins, Mike Frank, and Noah D. Goodman. 2017. Convention-formation in iterated reference games. Cognitive Science. Jiani Huang, Calvin Smith, Osbert Bastani, Rishabh Singh, Aws Albarghouthi, and Mayur Naik. 2020. Generating programmatic referring expressions via program synthesis. In ICML. Srinivasan Janarthanam and Oliver Lemon. 2010. Adaptive referring expression generation in spoken dialogue systems: Evaluation with real users. In SIGDIAL. Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. 2014. ReferItGame: Referring to objects in photographs of natural scenes. In EMNLP. Noriyuki Kojima, Alane Suhr, and Yoav Artzi. 2021. Continual learning for grounded instruction generation by observing human following behavior. TACL, 9:13031319. Barbara Landau. 2017. Update on what and where in spatial language: new division of labor for spatial terms. Cognitive Science, 41(S2):321350. Herbert H. Clark and Deanna Wilkes-Gibbs. 1986. Referring as collaborative process. Cognition, 22(1):1 39. Barbara Landau. 2018. Learning simple spatial terms: Topics in Cognitive Science, Core and more. 12(1):91114. Ielka van der Sluis and Saturnino Luz. 2011. crosslinguistic study on the production of multimodal referring expressions in dialogue. In European Workshop on Natural Language Generation. Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. 2023. Scannet++: highfidelity dataset of 3d indoor scenes. In ICCV. Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. 2023. Ferret: Refer and ground anything anywhere at any granularity. arXiv preprint arXiv:2310.07704. S. C. Levinson and D. P. Wilkins. 2006. Grammars of space: Explorations in cognitive diversity. New York: Cambridge University Press. Andy Liu, Hao Zhu, Emmy Liu, Yonatan Bisk, and Graham Neubig. 2023. Computational language acquisition with theory of mind. In ICLR. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024. Improved baselines with visual instruction tuning. In CVPR. Chuofan Ma, Yi Jiang, Jiannan Wu, Zehuan Yuan, and Xiaojuan Qi. 2024. Groma: Localized visual tokenization for grounding multimodal large language models. Preprint, arXiv:2404.13013. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR. Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. 2019. Habitat: platform for embodied ai research. In ICCV, pages 93399347. Michael F. Schober. 1993. Spatial perspective-taking in conversation. Cognition, 47(1):124. John Schulman, Filip Wolski, Prafulla Dhariwal, ProxPreprint, Alec Radford, and Oleg Klimov. 2017. imal policy optimization algorithms. arXiv:1707.06347. Sanjay Subramanian, Medhini Narasimhan, Kushal Khangaonkar, Kevin Yang, Arsha Nagrani, Cordelia Schmid, Andy Zeng, Trevor Darrell, and Dan Klein. 2023. Modular visual question answering via code generation. In ACL. Alane Suhr and Yoav Artzi. 2023. Continual learning for instruction following from realtime feedback. In NeurIPS. Dídac Surís, Sachit Menon, and Carl Vondrick. 2023. ViperGPT: Visual inference via python execution for reasoning. CVPR. Holly A. Taylor and Barbara Tversky. 1996. Perspective in spatial descriptions. Journal of Memory and Language, 35(3):371391. Takuma Udagawa and Akiko Aizawa. 2019. natural language corpus of common grounding under continuous and partially-observable context. AAAI. Takuma Udagawa, Takato Yamazaki, and Akiko Aizawa. 2020. linguistic analysis of visually grounded dialogues based on spatial expressions. In Findings of EMNLP."
        },
        {
            "title": "A Data",
            "content": "A.1 Scene Generation Agent placement. We impose three constraints on agent placement to help more efficient scene generation pipeline: Maximum distance between the agents: Let dmax be the maximum allowed distance between the speaker and the listener. Denoting the positions of the speaker and listener as ρs and ρl, respectively, we require that ρs ρl dmax. We use dmax = 10. Field of view overlap: Let Fovs and Fovl be the fields of view of the speaker and listener, respectively. We require that the intersection of their fields of view is non-empty, i.e., Fovs Fovl = . Relative viewing angle: Let ψs and ψl be the horizontal viewing angles of the speaker and listener, respectively, relative to common reference direction. The relative viewing angle between the agents is given by ψ = min(ψsψl, 360ψsψl). We can place the agents with pre-set relative viewing angle by satisfying C0 ψ C1, where C0, C1 is the viewing angle difference bounds we set. ψ Referent placement. We impose three constraints on referents placement so they dont stack, become obstructed, or float in the air to meet real world physics standards: Visibility constraint: Let Viss and Visl be the sets of points visible from the speakers and listeners cameras, respectively. For each referent ri, we require that ri Viss Visl. Physically-based placement: Let , Y, be the sets of valid x, y, and coordinates within the environment bounds. For each referent ri, we randomly sample coordinates (xi, yi, zi) and drop the referent using gravitational physical simulation until it comes to rest on solid horizontal surface. Minimum distance: Let dmin be the minimum required distance between any two referents. For all pairs of referents ri and rj, where = j, we enforce rirj dmin. We use dmin = 0.3 . Scene rendering. Our environment supports rendering observations at different resolutions; e.g., we use = 720 and = 1280 for HD resolution. For environment generation, we use Quadro RTX 6000 for graphics rendering for single process. We parallelize data generation with Habitat-Sim with 4 Quadro RTX 6000. Scene rejection sampling. We use GPT-4v to discard low quality images rendering during the dataset generation. We use the following prompt: Please analyze the following image and provide score from 0 to 10 based on these criteria: The image must contain exactly 3 red spheres. If there are more or fewer than 3 red spheres, the score should be 0. The image should have high perceptual quality. Consider factors such as: Resolution: The image should be clear Lighting: and not pixelated or blurry. The have adequate lighting, without extreme darkness or overexposure. should image Focus: The subject of the image (the red spheres) should be in focus. Contrast: The image should have good contrast, allowing the red spheres to be easily distinguishable from the background. The image should not have any visible artifacts, such as: Compression artifacts: There should be no visible compression artifacts, such as blocky patterns or color banding. Noise: The image should not have excessive noise or graininess. Distortions: The image should not have any distortions, such as warping or stretching. A.2 Adversarial Referent Placement For each training iteration, the vision transformer (ViT-s/16) takes as input the speaker view, and the available object placement locations and speaker and listener locations processed as (x, y, z) coordinates flattened into normalized array. The model is trained to output the hard location from the input object placement locations as single-choice pipeline. A.3 Crowdsourcing For speakers and listeners we prompt the user to follow description and tutorial. When annotating, they still have access to the tutorial. They are provided the following task description: We engage participants in virtual environment where they assume the roles of Speaker and Listener. The task involves communication and spatial reasoning, requiring the Speaker to describe the location of specific objects within the environment, which are visible to them but not to the Listener. The Listener then interprets these descriptions to identify the objects accurately. Data collected from these interactions helps us understand the effectiveness of communication strategies and spatial language in varied settings. This study aims to improve collaborative tasks between humans and AI agents, enhancing how they interact within real-world environments. We qualify participants from the USA who are fluent in English. Users are informed their data will be used for research purposes. Our study is determined exempt from UC Berkeley CPHS. We manually check human data for non-conforming text. This step includes excluding private user information or offensive content."
        },
        {
            "title": "B Experiments",
            "content": "B.1 Experimental Setup the instruction-tuned vision and We prompt language models to output speaker and listener text. Except for the model-specific architecture input formatting. We use the following prompts: Speaker Prompt: Describe the location of the blue sphere relative to the environment features, relative to your view and the other persons view, and in contrast with other red spheres. Listener Prompt: An image filled with several identical red spheres and blue sphere. Your task is to identify the specific red sphere of interest from among several possible candidates. To detailed assist description highlighting unique characteristics or positions of the sphere. receive will you, you Your objective is to determine the precise location of this sphere in the image and mark it with bounding box. Consider factors such as lighting, reflections, shadows, relative position to other objects, and any unique attributes mentioned in the description. You should analyze how these details help to pinpoint the exact sphere among the identical ones. Once you have identified the sphere, outline its position using bounding box and provide its coordinates in the format: x0 (left), y0 (top), x1 (right), y1 (bottom) Additionally, explain your reasoning in detail for why you chose this specific location for the bounding box. For example: Based on the description, the sphere is near the window on the left side, and the distinct light reflection on its surface sets it apart from the others. This suggests its location as... , Bounding box coordinates: [0.23, 0.44, 0.30, 0.46]. Be aware that the description might offer different viewpoint of the scene, so be prepared to adjust your analysis accordingly. Choose [candidate bounding boxes] Format for Response: Reasoning for location choice: [Your detailed explanation here] Bounding box coordinates: [x0, y0, x1, y1] to Feel free observations or helped you make the distinction. incorporate contrasting nuanced that following elements bounding boxes: from any the B."
        },
        {
            "title": "Influence of Speaker Visibility",
            "content": "Listeners Human GPT-4o Visible Not Visible Visible Not Visible Speakers Human GPT-4o 87.5 65.8 86.1 65.4 67.2 60.4 66.0 59.2 Table 3: Influence of speaker visibility to listener on listener performance. In 26% of generated scenes, the speaker is visible to the listener agent. We find that for human speakers, the visibility of the speaker significantly (though only slightly) increases communicative success (p < 0.01 using paired t-test), while the difference is not significant for GPT-4o based speakers. B.3 Error Example We analyze the frequency of several common communication errors in collaborative tasks involving both human and automated speakers interacting with human listeners, with varying degrees of task difficulty. Out-of-context reference is when speaker reference context that is not in listeners view;. Perspective misalignment is when speaker reference its own perspective which will change drammatically when switched to listeners perspective. Ambiguity is that speaker expression can resolve to different meanings according to views. Relative position error is when the speaker expression describes wrong relative position like to the left of. Expression error is simply wrong expression. Misunderstanding Figure 4: Impact of task difficulty on communication errors between speaker and listener for Human, GPT, LLaVA speakers. rather than next to the table, further complicating accurate communication. B.4 View Overlap Analysis Figure 5: LLaVA speaker example that leads to incorrect listener selection. is when the speaker expression is unambiguously correct but listener fails to resolve it. The results are presented in Fig 4. It is evident that the error frequency in collaborations involving LLaVA speakers is generally higher than other speakers. Most errors are predominantly out-of-context reference, perspective misalignment, and ambiguity. For example, in Figure 5, LLaVA mistakenly reference objects that are not in the view of the listener. The impact of facing angles and distances on communication is also significant. We find that errors are most prevalent when the listener and speaker are facing each other at angles between 120-180 degrees. In these situations, directional terms such as left and right often become inverted, especially when speakers fail to clarify whose perspective is being used. Moreover, with the visibility of both parties, speaker might use human as reference point, but the listener typically assumes human refers to the speaker, leading to selections in the opposite direction. Additionally, as the distance between speaker and listener increases, the descriptions provided by speakers tend to become more vague, opting for broader reference points such as on the left side of the wall Figure 6: Overlap of object and distribution of correct listener selection. We perform analysis on speaker and listener view overlap, which is calculated by the percentage of objects area seen by speaker and listener. We use logistic regression on individual data points with likelihood ratio test (LRT) both p-values<0.001. And we calculate accuracy over 0.02 interval of buckets on the overlap percentage for the scatter plot and Chi-Square test with p-value<0.05. Higher overlap usually means speaker and listener have close view pose and position. We can see from the plot that for both adversarial and random placements, as the view overlap increases, the performance is better. B.5 AI Assistants Usage When conducting this research, we use AI to enchance our coding efficiency and quality. We use ChatGPT 10 and Claude.ai11 to assist in writing 10https://chat.openai.com/ 11https://claude.ai code for dataset generation and the human study website server."
        }
    ],
    "affiliations": [
        "University of California, Berkeley"
    ]
}