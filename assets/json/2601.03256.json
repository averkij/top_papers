{
    "paper_title": "Muses: Designing, Composing, Generating Nonexistent Fantasy 3D Creatures without Training",
    "authors": [
        "Hexiao Lu",
        "Xiaokun Sun",
        "Zeyu Cai",
        "Hao Guo",
        "Ying Tai",
        "Jian Yang",
        "Zhenyu Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Muses, the first training-free method for fantastic 3D creature generation in a feed-forward paradigm. Previous methods, which rely on part-aware optimization, manual assembly, or 2D image generation, often produce unrealistic or incoherent 3D assets due to the challenges of intricate part-level manipulation and limited out-of-domain generation. In contrast, Muses leverages the 3D skeleton, a fundamental representation of biological forms, to explicitly and rationally compose diverse elements. This skeletal foundation formalizes 3D content creation as a structure-aware pipeline of design, composition, and generation. Muses begins by constructing a creatively composed 3D skeleton with coherent layout and scale through graph-constrained reasoning. This skeleton then guides a voxel-based assembly process within a structured latent space, integrating regions from different objects. Finally, image-guided appearance modeling under skeletal conditions is applied to generate a style-consistent and harmonious texture for the assembled shape. Extensive experiments establish Muses' state-of-the-art performance in terms of visual fidelity and alignment with textual descriptions, and potential on flexible 3D object editing. Project page: https://luhexiao.github.io/Muses.github.io/."
        },
        {
            "title": "Start",
            "content": "Muses: Designing, Composing, Generating Nonexistent Fantasy 3D Creatures without Training Hexiao Lu1,2 Xiaokun Sun1, Zeyu Cai1, Hao Guo2, Ying Tai1, Jian Yang1, Zhenyu Zhang1 1Nanjing University Equal contribution 2China Agricultural University Corresponding Author luhexiao@cau.edu.cn, xiaokun sun@smail.nju.edu.cn, caizeyu010612@gmail.com guohaolys@cau.edu.cn, {yingtai, csjyang}@nju.edu.cn, zhangjesse@foxmail.com 6 2 0 2 6 ] . [ 1 6 5 2 3 0 . 1 0 6 2 : r Figure 1. Generated nonexistent fantastic 3D creatures including animals, humanoids, and fictional characters by Muses. Driven by 3D skeleton, Muses is able to design basic 3D structures, compose different concepts, and generate high-fidelity creative 3D assets. Although the content comes from different sources, the generated creatures contain harmonious geometry and textures across different styles."
        },
        {
            "title": "Abstract",
            "content": "We present Muses, the first training-free method for fantastic 3D creature generation in feed-forward paradigm. Previous methods, which rely on part-aware optimization, manual assembly, or 2D image generation, often produce unrealistic or incoherent 3D assets due to the challenges of intricate part-level manipulation and limited out-of-domain generation. In contrast, Muses leverages the 3D skeletona fundamental representation of biological formsto explicitly and rationally compose diverse elements. This skeletal foundation formalizes 3D content creation as 1 structure-aware pipeline of design, composition, and generation. Muses begins by constructing creatively composed 3D skeleton with coherent layout and scale through graphconstrained reasoning. This skeleton then guides voxelbased assembly process within structured latent space, integrating regions from different objects. Finally, imageguided appearance modeling under skeletal conditions is applied to generate style-consistent and harmonious texture for the assembled shape. Extensive experiments establish Muses state-of-the-art performance in terms of visual fidelity and alignment with textual descriptions, and potential on flexible 3D object editing. Project page: https: //luhexiao.github.io/Muses.github.io/. 1. Introduction Creating 3D content continues to draw significant attention due to its broad applications in vision. Traditional approaches recover 3D structure from images via the graphics pipelines, such as shape-from-shading [32, 73], multi-view geometry [13, 67], and analysis-by-synthesis with 3D morphable models (3DMM) [1, 33]. Recently, the rise of generative AI [43], advances in 3D representations [18, 34, 52], and the availability of large-scale 3D object datasets [6, 7] have driven substantial progress in 3D content creation. Existing 3D content creation methods mainly fall into three categories: distilling 2D generative priors into optimized 3D representations [29, 39, 75]; synthesizing 2D multi-view images followed by 3D reconstruction [16, 54, 65]; and training feed-forward models on large-scale 3D datasets to directly generate 3D content [12, 20, 55, 58]. These approaches can produce 3D objects that align with text to some extent or infer plausible geometry and texture from images. However, when the target is highly creativefor example, creature composed of tiger body, dragon wings, quadrupedal robot legs, nine fox tails, and an argali headthey often fail due to the limited ability to compose and control such complex content. Intuitively, there are two main strategies to compose different contents and generate fantastic 3D creatures: utilizing part-level operations, or depending on 2D complex image creation. The first exploits part-aware knowledge, using part affinity [22] or part-level generation and composition [2, 3, 62, 66] to fuse content and concepts. However, these methods face two limitations: controlling part granularity for object design is intricate, and even when individual 3D parts are obtained, blending them into coherent wholeespecially at the interfacesremains challenging. As illustrated in Fig. 2-(a, c), using part affinity for optimization [22] cannot produce realistic 3D creature, and performing part-level generation and composition results in significant artifacts. The second strategy first produces creative 2D images [11, 42, 47, 56] from different contents, and Figure 2. Compared with (a) methods that distill part-level affinity from 2D generative priors [22], (b) methods that lift creative 2D images [56] to 3D [58], and (c) methods that perform part-level generation [66] (where (c) is obtained by manually assembling the generated parts), Muses generates creatures that better preserve creative intent and achieve higher structural coherence. then applies image-to-3D generation with feed-forward model [58]. This approach is highly sensitive to the quality of the generated images and is constrained by the scale of 3D data, making it struggle to maintain realism and harmony in the produced 3D models of complex concepts or nonexistent objects. Results in Fig. 2-(b) demonstrate such limitation. Therefore, developing methods that can generate entirely new 3D content with creative concepts remains an urgent and important open problem. In this paper, we propose Muses, novel framework to generate nonexistent fantastic 3D creatures with trainingfree paradigm. Instead of relying on part-level processing or 2D image creation, we unleash the power of 3D skeleton that serves as foundation representation of any 3D creature. By modifying the 3D skeletons, we bridge the design and generation of complex creative 3D creature in feedforward approaches, enhancing the realism of geometry and texture. Concretely, given prompts of objects from which we want to extract concepts for creation, we employ Trellis [58] to obtain 3D assets and an auto-rigging solution to generate corresponding skeletons. Building on these, our fantastic 3D creature creation follows design-composegenerate paradigm. We propose graph-constrained LLM reasoning method to design creative 3D skeleton with reasonable layout and scale. Based on the created skeleton, we perform voxel-based content assembling on structured 3D latent, suitably fusing the skeletal regions from different objects. Finally, we propose an image-guided appearance modeling approach to generate style-consistent and harmonious 3D texture for more flexible creation. As illustrated in the teaser image and Fig. 2, Muses is able to generate diverse and high-quality 3D fantastic creatures. In summary, our contributions are as follows: We propose Muses, training-free framework that gener2 ates highly creative and fantastic 3D objects with inherent skeletal structures, composed of concepts from different creatures. Muses can be well adapted to different modern 3D generation models. We propose novel skeleton-based method that designs and composes skeletal structures for nonexistent creatures. Based on such basic structures, we then propose geometry and texture generation modules that produce reasonable, harmonious, and style-consistent 3D creatures. Extensive qualitative and quantitative evaluations demonstrate that Muses attains superior generation fidelity and efficiency compared to state-of-the-art methods. 2. Related works 2.1. 3D object generation Rapid advances in 2D diffusion models have catalyzed progress in 3D generation. Foundational work such as DreamFusion [38] and SJC [51] introduced Score Distillation Sampling (SDS) to optimize parametric 3D representations under pretrained 2D generative prior, and subsequent methods [24, 26, 39, 53, 75] improved fidelity, diversity, speed, and multi-view consistency. Nevertheless, SDS-based approaches still face well-known challenges, including the multi-face Janus phenomenon and substantial per-instance optimization time. line of workincluding MVDream [46], Wonder3D [31], Instant3D [21], and SyncDreamer [30]casts 3D generation as multi-view image synthesis. With the release of Objaverse [7] and ObjaverseXL [6], native 3D generative frameworks have accelerated: methods like VecSet [70] and related work [4, 17, 20, 23, 71, 74] encode point clouds as vector-set tokens, while TRELLIS [58] and subsequent efforts [4, 25, 57, 68] represent 3D assets as sparse voxels. Despite these advances, current feed-forward pipelines remain inadequate for generating genuinely out-of-distribution concepts. 2.2. Part-aware 3D generation Representing 3D shapes in semantic parts is valuable for both shape analysis and synthesis. PartGen [2] and PhyCAGE [60] employ multi-view diffusion models for segmenting and completing compositional 3D objects. HoloPart [64], PartCrafter [27], and PartPacker [50] leverage DiT-based generative models to achieve part-level synthesis. Frankenstein [59] compresses SDFs into latent triplane space via VAE. CoPart [8] and BANG [72] introduce 3D box conditions to reduce part ambiguity. Building on Trellis [58], OmniPart [66] autoregressively produces variable-length sequences of bounding boxes to guide part synthesis, while AutoPartGen [3], based on VecSet [70], sequentially generates coherent object parts. MeshCoder [5] feeds point clouds to an LLM and translates them into executable scripts to enhance semantic part understanding. Notably, DreamBeast [22] trains 3D part-affinity representation to generate part-aware fantastical animals. However, it supports only three parts and is bottlenecked by the perinstance optimization cost of SDS. Despite these advances, part granularity remains difficult to control, and direct part composition often yields suboptimal results. 2.3. Creative 2D image generation More recently, text-to-image models have introduced tasks specifically targeting creativity. Pioneering conceptlearning methods such as Textual Inversion [14] and DreamBooth [44] require multiple images to encode single visual concept. Most subsequent approaches [10, 11, 15, 36, 37, 45, 56] learn concepts at the image or subject level; for example, pOps [41] and IP-Composer [9] work in CLIP space to enable compositional generation. Growing interest has shifted toward finer-grained concept learning: PartCraft [35] decomposes images into components for selective recombination; Piece-it-Together [42] derives representation from IP-Adapter+ [69] to integrate subset of components into coherent concept; and Chimera [47] broadens the element taxonomy. However, combining finegrained components often yields misaligned or inconsistent composites. Moreover, when lifting such creative 2D images into 3D, it remains challenging to achieve geometrically reasonable and visually harmonious results. In contrast to prior work that relies on part-level assembly or 2D-driven creativity, Muses mines 3D skeletal information to generate fully realized nonexistent fantasy 3D creatures, capturing the essence of creature structure. 3. Preliminaries Native 3D Generation Models can be divided into two families: VecSet-based and structured-latent (SLAT)-based approaches. Relative to VecSet models, SLAT methodssuch as Trellis [58] and Hi3DGen [68]offer more explicit representations and better decoupling characteristics. Accordingly, without loss of generality, this work adopts the pioneering Trellis [58] framework as the backSLAT is defined as = {(zi, pi)}L i=1, where bone. zi RC denotes local latent at the corresponding active voxel position pi {0, 1, . . . , 1}3, where is the grid resolution and 3. The inference pipeline is organized into two sequential stages. In the first stage, transformer-based generator TS produces low-resolution grid RDDDCS . This grid is then passed to latent feature decoder DS, which reconstructs dense occupancy grid {0, 1}N N . The binary grid is subsequently converted into sparse structural representation {pi}L i=1. In the second stage, another transformer TL takes {pi}L i=1 as input and predicts the corresponding latent features {zi}L i=1 with fine-grained geometry and texture. As discussed in Sec. 1, directly creating non-existent creatures 3 Figure 3. Overview of Muses. Our framework automates fantastic creature generation through 3D skeleton-driven pipeline of design, composition, and generation. Given text prompt, Stage parses it into concepts, generates corresponding 3D assets {X}M m=1 and m=1, and uses graph classification with LLM-guided reasoning to produce text-aligned skeleton G. In Stage skeletons {G = (V, E)}M II, this skeleton guides part assembly in structured latent space (SLAT), yielding composed latent code Z. In Stage III, is decoded into coarse 3D creature X, which guides geometry-invariant texture editing and undergoes final style-conscious refinement to produce the detailed, harmonious output X. The entire pipeline is automatic, training-free, and feed-forward. with Trellis leads to suboptimal results. To address this, we link the design-to-generation process via an explicit skeleton structure, which will be discussed in the following. 4. Methodology In this section, we primarily describe the proposed Muses method without training. Given textual prompts that contain descriptions of fantastic creature, our aim is to generate nonexistent fantasy creatures. The pipeline first designs skeletal structure and composes SLAT representations according to the creative concept, and then generates styleconsistent texture while keeping the geometry unchanged. As illustrated in Fig.3, the process consists of three stages: skeleton-guided concept design (in Sec. 4.1), SLAT-based content composition (in Sec. 4.2), and style-consistent texture generation (in Sec. 4.3). 4.1. Skeleton-guided concept design Existing design methods often struggle to control semantic granularity in part-level settings or to maintain consistency when lifting 2D creative concepts to 3D realizations. In contrast, we ground our design process in explicit skeletal representations. We first introduce graph-based heuristic skeleton classification, and then propose LLM-guided skeleton assembly to construct controllable, 3D-consistent creature structures. It is worth noting that our method is skeleton-based and can therefore handle broad range of skeletonized categories, including animals, humans, robots, and virtual characters. However, it is not suitable for abstract objects that cannot be formalized with skeleton. Graph-based skeleton classification: As illustrated in Fig. 4, given 3D assets {X}M m=1 and their associated skeletons {G = (V, E)}M m=1 where is the number of creaFigure 4. Skeleton-guided concept design. tures mentioned in the prompt C, Rv3 is joint positions and Ne2 represents topological bone connections, our goal is to structurally classify them into body, wings, legs, head, and tail. As illustrated in Fig. 4, to eliminate small or isolated branches (e.g., claws, antlers), we first perform graph processing step consisting of connectedcomponent analysis, redundant-node removal, and path optimization, yielding cleaned skeleton (cid:101)G. We then estimate the dominant orientation δ of which is later used for symmetry tests and assembly. Below, we perform semantic decomposition using set of heuristic rules. Since the anatomical root is usually at or near the pelvis, we choose the beginning node: = (bx, by, bz) = r, arg max uN (r) if deg(r) 3, otherwise, deg(u), (1) 4 From leaf nodes Vleaf , we further select Vlow = {v Vleaf vy < by} as leg candidates. By jointly considering symmetry about δ and relative height in axis, these candidate paths are labeled as Gleg. path whose endpoint is approximately centered along δ and extends posteriorly is Gtail. To detect the trunk junctioni.e., the node where the body connects to head and forelimbs/wingswe search along δ from for node = arg min vV,deg(v)>=4 v, ˆδ, ˆδ = δ (cid:13)δ(cid:13) (cid:13) (cid:13) (2) The path from to is Gbody. Branches emanating from forming symmetric pair in the plane orthogonal to are Gwing or Gleg. Any remaining branch from is Ghead. This rule set induces partition for typical articulated skeletons (animals and humanoids). For fish-like shapes, we use the degenerate assignment Gbody = (cid:101)G. LLM-guided assembly reasoning: Given dism=1 = continuous discrete skeleton each {Gbody, Gleg, Gwing, Gtail, Ghead}, where is sub-skeleton extracted from (cid:101)G, together with the corresponding orientation = {δm}M m=1, our goal is to generate geometrically consistent layout. To cover typical spatial manipulations, we define three primitive editing operators acting on target skeleton ˆG: = {G}M Rot( ˆG; θ) [Rotate, ˆG, θ], Trans( ˆG; t, λ) [Translate, ˆG, t, λ], Scale( ˆG; α) [Scale, ˆG, α]. (3) where θ, t, λ, α are respectively the rotation angle, translation direction, translation distance, and scale factor. An LLM (e.g., Qwen-Plus [63]) is given the natural-language assembly request and structured attributes of each candidate skeletoncategory, position, size, and orientation. The model is instructed to infer the connection relation and decompose the request into sequence of primitive operators of the above form. We further allow multiplicity constraints from the prompt C. If contains count (e.g., two heads), the LLM instantiates copies of the same skeleton structure and places them symmetrically. In this way, the whole process becomes mapping, fLLM : ( G, , C) = { Gbody, Gleg, Gwing, Gtail, Ghead}. 4.2. SLAT-based content composition As shown in Fig.3, instead of manually stitching part-level assets like X-Part [61], we leverage the created skeleton to establish an explicit correspondence between skeletal structure and the SLAT representation. Concretely, we first perform region extraction via skinning weights to map skeletal segments to their geometric support, and then propose SLAT-based content assembly, so that the final composition remains structurally aligned with the skeleton while inheriting the controllability of SLAT. Skeleton-to-SLAT Region Mapping: Given the reasoned skeleton G, we first predict skinning weight matrix RQJ , where is the number of vertices in the mesh = {xi R3}Q i=1 and is the number of bones. Each entry W[i, j] measures the influence of joint on mesh vertex xi, which makes suitable for skeleton-to-region mapping. We aggregate and normalize joint-level weights into region-level weights (cid:102)W RQ G: (cid:102)W[i, ℓ] = max W[i, j] (cid:80) (cid:16)(cid:80) ℓ=1 j:Gℓ (cid:80) j:Gℓ W[i, j], ε (cid:17) , ε = 1012 (4) where Gℓ G. For SLAT {pi}L i=1, we find for each pi its nearest mesh vertices Nk(pi) in and then calculate normalized inverse-distance weights βi,s = αi,s s=1 αi,s (cid:80)k , αi,s = 1 (cid:13)pi xis max((cid:13) (cid:13) (cid:13)2, εd) (5) βi,s (cid:102)W[is, ℓ]. Thus each SLAT inherits region-aware where = 1, . . . , k, εd > 0. Finally, we transfer the region-level weights in (4) to SLAT-level weights WSLAT RL via weighted averaging WSLAT[i, ℓ] = (cid:80) s=1 weight aligned with the skeleton-defined semantic regions. Voxel-based geometric and texture interpolation: Considering that direct interpolation in the explicit 643 SLAT space still leaves some large combination gaps due to the sparsity of active voxels, we instead perform interpolation in the more compact 163 voxel space S. For gaps that arise when combining different regions, we perform linear interpolation on voxels S, weights WSLAT, and features {zi}L i=1, ensuring smooth and coherent geometry, as well as relatively harmonious texture across the regions. When multiple regions occupy the same voxel, we merge the corresponding weights and features: zcomp = (cid:88) i= wi zi = (cid:80)n i=1 wi zi (cid:80)n j=1 wj , (cid:88) i= wi = 1 (6) where is the number of overlapping parts at particular voxel. We then decode the resulting representation = i=1 to generate coarse creature X. {(z i)}L i, 4.3. Style-consistent texture generation As illustrated in Fig.3, the composed fantasy creature is geometrically reasonable but visually rigid, especially in terms of color coherence and surface detail. Hence, we introduce style-consistent texture generation approach that harmonizes appearance across all regions, enhancing realism and expanding the diversity of attainable visual styles. 5 Figure 5. Comparison with the state-of-the-art methods. Note that DreamBeast [22] cannot handle contents with more than three animals, and OmniPart [66] requires manual stitching. Our method generates fantastic 3D creatures with superior quality and textual alignment. Geometrically invariant texture editing: As the appearance modeling depends on the given 2D image, providing geometrically-aligned style image contributes to finer texture generation. To this end, we propose geometrically invariant texture editing method, which produces reasonably styled and harmonious textures. Given the coarse creature X, we render the optimal frame from as the reference image for texture editing. Based on the conditioning image I, we guide the texture generation process using the FLUX.1 Kontext model [19]: FLUX Kontext(I, Cpos, Cneg, γ) (7) Where Cpos and Cneg are the positive and negative prompt embeddings respectively, and γ represents other parameters. Cpos and Cneg primarily emphasize preserving the geometric structure of the input while generating texture that aligns with specific artistic style, such as mythological style, Studio Ghibli style, and Steampunk style. Style-conscious creative generation: Given the edited image and the coarse geometry {p i=1, we obtain the SLAT {z i=1 using the second-stage TL: }L i}L = {(z , i )}L i=1 TL(I , {p i}L i=1) (8) After decoding the latent z, we obtain refined creature 6 Table 1. Comparison with the state-of-the-art methods. Method DreamBeast [22] GaussianDreamer [49] UNO [56] + Trellis [58] Trellis-Text-to-3D [58] OmniPart [66] Ours (full) CLIP VQA 0.2450 0.2287 0.2386 0.2432 0.2690 0. 0.4948 0.5009 0.5085 0.7565 0.8151 0.9254 Visual fidelity 6.15 2.27 1.94 10.36 12.62 66.67 Text alignment 0.63 1.27 0.32 2.54 9.84 85.40 Decoder(z), which now incorporates aesthetically pleasing textures that are geometrically consistent with X. 5. Experiment 5.1. Setup Implementation details: Our framework is built upon the Trellis [58] backbone, using classifier-free guidance scale of 5.0 and sampling steps of 25, and all experiments are conducted on single NVIDIA RTX A6000 GPU. Skeleton generation and skinning weight prediction follow Puppeteer [48], using block depth of 1. We use Qwenplus [63] for LLM-guided assembly reasoning, and FLUX.1 Kontext [19] for style-image editing. Under this setup, single instance can be generated in under one minute. Evaluation Protocol: We evaluate Muses from both automatic and human perspectives. To assess textimage alignment, we randomly select 30 samples and evaluate CLIPFigure 6. Ablation study of Muses. We observe that each component of our method contributes to performance. The assets without LLM-guided design contain regions of improper scale, unreasonable orientation or position. The skinning weight and geometry/texture interpolation guarantee suitable composed results without holes or artifacts. Without geometrically-invariant texture editing, the final appearance patterns fail to correspond to the semantic regions, producing artifacts or unreasonable texture. Without style-consistent generation, the coarse 3D creature contains disharmonious contents from source assets. In contrast, our full method produces high-fidelity fantastic 3D creatures, and the appearance can be flexibly edited. Score [40]. However, CLIPScore is known to be less reliable when the textual input contains highly compositional or combinatorial descriptions. To compensate for this limitation, we further use VQAScore [28]. In addition, we conduct user study to capture human preferences. We randomly sample 10 examples and present them to 60 volunteers. Participants are asked to rate the generation quality and textimage consistency of each result, and choose the method they prefer among competing approaches. More details on the protocol can be found in Appendix. 5.2. Comparison with the state-of-the-art In this section, we compare our method with four categories of state-of-the-art approaches: (i) methods that distill 2D generative priors into 3D space (DreamBeast [22] and DreamGaussian [49]); (ii) methods that first synthesize creative 2D imagery and then lift it to 3D (UNO [56] + Trellis [58]); (iii) feedforward 3D generation models that directly output 3D content in single pass (Trellis-Textto-3D [58]); and (iv) part-level generation methods that decompose objects into components (OmniPart [66]). Since part-level methods can only decompose an object into parts but cannot automatically assemble components, we manually assemble the decomposed outputs of OmniPart. The quantitative comparisons in Tab. 1 show that our method consistently outperforms all baselines across all metrics. Fig. 5 further provides qualitative analysis. DreamBeast and DreamGaussian fail to produce realistic 3D creatures; DreamBeast is additionally limited to at most three animal species, and both SDS-based methods incur substantial perinstance optimization cost. UNO + Trellis is highly sensitive to the quality of the generated 2D images, so unsuitable compositions directly degrade the resulting 3D assets. Even when the 2D images are plausibly combined, the corresponding 3D results are often unsatisfactory because the targets lie far outside the training distribution. The Trellis Text-to-3D model is even less capable of aligning with complex, highly compositional descriptions. OmniPart frequently fails to correctly decompose animal assets (e.g., separating the body and head), and the subsequent combination of parts requires manual assembly, which is timeIn contrast, our method consuming and labor-intensive. produces fantastic 3D creatures that are well aligned with the input descriptions and exhibit significantly better fidelity and structural harmony. We adopt UNO [56] as our creative 2D generator due to its strong concept-fusion capability; comparisons with alternative 2D generators (e.g., FLUX [19]) are provided in the Appendix. 5.3. Ablation study We conduct detailed ablation studies in Fig. 6 and Tab. 2 to validate the effectiveness of each component in Muses. 7 Table 2. Ablation study of Muses. VQA1: VQA-Score based on CLIP-FlanT5. VQA2: VQA-Score based on ShareGPT4V. Method w/o LLM reasoning w/o skinning weight w/o interpolation w/o geometrically invariant w/o style consistency ours (full) CLIP VQA1 VQA2 0.7311 0.6967 0.2573 0.7081 0.7090 0.2664 0.7366 0.7326 0.2695 0.7075 0.7990 0.2532 0.7902 0.8359 0.2806 0.8496 0.9254 0. LLM-guided design: We compare (i) directly connecting different sub-skeletons according to graph rules (without LLM reasoning), and (ii) generating layout with our LLM-guided assembly reasoning. As shown in Tab. 2 and the red circles in Fig. 6, although the rule-based method can follow the skeleton topology, it lacks semantic awareness of region proportions and relative orientations. Skeleton-to-SLAT region mapping: We compare two strategies for mapping skeletal structures to SLAT: (i) using skinning weights and (ii) directly assigning regions based on nearest-neighbor distances (without skinning). As illustrated in Tab. 2 and the green circles in Fig. 6, although nearest-neighbor mapping is computationally efficient, it is prone to overand under-segmentation. Geometric and texture interpolation: We also study the effects of interpolation at junctions. The baseline without interpolation means interpolation directly in the explicit 643 SLAT space. We observe that such simple stitching of extracted regions tends to produce visible seams, voids, and misalignments in the purple circles of Fig. 6. Style-consistent texture generation: The baseline without geometrically-invariant texture editing means that we use the given description of fantastic creatures to directly generate an image for texture editing. The baseline without style consistency corresponds to using the coarse generated 3D creature X. The results in Tab. 2 and the pink and yellow circles in Fig. 6 show degraded performance without style-consistent texture generation. 5.4. Application We show two potential applications beyond creating novel 3D creature models. The first one is 3D editing based on skeleton-aware design. As illustrated in Fig. 7-(a), designing the skeleton backbone, our method is able to perform natural and disentangled part-level 3D editing while keeping other regions unchanged. The second one is texture editing. Leveraging our geometrically invariant texture editing method, we generate different styled images based on the rendered image of to perform structure-aligned appearance changing. As illustrated in Fig. 7-(b), we can flexibly and harmoniously edit assets with various kinds of styles. Figure 7. Applications of geometry and texture editing. Figure 8. Failure cases. 5.5. Limitation The limitations of our method mainly come from two aspects: failed generations of Trellis and improper initialization of the 3D skeleton. As illustrated in Fig. 8-(a), Trellis cannot generate realistic 3D model of peacock, so we struggle to extract meaningful skeletal parts or content from such 3D peacock. As illustrated in Fig. 8-(b), Puppeteer fails to generate reasonable 3D skeleton, so we cannot perform the design stage. Note that these failure cases can be resolved by more powerful 3D generation methods and skeleton modeling approaches. 6. Conclusion In this paper, we propose Muses, training-free framework for generating fantasy 3D creatures. By leveraging the 3D skeletonan essential representation of biological formsMuses composes diverse elements following designcomposegenerate paradigm. Combining skeletonguided design, SLAT-based content composition, and styleconsistent texture generation, Muses produces 3D creatures that are both geometrically coherent and visually striking. Extensive experiments on challenging compositional prompts demonstrate that Muses achieves state-of-the-art 8 performance in visual fidelity and textual alignment. In the future, we aim to expand Muses into flexible 3D object editing tool, further unlocking new possibilities for interactive applications in gaming, virtual reality, and animation."
        },
        {
            "title": "References",
            "content": "[1] Volker Blanz and Thomas Vetter. morphable model for the synthesis of 3d faces. In Seminal Graphics Papers: Pushing the Boundaries, Volume 2, pages 157164. 1999. 2 [2] Minghao Chen, Roman Shapovalov, Iro Laina, Tom Monnier, Jianyuan Wang, David Novotny, and Andrea Vedaldi. Partgen: Part-level 3d generation and reconstruction with In Proceedings of the Commulti-view diffusion models. puter Vision and Pattern Recognition Conference, pages 58815892, 2025. 2, 3 [3] Minghao Chen, Jianyuan Wang, Roman Shapovalov, Tom Monnier, Hyunyoung Jung, Dilin Wang, Rakesh Ranjan, Iro Laina, and Andrea Vedaldi. Autopartgen: AutogresarXiv preprint sive 3d part generation and discovery. arXiv:2507.13346, 2025. 2, 3 [4] Yiwen Chen, Zhihao Li, Yikai Wang, Hu Zhang, Qin Li, Chi Zhang, and Guosheng Lin. Ultra3d: Efficient and highfidelity 3d generation with part attention. arXiv preprint arXiv:2507.17745, 2025. 3 [5] Bingquan Dai, Li Ray Luo, Qihong Tang, Jie Wang, Xinyu Lian, Hao Xu, Minghan Qin, Xudong Xu, Bo Dai, Haoqian Wang, et al. Meshcoder: Llm-powered structured mesh code generation from point clouds. arXiv preprint arXiv:2508.14879, 2025. 3 [6] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: universe of 10m+ 3d objects. Advances in Neural Information Processing Systems, 36:3579935813, 2023. 2, 3 [7] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: In Proceedings of universe of annotated 3d objects. the IEEE/CVF conference on computer vision and pattern recognition, pages 1314213153, 2023. 2, [8] Shaocong Dong, Lihe Ding, Xiao Chen, Yaokun Li, Yuxin Wang, Yucheng Wang, Qi Wang, Jaehyeok Kim, Chenjian Gao, Zhanpeng Huang, et al. From one to more: ContexIn Proceedings of the tual part latents for 3d generation. IEEE/CVF International Conference on Computer Vision, pages 82308240, 2025. 3 [9] Sara Dorfman, Dana Cohen-Bar, Rinon Gal, and Daniel Cohen-Or. Ip-composer: Semantic composition of visual concepts. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 111, 2025. 3 [10] Fu Feng, Yucheng Xie, Xu Yang, Jing Wang, and Xin Geng. Distribution-conditional generation: From class distribution to creative generation. arXiv preprint arXiv:2505.03667, 2025. 3 mantic understanding of creative generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1844418454, 2025. 2, 3 [12] Jiashi Feng, Xiu Li, Jing Lin, Jiahang Liu, Gaohong Liu, Weiqiang Lou, Su Ma, Guang Shi, Qinlong Wang, Jun Wang, et al. Seed3d 1.0: From images to high-fidelity simulationready 3d assets. arXiv preprint arXiv:2510.19944, 2025. 2 [13] Yasutaka Furukawa, Carlos Hernandez, et al. Multi-view stereo: tutorial. Foundations and trends in Computer Graphics and Vision, 9(1-2):1148, 2015. [14] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 3 [15] Daniel Garibi, Shahar Yadin, Roni Paiss, Omer Tov, Shiran Zada, Ariel Ephrat, Tomer Michaeli, Inbar Mosseri, and Tali Dekel. Tokenverse: Versatile multi-concept personalization in token modulation space. ACM Transactions On Graphics (TOG), 44(4):111, 2025. 3 [16] Zehuan Huang, Yuan-Chen Guo, Haoran Wang, Ran Yi, Lizhuang Ma, Yan-Pei Cao, and Lu Sheng. Mv-adapter: Multi-view consistent image generation made easy. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1637716387, 2025. 2 [17] Team Hunyuan3D, Shuhui Yang, Mingxin Yang, Yifei Feng, Xin Huang, Sheng Zhang, Zebin He, Di Luo, Haolin Liu, Yunfei Zhao, et al. Hunyuan3d 2.1: From images to highfidelity 3d assets with production-ready pbr material. arXiv preprint arXiv:2506.15442, 2025. 3 [18] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 2 [19] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742, 2025. 6, [20] Zeqiang Lai, Yunfei Zhao, Haolin Liu, Zibo Zhao, Qingxiang Lin, Huiwen Shi, Xianghui Yang, Mingxin Yang, Shuhui Yang, Yifei Feng, et al. Hunyuan3d 2.5: Towards highfidelity 3d assets generation with ultimate details. arXiv preprint arXiv:2506.16504, 2025. 2, 3 [21] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. arXiv preprint arXiv:2311.06214, 2023. 3 [22] Runjia Li, Junlin Han, Luke Melas-Kyriazi, Chunyi Sun, Zhaochong An, Zhongrui Gui, Shuyang Sun, Philip Torr, and Tomas Jakab. Dreambeast: Distilling 3d fantastical animals with part-aware knowledge transfer. In 2025 International Conference on 3D Vision (3DV), pages 12431252. IEEE, 2025. 2, 3, 6, 7 [11] Fu Feng, Yucheng Xie, Xu Yang, Jing Wang, and Xin Geng. Redefining creative in dictionary: Towards an enhanced se- [23] Weiyu Li, Jiarui Liu, Hongyu Yan, Rui Chen, Yixun Liang, CraftsXuelin Chen, Ping Tan, and Xiaoxiao Long. 9 man3d: High-fidelity mesh generation with 3d native genarXiv preprint eration and interactive geometry refiner. arXiv:2405.14979, 2024. 3 [24] Zongrui Li, Minghui Hu, Qian Zheng, and Xudong Jiang. Connecting consistency distillation to score distillation for text-to-3d generation. In European Conference on Computer Vision, pages 274291. Springer, 2024. 3 [25] Zhihao Li, Yufei Wang, Heliang Zheng, Yihao Luo, and Bihan Wen. Sparc3d: Sparse representation and construction for high-resolution 3d shapes modeling. arXiv preprint arXiv:2505.14521, 2025. 3 [26] Yixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, and Yingcong Chen. Luciddreamer: Towards highfidelity text-to-3d generation via interval score matching. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 65176526, 2024. 3 [27] Yuchen Lin, Chenguo Lin, Panwang Pan, Honglei Yan, Yiqiang Feng, Yadong Mu, and Katerina Fragkiadaki. Partcrafter: Structured 3d mesh generation via compoarXiv preprint sitional arXiv:2506.05573, 2025. 3 latent diffusion transformers. [28] Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan. Evaluating text-to-visual generation with image-to-text generation. In European Conference on Computer Vision, pages 366384. Springer, 2024. 7 [29] Fangfu Liu, Junliang Ye, Yikai Wang, Hanyang Wang, Zhengyi Wang, Jun Zhu, and Yueqi Duan. Dreamreward-x: Boosting high-quality 3d generation with human preference alignment. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. 2 [30] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from single-view image. arXiv preprint arXiv:2309.03453, 2023. 3 [31] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 99709980, 2024. 3 [32] Guoyu Lu. Shading meets motion: Self-supervised indoor 3d reconstruction via simultaneous shape-from-shading and In Proceedings of the Computer structure-from-motion. Vision and Pattern Recognition Conference, pages 16508 16519, 2025. 2 [33] Yuxi Mi, Zhizhou Zhong, Yuge Huang, Qiuyang Yuan, Xuan Zhao, Jianqing Xu, Shouhong Ding, Shaoming Wang, Rizen Guo, and Shuigeng Zhou. Data synthesis with diverse styles for face recognition via 3dmm-guided diffusion. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2120321214, 2025. [34] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 2 [35] Kam Woh Ng, Xiatian Zhu, Yi-Zhe Song, and Tao Xiang. In European Partcraft: Crafting creative objects by parts. Conference on Computer Vision, pages 420437. Springer, 2024. 3 [36] Gaurav Parmar, Or Patashnik, Kuan-Chieh Wang, Daniil Ostashev, Srinivasa Narasimhan, Jun-Yan Zhu, Daniel CohenObject-level visual prompts Or, and Kfir Aberman. arXiv preprint for compositional arXiv:2501.01424, 2025. 3 image generation. [37] Or Patashnik, Rinon Gal, Daniil Ostashev, Sergey Tulyakov, Kfir Aberman, and Daniel Cohen-Or. Nested attention: Semantic-aware attention values for concept personalization. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers. Association for Computing Machinery, 2025. 3 [38] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. 3 [39] Yiming Qin, Zhu Xu, and Yang Liu. Apply hierarchicalchain-of-generation to complex attributes text-to-3d generIn Proceedings of the Computer Vision and Pattern ation. Recognition Conference, pages 1852118530, 2025. 2, 3 [40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [41] Elad Richardson, Yuval Alaluf, Ali Mahdavi-Amiri, and Daniel Cohen-Or. pops: Photo-inspired diffusion operators. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 112, 2025. 3 [42] Elad Richardson, Kfir Goldberg, Yuval Alaluf, and Daniel Cohen-Or. Piece it together: Part-based concepting with ippriors. arXiv preprint arXiv:2503.10365, 2025. 2, 3 [43] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2 [44] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF conference generation. on computer vision and pattern recognition, pages 22500 22510, 2023. 3 [45] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instantbooth: Personalized text-to-image generation without test-time finetuning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 85438552, 2024. [46] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512, 2023. 3 [47] Shivam Singh, Yiming Chen, Agneet Chatterjee, Amit Raj, James Hays, Yezhou Yang, and Chitra Baral. Chimera: 10 Compositional image generation using part-based concepting. arXiv preprint arXiv:2510.18083, 2025. 2, 3 [48] Chaoyue Song, Xiu Li, Fan Yang, Zhongcong Xu, Jiacheng Wei, Fayao Liu, Jiashi Feng, Guosheng Lin, and Jianfeng Zhang. Puppeteer: Rig and animate your 3d models. arXiv preprint arXiv:2508.10898, 2025. 6 [49] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024, 2024. 6, [50] Jiaxiang Tang, Ruijie Lu, Zhaoshuo Li, Zekun Hao, Xuan Li, Fangyin Wei, Shuran Song, Gang Zeng, Ming-Yu Liu, and Tsung-Yi Lin. Efficient part-level 3d object generation via dual volume packing. arXiv preprint arXiv:2506.09980, 2025. 3 [51] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond Yeh, and Greg Shakhnarovich. Score jacobian chaining: Lifting In Propretrained 2d diffusion models for 3d generation. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1261912629, 2023. 3 [52] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. arXiv preprint arXiv:2106.10689, 2021. 2 [53] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in neural information processing systems, 36: 84068441, 2023. 3 [54] Hao Wen, Zehuan Huang, Yaohui Wang, Xinyuan Chen, and Lu Sheng. Ouroboros3d: Image-to-3d generation via 3daware recursive diffusion. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 21631 21641, 2025. 2 [55] Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Jingxi Xu, Philip Torr, Xun Cao, and Yao Yao. Direct3d: Scalable image-to-3d generation via 3d latent diffusion transformer. Advances in Neural Information Processing Systems, 37:121859121881, 2024. 2 [56] Shaojin Wu, Mengqi Huang, Wenxu Wu, Yufeng Cheng, Fei Ding, and Qian He. Less-to-more generalization: Unlocking more controllability by in-context generation. arXiv preprint arXiv:2504.02160, 2025. 2, 3, 6, [57] Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Yikang Yang, Yajie Bao, Jiachen Qian, Siyu Zhu, Xun Cao, Philip Torr, et al. Direct3d-s2: Gigascale 3d generation arXiv preprint made easy with spatial sparse attention. arXiv:2505.17412, 2025. 3 [58] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2146921480, 2025. 2, 3, 6, 7 [59] Han Yan, Yang Li, Zhennan Wu, Shenzhou Chen, Weixuan Sun, Taizhang Shang, Weizhe Liu, Tian Chen, Xiaqiang Dai, Chao Ma, et al. Frankenstein: Generating semanticIn SIGGRAPH compositional 3d scenes in one tri-plane. Asia 2024 Conference Papers, pages 111, 2024. 3 [60] Han Yan, Mingrui Zhang, Yang Li, Chao Ma, and Pan Ji. Phycage: Physically plausible compositional 3d asset generation from single image. arXiv preprint arXiv:2411.18548, 2024. 3 [61] Xinhao Yan, Jiachen Xu, Yang Li, Changfeng Ma, Yunhan Yang, Chunshi Wang, Zibo Zhao, Zeqiang Lai, Yunfei Zhao, Zhuo Chen, and Chunchao Guo. X-part: high fidelity and structure coherent shape decomposition, 2025. [62] Xinhao Yan, Jiachen Xu, Yang Li, Changfeng Ma, Yunhan Yang, Chunshi Wang, Zibo Zhao, Zeqiang Lai, Yunfei Zhao, Zhuo Chen, et al. X-part: high fidelity and structure coherent shape decomposition. arXiv preprint arXiv:2509.08643, 2025. 2 [63] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 5, 6 [64] Yunhan Yang, Yuan-Chen Guo, Yukun Huang, Zi-Xin Zou, Zhipeng Yu, Yangguang Li, Yan-Pei Cao, and Xihui Liu. Holopart: Generative 3d part amodal segmentation. arXiv preprint arXiv:2504.07943, 2025. 3 [65] Yuxiao Yang, Xiaoxiao Long, Zhiyang Dou, Cheng Lin, Yuan Liu, Qingsong Yan, Yuexin Ma, Haoqian Wang, Zhiqiang Wu, and Wei Yin. Wonder3d++: Cross-domain diffusion for high-fidelity 3d generation from single imIEEE Transactions on Pattern Analysis and Machine age. Intelligence, 2025. 2 [66] Yunhan Yang, Yufan Zhou, Yuan-Chen Guo, Zi-Xin Zou, Yukun Huang, Ying-Tian Liu, Hao Xu, Ding Liang, YanPei Cao, and Xihui Liu. Omnipart: Part-aware 3d generation with semantic decoupling and structural cohesion. arXiv preprint arXiv:2507.06165, 2025. 2, 3, 6, 7 [67] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. Mvsnet: Depth inference for unstructured multi-view stereo. In Proceedings of the European conference on computer vision (ECCV), pages 767783, 2018. [68] Chongjie Ye, Yushuang Wu, Ziteng Lu, Jiahao Chang, Xiaoyang Guo, Jiaqing Zhou, Hao Zhao, and Xiaoguang Han. Hi3dgen: High-fidelity 3d geometry generation from images via normal bridging. arXiv preprint arXiv:2503.22236, 3:2, 2025. 3 [69] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 3 [70] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: 3d shape representation for neural fields and generative diffusion models. ACM Transactions On Graphics (TOG), 42(4):116, 2023. 3 [71] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM Transactions on Graphics (TOG), 43(4):120, 2024. 3 11 [72] Longwen Zhang, Qixuan Zhang, Haoran Jiang, Yinuo Bai, Wei Yang, Lan Xu, and Jingyi Yu. Bang: Dividing 3d assets via generative exploded dynamics. ACM Transactions on Graphics (TOG), 44(4):121, 2025. [73] Ruo Zhang, Ping-Sing Tsai, James Edwin Cryer, and Mubarak Shah. Shape-from-shading: survey. IEEE transactions on pattern analysis and machine intelligence, 21(8): 690706, 2002. 2 [74] Zibo Zhao, Wen Liu, Xin Chen, Xianfang Zeng, Rui Wang, Pei Cheng, Bin Fu, Tao Chen, Gang Yu, and Shenghua Gao. Michelangelo: Conditional 3d shape generation based on shape-image-text aligned latent representation. Advances in neural information processing systems, 36:7396973982, 2023. 3 [75] Jiahao Zhu, Zixuan Chen, Guangcong Wang, Xiaohua Xie, and Yi Zhou. Segmentdreamer: Towards high-fidelity textto-3d synthesis with segmented consistency trajectory distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1586415874, 2025. 2,"
        }
    ],
    "affiliations": [
        "China Agricultural University",
        "Nanjing University"
    ]
}