{
    "paper_title": "D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied AI",
    "authors": [
        "Suwhan Choi",
        "Jaeyoon Jung",
        "Haebin Seong",
        "Minchan Kim",
        "Minyeong Kim",
        "Yongjun Cho",
        "Yoonshik Kim",
        "Yubeen Park",
        "Youngjae Yu",
        "Yunsung Lee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models leverage internet-scale text data, yet embodied AI remains constrained by the prohibitive costs of physical trajectory collection. Desktop environments -- particularly gaming -- offer a compelling alternative: they provide rich sensorimotor interactions at scale while maintaining the structured observation-action coupling essential for embodied learning. We present D2E (Desktop to Embodied AI), a framework that demonstrates desktop interactions can serve as an effective pretraining substrate for robotics embodied AI tasks. Unlike prior work that remained domain-specific (e.g., VPT for Minecraft) or kept data proprietary (e.g., SIMA), D2E establishes a complete pipeline from scalable desktop data collection to verified transfer in embodied domains. Our framework comprises three components: (1) the OWA Toolkit that unifies diverse desktop interactions into a standardized format with 152x compression, (2) the Generalist-IDM that achieves strong zero-shot generalization across unseen games through timestamp-based event prediction, enabling internet-scale pseudo-labeling, and (3) VAPT that transfers desktop-pretrained representations to physical manipulation and navigation. Using 1.3K+ hours of data (259 hours of human demonstrations, and 1K+ hours of pseudo-labeled gameplay), we achieve a total of 96.6% success rate on LIBERO manipulation and 83.3% on CANVAS navigation benchmarks. This validates that sensorimotor primitives in digital interactions exhibit sufficient invariance to transfer meaningfully to physical embodied tasks, establishing desktop pretraining as a practical paradigm for robotics. We will make all our work public, including the OWA toolkit, datasets of human-collected and pseudo-labeled, and VAPT-trained models available at https://worv-ai.github.io/d2e/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 4 8 6 5 0 . 0 1 5 2 : r a"
        },
        {
            "title": "Under Review",
            "content": "D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied AI Suhwan Choi MAUM.AI Jaeyoon Jung MAUM.AI Haebin Seong MAUM.AI Minchan Kim MAUM.AI"
        },
        {
            "title": "Minyeong Kim\nStanford University",
            "content": "Yongjun Cho MAUM.AI Yoonshik Kim MAUM.AI Yubeen Park MAUM.AI Youngjae Yu Seoul National University Yunsung Lee MAUM.AI"
        },
        {
            "title": "Abstract",
            "content": "Large language models leverage internet-scale text data, yet embodied AI remains constrained by the prohibitive costs of physical trajectory collection. Desktop environmentsparticularly gamingoffer compelling alternative: they provide rich sensorimotor interactions at scale while maintaining the structured observation-action coupling essential for embodied learning. We present D2E (Desktop to Embodied AI), framework that demonstrates desktop interactions can serve as an effective pretraining substrate for robotics embodied AI tasks. Unlike prior work that remained domainspecific (e.g., VPT for Minecraft) or kept data proprietary (e.g., SIMA), D2E establishes complete pipeline from scalable desktop data collection to verified transfer in embodied domains. Our framework comprises three components: (1) the OWA Toolkit that unifies diverse desktop interactions into standardized format with 152 compression, (2) the Generalist-IDM that achieves strong zero-shot generalization across unseen games through timestamp-based event prediction, enabling internet-scale pseudo-labeling, and (3) VAPT that transfers desktop-pretrained representations to physical manipulation and navigation. Using 1.3K+ hours of data (259 hours of human demonstrations, and 1K+ hours of pseudo-labeled gameplay), we achieve total of 96.6% success rate on LIBERO manipulation and 83.3% on CANVAS navigation benchmarks. This validates that sensorimotor primitives in digital interactions exhibit sufficient invariance to transfer meaningfully to physical embodied tasks, establishing desktop pretraining as practical paradigm for robotics. We will make all our work public, including the OWA toolkit, datasets of human-collected and pseudo-labeled, and VAPT-trained models available at https://worv-ai.github.io/d2e/"
        },
        {
            "title": "Introduction",
            "content": "Large-scale datasets have driven recent progress in large language models (LLMs) (Kaplan et al., 2020; Hoffmann et al., 2022), where pretraining on internet-scale resources enables strong generalization across diverse downstream tasks. In contrast, embodied AI has yet to experience such scaling breakthrough. Unlike text, which can be collected from the web with minimum effort, embodied trajectories demand specialized hardware, costly human operation, and complex pipelines for annotation (Mandlekar et al., 2019; Qin et al., 2023; Fu et al., 2024; Cheng et al., 2024; Park et al., 2024). As result, most existing datasets remain relatively small, domain-specific, and fragmented across incompatible formats (Geng et al., 2025), preventing the emergence of true data flywheel for embodied AI. Equal contribution. Corresponding authors"
        },
        {
            "title": "Under Review",
            "content": "Figure 1: Overview of D2E framework. (1) The OWA Toolkit captures 335.6 hours of rich desktop demonstrations across 31 games with 152 compression. (2) The Generalist-IDM uses nextevent prediction with temporal offset (NEP-τ) to achieve OOD generalization, enabling pseudolabeling of 1K+ hours of YouTube gameplay. (3) Vision-Action Pretraining transfers desktoppretrained representations to embodied AI, achieving 96.6% success on LIBERO manipulation and 83.3% on CANVAS navigation benchmarks which demonstrates desktop-to-robotics transfer. Desktop interactionsscreen, keyboard, and mouseoffer compelling alternative for scaling vision-action learning (Baker et al., 2022; Raad et al., 2024). These interfaces are standardized, human-centric, and naturally abundant: millions of users generate rich interaction trajectories through everyday digital activities. Crucially, desktop environments preserve the tight observation-action coupling essential for embodied learning while abstracting away hardware-specific constraints (Tang et al., 2025; Shridhar et al., 2020; Raad et al., 2024). Gaming interactions, in particular, exhibit complex sensorimotor patternsnavigation, object manipulation, strategic planningthat mirror many embodied AI challenges, yet are freely shared at internet scale through gameplay videos. We introduce D2E (Desktop to Embodied AI), framework that systematically transforms desktop interactions into scalable pretraining substrate for embodied AI. D2E addresses two fundamental challenges: establishing unified pipeline for high-quality desktop data collection, and extending beyond manual annotations to leverage the vast repository of unlabeled internet videos. Our first contribution, the Open-World Agents (OWA) Toolkit, provides the infrastructure for scalable desktop data capture. Built on Windows APIs and GStreamer (Microsoft Corporation; GStreamer Team), OWAs ocap recorder synchronizes multimodal streamsscreen, keyboard, and mouseinto time-aligned events, while our OWAMcap format achieves order-of-magnitude compression improvements over existing formats. Through OWA, we collected 335 hours of human demonstrations across 31 diverse games and applications, establishing foundation for desktop-based pretraining. Beyond human demonstrations, we introduce the Generalist Inverse Dynamics Model (Generalist-IDM) to demonstrate pathway toward internet-scale data collection. By reformulating action prediction as timestamp-aware next-event prediction (NEP-τ ), our model achieves strong zero-shot generalizationsubstantially outperforming specialist baselines on unseen games with minimal compute requirements. This generalization capability enables automatic pseudo-labeling of YouTube gameplay videos, expanding our dataset by over 1, 000 hours without additional human annotation. We demonstrate that desktop-pretrained representations transfer meaningfully to physical robotics through Vision-Action PreTraining (VAPT). Models pretrained on our combined desktop corpus show consistent improvements on standardized benchmarks: It achieves total success rate of 96.6% on LIBERO manipulation (Liu et al., 2023) and 83.3% on CANVAS navigation (Choi et al., 2024). These results establish, for the first time,"
        },
        {
            "title": "Under Review",
            "content": "that the sensorimotor patterns learned from desktop interactions can directly enhance performance in embodied AI domains, validating desktop data as practical alternative to costly physical data collection. Our contributions are threefold: 1. OWA Toolkit: framework that contains ocap for synchronized event recording with FHD/QHD 60 Hz support, OWAMcap format for compact storage, and an optimized data pipeline for ML trainingachieving up to 152 compression and 41 lower average disk read per image compared to TorchCodec; used to collect 335 hours of human demonstrations. 2. Generalist-IDM: An inverse dynamics model that outperforms game-specific Specialist IDMs, exhibiting out-of-domain generalization and in-context adaptation (e.g., calibrating mouse scale). Trained on OWA-collected data with around 192 H100-hours ( $800), the strong generalization of Generalist-IDM allows us to pseudo-label over 1K+ hours of YouTube gameplay. 3. VAPT foundation model: vision-action pretrained model trained on 1.3K hours of desktop data from OWA and Generalist-IDM pseudo-labeling, transferring desktop knowledge to robotics. VAPT achieves 96.6% success on manipulation (LIBERO) and 83.3% on navigation (CANVAS)."
        },
        {
            "title": "2 Related Work",
            "content": "Collecting Data for Vision-Action Pretraining. Large-scale vision-action (or visionlanguage-action) pretraining depends on multimodal corpora that pair perception with grounded actions across diverse tasks (Kaplan et al., 2020; Hoffmann et al., 2022). Recent embodied agents unify perception and control in single model across heterogeneous domains (Reed et al., 2022; Firoozi et al., 2024; Wen et al.). In robotics, resources are emerging: RT-1 (Brohan et al., 2022) and RT-2 (Zitkovich et al., 2023) scale visionlanguageaction to real robots; Open X-Embodiment aggregates heterogeneous datasets to train RT-X models (ONeill et al., 2024); and LeRobot (Cadene et al., 2024) lowers the barrier to collecting and reusing real-world datasets. Despite this progress, assembling real-robot interaction at meaningful scale remains challenging because of fragmented tooling, hardware overhead, and safety constraints (Xing et al., 2025; Park et al., 2024; Geng et al., 2025). Similarly, desktop interfaces lack open, standardized corpora and toolkits, bottlenecking vision-action pretraining (Tang et al., 2025; Chen et al., 2025). VPT (Baker et al., 2022) offers human-annotated and pseudo-labeled Minecraft trajectories but remains single-domain, while SIMA (Raad et al., 2024) demonstrates cross-game generalization through unified interface yet keeps data proprietary. PLAICraft (He et al., 2025) advances multimodal Minecraft logging, but these efforts are environment-specific; broad cross-application generalization requires unified schemas that cover diverse desktop applications (McCarthy et al., 2025). Unlike prior single-domain or proprietary efforts, we contribute open, unified, multi-game desktopaction dataset (31 games; 335h) and an open-source toolkit, explicitly validated for transfer to embodied tasks. Inverse Dynamics Models. Agents observe the states up to time 1 and predict the action at time t. In contrast, Inverse Dynamics Models (IDMs) condition on surrounding statespast and futureto infer the action taken at time t. IDMs have been pivotal for scaling imitation learning to Internet-scale datasets, serving as pseudo-labelers for otherwise unlabeled action data (Ye et al., 2024; Bjorck et al., 2025). In robot manipulation, UniPi (Du et al., 2023) explores text-guided video generation to couple language grounding with policy learning, and LAPA (Ye et al., 2024) shows that latent action pretraining from videos can improve scalability and robustness. On the desktop side, VPT (Baker et al., 2022) trained Specialist IDM on human-annotated Minecraft trajectories and used it to pseudo-label thousands of hours of Minecraft gameplay on YouTube. We demonstrate the potential of Generalist-IDM, spanning multi-game, desktop-wide settings (McCarthy et al., 2025). Our design also differs from common tick-based IDMs (Baker et al., 2022; Ye et al., 2024), which fix prediction window (e.g., 50 ms) and thus must emit prediction each tickinefficient in"
        },
        {
            "title": "Under Review",
            "content": "sparse-event regimes and coarse in temporal resolution. Instead, our IDM predicts the event and its timestamp, enabling event-driven modeling that avoids no-op ticks and makes more efficient use of inference context."
        },
        {
            "title": "3 Open-World Agents Toolkit",
            "content": "Figure 2: OWA Toolkits recording and storage architecture. (Left) ocap recorder captures perfectly synchronized multimodal streamsvideo (60 FPS), audio, mouse events, keyboard inputs, and window stateswith precise time alignment, enabling accurate reconstruction of desktop interactions. (Right) OWAMcap format revolutionizes desktop data storage through its dual-layer architecture: standardized MCAP container for crash-safe metadata and event logging, paired with external media referencing for optimized video storage using H.265 codec (217 compression). This design achieves dramatic storage reduction152 for VPT dataset (1.06 TiB 7.12 GiB) and 34.45 for CS:GO dataset (689 GiB 20 GiB)while maintaining event fidelity and enabling efficient random access for training. We introduce the Open-World Agents (OWA) Toolkit alongside large-scale desktop data, establishing both the infrastructure and data foundation for embodied AI research. The toolkit provides unified interface (Zhang et al., 2024; 2025) for capturing interaction patterns across diverse applications without domain-specific action space definitions, while our data release demonstrates the practical scalability and diversity achievable through this standardized approach. 3.1 ocap: Synchronized Desktop Recorder Existing desktop recording tools lack critical features for desktop data collection. Content creation tools like OBS Studio (OBS Project) focus on streaming quality, while action modeling requires synchronized input event logging to capture the precise keyboard and mouse actions that caused visual changes. The ocap (Omnimodal CAPture) tool addresses this gap by capturing desktop signals in synchronized manner, recording video, audio, keyboard, and mouse interactions with high temporal precision. Figure 2 (Left) illustrates an event timeline where these multimodal streams are well synchronized. By leveraging hardware acceleration using Windows APIs, we achieve real-time FHD/QHD recording at 60 Hz on consumer-grade GPUs with low overhead, ensuring that normal user activities remain unaffected and effectively lowering the hardware barrier for large-scale data collection. Implementation details are in Appendix A. 3.2 OWAMcap: Standardized Data Format Prior desktop datasets suffer from storage inefficiency and poor random access capabilities. Existing approaches (Baker et al., 2022; Pearce & Zhu, 2022) either store imageencoded frames in monolithic tables unsuitable for real-time recording, or use formats like JSONL that lack proper indexing and crash-safety. To address these limitations, we introduce OWAMcap (Figure 2, Right), which extends the industry-standard MCAP for-"
        },
        {
            "title": "Under Review",
            "content": "mat (Foxglove, 2022)widely adopted in robotics for multimodal sensor logging and providing efficient indexing, crash-safe writes, and broad ecosystem supportwith two key desktop-specific additions. First, we define standardized message schemas for desktop events (screen, keyboard, mouse) based on Windows APIs, enabling unified processing across different datasets without complex post-processing logic. Unlike other formats (e.g., RLDS (Ramos et al., 2021)) that lack solid message definitions, our standardized schemas allow users to process identical message sets through single pipeline for foundation model training. Second, MediaRef enables efficient video storage while maintaining MCAP compatibility. Raw video captures and image encoding approaches like PNG are prohibitively large for foundation model training, making efficient compression essential. MediaRef addresses this by enabling modern video codecs (H.265), achieving 217 compression over raw captures and 68 over PNG while maintaining sufficient visual quality for agent training  (Table 8)  . 3.3 Optimized Data Pipeline Training foundation models on OWAMcap data requires specialized data loading strategies to maximize throughput, as I/O and data pipeline bottlenecks have been identified as critical limitations in large-scale video model training (Zhao & Krähenbühl, 2023; Leclerc et al., 2023). We present four-stage optimized pipeline: (1) Media transcoding with optimized x264 parameters for consistent random access; (2) Event dataset conversion to HuggingFace datasets (Lhoest et al., 2021) format for efficient sequential and random access; (3) Fixed Sequence Length Dataset (FSLDataset) generation through tokenization and packing to maximize training throughput; (4) On-the-fly media loading with adaptive batch decoding that defers expensive media operations until training time. Our complete data pipeline optimizations are detailed in Appendix A.7, with comprehensive benchmark configurations provided in Appendix A.8. Fixed Sequence Length Dataset (FSLDataset) To optimize training throughput, we introduce FSLDataset that packs sequences to uniform lengths while preserving episode structure. Unlike conventional random concatenation, FSLDataset sequentially lists events within each episode up to the maximum sequence length, terminating at episode completion. This design enables consistent batch processing and converts fine-grained random access into coarse, coalesced patterns for improved I/O efficiency. Adaptive Batch Decoding Strategy Video decoding requires seeking to keyframes and then sequentially decoding frames, as compressed video formats cannot decode arbitrary frames independently. Our adaptive batch decoding algorithm (1) seeks to the target frame; (2) demuxes and decodes until keyframe is encountered; (3) upon hitting keyframe, resumes seeking to the target frame. This provides consistent performance across fine-grained, coarse-grained, and mixed access patterns. Benchmarking Media Decoding on FSLDataset We evaluate our optimized pipeline on representative FSLDataset containing 64 episodes of 5-minute Minecraft gameplay at 640360 resolution and 20 Hz. The baseline uses single-frame decoding per frame, while TorchCodec and our approach use batch decoding for all frames within each FSLDataset sample. Throughput is measured as images processed per second, while I/O efficiency is measured as average disk read per image using isolated filesystem monitoring. Combining these optimizationsoptimized x264 parameters and adaptive batch decodingour complete pipeline achieves 119.16 img/s (10.2 over baseline) while reducing average disk read per image to 18.73 KB (3.4 less than baseline and 41 less than TorchCodec (PyTorch Team, 2024)). Table 1 summarizes results across different configurations. InternVL3-1B Training Throughput Using the FSLDataset from the media decoding benchmark, we benchmarked InternVL3-1B training throughput on single H100 GPU. Our optimized pipeline achieves 4.77 it/s with 1 dataloading worker, while the baseline requires 16 workers to reach comparable throughput (4.55 it/s), demonstrating 16 efficiency gains."
        },
        {
            "title": "Under Review",
            "content": "Moreover, the baseline performance saturates beyond 8 workers, indicating fundamental I/O bottlenecks that our optimizations successfully address  (Table 2)  . Table 1: Media decoding benchmark on FSLDataset (Minecraft, 645 min, 640360 @ 20Hz). Configuration Throughput Avg. Read (KB/img) (img/s) Baseline + optimized x264 + TorchCodec + Ours 11.68 24.25 79.73 119.16 63.46 41.69 770.39 18.73 Table 2: InternVL3-1B training throughput on FSLDataset. Configuration Throughput (it/s) Ours (1 worker) Ours (4 workers) Baseline (8 workers) Baseline (12 workers) Baseline (16 workers) 4.77 4.73 3.79 4.42 4. Figure 3: Our FSLDataset design, coupled with batched decoding API, converts finegrained random I/O into coarse, coalesced random access, thereby avoiding the limitations of large-scale filesystems that are inefficient for small random reads. 3.4 Collecting Human Demonstrations at Scale We collect desktop dataset that provides high-quality, synchronized multimodal signals for vision-action pretraining. While the OWA Toolkit can capture arbitrary desktop tasks (e.g., web surfing, productivity applications) with multimodal eventsincluding the screen, mouse, and keyboardwe focus on gameplay interactions. Gameplay data offer behavioral diversity while minimizing privacy concerns, which enables broad community contribution and data sharing. Using the ocap desktop recorder for efficient collection, 14 human annotators recorded the dataset. The dataset comprises 335 hours of newly collected human demonstrations across 31 games. It spans diverse genres, including 3D third-person games such as GTA and Cyberpunk 2077, first-person games like Apex Legends and Minecraft, and 2D top-down games like Brotato and Stardew Valley. This variety captures wide range of visual environments and interaction styles, making it well-suited for vision-action pretraining. Further details on the dataset and collection process are provided in Appendix B."
        },
        {
            "title": "4 Generalist Inverse Dynamics Model",
            "content": "Collecting large-scale action data through manual demonstrations is infeasible due to prohibitive costs. The OWA Toolkit (Section 3) closes the instrumentation gap and standardizes over 2.6k hours of synchronized trajectories  (Table 10)  , yet human capture alone remains bottleneck relative to the ocean of unlabeled gameplay available online. VPT (Baker et al., 2022) addressed this by leveraging Inverse Dynamics Models (IDMs) to pseudo-label YouTube videos, but was limited to Minecraft, restricting generalization and dataset diversity. We train Generalist-IDM on our multi-domain corpus collected via the OWA Toolkit, enabling generalization across heterogeneous interaction patterns. Our model can infer actions in out-of-distribution environments never seen during training, as demonstrated in Section 5.1. This capability enables pseudo-labeling of large-scale YouTube gameplay videos across diverse games, laying the foundation for internet-scale dataset collection. 4.1 Timestamp-Based Event Tokenization We represent desktop interactions as discrete events, each serialized into short token sequence bounded by <EVENT_START> and <EVENT_END>. Observation events capture screen updates (Screen Events), while action events represent user inputs: Keyboard Events (key"
        },
        {
            "title": "Under Review",
            "content": "presses/releases) and Mouse Events (clicks, movements, scrolls). This event-level serialization unifies heterogeneous inputs into consistent sequential representation for transformer modeling (Vaswani et al., 2017). For example, the tokens emitted for single event follow the format below: <EVENT_START>{TYPE}{TIMESTAMP}{DETAIL}</EVENT_END> (1) While most existing IDMs adopt tick-based prediction (Baker et al., 2022; Ye et al., 2024)predicting actions at fixed intervalsour design employs timestamp-based prediction. Unlike tick-based approaches that use fixed prediction window (e.g., 50 ms), our IDM directly predicts both the event and its timestamp, preserving the asynchronous timing captured by ocap and converted corpora. This design provides two key advantages. First, it maintains cross-modal alignment without resampling, allowing screen, keyboard, and mouse streams to stay synchronized even when their natural cadences differ. Second, timestamp-based prediction avoids generating empty ticks when no actions occur. By skipping unnecessary no-op tokens, our approach makes more efficient use of the limited inference context, enabling denser packing of relevant information and improving the efficiency of both learning and inference. detailed specification of the event tokenization process is provided in Appendix C. 4.2 NEP-τ : Next-Event Prediction with Temporal Offset Once raw desktop interactions are converted to event token sequences, we train the Generalist-IDM with next-event-prediction objective. Given trajectory consisting of observed states and actions (o1, a1, o2, a2, . . . , oT ), where each action at is taken at state ot and leads to state ot+1, the goal is to predict action at based on all preceding observations and actions. This objective enables the model to learn mappings between observed states and actions while preserving temporal dependencies within the trajectory. LNEP = E(o1:T ,a1:T )D log Pθ (cid:0)at (cid:12) (cid:12) o1:t, a1:t1 (cid:1) # \" t=1 (2) Inspired by IDM-K (Tot et al., 2025), which conditions on extended future trajectories to improve inverse dynamics, we adopt NEP-τ , temporal-offset variant of NEP. Unlike IDMK, which jointly encodes entire past and future trajectories, our method simply rearranges the (observation, action) sequences by shifting the observation window forward by τ steps. This allows the model to incorporate future observations up to τ steps ahead without encoding entire future trajectories, enhancing temporal consistency. Formally, the objective is: LNEP-τ = E(o1:T ,a1:T )D \" t=1 (cid:16) log Pθ at (cid:12) (cid:12) (cid:12) o1: min(t+τ, ), a1:t (cid:17) # (3) 4.3 Pseudo-Labeling with YouTube Gameplay Videos We focus on pseudo-labeling gameplay videos because they are abundant, actively shared, and largely free of personally identifiable content, sidestepping the privacy concerns. YouTube gameplay footage also exhibits consistent HUD layouts and frame rates, which align well with the OWA Toolkits event schema. Our pipeline first curates long-form gameplay uploads with permissive licenses, retrieves them at 20 Hz, and converts the frames into Screen events so they can be fed through the same tokenizer used for human demonstrations. Building on this, we train the Generalist-IDM using the InternVL3-1B (Zhu et al., 2025) architecture with the NEP-τ objective. The Generalist-IDM then produces the corresponding Keyboard and Mouse events via the NEP-τ objective, after which we apply consistency checksincluding removing extended inactive spans as described in Appendix Bbefore materializing the pseudo-labels. Applying this procedure contributes 1055 hours of additional trajectories across twenty publicly shared titles, as summarized in Table 12, complementing the curated corpus described in Table 10 and Section 3. Importantly, because our model is designed to be generalist, we do not require any filtering of domain-specific interfaces such as inventory menus or map screens. Instead, these heterogeneous visual contexts are naturally included as part of the pseudo-labeled demonstrations, broadening the scope"
        },
        {
            "title": "Under Review",
            "content": "of training data without additional heuristics. These pseudo-labeled trajectories form the seed for scaling desktop vision-action pretraining to internet-scale data sources."
        },
        {
            "title": "5 Results",
            "content": "5.1 Performance of the Generalist-IDM In-Distribution Performance. We begin by evaluating the Generalist-IDM on six indistribution video games spanning both 2D and 3D settings, comparing its performance to Specialist-IDMs trained individually on each game. We employ an autoregressive inference pipeline to generate actions and evaluate model performance across multiple metrics. Further details are provided in Appendix F. As shown in Table 3 and Table 4, our Generalist-IDM achieves strong performance across all environments. Notably, it yields large gains in Pearson correlation (e.g., +39.5 points on Stardew Valley X) and Keyboard accuracy (e.g., +57.6 points on Brotato), demonstrating robust generalization over diverse control dynamics. Game Model Pearson Scale Ratio Keypress Acc. Kbd Mouse Game Model Pearson Scale Ratio Keypress Acc. Kbd Mouse Brotato Stardew Valley Core Keeper 65.92 IDM G-IDM 73. IDM 43.47 G-IDM 82.98 IDM 48.03 G-IDM 77.25 67.56 82.03 63.69 75.57 62.09 64.55 1.04 1. 1.19 1.13 1.15 1.43 1.04 1.29 1.18 1.17 1.17 1.51 28.80 86. 69.35 74.35 69.42 70.00 97.59 98.50 91.90 96.43 92.33 94.01 Apex Legends GTA Minecraft 65.16 IDM G-IDM 83.90 IDM 63.64 G-IDM 79.44 IDM 59.83 G-IDM 80.29 57.84 85. 81.08 83.89 63.83 78.38 1.29 1.13 1.39 1.09 1.20 1.24 1.25 1. 1.23 1.42 1.22 1.27 67.47 76.55 58.13 69.83 53.54 60.97 99.33 99. 94.65 94.11 82.48 91.65 Table 3: Evaluation results on 2D games Table 4: Evaluation results on 3D games Out-of-Distribution Generalization. We evaluate the generalization of our GeneralistIDM on two unseen games: Battlefield 6 (3D) and Ogu and the Secret Forest (2D). In Battlefield 6, it matches or slightly outperforms the Specialist-IDM (achieving 63% keyboard accuracy), indicating solid transfer to an unseen FPS similar to the training set. Moreover, when provided with few-shot prefix that fills the first 2048 tokens in our streaming inference, the predicted scale ratio improves significantlyindicating that the Generalist-IDM exhibits in-context ability to adapt to mouse sensitivity. In Ogu and the Secret Forest, it more than doubles the Specialist-IDMs performance (from about 12% to nearly 28%), showing that Generalist-IDM delivers substantial gains even under large domain gap. Taken together, these results demonstrate that Generalist-IDM is capable of adapting across both familiar and substantially different environments. Model Pearson Scale Ratio Keypress Acc. Kbd Mouse IDM (FT) G-IDM (ZS) G-IDM (FS) G-IDM (FT) 57.28 57.36 56.79 54. IDM (FT) G-IDM (ZS) G-IDM (FS) G-IDM (FT) Battlefield 6 1.00 61.74 3.13 63.17 1.07 63.40 1.06 62.89 Ogu Forest 1.00 3.56 1.05 1.04 62.44 47.75 52.64 58.55 11.73 27.80 27.97 26.88 94.55 92.11 93.89 93.41 Table 5: Out-of-distribution performance on unseen 3D and 2D games. Note that Ogu Forest uses only keyboard inputs. Figure 4: Trajectory of Battlefield 6. 5.2 Transferability to Downstream Tasks To evaluate transferability to downstream tasks, we use the InternVL3-1B model as our backbone, which is also the architecture used in our Generalist-IDM. We train this model"
        },
        {
            "title": "Under Review",
            "content": "under two settings: VAPT without pseudo-labels, which uses only the human-collected dataset (259 hours), and VAPT with pseudo-labels, which augments the human data with pseudo-labeled dataset generated from YouTube videos with the Generalist-IDM, resulting in total of over 1.3K hours of training data. Further details can be found in Appendix Robot Manipulation. For robot manipulation, we evaluate on the LIBERO benchmark (Liu et al., 2023). As shown in Table 6, the baseline (InternVL3-1B) performs relatively poorly. In contrast, VAPT without pseudo-labels achieves substantial improvement, reaching 96.6% on Total and 93.6% on long-horizon tasks. These results are comparable to or even surpass much larger models such as OpenVLA (7B) and SmolVLA (2.25B). Interestingly, and contrary to intuition, incorporating pseudo-labels does not provide additional gains on manipulation tasks. We attribute this to the nature of manipulation tasks, where precise human supervision is more critical than data scale and diversity, making pseudo-labels less effective. Overall, our approach demonstrates that even with only 1B parameters, it can match or outperform significantly larger policies, with particularly strong advantages on long-horizon tasks where careful action sequencing is essential. Method Params VLA Pt Spatial Object Goal 10 (long) Total Octo (Octo Model Team et al., 2024) OpenVLA (Kim et al., 2024) DiT Policy (Dasari et al., 2025) pi0 Black et al. (2024) SmolVLA (Shukor et al., 2025) PI-KI (Driess et al., 2025) OpenVLA-OFT (Kim et al., 2025) Baseline (InternVL3-1B) + VAPT w/o pseudo + VAPT w/ pseudo 93M 7B 115M 3.3B 2.25B 300M 7B 1B 1B 1B Yes Yes No Yes No Yes Yes No No No 78.9 84.7 84.2 90.0 93.0 98.0 97. 94.4 95.8 89.6 85.7 88.4 96.3 86.0 94.0 97.8 98.4 97.0 98.4 98.2 84.6 79.2 85.4 95.0 91.0 95.6 97.9 93.6 98.6 93.8 51.1 53.7 63.8 73.0 77,0 85.8 94. 54.2 93.6 87.2 75.1 76.5 82.4 86.0 88.7 94.3 97.1 84.8 96.6 92.2 Table 6: Results on Libero tasks (success rates, %). Robot Navigation. For robot navigation, we evaluate on the CANVAS benchmark Choi et al. (2024), challenging navigation benchmark that tests robustness to both misleading and precise instructions across diverse simulated environments. Compared to the baseline, our VAPT framework shows clear gains: without pseudo-labels, performance matches the baseline (75.3%), but with pseudo-labeled demonstrations it rises to 83.3%, an 8-point improvement. The effect is strongest under misleading instructions, as in sim_orchard (86.7% vs. 53.3%) and sim_street_sidewalk (73.3% vs. 40.0%), while precise instructions remain near ceiling. These results show that pseudo-labeling is highly effective for navigation tasks, where success depends more on high-level planning than on the precise low-level control required in manipulation. Environment Instruction Baseline VAPT w/o pseudo VAPT w/ pseudo sim_gallery sim_office sim_orchard sim_street_road sim_street_sidewalk misleading precise misleading precise misleading precise misleading precise misleading precise 53.3 (8/15) 100.0 (15/15) 100.0 (15/15) 100.0 (15/15) 53.3 (8/15) 40.0 (6/15) 94.4 (17/18) 100.0 (12/12) 40.0 (6/15) 73.3 (11/15) 33.3 (5/15) 93.3 (14/15) 93.3 (14/15) 100.0 (15/15) 53.3 (8/15) 53.3 (8/15) 88.9 (16/18) 91.7 (11/12) 53.3 (8/15) 93.3 (14/15) 53.3 (8/15) 93.3 (14/15) 100.0 (15/15) 100.0 (15/15) 86.7 (13/15) 60.0 (9/15) 88.9 (16/18) 100.0 (12/12) 73.3 (11/15) 80.0 (12/15) Total Overall 75.3 (113/150) 75.3 (113/150) 83.3 (125/150) Table 7: Results on CANVAS tasks (success rates, %)"
        },
        {
            "title": "6 Conclusion",
            "content": "Embodied AI has long struggled with the prohibitive cost of collecting large-scale physical interaction data, limiting its ability to benefit from internet-scale resources. To address this challenge, we proposed using desktop interactions as an abundant and low-cost substrate for pretraining. Our contributions are threefold: (1) the OWA Toolkit, which standardizes and compresses diverse desktop data into scalable format; (2) the Generalist-IDM, timestamp-based inverse dynamics model that generalizes across unseen games and demonstrates pathway toward internet-scale pseudo-labeling; and (3) VAPT, which explores the transfer of desktop-pretrained representations to robotics tasks. Leveraging 1.3K+ hours of human and pseudo-labeled data, our framework achieves 96.6% success on LIBERO manipulation and 83.3% on CANVAS navigation, demonstrating that digital sensorimotor patterns can directly improve embodied AI benchmarks. We release all our tools, datasets, and models publicly to enable the community to build upon this foundation and further investigate desktop-to-embodied transfer. These results establish desktop data as practical and scalable resource for advancing embodied intelligence, opening new path toward general-purpose agents without relying on prohibitively expensive physical data collection."
        },
        {
            "title": "Reproducibility Statement",
            "content": "To ensure full reproducibility of our work, we release comprehensive resources and documentation. All source code for the OWA Toolkit (ocap recorder and OWAMcap format implementation), Generalist-IDM training, and downstream task fine-tuning is publicly available at https://anonymous.4open.science/r/Generalist-IDM-9B13, including detailed installation instructions and usage examples. The complete 2.6K hour desktop dataset (335 hours newly collected, 2.3K hours converted) and 1K+ hours of pseudo-labeled data are accessible through the same repository with standardized OWAMcap format specifications described in Section 3 and Appendix A. Pre-trained model weights for both Generalist-IDM and VAPT foundation models are provided along with training configurations. Hyperparameters and training schedules are detailed in Appendix E, including batch sizes, learning rates, and hardware requirements (8 H100 GPUs for IDM training). Data preprocessing pipelines, including temporal offset implementation (Section 4) and event tokenization schemes (Appendix C), are fully documented with reference implementations. Evaluation protocols and metrics are specified in Section with corresponding evaluation scripts in the repository. For compute-constrained researchers, we release smaller dataset subsets and checkpoint models at various training stages to facilitate partial reproduction and ablation studies."
        },
        {
            "title": "References",
            "content": "Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos. Advances in Neural Information Processing Systems, 35:2463924654, 2022. Johan Bjorck, Fernando Castañeda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, et al. Gr00t n1: An open foundation model for generalist humanoid robots. arXiv preprint arXiv:2503.14734, 2025. Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. π0: vision-languageaction flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. Remi Cadene, Simon Alibert, Alexander Soare, Quentin Gallouedec, Adil Zouitine, Steven Palma, Pepijn Kooijmans, Michel Aractingi, Mustafa Shukor, Dana Aubakirova, Martino"
        },
        {
            "title": "Under Review",
            "content": "Russi, Francesco Capuano, Caroline Pascal, Jade Choghari, Jess Moss, and Thomas Wolf. Lerobot: State-of-the-art machine learning for real-world robotics in pytorch. https: //github.com/huggingface/lerobot, 2024. Dongping Chen, Yue Huang, Siyuan Wu, Jingyu Tang, Huichi Zhou, Qihui Zhang, Zhigang He, Yilin Bai, Chujie Gao, Liuyi Chen, Yiqiang Li, Chenlong Wang, Yue Yu, Tianshuo Zhou, Zhen Li, Yi Gui, Yao Wan, Pan Zhou, Jianfeng Gao, and Lichao Sun. GUI-world: video benchmark and dataset for multimodal GUI-oriented understanding. In The Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?id=QarKTT5brZ. Xuxin Cheng, Jialong Li, Shiqi Yang, Ge Yang, and Xiaolong Wang. Open-television: Teleoperation with immersive active visual feedback. arXiv preprint arXiv:2407.01512, 2024. Suhwan Choi, Yongjun Cho, Minchan Kim, Jaeyoon Jung, Myunchul Joe, Yubeen Park, Minseo Kim, Sungwoong Kim, Sungjae Lee, Hwiseong Park, et al. Canvas: Commonsense-aware navigation system for intuitive human-robot interaction. arXiv preprint arXiv:2410.01273, 2024. Sudeep Dasari, Oier Mees, Sebastian Zhao, Mohan Kumar Srirama, and Sergey Levine. The ingredients for robotic diffusion transformers. In 2025 IEEE International Conference on Robotics and Automation (ICRA), pp. 1561715625. IEEE, 2025. Danny Driess, Jost Tobias Springenberg, Brian Ichter, Lili Yu, Adrian Li-Bell, Karl Pertsch, Allen Ren, Homer Walke, Quan Vuong, Lucy Xiaoyang Shi, et al. Knowledge insulating vision-language-action models: Train fast, run fast, generalize better. arXiv preprint arXiv:2505.23705, 2025. Yilun Du, Sherry Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Josh Tenenbaum, Dale Schuurmans, and Pieter Abbeel. Learning universal policies via text-guided video generation. Advances in neural information processing systems, 36:91569172, 2023. Roya Firoozi, Johnathan Tucker, Stephen Tian, Anirudha Majumdar, Jiankai Sun, Weiyu Liu, Yuke Zhu, Shuran Song, Ashish Kapoor, Karol Hausman, et al. Foundation models in robotics: Applications, challenges, and the future. The International Journal of Robotics Research, 2024. doi: https://doi.org/10.1177/02783649241281508. Foxglove. Evaluation of robotics data recording file formats. Technical Report, October 2021. URL https://mcap.dev/files/evaluation.pdf. Technical report; accessed 2025-08-23. Foxglove. Introducing the mcap file format. Foxglove Blog, February 2022. URL https:// foxglove.dev/blog/introducing-the-mcap-file-format. Blog post; accessed 202508-23. Foxglove Developers. Mcap: serialization-agnostic log container file format, 2024. URL https://mcap.dev. Available from https://github.com/foxglove/mcap. Zipeng Fu, Tony Z. Zhao, and Chelsea Finn. Mobile aloha: Learning bimanual mobile manipulation with low-cost whole-body teleoperation. In Conference on Robot Learning (CoRL), 2024. Haoran Geng, Feishi Wang, Songlin Wei, Yuyang Li, Bangjun Wang, Boshi An, Charlie Tianyue Cheng, Haozhe Lou, Peihao Li, Yen-Jen Wang, et al. Roboverse: Towards unified platform, dataset and benchmark for scalable and generalizable robot learning. arXiv preprint arXiv:2504.18904, 2025. GStreamer Team. Gstreamer: Open source multimedia framework. URL https:// gstreamer.freedesktop.org/. Official GStreamer Documentation. Yingchen He, Christian Weilbach, Martyna Wojciechowska, Yuxuan Zhang, and Frank Wood. Plaicraft: Large-scale time-aligned vision-speech-action dataset for embodied ai. arXiv preprint arXiv:2505.12707, 2025."
        },
        {
            "title": "Under Review",
            "content": "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. ITU-T. High efficiency video coding. Recommendation H.265, International Telecommunication Union, 2024. Also published as ISO/IEC 23008-2. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Anikait Srinivas, Jacky Liang, Pieter Abbeel, and Chelsea Finn. Openvla: An opensource vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. Moo Jin Kim, Chelsea Finn, and Percy Liang. Fine-tuning vision-language-action models: Optimizing speed and success. arXiv preprint arXiv:2502.19645, 2025. Guillaume Leclerc, Andrew Ilyas, Logan Engstrom, Sung Min Park, Hadi Salman, and Aleksander Madry. Ffcv: Accelerating training by removing data bottlenecks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12011 12020, 2023. Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Šaško, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clément Delangue, Théo Matussière, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, François Lagunas, Alexander Rush, and Thomas Wolf. Datasets: community library for natural language processing. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 175184, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. URL https://aclanthology.org/2021.emnlp-demo.21. Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. Libero: Benchmarking knowledge transfer for lifelong robot learning. Advances in Neural Information Processing Systems, 36:4477644791, 2023. Ajay Mandlekar, Jonathan Booher, Max Spero, Albert Tung, Anchit Gupta, Yuke Zhu, Animesh Garg, Silvio Savarese, and Li Fei-Fei. Scaling robot supervision to hundreds of hours with roboturk: Robotic manipulation dataset through human reasoning and dexterity. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2019. Robert McCarthy, Daniel CH Tan, Dominik Schmidt, Fernando Acero, Nathan Herr, Yilun Du, Thomas Thuruthel, and Zhibin Li. Towards generalist robot learning from internet video: survey. Journal of Artificial Intelligence Research, 83, 2025. Microsoft Corporation. Dxgi overview. URL https://docs.microsoft.com/en-us/ windows/win32/direct3ddxgi/d3d10-graphics-programming-guide-dxgi. Microsoft Developer Documentation. OBS Project. Obs studio. URL https://obsproject.com/. Open Broadcaster Software Studio - Free and open source software for video recording and live streaming. Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Charles Xu, Jianlan Luo, Tobias Kreiman, You Liang Tan, Lawrence Yunliang Chen, Pannag Sanketi, Quan Vuong, Ted Xiao, Dorsa Sadigh, Chelsea Finn, and Sergey Levine. Octo: An open-source generalist robot policy. In Proceedings of Robotics: Science and Systems, Delft, Netherlands, 2024."
        },
        {
            "title": "Under Review",
            "content": "Abby ONeill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, et al. Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 68926903. IEEE, 2024. Younghyo Park, Jagdeep Singh Bhatia, Lars Ankile, and Pulkit Agrawal. Dexhub and dart: Towards internet scale robot data collection. arXiv preprint arXiv:2411.02214, 2024. Tim Pearce and Jun Zhu. Counter-strike deathmatch with large-scale behavioural cloning. In 2022 IEEE Conference on Games (CoG), pp. 104111. IEEE, 2022. PyTorch Team. Torchcodec: pytorch library for video decoding. https://github.com/ pytorch/torchcodec, 2024. PyTorch video decoding library. Yuzhe Qin, Wei Yang, Binghao Huang, Karl Van Wyk, Hao Su, Xiaolong Wang, Yu-Wei Chao, and Dieter Fox. Anyteleop: general vision-based dexterous robot arm-hand teleoperation system. In Robotics: Science and Systems, 2023. Maria Abi Raad, Arun Ahuja, Catarina Barros, Frederic Besse, Andrew Bolt, Adrian Bolton, Bethanie Brownfield, Gavin Buttimore, Max Cant, Sarah Chakera, et al. Scaling instructable agents across many simulated worlds. arXiv preprint arXiv:2404.10179, 2024. Sabela Ramos, Sertan Girgin, Léonard Hussenot, Damien Vincent, Hanna Yakubovich, Daniel Toyama, Anita Gergely, Piotr Stanczyk, Raphael Marinier, Jeremiah Harmsen, Olivier Pietquin, and Nikola Momchev. Rlds: an ecosystem to generate, share and use datasets in reinforcement learning, 2021. Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. generalist agent. arXiv preprint arXiv:2205.06175, 2022. Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1074010749, 2020. Mustafa Shukor, Dana Aubakirova, Francesco Capuano, Pepijn Kooijmans, Steven Palma, Adil Zouitine, Michel Aractingi, Caroline Pascal, Martino Russi, Andres Marafioti, et al. Smolvla: vision-language-action model for affordable and efficient robotics. arXiv preprint arXiv:2506.01844, 2025. Gary Sullivan, Jens-Rainer Ohm, Woo-Jin Han, and Thomas Wiegand. Overview of the high efficiency video coding (hevc) standard. IEEE Transactions on circuits and systems for video technology, 22(12):16491668, 2012. Fei Tang, Haolei Xu, Hang Zhang, Siqi Chen, Xingyu Wu, Yongliang Shen, Wenqi Zhang, Guiyang Hou, Zeqi Tan, Yuchen Yan, et al. survey on (m) llm-based gui agents. arXiv preprint arXiv:2504.13865, 2025. Marko Tot, Shu Ishida, Abdelhak Lemkhenter, David Bignell, Pallavi Choudhury, Chris Lovett, Luis França, Matheus Ribeiro Furtado de Mendonça, Tarun Gupta, Darren Gehring, et al. Adapting world model for trajectory following in 3d game. arXiv preprint arXiv:2504.12299, 2025. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Junjie Wen, Yichen Zhu, Minjie Zhu, Zhibin Tang, Jinming Li, Zhongyi Zhou, Xiaoyu Liu, Chaomin Shen, Yaxin Peng, and Feifei Feng. Diffusionvla: Scaling robot foundation models via unified diffusion and autoregression. In Forty-second International Conference on Machine Learning."
        },
        {
            "title": "Under Review",
            "content": "Youguang Xing, Xu Luo, Junlin Xie, Lianli Gao, Hengtao Shen, and Jingkuan Song. Shortcut learning in generalist robot policies: The role of dataset diversity and fragmentation. arXiv preprint arXiv:2508.06426, 2025. Qwen An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxin Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yi-Chao Zhang, Yunyang Wan, Yuqi Liu, Zeyu Cui, Zhenru Zhang, Zihan Qiu, Shanghaoran Quan, and Zekun Wang. Qwen2.5 technical report. ArXiv, 2024. Seonghyeon Ye, Joel Jang, Byeongguk Jeon, Sejune Joo, Jianwei Yang, Baolin Peng, Ajay Mandlekar, Reuben Tan, Yu-Wei Chao, Bill Yuchen Lin, et al. Latent action pretraining from videos. arXiv preprint arXiv:2410.11758, 2024. Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang, Qingwei Lin, Saravan Rajmohan, et al. Ufo: ui-focused agent for windows os interaction. arXiv preprint arXiv:2402.07939, 2024. Chaoyun Zhang, He Huang, Chiming Ni, Jian Mu, Si Qin, Shilin He, Lu Wang, Fangkai arXiv preprint Ufo2: The desktop agentos. Yang, Pu Zhao, Chao Du, et al. arXiv:2504.14603, 2025. Hao Zhao and Philipp Krähenbühl. Training large video model on single machine in day. arXiv preprint arXiv:2309.16669, 2023. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In Conference on Robot Learning, pp. 21652183. PMLR, 2023."
        },
        {
            "title": "A OWA Toolkit Details",
            "content": "A.1 Format Comparison Prior desktop datasets commonly adopt one of two storage strategies. The LeRobot dataset (Cadene et al., 2024), CS:GO dataset (Pearce & Zhu, 2022), and the CraftJarvis \"minecraft-vla-sft\" dataset (He et al., 2025) store image-encoded frames directly in single, monolithic table. While this layout is sufficient for training, it is ill-suited for recording because long-table stores typically do not support efficient real-time appends. By contrast, the VPT dataset (Baker et al., 2022) packages each sample as an MP4JSONL pair. However, JSONL lacks the ability to interleave heterogeneous, typed streams with chunking and indexing. In practice, this limitation results in poor or unavailable topic-wise random seeking and reduced crash-safety, as writes are unreliable under unexpected termination. Furthermore, datasets that rely on image encoding are substantially less storage-efficient compared to standard video codecs. The robotics community has encountered similar multimodal logging challenges. Traditional ROS bags exhibit performance and extensibility limitations (Foxglove, 2021), which motivated the development of the MCAP format (Foxglove, 2022): an open-source container format designed with efficient indexing and compression. MCAP has since become the de facto logging standard for ROS 2 (Foxglove, 2022; Foxglove Developers, 2024), demonstrating the benefits of specialized data formats for embodied AI research. However, no equivalent standard has been established for desktop datasets, motivating our introduction of the OWAMcap format. A.2 Compression Efficiency OWAMcap achieves substantial storage savings across multiple datasets, demonstrating its efficiency and scalability. For the CS:GO dataset (Pearce & Zhu, 2022), replacing the original HDF5 storage with OWAMcap (mkv+mcap) reduces the storage requirement from 689 GiB to 20 GiBa 34.45 reduction. Similarly, converting the VPT dataset (Baker et al., 2022) from JSONL to OWAMcap (mcap format) shrinks disk usage from 1.06 TiB to 7.12 GiB, achieving 152 reduction. This significant compression arises from two different aspects: (1) from using video encoding instead of saving raw image buffer on the CS:GO datasets HDF5 and (2) from mcaps efficiency in representing/storing information on the VPT datasets jsonl. A.3 Video Compression Performance OWAMcaps another advantage of MediaRef, flexible system supporting storing media on (1) embedded (2) external media. We support storing media on both external image file and external video file. This flexible design leads opportunity to acquire significant compression efficiency of video encoding, such as H.265/HEVC. To further evaluate the benefits of video encoding, we benchmarked video compression performance for various encoding. Table 8 shows that video encoding provides superior compression rates while maintaining visual quality, enabling large-scale storage without compromising data fidelity. ocap is storing all media in H.265 by default and we observed similar compression ratio for recorded files. A.4 ocap Architecture The implementation of ocap is designed to maximize recording performance and reliability. ocap leverages Windows APIs, including DXGI (Microsoft Corporation) for hardwareaccelerated screen capture, WASAPI for low-latency audio recording, and direct input event capture for precise keyboard and mouse logging. The media pipeline is built on GStreamer (GStreamer Team) and employs H.265/HEVC encoding (ITU-T, 2024; Sullivan et al., 2012) to achieve high compression efficiency while maintaining visual quality. The overall architecture, shown in Figure 5, integrates video, audio, and interaction streams within the OWAMcap format while ensuring synchronized, crash-safe recording."
        },
        {
            "title": "Size per Frame Total Size Compression Ratio",
            "content": "Raw BGRA PNG JPEG (Quality 85) H.265 (keyframe 0.5s)"
        },
        {
            "title": "1.31 GB 3.2×\n135 MB 31.9×\n19.6 MB 217.8×",
            "content": "Table 8: Compression performance comparison for various encoding on our recorded Minecraft video. Desktop screen capture at 19201080 resolution, 12 seconds @ 60 Hz. H.265 encoding uses nvd3d11h265enc for hardware acceleration. Video encoding yields significantly higher compression ratios than other formats. ocap is storing all media in H.265 by default and we observed similar compression ratio for recorded files. Note that size per frame for H.265 is an average over all frames, as keyframes are larger. Figure 5: Architecture of ocap desktop recorder. A.5 Screen Capture Performance Benchmarks ocap employs H.265/HEVC encoding for video content and AAC encoding for audio streams, enabling real-time recording with minimal system overhead. Table 9 compares the capture performance of ocap against existing alternatives, showing that our implementation consistently achieves higher frame rates and lower CPU utilization while preserving recording fidelity. Library Avg. Time per Frame Relative Speed owa.env.gst pyscreenshot PIL MSS PyQt5 5.7 ms 33 ms 34 ms 37 ms 137 ms 1.0 (baseline) 5.8 slower 6.0 slower 6.5 slower 24 slower Table 9: Screen capture performance comparison. Benchmarked on Intel i5-11400 with GTX 1650. ocap achieves 6 faster performance than common alternatives through Windows API and GStreamer integration. A.6 Comparison with Existing Recorders To assess feature coverage and efficiency, we compared ocap against commonly used desktop recording frameworks. As shown in Figure 6, ocap is the only system that provides synchronized multimodal recording, robust crash-safety guarantees, and efficient compres-"
        },
        {
            "title": "Under Review",
            "content": "sion in single framework. These advantages make ocap uniquely comprehensive solution for large-scale desktop interaction logging. Figure 6: Comparison of key features between ocap and other desktop recording tools. A.7 Data Pipeline Optimizations Our data pipeline incorporates several key optimizations to address the limitations of conventional video processing approaches for foundation model training. Baseline Video Properties To understand the limitations of default video encoding, we analyzed representative sample from our dataset. The baseline configuration uses default x264 parameters, resulting in variable GOP structure that impacts random access performance. Frame type distribution shows: I-frames 57 (0.9%), B-frames 3847 (64.1%), Pframes 2096 (34.9%). I-frame interval analysis reveals significant variability: minimum 1.35s, maximum 12.50s, average 5.32s, median 4.83s. This variable GOP size creates inconsistent seeking performance, motivating our optimized x264 parameters. Optimized x264 Parameters Default video encoding with x264 creates variable GOP structures with unpredictable keyframe intervals, causing inconsistent random access performance during training. Our optimization fixes keyframe intervals to 30 frames (1.5 seconds at 20 Hz) and disables B-frames entirely. This creates predictable GOP structure: I-P-P-P- ...-P-I, enabling consistent random access performance. The elimination of B-frames reduces decoding complexity during seeking operations, while fixed keyframe intervals ensure uniform seeking distances. FSLDataset Construction FSLDataset preserves episode temporal structure during sequence packing. For each episode, we sequentially list all events (screen, keyboard, mouse) in chronological order, then concatenate episodes sequentially until reaching the maximum sequence length (e.g., 4096 tokens). When an episode completes before reaching the maximum length, packing terminates immediately and remaining positions are padded. This approach maintains episode coherence while enabling uniform sequence lengths for efficient batch processing."
        },
        {
            "title": "Under Review",
            "content": "Adaptive Batch Decoding Strategy The baseline configuration uses single-frame decoding where each frame within an FSLDataset sample requires individual video seek and decode operations. For an FSLDataset sample containing frames, the baseline performs separate video decoder calls, each involving: (1) seeking to the target frame position, (2) decoding from the nearest keyframe to the target frame, and (3) extracting the single target frame. This approach results in significant redundant I/O operations when multiple frames from the same video segment are needed. Our adaptive batch decoding strategy processes all frames within each FSLDataset sample through single batched operation, eliminating redundant seeking and keyframe decoding overhead. Both TorchCodec (PyTorch Team, 2024) v0.6.0 and our implementation use this per-sample batching approach: for each FSLDataset sample, we issue single batched query that requests all images within the sample at once (no cross-sample batching or parallel workers). A.8 Benchmark Configuration To quantify the effect of our optimized pipeline, we conduct comprehensive benchmarks across different configurations and training scenarios. Media Decoding Benchmark Setup The media decoding benchmark uses representative FSLDataset containing 64 episodes of 5-minute Minecraft gameplay recorded at 640360 resolution and 20 Hz frame rate. The FSLDataset is configured with fixed sequence length of 4096 tokens, where all sequences are tokenized and packed to this uniform length. We measure performance using single-worker random-access iteration and report: (i) image throughput (img/s) calculated by dividing the total number of images by the time required to process all images during decoding, and (ii) average disk bytes read per image (KB/img) obtained by monitoring total bytes read during iteration divided by the number of images, capturing seeking and GOP decode overhead. For I/O efficiency measurement, we create an isolated temporary filesystem and store all media data referenced by the FSLDataset in this dedicated path. During benchmarking, we monitor the total amount of data read from this filesystem to obtain precise I/O measurements. Progressive configurations test: (1) baseline with default x264 parameters and single-frame decoding, (2) baseline + optimized x264 parameters, (3) optimized x264 + TorchCodec v0.6.0 batch decoding, and (4) optimized x264 + our adaptive batch decoding strategy. All benchmark experiments were conducted three times to ensure result stability, with all runs showing consistent performance within measurement variance. The reported results represent the final experimental run. InternVL3-1B Training Configuration Model training benchmarks use single H100 GPU with batch size 1, DeepSpeed Zero1 for memory optimization, FlashAttention 3 for efficient attention computation, and context length 4096 tokens. The baseline configuration uses default x264 parameters without batch decoding, while our optimized pipeline combines optimized x264 parameters with adaptive batch decoding API. We measure training throughput (iterations per second) across different numbers of dataloader workers to evaluate scalability and efficiency gains."
        },
        {
            "title": "B Dataset Details",
            "content": "B.1 Collection and Quality Assurance We collected the dataset using distributed approach supported by contributions from community volunteers. To ensure participant privacy, we applied automated detection techniques followed by manual review to remove any sensitive information. Quality assurance involved both automated and manual procedures. Automated validation checked for temporal alignment issues and corrupted recordings, while human annotators manually evaluated"
        },
        {
            "title": "Under Review",
            "content": "Game/Application Apex Legends Euro Truck Simulator 2 Stardew Valley Cyberpunk 2077 Rainbow Six Siege Grand Theft Auto Slime Rancher Medieval Dynasty Dinkum Raft Satisfactory Minecraft (SP 1.21.8) Grounded Ready Or Not Counter-Strike 2 Core Keeper Barony Monster Hunter Wilds Brotato PUBG: Battlegrounds Category Genre External Hours FPS ID Driving ID Top-Down Sim ID Open-World, RPG ID FPS ID Open-World, Driving ID Simulation ID Simulation, RPG ID Sandbox, Survival ID Survival, Co-op ID ID Factory-Building ID Open-World, Sandbox Survival, Co-op ID Tactical FPS ID ID FPS Sandbox, Survival ID Roguelike RPG ID ID Action RPG Top-Down Shooter ID FPS, Battle Royale ID No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No 25.8 19.7 16.1 14.6 13.8 11.7 11.1 10.7 10.5 10.3 10.1 10.1 10.1 10.0 9.9 9.4 9.3 8.7 6.1 4.9 258.7 2.3 2.3 17.3 14.1 10.3 10.1 2.3 2.8 2.3 2.0 1.8 0.7 0.3 335.6 Total Used for Train and test OOD OOD Collection Adventure, Puzzle Ogu and the Secret Forest FPS Battlefield 6 (Open Beta) Eternal Return MOBA, Survival MapleStory Worlds-Southperry (EA) Collection Open-World, Sandbox FPS, Hero Shooter Overwatch Collection Enshrouded Survival, RPG Collection Collection Ogu and the Secret Forest Adventure, Puzzle Collection Top-Down Platformer Vampire Survivors Collection Battlefield 6 (Open Beta) FPS Roguelike Platformer Collection Skul Casual/Arcade Collection PEAK Platformer, Co-op Collection Super Bunny Man FPS Collection VALORANT Total (Collected) Table 10: Collected desktop data statistics. The dataset includes internally collected demonstrations across diverse games and applications. the realism and fidelity of recorded behaviors. The final dataset captures wide range of desktop interaction patterns, including navigation behaviors, application switching, text input, menu interactions, and multi-step task execution. B.2 Annotator Calibration and Protocols Before recording, contributors completed an ocap calibration wizard that verified refresh rate, display resolution, cursor fidelity, and input-device mapping. Annotatorseither modestly compensated participants or volunteersfollowed standardized game prompts covering navigation, combat, and resource-management scenarios; detailed environment statistics are listed in Table 10. All sessions were screen-captured at FHD or QHD 60 Hz with synchronized mouse and keyboard traces, and ocaps turnkey workflow meant anyone could gather synchronized data with minimal setup; annotators re-ran the calibration sequence whenever their hardware changed. B.3 Converted Data The converted dataset includes Minecraft demonstrations from Baker et al. (Baker et al., 2022) and Counter-Strike 2 data from Pearce et al. (Pearce & Zhu, 2022). These external"
        },
        {
            "title": "Under Review",
            "content": "Game/Application Category Genre External Hours Minecraft - VPT (Baker et al., 2022) CSGO - CS_DM (Pearce & Zhu, 2022) Converted Converted Open-World, Sandbox FPS Total (Converted) Yes Yes 2194 100 2294.0 Table 11: Converted dataset statistics. Converted data from existing public benchmarks complement the collected corpus. sources were standardized into the OWAMcap format, ensuring consistency and seamless integration across different datasets. B.4 Preprocessed Dataset Before training, we applied preprocessing to handle temporal offsets. Specifically, after applying temporal offset τ , only the sequences of action labels were shifted, while the observations remained unchanged. Additionally, we filtered out inactive segments where no actions occurred for extended periods to reduce noise and improve training efficiency. B.5 Pseudo-labeled Dataset We collect high-quality YouTube gameplay videos through combination of targeted search and bulk download. For the search phase, we used the query template GAME_NAME no commentary, where the term no commentary is widely understood to indicate pure gameplay videos without additional overlays, commentary, or editing. After obtaining video links, we downloaded the videos using the open-source tool yt-dlp. To ensure consistency, we restricted the maximum resolution to 480p. In addition, frequent cookie renewal and download rate cap of 62.5 Mb/s were necessary to bypass YouTubes automated bot detection mechanisms. Through this pipeline, we successfully curated over 1,000 hours of high-quality gameplay footage for pseudo-labeling. The total collected video duration per game is summarized in Table 12."
        },
        {
            "title": "C Event Tokenization Details",
            "content": "To train the Generalist IDM effectively, raw desktop interaction logs must be converted into structured representation that the model can understand. We represent the entire interaction sequence as stream of discrete event tokens. Each event corresponds to either an observation or an action. Observation events capture changes in the visual state of the environment, such as screen updates (Screen Events), while action events represent user inputs, including Keyboard Events (key presses and releases) and Mouse Events (clicks, movements, and scrolls). By tokenizing data at the event level, we unify heterogeneous inputs into consistent, sequential representation that can be modeled effectively using single decoder-only transformer. This representation accommodates both asynchronous observations and actions while preserving fine-grained temporal alignment between them. C.1 Event Token We append specialized tokens to the models vocabulary for desktop interaction modeling. Event structure tokens (<EVENT_START> and <EVENT_END>) delineate the boundaries of interaction sequences, while event type tokens (<KEYBOARD>, <MOUSE>, <SCREEN>) semantically categorize the modality of each event. Numeric encoding tokens (<0> to <9>) serve multiple purposes:"
        },
        {
            "title": "Game",
            "content": "Duration (h) Stardew Valley Minecraft Monster Hunter Wilds Dinkum Satisfactory Cyberpunk 2077 Medieval Dynasty Raft Core Keeper Euro Truck Simulator 2 Grounded Rainbow Six GTA 5 Brotato PUBG Counter-Strike 2 Apex Legends Slime Rancher Ready or Not Barony 69.7 62.8 63.3 60.8 59.8 58.5 58.4 58.0 58.0 57.3 57.2 56.3 54.1 52.6 50.7 49.8 48.7 33.3 29.0 16.7 Total 1054.8 Table 12: Pseudo-labeled Duration by Game (G-IDM). Total effective hours of successfully processed pseudo-labeled data per game. Mouse movement deltas are encoded using configurable base system (default: [2, 10, 10, 10]), allowing efficient representation of signed values within 1999 pixel range. Mouse scroll values are similarly quantized using base-10 tokens. Timestamps are encoded using temporal bases (default: [10, 10, 10]), covering 10-second window with 10ms resolution. Timestamps are cyclic, wrapping from 999 back to 000. Mouse interaction tokens include: Sign tokens (<SIGN_PLUS>, <SIGN_MINUS>) for indicating the direction of movement deltas, Mouse button tokens (<MB_0> to <MB_15>) for encoding mouse button flags in hexadecimal. Keyboard interaction tokens consist of: Virtual key code tokens (<VK_0> to <VK_255>) to represent all Windows virtual key inputs, Action tokens (<press>, <release>) to indicate key state transitions. This factorized token design creates modular, modality-specific token spaces while maintaining compact vocabulary. Mouse button flag definitions are provided in Table 13, and the full virtual key code mapping is shown in Table 14. C.2 Event Token Structure All event tokens follow consistent structure: <EVENT_START> < event_type >< timestamp >< event_detail > <EVENT_END>"
        },
        {
            "title": "Under Review",
            "content": "Flag Name RI_MOUSE_NOP RI_MOUSE_LEFT_BUTTON_DOWN/UP RI_MOUSE_RIGHT_BUTTON_DOWN/UP RI_MOUSE_MIDDLE_BUTTON_DOWN/UP 0x0010/0x0020 Middle button press/release Side button 4 press/release RI_MOUSE_BUTTON_4_DOWN/UP Side button 5 press/release RI_MOUSE_BUTTON_5_DOWN/UP Vertical scroll wheel RI_MOUSE_WHEEL Horizontal scroll wheel RI_MOUSE_HWHEEL 0x0001/0x0002 Left button press/release 0x0004/0x0008 Right button press/release 0x0040/0x0080 0x0100/0x0200 0x0400 0x0800 Description No operation Hex Value 0x0000 Table 13: Windows Raw Mouse Button Flags Key Name VK Code Description 1 LBUTTON 2 RBUTTON 3 CANCEL 4 MBUTTON 5 XBUTTON1 6 XBUTTON2 8 BACK 9 TAB 12 CLEAR 13 RETURN 16 SHIFT 17 CONTROL 18 MENU 19 PAUSE 20 CAPITAL 27 ESCAPE 32 SPACE 33 PRIOR 34 NEXT 35 END 36 HOME 37 LEFT 38 UP 39 RIGHT 40 DOWN 45 INSERT 46 DELETE Left mouse button Right mouse button Control-break Middle mouse button X1 mouse button X2 mouse button Backspace key Tab key Clear key Enter key Shift key Ctrl key Alt key Pause key Caps Lock Esc key Spacebar Page Up Page Down End key Home key Left arrow Up arrow Right arrow Down arrow Insert key Delete key Key Name KEY_0KEY_9 KEY_AKEY_Z LWIN RWIN APPS NUMPAD09 MULTIPLY ADD SUBTRACT DECIMAL DIVIDE F1F12 NUMLOCK SCROLL LSHIFT RSHIFT LCONTROL RCONTROL LMENU RMENU OEM_1 OEM_PLUS OEM_COMMA OEM_MINUS OEM_PERIOD OEM_2 OEM_3 VK Code Description 4857 6590 91 92 93 96105 106 107 109 110 111 112123 144 145 160 161 162 163 164 165 186 187 188 189 190 191 192 09 keys AZ keys Left Windows key Right Windows key Applications key Numpad 09 Numpad * Numpad + Numpad - Numpad . Numpad / F1F12 function keys Num Lock Scroll Lock Left Shift Right Shift Left Ctrl Right Ctrl Left Alt Right Alt ; : key = + key , < key - _ key . > key / ? key key Table 14: Windows Virtual Key Codes where: <EVENT_START> and <EVENT_END> are special tokens that delimit each event <timestamp> encodes the precise timing of the event in nanoseconds <event_type> specifies the type of event (e.g., <SCREEN>, <KEYBOARD>, <MOUSE>) <event_data> contains event-specific information C.3 Screen Events Screen events capture visual observations from the desktop environment. Each screen event contains an image token sequence: <EVENT_START><SCREEN> < timestamp >< image_tokens > <EVENT_END> For example: <EVENT_START><SCREEN><1><8><5><IMG_CONTEXT>256<EVENT_END> The timestamp <1><8><5> represents 185 time units, and the image is encoded using 256 visual tokens following the InternVL3 tokenization scheme."
        },
        {
            "title": "Under Review",
            "content": "C.4 Keyboard Events Keyboard events encode key press and release actions using virtual key code tokens: <EVENT_START><KEYBOARD> < timestamp >< vk_token >< action > <EVENT_END> For example: <EVENT_START><KEYBOARD><2><0><0><VK_32><release><EVENT_END> This represents key release event at timestamp 200, where <VK_32> corresponds to the spacebar. The action can be either <press> or <release>. C.5 Mouse Events Mouse events are the most complex among input modalities, as they encode continuous movement, discrete button states, and scroll actions. <EVENT_START><MOUSE><timestamp><dx_sign><dx_magnitude><dy_sign> <dy_magnitude><button_flags><scroll_data><EVENT_END> The optional <scroll_data> token is included only when the <button_flags> field indicates the presence of scroll wheel activity. Mouse Movement Example. Consider the following mouse event: <EVENT_START><MOUSE><2><4><5><SIGN_PLUS><0><0><0><2><SIGN_MINUS> <0><0><1><9><MB_4><MB_8><MB_0><SIGN_PLUS><0><EVENT_END> This token sequence is decoded as follows: Timestamp: <2><4><5> represents 2 100 + 4 10 + 5 = 245 time units. Mouse Displacement: The displacement uses separate sign and magnitude encoding: dx: <SIGN_PLUS><0><0><0><2> = +(0 1000 + 0 100 + 0 10 + 2) + 2 pixels dy: <SIGN_MINUS><0><0><1><9> = (0 1000 + 0 100 + 1 10 + 9) = 19 pixels (4) (5) Button Flags: <MB_4><MB_8><MB_0> encodes button states as hexadecimal digits: 0x48016 = 115210. This corresponds to: 0x400: Vertical scroll wheel event 0x080: Mouse button 4 (side button) released Scroll Data: <SIGN_PLUS><0> indicates no scroll delta (magnitude 0). Final Interpretation: Mouse moved dx = +2, dy = 19 pixels at timestamp 245, with scroll wheel activity and side button release."
        },
        {
            "title": "D Model Architecture Details",
            "content": "For Generalist-IDM, we adopt the InternVL3-1B model (Zhu et al., 2025), which integrates InternViT as the vision encoder and Qwen2.5 (Yang et al., 2024) as the language backbone. InternVL3 is trained with native multimodal pretraining and demonstrates strong performance on videotext interleaved tasks, making it suitable foundation for our work. We expand the models tokenizer by adding additional event tokens to represent events in our desktop data. Furthermore, we transfer the semantic initialization from corresponding regular language tokens to the newly added event tokens."
        },
        {
            "title": "E Training Details",
            "content": "The Generalist-IDM was trained on 8 H100 GPUs (80GB) for approximately 24 hours, totaling 192 H100-hours. The entire training process incurred cost of only $800 for training on 259 hours of human-collected data, highlighting the efficiency enabled by our OWA Toolkit. We used the following training schedules: Generalist-IDM: 5 epochs Specialist-IDM: 5 epochs Generalist-IDM (fine-tuning): 3 epochs VAPT (w/o pseudo): 3 epochs on the human-collected vision-action dataset VAPT (w/ pseudo): 1 epoch on the pseudo-labeled dataset, followed by 3 epochs on the human-collected dataset All experiments were conducted using identical hyperparameters: global batch size of 128, learning rate of 2 105, and the AdamW optimizer."
        },
        {
            "title": "F Evaluation Details",
            "content": "F.1 Generation Methods We implemented an efficient autoregressive inference pipeline for predicting keyboard and mouse actions from desktop screen captures or YouTube videos. Starting from MCAP files containing synchronized, timestamped data streams (screen captures and mouse/keyboard events), we resample the events at fixed intervals (50 ms for screen and mouse events, pass-through for keyboard inputs) and tokenize them as described in Appendix C. dynamic context manager maintains sliding window of recent events with efficient embedding caching, using token context length of 2048. To accelerate inference, we apply several optimization techniques, including PyTorch model compilation, FlashAttention, and mixedprecision computation with bfloat16. For multi-GPU inference, we leverage NVIDIA MPS. The generated token sequences are decoded back into structured MCAP events and subsequently evaluated. For pseudo-labeling YouTube videos, we generate MCAP files consisting of two-minute segments of screen events, excluding the first minute and last two minutes to mitigate the influence of introductions and outros. Throughout this work, we evaluate the Generalist-IDM using fully autoregressive action decoding, both for the experiments in Section 5 and for pseudo-labeling YouTube videos. Teacher forcing was not used. F.2 Evaluation Metrics We evaluate the performance of Generalist-IDM using set of fine-grained metrics that capture the correctness of predicted actions. For mouse movements, we use Pearson correlation (X/Y axes) and Scale ratio (X/Y axes) to capture the directional and spatial shape of the path and the temporal ordering of points. For discrete actions, such as keyboard presses and mouse button clicks, we report classification accuracy. All metrics are calculated over non-overlapping 50ms temporal bins, enabling precise alignment between predicted and ground truth event sequences. The Scale ratio metrics, including scale_ratio_x and scale_ratio_y, measure relative scaling differences between ground-truth and predicted mouse movements along the and axes. They quantify how much predictions are stretched or compressed compared to the source movements. Formally, for bins with source vectors si = (si,x, si,y) and predicted vectors di = (di,x, di,y):"
        },
        {
            "title": "Under Review",
            "content": "scale_ratio_x = scale_ratio_y = 1 1 1 1 n"
        },
        {
            "title": "Pn\nPn",
            "content": "i=1 si,x i=1 di,x i=1 si,y i=1 di,y , . (6) (7) To ensure interpretability, ratios < 1 are inverted so that all values are 1. Interpretation: 1.0: perfect scaling match between source and prediction > 1.0: scaling mismatch, where larger values indicate greater discrepancy"
        },
        {
            "title": "G Downstream Details",
            "content": "G.1 Robot Manipulation We train manipulation policy identical to openvla-oft (Kim et al., 2025), except that the visionlanguage backbone is replaced with InternVL3-1B (or its OWA variant). The policy retains the L1 regression head for continuous action prediction, employs bidirectional attention in the policy stack, and uses parallel decoding with action chunking (chunk size = 8). The inputs consist of third-person image, wrist-camera image, the robot proprioceptive state, and language instruction, resulting in two images per step (exocentric and egocentric). Training uses filtered dataset where unsuccessful demonstrations are removed. Optimization follows the openvla-oft recipe: LoRA rank 32, learning rate 5 104, batch size 8, and image augmentation enabled. Linear decay is applied after 10,000 steps, with total training budget of 15,005 steps. Checkpoints are saved every 1,000 steps, keeping both periodic and latest versions. Training is conducted on single node with 8 GPUs via torchrun, with the same launch flags as openvla-oft, except for swapping the backbone to InternVL3-1B/OWA. Evaluation is performed on the LIBERO benchmark (Liu et al., 2023), which includes four suites of manipulation tasks: (1) Spatial, varying scene layouts with fixed objects; (2) Object, varying the set of objects in fixed scene; (3) Goal, testing goal-conditioned behavior; and (4) Long (LIBERO-10), long-horizon compositional tasks involving diverse objects, layouts, and goals. We report the average success rate over 500 episodes for each suite. G.2 Robot Navigation We established baseline following CANVAS (Choi et al., 2024) by training an InternVL3based model architecture on the COMMAND dataset. The baseline model was initialized with the default InternVL3 weights, whereas the VAPT w/o pseudo and VAPT pseudo were trained from pretrained weights. All models were trained with full parameter unfreezing. For optimization, we employed AdamW with separate learning rates: 2 105 for the LLM, and 5105 for both the projector and vision encoder. Training was conducted with batch size of 32 over 5 epochs, and each model utilized 128 waypoint tokens. In the main experiments, inference of CANVAS models was performed on single NVIDIA H100 GPU. All evaluations were repeated three times per test dataset with randomized initial orientations."
        },
        {
            "title": "H Ethics Statement",
            "content": "We acknowledge and adhere to the ICLR Code of Ethics."
        },
        {
            "title": "Under Review",
            "content": "Human Data Collection. Our dataset was collected from 14 volunteer annotators who provided informed consent for gameplay recordings. Participants were fully informed about screen capture and input logging procedures and could withdraw at any time. All data underwent automated and manual review to remove any personally identifiable information before research use. Public Data Usage. We processed only publicly available YouTube videos with permissive licenses for pseudo-labeling. Our focus on gaming content inherently minimizes privacy concerns compared to general desktop recording, as gaming interfaces rarely contain sensitive personal information. Transparency and Responsible Release. To ensure responsible use, we will publicly release all code, data collection tools, and model weights with comprehensive documentation. We acknowledge that vision-action models could have dual-use potential; however, our focus on standardized gaming environments and transparent methodology helps mitigate misuse risks. Our computational approach (requiring only modest GPU resources) democratizes access while reducing environmental impact compared to large-scale training paradigms."
        },
        {
            "title": "I Limitations",
            "content": "We evaluate our approach exclusively on simulation benchmarks to establish reproducible baselines, with real robot validation deferred to future work. The differential impact of pseudo-labels (improving navigation but degrading manipulation) suggests task-specific transfer mechanisms that require further investigation. Our dataset focuses primarily on gaming interactions, which may not capture the full spectrum of desktop activities relevant to general-purpose robotics. Despite these constraints, our framework democratizes embodied AI research by reducing storage requirements by 152 and training costs to under $1000, making large-scale vision-action pretraining accessible to resource-limited academic labs."
        }
    ],
    "affiliations": [
        "MAUM.AI",
        "Seoul National University"
    ]
}