{
    "paper_title": "Taipan: Efficient and Expressive State Space Language Models with Selective Attention",
    "authors": [
        "Chien Van Nguyen",
        "Huy Huu Nguyen",
        "Thang M. Pham",
        "Ruiyi Zhang",
        "Hanieh Deilamsalehy",
        "Puneet Mathur",
        "Ryan A. Rossi",
        "Trung Bui",
        "Viet Dac Lai",
        "Franck Dernoncourt",
        "Thien Huu Nguyen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Efficient long-context language modeling remains a significant challenge in Natural Language Processing (NLP). While Transformers dominate language tasks, they struggle with long sequences due to quadratic computational complexity in training and linearly scaling memory costs during inference. Recent State Space Models (SSMs) such as Mamba offer alternatives with constant memory usage, but they underperform in tasks requiring extensive in-context retrieval. We introduce Taipan, a novel hybrid architecture that combines Mamba-2 with Selective Attention Layers (SALs). These SALs identify tokens requiring long-range interactions, remove less important features, and then augment their representations using the attention module. This approach balances Mamba's efficiency with Transformer-like performance in memory-intensive tasks. By constraining the attention budget, Taipan extends accurate predictions to context lengths of up to 1 million tokens while preserving computational efficiency. Our experiments demonstrate Taipan's superior performance across various scales and tasks, offering a promising solution for efficient long-context language modeling."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 2 ] . [ 1 2 7 5 8 1 . 0 1 4 2 : r TAIPAN: EFFICIENT AND EXPRESSIVE STATE SPACE LANGUAGE MODELS WITH SELECTIVE ATTENTION Chien Van Nguyen1, Huy Huu Nguyen1, Thang Pham2 Ruiyi Zhang3, Hanieh Deilamsalehy3, Puneet Mathur3, Ryan A. Rossi3 Trung Bui3, Viet Dac Lai3, Franck Dernoncourt3, Thien Huu Nguyen1 1University of Oregon 2Auburn University 3Adobe Research {chienn,huy,thienn}@uoregon.edu {bui,daclai,franck.dernoncourt}@adobe.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Efficient long-context language modeling remains significant challenge in Natural Language Processing (NLP). While Transformers dominate language tasks, they struggle with long sequences due to quadratic computational complexity in training and linearly scaling memory costs during inference. Recent State Space Models (SSMs) such as Mamba offer alternatives with constant memory usage, but they underperform in tasks requiring extensive in-context retrieval. We introduce Taipan, novel hybrid architecture that combines Mamba-2 with Selective Attention Layers (SALs). These SALs identify tokens requiring long-range interactions, remove less important features, and then augment their representations using the attention module. This approach balances Mambas efficiency with Transformer-like performance in memory-intensive tasks. By constraining the attention budget, Taipan extends accurate predictions to context lengths of up to 1 million tokens while preserving computational efficiency. Our experiments demonstrate Taipans superior performance across various scales and tasks, offering promising solution for efficient long-context language modeling."
        },
        {
            "title": "INTRODUCTION",
            "content": "Transformer-based architectures Vaswani (2017); Brown (2020) have revolutionized Natural Language Processing (NLP), delivering exceptional performance across diverse language modeling tasks Touvron et al. (2023). This success stems from their ability to capture complex word dependencies using the self-attention mechanism. In addition, Transformers are highly scalable and well-suited for parallel training on large datasets. However, despite their success, they still face notable challenges when handling long-context sequences. Specifically, the self-attention mechanism suffers from quadratic computational complexity, and the memory requirement grows linearly with context length during inference, as the model must store key-value vectors for the entire context. These factors impose practical constraints on sequence length due to the high computational and memory costs. To this end, recent advancements in recurrent-based architectures, particularly State Space Models (SSMs) Gu et al. (2021b;a), have emerged as promising alternatives for efficient language modeling Gu & Dao (2023); Dao & Gu (2024). SSMs offer constant memory usage during inference, and architectures like Mamba-2 Dao & Gu (2024), variant of SSMs, have demonstrated performance comparable to Transformers in certain language tasks Waleffe et al. (2024). Some studies even suggest that SSMs can outperform Transformers in areas like state tracking Merrill et al. (2024) due to their Markovian nature. However, despite these advancements, SSM-based models still fall short in scenarios requiring in-context retrieval or handling complex long-range dependencies Arora et al. (2024); Waleffe et al. (2024). To address these challenges, we introduce Taipan, hybrid architecture that combines the efficiency of Mamba with enhanced long-range dependency handling through Selective Attention Lay1 Figure 1: Model Performance Comparison. a) Perplexity across different context lengths. Lower perplexity indicates better performance. b) Latency comparison of models at various generation lengths. Taipan exhibits significantly lower latency and superior scaling compared to other strong baselines for longer sequences. ers (SALs). While Mamba is highly efficient, it relies on the Markov assumptionwhere predictions are based solely on the last hidden statewhich can lead to information loss for tokens that need interactions with distant tokens. To mitigate this, Taipan incorporates SALs that strategically select key tokens in the input sequence requiring long-range dependencies. These selected tokens first undergo feature refinement to remove unimportant information, and then are passed through an attention module to capture long-range dependencies. Less critical tokens bypass the attention step, as we hypothesize that their Markovian representations from Mamba contain sufficient information for accurate prediction, obviating the need for additional attention-based augmentation. This selective approach enables Taipan to balance Mambas computational efficiency with enhanced long-range modeling capabilities. SALs play crucial role in Taipans design, both in enhancing performance and ensuring computational efficiency. By focusing the attention mechanism on subset of important tokens, SALs reduce the computational costs that come from attention modules. This targeted approach enables Taipan to excel in memory-intensive tasks while maintaining efficiency during both training and inference. Importantly, Taipan retains the linear memory usage characteristic of SSMs, offering significant advantage over traditional Transformer models in handling extremely long sequences. We scale Taipan to 190M, 450M, and 1.3B parameters, pre-training on 100B tokens. Experimental results demonstrate Taipans superior performance across wide range of tasks. In zero-shot language modeling evaluations, Taipan consistently outperforms both Transformer and Mamba baselines, showcasing its strong general language understanding capabilities. More notably, in memoryintensive tasks such as long-context retrieval and structured information extraction, Taipan exhibits significant improvements over Mamba-2, addressing key limitation of existing recurrent-based models. Furthermore, Taipan demonstrates remarkable extrapolation capabilities, maintaining high performance on sequences up to 1 million tokens in context-length - while preserving efficient generation capabilities. This combination of broad task proficiency, superior performance in memoryintensive scenarios, and exceptional long-context modeling positions Taipan as versatile and powerful architecture for advanced language processing tasks."
        },
        {
            "title": "2 BACKGROUND",
            "content": "This section briefly overviews the foundational architectures relevant to our work. We first review Causal Self-Attention Vaswani (2017), the core mechanism of Transformer models. We then discuss Linear Attention Katharopoulos et al. (2020), an efficient variant that achieves linear complexity. Finally, we examine Mamba-2, recent architecture that generalizes Linear Attention using structured state-space models (SSMs) Dao & Gu (2024). We emphasize how each model balances computational efficiency and recall accuracy, particularly in memory-intensive tasks ."
        },
        {
            "title": "2.1 CAUSAL SELF-ATTENTION",
            "content": "Causal Self-Attention is the key component in Transformer architectures that allows each token in sequence to attend to all other previous tokens (Vaswani, 2017). Given an input sequence = [x1, . . . , xL] RLd, where is the sequence length and is the embedding dimension, self-attention firsts computes the query, key, and value vectors for each token via linear projections: qi = WQxi, ki = WKxi, vi = WV xi where WQ, WK, WV Rdd are learnable weight matrices. Then, the attention output oi for each token xi will be calculated as weighted sum of the value vectors over the distribution of similarity matrix between its query vector and previous key vectors: oi = (cid:88) t= exp(q kt/ j=1 exp(q d) kj/ (cid:80)i vt d) The non-linear softmax distribution allows the models to capture intricate relationships between tokens, and concentrate on salient features Qin et al. (2022); Zhao et al. (2019). As such, selfattention can encode complex language patterns and long-range dependencies that are crucial for complex language understanding and generation tasks. 2.2 LINEAR ATTENTION To address the quadratic complexity, recent work has shown that it is possible to achieve linear complexity with the attention mechanism by replacing the softmax attention with dot-product attention (Shen et al., 2021; Katharopoulos et al., 2020). Given feature transformation ϕ(x), causal self-attention can be rewritten as: oi = (cid:88) t= ϕ(qi)ϕ(kt) j=1 ϕ(qi)ϕ(kj) (cid:80)i vt Then, using the associate property of matrix multiplication, this can be reformulated as: oi = ϕ(qi) (cid:80)i ϕ(qi) (cid:80)i t=1 ϕ(kt)v t=1 ϕ(kt) Let Si = (cid:80)i form: t=1 ϕ(kt)v and zi = (cid:80)i t=1 ϕ(kt). We can then rewrite the equation in recurrent Si = Si1 + ϕ(ki)v Siϕ(qi) ϕ(qi) oi = Siϕ(qi) This formulation allows for efficient training and inference. Let Q, K, RLd be the query, key, and value matrices of the sequence input X. During training, we can use the matrix multiplication form: = (QK ML)V, where ML is causal mask. At inference time, we can use the recurrent form for efficient sequential processing. However, despite its computational efficiency, linear attention has notable limitations compared to softmax attention. The dot-product approximation in linear attention lacks the nonlinear normalization of softmax, often resulting in more uniform distribution of attention weights Han et al. (2023). This uniformity can impair the models ability to focus sharply on specific and relevant tokens. Consequently, linear attention models may underperform in tasks requiring precise in-context retrieval or focused attention on particular input segments Han et al. (2023). 2.3 MAMBA-2 Mamba Gu & Dao (2023) is variant of structured state space models (SSMs) that uses the selective data-dependent mechanism. Mamba-2 Dao & Gu (2024) builds on this foundation, revealing deep 3 Figure 2: An overview of the Taipan architecture. connections between SSMs and linear attention Katharopoulos et al. (2020) through the framework of structured state-space duality (SSD). The core of Mamba-2 can be defined by using the recurrent form: ht = Atht1 + Btxt ot = Ctht where At is further simplified to scalar multiplied by the identity matrix. This formulation allows Mamba-2 to be interpreted as generalization of linear attention. The key insight of Mamba-2 is that this recurrence can be equivalently expressed as matrix multiplication: Ot = (Lt CtB )Xt where is 1-semiseparable matrix. This matrix form reveals the duality between the recurrent (linear-time) and attention-like (quadratic-time) computations. Also, the 1-semiseparable matrix encodes the temporal dependencies, while CB represents content-based interactions similar to attention. This formulation generalizes linear attention, which can be seen as special case where is the all-ones lower triangular matrix. While Mamba-2 is efficient, it shares the same limitations as Linear Attention in terms of precise memory recall Arora et al. (2024); Wen et al. (2024), leading to reduced performance in tasks that demand accurate retrieval of specific sections in the input sequence."
        },
        {
            "title": "3 TAIPAN MODEL",
            "content": "To address the limited modeling capabilities of Mamba-2 and Linear Attention while preserving their computational efficiency, we introduce Taipan, new architecture for sequence encoding in language modeling. In Taipan, we strategically incorporate Selective Attention Layers (SALs) within the Mamba framework, as shown in Figure 2. SALs are inserted after every Mamba-2 blocks, creating hybrid structure that combines Mamba-2s efficiency with Transformer-style attention for effective sequence representation. The core of SALs is gating network that identifies important tokens for enhanced representation modeling. These tokens undergo two phases: (1) feature refinement to filter out irrelevant information and (2) representation augmentation via softmax attention. This allows Taipan to capture complex, non-Markovian dependencies when necessary. Taipan processes input through Mamba-2 blocks, with SALs periodically refining key token representations. These enhanced representations are then passed into the subsequent Mamba-2 layers, 4 Figure 3: Attention mechanisms in Taipans Selective Attention Layers. White areas indicate no attention. (a) Full Causal Attention (b) Sliding Window Attention (w = 4) (c) Selective Attention (C = 0.3, = 5) influencing further processing. This hybrid structure balances Mamba-2s efficiency with the expressive power of SALs, enabling Taipan to excel in tasks requiring both speed and accurate information retrieval. The following sections detail each components structure and function. 3.1 SELECTIVE ATTENTION LAYERS Selective Attention Layers (SALs) are the key innovation in Taipan, designed to enhance the models ability to focus on critical tokens while maintaining overall efficiency. These layers employ lightweight gating network Gθ to dynamically determine which tokens should undergo softmax attention processing. For each token hidden representation hi in the input sequence, the gating network computes score vector: si = Gθ(hi) (1) where Gθ : Rd R2 is parameterized by θ. This score vector si = [si,0, si,1] serves two purposes: 1) it is used to generate binary mask mi for token selection, and 2) it guides feature refinement. To maintain differentiability while allowing for discrete token selection, we employ the StraightThrough Gumbel-Softmax trick Jang et al. (2017). binary mask mi is generated from si to select tokens during the forward pass of the network: mi = argmax(GumbelSoftmax(si, τ )) (2) where τ is the temperature parameter. hi will only be selected for attention processing if mi = 1. For the backward pass, we instead use continuous Gumbel-Softmax approximation of mi to achieve computation differentiability for the network: mi = I[mi = 0] exp((si,0 + g0)/τ ) + I[mi = 1] exp((si,1 + g1)/τ ) exp((si,0 + g0)/τ ) + exp((si,1 + g1)/τ ) (3) where I[] is the indicator function, and g0 and g1 are i.i.d samples from the Gumbel(0, 1) distribution. In this way, we are able to train our entire model, including the gating network, in an end-to-end fashion for language modeling. For the selected tokens (those with mask value mi of 1), we compute their attention-based representations: oi = Attention(qi, K, V) (4) where qi is the query vector for the i-th selected token (denoted hs value matrices for previous tokens. ), and and are the key and In our model, the score vector si is also used to refine the representations of selected tokens. We employ the softmax of si to compute the mixing weights: [1 αi, αi] = softmax(si). The final output for selected token hs is weighted combination: = (1 αi)hs hs + αioi (5) As such, Taipan can adaptively preserve key information in hs while enriching the representation In other words, αi acts as the data-dependent factor, filtering out with the attention output oi. unimportant features from the original representation while integrating richer information from the attention outputs. Here, it is important to note that unselected tokens (i.e., mi = 0) skip the attention module and retain their original representations from Mamba-2. Finally, all token representations are passed through residual SwiGLU Shazeer (2020) layer: = + SwiGLU(h) (6) This final transformation ensures that all token representations undergo consistent non-linear processing before being passed to the next layer in the network, enhancing the models ability to capture complex dependencies."
        },
        {
            "title": "3.2 SLIDING WINDOW ATTENTION",
            "content": "To maintain linear time complexity while leveraging the benefits of attention, Taipan employs Sliding Window Attention (SWA) Beltagy et al. (2020). SWAs computational complexity scales linearly with sequence length, allowing Taipan to handle theoretically unlimited context lengths during inference. Importantly, the combination of Selective Attention and Sliding Window Attention in Taipan leads to significantly sparser attention weight map compared to full attention or standard windowed attention (Figure 3), thus enhancing the computational efficiency of Selective Attention for processing long sequences for our model. In addition, the sparser attention map allows us to afford longer sliding window (i.e., = 2048 in our work) to effectively capture longer-range dependencies for input sequences. In this way, our designed Taipan architecture offers mechanism to balance the efficient processing of long sequences with the ability to capture important long-range dependencies, thereby addressing key limitation of existing efficient attention mechanisms. Finally, removing positional embeddings from the Attention Module improves extrapolation capabilities, suggesting that the model can better generalize temporal relationships. We explore this impact of positional embeddings in more detail in Section 5.2. 3.3 TRAINING AND INFERENCE To better balance efficiency and expressiveness, we introduce an attention budget constraint. Given predefined budget C, representing the desired fraction of tokens to receive attention, we incorporate constraint loss into our training objective: (cid:80)L (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:88) Lconstraint = i=1 mi (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) 2 Here, is the number of SALs, is the sequence length, and (cid:80)L i=1 mi represents the number of tokens selected for attention processing. During training, we employ the Straight Through Gumbel Softmax estimator for mi in the backward pass Jang et al. (2017); Bengio et al. (2013), ensuring differentiability while maintaining discrete token selection in the forward pass, thereby enabling end-to-end training of the entire model. As such, our overall training objective includes standard cross-entropy loss LCE for language modeling and the budget constraint term: = LCE+λLconstraint, where λ is hyperparameter. (7) n= During inference, Taipan processes input tokens sequentially through Mamba-2 blocks. At each Selective Attention Layer, the gating network Gθ computes score vector si = Gθ(hi) for each token representation hi. This score computes binary mask mi to determine if hi should be used for attention processing. Consequently, our selective attention approach maintains Mamba-2s efficiency for most tokens while applying targeted attention to critical elements, enabling effective long-range dependency modeling with minimal computational overhead."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "We conducted extensive experiments to evaluate Taipans performance across various scales and tasks. Our evaluation strategy focuses on three main areas: (1) zero-shot evaluation on diverse 6 benchmarks to demonstrate Taipans general language understanding capabilities (Section 4.2), (2) in-context retrieval tasks to assess Taipans ability to retrieve information from historical contexts (Section 4.3), and (3) extrapolation ability in long-context scenarios to evaluate performance on extremely long sequences (Section 4.4)."
        },
        {
            "title": "4.1 EXPERIMENTAL SETUP",
            "content": "We evaluate Taipan across three model sizes: 190M, 450M, and 1.3B parameters. To ensure comprehensive and fair comparison, we benchmark Taipan against three strong baselines: Transformer++ Touvron et al. (2023): An enhanced version of the LLaMA architecture Touvron et al. (2023), incorporating Rotary Positional Embeddings Su et al. (2024), SwiGLU Shazeer (2020), and RMSNorm Zhang & Sennrich (2019). Mamba-2 Dao & Gu (2024): state-of-the-art linear RNN model based on State Space Models (SSMs). Each Mamba-2 block consists of depthwise convolutional layer Poli et al. (2023); Gu & Dao (2023), an SSM layer Dao & Gu (2024), and MLP layers. Jamba Lieber et al. (2024): hybrid model combining full Causal Self-Attention layers (with Rotary Position Embedding Su et al. (2024)) and Mamba-2 layers. Unlike Taipan, Jamba uses full Causal self-attention instead of selective attention, retains positional embeddings, and lacks feature refinement mechanism. Implementation Details We train all models from scratch in three configurations: 190M, 450M, and 1.3B parameters. The training process is consistent across configurations with the following hyperparameters: batch size of 0.5M tokens per step, cosine learning rate schedule with 2000 warm-up steps, and AdamW Loshchilov (2017) optimization with peak learning rate of 5e 4 decaying to final rate of 1e 5. We apply weight decay of 0.01 and use gradient clipping with maximum value of 1.0. All models are trained with fixed context length of 4096 tokens. The training data size varies by model scale: the 190M model is trained on 27 billion tokens, while the 450M and 1.3B models are trained on 100 billion tokens. The dataset details can be found in Appendix A. For Taipan-specific implementation, we use hybrid ratio of 6 : 1, inserting Selective Attention Layer (SAL) after every 6 Mamba-2 Blocks. The Mamba-2 blocks are kept identical to the original work Dao & Gu (2024). We set the attention capacity = 0.15. The sliding window attention mechanism uses window size (w) of 2048 tokens. Params & Data Model Wino. PIQA Hella. ARCE ARCC OB. Truth. RACE BoolQ Avg. Transformer++ 190M Mamba Jamba 27B Taipan Transformer++ 450M Mamba Jamba 100B 1.3B 100B Taipan Transformer++ Mamba Jamba Taipan 47.1 49.6 49. 51.0 51.5 52.7 53.1 53.0 53.8 55.2 54.7 57.0 60.9 60.7 60.3 62.6 67.6 68.9 69.3 69.6 71.6 73.0 73. 74.9 27.9 29.3 29.2 29.4 42.3 42.7 44.3 46.6 53.8 55.6 55.8 57.9 42.2 45.3 46. 46.7 60.8 61.4 62.6 65.6 63.2 70.7 69.7 71.2 20.5 21.8 21.4 20.7 27.7 27.1 28.7 32.9 36.3 38.0 37. 39.3 18.9 20.6 18.5 21.8 33.4 34.0 34.4 36.6 36.4 39.0 41.8 40.4 42.9 40.8 39. 41.1 39.2 38.5 37.5 38.6 44.0 39.9 40.4 43.0 25.4 27.2 27.4 26.6 30.5 29.3 31.3 30.7 31.2 32.0 32. 34.4 57.2 59.3 58.6 58.7 54.7 53.2 55.7 60.4 59.4 61.8 59.2 61.5 38.1 39.4 39. 39.9 45.3 45.3 46.3 48.2 49.9 51.7 51.8 53.3 Table 1: Zero shot results of Taipan against baseline models. 4.2 LANGUAGE MODELING PERFORMANCE We report the zero-shot performance of Taipan and baseline models on diverse set of commonsense reasoning and question-answering tasks. These include Winograd (Wino.) Sakaguchi et al. 7 (2021), PIQA Bisk et al. (2020), HellaSwag (Hella.) Zellers et al. (2019), ARC-easy and ARCchallenge (ARCe & ARCc) Clark et al. (2018), OpenbookQA (OB.) Mihaylov et al. (2018), TruthfulQA (Truth.) Lin et al. (2021), RACE Lai et al. (2017), and BoolQ Clark et al. (2019). It is worth noting that these tasks are brief and do not involve in-context learning, thus inadequately demonstrating long-context modeling or in-context learning retrieval abilities. Table 1 presents the zero-shot results for models of three sizes: 190M, 450M, and 1.3B parameters. The results are evaluated using the lm-evaluation-harness1 Gao et al. (2024) framework. As can be seen, Taipan consistently outperforms the baseline models across most tasks for all model sizes. Notably, the performance gap widens as the model size increases, with the 1.3B Taipan model showing significant improvements over other baselines. This suggests that Taipans architecture effectively captures and utilizes linguistic patterns, even in tasks that do not fully showcase its longcontext modeling capabilities. 4.3 IN-CONTEXT RECALL-INTENSIVE PERFORMANCE To evaluate Taipans proficiency in precise in-context retrieval, we assessed all models on set of recall-intensive tasks Arora et al. (2024). These tasks are designed to test models ability to extract and utilize information from longer contexts, capability particularly relevant to Taipans architecture. Our evaluation suite includes two types of tasks: structured information extraction and question answering. For structured information extraction, we used the SWDE and FDA tasks Arora et al. (2024), which involve extracting structured data from HTML and PDF documents, respectively. To assess question-answering capabilities, we employed SQuAD Rajpurkar et al. (2018), which requires models to ground their answers in provided documents. 450M Params Model 43.0 27.9 35.4 41.4 36.6 16.7 29.4 32.9 48.7 9.8 36.6 39. 18.1 12.5 16.3 17.8 SWDE FDA SQuAD Avg. Transformer++ Mamba Jamba Taipan Table 4 demonstrates Taipans significant performance advantages over both Mamba and Jamba in in-context retrieval tasks. Notably, Taipan achieves this superiority while consuming fewer computational resources than Jamba, which utilizes full attention mechanisms. This efficiency is attributed to Taipans architecture, which combines Mamba-like elements with selective attention mechanisms, allowing less important features. it We also notice that Transformers excel at memory-intensive tasks in this experiment; however, they are constrained by linear memory scaling with sequence length, limiting their effectiveness and applicability for very long sequences. In contrast, Taipan maintains constant memory usage, offering more efficient solution for processing long documents. Transformer++ Mamba Jamba Taipan Figure 4: Performance on in-context retrieval tasks. 41.2 31.2 33.4 36.9 64.5 32.3 49.7 59.7 56.6 37.4 46.5 52.7 64.2 48.6 56.4 61.5 to filter out 1.3B 4.4 LONG-CONTEXT EXTRAPOLATION Figure 1 illustrates Taipans superior performance in handling extended sequences compared to Transformer, Jamba, and Mamba models. In perplexity evaluations across context lengths from 1K to 1M tokens (Figure 1a), Taipan yields the lowest perplexity, particularly excelling beyond the training context length. This performance contrasts sharply with other models: Transformers struggle with longer contexts due to quadratic computational complexity and linear memory scaling with sequence length, often leading to out-of-memory errors. Jamba, despite its hybrid nature, faces similar challenges due to its use of full attention mechanisms. Both Transformer and Jamba models exhibit limited extrapolation ability beyond their training context lengths. Mamba, while more efficient than Transformers and Jamba, still shows performance degradation for very long sequences. 1https://github.com/EleutherAI/lm-evaluation-harness 8 Latency comparisons (Figure 1b) further highlight Taipans exceptional efficiency. It demonstrates the lowest latency among all models, with linear scaling across sequence lengths. This contrasts with the quadratic scaling of Transformers and higher latency growth of Jamba. Notably, Taipan consistently outperforms Mamba-2, primarily due to its selective attention mechanism."
        },
        {
            "title": "5 ABLATION STUDY",
            "content": "We conducted comprehensive ablation study to investigate the effect of the two key components in Taipans architecture, i.e., the attention budget capacity and the inclusion of Positional Embeddings in the SALs, on its performance and efficacy."
        },
        {
            "title": "5.1 EFFECT OF ATTENTION BUDGET CAPACITY",
            "content": "Our first experiment aimed to determine the optimal value of Capacity that would maintain computational efficiency while maximizing performance on downstream tasks. We trained multiple variants of Taipan, each with 1.3B parameters, using different Capacity values: 0.10, 0.15, 0.20, and 0.25. Each variant was trained for 24, 000 steps, allowing us to observe both the immediate impact of different values and their effect on model performance over time. We evaluated the performance of each variant at regular intervals on two representative tasks: SWDE Arora et al. (2024) (for structured information extraction) and HellaSwag Zellers et al. (2019) (for commonsense reasoning). These tasks were chosen to assess both the models ability to handle long-context retrieval and its general language understanding capabilities. Figure 5: Effect of Attention Budget Capacity on Taipans Performance As illustrated in Figure 5, Taipan achieves optimal performance with Capacity = 0.15. We observed that increasing beyond 0.15 does not lead to significant improvements in results while increasing computational costs. Conversely, reducing below 0.15 resulted in noticeable drop in performance on tasks requiring precise in-context retrieval or complex long-range dependencies. These findings support our hypothesis that computational demands vary across tokens, with many adequately represented by Mambas Markovian structure without requiring attention mechanisms. By selectively applying attention only to tokens that benefit from it, Taipan optimizes resource allocation, enabling high performance while improving computational efficiency. 5. IMPACT OF POSITIONAL EMBEDDINGS Our second experiment investigated the impact of Positional Embeddings in Taipans Attention mechanism, focusing on the models ability to handle and generalize to various context lengths. We trained two variants of the 1.3B parameter Taipan model for 24, 000 steps with fixed context length of 4096 tokens. One variant incorporates Rotary Positional Embeddings Su et al. (2024) in the Selective Attention layers, while the other excludes them. Figure 6 illustrates the performance of both variants in terms of perplexity across different context lengths. 9 The results reveal that Taipan without Positional Embeddings performs superiorly in generalizing context lengths beyond the training context. Both variants show comparable performance for sequences similar to or shorter than the training context length. However, as the sequence length increases, the performance gap between the two variants widens, with Taipan without Positional Embeddings maintaining lower perplexity scores. This suggests that the absence of Positional Embeddings enables more robust scaling to longer sequences. We attribute this improved generalization to the models increased reliance on attention representation rather than positional biases."
        },
        {
            "title": "6 RELATED WORK",
            "content": "Our approach builds on foundation of relevant previous research. We will now discuss key studies that inform our methodology. State Space Models: SSMs have emerged as promising approach in attention-free architectures for language processing tasks. These models offer improved computational and memory efficiency compared to traditional attention-based models. The development of SSMs has progressed through several key iterations: S4 Gu et al. (2021a) introduced the first structured SSM, focusing on diagonal and diagonal plus low-rank (DPLR) structures. Subsequent variants like DSS Gupta et al. (2022), S4D Gu et al. (2022), and S5 Smith et al. (2023) improved on this foundation. Frameworks like GSS Mehta et al. (2023), H3 Fu et al. (2023), and RetNet Sun et al. (2023) incorporated SSMs into broader neural network architectures, often combining them with gating mechanisms or efficient attention approximations. Recently, Mamba Gu & Dao (2023) introduced time-varying or selective SSMs, which addresses limitations of static dynamics in previous SSMs by incorporating input-dependent state transitions, leading to improved performance in various tasks. Figure 6: Perplexity comparison of Taipan variants with and without Positional Embeddings across different context lengths. Lower perplexity indicates better performance. Hybrid Architecture: Several recent studies H3 Fu et al. (2023), Griffin De et al. (2024), Zamba Glorioso et al. (2024), Jamba Lieber et al. (2024) suggest the potential of blending SSM and the attention mechanism. These hybrid designs show promise in outperforming both traditional Transformers and pure SSM architectures, such as Mamba, particularly in scenarios requiring in-context learning capabilities. Long Context Models: Recent advancements in sequence modeling have pushed the boundaries of context length, each with distinct approaches and challenges. Recurrent Memory Transformer Bulatov et al. (2023) demonstrated 1M token processing, but primarily on synthetic memorization tasks. LongNet Ding et al. (2023) proposed scalability to 1B tokens, yet practical evaluations were limited to sequences under 100K tokens. Hyena/HyenaDNA Poli et al. (2023); Nguyen et al. (2023) claimed 1M token context, but faced efficiency issues at longer lengths. Mamba Gu & Dao (2023) showed consistent improvements up to 1M tokens in DNA modeling and competitive performance across various language tasks."
        },
        {
            "title": "7 CONCLUSION",
            "content": "Taipan presents significant advancement in long-context language modeling by combining the efficiency of Mamba with strategically placed Selective Attention Layers. Our experiments demonstrate Taipans superior performance across various scales and tasks, particularly in scenarios requiring extensive in-context retrieval, while maintaining computational efficiency. key insight is that not all tokens require the same computational resources. Taipans architecture leverages this observation through its selective attention mechanism, which dynamically allocates computational resources 10 based on token importance. This hybrid approach addresses limitations of both Transformers and SSMs, offering promising solution for efficient, large-scale language processing. Future work could explore further optimizations and applications of this architecture."
        },
        {
            "title": "REFERENCES",
            "content": "Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri Rudra, and Christopher Re. Simple linear attention language models balance the recall-throughput tradeoff. arXiv preprint arXiv:2402.18668, 2024. Iz Beltagy, Matthew Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von URL https://huggingface.co/datasets/ Smollm-corpus, 2024. Werra. HuggingFaceTB/smollm-corpus. Yoshua Bengio, Nicholas Leonard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. CoRR, abs/1308.3432, 2013. URL http://arxiv.org/abs/1308.3432. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 74327439, 2020. Tom Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. Aydar Bulatov, Yuri Kuratov, and Mikhail S. Burtsev. Scaling transformer to 1m tokens and beyond with rmt. ArXiv, abs/2304.11062, 2023. URL https://api.semanticscholar.org/ CorpusID:258291566. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. arXiv preprint arXiv:2405.21060, 2024. Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, and Caglar Gulcehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024. URL https://arxiv.org/abs/2402.19427. Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, and Furu Wei. Longnet: Scaling transformers to 1, 000, 000, 000 tokens. CoRR, abs/2307.02486, 2023. doi: 10.48550/ARXIV.2307.02486. URL https://doi.org/10. 48550/arXiv.2307.02486. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Daniel Fu, Tri Dao, Khaled Kamal Saab, Armin Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Towards language modeling with state space models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview. net/forum?id=COZDy0WYGg. 11 Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 07 2024. URL https://zenodo.org/records/ 12608602. Paolo Glorioso, Quentin Anthony, Yury Tokpanov, James Whittington, Jonathan Pilault, Adam Ibrahim, and Beren Millidge. Zamba: compact 7b ssm hybrid model, 2024. URL https: //arxiv.org/abs/2405.16712. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021a. Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher Re. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in neural information processing systems, 34:572585, 2021b. On the parameterizaAlbert Gu, Karan Goel, Ankit Gupta, tion and initialization of diagonal In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 3597135983. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2022/ 2022. file/e9a32fade47b906de908431991440f7c-Paper-Conference.pdf. and Christopher Re. state space models. Ankit Gupta, Albert Gu, and Jonathan Berant. tive as structured state spaces. grave, K. Cho, Systems, https://proceedings.neurips.cc/paper_files/paper/2022/file/ 9156b0f6dfa9bbd18c79cc459ef5d61c-Paper-Conference.pdf. Diagonal state spaces are as effecIn S. Koyejo, S. Mohamed, A. Agarwal, D. BelInformation Processing URL Inc., in Neural pp. 2298222994. Curran Associates, and A. Oh (eds.), Advances volume 35, 2022. Dongchen Han, Xuran Pan, Yizeng Han, Shiji Song, and Gao Huang. Flatten transformer: Vision transformer using focused linear attention. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 59615971, October 2023. Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In International Conference on Learning Representations, 2017. URL https://openreview. net/forum?id=rkE3y85ee. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are In International conference on RNNs: Fast autoregressive transformers with linear attention. machine learning, pp. 51565165. PMLR, 2020. Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale In Proceedings of the 2017 Conference ReAding comprehension dataset from examinations. on Empirical Methods in Natural Language Processing, pp. 785794, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1082. URL https://aclanthology.org/D17-1082. Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. Numinamath. [https://huggingface. co/AI-MO/NuminaMath-CoT](https://github.com/project-numina/ aimo-progress-prize/blob/main/report/numina_dataset.pdf), 2024. Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Joao 12 Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Munoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder: may the source be with you! 2023. Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et al. Jamba: hybrid transformermamba language model. arXiv preprint arXiv:2403.19887, 2024. Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021. Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=5MkYIYCbva. William Merrill, Jackson Petty, and Ashish Sabharwal. The illusion of state in state-space models. arXiv preprint arXiv:2404.08819, 2024. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In EMNLP, 2018. Arindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah. Orca-math: Unlocking the potential of slms in grade school math. arXiv preprint arXiv:2402.14830, 2024. Eric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, Clayton Rabideau, Stefano Massaroli, Yoshua Bengio, Stefano Ermon, Stephen A. Baccus, and Chris Re. Hyenadna: Long-range genomic sequence modeling at single nucleotide resolution, 2023. URL https://arxiv.org/abs/2306.15794. Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An open dataset of high-quality mathematical web text, 2023. Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Re. Hyena hierarchy: Towards larger convolutional language models. In International Conference on Machine Learning, pp. 2804328078. PMLR, 2023. Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingcosformer: Rethinking softmax in attention. arXiv preprint peng Kong, and Yiran Zhong. arXiv:2202.08791, 2022. Pranav Rajpurkar, Jian Zhang, and Percy Liang. Know what you dont know: Unanswerable questions for squad. In ACL 2018, 2018. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Efficient attention: Attention with linear complexities. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pp. 35313539, 2021. 13 Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=Ai8Hw3AXqks. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: successor to transformer for large language models, 2023. URL https://arxiv.org/abs/2307.08621. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. Roger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, et al. An empirical study of mambabased language models. arXiv preprint arXiv:2406.07887, 2024. Kaiyue Wen, Xingyu Dang, and Kaifeng Lyu. Rnns are not transformers (yet): The key bottleneck on in-context retrieval. arXiv preprint arXiv:2402.18510, 2024. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can In Anna Korhonen, David Traum, and Lluıs M`arquez machine really finish your sentence? (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 47914800. Association for Computational Linguistics, 2019. URL https: //aclanthology.org/P19-1472. Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. Guangxiang Zhao, Junyang Lin, Zhiyuan Zhang, Xuancheng Ren, Qi Su, and Xu Sun. ExarXiv preprint plicit sparse transformer: Concentrated attention through explicit selection. arXiv:1912.11637, 2019."
        },
        {
            "title": "A DATASETS",
            "content": "Our training data comprises diverse set of datasets, carefully curated to ensure breadth and depth across various domains. This diverse collection includes specialized mathematics datasets (MetaMathQA Yu et al. (2023), NuminaMath-CoT LI et al. (2024), OpenWebMath Paster et al. (2023), Orca-Math Mitra et al. (2024)), high-quality web data (Fineweb-Edu-dedup Ben Allal et al. (2024)), synthetic data (Cosmopedia-v2 Ben Allal et al. (2024)), code data (Starcoderdata-python-edu Li et al. (2023)), and general knowledge sources (Wikipedia). This comprehensive approach aims to enable our model to handle wide array of language modeling tasks. The inclusion of both domainspecific and broad-coverage datasets is designed to enhance the models versatility and robustness across language modeling tasks. All datasets were tokenized using the LLama3s tokenizer Dubey et al. (2024), resulting in 300B tokens. The training data size varies by model scale: the 190M model is trained on 27 billion tokens (exclusively from Cosmopedia-v2), while the 450M and 1.3B models are trained on 100 billion tokens sampled from the combination of datasets mentioned above. Below are detailed descriptions of each dataset used: 14 1. MetaMathQA Yu et al. (2023): comprehensive mathematics dataset designed to enhance the models mathematical reasoning and problem-solving abilities. 2. NuminaMath-CoT LI et al. (2024): chain-of-thought mathematics dataset that promotes step-by-step reasoning in mathematical problem-solving. 3. Cosmopedia-v2 Ben Allal et al. (2024): large-scale synthetic dataset for pre-training, consisting of over 39 million textbooks, blog posts, and stories. 4. Fineweb-Edu-dedup Ben Allal et al. (2024): high-quality subset of the FineWeb-Edu dataset, containing 220 billion tokens of educational web pages. This dataset was filtered using an educational quality classifier to retain only the most valuable educational content. 5. OpenWebMath Paster et al. (2023): diverse collection of mathematical content from over 130,000 different domains, including forums, educational pages, and blogs. It covers mathematics, physics, statistics, computer science, and related fields. 6. Starcoderdata-Edu Li et al. (2023): subset of the Starcoder dataset, specifically filtered for high-quality educational content related to Python programming. This dataset aims to enhance the models coding capabilities. 7. Orca-Math Mitra et al. (2024): dataset focused on mathematical word problems, designed to improve the models ability to interpret and solve practical mathematical scenarios. 8. Wikipedia: An English Wikipedia dataset providing broad range of general knowledge across various topics."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "Auburn University",
        "University of Oregon"
    ]
}