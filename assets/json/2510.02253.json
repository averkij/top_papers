{
    "paper_title": "DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing",
    "authors": [
        "Zihan Zhou",
        "Shilin Lu",
        "Shuli Leng",
        "Shaocong Zhang",
        "Zhuming Lian",
        "Xinlei Yu",
        "Adams Wai-Kin Kong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Drag-based image editing has long suffered from distortions in the target region, largely because the priors of earlier base models, Stable Diffusion, are insufficient to project optimized latents back onto the natural image manifold. With the shift from UNet-based DDPMs to more scalable DiT with flow matching (e.g., SD3.5, FLUX), generative priors have become significantly stronger, enabling advances across diverse editing tasks. However, drag-based editing has yet to benefit from these stronger priors. This work proposes the first framework to effectively harness FLUX's rich prior for drag-based editing, dubbed DragFlow, achieving substantial gains over baselines. We first show that directly applying point-based drag editing to DiTs performs poorly: unlike the highly compressed features of UNets, DiT features are insufficiently structured to provide reliable guidance for point-wise motion supervision. To overcome this limitation, DragFlow introduces a region-based editing paradigm, where affine transformations enable richer and more consistent feature supervision. Additionally, we integrate pretrained open-domain personalization adapters (e.g., IP-Adapter) to enhance subject consistency, while preserving background fidelity through gradient mask-based hard constraints. Multimodal large language models (MLLMs) are further employed to resolve task ambiguities. For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench) featuring region-level dragging instructions. Extensive experiments on DragBench-DR and ReD Bench show that DragFlow surpasses both point-based and region-based baselines, setting a new state-of-the-art in drag-based image editing. Code and datasets will be publicly available upon publication."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 3 5 2 2 0 . 0 1 5 2 : r Preprint DRAGFLOW: UNLEASHING DIT PRIORS WITH REGION BASED SUPERVISION FOR DRAG EDITING Zihan Zhou1,, Shilin Lu1,, Shuli Leng1, Shaocong Zhang1, Zhuming Lian1, Xinlei Yu2, Adams Wai-Kin Kong1 1Nanyang Technological University, {zihan010, shilin002, nie25.ls3409, zhan0711, zhuming001}@e.ntu.edu.sg xinlei.yu@u.nus.edu adamskong@ntu.edu.sg 2National University of Singapore Figure 1: Comparison of drag-editing results between baselines and our method, DragFlow. DragFlow successfully unleashes FLUXs stronger generative prior, removing the distortions that previous methods produced on challenging scenarios."
        },
        {
            "title": "ABSTRACT",
            "content": "Drag-based image editing has long suffered from distortions in the target region, largely because the priors of earlier base models, Stable Diffusion, are insufficient to project optimized latents back onto the natural image manifold. With the shift from UNet-based DDPMs to more scalable DiT with flow matching (e.g., SD3.5, FLUX), generative priors have become significantly stronger, enabling advances across diverse editing tasks. However, drag-based editing has yet to benefit from these stronger priors. This work proposes the first framework to effectively harness FLUXs rich prior for drag-based editing, dubbed DragFlow, achieving substantial gains over baselines. We first show that directly applying point-based drag editing to DiTs performs poorly: unlike the highly compressed features of UNets, DiT features are insufficiently structured to provide reliable guidance for point-wise motion supervision. To overcome this limitation, DragFlow introduces regionbased editing paradigm, where affine transformations enable richer and more consistent feature supervision. Additionally, we integrate pretrained open-domain personalization adapters (e.g., IP-Adapter) to enhance subject consistency, while preserving background fidelity through gradient mask-based hard constraints. Multimodal large language models (MLLMs) are further employed to resolve task ambiguities. For evaluation, we curate novel Region-based Dragging benchmark (ReD Bench) featuring region-level dragging instructions. Extensive experiments on DragBench-DR and ReD Bench show that DragFlow surpasses both pointEqual Contribution. 1 Preprint based and region-based baselines, setting new state-of-the-art in drag-based image editing. Code and datasets will be publicly available upon publication."
        },
        {
            "title": "INTRODUCTION",
            "content": "Text-driven image editing (Labs et al., 2025) has made impressive progress, but natural language often underspecifies geometry and locality, leading to unintended changes. Drag-based image editing (Pan et al., 2023; Jiang et al., 2025; Xia et al., 2025) bridges this gap by enabling users to specify finer-grained, spatially localized motions through interactive drag instructions, yielding more controllable edits. Despite their success, however, these methods often introduce unnatural deformations and distortions, especially in images with intricate details or complex structures. We attribute this limitation to the insufficient generative prior of Stable Diffusion (SD) (Rombach et al., 2022a), the predominant base model, which struggles to constrain optimized latents back onto the natural image manifold. Past findings align with this view: applying nearly identical loss functions for drag editing yields far fewer unnatural distortions when using SD-based priors compared to GAN (Shi et al., 2024b). In parallel, recent advances in generative modeling have shifted from UNet-based DDPMs to more scalable Diffusion Transformers (DiTs) (Peebles & Xie, 2023) trained with flow matching (Lipman et al., 2022) (e.g., SD 3.5 (Esser et al., 2024a), FLUX.1-dev (Black Forest Labs, 2024)), yielding substantially stronger priors that have propelled progress across various editing tasks (Lu et al., 2025; Yan et al., 2025; Wei et al., 2025; Deng et al., 2024; Wang et al., 2024; Rout et al., 2024). Yet, drag-based editing has not capitalized on these enhanced priors. In this work, we pioneer the exploration of leveraging stronger generative prior for drag editing. We first observe that directly applying previous drag editing methods to DiTs yields suboptimal results. Through detailed analysis of features extracted from U-Nets and DiTs, we identify two core obstacles. First, point-based objectives used by prior drag methods mismatch DiT representations. U-Net bottlenecks produce spatially compact, highly compressed features that aggregate high-level semantics over broad receptive fields; supervising single feature-map location therefore carries strong semantic evidence. DiTs, in contrast, yield finer-grained, spatially precise features with narrower receptive fields. Directly applying point-wise motion or tracking losses to DiTs provides weak semantic supervision and degrades editing effectiveness. Second, modern DiT models like FLUX are classifier-free-guidance (CFG)distilled, which exacerbates inversion drift. As result, standard key-value (KV) injection is insufficient to preserve subject identity consistency during drag edits. To harness the potent priors of DiT-based models for drag-based editing, we introduce DragFlow, novel region-based editing framework. DragFlow departs from point supervision and rethinks inversion and background handling to align with DiT feature geometry and the realities of CFG-distilled models. DragFlow advances the state of the art through three key innovations: (i) region-level motion supervision, which delivers richer and more consistent feature guidance via affine transformations; (ii) replacement of traditional background consistency losses with hard constraints that preserve the background while updating only the editable region; and (iii) adapter-enhanced inversion, which injects subject representations from pretrained open-domain personalization adapter (e.g., IP-Adapter (Ye et al., 2023)) into the base models prior, achieving markedly superior subject fidelity under edits. Together, these components make drag-based editing practical with DiT backbones: they harness the stronger generative prior without sacrificing controllability, reducing deformation artifacts and improving faithfulness on complex, detail-rich images. For evaluation, we introduce the Region-based Dragging Benchmark (ReD Bench). Each sample in ReD Bench is equipped with point-to-region alignment, explicit task tags spanning relocation, deformation, and rotation, and contextual descriptions that clarify user intent. We validate DragFlow extensively on both DragBench-DR (Lu et al., 2024a) and ReD Bench. Results demonstrate that DragFlow consistently outperforms state-of-the-art (SOTA) baselines."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Recent advances in diffusion models have led to surge of interactive image-editing techniques, enabling users to intuitively reposition or deform specific regions of an image through drag-based interactions. Existing methods for drag-based editing can be broadly grouped into three categories. 2 Preprint (i) Optimization-based methods (Xia et al., 2025; Ling et al., 2024; Liu et al., 2024; Zhang et al., 2024c; Jiang et al., 2025; Shi et al., 2024b; Karras et al., 2022; Mou et al., 2023; 2024; Lin et al., 2025; Hou et al., 2024; Cui et al., 2024; Luo et al., 2024; Choi et al., 2024), which is the most prevalent category, iteratively refine inverted noisy latents during inference. These techniques are predominantly point-based: they accept point-wise drag instructions as input and employ motion supervision and point tracking, both of which operate at the point level. However, they often yield unnatural deformations or distortions in the edited images, primarily because the optimized latents deviate from the natural image manifold learned by the base model, residing in out-of-distribution regions. Thus, many studies have focused on more judicious optimization strategies to ensure that the resulting latents can be more readily mapped back to plausible natural images. (ii) Finetuningbased methods (Shin et al., 2024; Shi et al., 2024a), which adapt base text-to-image (T2I) diffusion model using curated video datasets. Yet, the inherent mismatch between video data and the precise instructions required for drag editing, coupled with the scarcity of high-quality, large-scale training data, limits their generalization. These methods usually fail to achieve complete drag effects and are prone to distortions. (iii) Methods that avoid both finetuning and optimization (e.g., RegionDrag (Lu et al., 2024a) and FastDrag (Zhao et al., 2024f)), instead directly copying and pasting noisy latent patches to target locations computed via predefined mapping functions during inference. While this approach significantly enhances efficiency, it heavily relies on handcrafted priors for the mapping functions, often resulting in edited images that lack faithfulness and realism. Our method falls within the optimization-based paradigm but innovates by replacing point-based motion supervision with region-level supervision, thereby enabling drag capabilities in DiTs. Like RegionDrag, our approach accepts region-based inputs; however, whereas RegionDrag requires users to manually predefine the target region mask (a challenging task in non-rigid scenarios), we only necessitate specifying target point serving as the regions center. Moreover, RegionDrag performs point-wise copy-pasting within noisy latents, which, due to the handcrafted nature of its mappings, often fails to preserve internal region structures. In contrast, we treat the region as cohesive unit, extracting holistic regional features to serve as supervision signals during latent optimization, thereby ensuring the integrity of internal structures."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "In this section, we begin by elucidating the inherent limitations of previous point-based drag editing frameworks when adapted to DiTs (Sec. 3.1). Building on this analysis, we introduce region-level affine supervision strategy (Sec. 3.2). To further enhance fidelity, we incorporate hardconstrained background preservation (Sec. 3.3) and adapter-enhanced subject consistency mechanisms (Sec. 3.4), addressing inversion drifts in DiTs. 3.1 WHY POINT-BASED DRAG FAILS ON DIT To harness the robust prior of FLUX for drag-based image editing, we initially applied established point-based drag editing frameworks, including image inversion, motion supervision, point tracking, and key-value injection, directly to FLUX. Surprisingly, as shown in Fig. 6, this straightforward adaptation offers only limited improvements compared to its counterpart in SD. We attribute this performance gap to fundamental differences in the feature granularity extracted by the DiT and UNet, which significantly impact the effectiveness of point-wise motion supervision and tracking methods. As illustrated in Fig. 2, the UNet Figure 2: Comparison of feature maps extracted from UNet and DiT at the same denoising step. UNet produces spatially compact, highly compressed features that capture high-level semantic information, whereas DiT generates finer-grained, spatially precise representations. Preprint Figure 3: Overview of the DragFlow framework. The original image is inverted into noisy latent space and iteratively optimized under the proposed region-level affine supervision. Subject consistency is reinforced through key-value (KV) injection and our adapter-enhanced inversion, while background fidelity is maintained via gradient mask-based hard constraints. In addition, multimodal large language model (MLLM) is employed to better interpret and clarify user intents. architecture, due to its bottleneck design, produces spatially compact and highly compressed features that encapsulate high-level semantic representations. In contrast, DiT generates finer-grained, spatially precise features. Consequently, in UNet, each point on the feature map aggregates semantic information from broad receptive field in the input image, whereas in DiT, each point corresponds to narrower region. This difference directly impacts point-based methods, which rely on computing losses for motion supervision and point tracking using individual points on the feature map. In UNet, the broader receptive field of each feature point provides rich semantic context, enabling effective motion supervision and tracking. However, in DiT, the finer-grained features, with their narrower receptive fields, capture less semantic information per point, undermining the efficacy of these point-based methods when applied directly to DiT. 3.2 REGION-LEVEL AFFINE SUPERVISION To leverage the powerful prior of FLUX for drag-based image editing, we introduce DragFlow, region-based framework. User Input Specification. In our framework, the user designates source region masks {Mi}N i=1, each paired with corresponding target point ti = (xi, yi). The target point serves as the centroid of the target region. The expected target region mask can be obtained via an affine transformation (see Appendix C.1 for details). As illustrated in Fig. 3, we harness the capabilities of multimodal large language model (MLLM), GPT-5 (OpenAI, 2025), to infer users underlying intentions and thereby facilitate drag-based editing. The model receives as input the original image together with userprovided drag instructions (i.e., source region masks and target points). We then prompt the MLLM with carefully designed in-context examples, which guide it to produce two outputs: (i) class label indicating the type of editing operation, and (ii) textual description articulating the inferred editing intent (see Appendix D.3 for details on the prompting strategy). The class label determines which affine transformation is applied, while the textual description serves as natural-language prompt for the generative model during the drag-editing process. Iterative Latent Optimization. Given an input image x, it is first encoded by the VAE to produce the latent z, which is then inverted to obtain the noisy latent zt where [0, ]. We optimize zt over iterations where [0, K], denoted as z(k) produces an output image that fulfills the user-specified drag operations. This optimization is guided by the , such that subsequent denoising of z(k) 4 Preprint following loss function: LDrag = (cid:88) i=1 γi (cid:13) (cid:13)M (k) (cid:13) (cid:17) (cid:16) z(k) sg (cid:104) (0) (cid:16) z(0) (cid:17)(cid:105)(cid:13) (cid:13) (cid:13)1 , where (cid:88) i=1 γi = 1. (1) zt is the initial unoptimized latent, while z(k) Here, z(0) represents the latent after optimization iterations. The function () extracts features from DiT, with the specific feature layers detailed in Appendix C.5. The operator sg[] denotes stop-gradient. The weighting coefficient γi balances multiple drag operations within the same image, determined adaptively according to the relative size of the corresponding manipulated regions (see Appendix D.1 for the formulation in details). Finally, (0) Mi is the user-provided mask for the source region, and (k) specifies the corresponding target region, where we enforce similarity to the features of the source region. Affine Transformation for Mask Propagation. The target mask (k) mask (0) via an affine transformation: is derived from the source (k) (cid:16) = Ω (0) , ξ(k) (cid:17) , ξ(k) = (cid:18) (ti bi), (Relocation & Deformation) (cid:19) biaiti, ai , (Rotation) (2) where Ω applies the affine transformation (affine computation detailed in Appendix C) to (0) with parameters governed by ξ(k) . Different drag types influence the affine matrix parameters distinctly: for relocation and deformation, these are determined by the vector from the target point ti to the centroid bi of the source region, where bi = (1/M (0) q; for rotation, they are governed by the angle biaiti formed by ti, bi, and the user-specified anchor point ai. The linear schedule k/K moves the mask smoothly from the source configuration toward the target over iterations. qM (0) ) (cid:80) i Why Region-level Supervision. This formulation causes the target mask (k) to translate (or rotate) progressively from the source centroid toward the target point as increases. Intuitively, it transports the objects features from the source region to the destination step by step. Although this echoes the high-level idea of point-based dragging, there are two crucial differences: Feature granularity. Point-based methods compare features only at handle point, either against its previous location or against the original source point. In contrast, we match features between entire source and target regions. Region-level supervision provides richer semantic context and mitigates myopic gradients, which leads to more effective latent updates on FLUX. Tracking requirement. Point-based approaches require handle point tracking to keep the extracted features aligned with the moving content. Without tracking, naively advancing the handle along straight line is brittle: even slight deviation of the optimized content from that line causes subsequent local features to mismatch the intended structure, causing error accumulation and eventual drag failure. Our region-level supervision avoids this failure mode. Because we compare features over regions rather than at single point, we do not need to pinpoint feature extraction location on the object at each step. This makes the procedure substantially more stable and robust, eliminating the need for explicit tracking; it suffices to shift the source region mask along the path from the source centroid to the target point. 3.3 BACKGROUND PRESERVATION Prior work typically enforces fidelity in non-editable regions via an auxiliary consistency loss: LBG = (cid:16) (cid:13) (cid:13) (cid:13) (cid:17) t1 sg[z(0) z(k) t1] (1 B) (cid:13) (cid:13) (cid:13) , (3) where denotes the mask that specifies the editable region. In practice, this term competes with the feature-matching objective, making performance highly sensitive to its weight. The issue is exacerbated in FLUX, CFGdistilled model that exhibits larger image-inversion drift than non-distilled counterparts. Compounding this, the consistency loss is evaluated against the inverted latent z(0) t1, 5 Preprint which is treated as ground truth; when the inversion is biased, this target is unreliable, and the loss misguides optimization rather than helping it. Instead of balancing competing losses, we hard constrain the background and update only the editable region: z(k+1) = z(k) α + (1 B) zorig , (4) (cid:32) (cid:33) LDrag z(k) where denotes the mask that specifies the editable region (see Appendix D.2 for extraction details), and zorig is obtained from pure reconstruction path. Implementationally, this requires an additional reconstruction branch that starts from the inverted latent zt; the overhead is modest and, critically, it yields substantially better background preservation in challenging FLUX settings. 3.4 SUBJECT CONSISTENCY ENHANCEMENT While our proposed region-level affine supervision enables drag-based editing using the prior of FLUX, it still suffers from subject inconsistency between the source and edited images. natural remedy is the KV injection, which is widely used when SD serves as the base model. In FLUX, however, KV injection underperforms, as shown in Fig. 4 (left). We attribute this gap to FLUX being CFG-distilled model, which exhibits more pronounced inversion drift compared to non-distilled counterparts, as evidenced in Tab. 1. Method Table 1: Inversion Performance (3,000 images). To address this, we introduce adapter-enhanced inversion for DiT-based models. Specifically, pretrained open-domain personalization adapters (e.g., IP-Adapter (Ye et al., 2023), PuLID (Guo et al., 2024), and InstantCharacter (Tao et al., 2025)) are trained to extract subjects representation from reference image, enabling its seamless integration into T2I base model for generation across varied contexts. Leveraging this insight, we propose employing such adapters as auxiliary subject representation extractors. Without any additional fine-tuning, we inject the adapters subject representation into the model prior, which substantially improves inversion quality (Tab. 1) and yields visibly better subject consistency under edits (Fig. 4 (right)). DPM-Solver (Lu et al., 2022) Inv. (SD) Fireflow Inv. w/o adapter (FLUX) Fireflow Inv. w/ adapter (FLUX) 0.167 0.283 0.173 0.799 0.703 0.784 26.31 20.43 25.87 LPIPS PSNR SSIM"
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 IMPLEMENTATION DETAILS We implement DragFlow using FLUX.1dev (Black Forest Labs, 2024) as the base model. We adopt FireFlow (Deng et al., 2024) as the inversion algorithm for FLUX, employing 25 diffusion steps, with 6 steps skipped and drag editing commencing at the 19th step. We perform optimization at the 7th denoising step over 70 iterations, using learning rate of 1000 for the first 50 iterations and 1200 for the final 20. The adapter employed is InstantCharacter (Tao et al., 2025). Additional implementation details are provided in Appendix C.5. 4.2 EXPERIMENTAL SETUP Benchmark. To facilitate systematic evaluation of region-based image drag-editing methods, we introduce new Region-based Dragging Bench (ReD Bench). Existing datasets are often limited in scope. For example, DragBench (Shi et al., 2024b) primarily provides Figure 4: Visualization of the effect of adapterenhanced inversion on subject consistency, compared with KV injection alone. 6 Preprint Table 2: Comparison of composition performance across two benchmarks. Optimal results are bolded, where the second-best own underlines. Abbreviation: OPT optimization-based; FT fine-tuning-based; NFT neither fine-tuning nor optimization-based. Benchmark Method Category #Params (M) ReD Bench DragBench-DR RegionDrag (Lu et al., 2024a) FastDrag (Zhao et al., 2024f) InstantDrag (Shin et al., 2024) DragLoRA (Xia et al., 2025) FreeDrag (Ling et al., 2024) DragNoise (Liu et al., 2024) GoodDrag (Zhang et al., 2024c) CLIPDrag (Jiang et al., 2025) DragDiffusion (Shi et al., 2024b) DragFlow (Ours) RegionDrag (Lu et al., 2024a) FastDrag (Zhao et al., 2024f) InstantDrag (Shin et al., 2024) DragLoRA (Xia et al., 2025) FreeDrag (Ling et al., 2024) DragNoise (Liu et al., 2024) GoodDrag (Zhang et al., 2024c) CLIPDrag (Jiang et al., 2025) DragDiffusion (Shi et al., 2024b) DragFlow (Ours) NFT NFT FT OPT OPT OPT OPT OPT OPT OPT NFT NFT FT OPT OPT OPT OPT OPT OPT OPT 0 0 914 3.19 0.07 0.33 0.07 0.07 0.07 0 0 0 914 3.19 0.07 0.33 0.07 0.07 0.07 0 Image Fidelity Mean Distance IFbg 1.000 0.928 0.930 0.927 0.941 0.942 0.935 0.952 0.944 0. 1.000 0.938 0.944 0.942 0.955 0.956 0.948 0.962 0.954 0.969 IFs2t 0.957 0.950 0.949 0.950 0.947 0.932 0.956 0.942 0.948 0.958 0.942 0.947 0.945 0.941 0.946 0.943 0.946 0.945 0.944 0.948 IFs2s MD1 MD2 33.69 0.957 23.37 0.941 24.38 0.946 26.04 0.938 30.31 0.956 45.46 0.975 20.38 0.942 33.84 0.965 32.15 0.947 19.46 0.934 6.38 5.00 4.54 4.86 6.08 8.85 4.50 6.98 5.65 4.48 0.960 0.952 0.966 0.952 0.967 0.977 0.956 0.972 0.958 0. 32.32 35.96 36.26 42.03 34.77 39.31 37.87 38.06 39.41 31.59 6.31 6.60 6.99 6.77 6.81 7.69 6.91 7.45 7.05 5.93 point-to-point guidance for dragging operations, which fails to capture the complexity of regionlevel manipulation. Extensions of DragBench with coarse region annotations also fall short, as they lack explicit point-to-region alignment and operation-specific instructions, such as task tags and anchor points. In ReD, each sample not only includes point-level operations, but also offers the translated regionto-region correspondences, together with explicit task labels covering the three most common categories of dragging (i.e., relocation, deformation, and rotation). Moreover, ReD is enriched with detailed contextual descriptions and intent prompts. This richer supervision allows ReD to serve as more reliable testbed for assessing our approach and comparing it with range of SOTA baselines. Evaluation Metrics. Following prior work (Liu et al., 2024; Zhang et al., 2024c; Shi et al., 2024b), we evaluate drag-based editing using combination of Mean Distance (MD) and Image Fidelity (IF). The standard MD metric (Shi et al., 2024b) quantifies the spatial correspondence of dragged content. We employ masked variant, denoted as MD1, which computes correspondences only within the edited region, providing more focused evaluation of alignment quality. In addition, we also adopt the variant proposed by Lu et al. (2024a), denoted as MD2. IF assesses visual consistency between the original and edited images via LPIPS (Zhang et al., 2018). We employ three variants for finegrained analysis: IFbg: LPIPS computed on non-edited regions, capturing how well background content is preserved. IFs2t: LPIPS between the original source region and the edited target region, indicating how faithfully source content is transferred. IFs2s: LPIPS between the original and edited source regions, measuring how effectively the source is cleared after transfer. 4.3 COMPARISON WITH BASELINES Quantitative Analysis. As shown in Tab. 2, our method achieves the lowest MD across both benchmarks, demonstrating the strongest spatial correspondence between user instructions and the resulting drag operations. The superior performance on IFs2s highlights the methods ability to deliver precise and reliable content manipulation with high structural consistency and completeness. Although our approach ranks second on IFbg, the margin is marginal and largely attributable to the inherent inversion limitations of the CFG-distilled model. Nevertheless, our gradient maskbased hard constraints ensure robust background integrity, allowing our method to outperform most baselines despite these limitations. Qualitative Analysis. Fig. 5 presents side-by-side comparisons with representative baselines. Our method consistently produces edits that accurately follow the specified dragging operations while preserving global scene coherence. In contrast, RegionDrag and InstantDrag often introduce struc7 Preprint Figure 5: Qualitative comparison of our method with multiple baselines in challenging scenarios. tural distortions, while FreeDrag and FastDrag struggle with complex transformations such as rotations. CLIPDrag and DragLoRA frequently misinterpret relocation as deformation, leading to unintended artifacts. Across all these scenarios, our approach demonstrates superior structural preservation, faithful intent alignment, and robust performance over diverse range of editing tasks. 4.4 ABLATION STUDY To assess the contribution of each component, we perform ablation studies (results shown in Fig. 6 and Tab. 3) by progressively incorporating modules into our framework. Specifically, we examine the impact of adopting (a) region-based manipulation, introducing (b) mask-led background preservation, and applying (c) adapter-enhanced inversion. In (a), transitioning from points to regionbased control leads to consistent gains across all evaluation metrics. Compared with the baseline, regional manipulation reduces MD1 by 19.95 and increases IFs2t by 0.027, highlighting its ability to provide semantically richer and more coherent feature guidance. These results support our hypothesis that the regional affine strategy constitutes more principled paradigm for modern generative 8 Preprint Figure 6: Qualitative ablation study comparing different variants of our framework. Table 3: Ablation study examining the impact of key components. Configs Baseline (Point-based FLUX) + Region-Level Affine Supervision + Background Preservation + Adapter-Enhanced Inversion Image Fidelity Mean Distance IFbg 0.765 0.757 0.925 0.991 IFs2t 0.932 0.946 0.948 0. IFs2s MD1 MD2 51.21 0.962 0.936 31.26 29.67 0.943 20.15 0.938 9.38 5.88 5.39 4.48 priors, effectively alleviating the intrinsic limitations of sparse point-level supervision. For (b), our gradient mask design substantially improves background consistency, with IFbg rising from 0.757 to 0.925, underscoring its effectiveness in preserving global backdrop integrity. Finally, the adapterenhanced inversion in (c) significantly strengthens subject fidelity, as reflected by an increase in IFs2t from 0.948 to 0.959, thereby confirming its capability to maintain foreground consistency under drag-editing. Taken together, these results clearly demonstrate that each component is effective on its own while synergizing to validate the overall design of our method."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We propose DragFlow, the first drag-based image editing framework tailored for DiTs. By rethinking supervision, inversion, and background handling in light of the unique representational and training properties of DiTs, DragFlow unlocks their powerful generative priors for controllable, fine-grained drag editing. Our three contributions, namely region-level motion supervision, background hard constraints, and adapter-enhanced inversion, collectively address the weaknesses of prior drag approaches, mitigating deformation artifacts while preserving subject identity and image realism. To support rigorous evaluation, we introduce ReD Bench, benchmark designed around region-aware annotations and explicit task categories. Across both ReD Bench and DragBench-DR, DragFlow consistently surpasses existing state-of-the-art methods, demonstrating stronger faithfulness, better controllability, and higher-quality outputs. Limitations and Future Work. Since the FLUX model we employ for drag-based editing is CFG-distilled variant, its inversion drift is notably larger than that of non-distilled counterparts. Although we mitigate this issue through adapter-enhanced inversion, images with highly intricate structures still exhibit detail loss in the reconstruction. Consequently, the drag-editing results inherit these artifacts, leading to degraded visual quality. Future research could benefit from techniques or advanced adapter architectures that further strengthen inversion fidelity, thereby reducing such artifacts and enhancing overall editing performance. 9 Preprint"
        },
        {
            "title": "REFERENCES",
            "content": "Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended latent diffusion. ACM transactions on graphics (TOG), 42(4):111, 2023. Black Forest Labs. Flux.1 [dev]. https://huggingface.co/black-forest-labs/ FLUX.1-dev, 2024. Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1839218402, 2023. Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023. Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zeroshot object-level image customization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 65936602, 2024. Gayoon Choi, Taejin Jeong, Sujung Hong, and Seong Jae Hwang. Dragtext: Rethinking text embedding in point-based image editing, 2024. URL https://arxiv.org/abs/2407.17843. Yutao Cui, Xiaotong Zhao, Guozhen Zhang, Shengming Cao, Kai Ma, and Limin Wang. Stabledrag: Stable dragging for point-based image editing, 2024. URL https://arxiv.org/abs/ 2403.04437. Yingying Deng, Xiangyu He, Changwang Mei, Peisong Wang, and Fan Tang. Fireflow: Fast inversion of rectified flow for image semantic editing, 2024. URL https://arxiv.org/abs/ 2412.07517. Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-to-image generation via hierarchical transformers. Advances in Neural Information Processing Systems, 35:1689016902, 2022. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024a. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024b. Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang, and Zhe Gan. GuidarXiv preprint ing instruction-based image editing via multimodal large language models. arXiv:2309.17102, 2023. Daiheng Gao, Shilin Lu, Shaw Walters, Wenbo Zhou, Jiaming Chu, Jie Zhang, Bang Zhang, Mengxi Jia, Jian Zhao, Zhaoxin Fan, et al. Eraseanything: Enabling concept erasure in rectified flow transformers. arXiv preprint arXiv:2412.20413, 2024. Zinan Guo, Yanze Wu, Chen Zhuowei, Peng Zhang, Qian He, et al. Pulid: Pure and lightning id customization via contrastive alignment. Advances in neural information processing systems, 37: 3677736804, 2024. Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. Xingzhong Hou, Boxiao Liu, Yi Zhang, Jihao Liu, Yu Liu, and Haihang You. Easydrag: Efficient point-based manipulation on diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 84048413, 2024. 10 Preprint Ziqi Jiang, Zhen Wang, and Long Chen. Clipdrag: Combining text-based and drag-based instructions for image editing, 2025. URL https://arxiv.org/abs/2410.03097. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models. Advances in neural information processing systems, 35:2656526577, 2022. Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Diffusionclip: Text-guided diffusion models for robust image manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 24262435, June 2022. Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742, 2025. Leyang Li, Shilin Lu, Yan Ren, and Adams Wai-Kin Kong. Set you straight: Auto-steering denoising trajectories to sidestep unwanted concepts. arXiv preprint arXiv:2504.12782, 2025. Xiaojian Lin, Hanhui Li, Yuhao Cheng, Yiqiang Yan, and Xiaodan Liang. Gdrag: Towards generalpurpose interactive editing with anti-ambiguity point diffusion. In The Thirteenth International Conference on Learning Representations, 2025. Pengyang Ling, Lin Chen, Pan Zhang, Huaian Chen, Yi Jin, and Jinjin Zheng. Freedrag: Feature In 2024 IEEE/CVF Conference on Computer dragging for reliable point-based image editing. Vision and Pattern Recognition (CVPR), pp. 68606870. IEEE, 2024. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Haofeng Liu, Chenshu Xu, Yifei Yang, Lihua Zeng, and Shengfeng He. Drag your noise: Interactive point-based editing via diffusion semantic propagation, 2024. URL https://arxiv.org/ abs/2404.01050. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in neural information processing systems, 35:57755787, 2022. Jingyi Lu and Kai Han. Inpaint4drag: Repurposing inpainting models for drag-based image editing via bidirectional warping. arXiv preprint arXiv:2509.04582, 2025. Jingyi Lu, Xinghui Li, and Kai Han. Regiondrag: Fast region-based image editing with diffusion models, 2024a. URL https://arxiv.org/abs/2407.18247. Shilin Lu, Yanzhu Liu, and Adams Wai-Kin Kong. Tf-icon: Diffusion-based training-free crossdomain image composition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 22942305, 2023. Shilin Lu, Zilan Wang, Leyang Li, Yanzhu Liu, and Adams Wai-Kin Kong. Mace: Mass concept erasure in diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 64306440, 2024b. Shilin Lu, Zihan Zhou, Jiayou Lu, Yuanzhi Zhu, and Adams Wai-Kin Kong. Robust watermarking using generative priors against image editing: From benchmarking to advances. arXiv preprint arXiv:2410.18775, 2024c. Shilin Lu, Zhuming Lian, Zihan Zhou, Shaocong Zhang, Chen Zhao, and Adams Wai-Kin Kong. Does flux already know how to perform physically plausible image composition? arXiv preprint arXiv:2509.21278, 2025. Minxing Luo, Wentao Cheng, and Jian Yang. Rotationdrag: Point-based image editing with rotated diffusion features, 2024. URL https://arxiv.org/abs/2401.06442. 11 Preprint Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and Jian Zhang. Dragondiffusion: Enabling drag-style manipulation on diffusion models. arXiv preprint arXiv:2307.02421, 2023. Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and Jian Zhang. Diffeditor: Boosting accuracy and flexibility on diffusion-based image editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 84888497, 2024. Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. Shen Nie, Hanzhong Allan Guo, Cheng Lu, Yuhao Zhou, Chenyu Zheng, and Chongxuan Li. The blessing of randomness: Sde beats ode in general diffusion-based image editing, 2024. URL https://arxiv.org/abs/2311.01410. OpenAI. Introducing gpt-5, 2025. URL https://openai.com/index/ introducing-gpt-5/. Xingang Pan, Ayush Tewari, Thomas Leimkuhler, Lingjie Liu, Abhimitra Meka, and Christian Theobalt. Drag your gan: Interactive point-based manipulation on the generative image manifold. In Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Proceedings, SIGGRAPH 23, pp. 111. ACM, July 2023. doi: 10.1145/3588432.3591500. URL http://dx.doi.org/10.1145/3588432.3591500. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. Yan Ren, Shilin Lu, and Adams Wai-Kin Kong. All that glitters is not gold: Key-secured 3d secrets within 3d gaussian splatting. arXiv preprint arXiv:2503.07191, 2025. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022a. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models, 2022b. URL https://arxiv.org/ abs/2112.10752. Litu Rout, Yujia Chen, Nataniel Ruiz, Constantine Caramanis, Sanjay Shakkottai, and Wen-Sheng Chu. Semantic image inversion and editing using rectified stochastic differential equations. arXiv preprint arXiv:2410.10792, 2024. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 22500 22510, 2023. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:3647936494, 2022. Yujun Shi, Jun Hao Liew, Hanshu Yan, Vincent YF Tan, and Jiashi Feng. Lightningdrag: Lightning fast and accurate drag-based image editing emerging from videos. arXiv preprint arXiv:2405.13722, 2024a. Yujun Shi, Chuhui Xue, Jun Hao Liew, Jiachun Pan, Hanshu Yan, Wenqing Zhang, Vincent YF Tan, and Song Bai. Dragdiffusion: Harnessing diffusion models for interactive point-based image editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 88398849, 2024b. 12 Preprint Joonghyuk Shin, Daehyeon Choi, and Jaesik Park. Instantdrag: Improving interactivity in dragbased image editing. In SIGGRAPH Asia 2024 Conference Papers, pp. 110, 2024. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models, 2022. URL https://arxiv.org/abs/2010.02502. Jiale Tao, Yanbing Zhang, Qixun Wang, Yiji Cheng, Haofan Wang, Xu Bai, Zhengguang Zhou, Ruihuang Li, Linqing Wang, Chunyu Wang, et al. Instantcharacter: Personalize any characters with scalable diffusion transformer framework. arXiv preprint arXiv:2504.12395, 2025. Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 19211930, June 2023. Jiangshan Wang, Junfu Pu, Zhongang Qi, Jiayi Guo, Yue Ma, Nisha Huang, Yuxin Chen, Xiu Li, and Ying Shan. Taming rectified flow for inversion and editing. arXiv preprint arXiv:2411.04746, 2024. Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-Tuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah Laszlo, David J. Fleet, Radu Soricut, Jason Baldridge, Mohammad Norouzi, Peter Anderson, and William Chan. Imagen editor and editbench: Advancing and evaluating textguided image inpainting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1835918369, June 2023a. Yanghao Wang and Long Chen. Inversion circle interpolation: Diffusion-based image augmentation for data-scarce classification. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2556025569, 2025a. Yanghao Wang and Long Chen. Noise matters: Optimizing matching noise for diffusion classifiers. arXiv preprint arXiv:2508.11330, 2025b. Yanghao Wang, Zhongqi Yue, Xian-Sheng Hua, and Hanwang Zhang. Random boxes are openworld object detectors. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 62336243, 2023b. Tianyi Wei, yifan Zhou, Dongdong Chen, and Xingang Pan. Freeflux: Understanding and exploiting layer-specific roles in rope-based mmdit for versatile image editing. Proceedings of the IEEE/CVF International Conference on Computer Vision, 2025. Siwei Xia, Li Sun, Tiantian Sun, and Qingli Li. Draglora: Online optimization of lora adapters for drag-based image editing in diffusion model, 2025. URL https://arxiv.org/abs/ 2505.12427. Rui Xie, Chen Zhao, Kai Zhang, Zhenyu Zhang, Jun Zhou, Jian Yang, and Ying Tai. Addsr: Accelerating diffusion-based blind super-resolution with adversarial diffusion distillation. arXiv preprint arXiv:2404.01717, 2024. Zexuan Yan, Yue Ma, Chang Zou, Wenteng Chen, Qifeng Chen, and Linfeng Zhang. Eedit: Rethinking the spatial and temporal redundancy for efficient image editing. arXiv preprint arXiv:2503.10270, 2025. Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and In ProFang Wen. Paint by example: Exemplar-based image editing with diffusion models. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18381 18391, 2023. Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for contentrich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022. Preprint Xinlei Yu, Zhangquan Chen, Yudong Zhang, Shilin Lu, Ruolin Shen, Jiangning Zhang, Xiaobin Hu, Yanwei Fu, and Shuicheng Yan. Visual document understanding and question answering: multi-agent collaboration framework with test-time scaling. arXiv preprint arXiv:2508.03404, 2025a. Xinlei Yu, Chengming Xu, Guibin Zhang, Yongbo He, Zhangquan Chen, Zhucun Xue, Jiangning Zhang, Yue Liao, Xiaobin Hu, Yu-Gang Jiang, et al. Visual multi-agent system: Mitigating hallucination snowballing via visual flow. arXiv preprint arXiv:2509.21789, 2025b. Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instruction-guided image editing. Advances in Neural Information Processing Systems, 36, 2024a. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586595, 2018. Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, et al. Hive: Harnessing human feedback for instructional In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern visual editing. Recognition, pp. 90269036, 2024b. Zewei Zhang, Huan Liu, Jun Chen, and Xiangyu Xu. Gooddrag: Towards good practices for drag editing with diffusion models, 2024c. URL https://arxiv.org/abs/2404.07206. Chen Zhao, Weiling Cai, Chenyu Dong, and Chengwei Hu. Wavelet-based fourier information interaction with frequency diffusion adjustment for underwater image restoration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 82818291, 2024a. Chen Zhao, Weiling Cai, Chenyu Dong, and Ziqi Zeng. Toward sufficient spatial-frequency interaction for gradient-aware underwater image enhancement. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 32203224. IEEE, 2024b. Chen Zhao, Weiling Cai, Chengwei Hu, and Zheng Yuan. Cycle contrastive adversarial learning with structural consistency for unsupervised high-quality image deraining transformer. Neural Networks, pp. 106428, 2024c. Chen Zhao, Chenyu Dong, and Weiling Cai. Learning physical-aware diffusion model based on transformer for underwater image enhancement. arXiv preprint arXiv:2403.01497, 2024d. Chen Zhao, Wei-Ling Cai, and Zheng Yuan. Spectral normalization and dual contrastive regularization for image-to-image translation. The Visual Computer, pp. 112, 2025a. Chen Zhao, Zhizhou Chen, Yunzhe Xu, Enxuan Gu, Jian Li, Zili Yi, Qian Wang, Jian Yang, and Ying Tai. From zero to detail: Deconstructing ultra-high-definition image restoration from progressive spectral perspective. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1793517946, 2025b. Haozhe Zhao, Xiaojian Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. arXiv preprint arXiv:2407.05282, 2024e. Xuanjia Zhao, Jian Guan, Congyi Fan, Dongli Xu, Youtian Lin, Haiwei Pan, and Pengming Feng. Fastdrag: Manipulate anything in one step. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024f. Dewei Zhou, Zongxin Yang, and Yi Yang. Pyramid diffusion models for low-light image enhancement. arXiv preprint arXiv:2305.10028, 2023. Dewei Zhou, You Li, Fan Ma, Zongxin Yang, and Yi Yang. Migc: Multi-instance generation controller for text-to-image synthesis. arXiv preprint arXiv:2402.05408, 2024a. 14 Preprint Dewei Zhou, You Li, Fan Ma, Zongxin Yang, and Yi Yang. Migc++: Advanced multi-instance generation controller for image synthesis. arXiv preprint arXiv:2407.02329, 2024b. Dewei Zhou, Ji Xie, Zongxin Yang, and Yi Yang. 3dis: Depth-driven decoupled instance synthesis for text-to-image generation. arXiv preprint arXiv:2410.12669, 2024c. Dewei Zhou, Mingwei Li, Zongxin Yang, and Yi Yang. Dreamrenderer: Taming multi-instance attribute control in large-scale text-to-image models. arXiv preprint arXiv:2503.12885, 2025a. Dewei Zhou, Ji Xie, Zongxin Yang, and Yi Yang. 3dis-flux: simple and efficient multi-instance generation with dit rendering. arXiv preprint arXiv:2501.05131, 2025b. Yuanzhi Zhu, Ruiqing Wang, Shilin Lu, Junnan Li, Hanshu Yan, and Kai Zhang. Oftsr: Onestep flow for image super-resolution with tunable fidelity-realism trade-offs. arXiv preprint arXiv:2412.09465, 2024. Yuanzhi Zhu, Xi Wang, Stephane Lathuili`ere, and Vicky Kalogeiton. Dimo: Distilling masked diffusion models into one-step generator. arXiv preprint arXiv:2503.15457, 2025. Junhao Zhuang, Yanhong Zeng, Wenran Liu, Chun Yuan, and Kai Chen. task is worth one word: Learning with task prompts for high-quality versatile image inpainting, 2023. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 17 17 18 19 20 20 20 21 23 24 24 25 25 26 27 27 28 28 28 29 30 Preprint"
        },
        {
            "title": "Table of Contents",
            "content": "A Additional Related Work Preliminary Information B.1 Diffusion Methods . . B.2 Point-based Image Drag-editing . . . . . . . . . . . . . . . Implementation Details C.1 Progressive Transformation in Subtasks . . . C.2 Relocation Tasks . . . . C.3 Deformation Tasks . . C.4 Rotation Tasks . . . . C.5 Details about Experimental Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Adaptive Input Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.1 Region Weight Regularization for Multi-Operations D.2 Adaptive Gradient Mask Generation . . D.3 Leveraging MLLM for Prompt and Tag Generation . . . . . . . Criterion Details E.1 Computation of Image Fidelity (IF) . . E.2 Computation of Mean Distance (MD) . . . . . . . . . . . . . . . Additional Baseline Information Benchmark Details G.1 Formation of the ReD Benchmark . . G.2 Adoption of the DragBench-DR Benchmark . . G.3 Demonstration of Data Samples . . . . . . . . . . . . . . . . . . . . . Extra Qualitative Results LLM Usage Statement 16 Preprint"
        },
        {
            "title": "A ADDITIONAL RELATED WORK",
            "content": "Generative Model-based Image Editing. Recent and significant advancements in generative models (Chang et al., 2023; Ding et al., 2022; Nichol et al., 2021; Ramesh et al., 2022; Rombach et al., 2022a; Saharia et al., 2022; Yu et al., 2022; Peebles & Xie, 2023; Esser et al., 2024a) have enhanced many applications (Avrahami et al., 2023; Ruiz et al., 2023; Hertz et al., 2022; Kim et al., 2022; Tumanyan et al., 2023; Zhou et al., 2025a;b; 2023; 2024a;b;c; Zhao et al., 2024a;d;c; 2025b;a; 2024b; Xie et al., 2024; Wang et al., 2023b; Wang & Chen, 2025a;b; Lu et al., 2024b;c; Li et al., 2025; Ren et al., 2025; Gao et al., 2024; Zhu et al., 2024; 2025; Yu et al., 2025a;b). In this study, we focus on real image editing, which allows users to freely modify actual photographs, producing highly realistic results. Typically, the inputs for image editing include an image and various conditions that help users accurately describe their desired changes. These conditions can encompass text prompts using natural language to specify the edits (Brooks et al., 2023; Zhang et al., 2024b; Fu et al., 2023; Zhang et al., 2024a), region masks to designate areas for modification (Zhao et al., 2024e; Zhuang et al., 2023; Wang et al., 2023a), additional images to provide desired styles or objects (Chen et al., 2024; Lu et al., 2023; Yang et al., 2023), and drag points (Lu & Han, 2025; Nie et al., 2024) that enable users to interactively move specific points in the image to target positions."
        },
        {
            "title": "B PRELIMINARY INFORMATION",
            "content": "This section provides essential background knowledge on diffusion models and point-based image drag-editing techniques. We begin by discussing the evolution of diffusion methods, tracing the transition from early frameworks of Stable Diffusion (SD) Series with Denoising Diffusion Implicit Models (DDIM) (Song et al., 2022) to more advanced rectified flow models and ODE solutions. These advancements enhance the generative process, facilitating more efficient and deterministic sampling. In the latter part, we explore point-based drag-editing methods, highlighting how they allow users to manipulate key points for image modification directly. This approach contrasts with region-based methods by leveraging specific user-given control points for supervision and tracking. B.1 DIFFUSION METHODS Recent advances in text-to-image generative models have driven rapid evolution in architectural paradigms, providing strong impetus for advancing existing image editing methods. Innovations in newer architectures and learning objectives enrich prior knowledge and exhibit significantly improved capacity in data comprehension and semantic alignment. Early UNetbased diffusion frameworks, such as SD 1.5 and SD 2 from Rombach et al. (2022b), have been increasingly supplanted by DiT-based rectified flow designs with more robust pre-trained priors (e.g., FLUX.1 (Black Forest Labs, 2024) and SD 3 (Esser et al., 2024b)). Stable Diffusion with DDIM Inversion. Early drag-based image editing techniques primarily use SD as the foundational framework. They leverage SDs UNet architecture for noise prediction and rely on DDIM inversion to derive noisy latents zt from the clean latent encoding z0 of an input image x. DDIM operates through two core processes: forward inversion process that iteratively adds noise to transform clean latents into noisy ones, and backward denoising process that removes noise to generate edited outputs. To formalize these processes, we first define αt = (cid:81)t s=1(1 βs), where βs is the noise schedule at step s, and [0, ] indexes the diffusion steps, with = 0 representing the clean state and = the fully noisy state. Starting from the clean latent z0 (i.e., the latent output of the VAE encoder), the forward inversion process computes zt from zt1 by first using the UNet noise predictor ϵθ(, ) to estimate the noise in zt1, which reconstructs an approximation of the original clean latent ˆz0. This estimated clean latent is then perturbed to produce the next noisy latent zt: αt ˆz0 + 1 αt ϵθ(zt1, 1), zt = (5) αt ˆz0 retains scaled version of the estimated clean signal, and where the term adds controlled noise with scaling factors aligning with the pre-defined schedule αt. 1 αt ϵθ() Oppositelly, the backward denoising process iteratively refines zt to zt1, gradually reducing noise. From the current noisy latent zt, the model first estimates the clean latent ˆz0 by inverting the noise 17 Preprint additionsubtracting the predicted noise ϵθ(zt, t) (scaled by the schedule 1 αt): zt ˆz0 = 1 αt ϵθ(zt, t) αt . (6) Using this estimated clean latent ˆz0, the denoised latent zt1 is computed by re-adding controlled amount of noise to ˆz0, with the noise level reduced compared to zt, such as zt1 = αt1 ˆz0 + (cid:112)1 αt1 ϵθ(zt, t). (7) This re-noising step ensures gradual transition toward the clean state: as decreases, αt1 increases to approaching 1, so the weight on the estimated clean structure ˆz0 grows, while the weight on the noise term shrinks. This stepwise denoising is critical for editing, as the iterative formulation enables conditional guidance (e.g., text prompts or drag constraints) to be seamlessly injected at each stage, thereby ensuring precise and progressively refined control over the final output. Rectified Flow with ODE Solver In contrast, rectified flow models, as adopted in our DragFlow framework leveraging the DiT prior, reformulate the generative process through approximate straight-line trajectories between data and noise distributions, enabling more efficient and deterministic sampling with fewer steps. For simplicity, we normalize the time parameter to [0, 1]. The forward process linearly interpolates the latent as zt = (1 t)z0 + tϵ, (8) where z0 is the clean VAE-encoded latent (same as the UNets), and ϵ (0, I) is standard Gaussian noise. The associated velocity field is constant, defined as dzt dt = ϵ z0. In this way, the model can model parameterized velocity vθ(zt, t) ϵ z0 via objectives like conditional flow matching, by minimizing = Et,zt,ϵ vθ(zt, t) (ϵ z0)2 2 . (9) For the backward denoising process, we solve the ordinary differential equation (ODE) dz/dt = vθ(z, t) backward in time from = 1 to = 0, starting from z1 ϵ. In discrete steps (e.g., via Fireflow Solver (Deng et al., 2024)), this approximates: ztt = zt + (t) vθ(zt, t), (10) where > 0 is the step size. Conversely, to obtain the noisy latent zt during inversion (from clean z0 at = 0 to z1 at = 1), we integrate the ODE forward: zt+t = zt + vθ(zt, t). (11) This straight-path formulation in rectified flow supports fewer function evaluations compared to the curved trajectories with more complicated noise schedules in DDIM, enabling our region-level affine supervision in DragFlow to leverage more robust priors for drag-based editing. B.2 POINT-BASED IMAGE DRAG-EDITING Point-based drag editing was first introduced by DragGAN (Pan et al., 2023), as an interactive paradigm for image manipulation, enabling users to directly move key points on an image to achieve desired transformations. Unlike text-guided methods, which often struggle with ambiguity in complex scenes, point-based dragging encodes editing intentions through spatially localized control points. This approach aligns well with users intuitive interaction patterns, providing straightforward yet effective means of specifying editing goals. User Input. Each editing task requires the following basic inputs: An original image (converted to the initial latent z0 through VAE encoding). set of handle points {hi}n Corresponding target points {ti}n Mask to protect or constrain regions that should remain unchanged. i=1 indicating locations to be manipulated. i=1 representing desired positions after dragging. Preprint Core Components. The workflow typically consists of two interconnected modules: 1. Motion Supervision (MS). MS is designed to ensure the model preserves image features while enforcing alignment between source and target points. MS computes losses based on: Alignment Loss: Measures feature differences between patches around original handle points and patches around their current predicted locations. (cid:88) (cid:88) (cid:88) Lalign = (cid:13) (cid:13)Fq(zk ) sg(Fp(z0 ))(cid:13) (cid:13)1 , (12) pΩ(h0 ,r) qΩ(hk ,r) where () extracts local features, Ω(, r) denotes patch of radius r, and sg() is the stop-gradient operator. and define the source patch and the manipulated patches, respectively. Smoothness Loss: Encourages gradual changes in the feature space. Lsmooth = (cid:88) (cid:88) qΩ(hk ,r) (cid:13) (cid:13)Fq(zk ) sg(Fq(zk ))(cid:13) (cid:13)1 , Mask Loss: Penalizes unintended modifications outside user-defined regions. Lmask = (zk sg(z0 )) (1 B)1, The overall motion supervision loss is: LM = βLalign + (1 β)Lsmooth + λLmask, which is backpropagated to iteratively update the latent code: zk+1 = zk lr LM zk . (13) (14) (15) (16) 2. Point Tracking (PT). PT updates handle point locations across diffusion steps to ensure , the new location is determined they follow intended trajectories. For each handle point hk via nearest-neighbor feature matching: hk+1 = argminqΩ(hk ,r2)Fq(zk+1 ) Fh0 (z0 )1. (17) Workflow Summary. The point-based editing process proceeds iteratively: 1. Apply several MS steps to align features toward target positions while preserving image consistency during the updating processes. 2. Perform PT to update handle points based on feature tracking. 3. Repeat the MS and PT cycle until the handle points converge to their targets. To sum up, classic point-based drag editing leverages feature-level supervision and PT to enable localized manipulations, ensuring that the edited image remains coherent with the original while reflecting user-specified modifications. However, this pipeline inherently suffers from several limitations: nearest-neighbor search and PT introduce high uncertainty during optimization; the explicit influence of each control point is confined to narrow feature neighborhood, which restricts the scope of effective guidance. By contrast, region-based dragging naturally extends these ideas by operating over semantically coherent masks, thereby offering more stable, interpretable, and semantically meaningful editing outcomes."
        },
        {
            "title": "C IMPLEMENTATION DETAILS",
            "content": "This section highlights the core motion strategy of region-based drag operations through progressive affine transformations. We define multiple types of drag operations, each designed to enhance user control and precision in image editing. Subsequent subsections detail the implementations of these operations, all leveraging progressively interpolated affine transformations for seamless transitions. Additionally, the final subsection offers supplementary settings related to the experimental section. 19 Preprint C.1 PROGRESSIVE TRANSFORMATION IN SUBTASKS Operation in Subtasks. As introduced in the main text, we define three types of drag operations: relocation, local deformation, and rotation. Recall these definitions, the source masks Mi denote the initial regions, with centroids bi guiding relocation and local deformation toward the target region indicated by the centroid ti. For rotation, the anchor ai specifies the pivot. Each transformation is realized through affine updates, with parameters ξ(k) interpolated over iterations to gradually propagate the source mask toward its target configuration. In the following subsections, we detail how these subtask settings facilitate the affine transformation in region-based image dragging. Progressive Motion Schedule. pute the full motion schedule ξ(K) In practice, for each region-specific drag operation, we only comduring the initial update iteration: (cid:40)(ti bi), ξ(K) = (biaiti, ai) , (Rotation) , (Relocation & Deformation) (18) where bi, ti, and ai denote the begin, target, and anchor points, respectively. Rather than recomputing the dragged and left distances at every iteration, subsequent steps obtain their progressive schedules directly via linear interpolation: ξ(k) = , (19) This design ensures that the motion evolves smoothly from the initial to the target state. The resulting progressive motion schedule is then injected into the affine transformation operator Ω(, ξ(k) ), to enforce consistent supervision over the sequence of incremental updates. [0, 1]. ξ(K) C.2 RELOCATION TASKS Relocation involves shifting an entire region to new position while preserving its original geometry and scale. Recall our definition in Subsec. 3.2 This operation is parameterized by displacement vector derived from the difference between the target point ti and the source centroid bi, scaled linearly as ξ(k) = (ti bi). To apply this, for example, we can consider point = (u, v) within the source region mask (0) The transformed point = (u, v) can be computed via homogeneous coordinates: . (cid:35) (cid:34)u 1 = (cid:34)1 0 0 0 du 1 dv 1 0 (cid:35) (cid:35) (cid:34)u 1 , (20) where (du, dv) is the displacement vector from ξ(k) . Breaking down the matrix: The top-left 2 2 submatrix is the identity, ensuring no rotation or scaling; The relocation components du and dv in the first two rows of the third column directly add to the coordinates: = + du, = + dv; The bottom row maintains homogeneity. This matrix is applied across the whole operation patch, resulting in an efficient shift of the entire region in dragging steps. C.3 DEFORMATION TASKS Deformation enables localized adjustments by selectively displacing subregions, effectively altering the overall shape without global rigidity. In our setup, this is treated similarly to relocation, utilizing the same affine transformation method as described in the relocation operation. The key difference lies in its application: it targets only the edge areas of the object to be edited based on scene requirements, achieving effects such as elongation or shortening. This selective application distinguishes deformation from full-region relocation, allowing for intuitive shape modifications as visualized in our methods overview. Preprint Figure 7: Visualization of DiT latent features (Sample 1 out of 2) based on PCA using the top 5 principal components. C.4 ROTATION TASKS Rotation pivots region around fixed anchor, reorienting it by progressively interpolated angle. The rotation angle is determined geometrically from the triangle formed by bi (i.e., the source region centroid), ai (i.e., the user-specified anchor), and ti (i.e., the target region centroid). Over iterations, the interpolated parameters are defined as = (cid:0) ξ(k) biaiti, ai (cid:1) , which guarantees smooth progression from the original orientation toward the desired angular displacement. For point = (w, z) in (k) ξ(k) , the updated coordinates = (w, z) are obtained as , rotated around anchor = (wc, zc) by an angle ϕ derived from (cid:35) (cid:34)w 1 = (cid:34)1 0 wc zc 0 1 1 0 0 (cid:35) (cid:34)cos ϕ sin ϕ 0 0 1 cos ϕ sin ϕ 0 (cid:35) (cid:34)1 0 0 (cid:35) 0 wc 1 zc 1 0 (cid:35) (cid:34)w 1 . (21) This composite transformation can be interpreted step by step: The rightmost matrix translates the point so that the anchor ai coincides with the origin, removing bias from the global coordinate system. The central rotation matrix applies the angular displacement ϕ, producing the intermediate rotated coordinates (w, z) where = cos ϕ sin ϕ and = sin ϕ + cos ϕ. Preprint Figure 8: Visualization of DiT latent features (Sample 2 out of 2) based on PCA using the top 5 principal components. The leftmost matrix translates the rotated point back into the original coordinate frame, re-centering the result around ai. This decomposition not only clarifies the geometric intuition behind rotation but also integrates seamlessly with our iterative framework: at each step k, the patch masks (k) are updated under this rotation transformation, ensuring gradual, controlled reorientation. Compared with relocation and deformation, rotation requires an explicit anchor to define the pivot, highlighting its distinct interaction design while still being governed by the same affine transformation principles. C.5 DETAILS ABOUT EXPERIMENTAL SETTINGS Layers for Feature Manipulation. We empirically identify the 17th and 18th double-stream blocks of the DiT backbone as the most effective positions for applying drag optimization. To substantiate this choice, we conduct visualization-based analysis of the main DiT modules (refer to Fig. 7 and Fig. 8 for details). Our selection is guided by the principle that effective feature blocks should retain high representational fidelity to the input. While certain blocks may encode large amounts of information, much of it can be dominated by noise. By examining PCA visualizations, we qualitatively assess whether the leading components align with the appearance of the original image, thereby confirming that the selected layers capture the essential visual characteristics required for precise and robust editing. As illustrated in the figures, the latent representations from DOUBLE-17 and DOUBLE-18 retain rich and flexible features, which can be effectively manipulated during editing. In contrast, certain blocks (e.g., SINGLE-20 and SINGLE-30) exhibit overly clean latents, making it difficult to perform meaningful edits. Other blocks (e.g., DOUBLE-04 and SINGLE-06) contain too few semantically informative features, where optimization tends to struggle in preserving 22 Preprint Figure 9: Region operation masks M(k) created by progressive affine transformations at each step across multiple subtasks. Each dragging process consists of 50 steps (k = 0 to 49), followed by 20 additional steps (k = 50 to 69) that repeat the final motion iteration to further refine the feature quality of the post-dragging region. source identity and achieving precise dragging control. Taken together, these comparisons suggest that our default setting, by using DOUBLE-17 and DOUBLE-18, offers favorable balance: it preserves stable feature representations with sufficient semantic and spatial information, while also maintaining rich identity-related features to support effective editing. This justifies its adoption as the backbone choice in DragFlow. Layers for KV Operations. Following the ID preservation design introduced in FreeFlux (Wei et al., 2025), we select subset of layers for KV injection. Specifically, we inject into double-stream blocks at layers [0, 7, 8, 9, 10, 18] and single-stream blocks at layers [6, 9, 18, 23, 26, 31, 37]. During inversion, KV caches are maintained at all valid timesteps, and the corresponding caches are injected back at the corresponding sampling timesteps. To ensure that drag optimization remains consistent with the expected denoising dynamics, we apply the same KV injection during the feature extraction in dragging, consistent with the actual denoising process. Affine Transformation Steps. During the dragging process, the motion from the source region to the target region is primarily executed over the first 50 steps (k = 0 to 49), where each iteration progressively applies affine transformations to the latent representations, thereby facilitating the drag procedure through gradient updates. After this stage, DRAGFLOW performs an additional 20 optimization steps (k = 50 to 69) by repeating the final affine transformation, which further refines the feature expression of the dragged object, helps it better adapt to the new semantic context, and enhances the consistency between the preand post-dragging regions. As illustrated in Fig. 9, the patch operation mask M(k) , obtained through progressive transformations, evolves smoothly with increasing k, thereby driving the gradient optimization process to ensure effective dragging. i"
        },
        {
            "title": "D ADAPTIVE INPUT PROCESSING",
            "content": "To ensure robust and user-aligned editing, DragFlow incorporates an adaptive input processing pipeline that systematically addresses three key challenges. First, when multiple operations are applied to single image, we introduce an adaptive weighting scheme to balance region-level optimization and prevent dominance by large areas (see Appendix D.1). Second, to protect uneditable content during optimization, we design an adaptive gradient mask generation strategy that provides strict spatial constraints while maintaining flexibility for complex transformations (refer to Appendix D.2). Finally, to further reduce user effort and improve precision, we leverage multimodal large language model (MLLM) to automatically generate candidate prompts and semantic tags, 23 Preprint which are subsequently refined through minimal human intervention (detailed in Appendix D.3). Together, these components form cohesive input processing framework that enhances both the accuracy and practicality of DragFlow in real-world interactive editing scenarios. D.1 REGION WEIGHT REGULARIZATION FOR MULTI-OPERATIONS When multiple drag operations are specified for single image (i.e., > 1), the loss function in Eq. 1 incorporates weighting coefficients {γi}N i=1 to balance the influence of each region, preventing larger regions from dominating the optimization. These weights are computed adaptively based on the relative sizes of the manipulated regions, ensuring equitable gradient contributions. Formally, for each operation i, let Si denote the relative size of the source mask (0) , defined as: Si = (cid:80) (0) (0) , (22) is the number of non-zero (unmasked) pixels, and (0) where (cid:80) (0) elements in the mask. Under this setup, raw weight wi can then be calculated as: (cid:18) (cid:19) is the total number of wi = clamp 1.0 + , 1.0, 5.0 , (23) 0.5 Si + 0. where clamp(, a, b) restricts the value to the interval [a, b]. This formulation assigns higher weights to smaller regions to amplify their impact. And the total raw weight sum is = (cid:80)N i=1 wi. The normalized weights are: 1 γi = if = 0, wi otherwise. (24) If = 1, we set γ1 = 1 directly. This adaptive scheme ensures (cid:80)N optimization across diverse region scales. i=1 γi = 1, promoting balanced D.2 ADAPTIVE GRADIENT MASK GENERATION To sufficiently protect uneditable regions in our procedure, we generate an adaptive bounding mask that only uncovers the edited areas. This mask is derived from the user-provided source masks {M (0) i=1, providing static envelope for feature preservation during optimization. It differs from the iterative patch masks (k) by serving as holistic boundary for the entire dragging process. i=1 and their target points {ti}N }N Compare to Mask Loss. Compared to the conventional mask loss strategy, gradient masking offers more precise mechanism for constraining gradient flow, thereby preventing optimization from introducing unintended perturbations into uneditable regions. key limitation of mask loss is that it can only partially restrict future updates, while inadvertently retaining undesired changes that have already been introduced. In contrast, gradient masking directly blocks gradient propagation to these regions, ensuring that they remain unaffected throughout the optimization process. = Ω(M (0) Automatic Mask Creation. For each source mask (0) , we first apply the appropriate affine transformation (as defined in Eq. 3.2) with full interpolation (i.e., k=K) to obtain the final target mask (K) = ti bi for relocation/deformation or (biaiti, ai) for rotation. Next, for each i, we compute the union mask Ui = (0) . To ensure comprehensive coverage, we enclose Ui with minimal rotated bounding rectangle. The points in Ui are collected as Pi = {p Ui(p) = 1}. The rectangle parameters are then derived as (K) ), where ξ(K) , ξ(K) i (ci, (wi, hi), ϕi) = minAreaRect(Pi), (25) 24 Preprint where ci is the center, (wi, hi) are the dimensions, and ϕi is the rotation angle. The vertices of this rectangle are obtained via Vi = boxPoints((ci, (wi, hi), ϕi)). (26) Since multiple operations may exist, each yielding an independent rectangle, we merge them into single adaptive mask by filling all convex polygons from an image-shape initiated canvas : = (cid:91) i= fillConvexPoly(, Vi). (27) Finally, is binarized to [0, 1], ensuring no excessive expansion while covering all transformed regions for effective integration into the latent optimization. D.3 LEVERAGING MLLM FOR PROMPT AND TAG GENERATION In practical usage, the DragFlow framework provides user-friendly interactive interface, enhanced by multimodal large language model (MLLM) to support intuitive and precise image editing. The system emphasizes high automation to reduce user effort, while two-step confirmation workflow ensures that user intentions are accurately communicated and ambiguities are minimized. The interaction begins with the user providing an input image and roughly indicating the target region via simple scribble, along with click specifying the desired target position. The system converts the scribbled region into an initial operational mask representing the affected area. Leveraging an automatic mask generation algorithm Appendix D.2, users do not need to redraw the target region for task specification. The original image and operational mask are then processed by the MLLM, which infers the users editing intent and proposes set of candidate prompts. Specifically, the MLLM generates ten potential prompts paired with corresponding task classes, from which users select the one that best reflects their intended operation. After confirming the operational intent and task class, the interface produces preview of the expected outcome based on the current operation parameters. For rotation operations, users specify an additional anchor point as the rotation center, which can be interactively adjusted while observing real-time updates in the preview. This iterative adjustment allows for fine-grained control, ensuring the final result closely aligns with user expectations. By combining automated inference with user-in-the-loop refinement, this design streamlines the editing process while faithfully translating high-level user intentions into precise image manipulations. Here we present the prompt used for the prompt and tag generation procedure: Refer to the original image, and the dragged image with the blue starting region, estimated green target region, and the arrow direction. You need to describe the content and the object for editing of the picture in English, in terms of background details and editing changes. Then you should guess the editing intents from the user by selecting one label for each answer, where the label classes have relocation, deformation, rotation. Your tasks: - (a) You should first provide detailed description about the original image (e.g., include, but are not limited to objects, spatial relationship, color, style, structure). Then try to describe the motion/editing in short words. - (b) You should provide the ten most possible guesses about the static condition of the after-dragged image, and at most 60 words for each. See if you can provide more details to facilitate the editing."
        },
        {
            "title": "E CRITERION DETAILS",
            "content": "To systematically evaluate the effectiveness of dragging operations, we introduce set of quantitative criteria that assess both visual fidelity and spatial accuracy. These criteria fall into two complementary families: Image Fidelity (IF), which measures the extent to which semantic content, source 25 Preprint Figure 10: For IFs2t and IFs2s, the criterion computation considers only the feature discrepancies within the labeled blocks. In IFs2t, the purple region on the left image denotes (M (K) xaff), corresponding to the source region of (M (0) x), while the red patch on the right image represents the post-drag target (M (K) x). By contrast, the criterion IFs2s compares the same purple original region (0) across two images: the source (left) and the dragged result (right). Lastly, IFbg evaluates all uneditable areas, as indicated by the black areas on the gradient mask as (1 B). regions, and background integrity are preserved or altered as intended (detailed in Appendix E.1); and Mean Distance (MD), which provides geometric and feature-based assessments of the dragging process (see Appendix E.2). Together, these metrics capture different yet complementary aspects of editing quality, enabling fair and comprehensive evaluation of diverse dragging approaches. E.1 COMPUTATION OF IMAGE FIDELITY (IF) IFs2t Evalutaion. The identity fidelity from the source to the target (i.e., IFs2t) evaluates how well original source features are preserved in the target regions after moving, promoting semantic consistency in dragged content. higher IFb2t signifies superior fidelity, indicating minimal perceptual loss during the feature transfer. For each region i, we first affine-transform the masked original image to align with the target configuration using the full interpolation parameter of = K: (cid:16) xaff = Ω (0) x, ξ(K) (cid:17) , (28) followed by masking both sides with the target region mask (K) . Then the score is averaged as: IFs2t = 1 1 (cid:88) i=1 (cid:16) LPIPS (K) xaff, (K) x(cid:17) . (29) IFs2s Evalutaion. To assess the extent to which original features are effectively removed from the source regions after editing, we define the identity fidelity from source region to source region (i.e., IFs2s), which measures the perceptual dissimilarity between the source region features in the original image and the edited image x. lower IFs2s indicates better performance, as it reflects greater divergence and implies successful moving-out of the selected features. Formally, for each 26 Preprint source region i, we compute the LPIPS distance on region-masked image tensors and get the final mean score over all task regions via IFs2s = 1 1 N (cid:88) i=1 LPIPS (cid:16) (0) x, (0) x(cid:17) , (30) where indicates the element-wise multiplication, and (0) (i.e., the operation region given by the user); indicates the original region mask IFbg Evalutaion. To ensure background preservation outside edited regions, we introduce the background identity fidelity IFbg, quantifying feature consistency in protected areas defined by the complement of the adaptive gradient mask B. higher IFbg denotes better integrity, with minimal changes to non-targeted zones. Using the protection mask (1 B), we yield the score: IFbg = 1 LPIPS ((1 B) x, (1 B) x) . (31) To aid interpretation, Fig. 10 presents visual example demonstrating how the three proposed criteria are applied in practice. E.2 COMPUTATION OF MEAN DISTANCE (MD) MD1 Implementation. We implement MD1 following the existing criteria established by Xia et al. (2025). MD1 enables precise feature matching within the uneditable region, allowing us to validate the effectiveness of the dragging procedure by measuring the distance between the centroid of the source feature region and its most similar corresponding feature. MD2 Implementation. Building upon the original MD2 design proposed in Lu et al. (2024a), we introduce an enhanced version that provides more precise and informative feedback for region-based drag operations. Unlike the original method, which computes feature matching distances based on manually annotated sample points, our approach automatically evaluates feature differences around the centroid scope of the preand post-drag regions. By leveraging this centroid-based formulation, we eliminate the inaccuracies and subjective biases inherent in manual annotations and also ensure more consistent metric for assessing the effectiveness of various dragging strategies. This improvement allows for more faithful reflection of the actual feature transformations induced by the dragging process and facilitates fairer comparisons across different methods."
        },
        {
            "title": "F ADDITIONAL BASELINE INFORMATION",
            "content": "Here we provide the official project pages for the baseline methods used in our comparisons. All implementations follow the default configurations and instructions provided by their authors: 1 CLIPDrag: https://github.com/HKUST-LongGroup/CLIPDrag 2 DragDiffusion: https://github.com/Yujun-Shi/DragDiffusion 3 DragLoRA: https://github.com/Sylvie-X/DragLoRA 4 DragNoise: https://github.com/haofengl/DragNoise 5 FastDrag: https://github.com/XuanjiaZ/FastDrag 6 FreeDrag: https://github.com/LPengYang/FreeDrag 7 GoodDrag: https://github.com/zewei-Zhang/GoodDrag 8 InstantDrag: https://github.com/SNU-VGILab/InstantDrag 9 RegionDrag: https://github.com/Visual-AI/RegionDrag 27 Preprint Figure 11: Examples of real data samples from our ReD benchmark: the first row shows the expected dragging results, where the green region is estimated from the user-specified target centroid (see instruction); the second row presents the source images, while the third row highlights the usermarked operation regions in the form of masks, which may include multiple valid regions; and the last row depicts the adaptively generated masks derived from Appendix D.2. Those two examples correspond to the instructions provided in Appendix G.3."
        },
        {
            "title": "G BENCHMARK DETAILS",
            "content": "G.1 FORMATION OF THE ReD BENCHMARK To evaluate model performance on the regional drag-based image editing task, we introduce new benchmark, the Regional-based Dragging (ReD) Bench, consisting of 120 images annotated with precise drag instructions at both point and region levels. Each manipulation in the dataset is associated with an intention label, selected from relocation, deformation, or rotation. For every image, we provide two complementary instruction sets corresponding to point-based and region-based dragging. The region-based annotations are supplied as multiple PNG masks, with each region uniquely represented by its centroid for cross-reference. The drag annotations include multiple start-to-target point pairs, which can be directly aligned with the region annotations, ensuring consistency in task intention. Additionally, we provide background prompts and editing intention prompts for each image to facilitate multimodal tasks, along with masks generated using the DragFlow automatic masker Appendix D.2. These design choices enable more faithful representation of user intents underlying the provided drag instructions. G.2 ADOPTION OF THE DragBench-DR BENCHMARK To further assess the effectiveness of DragFlow on broader spectrum of images, we adapt and evaluate it on DragBench-DR (Lu et al., 2024a). DragBench-DR extends the classic point-based dragging benchmark DragBench to region-based operations. Unlike the original benchmark, which relies on sparse point guidance, DragBench-DR formulates edits over regions, thereby providing clearer reflection of user intentions. For evaluation, the accompanying metrics compare the predrag source region with the post-drag target region by computing differences over pre-annotated correspondence points. Despite this extension, as noted by the authors, DragBench-DR remains consistent with its point-based counterpart, while more effectively capturing region-level semantics in interactive editing tasks. While DragBench-DR extends the benchmark to region-based operations, its evaluation protocol (named MD2) still relies on point-based comparisons: differences are computed between pre-drag and post-drag regions using pre-annotated correspondence points. This design can introduce mismatches, as region-level edits are not faithfully captured by sparse point feature correspondences, leading to potentially unfair assessments of region dragging methods. To better align the evaluation with region-based editing and integrate existing datasets into our experiment, we update the 28 Preprint feature comparison criterion by replacing point-based annotations with an automatic centroid-based formulation (see Appendix E.2 for more information). G.3 DEMONSTRATION OF DATA SAMPLES We present two real data samples (i.e., SAMPLE (A) and SAMPLE (B)) from our ReD benchmark. The corresponding instructions are provided as follows, and the images are shown in Fig. 11. 1 { % SAMPLE (A) % \"region_operations\": { \"0\": { \"task\": \"rotation\", \"centroids\": [[337, 175], [379, 179]], \"anchors\": [351, 256] } }, \"point_operations\": { \"begin_points\": [[326, 111], [342, 190]], \"target_points\": [[400, 116],[376, 198]] }, \"background_prompt\": \"From rear view, student in blue denim jacket raises their hand in classroom. Wooden desks, large windows (letting in light), and distant teacher form the backdrop. The scene captures an engaged learning moment, with realistic, observational style.\", \"editing_prompt\": \" The student in blue denim jacket moves his arm rightward, with his hand closer to the right side on this image.\" 3 4 6 7 8 9 10 12 13 14 15 } 1 { % SAMPLE (B) % 2 \"region_operations\": { 3 4 5 6 7 9 10 11 12 13 15 16 17 18 19 21 22 23 24 25 } \"0\": { \"task\": \"deformation\", \"centroids\": [[251, 52], [357, 52]], \"anchors\": null }, \"1\": { \"task\": \"deformation\", \"centroids\": [[281, 200], [192, 195]], \"anchors\": null }, \"2\": { \"task\": \"deformation\", \"centroids\": [[221, 335], [307, 335]], \"anchors\": null } }, \"point_operations\": { \"begin_points\": [[284, 11], [244, 96], [280, 165], [287, 235], [243, 305], [244, 365]], \"target_points\": [[392, 11], [356, 97], [193, 162], [199, 233], [332, 306], [335, 367]] } \"background_prompt\": \"The image is an aerial view of coastal scene. Theres beach with light - colored sand between dense forest (with green, yellow, and orange foliage) and turquoise - blue sea. The forest covers the left side, the beach runs along the middle, and the sea is on the right.\", \"editing_prompt\": \"The top and bottom sections of the beach are narrowed to the outside, and the middle part is narrowed inside, altering the coastline shape to form bay.\" 29 Preprint"
        },
        {
            "title": "H EXTRA QUALITATIVE RESULTS",
            "content": "In addition to the qualitative studies reported in the main experiments, we provide further examples in Fig. 12 and Fig. 13. These additional visualizations help to illustrate the advantages of our approach across diverse editing scenarios."
        },
        {
            "title": "I LLM USAGE STATEMENT",
            "content": "We used large language models for text polishing and grammar correction during manuscript preparation. No LLMs were involved in the design of the method, experiments, or analysis. All content has been carefully verified and validated by the authors. 30 Preprint Figure 12: Extra qualitative comparison (Part 1 out of 2) of DragFlow with multiple baselines. 31 Preprint Figure 13: Extra qualitative comparison (Part 2 out of 2) of DragFlow with multiple baselines."
        }
    ],
    "affiliations": [
        "Nanyang Technological University",
        "National University of Singapore"
    ]
}