{
    "paper_title": "\"Principal Components\" Enable A New Language of Images",
    "authors": [
        "Xin Wen",
        "Bingchen Zhao",
        "Ismail Elezi",
        "Jiankang Deng",
        "Xiaojuan Qi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce a novel visual tokenization framework that embeds a provable PCA-like structure into the latent token space. While existing visual tokenizers primarily optimize for reconstruction fidelity, they often neglect the structural properties of the latent space -- a critical factor for both interpretability and downstream tasks. Our method generates a 1D causal token sequence for images, where each successive token contributes non-overlapping information with mathematically guaranteed decreasing explained variance, analogous to principal component analysis. This structural constraint ensures the tokenizer extracts the most salient visual features first, with each subsequent token adding diminishing yet complementary information. Additionally, we identified and resolved a semantic-spectrum coupling effect that causes the unwanted entanglement of high-level semantic content and low-level spectral details in the tokens by leveraging a diffusion decoder. Experiments demonstrate that our approach achieves state-of-the-art reconstruction performance and enables better interpretability to align with the human vision system. Moreover, auto-regressive models trained on our token sequences achieve performance comparable to current state-of-the-art methods while requiring fewer tokens for training and inference."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 5 8 6 8 0 . 3 0 5 2 : r Principal Components Enable New Language of Images Xin Wen1 Bingchen Zhao2 Equal Contribution Ismail Elezi3 Project Lead Jiankang Deng4 Xiaojuan Qi1 Corresponding Author 1University of Hong Kong 2University of Edinburgh 3Noahs Ark Lab 4Imperial College London https://visual-gen.github.io/semanticist/ Figure 1. Image reconstruction using our structured visual tokenization approach, which uniquely enables decoding at any token count. Each column shows reconstructions resulting from progressively increasing the number of tokens, from single token to 256 tokens. Unlike conventional tokenizers that require fixed number of tokens for meaningful decoding, our method ensures that each token incrementally refines the image, with earlier tokens capturing the most salient features and later ones adding finer details. This demonstrates the flexibility and effectiveness of our approach in producing coherent images even with very few tokens (view more in Fig. 17 in the Appendix)."
        },
        {
            "title": "Abstract",
            "content": "We introduce novel visual tokenization framework that embeds provable PCA-like structure into the latent token space. While existing visual tokenizers primarily optimize for reconstruction fidelity, they often neglect the structural properties of the latent spacea critical factor for both interpretability and downstream tasks. Our method generates 1D causal token sequence for images, where each successive token contributes non-overlapping information with mathematically guaranteed decreasing explained variance, analogous to principal component analysis. This structural constraint ensures the tokenizer extracts the most salient visual features first, with each subsequent token adding diminishing yet complementary information. Additionally, we identified and resolved semantic-spectrum coupling effect that causes the unwanted entanglement of high-level semantic content and low-level spectral details in the tokens by leveraging diffusion decoder. Experiments demonstrate that our approach achieves state-of-the-art reconstruction performance and enables better interpretability to align with the human vision system. Moreover, auto-regressive models trained on our token sequences achieve performance comparable to current state-of-the-art methods while requiring fewer tokens for training and inference. 1. Introduction The limits of my language mean the limits of my world. Ludwig Wittgenstein Tractatus Logico-Philosophicus The pursuit of compact visual representations has long been fundamental goal, driving advancements in visual recognition [20, 50] and image generation [53, 60]. One of the earliest approaches, Principal Component Analysis (PCA) [46], achieves this by introducing decorrelated, orthogonal components that capture the most significant variations in the data in progressively diminishing manner (i.e., (a) Semantic-spectrum coupling. Comparison of the frequency-power spectra for different tokenizers. Here, we decompose the tokens from the tokenizers to demonstrate their contribution to the spectrum of the generated image. The VQ-VAE tokenizer [48] is decomposed by performing PCA in its latent token space, and the 1D TiTok [60] is decomposed by replacing all but the first tokens with mean token. For SEMANTICIST, on the other hand, we can clearly see that with any number of tokens, the spectrum remains closely matched with the original image, demonstrating that SEMANTICIST can decouple semantics and spectrum in its tokenization process. (b) Our tokenizer decomposes the image into visual concepts following PCA-like coarse-to-fine structure where first few tokens capture most semantic information and the rest refine the details. Figure 2. Spectrum analysis and the PCA-like structure of our tokenizer. orderliness), thereby reducing redundancies. This enables PCA to effectively reduce dimensionality while preserving essential information, making it powerful tool for compact representation. Building on this foundation, Hinton and Salakhutdinov [20] proposed nonlinear generalization of PCA using autoencoders, which further emphasizes structured latent space for effective learning and reconstruction error minimization. While modern approaches such as (vector quantized) variational autoencoders [24, 53] share similar goals as earlier methodscompressing images into compact, lowdimensional space while minimizing reconstruction errors they have largely abandoned the inherent structural properties, such as orthogonality and orderliness, that were critical to the success of earlier PCA-based techniques. For instance, mainstream methods employ 2D latent space [24, 48, 53], where image patches are encoded into latent vectors arranged in 2D grid. While this approach achieves high reconstruction quality, it introduces redundancies that scale poorly as image resolution increases. More recently, 1D tokenizers [12, 60] have been proposed to find more compact set of latent codes for image representation. Although these methods more closely resemble earlier approaches, they lack structural constraints on latent vectors, making optimization challenging and often resulting in high reconstruction errors. We further investigate the latent space of state-of-theart methods, including VQ-VAE [48] and TiTok [60], and find that the lack of structured latent space leads to an inherent tendency for their learned representations to couple significant semantic-level content with less significant lowlevel spectral informationa phenomenon we refer to as semantic-spectrum coupling. As shown in Fig. 2a, increasing the number of latent codes simultaneously affects both the power spectrum (reflecting low-level intensity information) and the reconstruction of semantic content in the image. Further details on this coupling effect are presented in Fig. 5. The above motivates us to ask: Can insights from classic PCA techniques be integrated with modern 1D tokenizers to achieve compact, structured representation of images one that reduces redundancy while effectively decoupling semantic information from less important low-level details? To this end, we reintroduce PCA-like structure incorporating both orthogonality and orderlinessinto 1D latent tokens. Specifically, we propose dynamic nested classifier-free guidance (CFG) strategy during training to induce an orderliness bias in the tokens, enforcing the emergence of PCA-like structure where token importance progressively decreases. This is achieved by incrementally replacing later tokens in the 1D sequence with shared null condition token at an increasing probability, thereby encouraging earlier tokens to capture the most semantically significant features. This strategy also implicitly promotes orthogonal contributions among tokens (see Appendix A). By doing so, our approach ensures coarse-to-fine token hierarchy with decreasing importance, where each token contributes unique information to reconstruct the image. This stands in contrast to previous 1D tokenizers [12, 60], which enforce 1D structure but lack the orderliness and orthogonality properties that our method introduces (see Fig. 2a). Moreover, the PCA-like structural property enables flexible encoding of the image by using the most significant tokens. However, inducing PCA-like structure alone is insufficient. The visual world exists in high-dimensional space, and the nested CFG technique might converge to an arbitrary PCA-like structure that does not necessarily disentangle semantically significant content from less important low-level details. To ensure semantically meaningful features emerge in the latent tokens, we propose leveraging diffusion2 based decoder that follows spectral autoregressive process [10, 42], which progressively reconstructs images from low to high frequencies, conditioned on our 1D latent codes. By doing so, the 1D latent tokens are encouraged to focus on capturing semantically significant information while avoiding entanglement with low-level spectral information. Finally, the 1D latent codes derived from our tokenizer exhibit PCA-like hierarchical structure with progressively diminishing significance while also being semantically meaningful. As shown in Fig. 1, all reconstructed images retain the semantic essence of the original, while just 64 tokens are sufficient for high-quality reconstruction. Moreover, as illustrated in Fig. 2a, the power spectrum profile remains nearly identical to that of the original image, regardless of the number of tokens. This suggests that the latent tokens effectively capture semantically significant information while avoiding entanglement with low-level spectral details. Notably, the coarse-to-fine structure of latent tokens mirrors the global precedence effect in human vision [14, 32], phenomenon corroborated by our human perceptual evaluations in Appendix D.1. In our experiments, we demonstrate that SEMANTICIST can achieve state-of-the-art (SOTA) reconstruction FID scores [19] on the ImageNet validation set, surpassing the previous SOTA tokenizer by almost 10% in FID. SEMANTICIST maintains this SOTA reconstruction performance while maintaining compact latent suitable for generative modeling. The autoregressive model trained on the tokens generated by SEMANTICIST can achieve comparable performance with the SOTA models while requiring only 32 tokens for training and inference. Additionally, linear probing in the latent space generated by SEMANTICIST also performs up to 63.5% top-1 classification accuracy, indicating SEMANTICIST can capture not only the essence for reconstructing high fidelity images but also the linear separable features. 2. Related Work Image tokenization aims to transform images into set of compact latent tokens that reduces the computation complexity for generative models like diffusion models and autoregressive models. Thus, we view image tokenization as way of decomposing image into learnable language for the generative model. VQ-VAE [53] is among the most widely used visual tokenizers, combining vector quantization into the VAE [24] framework, VQ-VAE can generate discrete tokens for the input image. Improvements over the VQ-VAE have also been proposed, such as VQGAN [13] which introduces adversarial loss to improve the reconstruction quality, and RQ-VAE [26] which introduces multiple vector quantization stages. The insufficient usage of codebook for vector quantization in VQ-VAE has also raised issues, MAGVITv2 [58] introduces Look-up Free Quantization (LFQ) to alleviate this issue. Semantic information from pre-trained visual foundation model [18, 34, 38] has also been shown to be beneficial for improving codebook usage [64], improve reconstruction [56], and enable better generation by diffusion models [6]. Maskbit [55] proposed modernized VQGAN framework with novel binary quantized token mechanism that enables state-of-the-art conditional image generation performance. Most recently, [17] demonstrates the scaling laws for vision transformer-based [11] visual tokenizers trained with perceptual and adversarial losses. Though effective, these models tokenize the image into 2-dimensional array of tokens where there is no obvious way of performing causal auto-regressive modeling. TiTok [60] and SEED [15] are among the first works to introduce tokenizers that generate tokens with 1-dimensional causal dependency. This dependency enables large language models to understand and generate images. The causal ordering has also been shown to be especially helpful for autoregressive generative models [39]. VAR [49] takes another approach by formulating visual autoregressive modeling as nextscale prediction problem. multi-scale residual quantization (MSRQ) tokenizer is proposed in [49] which tokenizes the image in low-to-high resolution fashion. Following the development of the 1D tokenizers, ALIT [12] demonstrates that 1D tokens can be made adaptive to the image content, leading to an adaptive length tokenizer that can adjust its token length by considering the image entropy, familiarity, and downstream tasks. However, due to the semantic-spectrum coupling effect within these 1D causal tokenizers, the structure within the token sequence generated by these tokenizers is still not clear. In this work, we introduce novel tokenizer that is able to encode an image to 1D causal sequence with provable PCA-like structures. By decoupling the semantic features and the spectrum information with diffusion decoder, this tokenizer is not only useful for vision generative modeling but also closely aligns with human perception and enables better interpretability and downstream performance. Modern generative vision modeling can be roughly divided to two categories, diffusion-based modeling [35] and autoregressive-based modeling [48]. Both modeling techniques typically require visual tokenizer to compress the input visual signal to compact space for efficient learning. Diffusion models demonstrate strong performance since it was introduced [22], they typically follow an iterative refinement process, which gradually denoises from noisy image sampled from Gaussian distribution to clean image. Developments have made efforts toward sharper sample generation [9, 22], and faster generation [47]. The key development in diffusion models is the introduction of latent diffusion models [43] which allows the diffusion process to be performed in the latent space of tokenizer [13]. This drastically reduces the computation cost of the diffusion process and enables many more applications [35, 40]. Moreover, theoretical understanding for diffusion models has shown 3 that the denoising process can be roughly described as spectral autoregressive process [10, 42] where the model uses all previously seen lower frequency information to generate higher frequency information. Auto-regressive models have also been developed for vision modeling, they typically follow left-to-right generation process where the model predicts the next pixel given the previous pixels [51, 52]. Recent works have been developing more advanced auto-regressive models leveraging the architecture improvements from NLP and advanced image tokenization [48]. Auto-regressive models would naturally require an order for which to generate the tokens, some works apply random masking to allow the model to learn random order [4, 28, 59]. VAR [49] introduces novel paradigm of next-scale-prediction formulation for visual auto-regressive modeling, introducing natural orderscaleto auto-regressive modeling. In this work, in observation of the semantic spectrum coupling, we leverage diffusion as decoder for our tokenizer for its spectral auto-regression property to decouple semantic from spectrum information. Additionally, in our experiments, we demonstrate that we can train auto-regressive models on the tokens generated by our tokenizers to achieve comparable performance with the state-of-the-art models. 3. Preliminary In this section, we provide concise summary of the denoising diffusion model [22] as preliminary for understanding our SEMANTICIST architecture. 3.1. Denoising Diffusion Models -step Denoising Diffusion Probabilistic Model (DDPM) [22] consists of two processes: the forward process (also referred to as diffusion process), and the reverse inference process. The forward process from data x0 qdata(x0) to the latent variable xT can be formulated as fixed Markov chain: q(x1, ..., xT x0) = (cid:81)T t=1 q(xtxt1), where q(xtxt1) = (xt; 1 βtxt1, βtI) is normal distribution, βt is small positive constant. The forward process gradually perturbs x0 to latent variable with an isotropic Gaussian distribution platent(xT ) = (0, I). The reverse process strives to predict the original data x0 from the latent variable xT (0, I) through another Markov chain: pθ(x0, ..., xT 1xT ) = (cid:81)T t=1 pθ(xt1xt). The training objective of DDPM is to optimize the Evidence Lower Bound (ELBO): = Ex0,ϵϵ ϵθ(xt, t)2 2, where ϵ is the Gaussian noise in xt which is equivalent to xt ln q(xtx0), ϵθ is the model trained to estimate ϵ. Conditional diffusion models [43] maintain the forward process and directly inject the condition into the training objective: = Ex0,ϵϵ ϵθ(xt, z, t)2 2 , where is the condition for generating an image with specific semantics. Except for the conditioning mechanism, Figure 3. SEMANTICIST tokenizer architecture. The ViT encoder resamples the 2D image patch tokens into 1D causal sequence of concept tokens. These concept tokens are then used as conditions to the DiT decoder to reconstruct the original image. To induce PCA-like structure in the concept tokens, we apply nested CFG. the Latent Diffusion Model (LDM) [43] takes the diffusion and inference processes in the latent space of VQGAN [13], which is proven to be more efficient and generalizable than operating on the original image pixels. 4. SEMANTICIST Architecture 4.1. Overview The design of SEMANTICIST aims to generate compact latent code representation of the visual content with mathematically-guaranteed structure. As first step to induce PCA-like structure, we need to be able to encode the images to sequence with 1D causal ordering. Thus we require an encoder : RHW RKD to map the input image of shape to causally ordered tokens, each with dimension of D. We leverage the vision transformer [11] (ViT) architecture to implement this encoder for SEMANTICIST. Specifically, we first encode the input image x0 RHW to sequence of image patches Xpatch, we also randomly initialize set of concept tokens Xconcept = {z1, z2, . . . , zK} to be passed into the transformer model. Thus, the transformer model within the encoder takes the below concatenate token sequence for processing: = [Xcls; Xpatch; Xconcept] , where Xcls is the [CLS] token. For the patch tokens Xpatch and the [CLS] we do not perform any masking to mimic the standard ViT behavior and for the concept tokens Xconcept, we apply causal attention masking such that only preceding tokens are visible (as illustrated in Fig. 2b) to enforce them learn causally ordered tokenization. After the ViT has processed the tokens, the output concept tokens Xconcept = {z1, z2, . . . , zK} are used as the condition input to the diffusion-based decoder : RKD RHW 4 RHW to learn to denoise noisy version xt of the input image x0. Doing this alone would allow the encoder to encode the information about the image into the concept tokens Xconcept. However, this information is not structured. To induce PCA-like structure with the concept tokens, we apply the nested CFG technique : RKD RKD that will be introduced in later sections. To summarize, the training process of SEMANTICIST is to minimize this training loss similar to the training of conditional diffusion model: decaying property similar to PCA [46], we introduce nested classifier-free guidance (CFG) function : RKD {1, . . . , K} RKD , inspired by nested dropout [23, 41] and Matryoshka representations [25]. Specifically, we sample an integer U{1, . . . , K} and apply to the concept tokens Xconcept = (z1, . . . , zK) as follows: = Ex0,ϵϵ D(xt, (E(X)), t)2 2 , (Xconcept, k) = (z1, . . . , zk1, z, . . . , z) , where is the condition for the forward process timestep, and ϵ is the noise at timestep t. Note that SEMANTICIST generates continuous tokens instead of quantized tokens like previous works [48, 60]. The reason is that we hope SEMANTICIST to capture the PCA-like variance decay structure, which is hard to capture when using quantized tokens. With the usage of Diffusion Loss [28], our experiments on autoregressive modeling with continuous tokens have shown that this design does not affect generative modeling performance. 4.2. Diffusion-based Decoder The decoder for SEMANTICIST is based on the conditional denoising diffusion model. The decoder is implemented using the Diffusion-Transformer (DiT) architecture [35], with the condition Xconcept injected by cross-attention. For efficient training, we adopt the LDM technique by training the decoder on the latent space of pretrained VAE model [43]. This design choice stems from the observation of the semantic-spectrum coupling effect. Whereas, if deterministic decoder is used to directly regress the pixel values like previous state-of-the-art visual tokenizers, the token space learned by the encoder will entangle the semantic content and the spectral information of the image. This will prevent the encoder from learning semantic meaningful PCA-like structure where the first tokens capture the most important semantic contents. In Sec. 3, we describe the diffusion forward process as gradually corrupting the image with Gaussian noise, which is filtering out more and more highfrequency information. Since the training objective for the diffusion model is to reverse this forward process, the model naturally learns to generate low-frequency information first and then high-frequency details. This is described in [36, 42] as spectral auto-regressive process where the diffusion process itself can already generate the spectral information, leaving the conditions Xconcept to be able to encode most semantic information rather than spectral information. 4.3. Inducing the PCA-like Structure Although the encoder and diffusion-based decoder can produce high-quality reconstructions, their concept tokens Xconcept lack any explicit structural regularization beyond causal ordering. To impose hierarchical variance5 where is learnable null-condition token shared across all masked positions. Intuitively, forcing positions k, . . . , to become null tokens compels the earlier tokens z1, . . . , zk1 to encode the most salient semantic content. Over the course of training, the uniform sampling of induces coarse-to-fine hierarchy in the concept tokens, mirroring the variance-decaying property of PCA [46]. From classifier-free guidance [21] perspective, each token zk can be viewed as conditional signal, and applying effectively provides an unconditional pathway for later tokens. In Appendix A, we formally show that this procedure yields PCA-like structure in which the earliest tokens capture the largest share of variance. high-level illustration of our overall architecture is provided in Fig. 3. 4.4. Auto-regressive Modeling with SEMANTICIST token sequences Xconcept = With the learned latent {z1, z2, . . . , zK} obtained from well-trained encoder, we can train an auto-regressive model for image generation. Specifically, we leverage the architecture of LlamaGen [48] for auto-regressive modeling, which is modern variant of GPT [37] where pre-norm [54] is applied with RMSNorm [62], and SwiGLU [45] activation function is used. As SEMANTICIST adopts continuous tokens, the prediction head of our auto-regressive model is denoising MLP following MAR [28] that is supervised by diffusion process [22]. Specifically, the auto-regressive model is modeling next token prediction problem of p(zkz<k, c) = Gdiscrete(z<k, c), where is the class label embedding and Gdiscrete being the causal transformer to predict the next token with all previous tokens. If the latent tokens generated by was quantized, we can directly leverage the softmax prediction head to obtain this next token prediction [48]. However, generates continuous tokens. Thus we leverage the design from [28] to instead predict condition mk from all previous tokens z<k and c, mk = G(z<k, c). This condition mk is used to condition small diffusion MLP model to generate the k-th token zk from noise zT . Specifically, the auto-regressive model and the diffusion MLP model is trained with similar diffusion loss as defined in Sec. 3: LG = k,ϵϵ (zt z0 k, G(z<k, c), t)2 2 , Method MaskBit [55] RCG (cond.) [27] MAR [28] TiTok-S-128 [60] TiTok-L-32 [60] VQGAN [13] ViT-VQGAN [57] RQ-VAE [26] VAR [49] ImageFolder [29] LlamaGen [48] CRT [39] Causal MAR [28] SEMANTICIST w/ DiT-L SEMANTICIST w/ DiT-XL #Token Dim. VQ rFID 1.61 1.22 1.71 2.21 7.94 1.28 3.20 0.90 0.80 2.19 2.36 1.22 0.78 0.72 256 1 256 128 32 256 1024 256 680 286 256 256 256 256 256 12 256 16 16 8 16 32 256 32 32 8 8 16 16 PSNR 20.79 21.61 21.43 SSIM 0.675 0.626 0.613 Gen. Model MaskBit MAGE-L MAR-L MaskGIT-L MaskGIT-L Tam. Trans. VIM-L RQ-Trans. VAR-d16 VAR-d16 LlamaGen-L LlamaGen-L MAR-L ϵLlamaGen-L ϵLlamaGen-L Type Mask. Mask. Mask. Mask. Mask. AR AR AR VAR VAR AR AR AR AR AR #Token 256 1 256 128 32 256 1024 256 680 286 256 256 256 32 #Step 256 20 64 64 8 256 1024 64 10 10 256 256 256 32 32 gFID 1.52 3.49 1.78 1.97 2.77 5.20 4.17 3.80 3.30 2.60 3.80 2.75 4.07 2.57 2.57 IS 328.6 215.5 296.0 281.8 194.0 280.3 175.1 323.7 274.4 295.0 248.3 265.2 232.4 260.9 254.0 Table 1. Reconstruction and generation performance on ImageNet. Dim. denotes the dimension of the tokens, and #Step denotes the number of steps needed for generating the complete image. #Token stands for the number of tokens used for image reconstruction (left) and generation (right), respectively. where z0 is the ground truth next token zk. This continuous autoregressive modeling design enables the usage of the continuous tokens from SEMANTICIST to perform generative autoregressive modeling with comparable performance to the current state-of-the-art generative models. As this model uses the noise ϵ as learning objective, we term this generative model as ϵLlamaGen. 5. Experiments Following the common practice [28], we experiment on ImageNet [8] at resolution of 256256, and report FID [19] and IS [44] tested with the evaluation suite provided by [9]. 5.1. Implementation Details SEMANTICIST autoencoder. The encoder of SEMANTICIST is standard ViT-B/16 [11], except for additional concept tokens and causal attention masks applied to them. We fix the size (token count dimension) of concept tokens representing an image as 4096 and considered four variants with token dimensions ranging from 16 to 256. Unless otherwise specified, we use 16-dimensional concept tokens (denoted as d16256) by default, which are more friendly for reconstruction as shown in Fig. 14 in the Appendix. Before being fed to the decoder, the concept tokens are normalized by their own mean and variance following [27]. The decoder is DiT [35] with patch size of 2. We experiment across different scales (B, L, and XL) and take DiT-L as default. The decoder operates on the latent space of publicly available KL-16 VAE provided by [28] to reduce computation cost. The VAE is frozen along training, and both the encoder and decoder are trained from scratch. To enforce the quality of the learned concept tokens and stabilize training, we apply REPA [61] with DINOv2-B [34] as regularizer to the 8th layer of the DiT decoder. Auto-regressive image modeling. We validate the effectiveness of SEMANTICIST autoencoder by training autoregressive image generation models using LlamaGen [48] combined with diffusion loss [28]. The input sequence is prepended with [CLS] token for class conditioning, which is randomly dropped out with probability 0.1 during training for classifier-free guidance. At inference time, we use CFG schedule following [5, 28], and do not apply temperature sampling. Note that the implementation can be general. While our preliminary validation demonstrates promising results, we anticipate better configurations in future work. 5.2. Reconstruction and Generation Quality Tab. 1 presents the comparison of SEMANTICIST to state-ofthe image tokenizers and accompanying generative models. The comparisons on the image reconstructions are made with the variant of the state-of-the-art models with similar sized latent space (i.e., token count token dimensionthe number of floating point numbers used). SEMANTICIST demonstrates superior reconstruction performance in terms of the rFID score compared to all previous works. Advancing the state-of-the-art performance in image reconstruction by 10% in rFID score compared to the next best model of ImageFolder [29] with more compact latent space (28632 for [29] and 25616 for ours). In terms of generative modeling, the results in Tab. 1 demonstrate that SEMANTICIST can obtain gFID score comparable to the state-of-the-art tokenizers for standard auto-regressive (AR) modeling. Remarkably, the unique PCA-like structure within the latent space of SEMANTICIST enables efficient generative modeling with fewer tokens. Specifically, ϵLlamaGen only requires to be trained and evaluated on the first 32 tokens from SEMANTICIST to achieve gFID score comparable with the state-of-theart generative models on ImageNet. This efficiency in the 6 Figure 4. The explained variance ratio from SEMANTICISTs PCAlike structure and the linear probing accuracy on the tokens. number of tokens used for generative modeling would allow ϵLlamaGen to use much smaller number of inference steps to achieve better results. 5.3. Representation and Structure We study the property of the structured latent space of SEMANTICIST. In Fig. 4, we first generate PCA-like variance explained plot by varying the number of tokens used to compute the averaged diffusion loss over the validation set and all diffusion timesteps. After obtaining the averaged loss, the loss value of using all null conditions is treated as an upper bound and the loss value of using all conditions is treated as the lower bound, and we plot the reduction percentage for the contribution of each token to the diffusion loss. It is very clear that SEMANTICIST forms PCA-like structure among the contribution of its tokens. Furthermore, we perform linear probing experiments on the extracted tokens, the plots are also available in Fig. 4. Comparing this against the variance-explained curve, we can see the linear probing accuracy can reach the highest performance when using low number of tokens and then gradually decrease as more tokens are used. This reveals that SEMANTICIST tends to store the most salient features (i.e., the object category) in the first few tokens, and thus, they benefit from linear probing. And yet, when adding more tokens, details of the scene are added, which causes the linear probing readout to lose category information. 5.4. Semantic-spectrum Coupling We further demonstrate the semantic-spectrum coupling effect in Fig. 5. The power-frequency plots are obtained by taking the 2D Fourier transform of the image, then averaging the magnitude-squared values over concentric circles in the frequency domain to get 1D power distribution. The horizontal axis represents spatial frequency (higher values correspond to finer details), and the vertical axis shows the Figure 5. Reconstructed images and their corresponding powerfrequency plots, illustrating semantic-spectrum coupling. Each column shows reconstructions using only the first tokens, increasing from left to right, alongside plot of the reconstructed images frequency power (blue) overlaid on the ground-truth (red) image. Figure 6. Scaling behavior of different sized DiT decoder (qualitative results can be found in Fig. 14 in the Appendix). power at each frequency. As we can see from Fig. 5, for the TiTok [60] model, the semantic content only emerges from using 2/4 to 3/4 of all tokens. From the power-frequency plot, it is clear that the model can not match the correct power distribution with fewer tokens and that when adding more tokens, not only is the power distribution shifting toward the ground truth distribution, but also, the semantic content emerges. This is what we term the semantic-spectrum coupling effect, where when adding more tokens, both semantic content and spectral information are encoded. On the other hand, it is very clear that SEMANTICIST is able to match the power distribution of the ground truth with only 1/4 of the token, and later tokens contribute more to the semantic content of the image, successfully disentangling semantic Figure 7. Examples of the intermediate generation results of ϵLlamaGen-L trained on SEMANTICIST tokens (see more from Fig. 18). content from the spectral information. 5.5. Ablation Study We ablate the effect of the scale of the diffusion decoder in SEMANTICIST, the number of tokens used for reconstruction, and the strength of classifier-free-guidance in Fig. 6. SEMANTICIST follows very clear scaling behavior in terms of the number of tokens and the model size. Notably, strengthening the classifier-free-guidance scale can greatly help the reconstruction performance with smaller model and fewer number of tokens. More ablation studies on ϵLlamaGen and SEMANTICIST are available in Appendix D.4. 5.6. Discussion on Qualitative Results In Fig. 1, we have shown that SEMANTICIST can consistently produce semantic-meaningful high-quality images with any number of tokens that progressively refine towards the reconstruction target. Notably, the majority of the original image can be represented with as few as 32 tokens. This is especially helpful when we want to re-purpose the tokens for class-conditional image generation, in which the task is to produce semantic-consistent images instead of exact reconstruction. Thus, we can train the ϵLlamaGen autoregressive model efficiently with only the first few concept tokens (32 in our case). In Figs. 7 and 18, we demonstrate the effectiveness of auto-regressive modeling following this strategy. The model is ϵLlamaGen-L trained on the first 32 concept tokens of SEMANTICIST (w/ DiT-XL) tokenizer. It is encouraging to see that the first token the model generates already sketches the majority of the scene well and even generates highly faithful images in easier cases like animals. When more tokens are generated, the image is gradually refined and converges to highly detailed and visually coherent generation. 6. Conclusion We introduce SEMANTICIST, PCA-like structured 1D tokenizer that addresses semantic-spectrum coupling through dynamic nested classifier-free guidance strategy and diffusion-based decoder. Our method enforces coarse-tofine token hierarchy that captures essential semantic features while maintaining compact latent representation. Our experiments demonstrate that SEMANTICIST achieves state-ofthe-art reconstruction FID scores on the ImageNet validation set, surpassing the previous SOTA tokenizer by almost 10% in FID. Moreover, the auto-regressive model ϵLlamaGen trained on SEMANTICISTs tokens attains comparable per8 formance to current SOTA methods while requiring only 32 tokens for training and inference. Additionally, linear probing in the latent space yields up to 63.5% top-1 classification accuracy, confirming the effectiveness of our approach in capturing semantic information. These results highlight the promise of SEMANTICIST for high-fidelity image reconstruction and generative modeling, paving the way for more efficient and compact visual representations."
        },
        {
            "title": "Limitations and Broader Impacts",
            "content": "Our tokenizer contributes to structured visual representation learning, which may benefit image compression, retrieval, and generation. However, like other generative models, it could also be misused for deepfake creation, misinformation, or automated content manipulation. Ensuring responsible use and implementing safeguards remains an important consideration for future research. SEMANTICIST also presents several limitations, for example, we employ diffusionbased decoder, but alternative generative models like flow matching or consistency models could potentially improve efficiency. Additionally, our framework enforces PCA-like structure, further refinements, such as adaptive tokenization or hierarchical models, could enhance flexibility."
        },
        {
            "title": "Acknowledgement",
            "content": "This work has been supported by Hong Kong Research Grant CouncilEarly Career Scheme (Grant No. 27209621), General Research Fund Scheme (Grant No. 17202422), and RGC Research Matching Grant Scheme (RMGS). We sincerely appreciate the dedicated support we received from the participants of the human study. We are also grateful to Anlin Zheng and Haochen Wang for helpful suggestions on the design of technical details."
        },
        {
            "title": "Author Contribution Statement",
            "content": "X.W. and B.Z. conceived the study and guided its overall direction and planning. X.W. proposed the original idea of semantically meaningful decomposition for image tokenization. B.Z. developed the theoretical framework for nested CFG and the semantic spectrum coupling effect and conducted the initial feasibility experiments. X.W. further refined the model architecture and scaled the study to ImageNet. B.Z. led the initial draft writing, while X.W. designed the figures and plots. I.E., J.D., and X.Q. provided valuable feedback on the manuscript. All authors contributed critical feedback, shaping the research, analysis, and final manuscript."
        },
        {
            "title": "References",
            "content": "[1] Roman Bachmann, Jesse Allardice, David Mizrahi, Enrico Fini, Oguzhan Fatih Kar, Elmira Amirloo, Alaaeldin ElNouby, Amir Zamir, and Afshin Dehghan. FlexTok: Resampling images into 1d token sequences of flexible length. arXiv:2502.13967, 2025. 2 [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5VL technical report. arXiv:2502.13923, 2025. 3 [3] Jerome S. Bruner and Mary C. Potter. Interference in visual recognition. Science, 144(3617):424425, 1964. 2 [4] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. MaskGIT: Masked generative image transformer. In CVPR, 2022. [5] Huiwen Chang, Han Zhang, Jarred Barber, Aaron Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Patrick Murphy, William T. Freeman, Michael Rubinstein, Yuanzhen Li, and Dilip Krishnan. Muse: Text-to-image generation via masked generative Transformers. In ICML, 2023. 6, 3 [6] Hao Chen, Yujin Han, Fangyi Chen, Xiang Li, Yidong Wang, Jindong Wang, Ze Wang, Zicheng Liu, Difan Zou, and Bhiksha Raj. Masked autoencoders are effective tokenizers for diffusion models. arXiv:2502.03444, 2025. 3 [7] Yinbo Chen, Rohit Girdhar, Xiaolong Wang, Sai Saketh Rambhatla, and Ishan Misra. Diffusion autoencoders are scalable image tokenizers. arXiv:2501.18593, 2025. 2 [8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: large-scale hierarchical image database. In CVPR, 2009. 6, 2 [9] Prafulla Dhariwal and Alex Nichol. Diffusion models beat GANs on image synthesis. In NeurIPS, 2021. 3, 6 [10] Sander Dieleman. Diffusion is spectral autoregression, 2024. 3, [11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 3, 4, 6, 2 [12] Shivam Duggal, Phillip Isola, Antonio Torralba, and William T. Freeman. Adaptive length image tokenization via recurrent allocation. In ICLR, 2025. 2, 3 [13] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming Transformers for high-resolution image synthesis. In CVPR, 2021. 3, 4, 6 [14] Li Fei-Fei, Asha Iyer, Christof Koch, and Pietro Perona. What do we perceive in glance of real-world scene? Journal of Vision, 7(1):1010, 2007. 3, 2 [15] Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan. Planting seed of vision in large language model. arXiv:2307.08041, 2023. 3 [16] Yuying Ge, Yizhuo Li, Yixiao Ge, and Ying Shan. Divot: Diffusion powers video tokenizer for comprehension and generation. arXiv:2412.04432, 2024. [17] Philippe Hansen-Estruch, David Yan, Ching-Yao Chung, Orr Zohar, Jialiang Wang, Tingbo Hou, Tao Xu, Sriram Vishwanath, Peter Vajda, and Xinlei Chen. Learnings from 9 scaling visual tokenizers for reconstruction and generation. arXiv:2501.09755, 2025. 3 [18] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2022. 3 [19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by two time-scale update rule converge to local Nash equilibrium. In NeurIPS, 2017. 3, 6 [20] Geoffrey E. Hinton and Ruslan R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313 (5786):504507, 2006. 1, [21] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv:2207.12598, 2022. 5 [22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 3, 4, 5 [23] Matthew Ho, Xiaosheng Zhao, and Benjamin Wandelt. Information-ordered bottlenecks for adaptive semantic compression. arXiv:2305.11213, 2023. 5 [24] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014. 2, 3 [25] Aditya Kusupati, Gantavya Bhatt, Aniket Rege, Matthew Wallingford, Aditya Sinha, Vivek Ramanujan, William Howard-Snyder, Kaifeng Chen, Sham Kakade, Prateek Jain, and Ali Farhadi. Matryoshka representation learning. In NeurIPS, 2022. 5 [26] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In CVPR, 2022. 3, 6 [27] Tianhong Li, Dina Katabi, and Kaiming He. Return of unconditional generation: self-supervised representation generation method. In NeurIPS, 2024. 6, 2 [28] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. In NeurIPS, 2024. 4, 5, 6, 3 [29] Xiang Li, Kai Qiu, Hao Chen, Jason Kuen, Jiuxiang Gu, Imagefolder: Autoregressive Bhiksha Raj, and Zhe Lin. image generation with folded tokens. In ICLR, 2025. 6 [30] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. 2 [31] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. SiT: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In ECCV, 2024. 3 [32] David Navon. Forest before trees: The precedence of global features in visual perception. Cognitive Psychology, 9(3): 353383, 1977. 3, 2 [33] Aude Oliva and Philippe G. Schyns. Diagnostic colors mediate scene recognition. Cognitive Psychology, 41(2):176210, 2000. 2 [34] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. TMLR, 2024. 3, 6 [35] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. 3, 5, 6, 2 [36] Konpat Preechakul, Nattanat Chatthee, Suttisak Wizadwongsa, and Supasorn Suwajanakorn. Diffusion autoencoders: Toward meaningful and decodable representation. In CVPR, 2022. 5, 2 [37] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training, 2018. 5 [38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. 3, [39] Vivek Ramanujan, Kushal Tirumala, Armen Aghajanyan, Luke Zettlemoyer, and Ali Farhadi. When worse is better: Navigating the compression-generation tradeoff in visual tokenization. arXiv:2412.16326, 2024. 3, 6 [40] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with CLIP latents. arXiv:2204.06125, 2022. 3 [41] Oren Rippel, Michael Gelbart, and Ryan Adams. Learning ordered representations with nested dropout. In ICML, 2014. 5 [42] Severi Rissanen, Markus Heinonen, and Arno Solin. Generative modelling with inverse heat dissipation. arXiv:2206.13397, 2022. 3, 4, 5 [43] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 3, 4, 5 [44] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training GANs. In NeurIPS, 2016. [45] Noam Shazeer. GLU variants improve Transformer. arXiv:2002.05202, 2020. 5 [46] Jonathon Shlens. tutorial on principal component analysis. arXiv:1404.1100, 2014. 1, 5 [47] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. 3 [48] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv:2406.06525, 2024. 2, 3, 4, 5, 6 [49] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. In NeurIPS, 2024. 3, 4, 6 [50] Matthew A. Turk and Alex P. Pentland. Face recognition using eigenfaces. In CVPR, 1991. 1 [51] Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In ICML, 2016. 10 [52] Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Koray Kavukcuoglu. Conditional image generation with PixelCNN decoders. In NeurIPS, 2016. 4 [53] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In NeurIPS, 2017. 1, 2, 3 [54] Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao. Learning deep Transformer models for machine translation. In ACL, 2019. 5 [55] Mark Weber, Lijun Yu, Qihang Yu, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. MaskBit: Embedding-free image generation via bit tokens. TMLR, 2024. 3, 6 [56] Jingfeng Yao and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. arXiv:2501.01423, 2025. 3 [57] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved VQGAN. In ICLR, 2022. [58] Lijun Yu, Jose Lezama, Nitesh B. Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, Alexander G. Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa, David A. Ross, and Lu Jiang. Language model beats diffusion tokenizer is key to visual generation. In ICLR, 2024. 3 [59] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and LiangChieh Chen. Randomized autoregressive visual generation. arXiv:2411.00776, 2024. 4 [60] Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is worth In NeurIPS, 32 tokens for reconstruction and generation. 2024. 1, 2, 3, 5, 6, 7, 4 [61] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. In ICLR, 2025. 6, 4, 5 [62] Biao Zhang and Rico Sennrich. Root mean square layer normalization. In NeurIPS, 2019. [63] Long Zhao, Sanghyun Woo, Ziyu Wan, Yandong Li, Han Zhang, Boqing Gong, Hartwig Adam, Xuhui Jia, and Ting Liu. ϵ-VAE: Denoising as visual decoding. arXiv:2410.04081, 2024. 2 [64] Lei Zhu, Fangyun Wei, Yanye Lu, and Dong Chen. Scaling the codebook size of VQ-GAN to 100,000 with utilization rate of 99%. In NeurIPS, 2024. 3 11 Principal Components Enable New Language of Images"
        },
        {
            "title": "Contents",
            "content": "A. Proof for PCA-like structure B. Additional Related Work . . . . B.1. Concurrent Related Work . . B.2. Related Work on Human Perception . . . . . B.3. Related Work on Diffusion-Based Tokenizers . . . . C. Additional Implementation Details . C.1. Semanticist Autoencoder . . . C.2. Auto-regressive Modeling . . . . . . . . . . . . . . . . . D. Additional Experiment Results D.1. Human Perception Test . D.2. Zero-Shot CLIP on Reconstructed Images D.3. Semantic Spectrum Coupling Effect Results . . . . D.4. Additional Ablation Study . . . . . . D.5. Qualitative Results . . . . . . . . . . . . . . . . . . . . . . . . . 2 2 2 2 2 2 3 3 3 4 4 4 5 A. Proof for PCA-like structure The conditional denoising diffusion model is using neural network ϵθ(xt, z, t) to approximated the score function xt ln q(xtx0) which guides the transition from noised image xt to the clean image x0. For the conditional diffusion decoder in SEMANTICIST, the score function can be decomposed as: ϵθ(xt, z1, . . . , zk) = ϵθ(xt, ) + (cid:88) i=1 γiϵθ(xt, zi) , where is the null condition, γi is the guidance scale, and ϵθ(xt, zi) = ϵθ(xt, z1, . . . , zi) ϵθ(xt, z1, . . . , zi1) represents the increment contribution of the concept token condition zi to the score function. Thus, we can rewrite the diffusion training objective with conditions with the following: Lk = (cid:20)(cid:13) (cid:13)ϵ (cid:0)ϵθ(xt, ) + (cid:13) γiϵθ(xt, zi)(cid:1)(cid:13) (cid:13) (cid:13) 2(cid:21) . (cid:88) i=1 the residual term ϵ Since model is at convergence, ϵθ(xt, ) (cid:80)k j=1 γjϵθ(xt, zj) can not be further reduced by making further changes to the adjustment from the i-th concept token ϵθ(xt, zj). In other words, the residual term and all active conditions ϵθ(xt, zj) are orthogonal to each other. Next, we can use induction to prove that at convergence, all ϵθ(xt, zj) terms are orthogonal to each other similar to PCA. For the case of = 1, we only use one concept token to condition the model, thus we can have: [(ϵ ϵθ(xt, ) γ1ϵθ(xt, z1)) ϵθ(xt, z1)] = 0 . For the case of = 2, for (i = 1, 2), we have: (cid:20)(cid:16) ϵ ϵθ(xt, ) 2 (cid:88) j=1 γjϵθ(xt, zj) (cid:17) (cid:21) ϵθ(xt, zi) = 0 . By substituting the = 1 case into this, it can be seen that (cid:2)ϵθ(xt, z1)ϵθ(xt, z2)(cid:3) = 0. Assuming this orthogonality holds for the first 1 concept tokens: (cid:2)ϵθ(xt, zi)ϵθ(xt, zj)(cid:3) = 0 i, < k, = j. Then for < k, by substituting ϵ ϵθ(xt, ) = k1 (cid:88) j=1 we can have: γjϵθ(xt, zj) + γkϵθ(xt, zk) , (cid:2)ϵθ(xt, zi)ϵθ(xt, zk)(cid:3) = 0 . Thus, the orthogonality propagates to all pairs (zi, zk) for < k. By induction, we have orthogonality between all pairs of concept tokens. Variance Explained Hierarchy. Assuming the true noise ϵ can be reconstructed using the conditional model, we have: ϵ ϵθ(xt, ) + (cid:88) i= γiϵθ(xt, zi) + residual . Given the orthogonality of ϵθ(xt, zi) we have proven earlier, the total variance can be decomposed as: Orthogonality between contribution of concept tokens. the gradient of Lk w.r.t At the optimal convergence, ϵθ(xt, zi) is zero, thus give us: Var(ϵ) = (cid:88) i= Var(γiϵ(xt, zi)) + Var(residual) . Lk ϵθ(xt, zi) = (cid:104)(cid:0)ϵ ϵθ(xt, ) (cid:88) j= γjϵθ(xt, zj)(cid:1)γi (cid:105) Let λi = Var(γiϵθ(xt, zi)), representing the variance explained by concept token condition zi. Our dropout design would have the training objective forces: = 0 . λ1 λ2 λk , as each concept token zi is trained to explain the maximal residual variance after accounting for concept tokens z1, . . . , zi1. Thus, combining the orthogonality and the variance decay, SEMANTICIST provably grounds the emergence of PCAlike hierarchical structure in the learned concept tokens. Providing simple, effective, and explainable architecture for visual tokenization. B. Additional Related Work B.1. Concurrent Related Work Concurrent work [1] introduces 1D tokenizer that focuses on adaptive-length tokenization by resampling sequences of 1D tokens from pre-trained 2D VAE tokens. In contrast, our approach is motivated by fundamentally different objective reintroducing PCA-like structure into visual tokenization to enforce structured, hierarchical latent representation. Furthermore, our tokenizer is continuous rather than discrete, setting it apart from [1] and allowing it to better capture the variance-decaying properties inherent to PCA. Additionally, we identify and resolve the semantic-spectrum coupling effect, key limitation in existing visual tokenization methods that has not been previously addressed. B.2. Related Work on Human Perception Human perception of visual stimuli has been shown to follow the global precedence effect [32], where the global information of the scene is processed before the local information. In [14], controlled experiments of presentation time on human perception of visual scene have further confirmed with the global precedence effect, where less information (presentation time) is needed to access the non-semantic, sensory-related information of the scene compared to the semantically meaningful, objector scene-related information. Similar results have been reported in [3], where sensory attributes are more likely to be processed when the scene is blurred. Moreover, [33] have suggested that reliable structural information can be quickly extracted based on coarse spatial scale information. These results suggest that human perception of visual stimuli is hierarchical, where the global information of the scene is processed before the local information. As we have shown in the main paper, SEMANTICIST can naturally emerge similar hierarchical structure in the token sequence, where the first few tokens encode the global information of the scene and the following tokens encode the local information of the scene. This hierarchical structure is provable to be PCA-like, which is similar to how human perception of visual stimuli is hierarchical. B.3. Related Work on Diffusion-Based Tokenizers The usage of diffusion-based decoder has been explored by several works [7, 16, 63]. [63] proposed the usage of diffusion-based decoder as paradigm shift from single-step reconstruction of previous tokenizers to the diffusion-based iterative refinement process. [7] further scales this idea on more modern DiT [35] architecture and describes the scaling law for such diffusion-based tokenizer. [16] applied this idea to video tokenizer, enabling better reconstruction and understanding of video content. However, these previous works overlook the benefit of the diffusion-based decoder in that it can disentangle the semantic content from the spectral information. Additionally, these works still apply the 2D grid-based structure for encoding the image without considering the latent structure of the token space. C. Additional Implementation Details C.1. Semanticist Autoencoder Model architecture. As shown in Fig. 3, the SEMANTICIST tokenizer follows the diffusion autoencoder [27, 36] paradigm: visual encoder takes RGB images as input and encodes them into latent embeddings to condition diffusion model for reconstruction. In our case, the visual encoder is ViT-B/16 [11] with sequence of concept tokens concatenated with image patches as input. The concept tokens have full attention with patch tokens, but are causal to each other. Before being fed to the decoder, the concept tokens also go through linear projector, and are then normalized by their mean and variance. To stabilize training, we also apply drop path with probability of 0.1 to the ViT. For the DiT decoder, we concatenate the patch tokens (condition) with noisy patches as input, and the timesteps are still incorporated via AdaLN following common practice [35]. Nested classifier-free guidance (CFG). For the DiT decoder, we randomly initialize (number of concept tokens) learnable null-conditioning tokens. During each training iteration, we uniformly sample concept token index k, and corresponding null tokens replace all tokens with larger indices. To facilitate the learning of the encoder, we do not enable nested CFG in the first 50 training epochs. During inference, CFG can be applied to concept tokens independently following the standard practice [35]. Specifically, Training. We follow [27] for training details of the tokenizer. is trained using the the model AdamW [30] optimizer on ImageNet [8] for 400 epochs with batch size of 2048. The base learning rate is 2.5e-5, which is scaled by lr = lrbasebatch size/256. The learning rate is also warmed up linearly during the first 100 epochs, and then gradually decayed following the cosine schedule. No weight decay is applied, and β1 and β2 of AdamW are set to 0.9 and 0.95. During training, the image is resized so that the smaller side is of length 256, and then randomly flipped and cropped to 256256. We also apply gradient clipping 2 of 3.0 to stabilize training. The parameters of the model are maintained using exponential moving average (EMA) with momentum of 0.999. Inference. Because of the nature of the PCA structure, it is possible to obtain reasonable reconstruction results with only the first few concept tokens. In implementation, we achieve this by padding missing tokens with their corresponding null conditioning tokens and then feeding the full sequence to the DiT decoder. C.2. Auto-regressive Modeling Model architecture. The ϵLlamaGen roughly follows the LlamaGen architecture with the only change of using diffusion MLP as the prediction head instead of softmax head. To perform the classifier-free-guidance, we use one [CLS] token to guide the generation process of ϵLlamaGen. As certain configurations of SEMANTICIST can yield highdimensional tokens, we made few adjustments to the model architecture of ϵLlamaGen to allow it to learn with highdimensional tokens. Specifically, we use 12-layer MLP with each layer having 1536 hidden neurons as the prediction head and use the stochastic interpolant formulation [31] to train the diffusion MLP. The classifier-free guidance is also slightly modified: we concatenate the [CLS] token with the input to the diffusion MLP along the feature axis and then project back to the original feature dimension to feed into the diffusion MLP. These changes allow us to train auto-regressive models on high-dimensional (e.g., 256dimensional) tokens with improved stability compared to the original version proposed in [28]. However, we expect future research to drastically simplify this model architecture. Training. The ϵLlamaGen is trained for 400 epochs with cached latents generated by pretrained SEMANTICIST on the ImageNet dataset with TenCrop and random horizontal flipping augmentations. We use batch size of 2048, and apply 100-epoch warmup for the base learning rate of 1e-4, which is scaled similarly as the SEMANTICIST w.r.t. the batch size. After warmup, the learning rate is fixed. Weight decay of 0.05 and gradient clipping of 1.0 are applied. In our experiments, we find that later concept tokens have diminishing returns or are even harmful for ϵLlamaGen, thus only train ϵLlamaGen with the first few tokens. Specifically, the ϵLlamaGen-L model is trained with 32 concept tokens. Inference. In the inference stage, we use the same linear classifier-free guidance schedule as MAR [28] and MUSE [5]. The schedule tunes down the guidance scale of small-indexed tokens to improve the diversity of generated samples, thus being more friendly for gFID. When reporting gFID, we disable CFG for SEMANTICISTs DiT Figure 8. The preference score from the human perception test, all models and test configurations obtained score close to 0.5, indicating SEMANTICIST can encode image as effectively as how human language encodes the image. decoder, tune the guidance scale of the autoregressive model, and report the best performance. D. Additional Experiment Results D.1. Human Perception Test We are interested in understanding whether the tokens learned by SEMANTICIST follow human-like perception effect, namely the global precedence effect [32] where the global shape and semantics are picked up within very short period of exposure. Thus we designed human perception test to evaluate whether SEMANTICIST generates tokens that closely follow human perception. Specifically, we generate images by only reconstructing from the first two tokens from SEMANTICIST. Distractor images are also generated by first captioning the image with Qwen2.5VL [2] and then generate the image with stable-diffusion model [43]. Following the setup of [14], we only reveal the generated images and the distractors by very short reveal time, and then ask the participants to choose which images more closely align with the original image. For evaluation, we give the participants preference to distractor image zero points, the preference to the generated image one point, and in the case of tie, we give 0.5 points. Fig. 8 presents the averaged preference score with different token dimensions and reveal time. SEMANTICIST is able to obtain score close to 0.5 under all cases, indicating that SEMANTICIST can encode the images global semantic content close to how state-of-the-art vision language models [2] encode the image in language space. web-based human perception test interface is provided along with this appendix. 3 Figure 9. CLIP zero-shot accuracy on reconstructed images. D.2. Zero-Shot CLIP on Reconstructed Images We also study the property of the SEMANTICIST latent space by reconstructing from it. Fig. 9 demonstrate the zero-shot accuracy of pretrained CLIP [38] model on the imagenet validation set reconstructed by SEMANTICIST. For all model variants, the zero-shot performance improves with the number of tokens, with models using more dimensions per token achieving better performance with smaller number of tokens, indicating that with more dimensions, SEMANTICIST is able to learn the semantic content with less tokens. Fig. 6 provides the rFID score on the imagenet validation set with varying number of tokens, similar conclusions can be drawn. Additionally, Fig. 6 also provides the scaling behavior of SEMANTICIST, we can observe that SEMANTICIST not only enjoys structured latent space, but also demonstrates promising scaling. D.3. Semantic Spectrum Coupling Effect Results In Fig. 10, we present the power frequency plot of performing PCA to decompose the latent token space of TiTok [60]. Similar effect as the PCA decomposition on VQ-VAE [48] and first token decomposition on TiTok [60] are observed. This result further demonstrates that the latent space of TiTok [60] entangles the semantic contents and the spectral information. D.4. Additional Ablation Study In Fig. 11, we show the results of SEMANTICIST with d6464 tokens trained with or without REPA [61] evaluated by reconstruction FID on ImageNet 50K validation set. Despite the performance with full tokens being similar, adding REPA significantly improves the contribution of each (especially the first few) tokens. This naturally fits our need of PCA-like structure and is thus adopted as default. We also compared reconstruction performance of different concept token dimensions. We fix the product beFigure 10. Frequency-power spectra of Titok decomposed with PCA at feature dimensions. The learning of semantic contents and spectral information are coupled. Figure 11. Ablation on the use of REPA (with d6464 concept tokens, DiT-L/2 decoder, see qualitative results in Fig. 16). REPA improves the information density in preceding tokens. tween the number of tokens and the dimension per token to be 4096, and investigate 256-dimensional (d25616), 128-dimensional (d12832), 64-dimensional (d6464), and 16-dimensional (d16256) tokens. As shown in Fig. 12, all configurations can learn ordered representations, with higher-dimensional ones containing more information per token. However, lower-dimensional tokens are more friendly for reconstruction tasks as they achieve better rFID. 4 Figure 12. Reconstruction performance of different encoder configurations on ImageNet val 50K benchmark. larger number of lowerdimensional tokens is more friendly for reconstruction tasks. Figure 13. Qualitative results of different token dimensions. Higher dimensional tokens encode more information, and lower dimensional tokens achieve clearer semantic decoupling and achieve better reconstruction. Fig. 16 presents qualitative results with or without the usage of REPA [61]. It is clear that the usage of REPA did not visually improve the final reconstruction by much, yet with fewer tokens, the model with REPA demonstrates more faithful semantic details with the original image. Fig. 17 demonstrates the reconstruction results of more randomly sampled images, and Fig. 18 illustrates more intermediate results of auto-regressive image generation. D.5. Qualitative Results In Fig. 13, reconstruction results from using different numbers of token dimensions are presented. As the dimension for one token becomes large, more semantic content can be encoded into it, thus allowing SEMANTICIST to generate faithful reconstructions of the original image. In Fig. 14, the reconstructed results for different scaled DiT decoders are presented. These models are trained with the same dimension for the tokens that are 16-dimensional. We can see that as the model scales up, the reconstructed images with fewer tokens become more and more realistic and appealing. Fig. 15 shows the reconstruction of the same SEMANTICIST tokenizer with different CFG guidance scales at inference time (CFG=1.0 indicates not applying CFG). It can be seen that the guidance scale has very strong correlation with the aesthetics of generated images. 5 Figure 14. Qualitative results of different DiT decoder scales (DiT-B/2, DiT-L/2, and DiT-XL/2) with d16256 tokens. The quality of images generated with fewer tokens improves consistently as the decoder scales up. Figure 15. Qualitative results of different CFG guidance scales for DiT decoder, which clearly controls image aesthetics. 6 Figure 16. Qualitative results on effects of REPA (with d6464 concept tokens). Instead of improving final reconstruction much, the benefit of REPA is mainly attributed to more faithful semantics in intermediate results. 7 Figure 17. More reconstruction results of SEMANTICIST autoencoder (with d16256 concepts tokens and DiT-XL/2 decoder). 8 Figure 18. More visualization of intermediate results of auto-regressive image generation."
        }
    ],
    "affiliations": [
        "Imperial College London",
        "Noahs Ark Lab",
        "University of Edinburgh",
        "University of Hong Kong"
    ]
}