{
    "paper_title": "DCPO: Dynamic Clipping Policy Optimization",
    "authors": [
        "Shihui Yang",
        "Chengfeng Dou",
        "Peidong Guo",
        "Kai Lu",
        "Qiang Ju",
        "Fei Deng",
        "Rihui Xin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a promising framework for enhancing the reasoning capabilities of large language models. However, existing approaches such as GRPO often suffer from zero gradients. This problem arises primarily due to fixed clipping bounds for token-level probability ratios and the standardization of identical rewards, which can lead to ineffective gradient updates and underutilization of generated responses. In this work, we propose Dynamic Clipping Policy Optimization (DCPO), which introduces a dynamic clipping strategy that adaptively adjusts the clipping bounds based on token-specific prior probabilities to enhance token-level exploration, and a smooth advantage standardization technique that standardizes rewards across cumulative training steps to improve the response-level effective utilization of generated responses. DCPO achieved state-of-the-art performance on four benchmarks based on four different models. In particular, DCPO achieved an Avg@1 of 46.7 under greedy decoding and an Avg@32 of 38.8 under 32 times sampling on the AIME24 benchmark, surpassing both DAPO (36.7/31.6) and GRPO (36.7/32.1) on the Qwen2.5-Math-7B model. On the AIME25 benchmark based on Qwen2.5-14B, DCPO achieves a performance of (23.3/19.0), surpassing GRPO (13.3/10.5) and DAPO (20.0/15.3). Furthermore, DCPO achieved an average 28% improvement in the nonzero advantage over GRPO in four models, doubled the training efficiency over DAPO, and significantly reduced the token clipping ratio by an order of magnitude compared to both GRPO and DAPO, while achieving superior performance. These results highlight DCPO's effectiveness in leveraging generated data more efficiently for reinforcement learning in large language models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 3 3 3 2 0 . 9 0 5 2 : r DCPO: Dynamic Clipping Policy Optimization DCPO: DYNAMIC CLIPPING POLICY OPTIMIZATION Shihui Yang, Chengfeng Dou, Peidong Guo, Kai Lu Qiang Ju, Fei Deng, Rihui Xin Baichuan.inc yangshihui@baichuan-inc.com https://github.com/lime-RL/DCPO"
        },
        {
            "title": "ABSTRACT",
            "content": "Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as promising framework for enhancing the reasoning capabilities of large language models. However, existing approaches such as GRPO often suffer from zero gradients. This problem arises primarily due to fixed clipping bounds for token-level probability ratios and the standardization of identical rewards, which can lead to ineffective gradient updates and underutilization of generated responses. In this work, we propose Dynamic Clipping Policy Optimization (DCPO), which introduces dynamic clipping strategy that adaptively adjusts the clipping bounds based on token-specific prior probabilities to enhance token-level exploration, and smooth advantage standardization technique that standardizes rewards across cumulative training steps to improve the response-level effective utilization of generated responses. DCPO achieved state-of-the-art performance on four benchmarks based on four different models. In particular, DCPO achieved an Avg@1 of 46.7 under greedy decoding and an Avg@32 of 38.8 under 32 times sampling on the AIME24 benchmark, surpassing both DAPO (36.7/31.6) and GRPO (36.7/32.1) on the Qwen2.5-Math-7B model. On the AIME25 benchmark based on Qwen2.5-14B, DCPO achieves performance of (23.3/19.0), surpassing GRPO (13.3/10.5) and DAPO (20.0/15.3). Furthermore, DCPO achieved an average 28% improvement in the nonzero advantage over GRPO in four models, doubled the training efficiency over DAPO, and significantly reduced the token clipping ratio by an order of magnitude compared to both GRPO and DAPO, while achieving superior performance. These results highlight DCPOs effectiveness in leveraging generated data more efficiently for reinforcement learning in large language models."
        },
        {
            "title": "INTRODUCTION",
            "content": "In recent years, the reasoning capabilities of large language models (LLMs) have garnered increasing attention (OpenAI, 2025; Guo et al., 2025; Comanici et al., 2025). Reinforcement Learning from Verifiable Rewards (RLVR) represents promising technique for enhancing reasoning capabilities. This method leverages rule-based outcome rewards to optimize LLMs, specifically enhancing the performance of LLMs in domains requiring reflective decision-making, such as mathematics and coding (DeepSeek-AI et al., 2024). To support RLVR, the GRPO (DeepSeek-AI et al., 2024; Liu et al., 2025; Yuan et al., 2025) family of algorithms was developed. Although the robust performance of GRPO has been extensively validated in numerous studies, recent research, such as DAPO (Yu et al., 2025), has identified critical limitations. Specifically, the GRPO suffers from entropy collapse, characterized by the advantage values decaying to zero during later training stages. Furthermore, inappropriate weight-clipping can discard rarely sampled learning signals, thus overly limiting the diversity of model exploration. To address these challenges, for example, DAPO introduces the Clip-Higher strategy to mitigate entropy collapse by relaxing the upper clipping bound, and the Dynamic Sampling strategy to filter out samples with uniform rewards, ensuring that every batch contains only samples with valid gradients. 1 DCPO: Dynamic Clipping Policy Optimization However, these methods have two notable limitations: (i) Response-Level Inefficiency: Dynamic sampling inherently reduces the sampling efficiency, leading to slower training convergence. (ii) token clipping deficit: Our analysis reveals that the optimal clipping bound varies across tokens, making uniform upper bound suboptimal. Although some others avoid directly discarding zero-gradient responses, these limitations remain prevalent. In this work, we introduce Dynamic Clipping Policy Optimization (DCPO), novel approach designed to address the key limitations of previous methods. First, we introduce smoothing technique that standardizes advantages across cumulative steps to address the gradient descent problem caused by zeroing out the advantages of responses with the same reward. This method aggregates the reward distribution of all cumulatively generated responses with the current generated distribution, allowing more efficient response-level exploration(see Section 2). Second, we propose dynamic clipping method that adaptively adjusts the clipping bounds based on the old tokenspecific probabilities to mitigate the drawbacks of the fixed clipping bounds, rather than setting different fixed upper and lower clipping bounds. Specifically, tokens with lower prior probabilities are granted wider clipping bounds, allowing more space for token-level exploration(see Section 2 and Appendix A.6). In addition, DCPO only computes the mean loss across tokens within each individual response instead of averaging the loss over an entire batch, which preserves the relative advantage structure among responses to the same prompt and avoids the dilution effect introduced by batch-level averaging. Finally, we merged Math-DAPO-17k (Yu et al., 2025) with levels 3-5 of the MATH dataset (Liu et al., 2025) as the training dataset and selected four common mathematical reasoning benchmarks to evaluate performance. We conducted experiments on four different models and used GRPO and DAPO as baselines. The experimental results show that DCPO consistently achieved the superior preference, with particularly large gains on the challenging AIME benchmark: on AIME24-Avg@32 with 32-time sampling, DCPO-7B scored 38.8 vs. 32.1 (GRPO) and 31.6 (DAPO), and on AIME25Avg@32, DCPO-14B reached 19.0 vs. 10.5 and 15.3. Moreover, DCPO increased the average nonzero advantage ratio by 28% over GRPO, doubled training efficiency over DAPO, and reduced token clipping by an order of magnitude compared to all baselines, while delivering superior performance. For preliminary details, including loss and advantage calculation methods for previous algorithms such as GRPO and DAPO, see the Appendix A.2."
        },
        {
            "title": "2 DYNAMIC-ADAPTIVE CLIPPING BOUNDS FOR RL(DAC)",
            "content": "Clipping the probability ratio is widely adopted technique in reinforcement learning to stabilize policy updates and prevent large, destabilizing parameter shifts during optimization. This mechanism is particularly important when applying importance sampling in off-policy settings. When importance sampling is reweighted by the ratio r(x) between the new probability p(x) and the old probability q(x), an unbiased estimate of Eq(x)[f (x) p(x) q(x) ] can be obtained, but the variance of such an estimate can be significantly inflated, as described in Equation (1). Varxq (x) Varxp [f (x)] =Exp (cid:20) (cid:21) p(x) q(x) (cid:20) (x)2( p(x) q(x) (cid:21) 1) = (cid:90) (x)2( p(x) q(x) 1)p(x)dx (1) To mitigate this variance inflation, prior works(e.g., Schulman et al. (2017)) constrain the probability ratio within fixed clipping bound by hyperparameter ϵ as Equation (2). r(x) 1 ϵ (2) However, symmetric fixed clipping bound constrains the models exploration of the diversity space, particularly in regions where the old policy assigns low probability mass. In these scenarios, the absolute bound for decreasing probabilities is inherently limited, restricting the capacity for meaningful policy updates. Although methods like DAPO attempt to address this issue with asymmetric clipping bounds, the fundamental limitation persists. From the perspective of controlling the variance-bias, more practical approach is to impose probability-dependent constraints, as formalized in Equation (3). (r(x) 1)p(x) ϵ (3) DCPO: Dynamic Clipping Policy Optimization Substituting p(x) = r(x)q(x) into Equation (3) and ensuring non-negativity of the square root term by taking its maximum with zero, we obtain closed-form solution for r(x). This yields dynamicadaptive clipping bounds that vary adaptively with q(x) and hyperparameters ϵlow and ϵhigh, as specified in Equation (4). 0.5 + (cid:115) 1 2 (cid:18) max 1 4ϵlow (x) (cid:19) , 0 (x) 0.5 + (cid:115) 1 + 1 2 4ϵhigh (x) (4) This dynamic-adaptive clipping method allows for greater policy exploration in low-probability regions. Furthermore, inspired by the dual clipping method (Ye et al., 2020), we set the maximum bounds for both positive and negative advantages to 10 to prevent excessive clipping bounds from overtraining. For detailed derivation and analysis of these dynamic clipping bounds from the importance sampling framework, please see Appendix A.5 and Appendix A.6."
        },
        {
            "title": "3 SMOOTH ADVANTAGE STANDARDIZATION(SAS)",
            "content": "In previous works, such as GRPO and DAPO, the advantage ˆAi new,j of the j-th response is the standardized result of only taking the rewards Ri j=1,...,G for the same prompt at the i-th step, as Equation (11) in the Appendix A.3. However, this approach can lead to several issues: First, when randomness in response sampling causes all rewards to be the same at given step, the advantage becomes zero, preventing the prompt from contributing to parameter updates despite potentially valuable differences in reasoning trajectories. Second, randomness in high-entropy sampling can yield highly skewed label counts, causing large fluctuations in standardized advantage values across steps, even reversing signs, thus destabilizing training. At the same time, recent work (Yue et al., 2025) suggests that RLVR primarily refines policy to adjust the likelihood of correct samples rather than fundamentally altering the existing distribution. Given these considerations, the overall reward distribution of responses to the same prompt can be considered as drawn from the same global distribution throughout the course of training, which motivates cumulative standardization as Equation (5). ˆAi total,j = (cid:0)Ri total µi σi total (cid:1) (5) where the statistics are computed over all responses generated so far for the same prompt. To mitigate fluctuations of both step-specific standardization ( ˆAi new,j) and cumulative standardizai tion ( ˆAi total,j), we introduce two smoothing functions, ˆSA total,j, which represent the weighted average between the two standardization methods with the weights changing over step in Equation (6). new,j and ˆSA ˆSA new,j = 1 ˆAi new,j + 1 ˆAi total,j, ˆSA total,j = 1 ˆAi new,j + 1 ˆAi total,j (6) ˆSA new,j places more weight on the current distribution as training progresses, reflecting preference for adapting to the latest policy outputs. Conversely, ˆSA total,j places more weight on the overall cumulative distribution, viewing the deviation of the current step as fine-tuning adjustment within the broader distribution. Finally, our final advantage ˆAi is defined as the smoothed advantage with the smaller absolute value to reduce the impact of the respective fluctuations of cumulative standardization and standardization of the current step on training stability, defined as Equation (7) ˆAi = (cid:40) ˆSA new,j, when ˆSA ˆSA total,j, otherwise new,j < ˆSA total,j (7) Once the prompt participates in the model optimization, the responses generated in the subsequent steps will participate in the model optimization regardless of whether the current advantage is 0 or not. 3 DCPO: Dynamic Clipping Policy Optimization Consequently, when identical rewards occur within step, these responses will also update the model with the advantage of 1 total,j, preserving useful learning signals and improving data efficiency. ˆAi"
        },
        {
            "title": "3.1 OTM LOSS",
            "content": "Although the Token-Level Mean loss(TLM) (the loss Equation (13) used in DAPO) balances all token contributions, it disrupts the relative relationship between different responses of different lengths. For instance, consider two responses to the same prompt: The response has an advantage of 1 and length of 500, and the response has an advantage of 0.5 and length of 1500. Under 500+1500 = 0.75. Therefore, the 500+1500 = 0.25, and for by TLM, the loss for is weighted by total loss contribution of is 1 0.25 = 0.25 and of is 0.5 0.75 = 0.375. This results in having larger influence (0.375) despite its lower advantage (0.5). 1500 500 At the same time, for the average across batches, the Sequence-Level Mean loss (SLM) (the loss Equation (12) used in GRPO) weakens the size of the advantage, equivalent to reducing it by times after calculation for advantages. Especially when the batch size is large, it will destroy the standardized results of the original advantages between the correlations. We believe that standardized advantages already have relative relationships across responses, and the average across all tokens in the same response with oi not only maintains the original relative relationship, but also distributes the importance equally to each token in the current response. As result, we only averaged the tokens within the one response without averaging all responses in the same batch. This is called Only Token Mean loss (OTM) as Equation (8). TDCPO (θ) = (cid:88) i=1 1 oi oi (cid:88) t=1 (cid:16) min ri,t(θ) ˆAi,t, clip (ri,t(θ), 1 εlow, 1 + εhigh) ˆAi,t (cid:17) (8) As in DAPO, the loss of DCPO is not constrained by the KL divergence. For the lower and upper clipping bounds, we used the dynamic clipping bounds introduced in Section 2."
        },
        {
            "title": "4 EXPERIMENTAL SETUP",
            "content": "Models and Datasets We experimented with four different models Qwen2.5-Math-1.5B-Instruct, Qwen2.5-3B (common base), Qwen2.5-Math-7B (math base) and Qwen2.5-14B (common base) (Yang et al., 2024a;b), and utilized merged training dataset of approximately 25k mathematical problems, which combines the DAPO-Math-17K corpus (Yu et al., 2025) with the levels 3-5 of the MATH subdataset (Hendrycks et al., 2021b; Liu et al., 2025). All problems were processed using the Qwen-Math template (see Appendix A.8) for both training and evaluation. Evaluation We evaluated our models on four widely used mathematical reasoning benchmarks: MATH500 (Hendrycks et al., 2021a), AMC23 (Ouyang et al., 2022), AIME24 (Li et al., 2024), and AIME25 (MAA). For MATH500, we only report Avg@1 due to its larger size (500 problems). For the smaller AMC23 (40 problems), AIME24 (30 problems), and AIME25 (30 problems) datasets, we report both Avg@1 and Avg@32 to ensure robust and comprehensive evaluation. Avg@1: This metric represents the standard accuracy achieved using greedy decoding. Measures the performance of the best prediction of the model. Avg@32: This metric calculates the average accuracy of 32 sampled responses per problem, using temperature of 1.0 and top of 1.0. This metric provides insight into the robustness and stability of the trained policy distribution. Baseline Settings We used GRPO and DAPO as baselines and added GSPO to the ablation baselines. We set symmetric clipping threshold of ϵ = 0.2 for GRPO and lower/upper clipping thresholds (0.2, 0.28) with an additional 512-token soft-punish cache for DAPO, lower/upper clipping thresholds (3e 4, 4e 4) for GSPO used in ablation studies, and low/high clipping thresholds (0.16, 0.2) for DCPO, preserving the same clipping points (q(x), r(x)) = ( 1 1+ϵ , 1 + ϵ) and (q(x), r(x)) = (1, 1 ϵ) as GRPO (Figure 4d), and set the maximum probability ratio to 10 for both positive and negative 4 DCPO: Dynamic Clipping Policy Optimization advantages, different from DAPO. The other common settings, such as the codebase and training hyperparameters, are described in the Appendix A.9."
        },
        {
            "title": "5.1 MAIN RESULTS",
            "content": "Table 1: Performance across benchmarks. Avg@1: greedy decoding; Avg@32: sampling 32 times. Boldface shows the best values, and shows improvement over baselines. Model MATH500 (Avg@1) AMC23 (Avg@1/32) AIME24 (Avg@1/32) AIME25 (Avg@1/32) Average (Avg@1/32) base GRPO DAPO DCPO GRP DAP base GRPO DAPO DCPO GRP DAP base GRPO DAPO DCPO GRP DAP base GRPO DAPO DCPO GRP DAP 73.6 77.2 76.0 77.2 +0.0 +1.2 46.4 69.2 72.4 71. +2.0 -1.2 50.4 81.6 83.0 82.5 +0.9 -0.5 60.8 81.2 83.4 84.6 +3.4 +1.2 Qwen2.5-Math-1.5B-Instruct 57.5/49.4 70.0/68.4 80.0/70.6 75.0/70.8 +5.0/+2.4 -5.0/+0.2 10.0/10.0 16.7/14.0 20.0/13.5 20.0/15.6 +3.3/+1.6 +0.0/+2.1 Qwen2.5-3B[Base] 27.5/7.8 62.5/51.6 57.5/54.0 62.5/55. +0.0/+4.2 +5.0/+1.8 3.3/0.1 10.0/7.5 10.0/8.3 3.3/7.5 -6.7/+0.0 -6.7/-0.8 Qwen2.5-Math-7B[Base] 40.0/19.5 77.5/75.9 72.5/80.7 82.6/79.8 13.3/6.0 36.7/32.1 36.7/31.6 46.7/38. +5.1/+4.9 +10.1/-0.9 +10.0/+6.7 +10.0/+7.2 Qwen2.5-14B[Base] 47.5/16.4 75.0/65.6 87.5/85.1 85.0/79.9 +10.0/+14.3 -2.5/-5.2 3.3/1.3 13.3/17.6 16.7/16.4 20.0/18. +6.7/+0.6 +3.3/+1.8 3.3/6.1 20.0/13.5 14.4/12.5 16.7/12.1 -3.3/-1.4 +2.3/-0.4 3.3/0.7 6.7/4.5 6.7/3.9 10.0/4.7 +3.3/+0.2 +3.3/+0.8 3.3/1.5 16.7/16.7 23.3/14.9 16.7/17. +0.0/+0.5 -6.6/+2.3 3.3/1.1 13.3/10.5 20.0/15.3 23.3/19.0 +10.0/+8.5 +3.3/+3.7 36.1/21.8 46.0/32.0 46.5/32.4 47.2/32.8 +1.2/+0.8 +0.7/+0.4 20.1/2.9 36.3/21.0 36.6/23.1 37.6/22. +1.3/+1.7 +1.0/-0.4 28.4/9.3 53.1/41.6 53.9/42.4 57.1/45.2 +4.0/+3.6 +3.3/+2.8 28.7/6.3 45.7/31.3 51.9/38.9 53.2/39.0 +6.5/+7.7 +1.3/+0.1 Table 1 summarizes the performance of various preference optimization methods, including GRPO, DAPO, and our proposed DCPO, on four math reasoning benchmarks: MATH500, AMC23, AIME24, and AIME25. We report both greedy decoding results (Avg@1) and sampling-based decoding performance (Avg@32), providing comprehensive evaluation of the robustness and generalization of each method. Across all model scales, DCPO consistently demonstrates superior or comparable performance relative to GRPO and DAPO, indicating its effectiveness as general-purpose preference optimization technique in reinforcement learning of LLMs. Overall Superiority of DCPO On the Qwen2.5-Math-1.5B-Instruct, DCPO achieves the highest average scores (47.2/32.8), exceeding GRPO (+1.2/+0.8) and DAPO (+0.7/+0.4). On Qwen2.5-3B, DCPO yields (37.6/22.7), exceeding GRPO (+1.3/+1.7) and performing competitively with DAPO, with notable improvement in Avg@1 (+1.0) and only marginal gap in Avg@32 (-0.4). For Qwen2.5-Math-7B, DCPO attains (57.1/45.2), surpassing GRPO (+4.0/+3.6) and DAPO (+3.3/+2.8). On the largest tested model, Qwen2.5-14B, DCPO delivers strong result of (53.2/39.0), offering substantial improvements over GRPO (+6.5/+7.7) and outperforming DAPO by (+1.3/+0.1) in Avg@1 5 DCPO: Dynamic Clipping Policy Optimization and Avg@32. These results highlight DCPOs robust improvements across different model capacities and its superior alignment to the reward signal under both greedy and stochastic decoding paradigms. Robustness on AIME under Sampling (Avg@32) particularly notable trend emerges on the AIME24 and AIME25 datasets, high-difficulty competition math benchmarks designed to stress-test models reasoning ability. Under Avg@32, which evaluates the models ability to generate correct responses under high-entropy sampling, DCPO exhibits significantly enhanced robustness on the 7B and 14B scales: On AIME24-Avg@32, DCPO-7B achieves 38.8, outperforming GRPO (32.1) and DAPO (31.6). On AIME25-Avg@32, DCPO-14B reaches 19.0, exceeding GRPO (10.5) and DAPO (15.3). These gains suggest that DCPO not only improves average case performance, but also enhances the diversity and reliability of correct generations, which is particularly critical under sampling-based decoding where beam search or temperature-driven variability is present. The methods ability to optimize both reward fidelity and exploration under high entropy decoding highlights its practical value in real-world settings where multiple plausible outputs are sampled. Figure 1: TCR across models and methods. 5.2 THE TOKEN CLIPPING RATIO (TCR) We use the Token Clipping Ratio (TCR) as the proportion of tokens excluded from policy updates during back-propagation due to clipping. As shown in Figure 1, the behavior of the TCR varies significantly across different reinforcement learning methods. TCR = Average (cid:32) (cid:88) m=1 Number of clipped tokens in microm Total number of tokens in microm (cid:33) (9) where microm denotes the m-th microbatch and is the number of microbatches per training step. In GRPO training, the TCR exhibits divergent trends based on the size of model. For smaller models (Qwen2.5-Math-1.5B and Qwen2.5-3B), the TCR increases with training steps, while for larger models (Qwen2.5-Math-7B and Qwen2.5-14B), it gradually decreases. In contrast, DAPO consistently shows an upward trajectory in the TCR for all models. This indicates that as DAPO training progresses, an increasing number of tokens are clipped, leading to higher proportion of policy updates based on partial or truncated responses. However, both GRPO and DAPO exhibit 6 DCPO: Dynamic Clipping Policy Optimization significant fluctuations and occasional abnormal spikes in the TCR, which may introduce instability into the optimization process and potentially lead to entropy collapses. In contrast, DCPO maintains remarkably stable and efficient training state. Regardless of the size of the model or the training stage, the TCR for DCPO remains relatively constant and an order of magnitude lower than that of GRPO and DAPO. This lower and stabler TCR signifies that DCPO discards fewer tokens, allowing greater portion of complete response sequences to contribute to the policy updates and freeing up more space for the model to explore the diversity of rare tokens. This finding highlights DCPOs superior sample efficiency compared to baselines while tightly controlling the space for outlier variation. Taking Qwen2.5-Math-7B as an example, we observe that the proportion of token distribution quickly concentrates on high-confidence tokens: after approximately 60 steps, about 95% of generated tokens have probability (q(x) > 0.9); this fraction rises to about 97% after approximately 100 steps and continues to increase in later stages of training. As illustrated in Figure 4d, once the old probability 1 satisfies (q(x) 0.83), the new probability p(x) will participate in the model updates 1+ϵgrpo within the interval [ 1 1+ϵ , 1] without being discarded regardless of any clipping methods. Consequently, the vast majority of tokens lie in region that is never clipped, which explains the low overall TCR observed in practice. Meanwhile, Figure 1 shows that the TCR grows during the later phases of most training tasks, indicating that an increasingly large proportion of low-probability tokens may be discarded and thus restricting the models ability to explore the diversity of rare tokens. Previous work (Wang et al., 2025) demonstrated that high-entropy tokens, which are precisely the low-probability tokens, are the primary drivers of the models emergence of reasoning capabilities. Because entropy and probability are inversely related, the rarer token, the more informational content it carries. Therefore, providing broader admissible update range for such low-probability tokens enlarges the effective exploration space of the model, facilitating the acquisition of diverse reasoning capabilities. These empirical and theoretical findings justify our proposed dynamic-adaptive clipping scheme, which relaxes clipping for tokens with q(x) < 1 1+ϵ while preserving the safe interval for highconfidence tokens. By adaptively expanding the update interval for the high-entropy tail of the distribution, the method restores the models capacity to learn from rare and informative tokens and consequently enhances its reasoning ability. 5.3 THE RESPONSE UTILIZATION RATIO (RUR) During RLVR training, policy update is contingent on non-zero gradient for given response. We quantify the efficiency of this process with metric defined as the Response Utilization Ratio (RUR), which is the percentage of non-zero advantage of generated responses that participate in the policy update. RUR = Number of responses with non-zero advantage Total number of generated responses 100% (10) Table 2 summarizes the overall average RUR for responses generated during GRPO training is low, with an overall mean of only 44.6% across four different models. This indicates that more than half of generated responses are discarded, highlighting the significant inefficiency in response utilization during model diversity exploration. In contrast, our proposed DCPO method achieves an average RUR of approximately 70% after the first epoch. This represents substantial improvement, with an absolute increase of 28% and relative improvement of 64% over the GRPOs. Table 2: Average RUR over 400 training steps. model Qwen2.5-Math-1.5B-Instruct Qwen2.5-3B Qwen2.5-Math-7B Qwen2.5-14B Average GRPO DCPO 45.6% 67.1% 48.3% 74.3% 37.4% 73.2% 43.9% 72.4% 43.8% 71.8% 7 DCPO: Dynamic Clipping Policy Optimization Figure 2 demonstrates that the RUR for GRPO training decreases significantly over time, regardless of the size or type of model. Notably, RUR drops sharply from above 90% to below 50%. For the Qwen2.5-Math-7B, for instance, the RUR plummets to as low as 30% after approximately 200 training steps. This finding suggests that GRPOs reliance on current advantage standardization results in significant fraction of zero gradients, indicating that the models diversity of exploration is notably inefficient in the later stages of training. In contrast, our proposed DCPO method, which employs smooth advantage standardization approach, consistently maintains high RUR. After the first epoch, the RUR quickly stabilizes around 70% and continues to improve with further training. We observed that the RUR exceeded 90% in the first few steps, but from the second epoch to the end of training, it remained around 70% and did not reach 100%. We analyze the reasons for these phenomena and demonstrate the potential value of the SAS mechanism. First, the initial RUR exceeding 90% is largely due to the output of the early-stage model that does not comply with the required formatting, leading to negative rewards and inflated RUR values. This suggests that different models can quickly learn fixed answer patterns, so further emphasis should be placed on exploring the models reasoning capabilities in subsequent training. Second, our analysis reveals two main factors that contribute to the consistently zero advantage, and thus to the RUR stabilizing around 70% rather than reaching 100%: (i) Nearly half of these math problems are simple enough that the model consistently solves them correctly, and it is justifiable to exclude these from policy updates because the core capabilities of the model do not require further refinement in these instances. (ii) The remaining instances correspond to being consistently erroneous. This may arise from two factors: On the one hand, some problems are too challenging for the current model to solve, even with correct ground-truth label. On the other hand, the DAPO-Math-17K dataset contains some items whose labels were converted to mislabeled integers, introducing potential incorrect labels. Discarding responses with zero advantage in these cases acts as form of filtering, mitigating the risk of updating the model based on noisy or mislabeled data. Unlike GRPO and DCPO, DAPO discards responses with identical labels from generated data, making direct RUR comparison difficult. Therefore, we evaluated the training efficiency of DAPO by comparing the required number of generated responses and the training time to complete the same number of update steps. To alleviate the low sampling efficiency caused by the low RUR of GRPO, we set the generation batch size at 1-4 times the training batch size and adjust it dynamically when training DAPO. Experiments show that, across four models, DAPO requires 3 to 5 times more generated responses than DCPO to achieve the same number of parameter updates, resulting in at least doubling the training GPU hours without yielding better performance. Furthermore, we observed that DAPOs data utilization efficiency decreases with model size. For example, the proportion of responses used by DAPO based on Qwen2.5-14B dropped to less than 30% after about 300 steps. This indicates that the dynamic sampling approach of DAPO has low data utilization and training efficiency, and misses out on potential exploration of model diversity. In summary, DCPO represents significant advancement over both GRPO and DAPO by substantially improving response utilization and training efficiency, fully utilizing sampled samples to provide more exploration space for model diversity, and improving performance. Figure 2: RUR progresses during training. Figure 3: Ablation using the average Avg@32 based on Qwen2.5-Math-7B. 8 DCPO: Dynamic Clipping Policy Optimization"
        },
        {
            "title": "5.4 ABLATION STUDIES",
            "content": "We conducted an ablation study on Qwen2.5-Math-7B to assess the contribution of each component in DCPO, using Avg@32 as the evaluation metric. This metric highlights the robustness and stability of the learned policy distribution. To ensure fairness, each experiment modifies single component of the baseline GRPO framework while keeping all other settings identical. In addition, we added GSPO (Zheng et al., 2025) as an additional baseline. GSPO uses sequence-level clipping instead of token-level clipping to reduce the proportion of high-entropy responses participating in model updates, resulting in most responses with non-zero advantage being wasted. The learning curves among 20 to 400 steps are shown in Figure 3. The learning curves for the three individual components show that the model continuously improves its inference ability. The model surpasses GRPO and attains (or exceeds) the performance of DAPO and GSPO. Consequently, each modification contributes distinct benefit, and their combination yields substantial overall gain on baselines, demonstrating the effectiveness of the DCPO pipeline over baselines in reinforcement learning tasks. In the later stages of training, the performance of baselines plateaus and shows no further improvement, while DCPO continues to exhibit an upward trend, both at the component level and overall. Ablation studies confirm that each component contributes positively to training efficiency, and their synergistic integration leads to cumulative gains and stability. Detailed performance is presented below. (i) Only-Token-Mean loss (OTM): When GRPO aggregated the loss within single response rather than across the whole batch, the resulting model attains performance that outperforms baselines, which suffer from lower data-utilization and fluctuating performance. (ii) Smoothed Advantage Standardization (SAS):Replaces the traditional stepwise advantage with the cumulative smoothed advantage, which significantly improves the performance of GRPO and approaches the performance of DAPO. The gain demonstrates that aggregating reward statistics across training steps stabilizes optimization and enhances final performance. (iii) Dynamic Adaptive Clipping (DAC): Replacement of the fixed clipping boundary of GRPO with probabilistically adaptive clipping boundary yields significant improvements. DAC itself surpasses GRPO, DAPO, and GSPO, and is more stable than using only OTM. (iv) Combination of OTM + SAS + DAC (full DCPO): Combining these three mechanisms achieves the best and most stable overall results, significantly outperforming GRPO, DAPO, and GSPO. The synergy between OTM, SAS, and DAC demonstrates that each design choice contributes uniquely to data-efficient and stable reinforcement learning, improving model performance and robustness for large language models. In general, the ablation study confirms that each component of DCPO contributes positively to overall performance, and their combination leads to substantial cumulative gains. The results validate the effectiveness of the proposed mechanisms in improving data efficiency and stability in reinforcement learning for LLMs."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we attempt to address key limitations of existing policy optimization methods: restricted token-level exploration due to fixed cropping in low-probability regions, and significantly low sample efficiency due to the current steps reward standardization. To this end, we introduced Dynamic Clipping Policy Optimization (DCPO), novel reinforcement learning pipeline that enhances the reasoning capabilities of large language models through two key innovations: (i) dynamically adaptive token-level clipping mechanism that adjusts bounds based on old policy probabilities, enabling more efficient exploration of rare tokens with low probabilities, and (ii) Cumulative-Smooth Advantage Standardization that aggregates the cumulative reward distribution across optimization steps, mitigating reward randomness from high-entropy sampling. Extensive experiments on four mathematical reasoning benchmarks with four models demonstrate DCPOs effectiveness. For example, the DCPO-7B variant achieved an Avg@32 score of 38.8 on the challenging AIME24 benchmark, outperforming GRPO (32.1) and DAPO (31.6). In addition, DCPO increased the response utilization ratio by 28% over GRPO, doubled the training efficiency compared to DAPO, and reduced the token clipping ratio by an order of magnitude. These results highlight DCPOs ability to significantly improve data utilization and diversity exploration in RLVR. Future work will explore the extension of DCPO to other domains, such as code generation and semantic reasoning. 9 DCPO: Dynamic Clipping Policy Optimization"
        },
        {
            "title": "REFERENCES",
            "content": "Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Yang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, and Zizheng Pan. Deepseek-v3 technical report, 2024. URL https://arxiv.org/abs/2412.19437. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021a. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. Sort, 2(4): 06, 2021b. Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13:9, 2024. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025. MAA. American invitational mathematics examination (AIME). Mathematics Competition Series, 2024. URL https://maa.org/math-competitions/aime. OpenAI. Introducing openai o3 and o4-mini. 2025. URL https://openai.com/index/ introducing-o3-and-o4-mini/. DCPO: Dynamic Clipping Policy Optimization Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv:2409.19256, 2024. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939, 2025. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024a. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024b. Deheng Ye, Zhao Liu, Mingfei Sun, Bei Shi, Peilin Zhao, Hao Wu, Hongsheng Yu, Shaojie Yang, Xipeng Wu, Qingwei Guo, et al. Mastering complex control in moba games with deep reinforcement learning. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 66726679, 2020. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, Xiangpeng Wei, et al. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. 11 DCPO: Dynamic Clipping Policy Optimization"
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 MAIN SYMBOL DEFINITIONS θ πθ(oiq) πθold (oiq) πref β ri,t(θ) ˆAi,t oi DKL(πθπref ) ε εlow and εhigh Number of responses generated per prompt at each step. Parameters of the actor model. Probability of response oi given prompt under parameters θ. Probability of response oi given prompt under previous parameters θold. Reference policy for KL divergence computation. Coefficient for KL divergence term (controls policy update vs. constraint trade-off). Probability ratio πθ(ot πθold (ot for token in response oi. iq,o1:t1 ) iq,o1:t1 ) Standardized advantage estimate for token in response oi. The length of the response in tokens. KL divergence between current and reference policies. Probability ratio clipping threshold. Lower/upper thresholds for probability ratio clipping. Maximum allowed response length. new new Reward variance across responses at step (0 if = 0). Mean reward across responses generated at step (0 if = 0). is equivalent(a, oi)Binary function indicating whether response oi matches answer a. µi σi µi σi µi σi Mean reward across responses from steps 1 to (0 if = 0). Reward variance across responses from steps 1 to (0 if = 0). total old old Mean reward across (i 1) responses from steps 1 to 1 (0 if 1). Reward variance across (i 1) responses from steps 1 to 1 (0 if 1). total A.2 PRELIMINARY A.3 ADVANTAGE CALCULATION In previous works, including GRPO and DAPO, the advantage ˆAi is calculated by standardizing the reward Ri rewards of the generated responses at the i-th step, as shown in Equation (11). j,t for the token in the response against the mean µi and standard deviation σi of the ˆAi j,t = (cid:0)Ri µi(cid:1) σi (11) When the rewards for the same prompt are identical, responses with zero advantages do not contribute to model update, resulting in response waste. A.3.1 GRPO Shao et al. (2024) proposed the Group Relative Policy Optimization(GRPO), which first samples responses for each query, assigns rewards via rule-based reward function, and estimates tokenlevel advantages as in Equation (11). Finally, the model parameters are updated using Equation (12), referred to as Sequence Level Mean loss (SLM) as: 12 DCPO: Dynamic Clipping Policy Optimization TGRPO(θ) ="
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 oi t=1 oi (cid:88) (cid:16) min ri,t (θ) ˆAi,t, clip(ri,t (θ) , 1 ε, 1 + ε) ˆAi,t (cid:17) βDKL(πθπref) (12) The first term averages token-level advantages over all tokens in response and then over all responses in the batch. The DKL penalty constrains the policy from deviating too far from reference policy (e.g., pre-trained model). The fixed clipping threshold ϵ prevents overly large updates by limiting the change in token probability ratios. This averaging strategy, though widely adopted, inherently dilutes per-response importance. A.3.2 DAPO Similarly, Yu et al. (2025) propose Dynamic sAmpling Policy Optimization (DAPO), which observed that in GRPO, longer sequences exert disproportionate influence on gradient update. They standardize token counts across the entire batch, leading to the Token Level Mean loss (TLM) as Equation (13). TDAPO (θ) = 1 i=1 oi (cid:80)G (cid:88) oi (cid:88) (cid:16) min i=1 t=1 ri,t(θ) ˆAi,t, clip (ri,t(θ), 1 εlow, 1 + εhigh) ˆAi,t (cid:17) (13) subject to 0 < {oi is equivalent(a, oi)} < G. It discards the prompts whose all responses share identical reward, and regenerates responses to maintain batch size, leading to severe data inefficiency. A.3.3 GSPO Zheng et al. (2025) propose Group Sequence Policy Optimization (GSPO), arguing that token-level clipping can inject high-variance noise into gradient estimates. This noise tends to accumulate over long sequences and is further amplified by the clipping operation itself. GSPO alleviates the problem by replacing token-wise clipping with sequence-wise clipping, which yields more stable and effective training signal. TGSPO (θ) = 1 G (cid:88) i=1 (cid:16) min si (θ) ˆAi, clip(si (θ) , 1 ε, 1 + ε) ˆAi (cid:17) where si(θ) = exp( 1 oi oi (cid:88) t=1 log( πθ(oj,tq) πθold (oj,tq) )) (14) GSPO discards more than 10% of responses with non-zero advantages, resulting in high data waste and training efficiency. A.4 REWARD CALCULATION To provide verifiable and interpretable supervision signal, we adopt rule-based reward model that evaluates each generated response based on the correctness of the answers and the compliance with the format, as Equation (15). Specifically, reward of +1 is assigned when both the format and the answer are correct; reward of 0 is given when the format is correct but the answer is incorrect; and reward of -1 is assigned when the format is incorrect. Ri = 1, 0, 1, correct format and correct answer correct format but incorrect answer incorrect format (15) where is the index of the response group for given problem in the i-th training step, and the reward is calculated based on the format and correctness of the response. 13 DCPO: Dynamic Clipping Policy Optimization A.5 HYPOTHETICAL REASONING PROCESS FOR DYNAMIC-ADAPTIVE CLIPPING BOUNDS Clipping the probability ratio is crucial technique in reinforcement learning to stabilize policy optimization and prevent large, destabilizing parameter update. This mechanism is particularly important when using importance sampling to estimate expected values under new policy from samples generated by an old policy. The expected value of function (x) under the new probability p(x) can be rewritten as an expectation under the old probability q(x) by importance sampling weight, as shown in equation 16: (cid:90) (x)p(x) dx = (cid:90) (x) p(x) q(x) q(x) dx = Exq (cid:20) (x) (cid:21) p(x) q(x) (16) Although this estimator is unbiased, its variance can be significantly inflated, which is common challenge in importance sampling. The true variance of the function (x) under the new policy is given by Equation (17), whereas the variance of the importance-sampled estimator under the old policy is given by Equation (18): Varxp[f (x)] = Exp (cid:2)f (x)2(cid:3) (Exp[f (x)])2 (cid:20) Varxq (x) (cid:21) p(x) q(x) = Exq = Exp (cid:34)(cid:18) (x) p(x) q(x) (cid:21) (cid:20) (x)2 p(x) q(x) (cid:19)2(cid:35) (cid:18) Exq (cid:20) (x) (cid:21)(cid:19)2 p(x) q(x) (Exp[f (x)])2 (17) (18) The difference between the importance-sampled variance and the true variance highlights the potential for inflation, as shown in Equation (19): (cid:20) Varxq (x) (cid:21) p(x) q(x) Varxp [f (x)] =Exp (cid:20) (x)2( p(x) q(x) (cid:21) 1) = (cid:90) (x)2( p(x) q(x) 1)p(x)dx (19) To mitigate this difference, many prior works, including PPO Schulman et al. (2017) and RLVR methods (DeepSeek-AI et al., 2024; Liu et al., 2025), employ fixed symmetric clipping bound on the probability ratio r(x) = p(x) q(x) , as defined in Equation (20): r(x) 1 ϵ (20) However, fixed clipping bound inherently constrains the policys capacity for exploration, particularly problematic in regions where the old policy assigns low probability mass. In such cases, the absolute bound for low probabilities is severely limited, restricting the potential for meaningful policy updates. While methods like DAPO attempt to address this by introducing asymmetric clipping bounds, the fundamental limitation of non-adaptive constraint remains. In addition, recent studies such as Yue et al. (2025) suggest that the reasoning paths leveraged by RLVR models are largely present within the sampling distribution of the base model. This is further evidenced by the minimal parameter shift observed during off-policy batch update, as reflected in the small fraction of clipped parameters. Given this stability, we propose more practical alternative to constrain the probability ratio r(x) through the dynamic-adaptive mechanism by including the probability in the restriction as in Equation (21), replacing the conventional fixed clipping approach of Equation (20). ( p(x) q(x) 1)p(x) ϵ 14 (21) DCPO: Dynamic Clipping Policy Optimization Expanding the substitution of p(x) = r(x)q(x) into the inequality of previous equation yields the expression in Equation (22). ϵ (r(x) 1)r(x) q(x) ϵ (22) For this clipping ratio to be mathematically valid and meaningful, we must adhere to the fundamental constraints of probability distributions and their ratios. Specifically, as shown in Equation (23), the probabilities p(x) and q(x) must be nonnegative and less than or equal to 1. Consequently, the probability ratio r(x) must also be non-negative, and the clipping hyper-parameter ϵ must be non-negative. 0 (x) 1 0 (x) 1 0 (x) 0 ϵ (23) To ensure the validity and practical applicability of the clipping bound derived from Equation (22), we define the necessary conditions for its bounds as outlined in Equation (24). (cid:113) lowge (q (x) , ϵ) = 0.5 1 2 highle (q (x) , ϵ) = 0.5 + 1 2 (cid:113) lowle (q (x) , ϵ) = 0.5 1 2 highge (q (x) , ϵ) = 0.5 + 1 2 1 + 4ϵ q(x) 1 + 4ϵ q(x) q(x) , 1 4ϵ 1 4ϵ (cid:113) (cid:113) q(x) , S.t. S.t. q(x)2 4q(x) ϵ q(x)2 4q(x) ϵ 0 (24) As we can get from Equation (22) to Equation (24), the probability ratio r(x) should be met by the following conditions: (cid:40) lowge (q (x) , ϵ) 0 r(x) highle (q (x) , ϵ) , r(x) lowle (q (x) , ϵ) highge (q (x) , ϵ) r(x), when Varxq otherwise (cid:104) (x) p(x) q(x) (cid:105) Varxp [f (x)] (25) For the clipping bound, we consider the overlapping region centered around the ratio value of 1 to minimize the discrepancy of bounds between the two conditions and to maintain consistency with the conventional fixed bound of GRPO, and define the following conditions to ensure the validity of the clipping bound: highge (q (x) , ϵ) r(x) highle (q (x) , ϵ) q(x)2 4q(x) ϵ S.t. (26) For lowle and highge, to ensure that the square number is not less than zero, we use the method of comparing with 0 and taking the maximum value of the approximation like max(1 4ϵ q(x) , 0). Finally, we can get the following conditions, as dynamic-adaptive clipping bounds for importance sampling: 0.5 + (cid:115) 1 (cid:18) max 1 4ϵlow (x) (cid:19) , (x) 0.5 + (cid:115) 1 + 1 2 4ϵhigh (x) (27) For the upper and lower clipping bounds in Inequality 27, we set different upper and lower clipping thresholds ϵhigh and ϵlow hyperparameters to align the clipping bound with the GRPO at specific points. 15 DCPO: Dynamic Clipping Policy Optimization A.6 ANALYZING THE DIFFERENCES BETWEEN DYNAMIC-ADAPTIVE CLIPPING AND FIXED"
        },
        {
            "title": "CLIPPING",
            "content": "To ensure that our dynamic adaptive clipping bounds align with the fixed symmetric clipping bounds at the boundary points ( 1 1+ϵ , 1) and (1, 1 ϵ), as shown in Figure 4d, we derive the specific values for ϵlow and ϵhigh by solving the equations in Equation (28). (cid:40)q(x) = 1 1+ϵ 1 = 0.5 + 1 2 (cid:113) (cid:40)q(x) = 1 1 ϵ = 0.5 + 1 2 (cid:113) max(1 4ϵlow q(x) , 0) 1 + 4ϵhigh q(x) (28) Typically, in previous practices(e.g., GRPO), the fixed clipping threshold ϵ is set to 0.2, and we can get our clipping hyperparameters to be approximately ϵlow = 0.16 and ϵhigh = 0.2, while ϵlow = ϵhigh has different purpose from the asymmetric fixed clipping setting in DAPO. (a) Not Clipped p(x) regions. (b) r(x) bounds (q(x) [0, 1]). (c) r(x) bounds (q(x) [0, 0.1]). (d) r(x) bounds (q(x) [0.1, 1]). Figure 4: Clipping bound comparisons. Lines show bounds for fixed clipping (ε = 0.2) vs. dynamicadaptive clipping (εlow = 0.16, εhigh = 0.2). Dynamic versus fixed clipping: Figure 4 illustrates how the bound behaves under the fixed clipping scheme used in GRPO ([1 ε, 1 + ε]) and under our probability-aware adaptive clipping. For token with an old probability q(x), the fixed bound provides constant relative clipping width 2ε. Consequently, as q(x) decreases, the absolute exploration space allowed shrinks linearly (Figure 4a). In contrast, the relative width of dynamic adaptation is inversely proportional to q(x), so the exploration space remains roughly constant throughout the probability range. DCPO: Dynamic Clipping Policy Optimization Effect of decreasing q(x): When q(x) becomes small, the admissible interval [1 ε, 1 + ε] of the fixed clipping scheme is independent of q(x), so the models ability to explore low-probability tokens does not improve, while the dynamic adaptive bound expands rapidly (Figure 4b), resulting in larger relative space for rare tokens, which is exactly the behavior required for efficient RL. Behavior for extremely low probabilities: Figure 4c shows the clipping interval when q(x) [0, 0.1]. As q(x) 0, the upper bound for r(x) of dynamic-adaptive clipping increases rapidly inversely proportional to the square root, while the corresponding value for fixed clipping remains constant. Inspired by the dual-clipping strategy(Ye et al., 2020), we enforce hard ceiling rmax = 10 to prevent the resulting instabilities (gradient vanishing, explosion, etc.). The gray region in Figure 4c corresponds to approximately q(x) 2 103, where the upper limit of adaptive clipping is set to rmax. Upper and lower bounds: For the lower bound, when q(x) 4ε the dynamic-adaptive clipping yields dynamiclow = 0.5, whereas the fixed clipping retains the constantf ixedlow = 1 ε = 0.8. When q(x) increases beyond this threshold, the dynamic-adaptive lower bound smoothly contracts towards the fixed value. Similarly, the dynamic-adaptive upper bound exceeds the fixed bound whenever q(x) < , and merges with it for larger q(x). Thus, the adaptive interval [dynamiclow (q (x) , ϵlow) , dynamichigh (q (x) , ϵhigh)] provides probability-dependent widening of the admissible region for rare tokens while preserving the tightness of the fixed interval for common tokens (Figure 4d). 1 1+εhigh Positive versus negative advantage: The loss in Equation (8) contains both positive and negative advantage term. During the operation with minimum loss value, the ratio r(x) for positive advantage is clipped in the interval [0, dynamichigh(q(x), ϵhigh)]. For negative advantage, the clipping interval [dynamiclow(q(x), ϵlow), ). Due to an excessively large r(x), which can destabilize training, we limit the ratio for both positive and negative advantages at rmax = 10. This precaution is inspired by Ye et al. (2020) and ensures that the adaptive limits do not produce excessively high importance weights. In short, regardless of positive or negative advantages, compared to fixed clipping, dynamic adaptive clipping provides the model with more space to explore diversity when rare tokens have low probability. A.7 THE DEFINITIONS OF THE ADVANTAGE STANDARDIZATION Let denote the i-th iteration where prompts are used to generate responses and update the model parameters. We define the following statistics for reward normalization. For the average for the rewards of responses, we define the equations as the group of functions as in Equation (29): new = 1 µi (cid:80) j=1 Ri µi old = 1 G(i1) µtotal = 1 Rk (29) i1 (cid:80) k=1 (cid:88) (cid:80) j=1 (cid:88) k= j=1 Rk = 1 (cid:0)µi new + (i 1) µi old (cid:1) For the variance for the rewards of the responses, we define the equations as the group of functions as in Equation (30): 17 DCPO: Dynamic Clipping Policy Optimization (cid:0)Ri µi new (cid:1)2 (cid:80) k=1 (cid:80) j=1 (cid:0)Rk µk old (cid:1)2 (cid:115) σi new = (cid:115) 1 G (cid:80) j=1 σi old = 1 G(i1) (cid:118) (cid:117) (cid:117) (cid:116) σi total ="
        },
        {
            "title": "1\nG ∗ i",
            "content": "i (cid:88) (cid:88) k=1 j=1 (cid:0)Rk µk total (cid:1)2 (30) (cid:115) = 1 (cid:18) σi new 2 + (i 1) σi old 2 + 1 (cid:0)µi old µi new (cid:1)2(cid:19) To reduce resource usage, for the calculation of the value ofσi tion (29), we use equivalent calculations of µi new, µi of original rewards. new, σi total andµi old andσi total in Equation (30) and Equaold instead of direct calculations A.8 THE TEMPLATE FOR EXPERIMENTS We use the following template for all experiments in this paper, which is based on the Qwen-Math template (Yang et al., 2024b). Qwen-Math Template <im start>system Please reason step by step, and put your final answer within boxed{}. <im end> <im start>user {problem} <im end> <im start>assistant A.9 TRAINING COMMON SETTING Training was conducted for 400 steps with batch size of 512 for response generation and minibatch size of 32 for parameter update. We used temperature equal to 1 and top equal to 1 to generate G=16 responses for each problem, while the prompt max length was set to 1,024 and the response max length was set to 3,072, which is much smaller than the maximum length of 20k used in the original DAPO. We adapted our training codebase from Verl (Sheng et al., 2024) on 32xH20 GPUs. Unless stated otherwise, the hyperparameters matched those of DAPO. For loss computation, we retained the origi1 nal 1 oi weighted average formulation from GRPO or DAPO, instead of the approximate weighted calculation used in Verl, to ensure compatibility with different microbatch sizes. or (cid:80)G i= A.10 THE DETAILED PERFORMANCE OF MODELS Avg@1 Trend Figure 5 shows the performance of the model using the average Avg@1 metric. This metric is calculated by averaging the greedy decoding (Avg@1) results on the MATH500, AMC23, AIME24, and AIME25 benchmarks and represents the best performance of the model. As can be seen in the figure, DCPO consistently outperformed GRPO and DAPO across all models throughout the experiment. Furthermore, its overall performance was more stable than the baseline, without the significant performance drop seen in GRPO or the significant fluctuations seen in DAPO. This suggests that DCPO can stimulate stronger model reasoning capabilities during training, providing greater potential for the model to explore even better performance. Avg@32 Trend Figure 6 shows the Avg@32 performance of DCPO with GRPO and DAPO at different model sizes on AMC23, AIME24, and AIME25 benchmarks, clearly demonstrating 18 DCPO: Dynamic Clipping Policy Optimization Figure 5: Avg@1 performance across benchmarks Figure 6: Avg@32 performance across benchmarks. 19 DCPO: Dynamic Clipping Policy Optimization DCPOs exceptional robustness and stability. In contrast, GRPO exhibits significant fluctuation and occasional performance collapse. By GRPO, the Qwen2.5-14B model even experiences significant performance drop after approximately 170 training steps. Across all evaluated models (from 1.5B to 14B), the DCPO performance trajectory exhibits consistently upward trend with minimal fluctuation. Even in the early stages of training, DCPO maintains the fastest growth rate and maintains its upward trend. Furthermore, compared to DAPO, DCPO achieves superior performance while saving half of training time and resources. These experimental results demonstrate the effectiveness and robustness of dynamic-adaptive clipping and smoothing advantage standardization to improve the exploration at the response level and the label level of diversity. Figure 7: The Entropy of the models during Training. Entropy Trend Figure 7 shows that GRPO exhibits sharp entropy decay after an initial warm-up phase and quickly stabilizes in flat, low-entropy range. This entropy collapse reflects rapid loss of policy randomness, reducing the diversity of sampled responses, and thus limiting subsequent policy optimization. The behavior of DAPO and DCPO is model dependent. In the Instruct model (Qwen2.5-Math-1.5B), DAPO, DCPO, and GRPO follow similar declining trajectories. In contrast, for the larger base models (Qwen2.5-3B, Qwen2.5-Math-7B, and Qwen2.5-14B), the trends differ significantly. DAPO maintains the highest entropy value, showing clear upward trend accompanied by large fluctuations. In contrast, DCPO reaches moderate entropy level: its curve is significantly smoother and remains confined to bounded band between the low-entropy region of GRPO and the high-variance region of DAPO. These observations suggest U-shaped relationship between entropy and training performance. Excessively low entropy deprives the learner of exploratory signal, leading to premature convergence, while excessively high or unstable entropy compromises training stability. DCPO maintains entropy within moderate and well-balanced range, promoting convergence and sufficient exploration, benefit that is particularly pronounced as model capacity grows. Therefore, DCPOs entropy regulation mechanism may underlie its superior inference performance and enable more robust policy learning across models of varying sizes."
        }
    ],
    "affiliations": [
        "Baichuan.inc"
    ]
}