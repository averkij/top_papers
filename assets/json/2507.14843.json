{
    "paper_title": "The Invisible Leash: Why RLVR May Not Escape Its Origin",
    "authors": [
        "Fang Wu",
        "Weihao Xuan",
        "Ximing Lu",
        "Zaid Harchaoui",
        "Yejin Choi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in large reasoning models highlight Reinforcement Learning with Verifiable Rewards (RLVR) as a promising method for enhancing AI's capabilities, particularly in solving complex logical tasks. However, it remains unclear whether RLVR truly expands a model's reasoning boundary or merely amplifies high-reward outputs that the base model already knows for improved precision. This study presents a theoretical and empirical investigation that provides fresh insights into the potential limits of RLVR. First, we offer a new theoretical perspective that RLVR is constrained by the base model's support-unable to sample solutions with zero initial probability-and operates as a conservative reweighting mechanism that may restrict the discovery of entirely original solutions. We also identify an entropy-reward tradeoff: while RLVR reliably enhances precision, it may progressively narrow exploration and potentially overlook correct yet underrepresented solutions. Extensive empirical experiments validate that while RLVR consistently improves pass@1, the shrinkage of empirical support generally outweighs the expansion of empirical support under larger sampling budgets, failing to recover correct answers that were previously accessible to the base model. Interestingly, we also observe that while RLVR sometimes increases token-level entropy, resulting in greater uncertainty at each generation step, answer-level entropy declines, indicating that these seemingly more uncertain paths ultimately converge onto a smaller set of distinct answers. Taken together, these findings reveal potential limits of RLVR in extending reasoning horizons. Breaking this invisible leash may require future algorithmic innovations such as explicit exploration mechanisms or hybrid strategies that seed probability mass into underrepresented solution regions."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 3 4 8 4 1 . 7 0 5 2 : r Preprint. THE INVISIBLE LEASH: WHY RLVR MAY NOT ESCAPE ITS ORIGIN Fang Wu Weihao Xuan, Ximing Lu Zaid Harchaoui Yejin Choi Stanford University University of Tokyo RIKEN AIP University of Washington {fangwu97, yejinc}@stanford.edu xuan@ms.k.u-tokyo.ac.jp lux32@cs.washington.edu zaid@uw.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent advances in large reasoning models highlight Reinforcement Learning with Verifiable Rewards (RLVR) as promising method for enhancing AIs capabilities, particularly in solving complex logical tasks. However, it remains unclear whether RLVR truly expands models reasoning boundary or merely amplifies high-reward outputs that the base model already knows for improved precision. This study presents theoretical and empirical investigation that provides fresh insights into the potential limits of RLVR. First, we offer new theoretical perspective that RLVR is constrained by the base models supportunable to sample solutions with zero initial probabilityand operates as conservative reweighting mechanism that may restrict the discovery of entirely original solutions. We also identify an entropyreward tradeoff: while RLVR reliably enhances precision, it may progressively narrow exploration and potentially overlook correct yet underrepresented solutions. Extensive empirical experiments validate that while RLVR consistently improves pass@1, the shrinkage of empirical support generally outweighs the expansion of empirical support under larger sampling budgets, failing to recover correct answers that were previously accessible to the base model. Interestingly, we also observe that while RLVR sometimes increases token-level entropyresulting in greater uncertainty at each generation stepanswer-level entropy declines, indicating that these seemingly more uncertain paths ultimately converge onto smaller set of distinct answers. Taken together, these findings reveal potential limits of RLVR in extending reasoning horizons. Breaking this invisible leash may require future algorithmic innovations such as explicit exploration mechanisms or hybrid strategies that seed probability mass into underrepresented solution regions. The limits of my language mean the limits of my world. Ludwig Wittgenstein"
        },
        {
            "title": "INTRODUCTION",
            "content": "The rise of large reasoning models, such as DeepSeek-R1 (Guo et al., 2025) and OpenAI-o3 (Jaech et al., 2024), marks breakthrough in AI capabilities, particularly in solving complex logical tasks involving mathematics (Luo et al., 2025c; Zeng et al., 2025) and programming (Luo et al., 2025b; Liu & Zhang, 2025). The key ingredient behind this remarkable progress is large-scale Reinforcement Learning with Verifiable Rewards (RLVR), where pretrained base modelor one finetuned on long-form Chain-of-Thought (CoT) datais optimized via reinforcement learning (RL) using simple, automatically computed rewards. Despite the empirical success, fundamental question remains under active debate within the research community: does RLVR expand base models reasoning capabilities, or does it simply reinforce patterns the base model already knows, sometimes at the expense of exploring alternative correct solutions? Equal contribution. Corresponding author. 1 Preprint. Figure 1: Empirical-support dynamics of RLVR across tasks. Left: Conceptual illustration of empirical support under threshold ϵ. We define four regions based on whether correct completion is assigned non-negligible probability mass by the base model and the RLVR model πθ: Empirical Support Preservation covers completions with q(yx) > ϵ and πθ(yx) > ϵ; Empirical Support Shrinkage includes correct completions downweighted by RLVR below ϵ; Empirical Support Expansion includes completions that RLVR newly upweights above ϵ despite negligible base model mass; and Out of Support refers to completions missed by both. Right: Pie charts showing the proportion of completions in each category across diverse reasoning tasks. Support preservation dominates, but shrinkage consistently outweighs expansionhighlighting RLVRs tendency to sharpen within the base models support rather than expand into new solution modes. Recent studies offer divergent perspectives on this question. On the one hand, several works (Yue et al., 2025a; Zhao et al., 2025b; Shah et al., 2025; Ma et al., 2025; He et al., 2025) highlight paradoxical failure mode: while RLVR-trained models outperform their base models on pass@k at low sampling budgets (e.g., = 1), base models achieve higher pass@k scores when is large, suggesting narrowing of the reasoning horizon after RLVR training. Some even report that RLVR-trained models benefit from seemingly random or spurious reward signals (Shao et al., 2025), raising questions about whether the observed improvements genuinely reflect enhanced reasoning. On the other hand, Liu et al. (2025) report that previous studies focused primarily on special domains such as math, where base models may have been over-trained, which can then lead to premature termination of RLVR unless the level of entropy is carefully controlled. They then demonstrate that RLVR can expand the reasoning horizon considerably on certain domains, such as Reasoning Gym, where the base models struggle, with marked improvement on pass@k at large k. While seeking definitive answer to this debate remains an open challenge, we present theoretical and empirical investigation that provides novel insights on the potential limits of RLVR when using the currently competitive RLVR recipe. We first present novel theoretical perspective that RLVR predominantly preserves the support of the base model. The intuition is that LLMs cannot sample solutions that have zero probability mass from the initial distribution; thus, the support of the base models inherently restricts the discovery of truly original reasoning patterns. Additionally, we provide unified view of the RLVR objective via variational inference, revealing why RLVR is inherently conservative: it makes minimal updates to the base models distribution, preserving relative probabilities within the reward-consistent subset. Lastly, we highlight the entropy-reward tradeoff: while RLVR reliably enhances precision, it may also progressively narrow the exploration of reasoning trajectories, potentially overlooking correct yet underrepresented solutions. Empirically, we validate these theoretical insights via extensive experiments across diverse domains, including mathematics, logical reasoning, factual QA, and code generation tasks. To characterize RLVRs impact on the output distribution, we introduce the notion of empirical supportthe set of correct completions assigned non-negligible probability under models sampling distribution. We find that while RLVR consistently improves pass@1, the shrinkage of empirical support generally outweighs the expansion of empirical support under larger sampling budgets. While RLVR can occasionally assign non-negligible probability mass to previously underrepresented correct completions (empirical-support expansion), we observe that the oppositeempirical-support shrinkageis more frequent: RLVR often fails to recover correct answers that were previously accessible to the base model. This trend highlights RLVRs role as conservative reweighting mechanism rather than driver of fundamentally novel reasoning modes. To further understand how 2 Preprint. RLVR reshapes the sampling distribution, we decouple local uncertainty and global diversity via two entropy metrics: token-level and answer-level entropy. Interestingly, we observe that RLVR sometimes increases token-level entropyreflecting greater uncertainty at each generation steplikely due to longer reasoning chains or more complex intermediate decisions. Yet, answer-level entropy declines, indicating that these more uncertain generation paths ultimately collapse onto smaller set of distinct answers. This contrast reveals that RLVR models may appear more exploratory at the step level, even as they converge on fewer final completions. Taken together, these findings suggest that there might exist inherent limits in RLVR for extending LLMs reasoning horizons despite its empirical success. To break this invisible leash, RLVR may need augmenting with explicit exploration or hybrid strategies that seed probability mass into underrepresented regions of the solution space. We hope this work offers novel insights into RLVRs strengths and limitations, guiding future efforts in building LLM systems that can unlock genuinely new reasoning capacity."
        },
        {
            "title": "2 THEORETICAL LIMITS OF RLVR",
            "content": "2.1 PRELIMINARIES Let denote the space of natural language prompts, and denote the space of token sequences (e.g., reasoning traces or completions). For fixed prompt , q(y x) is the output distribution of the base model, and R(x, y) {0, 1} is verifiable reward function indicating whether is correct solution. Various RLVR algorithms, including PPO (Schulman et al., 2017), RLOO (Kool et al., 2019), GRPO (Guo et al., 2025), DAPO (Yu et al., 2025), or REINFORCE++ (Hu, 2025), learn new distribution πθ(y x) to optimize different variants of the following regularized objective: max θ Eyπθ(x),xD (cid:20) R(x, y) β1 log πθ(y x) q(y x) (cid:21) , (1) where is the distribution of prompts. An optional log ratio corresponds to regularized policy update that penalizes divergence from the base model controlled by hyperparameter β > 0. 2.2 SUPPORT PRESERVATION: WHY RLVR RARELY DISCOVERS NEW MODES We begin by formalizing core limitation of RLVR: it is inherently constrained to operate within the support of the base models distribution. Since RLVR relies on gradient signals derived from samples generated by the base model, it cannot assign nonzero probability to any solution that can never be sampled from q( x). As result, any correct output with q(y x) = 0 remains inaccessible to policy gradient updates, regardless of reward. Definition 2.1 (Support of Correct Completions) Let = {y R(x, y) = 1} denote the set of correct completions under the reward function R. Then the effective support on correct completions of distribution p(y x) is defined as supp(p) := {y p(y x) > 0} . We formalize this intuition with the following theorem, which makes precise how RLVRs reliance on the base models sampling prevents discovering truly new solutions. Theorem 2.2 (Support Preservation under RLVR) Let πθ(y x) be the RLVR-trained distribution obtained via standard on-policy gradient updates on verifiable rewards R. Then for all , supp(πθ( x)) supp(q( x)). In particular, if q(y x) = 0 for some correct solution y, then RLVR cannot discover y. Corollary 2.3 (Asymptotic Sampling Upper Bound) Let pass@kp(x) be the probability that at least one out of samples yi p( x) is correct, i.e. pass@kp(x) = 1 (cid:0)Pryp[R(x, y) = 0](cid:1)k . Under the conditions of Thm. 2.2 and the sampling independence, we have pass@kq(x). lim sup pass@kπθ (x) lim sup 3 Preprint. Those theorems formalize critical limitation of RLVR: its optimization cannot expand the search space beyond the initial support of the base model. This limitation arises because on-policy sampling means the model updates only from what it already samples lacking representational coverage means no gradient can ever push probability mass toward truly unseen solutions. Even when rewards provide clear training signal, RLVR cannot access or discover solutions that the base model assigns zero probability. Proofs are in Appx. A.1 and A.2. This manifests as trade-off between sharpness and diversity: RLVR can improve pass@1 by concentrating mass on known high-reward modes but tends to reduce pass@k performance for larger k, where broader coverage is beneficial. By contrast, the base model may occasionally sample correct answers from its long-tail distribution, giving it statistical edge under high-k evaluations (Yue et al., 2025a; Liu et al., 2025). This asymptotic upper bound captures ceiling: no matter how many samples are drawn, the RLVR-trained model cannot exceed the base models pass@k in the limit. Empirical-Support Relaxation. Thm. 2.2 assumes that has exact zeros in its support and RLVR operates strictly on-policy. However, these conditions rarely hold in practice. Softmax layers yield strictly positive probabilities across all tokens, making the nominal support of span the entire space Y. This factor, along with sampling noise and or temperature scaling, contributes to what we refer to as empirical support diffusion: over time, the model may assign growing probability mass to completions that initially had negligiblebut still nonzeroprobability under the base model. While q(y x) is technically positive for all due to the softmax, many completions lie so deep in the tail that they are effectively invisible to the training algorithm under finite sampling. To formalize this, we develop relaxation and define the empirical support under ϵ as suppϵ(q) := {y q(y x) > ϵ} , where ϵ > 0 denotes small cutoff (e.g., 104) that separates completions with practically observable likelihood from those that are statistically negligible. Completions outside this threshold are unlikely to be sampled in typical on-policy RL settings with finite rollouts. The choice of ϵ is thus crucial for assessing which completions are empirically reachable. Intuitively, ϵ should correspond to the minimum probability required for correct completion to appear within samples. We derive principled estimate for this threshold based on sampling confidence bounds in Appx. A.7. Definition 2.4 (Empirical-Support Expansion and Shrinkage) Given threshold ϵ > 0, We say RLVR achieves empirical-support expansion under threshold ϵ if suppϵ(πθ) suppϵ(q) = , i.e. there exists at least one completion such that q(y x) ϵ but πθ(y x) > ϵ. That is, the RLVR-trained model assigns non-negligible probability mass to correct completions that were effectively negligible under the base model. We say RLVR exhibits empirical-support shrinkage under threshold ϵ if suppϵ(q) suppϵ(πθ) = , i.e. there exists at least one completion such that q(y x) > ϵ but πθ(y x) ϵ. This formalizes the phenomenon where RLVR concentrates probability mass onto narrower subset of outputs, effectively excluding correct solutions that were previously accessible under the base model. Recall is the set of correct completions, and let := suppϵ(q) denote the set of low-density completions. We consider single-step RLVR update of the form: πθ( x) = (1 γ)(cid:101)πθ( x) + γ πe( x), where (cid:101)πθ represents reward-weighted distribution (e.g., exponential tilting), πe denotes an exploration distribution, and γ [0, 1] is the mixing weight. This formula arises by considering an interpolation between reward-tilted distribution πθ, which sharpens on known high-reward modes, and an explicit exploration distribution πe that seeds probability mass into underexplored regions. The mixing parameter γ controls the exploration-exploitation balance. 4 Preprint. Theorem 2.5 (Empirical-Support Preservation) Given fixed τ > 0 and under the update rule above, if the preceding policy satisfies the KL budget KL(cid:0)πθ q(cid:1) δ, the probability mass that the updated policy assigns to any in the lowdensity tail obeys πθ(y x) γ + (1 γ) eβ (cid:0)τ + 2δ(cid:1). Thus, unless γ (exploration weight) or τ (tail threshold) is substantially large, the probability mass assigned to regions outside the base models empirical support remains negligible. In practice, RLVR algorithms typically impose strong KL regularization (small δ), and use conservative temperature settings (small β). These choices jointly control the amplification factor eβ 2δ. When combined with minimal exploration (small γ), πθ(y x) and the additive tolerance remains negligible for completions in the low-density tail S. Consequently, RLVR tends to behave like probability sharpeningconcentrating mass around the high-probability modes of qrather than exploring or discovering entirely novel solutions. Overcoming this tendency requires explicit exploration mechanisms or off-policy data sources that intentionally seed mass into new regions. For instance, Xie et al. (2024) proposes an exploration-augmented preference optimization that addresses similar constraints in RL from human feedback (RLHF). In this sense, RLVR inherits both the inductive biases and structural limitations of its initialization. Without deliberate intervention or scaling, it remains confined to the functional expressivity of the base model. Our framework formalizes why RLVR often improves sampling efficiency but rarely produces qualitatively new reasoning capabilities. We further explore these dynamics in the KL-free regime in Sec. 2.3, which clarifies how removing explicit regularization changes support behavior. Proof is provided in Appx. A.3. 2.3 VARIATIONAL AND CONSERVATIVE POLICY UPDATE We now present unified view of the RLVR objective through the lens of variational inference. This reveals why RLVR is inherently conservative: it makes minimal updates to the base distribution while ensuring improved performance. Proposition 2.6 (KL Projection onto Reward-Consistent Distributions) Let (Y) be the probability simplex over the finite output space Y. Define the set of feasible policies that achieve at least target expected reward ρ: Pρ := {p(y x) (Y) Ep[R(x, y)] ρ} . Then the solution to the variational problem, minπPρ KL(π q), is the distribution within Pρ that is closest in KL divergence to the base model. The optimal policy takes the form: π(y x) q(y x) exp(βR(x, y)), where β R0 is the dual variable associated with the reward constraint and β = 0 degenerates to the base policy q. (cid:2)R(x, y)(cid:3) 1 Notably, by standard convex duality, this solution also arises as the optimizer of the entropyβ KL(cid:0)π q(cid:1), which softens the constraint into regularized problem maxπq Eπ penalty. Thus, RLVR can be interpreted either as hard projection onto the closest distribution achieving the reward target, or as soft trade-off that balances expected reward with closeness to the base model. Similar exponential tilting policy improvement oracles have been analyzed in the context of KL-regularized contextual bandits and RLHF (Zhao et al., 2024), though their focus is on sample complexity under coverage. KL-Free Limit. relevant special case is the KL-free limit, where explicit KL regularization is removed (β ) (Wei et al., 2023; Yu et al., 2025; Luo et al., 2025a; Yue et al., 2025b). In this regime, RLVR simplifies to hard-filtered projection onto reward-maximizing completions. Corollary 2.7 (KL-Free Projection) In the limit β , the RLVR update converges to the renormalized restriction of the base model to the correct completion set: lim β πβ(y x) = q(y x) 1{y C} (cid:80) yC q(y x) . 5 Preprint. Together, Prop. 2.6 and Cor. 2.7 illustrate continuum of RLVR behaviorsfrom softly regularized reweighting (small β) to sharply constrained filtering (large β). Even in the KL-free limit, updates remain fundamentally anchored to the base models distribution, preserving relative probabilities within the reward-consistent subset. Consequently, while this projection ensures stable, efficient updates, it inherently limits RLVRs exploratory capacity. As established in Thm. 2.2, RLVR remains confined to the initial support of the base model unless explicit mechanisms introduce meaningful probability mass to new regions. Thus, the variational interpretation clarifies RLVRs strengths in improving precision and efficiency within existing capabilities, alongside its limitations in fundamentally expanding model reasoning. detailed proof is provided in Appx. A.4 and A.5."
        },
        {
            "title": "2.4 ENTROPY–REWARD TRADE-OFF: PRECISION AT THE COST OF ANSWER DIVERSITY",
            "content": "Another structural property of RLVR is its tendency to systematically reduce the entropy of the answer distribution. This behavior arises naturally from reward optimization, which statistically favors sharper distributions concentrated on high-reward completions. While such entropy reduction is beneficial in domains like board games or mathwhere precision is paramountit may also suppress valuable diversity in contexts that benefit from broader coverage or multiple valid outputs, such as story or dialogue generation (Chen et al., 2023) and coding copilots (Peng et al., 2023). Theorem 2.8 (Entropy Reduction and PrecisionCoverage Trade-off) Assume finite output space and define the Shannon entropy of distribution as H[p] := (cid:80) yY p(y x) log p(y x). Then the following statements hold: (a) Entropy reduction. Any RLVR update πθ satisfies with equality only if the reward is constant on the support of q. H[πθ] H[q], (b) Trade-off with coverage. Lower entropy increases sampling precision for small budgets, but for large k, reduces the diversity of explored outputspotentially missing alternative correct completions. This trade-off underpins RLVRs empirical strengths in tasks with narrowly defined optimal solutions such as mathematical proofs or tactical game endgames (where precision is paramount), while also emphasizing the need for explicit diversity mechanisms in more open-ended domains such as code generation, creative writing (Feizi et al., 2023; Ding et al., 2024), or brainstorming (Chang & Li, 2025). Importantly, entropy reduction is not inherently undesirable: when task admits unique correct solution, lower answer-level entropy simply reflects desirable convergence. Importantly, even in multi-solution domains, concentrating mass on narrower set may still be desirable under constrained compute budgets. However, our results show that entropy reduction can still lead to empirical-support shrinkage even in predominantly single-solution domains like math, where RLVR sometimes fails to recover valid completions still accessible to the more diverse base model. This highlights that entropy-induced narrowing is general phenomenon, not limited to multi-solution tasks, underscoring the broader need for explicit exploration or diversity-promoting strategies. Complete proofs are provided in Appx. A.6."
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "3.1 EVIDENCE OF HIDDEN-SUPPORT DYNAMICS Setup. We adopt ProRL (Liu et al., 2025) as our RLVR method due to its robust long-horizon training framework. Starting from DeepSeek-R1-Distill-Qwen-1.5B as the base model, ProRLs Nemotron-Research-Reasoning-Qwen-1.5B leverages GRPO enhanced with decoupled clipping, dynamic sampling, KL divergence regularization, and periodic reference resets to sustain exploration and prevent entropy collapse throughout prolonged RL training. Performance is evaluated across two categories. (1) math reasoning tasks : MATH500 (Hendrycks et al., 2021), Minerva (Lewkowycz et al., 2022), OlympiadBench (He et al., 2024), AIME 2024, (2) non-math reasoning tasks: SimpleQA (Wei et al., 2024) AIME 2025, and AMC 2023. 6 Preprint. Table 1: Empirical-support categorization across math reasoning benchmarks under high sampling budgets. Each completion is categorized by correctness and support status: Preservation indicates the solution is found by both base and ProRL; Shrinkage indicates the base model found it but ProRL did not; Expansion indicates only ProRL found it; and denotes solutions found by neither. The bottom rows report the overall accuracy of each model on the corresponding benchmark. Category Preservation Shrinkage Expansion Accuracy Correctness AIME2024 AIME2025 AMC Math Minerva Olympiad Base ProRL pass@ pass@8192 pass@8192 pass@8192 pass@8192 pass@8192 23 3 0 20 3 0 7 39 1 0 0 494 4 0 2 173 22 0 77 600 26 3 46 Base ProRL 86.7% 76.7% 76.7% 66.7% 100% 97.5% 99.6% 98.8% 71.7% 63.6% 92.7% 89.3% Table 2: Empirical-support categorization across non-math reasoning benchmarks. Category Preservation Shrinkage Expansion Accuracy Correctness SimpleQA LiveBench-R LiveBench-C LiveBench-L SciBench Base ProRL Base ProRL pass@2048 pass@ pass@2048 pass@2048 pass@2048 79 13 3 338 21.2% 18.9% 94 6 0 59 17 7 45 6 5 3 36 100.0% 94.0% 59.4% 51.6% 22.0% 18.0% 616 35 10 94.1% 90.4% (factuality), LiveBench (White et al., 2025) (logical reasoning, coding, language comprehension), SciBench (Wang et al., 2023) (multi-domain scientific problem-solving), and Reasoning Gym (Stojanovski et al., 2025) (cognition, geometry, graph theory, and common games). These benchIn Reasoning Gym, we especially focus on marks contain more general reasoning questions. tasks that ProRL explicitly highlighted as challenging for the base model. For SimpleQA, we use GPT-4.1 (Achiam et al., 2023) as the judge. The sampling is set at = 8192 for math tasks, {1024, 2048, 4096, 8192, 16384} for Reasoning Gym, and = 2048 for non-math datasets, ensuring that any unreachable completion is below pretty low threshold under empirical support of the base model. More detailed implementation is provided in Appx. B. 3.1.1 RESULTS: PREDOMINANT PRESERVATION WITH LIMITED EXPANSION Support preservation dominates. Across most tasks, RLVR primarily sharpens the distribution within the effective support of the base model, aligning with our theoretical guarantees (Thm. 2.2 and 2.5). This is evident on Reasoning Gym tasks such as graph color and palindrome, where RLVR accelerates convergence toward near-perfect pass@k under large sampling budgets  (Fig. 2)  . Heatmaps and overlap counts in Tabs. 1 and 2 further highlight this predominant support preservation: for example, RLVR and the base model jointly recover 600 correct completions on Olympiad and 616 on SciBench, underscoring how RLVR chiefly reweights probability mass within the high-reward regions already represented by the base model. Selective empirical-support expansion. Nonetheless, RLVR does occasionally assign nonnegligible probability mass to completions that were effectively negligible under the base models empirical support, uncovering genuinely new correct solutions. For instance, on OlympiadBench, it discovers 3 additional solutions; on SimpleQA and SciBench, 3 and 10, respectively. Likewise, Reasoning Gym tasks like graph color vertex20 and arc 1d demonstrate striking empiricalsupport expansion  (Fig. 4)  , where RLVR achieves near-perfect pass@k despite the base model struggling even under extensive sampling. These examples suggest that RLVR scaling may, at times, redistribute mass into underexplored solution modes, modestly broadening effective support. Preprint. Figure 2: Pass@k curves on tasks like Graph Coloring, Palindrome Generation, and Advanced Geometry, illustrating RLVRs typical empirical-support preservation. Figure 3: Examples of empirical-support shrinkage on Reasoning Gym tasks such as Leg Counting, Family Relationships, and Power Function. Frequent empirical-support shrinkage. However, we find that empirical-support shrinkagewhere RLVR fails to recover correct completions accessible to the base modelis even more pronounced. This aligns with the entropy-reducing, mode-concentrating effects predicted by Thm. 2.8. On the math benchmarks, RLVR misses 3 solutions on AIME2024, 3 on AIME2025, and experiences sharper losses on Minerva (22) and OlympiadBench (26). Similarly, it forfeits 13 and 35 correct completions on SimpleQA and SciBench, respectively. In Reasoning Gym, tasks such as leg counting, family relationships, and power function illustrate this vividly: RLVRs distributions become markedly sharper  (Fig. 3)  , rapidly saturating pass@k yet failing to explore alternative valid outputs that the more entropic base model uncovered. Perplexity analysis on support constraints. Tab. 3 presents perplexity scores under two complementary settings. When evaluated against external reasoning traces from DeepSeek-R1 and Claude Sonnet 4 with extended thinkinglikely outside the base models supportRLVR shows markedly higher perplexity (e.g., AIME 2024 rising from 8.76 to 14.91), confirming that it cannot assign mass to fundamentally novel solution modes (Thm. 2.2). Note that differences in language style and reasoning format across external references (e.g., Claude vs DeepSeek) also contribute to perplexity gaps, beyond purely structural support constraints. Breakdowns by correctness patterns highlight the precisioncoverage trade-off (Thm. 2.8): in shrinkage cases, ProRLs perplexity rises when it fails to recover solutions still accessible to the base, reflecting entropy-driven concentration. Meanwhile, Preprint. Figure 4: Rare instances of empirical-support expansion under RLVR, as seen in Boxnet, Dice, and Arc 1D tasks. Table 3: Perplexity of reasoning tokens from base and ProRL models across math benchmarks, segmented by correctness patterns and reference types. Top: on problems correctly solved by the base model but not ProRL, perplexity is measured against the base models reasoning traces. Middle: on problems correctly solved by ProRL but not the base, perplexity is measured against ProRLs traces. Bottom: on problems unsolved by both, perplexity is computed against external references (DeepSeek-R1 and Claude Sonnet 4), reflecting each models compatibility with broader solution modes. Correctness Reference Target AIME 2024 AIME 2025 Olympiad Base, ProRL Base Base, ProRL ProRL Base, ProRL DeepSeek-R Claude Sonnet 4 Base ProRL Base ProRL Base ProRL Base ProRL 1.36 1. - - 1.82 2.20 8.76 14.91 1.47 1.84 - - 1.75 2. 6.05 9.76 1.30 1.50 1.52 1.32 1.62 1.94 5.98 9.55 the modest perplexity gaps in rare expansion cases indicate these new completions were actually drawn from the bases long-tail low-density regionsamplified but not truly beyond its support. Overall takeaway: conservative optimization. granular comparison reveals that empiricalsupport shrinkage generally outweighs expansion. Across Minerva and OlympiadBench, RLVR gains only 3 new completions but loses 48 previously found by the base model; on SimpleQA and SciBench, it gains 13 yet forfeits 48. Reasoning Gym presents nuanced picture, with tasks like boxnet and arc 1d showing notable expansion, while others, such as palindrome generation hard, exhibit classic shrinkage patterns. Overall, these findings reinforce that RLVR chiefly acts as sampling reweighting mechanismconcentrating probability mass within the existing representational landscape of the base modeloffering higher precision but limited robust exploration. This resonates with the Temporal Forgetting phenomenon (Li et al., 2025), where fine-tuning often erases paths previously solvable at intermediate stages, affecting up to 56% of final failures. Together, our results underscore RLVRs role as precision enhancer rather than broad driver of novel reasoning discovery. 3.2 ENTROPY REDUCTION AND THE S@K TRADE-OFF Setup. To study how RLVR reshapes the sampling distribution, we examine the base model and RLVR with medium sampling budget = 32 on the math reasoning benchmarks. We quantify changes in the output distribution using two entropy metrics: Token-Level Entropy: Let denote the vocabulary and y(i) = (y(i) (i)) denote the i-th generated sequence of length (i) for 1 . At each timestep t, the model outputs probability distribution p(i) (v) over vocabulary tokens V. The entropy of this distribution = (cid:80) vV p(i) is given by: (i) (v). The average token-level entropy over all (v) log p(i) 2 , . . . , y(i) 1 , y(i) 9 Preprint. Table 4: Summary of avg@32 accuracy, response length, and entropy metrics across math reasoning benchmarks (row colors: RLVR models). RLVR consistently improves accuracy and alters distributional properties. While answer-level entropy consistently decreases, token-level entropy shows more varied behavior across models. base models, Metric Model AIME 2024 AMC 2023 MATH 500 Minerva Olympiad Avg. avg@32 Acc. (%) Response Length Token-Level Entropy Answer-Level Entropy DeepSeek-1.5B ProRL-1.5B DeepSeek-7B AceReason-7B Skywork-7B DeepSeek-14B AceReason-14B Qwen2.5-32B DAPO-32B DeepSeek-1.5B ProRL-1.5B DeepSeek-7B AceReason-7B Skywork-7B DeepSeek-14B AceReason-14B Qwen2.5-32B DAPO-32B DeepSeek-1.5B ProRL-1.5B DeepSeek-7B AceReason-7B Skywork-7B DeepSeek-14B AceReason-14B Qwen2.5-32B DAPO-32B DeepSeek-1.5B ProRL-1.5B DeepSeek-7B AceReason-7B Skywork-7B DeepSeek-14B AceReason-14B Qwen2.5-32B DAPO-32B 31.15 45.62 53.23 65.83 67.40 67.81 77.29 18.12 51.25 16363 7786 13613 10740 15628 11295 13871 1247 6908 0.45 0.47 0.38 0.18 0.14 0.33 0.12 0.17 0.26 2.15 1.24 1.47 0.96 0.97 1.01 0.66 2.37 1.12 72.81 85.70 89.30 95.08 93.59 95.39 98.67 55.23 92.81 9979 6294 6402 5961 8282 5735 7239 874 0.40 0.51 0.34 0.23 0.16 0.30 0.13 0.16 0.19 0.91 0.35 0.36 0.14 0.20 0.14 0.06 1.32 0.09 85.01 92.01 93.95 95.81 95.73 95.28 97.01 75.84 80.75 5700 5070 4125 4313 5735 3781 4622 585 3386 0.42 0.54 0.35 0.27 0.19 0.32 0.15 0.15 0.27 0.46 0.18 0.18 0.11 0.12 0.13 0.07 0.68 0. 32.18 39.27 43.07 45.35 43.81 46.43 47.20 24.55 32.50 8194 6569 5595 6261 8742 4919 7720 3544 5665 0.49 0.55 0.39 0.24 0.17 0.35 0.15 0.28 0.44 1.65 0.90 0.96 0.77 0.80 0.83 0.67 2.27 0.96 51.55 64.56 66.67 73.92 73.05 72.06 77.74 41.40 49.15 11873 6678 8988 7703 12094 8042 10033 881 0.44 0.52 0.38 0.23 0.16 0.33 0.14 0.15 0.30 1.33 0.63 0.80 0.53 0.58 0.59 0.44 1.41 0.63 54.54 65.43 69.24 75.20 74.71 75.39 79.58 43.03 61.29 10422 6479 7745 6995 10096 6755 8697 1426 4989 0.44 0.52 0.37 0.23 0.16 0.33 0.14 0.18 0.29 1.30 0.66 0.75 0.50 0.54 0.54 0.38 1.61 0. sequences and their timesteps is computed as: TokenEntropy = 1 capturing the local uncertainty at each generation step. (cid:80)N i=1 (cid:16) 1 (i) (cid:80)T (i) t=1 (i) (cid:17) , Answer-level Entropy: Let {o(1), . . . , o(N )} denote the answers extracted from each generM } be the unique . Then: j=1 pj log pj. This captures global diversity over output completions, ated sequence y(i) (using NA for incomplete outputs), and let {o answers. Let fj be the frequency of answer AnswerEntropy = (cid:80)M with lower values indicating increased mode collapse. , with empirical probability pj = fj 1, . . . , 3.2.1 RESULTS: PRECISION GAINS, ENTROPY DYNAMICS, AND TRADE-OFFS Consistent gains in precision, but sharper global distributions. As shown in Tab. 4, RLVR consistently improves pass@1 accuracy across all benchmarks, raising average performance from 48.9% to 65.4% for ProRL and from 43.0% to 61.29% for DAPO, which underscores its strength in reweighting probability mass toward high-reward completions and concentrating on likely correct 10 Preprint. answers (Dang et al., 2025). However, this increased precision comes at cost: RLVR systematically reduces answer-level entropy, indicating collapse onto fewer distinct solutions and empirically validating our theoretical prediction (Thm. 2.8) that reward optimization sharpens output distributions around known modes, thereby reducing effective support coverage. Notably, intrinsically harder tasks, such as AIME or Minerva, still exhibit higher absolute answer-level entropy for both the base and RLVR models, suggesting that challenging problems inherently foster broader solution spaces that require exploration over more diverse completions. Decoupled local uncertainty and global diversity. Interestingly, while answer-level entropy consistently declines across all benchmarks, token-level entropy exhibits more varied behavior. In some modelssuch as ProRL and DAPOit increases, suggesting greater local uncertainty during generation, possibly due to longer or more elaborated reasoning chains that introduce additional decision points or forking tokens (Wang et al., 2025). However, this pattern is far from universal: other RLVR models like AceReason and Skywork display similar or even lower token-level entropy relative to their base counterparts, and prior work has documented sharp entropy collapse in early training phases (Cui et al., 2025). More importantly, increased token-level entropy does not imply greater exploration of the output space. Despite appearing more stochastic at the step level, RLVR models frequently converge onto smaller set of final answersreflected in lower answer-level entropy. Notably, even between two models built on the same base (DeepSeek-7B), Skywork-7B shows lower token-level entropy than AceReason-7B, yet exhibits higher answer-level entropy. This contrast highlights that local uncertainty does not reliably predict the diversity of final solutions, revealing critical decoupling between local uncertainty and global diversity. We refer to this phenomenon as local stochasticity without global exploration: the model exhibits variability in generation but ultimately collapses to narrow set of solutions. Thus, token-level entropy should not be conflated with genuine exploratory behavior, and interpreting entropy dynamics in RLVR requires distinguishing between stepwise uncertainty and overall support expansion. Implications. Taken together, these findings reveal fundamental trade-off in RLVR: it improves precision by amplifying high-reward outputs, but simultaneously narrows the diversity of global solutions. This limitation is especially consequential in domains that admit multiple valid answers or benefit from creative reasoning, underscoring the need for explicit exploration mechanisms or diversity-promoting strategies to complement standard RLVR. Moreover, the observed divergence between token-level and answer-level entropy highlights the need for more nuanced interpretation of stochasticity in reward-optimized modelsshowing that precision gains often come at the expense of global diversity, and that maintaining controlled variability is critical for sustaining effective exploration."
        },
        {
            "title": "4 CONCLUSION",
            "content": "We presented unified theoretical and empirical analysis revealing that RLVR primarily acts as conservative sampling reweighting mechanism: it improves precision by sharpening distributions around known high-reward trajectories, yet largely preserves the support of the base model. Importantly, we found that this sharpening does not merely prune incorrect outputsit can also concentrate probability mass on narrower subset of correct solutions, occasionally excluding valid alternatives that the more diverse base model could still recover. This highlights hidden trade-off between enhanced precision and comprehensive reasoning coverage. Notably, the divergence between token-level uncertainty and answer-level diversity also indicates that stepwise stochasticity alone is insufficient for global exploration, motivating future work to explicitly bridge this gap. Our findings suggest that to expand reasoning capabilities beyond the base models scope, RLVR must be coupled with explicit exploration strategies or off-policy mechanisms that seed probability mass into underrepresented regions of the solution space."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical 11 Preprint. report. arXiv preprint arXiv:2303.08774, 2023. Shivam Agarwal, Zimin Zhang, Lifan Yuan, Jiawei Han, and Hao Peng. The unreasonable effectiveness of entropy minimization in llm reasoning. arXiv preprint arXiv:2505.15134, 2025. Chenxin An, Zhihui Xie, Xiaonan Li, Lei Li, Jun Zhang, Shansan Gong, Ming Zhong, Jingjing Xu, Xipeng Qiu, Mingxuan Wang, and Lingpeng Kong. Polaris: post-training recipe for scaling reinforcement learning on advanced reasoning models, 2025. URL https://hkunlp. github.io/blog/2025/Polaris. Sanghwan Bae, Jiwoo Hong, Min Young Lee, Hanbyul Kim, JeongYeon Nam, and Donghyun Kwak. Online difficulty filtering for reasoning oriented reinforcement learning. arXiv preprint arXiv:2504.03380, 2025. Hung-Fu Chang and Tong Li. framework for collaborating large language model tool in brainstorming for triggering creative thoughts. Thinking Skills and Creativity, pp. 101755, 2025. Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Borje Karlsson, Jie Fu, and Yemin Shi. Autoagents: framework for automatic agent generation. arXiv preprint arXiv:2309.17288, 2023. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025. Xingyu Dang, Christina Baek, Zico Kolter, and Aditi Raghunathan. Assessing diversity collapse in reasoning. In Scaling Self-Improving Foundation Models without Human Supervision, 2025. Shuangrui Ding, Zihan Liu, Xiaoyi Dong, Pan Zhang, Rui Qian, Conghui He, Dahua Lin, and Jiaqi Wang. Songcomposer: large language model for lyric and melody composition in song generation. arXiv preprint arXiv:2402.17645, 2024. Soheil Feizi, MohammadTaghi Hajiaghayi, Keivan Rezaei, and Suho Shin. Online advertisements with llms: Opportunities and challenges. arXiv preprint arXiv:2311.07601, 2023. Daya Guo, Dejian Yang, Haowei Zhang, and Junxiao Song. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.07570, 2025. URL https://arxiv.org/abs/2501.07570. Nathan Habib, Clementine Fourrier, Hynek Kydlıˇcek, Thomas Wolf, and Lewis Tunstall. Lighteval: lightweight framework for llm evaluation, 2023. URL https://github.com/ huggingface/lighteval. Andre He, Daniel Fried, and Sean Welleck. Rewarding the unlikely: Lifting grpo beyond distribution sharpening. arXiv preprint arXiv:2506.02355, 2025. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Jian Hu. Reinforce++: simple and efficient approach for aligning large language models. arXiv preprint arXiv:2501.03262, 2025. Aaron Jaech et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Wouter Kool, Herke van Hoof, and Max Welling. Buy 4 reinforce samples, get baseline for free! ICLR 2019 Workshop drlStructPred, 2019. 12 Preprint. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:38433857, 2022. Yuetai Li, Zhangchen Xu, Fengqing Jiang, Bhaskar Ramasubramanian, Luyao Niu, Bill Yuchen Lin, Xiang Yue, and Radha Poovendran. Temporal sampling for forgotten reasoning in llms. arXiv preprint arXiv:2505.20196, 2025. Jiawei Liu and Lingming Zhang. Code-r1: Reproducing r1 for code with reliable rewards. 2025. Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, and Yi Dong. Prorl: Prolonged reinforcement learning expands reasoning boundaries in large language models. arXiv preprint arXiv:2505.24864, 2025. Michael Luo, Sijun Tan, Roy Huang, Ameen Patel, Alpay Ariyak, Qingyang Wu, Xiaoxiang Shi, Rachel Xin, Colin Cai, Maurice Weber, et al. Deepcoder: fully open-source 14b coder at o3-mini level. Notion Blog, 2025a. Michael Luo, Sijun Tan, Roy Huang, Xiaoxiang Shi, Rachel Xin, Colin Cai, Ameen Patel, Alpay Ariyak, Qingyang Wu, Ce Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepcoder: fully open-source 14b coder at o3-mini level. https://pretty-radio-b75.notion.site/DeepCoderA-Fully-Open-Source-14B-Coder-at-O3-mini-Level-1cf81902c14680b3bee5eb349a512a51, 2025b. Notion Blog. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl, 2025c. Notion Blog. Lu Ma, Hao Liang, Meiyi Qiang, Lexiang Tang, Xiaochen Ma, Zhen Hao Wong, Junbo Niu, Chengyu Shen, Runming He, Bin Cui, et al. Learning what reinforcement learning cant: Interleaved online fine-tuning for hardest questions. arXiv preprint arXiv:2506.07527, 2025. Sida Peng, Eirini Kalliamvakou, Peter Cihon, and Mert Demirer. The impact of ai on developer productivity: Evidence from github copilot. arXiv preprint arXiv:2302.06590, 2023. Mihir Prabhudesai, Lili Chen, Alex Ippoliti, Katerina Fragkiadaki, Hao Liu, and Deepak Pathak. Maximizing confidence alone improves reasoning. arXiv preprint arXiv:2505.22660, 2025. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Sheikh Shafayat, Fahim Tajwar, Ruslan Salakhutdinov, Jeff Schneider, and Andrea Zanette. Can large reasoning models self-train? arXiv preprint arXiv:2505.21444, 2025. Darsh J. Shah et al. Rethinking reflection in pre-training. arXiv preprint arXiv:2504.04022, 2025. Rulin Shao, Shuyue Stella Li, Rui Xin, Scott Geng, Yiping Wang, Sewoong Oh, Simon Shaolei Du, Nathan Lambert, Sewon Min, Ranjay Krishna, Yulia Tsvetkov, Hannaneh Hajishirzi, Pang Wei Koh, and Luke Zettlemoyer. Spurious rewards: Rethinking training signals in rlvr, 2025. Notion Blog. Zafir Stojanovski, Oliver Stanley, Joe Sharratt, Richard Jones, Abdulhakeem Adefioye, Jean Kaddour, and Andreas Kopf. Reasoning gym: Reasoning environments for reinforcement learning with verifiable rewards, 2025. URL https://arxiv.org/abs/2505.24760. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939, 2025. 13 Preprint. Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. Scibench: Evaluating college-level scientific problem-solving abilities of large language models. arXiv preprint arXiv:2307.10635, 2023. Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. Measuring short-form factuality in large language models. arXiv preprint arXiv:2411.04368, 2024. Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lu, Rui Hu, et al. Skywork: more open bilingual foundation model. arXiv preprint arXiv:2310.19341, 2023. Colin White, Samuel Dooley, Manley Roberts, et al. Livebench: challenging, contaminationlimited llm benchmark. In International Conference on Learning Representations (ICLR), 2025. Tengyang Xie, Dylan Foster, Akshay Krishnamurthy, Corby Rosset, Ahmed Awadallah, and Alexander Rakhlin. Exploratory preference optimization: Harnessing implicit q*-approximation for sample-efficient rlhf. arXiv preprint arXiv:2405.21046, 2024. Wei Xiong, Jiarui Yao, Yuhui Xu, Bo Pang, Lei Wang, Doyen Sahoo, Junnan Li, Nan Jiang, Tong Zhang, Caiming Xiong, et al. minimalist approach to llm reasoning: from rejection sampling to reinforce. arXiv preprint arXiv:2504.11343, 2025. Yixuan Even Xu, Yash Savani, Fei Fang, and Zico Kolter. Not all rollouts are useful: Down-sampling rollouts in llm reinforcement learning. arXiv preprint arXiv:2504.13818, 2025. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025a. URL https://arxiv.org/abs/2504.13837. Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, et al. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025b. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerlzoo: Investigating and taming zero reinforcement learning for open base models in the wild, 2025. URL https://arxiv.org/abs/2503.18892. Qingyang Zhang, Haitao Wu, Changqing Zhang, Peilin Zhao, and Yatao Bian. Right question is already half the answer: Fully unsupervised llm reasoning incentivization. arXiv preprint arXiv:2504.05812, 2025a. Xiaojiang Zhang et al. Srpo: cross-domain implementation of large-scale reinforcement learning on llm. arXiv preprint arXiv:2504.14286, 2025b. Andrew Zhao et al. Absolute zero: Reinforced self-play reasoning with zero data. arXiv preprint arXiv:2505.03335, 2025a. Heyang Zhao, Chenlu Ye, Quanquan Gu, and Tong Zhang. Sharp analysis for kl-regularized contextual bandits and rlhf. arXiv preprint arXiv:2411.04625, 2024. Rosie Zhao et al. Echo chamber: Rl post-training amplifies behaviors learned in pretraining. arXiv preprint arXiv:2504.07912, 2025b. Xuandong Zhao, Zhewei Kang, Aosong Feng, Sergey Levine, and Dawn Song. Learning to reason without external rewards. arXiv preprint arXiv:2505.19590, 2025c. Xinyu Zhu, Mengzhou Xia, Zhepei Wei, Wei-Lin Chen, Danqi Chen, and Yu Meng. The surprising effectiveness of negative reinforcement in llm reasoning. arXiv preprint arXiv:2506.01347, 2025. Yuxin Zuo, Kaiyan Zhang, Shang Qu, Li Sheng, Xuekai Zhu, Biqing Qi, Youbang Sun, Ganqu Cui, Ning Ding, and Bowen Zhou. Ttrl: Test-time reinforcement learning. arXiv preprint arXiv:2504.16084, 2025. Preprint."
        },
        {
            "title": "A MATHEMATICAL ANALYSIS",
            "content": "A.1 PROOF OF THEOREM 2.2 Base case. By construction we initialize the RLVR policy to the base model:"
        },
        {
            "title": "Hence",
            "content": "πθ0(y x) = q(y x). supp(cid:0)πθ0 ( x)(cid:1) = supp(cid:0)q( x)(cid:1). Inductive step. Assume that at some iteration θ we have πθ(y x) = 0 for particular y. All standard policy-gradient updates (e.g. REINFORCE, PPO, GRPO) take the form θ = θ + η θEyπθ(x) (cid:104) R(x, y) β1 log πθ(yx) q(yx) (cid:105) , where η is the learning rate. Since the outer expectation is over πθ, any with πθ(y x) = 0 is never sampled and thus contributes no gradient component. Therefore and the support of πθ remains subset of that of q. πθ(y x) = 0, Conclusion. By induction, none of the updates can introduce positive probability mass on any for which q(y x) = 0. Equivalently, supp(cid:0)πθ( x)(cid:1) supp(cid:0)q( x)(cid:1), indicating that any correct solution with q(y x) = 0 remains unreachable by the RLVR policy. A.2 PROOF OF COROLLARY 2.3 From Thm. 2.2, support preservation implies Thus, for any C, supp(πθ(x)) supp(q(x)). πθ(yx) > 0 = q(yx) > 0. Define the total mass on correct completions by πθ(C) = Pr yπθ [R(x, y) = 1], q(C) = Pr yq [R(x, y) = 1]. Here, we assume that samples are independent across the different draws of LLMs; otherwise, we can only assert an upper bound using union bounds. As , the pass@k success probability to be written as pass@kπθ (x) = 1 (1 πθ(C))k (cid:40)1, πθ(C) > 0, 0, πθ(C) = 0, and similarly for q. Because support preservation ensures that any correct completion reachable under πθ must also be reachable under q, Hence, the asymptotic success probability satisfies πθ(C) > 0 = q(C) > 0. lim pass@kπθ (x) lim pass@kq(x). 15 Preprint. A.3 PROOF OF THEOREM 2.5 We consider the one-step RLVR policy update given by: πθ( x) = (1 γ) (cid:101)πθ( x) + γ πe( x), where (cid:101)πθ(y x) πθ(y x) exp(β R(x, y)) is the exponentially tilted distribution. πe( x) is an arbitrary exploration distribution, γ [0, 1] is the mixing coefficient, and β > 0 is the inverse temperature. We aim to bound πθ(y x) for any S. By the RLVR update rule and since πe(y x) 1, πθ(y x) = (1 γ) (cid:101)πθ(y x) + γ πe(y x) (1 γ) (cid:101)πθ(y x) + γ. Bound on (cid:101)πθ(y). Let = (cid:80) exponential tilting. Since exp(β r) 1, (cid:80) R(x, y) [0, 1], it implies yY πθ(y x) exp(β R(x, y)) be the normalizing constant of the yY πθ(y x) = 1. Then for all and (cid:101)πθ(y x) = πθ(y x)eβR(x,y) πθ(y x)eβR(x,y) πθ(y x)eβ. Tail mass of the current policy. Now we bound πθ(y x) in terms of q(y x). Using Pinskers inequality: KL(πθq) δ πθ q1 2δ (cid:12) (cid:12)πθ(y x) q(y x)(cid:12) (cid:12) Thus, for all S, πθ(y x) q(y x) + 2δ τ + 2δ. Final bound. Combining the above gives and so (cid:101)πθ(y x) eβ (τ + 2δ), πθ(y x) (1 γ) eβ(τ + 2δ) + γ . 2δ. A.4 PROOF OF PROPOSITION 2.6 We provide two closely related derivations to illuminate the same optimal solution from both hard-constrained and soft-regularized perspective. Convexity of Feasible Set Pρ. We first prove the convexity of Pρ. (cid:110) (Y) : (cid:80) , where (Y) denotes the probability simplex over Y. p(y)R(x, y) ρ (cid:111) Recall Pρ = Take any two distributions p1, p2 Pρ and let λ [0, 1]. Consider the convex combination pλ := λp1 + (1 λ)p2. Since (Y) is convex, we have pλ (Y). Next, because p1, p2 Pρ, it follows that (cid:88) p1(y)R(x, y) ρ and (cid:88) p2(y)R(x, y) ρ. Thus, (cid:88) pλ(y)R(x, y) = λ (cid:88) p1(y)R(x, y) + (1 λ) (cid:88) Hence pλ Pρ. This shows that Pρ is convex. 16 p2(y)R(x, y) λρ + (1 λ)ρ = ρ. Preprint. Convexity, existence, and strong duality. We then verify the foundational properties of the optimization problem. Recall we wish to solve min πPρ KL(πq), where Pρ = π (Y) : (cid:40) (cid:41) π(y)R(x, y) ρ . (cid:88) The objective function KL(πq) is convex in π over the probability simplex (Y), since relative entropy is jointly convex and thus convex in π for fixed q. The feasible set Pρ is also convex. Moreover, if there exists strictly feasible distribution π such that (cid:80) π(y)R(x, y) > ρ, then by Slaters condition, strong duality holds. This guarantees that the optimal value of the primal problem equals the optimal value of its Lagrangian dual, and the Karush-Kuhn-Tucker (KKT) conditions characterize the optimal solution. In typical applicationswhere arises from softmax-based models with full supportsuch strictly feasible distributions exist, ensuring that our subsequent Lagrangian approach is valid. 1) Hard-constrained formulation (projection perspective). Consider the optimization problem: min π KL(πq) s.t. Eπ[R(x, y)] ρ, (cid:88) π(y x) = 1, π(y x) 0. Using the method of Lagrange multipliers, the Lagrangian is: L(π, β, λ) = π(y x) log (cid:88) π(y x) q(y x) β (cid:32) (cid:88) π(y x)R(x, y) ρ + λ (cid:33) (cid:33) π(y x) 1 . (cid:32) (cid:88) Here, we compute the derivative concerning π(y x) for fixed multipliers, thereby finding the stationary points of the Lagrangian. Specifically, we take derivative with respect to π(y x) and set it to zero: log Solving for π yields: π(y x) q(y x) + 1 βR(x, y) + λ = 0. π(y x) q(y x) exp(βR(x, y)). 2) Soft-regularized formulation (dual perspective). Alternatively, assume RLVR solves the entropy-regularized objective πθ = arg max πq Eyπ[R(x, y)] β1KL(π q), for some inverse temperature parameter β > 0. Here, the constraint π denotes that π is absolutely continuous with respect to q, meaning π(y x) > 0 only if q(y x) > 0.1The objective is equivalent to the following minimization: πθ = arg min π(Y) KL(π q) β Eyπ[R(x, y)]. The Lagrangian becomes L(π, λ) = π(y) log (cid:88) yY π(y) q(y) β (cid:88) yY π(y)R(x, y) + λ π(y) 1 , (cid:88) yY where λ is the Lagrange multiplier enforcing the normalization constraint. Taking the derivative with respect to π(y) and setting it to zero: π(y) = log π(y) q(y) + 1 βR(x, y) + λ = 0. 1Formally, absolute continuity π ensures that the KL divergence KL(π q) is finite. If π assigns positive mass to any output that assigns zero probability, the divergence becomes infinite. This condition also enforces support preservation: supp(π) supp(q). 17 Preprint. Solving for π(y) gives: π(y) = q(y) exp (βR(x, y) λ 1) . Letting the normalization constant be: = (cid:88) yY q(y) exp(βR(x, y)), we absorb constants into and write: πθ(y x) = q(y x) exp(βR(x, y)) . Both derivations recover the same exponentially tilted distribution that emphasizes high-reward completions relative to the base model. In the hard-constrained view, β is Lagrange multiplier tuned to meet the target reward ρ; in the soft-regularized view, β sets the strength of the trade-off between reward and divergence. This completes the constructive proof of Prop. 2.6. A.5 PROOF OF COROLLARY 2.7 Since R(x, y) {0, 1}, we have exp(cid:0)βR(x, y)(cid:1) = (cid:40)eβ if R(x, y) = 1, 1 if R(x, y) = 0. Thus the RLVR distribution becomes πβ(y x) = q(y x) exp(cid:0)βR(x, y)(cid:1) Zβ(x) = q(y x) (cid:2)eβ1{R(x, y) = 1} + 1{R(x, y) = 0}(cid:3) Zβ(x) , where Zβ(x) = eβ (cid:88) q(y x) + (cid:88) q(y x). y:R(x,y)=1 y:R(x,y)= As β , the term with eβ dominates whenever there exists at least one with R(x, y) = 1. Thus Zβ(x) eβ (cid:88) q(y x). yC Similarly, in the numerator, we have q(y x) exp(cid:0)βR(x, y)(cid:1) = (cid:40)q(y x) eβ if C, q(y x) otherwise. Dividing by Zβ(x) and taking β , the probabilities assigned to with R(x, y) = 0 vanish: πβ(y x) q(y x) eβ yC q(y x) eβ (cid:80) = q(y x) yC q(y x) (cid:80) 0 if C, otherwise. Thus we obtain lim β πβ(y x) = q(y x) 1{y C} (cid:80) yC q(y x) , 18 Preprint. A.6 PROOF OF THEOREM 2.8 (a) Entropy reduction. Consider the exponentially tilted distribution πθ(y x) = q(y x) exp(βR(x, y)) , with = (cid:88) yY q(y x) exp(βR(x, y)). By standard properties of KL divergence, KL(πθq) = πθ(y x) log πθ(y x) q(y x) 0. (cid:88) y"
        },
        {
            "title": "Rearranging gives",
            "content": "H[πθ] = H[q] KL(πθq) H[q]. Thus, any such RLVR update decreases entropy relative to the base distribution, unless the reward is constant (in which case πθ = q). (b) Trade-off with diversity at different sampling budgets. The RLVR-trained policy sharpens the probability mass around high-reward completions. Explicitly, where β > 0 controls concentration. πθ(y x) q(y x) exp(βR(x, y)), Small sampling budgets (k = 1): The increased probability on high-reward outputs generally improves single-shot success rates. Formally, pass@1πθ (x) = (cid:88) πθ(y x) > (cid:88) q(y x) = pass@1q(x), y:R(x,y)= y:R(x,y)=1 provided the reweighting boosts correct completions relative to incorrect ones. Large sampling budgets (k 1): However, reduced entropy leads to concentration on fewer modes. As β grows, πθ may collapse onto narrow subset of correct completions, neglecting other valid solutions accessible under the more dispersed q. Thus, lim sup pass@kπθ (x) < lim sup pass@kq(x), under typical conditions of entropy reduction and selective mass shifting. Loss of tail coverage: In particular, if there exist rare but correct completions that have small mass under but are further downweighted (or eliminated) by the tilting, then the total mass on correct completions can decrease: πθ(C) < q(C), = {y : R(x, y) = 1}. This restricts the long-run probability of recovering diverse solutions via large sampling. Conclusion. This establishes trade-off: RLVR improves sampling efficiency by concentrating probability on high-reward outputs (increasing pass@1), but this comes at the cost of reduced entropy and narrower exploration of the solution space (potentially lowering pass@k for large k). Empirical studies confirm this phenomenon in settings like code generation and symbolic reasoning, where many semantically distinct correct completions exist. A.7 ESTIMATING THE SAMPLING THRESHOLD ϵ FROM S@K We provide statistical analysis of the threshold ϵ in the pass@k sampling. Suppose we sample times from model π( x), and let be correct completion with unknown probability = π(y x). If is not observed in any of those samples, we can upper bound using the following argument. 19 Preprint. The probability of not sampling in single trial is 1 p, so the probability of missing it in all independent trials is (1 p)k. To ensure this event occurs with probability at most ζ, we solve: Taking logarithms of both sides: (1 p)k ζ. log(1 p) log ζ. Using the inequality log(1 p) for (0, 1), we get: (p) log ζ log ζ . Consequently, if the correct completion is not observed in samples, then with confidence 1 ζ, its probability satisfies: π(y x) log ζ . Example. then If = 8096 in the math reasoning tasks and we desire 95% confidence (i.e., ζ = 0.05), π(y x) log(0.05) 8096 2.996 8096 3.70 104."
        },
        {
            "title": "B EXPERIMENTAL DETAILS",
            "content": "We provide comprehensive details of the experimental setup, including dataset descriptions and evaluation methodologies. key aspect of our evaluation approach is the answer processing enhancement framework for Reasoning Gym, which addresses format compatibility challenges between base and ProRL models to ensure fair evaluation. B.1 EVALUATION SETTINGS We employed vLLM (Kwon et al., 2023) as the inference backend. For all models, we utilized sampling temperature of 0.6, top value of 0.95, and maximum response length of 32000. B.2 DATASETS Math benchmarks. We utilized the complete datasets from MATH500 (Hendrycks et al., 2021), Minerva (Lewkowycz et al., 2022), OlympiadBench (He et al., 2024), AIME 2024, AIME 2025, and AMC 2023 for evaluation. Non-math benchmarks. For SimpleQA (Wei et al., 2024), we uniformly sampled 10% of the original dataset (433 samples) to enable efficient large-scale evaluation under high-pass conditions. For LiveBench (White et al., 2025), we used the 2024-11-25 version available on HuggingFace. To ensure unambiguous evaluation, we focused exclusively on tasks with binary correct/incorrect judgments and excluded tasks involving intermediate floating-point judgments, as these lack clear correctness criteria. Based on this selection criterion, we evaluated the following subsets: web of lies v2 and spatial subsets for Reasoning tasks (LiveBench-R), the typos subset for Language tasks (LiveBench-L), and all available data for Coding tasks (LiveBench-C). For SciBench (Wang et al., 2023), we evaluated on the complete dataset. Reasoning Gym. For Reasoning Gym (Stojanovski et al., 2025), we employ the easy set from the version updated after commit 17a8431 in its repository as our default task configuration. This choice ensures consistency with the default task configuration used in prior evaluations, maintaining comparable experimental conditions. Additionally, we utilize the hard set as our challenging evaluation benchmark for further evaluations. 20 Preprint. B.3 ANSWER PROCESSING ENHANCEMENT IN REASONING GYM We identified significant evaluation challenges when testing the base model on Reasoning Gym. The ProRL model, having been trained on Reasoning Gym data, predominantly produces responses that conform to the expected format, leading to much higher accuracy scores. In contrast, the base model struggled with format adherence due to insufficiently detailed prompts, and its limited 1.5B parameter capacity made it particularly susceptible to evaluation inconsistencies. To address these issues, we enhanced both the answer extraction protocol and prompt design to ensure fair and objective accuracy assessments across both models. B.3.1 GENERAL ANSWER EXTRACTION PROTOCOL First, we enhanced the answer extraction protocol with hierarchical, priority-based extraction mechanism that processes responses through multiple fallback levels. Each level attempts to capture the models intended answer, and successful extraction at any level bypasses subsequent processing steps. The strategy first attempts to extract content using the Reasoning Gyms extract answer() function, which captures answers within <answer></answer> tags. This approach receives the highest priority since these tags represent Reasoning Gyms default format. When this method fails, the system searches for content within the final boxed{} formatting. For dice tasks using the base model, failed extract answer() attempts trigger additional processing through Lighteval (Habib et al., 2023)s math normalizer() function. This function handles boxed{} capture and converts a/b fractions to LATEX format frac{a}{b}. When extract answer() successfully captures a/b fraction answers, the system applies Lightevals fix slash b() function to achieve the same LATEX conversion. For non-dice tasks or when using ProRL models, failed extract answer() attempts utilize Lightevals last boxed only string() and remove boxed() functions. These functions locate content within the final boxed{}, primarily addressing cases where base model prompt modifications shifted from answer tags to boxed formatting. As final fallback, the system extracts content following </think> tags when all previous methods fail and the response contains these markers. This safety mechanism captures base model responses that ignore formatting requirements in lengthy tasks. B.3.2 TASK-SPECIFIC PROCESSING MODIFICATIONS Our core answer processing pipeline applies to both models, with additional processing steps designed primarily to address format compatibility issues commonly encountered with base model responses. Specifically, the processing logic for each task is enhanced as follows: dice The ground truth for dice tasks uses a/b fraction format. Base models frequently express fractions in LATEX format, requiring format standardization for accurate evaluation. For base models only, we convert ground truth fractions from a/b format to LATEX format frac{a}{b} to ensure both model answers and ground truth use consistent LATEX formatting. ProRL dice processing maintains a/b formatting for both model answers and ground truth, leveraging the dice samples present in its training data. prime actorization The ground truth format requires answers to be combinations of numbers and multiplication symbol (i.e., ) only. We implement three key modifications to ensure compatibility with this requirement. First, we standardize LATEX multiplication symbols by replacing times with to meet the evaluation requirements, as base models frequently use LATEX multiplication symbols instead of standard multiplication signs. Second, we expand LATEX exponentiation by converting formats like aˆb into repeated multiplication (a . . . for iterations), preventing errors when base models consolidate repeated factors into exponential notation. Third, we process response equations by retaining only right-side content when answers contain equals signs, transforming responses like 561 = 3 11 17 to 3 11 17 to eliminate question restatement that base models commonly include. 21 Preprint. palindrome generation The ground truth format expects palindromic character strings (sequences that read the same forwards and backwards). We remove excess whitespace to address frequent spacing issues in base model responses. This transformation converts spaced responses like g k to khgaghk, preventing string reversibility judgment failures that occur when spaces interfere with palindrome verification. advanced geometry The ground truth format requires floating-point numbers. Our processing includes three main steps to handle LATEX formatting issues commonly produced by base models. First, we remove redundant LATEX expressions by eliminating left and right markers while converting ˆcirc to symbol, addressing base models tendency to use LATEX for brackets and degree symbols. Second, we convert LATEX numerical expressions, transforming fractions frac{a}{b} and other LATEX formats (sqrt{a}, sin{a}, log{a}, pi, etc.) into three-decimal floating-point numbers using the latex2sympy2 extended librarys latex2sympy() function. Third, we evaluate arithmetic expressions containing radicals (such as 2 4 3) by converting them into three-decimal floating-point numbers using Pythons built-in mathematical functions, handling cases where base models output final results as arithmetic expressions rather than computed values. 16 + 5 power unction The ground truth format uses e-notation scientific notation. We convert mixed LATEX and arithmetic symbol scientific notation to ensure format consistency. The system transforms patterns like 2.361016 or 1.5105 to e-notation format (-2.36e-16, 1.5e5), preventing numerically correct but format-incompatible evaluation errors when base models use mixed LATEX and arithmetic symbols for scientific notation. arc 1d The ground truth format requires space-separated digit sequences. We handle two types of responses to meet this grid format requirement. For pure numerical responses, we insert spaces between consecutive digits, converting sequences like 22220000000000000000111 to 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1. For mixed numerical and textual responses, we extract digits and insert spaces, transforming LATEX grid formats like begin{array}{cccc} 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 7 & 3 & 0 & 0 & 4 & 6 end{array} to 0 0 0 0 0 0 0 0 7 3 0 0 4 6, addressing base models tendency to output correct answers in LATEX grid format. boxnet The ground truth format requires dictionary list formatting [{key: value}, ...]. We implement comprehensive JSON format cleaning to meet these evaluation requirements. Our processing pipeline includes several steps: rejecting pure numerical responses to prevent non-JSON format interference; removing JSON markdown wrappers that eliminate json {content} markers; converting single dictionaries to single-element dictionary lists (dict [dict]); and filtering illegal elements by removing non-dictionary components from JSON lists. Additionally, we clean nested structure values within individual dictionary entries. For nested [value1, value2, ...]}, lists, we extract the first element as the value ([{key1: ...] [{key1: value1}, ...]). For nested dictionaries, we select matching key values when available ([{key1: value2, ...}}, ...] [{key1: value1}, ...]) or default to the first element value when keys dont match value2}, ([{key1: ...]). These modifications preserve model response content to the maximum extent while ensuring ground truth format compliance. value3}}, ...] [{key1: {key2: value2, key3: value1, key2: {key1: B.4 ENTROPY ANALYSIS In entropy analysis, we configure the models with sampling temperature of 0.6, top Setup. value of 0.95, and maximum response length of 32000 tokens to balance response diversity and quality. Each model generates 32 completions per problem following the avg@32 evaluation protocol, and all reported metrics (accuracy, response length, token-level entropy, and answer-level entropy) are averaged across these 32 completions and across all test problems in each benchmark. Models. We evaluate diverse set of reasoning models to understand the entropy characteristics across different training paradigms and parameter scales, as summarized in the following table. 22 Preprint. Name DeepSeek-1.5B ProRL-1.5B DeepSeek-7B AceReason-7B Skywork-7B Table 5: Models evaluated in the entropy analysis. Full Model Name Type Parameters DeepSeek-R1-Distill-Qwen-1.5B Base Nemotron-Research-Reasoning-Qwen-1.5B RLVR DeepSeek-R1-Distill-Qwen-7B AceReason-Nemotron-7B Skywork-OR1-7B Base RLVR RLVR DeepSeek-14B AceReason-14B AceReason-Nemotron-14B DeepSeek-R1-Distill-Qwen-14B Qwen2.5-32B DAPO-32B Qwen2.5-32B DAPO-Qwen-32B Base RLVR Base RLVR 1.5B 1.5B 7B 7B 7B 14B 14B 32B 32B Entropy Computation. For token-level entropy computation, we employ teacher-forcing to obtain probability estimates. Specifically, after generating the 32 completions with the specified sampling parameters, we feed each generated sequence back to the model and perform single forward pass to compute the probability distribution over the vocabulary at each token position. Answer-level entropy is computed by first extracting the final answer from each completion using Lighteval (Habib et al., 2023), then calculating the entropy over the distribution of unique answers across the 32 completions. This approach allows us to compute both token-level and answer-level entropy directly from the models probability distributions without introducing additional sampling variance."
        },
        {
            "title": "C PRACTICAL ALGORITHMIC PATTERNS EXPLAINED BY RLVR THEORY",
            "content": "Recent methods in RLVR often utilize data-filtering strategies and self-supervised reward construction techniques to enhance training stability and reasoning capabilities. While empirically motivated, these techniques can be well understood through our RLVR theoretical framework. C.1 PROMPT FILTERING AND SELECTION HEURISTICS Several methods (Bae et al., 2025; Zhu et al., 2025) incorporate prompt selection and filtering heuristics to enhance training efficiency. common strategy is to dynamically filter or down-sample prompts that yield only incorrect completions (ACC = 0), thereby avoiding the instability and inefficiency these introduce. For example, Reinforce-Rej (Xiong et al., 2025) reported that retaining such prompts led to high gradient variance and degraded KL efficiency. PODS (Xu et al., 2025) advances this by actively sampling prompts with the highest reward variance to foster strong contrastive signals. Techniques like DAPOs Clip-Higher ensure each mini-batch contains balanced mix of correct and incorrect completions (0 < ACC < 1), sustaining reward variance and diversity. Meanwhile, Polaris (An et al., 2025) and SRPO (Zhang et al., 2025b) further refine selection by excluding trivial prompts (ACC = 1) that offer little gradient information. Though initially inspired by GRPO, which implicitly nullifies gradients on all-wrong prompts, these designs closely reflect our theoretical findings: Thm. 2.2 shows zero-accuracy prompts provide no useful signal, Thm. 2.8 warns that over-emphasizing easy prompts risks entropy collapse and diminished pass@k, and Prop. 2.6 underscores that meaningful updates rely on in-support reward variability. C.2 SELF-SUPERVISED BOOTSTRAP LEARNING Recent studies (Prabhudesai et al., 2025; Shao et al., 2025) explore self-supervised RL techniques that improve reasoning without external ground-truth labels. Unlike traditional RLVR relying on externally verifiable rewards, they build intrinsic reward signals from the models own outputs, leveraging internal consistency, semantic coherence, or self-play. For example, EMPO (Zhang et al., 2025a) minimizes semantic entropy across self-generated clusters, RLIF (Zhao et al., 2025c) uses model confidence scores (INTUITOR) as intrinsic rewards, SRT (Shafayat et al., 2025) and TTRL (Zuo et al., 2025) employ majority voting among completions, Absolute Zero (Zhao et al., 2025a) adopts self-play mechanism guided by environment feedback, and EM-RL (Agarwal et al., 2025) combines entropy regularization with gradient alignment. Even random intrinsic rewards can Preprint. yield improvements by exploiting model inductive biases (Shao et al., 2025). These approaches succeed by aligning rewards with natural structural redundancies in model outputs. EMPO and RLIF favor internally coherent or confident completions, SRT and TTRL reinforce consensus, Absolute Zero evolves proposal and solution generations through self-play. Viewed through our lens, they implicitly respect theoretical constraints: consistent with Thm. 2.2, they remain within the base models support through sampling or clustering; aligned with Thm. 2.8, they systematically reduce entropy, sharpening distributions over promising modes; and as shown by Prop. 2.6, they maintain stability via entropy-regularized or gradient-constrained updates."
        }
    ],
    "affiliations": [
        "RIKEN AIP",
        "Stanford University",
        "University of Tokyo",
        "University of Washington"
    ]
}