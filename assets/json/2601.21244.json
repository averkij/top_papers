{
    "paper_title": "Less Noise, More Voice: Reinforcement Learning for Reasoning via Instruction Purification",
    "authors": [
        "Yiju Guo",
        "Tianyi Hu",
        "Zexu Sun",
        "Yankai Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has advanced LLM reasoning, but remains constrained by inefficient exploration under limited rollout budgets, leading to low sampling success and unstable training in complex tasks. We find that many exploration failures arise not from problem difficulty, but from a small number of prompt tokens that introduce interference. Building on this insight, we propose the Less Noise Sampling Framework (LENS), which first prompts by identifying and removing interference tokens. then transfers successful rollouts from the purification process to supervise policy optimization on the original noisy prompts, enabling the model to learn to ignore interference in the real-world, noisy prompting settings. Experimental results show that LENS significantly outperforms GRPO, delivering higher performance and faster convergence, with a 3.88% average gain and over 1.6$\\times$ speedup. Our work highlights the critical role of pruning interference tokens in improving rollout efficiency, offering a new perspective for RLVR research."
        },
        {
            "title": "Start",
            "content": "Less Noise, More Voice: Reinforcement Learning for Reasoning via Instruction Purification"
        },
        {
            "title": "Yiju Guo",
            "content": ", Tianyi Hu (cid:13) (cid:11) , Zexu Sun> , Yankai Lin (cid:11) (cid:66) (cid:11) Gaoling School of Artificial Intelligence, Renmin University of China Department of Computer Science, Aarhus University > Baidu Inc. (cid:66){yijuguo, yankailin}@ruc.edu.cn (cid:13) 6 2 0 2 2 ] . [ 2 4 4 2 1 2 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has advanced LLM reasoning, but remains constrained by inefficient exploration under limited rollout budgets, leading to low sampling success and unstable training in complex tasks. We find that many exploration failures arise not from problem difficulty, but from small number of prompt tokens that introduce interference. Building on this insight, we propose the Less Noise Sampling Framework (LENS), which first prompts by identifying and removing interference tokens. then transfers successful rollouts from the purification process to supervise policy optimization on the original noisy prompts, enabling the model to learn to ignore interference in the real-world, noisy prompting settings. Experimental results show that LENS significantly outperforms GRPO, delivering higher performance and faster convergence, with 3.88% average gain and over 1.6 speedup. Our work highlights the critical role of pruning interference tokens in improving rollout efficiency, offering new perspective for RLVR research."
        },
        {
            "title": "Introduction",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR), such as GRPO (Shao et al., 2024a), has significantly advanced the reasoning capabilities of large language models (LLMs). However, RLVR fundamentally relies on sampling correct rollouts to generate informative learning signals (Yu et al., 2025; Zheng et al., 2025a). In complex reasoning tasks, reward sparsity arises from long-horizon decision making with delayed and binary feedback. When combined with high-dimensional action space, correct rollouts become exceedingly rare (Figure 2a), leading to lack of positive samples and causing training to collapse or become highly inefficient (Hare, 2019). Figure 1: An example of interference token purification: Removing few interference tokens corrects the reasoning rollout and turns it into successful one. To mitigate these issues, recent works have primarily followed two directions: (1) scaling exploration by increasing rollout (Xu et al., 2025; Yang et al., 2025b; Zhan et al., 2025; Xiong et al., 2025b), and (2) filtering zero-variance prompts (Yu et al., 2025; Zheng et al., 2025a). However, the former has substantially higher computational cost without improving efficiency, while the latter sacrifices exploration on challenging samples, limiting the models ability to solve complex problems. As result, neither approach truly addresses the core issue of inefficient exploration on challenging samples. To address this, we investigate why the model fails to explore successful rollouts. Through finegrained token-level analysis, surprisingly, we find that many failures arise not from problem difficulty, but from few (< 5%) tokens that introduce excessive interference, as shown in Figure 1 and Figure 2b. We define tokens that are likely to cause failures as Interference Tokens. Simply pruning these tokens improves rollout accuracy on previously failed DeepMath (He et al., 2025) samples by over 20% across all the model families (Figure 2c). Building on these insights, we introduce Less Noise Sampling Framework (LENS), an online selective rollout framework that improves high- (cid:66) Corresponding author: Yankai Lin. Figure 2: (a) Zero-Reward Prompt Analysis: Comparison of the zero-reward prompt ratio across different models and rollout sizes (n). LENS significantly reduces the proportion of zero-reward samples compared to GRPO, enhancing training efficiency. (b) Distribution of token-level Interference Scores (log scale): Only few tokens exhibit high interference. (c) Rollout Accuracy Improvement: Removing these interference tokens leads to an improvement in rollout success rates (Average@8). quality evolution by extracting informative learning signals from low-success prompts. In the first stage, LENS identifies and removes interference tokens within low-success prompts via interference score (Figure 2b), producing prompts that yield higher proportion of successful rollouts. In the second stage, LENS performs transfer from the purification process to the original noisy setting: successful rollouts generated under denoised prompts are used as high-reward supervision to calibrate policy optimization on the original prompts. Unlike standard filtering, this mechanism encourages the model to learn to ignore interference tokens, rather than merely fitting solutions under cleaner conditions, ultimately enhancing the robustness of LLM reasoning through self-exploration. Experimental results show that LENS significantly outperforms GRPO, achieving Pareto improvement in performanceefficiency trade-offs, with an average performance gain of 3.88% and over 1.6 faster convergence across seven math reasoning benchmarks. Furthermore, LENS exhibits superior performance over both scaling exploration and prompt filtering baselines while using substantially fewer computational resources. These results further provide empirical support for our hypothesis: low-success, challenging prompts contain valuable training signals, highlighting the critical role of pruning interference tokens in improving rollout efficiency, offering new perspective for RLVR research."
        },
        {
            "title": "2 LENS: Less Noise Sampling Framework",
            "content": "We introduce LENS (LEss Noise Sampling Framework), plug-and-play rollout framework designed to facilitate effective policy exploration. LENS consists of two key components: (1) Interference Token Identification and Purification: identifying and purifying interference tokens in low-success prompts through interference score; (2) Calibrated Rollout Policy Optimization (CRPO), an efficient posttraining algorithm that transfers learning signals from the denoised prompts produced by Component (1) to the original noisy prompts, equipping the model with the ability to ignore interference and perform robust reasoning under noisy inputs. 2."
        },
        {
            "title": "Interference Token Identification and\nPurification",
            "content": "In this section, we first describe how interference tokens are identified and then explain how prompt purification is performed. Interference Token Identification. We start from the observation that the reference model πref provides stable reference distribution learned from the training data. In contrast, large tokenlevel deviations of the learned policy πθ from this reference often signal over-optimization or spurious behavior driven by noise, which can destabilize exploration (Rafailov et al., 2024). Motivated by this intuition, we identify interference tokens by measuring the token-level deviation between the current policy and the reference model. Specifically, for token prefix and the next generated token a, we define Interference Score as: SI (s, a) (cid:12) (cid:12)log πθ(a s) log πref(a s)(cid:12) (cid:12) (1)"
        },
        {
            "title": "Tokens with large interference scores contribute\ndisproportionately to the KL divergence from the\nreference distribution and are therefore treated as",
            "content": "Figure 3: Method Overview. In the first stage, LENS identifies and purifies interference tokens within low-success prompts via Interference Score (Defined in Section 2.1), thereby generating higher proportion of successful rollouts. In the second stage, LENS uses successful rollouts from the denoised prompts as high-reward supervision to calibrate policy optimization on the original prompt, correcting gradient updates distorted by interference. interference tokens. Such deviations are commonly induced by reward over-optimization or noisy and misleading signals (Rafailov et al., 2024; Gao et al., 2023), and can hinder effective exploration and generalization in the high-dimensional token action space (Engstrom et al., 2020; Dai et al., 2025). Interference Purification. Based on the observations in Section 1 and the Interference Score, we propose token-wise inspection and pruning mechanism. By removing only small number of tokens, this mechanism effectively eliminates interference while preserving the original semantics of the prompt with minimal impact. Specifically, for the i-th prompt xi, which contains xi tokens after tokenization, we compute token-level Interference Scores for all the tokens in the prompt and rank the tokens in descending order. Then, we introduce deletion ratio γ and select the top = γ xi tokens to form an interference token set Ii. We define the denoised prompt as = xi Ii, which denotes removing all tokens in the interference set Ii. We set γ to small value (e.g., 1%5%) to preserve the original semantics, with further discussion in Section 4.3. 2.2 Calibrated Rollout Policy Optimization Therefore, we adopt Calibrated Rollout Policy Optimization (CRPO), which applies interference token purification (Section 2.1) to obtain rollouts with higher proportion of successful samples. When the original prompt exhibits low sampling success (i.e., success rate below τ ; see Appendix for sensitivity analysis), CRPO treats rollouts generated from the denoised prompt as source of transferable supervision and applies this signal to guide policy optimization on the original prompt xi, enabling interference-aware calibration. Such calibration equips the model to recognize interference under noisy prompts and thereby prevents training collapse. The complete algorithmic procedure is provided in Algorithm 1. Sample Reweighting. To mitigate the impact of interference-induced failures and properly incorporate the successful rollouts of the denoised prompt, we adopt sample reweighting strategy to calibrate the training signal. Let Yi = + denote the set of rollouts sampled from the original prompt xi, where + and are the sets of successful and failed rollouts, respectively. We define the initial sampling success rate as ai = + /Yi. While interference tokens can be identified, directly removing them is not always beneficial, as only about 20% of prompts exhibit improvement in rollout accuracy after removal (Figure 2c). In addition, we collect set of successful rollouts Pi by sampling from the denoised prompt i. To ensure that pruning provides genuine improvement, we activate rollout replacement only when Algorithm 1 CRPO: Calibrated Rollout Policy Optimization 1: Input: Policy πθ, reference policy πref , dataset D, group m, accuracy threshold τ , pruning count k. 2: for iteration = 1, . . . , do Sample batch = {xi}B 3: for each prompt xi do i=1 D. 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: Sample rollouts Yi = {yi,1, . . . , yi,m} πθ( xi). Partition Yi into success set + Compute initial success rate ai = + Initialize training group Gi Yi and prompt mapping xroll(y) xi, Gi. if ai < τ then and failure set /Yi. . Interference Token Identification and Purification. Interference Identification: Compute SI (t) = log πθ(t xi) log πref (t xi) for tokens in xi (Eq. 1). Obtain Sample rollouts if acc(x i) > ai then by deleting top-k tokens in xi with highest SI (t). i) and compute success rate acc(x πθ( i) = +/m. Let Pi be successful rollouts in . Select Ri Update Gi (Yi Ri) Pi. Update xroll(y) with Ri = min(Y for all Pi. , Pi). end if end if Policy Calibration: Compute importance ratios ρ(y; θ) (Eq. 5) and weights w(y) based on ai (Eq. 4). Compute calibrated advantages ˆA(y) (Eq. 6) and update πθ using L(θ) (Eq. 7). Calibrated Rollout Policy Optimization. end for 24: 25: end for the denoised prompt achieves higher empirical accuracy: gi = I(cid:2)acc(x i) > acc(xi)(cid:3). (2) When gi = 1, we replace subset of failed rollouts in with successful ones from Pi. Specifically, we randomly sample subset Ri with Ri = min(Y , Pi). Let Gi be the reconi structed rollout set: (cid:40) Gi = + (Y Yi, Ri) Pi, if gi = 1, otherwise. (3) We define the unnormalized weight w(y) for each rollout Gi by applying ai as scaling factor: w(y) = ai, 1 ai, , + Pi (Y Ri). denote the prompt variant used to sample rollout under the rollout policy πold. We apply importance correction by defining the ratio: ρ(y; θ) = πθ(y xi) πold(y xroll(y)) . (5) We compute group-relative advantages using weighted normalization over the reconstructed rollout set Gi, where σw(Gi) denotes the weighted standard deviation of rewards in Gi: ˆA(y) = r(y) (cid:80) w(y) r(y) yGi σw(Gi) . (6) Finally, we optimize PPO-style clipped surrogate objective with KL regularization: L(θ) = (cid:88) w(y) min (cid:16) ρ(y; θ) ˆA(y), (4) yGi (cid:17) CLIP(ρ(y; θ), 1 ϵ, 1 + ϵ) ˆA(y) (7) Objective Function. To enable transfer of learning signals from the denoised prompt back to the original prompt xi, we formulate unified objective that corrects for distribution mismatch and stabilizes policy optimization. Let xroll(y) {xi, i} + β DKL(πθ( xi) πref ( xi)) . Overall, CRPO provides selective and stable calibration mechanism that improves policy optimization by exploiting high-quality rollouts revealed through interference purification."
        },
        {
            "title": "3 Experiments",
            "content": "3.1 Experiment Settings Base Models. We conduct experiments across three distinct model families, Llama-3.2 (Meta, 2024), Qwen-2.5 (Team et al., 2024) and Qwen3 (Yang et al., 2025a), covering various parameter scales. Specifically, we utilize five models: Llama-3.2-3B-Instruct (Meta, 2024), Qwen2.5-3B, Qwen2.5-7B (Team et al., 2024), Qwen3-4B-Base and Qwen3-8B-Base (Yang et al., 2025a). Baseline. We evaluate LENS against vanilla GRPO (Shao et al., 2024b) and two representative strategies for handling zero-variance prompts: increased sampling and zero-variance filtering. For increased sampling, we implement GRPOextended, which doubles the rollout budget per instruction. For zero-variance filtering, we compare with DAPO (Yu et al., 2025) and GRESO (Zheng et al., 2025a), maintain training stability by discarding or skipping zero-variance prompts via post-rollout filtering and pre-rollout prediction, respectively. We also report their extended variants, DAPOextended and GRESOextended, which run twice the number of training epochs. Notably, LENS operates under strictly lower computational budget, without increasing rollout counts or training epochs. Training and Evaluation Datasets. For the RL training phase, we employ Openr1-Math-46k (Yan et al., 2025), large-scale, high-quality dataset designed for mathematical reasoning. In the evaluation, we assess model performance across diverse set of seven benchmarks: MATH500 (Hendrycks et al., 2021), AMC23 (AI-MO, 2024), AIME24, AIME25 (Li et al., 2024), GaokaoEN-2023 (Zhang et al., 2023), Minerva (Lewkowycz et al., 2022), and OlympiadBench (He et al., 2024). This selection covers broad range of difficulty levels, enabling comprehensive evaluation. Training Settings. We leverage GRPO as the basis for LENS, setting the KL coefficient β = 0.001 and the imitation coefficient γ = 0.001. Both preliminary and validation experiments are conducted using Llama-3.2, Qwen-2.5, and Qwen-3 models. We configure the maximum response length to 4096 tokens, the learning rate to 1 106, both the rollout and update batch sizes to 128, the number of rollouts to 8, top-p to 1, and the temperature to 1. Detailed training hyperparameters for GRPO are provided in Appendix A. Evaluation Settings. For evaluation, we set both the temperature and top-p to 1.0, with maximum generation length of 4, 096 tokens. We primarily report the Pass@1 accuracy. To ensure robust evaluation on high-difficulty benchmarks such as AMC23, AIME24, and AIME25, we present the results averaged over 16 generation samples. 3.2 Main Results and Analysis Table 1 summarizes the performance on Qwen34B-Base, Qwen3-8B-Base and Llama-3.2-3BInstruct, while results on Qwen2.5-3B and Qwen2.5-7B are included in Appendix 3. We draw two conclusions: (1) Superiority over rolloutintensive baselines. LENS consistently outperforms GRPO (Shao et al., 2024b) and GRPOextended given the same training corpus. This indicates that simply increasing the rollout budget is insufficient for generating informative samples when exploration is affected by interference tokens. In contrast, LENS improves sample efficiency by enabling the model to focus on critical information, thereby effectively enhancing its reasoning performance. (2) Advantage over filtering strategies. LENS also surpasses the pre-rollout filter GRESO (Zheng et al., 2025a) and the post-rollout filter DAPO (Yu et al., 2025), including in settings with fewer rollouts. These results suggest that aggressively discarding zero-variance prompts can limit capability expansion, particularly on challenging benchmarks, which explains why LENS yields larger improvements on such datasets (e.g. AMC23 & AIME24). Figure 4 illustrates the learning curves for Qwen3-4B-Base and Qwen3-8B-Base across MATH-500 and OlympiadBench. LENS consistently achieves higher progressive and final accuracy compared to GRPO across all configurations. Notably, on the more challenging OlympiadBench, LENS exhibits more stable and continuous improvement, in contrast to the fluctuations observed with GRPO. We attribute this advantage to two primary factors: (1) LENS enhances the models exploration capacity by effectively removing potential interference tokens, leading to improved sample quality, and facilitating more effective exploration of capability boundaries; (2) By contrasting correct responses from interference-free rollouts with erroneous responses in the original sampling, LENS helps the model focus on key information, thereby boosting its reasoning abilities. Model Llama3.2-3B-Instruct + GRPO + DAPO + GRESO + GRPOextended + DAPOextended + GRESOextended + LENSGRPO Qwen3-4B-Base + GRPO + DAPO + GRESO + GRPOextended + DAPOextended + GRESOextended + LENSGRPO Qwen3-8B-Base + GRPO + DAPO + GRESO + GRPOextended + DAPOextended + GRESOextended + LENSGRPO Pass@1 Average@16 MATH Minerva Olympiad GAO AMC23 AIME24 AIME25 40.02 51.60 53.00 51.80 51.20 52.20 53.20 55.80 72.20 79.40 80.20 80.80 78.20 80.80 80.60 83. 77.00 84.00 85.00 84.20 85.20 85.00 84.60 86.00 16.54 21.69 21.69 21.69 20.96 20.22 18.38 22.43 27.21 34.56 35.29 36.76 39.34 39.34 37.50 41.54 34.29 45.22 44.12 41.18 43.38 44.49 43.38 48.16 15.26 20.00 19.26 19.70 19.41 20.15 18.81 21.04 38.96 45.19 42.96 45.33 44.30 45.33 44.74 50. 41.19 48.15 52.00 49.93 52.15 52.44 50.81 53.33 35.58 44.68 47.01 45.45 44.68 48.31 46.75 48.83 65.19 70.91 68.57 69.61 70.39 68.57 69.06 72.99 64.68 73.25 73.51 71.69 76.01 74.55 75.32 75.32 10.16 22.81 26.09 26.09 26.76 27.34 27.19 29.84 26.72 53.28 52.97 55.78 54.53 53.44 58.75 60. 38.59 58.91 62.03 64.69 66.25 65.47 60.62 66.56 1.88 6.25 9.79 6.46 6.25 10.21 7.29 10.62 5.62 12.71 13.12 16.46 12.92 16.04 18.33 18.54 5.62 18.33 21.46 20.83 22.08 22.50 21.46 23.96 0.00 0.83 0.42 1.04 1.04 0.42 0.83 0.83 6.70 14.79 11.04 12.50 11.88 11.04 12.71 16. 5.83 18.75 17.92 15.42 19.17 18.75 19.38 20.21 Avg. 17.06 23.98 25.32 24.60 24.33 25.55 24.64 27.03 34.66 44.41 43.50 45.32 44.51 44.94 45.96 49.15 38.17 49.52 50.86 49.71 52.03 51.88 50.80 53.36 Table 1: Detailed evaluation results on seven math reasoning benchmarks. The best and second best results are in bold and underlined. (*) Even in the unfavorable setting where GRPOextended, DAPOextended and GRESOextended are trained for 2 more rollouts, LENS still outperforms them on the majority of benchmarks. Figure 4: Learning curves of LENS and GRPO across model scales and task difficulties. We compare Qwen34B/8B-Base backbones on MATH-500 (Medium) and OlympiadBench (High). LENS converges faster and achieves comparable or higher final accuracy than GRPO under the same training step, indicating more efficient optimization."
        },
        {
            "title": "4.1 Training Dynamics Analysis",
            "content": "In this section, we conduct training dynamics analysis (Section 4.1), efficiency analysis (Section 4.2), threshold sensitivity analysis (Section 4.3), pruning strategies analysis (Appendix E) and computational overhead (Appendix D). To demonstrate the enhanced sampling efficiency of LENS, we analyze the sampling accuracy distributions of GRPO, GRPOextended, and LENS across three training stages: early (Steps 1100), middle (Steps 101200), and late (Steps 201300), as Figure 5: Sampling accuracy distribution across three training phases. We compare the sampling distributions of GRPO, GRPOextended and LENS across the early, middle and late training stages. Figure 6: Training efficiency comparison of LENS and GRPO on MATH-500 and OlympiadBench. The gray dashed lines indicate the number of training steps required for both methods to reach the highest average accuracy of the baseline during the entire training period. LENS demonstrates superior sample efficiency and faster convergence. shown in Figure 5. Detailed training dynamics of LENS, including the evolution of training reward, policy entropy, and response length, are presented in Appendix F. Results. Our experimental analysis yields the following observations: Compared to the baselines, LENS substantially reduces the proportion of zeroreward prompts. As illustrated in Figure 5, the Failure category, representing prompts with no successful rollouts, is consistently reduced across all the training stages. Correspondingly, more prompts are shifted into the Mid and High categories, which contain more informative learning signals."
        },
        {
            "title": "4.2 Efficiency Analysis",
            "content": "We evaluate the learning efficiency of LENS relative to GRPO on MATH-500 and OlympiadBench. We quantify efficiency by the relative speedup in terms of gradient steps required to reach the peak average accuracy attained by the GRPO baseline. Results. Figure 6 illustrates the comparative learning curves of LENS and GRPO. Notably, LENS achieves the same peak performance as GRPO while requiring 1.67 fewer gradient steps on MATH-500 and attaining 1.64 acceleration on OlympiadBench. We attribute this substantial gain in training efficiency to the enhanced quality of the rollouts generated during exploration. By effectively mitigating noise from interference tokens, LENS provides more stable and informative rollouts, which significantly accelerates convergence compared to the vanilla GRPO."
        },
        {
            "title": "4.3 Threshold Sensitivity Analysis",
            "content": "We conduct sensitivity analysis on the pruning threshold for interference tokens using Qwen2.53B and Qwen2.5-7B on MATH-500. Figure 7 shows validation accuracy curves for various thresholds ranging from 1% to 5%, alongside the vanilla GRPO baseline. Detailed sensitivity analysis of the success rate threshold τ in Appendix C. Results. We observe two key findings: (1) LENS consistently matches or outperforms the GRPO throughout the entire training process across all evaluated thresholds (1%5%). (2) As highlighted in the insets, the optimal threshold exhibits clear Figure 7: Performance convergence of LENS on MATH-500 with Qwen2.5-3B/7B. LENS matches or exceeds GRPO throughout training. Insets highlight that the optimal threshold depends on model capacity: the weaker 3B model requires higher threshold, while the stronger 7B model achieves optimal results with lower threshold. correlation with model capacity: the Qwen2.5-3B model yields superior performance with higher threshold, whereas the more capable Qwen2.5-7B model achieves optimal performance with lower threshold. This pattern suggests that models with limited capacity are more strongly affected by interference tokens."
        },
        {
            "title": "5 Related Work",
            "content": "Credit Assignment. Credit assignment (Kazemnejad et al., 2024; Bentegeac et al., 2025; Chai et al., 2024) is crucial for improving Reinforcement Learning with Verifiable Rewards (RLVR) (Guo et al., 2025a; Liu et al., 2025; Yue et al., 2025; Yu et al., 2025; Shao et al., 2024a; Wen et al., 2025), helping the model identify key decision paths that contribute most to the final decision. Credit assignment methods use forward signals, such as attention (Chan et al., 2024; Li et al., 2025), log probabilities (Bentegeac et al., 2025), and entropy (Tan et al., 2025), to evaluate the importance of each token based on the models forward pass. However, existing methods primarily focus on output tokens during generation, while overlooking how distracting information within instruction tokens can mislead model behavior (Guo et al., 2025c). To address this gap, we propose extending the log-probability signal to the instruction level, analyzing and removing low-information instruction tokens to enhance the models focus during inference. This approach effectively improves the models sampling success rate and sample diversity. GRPO Signal Collapse. GRPO suffers from signal collapse that identical rewards in group lead to vanishing gradients, effectively halting further learning. Current studies can be categorized into 4 directions: (1) Prompt Filtering, removing extremely difficult prompts to maintain productive training set; (Yu et al., 2025; Xiong et al., 2025a; Zheng et al., 2025b); (2) Scaling Exploration, dynamically allocating greater number of rollouts to harder samples, thereby increasing reward variance within groups (Xu et al., 2025; Yang et al., 2025b; Zhan et al., 2025; Xiong et al., 2025b); and (3) Reward Function Design, revising the advantage computation to prevent vanishing gradients (Le et al., 2025). These approaches focus on optimizing rollout strategies or reward functions while overlooking the interference tokens in the instruction that degrade sample quality. To address this gap, we propose sampling framework to dynamically identify and purify these tokens, mitigating signal collapse and improving training efficiency."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we first reveal novel observation: in many cases, RLVR is only few interference tokens away from discovering correct rollout rollouts. Building on this insight, we propose LENS to identify interference tokens and to transfer successful rollouts generated from denoised prompts to calibrate policy optimization on the original noisy prompts. Extensive experiments demonstrate that LENS consistently outperforms GRPO in both performance and efficiency, with 3.88% average gain and over 1.6 speedup. LENS also exhibits better performance over both scaling exploration and prompt filtering baselines. Overall, our findings offer fundamentally new perspective on improving sampling efficiency in RLVR and open up promising directions for future research."
        },
        {
            "title": "Limitations",
            "content": "Our work has the following limitations: (1) Model Scale Constraints: Due to limited computational resources, our experiments were conducted on models with up to 8B parameters. Evaluating the performance and scalability of our method on largerscale models (e.g., 32B or 70B) remains an avenue for future research. (2) Narrow Reward Scenarios: The effectiveness of our approach has been validated primarily in tasks with binary rewards. Its applicability to more complex environments, such as those with multi-dimensional scoring, requires further investigation. (3) Algorithmic Integration: While we demonstrated the efficacy of our method within the GRPO framework, we have not yet explored its integration with other GRPObased variants. Specifically, our method could be combined with algorithms that optimize rollout frequency or reward functions, potentially enhancing exploration capabilities and training stability."
        },
        {
            "title": "References",
            "content": "AI-MO. 2024. Amc 2023. https://huggingface. co/datasets/AI-MO/aimo-validation-amc. Raphael Bentegeac, Bastien Le Guellec, Gregory Kuchcinski, Philippe Amouyel, and Aghiles Hamroun. 2025. Token probabilities to mitigate large language models overconfidence in answering medical questions: Quantitative study. Journal of medical Internet research, 27:e64348. Yekun Chai, Haoran Sun, Huang Fang, Shuohuan Wang, Yu Sun, and Hua Wu. 2024. Ma-rlhf: Reinforcement learning from human feedback with macro actions. arXiv preprint arXiv:2410.02743. Alex Chan, Hao Sun, Samuel Holt, and Mihaela Van Der Schaar. 2024. Dense reward for free in reinforcement learning from human feedback. arXiv preprint arXiv:2402.00782. Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. 2025. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758. Juntao Dai, Taiye Chen, Yaodong Yang, Qian Zheng, and Gang Pan. 2025. Mitigating reward overoptimization in rlhf via behavior-supported regularization. arXiv preprint arXiv:2503.18130. Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, and Aleksander Madry. 2020. Implementation matters in deep policy gradients: case study on ppo and trpo. arXiv preprint arXiv:2005.12729. Leo Gao, John Schulman, and Jacob Hilton. 2023. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pages 1083510866. PMLR. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025a. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025b. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Yiju Guo, Wenkai Yang, Zexu Sun, Ning Ding, Zhiyuan Liu, and Yankai Lin. 2025c. Learning to focus: Causal attention distillation via gradient-guided token pruning. arXiv preprint arXiv:2506.07851. Joshua Hare. 2019. Dealing with sparse rewards in reinforcement learning. arXiv preprint arXiv:1910.09281. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, and 1 others. 2024. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3828 3850. Zhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian Yu, Zhenwen Liang, Wenxuan Wang, and 1 others. 2025. Deepmath-103k: large-scale, challenging, decontaminated, and verifiable mathematical dataset for advancing reasoning. arXiv preprint arXiv:2504.11456. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874. Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville, and Nicolas Le Roux. 2024. Vineppo: Accurate credit assignment in rl for llm mathematical reasoning. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS24. Thanh-Long Le, Myeongho Jeon, Kim Vu, Viet Lai, and Eunho Yang. 2025. No prompt left behind: Exploiting zero-variance prompts in llm reinforcement learning via entropy-guided advantage shaping. arXiv preprint arXiv:2509.21880. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, and 1 others. 2022. Solving quantitative reasoning problems with language models. Advances in neural information processing systems, 35:38433857. Wei Xiong, Jiarui Yao, Yuhui Xu, Bo Pang, Lei Wang, Doyen Sahoo, Junnan Li, Nan Jiang, Tong Zhang, Caiming Xiong, and 1 others. 2025a. minimalist approach to llm reasoning: from rejection sampling to reinforce. arXiv preprint arXiv:2504.11343. Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, and 1 others. 2024. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13(9):9. Yang Li, Zhichen Dong, Yuhan Sun, Weixun Wang, Shaopan Xiong, Yijia Luo, Jiashun Liu, Han Lu, Jiamang Wang, Wenbo Su, and 1 others. 2025. Attention illuminates llm reasoning: The preplan-andanchor rhythm enables fine-grained policy optimization. arXiv preprint arXiv:2510.13554. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. 2025. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783. Llama Meta. 2024. 3.2: Revolutionizing edge ai and vision with open, customizable models, 2024. URL: https://ai. meta. com/blog/llama-3-2-connect-2024vision-edge-mobile-devices, 6. Rafael Rafailov, Joey Hejna, Ryan Park, and Chelsea Finn. 2024. From to : Your language model is secretly q-function. arXiv preprint arXiv:2404.12358. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, and 1 others. 2024a. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, and 1 others. 2024b. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Hongze Tan, Jianfei Pan, Jinghao Lin, Tao Chen, Zhihang Zheng, Zhihao Tang, and Haihua Yang. 2025. Gtpo and grpo-s: Token and sequence-level reward shaping with policy entropy. arXiv preprint arXiv:2508.04349. Qwen Team and 1 others. 2024. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2(3). Xumeng Wen, Zihan Liu, Shun Zheng, Shengyu Ye, Zhirong Wu, Yang Wang, Zhijian Xu, Xiao Liang, Junjie Li, Ziming Miao, and 1 others. 2025. Reinforcement learning with verifiable rewards implicitly incentivizes correct reasoning in base llms. arXiv preprint arXiv:2506.14245. Wei Xiong, Chenlu Ye, Baohao Liao, Hanze Dong, Xinxing Xu, Christof Monz, Jiang Bian, Nan Jiang, and Tong Zhang. 2025b. Reinforce-ada: An adaptive sampling framework for reinforce-style llm training. arXiv preprint arXiv:2510.04996. Yixuan Even Xu, Yash Savani, Fei Fang, and Zico Kolter. 2025. Not all rollouts are useful: Downsampling rollouts in llm reinforcement learning. arXiv preprint arXiv:2504.13818. Jianhao Yan, Yafu Li, Zican Hu, Zhi Wang, Ganqu Cui, Xiaoye Qu, Yu Cheng, and Yue Zhang. 2025. Learning to reason under off-policy guidance. Preprint, arXiv:2504.14945. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. 2025a. Qwen3 technical report. arXiv preprint arXiv:2505.09388. Zhicheng Yang, Zhijiang Guo, Yinya Huang, Yongxin Wang, Dongchun Xie, Yiwei Wang, Xiaodan Liang, and Jing Tang. 2025b. Depth-breadth synergy in rlvr: Unlocking llm reasoning gains with adaptive exploration. arXiv preprint arXiv:2508.13755. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, and 1 others. 2025. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476. Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, and 1 others. 2025. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118. Runzhe Zhan, Yafu Li, Zhi Wang, Xiaoye Qu, Dongrui Liu, Jing Shao, Derek Wong, and Yu Cheng. 2025. Exgrpo: Learning to reason from experience. arXiv preprint arXiv:2510.02245. Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, and Xipeng Qiu. 2023. Evaluating the performance of large language models on gaokao benchmark. arXiv preprint arXiv:2305.12474. Haizhong Zheng, Yang Zhou, Brian Bartoldson, Bhavya Kailkhura, Fan Lai, Jiawei Zhao, and Beidi Chen. 2025a. Act only when it pays: Efficient reinforcement learning for llm reasoning via selective rollouts. arXiv preprint arXiv:2506.02177. Haizhong Zheng, Yang Zhou, Brian Bartoldson, Bhavya Kailkhura, Fan Lai, Jiawei Zhao, and Beidi Chen. 2025b. Act only when it pays: Efficient reinforcement learning for llm reasoning via selective rollouts. arXiv preprint arXiv:2506.02177."
        },
        {
            "title": "A Detailed Training Settings",
            "content": "The complete training hyper-parameters in GRPO and LENS are put in Table 2. Hyper-parameter Value 128 128 8 512 Train Batch Size Micro Batch Size Rollout Maximum Prompt Length Maximum Response Length 4096 Temperature Top LR KL Coefficient 1.0 1.0 1 106 0. Table 2: Basic training hyper-parameters of both GRPO and LENS."
        },
        {
            "title": "B Performance on Various Models",
            "content": "In addition to Qwen3-4B-Base, Qwen3-8B-Base, and Llama3.2-3B-Instruct, we further evaluate LENS on Qwen2.5-3B and Qwen2.5-7B. The results are summarized in Table 3. Overall, LENS consistently achieves the strongest performance across all math reasoning benchmarks."
        },
        {
            "title": "C Success Rate Sensitivity Analysis",
            "content": "To investigate the impact of the success rate threshold τ on the stability of LENS, we conducted experiments with τ {0.125, 0.25, 0.375, 0.5}. Results. The main results are presented in Table 4. On one hand, τ = 0.125 concentrates the training signal on the most difficult samples near the models capability frontier, which particularly benefits high-difficulty benchmarks such as Minerva and AMC23. On the other hand, τ = 0.5 provides broader coverage of both mediumand high-difficulty samples, leading to the best aggregate performance and enhanced stability. Accordingly, we select τ = 0.5 as the default setting for all experiments."
        },
        {
            "title": "D Computational Overhead",
            "content": "We report detailed runtime and memory analyses of LENS compared with standard GRPO under identical hardware configurations, utilizing the verl framework on 8 NVIDIA A800 GPUs. LENS incurs higher wall-clock cost per update to prioritize signal quality, the specific average step times are detailed in Table 5. Results. LENS incurs computational overhead ranging from 1.27 to 1.62 compared to standard GRPO (G = 8). Notably, this remains significantly more efficient than the brute-force approach of doubling sample size (G = 16). Crucially, this additional computation translates directly into enhanced reasoning capabilities, evidenced by the 23% absolute accuracy gains reported in Table 1. These results demonstrate that LENS offers superior trade-off between efficiency and effectiveness, successfully converting marginal temporal costs into substantial reasoning capabilities."
        },
        {
            "title": "E Pruning Strategies Analysis",
            "content": "To demonstrate the effectiveness of our pruning strategy, we compare LENS with three representative alternatives: (1) Resampling, which uses an additional rollouts to replace unsuccessful rollouts with successful ones; (2) Random Pruning, which prunes the same fraction of tokens uniformly at random per instance; and (3) Gradient-based Pruning, which prunes tokens with the smallest gradient norm. For fairness, all methods share the same training setup and pruning ratio, and we evaluate them on seven benchmarks. Results. We present the results in Table 6. Our observations are as follows: LENS consistently outperforms the three baselines, achieving the best accuracy on seven benchmarks, which indicates more consistent and stable improvements than competing methods."
        },
        {
            "title": "F Training Dynamics Analysis",
            "content": "We evaluate the training dynamics (training reward, entropy, and response length) of LENS in comparison with GRPO (with rollout = 8) and GRPOextended (with rollout = 16) across two model scales: Qwen3-4B-Base and Qwen3-8BBase. Results. (1) Superior and stable training progress. As illustrated in Figures 8(a, d), LENS achieves steadier accuracy improvements compared to GRPO and GRPOextended across model scales. (2) Emergence of confident long-form reasoning. LENS yields more rapid gains in both accuracy and response length than the baselines while maintaining moderate entropy levels (Figure 8). Notably, the insets in Figures 8(c, e) reveal an earlier emergence of long-form reasoning beModel Qwen2.5-3B + GRPO + DAPO + GRESO + GRPOextended + LENS (Ours) Qwen2.5-7B + GRPO + DAPO + GRESO + GRPOextended + LENS Pass@1 Average@ MATH Minerva Olympiad GAO AMC23 AIME24 AIME25 56.00 65.00 67.40 65.40 66.20 68.20 62.60 76.60 77.00 77.80 78.00 77.40 26.10 27.21 27.21 28.31 27.21 27.94 27.21 35.29 36.40 37.50 36.76 37.87 25.33 27.26 29.48 32.00 26.81 29. 29.48 39.85 39.41 40.89 38.96 40.89 44.16 56.62 57.40 58.96 56.10 58.96 51.43 65.45 67.01 66.49 69.61 69.39 12.97 31.09 34.06 37.19 27.81 38.91 19.69 51.09 53.91 48.28 51.41 56.41 1.04 4.48 4.58 6.46 3.96 7. 3.12 11.46 11.04 14.17 15.62 14.58 0.21 1.88 1.04 2.50 2.29 3.12 0.42 5.83 8.96 6.67 5.62 10.62 Avg. 23.69 30.51 31.60 32.97 30.05 33.43 27.71 40.79 41.96 41.69 42.28 43. Table 3: Detailed evaluation results on seven math reasoning benchmarks. The best and second best results are in bold and underlined. (*) Even in the unfavorable setting where DAPO and GRESO are trained for 2 more epochs and GRPO (n = 16) uses more rollouts, LENS still outperforms them on the majority of benchmarks."
        },
        {
            "title": "Value",
            "content": "Pass@1 Average@"
        },
        {
            "title": "MATH Minerva Olympiad GAO",
            "content": "AMC23 AIME24 AIME25 τ = 0.125 τ = 0.250 τ = 0.375 τ = 0.500 67.00 67.40 68.00 68.20 29.78 28.31 26.47 27.94 29.04 29.78 29.78 29.78 56.88 57.92 56.36 58. 41.41 36.88 37.19 38.91 6.67 7.50 6.04 7.08 2.50 2.50 2.92 3.12 Avg. 33.33 32.90 32.39 33.43 Table 4: Validation result under different τ ."
        },
        {
            "title": "Model",
            "content": "Algorithm Group Size (G) Avg. Step Time (s) Relative Cost Qwen2.5-3B Qwen2.5-7B Qwen3-4B-Base Qwen3-8B-Base Llama-3.2-3B-Instruct GRPO GRPOextended LENS (Ours) GRPO GRPOextended LENS (Ours) GRPO GRPOextended LENS (Ours) GRPO GRPOextended LENS (Ours) GRPO GRPOextended LENS (Ours) 8 16 8 16 8 8 16 8 8 16 8 8 16 8 214 384 333 344 523 331 524 475 477 802 726 229 338 291 1.0 1.79 1.56 1.0 1.52 1.35 1.0 1.58 1.44 1.0 1.79 1.62 1.0 1.48 1.27 Table 5: Computational efficiency comparison on 8 A800. denotes the group size. Method Pass@1 Average@ MATH Minerva Olympiad GAO AMC23 AIME24 AIME25 GRPO Resampling Random Pruning Gradient-based Pruning LENS 65.00 66.40 65.80 65.60 68.20 27.21 26.47 26.10 26.84 27.94 27.26 29.48 29.19 29.78 29.78 56.62 57.92 57.66 58.18 58. 31.09 37.50 37.50 37.81 38.91 4.48 6.67 5.83 6.67 7.08 1.88 2.50 2.92 1.88 3.12 Avg. 30.51 32.42 32.14 32.39 33.43 Table 6: Validation result under different pruning strategies. Figure 8: Training dynamics across different model scales. Each row reports average accuracy, entropy, and response length during training for Qwen3-4B-Base (top) and Qwen3-8B-Base (bottom). Compared with GRPO and GRPOextended, LENS exhibits more consistent and stable trends. available in the projects GitHub repository upon release. AI Assistants We used AI assistants (ChatGPT) solely for textual and grammatical refinement, without influencing the core content or experimental results. haviors (the aha moment (Guo et al., 2025b)). Furthermore, the lower entropy exhibited by LENS (Figures 8(b, d)) indicates reduced uncertainty and more decisive reasoning during training (Cheng et al., 2025)."
        },
        {
            "title": "G Checklist",
            "content": "Potential Risks Our work does not involve any identifiable ethical or legal risks. Artifacts We check that the data does not contain any information that names or uniquely identifies individual people or offensive content. All models and datasets used in this work comply with their respective open-source or research licenses. We ensure that all artifacts are used strictly within the permitted scope of their terms. The Code we released will be under permissive open-source license, enabling reproducibility and reuse. Documentation for all artifacts will be updated and made"
        }
    ],
    "affiliations": [
        "Baidu Inc.",
        "Department of Computer Science, Aarhus University",
        "Gaoling School of Artificial Intelligence, Renmin University of China"
    ]
}