{
    "paper_title": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation",
    "authors": [
        "Letian Zhang",
        "Sucheng Ren",
        "Yanqing Liu",
        "Xianhang Li",
        "Zeyu Wang",
        "Yuyin Zhou",
        "Huaxiu Yao",
        "Zeyu Zheng",
        "Weili Nie",
        "Guilin Liu",
        "Zhiding Yu",
        "Cihang Xie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper presents a family of advanced vision encoder, named OpenVision 3, that learns a single, unified visual representation that can serve both image understanding and image generation. Our core architecture is simple: we feed VAE-compressed image latents to a ViT encoder and train its output to support two complementary roles. First, the encoder output is passed to the ViT-VAE decoder to reconstruct the original image, encouraging the representation to capture generative structure. Second, the same representation is optimized with contrastive learning and image-captioning objectives, strengthening semantic features. By jointly optimizing reconstruction- and semantics-driven signals in a shared latent space, the encoder learns representations that synergize and generalize well across both regimes. We validate this unified design through extensive downstream evaluations with the encoder frozen. For multimodal understanding, we plug the encoder into the LLaVA-1.5 framework: it performs comparably with a standard CLIP vision encoder (e.g., 62.4 vs 62.2 on SeedBench, and 83.7 vs 82.9 on POPE). For generation, we test it under the RAE framework: ours substantially surpasses the standard CLIP-based encoder (e.g., gFID: 1.89 vs 2.54 on ImageNet). We hope this work can spur future research on unified modeling."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 1 2 ] . e [ 1 9 6 3 5 1 . 1 0 6 2 : r OpenVision 3 : Family of Unified Visual Encoder for"
        },
        {
            "title": "Both Understanding and Generation",
            "content": "Letian Zhang* 1 Huaxiu Yao3 Sucheng Ren* 2 Zeyu Zheng4 Yanqing Liu1 Xianhang Li1 Zeyu Wang1 Yuyin Zhou Weili Nie5 Guilin Liu5 Zhiding Yu5 Cihang Xie1 1UC Santa Cruz 2JHU 3UNC-Chapel Hill 4UC Berkeley 5NVIDIA Project Page: https://ucsc-vlaa.github.io/OpenVision3/ Figure 1. An overview of OpenVision 3s architecture design and performance highlight. Left panel: The architecture of OpenVision 3. We employ frozen VAE and trainable ViT as the unified tokenizer, which produces tokens that are fed simultaneously into both the generation and understanding branches. Middle panel: The learning objectives of the generation branch and the understanding branch. For the generation branch, we focus on high-quality, pixel-level image reconstruction; concurrently, the understanding branch is optimized via joint contrastive learning and captioning objectives. Right panel: The performance summarization shows that OpenVision 3 outperforms other unified tokenizers and semantics-based encoders in rFID and gFID, while remaining competitive with CLIP in multimodal understanding ability. Abstract This paper presents family of advanced vision encoder, named OpenVision 3, that learns single, unified visual representation that can serve both image understanding and image generation. Our core architecture is simple: we feed VAE-compressed image latents to ViT encoder and train its output to support two complementary roles. First, the encoder output is passed to the ViT-VAE decoder to reconstruct the original image, encouraging the representation to capture generative structure. Second, the same representation is optimized with contrastive learning and image-captioning objectives, strengthening semantic features. By jointly optimizing reconstructionand semantics-driven signals in shared latent space, the encoder learns representations that synergize and generalize well across both regimes. We validate this unified design through extensive downstream evaluations with the encoder frozen. For multimodal understanding, we plug the encoder into the LLaVA-1.5 framework: it performs comparably with standard CLIP vision encoder (e.g., 62.4 vs. 62.2 on SeedBench, and 83.7 vs. 82.9 on POPE). For generation, we test it under the RAE framework: ours substantially surpasses the standard CLIP-based encoder (e.g., gFID: 1.89 vs. 2.54 on ImageNet). We hope this work can spur future research on unified modeling. 1 OpenVision 3: Family of Unified Visual Encoder for Both Understanding and Generation 1. Introduction Unified Multimodal Models (UMMs) has emerged as cornerstone of multimodal research, driven by the need for systems that seamlessly integrate visual understanding and generation. Their development is grounded in the Platonic Representation Hypothesis (Huh et al., 2024), which posits that different data modalities reflect shared underlying reality, and that learning unified multimodal representation enables mutual benefits across modalities while improving generalization. The success of representative proprietary UMMs such as GPT-4o (Hurst et al., 2024) and Gemini2.5 Flash (Comanici et al., 2025), as well as public models like BAGEL (Deng et al., 2025), further supports this view, showcasing strong capabilities in dialogue-based multi-turn generation, multimodal in-context learning, and fine-grained control over generated content. key challenge in developing native UMMs lies in how visual representations are encoded. Owing to the representational discrepancy between visual understanding and visual generation, common UMM design, exemplified by UniFluid (Fan et al., 2025), BAGEL (Deng et al., 2025), and MOGAO (Liao et al., 2025), employs two distinct visual tokenizers that encode the same image twice, producing one set of high-level semantic tokens and another set of low-level, pixel-reconstructable tokens. While effective, this approach increases system complexity and may hinder deeper synergy between understanding and generation. Another line of work attempts to bridge this gap through shared visual tokenizers. However, these approaches typically rely on quantized hidden representations, which inevitably introduce discretization errors and limit generation quality (e.g., TokenFlow (Qu et al., 2025), UniTok (Ma et al., 2025), and EMU3.5 (Cui et al., 2025)). As result, developing simple yet effective continuous visual tokenizer that naturally supports both visual understanding and generation remains an open and practically important challenge. This paper presents OpenVision 3 as step toward mitigating this challenge. Concretely, we build our tokenizer by stacking ViT encoder on top of well-trained VAE encoder. The output of the ViT encoder is further fed into two separate branches, one generation decoder that is trained to reconstruct the original image and enforce preservation of low-level visual information, and another understanding decoder that is trained by contrastive and captioning objectives, enhancing semantic supervision. Intriguingly, as analyzed in Section 4.5, this design choice can non-trivially synergize the learning of both fine-grained details and highlevel semantics, e.g., even optimizing understanding loss alone can lead to better reconstruction performance, and conversely, optimizing reconstruction alone can benefit semantic alignment. This behavior is also consistent with recent evidence that semantically informed tokenization can facilitate low-level reconstruction learning (Yu et al., 2024; Leng et al., 2025; Yao et al., 2025), and may even serve as direct drop-in replacement for purely reconstructionoriented tokenizers (Zheng et al., 2025). Our experiments validate the effectiveness of OpenVision 3 across understanding, reconstruction and generation. Crucially, in all downstream evaluations we keep the tokenizer/encoder frozen, ensuring that the reported gains reflect the quality and transferability of the learned visual representation rather than task-specific fine-tuning. For understanding evaluation, we integrate our tokenizer into the LLaVA-1.5 frameworkfor training and evaluate its performance across various standard multimodal. For the reconstruction evaluation, we evaluate the quality of reconstructed images OpenVision 3 on COCO (Lin et al., 2014) and ImageNet (Deng et al., 2009). For the generation evaluation, we train flow matching model following RAE (Zheng et al., 2025) on ImageNet. The results demonstrate that OpenVision 3 is comparable to CLIP in terms of understanding capabilities(e.g., 62.4 vs. 62.2 on SeedBench, and 83.7 vs. 82.9 on POPE), while surpassing existing unified tokenizers in image reconstruction (e.g., rFID: 0.22 vs. 0.36 on ImageNet). For image generation, our tokenizer outperforms standard CLIP-based encoder under the RAE framework by large margin (e.g., gFID: 1.89 vs. 2.54 on ImageNet). We hope that releasing OpenVision 3 will catalyze further research into more advanced unified vision tokenizers. 2. Related Work 2.1. Vision-Language Pretraining Vision-Language pretraining serves as the cornerstone of multimodal representation learning. Pioneering works, exemplified by CLIP, adopt contrastive learning as their core methodology to extract and align visual and textual features. This training paradigm was subsequently adopted by wide range of studies, such as LAION (Schuhmann et al., 2022), DataComp (Gadre et al., 2023), DFN (Fang et al., 2023), OpenCLIP (Cherti et al., 2023), MetaCLIP (Xu et al., 2023; Chuang et al., 2025) and CLIPA (Li et al., 2023b;a). Follow-up works have continuously explored alternative training regimes. CoCa (Yu et al., 2022) adds captioning loss on the multimodal decoder outputs which predicts text tokens autoregressively. SigLip (Zhai et al., 2023) proposes to replace contrastive loss with pairwise Sigmoid loss. SigLip2 (Tschannen et al., 2025) further extends this by incorporating captioning-based pretraining, self-distillation and masked prediction. The AM-RADIO (Ranzinger et al., 2024; Heinrich et al., 2025) series of works is dedicated to knowledge distillation from multiple teacher models. More recently, CLIPS (Liu et al., 2024b), OpenVision (Li et al., 2025), and OpenVision 2 (Liu et al., 2025a) have focused on the efficient utilization of captioning loss in 2 OpenVision 3: Family of Unified Visual Encoder for Both Understanding and Generation vision-language pretraining, demonstrating it to be lowcost yet high-performance approach. Our work builds upon this line of research and extend this efficient paradigm to unified multimodal learning. 2.2. Unified Tokenizer Extracting representative feature for both generation and understanding has been bottleneck for the development of unified modeling. Previous works mostly adapt separate encoder for the two kinds of features. For example, BAGEL takes FLUX-VAE (Labs, 2024; Labs et al., 2025) for lowlevel features and SigLIP2 for semantic features. UniWorldV1 (Lin et al., 2025) also computes the two types of features separately and then concatenates them. Contrast to above work, another line of studies focuses more on developing unified tokenizers that fuse semantic and pixel-level features. Inspired by the success of VQGAN (Esser et al., 2021), early unified tokenizers predominantly adapt discrete token design. Discrete tokenizers rely on vector quantization(VQ) to train representative unified codebooks. For example, TokenFlow (Qu et al., 2025) jointly optimizes semantic and pixel-level features by incorporating dual codebooks with shared mapping. UniTok (Ma et al., 2025) uses multi-codebook quantization to construct unified discrete representations. Lately, the prevalent trend has gradually shifted toward continuous tokenizers. Show-o2 (Xie et al., 2025) applies semantic and low-level projection to VAE latents and fuse dual features to produce unified feature space. More recently, the concurrent work, TUNA (Liu et al., 2025b), further simplifies this by connecting VAE and ViT as unified tokenizer, which is most related to our work. However, TUNA relies on pretrained ViT checkpoints and it remains non-transparent how to train such tokenizer. In our work, we train the ViT from scratch and propose an effective training paradigm for the unified tokenizer with unified representations. 3. Method 3.1. Motivation Developing unified tokenizer is pivotal step toward unifying generation and understanding, but it is often hindered by the difficulty of establishing unified feature space and high-efficient training. Previous studies have presented impressive methods to eliminate these obstacles. However, explorations into constructing unified representations remain in their preliminary stages, and the associated training pipelines still remain non-transparent to the community. In the following, we present our model, which constructs unified vision representation space through VAE and ViT in effective and straightforward way. We demonstrate to the research community how to train unified tokenizer efficiently from scratch within the VAE latent space. 3.2. OpenVision 3: unified tokenizer OpenVision 3 uses VAE encoder and vision transformer(ViT) to extract unified vision features. The input image RHW is first encoded by the VAE encoder Evae from FLUX.1-dev into VAE latents zvae, and the following training process is completely under the VAE latent space. Next, the VAE latents are fed into the ViT encoder Evit to extract the unified representations zu for both understanding tasks and generation tasks. During the VAE stage, the FLUX.1 VAE downsamples the image height and width by 8, respectively. Therefore, we adjust the patch size of the ViT to 22 so that the whole compression ratio is 16, which aligns with common settings. Formally, zvae = Evae(x) 8 8 Dvae zu = Evit(zvae) 16 16 Du (1) (2) where Dvae is the VAE latent channels, Du is the ViT dimensions. The encoded unified feature zu then goes into the reconstruction branch and the understanding branch to do decoding. OpenVision 3 employs two distinct branches to cultivate its ability to extract both generative and interpretive vision representations. The two branches are completely separate, and their respective architectures will be elaborated upon below. Reconstruction branch. The reconstruction decoding part mirrors the structure of the tokenizer, maintaining near-symmetrical configuration. Before the decoding, we first add noise to the unified representations in order to improve the generalization of generation ability. The perturbed feature zu is generated by adding Gaussian noise scaled by sample-specific intensity: zu = zu + σ ϵ, ϵ (0, I) (3) where σ is uniformly sampled from [0, τ ] for each instance in the batch, τ is constant. Then we use ViT decoder with patch size 11 and linear layer to convert the noised unified feature zu back into VAE latents ˆzvae. Next, the VAE decoder is applied to decode the ˆzvae into reconstruction image ˆx. The reconstruction loss includes the reconstruction loss of image ˆx and VAE latents ˆzvae, and perceptual loss based on LPIPS. The whole reconstruction loss can be formulated as: Lrec = ℓ1(x, ˆx) + βℓ1(zvae, ˆzvae) + λLLP IP S(x, ˆx) (4) Understanding branch. The paradigm of understanding branch generally follows OpenVision, where we do contrastive learning and image captioning. As shown in Figure 1, we use text encoder to extract the caption feature OpenVision 3: Family of Unified Visual Encoder for Both Understanding and Generation ztxt to calculate contrastive loss with the unified visual feature zu. In parallel, we utilize text decoder to perform autoregressive prediction of synthetic captions from the unified representations and calculate the corresponding captioning loss. Formally, the understanding loss can be formulated as: the training. For the two training stages, the global batch sizes are 8K and 4K, with cosine-decayed base learning rates of 8 106 and 4 107. For complete parameter details, please refer to Table 1.The model is trained on the DataComp dataset recaptioned by LLaVA-Llama-3 (Li et al., 2024b), which ensures the high quality of the training data. Lund = Lcaption + αLcontrastive(zu, ztxt) (5) The overall training objective is: 4. Experiments 4.1. Evaluation settings Loverall = ωrecLrec + ωundLund (6) We configure ωund as double that of ωrec during the training process. Reducing ωrec helps to preserve generative quality while ensuring that the understanding capability remains unimpaired. 3.3. Training settings Training stages and resolution. In accordance with the conclusions drawn in CLIPA, we employ progressive training strategy for the tokenizer, transitioning from lowresolution to high-resolution inputs. We first pre-train the tokenizer at 128128, and then finetune it with 224224 or 256256. The epoch distribution for the two training stages is maintained at around 10:1 ratio. By focusing most of the compute on low-resolution stages, this approach attains superior performance while significantly reducing the computational overhead typically associated with highresolution training. Training details. As depicted in Figure 1, we use pretrained FLUX.1 VAE and freeze it during the whole training process. All other components (including ViT encoder, ViT decoder, text encoder, text decoder, and linear layer) are randomly initialized and remain unfrozen throughout Resolution Global batch size Base learning rate Epochs (B size) Epochs (L size) Warmup Epochs LPIPS loss weight λ VAE latents loss weight β Contrastive loss weight α Rec. loss weight ωrec Und. loss weight ωund Pretraining Finetune 128 8192 8 106 1000 2000 40 256 4096 4 107 200 200 20 0.5 0.4 1.0 0.5 1.0 Table 1. Parameter configs for two stages of training. The epoch number is ImageNet-equivalent epochs (1 epoch 1.3M samples). To comprehensively evaluate the performance of our unified tokenizer, we evaluate the reconstruction, generation and understanding performance and report their results in Section 4.2. For the generation side, we follow RAE configs to train generative model with DiT and wide DDT head and evaluate the generation fedelity of OpenVision 3. For the understanding side, we train vision-language models with our tokenizer under LLaVA-1.5 frameworks (Liu et al., 2024a), and evaluate the understanding performance across range of downstream multimodal benchmarks. 4.2. Reconstruction performance As shown in Table 2, OpenVision 3 significantly outperforms existing unified tokenizers across all metrics. Previous unified models (Zheng et al., 2025; Ma et al., 2025; Wang et al., 2024; Wu et al., 2024) often struggle to maintain high reconstruction quality due to the trade-off required to align with semantic objectives (e.g., SigLIP alignment). For instance, on ImageNet, OpenVision 3 achieves PSNR of 30.33 dB, surpassing UniTok (25.34 dB) and Vila-U (22.24 dB) by wide margin. Similarly, in terms of perceptual quality, our model achieves an LPIPS score of 0.061, whereas the closest unified competitor, UniTok, lags behind at 0.132. This demonstrates that our architectureutilizing the VAE-ViT hybrid designsuccessfully mitigates the information loss typically associated with semantic compression. Moreover, even in comparison with specialized generation-oriented tokenizers (Rombach et al., 2022; Esser et al., 2024; et. al., 2025; Labs et al., 2025; Wan et al., 2025), our model maintains competitive or better results. 4.3. Generation performance As shown in we report generation Frechet inception distance (gFID), Inception Score (IS), Precision (Pre.), and Recall (Rec.) as evaluation metric. We present the generative performance of each tokenizer when paired with its respective compatible generator. For low-level tokenizer, we evaluate SD-VAE with traditional diffusion-based generative models (DiT and SiT) (Peebles & Xie, 2023; Ma et al., 2024). For semantic tokenizer, we select CLIP and RAE generator for fair comparison with our tokenizer.According to Table 4, OpenVision 3 outperforms these tokenizers across all the metrics. 4 OpenVision 3: Family of Unified Visual Encoder for Both Understanding and Generation Model ImageNet COCO PSNR SSIM LPIPs rFID PSNR SSIM LPIPs rFID Generation-oriented Tokenizer SD-VAE SD3-VAE Cosmos FLUX-VAE Wan2.1-VAE 26.26 31.29 25.07 32.86 31.34 0.745 0.886 0.700 0.917 0. Unified Tokenizer RAE (CLIP) UniTok OmniTokenizer Vila-U OpenVision 3 17.44 25.34 24.69 22.24 30.33 0.403 0.742 0.771 0.612 0.885 0.133 0.059 0.167 0.044 0.058 0.324 0.132 0.138 0.228 0.061 0.606 0.201 0.959 0.176 0. 1.06 0.362 1.411 4.231 0.216 25.99 31.18 24.74 32.73 31.19 16.98 24.95 24.31 21.89 30.20 0.759 0.894 0.711 0.923 0.895 0.394 0.750 0.779 0.620 0.893 0.130 0.056 0.165 0.041 0. 0.345 0.131 0.137 0.227 0.058 4.142 1.671 5.063 1.343 3.449 10.119 3.918 6.292 10.997 1.798 Table 2. Reconstruction performance of visual tokenizers. Evaluations are performed on the ImageNet and COCO validation sets. Images are resized and center-cropped to 256256. Metrics includes Peak signal-to-noise ratio (PSNR), Structural Similarity Index Measure(SSIM), Learned Perceptual Image Patch Similarity (LPIPS) and reconstruction Frechet inception distance (rFID). Method Vision Encoder # Tokens # Res. MME-P MME-C SeedBench ScienceQA GQA POPE OpenAI-CLIP OpenVision 3 OpenAI-CLIP OpenVision 3 B/16 VAE + B/ L/14 VAE + L/2 256 256 256 256 224 224 224 256 1399 1468 1380 318 287 292 299 62.2 62.4 65.4 66.0 73.7 73. 73.9 72.8 58.6 58.0 60.6 61.1 82.9 83.7 84.7 85.3 Table 3. Comparison of OpenVision 3 with OpenAI CLIP under LLaVA-1.5 framework. We evaluate the understanding performance of our tokenizer and CLIP on multiple multimodal benchmarks. Under the same image token numbers, our unified tokenizer performs on par with OpenAI CLIP, while surpassing it across some specific benchmarks. Tokenizer Generator gFID IS Pre. Rec. 4.4. Understanding performance SD-VAE SD-VAE UniTok CLIP OpenVision OpenVision 3 DiT SiT LlamaGen RAE RAE RAE 2.27 2.06 2.51 2.54 2.44 1.89 278.2 270.3 216.7 256.4 262.2 289.2 0.83 0.82 0.82 0.80 0.80 0.84 0.57 0.59 0.57 0.54 0.53 0. Table 4. Class-conditional image generation on ImageNet 256x256. We report gFID, Inception Score (IS), Precision (Pre.), and Recall (Rec.). For example, we achieve better gFID when compared to SDVAE with improved generator SiT(1.89 vs. 2.06). Our tokenizer also surpasses semantic encoders like CLIP (Radford et al., 2021) by large margin in generation(1.89 vs. 2.54). Visualization. As shown in Figure 4, we visualize the generated results with RAE and OpenVision 3, which show the strong capability of our tokenizer in generating highquality samples with great fidelity and diversity. 5 To evaluate the semantic representation capability of OpenVision 3, we integrate it into the LLaVA-1.5 framework and conduct training following its standard training configurations. Due to the fixed downsample size of VAE, we keep the same encoded token numbers with OpenAI CLIP for fair comparison. In Section 3.2, we compare our tokenizer with CLIP and present the results on multiple multimodal benchmarks, including MME (Fu et al., 2023), ScienceQA (Saikh et al., 2022), SeedBench (Li et al., 2024a), GQA (Hudson & Manning, 2019) and POPE (Li et al., 2023c). According to the table, OpenVision 3 can match or exceed the understanding performance of CLIP on general multimodal tasks. For example, our tokenizer consistantly surpasses CLIP on SeedBench (62.4 vs. 62.2 and 66.0 vs. 65.4) and POPE (83.7 vs. 82.9 and 85.3 vs. 84.7). It can be observed that our unified tokenizer is comparable to the understandingoriented CLIP in terms of semantic comprehension, and even demonstrates clear advantage in certain aspects. OpenVision 3: Family of Unified Visual Encoder for Both Understanding and Generation (a) pixel recon loss (b) latents recon loss (c) caption loss (d) contrastive loss Figure 2. Loss visualization with only semantic loss. We trained our tokenizer with and without the reconstruction loss, respectively. In Figures (a) and (b), both pixel-level and latent-level reconstruction losses decrease significantly even in the absence of explicit reconstruction signals. Figures (c) and (d) demonstrate that the incorporation of the reconstruction loss has no adverse impact on the losses of the understanding branch. (a) pixel recon loss (b) latents recon loss (c) caption loss (d) contrastive loss Figure 3. Loss visualization with only reconstruction loss. We trained our tokenizer with and without the understanding loss, respectively. In Figure (a), the inclusion of semantic loss leads to lower image reconstruction loss, suggesting that semantic supervision can, in turn, enhance reconstruction performance. Figures (c) and (d) reveal that both caption and contrastive losses decrease even without explicit semantic training, further demonstrating that the two objectives are mutually beneficial. 4.5. Interaction of understanding and reconstruction For unified tokenizers, balancing the capabilities of understanding and generation remains long-standing challenge. To investigate the mutual influence of these two objectives within our tokenizer, we conduct experiments by training the model exclusively with the understanding loss and exclusively with the reconstruction loss, respectively. Remove reconstruction loss. In Figure 2, we remove reconstruction loss and train only with semantic loss. The blue curve represents the baseline loss, while the red curve denotes the model trained without the reconstruction loss. According to the loss curves in Figure 2a and Figure 2b, even in the absence of reconstruction objectives, the reconstruction loss still exhibits substantial decline, suggesting that our semantic objectives contribute significantly to image reconstruction. Furthermore, comparing the red and blue curves in Figure 2c and Figure 2d, it is evident that the incorporation of the reconstruction loss leads to no significant change in either caption or contrastive loss. These observations collectively indicate mutually beneficial synergy between the two types of losses. Remove understanding loss. In Figure 3, we remove understanding loss and train only with reconstruction-driven signals. The red curves here denote the loss without 6 Figure 4. Qualitative results of class-conditional ImageNet-256 generation. Under the RAE framework, our OpenVision 3 is able to generate high quality images. OpenVision 3: Family of Unified Visual Encoder for Both Understanding and Generation reconstruction-driven signals. Figure 3c and Figure 3d show that in the absence of semantic supervision, the contrastive loss remains almost stagnant, whereas the caption loss exhibits marginal decline. This indicates that the reconstruction task intrinsically facilitates semantic tasks that are also generative in nature. Moreover, as seen in Figure 3a, the addition of semantic loss paradoxically improves reconstruction performance, providing further evidence of the synergistic relationship between these two branches. 5. Conclusion This work introduces OpenVision 3, unified vision encoder for both understanding and generation. We innovatively couple VAE with ViT to form unified architecture, and generate single, unified representation for different downstream tasks. For the efficient training of our tokenizer, we propose new training paradigm with both reconstructionand semantics-driven signals for joint learning. Comprehensive evaluations reveal that our model yields superior results across generative and understanding tasks through low-cost training. OpenVision 3 outperforms current other unified tokenizer in reconstruction and generation, and shows competitive ability with CLIP on semantic tasks. To facilitate future research in the community, we will fully open-source our training code, data, and tokenizer checkpoints."
        },
        {
            "title": "Acknowledgements",
            "content": "We would like to thank TPU Research Cloud (TRC) program and Google Cloud Research Credits program for supporting our computing needs."
        },
        {
            "title": "References",
            "content": "Cherti, M., Beaumont, R., Wightman, R., Wortsman, M., Ilharco, G., Gordon, C., Schuhmann, C., Schmidt, L., and Jitsev, J. Reproducible scaling laws for contrastive language-image learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 28182829, 2023. Chuang, Y.-S., Li, Y., Wang, D., Yeh, C.-F., Lyu, K., Raghavendra, R., Glass, J., Huang, L., Weston, J., Zettlemoyer, L., et al. Meta clip 2: worldwide scaling recipe. arXiv preprint arXiv:2507.22062, 2025. Comanici, G., Bieber, E., Schaekermann, M., Pasupat, I., Sachdeva, N., Dhillon, I., Blistein, M., Ram, O., Zhang, D., Rosen, E., et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Cui, Y., Chen, H., Deng, H., Huang, X., Li, X., Liu, J., Liu, Y., Luo, Z., Wang, J., Wang, W., et al. Emu3. 5: Native multimodal models are world learners. arXiv preprint arXiv:2510.26583, 2025. Deng, C., Zhu, D., Li, K., Gou, C., Li, F., Wang, Z., Zhong, S., Yu, W., Nie, X., Song, Z., et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee, 2009. Esser, P., Rombach, R., and Ommer, B. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1287312883, 2021. Esser, P., Kulal, S., Blattmann, A., Entezari, R., Muller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. et. al., N. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. Fan, L., Tang, L., Qin, S., Li, T., Yang, X., Qiao, S., Steiner, A., Sun, C., Li, Y., Zhu, T., et al. Unified autoregressive visual generation and understanding with continuous tokens. arXiv preprint arXiv:2503.13436, 2025. Fang, A., Jose, A. M., Jain, A., Schmidt, L., Toshev, A., and Shankar, V. Data filtering networks. arXiv preprint arXiv:2309.17425, 2023. Fu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Yang, J., Zheng, X., Li, K., Sun, X., et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. Gadre, S. Y., Ilharco, G., Fang, A., Hayase, J., Smyrnis, G., Nguyen, T., Marten, R., Wortsman, M., Ghosh, D., Zhang, J., et al. Datacomp: In search of the next generation of multimodal datasets. Advances in Neural Information Processing Systems, 36:2709227112, 2023. Heinrich, G., Ranzinger, M., Yin, H., Lu, Y., Kautz, J., Tao, A., Catanzaro, B., and Molchanov, P. Radiov2. 5: Improved baselines for agglomerative vision foundation In Proceedings of the Computer Vision and models. Pattern Recognition Conference, pp. 2248722497, 2025. Hudson, D. A. and Manning, C. D. Gqa: new dataset for real-world visual reasoning and compositional question answering. In CVPR, 2019. 7 OpenVision 3: Family of Unified Visual Encoder for Both Understanding and Generation Huh, M., Cheung, B., Wang, T., and Isola, P. The arXiv preprint platonic representation hypothesis. arXiv:2405.07987, 2024. Hurst, A., Lerer, A., Goucher, A. P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A., Radford, A., et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Labs, B. F. Flux. black-forest-labs/flux, 2024. https://github.com/ Labs, B. F., Batifol, S., Blattmann, A., Boesel, F., Consul, S., Diagne, C., Dockhorn, T., English, J., English, Z., Esser, P., et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742, 2025. Leng, X., Singh, J., Hou, Y., Xing, Z., Xie, S., and Zheng, L. Repa-e: Unlocking vae for end-to-end tuning with latent diffusion transformers. arXiv preprint arXiv:2504.10483, 2025. Li, B., Ge, Y., Ge, Y., Wang, G., Wang, R., Zhang, R., and Shan, Y. Seed-bench: Benchmarking multimodal large language models. In CVPR, 2024a. Li, X., Wang, Z., and Xie, C. Clipa-v2: Scaling clip training with 81.1% zero-shot imagenet accuracy within $10,000 budget; an extra $4,000 unlocks 81.8% accuracy. arXiv preprint arXiv:2306.15658, 2023a. Li, X., Wang, Z., and Xie, C. An inverse scaling law for clip training. Advances in Neural Information Processing Systems, 36:4906849087, 2023b. Li, X., Tu, H., Hui, M., Wang, Z., Zhao, B., Xiao, J., Ren, S., Mei, J., Liu, Q., Zheng, H., et al. What if we recaption billions of web images with llama-3? arXiv preprint arXiv:2406.08478, 2024b. Li, X., Liu, Y., Tu, H., and Xie, C. Openvision: fullyopen, cost-effective family of advanced vision encoders for multimodal learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 3977 3987, 2025. Li, Y., Du, Y., Zhou, K., Wang, J., Zhao, X., and Wen, J.-R. Evaluating object hallucination in large vision-language models. In EMNLP, 2023c. Liao, C., Liu, L., Wang, X., Luo, Z., Zhang, X., Zhao, W., Wu, J., Li, L., Tian, Z., and Huang, W. Mogao: An omni foundation model for interleaved multi-modal generation. arXiv preprint arXiv:2505.05472, 2025. Lin, B., Li, Z., Cheng, X., Niu, Y., Ye, Y., He, X., Yuan, S., Yu, W., Wang, S., Ge, Y., et al. Uniworld: High-resolution 8 semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In European conference on computer vision, pp. 740755. Springer, 2014. Liu, H., Li, C., Li, Y., and Lee, Y. J. Improved baselines In Proceedings of the with visual instruction tuning. IEEE/CVF conference on computer vision and pattern recognition, pp. 2629626306, 2024a. Liu, Y., Li, X., Wang, Z., Zhao, B., and Xie, C. Clips: An enhanced clip framework for learning with synthetic captions. arXiv preprint arXiv:2411.16828, 2024b. Liu, Y., Li, X., Zhang, L., Wang, Z., Zheng, Z., Zhou, Y., and Xie, C. Openvision 2: family of generative pretrained visual encoders for multimodal learning. arXiv preprint arXiv:2509.01644, 2025a. Liu, Z., Ren, W., Liu, H., Zhou, Z., Chen, S., Qiu, H., Huang, X., An, Z., Yang, F., Patel, A., et al. Tuna: Taming unified visual representations for native unified multimodal models. arXiv preprint arXiv:2512.02014, 2025b. Ma, C., Jiang, Y., Wu, J., Yang, J., Yu, X., Yuan, Z., Peng, B., and Qi, X. Unitok: unified tokenizer for visual generation and understanding. arXiv preprint arXiv:2502.20321, 2025. Ma, N., Goldstein, M., Albergo, M. S., Boffi, N. M., VandenEijnden, E., and Xie, S. Sit: Exploring flow and diffusionbased generative models with scalable interpolant transformers. In European Conference on Computer Vision, pp. 2340. Springer, 2024. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023. Qu, L., Zhang, H., Liu, Y., Wang, X., Jiang, Y., Gao, Y., Ye, H., Du, D. K., Yuan, Z., and Wu, X. Tokenflow: Unified image tokenizer for multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 25452555, 2025. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In ICML, 2021. Ranzinger, M., Heinrich, G., Kautz, J., and Molchanov, P. Am-radio: Agglomerative vision foundation model In Proceedings of the reduce all domains into one. IEEE/CVF conference on computer vision and pattern recognition, pp. 1249012500, 2024. OpenVision 3: Family of Unified Visual Encoder for Both Understanding and Generation Yu, S., Kwak, S., Jang, H., Jeong, J., Huang, J., Shin, J., and Xie, S. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024. Zhai, X., Mustafa, B., Kolesnikov, A., and Beyer, L. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1197511986, 2023. Zheng, B., Ma, N., Tong, S., and Xie, S. Diffusion transformers with representation autoencoders. arXiv preprint arXiv:2510.11690, 2025. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Saikh, T., Ghosal, T., Mittal, A., Ekbal, A., and Bhattacharyya, P. Scienceqa: novel resource for question answering on scholarly articles. In IJDL, 2022. Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35: 2527825294, 2022. Tschannen, M., Gritsenko, A., Wang, X., Naeem, M. F., Alabdulmohsin, I., Parthasarathy, N., Evans, T., Beyer, L., Xia, Y., Mustafa, B., et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. Wan, T., Wang, A., Ai, B., Wen, B., Mao, C., Xie, C.-W., Chen, D., Yu, F., Zhao, H., Yang, J., et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Wang, J., Jiang, Y., Yuan, Z., Peng, B., Wu, Z., and Jiang, Y.-G. Omnitokenizer: joint image-video tokenizer for visual generation. Advances in Neural Information Processing Systems, 37:2828128295, 2024. Wu, Y., Zhang, Z., Chen, J., Tang, H., Li, D., Fang, Y., Zhu, L., Xie, E., Yin, H., Yi, L., et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. Xie, J., Yang, Z., and Shou, M. Z. Improved native unified multimodal models. arXiv preprint arXiv:2506.15564, 2025. Show-o2: Xu, H., Xie, S., Tan, X. E., Huang, P.-Y., Howes, R., Sharma, V., Li, S.-W., Ghosh, G., Zettlemoyer, L., and Feichtenhofer, C. Demystifying clip data. arXiv preprint arXiv:2309.16671, 2023. Yao, J., Yang, B., and Wang, X. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1570315712, 2025. Yu, J., Wang, Z., Vasudevan, V., Yeung, L., Seyedhosseini, M., and Wu, Y. Coca: Contrastive captioners are imagetext foundation models. arXiv preprint arXiv:2205.01917, 2022."
        }
    ],
    "affiliations": [
        "JHU",
        "NVIDIA",
        "UC Berkeley",
        "UC Santa Cruz",
        "UNC-Chapel Hill"
    ]
}