{
    "paper_title": "GRIT: Teaching MLLMs to Think with Images",
    "authors": [
        "Yue Fan",
        "Xuehai He",
        "Diji Yang",
        "Kaizhi Zheng",
        "Ching-Chen Kuo",
        "Yuting Zheng",
        "Sravana Jyothi Narayanaraju",
        "Xinze Guan",
        "Xin Eric Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent studies have demonstrated the efficacy of using Reinforcement Learning (RL) in building reasoning models that articulate chains of thoughts prior to producing final answers. However, despite ongoing advances that aim at enabling reasoning for vision-language tasks, existing open-source visual reasoning models typically generate reasoning content with pure natural language, lacking explicit integration of visual information. This limits their ability to produce clearly articulated and visually grounded reasoning chains. To this end, we propose Grounded Reasoning with Images and Texts (GRIT), a novel method for training MLLMs to think with images. GRIT introduces a grounded reasoning paradigm, in which models generate reasoning chains that interleave natural language and explicit bounding box coordinates. These coordinates point to regions of the input image that the model consults during its reasoning process. Additionally, GRIT is equipped with a reinforcement learning approach, GRPO-GR, built upon the GRPO algorithm. GRPO-GR employs robust rewards focused on the final answer accuracy and format of the grounded reasoning output, which eliminates the need for data with reasoning chain annotations or explicit bounding box labels. As a result, GRIT achieves exceptional data efficiency, requiring as few as 20 image-question-answer triplets from existing datasets. Comprehensive evaluations demonstrate that GRIT effectively trains MLLMs to produce coherent and visually grounded reasoning chains, showing a successful unification of reasoning and grounding abilities."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 9 7 8 5 1 . 5 0 5 2 : r GRIT: Teaching MLLMs to Think with Images Yue Fan1 Xuehai He1 Diji Yang1 Kaizhi Zheng1 Ching-Chen Kuo2 Yuting Zheng2 Sravana Jyothi Narayanaraju2 Xinze Guan2 Xin Eric Wang1 1UC Santa Cruz 2eBay https://grounded-reasoning.github.io {yfan71,xwang366}@ucsc.edu Figure 1: Comparison of reasoning with pure natural language and grounded reasoning from GRIT that mixes explicit bounding boxes for image regions with chain of natural language thoughts. Our GRIT method enables MLLMs to perform grounded reasoning with only 20 training samples, realizing clear and reliable process of thinking with images."
        },
        {
            "title": "Abstract",
            "content": "Recent studies have demonstrated the efficacy of using Reinforcement Learning (RL) in building reasoning models that articulate chains of thoughts prior to producing final answers. However, despite ongoing advances that aim at enabling reasoning for vision-language tasks, existing open-source visual reasoning models typically generate reasoning content with pure natural language, lacking explicit integration of visual information. This limits their ability to produce clearly articulated and visually grounded reasoning chains. To this end, we propose Grounded Reasoning with Images and Texts (GRIT), novel method for training MLLMs to think with images. GRIT introduces grounded reasoning paradigm, in which models generate reasoning chains that interleave natural language and explicit bounding box coordinates. These coordinates point to regions of the input image that the model consults during its reasoning process. Additionally, GRIT is equipped with reinforcement learning approach, GRPO-GR, built upon the GRPO algorithm. GRPO-GR employs robust rewards focused on the final answer accuracy and format of the grounded reasoning output, which eliminates the need for data with reasoning chain annotations or explicit bounding box labels. As result, GRIT achieves exceptional data efficiency, requiring as few as 20 image-question-answer triplets from existing datasets. Comprehensive evaluations demonstrate that GRIT effectively trains MLLMs to produce coherent and visually grounded reasoning chains, showing successful unification of reasoning and grounding abilities. Preprint."
        },
        {
            "title": "Introduction",
            "content": "Reasoning models [1, 2, 3, 4, 5] are trained to articulate their problem-solving process through \"reasoning chain\" which comprises detailed, step-by-step deliberation and final answer. Recent studies have shown that such trained reasoning models are superior in reasoning than models directly prompted in the zero-shot Chain-of-Thought (CoT) [6] manner, where off-the-shelf models are simply guided to \"think aloud\" at inference time, often reflecting little of their true internal processes [7]. Reasoning models first became particularly prominent in the language domain [2, 6, 5, 8, 9], with models like DeepSeek-R1 [2] showcasing capabilities for complex tasks, and subsequently extended to the vision-language field [10, 11, 12]. Current open-source vision reasoning models yield reasoning chains consisting only of natural language when faced with multimodal inputs. This results in reasoning that is less clear and insufficiently grounded in the details of the visual input. To overcome this, crucial question is how to empower models to think with images, mirroring how humans refer to visual input in their thoughts. However, realizing this capability presents significant technical challenges. Current MLLMs are designed to generate language tokens, lacking the inherent mechanism to generate images directly within reasoning chain. Furthermore, processing and understanding reasoning sequences that interleave multiple visual elements poses substantial hurdle, as many MLLMs struggle with maintaining context across numerous images in one input. Beyond these technical challenges, data also presents major hurdle. In most cases, there is no unique correct reasoning path for complex multimodal questions, and human-annotated data explicitly incorporating visual evidence in reasoning steps is severely scarce. To address these challenges and enable more grounded visual reasoning in MLLMs, we propose the Grounded Reasoning with Images and Text (GRIT) method. GRIT introduces novel grounded reasoning paradigm where the model generates natural language reasoning chains that freely mix bounding box coordinates pinpointing relevant regions from the input image. These bounding boxes serve to indicate the specific visual information that the model is consulting in its reasoning process. To simplify the inference process, after the generation of bounding box coordinates, the model does not receive additional pixel inputs in the proposed grounded reasoning paradigm; instead, the model comprehends and utilizes the visual information indicated by these coordinates based on its understanding of the original input image. By cropping the input image with the generated bounding boxes, the resulting reasoning chain can be visualized as interleaved text and highlighted regions from the input image, as illustrated in Figure 1. To train MLLMs to produce reasoning chains in the grounded reasoning paradigm, GRIT employs GRPO-GR, reinforcement learning method built upon the GRPO algorithm. It is equipped with novel rewards specifically focused on the format of not only reasoning but also grounding, in addition to answer accuracy. Specifically, such format reward encourages reasoning outputs structured by thinking token pair (e.g., <think> and </think>) and rethink token pair (e.g., <rethink> and </rethink>); it also rewards the inclusion of syntactically valid bounding boxes within the generated sequence. As result, the rewards in GRPO-GR do not constrain the specific textual content of the reasoning steps or the semantic accuracy of the grounded regions, thus eliminating the need for data with reasoning chain annotations or explicit bounding box labels. As result, we find that the GRIT method is extremely data efficient: it enables MLLMs to acquire the grounded reasoning ability with very few data samples sourced from existing VQA datasets using only image-query-answer triplets. With the GRIT method, we train state-of-the-art MLLMsQwen 2.5-VL [3] and InternVL 3 [13] using only 20 imagequestionanswer triplets drawn from existing object-relation and counting VQA datasets, VSR [14] and TallyQA [15]. significant outcome of GRIT is that the trained models preserve their broad versatility, effectively handling not only visual question answering but also grounding-heavy referring expression comprehension tasks. In our experiments with variety of testing data collected from benchmarks for both VQA and referring expression comprehension, we reveal several key observations. Firstly, the trained models effectively unify the grounding and reasoning abilitieswhich were originally inherent but disconnected in the base MLLMswithin their grounded reasoning output. Secondly, through both qualitative and quantitative analysis, we reveal high correlation between the image regions referenced and the accompanying text in the reasoning chain produced by GRIT-trained models. Furthermore, we demonstrate that the generation of bounding boxes boosts the subsequent model reasoning to attend more effectively to the input visual information. Finally, we observe that as training data increases, models trained with GRIT show improved performance but it also reveals challenges for boosting generalizability. Our contributions are as follows: We propose Grounded Reasoning with Images and Text (GRIT), novel method that teaches MLLMs to think with images through grounded reasoning paradigm where models generate reasoning chains interleaving natural language with explicit bounding box coordinates. We develop GRPO-GR, reinforcement learning algorithm, which employs novel rewards that enable the grounded reasoning ability of MLLMs efficiently, using only image-questionanswer triplets without requiring dense reasoning chains or bounding box annotations. Through comprehensive evaluations, we demonstrate that MLLMs trained with GRIT, such as Qwen 2.5-VL and InternVL 3, successfully unify their grounding and reasoning abilities to produce accurate and coherent grounded reasoning."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Reinforcement Learning for Vision-Language Reasoning Recent studies have applied reinforcement learning (RL) with verifiable rewards to build visual reasoning models for visual question-answering tasks, extending approaches from language-only models, such as DeepSeek-R1 [2], to enhance Multimodal Large Language Models (MLLMs). For instance, R1-OneVision [12] and R1-V [16] focus on diagram reasoning and math problems, respectively, while Vision-R1 [11] emphasizes symbolic reasoning tasks. However, these methods often treat visual grounding and textual reasoning as separate or do not tightly integrate them into single generative process. VLM-R1 [10] applies RL to referring expression comprehension tasks, grounding-heavy task, rewarding bounding box, and answer accuracy. While effective for these tasks, VLM-R1 typically outputs only bounding boxes as final answers, with an implicit reasoning process, rather than an interpretable, interleaved trace of text and visual grounding. In contrast, our GRIT (Grounded Reasoning with Images and Text) framework uses RL to train MLLMs to freely mix grounding and reasoning within single generative trace. Models trained with GRIT produce interleaved chains of natural language and bounding box coordinates, enabling dynamic interplay where visual evidence informs textual logic, and vice-versa. While proprietary systems such as ChatGPT-o3/4 [17] have shown similar \"thinking with images\" capabilities, GRIT offers the first open-source approach to achieve this interleaved visual-textual reasoning via lightweight RL, without needing explicit annotations for intermediate reasoning or grounding steps. 2.2 Visual Chain-of-Thought Reasoning The idea of Chain-of-Thought (CoT) reasoning for vision-language tasks predates the RL-focused methods [18, 19, 20, 21], where models are prompted to generate reasoning chains that include visual cues. Early approaches like Multimodal-CoT [19] used multi-stage prompting, while others like CCoT [21] leveraged external tools like scene graphs. These often rely on prompting or auxiliary modules rather than learning an end-to-end generative process for interleaved reasoning. Other works aimed to learn visually grounded CoT with minimal supervision. UV-CoT [22] used self-generated bounding boxes and an auxiliary MLLM for supervision, but still largely separated the grounding and reasoning phases. VisCoT [23] fine-tuned models on dataset with detailed annotations for both textual rationales and corresponding bounding boxes. However, this requires high-quality, dense annotations linking each reasoning step to specific visual evidence. The GRIT method differs by enabling MLLMs to generate explicit, interpretable, and visually grounded reasoning steps from tasklevel reward signals alone, without requiring supervisory signals for bounding boxes or intermediate textual thoughts within the reasoning chain. Visual grounding (via bounding box coordinates) is embedded within the continuous reasoning chain. This allows models trained with GRIT to achieve form of grounded CoT where visual information is directly integrated into the thought process, enabling them to reason \"with\" images, not just \"about\" them."
        },
        {
            "title": "3 GRIT: Grounded Reasoning with Images and Text",
            "content": "3.1 Grounded Reasoning Paradigm The GRIT framework fosters straightforward, grounded reasoning paradigm in MLLMs, enabling more transparent and verifiable reasoning process. Given an image and textual question q, GRIT enables the model to generate two-part output (c, a): first, reasoning chain (starting with <think>), followed by concise final answer (after <answer>). The reasoning chain freely mixes natural-language text and optional bounding-box coordinates B. At any step of the token generation of c, the model can choose to generate bounding box cp or it can opt to continue generating natural language text cp . The decision is based on the input and all existing reasoning chain c1:p1. When the model has finished generating one or multiple bounding box coordinates at step q, these coordinates are intended to directly inform and shape the subsequent reasoning steps cq+1, cq+2, . . . . Crucially, the generation of subsequent tokens does not receive additional pixel inputs based on the generated bounding boxes. Instead, the model relies on its internal understanding, informed by these newly generated coordinates, to continue the reasoning process. This requires the model to learn to interpret its own grounding actions i.e. the bounding boxes, and integrate that understanding into its ongoing textual deliberation. Leveraging the inherent grounding and reasoning abilities in MLLMs, the proposed grounded reasoning paradigm encourages the model to unify these existing faculties to form the new grounded reasoning ability. Compared to alternative approaches such as generating pixel-level attention masks or adding the image region indicated by bounding boxes as additional input for multi-turn generation, the grounded reasoning paradigm is significantly more efficient. 3.2 Reinforcement Learning with GRPO-GR The GRIT method trains MLLMs via newly proposed reinforcement learning algorithm, GRPO-GR, for grounded reasoning ability. Built upon the Group-Relative Policy Optimisation (GRPO) [24] algorithm, GRPO-GR optimizes policy πθ to generate sequences of reasoning (c, a) based on rewards combining answer correctness with format adherence as shown in Figure 2. fixed prompt suffix is appended to the models input during training and inference, please refer to the Appendix for details. RL Formulation. The model acts as policy πθ that generates the output sequence (c, a) given the input (I, q). During training, for every imagequestion pair (I, q), we sample group of candidate completions {o1, . . . , oN } from the current policy πθ. For each completion oi, task reward ri = R(q, oi) is computed based on combination of components (detailed below). These rewards are used to derive group-normalised advantage: Ai = ri mean{r1, . . . , rN } std{r1, . . . , rN } + δ , (1) where δ is small constant (e.g., 108) for numerical stability. The task reward ri is composite signal comprising three components: grounded-reasoning-format reward (rformat), an optional grounded-target-counting reward (rcount), and GPT-aided answeraccuracy reward (rans). These components are designed to encourage the desired grounded reasoning behavior and accurate final answers. Grounded-reasoning-format reward (rformat). This reward encourages reasoning outputs structured by special token pairs and includes syntactically valid bounding boxes. It is composed of specialtoken-format signal sst and bounding-box-format signal sbf: rformat = sst + sbf, (2) rewards the correct usage and order of where sst tokens (<think>...</think> then <rethink>...</rethink>) within the reasoning chain c, which structure the reasoning, potentially across multiple steps. Each correctly placed token pair increments the reward by 0.5: sst = 0.5 I(correct think token pair) + 0.5 I(correct rethink token pair). The bounding box format signal sbf incentivizes the explicit generation of syntactically correct bounding boxes within c. These are detected via regex matching quadruplets of integers separated by commas, typically expected before rethink token. reward of 0.5 is assigned if at least one such bounding reasoning-format special 4 Figure 2: Model update via GRPO-GR. During GRPO-GR training, we sample group of model completions and calculate the grounded-reasoning-format reward (rformat), the optional groundedtarget-counting reward (rcount), and the GPT-aided answer-accuracy reward (rans). The rewards are used to calculate the group-normalized advantage and guide the policy optimization. box is present: sbf = 0.5 I(num_bboxes 1). This reward component encourages the required format and presence of visual grounding elements without constraining the textual content or semantic accuracy of the grounded regions themselves. Grounded-target-counting reward (rcount). This optional reward component is used specifically for training examples drawn from visual counting-related datasets (detailed in the experimental setup). It is set to 0.5 if the number of bounding boxes generated within the reasoning exactly matches the ground-truth count for the target object. This encourages the model to systematically generate the correct quantity of bounding boxes as part of its counting reasoning process. GPT-aided answer-accuracy reward (rans). This reward combines signals for the correctness of the final answer, offering more robust signal than rule-based checks alone by supplementing them with an external Vision-Language Model judge. It is computed as: rans = sGP + 0.1 sBLEU , where sGP is binary correctness score (0 or 1) from GPT-4o judge evaluating the question, predicted answer, and ground truth triplet (q, ˆa, a), and sBLEU is the sentence-level BLEU-1 similarity between ˆa and a. The GPT-4o prompt is fixed and detailed in Appendix D. We down-weight sBLEU due to its sensitivity to length mismatch, ensuring the highest reward is for closely matching ground truth answers. Optimization Objective. The policy parameters θ are updated by maximizing the GRPO objective JGRPO(θ) using the calculated group-normalised advantages Ai: JGRPO(θ) = 1 (cid:104) (cid:88) i=1 min(cid:0)si Ai, clip(cid:0)si, 1 ϵ, 1 + ϵ(cid:1)Ai (cid:1) β DKL (cid:0)πθ(q) πref(q)(cid:1)(cid:105) , (3) where πref is the reference policy (typically the initial pre-trained model), β controls the KL divergence penalty, ϵ defines the trust region for the importance ratio si = πθ(oiq) πθold (oiq) , and θold are the policy parameters before the update. This objective balances maximizing expected reward within each group against maintaining proximity to the reference policy, promoting stable learning."
        },
        {
            "title": "4 Experiments",
            "content": "We first evaluate the grounded reasoning performance of models trained using the GRIT method in both grounding and reasoning perspectives. Then, we further qualitatively and quantitatively analyze the interaction between the bounding boxes and other reasoning contents generated during grounded reasoning. Last but not least, we examine the impact of increasing the training data volume. 4.1 Setup Testing data. We evaluate models trained with GRIT on curated testing sets derived by sampling from six public datasets: Visual Spatial Reasoning (VSR) [14] focusing on spatial relation verification, TallyQA [15] on object counting, GQA [25] on compositional object spatial questions, MME [26] on diverse visual tasks including counting and position, MathVista-mini [27] on mathematical reasoning in visual contexts, and position subset of OVDEval [28] on open-vocabulary object grounding. For assessing the quality of bounding boxes generated during grounded reasoning outputs, we leverage available question-related bounding box annotations from VSR, TallyQA, and GQA and manually refine the bounding box coordinates for VSR and GQA data for our evaluation purpose. We provide more details, including data statistics, in Appendix A. 5 Table 1: Evaluation of the grounded reasoning accuracy. GRIT-trained models are compared with baselines across seven testing sets on GPT-as-judge answer accuracy score (ACC) and grounding IoU (GIoU). GRIT-trained models overall outperform baselines, demonstrating successful unification of grounding and reasoning abilities that are originally inherent but separated in MLLMs. TallyQA MathVista MME OVDEval GQA VSR ACC GIoU ACC GIoU ACC GIoU ACC ACC GIoU Direct query Chain-of-Thought One-shot ICL Few-shot fine-tuning GRIT Direct query Chain-of-Thought One-shot ICL Few-shot fine-tuning GRIT 49.5 37.5 13.2 59.7 72.9 52.9 6.4 3.4 54.0 64.9 0.00 0.122 0.213 0.216 0.325 0.000 0.428 0.435 0.381 0. 40.8 33.2 36.3 44.5 47.8 15.5 6.8 3.7 22.5 44.2 Qwen2.5-VL 3B [8] 0.00 0.113 0.268 0.284 0.447 55.4 39.5 20.4 64.6 62.8 0.00 0.269 0.441 0.475 0. InternVL3 2B [13] 0.000 0.279 0.275 0.116 0.324 29.4 4.1 2.6 46.8 63.2 0.000 0.292 0.435 0.114 0.457 58.5 33.0 29.1 45.0 59.8 43.0 7.5 14.1 17.4 48. 88.9 41.3 24.7 68.3 89.3 40.0 14.0 1.3 62.3 82.0 0.389 0.388 0.328 0.391 0.398 55.1 21.9 59.3 7.8 56.0 Training Data. Demonstrating the data efficiency of our GRIT method, we train on dataset of only 20 unique image-query-answer triplets. This small training set is drawn from the Visual-Spatial Reasoning (VSR) [14] and TallyQA [15] datasets. These data focus on tasks requiring both explicit visual grounding and multi-step reasoning, providing suitable testbed to evaluate GRITs ability to learn grounded reasoning formats with limited data. Please refer to Appendix for more details. Training Implementation. We train two pre-trained MLLMs, Qwen2.5-VL-3B and InternVL-3-2B, directly using the GRIT method with reinforcement learning. We train the models for 200 steps with total batch size of 128. During GRPO-GR training, we generate 4 candidate reasoning traces per input sample during training with learning rate of 210 e-6. The optimizer for the training is AdamW and Cosine scheduler is adopted. All training is conducted on 8 NVIDIA A100 (80GB) GPUs with Deepspeed Zero2 and the time for training each model is approximately 12 hours. 4.2 Research question 1: How accurate is the grounded reasoning from both grounding and reasoning perspectives? Metrics. To comprehensively assess model performance across testing sets, we evaluate two key aspects of their output: (1) the correctness of the natural language answer, evaluated by GPT-as-judge answer accuracy score [29], score between 0 (completely incorrect) and 1 (fully correct) assigned by GPT-4o to judge the correctness of the model-generated natural language answer. The GPT-4o is provided with the same prompt format as in the GPT-aided answer-accuracy reward during training (Section 3.2); (2) the accuracy of the grounding coordinates, measured by grounding IoU, the average Intersection over Union (IoU) between the union of all bounding boxes generated by the model (i.e., within its reasoning chain or as the direct answer for tasks like OVDEval) and the union of all corresponding ground-truth bounding boxes. Different from metrics in traditional object detection tasks, grounding IoU focuses on whether the collection of grounded regions in each grounded reasoning output from the model together aligns with the annotated question-critical image regions. Baselines. We include baselines built upon the same base MLLMs and the same data as the models trained with GRIT: (1) Direct Query directly feeds the original MLLMs only the task query alongside the input image in zero-shot manner, without any specific formatting or reasoning prompts, to assess its raw task-solving ability. (2) Chain-of-Thought (CoT) [6] feeds the original MLLMs queries with fixed prompt suffix, which instructs the model to generate step-by-step reasoning chains before the answer. The prompt suffix for the CoT baseline is identical to the prompt used for models trained with GRIT, measuring the models latent ability without any post-training. (3) One-shot In-Context Learning (ICL) [30] prefixes each test question with single, fixed exemplar consisting of {question, grounded-visual-thinking trace, answer}, instructing the model to imitate the format and reasoning style. (4) Few-shot SFT fine-tunes the MLLM on the same demonstrations employed for GRPO but with standard supervised learning, isolating the effect of reinforcement learning. 6 Question: How many zebras are pictured here? Model output: <think> There are six zebras in the picture. The coordinates for the zebras are as follows: 1. (200, 168, 248, 202) 2. (169, 159, 214, 186) 3. (76, 167, 108, 192) 4. (24, 173, 50, 197) 5. (51, 163, 70, 191) 6. (413, 159, 441, 189) 7. (463, 171, 483, 186) </think> <rethink> The coordinates provided for the zebras are accurate and cover all the zebras visible in the image. There are no overlapping or missing coordinates. </rethink> <answer> 7 Ground truth answer: (i) Model provides an answer with regions and then reflects on it in the subsequent reasoning. Question: Is the truck beneath the cat? Model output: <think> The bounding box coordinates for the truck are approximately (0, 209, 488, 364). The cat is positioned on top of this truck. </think> <rethink> The cat is on top of the truck, so the truck is beneath the cat. </rethink> <answer> Yes Ground truth answer: Yes (ii) Model first grounds critical image region in its reasoning and then analyze in the subsequent reasoning. Question: Is there knife in the image? Model output: <think> There is no knife present in the image. The focus is on pot of soup with carrots and other ingredients. The bounding box coordinates for the knife would be outside the image area. </think> <rethink> The image shows pot of soup with carrots and other ingredients, but there is no knife present. The bounding box coordinates for the knife would be outside the image area. </rethink> <answer> No Ground truth answer: no (iii) Model correctly handles queries about non-existent entities without any grounding action. Figure 3: Inference examples of Qwen2.5-VL-GRIT. Results. The results are summarized in Table 1. In contrast to baselines, models trained with GRIT overall achieve higher scores on the adopted testing data. Despite being trained on only 20 training samples, models trained with GRIT not only improve on GPT-as-judge answer accuracy Scores on VSR and TallyQA (the two datasets seen during training) but also generalize effectively to other out-of-domain data, indicating strong reasoning ability. The results on the grounding IoU metric show that models trained with GRIT, although not directly optimized for this metric, outperform baselines in locating question-relevant image regions during their reasoning. Notably, on the OVDEval testing data, models trained with GRIT achieve more accurate detection results than zero-shot MLLMs, highlighting their emerging improvements in grounding capabilities. These results demonstrate more successful unification of grounding and reasoning, where their integration demonstrably enhances the performance of both individual abilities. From the result, we also observe that baselines based on off-the-shelf MLLMs exhibit rigidity, where they tend to generate either only bounding boxes or the final answer. For CoT and one-shot ICL baselines, although they are prompted or guided by ICL to produce reasoning chain interleaved with bounding box coordinates, their grounding and reasoning functions are forced to operate concurrently. As result, they generally face severely deteriorated performance either in answer accuracy or grounding IoU, indicating that these capabilities remain largely separated and can interfere with each other, resulting in suboptimal performance. This suggests general disconnect between their inherent grounding and reasoning abilities. As for the Few-shot SFT baseline, while demonstrating more balanced performance across grounding IoU and GPT-as-judge answer accuracy score compared to the zero-shot baselines, it still achieves considerably lower scores than models trained with our GRIT method. This is likely because supervised fine-tuning primarily learns to mimic the surface form of outputting text interspersed with bounding boxes, rather than developing deeply integrated reasoning process where grounding logically informs each step. 7 Figure 4: Correlation between image regions and \"thoughts\" in grounded reasoning evaluated by our Vision-Language Reasoning Cross-Modal Correlation metric. The result shows that models trained with GRIT outperform baselines. Figure 5: Models average attention for image tokens during the generation of rethink segments. The overall higher curve for the original rethink segments of the GRIT-trained model shows that the bounding boxes generated facilitate stronger attention to the image input in subsequent reasoning. 4.3 Research question 2: How generated bounding boxes interact with natural-language contents during grounded reasoning? Qualitative Analysis of Grounded Reasoning Contents. We qualitatively showcase the grounded reasoning produced by models trained with GRIT through variety of input-output examples. major strength of training with the GRIT method is that its reward design imposes no rigid constraint on the content of the reasoning chain. Consequently, the model learns flexible reasoning strategies that adapt seamlessly to diverse queries. Specifically, as shown in Figure 3i, GRIT-trained models sometimes directly provide an answer while then reflecting the answer in subsequent reasoning; while in other instances, as in Figure 3ii, they perform the ground actions to identify visual evidence initially and subsequently analyze the selected image regions in their reasoning. Crucially, our models dynamically determine whether grounding is necessary, significantly reducing false-positive grounding instances. For example, in Figure 3iii, queries referencing non-existent entities in the input image do not prompt erroneous groundings, showcasing robust multimodal reasoning capability. Cross-modal Correlation of Images Regions and Thoughts. To systematically evaluate the coherence between image regions and the natural language contents interleaved in the grounded reasoning chain of models trained with GRIT, we introduce the Vision-Language Reasoning CrossModal Correlation metric. Given each model-generated reasoning chain (c), we extract the associated bounding boxes {cici B}. To establish rigorous evaluation, we randomly sample an equal number of bounding boxes from the input image as negative candidates {h0, ...hj} B. We draw these two sets of boxes separately on the input image and then ask GPT-4o to identify the one from two input images with the set of bounding boxes that corresponds most closely with the textual reasoning with bounding box coordinates masked. The prompt used is shown in Appendix D. Leveraging GPT-4os strong Set-of-Mark (SoM) capabilities [31], this process allows us to quantitatively assess semantic coherence. We repeat this process three times for robustness and report the average correlation score with standard deviation. We evaluate the output from models trained with GRIT against the Chain-of-Thought and Few-shot fine-tuning outputs using this cross-modal correlation metric on six testing sets (VSR, TallyQA, GQA, MME, and MathVista), excluding OVDEval as it primarily poses grounding challenge. In addition to evaluating model-generated outputs, we also manually create and evaluate 20 human-written reasoning chains with interleaved bounding boxes using the same vision-language reasoning cross-modal correlation method to establish human performance baseline. As shown in Figure 4, models trained with our GRIT framework outperform both baselines, showing highly correlated image region selection with textural reasoning, while still exhibiting gap when compared to the human-written reasoning chains, indicating room for future improvement. Influence of Grounding on Subsequent Reasoning. Models trained with GRIT interleave reasoning with grounding actions when producing visually integrated thought processes. To further understand how the bounding boxes generated impact subsequent reasoning, we examine attention scores for input visual tokens during inference. Specifically, we split outputs of Qwen2.5-VL model trained with GRIT using the <rethink> token into pre-rethink and rethink segments, where pre-rethink segments usually include bounding-box coordinates due to the optimization guided by the grounded8 reasoning-format reward detailed in Section 3.2. We then create an alternative pre-rethink segment by removing all bounding boxes from the pre-rethink segment, simulating the situation where no grounding action is done during the reasoning. Next, we feed the alternative pre-rethink segment back into the Qwen2.5-VL model trained with GRIT for continuous token generation. We refer to the newly generated content based on the modified input as the no-bounding-box rethink content. Finally, we compute and compare average attention scores across various layers for input visual tokens during the generation of both the original and no-bounding-box rethink content. Such comparison is repeated for 100 randomly selected data samples in the GQA subset, and due to the various generation lengths, we show the results for the first 50 tokens generated. From the results shown in Figure 5, we find that the average attention scores to each visual token is overall significantly higher in the original rethinking segment than the no-bounding-box rethink segment. This indicates the presence of bounding boxes in the original pre-rethink segment leads to increased visual attention in the following reasoning chain, which potentially benefits the image consistency of the reasoning process. 4.4 Research question 3: What is the effect of scaling training data? To understand how GRITs performance scales with increasing training data, beyond the data efficiency shown with 20 samples, we trained variants of the Qwen2.5-VL 2B model using 20, 500, and 7,000 imagequestionanswer triplets. These samples were taken from the VSR and TallyQA datasets, maintaining the same mix of data sources. We evaluate these trained variants on the testing data sets introduced in Section 4.1 with the GPT-as-judge answer accuracy score. Figure 6 presents the results across testing data categorized as either in-domain, sourced from the same datasets as training (VSR, TallyQA), or out-of-domain (GQA, MathVista-mini). The results show that answer accuracy generally increases with more training data used for GRIT. We find that the improvements in answer accuracy are more subtle for out-of-domain testing data compared to the growth observed on in-domain testing data, which highlights common challenge of generalization. Furthermore, we observe that the models performance growth diminishes, suggesting that continued improvement may require exponentially more data. We interpret this observation in light of recent studies suggesting that Reinforcement Learning with verifiable rewards primarily biases existing reasoning patterns towards rewarded outcomes rather than fundamentally changing them [32], with performance also heavily influenced by pre-training [33, 34]. This implies that to significantly enhance the out-of-domain performance of GRIT-trained models, increasing the diversity of training data is more critical than simply scaling up the data volume, highlighting key direction for future study. Figure 6: Effect of scaling training data on model performance."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we propose Grounded Reasoning with Images and Text (GRIT), novel method where models generate visually grounded reasoning chains by interleaving natural language with explicit bounding box coordinates referencing relevant image regions. We propose the grounded reasoning paradigm and the GRPO-GR reinforcement learning training method. As result, GRIT is extremely data efficient and requires neither dense reasoning chains nor explicit bounding box annotations. Our experiments demonstrate that GRIT effectively trains pre-trained MLLMs to produce accurate and coherent grounded reasoning. While limited resources focus our study on smaller MLLMs to validate the GRIT method and efficiency, rather than exploring peak performance with full-scale scaling, we identify key direction for further improving the generalizability of GRIT-trained model being data variety and model pre-training. Nonetheless, GRITs efficacy in instilling complex grounded reasoning with minimal data highlights its significant potential."
        },
        {
            "title": "6 Acknowledgments",
            "content": "We thank Qianqi Yan, Kaiwen Zhou, and Zhen Zhang, members of the ERIC Lab, for their valuable feedback and engaging discussions. We are also grateful to Orby AI for generously providing the computing resources."
        },
        {
            "title": "References",
            "content": "[1] OpenAI. Introducing introducing-openai-o1-preview/, 2024. openai o1-preview. https://openai.com/index/ [2] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [3] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [4] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. [5] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [6] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [7] Yanda Chen, Joe Benton, Ansh Radhakrishnan, Jonathan Uesato Carson Denison, John Schulman, Arushi Somani, Peter Hase, Misha Wagner Fabien Roger Vlad Mikulik, Sam Bowman, Jan Leike Jared Kaplan, et al. Reasoning models dont always say what they think. Anthropic Research, 2025. [8] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. [9] Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769, 2024. [10] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. [11] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. [12] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025. [13] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. [14] Fangyu Liu, Guy Emerson, and Nigel Collier. Visual spatial reasoning. Transactions of the Association for Computational Linguistics, 11:635651, 2023. 10 [15] Manoj Acharya, Kushal Kafle, and Christopher Kanan. Tallyqa: Answering complex counting questions. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 80768084, 2019. [16] Liang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci. R1-v: Reinforcing super generalization ability in vision-language models with less than $3. https://github.com/Deep-Agent/ R1-V, 2025. Accessed: 2025-02-02. [17] OpenAI. Introducing openai o3 and o4-mini. https://openai.com/index/ introducing-o3-and-o4-mini/, 2025. [18] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2022. [19] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923, 2023. [20] Zhenfang Chen, Qinhong Zhou, Yikang Shen, Yining Hong, Zhiqing Sun, Dan Gutfreund, and Chuang Gan. Visual chain-of-thought prompting for knowledge-based visual reasoning. In AAAI Conference on Artificial Intelligence, 2024. [21] Chancharik Mitra, Brandon Huang, Trevor Darrell, and Roei Herzig. Compositional chain-ofthought prompting for large multimodal models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1442014431, 2024. [22] Kesen Zhao, Beier Zhu, Qianru Sun, and Hanwang Zhang. Unsupervised visual chain-of-thought reasoning via preference optimization. arXiv preprint arXiv:2504.18397, 2025. [23] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual cot: Advancing multi-modal language models with comprehensive dataset and benchmark for chain-of-thought reasoning. Advances in Neural Information Processing Systems, 37:86128642, 2024. [24] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [25] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709, 2019. [26] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models, 2024. [27] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. [28] Yiyang Yao, Peng Liu, Tiancheng Zhao, Qianqian Zhang, Jiajia Liao, Chunxin Fang, Kyusong Lee, and Qing Wang. How to evaluate the generalization of detection? benchmark for comprehensive open-vocabulary detection. arXiv preprint arXiv:2308.13177, 2023. [29] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. [30] Tom Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. [31] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023. [32] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. [33] Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. Openvlthinker: An early exploration to complex vision-language reasoning via iterative selfimprovement. arXiv preprint arXiv:2503.17352, 2025. [34] Hardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, and Cihang Xie. Sft or rl? an early investigation into training r1-like reasoning large vision-language models. arXiv preprint arXiv:2504.11468, 2025. 12 Table 2: Statistics for the testing data used in the experiments. We collect the testing data from seven diverse benchmarks. Data source TallyQA GQA MathVista MME OVDEval VSR Counts 288 491 1000 240 2164 Avg question/answer length 6.7/1.0 6.0/1. 7.1/1.0 38.2/1.2 13.3/1.0 16.4/4 Ratio of multi-choice and yes/no questions (%) Ratio of annotated grounding targets (%) 71.2 0 58.9 70.8 100 58.8 25.6 25.3 - - 17."
        },
        {
            "title": "A Details for Testing Data",
            "content": "For our evaluation, we curated testing data derived from seven public open-source datasets covering range of visual reasoning and grounding tasks. The statistic for the testing data is shown in Table 2. VSR [14] tests spatial relation verification. For our VSR evaluation set, we source questionimage-answer triplets from the VSR subset of the Visual CoT benchmark [23] and manually filter out those with ambiguous answers. TallyQA [15] focuses on counting; we uniformly sample evaluation questions where the target object counts range from 0 to 9 to create our TallyQA evaluation set. GQA [25] offers scene-graph-grounded, compositional object spatial questions. We first take the GQA subset from the Visual CoT benchmark and then manually filter these to retain high-quality instances for our GQA evaluation set. From MME [26], we use only the counting, position, and existence subsets to broaden our evaluation scope. MathVista [27] evaluates mathematical reasoning in visual contexts. Following prior works, we adopt its TestMini split. Finally, OVDEval [28] is an open-vocabulary detection (OVD) testing set that requires the model to ground fine-grained semantics from the language query to the coordinates of visual features. We use its position subset and simplify it to object detection tasks with single target. Among these evaluation sets, those derived from VSR, TallyQA, and GQA are accompanied not only by ground-truth language answers but also by annotations of bounding boxes for image regions critical for deriving the answer. Specifically, we manually refine the bounding box coordinates from the Visual CoT benchmark for our VSR and GQA data. For our TallyQA evaluation sets, we adopt the original bounding box annotations. All evaluation sets, except for OVDEval, are VQA benchmarks where the required answer is single word or short phrase. In contrast, OVDEval differs as grounding is not an optional component of the reasoning chain but is explicitly required as the answer to queries."
        },
        {
            "title": "B Details for Training data",
            "content": "To demonstrate the data efficiency of our GRIT method, we collect small training dataset consisting of only 20 unique image-query-answer triplets. These triplets are sourced from existing open-source VQA datasets, covering both grounding and visual reasoning challenges. Specifically, we selected ten from the Visual Spatial Reasoning (VSR) dataset [14] and ten from the TallyQA dataset [15]. We chose examples from VSR as they typically involve object localization and spatial relation reasoning (e.g., \"Is the motorcycle away from the bird?\"). From TallyQA, we selected counting tasks (e.g., \"How many signs are on the post?\"), specifically ensuring the chosen examples uniformly cover object counts from 0 to 4, as these tasks naturally lend themselves to explicit grounded counting within the reasoning process. This curated dataset engages the models in multi-step visual analysis, serving to evaluate GRITs ability to strengthen the link between visual grounding and logical deliberation. 13 Table 3: Ablation study on the importance of counting data and grounded-target-counting reward. Comparison of the original GRIT-trained model, trained with 10 VSR + 10 TallyQA and counting reward, with an ablated variant, trained with 20 VSR without counting reward. Results show performance decrease in the ablated model. GIoU ACC GRIT GRIT w/o counting data & reward In-domain Out-of-domain 0.387 0.349 0.437 0.378 In-domain Out-of-domain 51.8 53.8 64.4 60.0 Ablation on Counting-related Training Data and Reward To validate the importance of including counting-related training data and the associated groundedtarget-counting reward within the GRIT method, we conduct an ablation experiment. The standard training utilizes data from TallyQA, which consists of queries about object quantities in images with single-digit numerical answers. For these counting tasks, we employ the grounded-target-counting reward (detailed in Section 3.2), designed to encourage the generation of flexible number of bounding boxes matching the count in the reasoning output. In the ablation, we train the InternVL-3 2B model with modified dataset and reward function. Instead of using 10 VSR and 10 TallyQA samples with the full reward set, we train with dataset of 20 VSR samples and exclude the grounded-target-counting reward during training. This allows us to isolate the contribution of the counting-related data and reward component. We report the GPT-as-judge answer accuracy score and the grounding IoU of both in-domain and out-of-domain data (same as in Section 4.4). The results, presented in Table 3, show that excluding the counting-related data and grounded-target-counting reward during training leads to significant performance decrease in grounding, as indicated by lower Grounding IoU score compared to the original GRIT-trained model. Furthermore, we observe that this exclusion negatively impacts the models answer accuracy on out-of-domain data. As result, this ablation study underscores the importance of including both counting-related training data and the grounded-target-counting reward within the GRIT method."
        },
        {
            "title": "D Prompts",
            "content": "We append the prompt shown in Figure 7 to the GRIT model training and model inference, as well as for the Chain-of-Though baseline in the experiments. The prompt provides models with the instruction to follow the grounded reasoning paradigm, however, as shown in the experiment result, MLLMs without training will face significant performance drop. This is due to the instruction in the prompt requiring the MLLMs to perform grounding and reasoning at the same time, which is very challenging for them in zero-shot manner. It is worth noticing that although the prompt mentions using \"JSON\" to show bounding boxes, the GRIT-trained model does not always adhere to such format. This is intentional, as the reward in GRPO-GR uses regex to identify valid bounding boxes, rather than relying on the JSON format, providing the RL optimization of model policy with more search space. It also indicates that the prompt suffix only provides an initialization for the policy which can be adjusted with minimal impact as long as it includes an instruction of the grounded reasoning paradigm. First, think between <think> and </think> while output necessary coordinates needed to answer the question in JSON with key bbox_2d. Then, based on the thinking contents and coordinates, rethink between <rethink> </rethink> and then answer the question after <answer>. Figure 7: Prompt suffix that is appended to the input of models during the training and inference. Additionally, in GPT-aided answer-accuracy reward of GRPO-GR and the GPT-as-judge answer accuracy score in the experiments, we adopt the prompt format in Figure 8. Furthermore, in the experiments where we compute the Vision-Language Reasoning Cross-Modal Correlation metric for model outputs, we leverage GPT-4o to determine if the bounding boxes in the grounded reasoning chains of models are more correlated to the natural language reasoning contents than the randomly sampled bounding boxes. The prompt format we used is in Figure 9. You are responsible for proofreading the answers, you need to give score to the models answer by referring to the standard answer, based on the given question. The full score is 1 point and the minimum score is 0 points. Please output the score in the json form \"{score: <score>}\". The evaluation criteria require that the closer the models answer is to the standard answer, the higher the score. Question: {$question} Standard answer: {$answer} Models answer: {$predicted_content} Figure 8: Prompt format for GPT-as-judge answer accuracy score and GPT-aided answer-accuracy reward. The $question and $answer are substituted with the query and ground truth in the data sample and $predicted_content is replaced by the model output. Please decide which image has the bounding boxes that match the following description: {$grounded_reasoning_masked} Reply with exactly \"Image 0\" or \"Image 1\". Figure 9: Prompt format for calculating the Vision-Language Reasoning Cross-Modal Correlation metric. Here, the input images for GPT-4o are the same image drawn with two sets of bounding boxes, one from the generated grounded reasoning chain and one from random sampling. The $grounded_reasoning_masked is the generated grounded reasoning chain with any bounding box coordinates removed."
        },
        {
            "title": "E More Examples",
            "content": "Question: What kind of animal stands on the ground? Model output: <think> The image shows two animals standing on the ground. The larger animal is cow with white and black coat, and the smaller animal is calf. The bounding box coordinates for the cow are approximately (42, 73, 433, 296), and for the calf, they are approximately (199, 160, 363, 293). </think> <rethink> The question asks about the animal that stands on the ground. Both the cow and the calf are standing on the ground, so either could be the correct answer. However, the cow is the larger and more prominent animal in the image. </rethink> <answer> cow Ground truth answer: cow (i) Our GRIT model accurately perceives the grounded target, reasons about the potential ambiguity in the question, and finally outputs correct answer. Question: Is there only one person appear in the image? Please answer yes or no. Model output: <think> There is only one person in the image. bbox_2d_1: [0, 28, 305, 364] bbox_2d_2: [275, 0, 410, 280] </think> <rethink> The image shows two people, one in the foreground playing tennis and another in the background. bbox_2d_1: [0, 28, 305, 364] bbox_2d_2: [275, 0, 410, 280] </rethink> <answer> No Ground truth answer: No (ii) Although the bounding boxes generated during the grounded reasoning are slightly off, our GRIT model is still able to analyze the related image regions and successfully correct its own wrong answer at the beginning and finally generate correct answer."
        }
    ],
    "affiliations": [
        "UC Santa Cruz",
        "eBay"
    ]
}