{
    "paper_title": "VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in One Step",
    "authors": [
        "Hanyang Wang",
        "Fangfu Liu",
        "Jiawei Chi",
        "Yueqi Duan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recovering 3D scenes from sparse views is a challenging task due to its inherent ill-posed problem. Conventional methods have developed specialized solutions (e.g., geometry regularization or feed-forward deterministic model) to mitigate the issue. However, they still suffer from performance degradation by minimal overlap across input views with insufficient visual information. Fortunately, recent video generative models show promise in addressing this challenge as they are capable of generating video clips with plausible 3D structures. Powered by large pretrained video diffusion models, some pioneering research start to explore the potential of video generative prior and create 3D scenes from sparse views. Despite impressive improvements, they are limited by slow inference time and the lack of 3D constraint, leading to inefficiencies and reconstruction artifacts that do not align with real-world geometry structure. In this paper, we propose VideoScene to distill the video diffusion model to generate 3D scenes in one step, aiming to build an efficient and effective tool to bridge the gap from video to 3D. Specifically, we design a 3D-aware leap flow distillation strategy to leap over time-consuming redundant information and train a dynamic denoising policy network to adaptively determine the optimal leap timestep during inference. Extensive experiments demonstrate that our VideoScene achieves faster and superior 3D scene generation results than previous video diffusion models, highlighting its potential as an efficient tool for future video to 3D applications. Project Page: https://hanyang-21.github.io/VideoScene"
        },
        {
            "title": "Start",
            "content": "VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in One Step Hanyang Wang*, Fangfu Liu*, Jiawei Chi, Yueqi Duan Tsinghua University 5 2 0 2 2 ] . [ 1 6 5 9 1 0 . 4 0 5 2 : r Figure 1. VideoScene enables one-step video generation of 3D scenes with strong structural consistency from just two input images. The top row shows the input sparse views and the following two rows show the output novel-view video frames."
        },
        {
            "title": "Abstract",
            "content": "Recovering 3D scenes from sparse views is challenging task due to its inherent ill-posed problem. Conventional methods have developed specialized solutions (e.g., geometry regularization or feed-forward deterministic model) to mitigate the issue. However, they still suffer from performance degradation by minimal overlap across input views with insufficient visual information. Fortunately, recent video generative models show promise in addressing this challenge as they are capable of generating video clips with plausible 3D structures. Powered by large pretrained video diffusion models, some pioneering research start to explore the potential of video generative prior and create 3D scenes from sparse views. Despite impressive improvements, they are limited by slow inference time and the lack of 3D constraint, leading to inefficiencies and reconstruction artifacts In that do not align with real-world geometry structure. this paper, we propose VideoScene to distill the video diffusion model to generate 3D scenes in one step, aiming to build an efficient and effective tool to bridge the gap from video to 3D. Specifically, we design 3D-aware leap flow distillation strategy to leap over time-consuming re- *Equal contribution. The corresponding author. dundant information and train dynamic denoising policy network to adaptively determine the optimal leap timestep during inference. Extensive experiments demonstrate that our VideoScene achieves faster and superior 3D scene generation results than previous video diffusion models, highlighting its potential as an efficient tool for future video to 3D applications. Project Page: https://hanyang21.github.io/VideoScene. 1. Introduction The demand for efficient 3D reconstruction is growing rapidly, driven by applications in real-time gaming [83], autonomous navigation [1], and beyond [57, 97]. Techniques like NeRF [59] and 3DGS [35] have pioneered highquality, dense-view reconstruction and demonstrated impressive performance in realistic scene generation. However, these methods typically require numerous professionally captured images, limiting accessibility [84]. To overcome this, researchers have begun exploring 3D reconstruction from sparse views [13, 84, 95, 99], reducing the input requirements to even two casually captured images. Previous work has developed various specialized methods to tackle the challenges of sparse-view reconstruction, such as geometry regularization techniques [70, 95] for 1 sparse inputs and feed-forward models [13, 17] trained to create 3D scene from few images. While these methods produce reasonable scene-level results from nearby viewpoints, they struggle to overcome the under-constrained nature of sparse-view reconstruction, where complex 3D structures have to be inferred from limited visual information [22]. The scarcity of views makes it difficult to recover complete and coherent 3D scene. Thus, there is need for methods that can both integrate minimal visual information and generate plausible missing details to reconstruct realistic 3D geometry. Recent advancements in video generative models [11, 30, 94, 96] offer new promise, as these models are capable of generating sequences with plausible 3D structures. Leveraging large, pretrained video diffusion models, early studies [16, 47, 49, 51, 76, 100] have explored using video generative priors for 3D scene creation from sparse views. Despite these advances, current models face significant limitations, particularly in two areas: (1) lengthy inference time as video diffusion models require multiple denoising steps to generate high-quality video from pure noisy input, making them far less efficient than many feedforward methods; (2) lack of 3D constraints as these models are trained on 2D video data, focusing on RGB space and temporal consistency rather than stable 3D geometry. As result, generated videos often lack spatial stability and accurate camera geometry, leading to reconstruction artifacts that hinder their effectiveness for real-world 3D applications. This motivates us to build an efficient and effective tool to bridge the gap from video to 3D. In this paper, we introduce VideoScene, novel video distillation framework that optimizes video diffusion models for efficient, one-step 3D scene generation. Our approach tackles the inefficiencies of traditional diffusion steps by reducing redundant informationsuch as dynamic motions and object interactionsthat detract from 3D consistency. We identify that the primary bottleneck stems from the low-information starting point of pure noise in traditional denoising, which makes distillation slow and unstable. To address this, we propose 3D-aware leap flow distillation strategy to leap over time-consuming denoising stages. Specifically, we first give two input images with corresponding camera poses (can also be estimated via COLMAP [67] or DUSt3R [86]). Here, We focus on image pairs for two main reasons. First, monocular 3D reconstruction is inherently ill-posed; using two images enables triangulation between rays from different viewpoints, allowing for more robust 3D estimation. Second, two-view geometry serves as the fundamental building block for multi-view geometry, which can be extended to full multi-view reconstruction. Then We use rapid, feed-forward sparse-view 3DGS model [17] to generate coarse but 3D-consistent scene, rendering frames along an interpolated camera path. This initial 3D-aware video establishes strong prior that guides subsequent diffusion steps, enabling us to leap over uncertain phases and focus on the deterministic target scene in the consistency distillation [75]. Additionally, we develop dynamic denoising policy network (DDPNet) that learns to adaptively select the optimal leap timestep during inference. This policy maximizes the use of 3D priors, balancing noise addition with information retention, thereby improving efficiency without sacrificing quality. Extensive experiments demonstrate that VideoScene outperforms existing video diffusion methods in both fidelity and speed across range of real-world datasets [7, 11, 45]. VideoScene shows significant potential as versatile plugand-play tool for future applications in 3D scene reconstruction from video generation models as shown in Fig. 1. In summary, our main contributions are: We introduce VideoScene, novel video distillation framework that distills video diffusion models to generate 3D scenes in one step. We propose 3D-aware leap flow distillation strategy to leap over low-information steps by providing 3D prior constraints. We design dynamic denoising policy network to decide optimal leap timesteps by integrating contextual bandit learning into the distillation process. Extensive experiments demonstrate that our VideoScene outperforms current methods in both quality and efficiency across diverse real-world datasets. 2. Related Work Video Generation. Efficient and high-quality video generation has become highly popular topic in recent research. Early methods [4, 23, 69, 81] primarily relied on generative adversarial networks (GANs), which often resulted in low quality and poor generalization to unseen domains [9]. With the rise of diffusion models in text-to-image generation, recent studies [8, 29, 30, 85, 88, 90, 92, 96] have explored the potential of diffusion-based text-to-video (T2V) generation, achieving promising results. Some methods [9, 85] focus specifically on generating temporally consistent videos, incorporating temporal modules into 2D UNet architectures and are trained on large-scale datasets. Many works have also explored image-conditioned video synthesis. Image-to-video (I2V) diffusion models, such as DynamiCrafter [94], SparseCtrl [26], and PixelDance [102], show strong versatility for tasks like video interpolation and transition. As datasets continue to grow, the SORA-like DiT-based (Transformer-based Diffusion Model) [11, 62] video generation models show clear advancements over earlier UNet-based models [8, 15, 25]. These models offer enhanced expressive capabilities, improved visual quality, and effective multi-modal integration. With these improvements, they achieve near-production-level performance and 2 demonstrate strong potential for commercial applications. Consistency Model. Diffusion models [28, 64, 71, 74] have demonstrated strong performance across various generative tasks. However, they face speed bottleneck due to the need for large number of inference steps. To address this, researchers have proposed several solutions, including ODE solvers [54, 71], adaptive step-size solvers [32], neural operators [104], and model distillation [37, 58, 65, 66]. Among these approaches, consistency models [75] have shown particular promise. Based on the probability flow ordinary differential equation (PF-ODE), consistency models are trained to map any point in the generation process back to the starting point, or the original clean image. This approach enables one-step image generation without losing the benefits of multi-step iterative sampling, supporting high-quality output. Consistency models can be derived in two ways: either through distillation from pretrained diffusion model (i.e., Consistency Distillation) or by training directly from scratch (i.e., Consistency Training). Building on this framework, LCM [55, 56] further explores consistency models within latent space to reduce memory consumption and enhance inference efficiency. Subsequent methods [24, 53, 73] have also refined these efficiency improvements, achieving impressive results. Video for 3D Reconstruction. In 3D reconstruction, methods like NeRF [59] and 3DGS [35] typically require hundreds of input images for per-scene optimization, which is impractical for casual users in real-world applications. Recent research [13, 17, 48, 50, 78, 91, 93] has focused on developing feed-forward models that generate 3D representations directly from only few input images. pixelSplat [13] devised pixel-aligned features for 3DGS reconstruction, using the epipolar transformer to better extract scene features. Following that, MVSplat [17] introduces multi-view feature extraction and cost volume construction to capture cross-view feature similarities for depth estimation. The per-view depth maps are predicted and unprojected to 3D to form Gaussian centers, producing highquality 3D Gaussians in faster way. While these models can produce highly realistic images from provided viewpoints, they often struggle to render high-quality details in areas not visible from these limited perspectives. To address this challenge, many studies [16, 47, 51, 76, 100] have employed large-scale video diffusion models to generate pseudo-dense views from sparse input, aiming to transfer generalization capabilities for the under-constrained sparseview reconstruction. However, the inefficiency of multistep denoising in diffusion models slows down these methods, and the generated video often lacks 3D constraints, resulting in structural inconsistencies and unrealistic motion. To address these limitations, we propose the VideoScene framework, which uses 3D-aware leap flow distillation to incorporate 3D priors for consistent 3D video generation while accelerating diffusion denoising in single step for fast, high-quality generation. 3. Method 3.1. Preliminaries By interpreting the guided reverse diffusion process as solving an augmented probability flow ODE (PF-ODE) [54, 74], consistency model [75] offers generative approach that supports efficient one-step or few-step generation by directly mapping noise to the initial point of PF-ODE trajectory (i.e., the solution of the PF-ODE). This mapping is achieved through consistency function, defined as : (xt, t) (cid:55) xϵ, where ϵ is fixed small positive number, is the current denoising step, and xt represents the noisy input. One important self-consistency property that the consistency function should satisfy can be formulated as: (xt, t) = (xt, t), t, [ϵ, ], (1) where donates the overall denoising step. This property ensures that the function outputs remain consistent across different denoising steps within the defined interval. The consistency function is parameterized as deep neural network fθ, with parameters θ to be learned through the Consistency Distillation. The distillation goal is to minimize the output discrepancy between random adjacent data points, thereby ensuring self-consistency in the sense of probability. Formally, the consistency loss is defined as follows: L(θ, θ; Φ) = Ex,t (cid:16) (cid:104) fθ(xtn+1 , tn+1), fθ (ˆxϕ tn , tn) (cid:17)(cid:105) , (2) where Φ() denotes the one-step ODE solver applied to PFODE, the model parameter θ is obtained from the exponential moving average (EMA) of θ, and d(, ) is chosen metric function for measuring the distance between two samples. Here, ˆxϕ tn is the estimation of xtn from xtn+1: ˆxϕ tn xtn+1 + (tn tn+1)Φ(xtn+1, tn+1; ϕ). (3) LCM [55] conducts the above consistency optimization in the latent space and applies classifier-free guidance [27] in Eq. 3 to inject control signals, such as textual prompts. ˆxϕ tn is estimated by teacher model with an ODE solver in consistency distillation. With well-trained consistency model fθ, we can generate samples by sampling from the initial distribution xT (0, 2I) and then evaluating the consistency model for xϵ = fθ(xT , ). Ideally, this involves only one forward pass through the consistency model and therefore generates samples in single step. 3.2. Challenges in Video to 3D Video diffusion models have shown impressive results and great potential, especially in 3D reconstruction. However, when generating videos in static scene, undesirable variations often arise [101]such as human motions, object 3 Figure 2. Pipeline of VideoScene. Given input pair views, we first generate coarse 3D representation with rapid feed-forward 3DGS model (i.e., MVSplat [17]), which enables accurate camera-trajectory-control rendering. The encoded rendering latent (input) and encoded input pairs latent (condition) are combined as input to the consistency model. Subsequently, forward diffusion operation is performed to add noise to the video. Then, the noised xr of timestep tn+1 and ˆxϕ 0 of timestep tn. Finally, the student model and DDPNet are updated independently through distillation loss and DDP loss. n+1 is sent to both the student and teacher model to predict videos xpred interactions, and environmental changesthat compromise the 3D consistency required for reliable reconstruction. Our goal is to ensure 3D consistency in generated videos with high efficiency, producing an effective tool to bridge the gap from video to 3D. In other words, we aim for outputs that adhere to 3D-consistent data distribution while excluding disruptive variations. straightforward approach to address this is fine-tuning the diffusion model on dedicated 3D dataset, forcing the model to align with 3D consistent distribution. This method has proven effective in various style transfer tasks, which similarly aim to preserve specific data distribution within larger, more diverse set. However, this method presents two main challenges. First, fine-tuning does not enhance diffusion efficiency: achieving high-quality video output still requires up to 50 denoising steps, which is computationally costly and time-intensive. Second, the generation process lacks controllability. The model, though fine-tuned, follows standard diffusion denoising procedure, starting from random distribution and gradually reaching the target data distribution via step-bystep denoising. This stochastic process makes it difficult to enforce consistent camera trajectory and 3D coherence, even when given image conditions. To address these challenges, we introduce VideoScene, novel distillation technique for one-step, 3D-consistent video generation to bridge the gap from video to 3D. Specifically, our approach incorporates 3D-aware leap flow distillation strategy (Sec. 3.3) and dynamic denoising policy network (Sec. 3.4) during both training and inference to maximize 3D priors, enhancing both efficiency and controllability of video generation. 4 3.3. 3D-Aware Leap Flow Distillation In the consistency distillation training [55], conventional noise scheduler samples an initial ground truth x0 from the data distribution and applies noise to generate xt at random timestep using forward diffusion as follows: xt = q(x0, ϵ, t) = αtx0 + σtϵ, [0, ], (4) where Gaussian noise ϵ (0, I), αt and σt define the signal-to-noise ratio (SNR) of the stochastic interpolant xt. Standard SNR schedules ensure that xT retains some lowfrequency information from x0, causing mismatch with inference, which starts from fully noisy xT . This mismatch leads to degradation during inference, especially with fewer denoising steps. While some studies [37, 44] suggest adjusting the noise schedule or applying backward distillation to maintain zero terminal SNR, we find these solutions insufficient for efficient performance. Observing that the initial denoising steps (where is near ) are particularly challenging due to limited prior information, we propose 3D-aware leap flow distillation strategy that aligns inference with training at an intermediate timestep t, [0, ], where < . Specifically, during distillation training, we first employ fast, feedforward sparse-view 3DGS [35] model, MVSplat [17], to generate coarse but 3D-consistent scene by matching and fusing view information with the cross-view transformer and cost volume. Then we render continuous frames from the coarse scene as follows: {IRender}T τ =1 = g(S(I Input, ci), o(ci)), = {0, 1} (5) where IInput represents the pairwise input image, ci is the corresponding camera pose, S(, ) is the sparse-view reconstruction model, o() is the interpolation camera trajectory, g(, ) is renderer given 3D representation and queried camera trajectory, and {IRender}T τ =1 is the rendered video. Although the video shows visual artifacts and blurred regions, it contains the scenes 3D geometric structure information as rendered from 3D representation. From it, we encode the rendered video {IRender}T τ =1 into latent space and sample xr 0, adding noise at randomly selected timestep according to Eq. 4, and training gradients are calculated as follows: (cid:16) (cid:104) tn+ fθ(xr tn (xr tn (xr LD(θ, θ; Φ) = Ex,t tn+1 , tn+1), fθ (ˆxϕ tn+1 ), tn) (6) where ˆxϕ ) is estimated by the teacher model with input xr tn+1 following Eq. 3 and fed into the EMA student model. To make it easier to understand, we refer to both the student and EMA student as one student video diffusion in Fig. 2. During inference, we also start from xr 0 and add noise at selected timestep t, and this selection follows policy network rather than random approach. This will be discussed in Sec 3.4. In summary, our 3D-aware leap flow distillation mitigates discrepancies between training and inference by avoiding reliance on ground-truth signals and bypassing inefficient early denoising steps near . This process accelerates overall denoising by effectively simulating training during inference, enabling the student model to leverage rich prior knowledge rather than starting from scratch. 3.4. Dynamic Denoising Policy Network During inference, we begin from the initial rendered video latents xr 0, and progressively add noise at chosen leap timestep t. The choice of is context-dependent: when the input video is of high quality, adding small amount of noise suffices to refine fine details while preserving overall structure. However, when input videos contain artifacts such as structural distortions, blurring, or lighting inconsistencies, larger noise addition is necessary as minimal noise step could introduce an unsuitable prior, resulting in degraded structure. Conversely, excessive noise risks overwhelming the models prior, yielding outputs close to pure noise and losing critical 3D information. Therefore, selecting an appropriate denoising timestep is essential for optimal inference performance. To better decide the noise level, we introduce policy network based on contextual bandit algorithm [10, 18, 61, 68]. This network acts as the agent and learns to dynamically select the best timestep for denoising. We model this selection as an independent decision process: given an environment state (i.e., input video latent xr 0), the agent (i.e., DDPNet with policy distribution πψ(txr 0; ψ)) decides an action (i.e., noise step [0, ]) and receive reward (i.e., loss LM SE). At each distillation training round, the (cid:17)(cid:105) decision of is made randomly and applied to an input latent xr (t) can be computed as: 0. The sample data estimate xpred 0 xpred 0 (t) = σtϵθ(xr xr αt , t) , (7) where ϵθ(, ) is the noise predictor of student diffusion model. After denoising, the predicted video output xpred is compared to the ground truth sequence x0 using MSE loss: 0 LM SE = stopgrad(xpred (t) x02 2). (8) , The grad is stopped here as the MSE loss is only used to update the policy network. We define the immediate reward r(xr 0, t) = LM SE, providing direct optimization target for the policy network. This reward signal encourages timestep selections that yield minimal reconstruction error. We achieve this through policy gradient optimization [61, 77], which adjusts ψ to increase the likelihood of timestep selections associated with higher rewards by minimizing LDDP (ψ): LDDP (ψ) = Etπψ(txr 0;ψ)[r(xr 0, t)], (9) It should be noted that during the training of the policy network, data from the distillation training is exclusively utilized by the policy network. To ensure stability in the distillation process, the policy network does not pass any gradients to the student model. 4. Experiments 4.1. Experiment Setup"
        },
        {
            "title": "To implement",
            "content": "Implementation Details. the proposed VideoScene, we choose MVSplat [17] as feed-forwad 3DGS model and pretrained CogVideoX-5B-I2V [96] (@ 720 480 resolution) as the video diffusion backbone. The pretrained CogVideoX costs more than 2 minutes with default 50 DDIM [71] steps. We first finetune the attention layers of the video model with 900 steps on the learning rate 1 104 for warm-up. Then we leverage the pre-trained 3D model with fixed parameters and conduct distillation training for 20k iterations. Our video diffusion was trained on 3D scene datasets RealEstate10K [105] by sampling 49 frames with batch size of 2. During training, only the attention layers within the transformer blocks of the video model are updated. The training is conducted on 8 NVIDIA A100 (80G) GPUs in two days, using the AdamW [52] optimizer with learning rate of 3105. The dynamic denoising policy network (DDPNet) follows CNN architecture, with 4 layers of 2D convolution along with corresponding normalization and activation layers. Since the policy network has significantly fewer parameters than the video diffusion model, it participates in full training only during the 5 Figure 3. Qualitative comparison. We can observe that baseline models suffer from issues such as blurriness, frame skipping, excessive motion, and shifts in the relative positioning of objects, while our VideoScene achieves higher output quality and improved 3D coherence. first 4,000 steps to prevent overfitting. Notice that the inference time of our VideoScene only costs within 3s (renderings from 3DGS feed-forward model [17] cost 0.5s and the distilled video generation with DDIM [71] sampler in one step costs 2.5s). Datasets. We assess VideoScene on the large-scale 3D scene dataset RealEstate10K [105]. RealEstate10K is dataset downloaded from YouTube, which is split into 67,477 training scenes and 7,289 test scenes. To better verify the effectiveness of VideoScene, we have established challenging benchmark, testing on 120 benchmark scenes with large angle variance. The dataset also provides estimated camera intrinsic and extrinsic parameters for each frame. We first fine-tune video interpolation model with the first and end frame guidance. For each scene video, we sample 49 contiguous frames with equal intervals and serve the first and last frames as the input for our video diffusion model. To further evaluate our cross-dataset generalization 6 Table 1. Quantitative Comparison. We compare 1-, 4-, and 50-step versions of models. Not only does VideoScene outperform other methods, but its 1-step results also remain closely comparable to the 50-step results, while other methods show significant decrease."
        },
        {
            "title": "Method",
            "content": "#Steps FVD Aesthetic Quality Subject Consistency Background Consistency Stable Video Diffusion [8] DynamiCrafter [94] CogVideoX-5B [96] VideoScene (Ours) 50 4 1 50 4 1 50 4 50 4 1 424.68 541.89 1220.80 458.27 512.50 846.85 521.04 662.13 753.02 98.67 175.84 103.42 0.4906 0.4040 0. 0.5336 0.4899 0.3737 0.5368 0.4486 0.3987 0.5570 0.5357 0.5416 0.9305 0.8728 0.7934 0.8898 0.8661 0.7474 0.9179 0.8489 0. 0.9320 0.9269 0.9259 0.9287 0.8952 0.8817 0.9349 0.9098 0.8627 0.9460 0.9168 0.8976 0.9407 0.9431 0.9461 ability, we also evaluate on the video dataset ACID [46], which contains natural landscape scenes with camera poses. Baselines and Metrics. As the fundamental unit of multiview geometry is two-view geometry, our goal is to use video diffusion to generate new, 3D-consistent viewpoints between the given two input views. To evaluate our approach, we compare it against several state-of-the-art opensource video frame interpolation models: Stable Video Diffusion [8], DynamiCrafter [94], and CogVideoX [96]. Stable Video Diffusion and DynamiCrafter employ UNetbased diffusion architecture, while CogVideoX utilizes diffusion transformer architecture. For quantitative evaluation, we assess the quality and temporal coherence of synthesized videos by reporting the Frechet Video Distance (FVD) [82]. Following the previous benchmark VBench [31], we also evaluate the Aesthetic Score, Subject Consistency, and Background Consistency of generated videos as our metrics. See supplementary for more details. 4.2. Comparison with Baselines We compare our VideoScene framework against three baseline models across different DDIM steps (1, 4, and 50) in Tab. 1. Visual comparisons of our one-step results versus the 50-step results of baselines are shown in Fig. 3. Our VideoScene outperforms state-of-the-art models, even with one-step denoising, achieving superior visual quality and spatial consistency with the ground truth. More comparison results are provided in the supplement material. Cross-dataset generalization. Leveraging the strong generative capabilities of the video diffusion model with 3D structure prior, VideoScene demonstrates natural advantage in generalizing to novel, out-of-distribution scenes shown in Fig. 1. To validate this generalizability, we conduct cross-dataset evaluation. For fair comparison, we train baseline models (DynamiCrafter [94] and CogVideoX [96] included) on the same 3D dataset [105] and directly test them on the ACID [46] dataset. As shown in Tab. 2 and Fig. 4, while baselines improve 3D consistency with 50-step inference after fine-tuning on 3D data, they fail to achieve clarity with one-step inference. Remarkably, our method not only achieves comparable performance to the baselines 50-step results but also surpasses them in one-step inference, highlighting its strong generalizability. Figure 4. Qualitative results in cross-dataset generalization. Models trained on the source dataset RealEstate10K are tested on ACID dataset. Fine-tuned models improve in 3D consistency but still fail with one-step. Structure matching comparison. To further assess geometric consistency, we evaluate the camera geometry alignment between frames in the generated videos following [42]. Specifically, we extract two frames at regular intervals from each generated video, creating pairs of two-view images. For each pair, we apply featurematching algorithm [60] to find corresponding points and Table 2. Quantitative results in cross-dataset generalization. Models trained on the source dataset RealEstate10K [105] are tested on unseen scenes from target datasets ACID [46], without any fine-tuning. Method 3D Training data #Steps FVD Aesthetic Quality Subject Consistency Background Consistency DynamiCrafter [94] CogVideoX-5B [96] / / DynamiCrafter [94] RealEstate10K [105] CogVideoX-5B [96] RealEstate10K [105] VideoScene (Ours) RealEstate10K [105] 50 1 50 1 50 50 1 50 1 242.76 453.24 867.31 537.48 96.10 595.87 114.04 464. 73.93 121.93 0.5191 0.4126 0.5212 0.4614 0.5096 0.4013 0.5491 0.4492 0.5602 0. 0.9464 0.8136 0.9628 0.8452 0.9524 0.8202 0.9637 0.8406 0.9598 0.9395 0.9527 0. 0.9720 0.9349 0.9545 0.8917 0.9593 0.9214 0.9573 0.9494 Figure 5. Matching results comparison. Green represents highquality matching results, while red represents discarded matching results. More green high-quality matches indicate higher level of geometric consistency between the two views. Figure 6. Visual results of ablation study. We ablate the design choices of 3D-aware leap flow distillation and dynamic denoising policy network (DDPNet). use RANSAC [21] with the fundamental matrix (epipolar constraint) to filter out incorrect matches. Fig. 5 shows that VideoScene achieves the highest number of correctly matched points, confirming superior geometry consistency. Table 3. Quantitative results of ablation study. We report the quantitative metrics ablations in RealEstate10K. Setup FVD Aesthetic Quality Subject Consistency Background Consistency Base rendered video w/o 3D-aware leap flow w/o DDPNet VideoScene (full model) 171.38 543.53 106.28 97.53 0.4769 0.4092 0.4897 0.5306 0.8794 0.7842 0.8850 0.9139 0.9240 0.9160 0.9205 0.9440 4.3. Ablation Study and Analysis We perform ablation studies to analyze the design choices within the VideoScene framework (see Tab. 3 and Fig. 6). The base rendered video represents the video generated directly from the 3D model. naive combination of 3Dfine-tuned video diffusion model with standard distillation acceleration methods [87] is referred to as w/o 3D-aware leap flow. Additionally, we perform ablation on DDPNet. Results indicate that while the rendered video from the 3D model has suboptimal quality, it provides coarse consistent information. Without 3D-aware leap flow distillation, generated frames suffer from inconsistencies, leading to blur and artifacts. The inclusion of DDPNet further enhances fine-grained details and corrects spatial distortions, demonstrating its effectiveness in optimal denoising decisions. 5. Conclusion In this paper, we introduce VideoScene, novel fast video generation framework that distills the video diffusion model to generate 3D scenes in one step. Specifically, we constrain the optimization with 3D prior and propose 3D-aware leap flow distillation strategy to leap over time-consuming redundant information. Moreover, we design dynamic denoising policy network to adaptively determine the optimal leap timestep during inference. Extensive experiments demonstrate the superiority of our proposed VideoScene in terms of efficiency and consistency in 3D structure, highlighting its potential as an efficient and effective tool to bridge the gap from video to 3D."
        },
        {
            "title": "References",
            "content": "[1] Michal Adamkiewicz, Timothy Chen, Adam Caccavale, Rachel Gardner, Preston Culbertson, Jeannette Bohg, and Mac Schwager. Vision-only robot navigation in neural radiance world. IEEE Robotics and Automation Letters, 7(2): 46064613, 2022. 1 [2] Rajeev Agrawal. Sample mean based index policies by o(log n) regret for the multi-armed bandit problem. Advances in applied probability, 27(4):10541078, 1995. 2 [3] Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In ICML, pages 127135. PMLR, 2013. 2 [4] Yogesh Balaji, Martin Renqiang Min, Bing Bai, Rama Chellappa, and Hans Peter Graf. Conditional gan with discriminative filter generation for text-to-video synthesis. In IJCAI, page 2, 2019. 2 [5] Yikun Ban, Jingrui He, and Curtiss Cook. Multi-facet contextual bandits: neural network perspective. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pages 3545, 2021. 2 [6] Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded In Proceedings of anti-aliased neural radiance fields. the IEEE/CVF conference on computer vision and pattern recognition, pages 54705479, 2022. 2, 3, 4, 7 [7] Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Zip-nerf: Anti-aliased In Proceedings of the grid-based neural radiance fields. IEEE/CVF International Conference on Computer Vision, pages 1969719705, 2023. 2 [8] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2, 7, 4, [9] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In CVPR, pages 22563 22575, 2023. 2 [10] Djallel Bouneffouf, Irina Rish, and Charu Aggarwal. Survey on applications of multi-armed and contextual bandits. In 2020 IEEE Congress on Evolutionary Computation (CEC), pages 18. IEEE, 2020. 5 [11] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators, 2024. 2, 4, 7 [12] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 96509660, 2021. 3 [13] David Charatan, Sizhe Lester Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian splats from im9 age pairs for scalable generalizable 3d reconstruction. CVPR, pages 1945719467, 2024. 1, 2, In [14] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1412414133, 2021. 5 [15] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In CVPR, pages 73107320, 2024. 2 [16] Yuedong Chen, Chuanxia Zheng, Haofei Xu, Bohan Zhuang, Andrea Vedaldi, Tat-Jen Cham, and Jianfei Cai. Mvsplat360: Feed-forward 360 scene synthesis from sparse views. In NeurIPS (NeurIPS), 2024. 2, 3 [17] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images. In ECCV, pages 370386. Springer, 2025. 2, 3, 4, 5, 6 [18] Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. In ProContextual bandits with linear payoff functions. ceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pages 208214. JMLR Workshop and Conference Proceedings, 2011. [19] Zhiwen Fan, Wenyan Cong, Kairun Wen, Kevin Wang, Jian Zhang, Xinghao Ding, Danfei Xu, Boris Ivanovic, Marco Pavone, Georgios Pavlakos, et al. Instantsplat: Unbounded sparse-view pose-free gaussian splatting in 40 seconds. arXiv preprint arXiv:2403.20309, 2, 2024. 3, 5 [20] Yuming Fang, Hanwei Zhu, Yan Zeng, Kede Ma, and Zhou Wang. Perceptual quality assessment of smartphone phoIn IEEE Conference on Computer Vision and tography. Pattern Recognition, pages 36773686, 2020. 3 [21] Martin Fischler and Robert Bolles. Random sample consensus: paradigm for model fitting with applications to image analysis and automated cartography. Communications of the ACM, 24(6):381395, 1981. 8 [22] Ruiqi Gao, Aleksander Holynski, Philipp Henzler, Arthur Brussee, Ricardo Martin-Brualla, Pratul Srinivasan, Jonathan Barron, and Ben Poole. Cat3d: Create anything in 3d with multi-view diffusion models. arXiv preprint arXiv:2405.10314, 2024. 2 [23] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and Devi Parikh. Long video generation with time-agnostic vqgan and timesensitive transformer. In ECCV, pages 102118. Springer, 2022. 2 [24] Zhengyang Geng, Ashwini Pokle, William Luo, Justin Lin, and Zico Kolter. Consistency models made easy. arXiv preprint arXiv:2406.14548, 2024. 3 [25] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. [26] Yuwei Guo, Ceyuan Yang, Anyi Rao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Sparsectrl: Adding sparse controls to text-to-video diffusion models. 348. Springer, 2025. 2 In ECCV, pages 330 puter Vision and Pattern Recognition, pages 2077520785, 2024. 3, 5 [27] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 3 [28] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 33:68406851, 2020. 3, 1 [29] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben ImaPoole, Mohammad Norouzi, David Fleet, et al. gen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. [30] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 2 [31] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In CVPR, 2024. 7, 2, 4 [32] Alexia Jolicoeur-Martineau, Ke Li, Remi Piche-Taillefer, Tal Kachman, and Ioannis Mitliagkas. Gotta go fast when generating data with score-based models. arXiv preprint arXiv:2105.14080, 2021. 3 [33] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, pages 2656526577, 2022. 1 [34] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 51485157, 2021. 3 [35] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 1, 3, 4, [36] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale ACM Transactions on Graphics scene reconstruction. (ToG), 36(4):113, 2017. 2, 3, 4, 7 [37] Jonas Kohler, Albert Pumarola, Edgar Schonfeld, Artsiom Sanakoyeu, Roshan Sumbaly, Peter Vajda, and Ali Thabet. Imagine flash: Accelerating emu diffusion models with backward distillation. arXiv preprint arXiv:2405.05224, 2024. 3, 4 [38] LAION-AI. Aesthetic predictor, 2024. Accessed: 2024-1120. 3 [39] John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side information. Advances in neural information processing systems, 20, 2007. 2 [40] Jiahe Li, Jiawei Zhang, Xiao Bai, Jin Zheng, Xin Ning, Jun Zhou, and Lin Gu. Dngaussian: Optimizing sparse-view 3d gaussian radiance fields with global-local depth normalization. In Proceedings of the IEEE/CVF Conference on Com- [41] Lihong Li, Wei Chu, John Langford, and Robert Schapire. contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th international conference on World wide web, pages 661 670, 2010. 2 [42] Xuanyi Li, Daquan Zhou, Chenxu Zhang, Shaodong Wei, Qibin Hou, and Ming-Ming Cheng. Sora generates videos arXiv preprint with stunning geometrical consistency. arXiv:2402.17403, 2024. 7 [43] Zhen Li, Zuo-Liang Zhu, Ling-Hao Han, Qibin Hou, ChunLe Guo, and Ming-Ming Cheng. Amt: All-pairs multifield transforms for efficient frame interpolation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 3 [44] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and sample steps are flawed. In WACV, pages 54045411, 2024. 4 [45] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. Dl3dv-10k: large-scale scene dataset for deep learning-based 3d vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2216022169, 2024. 2 [46] Andrew Liu, Richard Tucker, Varun Jampani, Ameesh Makadia, Noah Snavely, and Angjoo Kanazawa. Infinite nature: Perpetual view generation of natural scenes from single image. In ICCV, 2021. 7, [47] Fangfu Liu, Wenqiang Sun, Hanyang Wang, Yikai Wang, Haowen Sun, Junliang Ye, Jun Zhang, and Yueqi Duan. Reconx: Reconstruct any scene from sparse views with video diffusion model. arXiv preprint arXiv:2408.16767, 2024. 2, 3 [48] Fangfu Liu, Hanyang Wang, Weiliang Chen, Haowen Sun, and Yueqi Duan. Make-your-3d: Fast and consistent subject-driven 3d content generation. arXiv preprint arXiv:2403.09625, 2024. 3 [49] Fangfu Liu, Hanyang Wang, Shunyu Yao, Shengjun Zhang, Jie Zhou, and Yueqi Duan. Physics3d: Learning physical properties of 3d gaussians via video diffusion. arXiv preprint arXiv:2406.04338, 2024. 2 [50] Fangfu Liu, Diankun Wu, Yi Wei, Yongming Rao, and Yueqi Duan. Sherpa3d: Boosting high-fidelity text-to-3d In CVPR, pages 20763 generation via coarse 3d prior. 20774, 2024. 3 [51] Xi Liu, Chaoyi Zhou, 3dgsenhancer: Enhancing unbounded 3d gaussian splatting with view-consistent 2d diffusion priors. arXiv preprint arXiv:2410.16266, 2024. 2, and Siyu Huang. [52] Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 5 [53] Cheng Lu and Yang Song. Simplifying, stabilizing and scaling continuous-time consistency models. arXiv preprint arXiv:2410.11081, 2024. 3 [54] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. NeurIPS, 35:57755787, 2022. 3 Companion Proceedings of the ACM Web Conference 2023, pages 778782, 2023. 5 [55] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing highresolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. 3, 4 [56] Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick von Platen, Apolinario Passos, Longbo Huang, Jian Li, and Hang Zhao. Lcm-lora: universal stable-diffusion acceleration module. arXiv preprint arXiv:2311.05556, 2023. 3 [57] Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi, Jonathan Barron, Alexey Dosovitskiy, and Daniel Duckworth. Nerf in the wild: Neural radiance fields for unconstrained photo collections. In CVPR, pages 72107219, 2021. 1 [58] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In CVPR, pages 1429714306, 2023. 3 [59] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 1, 3, 2, 4, [60] Pauline Ng and Steven Henikoff. Sift: Predicting amino acid changes that affect protein function. Nucleic acids research, 31(13):38123814, 2003. 7 [61] Feiyang Pan, Qingpeng Cai, Pingzhong Tang, Fuzhen Zhuang, and Qing He. Policy gradients for contextual recommendations. In The World Wide Web Conference, pages 14211431, 2019. 5 [62] William Peebles and Saining Xie. Scalable diffusion modIn ICCV, pages 41954205, 2023. els with transformers. 2 [63] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning, pages 87488763. PMLR, 2021. 3 [64] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In CVPR, pages synthesis with latent diffusion models. 1068410695, 2022. 3, [65] Tim Salimans and Jonathan Ho. Progressive distillation arXiv preprint for fast sampling of diffusion models. arXiv:2202.00512, 2022. 3 [66] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and In Robin Rombach. Adversarial diffusion distillation. ECCV, pages 87103. Springer, 2025. 3 [67] Johannes Schonberger and Structure-from-motion revisited. 41044113, 2016. 2 Jan-Michael Frahm. In CVPR, pages [69] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. Stylegan-v: continuous video generator with the price, image quality and perks of stylegan2. In CVPR, pages 36263636, 2022. 2 [70] Nagabhushan Somraj, Adithyan Karanayil, and Rajiv Soundararajan. Simplenerf: Regularizing sparse input neural radiance fields with simpler solutions. In SIGGRAPH Asia 2023 Conference Papers, pages 111, 2023. 1 [71] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 3, 5, 6, 1 [72] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 1 and Stefano Ermon. arXiv preprint and Stefano Ermon. arXiv preprint [73] Yang Song and Prafulla Dhariwal. niques for training consistency models. arXiv:2310.14189, 2023. 3 Improved techarXiv preprint [74] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Scorebased generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 3, 1 [75] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya arXiv preprint Consistency models. Sutskever. arXiv:2303.01469, 2023. 2, 3, [76] Wenqiang Sun, Shuo Chen, Fangfu Liu, Zilong Chen, Yueqi Duan, Jun Zhang, and Yikai Wang. Dimensionx: Create any 3d and 4d scenes from single image with controllable video diffusion, 2024. 2, 3 [77] Richard Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. NeurIPS, 12, 1999. 5 [78] Stanislaw Szymanowicz, Chrisitian Rupprecht, and Andrea Vedaldi. Splatter image: Ultra-fast single-view 3d reconstruction. In CVPR, pages 1020810217, 2024. 3 [79] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs In Computer Vision field transforms for optical flow. ECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part II 16, pages 402419. Springer, 2020. 3 [80] William Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3-4):285294, 1933. 1 [81] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content for video generation. In CVPR, pages 15261535, 2018. 2 [82] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. [68] Qicai Shi, Feng Xiao, Douglas Pickard, Inga Chen, and Liang Chen. Deep neural network with linucb: contextual bandit approach for personalized recommendation. In [83] Dani Valevski, Yaniv Leviathan, Moab Arar, and Shlomi Fruchter. Diffusion models are real-time game engines. arXiv preprint arXiv:2408.14837, 2024. 1 11 ward: Text-to-3d generation with human preference. arXiv preprint arXiv:2403.14613, 2024. 1 [98] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4578 4587, 2021. 5 [99] Hanyang Yu, Xiaoxiao Long, and Ping Tan. Lm-gaussian: Boost sparse-view 3d gaussian splatting with large model priors. arXiv preprint arXiv:2409.03456, 2024. [100] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024. 2, 3, 5 [101] Ailing Zeng, Yuhang Yang, Weidong Chen, and Wei Liu. The dawn of video generation: Preliminary explorations with sora-like models. arXiv preprint arXiv:2410.05227, 2024. 3 [102] Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang Wei, Yuchen Zhang, and Hang Li. Make pixels dance: In CVPR, pages 8850 High-dynamic video generation. 8860, 2024. 2 [103] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 3 [104] Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, and Anima Anandkumar. Fast sampling of diffusion models via operator learning. In International conference on machine learning, pages 4239042402. PMLR, 2023. 3 [105] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. arXiv preprint arXiv:1805.09817, 2018. 5, 6, 7, [84] Guangcong Wang, Zhaoxi Chen, Chen Change Loy, and Ziwei Liu. Sparsenerf: Distilling depth ranking for few-shot novel view synthesis. In ICCV, pages 90659076, 2023. 1, 3, 5 [85] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. 2 [86] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In CVPR, pages 2069720709, 2024. 2, 3, 5 [87] Xiang Wang, Shiwei Zhang, Han Zhang, Yu Liu, Yingya Zhang, Changxin Gao, and Nong Sang. VidearXiv preprint olcm: Video latent consistency model. arXiv:2312.09109, 2023. 8 [88] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. NeurIPS, 36, 2024. 2 [89] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility IEEE transactions on image proto structural similarity. cessing, 13(4):600612, 2004. 3 [90] Yujie Wei, Shiwei Zhang, Zhiwu Qing, Hangjie Yuan, Zhiheng Liu, Yu Liu, Yingya Zhang, Jingren Zhou, and Hongming Shan. Dreamvideo: Composing your dream videos with customized subject and motion. In CVPR, pages 6537 6549, 2024. 2 [91] Christopher Wewer, Kevin Raj, Eddy Ilg, Bernt Schiele, and Jan Eric Lenssen. latentsplat: Autoencoding variational gaussians for fast generalizable 3d reconstruction. arXiv preprint arXiv:2403.16292, 2024. [92] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In ICCV, pages 76237633, 2023. 2 [93] Kailu Wu, Fangfu Liu, Zhihan Cai, Runjie Yan, Hanyang Wang, Yating Hu, Yueqi Duan, and Kaisheng Ma. Unique3d: High-quality and efficient 3d mesh generation from single image. arXiv preprint arXiv:2405.20343, 2024. 3 [94] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating open-domain images with video diffusion priors. In ECCV, pages 399417. Springer, 2025. 2, 7, 8, 4, 5 [95] Jiawei Yang, Marco Pavone, and Yue Wang. Freenerf: Improving few-shot neural rendering with free frequency regularization. In CVPR, pages 82548263, 2023. 1 [96] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-tovideo diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2, 5, 7, 8, 4 [97] Junliang Ye, Fangfu Liu, Qixiu Li, Zhengyi Wang, Yikai Wang, Xinzhou Wang, Yueqi Duan, and Jun Zhu. Dreamre12 VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in One Step"
        },
        {
            "title": "Supplementary Material",
            "content": "6. More Discussion of Preliminaries In this section, we provide more preliminaries about the diffusion model, consistency model [75], and contextual bandit [80]. 6.1. Diffusion Model Diffusion models [28, 64, 71, 74] generate data by progressively introducing Gaussian noise to the original data and subsequently sampling from the noised data through several denoising steps. Let pdata(x) denote the data distribution, The forward process is described by stochastic differential equation (SDE) [74] given by dxt = µ(xt, t)dt + σ(t)dw (10) where [0, ], > 0 denotes fixed time horizon, µ(, ) and σ() represent the drift and diffusion coefficients,respectively, and {wt}t[0,T ] is the standard Brownian motion. An important property of this SDE is the existence of an associated ordinary differential equation (ODE), known as the Probability Flow (PF) ODE [74], which deterministically describes the distributions evolution (cid:20) µ(xt, t) dxt = 1 2 (cid:21) σ2(t)xt log pt(xt) dt (11) where xt log pt(xt) is the score function of the intermediate distribution pt(xt). For practical purposes [33], simplified setting is often adopted, where µ(xt, t) = 0 and 2t. This yields the intermediate distributions σ(t) = pt(x) = pdata(x) (0, t2I), where convolution operation. Let π(x) = (0, 2I) and after sufficient noise adding process, the final distribution pT (x) will be closed to π(x). Sampling involves solving the empirical PF ODE: dxt dt = txt log pt(xt) (12) starting from sample xT (0, 2I) and running the ODE backward procedure with Numerical ODE solver like Euler [72] and Heun [33] solver, we can obtain solution trajectory {ˆxt}t[0,T ] and thus get approximate sample ˆx0 from the data distribution pdata(x). The backward process is typically stopped at = ϵ to avoid numerical instability, where ϵ is small positive number, and ˆxϵ is treated as the final approximate result. 6.2. Consistency Model Consistency model [75] is novel class of models that supports both one-step and iterative generation, providing trade-off between sample quality and computational efficiency. The consistency model can be trained either by distilling knowledge from pre-trained diffusion model or independently, without relying on pre-trained models. Formally, given solution trajectory {ˆxt}t[0,T ] sampled from Eq. 11, we define the consistency function as : (xt, t) (cid:55) xϵ. consistency function exhibits self-consistency property, meaning that its outputs remain consistent for any pair of (xt, t) points that lie along the same PF ODE trajectory. The goal of consistency model is to approximate this consistency function with fθ. Given any consistency function (, ), it must satisfy (xϵ, ϵ) = xϵ, implying that (, ϵ) acts as the identity function. This requirement is referred to as the boundary condition. It is imperative for all consistency models to adhere to this condition, as it is pivotal to the proper training of such models. There are several simple way to implement the boundary condition, for example we can parameterize fθ as fθ(x, t) = cskip(t)x + cout(t)Fθ(x, t) (13) where cskip(t) and cout(t) are differentiable functions such that cskip(ϵ) = 1 and cout(ϵ) = 0. This parameterization ensures that the consistency model is differentiable at = ϵ, provided that Fθ(x, t), cskip(t), and cout(t) are all differentiable, which is crucial for training continuous-time consistency models. Once consistency model fθ(, ) is welltrained, samples can be generated by first sampling from the initial distribution ˆxT (0, 2I), and then evaluating the consistency model for ˆxϵ = fθ(ˆxT , ). This generates sample in single forward pass through the consistency model. Additionally, the consistency model can be evaluated multiple times by alternating between denoising and noise injection steps to improve sample quality, thus offering flexible trade-off between computational cost and sample quality. This multi-step procedure also holds significant potential for zero-shot data editing applications. 6.3. Contextual Bandit Algorithm The Multi-Armed Bandit (MAB) problem, originally introduced by [80], is fundamental model in sequential decision-making under uncertainty. It is named after the analogy of gambler trying to maximize rewards from multiple slot machines (or arms), each with an unknown probability distribution of payouts. At each step, the agent must decide which arm to pull, aiming to maximize the cumulative reward over time. The core difficulty lies in addressing the exploration-exploitation dilemma: the agent needs to explore different arms to learn their reward dis1 Algorithm 1 3D-Aware Leap Flow Distillation 1: Input: 3D dataset D, initial model parameter θ, learning rate η, one-step ODE solver Φ(), distance metric d(, ), EMA rate µ, noise schedule αt, σt, timestep interval k, diffusion optimization timesteps , and encoder E() 2: Repeat 3: 4: 5: 6: 7: 8: 9: 10: τ =1 = g(S(I Input, ci) D, Sample ϵ (0, I) and tn+1 [0, ] Sample (I = {0, 1} tn tn+1 Render images {IRender}T xr 0 E({IRender}τ =1) αtn+1 xr xr tn+1 ˆxϕ tn xr LD(θ, θ; Φ) d(fθ(xr tn+1 θ θ η θLD(θ, θ; Φ) θ stopgrad(µθ + (1 µ)θ) 0 + σtn+1ϵ + (tn tn+1)Φ(xr tn+1 11: 12: 13: Until convergence , tn+1; ϕ) tn+1 , tn+1), fθ (ˆxϕ tn, tn)) Input, ci), o(ci)), = {0, 1} tributions while simultaneously exploiting the best-known arm to achieve immediate gains. Balancing exploration and exploitation is crucial because exploration uncovers potentially better options, while exploitation ensures short-term performance. Overemphasizing exploration can waste resources on suboptimal choices, whereas overly exploiting known options risks missing higher rewards. This trade-off is central to the design of MAB algorithms. MAB problems can be broadly categorized into two types: context-free bandits and contextual bandits. Contextfree bandits have been extensively studied, with popular algorithms such as the ϵ-greedy strategy [39] and the Upper Confidence Bound (UCB) algorithm [2]. These approaches assume that the rewards are solely determined by the arm In selection, without considering additional information. contrast, contextual bandits extend this framework by incorporating side information, or context, to model the expected reward for each arm. Contextual bandits leverage the context as input features, enabling more nuanced understanding of the reward function. For instance, algorithms like LinUCB [41] and Thompson Sampling for linear models [3] assume that the expected reward is linear function of the context. However, in practice, this linearity assumption often fails for complex, non-linear environments. To overcome this limitation, many works [5] have integrated deep neural networks (DNNs) with contextual bandit frameworks, significantly enhancing their representation power. In our approach, we adopt convolutional neural network (CNN) for contextual bandit algorithm to dynamically determine the optimal denoising timestep in video inference. Specifically, we model this problem as follows: State Representation: The environment state is defined by 0, capturing structural and percepthe input video latent xr tual details of the video. Agent and Policy: The agent is modeled using CNN policy network, which employs probabilistic policy πψ(txr 0; ψ) to select the timestep t. Action Selection: The action corresponds to the choice of timestep [0, ], representing the level of noise to add during the denoising process. Reward Signal: Feedback is provided in the form of reward signal, defined as the negative mean squared error loss (LM SE) between the denoised output and the ground truth. This reward quantifies the quality of the denoising process for the chosen t. Policy Update: The policy network updates its parameters ψ using the observed rewards, gradually learning to select the optimal for different contexts. By framing timestep selection as contextual bandit problem, our method adaptively balances structural preservation and artifact correction during video inference, achieving robust and high-quality results across diverse scenarios. 7. Additional Implementation Details We present the pseudo-code for 3D-aware distillation in Algorithm 1. We also provide details for additional experiments. Datesets. To further validate our strong generalizability, we test our method on the NeRF-LLFF [59], Sora [11], and more challenging outdoor datasets Mip-NeRF 360 [6] and Tank-and-Temples dataset [36]. For video-to-3D application, we also evaluate our method on the Mip-NeRF 360 [6] and Tank-and-Temples dataset [36]. Video Metrics. We utilize VBench [31] to evaluate the performance of our model by comparing it against several state-of-the-art, open-source video frame interpolation models. VBench provides comprehensive analysis of video generation quality by decomposing it into 16 distinct evaluation metrics, enabling detailed and multi-faceted asFigure 7. Visual results of the generative ability. We highlight the generated regions in the red boxes in the novel generated views. sessment of model performance. For our evaluation, we focus on key metrics such as Aesthetic Quality, Subject Consistency, and Background Consistency, which offer critical insights into the visual appeal and temporal coherence of the generated frames. Aesthetic Quality measures the visual appeal of the generated video. Utilizing the LAION aesthetic predictor [38], it gauges the artistic and aesthetic value perceived by humans for each video frame. This score reflects various aesthetic dimensions, such as the layout, the richness and harmony of colors, photorealism, naturalness, and the artistic quality of the video frames. Subject Consistency assesses whether the appearance of subject remains visually stable and coherent across all frames of video. This metric is computed by evaluating the similarity of DINO [12] features extracted from consecutive frames. Background Consistency evaluates the temporal consistency of the background scenes by calculating CLIP [63] feature similarity across frames. To comprehensively evaluate the quality of the generated videos, we included additional metrics from VBench in the supplementary material, including Motion Smoothness, Dynamic Degree, and Imaging Quality. Motion Smoothness measures the fluidity of motion within the generated video, evaluating how well the movement follows realistic, natural trajectories. This metric assesses whether the video adheres to the physical laws governing motion in the real world. By utilizing motion priors in the video frame interpolation model [43], it quantifies the temporal smoothness of the generated motions. Dynamic Degree is estimated by RAFT [79] to indicate the temporal quality of generated videos. In our setting of still-scene video generation, an excessively high Dynamic Degree indicates unnecessary motion of objects within the scene, while an overly low Dynamic Degree suggests prolonged static periods interrupted by abrupt changes in certain frames. Both scenarios are undesirable outcomes for our task. Imaging Quality refers to the level of distortion present in the generated frames and is assessed using the MUSIQ [34] image quality predictor, which has been trained on the SPAQ [20] dataset. Implementation Details for Video-to-3D Application. For the video-to-3D application, we evaluate the 3D reconstruction performance of our method on the Mip-NeRF 360 [6] and Tanks-and-Temples datasets [36]. Starting with two input images and corresponding camera poses estimated from DUSt3R [86], we first generate continuous video sequence interpolating between the two frames. From this sequence, we extract intermediate frames by sampling every seventh frame, resulting in seven new views from novel perspectives. These sampled frames are then processed using InstantSplat [19] for Gaussian optimizationbased 3D reconstruction from the generated novel views. To assess the quality of our approach, we compare it against SparseNeRF [84], the original 3DGS [35], and DNGaussian [40], with per-scene optimization serving as the benchmark. For quantitative evaluation, we report standard novel view synthesis (NVS) metrics, including PSNR, SSIM [89], and LPIPS [103]. 8. Additional Experiments and Analysis 8.1. More Visual Results We present additional visual results of our VideoScene framework in Fig. 15, showcasing its performance across 3 Figure 8. Quantitative comparison across steps. We evaluate the results of CogVideo, DynamiCrafter, Stable Video Diffusion (SVD), and VideoScene across 1, 10, 20, 30, 40, and 50 steps. VideoScene not only outperforms the other methods but also demonstrates remarkable consistency, with its 1-step results closely approximating its 50-step results, whereas other methods exhibit significant decline in performance over fewer steps. Figure 9. Comparisons with base renderings with severe artifacts. diverse datasets, including NeRF-LLFF [59], Sora [11], Mip-NeRF 360 [6], and Tanks-and-Temples [36]. These examples highlight the strong generalization capability of our method, effectively adapting to novel and out-ofdistribution scenarios, whether indoor or outdoor. We also provide additional visual results in Fig. 7 to further illustrate the generative capability of our model. When the input consists of two images with significantly different viewpoints, the intermediate regions often lack direct coverage by either input image. In such cases, model must rely on its generative ability to synthesize these unseen areas. As highlighted by the red boxes in Fig. 7, VideoScene successfully generates novel content for these unseen regions. This demonstrates not only the strong generative capacity of our model but also its ability to generalize effectively while maintaining high fidelity in reconstructing previously unobserved areas. 8.2. More Quantitative Comparison Results We provide comprehensive quantitative comparisons with baseline methods in Fig. 8, 12. In Fig. 8, we evaluate the performance of CogVideo [96], DynamiCrafter [94], Stable Video Diffusion (SVD) [8], and our VideoScene across different inference steps. The results demonstrate that VideoScene not only surpasses other methods in generation quality but also achieves results comparable to their 50-step outputs in just one step. In contrast, the one-step outputs of other methods fall significantly behind their 50step counterparts, highlighting the efficiency and effectiveness of our approach. Table 4. Quantitative comparison on Mip-Nerf 360 and Tank-andTemples datasets. We report the quantitative metrics with two input views for each scene."
        },
        {
            "title": "Method",
            "content": "PSNR SSIM LPIPS Mip-NeRF 360 3DGS SparseNeRF DNGaussian InstantSplat Ours Tank and Temples 3DGS SparseNeRF DNGaussian InstantSplat Ours 10.36 11.47 10.81 11.77 13.37 9.57 9.23 10.23 10.98 14. 0.108 0.190 0.133 0.171 0.283 0.108 0.191 0.156 0.381 0.394 0.776 0.716 0.727 0.715 0.550 0.779 0.632 0.643 0.619 0.564 In Fig. 12, we further evaluate our method across multiple dimensions using metrics from VBench [31], providing more systematic and holistic validation of our generative quality. Notably, the Dynamic Degree metric assesses both the dynamic motion of individual objects in the scene and overall camera motion. Our method carefully balances these aspects, preserving consistent camera motion while minimizing unstable object movements, resulting in wellFigure 10. Comparisons with 3D-aware diffusion model ViewCrafter. Figure 11. Comparisons with NeRF-based methods. Table 5. User study on the layout stability, smoothness, visual realism, and overall quality score in user study, rated on range of 1-10, with higher scores indicating better performance. Method Layout Stability Smoothness Visual Realism Overall Quality Stable Video Diffusion [8] DynamiCrafter [94] CogVideoX [96] Ours 6.48 7.02 7.83 8.39 7.29 7.01 7.53 8. 6.75 6.02 7.33 9.52 7.13 6.68 7.50 8.82 Figure 12. Quantitative comparison across additional dimensions. We further evaluate the 1-step and 50-step results by incorporating additional dimensions from the VBench metrics. balanced intermediate Dynamic Degree value. In comparison, DynamiCrafter exhibits higher values due to its inability to maintain relative object stability, leading to excessive motion. Conversely, CogVideo shows lower values, as it often produces videos with prolonged static periods interrupted by abrupt transitions, particularly between the first and second halves. These observations underscore the robustness and balanced performance of our approach. from sparse, unposed images. Using this approach, we use VideoScene to generate video frames from given two input views and optimize the generated frames for 3D Gaussian representations. We also compare our method against existing per-scene optimization techniques, including Instantsplat [19], DNGaussian [40], 3DGS [35], and SparseNeRF [84]. The results, presented in Tab. 4 and Fig. 13, demonstrate that our approach effectively preserves the geometric structure of the scene, avoiding issues such as the multi-face problem. Furthermore, our method exhibits strong generative capabilities, reconstructing regions beyond the coverage of input views. 8.3. More Qualitative Comparison Results 8.5. User Study In Fig. 9, we compare our VideoScene with MVSplat base renderings to show its effectiveness. In Fig. 10, we compare with another 3D-aware diffusion model [100], and in Fig. 11, we show more visual comparison with NeRF-based methods [14, 84, 98]. 8.4. Video-to-3D Applications We evaluate the geometric consistency of our generated frames to assess their suitability for downstream tasks such as 3D reconstruction. For this purpose, we utilize InstantSplat [19], 3D Gaussian Splatting (3DGS) method built on DUSt3R [86], which generates Gaussian splats For the user study, we show each volunteer five samples of generated video using random method. They can rate in four aspects: (1) layout stability. Users assess whether the scene layout in the video is spatially coherent and consistent. (2) smoothness. Users observe whether the frame rate is stable, whether actions are smooth, and whether there are any stuttering or frame-skipping issues. (3) visual realism. Users rate the similarity between the generated video and real video. (4) overall quality. All aspects are on scale of 1-10, with higher scores indicating better performance. We collect results from 30 volunteers shown in Table 5. We find users significantly prefer our method over these aspects. 5 Figure 13. Qualitative comparison on Mip-Nerf 360 and Tank-and-Temples. With two sparse views as input, our method achieves much better reconstruction quality compared with baselines. Figure 14. Fail case of passing directly through the closed door. 8.6. Failure Case Table 6. Empirical runtime comparisons. Significant semantic disparities between input views lead to failure cases (see Fig. 14). The generated video passes directly through the closed door rather than navigating around it to enter the room. 9. More Discussions 9.1. Discussion of Empirical Runtime We provide the runtime comparison in Tab 6. DynamiCrafter is efficient due to smaller model size and lower frame rates and resolutions, but ours is still much faster. Method SVD DynamiCrafter CogVideoX-5B ViewCrafter VideoScene (Ours) Runtime (s) Frames 933.89 25 21.14 16 179.45 49 206.13 2.98 49 is the video diffusion model itself, which is inherently unavoidable. Leap flow distillation, as strategy for video training, incurs computational cost comparable to that of video diffusion training, without introducing significant additional overhead. Table 7. Comparison on memory costs. 9.2. Discussion of limited computational resources Description Video Backbone (CogVideoX) Leap Flow Distillation DDPNet Total Training Cost 66 GB 10 GB 0.02 GB 76 GB Tab. 7 presents the comparison of memory costs on single A100. The primary consumer of computational resources 6 Figure 15. Visual results of VideoScene. We show visual results on NeRF-LLFF [59], Sora [11], Mip-NeRF 360 [6], and Tank-andTemples dataset [36] datasets. The first and last columns represent the input views, while the intermediate columns depict the generated views."
        }
    ],
    "affiliations": [
        "Tsinghua University"
    ]
}