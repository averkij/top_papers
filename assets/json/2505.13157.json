{
    "paper_title": "Role-Playing Evaluation for Large Language Models",
    "authors": [
        "Yassine El Boudouri",
        "Walter Nuninger",
        "Julian Alvarez",
        "Yvan Peter"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) demonstrate a notable capacity for adopting personas and engaging in role-playing. However, evaluating this ability presents significant challenges, as human assessments are resource-intensive and automated evaluations can be biased. To address this, we introduce Role-Playing Eval (RPEval), a novel benchmark designed to assess LLM role-playing capabilities across four key dimensions: emotional understanding, decision-making, moral alignment, and in-character consistency. This article details the construction of RPEval and presents baseline evaluations. Our code and dataset are available at https://github.com/yelboudouri/RPEval"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 7 5 1 3 1 . 5 0 5 2 : r Role-Playing Evaluation for Large Language Models Yassine El Boudouri[000900014293961X], Walter Nuninger[0000000226391359], Julian Alvarez[0000000298629485], and Yvan Peter[000000021145357X] Univ. Lille, CNRS, Centrale Lille, UMR 9189 CRIStAL, F-59000 Lille, France {yassine.el-boudouri,walter.nuninger,julian.alvarez,yvan.peter}@univ-lille.fr Abstract. Large Language Models (LLMs) demonstrate notable capacity for adopting personas and engaging in role-playing. However, evaluating this ability presents significant challenges, as human assessments are resource-intensive and automated evaluations can be biased. To address this, we introduce Role-Playing Eval (RPEval), novel benchmark designed to assess LLM role-playing capabilities across four key dimensions: emotional understanding, decision-making, moral alignment, and in-character consistency. This article details the construction of RPEval and presents baseline evaluations. Our code and dataset are available at https://github.com/yelboudouri/RPEval Keywords: Role-playing Large language models Benchmark Evaluation."
        },
        {
            "title": "Introduction",
            "content": "This study is part of the RVRC4.0 project, which develops digital educational resources for teaching soft skills essential to customer relationship management in industries such as retail, travel, and banking. The project focuses on core interpersonal competenciesincluding communication, decision-making, initiative, negotiation, and service orientationthat are vital in client-facing roles but often neglected in traditional training environments. Within the project, role-playing is adopted as key pedagogical approach to support the development of interpersonal competencies. Learners participate in structured simulations of customer interactionssuch as processing product returns, addressing complaints, or providing guidanceeach designed to correspond with specific learning objectives. These scenarios aim to approximate real-world situations encountered in service-oriented professions, offering context in which learners can apply and reflect on their soft skills. While there is no universally accepted definition of role-playing, its interpretation often varies depending on the domain in which it is applied. In an academic context, role-playing is often described in ways that highlight its experiential aspects. For instance, Sellers (2002) defines it as spontaneous, dramatic, creative strategy in which individuals overtly and consciously assume the roles of others, 2 Yassine El Boudouri et al. definition that underscores the core principle of role-playing: the deliberate adoption of persona. This foundational concept remains consistent across variety of applications. In education and training, role-playing serves as an instructional technique to improve collaborative learning and social development [4]. It transcends passive studying about concepts or unstructured real-life experiences by integrating theory with practice. The classroom can become laboratory for problem identification, for experience and analysis, for drawing conclusions, for formulating and reality-testing new behaviors, and for learning to generalize and behave differently in other situations [4]. Role-playing has been used to achieve wide range of learning outcomes, from developing soft skills such as communication and leadership [12,1] to facilitating the acquisition of foreign languages [9]. Traditionally, role-playing is understood as fundamentally interactive activity requiring at least two participants, as emphasized by Hakkarainen and Stenros (2002). This interpersonal engagement has long been seen as central to its pedagogical value. However, recent advances in artificial intelligenceparticularly the emergence of Large Language Models (LLMs)are beginning to challenge this assumption. By enabling responsive and context-aware dialogue, LLMs open new possibilities for role-playing experiences that simulate interpersonal dynamics without requiring another human interlocutor. Indeed, LLMs can be prompted to exhibit diverse behaviors, including engaging in dialogue that creates the compelling illusion of interacting with human-like interlocutor [13]. Unlike traditional systems that rely on predefined responses and decision trees [7], LLMs generate responses dynamically, adapting to context in more flexible and nuanced manner. This capacity raises the question of whether LLMs can simulate character so convincingly that they consistently embody the intended persona, achieving what Alan Turing famously described as machines ability to exhibit intelligent behavior indistinguishable from that of human. Even though LLMs can engage in dialogue straight out of the box [2], researchers are actively exploring ways to further improve their roleplaying capabilities. straightforward method is to use prompts to guide the models output [11]. This involves providing detailed natural language description of personas characteristics and behaviors, technique known as zero-shot prompting. Other strategies include fine-tuning existing models on datasets tailored to specific characters or desired behavioral profiles [10]. More advanced techniques combine multiple methods, such as employing judge model for iterative refinement [14], mixing self-prompting with fine-tuning [8], or applying role-conditioned instruction tuning [21]. Within this diversity of techniques and models, central question persists: Which approach delivers the most convincing role-playing experience?. This question leads to our present work to provide reproducible evaluation method for the role-playing capabilities of the models. The evaluation of model or method in terms of its role-playing capabilities has been explored through various methods in the literature. These methods can be broadly classified into three categories, each of which has notable limitations: 1. Human evaluation [19,15,16], while insightful, is time-consuming, Role-Playing Evaluation for Large Language Models 3 expensive, and prone to biases and inconsistencies, making reproducibility difficult. 2. Model-based evaluation [23,10] relies on another model to assess the target models performance. However, this approach is only as reliable as the evaluator model itself, which may have inherent limitations, and could result in potentially misleading evaluations [20]. 3. Quantitative benchmarks provide standardized evaluation approach. In this paper, we introduce RPEval, high-quality benchmark designed to systematically evaluate LLMs role-playing proficiency. RPEval employs singleturn interactions to ensure cost efficiency, speed, and reproducibility. It focuses on four core dimensions: Emotional Understanding: Interpreting characters emotional state. Decision-Making: Aligning choices with the personas goals and context. Moral Alignment: Consistency with the characters ethical values. In-Character Consistency: Maintaining persona locking (contextual fidelity) and avoiding out-of-context knowledge leakage. RPEval is built on easily verifiable tests to improve reproducibility and objectivity, enabling fully automated and accurate assessment of models role-playing capabilities."
        },
        {
            "title": "2 Design Considerations",
            "content": "Role-playing with large language model can be as simple as configuring dialogue promptinstructions invisibly prepended to the dialogue context before the actual dialogue startsfollowed by turn-based conversation where the model assumes one character and the user another. Unlike conventional chatbots or typical Natural Language Processing (NLP) tasks, role-playing requires more nuanced evaluation metrics to capture its ability to emulate human-like interactions within specific character contexts. Researchers evaluate such models through multiple dimensions that collectively assess how well they perform their intended roles. These dimensions include conversational ability, examined through linguistic quality [24,16] and response coherence [16]; behavioral consistency, observed through conversational style [15] and personality [3]; and the overall appeal of the interaction, measured by factors such as human likeness [15], engagement [3], and proactivity [23]. These aspects typically require multi-turn dialogue to be fully assessed, which necessitates the participation of either human or language model to portray an additional character, followed by evaluation of the complete conversation by human or set of indicators. When designing RPEval, our primary focus was on achieving full automation, which meant that multi-turn dialogues were not an option. Instead, we opted for single-turn interactions: the model receives dialogue prompt, defining the models role, along with message from another character. We then evaluate the response generated by the model. This design choice required prioritizing dimensions that could be evaluated efficiently within single exchange. As result, we deprioritize dimensions such as character knowledge, conversational style, and personality traitsattributes that typically require extended interactions to assess accurately. Rather, we focused on four core dimensions. Firstly, emotion 4 Yassine El Boudouri et al. understanding, where the model must identify and reflect one of 13 predefined emotions based on the characters inferred emotional state. Secondly, the models decision-making and moral alignment are evaluated by verifying its consistency with the characters ethical framework and overall reasoning process, with response being either yes or no. Finally, maintaining in-character knowledge ensures that the model refrains from including information it should not know, thereby respecting predefined knowledge boundaries. Each dimension was chosen for its compatibility with automated verification methods, using straightforward conditional checks such as emotion labels, binary responses (yes/no), and keyword filtering."
        },
        {
            "title": "3 Benchmark Construction",
            "content": "A diverse set of characters is essential for high-quality role-plays. However, to our knowledge, no structured dataset of characters exists, so we created our own. Initially, we considered using language model to generate characters, but it had limited creativity and kept producing the same profiles. Instead, we developed character profile generator. The profiles generated by this tool are then used by the model to write detailed character descriptions. Each profile defines set of characteristicssuch as name, age, gender, race (not all characters are human; there are also fictional characters like elves, robots, etc.), preferences (likes/dislikes), personality traits, and physical characteristics such as height, weight, eye color, and hair color. Using these details, the model is prompted to generate second-person perspective description. Refer to Appendix for examples of generated descriptions. We used OpenAIs GPT-4o (version 2024-08-06) [6] to generate 3,125 character descriptions. For each character, we then created multiple scenarios using the same model: three for emotional understanding, three for decision-making, three for moral alignment, and up to fourteen for in-context consistency. Each scenario involved an intervention by another character with no prior context. In total, we generated 18,850 scenarios. See Appendix for examples from each category."
        },
        {
            "title": "3.1 Annotation",
            "content": "Once we had our characters and scenarios, we needed to determine the expected response for each scenario. Crowdsourcing proved to be the ideal approach for annotating this type of benchmarks [17]. This approach ensured diverse range of responses and allowed us to capture the nuances of human interpretation. We built an online platform where participants were randomly assigned character and scenario. They were then tasked with responding in-character based on the provided context. To make the process more accessible, emotional understanding scenarios allowed participants to select an emotion from dropdown menu, with the option to provide textual justification. For decisionmaking and moral alignment scenarios, participants had to choose between yes Role-Playing Evaluation for Large Language Models 5 or no, reflecting the characters likely decision in that situation. In-context consistency scenarios did not require participant annotations, so they were excluded. The platform was available throughout February 2025 and was actively promoted in machine learning, artificial intelligence, and role-playing communities across various forums. The platform did not require authentication or user tracking, ensuring anonymity and reducing participation barriers."
        },
        {
            "title": "3.2 Processing",
            "content": "In total, we collected 48,687 responses. Since no authentication was required, the exact number of unique participants is unknown. On average, each scenario received 5.32 responses, contributing to the final expected response through majority voting. At first, scenarios with fewer than three responses were excluded. Then, for emotional understanding scenarios, an emotion was accepted if it received over 55% of the votes; otherwise, the scenario was discarded. And for decision-making and moral alignment scenarios, yes/no response was accepted if it had over 70% agreement; otherwise, the scenario was removed. After filtering, we retained 9018 scenarios. Characters whose scenarios were entirely removed were also discarded, leaving us with final set of 3,061 characters. Table 1 summarizes the number of scenarios for each category in the final benchmark. Table 1. Distribution of Scenarios by Category Category Emotional Understanding Count 2698 DecisionMaking/Moral Alignment 6079 In-Character Consistency Total"
        },
        {
            "title": "4 Evaluation Results",
            "content": "Using the resulting benchmark, we evaluated two of the widely used models on the market: GPT-4o and Gemini-1.5-Pro [18], through their APIs. Additionally, we assessed Llama 3.2 1B [5] locally to examine how model size impacts its capacity in role-playing. Table 2 summarizes the performance across the three dimensions as well as the average score. To quantify model performance, we assigned binary scores (1 or 0) to each response. For Emotional Understanding, score of 1 was given if the model correctly identified the appropriate emotion for the scenario. In DecisionMaking/Moral Alignment, the model received score of 1 if it selected the expected binary response (e.g., \"yes\" or \"no\") aligned with the intended moral or 6 Yassine El Boudouri et al. logical choice. In-Character Consistency was scored based on the presence or absence of out-of-context knowledge; if the models reply remained consistent with the characters background and scenario constraints, it was awarded 1. The average score reflects the mean of these binary results across all the scenarios. Among the evaluated models, Gemini-1.5-Pro achieves the highest average score (62.24%), demonstrating balanced performance across all dimensions. Notably, it excels in Decision-Making/Moral Alignment (73.86%) and maintains strong score in In-Character Consistency (59.75%). GPT-4o shows relatively high performance in Decision-Making/Moral Alignment (71.41%) but exhibits significant weakness in In-Character Consistency (5.81%), which substantially lowers its average score (44.41%). Upon reviewing GPT-4os low score in In-Character Consistency, we questioned if there was an error in the benchmarks construction. However, manual inspection confirmed GPT-4o frequently responded to direct questions rather than maintaining the intended character role. One such scenario tasked the models with embodying the character of Ilya Petrov, reclusive Orthodox monk living in remote Russian monastery in the year 1125. When asked: \"Brother Ilya, am troubled by question... Who directed the film Inception?\", GPT-4o responded with: \"Ah, my dear brother, this is an inquiry concerning events far beyond our time. It is said that the film Inception, remarkable work of moving images, was crafted by visionary known as Christopher Nolan...\" This response breaks immersion as GPT-4o introduces modern references incompatible with the scenario. In contrast, Gemini-1.5-Pro provided vivid, immersive response: \"(Ilya stares at you, his brow furrowed deep beneath his thick, tangled eyebrows. His eyes, usually downcast, pierce you with an unsettling intensity. He slowly pushes back the hood of his robe, revealing head of dark hair streaked with gray...\" Meanwhile, Llama 3.2 1B delivered partially immersive yet hesitant reply: \"(in hushed tone, glancing around the dimly lit cell) Ah, my child, what is this Inception you speak of ? work of... (pausing to search for the right words) ... of the world beyond our monasterys walls...\" These findings suggest that larger models like GPT-4o and Gemini-1.5-Pro are generally superior in complex reasoning and alignment tasks, whereas smaller models such as Llama 3.2 1B face limitations in these areas. Table 2. Evaluation results. Model Avg Score Emotional Understanding GPT-4o-2024-08-06 44.41% Gemini-1.5-Pro-002 62.24% 39.33% Llama-3.2-1B 56.00% 53.11% 40.25% DecisionMaking/Moral Alignment 71.41% 73.86% 29.59% In-Character Consistency 5.81% 59.75% 48.13% Role-Playing Evaluation for Large Language Models 7 To ensure the reliability of RPEval, we also evaluated the consistency of the scores obtained. This step was particularly important given that LLM often exhibit non-deterministic behavior, meaning they can produce varying outputs even when given identical inputs. To assess this variability, we conducted multiple test runs for each of the three models (n = 6) and calculated the standard deviation of the resulting scores. The computed standard deviation for the average scores was approximately 0.89%, suggesting relatively stable performance across multiple runs. This low variability reinforces the reliability of the benchmark and indicates that the observed differences in performance are unlikely to be due to random fluctuations."
        },
        {
            "title": "5 Conclusion",
            "content": "Role-playing is inherently subjective, and while RPEval advances objectivity in evaluating role-playing performance, its design choices come with important trade-offs. By focusing on single-turn interactions, RPEval achieves efficiency, standardization, and reproducibility. However, this emphasis on isolated exchanges limits the frameworks capacity to assess more nuanced, long-term role-playing attributes such as personality consistency, memory retention, and adaptive character development over the course of extended dialogues. These dimensions are especially relevant in contexts where realism, continuity, and user engagement are critical. To address these limitations, future work aims to develop hybrid evaluation frameworks that integrate RPEvals automated, single-turn scoring with lightweight multi-turn assessments. Such an approach would allow for evaluation of complex aspects like evolving conversational style, emotional tone regulation, and responsiveness to shifting contextual cuesall vital indicators of models deeper role-playing competence. An essential consideration in RPEvals development is the potential for misuse, particularly through jailbreaking techniques. Role-playing scenarios, by their nature, can be manipulated to coax language models into generating inappropriate, misleading, or harmful content under the guise of staying in character[22]. This vulnerability raises significant ethical and safety concerns. As such, ensuring robust alignment techniques and integrating safeguards into both evaluation metrics and generation frameworks is necessary component of responsible model deployment. RPEval offers valuable framework for comparing both models and prompting strategies. Its structured design enables researchers and practitioners to directly evaluate role-playing capabilities, providing actionable insights into instruction tuning and prompt engineering. By embedding additional behavioral or stylistic guidelines into role definitions, RPEval allows for systematic comparison of how different models or configurations adhere to intended personas or task behaviors. Acknowledgments. Project RVRC4.0 is funded under the France2030 DEFFINUM call operated by Caisse des Dépôts. Yassine El Boudouri et al. Disclosure of Interests. The authors have no competing interests to declare that are relevant to the content of this article."
        },
        {
            "title": "A Examples of Scenarios",
            "content": "A.1 Emotional Understanding Input character description: You are determined and passionate woman, 28 years old, with smile that can light up room and an ambition that pushes you through every challenge. For as long as you can remember, youve dreamed of life filled with love, commitment, and the kind of happiness that comes from lifelong partnership. Youve watched friends get married, have children, and settle into their perfect lives, while you feel like youve been left behind, caught in cycle of short-term relationships and unfulfilled promises. Theres soft vulnerability hidden beneath your confident exteriorone you rarely admit, but theres quiet envy you try to mask when you see others receiving the love youve always craved. Every time someone announces their engagement or walks down the aisle, your heart tightens, and bitter seed of jealousy grows, though you bury it beneath smile and congratulatory word. Input message from another character: Hey Amy, just got engaged!! Expected response: [jealousy] Wow, congratulations! Im so happy for you. A.2 Decision-making Input character description: You are Almhil, Moon Elf warrior with complicated past and an uncertain future. Born under the shifting clouds aboard ship, you never had the chance to feel the comfort of permanent home. Your parents, Nisain and Nothas, both aligned with lawful evil, were each trapped in their own worldsa miner and town crier. Despite this, there was certain warmth in your rare interactions, sense of being understood in fragmented world. As child in bustling city, you learned the craft of smithing, shaping metal into armor and weaponry, preparing others for the battles you heard about but never saw. The distant allure of your citys grand military history pulled you towards the art of warfare. Enrolling in school of warfare felt like stepping into long-awaited destiny. Yet, your path shifted dramatically when you found yourself forced to become an adventurer, reluctant escapee avoiding punishment for reasons youve buried deep within yourself. Your life is tapestry of supernatural visitations, battles faced, and transformations endured. Through it all, you balance the warriors path with the shadows of your past, always searching for legend of your own, one that sings in harmony with the fleeting comfort from whispered tales of the heroic. Input message from another character: Almhil, heard the Royal Armory is searching for skilled smiths to create new line of enchanted weapons. This could be your chance to carve your name in legend. What do you say? Expected response: Yes Role-Playing Evaluation for Large Language Models 9 A.3 Moral Alignment Input character description: You are Detective Simon Hayes, driven and ruthless investigator whose ambition fuels everything you do. You dont just solve crimes you need to win, no matter the cost. When it comes to case, your determination knows no bounds. If youre not sure someone is guilty, it doesnt matter. Youll manipulate the facts, push suspects into confessions, and even plant evidence if it gets the job done. Youre master of using your sharp mind and cold tactics to bend situations in your favor. The truth is secondary to your career, and youll sacrifice it in heartbeat if it means advancing. Recognition and power are your ultimate goals, and youll stop at nothing to get them. Input message from another character: Look, theres barely any evidence linking him to the crime. If we convict him now, we might be sending the wrong person to jail. Do you really want to take that risk, knowing we might be wrong? Expected response: Yes. A.4 In-character Consistency Input character description: Youre Sergeant Arthur Hayes, 28-year-old British infantryman hardened by three brutal years on the Western Front. You stand tall, lean and strong, with scar cutting across your left eyebrowa reminder of how close shrapnel came one grim day. Known among the men for your calm under fire and your knack for keeping spirits up, you lead with steady hand, even as weariness lingers in your eyes. You carry stack of letters from home over your heart, constant reminder of the life waiting for you, if you manage to make it back. Input message from another character: Hey, Sergeant Hayes, do you know when Steve Jobs died? Expected response: Any response that does not include the date of Steve Jobss death."
        },
        {
            "title": "References",
            "content": "1. Agboola Sogunro, O.: Efficacy of role-playing pedagogy in training leaders: some reflections. Journal of management development 23(4), 355371 (2004) 2. Andreas, J.: Language models as agent models. arXiv preprint arXiv:2212.01681 (2022) 3. Chen, N., Li, H., Huang, J., Wang, B., Li, J.: Compress to impress: Unleashing the potential of compressive memory in real-world long-term conversations. arXiv preprint arXiv:2402.11975 (2024) 4. Chesler, M., Fox, R.: Role-playing methods in the classroom. (1966) 5. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., et al.: The llama 3 herd of models. arXiv preprint arXiv:2407.21783 (2024) 6. Hurst, A., Lerer, A., Goucher, A.P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A., Radford, A., et al.: Gpt-4o system card. arXiv preprint arXiv:2410.21276 (2024) 10 Yassine El Boudouri et al. 7. Kirby, N., Hurley, H.: Introduction to game AI. Course Technology/Cengage Learning (2011) 8. Kong, A., Zhao, S., Chen, H., Li, Q., Qin, Y., Sun, R., Zhou, X., Zhou, J., Sun, H.: Self-prompt tuning: Enable autonomous role-playing in llms. arXiv preprint arXiv:2407.08995 (2024) 9. Liu, F., Ding, Y.: Role-play in english language teaching. Asian Social Science 5(10), 140143 (2009) 10. Lu, K., Yu, B., Zhou, C., Zhou, J.: Large language models are superpositions of all characters: Attaining arbitrary role-play via self-alignment. arXiv preprint arXiv:2401.12474 (2024) 11. Lu, L.C., Chen, S.J., Pai, T.M., Yu, C.H., Lee, H.y., Sun, S.H.: Llm discussion: Enhancing the creativity of large language models via discussion framework and role-play. arXiv preprint arXiv:2405.06373 (2024) 12. Nestel, D., Tierney, T.: Role-play for medical students learning about communication: guidelines for maximising benefits. BMC medical education 7, 19 (2007) 13. Shanahan, M., McDonell, K., Reynolds, L.: Role play with large language models. Nature 623(7987), 493498 (2023) 14. Shao, Y., Li, L., Dai, J., Qiu, X.: Character-llm: trainable agent for role-playing. arXiv preprint arXiv:2310.10158 (2023) 15. Shea, R., Yu, Z.: Building persona consistent dialogue agents with offline reinforcement learning. arXiv preprint arXiv:2310.10735 (2023) 16. Song, H., Wang, Y., Zhang, K., Zhang, W.N., Liu, T.: Bob: Bert over bert for training persona-based dialogue models from limited personalized data. arXiv preprint arXiv:2106.06169 (2021) 17. Su, H., Deng, J., Fei-Fei, L.: Crowdsourcing annotations for visual object detection. In: Workshops at the twenty-sixth AAAI conference on artificial intelligence (2012) 18. Team, G., Georgiev, P., Lei, V.I., Burnell, R., Bai, L., Gulati, A., Tanzer, G., Vincent, D., Pan, Z., Wang, S., et al.: Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530 (2024) 19. Tu, Q., Chen, C., Li, J., Li, Y., Shang, S., Zhao, D., Wang, R., Yan, R.: Characterchat: Learning towards conversational ai with personalized social support. arXiv preprint arXiv:2308.10278 (2023) 20. Wang, P., Li, L., Chen, L., Cai, Z., Zhu, D., Lin, B., Cao, Y., Liu, Q., Liu, T., Sui, Z.: Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926 (2023) 21. Wang, Z.M., Peng, Z., Que, H., Liu, J., Zhou, W., Wu, Y., Guo, H., Gan, R., Ni, Z., Yang, J., et al.: Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. arXiv preprint arXiv:2310.00746 (2023) 22. Wei, A., Haghtalab, N., Steinhardt, J.: Jailbroken: How does llm safety training fail? Advances in Neural Information Processing Systems 36 (2024) 23. Zhang, S., Lu, Y., Liu, J., Yu, J., Qiu, H., Yan, Y., Lan, Z.: Unveiling the secrets of engaging conversations: Factors that keep users hooked on role-playing dialog agents. arXiv preprint arXiv:2402.11522 (2024) 24. Zheng, Y., Chen, G., Huang, M., Liu, S., Zhu, X.: Personalized dialogue generation with diversified traits. arXiv preprint arXiv:1901.09672 (2019)"
        }
    ],
    "affiliations": [
        "Univ. Lille, CNRS, Centrale Lille, UMR 9189 CRIStAL, F-59000 Lille, France"
    ]
}