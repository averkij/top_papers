{
    "paper_title": "Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization",
    "authors": [
        "Nikita Kachaev",
        "Mikhail Kolosov",
        "Daniil Zelezetsky",
        "Alexey K. Kovalev",
        "Aleksandr I. Panov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The growing success of Vision-Language-Action (VLA) models stems from the promise that pretrained Vision-Language Models (VLMs) can endow agents with transferable world knowledge and vision-language (VL) grounding, laying a foundation for action models with broader generalization. Yet when these VLMs are adapted to the action modality, it remains unclear to what extent their original VL representations and knowledge are preserved. In this work, we conduct a systematic study of representation retention during VLA fine-tuning, showing that naive action fine-tuning leads to degradation of visual representations. To characterize and measure these effects, we probe VLA's hidden representations and analyze attention maps, further, we design a set of targeted tasks and methods that contrast VLA models with their counterpart VLMs, isolating changes in VL capabilities induced by action fine-tuning. We further evaluate a range of strategies for aligning visual representations and introduce a simple yet effective method that mitigates degradation and yields improved generalization to out-of-distribution (OOD) scenarios. Taken together, our analysis clarifies the trade-off between action fine-tuning and the degradation of VL representations and highlights practical approaches to recover inherited VL capabilities. Code is publicly available: https://blind-vla-paper.github.io"
        },
        {
            "title": "Start",
            "content": "Dont Blind Your VLA: Aligning Visual Representations for OOD Generalization Nikita Kachaev Cognitive AI Lab Moscow, Russia Mikhail Kolosov IAI MIPT Moscow, Russia Daniil Zelezetsky IAI MIPT Moscow, Russia Alexey K. Kovalev Cognitive AI Lab, IAI MIPT Moscow, Russia Aleksandr I. Panov Cognitive AI Lab, IAI MIPT Moscow, Russia 5 2 0 2 9 2 ] . [ 1 6 1 6 5 2 . 0 1 5 2 : r ABSTRACT The growing success of Vision-Language-Action (VLA) models stems from the promise that pretrained Vision-Language Models (VLMs) can endow agents with transferable world knowledge and vision-language (VL) grounding, laying foundation for action models with broader generalization. Yet when these VLMs are adapted to the action modality, it remains unclear to what extent their original VL representations and knowledge are preserved. In this work, we conduct systematic study of representation retention during VLA fine-tuning, showing that naive action fine-tuning leads to degradation of visual representations. To characterize and measure these effects, we probe VLAs hidden representations and analyze attention maps, further, we design set of targeted tasks and methods that contrast VLA models with their counterpart VLMs, isolating changes in VL capabilities induced by action fine-tuning. We further evaluate range of strategies for aligning visual representations and introduce simple yet effective method that mitigates degradation and yields improved generalization to out-of-distribution (OOD) scenarios. Taken together, our analysis clarifies the trade-off between action fine-tuning and the degradation of VL representations and highlights practical approaches to recover inherited VL capabilities. Code is publicly available: blind-vla-paper.github.io"
        },
        {
            "title": "1 INTRODUCTION\nVisionâ€“Language Models (VLMs) have demonstrated remarkable\nsuccess due to their ability to integrate large-scale multimodal\ndatasets, thereby acquiring semantic grounding and generalizable\nvisual-language (VL) representations [2, 3, 5, 16, 37, 48]. When ex-\nposed to novel visual or linguistic contexts, such models exhibit\nrobust cross-modal understanding and compositional perception\nâ€“ properties that underpin their strong zero and few-shot gener-\nalization beyond the training distribution. These advancements\nhave naturally inspired the extension of VLMs toward embodied\ndomains.",
            "content": "VisionLanguageAction (VLA) models represent prominent direction in this research trajectory. They adapt pretrained VLMs to action prediction tasks in robotic settings, with the goal of leveraging the semantic priors and cognition abilities inherited from large-scale visionlanguage pretraining. The underlying hypothesis is that, if appropriately adapted, VLA models can transfer the visualsemantic representations of their initial VLM to the action domain, enabling generalization to previously unseen scenes, instructions, and scenarios. However, in practice, adapting VLMs to the action modality often introduces new challenges. Several recent Under review Figure 1: Visual alignment method overview. Mid-level VLA features are projected onto normalized sphere and aligned with teacher embeddings, preserving visual semantics and improving OOD generalization. Bottom plots show comparison with standard SFT across three generalization axes on the Simpler-based benchmark [33]. studies [11, 15, 32, 36, 40] have shown that current VLA models struggle to maintain generalization in visually and linguistically complex tasks, raising questions about whether strong VL capabilities of VLMs truly transfer to embodied settings. This issue becomes the most evident during task-specific fine-tuning, where limited data diversity and datasets frequently lead to overfitting [13, 14, 40, 42, 54]. During large-scale robotic pretraining, recent works have attempted to mitigate this degradation by preserving multimodal understanding capabilities. Prior strategies include incorporating auxiliary reasoning objectives [10], applying multimodal co-training on web-scale data [52], or freezing pretrained visuallanguage backbones to preserve VL representations and improve instruction following [15, 38]. While these approaches help retain visionlanguage knowledge and improve generalization, they often depend on heavy supervision, high computational cost, or constrained model architecture. Yet, despite these advances at the pretraining stage, there remain no effective methods to address representation degradation during task-specific supervised finetuning (SFT) the critical phase where VLA models must adapt to certain robotic domains without losing their semantic grounding and VL abilities. Figure 2: Overview of the proposed method. (a, b) Training pipeline with visual alignment loss no extra overhead, only precomputed teacher features and lightweight regularization term during SFT. (c) Conceptual illustration of the loss landscape for VL tasks: the core idea is to optimize the model with respect to the action objective while preserving performance on VL understanding. In this work, we adopt realistic VLA deployment setting: starting from pretrained VLA and adapting it with limited data for supervised fine-tuning in chosen embodiment and domain. Under these constraints, we conduct systematic investigation into the degradation of VL representations and multimodal understanding abilities in VLA models and ask central question: Can we design simple yet effective method to recover the inherited VL representations during fine-tuning on robotic actions? To answer this question, we first examined the attention maps and feature activations of the VLA model in comparison to VLMs across matched image-instruction pairs from the robotics domain. Our analysis of attention maps revealed that: while the pretrained VLM accurately focuses on task-relevant objects, the fine-tuned VLA models often produce diffuse or misplaced activations, failing to attend to key entities under out-of-distribution (OOD) conditions (Figure 4). Next, we conducted t-SNE [46] analysis of intermediate representations across VLMs and VLAs layers, which exposed clear representation collapse [1, 4] in VLA models indicating that standard action fine-tuning compresses diverse internal features into narrow representation space, reducing representational diversity and generalization capacity. Next, we propose VL-Think task suite (section 4) to assess transfer of VL knowledge from VLMs to VLA models, benchmark several strong VLMs and compare OpenVLA7B [26] to its pretrained base (PrismaticVLM [25]). We observe systematic, domain-specific forgetting after action fine-tuning, indicating that VLAs lose VL knowledge about domains absent from the robotics fine-tuning data. To address this representational degradation, we introduce lightweight Visual Representation Alignment method inspired by the Platonic Representation Hypothesis [22]. This hypothesis suggests that large vision and language models tend to converge toward shared latent representation space that encodes general visual and semantic representations across generalist models. Our method explicitly constrains the visual representations of VLA to remain aligned with generalist vision model throughout fine-tuning. By maintaining this link, the VLA preserves semantic consistency while adapting its action policy to new tasks. The method adds negligible computational overhead and integrates seamlessly with SFT (Figure 2). Extensive experiments on different variations of Simpler [28] benchmark demonstrates that this alignment consistently improves out-of-distribution generalization yielding up to 10% relative gain over naive SFT  (Table 1)  . Our key contributions are as follows: (1) We systematically demonstrate that naive VLA fine-tuning induces representation collapse and attention sink relative to their initial VLM. (2) We introduce VL-Think, diagnostic task suite for assessing transfer of VL knowledge from VLMs across VLA models and show that VLA action fine-tuning lead to domainspecific forgetting. (3) We propose simple and efficient visual alignment method that anchors the VLAs vision representations to strong visual teacher features, preserving multimodal understanding and improving OOD generalization without added complexity (Figure 2). Taken together, our findings provide new insights into the tradeoff between action fine-tuning and representation degradation in VLA models. They underscore the importance of maintaining visuallanguage alignment during fine-tuning and provide practical recipe for building VLAs that do not blind the pretrained perceptual knowledge they rely upon."
        },
        {
            "title": "2.2 Representation alignment\nRecent studies reveal a consistent pattern: as models scale in pa-\nrameters, data, and tasks, their representations increasingly align\nacross architectures and modalities. The Platonic Representation",
            "content": "2 Figure 3: VL-Think Task Suite examples. Each panel illustrates pick-and-place episode where the agent must place an object on the board matching the instructed concept (e.g., color, number, symbol, or category). Hypothesis [22] frames this as convergence to shared statistical model of reality, independently trained vision and language encoders show semantically compatible spaces, and large languagefree visual models reach CLIP-level performance while naturally aligning with text [17, 17, 35]. Recent representation learning methods reinforce this trend: REPA [53] aligns diffusion hidden states to strong image encoders (faster training, better ImageNet quality), OLA-VLM [23] distills multi-teacher targets into intermediate LLM layers via predictive embedding losses, 3DRS [21] injects 3D-aware supervision with multi-view correspondence, and Geometry Forcing [51] aligns videodiffusion features with 3D backbone via angular/scale objectives for temporally consistent generations."
        },
        {
            "title": "3 PRELIMINARIES\nVLA architecture. Let the input multimodal token sequence to\nthe VLM backbone be",
            "content": "ğ‘¥1:ğ‘› = [ğ‘¥1:ğ‘˜, ğ‘¥ğ‘˜+1:ğ‘›]. (1) where ğ‘¥1:ğ‘˜ correspond to visual tokens and ğ‘¥ğ‘˜+1:ğ‘› correspond to textual instruction tokens. These tokens are obtained from two encoders: ğ‘¥1:ğ‘˜ = ğ¸image (ğ¼ ) Rğ‘˜ ğ‘‘ğ‘’ , ğ‘¥ğ‘˜+1:ğ‘› = ğ¸text (â„“) R(ğ‘›ğ‘˜ ) ğ‘‘ğ‘’ . (2) where ğ¸image and ğ¸text denote the image and text encoders into the common embedding space of dimension ğ‘‘ğ‘’ of the VLA model, and ğ¼ and â„“ are the input image and textual instruction, respectively. The combined sequence ğ‘¥1:ğ‘› is processed by multimodal Transformer backbone ğµğœƒ : Rğ‘›ğ‘‘ğ‘’ Rğ‘›ğ‘‘ğ‘’ with ğ¿ stacked layers. Denote the hidden states after layer ğ‘– by â„ğ‘– 1:ğ‘› Rğ‘›ğ‘‘ğ‘’ . Each layer updates the hidden states using standard self-attention with â„0 1:ğ‘› = Attention(â„ğ‘– 1 â„ğ‘– 1:ğ‘› ) + FFN(â„ğ‘– 1 1:ğ‘› ), (3) Autoregressive objective. Let ğ‘¦1:ğ‘š denote the target output tokens (from the same vocabulary as text tokens). At the decoding step ğ‘¡, the model conditions on the concatenation of the input and the previously generated tokens: 1:ğ‘› = ğ‘¥1:ğ‘›.: ğ‘– = 1, . . . , ğ¿. ğ‘¥ (ğ‘¡ ) 1:ğ‘›+ğ‘¡ 1 = [ ğ‘¥1:ğ‘›, ğ‘¦1:ğ‘¡ 1 ], 1:ğ‘›+ğ‘¡ 1 = ğµğœƒ(cid:0) ğ‘¥ (ğ‘¡ ) â„ğ¿ 1:ğ‘›+ğ‘¡ 1 (cid:1). (4) 3 The Transformer then defines the autoregressive distribution ğ‘ğœƒ(cid:0)ğ‘¦ğ‘¡ ğ‘¥1:ğ‘›, ğ‘¦1:ğ‘¡ 1 (cid:1) = softmax(cid:0)ğ‘Šğ‘œ â„ğ¿ ğ‘›+ğ‘¡ 1 (cid:1) (cid:2) ğ‘¦ğ‘¡ (cid:3) . (5) where ğ‘Šğ‘œ is the output projection to the token vocabulary, the causal mask in ğµğœƒ ensures that â„ğ¿ ğ‘›+ğ‘¡ 1 depends only on ğ‘¥1:ğ‘› and ğ‘¦1:ğ‘¡ 1. Training uses the standard next-token loss: LVLA (ğœƒ ) = E(ğ‘¥,ğ‘¦)D (cid:34) ğ‘›1 ğ‘—=1 ğ‘€ğ‘— log ğ‘ğœƒ(cid:0)ğ‘¦ ğ‘—+1 ğ‘¥1:ğ‘— (cid:1) (cid:35) . (6) with mask ğ‘€ selecting target positions (we consider the usual causal language-modeling setup)."
        },
        {
            "title": "4.1 Evaluation protocol\nTo quantify the gap in VL capabilities, we perform evaluations\nacross both VLA and VLM models.\nVLA evaluation. The agent observes RGB frames and language\ninstructions. The success rate is recorded if a well-known object is\nplaced on the correct target board. Since motion complexity is fixed,\nthis directly measures the modelâ€™s capacity to ground language in\nvisual categories rather than its manipulation skills.",
            "content": "VLM evaluation. To assess reasoning in robotics setup without actions, the same scenes are presented as static initial images with the probe: Do you see the <board_name>?. Answer yes or no. If yes, specify where: left, center, or right. response is counted as successful only if both the predicted board and its target location match the ground truth, yielding success rate that serves as an action-free measure of semantic grounding."
        },
        {
            "title": "4.2 VL-Think description\nTo reduce the embodiment and setup-specific adaptation bottle-\nnecks, VL-Think Task Suite is based on the realistic Simpler [28]\nbenchmark with WidowX-250S arm pick-and-place task. Each episode\nspawns a single source well-known object (carrot) positioned to\nyield 100% grasp reliability and multiple planar â€œboardsâ€ textured\nwith abstract categories (e.g., icons, shapes, numerals). A language\ninstruction specifies a single target concept (shape, color, icon class,\ndirection, or parity). The agent succeeds if it places the carrot on the\nboard that matches the instructed concept. By keeping the objects\nand action complexity fixed, the evaluation isolates VL skills while\nbounding execution complexity.",
            "content": "The VL-Think suite consists of eight board-selection tasks that probe different aspects of knowledge (see Figure 3). In each task, the agent must place the object on the board that matches the instructed concept: Shape the board whose graphic is the named geometric shape; e.g., Put the object on the star.), Color the board whose shape has the named color; e.g., Put the object on the blue shape, Traffic the board depicting one of 24 common traffic signs; e.g., the yield sign, Laundry care the board depicting one of 17 standard laundry symbols, e.g., Do not bleach, Weather the board depicting one of 9 common weather icons; e.g., sunny, cloudy, Directional arrow the board whose arrow points in the named direction: up, down, left, right, Public information the board depicting one of 14 public-information signs; e.g., no dogs allowed, and Numeral parity the board whose printed numeral matches the requested parity (odd or even); e.g., Put the object on the odd number."
        },
        {
            "title": "5 VL REPRESENTATIONS ANALYSIS\nIn this section, we ask: what happens to VL representations and\nknowledge in VLA models after action fine-tuning? Does knowl-\nedge transfer from VLMs actually occur, and is strong semantic\ngrounding retained?",
            "content": "To examine how strongly VL representations degrade in VLA models, we conduct complementary analyses. First, we use t-SNE [47] visualization to assess whether the model preserves structured and separable latent space for instruction-related tokens. Second, we analyze attention maps to evaluate how accurately the model focuses on objects referenced in the input instruction. Finally, using the VL-Think suite, we assess the transferability of VLM VL skills to VLA policies. Together, these methods provide intuitive and interpretable diagnostics of VL representation degradation and domain forgetting revealing whether the model maintains focused visual grounding, coherent latent organization and erodes domain-specific knowledge after action fine-tuning. Figure 4: Attention map comparison: the strongest and most semantically grounded attention appears around middle layers. OpenVLA fine-tuned with our proposed method (OpenVLA Align) maintains object-aligned focus in attention maps, while default OpenVLA SFT shows diffused and noisy patterns, indicating loss of visual-language grounding (for more results see Appendix Figure 6)."
        },
        {
            "title": "5.1 Attention sink\nTo further investigate how fine-tuning affects the VL grounding\ncapabilities of VLA models, we examine their attention maps, which\nreveal how effectively the model focuses on the object referenced in\na textual instruction. This analysis provides a direct probe into how\nwell the model maintains connection between visual and language\nfeatures. For each model, we visualize the attention maps for visual\npatch embeddings from the middle layers. Following prior studies\n[56], we observe (Figure 4) that the strongest and most semanti-\ncally meaningful attention patterns typically emerge in the middle\ntransformer layers (layers 14â€“24), where visionâ€“language fusion is\nthe most active.",
            "content": "Among the evaluated models, Qwen2.5-VL exhibits clear and relevant object-aligned attention, indicating that its attention is precisely localized on the queried object with minimal spatial noise. In contrast, OpenVLA displays substantial degradation in attention quality: the maps become diffuse, noisy, and weakly correlated with the target object indicating attention sink [24, 31]. Instead of concentrating on relevant image regions, the OpenVLAs attention maps frequently leak into irrelevant background regions or concentrate on distractor objects (for more results see subsection A.2). By contrast, our proposed Visual Representation Alignment approach remedies this issue: OpenVLA (Align) trained with it produces crisp, object-centric attention maps (see subsection A.2 for details)."
        },
        {
            "title": "5.2 Representations collapse\nTo analyze how action fine-tuning affects the internal VL repre-\nsentations of VLA models, we conducted a t-SNE representation\nprobe comparing Qwen2.5-VL [3], PrismaticVLM [25], and Open-\nVLA [26]. This experiment provides a qualitative view of how the\nsemantic structure in the latent space evolves through the action\ntraining process. We use the COCO dataset [30] and select sam-\nples from three common household object classes: cup, bottle, and\nknife. For each image, the model receives a textual query of the\nform â€œDo you see <object_name>?â€. Then we extract the embedding\ncorresponding to the token <object_name> from transformer layers\nand then project these embeddings into two dimensions using the\nt-SNE algorithm. Each point in the visualization is color-coded by\nits object class, allowing us to observe how distinct or entangled\nthe category clusters become.",
            "content": ""
        },
        {
            "title": "6.1 Visual representation alignment\nWe propose a lightweight visual alignment method that recover\ngeneralized and semantically consistent visual representations in-\nside a VLA model by regularizing its internal embeddings to remain\nclose to those of a frozen, pretrained vision teacher. In the Platonic\ninterpretation, the teacher encoder provides a more stable and\nsemantically precise projection of the generalized representation\nspace, while the VLAâ€™s own representations form a task-adapted\napproximation of this space. By minimizing their discrepancy, the\nmodel is guided back toward a common semantic structure.",
            "content": "img denote the frozen teacher encoder that produces patchLet ğ¸ level features ğ‘§1:ğ‘˜ = ğ¸ img (ğ¼ ) Rğ‘˜ ğ‘‘ğ‘¡ , (7) where each patch embedding ğ‘§ğ‘š1:ğ‘š captures localized visual semantics within the teachers high-level feature space. Within the VLA model, we select an internal layer ğ‘– that carries semantically rich visual information and extract the corresponding vision tokens â„ğ‘– 1:ğ‘˜ Rğ‘˜ ğ‘‘ğ‘’ . Since the dimensionalities differ, we propose projector ğ‘ƒğœ‘ : Rğ‘‘ğ‘’ Rğ‘‘ğ‘¡ and define ğ‘¢1:ğ‘˜ = ğ‘ƒğœ‘(cid:0)â„ğ‘– 1:ğ‘˜ We then compute patch-wise similarity between the students projected embeddings and the teachers features: (8) (cid:1). Lalign = 1 ğ‘˜ ğ‘˜ ğ‘—=1 Sim(cid:0)ğ‘¢ ğ‘—, ğ‘§ ğ‘— (cid:1), (9) This objective encourages the hidden representations from the VLAs latent feature space to remain aligned with the teachers generalized visual representations, helping preserve perceptual consistency across tasks and environments."
        },
        {
            "title": "6.2 Objective\nThe total loss integrates the standard autoregressive action objective\nwith the alignment term:",
            "content": "Ltotal = LVLA + ğœ† Lalign, ğœ† > 0. (10) Here, LVLA supervises policy learning within the current environment, while Lalign acts as regularizer that limits representational drift away from generalized visual features. Gradients propagate 5 Figure 5: t-SNE visualization of token embeddings for Qwen2.5-VL, PrismaticVLM, and OpenVLA. While PrismaticVLM and Qwen2.5-VL maintains well-separated clusters for target objects, OpenVLA shows huge overlap across classes, indicating that action fine-tuning causes representations collapse. Figure 5 illustrates this comparison for the middle layers, revealing how the latent space is organized across the different models layers. In the PrismaticVLM and Qwen2.5-VL, embeddings for the three categories form well-separated clusters reflecting coherent and semantically organized latent space typical of large-scale VLMs. In contrast, OpenVLA exhibits blurred and overlapping clusters, indicating that fine-tuning for robot control disrupts the structured organization of its inherited representations. This loss of separability corresponds to phenomenon akin to representation collapse [1, 4], where previously distinct VL representations converge into less discriminative subspaces."
        },
        {
            "title": "5.3 Domain forgetting in VLA models\nUsing the VL-Think task suite (section 4), we evaluate VL capabili-\nties across several state-of-the-art VLMs: InternVL3.5 [48], Ovis2.5\n[34], Qwen2.5-VL [3] and focus on OpenVLAâ€“7B [26] versus its pre-\ntrained base PrismaticVLM [25], which we use as an approximate\nupper bound. This comparison probes how much VL knowledge\nand semantic grounding skills persist after action fine-tuning.",
            "content": "Two clear trends emerge. First, strong VLMs achieve high success rate across all domains, reflecting robust semantic grounding. Second, action fine-tuning induces systematic, domain-specific forgetting in VLA models: relative to its pretrained counterpart, OpenVLA7B exhibits substantial drops in nearly all domains, with the largest declines in symbolic and abstract categories (traffic, arrows, public information, weather). We hypothesize that VLA models lose knowledge about domains that are absent in robotics fine-tuning datasets. The single domain where transfer persists is Color: the success rate remains at the level of the initial VLM, likely because color cues are directly useful for control and are implicitly present in robotics datasets. Table 1: OOD generalization performance across evaluation environments (mean SD). The proposed alignment objective yields consistent gains over SFT and frozen-encoder baselines, indicating enhanced robustness to OOD domain shifts. Method Semantic Vision Execution Carrot Instruct MultiCarrot MultiPlate Plate VisionImg Tex03 Tex05 Whole03 Whole Position EEPose PosChangeTo 0.490.02 0.740.02 Default 0.030.01 0.050.01 Freeze Align (ours) 0.610.01 0.830.03 0.280.02 0.010.01 0.350.02 0.730.02 0.810.01 0.670.01 0.550.03 0.710.02 0.560.01 0.430.02 0.340.01 0.430.02 0.020.01 0.030.01 0.020.01 0.030.01 0.010.01 0.010.01 0.010.01 0.030.01 0.030.01 0.490.02 0.750.01 0.860.02 0.700.02 0.670.02 0.790.02 0.600.02 0.580.02 0.380. 0.230.01 0.040.01 0.200.03 through the VLAs visual encoder ğ¸img, text encoder ğ¸text, and transformer backbone ğµğœƒ , while the teacher encoder ğ¸ img remains frozen, serving as fixed reference to stable perceptual structure. From the Platonic viewpoint, our method maintains an semantic prior to shared, generalized VL knowledge. Action fine-tuning alone narrows the models perceptual space toward the statistics of specific dataset or embodiment, causing the internal features to drift away from broad generalized representations. The alignment loss restores this balance by enforcing consistency between the students intermediate features and those of strong, pre-trained vision model that encodes more general visualsemantic relationships."
        },
        {
            "title": "7 EXPERIMENTS\n7.1 Evaluation setup\nWe evaluate our approach in several robotics environment based on\nthe Simpler [28, 43] using proposed VL-Think task suite (section 4)\nand adopted benchmark introduced in [33], designed to assess VLA\ngeneralization across three axes: Vision, Semantics, and Execution:",
            "content": "Vision variations alter foreground and background via dynamic textures and image-level noise, testing robustness to weak and strong visual perturbations. Semantics introduces unseen objects and receptacles, paraphrased instructions, and multi-object or distractor scenarios that challenge compositional reasoning. Execution changes low-level control conditions through randomized initial poses and mid-episode object repositioning, probing action-level robustness. OOD evaluation holds out at least one variation factor per axis, including 9 novel objects, 16 unseen receptacles, 5 new scene textures, and 16 distractor backgrounds. Additionally, we perform linear probing on ImageNet-100 [45] to quantify the quality of VLAs representations learned using different methods. Each model variant is evaluated over 128 randomized seeds, we report success as mean standard deviation (SD). In section 8 we use the paired Wilcoxon signed-rank test [50] with one-sided alternative, and report p-values. All models are trained for the same number of epochs with identical hyperparameters to ensure fair comparison."
        },
        {
            "title": "7.2 Training setup\nFor supervised fine-tuning, we collect 1400 expert demonstration\ntrajectories using the MPLib motion planner [18]. Training random-\nization spans 16 tables, 16 objects (yielding on average âˆ¼5 episodes\nper training variation), and multiple pose perturbations. During all",
            "content": "6 fine-tuning runs, LoRA adapters [20] are applied to all linear layers of the VLA."
        },
        {
            "title": "7.3 Baselines\nUsing a widely adopted open-source OpenVLA model, we com-\npare our proposed alignment method against several fine-tuning\nbaselines.",
            "content": "Default Standard supervised fine-tuning (SFT) using cross-entropy loss on demonstration data, serving as the primary baseline. Freeze SFT with the VLAs visual encoder weights frozen during training; this setup tests the hypothesis that frozen representations might help with generalization. Align (ours) SFT combined with our auxiliary visual representation alignment loss, described in subsection 6.1, which explicitly anchors the VLAs vision encoder to pretrained generalist vision teacher."
        },
        {
            "title": "7.4 Results: OOD Evaluation\nResults in Table 1 shows that our visual alignment method yields\nconsistent improvements across all evaluation axes: Semantic, Vi-\nsion, and Execution. This result underscores the effectiveness of\nvisual representation alignment in enhancing robustness to visual\nshifts, text instruction variations, texture changes, and background\nperturbations that frequently occur in real-world scenarios. The im-\nprovement indicates that aligning internal visual-language embed-\ndings not only stabilizes perception but also reinforces the semantic\ngrounding. Conversely, the Freeze baseline completely fails across\nall categories (as also observed in [49]), yielding near-zero perfor-\nmance. This confirms that simply freezing the pretrained visual\nencoder does not preserve useful representations during adaptation.\nWithout joint optimization, the frozen features become mismatched\nwith the evolving action components, leading to severe degradation\nof both perception and control.",
            "content": "Overall, these results validate that visual alignment serves as an effective regularizer against representation degradation, allowing the model to recover general-purpose visual semantics while adapting to new robotic environments."
        },
        {
            "title": "7.5 Results: Linear probing\nTo further evaluate the representational quality learned by our\nmodel, we conduct a linear probing analysis on the ImageNet-100\ndataset [45]. Specifically, we extract patch embeddings from the\nfinal layer of the C-RADIOv3 [19] teacher and from the intermediate\nvisual layers of different OpenVLA variants. Following standard\npractice in representation learning [22, 53], we freeze the visual",
            "content": "Table 2: VL-Think VLM results across eight domains. The benchmark reveals strong correlation between VL understanding and model scale: larger VLMs achieve higher overall success. However, OpenVLA7B fine-tuned for action shows clear VL degradation: its performance drops markedly compared to the original PrismaticVLM across all domains except color, where VL skills remain largely preserved. Model Arrow Color Laundry Parity PublicInfo Shape Traffic Weather InternVL3.5-4B InternVL3.5-8B Ovis2.5-2B Ovis2.5-9B Qwen2.5-7B Prismatic-DS-7B 0.80 0.02 0.67 0.03 0.84 0.02 0.93 0.02 0.66 0.03 0.47 0. 0.94 0.01 0.94 0.01 0.99 0.01 0.94 0.01 0.87 0.02 0.69 0.03 0.23 0.02 0.13 0.02 0.47 0.03 0.52 0.03 0.26 0.03 0.37 0.03 0.54 0.03 0.47 0.03 0.55 0.03 0.55 0.03 0.58 0.03 0.45 0.03 0.72 0.03 0.80 0.02 0.78 0.02 0.89 0.02 0.48 0.03 0.62 0.03 0.80 0.02 0.77 0.02 0.89 0.02 0.87 0.02 0.81 0.02 0.59 0.03 0.62 0.03 0.60 0.03 0.72 0.03 0.79 0.02 0.61 0.03 0.48 0. 0.75 0.03 0.80 0.02 0.88 0.02 0.98 0.01 0.70 0.03 0.62 0.03 OpenVLA-7B OpenVLA-7B Align 0.26 0.02 0.24 0.02 0.69 0.02 0.82 0.02 0.30 0.03 0.29 0.03 0.43 0.02 0.42 0. 0.24 0.02 0.30 0.03 0.40 0.02 0.48 0.02 0.29 0.02 0.28 0.03 0.32 0.03 0.27 0.02 encoders and train single linear classifier on top of their frozen features to measure the separability of semantic categories. This setup directly quantifies how linearly decodable the visual features remain after action fine-tuning. The results summarized in Table 3 reveal several consistent trends. As expected, the C-RADIOv3 teacher achieves the highest probing accuracy, reflecting its strong pretrained representations. Among the VLA variants, the OpenVLA fine-tuned with our proposed Visual Representation Alignment method outperforms both the pretrained checkpoint and the model fine-tuned with naive SFT. This improvement indicates that our alignment strategy effectively enhances the VLAs representations during action fine-tuning. In contrast, naive SFT substantially reduces probing accuracy relative to the pretrained model, confirming that standard fine-tuning harms representational quality. Our aligned model not only mitigates this degradation but surpasses the pretrained baseline, indicating that the alignment loss strengthens semantic consistency and leads to more transferable visual features."
        },
        {
            "title": "7.6 Results: VL-Think\nFollowing the experiments in subsection 5.3, we evaluate of Open-\nVLA fine-tuned with our proposed visual representation alignment\nmethod (OpenVLA-7B Align), under identical data, budget, and eval-\nuation settings. Results in Table 2 show that SFT-Align partially\nmitigates domain forgetting observed under default SFT. In particu-\nlar, performance on Color and Shape domains consistently improves\neven surpassing the PrismaticVLM upper bound, but leaving other\ndomains mostly unchanged.",
            "content": "These outcomes highlight both the promise and limits of the proposed representation alignment under constrained settings. We hypothesize that the modest size and diversity of the SFT dataset and the limited expressivity of LoRA updates are insufficient to Table 3: Linear probing results. OpenVLA Align retains stronger features than both the pretrained and SFT variants, closing much of the gap to the C-RADIOv3 teacher and demonstrating improved semantic consistency after action fine-tuning. Model C-RADIOv3 OpenVLA Align OpenVLA Pretrained OpenVLA SFT Accuracy (%) 87.31 82.13 79.88 77.48 7 restore less frequent VL concepts that are underrepresented in robotics data. We hypothesize that expanding data breadth and relaxing parameter-efficiency constraints will unlock broader gains beyond commonly represented domains. Verifying this hypothesis is an important direction for future work."
        },
        {
            "title": "8.1 Visual teacher models\nA key question in our approach concerns the choice of the teacher\nmodel that provides reference representations for alignment. From\nthe Platonic perspective, each vision foundation encoder captures\na different projection of broadly generalizable visual knowledge,\nand alignment to a stronger teacher helps preserve these high-level,\ntransferable abstractions within the VLA during fine-tuning. We\ntherefore examine whether foundation models trained on large-\nscale, diverse, and multi-view data yield better alignment and\nstronger transfer.",
            "content": "To test this, we evaluate several state-of-the-art vision encoders, including DINOv2 [39], SigLIP [55], C-RADIOv3 [19], and Theia [41]. As shown in Table 4, C-RADIOv3 achieves the best overall results, Table 4: Comparison of pretrained Teacher Vision Models across generalization dimensions. Values represent mean aggregated across all environments within each dimension and p-value. Best results per column are highlighted in bold (for more details see Table 11 from Appendix)."
        },
        {
            "title": "Execution",
            "content": "C-RADIOv3 DINOv2 SigLIP Theia 0.61 0.57 (p=0.05) 0.54 (p=0.01) 0.56 (p=0.03) 0.72 0.69 (p=0.12) 0.65 (p=0.03) 0.67 (p=0.05) 0.39 0.37 (p=0.43) 0.35 (p=0.09) 0.36 (p=0.15) Table 5: Comparison of alignment paradigms across generalization dimensions. Reported as mean across dimensions and p-value, best results are highlighted in bold. Method Semantic Vision Execution Backbone2Enc Enc2Enc 0.61 0.55 (p=0.01) 0.72 0.66 (p=0.04) 0.39 0.38 (p=0.64) indicating that stronger and more capable vision models those trained on large-scale, semantically rich, and multimodal data offer more stable and generalizable visual features for alignment. Such teachers serve as stronger Platonic anchors, guiding the VLA to align with transferable and semantically consistent representations that improve robustness across tasks and domains."
        },
        {
            "title": "8.2 Alignment method\nWe next evaluate different alignment paradigms to determine which\nlevel of the VLA model benefits most from visual representation\nalignment. Two principal strategies are tested:",
            "content": "(1) Backbone2Enc Aligning the representations of the VLAs transformer backbone to the final-layer features of the teachers visual encoder. (2) Enc2Enc Aligning the features of the VLAs own visual encoder directly to the teacher models final embeddings. Our experiments reveal  (Table 5)  that Backbone2Enc consistently yields stronger results. This indicates that the primary representational degradation occurs not in the early encoder layers but in the middle-to-late fusion layers, where VL integration and task-specific adaptation are most active. Regularizing these deeper representations appears crucial for maintaining visualsemantic consistency while allowing the lower layers to adapt freely to domain-specific low-level cues."
        },
        {
            "title": "8.3 Projector type\nTo evaluate how different projection mappings affect representation\nalignment, we compare several projector variants that map the\nVLAâ€™s hidden states Rğ‘‘e to the teacherâ€™s embedding space Rğ‘‘ğ‘¡ . All\nprojectors share identical inputâ€“output dimensions but differ in\ntheir internal transformation ğ‘ƒğœ‘ : Rğ‘‘e â†’ Rğ‘‘ğ‘¡ .",
            "content": "Table 6: Comparison of different projection methods across generalization dimensions (mean across dimensions, p-value). Each projection type was evaluated in both frozen and trainable variants (for detailed results see Table 10 from Appendix). Projector Freeze Semantic Vision Execution MLP MLP Cosine OrthProj FILM Whitening Spectral 0.61 0.54 (p<0.01) 0.59 (p=0.08) 0.55 (p=0.01) 0.54 (p<0.01) 0.56 (p=0.01) 0.58 (p=0.17) 0.72 0.71 (p=0.48) 0.71 (p=0.13) 0.71 (p=0.37) 0.69 (p=0.11) 0.72 (p=0.61) 0.71 (p=0.26) 0.39 0.32 (p=0.06) 0.38 (p=0.45) 0.38 (p=0.45) 0.35 (p=0.18) 0.44 (p=0.97) 0.39 (p=0.65) We examine multiple projection strategies, including linear, cosinesimilaritybased, orthogonal, spectral-normalized, FiLM-conditioned, Whiteningaffine, and MLP-based mappings. Our experiments show that the frozen MLP projector yields the most reliable, robust alignment across all evaluation dimensions. We hypothesize that freezing the projector is critical in our setup: when trainable, the model minimizes alignment loss primarily through projector adaptation rather than meaningful changes in the VLAs internal representations. In this case, the projector quickly learns to output embeddings that merely approximate the teachers space, effectively bypassing representational correction. We attribute this to two factors: the relatively small amount of alignment data and the substantial dimensionality gap between the vision teacher and the VLA backbone embeddings (ğ‘‘ğ‘¡ = 768, ğ‘‘ğ‘’ = 4096). Freezing the projector constrains this shortcut, forcing the alignment objective to act directly on the students hidden representations, yielding more semantically grounded and transferable feature alignment."
        },
        {
            "title": "8.4 Alignment layers",
            "content": "Table 7: Comparison of different layers for alignment across generalization dimensions (mean across dimensions, p-value) (for detailed results see Table 12 from Appendix)."
        },
        {
            "title": "Middle\nEarly\nLate",
            "content": "0.61 0.51 (p<0.01) 0.54 (p=0.03) 0.72 0.66 (p=0.04) 0.69 (p=0.83) 0.39 0.38 (p=0.85) 0.36 (p=0.52) We further investigate which layers within the VLA transformers backbone should be aligned to achieve the most effective representation recovery. Prior literature on VLM interpretability [56] and our own analyses (Figure 5.1) suggest that middle layers are primarily responsible for VL fusion and semantic grounding, whereas early layers encode low-level features and later layers specialize in action prediction. Accordingly, we perform experiments aligning different types of layers: Early, Middle, Late. The results  (Table 7)  confirm that the middle layers play central role in semantic grounding and and aligning them yields the most substantial improvements across generalization axes."
        },
        {
            "title": "8.5 Loss functions and alignment coefficient",
            "content": "Table 8: Comparison of different loss functions across generalization dimensions (mean across dimensions, p-value)."
        },
        {
            "title": "Execution",
            "content": "Cosine L2 InfoNCE 0.61 0.54 (p<0.01) 0.57 (p=0.05) 0.72 0.63 (p<0.01) 0.64 (p=0.04) 0.39 0.34 (p=0.05) 0.36 (p=0.21) Finally, we assess the impact of the alignment loss and its weighting coefficient. We test several variants, including cosine similarity (Cossim), L2, and contrastive NT-Xent [9] losses, across alignment coefficients ğœ† = {0.2, 0.5, 1.0, 3.0}. The results demonstrate  (Table 8)  that Cossim loss achieves the most stable and consistent improvements, particularly when the auxiliary weight is set to ğœ† = 0.2. This setting effectively constrains representation drift without overpowering the task objective."
        },
        {
            "title": "9 CONCLUSION\nIn this work, we examined how fine-tuning VLA models on robotic\ntasks leads to degradation of VL understanding and representation\nquality. To analyze this effect, we introduced the VL-Think diag-\nnostic suite and interpretability probes, including attention map\nanalyses and linear probing, which reveal how VL skills degrade\nduring action fine-tuning. To address this issue, we proposed a\nlightweight Visual Alignment method that anchors the VLA to\nits pretrained visual teacher, consistently improving OOD gener-\nalization across diverse domains including novel objects, unseen\nscene compositions, texture and lighting variations, and instruction\nparaphrases. Due to compute constraints, our study focused on\nfine-tuning rather than full-scale pretraining. We hope this study\nguides future efforts toward scalable robotic pretraining and sys-\ntematic evaluation of how VLAs inherit and retain VL knowledge\nfrom VLMs.",
            "content": "REFERENCES [1] Md Rifat Arefin, Gopeshh Subbaraj, Nicolas Gontier, Yann LeCun, Irina Rish, Ravid Shwartz-Ziv, and Christopher Pal. 2024. Seq-VCR: Preventing collapse in intermediate transformer representations for enhanced reasoning. arXiv preprint arXiv:2411.02344 (2024). [2] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. 2023. OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models. arXiv preprint (2023). https://doi.org/ 10.48550/arXiv.2308.01390 [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, and et al. 2025. Qwen2.5-VL Technical Report. arXiv:2502.13923 [cs.CV] https://arxiv.org/abs/2502.13923 [4] Federico Barbero, Andrea Banino, Steven Kapturowski, Dharshan Kumaran, JoÃ£o Madeira AraÃºjo, Oleksandr Vitvitskyi, Razvan Pascanu, and Petar VeliÄkoviÄ‡. 2024. Transformers need glasses! information over-squashing in language tasks. Advances in Neural Information Processing Systems 37 (2024), 9811198142. [5] Lucas Beyer, Andreas Steiner, AndrÃ© Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, Thomas Unterthiner, Daniel Keysers, Skanda Koppula, Fangyu Liu, Adam Grycner, Alexey Gritsenko, Neil Houlsby, Manoj Kumar, Keran Rong, Julian Eisenschlos, Rishabh Kabra, Matthias Bauer, Matko BoÅ¡njak, Xi Chen, Matthias Minderer, Paul Voigtlaender, Ioana Bica, Ivana Balazevic, Joan Puigcerver, Pinelopi Papalampidi, Olivier Henaff, Xi Xiong, Radu Soricut, Jeremiah Harmsen, and Xiaohua Zhai. 2024. PaliGemma: versatile 3B VLM for transfer. arXiv:2407.07726 [cs.CV] https://arxiv.org/abs/2407.07726 [6] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, and Ury Zhilinsky. 2024. ğœ‹0: Vision-Language-Action Flow Model for General Robot Control. arXiv:2410.24164 [cs.LG] https://arxiv.org/abs/2410.24164 [7] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alexander Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, 9 Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. 2023. RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control. arXiv:2307.15818 [cs.RO] https://arxiv.org/abs/2307.15818 [8] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. 2023. RT-1: Robotics Transformer for Real-World Control at Scale. arXiv:2212.06817 [cs.RO] https://arxiv.org/abs/ 2212.06817 [9] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. Simple Framework for Contrastive Learning of Visual Representations. arXiv:2002.05709 [cs.LG] https://arxiv.org/abs/2002.05709 [10] William Chen, Suneel Belkhale, Suvir Mirchandani, Oier Mees, Danny Driess, Karl Pertsch, and Sergey Levine. 2025. Training Strategies for Efficient Embodied Reasoning. arXiv:2505.08243 [cs.RO] https://arxiv.org/abs/2505.08243 [11] Zoey Chen, Sho Kiami, Abhishek Gupta, and Vikash Kumar. 2023. GenAug: Retargeting behaviors to unseen situations via Generative Augmentation. arXiv:2302.06671 [cs.RO] https://arxiv.org/abs/2302. [12] Egor Cherepanov, Nikita Kachaev, Alexey K. Kovalev, and Aleksandr I. Panov. 2025. Memory, Benchmark & Robots: Benchmark for Solving Complex Tasks with Reinforcement Learning. arXiv:2502.10550 [cs.LG] https://arxiv.org/abs/ 2502.10550 [13] Egor Cherepanov, Alexey K. Kovalev, and Aleksandr I. Panov. 2025. ELMUR: External Layer Memory with Update/Rewrite for Long-Horizon RL. arXiv:2510.07151 [cs.LG] https://arxiv.org/abs/2510.07151 [14] Chenhao Ding, Xinyuan Gao, Songlin Dong, Yuhang He, Qiang Wang, Alex Kot, and Yihong Gong. 2024. LOBG: Less Overfitting for Better Generalization in Vision-Language Models. arXiv preprint (2024). arXiv:2410.10247 https: //arxiv.org/abs/2410.10247 [15] Danny Driess, Jost Tobias Springenberg, Brian Ichter, Lili Yu, Adrian Li-Bell, Karl Pertsch, Allen Z. Ren, Homer Walke, Quan Vuong, Lucy Xiaoyang Shi, and Sergey Levine. 2025. Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better. arXiv:2505.23705 [cs.LG] https://arxiv. org/abs/2505.23705 [16] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. 2023. PaLM-E: An Embodied Multimodal Language Model. arXiv:2303.03378 [cs.LG] https://arxiv.org/abs/2303.03378 [17] David Fan, Shengbang Tong, Jiachen Zhu, Koustuv Sinha, Zhuang Liu, Xinlei Chen, Michael Rabbat, Nicolas Ballas, Yann LeCun, Amir Bar, and Saining Xie. 2025. Scaling Language-Free Visual Representation Learning. arXiv:2504.01017 [cs.CV] https://arxiv.org/abs/2504. [18] Yanjiang Guo, Jianke Zhang, Xiaoyu Chen, Xiang Ji, Yen-Jen Wang, Yucheng Hu, and Jianyu Chen. 2025. Improving Vision-Language-Action Model with Online Reinforcement Learning. arXiv:2501.16664 [cs.RO] https://arxiv.org/abs/2501. 16664 [19] Greg Heinrich, Mike Ranzinger, Hongxu, Yin, Yao Lu, Jan Kautz, Andrew Tao, Bryan Catanzaro, and Pavlo Molchanov. 2025. RADIOv2.5: Improved Baselines for Agglomerative Vision Foundation Models. arXiv:2412.07679 [cs.CV] https: //arxiv.org/abs/2412.07679 [20] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. LoRA: Low-Rank Adaptation of Large Language Models. arXiv:2106.09685 [cs.CL] https://arxiv.org/abs/2106.09685 [21] Xiaohu Huang, Jingjing Wu, Qunyi Xie, and Kai Han. 2025. MLLMs Need 3D-Aware Representation Supervision for Scene Understanding. arXiv:2506.01946 [cs.CV] https://arxiv.org/abs/2506.01946 [23] [22] Minyoung Huh, Brian Cheung, Tongzhou Wang, and Phillip Isola. 2024. The Platonic Representation Hypothesis. arXiv:2405.07987 [cs.LG] https://arxiv.org/ abs/2405.07987 Jitesh Jain, Zhengyuan Yang, Humphrey Shi, Jianfeng Gao, and Jianwei Yang. 2025. Elevating Visual Perception in Multimodal LLMs with Visual Embedding Distillation. arXiv:2412.09585 [cs.CV] https://arxiv.org/abs/2412.09585 [24] Seil Kang, Jinyeong Kim, Junhyeok Kim, and Seong Jae Hwang. 2025. See What You Are Told: Visual Attention Sink in Large Multimodal Models. arXiv:2503.03321 [cs.CV] https://arxiv.org/abs/2503. [25] Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh. 2024. Prismatic VLMs: Investigating the Design Space Diverse Vision Foundation Models for Robot Learning. arXiv:2407.20179 [cs.RO] https://arxiv.org/abs/2407.20179 [42] Aleksei Staroverov, Andrey Gorodetsky, Andrei Krishtopik, Uliana Izmesteva, Dmitry Yudin, Alexey Kovalev, and Aleksandr Panov. 2023. Fine-tuning multimodal transformer models for generating actions in virtual and real environments. Ieee Access 11 (2023), 130548130559. [43] Stone Tao, Fanbo Xiang, Arth Shukla, Yuzhe Qin, Xander Hinrichsen, Xiaodi Yuan, Chen Bao, Xinsong Lin, Yulin Liu, Tse kai Chan, Yuan Gao, Xuanlin Li, Tongzhou Mu, Nan Xiao, Arnav Gurha, Viswesh Nagaswamy Rajesh, Yong Woo Choi, Yen-Ru Chen, Zhiao Huang, Roberto Calandra, Rui Chen, Shan Luo, and Hao Su. 2025. ManiSkill3: GPU Parallelized Robotics Simulation and Rendering for Generalizable Embodied AI. arXiv:2410.00425 [cs.RO] https://arxiv.org/abs/ 2410.00425 [44] Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, Jianlan Luo, You Liang Tan, Lawrence Yunliang Chen, Pannag Sanketi, Quan Vuong, Ted Xiao, Dorsa Sadigh, Chelsea Finn, and Sergey Levine. 2024. Octo: An OpenSource Generalist Robot Policy. arXiv:2405.12213 [cs.RO] https://arxiv.org/abs/ 2405.12213 [45] Yonglong Tian, Xinlei Chen, Surya Ganguli, and Phillip Isola. 2020. What makes for good views for contrastive learning?. In International Conference on Machine Learning (ICML). 1024210252. [46] Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing Data using t-SNE. Journal of Machine Learning Research 9 (2008), 25792605. https://www. jmlr.org/papers/v9/vandermaaten08a.html [47] Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing Data using t-SNE. Journal of Machine Learning Research 9, 86 (2008), 25792605. http: //jmlr.org/papers/v9/vandermaaten08a.html [48] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, and et al. 2025. InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency. arXiv:2508.18265 [cs.CV] https://arxiv. org/abs/2508.18265 [49] Yihao Wang, Pengxiang Ding, Lingxiao Li, Can Cui, Zirui Ge, Xinyang Tong, Wenxuan Song, Han Zhao, Wei Zhao, Pengxu Hou, Siteng Huang, Yifan Tang, Wenhui Wang, Ru Zhang, Jianyi Liu, and Donglin Wang. 2025. VLAAdapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model. arXiv:2509.09372 [cs.RO] https://arxiv.org/abs/2509.09372 [50] Frank Wilcoxon. 1945. Individual Comparisons by Ranking Methods. Biometrics Bulletin 1, 6 (1945), 8083. https://doi.org/10.2307/ [51] Haoyu Wu, Diankun Wu, Tianyu He, Junliang Guo, Yang Ye, Yueqi Duan, and Jiang Bian. 2025. Geometry Forcing: Marrying Video Diffusion and 3D Representation for Consistent World Modeling. arXiv:2507.07982 [cs.CV] https://arxiv.org/abs/2507.07982 Jianwei Yang, Reuben Tan, Qianhui Wu, Ruijie Zheng, Baolin Peng, Yongyuan Liang, Yu Gu, Mu Cai, Seonghyeon Ye, Joel Jang, Yuquan Deng, Lars Liden, and Jianfeng Gao. 2025. Magma: Foundation Model for Multimodal AI Agents. arXiv:2502.13130 [cs.CV] https://arxiv.org/abs/2502.13130 [52] [53] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. 2025. Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think. In The Thirteenth International Conference on Learning Representations. https://openreview.net/ forum?id=DJSZGGZYVi [54] Yuhang Zang, Hanlin Goh, Josh Susskind, and Chen Huang. 2024. Overcoming the Pitfalls of Vision-Language Model Finetuning for OOD Generalization. arXiv preprint (2024). arXiv:2401.15914 https://arxiv.org/abs/2401.15914 [55] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. 2023. arXiv:2303.15343 [cs.CV] Sigmoid Loss for Language Image Pre-Training. https://arxiv.org/abs/2303. [56] Wanyue Zhang, Yibin Huang, Yangbin Xu, JingJing Huang, Helu Zhi, Shuo Ren, Wang Xu, and Jiajun Zhang. 2025. Why do mllms struggle with spatial understanding? systematic analysis from data to architecture. arXiv preprint arXiv:2509.02359 (2025). of Visually-Conditioned Language Models. arXiv:2402.07865 [cs.CV] https: //arxiv.org/abs/2402.07865 [26] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. 2024. OpenVLA: An Open-Source VisionLanguage-Action Model. arXiv:2406.09246 [cs.RO] https://arxiv.org/abs/2406. 09246 Jason Lee, Jiafei Duan, Haoquan Fang, Yuquan Deng, Shuo Liu, Boyang Li, Bohan Fang, Jieyu Zhang, Yi Ru Wang, Sangho Lee, Winson Han, Wilbert Pumacay, Angelica Wu, Rose Hendrix, Karen Farley, Eli VanderBilt, Ali Farhadi, Dieter Fox, and Ranjay Krishna. 2025. MolmoAct: Action Reasoning Models that can Reason in Space. arXiv:2508.07917 [cs.RO] https://arxiv.org/abs/2508.07917 [27] [28] Xuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, Sergey Levine, Jiajun Wu, Chelsea Finn, Hao Su, Quan Vuong, and Ted Xiao. 2024. Evaluating RealWorld Robot Manipulation Policies in Simulation. arXiv:2405.05941 [cs.RO] https://arxiv.org/abs/2405.05941 [29] Fanqi Lin, Ruiqian Nai, Yingdong Hu, Jiacheng You, Junming Zhao, and Yang Gao. 2025. OneTwoVLA: Unified Vision-Language-Action Model with Adaptive Reasoning. arXiv:2505.11917 [cs.RO] https://arxiv.org/abs/2505. [30] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr DollÃ¡r, and Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In European conference on computer vision. Springer, 740755. [31] Zhihang Lin, Mingbao Lin, Luxi Lin, and Rongrong Ji. 2025. Boosting Multimodal Large Language Models with Visual Tokens Withdrawal for Rapid Inference. arXiv:2405.05803 [cs.CV] https://arxiv.org/abs/2405.05803 [32] Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. 2023. LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning. arXiv:2306.03310 [cs.AI] https://arxiv.org/abs/2306.03310 Jijia Liu, Feng Gao, Bingwen Wei, Xinlei Chen, Qingmin Liao, Yi Wu, Chao Yu, and Yu Wang. 2025. What Can RL Bring to VLA Generalization? An Empirical Study. arXiv:2505.19789 [cs.LG] https://arxiv.org/abs/2505.19789 [33] [34] Shiyin Lu, Yang Li, Yu Xia, Yuwei Hu, Shanshan Zhao, Yanqing Ma, Zhichao arXiv:2508.11737 [cs.CV] Wei, and et al. 2025. Ovis2.5 Technical Report. https://arxiv.org/abs/2508.11737 [35] Mayug Maniparambil, Raiymbek Akshulakov, Yasser Abdelaziz Dahou Djilali, Sanath Narayan, Mohamed El Amine Seddik, Karttikeya Mangalam, and Noel E. OConnor. 2024. Do Vision and Language Encoders Represent the World Similarly? arXiv:2401.05224 [cs.CV] https://arxiv.org/abs/2401. [36] Oier Mees, Lukas Hermann, Erick Rosete-Beas, and Wolfram Burgard. 2022. CALVIN: Benchmark for Language-Conditioned Policy Learning for LongHorizon Robot Manipulation Tasks. arXiv:2112.03227 [cs.RO] https://arxiv.org/ abs/2112.03227 [37] NVIDIA, :, Alisson Azzolini, Junjie Bai, Hannah Brandon, Jiaxin Cao, Prithvijit Chattopadhyay, Huayu Chen, Jinju Chu, Yin Cui, Jenna Diamond, Yifan Ding, Liang Feng, Francesco Ferroni, Rama Govindaraju, Jinwei Gu, Siddharth Gururani, Imad El Hanafi, Zekun Hao, Jacob Huffman, Jingyi Jin, Brendan Johnson, Rizwan Khan, George Kurian, Elena Lantz, Nayeon Lee, Zhaoshuo Li, Xuan Li, Maosheng Liao, Tsung-Yi Lin, Yen-Chen Lin, Ming-Yu Liu, Xiangyu Lu, Alice Luo, Andrew Mathau, Yun Ni, Lindsey Pavao, Wei Ping, David W. Romero, Misha Smelyanskiy, Shuran Song, Lyne Tchapmi, Andrew Z. Wang, Boxin Wang, Haoxiang Wang, Fangyin Wei, Jiashu Xu, Yao Xu, Dinghao Yang, Xiaodong Yang, Zhuolin Yang, Jingxu Zhang, Xiaohui Zeng, and Zhe Zhang. 2025. Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning. arXiv:2503.15558 [cs.AI] https://arxiv.org/abs/2503.15558 [38] NVIDIA, :, Johan Bjorck, Fernando CastaÃ±eda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi \"Jim\" Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, Joel Jang, Zhenyu Jiang, Jan Kautz, Kaushil Kundalia, Lawrence Lao, Zhiqi Li, Zongyu Lin, Kevin Lin, Guilin Liu, Edith Llontop, Loic Magne, Ajay Mandlekar, Avnish Narayan, Soroush Nasiriany, Scott Reed, You Liang Tan, Guanzhi Wang, Zu Wang, Jing Wang, Qi Wang, Jiannan Xiang, Yuqi Xie, Yinzhen Xu, Zhenjia Xu, Seonghyeon Ye, Zhiding Yu, Ao Zhang, Hao Zhang, Yizhou Zhao, Ruijie Zheng, and Yuke Zhu. 2025. GR00T N1: An Open Foundation Model for Generalist Humanoid Robots. arXiv:2503.14734 [cs.RO] https://arxiv.org/abs/2503.14734 [39] Maxime Oquab, TimothÃ©e Darcet, ThÃ©o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, HervÃ© Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. 2024. DINOv2: Learning Robust Visual Features without Supervision. arXiv:2304.07193 [cs.CV] https://arxiv.org/abs/2304.07193 [40] Daria Pugacheva, Andrey Moskalenko, Denis Shepelev, Andrey Kuznetsov, Vlad Shakhuro, and Elena Tutubalina. 2025. Bring the Apple, Not the Sofa: Impact of Irrelevant Context in Embodied AI Commands on VLA Models. arXiv:2510.07067 [cs.RO] https://arxiv.org/abs/2510.07067 Jinghuan Shang, Karl Schmeckpeper, Brandon B. May, Maria Vittoria Minniti, Tarik Kelestemur, David Watkins, and Laura Herlant. 2024. Theia: Distilling [41] 10 APPENDIX This appendix provides additional technical details, extended results, and supplementary materials that support and complement the main findings presented in the paper. We include comprehensive ablations on alignment strategies, projector architectures, teacher models, and layer selection and alignment coefficient. These materials aim to enhance the transparency, reproducibility, and interpretability of our proposed method. A.1 Training Hyperparameters Table 9 presents the training parameters used during the visual alignment fine-tuning. All other training parameters remained unchanged across both experiments and model variants. Table 9: Best training configuration for OpenVLA fine-tuning with visual alignment. All other methods were trained with identical hyperparameters except for the alignment-specific settings."
        },
        {
            "title": "Parameter",
            "content": "Fine-tuning steps Batch size Gradient accumulation steps Learning rate LoRA rank Alignment coefficient Alignment projector Alignment method Aligned layers Mode Projector dimension Freeze alignment projector"
        },
        {
            "title": "Value",
            "content": "60000 8 1 5 104 32 0.2 MLP Ln&D Backbone2Enc 16 alig 2048 A.2 Attention maps visualization To further validate the qualitative effect of our alignment objective, we visualize attention maps for Qwen2.5-VL, OpenVLA SFT, and OpenVLA Align (ours) across middle layers of the internal transformer backbone. These layers correspond to the region of strongest visionlanguage fusion, where attention patterns most directly reflect the models visual grounding quality. As shown in Figure 6, the default OpenVLA SFT exhibits diffuse and spatially inconsistent attention, often extending beyond the queried object. In contrast, our OpenVLA Align model restores sharp, localized focus on task-relevant regions. This confirms that the proposed visual alignment effectively mitigates attention sink introduced by naive fine-tuning and preserves coherent objectcentered attention. Figure 6: Attention maps across middle layers Qwen2.5-VL, OpenVLA SFT, and OpenVLA Align. The proposed alignment method restores sharp, object-centered attention patterns, improving visual grounding degraded by standard fine-tuning. Question: \"Do you see hamburgerbaguette?\". = 30, max iterations = 1000, fixed random seed = 42 (all other parameters at library defaults). These plots are illustrative only (t-SNE distorts global geometry); quantitative conclusions come from linear probing in subsection 7.5. A.4 Linear probing Table 14: Linear probing configuration on ImageNet-100 with frozen backbone and mean-pooled token features."
        },
        {
            "title": "Value",
            "content": "Feature pooling Optimizer Weight decay Learning rate Epochs Batch size (train/val/test) Mean over visual embs. SGD (momentum 0.9) 0.0 0.1 40 128 A.3 t-SNE visualization In subsection 5.2 we show t-SNE [46] of internal representations for the VLM models and OpenVLA. To keep comparisons strict, we use an out-of-the-box t-SNE implementation with no tuning: perplexity For reproducibility and fair comparison, we evaluate representational quality with frozen-feature linear probe under single, fixed configuration (see Table 14). Concretely, we extract patch embeddings (mean-pooled to single vector) from the final C-RADIOv3 11 Table 10: Comparison on projection methods, table shows performance across environments (mean SD). Method Freeze Semantic Vision Execution Carrot Instruct MultiCarrot MultiPlate Plate VisionImg Tex03 Tex05 Whole03 Whole05 Position EEPose PosChangeTo MLP MLP Cosine Cosine FILM FILM OrthProj OrthProj RFF RFF Whitening Whitening Spectral Default - 0.490.05 0.790.02 0.610.01 0.830.03 0.490.04 0.820.03 0.530.02 0.790.01 0.530.04 0.840.03 0.520.02 0.770.02 0.540.01 0.780.03 0.490.04 0.770.02 0.460.02 0.740.03 0.560.03 0.800.02 0.460.03 0.740.04 0.520.03 0.790.02 0.530.02 0.810.03 0.490.02 0.740.02 0.270.01 0.350.02 0.300.06 0.340.04 0.350.04 0.340.03 0.310.03 0.330.02 0.310.03 0.300.04 0.210.02 0.320.02 0.340.07 0.280.02 0.720.01 0.850.01 0.730.02 0.580.03 0.770.03 0.660.02 0.530.05 0.310.01 0.460.02 0.490.02 0.750.01 0.860.02 0.700.02 0.670.02 0.800.02 0.600.02 0.580.02 0.380.02 0.450.05 0.730.03 0.750.03 0.710.04 0.540.05 0.770.04 0.600.04 0.510.01 0.320.05 0.550.02 0.690.05 0.800.01 0.730.03 0.600.03 0.750.03 0.610.03 0.550.09 0.350.03 0.680.01 0.830.03 0.720.04 0.610.02 0.750.06 0.600.07 0.580.09 0.310.03 0.440.03 0.670.03 0.790.03 0.750.03 0.530.04 0.760.01 0.640.04 0.570.02 0.270.03 0.430.05 0.710.04 0.840.02 0.730.01 0.580.01 0.720.05 0.600.03 0.570.04 0.290.02 0.440.06 0.700.04 0.860.03 0.770.01 0.580.05 0.740.06 0.630.03 0.550.08 0.380.03 0.480.08 0.700.02 0.840.04 0.720.03 0.610.03 0.750.05 0.560.04 0.560.02 0.320.04 0.430.04 0.700.02 0.780.02 0.720.02 0.560.02 0.720.03 0.620.02 0.570.01 0.270.03 0.480.02 0.640.02 0.800.05 0.720.03 0.570.04 0.710.02 0.570.04 0.410.03 0.180.01 0.330.02 0.720.02 0.810.03 0.750.05 0.640.04 0.770.01 0.680.01 0.630.03 0.390.02 0.480.02 0.740.05 0.910.03 0.730.03 0.540.06 0.750.03 0.610.03 0.640.04 0.320.03 0.510.04 0.730.02 0.810.01 0.670.01 0.550.03 0.710.02 0.560.01 0.430.02 0.340.01 0.430.02 0.210.04 0.200.03 0.290.07 0.250.04 0.260.03 0.240.02 0.200.02 0.230.03 0.200.03 0.270.04 0.120.01 0.320.05 0.230.04 0.230.01 Table 11: Comparison on vision teacher models, table shows performance across environments (mean SD). Teacher Semantic Vision Execution Carrot Instruct MultiCarrot MultiPlate Plate VisionImg Tex03 Tex05 Whole03 Whole05 Position EEPose PosChangeTo C-RADIOv3-ViT-L 0.610.01 0.830.03 0.490.02 0.740.04 DINOv2-ViT-L 0.550.04 0.840.01 DINOv2-ViT-G 0.520.03 0.760.04 Theia 0.360.03 0.650.01 SigLIP 0.490.02 0.740.02 Default 0.350.02 0.310.02 0.320.05 0.290.03 0.150.02 0.280.02 0.490.02 0.750.01 0.860.02 0.700.02 0.670.02 0.800.02 0.600.02 0.580.02 0.380.02 0.730.03 0.800.01 0.720.04 0.570.04 0.720.01 0.650.02 0.550.03 0.330.02 0.460.03 0.740.01 0.830.02 0.720.01 0.600.03 0.720.02 0.580.02 0.580.04 0.340.02 0.420.03 0.700.01 0.740.04 0.700.03 0.530.05 0.720.08 0.600.02 0.530.02 0.410.03 0.390.01 0.570.06 0.690.05 0.580.05 0.470.02 0.640.05 0.480.04 0.520.04 0.320.02 0.260.06 0.730.02 0.810.01 0.670.01 0.550.03 0.710.02 0.560.01 0.430.02 0.340.01 0.430.02 0.200.03 0.210.03 0.200.04 0.210.04 0.180.04 0.230.01 Table 12: Comparison on different aligning layers, table shows performance across environments (mean SD). Teacher Layer (L) Semantic Vision Execution Carrot Instruct MultiCarrot MultiPlate Plate VisionImg Tex03 Tex05 Whole03 Whole05 Position EEPose PosChangeTo C-RADIOv3-ViT-L C-RADIOv3-ViT-L C-RADIOv3-ViT-L C-RADIOv3-ViT-L C-RADIOv3-ViT-L C-RADIOv3-ViT-L C-RADIOv3-ViT-L C-RADIOv3-ViT-L C-RADIOv3-ViT-L 8 16 20 22 26 30 8-12 12-16 160.490.02 0.790.02 0.610.01 0.830.03 0.540.02 0.810.01 0.540.01 0.770.01 0.540.01 0.790.01 0.540.01 0.790.01 0.430.03 0.800.03 0.490.02 0.740.03 0.480.03 0.820.03 0.270.04 0.350.02 0.310.02 0.320.02 0.310.02 0.310.02 0.290.05 0.290.03 0.280.03 0.760.04 0.840.03 0.720.03 0.600.02 0.760.04 0.590.07 0.590.08 0.390.03 0.450.01 0.750.01 0.860.02 0.700.02 0.670.02 0.800.02 0.600.02 0.580.02 0.380.02 0.490.02 0.510.02 0.720.04 0.890.02 0.700.03 0.630.01 0.790.01 0.660.03 0.630.02 0.360.02 0.520.04 0.770.02 0.790.01 0.740.01 0.610.01 0.720.02 0.600.01 0.590.02 0.320.03 0.770.03 0.870.01 0.760.04 0.640.01 0.800.02 0.610.03 0.600.03 0.320.01 0.460.03 0.770.04 0.870.01 0.760.04 0.640.01 0.790.01 0.610.02 0.600.03 0.320.01 0.460.02 0.710.02 0.830.03 0.690.01 0.540.01 0.770.03 0.580.03 0.530.02 0.320.03 0.400.01 0.730.02 0.820.01 0.730.04 0.650.03 0.730.01 0.620.02 0.520.06 0.320.02 0.430.01 0.730.01 0.820.05 0.730.03 0.550.04 0.710.02 0.570.01 0.600.03 0.310.03 0.460.04 0.230.06 0.200.03 0.230.01 0.190.01 0.250.02 0.250.02 0.210.02 0.110.02 0.190.02 Table 13: Comparison on different aligment coefficients, table shows performance across environments (mean SD). Teacher Coeff. (C) Semantic Vision Execution Carrot Instruct MultiCarrot MultiPlate Plate VisionImg Tex03 Tex05 Whole03 Whole05 Position EEPose PosChangeTo C-RADIOv3-ViT-L C-RADIOv3-ViT-L C-RADIOv3-ViT-L C-RADIOv3-ViT-L C-RADIOv3-ViT-L 0.2 0.5 1.0 2.0 3.0 0.610.01 0.830.03 0.540.02 0.760.01 0.510.04 0.770.02 0.580.03 0.780.01 0.520.10 0.780.03 0.350.02 0.320.01 0.310.05 0.320.04 0.240.03 0.490.02 0.750.01 0.860.02 0.700.02 0.670.02 0.800.02 0.600.02 0.580.02 0.380.02 0.450.03 0.720.02 0.800.03 0.720.02 0.540.02 0.740.03 0.580.01 0.550.03 0.350.02 0.470.02 0.790.03 0.800.02 0.700.03 0.550.02 0.730.01 0.610.03 0.580.06 0.320.07 0.790.04 0.860.03 0.690.05 0.580.07 0.770.04 0.610.06 0.560.01 0.330.03 0.460.08 0.730.03 0.800.01 0.790.02 0.660.05 0.770.04 0.570.03 0.550.02 0.370.03 0.360. 0.200.03 0.180.03 0.140.06 0.160.06 0.150.01 teacher and from intermediate visual layers of each OpenVLA variant. single linear classifier is trained on top of these frozen features. All hyperparameters are held constant across models and layers, and the same random seed and data split are used for every run. Due to computational constraints, we operate on reduced ImageNet-100. We report top-1 accuracy on the evaluation split, 12 A.6 Different projection approaches Below, we provide detailed formulations of the various projectors used in subsection 8.3 of our experiments. Cosine Projector. normalized linear projection that preserves angular similarity: ğ‘§ = ğ‘Š â„ ğ‘Š â„2 , ğ‘Š Rğ‘‘ğ‘§ ğ‘‘hidden . (11) Orthogonal Projector. fixed linear transform with orthonormal columns: ğ‘Š ğ‘Š = ğ¼ğ‘‘ğ‘§ , ğ‘§ = ğ‘Š â„. (12) Random Fourier Feature (RFF) Projector. fixed randomized mapping that implicitly approximates kernel feature space: ğ‘§ = 2 ğ· cos(ğ‘Š â„ + ğ‘), ğ‘Šğ‘– ğ‘— (0, ğ›¾ 2), ğ‘ğ‘– (0, 2ğœ‹). WhiteningAffine Projector. Combines feature whitening and (13) affine normalization: ğ‘§ = Î›1/2 (â„ ğœ‡) + ğ‘, (14) SpectralNorm Projector. constrained linear mapping enforcing bounded operator norm: ğ‘§ = ğ‘Š â„, ğ‘Š 2 1. (15) without per-any tuning, so any differences reflect only the underlying representations rather than linear probe tuning changes. A.5 Ablations This section provides the complete ablation results that underlie the analyses in section 8, Vision teachers  (Table 11)  , Alignment projectors  (Table 10)  , Alignment layers  (Table 9)  , Alignment coefficients  (Table 13)  . In the ablation studies presented in section 8, we test the hypothesis that given model variant (denoted B) outperforms the baseline variant (A) in terms of success rate. For each pairwise comparison, we fix all other parameters altering only the component under investigation (e.g., alignment objective, layer depth, projection type). This ensures that any observed performance difference can be attributed solely to the ablated design choice. To assess statistical significance, we use the paired Wilcoxon signed-rank test [50], non-parametric test suited for comparing two matched samples that do not necessarily follow normal distribution. The unit of analysis is the per-seed success rate over matched trials, evaluated independently for each environment type (Semantic, Vision, Execution). The test uses one-sided alternative hypothesis (ğ»1: > A), corresponding to our directional research question and report the exact p-value. All comparisons are conducted over 128 shared random seeds, ensuring that each seedenvironment pair is identical across the methods being compared. This careful experimental control allows us to draw meaningful conclusions about the contribution of each individual design choice."
        }
    ],
    "affiliations": [
        "Cognitive AI Lab Moscow, Russia",
        "Cognitive AI Lab, IAI MIPT Moscow, Russia",
        "IAI MIPT Moscow, Russia"
    ]
}