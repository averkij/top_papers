{
    "paper_title": "Leopard: A Vision Language Model For Text-Rich Multi-Image Tasks",
    "authors": [
        "Mengzhao Jia",
        "Wenhao Yu",
        "Kaixin Ma",
        "Tianqing Fang",
        "Zhihan Zhang",
        "Siru Ouyang",
        "Hongming Zhang",
        "Meng Jiang",
        "Dong Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text-rich images, where text serves as the central visual element guiding the overall understanding, are prevalent in real-world applications, such as presentation slides, scanned documents, and webpage snapshots. Tasks involving multiple text-rich images are especially challenging, as they require not only understanding the content of individual images but reasoning about inter-relationships and logical flows across multiple visual inputs. Despite the importance of these scenarios, current multimodal large language models (MLLMs) struggle to handle such tasks due to two key challenges: (1) the scarcity of high-quality instruction tuning datasets for text-rich multi-image scenarios, and (2) the difficulty in balancing image resolution with visual feature sequence length. To address these challenges, we propose Leopard, a MLLM designed specifically for handling vision-language tasks involving multiple text-rich images. First, we curated about one million high-quality multimodal instruction-tuning data, tailored to text-rich, multi-image scenarios. Second, we developed an adaptive high-resolution multi-image encoding module to dynamically optimize the allocation of visual sequence length based on the original aspect ratios and resolutions of the input images. Experiments across a wide range of benchmarks demonstrate our model's superior capabilities in text-rich, multi-image evaluations and competitive performance in general domain evaluations."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 ] . [ 2 4 4 7 1 0 . 0 1 4 2 : r a"
        },
        {
            "title": "LEOPARD",
            "content": ": Vision Language Model for Text-Rich Multi-Image Tasks Mengzhao Jia1* Wenhao Yu2 Kaixin Ma2 Tianqing Fang2 Zhihan Zhang1* Siru Ouyang3* Hongming Zhang2 Meng Jiang1 Dong Yu2 1University of Notre Dame 2Tencent AI Seattle Lab 3UIUC 1mjia2@nd.edu; 2{wenhaowyu, kaixinma}@global.tencent.com * Interns at Tencent AI Seattle Lab, Core Contributors"
        },
        {
            "title": "Abstract",
            "content": "Text-rich images, where text serves as the central visual element guiding the overall understanding, are prevalent in real-world applications, such as presentation slides, scanned documents, and webpage snapshots. Tasks involving multiple text-rich images are especially challenging, as they require not only understanding the content of individual images but reasoning about inter-relationships and logical flows across multiple visual inputs. Despite the importance of these scenarios, current multimodal large language models (MLLMs) struggle to handle such tasks due to two key challenges: (1) the scarcity of high-quality instruction tuning datasets for text-rich multi-image scenarios, and (2) the difficulty in balancing image resolution with visual feature sequence length. To address these challenges, we propose LEOPARD, MLLM designed specifically for handling vision-language tasks involving multiple text-rich images. First, we curated about one million high-quality multimodal instruction-tuning data, tailored to text-rich, multi-image scenarios. Second, we developed an adaptive high-resolution multi-image encoding module to dynamically optimize the allocation of visual sequence length based on the original aspect ratios and resolutions of the input images. Experiments across wide range of benchmarks demonstrate our models superior capabilities in text-rich, multi-image evaluations and competitive performance in general domain evaluations. Our code is available at https://github.com/Jill0001/Leopard. 1. Introduction Multimodal large language models (MLLMs) have revolutionized vision-language tasks, driving advancements in variety of areas such as image captioning and object detection [67, 72, 74]. These improvements extend to applications involving text-rich images where text serves as the primary visual element guiding image comprehension, such as visual document understanding [51] and scene text recognition [59]. Traditional OCR-based pipelines in these text-rich visual scenarios are being replaced by end-to-end approaches that directly encode intertwined multimodal inputs [63, 69, 75], leading to improved efficiency and accuracy in handling text-rich images. Despite these advancements, the majority of existing open-source MLLMs, like LLaVAR [75] and mPlugDocOwl-1.5 [16], have primarily focused on optimizing performance for text-rich single-image tasks. This focus inherently limits their applicability in many real-world scenarios, where tasks often involve multiple inter-connected images. For instance, multi-page visual document understanding requires integrating information spread across different pages to capture the logical flow across the whole document [25, 64]. To understand presentation slides, grasping the overarching narrative requires understanding multiple slides with unique but interrelated content [62]. These vision-language tasks on multiple text-rich images require advanced capabilities that go beyond merely recognizing text and visuals within single image; they involve understanding and reasoning about relationships and logical flows across multiple visual inputs. While some models such as OpenFlamingo [3], VILA [38], Idefics2 [28] have made strides toward supporting multi-image inputs, they mainly focus on scenarios with natural images but fall short in understanding sequences of text-rich images with interrelated textual and visual information. We plot the performance of representatives of the aforementioned models in Figure 1. Upon examining their training data and model architecture, we identified two primary limitations. First, there is scarcity of high-quality instruction tuning datasets on text-rich multi-image scenarios. Existing visual instruction tuning datasets for text-rich images are predominantly based on single-image inputs [22, 48, 59, 63], which limits the model ability to generalize and reason across multiple images. Second, in text-rich multi-image scenarios, 1 Figure 1. Left: demonstration of text-rich multi-image task. Models need to reason about the textual content across multiple images to answer the question correctly. LEOPARD successfully generates the right answer while baselines fail. Right: Evaluation results of LEOPARD and three baselines. Our model surpasses its counterparts across text-rich multi-image benchmarks by large margin, maintaining comparable performance on single and general evaluations. there is challenge of balancing image resolution and sequence length limitations. Many general-domain MLLMs adopt the low-resolution settings of pre-trained visual encoders [21, 38]. However, for text-rich images, such as scientific reports, recognizing text content becomes difficult at low resolutions. While some approaches overcome this in singleimage settings by splitting the original image to preserve high-resolution details [16, 40], this approach is less effective when applied to multiple images, as it quickly exceeds models maximum sequence length. Moreover, compressing such long-sequence representations into shorter ones leads to significant information loss, thereby degrading model performance [3, 27]. Thus, critical balance must be struck between maintaining sufficient visual detail and keeping sequence lengths manageable. In this paper, we introduce novel multimodal large language model, named LEOPARD1. LEOPARD is specifically designed to handle complex text-rich, multi-image tasks. To train LEOPARD, we first curated about one million highquality multimodal instruction-tuning data, tailored to the text-rich, multi-image scenarios. This dataset spans three key domains that are commonly encountered in real-world scenarios: (1) multi-page documents, (2) multi-charts and multi-tables, (3) webpage trajectories. These scenarios capture the increasing complexity and multimodal nature of modern digital information. In addition, to enable highresolution encoding in multi-image inputs, we equipped 1Leopards have remarkable visual adaptations that allow them to track prey both from afar and up close, making them highly efficient hunters. LEOPARD with an adaptive high-resolution multi-image encoding module. Specifically, it dynamically optimizes the allocation of visual sequence length based on the original aspect ratios and resolutions of the input images. We then apply pixel shuffling to losslessly compress [8] long visual feature sequences into shorter ones. This approach allows the model to accommodate multiple high-resolution images without compromising detail or clarity. We conducted experiments on 13 vision-language benchmark datasets, evaluating LEOPARD from multiple perspectives. Consistent improvements were observed when training LEOPARD with two distinct base model architectures: LLaVA and Idefics2. Our results demonstrate LEOPARDs superior performance on 5 text-rich, multi-image benchmarks, outperforming the best open-source MLLM by an average of +9.61 points. Moreover, LEOPARD remains highly competitive in text-rich single-image tasks and general-domain vision-language benchmarks, achieving comparable results to state-of-the-art MLLMs without extensive fine-tuning. Further ablation studies confirm the effectiveness of our instruction-tuning dataset and the adaptive high-resolution encoding module. These findings highlight LEOPARDs strong performance across various multimodal applications. 2. Related Work Multimodal Large Language Models (MLLMs). Many approaches have been proposed for building MLLMs, leveraging different architectural designs. widely adopted approach is the decoder-only architecture, exemplified by LLaVA [41], Emu2 [61], and Intern-VL [9]. These models typically incorporated visual encoder to encode images, vision-language connector to project visual features into the language feature space, and language model that processes both visual and textual information jointly. Another line of work employed cross-attention architectures where encoded image features are integrated with textual tokens via crossattention layers, as seen in Flamingo [1], OpenFlamingo [3] and CogVLM [66]. Such design allows models to retain the benefits of fully intact language model but introduces new parameters to manage the visual-textual interplay. Text-rich MLLMs. Text-rich images are traditionally processed in pipelines [18, 60], where an OCR module first recognized text from the image, followed by processing through language model. To improve efficiency and avoid error propagation, with the advent of MLLMs, end-to-end approaches become more popular recently. For instance, LLaVAR [75] utilized dataset of 400K instances with OCR-enhanced text to outperform LLaVA on various textrich VQA tasks. Subsequent models such as UReader [70], TextMonkey [44], and Mplug-DocOwl-1.5 [16] recognized the importance of high-resolution encoding for accurate text comprehension, so they adopted strategies that cropped single images into multiple sub-images to preserve the original resolution during visual encoding. However, these approaches are primarily trained on single-image data, and struggle to generalize effectively to multi-image scenarios. Furthermore, the straightforward partitioning technique encounters challenges with multi-image inputs, as the sequence length rapidly increases with the number of images. Multi-image MLLMs. Efforts have been made in training MLLMs with multi-image inputs due to the prevalence of multi-image scenarios in real-world applications. Mantis [21] introduced multi-image instruction tuning dataset on variety of natural image scenarios. Besides, both VILA [38] and Idefics-2 [28] incorporated image-text interleaved data during their pre-training. LLaVA-Next-Interleave [33] further extended this by incorporating videos and multi-view 3D data into the training pipeline. However, these works primarily target natural images and general visual understanding, leaving gap in handling text-rich, multi-image scenarios. Natural images typically follow different distribution from text-rich images and often do not demand high-resolution processing. As result, many existing multi-image MLLMs struggle to generalize to text-rich scenarios. Our work aims to address this gap by specifically focusing on multi-image settings where text-rich images are the primary input. Very recently (in 08/2024 and 09/2024), multi-image training for MLLMs has attracted intense attention from researchers. Several concurrent efforts have included multi-image interleaved data to train their models, such as LLaVA-OneVision 08/2024 [31], Idefics3 (08/2024, 26), NVLM (09/2024, 10), mPlug-DocOwl-2 (09/2024, 17), Molmo (09/2024, 11) and Qwen2-VL (09/2024, 65). This trending paradigm highlights the significant practical value of multi-image MLLMs by enhancing their ability to tackle wide range of real-world applications. The incorporation of multi-image instruction tuning data is therefore of paramount importance. 3. Method LEOPARD follows the typical design of decoder-only vision language models [33, 40, 41], including visual encoder, vision language connector, and language model (LM), as shown in Figure 2 (④⑤). Specially, the input images are first passed through the visual encoder, which extracts high-level visual features and captures essential semantic information. These visual features are then projected into the language representation space via the vision-language connector. After this transformation, the visual tokens are interleaved with the textual tokens, resulting in sequence of interleaved text-visual tokens. This interleaved sequence is then fed into the LM, which processes these inputs in causal manner, leveraging the contextual dependencies between text and visual information to generate coherent outputs that align with both modalities. 3.1. Multi-image text-rich Instruction Turning"
        },
        {
            "title": "Dataset",
            "content": "To train LEOPARD, we construct large instruction-tuning dataset named LEOPARD-INSTRUCT, comprising 925K instances, with 739K specifically designed for text-rich, multiimage scenarios. While we extensively surveyed existing open-source datasets, we only identified 154K usable textrich, multi-image samples, which is far from sufficient for effective instruction tuning, as shown in prior MLLM studies [21, 28, 33]. To address this data scarcity, we developed several data collection pipelines to collect high-quality text-rich, multi-image data, resulting in additional 585K instances. Each instance consists of set of images along with corresponding task instructions and responses. The dataset details are presented in Table 1, and detailed breakdown of its composition can be found in Appendix A.1. Documents and Slides are common sources of multi-image data that primarily contain text and require cross-page context integration to fully understand the information. These data is collected in three ways. First, we include 69K public multi-page document and slide datasets [25, 62, 64, 79], covering variety of document types such as scanned handwriting, printed documents, and digital PDFs. Second, we adapt two single-page document datasets, DocVQA [51] and ArxivQA [34], for multi-image settings. Following [21], we randomly merge 2 to 4 single-page instances by concatenating their respective images and Q-A pairs. Prompts like in the second image are added to direct the models focus to the appropriate image. These merged 3 Figure 2. The overall model pipeline. Given ① raw image inputs, ② we first compute the optimal allocation of sub-image numbers and splitting strategy for all images based on their resolution and aspect ratio. ③ The images undergo padding, resizing, and splitting operations. ④ Both sub-images and resized original images are then encoded into sequence of visual features. These sequences subsequently undergo pixel shuffle operation that concatenates every four features. ⑤ The visual features are projected into the language embedding space via vision-language connector. Finally, the large language model then integrates these visual and language embeddings to generate responses. samples help the model learn how natural language references align with corresponding image features. Third, we collect raw slides from [57] and SlideShare2, and use GPT4o to generate Q-A pairs and reasoning steps. We show the prompt to GPT in Figure 5. Upon manually reviewing 100 instances annotated by GPT-4o, we found an accuracy rate over 90%, indicating high annotation quality. Tables and Charts provide highly organized, structured quantitative information, often involving complex data patterns and relationships, requiring the integration of both visual and textual elements for accurate interpretation. To address the lack of instruction tuning data involving multiple tables or charts, we use the following strategies. First, we include 21K open-source multi-chart and multi-table datasets [54, 77], originally stored in JSON or DataFrame formats. We programmatically render these tables as images, converting them into multimodal data. Details of rendering can be found in Appendix A.3 Second, We utilize the TableGPT [35] dataset and split each table into multiple sub-tables, then convert them into figures, thereby creating multi-modal, multi-table instruction data. Third, we apply the same merging strategy used for combining single-page documents to synthesize multi-image datasets. This approach integrates several single-chart datasets, including ChartGemma [49], ChartQA [48], DVQA [22], and FigureQA [23]. Besides, we generate new multi-chart data from social reports of the Pew Research Center3 that feature multiple interrelated charts within the articles under the same topic. We download charts from the website and use GPT-4o to create 20K Q-A pairs that require multi-chart understanding. Webpage Snapshots consist of sequential images representing web pages, providing visual context for user interactions and task flows. Understanding webpage is critical skill for MLLMs to evolve into fully autonomous web agents [12, 14]. To collect relevant data, we format several web-related mul2https://www.slideshare.net 3https://www.pewresearch.org"
        },
        {
            "title": "Data Types",
            "content": "# Instances"
        },
        {
            "title": "Proportion",
            "content": "Total Samples Single-image Multi-image *Public *New (Ours) Rationales *Existing *New (Ours) *None"
        },
        {
            "title": "Domains\nDocuments\nSlide Decks\nTables\nCharts\nWebpages\nOthers",
            "content": "925K 186K 739K 154K 585K 214K 250K 461K 192K 16K 48K 353K 55K 261K 20.10% 79.89% 16.65% 63.24% 23.14% 27.02% 49.84% 20.76% 1.73% 5.19% 38.16% 5.95% 28.22% Table 1. Data statistics of the LEOPARD-INSTRUCT dataset. timodal datasets into Q-A structure as follows: 1. Web action prediction data: We include Mind2Web [12] and OmniACT [24], where we divide long web snapshots into multiple sub-figures, and plot bounding boxes based on the coordinates of web elements. Then GPT-4o is used to convert the original action data into Q-A format, where the task is to identify the correct element to interact with. 2. Web-based classification data: We incorporate WebScreenshots [4], WebVision [36], and WebUI [68]. We utilize the web snapshots in these datasets and employ GPT-4o to generate Q-A pairs on webpage understanding, including chain-of-thought reasoning steps. The prompting details are provided in Figure 6. Augmenting with Rationales. In contrast to single-image tasks, multi-image scenarios typically require MLLMs to integrate information across multiple images, making crossimage reasoning difficult to train when only the final answer is provided [19, 78]. To address this, we employ GPT-4o to generate chain-of-thought (CoT) rationales for inherently multi-image datasets (excluding those formed by merging single-image data) that lack CoT annotations. This results in 250K instances with GPT-annotated reasoning, with the prompt detailed in Figure 7. Other Domains. We also include datasets from various other domains such as maps (MapQA, 6), infographics (InfographicVQA, 50), mathematical diagrams (MathV360K, 58), and abstractive diagrams (IconQA, 47). We also incorporate mixed-domain datasets for text-rich images, including LLaVAR [75], Monkey, 37, and mPlugDocReason [16]. We remove duplicate subsets from these mixed-domain datasets. Among these datasets, 64K samples consist of multi-image data, while the remaining are single-image samples. To preserve natural image understanding ability, we add 313K samples from ShareGPT4V [7], an instruction dataset for natural images. 3.2. Adaptive High-resolution Multi-image"
        },
        {
            "title": "Encoding",
            "content": "Image resolution significantly influences the visual perception and understanding capabilities of MLLMs, particularly when processing text-rich images. Low-resolution images often cause printed text to become blurred or unreadable, resulting in misinterpretations, perception errors, and visual hallucinations. The visual resolution of most existing MLLMs is determined by their pre-trained visual encoders, which are typically limited to low resolutions such as 224 224 or 336 336 pixels [21, 38, 39]. These low-resolution constraints can hinder MLLMs to accurately understand textual information embedded within images. To overcome these limitations, natural solution is dividing high-resolution image into multiple smaller subimages, each of which is independently processed by the models visual encoder [13, 40]. This partitioning allows for the extraction of more fine-grained visual details, making it possible to capture small or densely packed textual elements. However, major drawback of this approach is that it significantly increases the length of visual feature sequence. When applied to scenarios involving multiple image inputs, the feature sequences are easily exceeding the models maximum sequence length limit. To address the issue, we follow the image-splitting idea and propose novel adaptive high-resolution multi-image encoding strategy as follows. Image Allocation Computing: To prevent the number of sub-image visual features from exceeding the LLMs maximum sequence length, we first set budget 4 for the total number of sub-images. We allocate this budget proportionally to each input image based on their original sizes. For each image with dimensions hi wi, we calculate the initial number of sub-images Si as: Si = (cid:23) (cid:22) hi (cid:107) , (cid:106) wi (1) where is the resolution of visual encoder (e.g., = 364 pixels). If the total number of patches satisfies (cid:80) Si , we proceed with these sub-image counts. Otherwise, we scale down these counts proportionally using scaling factor α = (cid:80) , resulting in adjusted sub-image counts: Si = αSi . (2) 4M is hyperparameter, and we provide experiments on varying different in Figure 3. 5 Backbone LLM Param. PT. IT. Models Otter-9B [30] Emu2-Chat [61] MM1-7B-Chat [52] VILA1.5-8B [38] mPlug-DocOwl-1.5 [16] Idefics2-8B [28] LLaVA-NeXT-Inter [33] Mantis-LLaVA [21] Mantis-Idefics2 [21] Visual Encoder CLIP ViT-L EVA-02-CLIP CLIP ViT-H SigLIP CLIP ViT-L SigLIP SigLIP SigLIP SigLIP Resolution 2242 4482 3782 3842 4482 (x9 crops) 9802 AnyRes 3842 9802 LLaMA-7B LLaMA-33B - LLaMA3-8B LLaMA-7B Mistral-7B Qwen1.5-7B LLaMA3-8B Mistral-7B LEOPARD-LLaVA (Ours) LEOPARD-Idefics2 (Ours) SigLIP SigLIP Adapt HR. 9802 LLaMA3.1-8B Mistral-7B 9B 37B 7B 8B 8B 8B 7B 8B 8B 8B 8B - - 30M 5.1M 160M 1.5M 50M 1M 1M 4M 350M 20M 1.3M 1.2M 0.5M 1M 350M 1M 0.5M 1.2M 350M 1.2M Table 2. detailed comparison of the model training details, including image resolution, vision encoder, backbone LLM, number of parameters (Param.), pre-training (PT.) data size, and instruction tuning (IT.) data size of baselines. AnyRes denotes the resolution selecting method proposed by [40] and Adapt HR. represents the proposed adaptive high-resolution multi-image encoding strategy. and Image Partitioning: For each image, we perform grid search over possible number of rows and columns (where 1 r, i) to find the optimal cropping configuration that maximizes the effective resolution within the allocated sub-images [32]. This configuration results in the original image being padded and resized to target resolution of (h = v). We then divide the image into sub-images of size (v v). Additionally, the original image is directly resized to (v v), which provides global view of the visual content. = v, Image Encoding: Most vision encoders transform an image into sequence of visual features RLd, where represents the sequence length and denotes the feature dimension. Typically, is in the hundreds, e.g. the SigLIP encoder yields visual feature sequence in the shape of = 676 and = 1152 for the input image. Given that most LLMs have sequence length of only 8K tokens, this implies that without any text input, the model can encode at most 12 images, which severely limits the image allocation budget. To mitigate this issue, inspired by the pixel shuffling operation [8, 29], we apply similar strategy to the visual features. Specifically, we concatenate adjacent visual features along the feature dimension, effectively reducing the sequence length by factor of n. This results in compressed visual feature sequence L nd. By decreasing the sequence length in this way, we are able to accommodate more images within the sequence length constraints of the LLM. To incorporate visual features into the LLM, we first project the encoded visual feature sequences into the textual input embedding space using vision-language connector. Since the partitioned images yield feature sequences of variable length, we introduce special tokens into the textual input to demarcate the image features to help the model distinguish visual features. Specifically, the sequence for the i-th image is formatted as: {Image i: <Img> <Visual Feature Sequence> < /Img>}, where <Img> and < /Img> are 6 special tokens. An illustrative example of this sequence formatting is provided in Figure 2. 4. Experiment 4.1. Implementation Details Model Architecture. We train our models on two base architectures: LLaVA [39] and Idefics2 [28]. For LEOPARDLLaVA, we use SigLIP-SO-400M [73] with 364364 image resolutions as the visual encoder since it supports larger resolution than the commonly used 224 224 resolution CLIP visual encoder [55]. Each image is encoded into sequence of 26 26 = 676 visual features under patch size of 14. With the visual feature pixel shuffling strategy, each image is further processed into sequence of 169 visual features. We limit the maximum number of images (M ) in each sample to 50, which produces up to 8, 450 visual features in total. Following (author?) [39], we adopt two-layer MLPs as the visual-language connector. We use LLaMA-3.1 [53] as the LM. For LEOPARD-Idefics2, we follow the architecture of Idefics2-8B which uses SigLIP-SO-400M as the visual encoder but increases its image resolution to 980980 to make the text legible. The features outputted by the visual encoder are compressed with feature resampler into 64 tokens per image. Idefics2-8B adopts the Mistral-7B [20] as the LM. Training Details. When training LEOPARD-LLaVA, we first train the visual-language connector using LLaVAs 558K multimodal pre-training dataset. Subsequently, we fine-tune the model (with both the connector and the LM unfrozen) using our LEOPARD-INSTRUCT data. As for LEOPARDIdefics2, it is pre-trained on dataset comprised of over 350M multimodal samples. Given the computational challenges of reproducing such extensive pre-training, and to ensure fair comparison with baselines that utilize the pretrained Idefics2 checkpoint, we directly adopt Idefics2 viModels Otter-9B Emu2-Chat MM1-7B-Chat VILA-LLaMA3-8B mPlug-DocOwl-1.5 Idefics2-8B LLaVA-NeXT-Inter Mantis-LLaVA Mantis-Idefics2 LEOPARD-LLaVA LEOPARD-Idefics2 Text-Rich Multi-Image Text-Rich Single-Image MVQAD DUDE SlideVQA MCQA MH Multi Avg. VQAT VQAD VWB Avg. 0.17 17.58 - 30.75 35.85 46.67 39.92 31.89 51.61 53.90 66.06 0.15 13.79 - 19.75 16.94 23.06 24.04 17.73 27.74 35.94 40. 5.95 0.60 - 24.72 4.54 25.14 23.46 16.81 24.02 23.83 34.93 1.08 2.40 - 1.87 0.26 2.59 14.34 9.72 12.97 9.68 18.03 0.14 0.72 - 3.66 0.86 9.89 3.55 3.46 5.48 10.76 10. 1.50 7.02 - 16.15 11.69 21.47 21.06 15.92 24.36 26.82 33.97 23.18 66.60 72.80 66.30 68.60 70.40 62.76 59.20 63.50 67.70 80.40 3.53 5.44 - 30.38 82.20 67.30 75.70 39.02 54.03 68.07 74. 10.20 12.30 18.17 30.07 - - 23.37 40.02 29.80 60.20 23.76 53.82 21.36 53.27 17.88 38.70 22.47 46.67 24.91 53.56 25.60 60.26 Table 3. Experiment results of baseline models and LEOPARD on 8 benchmarks of text-rich images. We use abbreviated benchmark names due to space limits. MVQAD: Multi-page DocVQA, MCQA: MultiChartQA, MH: MultiHiertt, VQAT : TextVQA, VQAD: DocVQA, VWB: VisualWebBench. Following [64], for MVQAD, DUDE, and VQAD, we use average normalized levenshtein similarity [5] as the metric. For other benchmarks, accuracy is used as the metric, which measures if the predicted answer exactly matches any target answer. sual feature resampler and fine-tune the model on the LEOPARD-INSTRUCT dataset. We train both LEOPARD-LLaVA and LEOPARD-Idefics2 on 64 A100-40G GPUs with global batch size of 128. We use the AdamW optimizer with β1 = 0.9, β2 = 0.999. Following [21], we use learning rate of 1 105 for LEOPARD-LLaVA and 5 106 for LEOPARD-Idefics2 to protect its pretrian knowledge. We use cosine learning rate scheduler with linear learning rate warm-up for the first 3% steps. All model variants are trained 1 epoch under the same hyperparameters. It takes around 120 GPU days to train LEOPARD under both settings. 4.2. Baseline Models We compare LEOPARD against range of existing opensource MLLMs that support multi-image inputs. These baseline models include Otter-9B [30], Emu2-Chat-34B [61], MM1-7B-Chat [52], Mantis [21], VILA [38], Idefics28B [28], and LLaVA-NeXT-Interleave [33]. Models that only support single image input are excluded from our comparisons, except for mPlug-DocOwl1.5 [16], as it is primarily trained on visual document data and demonstrates strong capabilities on text-rich image tasks. Table 2 demonstrates detailed comparison of the model training details of between baseline models and our proposed LEOPARD, which highlights their architecture, image resolution and training data differences. 4.3. Evaluating Benchmarks We evaluated LEOPARD and baseline methods across three categories of vision-language tasks on (1) single text-rich image evaluation, (2) multiple text-rich images evaluation, and (3) general reasoning evaluation. Benchmarks for (1) include TextVQA [59], DocVQA [51], and VisuModels MIRB MiBench MMMU MathVista SQAI Avg. 20.74 36.02 - Otter-9B Emu2-Chat MM1-7B-Chat VILA-LLaMA3-8B 40.87 mPlug-DocOwl-1.5 25.39 Idefics2-8B 33.02 LLaVA-NeXT-Inter 44.38 40.76 Mantis-LLaVA Mantis-Idefics2 41.80 LEOPARD-LLaVA 42.00 41.38 LEOPARD-Idefics 43.72 58.93 - 53.70 40.80 46.39 74.52 59.96 56.80 60.80 61.74 30.89 34.10 37.00 36.90 35.44 42.90 38.44 40.10 41.10 43.00 40.11 22.00 30.40 35.90 35.40 29.50 45.00 32.10 34.40 40.40 45.50 44. - 60.44 35.55 65.69 45.03 72.60 79.90 49.35 64.40 39.11 89.04 51.27 72.63 52.41 74.90 50.02 81.30 52.28 85.57 55.37 90.38 55.68 Table 4. Experimental results on general domain benchmarks. We abbreviate the Image split of ScienceQA as SQAI . alWebBench [43]. Benchmarks for (2) include Multi-page DocVQA [64], DUDE [25], SlideVQA [62], Multihiertt [77], and MultiChartQA [2], which cover diverse range of typical multi-image tasks, such as document understanding and slide question answering. Benchmarks for (3) include MMMU [71], MathVista [45], ScienceQA [56], MIRB [76] and MiBench [42], which evaluate MLLMs from different perspectives, including world knowledge, mathematics, and scientific reasoning etc. 4.4. Main Experimental Results Question 1: How does LEOPARD compare to state-of-theart MLLMs on vision-language tasks? LEOPARD achieves outstanding performance on text-rich, multi-image benchmarks, as shown in Table 3. Notably, both LEOPARD-LLaVA and LEOPARD-Idefics2 significantly outperform all baselines. LEOPARD-Idefics2 becomes the strongest open-source MLLM in this area, achieving an average improvement of 9.61 points over the previous best 7 Ablation Settings Text-Rich Multi-Image Text-Rich Single General MVQAD DUDE SlidesVQA Multi Avg. TextVQA DocVQA MMMU MathVista () Our Best Setting (as in Table 3): LLaMA-3.1 + Adaptive + 1 37.89 LEOPARD-LLaVA (1) Effect of Adaptive High-Resolution Encoding: LLaMA-3.1 + 1 53.90 23.83 35.94 26. 20.93 - w/o Adaptive - with LLaMA-3 .1 40.44 (2) Effect of Backbone LLMs: LLaMA-3 + Adaptive + 1 48.66 25.75 (3) Effect of Data Domains: LLaMA-3.1 + Adaptive 23.10 18.73 20.79 - with chart web - with doc web - with doc chart 29.50 35.65 35.70 43.79 54.33 54.62 32.64 29.17(8.7) 1 35.68(2.2) 1 1 1 67.70 1 60.18 68.07 43.00 45.50 44. 41.00 42.40 67.08 54.92 41.22 42. 32.13(5.7) 36.23(1.7) 37.02(0.9) 66.78 66.86 67.40 56.60 50.78 67.82 40.67 41.89 41.78 44.80 39.60 44.00 Table 5. Ablation studies on LEOPARD-LLaVA from four different perspectives: (1) evaluating the impact of Adaptive High-Resolution Encoding, (2) pre-training LLaVA by initializing with checkpoints from either LLaMA-3 or LLaMA-3.1 , and (3) examining the impact of using different data domains for instruction tuning, including doc , chart , and web . performance. In single-image text-rich scenarios, LEOPARD outperforms several recent strong models, including VILA and LLaVA-NeXT. LEOPARD even achieves slightly higher average scores than the state-of-the-art mPlug model, despite mPlug being trained on 4M single-image data while LEOPARD is tuned on <200K. This demonstrates that training on multi-image data from LEOPARD-INSTRUCT also benefits model performance on single-image tasks. In addition, we evaluate LEOPARD on general-domain benchmarks which contain both multi-image and singleimage instances. As shown in Table 4, LEOPARD outperforms other open-source MLLMs on these benchmarks. Remarkably, LEOPARD surpasses Mantis, its counterpart multi-image model trained on the same foundational architecture and comparable volume of data. This performance demonstrates the high quality and diversity of the LEOPARDINSTRUCT dataset, which effectively preserves our models general image understanding capabilities. Question 2: Is the one-million text-rich multi-image dataset effective for instruction tuning? Mantis-Idefics2 is trained on combination of natural multiimage data and text-rich single-image data. However, LEOPARD-Idefics2 outperforms Mantis-Idefics2 by 12.8 points on text-rich multi-image benchmarks. This disparity indicates that developing strong multi-image text-rich capabilities through cross-domain transfer, such as with Mantis data, presents significant challenges. This finding underscores the importance of optimizing LEOPARD using high-quality, diverse, and well-curated multi-image text-rich datasets that are specifically tailored for complex multi-image scenarios. Furthermore, LEOPARD-Idefics2 surpasses its base model, Idefics2, by 6.4 points across three single-image text-rich benchmarks, though Idefics2 is trained on over 20M instruction data that includes text-rich tasks like DocVQA and TextVQA. This highlights that the LEOPARD-INSTRUCT provides unique advantages to MLLMs that are not adequately addressed by existing datasets. Question 3: Does Adaptive high-resolution multi-image encoding improve MLLM performance? To assess the effectiveness of the proposed adaptive highresolution multi-image encoding, we compared LEOPARD with variant that excludes this feature (i.e., w/o Adaptive in Table 5). We notice significant performance decline across all text-rich benchmarks, particularly on document-related benchmarks like DocVQA (-23.4), Multipage DocVQA (-13.5), and DUDE (-9.8). This observation supports our hypothesis that high-resolution image encoding is especially beneficial for text-rich images, particularly with dense text content such as document pages. 4.5. More Analysis Question 4: How does data from different domains contribute to instruction tuning? LEOPARD-INSTRUCT mainly cover three main domains, i.e. documents & slides ( doc ), tables & charts ( chart ), and websites ( web ). To assess the impact of data from different domains, we conduct ablation studies on three variants of LEOPARD, with the results presented in Table 5 Removing any part of the training data results in performance degradation. The most significant drop occurs when we exclude document data while removing web data leads to slight decrease. However, the mixed-domain datasets, such as LLaVAR and mPlugDocReason, also contain data in these domains which are challenging to isolate and ablate. This may contribute to the relatively preserved performance even after the ablation of certain data sources. Question 5: What is the influence of different image budgets in adaptive multi-image encoding? In our adaptive multi-image encoding module, we define 8 Figure 3. Impact of the sub-image budget on the resulting model across four benchmarks. w/o indicates no partitioning into sub-images. budget for the maximum number of sub-images that the model can process. To evaluate the impact of such image partitioning, we train LEOPARD using different values of : 25, 50, 75, as well as baseline setting where no image partitioning is applied and the number of sub-images equals the number of original images. According to the results plotted in Figure 3, model performance peaks or plateaus when is set around 50. Thus, we adopt 50 as the default value for training LEOPARD. These results show that increasing image numbers does not consistently improve performance, as input sequences can become excessively long and even exceed the models sequence length limit. Question 6: How does the backbone language model affect the performance? To ensure fair comparison with multi-image competitor models, Mantis-LLaVA and VILA1.5, we also evaluate variant of LEOPARD using LLaMA-3 instead of LLaMA-3.1 , aligning its backbone language model architecture with these two baselines. According to Table 5, this substitution results in only slight drop in average performance on text-rich multi-image tasks (2.2). Nevertheless, comparing with results in Table 3, LEOPARD-LLaMA-3 still substantially outperforms both baselines in all tasks, such as Multi-page DocVQA (+16.8 over Mantis and +17.9 over VILA) and DUDE (+14.9 over Mantis and +12.9 over VILA). These results indicate that LEOPARDs superior performance is not simply result of the upgraded backbone large language models. 5. Conclusion In this paper, we introduce LEOPARD, novel MLLM specifically designed for text-rich, multi-image tasks. LEOPARD is equipped with two key innovations: (1) LEOPARDINSTRUCT, large-scale instruction-tuning dataset that encompasses wide range of text-rich, multi-image instructions, and (2) an adaptive image encoding module capable of processing multiple high-resolution images efficiently. Our experimental results across diverse benchmarks highlight LEOPARDs superior performance compared to existing open-source MLLMs, particularly in text-rich multi-image scenarios. Further analysis and ablation studies underscore the effectiveness of both the collected dataset and adaptive encoding strategy, solidifying LEOPARDs contribution to multimodal research."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: visual language model for few-shot learning, 2022. 3 [2] Anonymous. benchmark for multi-chart question answering. Under Review, 2024. 7 [3] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Yitzhak Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo: An open-source framework for training large autoregressive vision-language models. CoRR, abs/2308.01390, 2023. 1, 2, 3 [4] Fahri Aydos. Webscreenshots, 2020. 5, 15 [5] Ali Furkan Biten, Rub`en Tito, Andres Mafla, Lluıs Gomez, Marcal Rusinol, Minesh Mathew, C. V. Jawahar, Ernest Valveny, and Dimosthenis Karatzas. ICDAR 2019 competition on scene text visual question answering. In 2019 International Conference on Document Analysis and Recognition, ICDAR 2019, Sydney, Australia, September 20-25, 2019, pages 1563 1570. IEEE, 2019. 7 [6] Shuaichen Chang, David Palzer, Jialin Li, Eric Fosler-Lussier, and Ningchuan Xiao. Mapqa: dataset for question answering on choropleth maps. CoRR, abs/2211.08545, 2022. 5, [7] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. CoRR, abs/2311.12793, 2023. 5 [8] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. How far are we to gpt-4v? closing the gap 9 to commercial multimodal models with open-source suites, 2024. 2, 6 [9] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. 3 [10] Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuoling Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nvlm: Open frontier-class multimodal llms. arXiv preprint arXiv:2409.11402, 2024. 3 [11] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. [12] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samual Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. 4, 5, 15 [13] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang, Haodong Duan, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Zhe Chen, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Kai Chen, Conghui He, Xingcheng Zhang, Jifeng Dai, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer2-4khd: pioneering large vision-language model handling resolutions from 336 pixels to 4k HD. CoRR, abs/2404.06512, 2024. 5 [14] Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.13919, 2024. 4 [15] Yu-Chung Hsiao, Fedir Zubach, Maria Wang, and Jindong Chen. Screenqa: Large-scale question-answer pairs over mobile app screenshots, 2024. 15 [16] Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, Fei Huang, and Jingren Zhou. mplug-docowl 1.5: Unified structure learning for ocrfree document understanding. CoRR, abs/2403.12895, 2024. 1, 2, 3, 5, 6, 7, 15 [17] Anwen Hu, Haiyang Xu, Liang Zhang, Jiabo Ye, Ming Yan, Ji Zhang, Qin Jin, Fei Huang, and Jingren Zhou. mplug-docowl2: High-resolution compressing for ocrfree multi-page document understanding. arXiv preprint arXiv:2409.03420, 2024. 3 [18] Ronghang Hu, Amanpreet Singh, Trevor Darrell, and MarIterative answer prediction with pointercus Rohrbach. In 2020 augmented multimodal transformers for textvqa. IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, 2020. [19] Yushi Hu, Otilia Stretcu, Chun-Ta Lu, Krishnamurthy Viswanathan, Kenji Hata, Enming Luo, Ranjay Krishna, and Ariel Fuxman. Visual program distillation: Distilling tools and programmatic reasoning into vision-language models. CoRR, abs/2312.03052, 2023. 5 [20] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b. CoRR, abs/2310.06825, 2023. 6 [21] Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Liu, and Wenhu Chen. MANTIS: interleaved multiimage instruction tuning. CoRR, abs/2405.01483, 2024. 2, 3, 5, 6, 7, 14 [22] Kushal Kafle, Brian L. Price, Scott Cohen, and Christopher Kanan. DVQA: understanding data visualizations via question answering. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 56485656. Computer Vision Foundation / IEEE Computer Society, 2018. 1, 4, 15 [23] Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Akos Kadar, Adam Trischler, and Yoshua Bengio. Figureqa: An annotated figure dataset for visual reasoning. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Workshop Track Proceedings. OpenReview.net, 2018. 4, 15 [24] Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem AlShikh, and Ruslan Salakhutdinov. Omniact: dataset and benchmark for enabling multimodal generalist autonomous agents for desktop and web. CoRR, abs/2402.17553, 2024. 5, 15 [25] Jordy Van Landeghem, Rafal Powalski, Rub`en Tito, Dawid Jurkiewicz, Matthew B. Blaschko, Lukasz Borchmann, Mickael Coustaty, Sien Moens, Michal Pietruszka, Bertrand Anckaert, Tomasz Stanislawek, Pawel Joziak, and Ernest Valveny. Document understanding dataset and evaluation (DUDE). In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pages 19471 19483. IEEE, 2023. 1, 3, 7, [26] Hugo Laurencon, Andres Marafioti, Victor Sanh, and Leo Tronchon. Building and better understanding vision-language arXiv preprint models: arXiv:2408.12637, 2024. 3 insights and future directions. [27] Hugo Laurencon, Lucile Saulnier, Leo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, and Victor Sanh. OBELICS: an open webscale filtered dataset of interleaved image-text documents. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. 2 [28] Hugo Laurencon, Leo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? CoRR, abs/2405.02246, 2024. 1, 3, 6, 7, 14 [29] Hugo Laurencon, Andres Marafioti, Victor Sanh, and Leo Tronchon. Building and better understanding vision-language models: insights and future directions, 2024. 6 [30] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: multi-modal model with in-context instruction tuning. CoRR, abs/2305.03726, 2023. 6, 10 [31] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 3 [32] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer, 2024. 6 [33] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. CoRR, abs/2407.07895, 2024. 3, 6, 7, 14 [34] Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and Qi Liu. Multimodal arxiv: dataset for improving scientific comprehension of large vision-language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1436914387. Association for Computational Linguistics, 2024. 3, 15 [35] Peng Li, Yeye He, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang, Danielle Rifinski Fainman, Dongmei Zhang, and Surajit Chaudhuri. Table-gpt: Table fine-tuned GPT for diverse table tasks. Proc. ACM Manag. Data, 2(3):176, 2024. 4, [36] Wen Li, Limin Wang, Wei Li, Eirikur Agustsson, and Luc Van Gool. Webvision database: Visual learning and understanding from web data. CoRR, abs/1708.02862, 2017. 5, 15 [37] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Monkey: Image resolution and text label are important things for large multi-modal models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2676326773, 2024. 5, 15 [38] Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. VILA: on pre-training for visual language models. CoRR, abs/2312.07533, 2023. 1, 2, 3, 5, 6, 7 [39] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. CoRR, abs/2310.03744, 2023. 5, 6 [40] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. 2, 3, 5, 6 [41] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. [42] Haowei Liu, Xi Zhang, Haiyang Xu, Yaya Shi, Chaoya Jiang, Ming Yan, Ji Zhang, Fei Huang, Chunfeng Yuan, Bing Li, and Weiming Hu. Mibench: Evaluating multimodal large language models over multiple images. CoRR, abs/2407.15272, 2024. 7 [43] Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham Neubig, Yuanzhi Li, and Xiang Yue. Visualwebbench: How far have multimodal llms evolved in web page understanding and grounding? CoRR, abs/2404.05955, 2024. 7 [44] Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma, Shuo Zhang, and Xiang Bai. Textmonkey: An ocr-free large multimodal model for understanding document. arxiv preprint, 2403.04473, 2024. 3 [45] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. 7 [46] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, SongChun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for In The Eleventh semi-structured mathematical reasoning. International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. [47] Pan Lu, Liang Qiu, Jiaqi Chen, Tanglin Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Iconqa: new benchmark for abstract diagram understanding and visual language reasoning. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021. 5, 15 [48] Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pages 22632279, 2022. 1, 4, 15 [49] Ahmed Masry, Megh Thakkar, Aayush Bajaj, Aaryaman Kartha, Enamul Hoque, and Shafiq Joty. Chartgemma: Visual instruction-tuning for chart reasoning in the wild. arXiv preprint arXiv:2407.04172, 2024. 4, 15 [50] Minesh Mathew, Viraj Bagal, Rub`en Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 16971706, 2022. 5, 15 [51] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. 1, 3, 7, 15 [52] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, Anton Belyi, Haotian Zhang, Karanjeet Singh, Doug Kang, Ankur Jain, Hongyu H`e, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Guoli Yin, Mark Lee, Zirui Wang, Ruoming Pang, Peter Grasch, Alexander Toshev, and Yinfei Yang. MM1: methods, analysis & insights from multimodal LLM pre-training. CoRR, abs/2403.09611, 2024. 6, 7 [53] Meta, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozi`ere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, 11 Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. 6 [54] Vaishali Pal, Andrew Yates, Evangelos Kanoulas, and Maarten de Rijke. MultiTabQA: Generating tabular answers for multi-table question answering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Toronto, Canada, 2023. Association for Computational Linguistics. 4, 15 [55] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 8748 8763. PMLR, 2021. 6 [56] Tanik Saikh, Tirthankar Ghosal, Amish Mittal, Asif Ekbal, and Pushpak Bhattacharyya. Scienceqa: novel resource for question answering on scholarly articles. Int. J. Digit. Libr., 23(3):289301, 2022. 7 [57] Athar Sefid, Prasenjit Mitra, Jian Wu, and Lee Giles. Extractive research slide generation using windowed labeling ranking. In Proceedings of the Second Workshop on Scholarly Document Processing. Association for Computational Linguistics, 2021. 4, [58] Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, and Roy Ka-Wei Lee. Mathllava: Bootstrapping mathematical reasoning for multimodal large language models. CoRR, abs/2406.17294, 2024. 5, 15 [59] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83178326, 2019. 1, 7 [60] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards VQA models that can read. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, 2019. 3 [61] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. CoRR, abs/2312.13286, 2023. 3, 6, 7 [62] Ryota Tanaka, Kyosuke Nishida, Kosuke Nishida, Taku Hasegawa, Itsumi Saito, and Kuniko Saito. Slidevqa: dataset for document visual question answering on multiple images. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 1363613645, 2023. 1, 3, 7, 15 [63] Jingqun Tang, Chunhui Lin, Zhen Zhao, Shu Wei, Binghong Wu, Qi Liu, Hao Feng, Yang Li, Siqi Wang, Lei Liao, Wei Shi, Yuliang Liu, Hao Liu, Yuan Xie, Xiang Bai, and Can Huang. Textsquare: Scaling up text-centric visual instruction tuning. CoRR, abs/2404.12803, 2024. [64] Rub`en Tito, Dimosthenis Karatzas, and Ernest Valveny. Hierarchical multimodal transformers for multi-page docvqa. CoRR, abs/2212.05935, 2022. 1, 3, 7, 15 [65] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 3 [66] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. Cogvlm: Visual expert for pretrained language models, 2023. 3 [67] Xiao Wang, Guangyao Chen, Guangwu Qian, Pengcheng Gao, Xiao-Yong Wei, Yaowei Wang, Yonghong Tian, and Wen Gao. Large-scale multi-modal pre-trained models: comprehensive survey. Machine Intelligence Research, 20(4):447482, 2023. 1 [68] Jason Wu, Siyan Wang, Siman Shen, Yi-Hao Peng, Jeffrey Nichols, and Jeffrey P. Bigham. Webui: dataset for enhancing visual UI understanding with web semantics. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, CHI 2023, Hamburg, Germany, April 23-28, 2023, pages 286:1286:14. ACM, 2023. 5, 15 [69] Yang Wu, Shilong Wang, Hao Yang, Tian Zheng, Hongbo Zhang, Yanyan Zhao, and Bing Qin. An early evaluation of gpt-4v (ision). arXiv preprint arXiv:2310.16534, 2023. 1 [70] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Guohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji Zhang, Qin Jin, Liang He, Xin Lin, and Fei Huang. UReader: Universal OCR-free visually-situated language understanding with multimodal large language model. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, Dec. 2023. Association for Computational Linguistics. 3 [71] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. 7 [72] Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, and Chen Change Loy. Contextual object detection with mulInternational Journal of timodal large language models. Computer Vision, pages 119, 2024. [73] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and 12 Lucas Beyer. Sigmoid loss for language image pre-training. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pages 11941 11952. IEEE, 2023. 6 [74] Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. Vision-language models for vision tasks: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 1 [75] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. Llavar: Enhanced visual instruction tuning for text-rich image understanding. CoRR, abs/2306.17107, 2023. 1, 3, 5, 15 [76] Bingchen Zhao, Yongshuo Zong, Letian Zhang, and Timothy M. Hospedales. Benchmarking multi-image understanding in vision and language models: Perception, knowledge, reasoning, and multi-hop reasoning. CoRR, abs/2406.12742, 2024. [77] Yilun Zhao, Yunxiang Li, Chenying Li, and Rui Zhang. Multihiertt: Numerical reasoning over multi hierarchical tabular and textual data. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 65886600. Association for Computational Linguistics, 2022. 4, 7, 15 [78] Ge Zheng, Bin Yang, Jiajin Tang, Hong-Yu Zhou, and Sibei Yang. Ddcot: Duty-distinct chain-of-thought prompting for multimodal reasoning in language models. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. 5 [79] Fengbin Zhu, Wenqiang Lei, Fuli Feng, Chao Wang, Haozhou Zhang, and Tat-Seng Chua. Towards complex document understanding by discrete reasoning. In MM 22: The 30th ACM International Conference on Multimedia, Lisboa, Portugal, October 10 - 14, 2022, pages 48574866. ACM, 2022. 3, 15 13 A. Appendix A.1. Leopard-Instruct To train LEOPARD, we created large instruction-tuning dataset, LEOPARD-INSTRUCT, with 925K instances, including 739K designed for text-rich, multi-image scenarios. Despite surveying existing datasets, we found only 154K suitable text-rich, multi-image samples insufficient for effective instruction tuning, which is far from sufficient for effective instruction tuning, as shown in prior MLLM studies [21, 28, 33]. To overcome this limitation, we developed several data collection pipelines to collect high-quality text-rich, multi-image data, resulting in additional 585K instances. Table 6 provides detailed breakdown of the composition of the LEOPARD-INSTRUCT dataset. This table includes the name, domain, and sample size of sub-datasets. Additionally, it specifies how we construct multi-image samples, the number of images per sample, and the presence of rationales. We draw chart to illustrate the data composition of LEOPARD-INSTRUCT dataset 4. A.2. Prompts"
        },
        {
            "title": "We specify the prompt used during the data construction",
            "content": "process as follows: A.3. Details of Table Rendering To convert the textual table dataset into multimodal dataset, the JSON or DataFrame format data is transformed into tabular images using Python. We utilize three Python packages, i.e. dataframe image5, pandas6, and matplotlib7 with various styling to enhance the diversity of the rendered images. To ensure the clarity and legibility of the plotted images, the original data is filtered by excluding any tables that contain more than 20 rows. This threshold was set to maintain the recognizability of the resulting images. A.4. Qualitative Results We show two examples to give an illustrative demonstration of the models performance. As can be seen from Figure 8, LEOPARD can not only capture detailed data in multiple tables precisely but also perform cross-table calculations, therefore it can answer the complex question correctly. Another example is demonstrated in Figure 9. LEOPARD can accurately perceive the prominent information under high-resolution four-page document, demonstration effective text-rich abilities under multi-image scenarios. 5https://github.com/dexplo/dataframe_image. 6https://pandas.pydata.org/. 7https://matplotlib.org/."
        },
        {
            "title": "Dataset",
            "content": "Domain Multi-image"
        },
        {
            "title": "Rationales",
            "content": "#Samples (K) Doc ArxivQA [34] Doc DUDE [25] Doc MP-DocVQA [64] Doc DocVQA [51] Doc TAT-DQA [79] Slides SlidesGeneration [57] Slides SlidesVQA [62] Slides Slideshare Table Multihiertt [77] Table MultiTabQA [54] Table TableGPT [35] Table TabMWP [46] Chart ChartGemma [49] Chart DVQA [22] Chart FigureQA [23] Chart ChartQA [48] Chart Pew MultiChart Mind2Web [12] Web WebsiteScreenshots [4] Web Web Omniact [24] Web RICO [15] Web WebVision [36] Web WebUI [68] Mix LLaVAR [75] Mix MathV360k [58] Mix Monkey [37] Mix MPlugDocReason [16] Other IconQA [47] Other InfographicVQA [50] Other MapQA [6]"
        },
        {
            "title": "Total",
            "content": "- - 1-3 1-50 1-20 1 2-5 1-20 20 2-8 3-7 1-2 2 1 1-4 1-3 1-2 2 2 1-5 1 1 1-4 1 1 1 1 1-3 1 1-6 1 1-2 - Existing Augmented Augmented None Augmented Augmented Augmented Augmented Existing/Augmented Augmented Existing Existing Existing None None Augmented Augmented None Augmented None None Existing None Existing None None Existing Augmented Augmented None - 81 23 36 39 13 3 10 3 15 6 4 23 65 200 36 32 20 7 2 1 25 1 19 15 38 92 25 64 23 4 925 Table 6. Details of the constructed LEOPARD-INSTRUCT dataset. Images denotes the image number of one sample in each dataset. 15 Figure 4. An illustration of the proportion of sub-datasets and domains in the proposed dataset. Figure 5. The prompt used for generating Q-A pairs with rationales for slide decks data. Figure 6. The prompt used for generating Q-A pairs with rationales for webpage data. Figure 7. We use this prompt for the generation of chain-of-thought rationales given original question, answer, and images. 17 18 Figure 8. An example of multi-table reasoning of LEOPARD. Figure 9. An example of multi-page document question answering of LEOPARD."
        }
    ],
    "affiliations": [
        "Tencent AI Seattle Lab",
        "UIUC",
        "University of Notre Dame"
    ]
}