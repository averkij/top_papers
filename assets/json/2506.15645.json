{
    "paper_title": "Demystifying the Visual Quality Paradox in Multimodal Large Language Models",
    "authors": [
        "Shuo Xing",
        "Lanqing Guo",
        "Hongyuan Hua",
        "Seoyoung Lee",
        "Peiran Li",
        "Yufei Wang",
        "Zhangyang Wang",
        "Zhengzhong Tu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent Multimodal Large Language Models (MLLMs) excel on benchmark vision-language tasks, yet little is known about how input visual quality shapes their responses. Does higher perceptual quality of images already translate to better MLLM understanding? We conduct the first systematic study spanning leading MLLMs and a suite of vision-language benchmarks, applying controlled degradations and stylistic shifts to each image. Surprisingly, we uncover a visual-quality paradox: model, task, and even individual-instance performance can improve when images deviate from human-perceived fidelity. Off-the-shelf restoration pipelines fail to reconcile these idiosyncratic preferences. To close the gap, we introduce Visual-Quality Test-Time Tuning (VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable, low-rank kernel before the frozen vision encoder to modulate frequency content; and (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT dynamically adjusts each input image in a single forward pass, aligning it with task-specific model preferences. Across the evaluated MLLMs and all datasets, VQ-TTT lifts significant average accuracy, with no external models, cached features, or extra training data. These findings redefine ``better'' visual inputs for MLLMs and highlight the need for adaptive, rather than universally ``clean'', imagery, in the new era of AI being the main data customer."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 5 4 6 5 1 . 6 0 5 2 : r a"
        },
        {
            "title": "Demystifying the Visual Quality Paradox in\nMultimodal Large Language Models",
            "content": "Shuo Xing1 , Lanqing Guo2, Hongyuan Hua3, Seoyoung Lee2, Peiran Li1, Yufei Wang4, Zhangyang Wang2, Zhengzhong Tu1 1Texas A&M University 3University of Toronto 2University of Texas at Austin 4Nanyang Technological University"
        },
        {
            "title": "Abstract",
            "content": "Recent Multimodal Large Language Models (MLLMs) excel on benchmark visionlanguage tasks, yet little is known about how input visual quality shapes their responses. Does higher perceptual quality of images already translate to better MLLM understanding? We conduct the first systematic study spanning leading MLLMs and suite of vision-language benchmarks, applying controlled degradations and stylistic shifts to each image. Surprisingly, we uncover visual-quality paradox: model, task, and even individual-instance performance can improve when images deviate from human-perceived fidelity. Off-the-shelf restoration pipelines fail to reconcile these idiosyncratic preferences. To close the gap, we introduce Visual-Quality Test-Time Tuning (VQ-TTT)a lightweight adaptation module that: (1) inserts learnable, low-rank kernel before the frozen vision encoder to modulate frequency content; and (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT dynamically adjusts each input image in single forward pass, aligning it with task-specific model preferences. Across the evaluated MLLMs and all datasets, VQ-TTT lifts significant average accuracy, with no external models, cached features, or extra training data. These findings redefine better visual inputs for MLLMs and highlight the need for adaptive, rather than universally clean, imagery, in the new era of AI being the main data customer."
        },
        {
            "title": "Introduction",
            "content": "Multimodal Large Language Models (MLLMs), which extend the capabilities of traditional language models to jointly process vision and language inputs, have recently achieved remarkable progress across wide range of benchmarks [Li et al., 2022, 2023b, Liu et al., 2024, Li et al., 2024b, Meta, 2024, Bai et al., 2023a, Wang et al., 2024, Lu et al., 2024a, Wu et al., 2024, Bai et al., 2025, Team, 2024, Beyer et al., 2024, Abdin et al., 2024, Abouelenin et al., 2025]. These models excel in tasks such as visual question answering [Singh et al., 2019b, Lu et al., 2022, Fu et al., 2023, Hudson and Manning, 2019, Lu et al., 2024b, Gurari et al., 2018, Yue et al., 2024, Xing et al., 2025a, Li et al., 2025], captioning [Chen et al., 2015, Agrawal et al., 2019], and image-text retrieval [Xing et al., 2025b, Yang et al., 2023, Yasunaga et al., 2022, Hu et al., 2025], often demonstrating impressive zero-shot and few-shot generalization. However, despite this progress, critical and understudied question remains: Equal Contribution. Email: {shuoxing,tzz}@tamu.edu Corresponding author. Preprint. Under review. (Q) How does the perceptual quality of input images affect the performance of MLLMs? How to ensure robust performance when input images suffer from diverse degradations? Intuitively, one might expect that cleaner, sharper, and more natural-looking images would lead to better model understanding, mirroring the preferences of human observers. Yet, in this work, we present counterintuitive and surprising phenomenonwhat we term the visual-quality paradox: for many MLLMs, performance on vision-language tasks can improve when images deviate from conventional notions of visual fidelity. This paradox challenges common assumption in multimodal research and highlights fundamental misalignment between human-centric image quality and model-preferred input representations. To investigate this, we systematically evaluate state-of-the-art MLLMs under 5 common degradation families (noise, motion blur, defocus blur, snow, fog) at multiple severities, spanning 13 visionlanguage datasets. Our findings reveal that not only do different models exhibit distinct quality preferences, but even within single model, performance can vary across tasks or image instances in nontrivial ways. Moreover, we show that standard image restoration techniques, including strong pretrained pipelines and co-training approaches, are insufficient to recover or align with these idiosyncratic model behaviors. To address this, we propose Visual-Quality Test-Time Tuning (VQ-TTT), lightweight and plug-and-play adaptation strategy that modulates the input image at test time, without altering the backbone model or requiring additional training data. VQ-TTT consists of two components: (1) learnable frequency-selective kernel layer inserted before the vision encoder, and (2) shallow-layer LoRA tuning for rapid adaptation. This design allows VQ-TTT to dynamically reshape image frequency content to better match the models task-specific preferences in single forward pass. Across extensive experiments, VQ-TTT demonstrates consistent and significant improvements over baseline and restoration-augmented MLLMs, achieving significant accuracy gain, while introducing negligible computational overhead. These results advocate for paradigm shift: instead of relying on universally clean images, we argue for model-aligned visual adaptation that tailors inputs to the unique preferences of each task and architecture. Our main findings are summarized as follows: Empirical discovery of the visual-quality paradox. Our analysis reveals that higher photographic fidelity does not uniformly benefit MLLMs; some tasks and models perform best on images that humans deem degraded, challenging prevailing assumptions. Analysis behind the counterintuitive performance raise. We employ relative attention and the logit lens technique to investigate how visual degradation influences model behavior, revealing that degradation can encourage MLLMs to focus more sharply and promote semantic alignment. Restoration alone is insufficient. Even strong pretrained restoration networks, when pipelined before an MLLM or co-trained with it, recover only fraction of lost performance and occasionally hurt accuracy, underscoring the need for model-aligned adaptation. VQ-TTT: task-adaptive visual-quality modulation. We propose plug-and-play test-time tunercombining learnable frequency-selective kernel with shallow-layer LoRAthat requires <1% of original model parameters and no additional data. State-of-the-art robustness gains with minimal cost. VQ-TTT consistently boosts performance across all evaluated MLLMs, raising robustness scores by up to 8.6% while adding negligible latency and memory overhead, and without altering downstream training pipelines."
        },
        {
            "title": "2 Preliminaries",
            "content": "To investigate how the quality of visual input affects the performance of MLLMs, we conduct comprehensive evaluation on VQA benchmarks using systematically corrupted image inputs. In this section, we first provide brief overview of MLLMs, followed by description of the relative attention and logit lens techniques employed for our analysis. MLLM MLLMs typically consist of three main components: vision encoder, projector, and an LLM backbone. Given multimodal query (x, v), where denotes the textual instruction and represents the input image, the vision encoder first processes the image into sequence of image tokens. These tokens are then projected into the text embedding space by the projector and 2 subsequently fed both the image and text tokens into the LLM backbone, which generates the final output in an autoregressive manner. st(x, v) Ë†Ak Relative Attention Relative attention refers to the semantically normalized spatial attention from the answer to the image [Zhang et al., 2025]. Specifically, the spatial answer-to-image attention is denoted as the tensor product of the answer-to-token and token-to-image attention, Am sik(x, v) = ti(x,v) (m, is the layer indices of the LLM backbone and projector). Here, Ë†Am Ë†Am st(x, v) represents the average answer-to-token attention across all heads at LLM layer m, and Ë†Ak ti(x, v) denotes the average token-to-image attention across all heads at projector layer k. To emphasize semantically relevant attention, the spatial attention is then normalized by fixed reference instruction =Write general description of the image., i.e. Arel(x, v) = Asi(x, v)/Asi(x, v). Logit Lens Logit lens is technique that decodes the hidden states from intermediate layers of transformer into probability distributions over the vocabulary [nostalgebraist, 2020]. This method enables interpretation of the models latent representations by revealing the information embedded at various depths of the network."
        },
        {
            "title": "3 Are VLMs Robust to Visual Quality?",
            "content": "In this section, we evaluate the robustness and performance of MLLMs using Visual Question Answering (VQA) benchmarks under various visual degradations. We specifically examine three broad categories of image degradation: noise (Gaussian noise), blurring (motion blur and defocus blur), and adverse weather conditions (snow and fog). Each category introduces distinct distortion patterns reflecting real-world conditions: noise adds high-frequency artifacts, blurring removes high-frequency details, and weather conditions like snow and fog occlude critical visual information. By applying these diverse degradations, we aim to thoroughly assess the models sensitivity and resilience to realistic image perturbations. 3.1 Experimental Setup All image degradations are synthetically generated using the imagecorruptions library [Michaelis et al., 2019]. All experiments were performed on computing cluster with 8 NVIDIA A6000 Ada GPUs. Detailed descriptions of the degradation methods, parameter settings, and implementation specifics are provided in the Appendix. Tasks and Datasets To comprehensively evaluate the robustness and generalization ability of MLLMs under degraded visual conditions, we conduct experiments on several commonly adopted VQA benchmarks. Specifically, we utilize the following standardized VQA tasks and datasets: MathVista [Lu et al., 2024b]: consolidated Mathematical reasoning benchmark with (1) seven mathematical reasoning types: algebraic reasoning (ALG), arithmetic reasoning (ARI), geometry reasoning (GEO), logical reasoning (LOG), numeric common sense (NUM), scientific reasoning (SCI), and statistical reasoning (STA); and (2) five primary tasks: figure question answering (FQA), geometry problem solving (GPS), math word problem (MWP), textbook question answering (TQA), and visual question answering (VQA). MMMU [Yue et al., 2024]: comprehensive dataset comprises 11.5K multimodal questions sourced from authentic college exams, quizzes, and textbooks. It covers six broad academic disciplinesArt & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Technology & Engineeringencompassing total of 30 subjects and 183 detailed subfields. ScienceQA [Lu et al., 2022]: comprehensive benchmark that consists of 21K multimodal multiple-choice questions spanning diverse scientific domainsincluding natural science, social science, and language science. The questions are systematically organized into 26 high-level topics, 127 categories, and 379 distinct skills. TextVQA [Singh et al., 2019b]: novel dataset containing 8K multimodal open-ended, text-based questions that include the Optical Character Recognition (OCR) as module, requiring the model to read and reason about the text in the image to be answered. 3 MME [Fu et al., 2023]: dataset that evaluates both perception (OCR, coarse-grained and finegrained object recognition) and cognition (commonsense reasoning, numerical calculation, text translation, and code reasoning) abilities of MLLMs on total of 14 subtasks. Model MathVista MMMU ScienceQAT ScienceQAI TextVQA MMEP MMEC LLaVA-v1.5-7B +Gaussian Noise +Gaussian Noise+RN +Motion Blur +Motion Blur+RN +Defocus Blur +Defocus Blur+RN +Snow +Snow+RM +Fog +Fog+RM LLaVa-v1.6-Mistral-7B +Gaussian Noise +Gaussian Noise+RN +Motion Blur +Motion Blur+RN +Defocus Blur +Defocus Blur+RN +Snow +Snow+RM +Fog +Fog+RM Qwen-2.5-VL-3B-instruct +Gaussian Noise +Gaussian Noise+RN +Motion Blur +Motion Blur+RN +Defocus Blur +Defocus Blur+RN +Snow +Snow+RM +Fog +Fog+RM 23.3 24.2 17.2 24.4 17.8 24.4 16.6 24.1 23.1 24.8 23.8 26.5 29.8 27.4 25.2 27.8 25.7 25.8 28.7 26.5 30.7 25.4 61.6 57.5 50.2 56.6 48.6 56.5 40.0 52.9 45.6 61.6 48. 28.7 28.6 31.0 29.5 31.0 29.5 31.0 28.2 31.0 28.6 31.0 34.2 33.7 34.8 33.6 22.3 34.2 22.3 33.7 22.3 34.2 34.8 43.7 42.1 40.2 41.4 41.3 39.7 39.0 41.8 40.5 43.2 42.1 66.02 66.38 65.95 66.12 66.14 66.19 66.42 65.36 66.05 65.39 65.83 76.02 76.44 75.95 76.09 76.09 76.4 76.63 76.00 76.11 76.00 76.23 75.71 73.90 37.92 74.02 38.01 73.97 37.96 73.90 37.87 74.89 37. 64.85 65.59 64.70 65.05 65.10 65.20 65.69 63.46 64.85 63.51 64.40 71.34 72.24 71.24 71.49 71.54 72.14 72.68 71.29 71.59 71.29 71.84 79.28 75.81 0.00 75.81 0.00 75.66 0.00 75.41 0.00 77.54 0.00 58.18 56.53 56.10 54.33 56.77 54.13 53.76 53.24 53.85 57.13 56.68 63.80 59.09 58.66 55.13 59.35 51.54 51.57 55.39 56.38 62.65 61.16 77.89 65.50 69.28 61.39 70.33 54.18 53.86 64.61 65.35 73.69 73. 1510.28 1431.58 1419.18 1454.98 1473.19 1435.93 1435.69 1405.89 1391.24 1446.64 1429.61 1494.22 1461.04 1438.57 1454.99 1474.13 1395.71 1371.05 1429.05 1416.26 1464.83 1477.21 1515.32 1430.75 1439.65 1396.84 1502.61 1346.74 1399.06 1427.12 1443.55 1499.79 1478.08 357.85 341.78 345.00 361.42 317.50 326.07 347.86 330.35 315.36 342.85 350.71 323.92 307.14 303.57 339.64 298.21 275.71 285.36 305.71 320 307.14 304.29 615.00 597.14 580.00 574.64 557.50 555.35 530.71 554.28 581.43 605.71 626. Table 1: Performance of MLLMs across common degradations and restorations (transformer-based). Pretrained Restoration Intuitively, introducing degradations to the input images in VQA benchmark is likely to cause performance decline in MLLMs, as such perturbations compromise the quality of visual information and hinder the models ability to accurately perceive and reason over the input. Furthermore, we further explore the effectiveness of integrating off-the-shelf pretrained image restoration models as preprocessing step to recover corrupted visual inputs, investigating the extent to which these restoration models can mitigate the negative effects of image degradation and improve the downstream performance of MLLMs on VQA tasks. Specifically, we investigate two primary categories of restoration modelstransformer-based and diffusion-based approachesto restore degraded image inputs. The utilized models include: NAFNet [Chen et al., 2022] is lightweight transformer-based image restoration model that replaces traiditonal nonlinear activations, such as ReLU, GELU, and softmax, with SimpleGate mechanism for efficient image restoration. We adopt the official pretrained checkpoints for deblurring and denoising tasks, denoted as RN . MWFormer [Zhu et al., 2024] introduces weather-aware transformer framework tailored for real-world degradations such as snow and fog. It integrates degradation-aware mechanisms that adaptively handle different weather-induced distortions. In our experiment, we apply it to snow and fog, denoted as RM. SUPIR [Yu et al., 2024] is diffusion-based model trained with 20M high-quality image-text pairs. It uses prompt-based guidance and learns to avoid visual artifacts by incorporating low-quality samples during training. We apply it for motion and defocus blur restoration, denoted as RS . DiffBIR [Lin et al., 2024] introduces two-staged restoration pipeline that decouples the restoration removal and information regeneration. It utilizes generative diffusion priors to reconstruct high-fidelity images. We use it for Gaussian noise restoration, denoted as RDB. DA-CLIP [Luo et al., 2023a] integrates CLIP with diffusion-based IR-SDE Luo et al. [2023b] framework using cross-attention for semantic guidance. It serves as an additional restoration model for snow and fog, denoted as RDC. The results of the diffusion-based restoration models are included in the appendix. 3.2 Results and Analysis In this section, we provide comprehensive analysis of MLLM performance on VQA tasks with degraded image inputs. We also evaluate the effectiveness of applying existing image restoration models in improving model robustness under such conditions. We conduct experiments on VQA benchmarks with LLaVA-v1.5-7B [Liu et al., 2024], LLaVA-v1.6-Mistral-7B [Li et al., 2024b], and Qwen-2.5-VL-3B-instruct [Bai et al., 2025]. The results are presented in the Table 7. Figure 1: Visualizaion of the relative attention of LLaVA-v1.5-7B on image-question pair (image 1301) of the ScienceQA dataset. Lighter regions indicate higher attention weights, while darker regions represent lower attention weights. The Paradox of High Image Quality As shown in Table 7, existing MLLMs generally exhibit significant performance variability when presented with degraded image inputs, demonstrating their vulnerability to common visual perturbations. For tasks involving text and object recognitionsuch as TextVQA [Singh et al., 2019a] and the perception tasks in MME [Fu et al., 2023]all evaluated models suffer notable performance drops. Among the various degradation types, MLLMs show relatively greater robustness to adverse weather corruptions, particularly fog, when performing recognition tasks. In contrast, blurring effects (i.e., motion blur and defocus blur) tend to cause the most severe degradation in performance, likely due to the loss of fine-grained spatial and textual details essential for accurate visual reasoning. Surprisingly, MLLMs performance does not always deteriorate when images deviate from humanperceived fidelity. For cognitively demanding understanding and reasoning tasks (e.g., MathVista, ScienceQA), the introduction of certain degradations can, in some cases, lead to non-trivial performance improvements. For instance, LLaVA-v1.5-7B [Liu et al., 2023] shows an average performance gain of 1.08 points on MathVista [Lu et al., 2024b] when evaluated on degraded images. Similarly, LLaVA-v1.6-Mistral-7B [Li et al., 2024b] shows an average increase of 0.35 in image-based accuracy on ScienceQA [Lu et al., 2022]. This counterintuitive phenomenon may suggest that certain types of visual perturbations may potentially guide the models attention toward salient features, suppressing irrelevant visual details and encouraging the model to focus on the essential semantic content required 5 for reasoning. Alternatively, it may reflect mismatch between human-centric fidelity metrics and the actual visual features exploited by MLLMs. To further investigate the underlying causes of this paradox, we conduct detailed analysis of attention distributions, token-level predictions, and intermediate representations across different layers of the MLLMs under degraded image inputs. How Visual Perturbations Enhance MLLMs In this section, we employ the techniques of relative attention and logit lens to further analyze how image degradation affects the performance of MLLMs on cognitively demanding understanding and reasoning tasks. Our analysis aims to uncover underlying mechanisms that may explain the observed performance improvements under certain degraded conditions. To investigate the underlying cause of the observed paradox, we use LLaVA-v1.5-7B [Liu et al., 2023] as representative example and visualize the relative attention heatmap, as illustrated in Figure 1. Qualitatively, the heatmap illustrates the extent to which different image regions contribute to the generation of the final answereffectively revealing where the MLLM looks when answering the VQA. We can observe that the MLLMs relative attention tends to concentrate more on key regions relevant to answering the question. This suggests that the introduction of noise does not degrade the models ability to attend to important image areas; instead, it may even sharpen or intensify its focus. To further quantify this effect, we compute the entropy of the relative attention maps across five types of degradation, each evaluated at varying severity levels. As shown in Figure 2, introducing image degradations in the ScienceQA dataset generally results in decrease in relative attention entropy for LLaVA-v1.5-7B [Liu et al., 2023], supporting our qualitative case study findings, that MLLM tends to focus more on key image regions relevant to answering the question. Furthermore, as the severity level of degradation increases, the relative attention becomes even more concentrated, indicating potential shift in the models strategy to cope with reduced visual fidelity by prioritizing semantically important regions. Figure 2: Changes in relative attention entropy of LLaVA-v1.5-7B on ScienceQA dataset across different levels of image degradation. Furthermore, we apply the Logit Lens [nostalgebraist, 2020] technique to analyze the behavior of MLLMs when presented with degraded image inputs. For consistency, we continue to use LLaVAv1.5-7B [Liu et al., 2023] as representative case study, with additional results provided in the Appendix. Figure 3 presents the logit lens visualization for germinating plant image-question pair, comparing the outputs when fed with the original image and Gaussian-noised image. Semantically relevant tokens start emerging in shallow layers (Layers 7 to 9) for both input types, indicating that early-layer representations already encode meaningful semantic associations. Furthermore, as layer depth increases, the model exhibits greater confidence in predicting tokens that closely align with the image content, reflecting progressive refinement of visual-semantic associations through deeper layers. Notably, for the 50th image token, the most probable subsequent token is plants. Interestingly, the MLLM provided with the degraded image input successfully decodes this expected token, whereas the original image input results in decoding irrelevant tokens at the last layer. Although the logit lens method does not directly measure the visual understanding capability of the MLLM, these observations suggest that degraded images might unexpectedly guide the model toward stronger semantic coherence in certain contexts, highlighting intriguing dynamics in how MLLMs handle visual degradations. Degradation Then Restoration Arent Cure-All 6 Figure 3: The logit lens of LLaVA-v1.5-7B model with image-question pair (image 180) of the ScienceQA dataset. We present the next token distribution of the 50th image token for each layer in the heatmap. Lighter regions indicate higher probability, while darker regions represent lower probability. In Table 7 and the Appendix, we present the performance of MLLMs on VQA benchmarks using recovered image inputs, where the degraded images have been processed by off-the-shelf pretrained image restoration models. While these restoration models achieve favorable scores based on standard image quality assessment metrics, they do not consistently translate into improved performance for MLLMs. In some cases, for both perception and cognition tasks, the restored images even lead to worse performance than the directly degraded inputs (like ScienceQA and TextVQA for LLaVA-v1.5-7B). This counter-intuitive finding indicates that current image restoration approaches optimize for perceptual quality metrics that do not align with the visual features most critical for MLLMs reasoning capabilities. The restoration process appears to prioritize visual aesthetics over preserving the semantic content necessary for complex visual understanding tasks. While human perception may find the restored images visually improved, the MLLMs can struggle to extract meaningful features from them, indicating mismatch between human visual quality assessment and machine vision preference. Takeaways: MLLMs performance does not always deteriorate when images deviate from human perceived fidelity, degradation can raise the MLLMs performance on cognitively demanding understanding and reasoning tasks. Introducing visual degradation can help MLLMs focus more precisely on regions relevant to the input query, leading to improved semantic coherence. Off-the-shelf pretrained image restoration models may not recover the visual features most important for MLLM, highlighting mismatch between human-centric quality metrics and the visual cues leveraged by machine vision."
        },
        {
            "title": "4 How to Modulate Optimal Visual Quality for VLMs?",
            "content": "To address the limitations of off-the-shelf image restoration approaches in enhancing MLLM performance on degraded inputs, we propose simple yet effective test-time tuning method tailored for MLLMs in this section. 7 4.1 VQ-TTT: Visual-Quality Test-Time Tuning Motivation. Benchmark results reveal that, in certain scenarios, providing clearer visual inputs does not necessarily lead to improved performance in vision-language models (VLMs)a phenomenon that is not coincidental. This observation suggests that the optimal visual input quality may vary depending on the specific downstream task or the architectural characteristics of the VLM. In other words, different tasks and models may exhibit distinct preferences for the frequency characteristics or levels of detail in the input images. To address this, we introduce lightweight and cost-effective solution to dynamically realize such case-by-case preferences. Specifically, we insert learnable kernel before the frozen vision encoder, which enables adaptive modulation of the input images visual quality in taskor model-aware manner. i.e., Learnable Kernel for Adaptive Visual Quality. To modulate image quality, learnable blurring or sharpening, we introduce learnable kernel layer that adaptively interpolates between the input image and its blurred version. Given an input image v, the modulated output is defined as: = (1 + b)v (v KÏƒ) (1) where is learnable scalar blending coefficient, denotes channel-wise (depthwise) convolution, and KÏƒ is separable Gaussian kernel with learnable standard deviation Ïƒ. This simple yet flexible formulation enables the model to adaptively modulate the input image toward either sharpening (b > 0) or blurring (b < 0) using only two learnable parameters. Figure 4: Illustration of VQ-TTT framework. Test-Time Visual Quality Modulation. To bridge the gap between the modulated image distribution and the original input distribution of vision-language models (VLMs), we introduce lightweight LoRA adapters into the shallow layers of the CLIP image encoder. Specifically, we insert LoRA modules into the first two layers, enabling adaptation with only small number of parameters while keeping the original, well-pretrained weights of the VLM frozen. To conduct test-time tuning, given VLM that predicts distribution over textual outputs conditioned on an image-text pair, i.e., modulated image and text pair, the entropy minimization loss can be defined as: (cid:88) Lentropy = pÎ¸(y v, t) log pÎ¸(y v, t) (2) where pÎ¸(y v, t) is the models predicted probability of output given the input pair (v, t), with parameters Î¸. This objective encourages the model to produce confident, low-entropy predictions conditioned on multimodal inputs. yY 4.2 Results and Analysis We adopt LLaVA series models as representative baselines to evaluate the effectiveness of our proposed VQ-TTT. Table 2 presents detailed comparison between the original baselines and their VQ-TTT-enhanced counterparts. Across range of benchmark datasets, our method consistently achieves performance gains of up to 4.5%, demonstrating its broad applicability. An exception is observed in the recognition metric on the MME dataset, where minor decline occurs due to the inherent trade-off between perceptual quality and recognition accuracy. Notably, our test-time tuning strategy operates on an extremely lightweight module, comprising only two learnable parameters within kernel layer and approximately 0.1M additional parameters from plugged-in LoRA adapter. Full configuration details and parameter breakdowns are provided in the supplementary material. Importantly, VQ-TTT enables adaptive modulation of visual quality, allowing the model to dynamically tailor its outputs to the specific requirements of each task and evaluation dataset. 8 Model LLaVA-v1.5-7B +VQ-TTT MathVista MMMU ScienceQAT 23.3 24.41. 66.02 66.330.3 28.7 31.42.7 ScienceQAI TextVQA MMEP 1500.13 58.18 64.85 1512.2312 58.660.5 65.440.6 MMEC 316.43 307.868.6 LLaVa-v1.6-Mistral-7B 26.5 +VQ-TTT 27.51. 34.2 34.40.2 76.02 76.280.3 71.34 71.940.6 63.80 63.910.1 1494.22 1503.889.6 323.92 318.935. Table 2: Performance of VQ-TTT on LLaVA-v1.5-7B and LLaVa-v1.6-Mistral-7B."
        },
        {
            "title": "5 Related Work",
            "content": "5.1 Multimodal Large Language Models Multimodal Large Language Models (MLLMs)[Li et al., 2022, 2023b, Liu et al., 2024, Li et al., 2024b, Meta, 2024, Bai et al., 2023a, Wang et al., 2024, Lu et al., 2024a, Wu et al., 2024, Bai et al., 2025, Team, 2024, Beyer et al., 2024, Abdin et al., 2024, Abouelenin et al., 2025] build upon the language understanding and reasoning strengths of foundational LLMs[Devlin et al., 2018, Radford et al., 2019, Brown et al., 2020, Team et al., 2023, Roziere et al., 2023, Touvron et al., 2023a,b, Raffel et al., 2020, Yang et al., 2024, Team, 2024] by incorporating visual processing capabilities. These models typically rely on visual encoderssuch as CLIP [Radford et al., 2021]to transform image inputs into embedding representations that are then projected into the shared space of language embeddings. This cross-modal alignment has enabled wide range of real-world applications, including biomedical image analysis [Moor et al., 2023, Li et al., 2024a], autonomous driving [Shao et al., 2024, Tian et al., 2024, Sima et al., 2023, Xing et al., 2024b, Wang et al., 2025, Ma et al., 2025, Xing et al., 2024a, Luo et al., 2025], and embodied AI in robotics [Rana et al., 2023, Kim et al., 2024, Chen et al., 2025]. 5.2 Test-Time Training in VLMs Test-time training (TTT) has emerged as promising strategy to adapt pre-trained models to distribution shifts or downstream tasks without requiring full model retraining. While TTT has been extensively explored in vision Sun et al. [2020], Wang et al. [2021] and language Zhang et al. [2023a] domains, its application to vision-language models (VLMs) remains relatively underexplored. For VLMs, test-time adaptation is more challenging due to the dual-modal nature of inputs and the high computational cost of inference. Recent work has begun to address this: for instance,Zhang et al. [2023b] proposes lightweight adapters for modulating vision-language alignment during inference, whileLi et al. [2024c] explores visual prompt tuning to adapt frozen VLMs to new visual domains. 5.3 Image Restoration and Co-Training Image restoration has been extensively studied to recover clean visual signals from degraded inputs, with applications spanning denoising, deblurring, and super-resolution Zhang et al. [2017], Dong et al. [2016], Zamir et al. [2021]. Recent works have leveraged deep learning to achieve high-quality restoration, often optimizing perceptual fidelity using adversarial losses Ledig et al. [2017] or learned priors Ulyanov et al. [2018], Saha et al. [2021]. However, such methods typically aim to match human perception, assuming that cleaner images universally benefit downstream tasks. Contrary to this assumption, recent studies suggest that restored images may not always align with task-specific model preferences, particularly in the context of vision-language models (VLMs) Bai et al. [2023b]. This observation motivates shift from traditional restoration objectives toward taskaware or model-centric enhancement. Some recent approaches explore joint training of restoration and recognition networks Li et al. [2023a], Yang et al. [2020], or co-training paradigms that integrate auxiliary tasks to improve robustness Zhang et al. [2022]."
        },
        {
            "title": "6 Conclusion",
            "content": "Our study reveals that higher perceptual visual quality does not always lead to better understanding in MLLMs, challenging conventional assumptions. Through systematic analysis across diverse models and benchmarks, we uncover visual-quality paradox, where degraded or stylistically altered 9 inputs can unexpectedly enhance performance. To address this, we propose VQ-TTT, lightweight and adaptive test-time tuning strategy that aligns input images with model-specific preferences using minimal learnable parameters. VQ-TTT delivers consistent gains across wide range of models and tasks without relying on additional data or training. These results underscore the importance of rethinking input preparation in vision-language systems and advocate for adaptive, task-aware visual processing tailored to MLLMs internal biases."
        },
        {
            "title": "References",
            "content": "A. Abdelhamed, S. Lin, and M. S. Brown. high-quality denoising dataset for smartphone cameras. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1692 1700, 2018. M. Abdin, J. Aneja, H. Awadalla, A. Awadallah, A. A. Awan, N. Bach, A. Bahree, A. Bakhtiari, J. Bao, H. Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. A. Abouelenin, A. Ashfaq, A. Atkinson, H. Awadalla, N. Bach, J. Bao, A. Benhaim, M. Cai, V. Chaudhary, C. Chen, et al. Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras. arXiv preprint arXiv:2503.01743, 2025. H. Agrawal, K. Desai, Y. Wang, X. Chen, R. Jain, M. Johnson, D. Batra, D. Parikh, S. Lee, and In Proceedings of the IEEE/CVF P. Anderson. Nocaps: Novel object captioning at scale. international conference on computer vision, pages 89488957, 2019. J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023a. S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Y. Bai, X. Zhang, Y. Wang, Y. Fu, L. Yuan, and J. Gao. Align before generate: Vision-language pretraining with contrastive learning and knowledge distillation. arXiv preprint arXiv:2302.14045, 2023b. L. Beyer, A. Steiner, A. S. Pinto, A. Kolesnikov, X. Wang, D. Salz, M. Neumann, I. Alabdulmohsin, M. Tschannen, E. Bugliarello, et al. Paligemma: versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726, 2024. T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. K. Chen, S. Xie, Z. Ma, and K. Goldberg. Robo2vlm: Visual question answering from large-scale in-the-wild robot manipulation datasets. arXiv preprint arXiv:2505.15517, 2025. L. Chen, X. Chu, X. Zhang, and J. Sun. Simple baselines for image restoration. In European conference on computer vision, pages 1733. Springer, 2022. X. Chen, H. Fang, T.-Y. Lin, R. Vedantam, S. Gupta, P. DollÃ¡r, and C. L. Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. C. Dong, C. C. Loy, K. He, and X. Tang. Image super-resolution using deep convolutional networks. In IEEE Transactions on Pattern Analysis and Machine Intelligence, volume 38, pages 295307. IEEE, 2016. C. Fu, P. Chen, Y. Shen, Y. Qin, M. Zhang, X. Lin, J. Yang, X. Zheng, K. Li, X. Sun, Y. Wu, and R. Ji. MME: Comprehensive Evaluation Benchmark for Multimodal Large Language Models. arXiv, June 2023. doi: 10.48550/arXiv.2306.13394. 10 D. Gurari, Q. Li, A. J. Stangl, A. Guo, C. Lin, K. Grauman, J. Luo, and J. P. Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 36083617, 2018. C.-W. Hu, Y. Wang, S. Xing, C.-J. Chen, and Z. Tu. mrag: Elucidating the design space of multi-modal retrieval-augmented generation. arXiv preprint arXiv:2505.24073, 2025. D. A. Hudson and C. D. Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709, 2019. M. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. Foster, G. Lam, P. Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. C. Ledig, L. Theis, F. HuszÃ¡r, J. Caballero, A. Cunningham, A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, and W. Shi. Photo-realistic single image super-resolution using generative adversarial network. CVPR, 2017. C. Li, C. Wong, S. Zhang, N. Usuyama, H. Liu, J. Yang, T. Naumann, H. Poon, and J. Gao. Llava-med: Training large language-and-vision assistant for biomedicine in one day. Advances in Neural Information Processing Systems, 36, 2024a. F. Li, Y. Wang, Q. Sun, L. Van Gool, and R. Timofte. Joint image restoration and recognition using single model. In CVPR, 2023a. F. Li, R. Zhang, H. Zhang, Y. Zhang, B. Li, W. Li, Z. Ma, and C. Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024b. H. Li, Z. Zhang, J. Liu, X. Wang, J. Wu, and Y. Wang. Visual prompt tuning for adapting visionlanguage models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024c. J. Li, D. Li, C. Xiong, and S. Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 1288812900. PMLR, 2022. J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023b. P. Li, X. Zou, Z. Wu, R. Li, S. Xing, H. Zheng, Z. Hu, Y. Wang, H. Li, Q. Yuan, et al. Safeflow: principled protocol for trustworthy and transactional autonomous agent systems. arXiv preprint arXiv:2506.07564, 2025. X. Lin, J. He, Z. Chen, Z. Lyu, B. Dai, F. Yu, Y. Qiao, W. Ouyang, and C. Dong. Diffbir: Toward blind image restoration with generative diffusion prior. In European Conference on Computer Vision, pages 430448. Springer, 2024. H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. URL https://arxiv.org/abs/2304.08485. H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. H. Lu, W. Liu, B. Zhang, B. Wang, K. Dong, B. Liu, J. Sun, T. Ren, Z. Li, H. Yang, et al. Deepseek-vl: towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024a. P. Lu, S. Mishra, T. Xia, L. Qiu, K.-W. Chang, S.-C. Zhu, O. Tafjord, P. Clark, and A. Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS), 2022. P. Lu, H. Bansal, T. Xia, J. Liu, C. Li, H. Hajishirzi, H. Cheng, K.-W. Chang, M. Galley, and J. Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In International Conference on Learning Representations (ICLR), 2024b. 11 X. Luo, F. Yang, F. Ding, X. Gao, S. Xing, Y. Zhou, Z. Tu, and C. Liu. V2x-unipool: Unifying multimodal perception and knowledge reasoning for autonomous driving. arXiv preprint arXiv:2506.02580, 2025. Z. Luo, F. K. Gustafsson, Z. Zhao, J. SjÃ¶lund, and T. B. SchÃ¶n. Controlling vision-language models for multi-task image restoration. arXiv preprint arXiv:2310.01018, 2023a. Z. Luo, F. K. Gustafsson, Z. Zhao, J. SjÃ¶lund, and T. B. SchÃ¶n. Image restoration with mean-reverting stochastic differential equations. arXiv preprint arXiv:2301.11699, 2023b. Y. Ma, W. Ye, C. Cui, H. Zhang, S. Xing, F. Ke, J. Wang, C. Miao, J. Chen, H. Rezatofighi, et al. Position: Prospective of autonomous driving-multimodal llms world models embodied intelligence ai alignment and mamba. In Proceedings of the Winter Conference on Applications of Computer Vision, pages 10101026, 2025. Meta."
        },
        {
            "title": "Llama",
            "content": "3.2:"
        },
        {
            "title": "Revolutionizing",
            "content": "edge ai tomizable models. llama-3-2-connect-2024-vision-edge-mobile-devices/. 2024."
        },
        {
            "title": "URL",
            "content": "and cushttps://ai.meta.com/blog/ vision with open, C. Michaelis, B. Mitzkus, R. Geirhos, E. Rusak, O. Bringmann, A. S. Ecker, M. Bethge, and W. Brendel. Benchmarking robustness in object detection: Autonomous driving when winter is coming. arXiv preprint arXiv:1907.07484, 2019. M. Moor, Q. Huang, S. Wu, M. Yasunaga, Y. Dalmia, J. Leskovec, C. Zakka, E. P. Reis, and P. Rajpurkar. Med-flamingo: multimodal medical few-shot learner. In Machine Learning for Health (ML4H), pages 353367. PMLR, 2023. S. Nah, T. Hyun Kim, and K. Mu Lee. Deep multi-scale convolutional neural network for dynamic scene deblurring. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 38833891, 2017. nostalgebraist. interpreting gpt: the logit lens. 2020. URL https://www.lesswrong.com/ posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens. A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, In J. Clark, et al. Learning transferable visual models from natural language supervision. International conference on machine learning, pages 87488763. PMLR, 2021. C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. K. Rana, J. Haviland, S. Garg, J. Abou-Chakra, I. Reid, and N. Suenderhauf. Sayplan: Grounding large language models using 3d scene graphs for scalable robot task planning. In 7th Annual Conference on Robot Learning, 2023. B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023. S. Saha, A. Sinha, and S. Bandyopadhyay. Image restoration using very deep convolutional encoderdecoder networks with symmetric skip connections. In Pattern Recognition Letters, volume 138, pages 185191. Elsevier, 2021. H. Shao, Y. Hu, L. Wang, G. Song, S. L. Waslander, Y. Liu, and H. Li. Lmdrive: Closed-loop end-to-end driving with large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1512015130, 2024. C. Sima, K. Renz, K. Chitta, L. Chen, H. Zhang, C. Xie, P. Luo, A. Geiger, and H. Li. Drivelm: Driving with graph visual question answering. arXiv preprint arXiv:2312.14150, 2023. 12 A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83178326, 2019a. A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83178326, 2019b. Q. Sun, A. Tzamarias, and B. Schiele. Test-time training with self-supervision for generalization under distribution shifts. In International Conference on Machine Learning (ICML), pages 92299248, 2020. G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Q. Team. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm. github.io/blog/qwen2.5/. X. Tian, J. Gu, B. Li, Y. Liu, C. Hu, Y. Wang, K. Zhan, P. Jia, X. Lang, and H. Zhao. Drivevlm: The convergence of autonomous driving and large vision-language models. arXiv preprint arXiv:2402.12289, 2024. H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. RoziÃ¨re, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. D. Ulyanov, A. Vedaldi, and V. Lempitsky. Deep image prior. CVPR, 2018. D. Wang, J. Bao, X. Dong, J.-Y. Zhu, and J. E. Gonzalez. Tent: Fully test-time adaptation by entropy minimization. In International Conference on Learning Representations (ICLR), 2021. P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K. Chen, X. Liu, J. Wang, W. Ge, Y. Fan, K. Dang, M. Du, X. Ren, R. Men, D. Liu, C. Zhou, J. Zhou, and J. Lin. Qwen2-vl: Enhancing visionlanguage models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. Y. Wang, S. Xing, C. Can, R. Li, H. Hua, K. Tian, Z. Mo, X. Gao, K. Wu, S. Zhou, et al. Generative ai for autonomous driving: Frontiers and opportunities. arXiv preprint arXiv:2505.08854, 2025. Z. Wu, X. Chen, Z. Pan, X. Liu, W. Liu, D. Dai, H. Gao, Y. Ma, C. Wu, B. Wang, et al. Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302, 2024. S. Xing, H. Hua, X. Gao, S. Zhu, R. Li, K. Tian, X. Li, H. Huang, T. Yang, Z. Wang, Y. Zhou, H. Yao, and Z. Tu. AutoTrust: Benchmarking Trustworthiness in Large Vision Language Models for Autonomous Driving. arXiv, Dec. 2024a. doi: 10.48550/arXiv.2412.15206. S. Xing, C. Qian, Y. Wang, H. Hua, K. Tian, Y. Zhou, and Z. Tu. Openemma: Open-source multimodal model for end-to-end autonomous driving. arXiv, Dec. 2024b. doi: 10.48550/arXiv.2412.15208. S. Xing, Z. Sun, S. Xie, K. Chen, Y. Huang, Y. Wang, J. Li, D. Song, and Z. Tu. Can large vision language models read maps like human? arXiv preprint arXiv:2503.14607, 2025a. S. Xing, Y. Wang, P. Li, R. Bai, Y. Wang, C. Qian, H. Yao, and Z. Tu. Re-align: Aligning vision language models via retrieval-augmented direct preference optimization. arXiv preprint arXiv:2502.13146, 2025b. 13 A. Yang, B. Yang, B. Hui, B. Zheng, B. Yu, C. Zhou, C. Li, C. Li, D. Liu, F. Huang, G. Dong, H. Wei, H. Lin, J. Tang, J. Wang, J. Yang, J. Tu, J. Zhang, J. Ma, J. Xu, J. Zhou, J. Bai, J. He, J. Lin, K. Dang, K. Lu, K. Chen, K. Yang, M. Li, M. Xue, N. Ni, P. Zhang, P. Wang, R. Peng, R. Men, R. Gao, R. Lin, S. Wang, S. Bai, S. Tan, T. Zhu, T. Li, T. Liu, W. Ge, X. Deng, X. Zhou, X. Ren, X. Zhang, X. Wei, X. Ren, Y. Fan, Y. Yao, Y. Zhang, Y. Wan, Y. Chu, Y. Liu, Z. Cui, Z. Zhang, and Z. Fan. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. C. Yang, Q. Li, M. Luo, F. Wu, and C. Xu. Multi-task learning for image super-resolution with auxiliary tasks. In ECCV, 2020. Z. Yang, W. Ping, Z. Liu, V. Korthikanti, W. Nie, D.-A. Huang, L. Fan, Z. Yu, S. Lan, B. Li, et al. Re-vilm: Retrieval-augmented visual language model for zero and few-shot image captioning. arXiv preprint arXiv:2302.04858, 2023. M. Yasunaga, A. Aghajanyan, W. Shi, R. James, J. Leskovec, P. Liang, M. Lewis, L. Zettlemoyer, and W.-t. Yih. Retrieval-augmented multimodal language modeling. arXiv preprint arXiv:2211.12561, 2022. F. Yu, J. Gu, Z. Li, J. Hu, X. Kong, X. Wang, J. He, Y. Qiao, and C. Dong. Scaling up to excellence: Practicing model scaling for photo-realistic image restoration in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2566925680, 2024. X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, C. Wei, B. Yu, R. Yuan, R. Sun, M. Yin, B. Zheng, Z. Yang, Y. Liu, W. Huang, H. Sun, Y. Su, and W. Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR, 2024. S. W. Zamir, A. Arora, S. H. Khan, M. Hayat, F. Khan, M. Yang, and L. Shao. Restormer: Efficient transformer for high-resolution image restoration. In CVPR, 2021. H. Zhang, Y. Liu, M. Tan, Q. Le, and A. A. Yu. Test-time adaptation for large language models via entropy minimization and alignment tuning. arXiv preprint arXiv:2305.07026, 2023a. J. Zhang, M. Khayatkhoei, P. Chhikara, and F. Ilievski. Mllms know where to look: Training-free perception of small visual details with multimodal llms. arXiv preprint arXiv:2502.17422, 2025. K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang. Beyond gaussian denoiser: Residual learning of deep cnn for image denoising. IEEE Transactions on Image Processing, 26(7):31423155, 2017. Y. Zhang, P. Sun, Y. Jiang, D. Yu, C. Weng, Z. Yuan, P. Luo, and T. Kong. Bytetrack: Multi-object tracking by associating every detection box. In ECCV, 2022. Y. Zhang, Q. Wang, P. Xie, J. Lin, and X. E. Wang. Test-time adaptation of vision-language models with cross-modal consistency. arXiv preprint arXiv:2310.10039, 2023b. R. Zhu, Z. Tu, J. Liu, A. C. Bovik, and Y. Fan. Mwformer: Multi-weather image restoration using degradation-aware transformers. IEEE Transactions on Image Processing, 2024."
        },
        {
            "title": "A Details of the Utilized Restoration Models",
            "content": "NAFNet [Chen et al., 2022] is transformer-based image restoration model that eliminates traditional nonlinear activation functions such as ReLU, GELU, and softmax, replacing them with the simpler SimpleGate mechanism, an element-wise multiplication operation. This design yields lightweight yet highly effective image restoration framework. The original authors trained NAFNet on the GoPro Nah et al. [2017] dataset for image deblurring tasks and on the SIDD Abdelhamed et al. [2018] dataset for image denoising tasks. In our experiments, we leverage the publicly available pretrained checkpoints to produce restored images for deblurring and denoising. In the subsequent table, we denote images restored by NAFNet as RN . 14 MWFormer [Zhu et al., 2024] is transformer-based model specifically designed for restoring images degraded by various adverse weather conditions. It integrates degradation-aware mechanisms that adaptively handle different weather-induced distortions. In our experiments, we apply MWFormer to restore images affected by snow and fog degradations. In the subsequent table, we denote images restored by MWFormer as RM. SUPIR [Yu et al., 2024] is diffusion-based image restoration model that leverages textual guidance and is trained on dataset comprising 20 million high-resolution, high-quality images. It incorporates low-quality samples into the training process and employs textual prompts to guide the model away from negative visual attributes, thus enhancing visual quality. In our experiments, SUPIR is utilized specifically for image deblurring tasks. In the subsequent table, we denote images restored by SUPIR as RS . Diffbir [Lin et al., 2024] introduces two-staged restoration pipeline that decouples the restoration removal and information regeneration. It utilizes generative diffusion priors to reconstruct highfidelity images from degraded inputs. In our experiment, we utilize it to restore images degraded by Gaussian noise, denoted as RDB. Daclip Luo et al. [2023a] integrates the CLIP model with diffusion-based restoration network, IR-SDE Luo et al. [2023b], to handle various degradation. It also introduced cross-attention mechanism to injects the content embedding into the diffusion process. In our experiments, Daclip serves as an additional restoration model for images degraded by snow and fog, denoted in the subsequent table as RDC."
        },
        {
            "title": "B Additional Experiments and Results",
            "content": "B.1 Evaluation of Additional Backbone Models with Degraded Visual Inputs In this section, we provide the evaluation results for additional MLLM backbones on MMMU and MathVista with the degraded image input. Model MathVista ALL FQA GPS MWP TQA VQA ALG ARI GEO LOG NUM SCI STA 54.9 61.8 42.7 45.2 Original 54.1 54.8 40.2 Gaussian Noise 42.0 47.5 26.9 40.9 32.5 Motion Blur 52.5 26.2 40.6 32.6 Defocus Blur 55.7 56.5 42.0 42.2 Snow 54.9 61.8 42.7 45.2 Fog 41.8 31.2 39.9 31.2 43.8 15.6 42.3 15.1 43.3 29.0 41.8 31.2 34.6 41.0 32.6 40.6 22.1 39.7 22.9 38.5 31.2 41.4 34.6 41. 8.1 13.5 10.8 13.5 5.4 8.1 54.4 51.3 44.9 48.1 50.6 54.4 29.2 29.2 27.8 26.4 25.7 29.2 57.6 50.6 27.9 26.8 51.3 57.6 36.9 34.6 33.0 34.6 33.5 36.9 Table 3: Performance of Phi-3-Vision under common image degradations. Model MathVista ALL FQA GPS MWP TQA VQA ALG ARI GEO LOG NUM SCI STA 56.6 52.8 40.6 Original 44.0 Gaussian Noise 41.6 54.1 50.5 37.4 54.9 30.6 36.7 34.7 Motion Blur 56.6 27.6 37.0 33.8 Defocus Blur 58.2 45.8 39.1 41.6 Snow Fog 56.6 52.8 40.6 44.0 38.5 33.9 35.1 34.9 37.5 17.7 37.0 15.1 38.9 34.4 38.5 33.9 37.4 38.1 36.0 35.1 26.3 36.0 25.5 33.5 35.1 37.7 37.4 38.1 13.5 13.5 16.2 16.2 18.9 13.5 35.4 32.6 29.2 29.2 31.9 35. 55.7 53.2 48.1 50.6 53.8 55.7 48.7 45.0 33.8 31.2 43.1 48.7 43.6 40.8 38.5 38.5 39.1 43.6 Table 4: Performance of Phi-3.5-Vision under common image degradations. B.2 Evaluation with Diffusion-Based Restoration Models Table 7 presents the complete evaluation results for LLaVA-v1.5-7B [Liu et al., 2023], LLaVAv1.6-Mistral-7B [Li et al., 2024b], and Qwen-2.5-VL-3B-instruct [Team, 2024] under common degradations, as well as transformer-based and diffusion-based model restorations. 15 Model Overall Original Gaussian Noise Motion Blur Defocus Blur Snow Fog 38.5 38.0 37.0 37.1 37.6 38.5 Art & Design 50.0 45.8 47.1 47.5 45.3 50. MMMU Business Science 33.5 32.9 31.9 31.0 32.6 33.5 30.2 30.0 29.1 29.0 29.7 30.2 Health & Medicine 41.0 41.5 41.0 41.3 41.6 41.0 Humanities & Social Science 62.2 61.7 60.5 60.8 61.4 62. Tech & Engineering 34.0 34.0 31.6 32.3 33.4 34.0 Table 5: Performance of Phi-3-Vision under common image degradations. Model Overall Original Gaussian Noise Motion Blur Defocus Blur Snow Fog 38.5 37.8 37.1 37.2 37.2 38. Art & Design 50.5 47.8 46.7 46.4 45.1 50.5 MMMU Business Science 32.0 31.7 31.1 30.3 31.4 32.0 30.8 30.0 29.5 30.5 30.0 30.8 Health & Medicine 41.6 40.8 41.1 39.8 40.6 41. Humanities & Social Science 62.0 60.6 59.1 60.4 60.8 62.0 Tech & Engineering 33.6 33.9 32.6 33.0 33.0 33.6 Table 6: Performance of Phi-3.5-Vision under common image degradations."
        },
        {
            "title": "C Examples of the Input Images with Degradations and Restorations",
            "content": "Examples of the input images with degradations and restorations are provided in Figure 5."
        },
        {
            "title": "D Broader Impacts",
            "content": "The social impacts of our work are mainly in three folds: 1. Accessibility: This work provides lightweight and training-free framework to improve the performance of Multimodal Large Language Models (MLLMs) across wide range of visual inputs. By adapting input images on-the-fly without requiring high-end GPUs, external models, or curated datasets, VQ-TTT enables broader and more inclusive use of MLLMs in real-world scenarios, especially where image quality is suboptimal or device capabilities are limited. 2. Applications: Our findings challenge the conventional assumption that higher perceptual fidelity always leads to better machine understanding, with implications for education, accessibility technologies, content moderation, and human-AI collaboration. VQ-TTT can improve MLLM reliability in diverse environments such as mobile photography, remote sensing, telemedicine, and surveillancewhere visual degradation is common and high accuracy is critical. 3. Open Access: We are committed to open-sourcing our code and models to encourage further exploration of visual quality modulation in MLLMs. By doing so, we aim to support transparency, reproducibility, and collective progress in building adaptive, robust vision-language systems that serve wide range of users and use cases. We do not foresee serious negative societal impacts from this work. Our method operates within the input space and enhances model alignment without altering foundational model behavior or generating misleading content. Instead, it highlights the importance of understanding and adapting to AI models perceptual biases for more reliable deployment. Model MathVista MMMU ScienceQAT ScienceQAI TextVQA MMEP MMEC LLaVA-v1.5-7B +Gaussian Noise +Gaussian Noise+RN +Gaussian Noise+RDB +Motion Blur +Motion Blur+RN +Motion Blur+RS +Defocus Blur +Defocus Blur+RN +Defocus Blur+RS +Snow +Snow+RM +Snow+RDC +Fog +Fog+RM +Fog+RDC LLaVa-v1.6-Mistral-7B +Gaussian Noise +Gaussian Noise+RN +Gaussian Noise+RDB +Motion Blur +Motion Blur+RN +Motion Blur+RS +Defocus Blur +Defocus Blur+RN +Defocus Blur+RS +Snow +Snow+RM +Snow+RDC +Fog +Fog+RM +Fog+RDC Qwen-2.5-VL-3B-instruct +Gaussian Noise +Gaussian Noise+RN +Gaussian Noise+RDB +Motion Blur +Motion Blur+RN +Motion Blur+RS +Defocus Blur +Defocus Blur+RN +Defocus Blur+RS +Snow +Snow+RM +Snow+RDC +Fog +Fog+RM +Fog+RDC VLM-R1 +Gaussian Noise +Gaussian Noise+RN +Gaussian Noise+RDB +Motion Blur +Motion Blur+RN +Motion Blur+RS +Defocus Blur +Defocus Blur+RN +Defocus Blur+RS +Snow +Snow+RM +Snow+RDC +Fog +Fog+RM +Fog+RDC 23.3 24.2 17.2 16.6 24.4 17.8 23.5 24.4 16.6 24.3 24.1 23.1 23.5 24.8 23.8 23.7 26.5 29.8 27.4 25.0 25.2 27.8 23.9 25.7 25.8 25.5 28.7 26.5 26.1 30.7 25.4 27.3 61.6 57.5 50.2 42.1 56.6 48.6 39.2 56.5 40.0 36.8 52.9 45.6 45.5 61.6 48.8 48.2 63.1 60.7 53.3 42.0 57.5 49.2 38.9 59.0 41.2 38.9 53.3 47.6 47.2 63.2 52.3 50.2 28.7 28.6 31.0 33.1 29.5 31.0 33.1 29.5 31.0 33.1 28.2 31.0 33.1 28.6 31.0 33. 34.2 33.7 34.8 34.7 33.6 22.3 34.7 34.2 22.3 34.7 33.7 22.3 34.7 34.2 34.8 34.7 43.7 42.1 40.2 40.6 41.4 41.3 40.6 39.7 39.0 39.8 41.8 40.5 41.8 43.2 42.1 43.9 44.2 43.0 42.6 40.2 42.7 42.3 39.9 41.6 40.5 39.8 42.3 41.9 41.8 44.2 43.4 44.1 66.02 66.38 65.95 65.53 66.12 66.14 66.23 66.19 66.42 66.09 65.36 66.05 65.76 65.39 65.83 65.64 76.02 76.44 75.95 75.78 76.09 76.09 75.88 76.4 76.63 76.09 76.00 76.11 75.95 76.00 76.23 76.26 75.71 73.90 37.92 37.99 74.02 38.01 38.03 73.97 37.96 37.77 73.90 37.87 37.92 74.89 37.82 37. 69.18 67.58 32.92 32.92 67.81 32.92 32.92 67.13 32.92 32.92 67.06 32.92 32.92 68.17 32.92 32.92 64.85 65.59 64.70 63.81 65.05 65.10 65.25 65.20 65.69 64.95 63.46 64.85 64.25 63.51 64.40 64.01 71.34 72.24 71.24 70.90 71.49 71.54 71.1 72.14 72.68 71.54 71.29 71.59 71.24 71.29 71.84 71.89 79.28 75.81 0.00 0.00 75.81 0.00 0.00 75.66 0.00 0.00 75.41 0.00 0.00 77.54 0.00 0.00 75.36 71.99 0.00 0.00 72.48 0.00 0.00 71.05 0.00 0.00 70.90 0.00 0.00 73.23 0.00 0.00 58.18 56.53 56.10 50.38 54.33 56.77 50.50 54.13 53.76 49.67 53.24 53.85 53.26 57.13 56.68 53. 63.80 59.09 58.66 44.25 55.13 59.35 44.26 51.54 51.57 44.63 55.39 56.38 49.55 62.65 61.16 50.16 77.89 65.50 69.28 38.28 61.39 70.33 38.29 54.18 53.86 40.36 64.61 65.35 48.95 73.69 73.62 51.83 76.58 64.96 68.70 37.72 60.90 69.85 37.91 53.84 52.89 40.16 62.99 64.27 48.66 72.55 72.68 51.53 1510.28 1431.58 1419.18 1367.62 1454.98 1473.19 1465.72 1435.93 1435.69 1365.60 1405.89 1391.24 1414.90 1446.64 1429.61 1412.66 1494.22 1461.04 1438.57 1366.93 1454.99 1474.13 1411.04 1395.71 1371.05 1372.05 1429.05 1416.26 1400.71 1464.83 1477.21 1404.33 1515.32 1430.75 1439.65 1352.65 1396.84 1502.61 1394.83 1346.74 1399.06 1380.93 1427.12 1443.55 1286.25 1499.79 1478.08 1396. 1523.19 1388.92 1436.39 1375.77 1487.96 1487.96 1411.46 1396.31 1390.23 1379.95 1360.11 1416.31 1292.01 1487.00 1478.15 1355.76 357.85 341.78 345.00 315.36 361.42 317.50 323.57 326.07 347.86 335.71 330.35 315.36 331.07 342.85 350.71 322.14 323.92 307.14 303.57 270.36 339.64 298.21 301.07 275.71 285.36 309.64 305.71 320 331.43 307.14 304.29 315.71 615.00 597.14 580.00 568.57 574.64 557.50 532.50 555.35 530.71 487.86 554.28 581.43 541.07 605.71 626.43 600.36 658.21 636.42 616.07 577.86 586.79 586.79 509.29 545.35 510.00 483.57 571.42 597.5 548.57 612.85 618.93 575.00 Table 7: Performance of MLLMs across common degradations and restorations. 17 Figure 5: Examples of the images with degradations and restorations."
        },
        {
            "title": "E Limitations",
            "content": "While VQ-TTT is designed to be lightweight and efficient, its simplicity also introduces limitations. To enable fast adaptation across diverse inputs, we intentionally restrict the number of tunable parameters through low-rank modulation and shallow-layer LoRA updates. However, this compact design may limit the models capacity to handle complex or highly nonlinear degradation patterns. In such cases, generalization performance may degrade, especially when the distortions fall outside the scope of the training-time assumptions. Additionally, our current evaluation is limited in scope. While we benchmark across several representative MLLMs and datasets, the coverage is not exhaustive. The number of tested models, vision-language tasks, and degradation types remains relatively narrow. Future work should investigate broader model families, more diverse real-world datasets, and wider spectrum of degradation modalities (e.g., motion blur, occlusion, sensor noise) to more comprehensively assess the robustness and applicability of VQ-TTT."
        }
    ],
    "affiliations": [
        "Nanyang Technological University",
        "Texas A&M University",
        "University of Texas at Austin",
        "University of Toronto"
    ]
}