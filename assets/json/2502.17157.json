{
    "paper_title": "DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks",
    "authors": [
        "Canyu Zhao",
        "Mingyu Liu",
        "Huanyi Zheng",
        "Muzhi Zhu",
        "Zhiyue Zhao",
        "Hao Chen",
        "Tong He",
        "Chunhua Shen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Our primary goal here is to create a good, generalist perception model that can tackle multiple tasks, within limits on computational resources and training data. To achieve this, we resort to text-to-image diffusion models pre-trained on billions of images. Our exhaustive evaluation metrics demonstrate that DICEPTION effectively tackles multiple perception tasks, achieving performance on par with state-of-the-art models. We achieve results on par with SAM-vit-h using only 0.06% of their data (e.g., 600K vs. 1B pixel-level annotated images). Inspired by Wang et al., DICEPTION formulates the outputs of various perception tasks using color encoding; and we show that the strategy of assigning random colors to different instances is highly effective in both entity segmentation and semantic segmentation. Unifying various perception tasks as conditional image generation enables us to fully leverage pre-trained text-to-image models. Thus, DICEPTION can be efficiently trained at a cost of orders of magnitude lower, compared to conventional models that were trained from scratch. When adapting our model to other tasks, it only requires fine-tuning on as few as 50 images and 1% of its parameters. DICEPTION provides valuable insights and a more promising solution for visual generalist models."
        },
        {
            "title": "Start",
            "content": "DICEPTION: Generalist Diffusion Model for Visual Perceptual Tasks Canyu Zhao1, Mingyu Liu1,2,* Huanyi Zheng1 Muzhi Zhu1 Zhiyue Zhao1 Hao Chen1 Tong He2 Chunhua Shen1 1 Zhejiang University 2 Shanghai AI Laboratory 5 2 0 2 4 ] . [ 1 7 5 1 7 1 . 2 0 5 2 : r Figure 1: We propose generalist diffusion model solving multiple perception tasks. Here we show the overall pipeline of the proposed DICEPTION. At each denoising step, the point embedding, input image latent, and task embedding remain fixed, while only the noise latent is updated."
        },
        {
            "title": "Abstract",
            "content": "Our primary goal here is to create good, generalist perception model that can tackle multiple tasks, within limits on computational resources and training data. To achieve this, we resort to text-to-image diffusion models pre-trained on billions of images and introduce our visual generalist model: DICEPTION. Our exhaustive evaluation metrics demonstrate that DICEPTION effectively tackles multiple perception tasks, achieving performance on par with state-of-the-art models. We achieve results on par with SAM-vit-h using only 0.06% of their data (e.g., 600K vs. 1B pixel-level annotated images). Inspired by Wang et al. [109], DICEPTION formulates the outputs of various perception tasks using color encoding; and we show that the strategy of assigning random colors to different instances is highly effective in both entity segmentation and semantic segmentation. Unifying various perception tasks as conditional image generation enables us to fully leverage pre-trained text-to-image models. Thus, DICEPTION *Equal Contribution can be efficiently trained at cost of orders of magnitude lower, compared to conventional models that were trained from scratch. When adapting our model to other tasks, it only requires fine-tuning on as few as 50 images and 1% of its parameters. DICEPTION provides valuable insights and more promising solution for visual generalist models. Project webpage, and huggingface demo are available. 1. Introduction Foundation models [49, 87, 118, 119, 117, 11, 7, 83, 75, 91, 6, 39], typically requiring extensive training on billions of data samples, play pivotal role in their respective domains. In natural language processing (NLP), current foundation models [9, 99, 100, 26] have already demonstrated the potential to serve as versatile solutions, solving diverse fundamental tasks and with minimal fine-tuning needed for new tasks. This success can be attributed to the relatively small representational differences among various language tasks. However, in the domain of computer vision, task representations can differ substantially, and up to date, we still Figure 2: With one single model, DICEPTION solves multiple tasks without relying on any task-specific modules (rows 1 to 3). The red dots in the figure indicate the input points used for point-prompted segmentation. DICEPTION preserves fine details in segmentation, such as hair (row 4). DICEPTION supports both human pose estimation and semantic segmentation (row 5, 6). DICEPTION can quickly adapt to new tasks by fine-tuning less than 1% of its parameters on as few as 50 images (row 7). For additional visualizations, please refer to Figures S1, S2, S4, S6, S7, S8, S9, S10, S11, S12, S13, S14 in the Appendix. 2 lack an effective approach to unify these distinct tasks. Consequently, existing vision foundation models usually excel at one single specific task, such as image segmentation [49, 87] or monocular depth estimation [118, 119, 117], because they are trained on data tailored exclusively to that task. Owing to the pronounced disparity in visual representations across tasks, coupled with the single-task specialization that characterizes current vision foundation models, fine-tuning these models for new tasks remains formidable challenge. Although some efforts [12, 75, 39, 88] have been made to learn universal visual representations for more generalized vision foundation models, their performance still falls noticeably short compared to specialized models. Recent studies [109, 69, 68, 72, 2, 113] on visual generalist models are predominantly trained from scratch, often requiring substantial computational resources and large datasets to achieve good results. Unfortunately, the price of collecting sufficiently large and high-quality multi-task dataset is substantial. Here, inspired by the success of diffusion models, we propose the hypothesis that leveraging their powerful priors can help mitigate the significant computational and data overhead for training powerful generalist models. While some existing works [47, 114, 38, 123, 93] have demonstrated that this is feasible in single-task scenarios, the potential of diffusion model priors in multi-task settings remains largely under-explored. In this paper, we successfully leverage the priors of diffusion models to achieve results on par or even better with the state-of-the-art models on various tasks with only minimal training data. We name our powerful visual generalist model DICEPTION. For each task, we require substantially less data than specialized foundation models. For instance, compared to SAM segmentation, DICEPTION achieves comparable performance using significantly smaller dataset of 600K samples, without any selective sampling. This is in sharp contrast to SAM which relies on massive dataset of 1 billion pixel-level annotated samples. This demonstrates the efficiency and robustness of DICEPTION, as it requires only 0.06% of the data used by SAM while still producing competitive results. The concurrent work One Diffusion [53] proposes unified model for image generation by treating different images as different views. However, their approach fails to distinguish between conditions and images, greatly limiting its effectiveness in perception tasks. More critically, their from-scratch training demands substantial data and computational resources. In contrast, our work focuses on image perception rather than generation, achieving on-par state-ofthe-art performance in perception tasks. See more details in Appendix 2.4. DICEPTION highlights that the generative image priors lead to surprisingly more efficient and effective pathways to generalist image understanding models. The entire training process is remarkably stable, eliminating the need to design complex training recipe. Even more notably, DICEPTION is capable of quickly adapting to new tasks using as few as 50 training images and fine-tuning less than 1% of its parameters. Moreover, we show that previously ineffective strategies in generalist models such as task control via prompts [113] and random color assignments for segmentation masks [109] are viable in our framework. Additionally, we investigate the role of 1-step denoising in multi-task settings, technique proven beneficial in singletask settings. DICEPTION provides valuable insights for the design and training of strong visual generalist models. In summary, our main contributions are as follows. We introduce DICEPTION, generalist model capable of performing multiple visual perception tasks. Extensive experiments demonstrate that DICEPTION can effectively address various tasks, achieving or par accuracy with state-of-the-art specialist models. When trained on new tasks, DICEPTION achieves high-quality results with minimal fine-tuning. Using only small number of images (even with 50 images), the model can achieve exceptional performance by fine-tuning less than 1% of its parameters. We conduct thorough set of experiments exploring various aspects of multitask models, including network architectures, and 1-step diffusion. We believe that our experimental findings offer valuable insights and will significantly benefit future research. 2. Related Work 2.1. Vision Foundation Models Vision foundation models are models that are trained on large-scale datasets and demonstrate excellent performance within their trained domains. Vision foundation models now exist for broad range of vision tasks, including monocular depth estimation [118, 119, 117, 7], object detection [11], segmentation [49, 87], multimodal tasks [83, 64], image and video generation [91, 28, 6], and more recently, emerging 3D models [105, 70]. While many works [110, 48, 58, 84, 133, 136] have sought to leverage the prior knowledge embedded in these models to tackle other tasks, such efforts often require complex network designs and intricate training strategies, typically transferring only to limited number of tasks. Some foundation models [88, 39, 75, 12] emphasize representation learning, aiming to solve diverse downstream tasks by relying on generalized features. However, the results of these methods often fall short when compared with specialized foundation models. In contrast, our approach ensures consistent accuracy across multiple tasks while also enabling swift adaptation to new downstream tasks. 3 2.2. Diffusion Models Diffusion models [28, 91, 6] have achieved remarkable success in image and video generation in recent years. The idea is to gradually add noise to the data and train model to reverse this process, denoising step by step to generate the result. Recent diffusion models [28] utilize flow matching [63, 1, 66] and the DiT architecture [77], making them more scalable and efficient. Diffusion models have enabled wide range of notable applications, including conditional image generation [130, 61, 124, 73, 82], image editing [8, 46, 116], story generation [107, 135], video generation [40, 35, 132, 122, 6, 50, 106], and video editing [13, 65, 14]. These successes underscore the substantial prior knowledge embedded in diffusion models. Building on this insight, many studies [114, 38, 123, 47, 136] leverage diffusion models for downstream image understanding tasks. However, these approaches typically require separate fine-tuning for each individual task. Recently, we find several concurrent works [112, 53] also use diffusion models for multitask learning. Yet, these methods often involve complex network architectures and training procedures, and their evaluations tend to focus only on very limited subset of image understanding results. In contrast, our DICEPTION offers simpler solution. We not only conduct detailed evaluations of our method across variety of tasks but also demonstrate that the simplicity, paired with the inherent strengths of diffusion models, can be sufficient to deliver strong results without relying on overly complicated setups. 2.3. Multi-task Generalist Models Recently, there has been surge of interest in exploring visual multitask learning. Some approaches [109, 111] draw inspiration from in-context learning in NLP, adapting it for the visual domain. Others [69, 68, 72, 2] have advocated for sequence modeling methods, utilizing transformer encoder-decoder architecture. In these approaches, different encoders map various tasks into shared representation space, and distinct decoders are employed to transform tokens into the outputs specific to each task. However, these methods face notable limitations: they need to train separate encoder and decoder for every individual task and they usually rely on substantial amounts of data to attain optimal performance. The recent success of high-quality Vision Language Models (VLMs) [64] has also encouraged researchers to leverage them for building multitask models. Yet, these VLM-based methods [4, 104, 16, 67, 89, 59] typically focus on multimodal understanding tasks, such as image captioning, rather than general visual perception tasks. Meanwhile, some approaches [97, 132, 76] combine diffusion models with autoregressive models, focusing primarily on instruction-following image generation or editing tasks, rather than addressing image perception tasks. Although certain studies [52, 45, 17, 34] have tried to apply VLMs to more advanced semantic perception tasks, they struggle to establish unified generalist visual model. 2.4. Compared with One Diffusion The concurrent work, One Diffusion [53], addresses multi-task image generation, whereas our approach focuses on multi-task image understanding. We excel at performing broader range of image understanding tasks with higher quality. While One Diffusions strategy of treating different images as different views benefits generation tasks, their failure to distinguish between conditions and images introduces harmful degrees of freedom for perception tasks, as illustrated in the red-highlighted regions of Figure S5. Specifically, when performing perception tasks, One Diffusion tends to generate an image similar to the original input, rather than the desired perceptual results. Although One Diffusion suggests that more detailed text prompts can lead to better results, we argue that performance in perception tasks should not overly depend on the quality of text prompts. In contrast, our method uses only simple task prompts to distinguish between different tasks, rather than allowing the text prompts to dominate the results. Crucially, while One Diffusion requires massive amount of data (75 million samples) and computational resources for from-scratch training, we leverage the priors of pretrained models and demonstrate that, with significantly less data (1.8 million samples), we achieve performance on par with state-of-the-art results. In the image understanding tasks shared by both approaches, we consistently produce more stable and higher-quality results than One Diffusion. 3. Method 3.1. Preliminary Recent diffusion models predominantly build upon flow matching methodologies [63, 1, 66] and the DiT architecture [77]. They aim to learn velocity field that effectively maps samples from source distribution to target distribution. The training process involves minimizing the discrepancy between the models predicted velocity and the ground-truth velocity, which is typically formulated as: Loss = Ez0,t,cvθ(zt, t, c) u(zt)2 2, (1) where is the condition, usually the text prompt. z0 is the clean image latent and zt is the noisy one. The learned velocity field corresponds to an ordinary differential equation (ODE), such that during inference, sample drawn from the source distribution can be transformed into the desired output by solving this ODE. 4 3.2. Unifying Task Representation into RGB Space The decision to unify representations of diverse tasks in RGB space was motivated by two key factors: (1) It Maximally leverages the priors in text-to-image models, which have been extensively trained within the RGB domain. (2) RGB serves as foundational representation in computer vision, providing common visual framework through which wide variety of tasks can be coherently and intuitively visualized. We focus on several of the most fundamental tasks in computer vision: monocular depth estimation, normal estimation, and segmentation. Segmentation, in particular, encompasses point-prompted segmentation, entity segmentation, and semantic segmentation. All these tasks can be unified within an RGB space, with the difference being the number of channels. For single-channel representations, such as depth maps and segmentation masks, we align them with RGB by repeating the channel data three times. For inherently three-channel representations, such as normal maps, we treat them directly as RGB images. Entity segmentation is to segment every instance in an image but with no category. We assign each mask within an image random color and merge them into three-channel RGB mask. Painter found that assigning color randomly makes the model hard to optimize. However, we find this approach has no adverse impact on the training and enables the model to effectively learn to distinguish different instances by painting them with different colors. Each instances mask can be extracted from the RGB mask using clustering algorithms during post-processing without significant performance degradation. We also apply the random color assignment in semantic segmentation. Unlike traditional semantic segmentation, our method is capable of segmenting instances of the same semantic category. By default, We use KMeans for mask extraction. Let xr denote the pre-unified raw representation for each task, and represents the unified RGB-like output representation. We formalize this process as: = Ψ(xr). dress this, we introduce minimal straightforward twolayer MLP Φ() that enables the model to understand the point prompt. Inspired by SAM, we apply sin-cos positional encoding to the point coordinates p, then pass them into the MLP Φ() to produce point embeddings that match the dimension of the input hidden states. We use two learnable embeddings to indicate whether the embedding is point embedding or not: ξp for point embeddings and ξnp for non-point embeddings. The processed point embedding is summed with ξp. For other tasks, we simply use ξnp as the point embedding. During training, we randomly select 15 points to guide the segmentation. When the number of selected points is fewer than 5, we pad the point embeddings to length of 5 with ξnp. When performing tasks that do not require point input, the point embedding is simply length-5 sequence, where each element is ξnp. By denoting the final point embedding as ξ, this process is formulated as: (cid:40) ξ = Concat(Φ(PE(p)) + ξp, ξnp) ξnp if point segmentation else (2) Input Formulation and Loss. DICEPTION introduces two additional inputs based on SD3: the input image and point embedding ξ. For the input image, we first apply VAE to down-sample it by factor of 8, after which it is 2 2 patchified into sequences. We denote this pre-processing as τ . Subsequently, the task prompt token e, point embedding ξ, noisy token zt, and input image token are concatenated to form the complete input. We follow the flow matching loss in training SD3. During training, the loss is applied solely to the noisy tokens: z0 = τ (x) = τ (x) Loss = Ez0,tvθ(zt, z, t, e, ξ) u(zt)2 2. (3) 3.3. DICEPTION: Unified Framework 3.4. Adapting to New Tasks Architecture. Our model adopts the same architecture as SD3. We aim to keep the architecture as unchanged as possible, fully leveraging the pretrained prior knowledge. We use simple task prompts to direct the model to perform various tasks, such as image2depth, image2normal, and image2segmentation. For point-prompted segmentation, naive approach is directly painting points on the image. But this strategy is highly sensitive to the size of the points. If the painted points are too large, they can obscure small regions, causing segmentation to fail. Conversely, if the painted points are too small, the model may lose relevant point information after VAE downsampling and patchification. To adIn practical scenarios, it is often necessary to enable models to quickly adapt to new tasks using only small amount of training data. Traditional specialized foundation models, however, are typically limited to tasks closely related to their domain and generally require substantial datasets and carefully designed network architectures for adaptation. Diffusion models, while powerful in many respects, cannot easily adapt to downstream tasks through fine-tuning only few parameters with limited data. DICEPTION effectively addresses this limitation. We conducted experiments on lung segmentation, tumor segmentation, and image highlighting, which represent tasks with varying degrees of overlap with the models original domain. We train fewer than 1% of the models parameters using LoRA [42] without any complex architectural modifications. Notably, despite the limited availability of training samples (50 per task), DICEPTION consistently delivered successful and high-quality performance across all target tasks. These results provide compelling evidence for the potential of DICEPTION as truly unified foundation model. 4. Experiments 4.1. Implementation Details Data. We randomly select 500k images from the OpenImages [51] dataset and use DepthPro [7] and StableNormal [123] to generate depth and normal data. For pointprompted segmentation, we randomly select 400k images from the SA-1B [49] dataset, as well as 200k images with fine-grained hair masks synthesized from the AM2k [56], AIM500 [57], and P3M-10k [55]. Entity segmentation data is from EntityV2 [81], while semantic segmentation data comes from the COCO-Rem [94], and human pose data is sourced from COCO [62]. For few-shot fine-tuning, we select 50 samples from the publicly labeled Chest X-Ray dataset [108], LOL-v2 [121], and Kaggles Brain Tumor dataset as training samples. More details can be found in Appendix A. Training. Our training lasts for 24 days using 4 NVIDIA H800 GPUs. We employ the AdamW optimizer with constant learning rate of 2e5 and batch size of 28 per GPU. We found that the training process is highly stable. However, the convergence speed for segmentation tasks was slower compared to depth and normal tasks. Therefore, we increased the proportion of segmentation data in each batch. Specifically, in each batch, depth and normal each account for 15%, point-prompted segmentation, entity segmentation, and semantic segmentation each account for 20%, and pose estimation accounts for 20%. We observe that, by the end of training, despite the loss no longer significantly decreasing, the models performance on segmentation tasks continues to improve. During few-shot fine-tuning, we apply rank-128 LoRA to all attention Q, K, and layers in the network, which accounts for less than 1% of the total network parameters. The task prompts for different tasks are image-to-segmentation lung, image-to-segmentation tumor, and image-tohighlight. LoRA training is conducted on single NVIDIA H100 GPU, with constant learning rate of 2e5 and batch size of 8. Please refer to Appendix for more fewshot fine-tuning visualizations. Inference. We follow the settings of the pretrained model and perform 28 steps of denoising during inference. The inference can be run on GPU of 24GB memory with batch size of 4. 4.2. Comparisons with Existing Methods We compare the performance of specialized models, existing multi-task models, and our DICEPTION across various tasks. Specifically, we evaluate depth using the same protocol as Genpercept, normal estimation using the same method as StableNormal, point-prompted segmentation using the same approach as SAM, and human keypoints using the same method as Painter. We also assess semantic and entity segmentation on the MS COCO dataset. For entity segmentation, we assigned all predicted categories to the same label. As in Tables 1 and 2, our DICEPTION significantly outperforms existing multi-task models and achieves performance on par with state-of-the-art specialized models or demonstrates only an acceptable performance decrease. For point-prompted segmentation, as shown in Figure 3, we achieve results on par with SAM-vit-h using only 0.06% of their data. SAM shows clear advantage only on certain out-of-distribution datasets that are outside the scope of our models training, such as WoodScape fisheye dataset. Notably, while most specialized models require extensive data or complex data pipelines, our method achieves excellent results with significantly less data, the majority of which is obtained through simple random selection. For entity segmentation, we observe that our model performs poorly on small objects. We believe that this is due to the limited amount of data, which causes the model to overfit larger objects. This problem can be solved by introducing more data towards small objects. We observe that, although our model generates highquality visualizations for human pose and semantic segmentation, the corresponding evaluation metrics remain relatively low. For human keypoints, this is primarily due to two factors: Firstly, we utilize skeletal-form RGB images rather than heatmaps. While the former produces visually appealing results, the extraction of keypoints during post-processing introduces considerable errors. Secondly, our evaluation follows the 192256 top-down human keypoints protocol. The original 192256 images are resized to 768768 before being input into the model, resulting in extremely blurred inputs that likely contribute to the diminished performance. Semantic segmentation also introduces considerable errors during post-processing. We offer comprehensive explanation of the metric degradation in the Appendix B.3. For individual images, we can adjust hyperparameters during the post-processing of RGB mask to achieve the optimal masks. However, different results require different optimal hyperparameters, and manually adjusting each sample in the validation set during evaluation is impractical 6 Table 1: Quantitative comparison of depth estimation with both specialized models and multi-task models on zero-shot datasets. Our visual generalist model can perform on par with state-of-the-art models. We use the same evaluation protocal () as Genpercept [114]. Method MiDaS [86] Omnidata [27] DPT-large [85] DepthAnything [118] DepthAnything v2 [119] Depth Pro [7] Metric3D v2 [43] DiverseDepth [125] LeReS [126] HDN [128] GeoWizard [31] DepthFM [33] Marigold [47] DMP Official [54] GeoWizard [31] DepthFM [33] Genpercept [114] Painter [109] Unified-IO [69] 4M-XL [72] OneDiffusion [53] Ours-single Ours Training Samples AbsRel KITTI [32] NYUv2 [74] ScanNet [23] DIODE [102] ETH3D [92] δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel 2M 0.236 12.2M 0.149 1.4M 0.100 63.5M 0.080 62.6M 0.080 0.055 - 0.052 16M 0.190 320K 0.149 354K 0.115 300K 0.097 280K 0.083 63K 0.099 74K 0.240 - 0.129 280K 0.174 63K 0.094 90K 0.324 24K 48K 0.188 759M 0.105 0.101 500K 0.081 500K 0.075 500K 0.630 0.835 0.901 0.946 0.943 0.974 0.979 0.704 0.784 0.867 0.921 0.934 0.916 0.622 0.851 0.718 0.923 0.393 0.699 0.896 0.908 0.942 0. 0.111 0.074 0.098 0.043 0.043 0.042 0.039 0.117 0.090 0.069 0.052 0.065 0.055 0.109 0.059 0.082 0.091 0.046 0.059 0.068 0.087 0.068 0.072 0.885 0.945 0.903 0.980 0.979 0.977 0.979 0.875 0.916 0.948 0.966 0.956 0.964 0.891 0.959 0.932 0.932 0.979 0.970 0.951 0.924 0.949 0.939 0.121 0.075 0.082 0.043 0.042 0.041 0.023 0.109 0.091 0.080 0.061 - 0.064 0.146 0.066 0.095 0.056 0.083 0.063 0.065 0.094 0.078 0.075 0.846 0.936 0.934 0.981 0.979 0.978 0.989 0.882 0.917 0.939 0.953 - 0.951 0.814 0.953 0.903 0.965 0.927 0.965 0.955 0.906 0.945 0.938 0.332 0.339 0.182 0.261 0.321 0.217 0.147 0.376 0.271 0.246 0.297 0.225 0.308 0.361 0.328 0.334 0.302 0.342 0.369 0.331 0.399 0.267 0.243 0.715 0.742 0.758 0.759 0.758 0.764 0.892 0.631 0.766 0.780 0.792 0.800 0.773 0.706 0.753 0.729 0.767 0.534 0.906 0.734 0.661 0.709 0. 0.184 0.166 0.078 0.058 0.066 0.043 0.040 0.228 0.171 0.121 0.064 - 0.065 0.128 0.077 0.101 0.066 0.203 0.103 0.070 0.072 0.059 0.053 δ1 0.752 0.778 0.946 0.984 0.983 0.974 0.983 0.694 0.777 0.833 0.961 - 0.960 0.857 0.940 0.902 0.957 0.644 0.906 0.953 0.949 0.969 0.967 Table 2: Quantitative comparison of surface normal estimation with both specialized models and multi-task models. All methods are evaluated with the same evaluation protocol of StableNormal [123]. Method NYUv2 [74] Training Samples mean med 11.25 22.5 30 mean med 11.25 22.5 30 mean med 11.25 22.5 30 DINSE [3] 160K 18.572 10.845 54.732 74.146 80.256 18.610 9.885 56.132 76.944 82.606 18.453 13.871 36.274 77.527 86.976 Geowizard [31] 280K 20.363 11.898 46.954 73.787 80.804 19.748 9.702 58.427 77.616 81.575 19.371 15.408 30.551 75.426 86.357 90K 20.896 11.516 50.712 73.037 79.216 18.600 8.293 64.697 79.329 82.978 18.348 13.367 39.178 79.819 88.551 90K 20.864 11.134 50.457 73.003 79.332 18.463 8.442 64.727 79.559 83.199 16.671 12.084 45.776 82.076 89.879 StableNormal [123] 250K 19.707 10.527 53.042 75.889 81.723 17.248 8.057 66.655 81.134 84.632 13.701 9.460 63.447 86.309 92. GenPercept [114] Marigold [47] DIODE-indoor [102] ScanNet [23] Painter [109] One Diffusion [53] - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Unified-IO [68] 210K 28.547 14.637 39.907 63.912 71.240 17.955 10.269 54.120 77.617 83.728 31.576 16.615 27.855 64.973 73.445 4M-XL [72] 759M 37.278 13.661 44.660 60.553 65.327 30.700 11.614 48.743 68.867 73.623 18.189 12.979 36.622 81.844 87.050 Ours-single 500K 18.267 10.238 52.393 76.802 83.113 19.892 12.424 45.930 74.341 81.965 17.611 8.912 62.030 80.827 86.474 Ours 500K 18.302 10.538 52.533 75.977 82.573 19.348 12.129 46.410 74.805 82.176 17.946 8.686 62.641 81.152 85.398 Table 3: Average recall (AR) of entity segmentation on the MS COCO validation set. Table 4: Evaluation of human keypoints estimation on MS COCO."
        },
        {
            "title": "Method",
            "content": "AR-small AR-medium AR-large HRNet[96] HRFormer[129] ViTPose[115] AP 76.3 77.2 78. Painter[109] Ours 57.8 72.5 EntityV2 [81] Ours-single Ours 0.313 0.123 0.121 0.551 0.424 0.439 0.683 0.648 0. due to the excessive labor involved. This leads to remarkable semantic segmentation visualizations produced by our method but relatively low metrics. We achieve excellent results by training solely on COCO-Rem. Furthermore, our 7 Figure 3: Comparisons of mIoU with SAM-vit-h. We achieve results on par with SAM using only 0.06% of their data (600K vs. 1B). Table 5: Evaluation of semantic segmentation on the MS COCO (with category ID). leave this as future work. 4.4. Ablations Method AP SparK [98] OneFormer [44] Mask2Former [18] Ours 33.2 50.1 49.2 45.1 semantic segmentation can distinguish different instances of the same semantic category. We believe we have strongly demonstrated the potential of our method for instance-level semantic segmentation with IDs. Please check Appendix for more details about post-processing. 4.3. Comparisons with Our Single-task Models For the training of single-task models, we ensure that the network architecture remains the same and the total amount of training data seen for each specific task is the same as that for the multi-task model. For example, if the multitask model trains for 100 iterations with 4 depth data samples per batch, the single-task model will also be trained for 100 iterations with 4 data samples per batch. In our current data setting (approximately 1.8 million samples), we have not observed significant gap between the multi-task and single-task models, nor have we seen trend of mutual promotion between different tasks, as shown by Ours-single in Tables 1, 2, 3 and Figure 3. We believe that it is more appropriate to explore with larger datasets in order to draw more solid conclusions. We 8 Multi-point Prompted Segmentation. Ambiguity is significant issue in point-prompted segmentation. For example, if point is placed on persons clothing, the model may segment the clothing, but the desired result is the person. Therefore, more points are needed to resolve this ambiguity. As illustrated in Figure 4, additional points help the model better segment the desired results. Figure 4: Comparisons between 1-point and 5-point segmentation of mIoU on all 23 validation datasets. Architecture of Diffusion Model. Before the advent of DiT [77], the UNet architecture was predominantly used in diffusion models. We also conduct multi-task experiments based on UNet pretrained model SDXL [78]. Specifically, we follow Marigolds [47] approach by expanding the first convolution layers input channels from 4 to 8 to accommodate image inputs, and similarly use task prompts to guide the model in solving different tasks. However, as shown in Figure S16, we find that this approach is not sufficiently effective, even for minimal multi-task scenario involving only depth and normal estimation. We attribute this limitation to the superior information aggregation capabilities of the Transformer architecture compared to UNet. While the UNet suffers from significant information loss during downsampling, the Transformer maintains more comprehensive representations, enabling it to perform better in multi-task scenarios. 5. Conclusion We have introduced DICEPTION, multi-task visual generalist model based on the diffusion model. Our approach unifies different tasks in the RGB space, leveraging the prior knowledge of pre-trained image generation model to achieve results that are on par with or acceptably lower than the results of specialized foundation models. We achieve good performance without carefully cherry-picking extremely high-quality data or by using an exceptionally large amount of data. Furthermore, for segmentation tasks, we demonstrate that the strategy of assigning random colors to different instances is highly effective in our framework, enabling highquality entity segmentation and semantic segmentation. In few-shot fine-tuning, we are able to achieve high-quality results with minimal data and minimal trainable parameters. We believe that DICEPTION sheds light on how to effectively use generative model priors to build strong visual generalist, enabling more efficient solutions to perception tasks."
        },
        {
            "title": "References",
            "content": "[1] Michael Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. arXiv preprint arXiv:2209.15571, 2022. [2] Roman Bachmann, Oguzhan Fatih Kar, David Mizrahi, Ali Garjani, Mingfei Gao, David Griffiths, Jiaming Hu, Afshin Dehghan, and Amir Zamir. 4m-21: An any-to-any vision model for tens of tasks and modalities. arXiv preprint arXiv:2406.09406, 2024. [3] Gwangbin Bae and Andrew Davison. Rethinking inductive biases for surface normal estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95359545, 2024. [4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. [5] Dina Bashkirova, Mohamed Abdelfattah, Ziliang Zhu, James Akl, Fadi Alladkani, Ping Hu, Vitaly Ablavsky, Berk Calli, Sarah Adel Bargal, and Kate Saenko. Zerowaste dataset: Towards deformable object segmentation in cluttered scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21147 21157, 2022. [6] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [7] Aleksei Bochkovskii, Amael Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan Richter, and Vladlen Koltun. Depth pro: Sharp monocular metric depth in less than second. arXiv preprint arXiv:2410.02073, 2024. [8] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instrucIn Proceedings of the IEEE/CVF Conference on tions. Computer Vision and Pattern Recognition, pages 18392 18402, 2023. [9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [10] Juan Caicedo, Allen Goodman, Kyle Karhohs, Beth Cimini, Jeanelle Ackerman, Marzieh Haghighi, CherKeng Heng, Tim Becker, Minh Doan, Claire McQuin, et al. Nucleus segmentation across imaging experiments: the 2018 data science bowl. Nature methods, 16(12):12471253, 2019. [11] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pages 213229. Springer, 2020. [12] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. [13] Duygu Ceylan, Chun-Hao Huang, and Niloy Mitra. In ProPix2video: Video editing using image diffusion. ceedings of the IEEE/CVF International Conference on Computer Vision, pages 2320623217, 2023. [14] Wenhao Chai, Xun Guo, Gaoang Wang, and Yan Lu. Stablevideo: Text-driven consistency-aware diffusion video In Proceedings of the IEEE/CVF International editing. Conference on Computer Vision, pages 2304023050, 2023. [15] Jiazhou Chen, Yanghui Xu, Shufang Lu, Ronghua Liang, and Liangliang Nan. 3-d instance segmentation of mvs buildings. IEEE Transactions on Geoscience and Remote Sensing, 60:114, 2022. [16] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. [17] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision language model. arXiv preprint arXiv:2406.01584, 2024. [18] Bowen Cheng, Ishan Misra, Alexander Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12901299, 2022. [19] Luca Ciampi, Carlos Santiago, Joao Costeira, Claudio Gennaro, and Giuseppe Amato. Night and Day Instance Segmented Park (NDISPark) Dataset: Collection of Images taken by Day and by Night for Vehicle Detection, Segmentation and Counting in Parking Areas, May 2022. [20] Luca Ciampi, Carlos Santiago, Joao Paulo Costeira, Claudio Gennaro, and Giuseppe Amato. Domain adaptation for In VISIGRAPP (5: VISAPP), traffic density estimation. pages 185195, 2021. [21] Nadav Cohen, Yael Newman, and Ariel Shamir. Semantic segmentation in art paintings. In Computer graphics forum, volume 41, pages 261275. Wiley Online Library, 2022. [22] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes In Prodataset for semantic urban scene understanding. ceedings of the IEEE conference on computer vision and pattern recognition, pages 32133223, 2016. [23] Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 58285839, 2017. [24] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Evangelos Kazakos, Jian Ma, Davide 10 Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. Rescaling egocentric vision: Collection, pipeline and chalInternational Journal of lenges for epic-kitchens-100. Computer Vision, pages 123, 2022. [25] Ahmad Darkhalil, Dandan Shan, Bin Zhu, Jian Ma, Amlan Kar, Richard Higgins, Sanja Fidler, David Fouhey, and Dima Damen. Epic-kitchens visor benchmark: Video segmentations and object relations. Advances in Neural Information Processing Systems, 35:1374513758, 2022. [26] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [27] Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir Zamir. Omnidata: scalable pipeline for making multitask mid-level vision datasets from 3d scans. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1078610796, 2021. [28] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. [29] Alireza Fathi, Xiaofeng Ren, and James Rehg. Learning to recognize objects in egocentric activities. In CVPR 2011, pages 32813288. IEEE, 2011. [30] Jean-Michel Fortin, Olivier Gamache, Vincent Grondin, Francois Pomerleau, and Philippe Gigu`ere. Instance segmentation for autonomous log grasping in forestry operations. In 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 60646071. IEEE, 2022. [31] Xiao Fu, Wei Yin, Mu Hu, Kaixuan Wang, Yuexin Ma, Ping Tan, Shaojie Shen, Dahua Lin, and Xiaoxiao Long. Geowizard: Unleashing the diffusion priors for 3d geometry estimation from single image. In European Conference on Computer Vision, pages 241258. Springer, 2024. [32] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The International Journal of Robotics Research, 32(11):12311237, 2013. [33] Ming Gui, Johannes Schusterbauer, Ulrich Prestel, Pingchuan Ma, Dmytro Kotovenko, Olga Grebenkova, Stefan Andreas Baumann, Vincent Tao Hu, and Bjorn Ommer. Depthfm: Fast monocular depth estimation with flow matching. arXiv preprint arXiv:2403.13788, 2024. [34] Qiushan Guo, Shalini De Mello, Hongxu Yin, Wonmin Byeon, Ka Chun Cheung, Yizhou Yu, Ping Luo, and Sifei Liu. Regiongpt: Towards region understanding vision language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13796 13806, 2024. [35] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. [36] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: dataset for large vocabulary instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 53565364, 2019. [37] Timm Haucke, Hjalmar Kuhl, and Volker Steinhage. Socrates: Introducing depth in visual wildlife monitoring using stereo vision. Sensors, 22(23):9082, 2022. [38] Jing He, Haodong Li, Wei Yin, Yixun Liang, Leheng Li, Kaiqiang Zhou, Hongbo Zhang, Bingbing Liu, and YingCong Chen. Lotus: Diffusion-based visual foundation model for high-quality dense prediction. arXiv preprint arXiv:2409.18124, 2024. [39] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1600016009, 2022. [40] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. [41] Jungseok Hong, Michael Fulton, and Junaed Sattar. Trashcan: semantically-segmented dataset towards visual detection of marine debris. arXiv preprint arXiv:2007.08097, 2020. [42] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. [43] Mu Hu, Wei Yin, Chi Zhang, Zhipeng Cai, Xiaoxiao Long, Hao Chen, Kaixuan Wang, Gang Yu, Chunhua Shen, and Shaojie Shen. Metric3d v2: versatile monocular geometric foundation model for zero-shot metric depth and surface normal estimation. arXiv preprint arXiv:2404.15506, 2024. [44] Jitesh Jain, Jiachen Li, Mang Tik Chiu, Ali Hassani, Nikita Orlov, and Humphrey Shi. Oneformer: One transformer In Proceedings of to rule universal image segmentation. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 29892998, 2023. [45] Qing Jiang, Yuqin Yang, Yuda Xiong, Yihao Chen, Zhaoyang Zeng, Tianhe Ren, Lei Zhang, et al. Chatrex: Taming multimodal llm for joint perception and understanding. arXiv preprint arXiv:2411.18363, 2024. [46] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60076017, 2023. [47] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 94929502, 2024. 11 [48] Samar Khanna, Medhanie Irgau, David Lobell, and Stefano Ermon. Explora: Parameter-efficient extended pretraining to adapt vision transformers under domain shifts. arXiv preprint arXiv:2406.10973, 2024. [49] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment In Proceedings of the IEEE/CVF International anything. Conference on Computer Vision, pages 40154026, 2023. [50] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [51] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International journal of computer vision, 128(7):1956 1981, 2020. [52] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95799589, 2024. [53] Duong H. Le, Tuan Pham, Sangho Lee, Christopher Clark, Aniruddha Kembhavi, Stephan Mandt, Ranjay Krishna, and Jiasen Lu. One diffusion to generate them all, 2024. [54] Hsin-Ying Lee, Hung-Yu Tseng, and Ming-Hsuan Yang. Exploiting diffusion prior for generalizable dense prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 78617871, 2024. [55] Jizhizi Li, Sihan Ma, Jing Zhang, and Dacheng Tao. Privacy-preserving portrait matting. In Proceedings of the 29th ACM international conference on multimedia, pages 35013509, 2021. [56] Jizhizi Li, Jing Zhang, Stephen Maybank, and Dacheng Tao. Bridging composite and real: towards end-to-end deep image matting. International Journal of Computer Vision, 130(2):246266, 2022. [57] Jizhizi Li, Jing Zhang, and Dacheng Tao. Deep automatic natural image matting. arXiv preprint arXiv:2107.07235, 2021. [58] Siyuan Li, Lei Ke, Martin Danelljan, Luigi Piccinelli, Mattia Segu, Luc Van Gool, and Fisher Yu. Matching anything by segmenting anything. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1896318973, 2024. [59] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. In European Conference on Computer Vision, pages 323340. Springer, 2025. [60] Yin Li, Zhefan Ye, and James Rehg. Delving into egocentric actions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 287295, 2015. [61] Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, Ming-Ming Cheng, and Ying Shan. Photomaker: Customizing realistic human photos via stacked id embedding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 86408650, 2024. [62] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollar. Microsoft coco: Common objects in context, 2015. [63] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [64] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. [65] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing with cross-attention control. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 85998608, 2024. [66] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. [67] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, et al. Deepseek-vl: towards real-world visionlanguage understanding. arXiv preprint arXiv:2403.05525, 2024. [68] Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision language audio and action. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2643926455, 2024. [69] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: unified model for vision, language, and multi-modal tasks. In The Eleventh International Conference on Learning Representations, 2022. [70] Baorui Ma, Huachen Gao, Haoge Deng, Zhengxiong Luo, Tiejun Huang, Lulu Tang, and Xinlong Wang. You see it, you got it: Learning 3d creation on pose-free videos at scale. arXiv preprint arXiv:2412.06699, 2024. [71] Massimo Minervini, Andreas Fischbach, Hanno Scharr, and Sotirios Tsaftaris. Finely-grained annotated datasets for image-based plant phenotyping. Pattern recognition letters, 81:8089, 2016. [72] David Mizrahi, Roman Bachmann, Oguzhan Kar, Teresa Yeo, Mingfei Gao, Afshin Dehghan, and Amir Zamir. 4m: Massively multimodal masked modeling. Advances in Neural Information Processing Systems, 36:5836358408, 2023. [73] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-toimage diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 4296 4304, 2024. 12 [74] Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob Indoor segmentation and support inference from Fergus. rgbd images. In ECCV, 2012. [75] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [76] Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, and Furu Wei. Kosmos-g: Generating images in context with multimodal large language models. arXiv preprint arXiv:2310.02992, 2023. [77] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4195 4205, 2023. [78] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion modarXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. [79] Mattia Pugliatti and Francesco Topputo. Doors: Dataset for boulders segmentation. statistical properties and blender setup, 2022. [80] Jiyang Qi, Yan Gao, Yao Hu, Xinggang Wang, Xiaoyu Liu, Xiang Bai, Serge Belongie, Alan Yuille, Philip HS Torr, and Song Bai. Occluded video instance segmentation: benchmark. International Journal of Computer Vision, 130(8):20222039, 2022. [81] Lu Qi, Jason Kuen, Weidong Guo, Tiancheng Shen, Jiuxiang Gu, Jiaya Jia, Zhe Lin, and Ming-Hsuan Yang. High-quality entity segmentation. arXiv preprint arXiv:2211.05776, 2022. [82] Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, Juan Carlos Niebles, Caiming Xiong, Silvio Savarese, et al. Unicontrol: unified diffusion model for controllable visual generation in the wild. arXiv preprint arXiv:2305.11147, 2023. [83] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language superIn International conference on machine learning, vision. pages 87488763. PMLR, 2021. [84] Frano Rajiˇc, Lei Ke, Yu-Wing Tai, Chi-Keung Tang, Martin Danelljan, and Fisher Yu. Segment anything meets point tracking. arXiv preprint arXiv:2307.01197, 2023. [85] Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1217912188, 2021. [86] Rene Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot crossdataset transfer. IEEE transactions on pattern analysis and machine intelligence, 44(3):16231637, 2020. [87] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. [88] Tianhe Ren, Yihao Chen, Qing Jiang, Zhaoyang Zeng, Yuda Xiong, Wenlong Liu, Zhengyu Ma, Junyi Shen, Yuan Gao, Xiaoke Jiang, et al. Dino-x: unified vision model for open-world object detection and understanding. arXiv preprint arXiv:2411.14347, 2024. [89] Zhongwei Ren, Zhicheng Huang, Yunchao Wei, Yao Zhao, Dongmei Fu, Jiashi Feng, and Xiaojie Jin. Pixellm: Pixel reasoning with large multimodal model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2637426383, 2024. [90] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua Susskind. Hypersim: photorealistic synthetic dataset for holistic indoor scene understanding. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1091210922, 2021. [91] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [92] Thomas Schops, Johannes Schonberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, and Andreas Geiger. multi-view stereo benchmark with highIn Proceedresolution images and multi-camera videos. ings of the IEEE conference on computer vision and pattern recognition, pages 32603269, 2017. [93] Jiahao Shao, Yuanbo Yang, Hongyu Zhou, Youmin Zhang, Yujun Shen, Matteo Poggi, and Yiyi Liao. Learning temporally consistent video depth from video diffusion priors. arXiv preprint arXiv:2406.01493, 2024. [94] Shweta Singh, Aayan Yadav, Jitesh Jain, Humphrey Shi, Justin Johnson, and Karan Desai. Benchmarking object detectors with coco: new path forward, 2024. [95] Corey Snyder and Minh Do. Streets: novel camera network dataset for traffic flow. Advances in Neural Information Processing Systems, 32, 2019. [96] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation learning for human pose estimation, 2019. [97] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models In Proceedings of the IEEE/CVF are in-context learners. Conference on Computer Vision and Pattern Recognition, pages 1439814409, 2024. [98] Keyu Tian, Yi Jiang, Qishuai Diao, Chen Lin, Liwei Wang, and Zehuan Yuan. Designing bert for convolutional networks: Sparse and hierarchical masked modeling. arXiv preprint arXiv:2301.03580, 2023. [99] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, 13 et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [100] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [101] Cameron Trotter, Georgia Atkinson, Matt Sharpe, Kirsten Richardson, Stephen McGough, Nick Wright, Ben Burville, and Per Berggren. Ndd20: large-scale few-shot dolphin dataset for coarse and fine-grained categorisation. arXiv preprint arXiv:2005.13359, 2020. [102] Igor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo, Haochen Wang, Falcon Dai, Andrea Daniele, Mohammadreza Mostajabi, Steven Basart, Matthew Walter, et al. Diode: dense indoor and outdoor depth dataset. arXiv preprint arXiv:1908.00463, 2019. [103] Boying Wang, Libo Zhang, Longyin Wen, Xianglong Liu, and Yanjun Wu. Towards real-world prohibited item deIn Proceedings tection: large-scale x-ray benchmark. of the IEEE/CVF international conference on computer vision, pages 54125421, 2021. [104] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [105] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2069720709, 2024. [106] Wen Wang, Qiuyu Wang, Kecheng Zheng, Hao Ouyang, Zhekai Chen, Biao Gong, Hao Chen, Yujun Shen, and Chunhua Shen. Framer: Interactive frame interpolation. arXiv preprint arXiv:2410.18978, 2024. [107] Wen Wang, Canyu Zhao, Hao Chen, Zhekai Chen, Kecheng Zheng, and Chunhua Shen. Autostory: Generating diverse storytelling images with minimal human efforts. International Journal of Computer Vision, pages 122, 2024. [108] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Summers. Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. In IEEE CVPR, volume 7, page 46. sn, 2017. [109] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Images speak in images: generalist Tiejun Huang. In Proceedings of painter for in-context visual learning. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 68306839, 2023. [110] Xuehao Wang, Feiyang Ye, and Yu Zhang. Task-aware lowrank adaptation of segment anything model. arXiv preprint arXiv:2403.10971, 2024. [111] Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, and Tiejun Huang. Seggpt: Segmenting everything in context. arXiv preprint arXiv:2304.03284, 2023. [112] Zhaoqing Wang, Xiaobo Xia, Runnan Chen, Dongdong Yu, Changhu Wang, Mingming Gong, and Tongliang Liu. arXiv Lavin-dit: Large vision diffusion transformer. preprint arXiv:2411.11505, 2024. [113] Yuling Xi, Hao Chen, Ning Wang, Peng Wang, Yanning Zhang, Chunhua Shen, and Yifan Liu. dynamic feature interaction framework for multi-task visual perception. International Journal of Computer Vision, 131(11):2977 2993, 2023. [114] Guangkai Xu, Yongtao Ge, Mingyu Liu, Chengxiang Fan, Kangyang Xie, Zhiyue Zhao, Hao Chen, and Chunhua Shen. Diffusion models trained with large data are transferable visual models. arXiv preprint arXiv:2403.06090, 2024. [115] Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao. Vitpose: Simple vision transformer baselines for human pose estimation. Advances in Neural Information Processing Systems, 35:3857138584, 2022. [116] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editing with diffusion In Proceedings of the IEEE/CVF Conference on models. Computer Vision and Pattern Recognition, pages 18381 18391, 2023. [117] Honghui Yang, Di Huang, Wei Yin, Chunhua Shen, Haifeng Liu, Xiaofei He, Binbin Lin, Wanli Ouyang, and Tong He. Depth any video with scalable synthetic data. arXiv preprint arXiv:2410.10815, 2024. [118] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1037110381, 2024. [119] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. arXiv preprint arXiv:2406.09414, 2024. [120] Lei Yang, Yan Zi Wei, Yisheng He, Wei Sun, Zhenhang Huang, Haibin Huang, and Haoqiang Fan. ishape: first step towards irregular shape instance segmentation. arXiv preprint arXiv:2109.15068, 2021. [121] Wenhan Yang, Shiqi Wang, Yuming Fang, Yue Wang, and Jiaying Liu. From fidelity to perceptual quality: semisupervised approach for low-light image enhancement. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 30633072, 2020. [122] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-tovideo diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [123] Chongjie Ye, Lingteng Qiu, Xiaodong Gu, Qi Zuo, Yushuang Wu, Zilong Dong, Liefeng Bo, Yuliang Xiu, and Xiaoguang Han. Stablenormal: Reducing diffusion variance for stable and sharp normal. ACM Transactions on Graphics (TOG), 43(6):118, 2024. [124] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-to14 image diffusion models. arXiv preprint arXiv:2308.06721, 2023. [125] Wei Yin, Xinlong Wang, Chunhua Shen, Yifan Liu, Zhi Tian, Songcen Xu, Changming Sun, and Dou Renyin. Diversedepth: Affine-invariant depth prediction using diverse data. arXiv preprint arXiv:2002.00569, 2020. [126] Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Long Mai, Simon Chen, and Chunhua Shen. Learning to recover 3d scene shape from single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 204213, 2021. [127] Senthil Yogamani, Ciaran Hughes, Jonathan Horgan, Ganesh Sistu, Padraig Varley, Derek ODea, Michal Uricar, Stefan Milz, Martin Simon, Karl Amende, et al. Woodscape: multi-task, multi-camera fisheye dataset for auIn Proceedings of the IEEE/CVF Intonomous driving. ternational Conference on Computer Vision, pages 9308 9318, 2019. [128] Qian Yu, Xiaoqi Zhao, Youwei Pang, Lihe Zhang, and Huchuan Lu. Multi-view aggregation network for diIn Proceedings of the chotomous image segmentation. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 39213930, 2024. [129] Yuhui Yuan, Rao Fu, Lang Huang, Weihong Lin, Chao Zhang, Xilin Chen, and Jingdong Wang. Hrformer: Highresolution transformer for dense prediction, 2021. [130] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding In conditional control to text-to-image diffusion models. Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. [131] Lingzhi Zhang, Shenghao Zhou, Simon Stent, and Jianbo Shi. Fine-grained egocentric hand-object segmentation: Dataset, model, and applications. In European Conference on Computer Vision, pages 127145. Springer, 2022. [132] Canyu Zhao, Mingyu Liu, Wen Wang, Weihua Chen, Fan Wang, Hao Chen, Bo Zhang, and Chunhua Shen. Moviedreamer: Hierarchical generation for coherent long visual sequence. arXiv preprint arXiv:2407.16655, 2024. [133] Zihan Zhong, Zhiqiang Tang, Tong He, Haoyang Fang, and Chun Yuan. Convolution meets lora: Parameter efficient finetuning for segment anything model. arXiv preprint arXiv:2401.17868, 2024. [134] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. International Journal of Computer Vision, 127:302321, 2019. [135] Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, and Qibin Hou. Storydiffusion: Consistent selfattention for long-range image and video generation. arXiv preprint arXiv:2405.01434, 2024. [136] Muzhi Zhu, Yang Liu, Zekai Luo, Chenchen Jing, Hao Chen, Guangkai Xu, Xinlong Wang, and Chunhua Shen. Unleashing the potential of the diffusion model in few-shot semantic segmentation. arXiv preprint arXiv:2410.02369, 2024."
        },
        {
            "title": "Appendix",
            "content": "A. Dataset We summarize the datasets used in our work in Table S1. The depth and normal data samples are obtained by randomly selecting 500K images from OpenImages [51] and labeling them using Depth Pro [7] and StableNormal [123], respectively. The 400K point segmentation data samples are obtained by randomly selecting images from the SA-1B dataset [49]. For the synthesis of point segmentation data, we extract the foreground from P3M10K [55], AIM500 [57] and AM2K [56], randomly applying transformations such as rotation, resizing, and flipping. These transformed foregrounds are then pasted onto different background images, resulting in 200K synthetic images with fine-grained hair segmentation. For the validation set, we evaluate depth using the same evaluation protocol as Genpercept [114], conducting tests on the NYUv2 [74], KITTI [32], ScanNet [23], DIODE [102], ETH3D [92]. Similarly, for normal estimation, we followed the evaluation protocol of StableNormal [123] and performed evaluations on the NYUv2 [74], ScanNet [23], DIODE [102]. For point segmentation, we conducted extensive comparisons across 23 datasets. The remaining tasks, including Entity Segmentation, Semantic Segmentation, and Human Keypoints, were evaluated on the MS COCO 2017 dataset [62]. We believe the comprehensive experiments provide solid evidence of the remarkable performance of our method. B. Post-processing B.1. Post-processing for Keypoints For keypoints, since all keypoints were labeled in red during training, our first step in post-processing is to exTable S1: Dataset detail. Training Task Depth Normal Point Segmentation Point Segmentation Human Pose Semantic Segmentation Entity Segmentation Data Samples 500K 500K 400K 200K 42K 120K 32K Dataset OpenImages [51] + Depth Pro [7] OpenImages [51] + StableNormal [123] SA-1B [49] P3M-10K [55], AIM500 [57] and AM2K [56] MS COCO 2017 [62] COCO-Rem [94] EntityV2 [81] Task Depth Normal Point Segmentation Entity Segmentation Semantic Segmentation Human Keypoints Validation Dataset NYUv2 [74], KITTI [32], ScanNet [23], DIODE [102], ETH3D [92] NYUv2 [74], ScanNet [23], DIODE [102] PPDLS [71], DOORS [79], TimberSeg [30], NDD20 [101] STREETS [95], iShape [120], ADE20K [134], OVIS [80] Plittersdorf [37], EgoHOS [131], IBD [15], WoodScape [127] TrashCan [41], GTEA [29, 60], NDISPark [20, 19], VISOR [24, 25] LVIS [36], Hypersim [90], Cityscapes [22], DRAM [21] BBBC038v1 [10], ZeroWaste [5], PIDRay [103] MS COCO 2017 [62] MS COCO 2017 [62] MS COCO 2017 [62] Algorithm 1 Keypoints Post-processing Input: human pose RGB x, GT keypoints Kgt, RGB tolerance σ, distance threshold ξ Output: extracted keypoints Kpred 1: = ExtractRedRegions(x, (255, 0, 0), σ) 2: xc = GetConnectedComponents(x) 3: = GetCircular(xc) 4: Kpred = 5: for do 6: 7: = ComputeCenterCoordinates(c) dmin = for Kgt do = ComputeEuclideanDistance(k, k) if < dmin then dmin = = GetKeypointType(k) 8: 9: 10: 11: 12: 13: continue end if end for if dmin < ξ then 14: 15: 16: 17: 18: Kpred = Kpred {(k, t)} 19: end for 20: return Kpred end if tract all red regions from the RGB output. Next, we identify all connected components within the extracted red regions. For each connected component, we further extract sub-regions that approximate circular shape. This step is crucial because, in some cases, multiple predicted keypoints may overlap, requiring us to separate them as much as possible. For example, when person clasps his hands together, the keypoints for both hands may overlap. Once the circular regions are identified, we compute their center points as the predicted keypoint coordinates. Since our model does not explicitly predict the type of each keypoint (e.g., hand, foot), we assign keypoint types by measuring the distance between the extracted keypoints and the ground truth (GT) keypoints. Each predicted keypoint is assigned the type of its nearest GT keypoint. To ensure robustness, we apply distance threshold, considering only those predicted keypoints that are sufficiently close to GT keypoint. Finally, all extracted keypoints that are successfully matched to GT keypoint form our final predicted keypoint coordinates after post-processing. The algorithm is shown in Algorithm 1. B.2. Post-processing for RGB Masks For entity segmentation and semantic segmentation RGB masks, we employ clustering algorithms to extract the object masks. Specifically, we first compute the histogram peaks for each of the three RGB channels and estimate the 16 Figure S1: Additional visualizations. Our one single model tackles multiple perception tasks. Figure S2: Segmentation results on furry objects. 17 Algorithm 2 Segmentation Post-processing Input: RGB segmentation mask m, RGB tolerance σ, area threshold ξ, kernel size k, connected components number threshold η, duplicate mask threshold β end if continue if IsCloseToBlack(c, σ) then Output: extracted masks Mpred 1: Get the number of peaks of the histogram of 2: Get the number of clusters = ean(p) 3: Get the clustered colors by = KMeans(m, n) 4: Mpred = 5: for do 6: 7: 8: 9: = GetMaskByRGB(m, c, σ) 10: = BinaryFillHoles(m) 11: = RefineWithMorphology(m, k) 12: 13: 14: 15: 16: = GetArea(m) if < ξ then continue end if = GetConnectedComponentsNumber(m) if > η then continue 17: 18: 19: 20: Mpred = Mpred {m} 21: end for 22: Mpred = RemoveDuplicateMasks(Mpred, β) 23: return Mpred end if number of clusters by averaging the peak counts across the three channels. We then use KMeans clustering to group the colors and identify the clustered regions in the RGB mask. For each identified cluster, we extract regions with RGB values close to the clusters centroid. This step is followed by morphological operations to refine the extracted masks, such as filling holes and removing small, fragmented regions. We further filter the masks by computing their area, excluding any regions that are too small to be meaningful. Additionally, we also consider the number of connected components within the extracted masks, discarding overly fragmented results that have too many connected components. Finally, we refine the extracted masks by calculating the Intersection over Union (IoU) between them, removing any duplicate or overlapping masks. The algorithm is shown in Algorithm 2. B.3. Performance Degradation of RGB Masks We observe that while the quality of our semantic segmentation visualizations is high, the average precision (AP) for certain categories remains unsatisfactory. For example, for the Person category, we conducted exhaustive experiments and achieved good visualization results (highlighted Table S2: When post-processing RGB masks, small regions and excessive numbers of objects significantly lead to performance degradation. Category AP 76.3 68.9 71.7 18.6 10.4 10."
        },
        {
            "title": "Bear\nDog\nCat\nPerson\nBird\nBook",
            "content": "by the green rectangle in Figure S3), but AP is low (as in Table S2). Although, clearly, there is room for us to improve the semantic segmentation results, we do not intend to fit the data bias of those existing datasets, as pointed out by other authors, e.g., [87]. We trace the root cause of this issue to degradation during post-processing, particularly due to small objects and an excessive number of objects. Specifically, during mask processing, we filter out small noise regions, but this also removes some positive samples, such as the crowd and the bird highlighted in red in rows 3 to 5 in Figure S3. However, if we do not filter these noise regions, they further degrade the results. In our setting, filtering noise regions results in better metrics compared to not filtering them. Additionally, when an image contains an excessive number of objects of the same category (as in row 6 of Figure S3), post-processing may erroneously group similarly colored but distinct objects into single class, leading to lower metrics. Furthermore, as in Table S2, we examine categories with fewer small objects and instances of those categories, such as bear, dog, and cat, and observe higher AP scores. However, for categories with opposite characteristics, their AP scores tend to be lower. Although we can optimize post-processing for individual datasets by adjusting hyperparameters for each image to achieve the best results, this approach becomes impractical for large-scale in-the-wild evaluation, as it requires significant manual effort. Consequently, the dependency on postprocessing remains limitation of our approach. C. Additional Results C.1. Additional Visualizations We present additional visualization results of our method across various tasks, as can be seen in Figures S1, S2, S4, S6, S7, S8, S9, S10, S11, S12, S13, S14. For point-prompted segmentation, we further compared our approach with SAM. These results strongly demonstrate the potential of DICEPTION. DICEPTION is capable of achieving high-quality results, even in challenging scenarios. FurFigure S3: When post-processing RGB masks, small regions and excessive numbers of objects lead to significant metric degradation. thermore, the few-shot fine-tuning of DICEPTION, which requires minimal data and trainable parameters, strongly demonstrates the remarkable transferability of DICEPTIONto tackle new tasks. Our DICEPTION is capable of further refining the segmentation of fine details, such as intricate hair structures. C.2. One-step Inference Does Not Work Genpercept [114] demonstrates that one-step diffusion significantly enhances both the speed and accuracy of perceptual tasks. However, our experimental results reveal notable increase in the proportion of failure cases when applying one-step diffusion in multi-task setting, as illustrated in Figure S15. We believe that this is due to the potential overlap of denoising trajectories for different tasks. These overlapping trajectories can interfere with each other, resulting in failure cases. In contrast, multi-step denoising helps to mitigate such interactions. In single-task setting, since the denoising trajectories pertain to single task, this approach is more effective and stable. D. Limitations Although our DICEPTION achieves great results across multiple tasks, our model, as diffusion model, leads to 19 Figure S4: Additional few-shot fine-tuning results on image highlighting. Figure S6: Additional few-shot fine-tuning results on lung segmentation and tumor segmentation. relatively longer inference times. On one H800, it takes an average of 0.8 seconds to process single image. On one 4090-GPU card, inference for one image takes approximately 2 seconds. We believe that this issue can be addressed through few-step diffusion techniques. For tasks like semantic segmentation, the high demands of post-processing make the large-scale evaluation challenging. Despite the limitations, we believe that DICEPTION is valuable exploration for generalist visual foundation models. Figure S5: Our segmentation not only separates semantically identical objects but also distinguishes different instances of the same category, achieving higher segmentation quality. Moreover, One Diffusion tends to generate an image similar to the input when performing image understanding tasks, as red-highlighted in the figure. 20 Figure S7: Additional depth estimation visualizations. 21 Figure S8: Additional normal visualizations. 22 Figure S9: Additional entity segmentation visualizations. 23 Figure S10: Additional point-prompted segmentation visualizations. 24 Figure S11: Comparison of the segmentation results between DICEPTION and SAM-vit-h with 1-point input. 25 Figure S12: Comparison of the segmentation results between DICEPTION and SAM-vit-h with 5-point input. 26 Figure S13: Additional pose estimation visualizations. 27 Figure S14: Additional semantic segmentation visualizations. 28 Figure S15: The model tends to produce more failure cases in 1-step scenario. Figure S16: UNet-based model fails to perform multi-task."
        }
    ],
    "affiliations": [
        "Shanghai AI Laboratory",
        "Zhejiang University"
    ]
}