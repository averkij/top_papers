{
    "paper_title": "OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer",
    "authors": [
        "Pengze Zhang",
        "Yanze Wu",
        "Mengtian Li",
        "Xu Bai",
        "Songtao Zhao",
        "Fulong Ye",
        "Chong Mou",
        "Xinghui Li",
        "Zhuowei Chen",
        "Qian He",
        "Mingyuan Gao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Videos convey richer information than images or text, capturing both spatial and temporal dynamics. However, most existing video customization methods rely on reference images or task-specific temporal priors, failing to fully exploit the rich spatio-temporal information inherent in videos, thereby limiting flexibility and generalization in video generation. To address these limitations, we propose OmniTransfer, a unified framework for spatio-temporal video transfer. It leverages multi-view information across frames to enhance appearance consistency and exploits temporal cues to enable fine-grained temporal control. To unify various video transfer tasks, OmniTransfer incorporates three key designs: Task-aware Positional Bias that adaptively leverages reference video information to improve temporal alignment or appearance consistency; Reference-decoupled Causal Learning separating reference and target branches to enable precise reference transfer while improving efficiency; and Task-adaptive Multimodal Alignment using multimodal semantic guidance to dynamically distinguish and tackle different tasks. Extensive experiments show that OmniTransfer outperforms existing methods in appearance (ID and style) and temporal transfer (camera movement and video effects), while matching pose-guided methods in motion transfer without using pose, establishing a new paradigm for flexible, high-fidelity video generation."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 0 2 ] . [ 1 0 5 2 4 1 . 1 0 6 2 : r OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer Pengze Zhang Yanze Wu Mengtian Li Xu Bai Songtao Zhao"
        },
        {
            "title": "Zhuowei Chen Qian He Mingyuan Gao",
            "content": "Intelligent Creation Lab, ByteDance"
        },
        {
            "title": "Abstract",
            "content": "Videos convey richer information than images or text, capturing both spatial and temporal dynamics. However, most existing video customization methods rely on reference images or task-specific temporal priors, failing to fully exploit the rich spatio-temporal information inherent in videos, thereby limiting flexibility and generalization in video generation. To address these limitations, we propose OmniTransfer, unified framework for spatio-temporal video transfer. It leverages multi-view information across frames to enhance appearance consistency and exploits temporal cues to enable fine-grained temporal control. To unify various video transfer tasks, OmniTransfer incorporates three key designs: Task-aware Positional Bias that adaptively leverages reference video information to improve temporal alignment or appearance consistency; Reference-decoupled Causal Learning separating reference and target branches to enable precise reference transfer while improving efficiency; and Task-adaptive Multimodal Alignment using multimodal semantic guidance to dynamically distinguish and tackle different tasks. Extensive experiments show that OmniTransfer outperforms existing methods in appearance (ID and style) and temporal transfer (camera movement and video effects), while matching pose-guided methods in motion transfer without using pose, establishing new paradigm for flexible, high-fidelity video generation. Date: January 20, 2026 Project Page: https://pangzecheung.github.io/OmniTransfer/"
        },
        {
            "title": "Introduction",
            "content": "We have all heard the old adage: picture is worth thousand wordsand if we follow that logic little further, video might be worth million. After all, static image freezes just one moment of light, texture, and form; video weaves those moments into something dynamic, carrying not just how things look, but how they move and change. These are details that neither words nor single image can fully convey. The same insight applies to diffusion-based video generation. reference video offers greater flexibility and capability than using an image or text alone. Not only can the model reference identity (ID) and style in spatial appearance aspects, but it can also exploit temporal information, including camera movement, motion, and visual effects, enabling more coherent and expressive video synthesis. Corresponding author, Project lead. This work is purely academic and non-commercial. Demo reference images/videos are from public domains or AI-generated. For copyright concerns, please contact us for the removal of relevant content. 1 Figure 1 OmniTransfer seamlessly unifies spatial appearance (ID and style) and temporal video transfer tasks (effect, motion and camera movement) within single framework, and exhibits strong generalization across unseen task combinations. Despite this potential, existing methods have yet to fully exploit their spatio-temporal information. 1) For spatial information, most approaches [38, 67, 71, 72] still mainly rely on pixel-level cues extracted from reference images. However, cues such as ID and style are inherently spatio-temporal, extending across multiple frames and views. This makes it difficult for single image to capture their full details, thereby limiting their effectiveness. 2) For temporal information, current approaches remain in its early stages: some depend on pre-defined physical priors (e.g., pose or camera position) [9, 19, 75, 78], some employ inversion-based reconstruction [22, 36], while others require additional fine-tuning for specific temporal dynamics during test time [47, 77]. Recently, CamCloneMaster [40] made an initial reference-based attempt on camera motion through temporal context concatenation, yet it struggles to generalize to in-the-wild conditions, and fails to address general temporal video reference tasks. These limitations highlight the need for unified framework that directly leverages temporal information from reference videos, enabling flexible and robust video customization in real-world scenarios. To tackle the aforementioned challenges, this paper introduces novel exploration in the domain of video reference, proposing an all-in-one framework for spatio-temporal video transfer, OmniTransfer. This framework not only integrates multi-frame reference information from the reference video at the spatial level, enhancing the consistency of reference video ID and style, but also effectively models temporal aspects such as motion, camera movement, and video effects, providing multi-dimensional control and unification over the video generation process. First, to unify video transfer tasks, we introduce Task-aware Positional Bias. For temporal transfer tasks, based on our assumption that video diffusion models inherently maintain temporal consistency through spatial context, we add spatial offsets to positional embeddings to preserve temporal alignment. For appearance transfer tasks, temporal offsets are applied to propagate appearance information across frames. Second, we introduce Reference-decoupled Causal Learning, which employs unidirectional transfer from reference to target, preventing simple copy-pasting. By separating the reference and target branches, the reference branch requires only single forward pass, reducing computational time by 20% compared to full-attention models. Finally, to improve semantic guidance and avoid cross-task confusion, we introduce Multimodal Large Language Model (MLLM) via Task-adaptive Multimodal Alignment module. This module leverages multiple sets of task-specific MetaQueries [46] to dynamically integrate and align semantic information across tasks, effectively meeting the requirements of each task. 2 In sum, our contribution can be summarized as follows. We propose OmniTransfer, unified framework for the new task of spatio-temporal video transfer that supports diverse tasks such as identity, style, effect, camera movement, and motion transfer, while generalizing seamlessly to their compositional combinations  (Fig. 1)  . We propose Task-aware Positional Bias, Reference-decoupled Causal Learning, and Task-adaptive Multimodal Alignment, each designed to unify various video customization tasks, enabling efficient and flexible spatio-temporal information transfer. Experiments show that OmniTransfer outperforms existing methods in appearance (ID, style) and temporal (camera movement, effects) transfer, while matching pose-guided methods in motion transfer without using pose. Moreover, benefiting from our model design, it achieves these improvements with 20% reduction in runtime compared to the basemodel architecture."
        },
        {
            "title": "2.1 Appearance reference task",
            "content": "The two main tasks in appearance reference are ID and style transfer. For ID transfer in images, approaches have evolved from adapter-based tuning [15, 29, 31, 57, 69, 70] to in-context learning [5, 16, 34, 45, 63]. In video ID transfer, most approaches still rely on single reference images for ID preservation [14, 18, 67, 72], while some works further explore broader subject-to-video generation tasks [7, 8, 23, 26, 35, 38, 44]. For instance, Phantom [38] concatenates ID features along the temporal dimension to maintain appearance consistency. Similarly, style transfer has been widely explored in image diffusion models [33, 54, 55, 64]. Early video stylization extends these models to the temporal domain via Image-to-Video paradigms [4, 12, 30], while recent Text-to-Video methods [6, 37, 59, 71] offer more flexible and controllable stylization. Despite these advances, these methods rely solely on single-image references, overlooking the rich multi-frame and multi-view cues inherent in videos. Our approach exploits these cues to achieve consistent appearance and stable temporal coherence."
        },
        {
            "title": "2.2 Temporal reference task",
            "content": "Temporal video customization covers motion generation, camera movement, and effect synthesis. Pose motion transfer initially relies on GAN-based warping [49, 50, 73, 76], while diffusion-based models [3, 9, 21, 41, 52, 60, 66, 75, 79, 80] improve temporal smoothness. However, they are limited by pose priors, as skeleton alignment may cause loss of appearance cues and hinder their extension to multi-person scenarios. Some studies explore more general motion control [28, 42, 47, 56, 68, 74, 77], but most rely on diffusion inversion or test-time finetuning. Camera motion generation typically uses explicit parameterization [17, 19, 32, 58, 62, 65, 78] and parameter-free attention inversion [22, 36], while CamCloneMaster [40] explores temporal context concatenation, yet resolution and generalization remain limited. Effect generation in industry often relies on LoRA-based [20] finetuning, whereas academic works [13, 39, 43] focus on precise spatial and temporal control of effects, but cannot generate new effects conditioned on reference video. In contrast, our OmniTransfer achieves unified motion, camera, and effect transfer without explicit priors, demonstrating strong generalization to in-the-wild scenarios."
        },
        {
            "title": "3 Preliminary",
            "content": "Our framework is built upon Wan2.1 I2V 14B [53] as the underlying diffusion model. The input latent lt Rf hw(2n+4) = [c, m, zt] concatenates three components along the channel dimension: latent noise zt Rf hwn obtained by adding timestep noise to VAE-compressed video features z; condition latent Rf hwn encoded from the condition image concatenated with zero-filled frames; binary mask latent Rf hw4 with values of 1 for preserved and 0 for generated frames. [, ] denotes feature concatenation along the channel dimension, and , h, w, represent frame number, height, width, and channel dimension, respectively. 3 Figure 2 OmniTransfer comprises three key components: 1) Task-aware Positional Bias: exploits the models inherent spatial and temporal context capabilities for diverse tasks. 2) Reference-decoupled Causal Learning: separates reference and target branches for causal and efficient transfer. 3) Task-adaptive Multimodal Alignment: leverages an MLLM to unify and enhance semantic understanding across tasks. Each Diffusion Transformer (DiT) block in Wan2.1 includes self-attention and cross-attention layers. The self-attention adopts 3D Rotary Positional Embedding (RoPE): Attn(Rθ(Q), Rθ(K), ) = softmax (cid:18) Rθ(Q) Rθ(K) (cid:19) V, (1) where = WQlt, = WKlt, = WV lt. Rθ() denotes the RoPE rotation applied to queries and keys, WQ, WK, WV are learnable projections, and is the feature dimension. Cross-attention integrates textual features as Attn(Q, Kp, Vp) with Kp and Vp derived from prompt p."
        },
        {
            "title": "4 Method",
            "content": "In this work, we study both appearance and temporal video reference, across several representative tasks summarized in Table 1. To better unify these tasks, we propose OmniTransfer, as illustrated in Fig. 2. It consists of four componentsReference Latent Construction, Task-aware Positional Bias, Reference-decoupled Causal Learning and Task-adaptive Multimodal Alignmentdescribed in detail below."
        },
        {
            "title": "4.1 Reference Latent Construction",
            "content": "To handle practical scenarios where reference and target videos may have different resolutions, we construct separate latents for the two inputs  (Fig. 2)  . For the target video, the latent is constructed in the same manner as in Section 3, i.e., ltgt Rf htgtwtgt(2n+4) = [ctgt, mtgt, zt tgt]. For the reference video, the ref]. The conditional latent cref latent representation is defined as: lref Rf hrefwref(2n+4) = [cref, mref, z0 corresponds to the feature encoded by the VAE. The mask latent mref is assigned task-specific flags: 1 for temporal tasks, 2 for ID transfer, and 3 for style transfer. Notably, the reference latent z0 is kept ref noise-free to maximally preserve its information. 4 Task Input Output-Vtgt Appearance Transfer (T2V)"
        },
        {
            "title": "ID Transfer\nStyle Transfer",
            "content": "Vref, Vref, ID from Vref, following prompt Style from Vref, following prompt Temporal Transfer (I2V)"
        },
        {
            "title": "Motion Transfer\nCamera Movement\nEffect Transfer",
            "content": "Vref, Vref, Vref, Motion from Vref, starting from Camera movement from Vref, starting from Effect from Vref, starting from Table 1 Overview of video reference tasks. Vref: reference video; I: first-frame image; p: prompt; Vtgt: generated video. Figure 3 Video diffusion models are inherently capable of handling temporal consistency through spatial context."
        },
        {
            "title": "4.2 Task-aware Positional Bias\nIC-Lora [24] proposes the assumption that “text-to-image models inherently possess in-context generation\ncapabilities”. This hypothesis has been well validated in image customization methods [5, 45, 63], and is\nalso supported in appearance-consistent video customization [38, 67, 72], where reference appearances are\nleveraged through temporal in-context learning. However, it remains unclear whether current video diffusion\nmodels exhibit comparable in-context capabilities for temporal consistency tasks. To investigate this, we\ndirectly evaluate the video consistency of two representative models, Wan 2.1 [53] and Seedance [11], under\nthe T2V setting, as shown in Fig. 3. We observe that when generating side-by-side videos, both models easily\nmaintain motion consistency across columns. In contrast, when generating two temporally consecutive shots,\nthe model fails to keep actions consistent across two shots. This observation motivates us to propose a new\nassumption for video models: video diffusion models are inherently capable of handling temporal consistency",
            "content": "5 through spatial context. Based on the proposed assumption, we introduce Task-aware Positional Bias. Specifically, for temporal reference tasks, we add an offset to the RoPE of the reference video along the spatial (width) dimension, aiming to leverage spatial in-context cues to enhance temporal consistency. The offset is set to the width of the target video, wtgt. In contrast, for appearance reference tasks, to exploit the temporal propagation of appearance information in the video model, we add an offset along the temporal dimension, equal to the number of frames in the target video, . In summary, we define the RoPE rotation of the reference latents as θ : (cid:40) θ() = Rθ(, = (0, wtgt, 0)), for temporal ref. Rθ(, = (f, 0, 0)), for appearance. ref., (2) where = (T , , ) represents the offsets applied along the temporal, width, and height dimensions."
        },
        {
            "title": "4.3 Reference-decoupled Causal Learning",
            "content": "A straightforward way to enable interaction between reference and target videos is through joint self-attention. However, our experiments reveal that this bidirectional attention mechanism may lead to two main issues in video transfer tasks: 1) The generated videos often exhibit simple copy-paste effect from the reference video. We attribute this to the reference branchs full access to the target video context, which encourages it to adopt target-like representation, resulting in direct copying of simple patterns. 2) Concatenating the reference and target videos for joint self-attention doubles the number of tokens, leading to fourfold increase in computational complexity, which is often impractical. To address the aforementioned issues, we propose fully decoupling the reference and target branches, as illustrated in Fig. 2. Formally, both reference and target tokens are first projected into the queries, keys, and values, yielding Qref, Kref, Vref, Qtgt, Ktgt and Vtgt. Next, task-aware positional bias is applied to Qref and Kref, while Ktgt and Vtgt use the standard RoPE positional encoding. Subsequently, attention interacts between the two branches in causal manner. The reference branch performs intra-branch self-attention to capture internal contextual dependencies: Attnref = Attn(R θ(Qref), θ(Kref), Vref), (3) while the target branch integrates information from both its own features and the reference features by concatenating the keys and values: Attntgt=Attn(Rθ(Qtgt),[Rθ(Ktgt);R θ(Kref)],[Vtgt;Vref]), (4) where [; ] denotes token-wise concatenation. We further decouple the time embeddings of the two branches. Specifically, the reference branch adopts fixed = 0, making it independent of the target videos noise level. Thanks to this design, the reference branch becomes time-invariant during inference, effectively reducing computational overhead and shortening the overall generation time by 20% compared to the standard architecture."
        },
        {
            "title": "4.4 Task-adaptive Multimodal Alignment",
            "content": "In multi-task video transfer, different tasks demand reference information with distinct semantic focuses. However, conventional in-context learning in diffusion models primarily captures shallow visual correspondences rather than semantic intent, limiting their adaptability across tasks. To overcome this, we replace the original T5 [48] features in Wan with representations from MLLM, i.e., Qwen-2.5-VL [2], introducing richer visualsemantic cues that enhance reference understanding and task-level alignment. The MLLM takes as input the first-frame tokens of the target video, the reference video tokens, template tokens, and prompt tokens. To extract task-specific representations, we draw inspiration from MetaQuery [46] and introduce set of learnable tokens dedicated to each task. For temporal tasks, the MetaQuery aggregates temporal cues from the reference video together with the targets first-frame content, effectively capturing 6 Figure 4 Qualitative comparison for appearance video transfer. The yellow box highlights how OmniTransfer captures richer appearance details from multiple cross-view video frames. The red box denotes the input image for image-reference methods. cross-frame dynamics in the generated sequence. For appearance tasks, it instead fuses identity or style information from the reference with the semantic context derived from the prompt tokens. To preserve the multimodal reasoning capability while enabling parameter-efficient adaptation, the MLLM is fine-tuned using LoRA [20]. For seamless integration with the diffusion model, its outputs are passed through three-layer MultiLayer Perceptron (MLP) and injected solely into the target branch, thereby preventing interference with the reference branch. Consequently, the cross-attention in the target branch is defined as Attn(Qtgt, KMLLM, VMLLM), where KMLLM and VMLLM denote the keys and values derived from the aligned MLLM features."
        },
        {
            "title": "5 Experiment",
            "content": "5."
        },
        {
            "title": "Implementation Details",
            "content": "Our training process is divided into three sequential stages with distinct optimization objectives. In the first stage, we train the DiT blocks via in-context learning. Subsequently, we freeze the DiT blocks and focus on training the connector to align the MLLM with diffusion models. Finally, we unfreeze all components to conduct joint fine-tuning. In terms of training hyperparameters, we set the learning rate to 1e 5, with batch size of 16. The three stages are trained for 10,000, 2,000, and 5,000 training steps, respectively. Due to the lack of public data sets containing reference video pairs at present, we collected our own data sets from the Internet to support spatio-temporal video transfer."
        },
        {
            "title": "5.2 Evaluation Details",
            "content": "To evaluate spatio-temporal video transfer, we curated dedicated test sets for each subtask. For ID transfer, 50 videos of diverse individuals are paired with two prompts each to test identity consistency. Style transfer includes 20 unseen visual styles, each with two prompts to assess stylistic variation. Effect transfer contains 7 Figure 5 Qualitative comparison for appearance video transfer. 50 unseen effects from visual-effects websites. Camera movement transfer uses 50 professionally shot videos with complex trajectories. Motion transfer comprises 50 popular dance videos covering diverse dynamic and fine-grained body motions."
        },
        {
            "title": "5.3 Comparison",
            "content": "5.3."
        },
        {
            "title": "ID Transfer",
            "content": "We compared our method with State-Of-The-Art (SOTA) video ID preservation approaches, including ConsisID [72], Phantom [38] and Stand-in [67]. Following Phantom, we measure textvideo alignment using the CLIP-T [61] score, and assess identity consistency with several face recognition models, including"
        },
        {
            "title": "Method",
            "content": "ConsisID [72] Phantom [38] Stand-in [67] Ours VSim-Arc 0.34 0.45 0.30 0.48 VSim-Cur 0.32 0.41 0.21 0. VSim-Glint 0.36 0.47 0.26 0.51 CLIP-T 21.54 20.34 20.38 20.35 Table 2 Quantitative comparison for ID Transfer."
        },
        {
            "title": "Method",
            "content": "StyleCrafter [37] StyleMaster [71] Ours VCSD 0.44 0.29 0.51 CLIP-T 24.72 26.82 27. Aesthetics 0.47 0.59 0.61 Table 3 Quantitative comparison for Style Transfer. Sim-Arc [10], Sim-Cur [25], and Sim-Glint [1]. We further extend these metrics to video-level similarity by matching four evenly sampled reference frames with all generated frames, denoted as VSim-Arc, VSim-Cur, and VSim-Glint. Leveraging multi-view facial information, our method achieves high facial similarity with natural, fluid motions. As shown in the yellow box of Fig. 4, it preserves fine details like acne across frames, which is difficult for image reference methods. In Fig. 4 (right), our method generates diverse posesincluding frontal, profile, and tilted viewswhile maintaining high facial similarity, highlighting the advantage of using video as the reference."
        },
        {
            "title": "5.3.2 Style Transfer",
            "content": "We compared our method with SOTA text-to-video stylization approaches, StyleCrafter [37] and StyleMaster [71]. Following StyleMaster, we evaluated textvideo alignment with CLIP-T, while aesthetics with the Aesthetics Score [27], and style consistency with the video CSD Score [51] (VCSD) using four sampled reference frames. Table 3 shows that our method outperforms others on all three metrics. For qualitative comparison in Fig. 4, we only include StyleMaster, as StyleCrafter yields lower visual quality due to its earlier UNet-based design. The results demonstrate that our method effectively captures style from the video, surpassing previous image-based methods."
        },
        {
            "title": "5.3.3 Effect Transfer",
            "content": "Since our test set comes from effect websites with effects that existing commercial LoRA models cannot reproduce, we instead compare our method with SOTA image-to-video models, Wan 2.1 I2V [53] and Seedance [11], using prompts generated by Qwen-2.5-VL [2] from the reference effect videos. With no standard metrics for effect consistency, we conduct user study with 20 volunteers, who rate effect fidelity, first-frame consistency, and overall visual quality on five-point scale. As shown in Table 4, our method achieves the highest scores on all three metrics. Qualitative results in Fig. 4 further show that only our method accurately reproduces the effects of the reference videos, outperforming Seedance and Wan I2V. This demonstrates that text alone is insufficient, emphasizing the value of temporal video references."
        },
        {
            "title": "5.3.4 Camera Movement Transfer",
            "content": "We compare our method with SOTA camera movement models, MotionClone [36] and CamCloneMaster [40]. Since the estimation of complex camera trajectory remains challenging problem, we conducted user study using the same setup as in Section 5.3.3, evaluating camera fidelity, image consistency, and overall quality."
        },
        {
            "title": "Method",
            "content": "Wan2.1 I2V [53] Seedance I2V [11] Ours Effect Fidelity 1.81 1.95 3.45 Image Consistency 2.89 3.20 3. Table 4 User study on Effect Transfer."
        },
        {
            "title": "Method",
            "content": "MotionClone [36] CamCloneMaster [40] Camera Fidelity 1.75 1.79 Image Consistency 1.23 1.45 Ours 4.19 3. Table 5 User study on Camera Movement Transfer. Quality 2.03 2.42 3.27 Quality 1.29 1.29 3.85 Figure 6 Qualitative ablation study comparison. Best viewed in zoom. As shown in Table 5, our method outperforms all others across the three metrics. The qualitative results in Fig. 5 show that competing methods produce only fixed-resolution outputs, resulting in unavoidable resizing or cropping. Moreover, our approach is the only one that successfully replicates camera movements from cinematic scenes (left) and complex professional tracking shots (right), demonstrating strong generalization capability."
        },
        {
            "title": "5.3.5 Motion Transfer",
            "content": "We compare our method with state-of-the-art motion transfer approaches, MimicMotion [75] and WanAnimate [9]. As the test set contains in-the-wild videos without paired ground truth, we conducted user study following Section 5.3.3, evaluating motion fidelity, image consistency, and overall quality. As shown in Table 5, despite using the smaller 14B Wan 2.1 model without additional pose input, our method achieves the highest image consistency, with motion consistency and quality comparable to WanAnimate, which relies on the larger 28B Wan 2.2 model. Qualitative comparisons in Fig. 6 show that baseline methods need to align the first frame to target pose, often causing appearance loss (bottom-right of Fig. 6) and unnatural motion. In contrast, our method requires no pose guidance, preserves appearance, produces natural motions, and easily extends to multi-person scenarios  (Fig. 1)  ."
        },
        {
            "title": "Method",
            "content": "MimicMotion [75] WanAnimate [9] Ours Motion Fidelity 2.67 3.71 3.62 Image Consistency 1.84 3.53 3. Quality 2.02 3.48 3.45 Table 6 User study on Motion Transfer."
        },
        {
            "title": "Method",
            "content": "Baseline +TPB +RCL +TMA (Full)"
        },
        {
            "title": "TPB",
            "content": "Appearance Consistency/Quality Temporal Consistency/Quality 2.36 / 2.53 2.82 / 2.86 3.10 / 3.16 3.27 / 3. 2.69 / 2.70 2.95 / 2.94 3.13 / 3.10 3.36 / 3."
        },
        {
            "title": "Time",
            "content": "180s 180s 142s 145s Table 7 Ablation study. Time indicates inference time per sample (480p, 81 frames, 8NVIDIA A100)."
        },
        {
            "title": "5.4 Ablation Study",
            "content": "Ablation experiments are conducted incrementally from Baseline by successively adding TPB, RCL and TMA: Baseline. We use vanilla in-context learning setup, where reference features are concatenated along the temporal dimension [38, 40], and full attention is applied. +TPB. This model incorporates the Task-aware Positional Bias (TPB) defined in Eq. (2). +RCL. Full attention is replaced by Reference-decoupled Causal Learning (RCL). +TMA (Full Model). The Task-adaptive Multimodal Alignment (TMA) is added to form the complete model. The ablation study is conducted on 20 cases for each of the appearance and temporal transfer tasks, with user study rating overall quality and reference consistency on five-point scale. The results are presented in Table 7, with qualitative comparison in Fig. 6 for more comprehensive analysis of each modules contributions. (1) Baseline: Without TPB, subtle motion cannot be effectively transferred (Fig. 6-e), and task confusion occurs where appearance cues leak into temporal transfer (Fig. 6-a). (2) +TPB: Fine-grained motion transfer is achieved by leveraging spatial context (Fig. 6-e), while task confusion is alleviated by different RoPE biases (Fig. 6-a). (3) +RCL: The copypaste issue is alleviated. For example, in Fig. 6-a and d, the identity is not fully copied; in Fig. 6-c the face appears more natural, highlighting the effectiveness of causal attention. Additionally, RCL also improves inference speed by 20%. (4) +TMA (Full Model): The TMA module substantially enhances semantic understanding. For instance, in Fig. 6-a, the model understands that the scene remains unchanged; in Fig. 6-b, it correctly understands and generates the money rather than simply copying reference patterns; and in Fig. 6-c, richer semantic control enables realistic details such as leather jacket, beard, and side-face angle. These results demonstrate the effectiveness of semantic guidance in improving scene controllability."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we introduced OmniTransfer, unified framework for spatio-temporal video transfer. By integrating Task-aware Positional Bias, Reference-decoupled Causal Learning, and Task-adaptive Multimodal Alignment, our approach effectively leverages multi-view and temporal information from reference videos, enabling fine-grained and consistent video generation across diverse tasks. Extensive experiments verify that OmniTransfer not only achieves superior performance in both appearance and temporal transfer but also establishes new paradigm for flexible, high-fidelity video generation."
        },
        {
            "title": "7 Acknowledgments",
            "content": "We would like to express our sincere gratitude to Junjie Luo, Pengqi Tu, Qi Chen, Qichao Sun and Wanquan Feng for their insightful discussions and valuable data contributions."
        },
        {
            "title": "References",
            "content": "[1] Xiang An, Xuhan Zhu, Yuan Gao, Yang Xiao, Yongle Zhao, Ziyong Feng, Lan Wu, Bin Qin, Ming Zhang, Debing Zhang, and Ying Fu. Partial fc: Training 10 million identities on single machine. In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), 2021. [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [3] Di Chang, Yichun Shi, Quankai Gao, Hongyi Xu, Jessica Fu, Guoxian Song, Qing Yan, Yizhe Zhu, Xiao Yang, and Mohammad Soleymani. Magicpose: Realistic human poses and facial expressions retargeting with identity-aware diffusion. In International Conference on Machine Learning (ICML), 2023. [4] Hila Chefer, Shiran Zada, Roni Paiss, Ariel Ephrat, Omer Tov, Michael Rubinstein, Lior Wolf, Tali Dekel, Tomer Michaeli, and Inbar Mosseri. Still-moving: Customized video generation without customized video data. ACM Transactions on Graphics (ToG), 43(6), 2024. [5] Bowen Chen, Mengyi Zhao, Haomiao Sun, Li Chen, Xu Wang, Kang Du, and Xinglong Wu. Xverse: Consistent multi-subject control of identity and semantic attributes via dit modulation. arXiv preprint arXiv:2506.21416, 2025. [6] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. [7] Jinshu Chen, Xinghui Li, Xu Bai, Tianxiang Ma, Pengze Zhang, Zhuowei Chen, Gen Li, Lijie Liu, Songtao Zhao, Bingchuan Li, and Qian He. Omniinsert: Mask-free video insertion of any reference via diffusion transformer models. https://arxiv.org/abs/2509.17627, 2025. [8] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Yuwei Fang, Kwot Sin Lee, Ivan Skorokhodov, Kfir Aberman, Jun-Yan Zhu, Ming-Hsuan Yang, and Sergey Tulyakov. Multi-subject open-set personalization in video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 60996110, 2025. [9] Gang Cheng, Xin Gao, Li Hu, Siqi Hu, Mingyang Huang, Chaonan Ji, Ju Li, Dechao Meng, Jinwei Qi, Penchong Qiao, et al. Wan-animate: Unified character animation and replacement with holistic replication. arXiv preprint arXiv:2509.14055, 2025. [10] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. [11] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. [12] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion features for consistent video editing. In International Conference on Learning Representations (ICLR), 2024. [13] Jiaqi Guo, Lianli Gao, Junchen Zhu, Jiaxin Zhang, Siyang Li, and Jingkuan Song. Magicvfx: Visual effects synthesis in just minutes. In Proceedings of the ACM International Conference on Multimedia (ACMM), page 82388246, 2024. [14] Xu Guo, Fulong Ye, Xinghui Li, Pengqi Tu, Pengze Zhang, Qichao Sun, Songtao Zhao, Xiangwang Hou, and Qian He. Dreamid-v:bridging the image-to-video gap for high-fidelity face swapping via diffusion transformer. https://arxiv.org/abs/2601.01425, 2026. 12 [15] Zinan Guo, Yanze Wu, Chen Zhuowei, Peng Zhang, Qian He, et al. Pulid: Pure and lightning id customization via contrastive alignment. Advances in Neural Information Processing Systems (NeurIPS), 37:3677736804, 2024. [16] Zinan Guo, Pengze Zhang, Yanze Wu, Chong Mou, Songtao Zhao, and Qian He. Musar: Exploring multi-subject customization from single-subject dataset via attention routing. arXiv preprint arXiv:2505.02823, 2025. [17] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2025. [18] Xuanhua He, Quande Liu, Shengju Qian, Xin Wang, Tao Hu, Ke Cao, Keyu Yan, and Jie Zhang. Id-animator: Zero-shot identity-preserving human video generation. ArXiv preprint, 2024. [19] Chen Hou and Zhibo Chen. Training-free camera control for video generation. In International Conference on Learning Representations (ICLR), 2025. [20] Edward Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations (ICLR), 2022. [21] Li Hu. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8153 8163, 2024. [22] Teng Hu, Jiangning Zhang, Ran Yi, Yating Wang, Hongrui Huang, Jieyu Weng, Yabiao Wang, and Lizhuang Ma. Motionmaster: Training-free camera motion transfer for video generation. arXiv preprint arXiv:2404.15789, 2024. [23] Teng Hu, Zhentao Yu, Zhengguang Zhou, Sen Liang, Yuan Zhou, Qin Lin, and Qinglin Lu. Hunyuancustom: multimodal-driven architecture for customized video generation. ArXiv preprint, 2025. [24] Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, Yutong Feng, Yu Liu, and Jingren Zhou. In-context lora for diffusion transformers. arXiv preprint arxiv:2410.23775, 2024. [25] Yuge Huang, Yuhan Wang, Ying Tai, Xiaoming Liu, Pengcheng Shen, Shaoxin Li, Jilin Li, and Feiyue Huang. Curricularface: Adaptive curriculum learning loss for deep face recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. [26] Yuzhou Huang, Ziyang Yuan, Quande Liu, Qiulin Wang, Xintao Wang, Ruimao Zhang, Pengfei Wan, Di Zhang, and Kun Gai. Conceptmaster: Multi-concept video customization on diffusion transformer models without test-time tuning. arXiv preprint arXiv:2501.04698, 2025. [27] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [28] Hyeonho Jeong, Geon Yeong Park, and Jong Chul Ye. Vmc: Video motion customization using temporal attention adaption for text-to-video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 92129221, 2024. [29] Liming Jiang, Qing Yan, Yumin Jia, Zichuan Liu, Hao Kang, and Xin Lu. Infiniteyou: Flexible photo recrafting while preserving your identity. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1089810907, 2025. [30] Max Ku, Cong Wei, Weiming Ren, Huan Yang, and Wenhu Chen. Anyv2v: plug-and-play framework for any video-to-video editing tasks. arXiv preprint arXiv:2403.14468, 2024. [31] Mengtian Li, Jinshu Chen, Wanquan Feng, Bingchuan Li, Fei Dai, Songtao Zhao, and Qian He. Hyperlora: Parameter-efficient adaptive generation for portrait synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1311413123, 2025. [32] Teng Li, Guangcong Zheng, Rui Jiang, Shuigen Zhan, Tao Wu, Yehao Lu, Yining Lin, Chuanyun Deng, Yepan Xiong, Min Chen, Lin Cheng, and Xi Li. Realcam-i2v: Real-world image-to-video generation with interactive complex camera control. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2878528796, 2025. 13 [33] Wen Li, Muyuan Fang, Cheng Zou, Biao Gong, Ruobing Zheng, Meng Wang, Jingdong Chen, and Ming Yang. Styletokenizer: Defining image style by single instance for controlling diffusion models. In European Conference on Computer Vision (ECCV), page 110126, 2024. [34] Xinghui Li, Qichao Sun, Pengze Zhang, Fulong Ye, Zhichao Liao, Wanquan Feng, Songtao Zhao, and Qian He. Anydressing: Customizable multi-garment virtual dressing via latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2372323733, 2025. [35] Feng Liang, Haoyu Ma, Zecheng He, Tingbo Hou, Ji Hou, Kunpeng Li, Xiaoliang Dai, Felix Juefei-Xu, Samaneh Azadi, Animesh Sinha, Peizhao Zhang, Peter Vajda, and Diana Marculescu. Movie weaver: Tuning-free multiconcept video personalization with anchored prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1314613156, 2025. [36] Pengyang Ling, Jiazi Bu, Pan Zhang, Xiaoyi Dong, Yuhang Zang, Tong Wu, Huaian Chen, Jiaqi Wang, and Yi Jin. Motionclone: Training-free motion cloning for controllable video generation. In International Conference on Learning Representations (ICLR), 2025. [37] Gongye Liu, Menghan Xia, Yong Zhang, Haoxin Chen, Jinbo Xing, Yibo Wang, Xintao Wang, Ying Shan, and Yujiu Yang. Stylecrafter: Taming artistic video diffusion with reference-augmented adapter learning. ACM Transactions on Graphics (ToG), 43(6), 2024. [38] Lijie Liu, Tianxiang Ma, Bingchuan Li, Zhuowei Chen, Jiawei Liu, Gen Li, Siyu Zhou, Qian He, and Xinglong Wu. Phantom: Subject-consistent video generation via cross-modal alignment. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1495114961, 2025. [39] Xinyu Liu, Ailing Zeng, Wei Xue, Harry Yang, Wenhan Luo, Qifeng Liu, and Yike Guo. Vfx creator: Animated visual effect generation with controllable diffusion transformer. arXiv preprint arXiv:2502.05979, 2025. [40] Yawen Luo, Jianhong Bai, Xiaoyu Shi, Menghan Xia, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, and Tianfan Xue. Camclonemaster: Enabling reference-based camera control for video generation. arXiv preprint arXiv:2506.03140, 2025. [41] Yuxuan Luo, Zhengkun Rong, Lizhen Wang, Longhao Zhang, Tianshu Hu, and Yongming Zhu. Dreamactor-m1: Holistic, expressive and robust human image animation with hybrid guidance. arXiv preprint arXiv:2504.01724, 2025. [42] Yue Ma, Yulong Liu, Qiyuan Zhu, Ayden Yang, Kunyu Feng, Xinhua Zhang, Zhifeng Li, Sirui Han, Chenyang Qi, and Qifeng Chen. Follow-your-motion: Video motion transfer via efficient spatial-temporal decoupled finetuning. arXiv preprint arXiv:2506.05207, 2025. [43] Fangyuan Mao, Aiming Hao, Jintao Chen, Dongxia Liu, Xiaokun Feng, Jiashu Zhu, Meiqi Wu, Chubin Chen, Jiahong Wu, and Xiangxiang Chu. Omni-effects: Unified and spatially-controllable visual effects generation. arXiv preprint arXiv:2508.07981, 2025. [44] Chong Mou, Qichao Sun, Yanze Wu, Pengze Zhang, Xinghui Li, Fulong Ye, Songtao Zhao, and Qian He. Instructx: Towards unified visual editing with mllm guidance. https://arxiv.org/abs/2510.08485, 2025. [45] Chong Mou, Yanze Wu, Wenxu Wu, Zinan Guo, Pengze Zhang, Yufeng Cheng, Yiming Luo, Fei Ding, Shiwen Zhang, Xinghui Li, et al. Dreamo: unified framework for image customization. arXiv preprint arXiv:2504.16915, 2025. [46] Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, Ji Hou, and Saining Xie. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. [47] Alexander Pondaven, Aliaksandr Siarohin, Sergey Tulyakov, Philip Torr, and Fabio Pizzati. Video motion transfer In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern with diffusion transformers. Recognition (CVPR), pages 2291122921, 2025. [48] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research (JMLR), 21(140):167, 2020. [49] Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. Advances in Neural Information Processing Systems (NeurIPS), 32, 2019. [50] Aliaksandr Siarohin, Oliver Woodford, Jian Ren, Menglei Chai, and Sergey Tulyakov. Motion representations for articulated animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1365313662, 2021. [51] Gowthami Somepalli, Anubhav Gupta, Kamal Gupta, Shramay Palta, Micah Goldblum, Jonas Geiping, Abhinav Shrivastava, and Tom Goldstein. Measuring style similarity in diffusion models. arXiv preprint arXiv:2404.01292, 2024. [52] Shuyuan Tu, Zhen Xing, Xintong Han, Zhi-Qi Cheng, Qi Dai, Chong Luo, and Zuxuan Wu. Stableanimator: Highquality identity-preserving human image animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2109621106, 2025. [53] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [54] Haofan Wang, Matteo Spinelli, Qixun Wang, Xu Bai, Zekui Qin, and Anthony Chen. Instantstyle: Free lunch towards style-preserving in text-to-image generation. arXiv preprint arXiv:2404.02733, 2024. [55] Haofan Wang, Peng Xing, Renyuan Huang, Hao Ai, Qixun Wang, and Xu Bai. Instantstyle-plus: Style transfer with content-preserving in text-to-image generation. arXiv preprint arXiv:2407.00788, 2024. [56] Luozhou Wang, Ziyang Mai, Guibao Shen, Yixun Liang, Xin Tao, Pengfei Wan, Di Zhang, Yijun Li, and Ying-Cong Chen. Motion inversion for video customization. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers (SIGGRAPH), 2025. [57] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, Anthony Chen, Huaxia Li, Xu Tang, and Yao Hu. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024. [58] Qinghe Wang, Yawen Luo, Xiaoyu Shi, Xu Jia, Huchuan Lu, Tianfan Xue, Xintao Wang, Pengfei Wan, Di Zhang, and Kun Gai. Cinemaster: 3d-aware and controllable framework for cinematic text-to-video generation. arXiv preprint arXiv:2502.08639, 2025. [59] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. Advances in Neural Information Processing Systems (NeurIPS), 36, 2024. [60] Xiang Wang, Shiwei Zhang, Longxiang Tang, Yingya Zhang, Changxin Gao, Yuehuan Wang, and Nong Sang. Unianimate-dit: Human image animation with large-scale video diffusion transformer. arXiv preprint arXiv:2504.11289, 2025. [61] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, et al. Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191, 2022. [62] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. arXiv preprint arXiv:2312.03641, 2024. [63] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1329413304, 2025. [64] Peng Xing, Haofan Wang, Yanpeng Sun, Qixun Wang, Xu Bai, Hao Ai, Renyuan Huang, and Zechao Li. Csgo: Content-style composition in text-to-image generation. arXiv preprint arXiv:2408.16766, 2024. [65] Dejia Xu, Weili Nie, Chao Liu, Sifei Liu, Jan Kautz, Zhangyang Wang, and Arash Vahdat. Camco: Cameracontrollable 3d-consistent image-to-video generation. arXiv preprint arXiv:2406.02509, 2024. [66] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human image animation using diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1481 1490, 2024. [67] Bowen Xue, Qixin Yan, Wenjing Wang, Hao Liu, and Chen Li. Stand-in: lightweight and plug-and-play identity control for video generation. arXiv preprint arXiv:2508.07901, 2025. [68] Danah Yatim, Rafail Fridman, Omer Bar-Tal, Yoni Kasten, and Tali Dekel. Space-time diffusion features for zero-shot text-driven motion transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 84668476, 2024. [69] Fulong Ye, Miao Hua, Pengze Zhang, Xinghui Li, Qichao Sun, Songtao Zhao, Qian He, and Xinglong Wu. Dreamid: High-fidelity and fast diffusion-based face swapping via triplet id group learning. In Proceedings of the SIGGRAPH Asia 2025 Conference Papers, 2025. [70] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. [71] Zixuan Ye, Huijuan Huang, Xintao Wang, Pengfei Wan, Di Zhang, and Wenhan Luo. Stylemaster: Stylize your video with artistic generation and translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 26302640, 2025. [72] Shenghai Yuan, Jinfa Huang, Xianyi He, Yunyang Ge, Yujun Shi, Liuhan Chen, Jiebo Luo, and Li Yuan. Identitypreserving text-to-video generation by frequency decomposition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1297812988, 2025. [73] Pengze Zhang, Lingxiao Yang, Jian-Huang Lai, and Xiaohua Xie. Exploring dual-task correlation for pose guided person image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 77137722, 2022. [74] Shiyi Zhang, Junhao Zhuang, Zhaoyang Zhang, Ying Shan, and Yansong Tang. Flexiact: Towards flexible action control in heterogeneous scenarios. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers (SIGGRAPH), 2025. [75] Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, JunqiCheng, Yuefeng Zhu, and FangYuan Zou. Mimicmotion: High-quality human motion video generation with confidence-aware pose guidance. In International Conference on Machine Learning (ICML), 2025. [76] Jian Zhao and Hui Zhang. Thin-plate spline motion model for image animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 36573666, 2022. [77] Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao Zhang, Jia-Wei Liu, Weijia Wu, Jussi Keppo, and In European Mike Zheng Shou. Motiondirector: Motion customization of text-to-video diffusion models. Conference on Computer Vision (ECCV), page 273290, 2024. [78] Guangcong Zheng, Teng Li, Rui Jiang, Yehao Lu, Tao Wu, and Xi Li. Cami2v: Camera-controlled image-to-video diffusion model. arXiv preprint arXiv:2410.15957, 2024. [79] Jingkai Zhou, Yifan Wu, Shikai Li, Min Wei, Chao Fan, Weihua Chen, Wei Jiang, and Fan Wang. Realisdance-dit: Simple yet strong baseline towards controllable character animation in the wild. arXiv preprint arXiv:2504.14977, 2025. [80] Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Zilong Dong, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu. Champ: Controllable and consistent human image animation with 3d parametric guidance. In European Conference on Computer Vision (ECCV), pages 145162, 2025."
        },
        {
            "title": "A Additional Comparison Results",
            "content": "We present additional comparisons for the ID transfer task in Fig. 7 and Fig. 8. Fig. 9 and Fig. 10 show further comparisons for the style transfer task. Fig. 11 and Fig. 12 provide more comparisons for effect transfer. Fig. 13 and Fig. 14 present additional results for camera motion transfer, and Fig. 15 and Fig. 16 show further comparisons for motion transfer."
        },
        {
            "title": "B Video Transfer Combination",
            "content": "By concatenating, respectively, the reference video tokens and the MLLM tokens across different tasks, our approach enables seamless combination of multiple video transfer operations. Fig. 17 and Fig. 18 demonstrate the superiority of our method in handling entirely unseen task combinations, highlighting its strong generalization capability. 17 Figure 7 Additional ID transfer comparison with ConsisID [72], Phantom [38], and Stand-in [67] (Set 1). 18 Figure 8 Additional ID transfer comparison with ConsisID [72], Phantom [38], and Stand-in [67] (Set 2). Figure 9 Additional style transfer comparison with StyleCrafter [37] and StyleMaster [71] (Set 1). 20 Figure 10 Additional style transfer comparison with StyleCrafter [37] and StyleMaster [71] (Set 2). 21 Figure 11 Additional effect transfer comparison with Wan2.1 I2V [53] and Seedance 1.0 I2V [11] (Set 1). Figure 12 Additional effect transfer comparison with Wan2.1 I2V [53] and Seedance 1.0 I2V [11] (Set 2). 23 Figure 13 Additional camera movement transfer comparison with MotionClone [36] and CamCloneMaster [40] (Set 1). 24 Figure 14 Additional camera movement transfer comparison with MotionClone [36] and CamCloneMaster [40] (Set 2). Figure 15 Additional motion transfer comparison with MimicMotion [75] and WanAnimate [9] (Set 1). Unavailable indicates that pose-based methods fail to generate results due to errors in pose extraction or pose alignment. 26 Figure 16 Additional motion transfer comparison with MimicMotion [75] and WanAnimate [9] (Set 2). Unavailable indicates that pose-based methods fail to generate results due to errors in pose extraction or pose alignment. 27 Figure 17 Video transfer combination results(Set 1). Figure 18 Video transfer combination results (Set 2)."
        }
    ],
    "affiliations": [
        "Intelligent Creation Lab, ByteDance"
    ]
}