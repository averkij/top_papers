{
    "paper_title": "VideoVista-CulturalLingo: 360$^\\circ$ Horizons-Bridging Cultures, Languages, and Domains in Video Comprehension",
    "authors": [
        "Xinyu Chen",
        "Yunxin Li",
        "Haoyuan Shi",
        "Baotian Hu",
        "Wenhan Luo",
        "Yaowei Wang",
        "Min Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Assessing the video comprehension capabilities of multimodal AI systems can effectively measure their understanding and reasoning abilities. Most video evaluation benchmarks are limited to a single language, typically English, and predominantly feature videos rooted in Western cultural contexts. In this paper, we present VideoVista-CulturalLingo, the first video evaluation benchmark designed to bridge cultural, linguistic, and domain divide in video comprehension. Our work differs from existing benchmarks in the following ways: 1) Cultural diversity, incorporating cultures from China, North America, and Europe; 2) Multi-linguistics, with questions presented in Chinese and English-two of the most widely spoken languages; and 3) Broad domain, featuring videos sourced from hundreds of human-created domains. VideoVista-CulturalLingo contains 1,389 videos and 3,134 QA pairs, and we have evaluated 24 recent open-source or proprietary video large models. From the experiment results, we observe that: 1) Existing models perform worse on Chinese-centric questions than Western-centric ones, particularly those related to Chinese history; 2) Current open-source models still exhibit limitations in temporal understanding, especially in the Event Localization task, achieving a maximum score of only 45.2%; 3) Mainstream models demonstrate strong performance in general scientific questions, while open-source models demonstrate weak performance in mathematics."
        },
        {
            "title": "Start",
            "content": "VideoVista-CulturalLingo: 360 Horizons-Bridging Cultures, Languages, and Domains in Video Comprehension Xinyu Chen1, Yunxin Li1, Haoyuan Shi1, Baotian Hu1*, Wenhan Luo2, Yaowei Wang1, Min Zhang1, 1Harbin Institute of Technology, Shenzhen, China, 2Hong Kong University of Science and Technology, {chenxinyuhitsz, liyunxin987}@163.com {hubaotian, zhangmin2021}@hit.edu.cn https://github.com/HITsz-TMG/VideoVista https://videovista-culturallingo.github.io/ https://huggingface.co/datasets/Uni-MoE/VideoVista-CulturalLingo 5 2 0 A 3 2 ] . [ 1 1 2 8 7 1 . 4 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Assessing the video comprehension capabilities of multimodal AI systems can effectively measure their understanding and reasoning abilities. Most video evaluation benchmarks are limited to single language, typically English, and predominantly feature videos rooted in Western cultural contexts. In this paper, we present VideoVista-CulturalLingo, the first video evaluation benchmark designed to bridge cultural, linguistic, and domain divide in video comprehension. Our work differs from existing benchmarks in the following ways: 1) Cultural diversity, incorporating cultures from China, North America, and Europe; 2) Multilinguistics, with questions presented in Chinese and Englishtwo of the most widely spoken languages; and 3) Broad domain, featuring videos sourced from hundreds of humancreated domains. VideoVista-CulturalLingo contains 1,389 videos and 3,134 QA pairs, and we have evaluated 24 recent open-source or proprietary video large models. From the experiment results, we observe that: 1) Existing models perform worse on Chinese-centric questions than Western-centric ones, particularly those related to Chinese history; 2) Current open-source models still exhibit limitations in temporal understanding, especially in the Event Localization task, achieving maximum score of only 45.2%; 3) Mainstream models demonstrate strong performance in general scientific questions, while open-source models demonstrate weak performance in mathematics."
        },
        {
            "title": "Introduction",
            "content": "Large Multimodal Models (LMMs) built upon Large Language Models (LLMs) have demonstrated unprecedented capabilities across various * Corresponding author. Figure 1: An example of Chinese Culture in VideoVista-CulturalLingo. The correct answer is highlighted in yellow. domains, including text, image, video, and audio over several years. Particularly in the past year, there has been surge in the development of LMMs capable of processing video inputs. The dramatic expansion in the length of video frame sequencesfrom just few frames to several hundreddemonstrates significant progress in video understanding capabilities. Meanwhile, video evaluation benchmarks have also emerged, evolving from early-stage basic video question answering tasks (Yu et al., 2019; Xu et al., 2017) to general video evaluation benchmarks (Fu et al., 2024; Zhou et al., 2024; Wang et al., 2024b). However, existing video evaluation benchmarks predominantly select videos from sources such as YouTube, Shutterstock, or established video datasets like Ego4D (Grauman et al., 2022) and Movie101 (Yue et al., 2023). These datasets are primarily Western-centric, with limited representation of Chinese-centric videos"
        },
        {
            "title": "Size",
            "content": "4 14 1,389 2,052 1,877.7 267.5 30 104 12 1,446 1,668 231 200 200 18 13 4 3,134 3,134 Figure 2: (Left) Comprehensive statistics from different perspectives. The durations reported are based on the statistics from the 2,052 video clips. The question and answer length is count in tokens; (Right) Videos in VideoVista-CulturalLingo is sourced hundreds of domains from 3 popular video websites across the world. In the video sourced from Xiaohongshu(RedNote), we only present 42 of the all domains. as shown in Figure 1. In addition, current video evaluation benchmarks tend to focus on specific events within the videos, neglecting the cultural context and connotations embedded in the content while overlooking the scientific principles and information that the videos are intended to convey. To advance the development of LMMs, we introduce VideoVista-CulturalLingo, the first video evaluation benchmark designed to bridge cultures, languages, and domains in video comprehension. In Figure 2, we present detailed statistics on the questions and videos in VideoVista-CulturalLingo. It comprises 3,134 questions organized into 14 tasks, spanning 2,052 video clips of varying lengths and reflecting both Western and Chinese cultures. English-language videos are sourced from YouTube, while Chinese videos are collected from Xiaohongshu and BiliBili. These videos cover hundreds of distinct domains, ranging from everyday life topicssuch as news reports, travel recommendations, sports events, and vlogsto scientific topics, including calculus, deep learning, organic chemistry, and quantum mechanics. To efficiently annotate such large-scale video dataset, we employ hybrid annotation framework that combines the strengths of both (M)LLMs and human efforts. This framework leverages the powerful capabilities of existing large models, such as Qwen2-VL (Wang et al., 2024a) and DeepSeekR1 (DeepSeek-AI et al., 2025), to generate an initial pool of question-options-answer (QA) pairs. Human annotators then select the high-quality questions from generated QA pairs and further refine them to enhance clarity and quality. We have evaluated 24 state-of-the-art (SOTA) LMMs, including proprietary LMMs such as GPT4o, Gemini-2.0-Flash, as well as open-source video LMMs like Qwen2.5-VL (Team, 2025) and VideoLLaMA3 (Zhang et al., 2025), and image LMMs such as Molmo (Deitke et al., 2024) and DeepSeek2-VL (Wu et al., 2024). Experimental results show that Gemini-2.0-Flash demonstrates the strongest performance among all models, achieving an accuracy score of 76.3%. Among open-source video LMMs, Qwen2.5-VL-72B achieves the highest score of 61.3%, with large performance gap compared to Gemini-2.0-Flash in video location tasks. Interestingly, Qwen2.5-VL performs best on cultural understanding, yet still achieves only 65.8% in Chinese cultural understanding. In summary, the main contributions are as follows: We present the first video evaluation benchmark that covers diverse domains, languages, and cultures in video comprehension. We introduce an autonomic video annotation framework, harnessing the strengths of (M)LLMs (including Qwen2-VL and DeepSeek-R1) and visual recognition tools (including SAM2) to improve the efficiency of video annotation. We conduct extensive experiments and in-depth analysis with VideoVista-CulturalLingo, revealing the limitations of existing LMMs in videos with different cultural or linguistic contexts."
        },
        {
            "title": "2 Related Work",
            "content": "Development of Video LMMs. Unified encoding methods for both image and video modalities have become the mainstream approach adopted by LMMs over the past year. LongVA (Zhang et al., 2024a) utilizes unified encoding method, UniRes, which allows models trained solely on image datasets to demonstrate strong potential in video evaluation tasks. Qwen2-VL (Wang et al., 2024a) and Qwen2.5VL (Team, 2025) introduce the MRoPE positional encoding, incorporating temporal, height, and width components, enabling unified positional modeling across text, image, and video modalities. LLaVA-Video (Zhang et al., 2024b) draws inspiration from the SlowFast approach, encoding video frames at varying granularities into visual sequences of different lengths, effectively addressing the issue of excessively long sequences during video encoding. Current LMMs (Chen et al., 2024c; Yao et al., 2024; Li et al., 2024a, 2025b, 2024c) are capable of unified encoding for image and video modalities, leveraging rich image modality data to enhance visual capabilities and demonstrate strong performance in video evaluation tasks. Progress of Video Benchmark. Video evaluation benchmarks have also made significant progress. Previously, evaluation datasets (Yu et al., 2019; Xu et al., 2017) typically involved posing broad questions and having the model generate one or few-word answer, which was then assessed for accuracy and scored by LLMs (Maaz et al., 2024). The videos used in these datasets were often limited to just few dozen seconds or minutes in length. Recent video benchmarks (Li et al., 2024b) have seen considerable improvements, both in the variety of evaluation tasks and the duration of the videos. Video-MME (Fu et al., 2024) has extended the evaluation video length to an hour, while also introducing twelve distinct evaluation tasks, including Temporal Reasoning and Information Synopsis. MLVU (Zhou et al., 2024) includes videos of varying lengths, ranging from 3 minutes to 2 hours, covering nine different evaluation tasks, such as Needle Question-Answering. The process of video benchmarks (Fang et al., 2024; Wang et al., 2024b; Liu et al., 2024a) have undoubtedly provided significant boost to the development of LMMs."
        },
        {
            "title": "3.1 Video Collecting and Preprocessing",
            "content": "The videos in our study can be divided into two categories: non-scientific and scientific videos. Nonscientific English videos are randomly crawled from YouTube, while their Chinese counterparts are collected from Xiaohongshu to ensure diversity within the dataset. The domains of these videos come from the original categories on the video platforms. For scientific videos, we first identified four major disciplines: mathematics, physics, chemistry, and computer science. Within each discipline, we further defined four representative subdisciplines, such as linear algebra in mathematics and quantum mechanics in physics. Domains of these videos are derived from search keywords. These sub-disciplines guide the collection of English scientific videos via the YouTube Data API. For Chinese scientific videos, human annotators manually collected videos from BiliBili. All videos undergo audio extraction via FFmpeg, followed by transcription using Whisper-Large-v3 with sentence-level timestamp alignment. An audio quality assessment pipeline is implemented using Qwen2.5-32B (Yang et al., 2024), evaluating three dimensions: logical coherence, continuity, and information density. Videos are subsequently classified as either audio-rich (high-quality speech) or audio-noisy (including silent videos). For audio-rich videos, the Qwen2.5-72B model segments transcriptions into contextually coherent paragraphs, which are synchronized with visual content through Whispers sentence-level alignment to generate short video clips. Audio-noisy videos are processed using the semantics-aware video splitting algorithm from Panda-70M (Chen et al., 2024b), which utilizes visual features to partition videos into semantically consistent segments. This process is illustrated in Figure 3 (a). To address the challenges of Chinese homophone ambiguity in transcriptions, we develop context-aware refinement module using Qwen2.572B. This module performs three key operations: (1) disambiguation of homophones through semantic analysis, (2) correction of domain-specific terminology, and (3) fluency enhancement, while strictly Figure 3: The three-stage annotation process of VideoVista-CulturalLingo. preserving original semantic content."
        },
        {
            "title": "3.2 Automatic QA Annotation",
            "content": "The annotation framework comprises four distinct tasks: Event, Culture, Object, and Science. Our pipeline employs Qwen2-VL-72B as the primary annotator, Qwen2.5-72B for text-only annotation tasks, and paraphrase-multilingual-MiniLM-L12v2 for embedding generation. For non-scientific tasks, DeepSeek-V3 (DeepSeek-AI et al., 2024) is employed as the question generator, while DeepSeek-R1 (DeepSeek-AI et al., 2025) is used for generating scientific questions. During the annotation process, while generating questions, four options and the correct answer are also created. The process of automatic QA annotation is illustrated in Figure 3 (b). The details and prompt for annotation is provided in Appendix D. Event. We input the segmented video clips and refined audio transcriptions into the event annotator to label the events occurring in each video segment. For the i-th segment, the model receives historical event annotations from the previous 1 segments to maintain temporal consistency. Each annotated segment follows the structure (event, audio, start, end), where start and end denote the timestamps marking the beginning and conclusion of the current video segment within the full video. The aggregated event sequence is then fed into the question generator, which generates questions of the corresponding task, along with four options for each question and correct answer. Specifically, for event prediction questions, the model is instructed to select the segment that is most logically related to the preceding context as the predicted content. During this process, each task is associated with specific prompt. Object. We feed videos into the object classifier to filter those videos that meet three criteria: realworld content, richness in objects, and motion in objects. The filtered videos are then processed by the object extractor to identify three to five primary objects followed by frame-wise presence detection via InternVL2-8B at 1fps sampling. The detected objects are processed through pipeline combining Grounding-DINO (Liu et al., 2023a) for bounding box prediction and SAM2 (Ravi et al., 2024) for image segmentation. The resulting information is then fed into the object description annotator to generate object-level descriptions that capture both the temporal and spatial aspects of each object. Finally, the object-level descriptions, along with the aggregated event sequence, are input into the question generator to generate the questions. Benchmarks #Videos #Clips Len.(s) #QA Pairs Anno. M.L. M.C M.D Open. MSRVTT-QA (Xu et al., 2017) MSVD-QA (Xu et al., 2017) TGIF-QA (Li et al., 2016) ActivityNet-QA (Yu et al., 2019) TVQA (Lei et al., 2018) NExT-QA (Xiao et al., 2021) MVBench (Li et al., 2023) EgoSchema (Mangalam et al., 2024) TempCompass (Liu et al., 2024a) Video-MME (Fu et al., 2024) VideoVista (Li et al., 2024b) MLVU (Zhou et al., 2024) LVBench (Wang et al., 2024b) MMBench-Video (Fang et al., 2024) VideoVista-CulturalLingo 2,990 504 9,575 800 2,179 1,000 3,641 5,063 410 900 894 1,323 500 600 1,389 2,990 504 9,575 800 15,253 1, 3,641 5,063 500 900 3,402 1,323 500 600 2,052 15.2 9.8 3.0 111.4 11.2 39.5 16.0 180.0 11.4 1024.0 131.0 720 4,101.0 165.4 72,821 13,157 8,506 8,000 15,253 8,564 4,000 5,063 7,540 2,700 24,906 2,593 1,549 1, 267.5 3,134 A&M A A&M A&M A&M A&M Table 1: The comparison of various benchmarks involves several key aspects: total number of videos (#Videos), number of clips (#Clips), average video duration (Len.), number of QA pairs (#QA Pairs), annotation method (Anno., where M/A indicates manual/automatic annotation), whether the videos span multiple language (M.L.),whether the videos span multiple culture background (M.C.) ,whether the videos span multiple duration levels (M.D.), and if the videos are sourced from diverse open domains (Open.) Culture. We input videos and audio transcriptions into the cultural classifier to evaluate their relationship to Chinese, American, and European cultures individually. Culturally relevant videos are then processed by the cultural concept extractor to identify the two most prominent cultural concepts. These cultural concepts are subsequently encoded into embeddings, which are used to retrieve the entries from pre-encoded Wikipedia data. Using these entries, along with local backup of Wikipedia, we can retrieve Wikipedia articles corresponding to the identified cultural concepts. By combining this external knowledge with the aggregated event sequence, we input the data into the specific question generator to generate the questions. Science. The video is input into the science classifier to evaluate its quality based on scientific thematic relevance and knowledge density. After filtering, the aggregated event sequence of the video is fed into the question generator, DeepSeek-R1, to generate questions. In our initial experiments, we observed two recurring issues: generated questions either relied excessively on domain knowledgedetached from the video itself and thus answerable without viewingor exhibited distractor choices that were either too divergent or too similar, producing items that were trivial or ambiguous. To resolve these shortcomings, we impose deterministic, rule-based constraints that (i) require every question to depend on video context for its solution and (ii) ensure balanced, pedagogically meaningful set of answer options. Specifically, each question presents four choices: Correct Option, Video Comprehension Error, Domain Knowledge Error, and Dual Error. This structured design rigorously evaluates models ability to integrate visual comprehension with scientific reasoning."
        },
        {
            "title": "3.3 Human Check and Revision",
            "content": "All candidate questions are first filtered linguistically using Qwen2.5-7B with the CircularEval strategy (Liu et al., 2023b) to remove any video-agnostic items. We then establish Gradiobased annotation platform that includes three assessment dimensions: correctness, type relevance, and video relevance. The correctness score ranges from 0 to 1, assessing whether the model-generated answer is correct; the type relevance score ranges from 0 to 2, evaluating the degree of relevance between the question and task type; and the video relevance score ranges from 0 to 2, determining the degree of relevance between the question and the video content, ensuring that questions are not unrelated to the video frames. Questions achieving maximum scores (score=5) across all dimensions are selected. For borderline cases (score=4), we utilize differentiated handling: first, for the question with wrong answer (correctness=0), we manually correct the answers; second, for the question with suboptimal type or video relevance, we manually refine the questions, options, and answers based on the original questions. We have illustrated this process in the Figure 3 (c). Specifically for cultural questions, two annotatorsone of whom is native speaker of the relevant cultureindependently assess each question to ensure cross-validation. Overall, this hybrid automatic/manual pipeline eliminates approximately 60 Model LLM Frames Overall Event Object Culture Science Open-source Video LMMs Vicuna-7B-v1.5 ShareGPT4Video (Chen et al., 2024a) VideoChat2-Mistral (KunChang et al., 2023) Mistral-7B-Instruct-v0.2 Video-LLaVA (Lin et al., 2023a) VideoLLaMA2 (Cheng et al., 2024) LLaVA-OneVision (Li et al., 2024a) MiniCPM-V 2.6 (Yao et al., 2024) mPLUG-Owl3 (Ye et al., 2024) Oryx-1.5 (Liu et al., 2024b) LLaVA-Video (Zhang et al., 2024b) Qwen2-VL (Wang et al., 2024a) InternVL2.5 (Chen et al., 2024c) MiniCPM-o 2.6 (Yao et al., 2024) TPO (Li et al., 2025a) InternVideo2.5 (Wang et al., 2025) VideoLLaMA3 (Zhang et al., 2025) Qwen2.5-VL-7B (Team, 2025) Qwen2.5-VL-72B (Team, 2025) Vicuna-7B-v1.5 Mistral-7B-Instruct-v0.2 Qwen2-7B-Instruct Qwen2-7B-Instruct Qwen2-7B-Instruct Qwen2.5-7B-Instruct Qwen2-7B-Instruct Qwen2-7B-Instruct Internlm2.5-7b-Chat Qwen2.5-7B-Instruct Qwen2-7B-Instruct Internlm2.5-7b-Chat Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-72B-Instruct 16f 16f 8f 32f 32f 1fps(64) 1fps(128) 128f 1fps(64) 1fps(300) 64f 1fps(64) 1fps(96) 1fps(512) 1fps(180) 1fps(300) 1fps(300) VILA1.5-13B (Lin et al., 2023b) VILA1.5-13B (Lin et al., 2023b) Molmo 7B-D (Deitke et al., 2024) Molmo 7B-D (Deitke et al., 2024) DeepSeek2-VL (Wu et al., 2024) DeepSeek2-VL (Wu et al., 2024) GPT-4o-2024-11-20 Gemini-1.5-Flash Gemini-2.0-Flash-Lite Gemini-2.0-Flash Open-source Image LMMs Vicuna-13B-v1.5 Vicuna-13B-v1.5 Qwen2-7B-Instruct Qwen2-7B-Instruct DeepSeekMoE-27B DeepSeekMoE-27B 1f 8f 1f 8f 1f 8f Proprietary LMMs GPT-4o Gemini-1.5-Flash Gemini-2.0-Flash-Lite Gemini-2.0-Flash 1fps(128) 1fps 1fps 1fps 25.6 29.6 38.2 31.4 41.8 42.9 49.9 41.4 51.0 49.7 52.0 49.0 50.6 52.0 60.7 54.3 61.3 33.3 36.9 38.3 40.3 40.9 42. 56.7 69.4 70.7 76.3 23.2 27.5 42.2 33.6 43.9 44.1 54.4 43.8 57.9 50.1 56.5 52.9 57.2 52.5 58.0 56.7 61.0 33.3 38.2 44.5 44.3 44.3 47.0 53.4 70.0 63.1 74.0 18.9 25.9 34.4 23.3 33.8 24.1 41.9 32.2 39.1 33.8 35.5 28.5 37.8 38.1 66.4 38.9 40.5 29.2 31.3 25.3 30.1 32.2 27. 38.2 65.8 71.6 77.1 31.4 34.7 34.5 34.9 38.8 49.4 45.0 37.6 48.8 54.8 56.1 55.9 49.6 58.2 53.1 55.2 71.2 33.9 38.2 39.8 41.8 39.3 44.4 68.0 59.0 63.1 68.0 34.1 33.1 41.1 36.6 53.5 62.9 60.1 55.8 60.3 68.0 65.7 67.1 60.4 65.9 64.4 73.3 83.3 39.2 41.9 46.5 48.0 50.5 57. 78.3 84.7 82.1 87.4 Table 2: Evaluation results on VideoVista-CulturalLingo benchmark. The large language model used by LMMs (LLM), frames sample strategy (Frames), overall evaluation scores (Overall), evaluation scores in Event Task(Event), evaluation scores in Object Task (Object), evaluation scores in Culture Task (Culture), evaluation scores in Science Task (Science). -[N f] indicates this LMM task frames uniformly sampled from video as input. -[N fps(M )] indicates this LMM uses frames per second uniformly sampled from video as input, with max frames number . We have highlighted the highest results in each tasks using bold. Meanwhile, the highest results within the 7B/8B open-source Video LMMs are highlighted with an underline."
        },
        {
            "title": "3.4 Statistic and Analysis",
            "content": "As shown in Figure 2, VideoVista-CulturalLingo consists of 2,052 video clips or full videos derived from 1,389 original videos, with an average duration of 267.5 seconds. Additionally, VideoVistaCulturalLingo contains 1,446 questions in Chinese and 1,668 questions in English, with comparable number of questions in both languages. In Table 1, we compare the key characteristics of our benchmark with others. Notably, VideoVistaCulturalLingo includes the largest collection of raw videos, totaling 1,389, among benchmarks that have videos multiple duration levels. These 1,389 original videos encompass diverse range of languages and cultural backgrounds, feature that sets our benchmark apart from previous ones."
        },
        {
            "title": "4.1 Baselines",
            "content": "We conducted evaluations on 17 open-source video LMMs, 3 image LMMs, and 4 proprietary LMMs, including the recently released Gemini2.0-Flash, Qwen2.5-VL (Team, 2025), VideoLLaMA3 (Zhang et al., 2025), DeepSeek2-VL (Wu et al., 2024), among others. The detailed experiment settings are shown in Appendix B."
        },
        {
            "title": "4.2 Main Results",
            "content": "As shown in Table 2, Qwen2.5-VL-72B exhibits the best performance among all open-source video LMMs, achieving an overall score of 61.3%. Additionally, VideoLLaMA3 demonstrates the best performance among all 7B/8B models, with an overall score of 60.7%. This is primarily due to VideoLLaMA3s exceptional capabilities in fine-grained object tasks, making it the only open-source LMM Model MiniCPM-o 2.6 InternVideo2.5 VideoLLaMA3 Qwen2.5-VL-72B GPT-4o Gemini-2.0-Flash Event Object Culture Science ED 83.6 80.5 77.9 79.2 86.3 92.9 EP 55.0 52.7 57.4 60.5 47.3 51.9 ES 53.1 60.3 61.7 78.9 70.3 73.7 EL 35.2 33.0 45.2 42.1 28.6 70.7 OTL OTS OSL 20.1 37.1 72.1 31. 29.4 87.2 52.4 61.2 64.1 67.0 61.2 74.8 35.7 31.8 56.6 49.7 46.5 59.1 CC 48.9 53.7 45.5 65.8 57.1 62.3 AC 56.3 56.3 55.8 67.8 71.9 64.8 EC 63.7 65.2 59.2 80.6 76.6 77.6 SS 72.1 72.1 70.2 86.4 81.6 88.2 COM 61.3 61.3 54.7 85.3 77.3 87.8 AP 69.5 64.0 64.0 79.3 80.5 81.7 SP 52.7 54.8 55.9 79.6 65.6 90.7 Table 3: Detailed Evaluation results on VideoVista-CulturalLingo benchmark. We only showcase 6 mainstream LMMs. Abbreviations used in the table: Event Description (ED), Event Prediction (EP), Event Sequence (ES), Event Localization (EL), Object Temporal Localization (OTL), Object Temporal Sequence (OTS), Object Spatial Localization (OSL), Chinese Culture (CC), American Culture (AC), European Culture (EC), Summarization & Synthesis (SS), Comparison & Contrast (COM), Application & Procedure (AP), Scientific Principle (SP). The full evaluation results are provided in the Appendix C.5, and an introduction to tasks is presented in Appendix E. (a) Culture-based Evaluation Results. (b) Language-based Evaluation Results. (c) Duration-based Evaluation Results. Figure 4: The LMMs performance divided by Culture, Language and Duration. The Duration in (c): <2 minutes (Short), 2-10 minutes (Medium), >10 minutes (Long). that can compete with proprietary LMMs in this task. In the event task, VideoLLaMA3 also outperforms all other 7B models. Among the opensource image LMMs, DeepSeek2-VL achieved the highest score of 42.6% under 8-frame uniform sampling, demonstrating its superior generalization capacity on sequential image data. However, this still shows gap compared to the leading opensource video LMMs, indicating that questions in VideoVista-CulturalLingo generally require longer video durations to answer. Among proprietary LMMs, Gemini-2.0-Flash clearly outperforms all others, surpassing the strongest open-source video LMM, Qwen2.5-VL-72B, by 15.0%. The largest performance gap between these two models is observed in fine-grained object understanding tasks."
        },
        {
            "title": "4.3 Detailed Analysis",
            "content": "We present the detailed evaluation results of 6 mainstream models across 14 sub-tasks in Table 3. Event. The Event task consists of four sub-tasks: Event Description, Event Prediction, Event Sequence, and Event Localization, all of which require the model to have coarse-grained understanding of video content. Current open-source video LMMs exhibit performance comparable to that of proprietary LMMs on the first three subtasks, but there remains gap in the Event Localization task when compared to Gemini-2.0-Flash, with performance difference of up to 25.5%. Object. The Object task consists of three subtasks: Object Temporal Localization, Object Temporal Sequence, and Object Spatial Localization, which assess the LMMs ability to perceive the spatial-temporal aspects of fine-grained objects in videos. Video-LLaMA3 and Gemini-2.0-Flash demonstrate strong temporal localization capabilities in the Object Temporal Localization task, achieving scores more than 30% higher than those of other LMMs. Additionally, both LMMs exhibit commendable spatial understanding in the Object Spatial Localization task. Culture. The Culture task consists of three subtasks: Chinese Culture, American Culture, and European Culture, primarily evaluating the models understanding and generalization abilities across different regional cultures. As shown in Figure 4a, compared to the more prevalent Western cultures in the training data, current LMMs exhibit relatively (a) Domains in YouTube (b) Domains in Xiaohongshu (c) Domains in BiliBili Figure 5: The LMMs performance divided by domains from 3 video sources: Gemini-2.0-Flash, GPT-4o, Qwen2.5-VL-72B, VideoLLaMA3, InternVideo2.5, MiniCPM-o 2.6. In Figures 5a and Figures 5b, we present only the 18 domains with the highest number of videos. In Figure 5c, we exclude domains containing fewer than 10 videos. The domains in these figures are represented by abbreviations, as described in Appendix A.2. However, there remains noticeable gap in performance within math. The detailed comparison is presented in the Appendix C.1."
        },
        {
            "title": "4.4 Ablation Study",
            "content": "Language. In Figure 4b, we present the performance differences of 6 mainstream LMMs on Chinese and English. The results in the figure are based on 7 subtasks from the culture and science tasks, as these subtasks contain more domain-specific terms, providing more accurate assessment of an LMMs capabilities in each respective language. The experiments reveal noticeable performance gap between the majority of mainstream LMMs when evaluated on Chinese versus English. Duration. In Figure 4c, we compare the performance of 6 mainstream LMMs across 4 subtasks of event task from videos of varying lengths. The experimental results indicate that as the video duration increases, the performance of model tends to decrease, including Gemini-2.0-Flash. Domain. In Figure 5, we illustrate the performance of LMMs across different video domains It can be observed on various video websites. that Gemini-2.0-Flash demonstrates strong performance across all domains of videos."
        },
        {
            "title": "4.5 Case Study",
            "content": "Figure 6 presents two cultural examples alongside evaluation results: the top panel illustrates Chinese cuisine scenario, and the bottom panel European cuisine scenario. In the Chinese example, the majority of LMMs erroneously choose C. Stir-fried Yellow Beef, hallmark Hunan Figure 6: Two cases from VideoVista-CulturalLingo. weaker recognition of Chinese Culture. Science. The Science task consists of four subtasks: Summarization & Synthesis, Comparison & Contrast, Application & Procedure, and Scientific Principle. The first three sub-tasks involve course-oriented educational videos, while the last one focuses on experimental videos. This task primarily evaluates the models ability to summarize, comprehend, and apply scientific knowledge from videos. The difficulty level covers general knowledge areas rather than in-depth specialized topics. The questions are relatively simple and can be answered with one or two-hop reasoning, so most models perform well in these tasks. We observe that existing open-source LMMs perform comparably to proprietary LMMs across most disciplines. dish. This mistake likely arises from conflation between Jiangxi and Hunan cuisinesboth characterized by liberal use of chili peppersand from the greater domestic and international visibility of Hunan cooking. Such errors reveal that Video-LMMs tend to default to dominant cultural representations, overlooking more localized culinary nuances. By contrast, in the European example all LMMs correctly select option D, indicating robust performance on Western culinary content. Together, these cases exemplify systematic bias: Video-LMMs achieve higher accuracy on Western cultural contexts but underperform on non-Western ones, such as videos rooted in Chinese culture."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we introduce the benchmark VideoVista-CulturalLingo, the first video evaluation benchmark that spans multiple languages, cultures, and domains. VideoVista-CulturalLingo includes comprehensive evaluation metrics, ranging from coarse-grained event understanding to fine-grained object recognition, and from exploring the cultural context of videos to uncovering their scientific implications, enabling comprehensive assessment of current LMMs capabilities on video tasks. Through our extensive experiments, we highlight weaknesses in the spatial-temporal localization abilities of existing open-source video LMMs and their limitations in recognizing Chinese culture. We hope that VideoVista-CulturalLingo will inspire the development and advancement of video LMMs."
        },
        {
            "title": "6 Acknowledge",
            "content": "We thank editor and reviewers for their efforts to help improve the quality of our paper. This work was supported by grants: Natural Science Foundation of China (No. 62422603)."
        },
        {
            "title": "Limitations",
            "content": "The proposed benchmark has several limitations: 1) The scientific questions in the benchmark lack domain-specific depth, which prevents them from effectively showcasing the models performance in specialized scientific fields. In future versions, we plan to incorporate more human expert annotators to enhance the professionalism and complexity of the scientific questions. 2) Due to limitations in the linguistic proficiency and backgrounds of the annotators, the benchmark questions are restricted to two major languages, Chinese and English. This excludes other widely spoken languages such as Spanish, Portuguese, German, and Japanese."
        },
        {
            "title": "References",
            "content": "Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, et al. 2024a. Sharegpt4video: Improving video understanding and generation with better captions. arXiv preprint arXiv:2406.04325. Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, MingHsuan Yang, and Sergey Tulyakov. 2024b. Panda70m: Captioning 70m videos with multiple crossmodality teachers. arXiv preprint arXiv:2402.19479. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. 2024c. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271. Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, and Lidong Bing. 2024. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Preprint, arXiv:2501.12948. DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Yang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, and Zizheng Pan. 2024. Deepseek-v3 technical report. Preprint, arXiv:2412.19437. Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Jen Dumas, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, and Aniruddha Kembhavi. 2024. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146. Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. 2024. Mmbench-video: long-form multi-shot benchmark for holistic video understanding. arXiv preprint arXiv:2406.14515. Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. 2024. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075. Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Abrham Gebreselasie, Cristina González, James Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jáchym Koláˇr, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will Price, Paola Ruiz, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Ziwei Zhao, Yunyi Zhu, Pablo Arbeláez, David Crandall, Dima Damen, Giovanni Maria Farinella, Christian Fuegen, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, and Jitendra Malik. 2022. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1899519012. Li KunChang, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. 2023. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355. Jie Lei, Licheng Yu, Mohit Bansal, and Tamara Berg. 2018. Tvqa: Localized, compositional video question answering. In EMNLP. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. 2024a. Llavaonevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. 2023. Mvbench: comprehensive multi-modal video understanding benchmark. arXiv preprint arXiv:2311.17005. Rui Li, Xiaohan Wang, Yuhui Zhang, Zeyu Wang, and Serena Yeung-Levy. 2025a. Temporal preference optimization for long-form video understanding. Preprint, arXiv:2501.13919. Yuncheng Li, Yale Song, Liangliang Cao, Joel Tetreault, Larry Goldberg, Alejandro Jaimes, and Jiebo Luo. 2016. Tgif: new dataset and benchmark on animated gif description. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 46414650. Yunxin Li, Xinyu Chen, Baotian Hu, Longyue Wang, Haoyuan Shi, and Min Zhang. 2024b. Videovista: versatile benchmark for video understanding and reasoning. Preprint, arXiv:2406.11303. Yunxin Li, Baotian Hu, Xinyu Chen, Lin Ma, Yong Xu, and Min Zhang. 2024c. Lmeye: An interactive perception network for large language models. IEEE Transactions on Multimedia, 26:1095210964. Yunxin Li, Shenyuan Jiang, Baotian Hu, Longyue Wang, Wanqi Zhong, Wenhan Luo, Lin Ma, and Min Zhang. 2025b. Uni-moe: Scaling unified multimodal llms with mixture of experts. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 115. Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. 2023a. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122. Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. 2023b. Vila: On pre-training for visual language models. Preprint, arXiv:2312.07533. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. 2023a. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. 2023b. Mmbench: Is your multi-modal model an all-around player? arXiv:2307.06281. Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. 2024a. Tempcompass: Do video llms arXiv preprint arXiv: really understand videos? 2403.00476. Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. 2024b. Oryx mllm: Ondemand spatial-temporal understanding at arbitrary resolution. arXiv preprint arXiv:2409.12961. Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. 2024. Video-chatgpt: Towards detailed video understanding via large vision and language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024). Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. 2024. Egoschema: diagnostic benchmark for very long-form video language understanding. In NeurIPS. Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollár, and Christoph Feichtenhofer. 2024. Sam 2: Segment anything in images and videos. Preprint, arXiv:2408.00714. Qwen Team. 2025. Qwen2.5-vl. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. 2024a. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191. Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Shiyu Huang, Bin Xu, Yuxiao Dong, Ming Ding, and Jie Tang. 2024b. Lvbench: An extreme long video understanding benchmark. Preprint, arXiv:2406.08035. Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyu Zeng, Chenting Wang, Changlian Ma, Haian Huang, Jianfei Gao, Min Dou, Kai Chen, Wenhai Wang, Yu Qiao, Yali Wang, and Limin Wang. 2025. Internvideo2.5: Empowering video mllms with long and rich context modeling. arXiv preprint arXiv:2501.12386. Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, Zhenda Xie, Yu Wu, Kai Hu, Jiawei Wang, Yaofeng Sun, Yukun Li, Yishi Piao, Kang Guan, Aixin Liu, Xin Xie, Yuxiang You, Kai Dong, Xingkai Yu, Haowei Zhang, Liang Zhao, Yisong Wang, and Chong Ruan. 2024. Deepseekvl2: Mixture-of-experts vision-language models for advanced multimodal understanding. Preprint, arXiv:2412.10302. Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. 2021. Next-qa: Next phase of questionanswering to explaining temporal actions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9777 9786. Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. 2017. Video question answering via gradually refined attention over appearance and motion. In ACM Multimedia. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2024. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. 2024. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800. Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. 2024. mplug-owl3: Towards long image-sequence understanding in multi-modal large language models. Preprint, arXiv:2408.04840. Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. 2019. Activitynet-qa: dataset for understanding complex web videos via question answering. In AAAI, pages 91279134. Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4669 4684. Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, Peng Jin, Wenqi Zhang, Fan Wang, Lidong Bing, and Deli Zhao. 2025. Videollama 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106. Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. 2024a. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852. Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. 2024b. Video instruction tuning with synthetic data. Preprint, arXiv:2410.02713. Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. 2024. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264."
        },
        {
            "title": "A Additional Dataset Statistics",
            "content": "A.1 Further Statistics In Figure 7a, we present the statistics for all task categories in VideoVista-CulturalLingo. In VideoVista-CulturalLingo, the number of English questions is slightly higher than that of Chinese questions, with an additional 222 English questions. The task type with the fewest questions in the dataset is \"Comparison & Contrast\", with total of only 75 questions, while the task type with the most questions is \"Object Temporal Localization,\" with total of 537 questions. Figure 7b (b) shows the temporal distribution of video clips. Due to the fine-grained object recognition task, the selected videos are often short segments of longer videos, resulting in larger proportion of videos that are under one minute in length in the dataset. However, VideoVista-CulturalLingo still contains 315 videos longer than 10 minutes, with these long videos primarily concentrated in the Event and Science task categories. A.2 Abbreviations of Domains Zihao Yue, Qi Zhang, Anwen Hu, Liang Zhang, Ziheng Wang, and Qin Jin. 2023. Movie101: new movie understanding benchmark. In Proceedings of the 61st We provided the abbreviations of domains in Figure 5 in Table 4. (a) The statistics of 14 subtasks divided by languages. (b) The statistics of duration of videos in VideoVistaCulturalLingo. Figure 7: (a) shows the quantity statistics for the 14 task categories under both Chinese and English languages. (b) presents the duration statistics of all video clips in VideoVista-CulturalLingo, measured in minutes."
        },
        {
            "title": "B Detailed Experiment Setting",
            "content": "B.1 Open-source Video LMMs We evaluated the newly released Qwen2.5VL (Team, 2025), VideoLLaMA3 (Zhang et al., 2025), InternVideo2.5 (Wang et al., 2025), and TPO (Li et al., 2025a) from 2025. Additionally, we evaluated several popular video-capable LMMs introduced in the past two years, including InternVL2.5 (Chen et al., 2024c), LLaVAVideo (Zhang et al., 2024b), mPLUG-Owl3 (Ye et al., 2024), and others. In evaluating open-source video LMMs, we use the default hyperparameters specified in their respective open-source implementations for inference. The temperature is generally set to 0 or 0.2, num_beamsis set to 1, do_sampleis set to False, and top_pis set to 1.0. The frame sampling methods for different video models are provided in the Table 2. Specifically, for the Qwen2.5-VL and Qwen2-VL models, we set the maximum resolution per frame to 224x224 to avoid excessively long sequence lengths. B.2 Open-source Image LMMs We also evaluated three open-source image LMMs on our benchmarks, including VILA 1.5 (Lin et al., 2023b), DeepSeek2-VL (Wu et al., 2024), and Molmo (Deitke et al., 2024). For open-source image LMMs, we employed two video input methods: uniform sampling of 1 frame and uniform sampling of 8 frames. In evaluating these open-source image LMMs, we also adopted the hyperparameter settings provided in the implementations for inference. Regardless of whether single-frame or eight-frame input is used for evaluation, all images are presented at their original resolution without compression. Specifically, due to an error in the official code of the Molmo model when inputting eight images simultaneously, we concatenated the eight images horizontally into single image and noted this in the prompt. An example of this image is Figure 8. B.3 Proprietary LMMs For proprietary LMMs, we evaluated the newly released Gemini 2.0-Flash and Gemini 2.0-FlashLite in February, which are currently the workhorse models of the Google Gemini series. Additionally, we conducted evaluations on other prominent proprietary LMMs, including GPT-4o and Gemini 1.5-Flash. In evaluating proprietary LMMs, we optimize API resource usage and accelerate the evaluation process by input multiple questions for each video. Thanks to the powerful instruction-following capability of Proprietary LMMs, they are able to return dictionary in the format of {\"question id\": \"prediction\"} accurately. Although this may introduce some evaluation bias, Proprietary LMMs still demonstrated exceptional performance on our benchmark. Additionally, when evaluating the GPT-4 model, we compressed all video frames to resolution of 512x512 for input. Figure 8: An example of eight images combined in horizontal layout."
        },
        {
            "title": "C Further Experiments",
            "content": "News & Politics Sports Entertainment Howto & Style People & Blogs Autos & Vehicles Education Travel & Events Film & Animation Comedy Chemical Experiments Science & Technology Artificial Intelligence Physics Experiment Pets & Animals Quantum Mechanics Calculus Linear Algebra"
        },
        {
            "title": "OC\nAM\nHSE\nMSE\nUP\nML\nDL\nQM",
            "content": "Table 4: Abbreviations of domains from different video websites in Figure 5. The Chinese domains have been translated into English using GPT-4o. C.1 Model Performance in Science For the third finding discussed in the abstract, we present detailed experimental results in Figure 9. We present performance comparison between the four best-performing open-source video LMMs and the strongest proprietary model, Gemini-2.0Flash. As shown in Figure 9a, the primary performance gap between open-source Video LMMs and proprietary LMMs in scientific tasks is observed in the Mathematics disciplines. Specifically, for Physics, Chemistry, and Computer Science questions, the top-performing open-source Video LMM, Qwen2.5-VL-72B, exhibits performance gap of less than 5% compared to Gemini-2.0-Flash. However, for Math questions, the gap between the two models increases to nearly 10%. In Figure 9b, we further compare the performance differences of various models across specific math sub-disciplines. It is evident that, regardless of whether the questions are in Chinese or English, existing open-source video LMMs still exhibit performance gap when compared to the proprietary LMM Gemini-2.0-Flash. The largest gaps are observed in the Calculus (English) and Statistics and Probability (English) categories, where the leading open-source video LMMs show performance difference exceeding 10% compared to Gemini-2.0Flash. C."
        },
        {
            "title": "Impact of Frame Sampling",
            "content": "We conduct an experiment to evaluate the frame sampling upper bound for event task questions using the Qwen2.5-VL-7B model, and the results are shown in the Figure 10. It can be observed that as the frame sampling upper upper bound increases, the overall evaluation performance of the model gradually improves. However, there is no significant leap in performance, which could be due to the fact that our final frame sampling upper limit of 300 is still not high enough. (a) Science-based Evaluation Results. (b) Math-based Evaluation Results. Figure 9: The Evaluation results in 4 disciplines and 4 math sub-disciplines. The experimental results in the figure represent the average values of the four scientific sub-tasks. In (a),we have list the four disciplines covered by the scientific videos in VideoVista-CulturalLingo: Math, Physics, Chemistry, and Computer Science ; In (b), we have listed four math sub-disciplines with larger number of questions: Calculus (English), Linear Algebra (English), Statistics and Probability (English), and Calculus (Chinese)/Advanced Mathematics. Figure 10: The Evaluation results divided by frames upper bound of Qwen2.5-VL-7B. We conducted experiments with four sampling methods at frame upper bound of 64, 128, 256, and 300 frames. C."
        },
        {
            "title": "Impact of Audio Information",
            "content": "We also conduct experiments using the Qwen2.5VL-7B model to investigate the impact of adding audio transcripts in VideoVista-CulturalLingo, with the experimental results are shown in the Figure 11. The input audio transcript is the unrefined version extracted from Whisper-Large-V3. It can be observed that incorporating additional informaFigure 11: The Evaluation results divided by whether input audio transcript into Qwen2.5-VL-7B. The audio transcript is extracted using Whisper-Large-V3. tion from the audio modality, the models performance improves in the tasks of Event, Culture, and Science. In the Science task, the improvement in model performance is most significant. This is likely because the audio in the science videos we selected is generally clear and explicit, covering the experimental and course-related information. However, in the Event and Culture tasks, the inclusion of audio transcripts only resulted in small improvement. We encourage LMMs to process both audio and video frames simultaneously, and therefore, we did not include the audio information Figure 12: The Evaluation results divided by whether input audio transcript into Qwen2.5-VL-7B. The audio transcript is extracted using Whisper-Large-V3. in our model evaluation. C."
        },
        {
            "title": "Impact of Temporal Relation",
            "content": "We conduct experiments to investigate the temporal relationship between the event of interest posed in the question and the corresponding video, with the experimental results are shown in the Figure 12. In the Event Localization task, we selected 75 video pairs, where each pair contained videos of similar durations, and the events in the questions occurred in the early (front half) and late (back half) parts of the video. The evaluation results of five major models on early vs. late questions are summarized in the table below. As shown, except for the powerful Gemini-2.0-Flash model, the accuracy for early questions is significantly higher than for late questions across the remaining Video-LMMs. This suggests that most existing Video-LMMs have stronger understanding of events occurring early in the video but tend to struggle with those that happen later. We guess that the phenomenon is caused by the bias in the models training data or the models ability to handle long-contexts. C.5 Detailed Experiment Results In Table 5, we provide detailed presentation of the performance of all evaluated models across 14 subtasks. In Tables 6 and 7, we present the detailed evaluation results used to plot Figures 4b and 4c. These evaluation results effectively demonstrate the models performance across different languages and video durations."
        },
        {
            "title": "D Detailed Annotations Pipeline",
            "content": "D.1 Prompt for Video Preprocessing We introduce the prompt to determine whether the audio of video is noisy above Figure 13 and the Figure 13: Prompt for Video Processing. Figure 14: Prompt for Audio Refine. prompt to split the video based on audio in below of Figure 13. Both two prompt are input to Qwen2.5-72B language model during the video preprocessing stage. In the Figure 14, we present the prompt used to refine the audio transcripts recognized by WhisperX, primarily aimed at eliminating homophones in Chinese, reducing ambiguity, and enhancing fluency. This process is also carried out using the Qwen2.5-72B language model. D.2 Prompt for QA Annotation In Figure 15, we present the system prompt used in our automatic QA annotation process for labeling video events. This system prompt is input into the Qwen2-VL-72B model, along with the corresponding video frames, audio information, and prior events, to annotate the events. In Figure 16, we present the specific prompt Model ShareGPT4Video VideoChat2-Mistral Video-LLaVA VideoLLaMA2 LLaVA-OneVision MiniCPM-V 2.6 mPLUG-Owl3 Oryx-1. LLaVA-Video Qwen2-VL InternVL2.5 MiniCPM-o 2.6 TPO InternVideo2. VideoLLaMA3 Qwen2.5-VL-7B Qwen2.5-VL-72B VILA1.5-13B[1f] VILA1.5-13B[8f] Molmo 7B-D[1f] Molmo 7B-D[8f] DeepSeek2-VL[1f] DeepSeek2-VL[8f] GPT-4o Gemini-1.5-Flash Gemini-2.0-Flash-Lite Gemini-2.0-Flash Event Object Culture Science ED EP ES EL OTL OTS OSL CC AC EC SS COM AP SP 29.2 38.5 51.3 36.3 47. 74.3 66.4 54.4 75.7 72.6 81. 83.6 75.2 80.5 77.9 75.2 79. 33.3 36.9 38.3 40.3 40.9 42. 86.3 92.5 87.2 92.9 20.2 28. 46.5 28.7 34.9 38.0 56.6 40. 57.4 51.2 57.4 55.0 56.6 52. 57.4 51.2 60.5 33.3 38.2 44. 44.3 44.3 47.0 47.3 42.6 44. 51.9 17.7 31.1 31.1 41.6 44. 41.1 52.2 45.9 48.3 56.9 59. 53.1 49.8 60.3 61.7 72.7 78. 29.2 31.3 25.3 30.1 32.2 27. 70.3 63.6 68.4 73.7 Open-source Video LMMs 10. 25.1 32.2 17.9 30.7 18.8 35. 33.1 33.7 30.0 35.9 20.1 31. 37.1 72.1 39.3 31.5 27.2 26. 24.3 15.5 35.0 35.0 61.2 24. 67.0 47.6 47.8 52.4 67.0 61. 64.1 56.3 67.0 30.8 27.6 42. 36.4 39.2 30.1 41.7 33.2 39. 36.0 30.4 35.7 38.8 31.8 56. 31.8 49.7 Open-source Image LMMs 26.8 23.1 26. 29.6 32.4 25.0 26.2 35.9 34. 45.6 33.0 33.0 34.6 45.1 19. 25.5 31.5 29.4 Proprietary LMMs 29.4 87. 87.5 87.2 61.2 69.9 63.1 74. 46.5 23.7 44.8 59.1 19.0 25. 27.7 25.1 36.4 44.6 37.7 35. 41.6 48.5 55.4 48.9 43.7 53. 45.5 51.9 65.8 31.6 23.4 39. 37.7 37.7 37.2 57.1 49.4 58. 62.3 23.7 19.3 41.6 29.6 44. 30.8 48.2 37.7 53.1 33.3 41. 35.2 48.2 33.0 45.2 40.1 42. 33.9 38.2 39.8 41.8 39.3 44. 28.6 69.4 63.8 70.7 34.7 40. 35.2 38.7 41.7 48.7 45.7 39. 51.3 54.8 47.7 56.3 50.8 56. 55.8 50.8 67.8 30.7 42.2 40. 44.2 38.2 40.7 71.9 61.3 61. 64.8 42.3 40.3 41.8 42.3 38. 55.7 52.7 38.3 54.7 62.2 65. 63.7 55.2 65.2 59.2 63.2 80. 39.8 51.2 39.8 44.3 42.3 56. 76.6 67.7 70.1 77.6 32.4 36. 42.6 36.4 55.1 70.6 62.1 58. 63.6 72.1 69.8 72.1 63.2 72. 70.2 80.5 86.4 36.8 43.4 46. 50.0 52.2 62.9 81.6 87.9 83. 88.2 48.0 44.0 38.7 42.7 44. 53.3 58.7 46.7 53.3 60.0 56. 61.3 50.7 61.3 54.7 65.3 85. 46.7 41.3 41.3 42.7 44.0 50. 77.3 87.7 81.3 87.8 32.9 23. 40.9 35.4 57.9 60.4 60.4 57. 61.0 66.5 65.2 69.5 62.8 64. 64.0 72.6 79.3 39.6 40.9 50. 49.4 49.4 53.0 80.5 82.9 80. 81.7 30.1 31.2 38.7 34.4 48. 52.7 54.8 51.6 52.7 65.6 62. 52.7 55.9 54.8 55.9 60.2 79. 39.8 39.8 45.2 44.1 52.7 54. 65.6 77.4 82.8 90.7 Table 5: Detailed Evaluation results on VideoVista-CulturalLingo benchmark. Abbreviations used in the table:Event Description (ED), Event Prediction (EP), Event Sequence (ES), Event Localization (EL), Object Temporal Localization (OTL), Object Temporal Sequence (OTS), Object Spatial Localization (OSL), Chinese Culture (CC), American Culture (AC), European Culture (EC), Summarization & Synthesis (SS), Comparison & Contrast (COM), Application & Procedure (AP), Scientific Principle (SP)."
        },
        {
            "title": "Short Medium",
            "content": "MiniCPM-o 2.6 InternVideo2.5 VideoLLaMA3 Qwen2.5-VL-72B GPT-4o Gemini-2.0-Flash 58.77 60.04 52.26 75.59 68.35 76.49 63.49 63.49 63.78 78.30 76.83 78. MiniCPM-o 2.6 InternVideo2.5 VideoLLaMA3 Qwen2.5-VL-72B GPT-4o Gemini-2.0-Flash 54.46 53.12 61.16 62.72 54.91 75.89 52.91 52.69 56.05 59.64 52.69 74."
        },
        {
            "title": "Long",
            "content": "44.30 48.10 50.63 59.49 49.37 62.03 Table 6: Model Performance by Video Language. used to generate Event Description questions in the automatic QA annotation process. During the generation of Event questions, only the aggregated event sequence is input, without any additional information. The model used in this process is the DeepSeek-V3 language model. In Figure 17, we present the specific prompt used to generate Chinese Culture questions in the automatic QA annotation process. Unlike the Event Table 7: Model Performance by Video Duration. The Duration: <2 minutes (Short), 2-10 minutes (Medium), >10 minutes (Long). Description task, in addition to inputting the aggregated event sequence, we also provide pre-retrieved cultural background information from Wikipedia using embeddings model, requiring the model to generate questions that necessitate both video content and cultural background knowledge to answer. The model used in this process is the DeepSeek-V3 Figure 15: Prompt for Event Annotation. Figure 16: Prompt for Event Description Quetions, Options and Answer Generation. language model. In Figure 18, we present the specific prompt used to generate Scientific Principle questions in the automatic QA annotation process. In contrast to the question generation above, where the options are more flexible, we strictly impose requirements on the model when generating options at this stage. This approach increases the complexity of the questions and prevents the possibility of answering the questions without reference to the video content. The model used in this process is the DeepSeek-R1 language model. D.3 Webpage for Human Scoring We built an annotation interface using Gradio, as shown in the Figure 19. Each annotator only needs to enter their name in the top left corner, watch the video, review question, options, and check whether the answers align. Then, they can select the appropriate score in the bottom right corner. For complex cultural questions, we provide the corresponding Wikipedia entry name within the Entry, enabling annotators to efficiently look up answers to questions they may not be familiar with. This benchmark includes total of ten annotators, each with at least an undergraduate degree and proficiency in both Chinese and English. D.4 Annotation Model We organize our annotation models into three complementary categories, chosen for their balance of accuracy, speed, and cost: First group: Small tool models. This includes WhisperX for audio extraction, MiniLM for embedding extraction, and Grounding-DINO, SAM2 for bounding box generation. We chose these models based on their open-source nature, accuracy, and inference speed. The tasks assigned to these models Figure 17: Prompt for Chinese Culture Quetions, Options and Answer Generation. Figure 18: Prompt for Scientific Principle Quetions, Options and Answer Generation. are relatively simple, often yielding high accuracy, making speed our primary evaluation criterion. For bounding box extraction, we also tested models like Florence2 and Grounding-DINO 1.5, but since the accuracy differences were minimal, we opted for the lighter, faster Grounding-DINO. Second group: Multimodal large models for video annotation. For video content annotation, we referenced video evaluation benchmarks such as Video-MME and MVBench. Among the opensource models, Qwen2-VL-72B demonstrated the strongest performance, so we selected it for video annotation. We also tested InternVL2-76B, but found that its limited frame sequence length hindered its ability to capture full video information. Third group: Powerful Large Language Models for question generation. In this category, we primarily used DeepSeek-V3 and DeepSeek-R1. For task categories like Event, Object, and Culture, we compared DeepSeek-V3 and GPT-4o models, judging the quality of generated questions through manual evaluation. While no significant quality difference was found, DeepSeek-V3 proved to be more cost-effective.For science-related questions, both models performed poorly, as the generated questions could be answered without watching the video, and the answer options were often too similar or ambiguous. To address this, we applied stricter constraints to ensure the questions required watching the video, and that the options were meaningful. The long-reasoning model DeepSeek-R1 effectively applied these rules, generating questions that were more appropriate. Besides, the chain-ofthoughts generated during the process also helped human annotators make quicker judgments about the appropriateness of the questions. Figure 19: Gradio Interface for scoring. Specifically, during the data annotation process, the Whisper, SAM2, Qwen series models, and InternVL series models were deployed for inference on local GPU servers. The DeepSeek-V3 and DeepSeek-R1 models is utilized the API services provided by the official 1. The specific Whisper model used in the experiment is WhisperX2, based on Whisper-large-V3. When obtaining Chinese transcripts, special initial prompt \"以下是中文 普通话句子\" was set to ensure that the model could correctly add punctuation. The pipeline used for annotating objects, which involves Grounding Dino and SAM2, is derived from Grounded-SAM23. D.5 External Resources The three websites to collect videos: YouTube4, Xiaohongshu(RedNote)5 and BiliBili6. The multilingual Wikipedia used in the automatic QA annotation pipeline was downloaded from Wikimedia Downloads7, and the extraction 1https://platform.deepseek.com/usage 2https://github.com/m-bain/whisperX 3https://github.com/IDEA-Research/ Grounded-SAM-2 4http://www.youtube.com 5https://www.xiaohongshu.com 6https://www.bilibili.com 7https://dumps.wikimedia.org/ and processing were performed using regular expression rules8. The tool used to collect videos from BiliBili is Downkyi9."
        },
        {
            "title": "E Case Data",
            "content": "In Figures 20-33, we present specific case for each proposed task type. Each case includes sampled frames from the video, along with the corresponding questions and options. The ground truth is highlighted in yellow. Event Description. The Event Description task primarily focuses on explaining how specific event in the video occurred, typically beginning with questions such as What or How. Event Prediction. The Event Prediction task primarily involves predicting the event most likely to occur after the input video ends. In this task, the selected video is typically segment from full video, such as clip spanning from 0 to 45 seconds of the full video Event Sequence. The Event Sequence task primarily asks about the order in which multiple events occur in the input video, requiring the model backup-index-bydb.html 8https://spaces.ac.cn/archives/4176 9https://github.com/leiurayer/downkyi to select the most accurate sequence of events from the options provided. Event Localization. The Event Sequence task primarily focuses on determining the order in which multiple events occur in the input video, requiring the model to select the most accurate sequence of events from the available options. Object Temporal Localization. The Object Temporal Localization task primarily requires identifying the timestamp of the first appearance of specific object in the video. The selected object typically occupies significant portion of the frame to ensure it is easily noticeable, avoiding objects that may be difficult for humans to detect. Comparison & Contrast. \"The Comparison & Contrast task primarily requires the model to compare the specific method described in the educational or popular science video with other similar methods, emphasizing the differences or distinctions between them. This task assesses the models ability to comprehend the key concepts presented in the video. Application & Procedure. The Application & Procedure task primarily requires the model to determine the operational procedure or application method of specific concept described in the educational or popular science video. This task assesses the models understanding of the key concepts presented in the video.\" Object Temporal Sequence. The Object Temporal Sequence task primarily focuses on determining the order in which multiple distinct objects appear in the video. Scientific Principle The Scientific Principle task requires the model to comprehend the scientific principles underlying the experimental procedures or phenomena presented in the video. Object Spatial Localization. The Object Spatial Localization task primarily requires identifying the spatial bounding boxes of specific object in the video at particular time, typically when the object first appears. The answer is provided in normalized format, represented as bounding boxes in the xyxy format. Chinese Culture. The Chinese Culture task primarily focuses on the Chinese cultural background presented in the video, covering areas such as traditional culture, culinary traditions, ancient history, and more. American Culture. The American Culture task primarily focuses on the American cultural background presented in the video, emphasizing areas such as political culture, superhero culture, pop culture, holiday traditions, and more. European Culture. The European Culture task primarily focuses on the European cultural background presented in the video, emphasizing areas such as cultural differences between European countries, football culture, culinary traditions, classical culture, and more. Summarization & Synthesis. The Summarization & Synthesis task primarily requires the model to summarize and synthesize the key points presented in educational or popular science videos, assessing the models ability to consolidate the essential concepts conveyed in the video. Figure 20: An Example of Event Description from VideoVista-CulturalLingo. Figure 21: An Example of Event Prediction from VideoVista-CulturalLingo. Figure 22: An Example of Event Sequence from VideoVista-CulturalLingo. Figure 23: An Example of Event Localization from VideoVista-CulturalLingo. Figure 24: An Example of Object Temporal Localization from VideoVista-CulturalLingo. Figure 25: An Example of Object Temporal Sequence from VideoVista-CulturalLingo. Figure 26: An Example of Object Spatial Localization from VideoVista-CulturalLingo. Figure 27: An Example of Chinese Culture from VideoVista-CulturalLingo. Figure 28: An Example of American Culture from VideoVista-CulturalLingo. Figure 29: An Example of European Culture from VideoVista-CulturalLingo. Figure 30: An Example of Summarization & Synthesis from VideoVista-CulturalLingo. Figure 31: An Example of Comparison & Contrast from VideoVista-CulturalLingo. Figure 32: An Example of Application & Procedure from VideoVista-CulturalLingo. Figure 33: An Example of Scientific Principle from VideoVista-CulturalLingo."
        }
    ],
    "affiliations": [
        "Harbin Institute of Technology, Shenzhen, China",
        "Hong Kong University of Science and Technology"
    ]
}