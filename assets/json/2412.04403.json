{
    "paper_title": "Establishing Task Scaling Laws via Compute-Efficient Model Ladders",
    "authors": [
        "Akshita Bhagia",
        "Jiacheng Liu",
        "Alexander Wettig",
        "David Heineman",
        "Oyvind Tafjord",
        "Ananya Harsh Jha",
        "Luca Soldaini",
        "Noah A. Smith",
        "Dirk Groeneveld",
        "Pang Wei Koh",
        "Jesse Dodge",
        "Hannaneh Hajishirzi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We develop task scaling laws and model ladders to predict the individual task performance of pretrained language models (LMs) in the overtrained setting. Standard power laws for language modeling loss cannot accurately model task performance. Therefore, we leverage a two-step prediction approach: first use model and data size to predict a task-specific loss, and then use this task loss to predict task performance. We train a set of small-scale \"ladder\" models, collect data points to fit the parameterized functions of the two prediction steps, and make predictions for two target models: a 7B model trained to 4T tokens and a 13B model trained to 5T tokens. Training the ladder models only costs 1% of the compute used for the target models. On four multiple-choice tasks written in ranked classification format, we can predict the accuracy of both target models within 2 points of absolute error. We have higher prediction error on four other tasks (average absolute error 6.9) and find that these are often tasks with higher variance in task metrics. We also find that using less compute to train fewer ladder models tends to deteriorate predictions. Finally, we empirically show that our design choices and the two-step approach lead to superior performance in establishing scaling laws."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 ] . [ 1 3 0 4 4 0 . 2 1 4 2 : r Preprint Establishing Task Scaling Laws via Compute-Efficient Model Ladders Jiacheng Liu Alexander Wettig David Heineman Akshita Bhagia Oyvind Tafjord Ananya Harsh Jha Luca Soldaini Noah A. Smith Dirk Groeneveld Pang Wei Koh Jesse Dodge Hannaneh Hajishirzi Allen Institute for Artificial Intelligence Paul G. Allen School of Computer Science & Engineering, University of Washington Princeton University * Project co-leads."
        },
        {
            "title": "Abstract",
            "content": "We develop task scaling laws and model ladders to predict the individual task performance of pretrained language models (LMs) in the overtrained setting. Standard power laws for language modeling loss cannot accurately model task performance. Therefore, we leverage two-step prediction approach: first use model and data size to predict task-specific loss, and then use this task loss to predict task performance. We train set of small-scale ladder models, collect data points to fit the parameterized functions of the two prediction steps, and make predictions for two target models: 7B model trained to 4T tokens and 13B model trained to 5T tokens. Training the ladder models only costs 1% of the compute used for the target models. On four multiple-choice tasks written in ranked classification format, we can predict the accuracy of both target models within 2 points of absolute error. We have higher prediction error on four other tasks (average absolute error 6.9) and find that these are often tasks with higher variance in task metrics. We also find that using less compute to train fewer ladder models tends to deteriorate predictions. Finally, we empirically show that our design choices and the two-step approach lead to superior performance in establishing scaling laws. Figure 1: Predicting MMLU accuracy with our method. We first use model size and data size to predict task loss on MMLU (step 1), and then use this task loss to predict task accuracy in ranked classification format (step 2). The chained plot shows end-to-end prediction from (model size, data size) to task accuracy. The functions in step 1 and 2 are fitted on data points collected from ladder models (markers colored in red, orange, green and cyan); 7B-4T and 13B-5T are the target models which we make predictions for. We report relative prediction error in the plot next to the target model point. 1 Preprint"
        },
        {
            "title": "Introduction",
            "content": "Language models (LMs) are expensive to pretrain. Given time and compute constraints, and given the variety of decisions that go into building an LM (e.g., picking good mixture of pretraining data, making optimal model architecture choices), being able to predict the task performance of large model before actually training it enables more efficient resource allocation and wider range of experimentation. However, it is challenging to extrapolate the task performance of large training run from models trained at smaller scales (Hu et al., 2023). Previous work has shown preliminary results on predicting the average performance over many tasks (Gadre et al., 2024), or for single selected task (ARC-Challenge) (Dubey et al., 2024), using at least 5% of the compute required to train the target models. It remains an open problem to develop general and compute-efficient method to predict the performance of LMs on individual tasks. In this paper, we tackle the challenge of predicting individual task performance of LMs as function of model size and training data size. In particular, we predict the performance of 7B and 13B models using 1% of the compute required to train them. In this work, we focus on multiple-choice tasks, and aim to predict the task accuracy where problems are written in the ranked classification (RC) format. We employ two-step approach to (1) use the number of model parameters and training tokens to predict task-specific loss, which is bits-per-byte loss on the correct answer given the question, then (2) use this task loss to predict accuracy. The closest existing work (Dubey et al., 2024) only examined compute-optimal models; our approach also works for the overtrained regime, which is important because most recent LMs are heavily overtrained (Dubey et al., 2024; Li et al., 2024; Bai et al., 2023). To fit the parameters of the two functions used to make our predictions, we measure the task loss and accuracy of variety of small models, varying model parameters between 190M and 1.3B non-embedding parameters, and data sizes between 1x and 10x of the Chinchilla-optimal data size for each model (Hoffmann et al., 2022). Training these ladder models altogether costs only 1% the combined compute of the two target model that we predict for: 7B model trained on 4T tokens, and 13B model trained on 5T tokens. 1 We apply our method on 8 selected tasks from OLMES (Gu et al., 2024) (e.g., MMLU, HellaSwag, ARC-Challenge). On 4 of these tasks, our predictions are within an absolute error of 2 points for the 7B-4T and 13B-5T models. Error on the other 4 tasks is higher, with an average absolute error of 6.9 points, and our analyses show that the metrics of these tasks have relatively high variance between neighboring model checkpoints. In 5, we conduct several analyses. We quantify the difficulty of the prediction task itself for the different target models by measuring the variance in the task loss and accuracy of the final few checkpoints (the higher the variance in the prediction target, the harder the task) (5.1). We show that using less compute to train fewer ladder models results in worse predictions (5.2). In 6, we study the impact of various design choices in our prediction method, such as using compute-FLOPs instead of (N, D) as the input (6.1), using different intermediate features such as task cross-entropy loss and general language modeling loss (6.2 and 6.3), and bypassing the intermediate feature altogether by making predictions in single step (6.4). We find that some tasks benefit from these alternative choices, but for the tasks that we care the most about (e.g., MMLU and HellaSwag), our two-step approach using task loss works better."
        },
        {
            "title": "2 Setup",
            "content": "We aim to predict the task performance of LMs with an arbitrary training scale the combination of model size (N) and number of training tokens (D). Since many recent LMs are heavily overtrained (Dubey et al., 2024; Li et al., 2024; Bai et al., 2023), we do not constrain 1We will release our code and experiments on Github. 2 Preprint Model size (N) 1xC data size (D) Batch size (sequences) Batch size (tokens) Training steps (1xC) Peak LR Warmup steps Dimension Num heads Num layers MLP ratio 190M 370M 760M 1.3B 7B-4T 13B-5T 190,354,176 3,807,083,520 7,425,249,280 15,164,405,760 25,587,916,800 758,220,288 371,262,464 128 524,288 7,272 9.7 104 363 192 786,432 9,452 7.8 104 472 320 1,310,720 11,580 6.1 104 1,279,395,840 6,887,575,552 13,202,396,160 1,024 4,194,304 3.0 104 2000 2,048 8,388,608 9.0 104 1000 384 1,572,864 16,279 5.2 104 813 768 12 12 8 1,024 16 16 8 1,536 16 16 2,048 16 16 8 4,096 32 32 5.375 5,120 40 40 4 Table 1: Model configuration. Up to 1.3B are the ladder models; 7B-4T and 13B-5T are the target models. and to stay close to the compute-optimal regime (Hoffmann et al., 2022). In particular, we make and validate our predictions on the OLMo 2 models after stage 1 pretraining and before stage 2 annealing (Ai2, 2024); these include 7B model trained with 4T tokens (7B-4T) and 13B model trained with 5T tokens (13B-5T), both trained from scratch with the same data mixture. We refer to the 7B-4T and 13B-5T models as target models. We consider multiple-choice tasks and compute task accuracy in the ranked classification (RC) format (2.2). The relationship between models accuracy on given task and its training scale is complex, so we use an intermediate task loss as bridge. This task loss is defined in 2.2. We predict given models accuracy on task by chaining two steps: step 1, using model size and training tokens (N, D) to predict the task loss; step 2, using the task loss to predict accuracy. We define parametric functions for each step (3.1, 3.2); we fit these parametric functions by training variety of small models and measuring their task loss and accuracy (2.1). For these models, we vary and D: model sizes between 190M and 1.3B parameters, trained to between 1x and 10x of the Chinchilla-optimal data size (but otherwise keeping the architecture, training data, etc. fixed). 2.1 Ladder models To predict the task loss and accuracy of large models, we extrapolate from data points collected from training many small-scale models (ladder models). These ladder models have the same architecture and are trained with the same data mix as the target models. The ladder models need to span relatively wide range of model size and training data size, while collectively costing very small fraction of compute used for training the large target models. We use four different model sizes { 190M, 370M, 760M, 1.3B } (considering only nonembedding parameters) by varying the width and depth of the transformer. For each model size, we train it to different multiples of the Chinchilla optimal number of tokens. We use = 20 as the Chinchilla optimal setting (Hoffmann et al., 2022) (denoted 1xC), and train each model with number of tokens { 1xC, 2xC, 5xC, 10xC }. In total, we train 4 4 = 16 ladder models. We save intermediate checkpoints every 200 steps for 1xC and 2xC runs, every 500 steps for 5xC runs, and every 1000 steps for 10xC runs. Table 1 lists the configurations of each model size. The target 7B-4T and 13B-5T models are trained with cosine LR schedule with linear warmup, where the LR decays to 10% of the peak LR over horizon of 5T tokens. We match this in the ladder models, where the decay horizon is adjusted to match the training data size of each model. Unfortunately, the OLMo 2 7B model was only trained to 3.9T tokens in stage 1 pretraining. Prior work has shown that LR decay has big impact on the performance of pretrained LMs. To account for the incomplete training, we train this model with an additional 50B tokens where we linearly decay the LR down to 10%. Our target 7B-4T model is thus trained on total of 3.95T tokens. 3 Preprint Original problem Task loss calculation woman is outside with bucket and dog. The dog is running around trying to avoid bath. She A. rinses the bucket off with soap and blow dry the dogs head. B. uses hose to keep it from getting soapy. C. gets the dog wet, then it runs away again. D. gets into bath tub with the dog. Answer: Question: woman is outside with bucket and dog. The dog is running around trying to avoid bath. She Answer: gets the dog wet, then it runs away again. Table 2: Problem formatting for computing task loss. Tokens on which task loss is computed is marked in green. Example taken from HellaSwag (Zellers et al., 2019). For the ladder models, we set the peak LR, batch size, and warmup steps by extrapolating from the configuration of 7B-4T using the method introduced in Porian et al. (2024). All other configurations follow from the 7B-4T model. Compute cost. Using 6ND to estimate the compute (Kaplan et al., 2020), the total amount of compute used for training the ladder models is 5.2 1021 FLOPs. This is merely 3.2% of that used for training the 7B-4T model (1.6 1023 FLOPs), 1.3% of the 13B-5T model (3.9 1023 FLOPs), and less than 1.0% of the two target models combined. 2.2 Task loss and accuracy In this work we focus on multiple-choice tasks. Here, we describe how we format multiplechoice problems to compute task loss and accuracy. Task loss. For multiple-choice problem, we define its task loss as the negative loglikelihood of the correct answer sequence, divided by its length in bytes. This is also known as the bits-per-byte (bpb) metric. Table 2 shows the problem formatting for computing task loss on an example. We use bpb instead of normalizing by number of tokens because bpb reduces the impact of tokenizer.2 In 6 we also study the effectiveness of alternative choices to task loss, like considering both correct and incorrect answers in the task loss, and using general language modeling loss. We found that some tasks may benefit from these alternative choices, but task loss works best for predicting task accuracy on MMLU, which is the task we care most about. Task accuracy. For task accuracy, we use the two commonly used formats: ranked classification (RC) and multiple-choice (MC). (Note that RC and MC are different formats to pose multiple-choice problems.) In RC, the predicted answer is the one with the minimum bpb loss. In MC, all the answer choices are included in the prompt, and the predicted answer is the answer code (e.g., A, B, C, etc.) with the smallest loss. See Gu et al. (2024) for detailed discussion on the RC vs. MC formats. In this paper we focus on the RC format. Evaluation. Following Gadre et al. (2024), we evaluate the goodness of prediction by computing its relative error against the actual task accuracy of the target model. The relative error is defined as Relative Error = prediction actual actual 100%. 2We anticipate this to be similar to the Normalized NLL per Char used in Dubey et al. (2024). 3RC reliably measures progress for models across wide range of scales, whereas MC capability does not emerge until the model has several billion parameters. 4 Preprint Task Split Examples test MMLU val HellaSwag test ARC-Challenge test ARC-Easy PIQA val CommonsenseQA val val Social IQa test OpenBookQA 14,042 10,042 1,172 2,376 1,838 1,221 1,954 Table 3: We predict 5-shot downstream task performance for 8 tasks. 2.3 Task selection We include the following 8 tasks from the OLMES evaluation suite (Gu et al., 2024): MMLU (Hendrycks et al., 2021), HellaSwag (Zellers et al., 2019), ARC-Challenge (Clark et al., 2018), ARC-Easy (Clark et al., 2018), PIQA (Bisk et al., 2020), CommonsenseQA (Talmor et al., 2019), Social IQa (Sap et al., 2019), and OpenBookQA (Mihaylov et al., 2018). We exclude two tasks from the OLMES suite, BoolQ (Clark et al., 2019) and Winogrande (Sakaguchi et al., 2020), as we do not observe monotonic scaling of task performance on the ladder models. 5.1 has further discussion on task predictability with the ladder. We use the test set whenever possible, and fall back to the validation set when the test set is not fully available. For all tasks, we use the 5-shot setting as provided in OLMES. Table 3 summarizes the tasks and statistics."
        },
        {
            "title": "3 Method",
            "content": "We break down the problem of task accuracy prediction into two steps: first predicting the task loss (taken as the intermediate feature), and then using that to predict the task accuracy. 6 discusses various alternative choices for the intermediate feature. 3.1 Step 1: Use (N, D) to predict task loss As proposed by Hoffmann et al. (2022) and followed by others (Muennighoff et al., 2023; Gadre et al., 2024; Zhang et al., 2024), the language modeling loss of model on held-out eval set can be modeled as power function with respect to and D: L(N, D) = A/Nα + B/Dβ + E, (1) where A, B, α, β, are parameters to fit. We postulate that the same functional form applies to the task loss. We will later validate this assumption by looking at the goodness of function fitting. (In 6.1 we ablate with using compute-FLOPs as input variable instead of (N, D), and in 6.2 and 6.3 we ablate with using alternative losses as the intermediate feature.) Function fitting. We take the loss value of the final checkpoint of each ladder model, forming dataset {(Ni, Di, Li)}n i=1 (where = 16), and use this data to fit the parameters of Equation 1. We fit separate set of parameters for each task. For parameter fitting, we follow the standard practice set in Hoffmann et al. (2022). We minimize the Huber loss between the (cid:1), where δ = logarithm of predicted and actual loss: 1 103. We optimize and in log space we apply transformation = log A, = log and optimize (a, b, α, β, E). We use the L-BFGS-B optimizer as implemented in scipy.minimize(), with constraints A, B, α, β, 0; with these constraints, the function is convex, and thus L-BFGS-B is guaranteed to converge.4 (cid:0) log ˆL(Ni, Di) log Li i=1 Huberδ 4To test if this function is convex, we calculate the Hessian matrix of second derivatives, and confirm that it is positive semi-definite everywhere. 5 Preprint To improve function fitting, we take the average task loss over the last 5 checkpoints of each ladder model. This reduces the noisy variance between model checkpoints. Results preview. The parameterized power function can fit the ladder models well, with relative fitting error ranging from 0.2% to 1.2% on all tasks. When predicting task loss of target models, across all tasks we get an average relative error of 5.2% for 7B-4T, and 6.6% for 13B-5T. We present results on individual tasks in 4, and ablate the design choices of step 1 in 6. 3.2 Step 2: Use task loss to predict accuracy Following Dubey et al. (2024), we model the mapping from task loss to accuracy with sigmoidal function: where a, b, k, L0 are parameters to fit. Acc(L) = 1 + ek(LL0) + (2) The choice of sigmoidal functional form is motivated as follows: weak model has high task loss and random task accuracy, and strong model has low task loss and high task accuracy that saturates at 100%. We observed (as shown in Figure 4) that the (Li, Acci) points collected from different ladder models do tend to fall on shared sigmoidal curve; this also applies to intermediate checkpoints of these models in addition to final checkpoints. Function fitting. To fit this function, we use data points from both final and intermediate checkpoints of the ladder models. The task loss and accuracy value of all intermediate and final checkpoints of all ladder models form dataset {(Li, Acci)}m i=1 (where 1400), which we use to fit the parameters of Equation 2. We fit separate set of parameters for each downstream task. We minimize the L2 loss between the predicted and i=1( ˆAcc(Li) Acci)2. We use non-linear least squares implemented by actual accuracy: 1 scipy.optimize.curve fit() to fit this equation, as sigmoid functions are not convex. The variation from checkpoint to checkpoint can be high. To smoothen the noise, we apply moving average on the task loss and task accuracy over all checkpoints of each training run, with window size of 5. We also discard the checkpoints from the first 10% of each training run as these are quite noisy, and add an extra data point (L = 0.0, Acc = 1.0) to the training pairs. This helps avoid the cases when the fitting function degenerates for very noisy data, and the ladder models are too small to start doing well on certain tasks. Results preview. The parameterized sigmoidal function can fit the ladder models well, with relative fitting error ranging from 0.4% to 2.6% on all tasks. When predicting task accuracy of target models with their actual task loss, across all tasks we get an average relative error of 3.7% for 7B-4T, and 3.5% for 13B-5T. We present results on individual tasks in 4, and ablate the design choices of step 1 in 6. 3.3 Chaining the two steps Finally, we chain the two steps together and predict the task accuracy of model of size trained on tokens. We first predict the task loss with the fitted function in step 1, and then insert this predicted task loss into the fitted function in step 2 to predict the task accuracy. Results preview. When predicting task accuracy of target models, across all tasks we get an average absolute error of 3.8 points for 7B-4T, and 4.2 points for 13B-5T. We present results on individual tasks in 4, and ablate the design choices of step 1 in 6."
        },
        {
            "title": "4 Results",
            "content": "We first discuss main prediction results where we chain step 1 and 2 together to make end-to-end predictions. Then we discuss detailed results in step 1 and step 2 individually. 6 Preprint 7B-4T 13B-5T Pred Actual Error %Error Pred Actual Error %Error 48.4 MMLU 82.5 HellaSwag ARC-Challenge 51.5 76.6 ARC-Easy 81.2 PIQA CommonsenseQA 75.7 58.7 Social IQa 44.2 OpenBookQA Average 49.0 81.3 61.9 84.6 82.0 72.6 59.9 49.4 0.6 1.2 10.4 8.0 0.8 3.1 1.2 5. 3.8 1.3% 51.3 1.4% 85.3 16.9% 52.7 9.4% 77.2 1.0% 82.1 4.2% 77.6 2.0% 59.9 10.6% 44.9 5.9% 51.6 83.2 63.8 87.2 83.0 74.1 61.6 48.6 0.3 2.1 11.1 10.0 0.9 3.5 1.7 3.7 4.2 0.7% 2.5% 17.5% 11.4% 1.1% 4.7% 2.7% 7.8% 6.1% Figure 2: Predicting the task accuracy for the target models. = 1xC; = 2xC; = 5xC; = 10xC. We report the average relative fitting error in parentheses following the task name, and prediction error in the plot next to the target model point. Main results. Figure 2 shows the prediction results on the 7B-4T and 13B-5T models. On four tasks MMLU, HellaSwag, PIQA, and Social IQa we predicted the accuracy of the 7B-4T and 13B-5T models within an absolute error of 2 points. We underestimated ARCChallenge and ARC-Easy due to the compounding error in the two steps we overestimated the task loss in step 1, and underestimated the task accuracy in step 2. In 5.1, we analyze the ability of the ladder to predict task performance by considering variation between checkpoints. We find that our results here track with the variation analysis (ARC-Easy and ARC-Challenge display higher variance). Step 1 results. Figure 3 shows the function fitting of step 1 on the ladder models, and the prediction for the target models. The function gives an average relative fitting error ranging from 0.2% to 1.2%, which suggests that Equation 1 is good functional form to describe task losses. When predicting the task loss for 7B-4T and 13B-5T, we have relative error within 3% on MMLU, HellaSwag, PIQA, and OpenBookQA. Our method underestimates the loss for CSQA , and overestimates on ARC-ChallengeARC-Easy, and Social IQa. Overestimating the task loss can lead us to eventually underestimate the task accuracy. Step 2 results. Figure 4 shows the function fitting of step 2 on the ladder models, and the prediction for the target models with their actual task loss. For all tasks, all data points can be well fitted with shared sigmoidal function, with the average relative fitting error ranging from 0.4% to 2.6%. When predicting for target models, we get within 3% relative error on MMLU, HellaSwag, PIQA, and CommonsenseQA. We overestimated Social IQa (+4.7%, +2.9%), and underestimated ARC-Challenge (-7.6%, -5.5%), ARC-Easy (-4.5%, -6.0%), and OpenBookQA (-8.2%, -6.0%). The underestimations happen when we cannot accurately 7 Preprint Figure 3: Task loss vs training scale (N, D), with fitting on the power function in Equation 1. = 5xC; = 10xC. We report the average relative fitting error in = 1xC; = 2xC; parentheses following the task name, and prediction error in the plot next to the target model point. In general, the power function can be well fitted to data points from the ladder models, and the fitted function can make rather reliable predictions for the targer models. See Table 7 for the parameters of the fitted functions. Figure 4: Task RC accuracy vs task loss, with fitting on the sigmoid function in Equation 2. We report the average relative fitting error in parentheses following the task name, and prediction error in the plot next to the target model point. For each task, the data points collected from ladder models lie on shared sigmoidal curve, and the fitted function can make rather reliable predictions for the target models. The shaded area represents the prediction intervals for the fitted function. See Table 7 for the parameters of the fitted functions. extrapolate the slope of the sigmoidal function from data points collected from the small ladder models. 8 Preprint Task Winogrande BoolQ CommonsenseQA OpenBookQA ARC-Easy ARC-Challenge MMLU Social IQa PIQA HellaSwag Std. dev. of final 1B checkpoints Loss SD10 0.0115 0.0068 0.0056 0.0047 0.0045 0.0037 0.0026 0.0024 0.0019 0.0007 Loss % SD10 0.75 % 1.76 % 0.56 % 0.34 % 0.66 % 0.40 % 0.26 % 0.23 % 0.19 % 0.09 % Accuracy SD10 0.0048 0.0186 0.0035 0.0095 0.0043 0.0040 0.0010 0.0032 0.0024 0.0016 Accuracy % SD10 0.77 % 2.86 % 0.55 % 2.51 % 0.61 % 1.00 % 0.28 % 0.61 % 0.31 % 0.25 % Predictions for 7B-4T Accuracy % Error Chained % Error Loss % Error 4.5 % 11.4 % 11.7 % 1.3 % 13.3 % 7.0 % 1.3 % 4.3 % 2.0 % 0.3 % 23.7 % 4.3 % 1.0 % 8.2 % 4.5 % 7.6 % 0.3 % 4.7 % 2.3 % 1.1 % 8.9 % 1.8 % 4.2 % 10.6 % 9.4 % 16.9 % 1.3 % 2.0 % 1.0 % 1.4 % Table 4: Absolute and relative standard deviation of last 10 training checkpoints (SD10) for the 1B-10xC model on task loss and accuracy, along with relative error for predicting the 7B4T model on task loss (step 1 only), accuracy (step 2 only) and chained accuracy (step 1 and step 2). We observe that tasks with above-average SD10 for 1B-10xC, highlighted in red, tend to have high prediction errors for the 7B-4T model. In particular, we observe strong Pearson correlation between loss std. dev. and accuracy prediction error (r = 0.821, = 0.004)."
        },
        {
            "title": "5 Analyses",
            "content": "5.1 Which tasks can the model ladder predict? Some tasks are inherently more challenging to predict reliably at larger model scales. For example, test set with an inadequate sample size, low-quality test instances or questions that are too difficult for small models to answer may result in task that is more challenging for the ladder to predict. We anticipate three sources of prediction failure: High variance in task loss, resulting in less reliable data points for function fitting. High variance in task accuracy, leading to higher spread of prediction targets. Random-chance task accuracy in the ladder models, due to task being too difficult to observe any signal from small models. Using the ladder models that have already been trained, we attempt to predict which tasks will exhibit high errors around the prediction target. We use the intermediate checkpoints of the largest ladder model to measure the noise for task loss and task accuracy. Failure due to random-chance accuracy is discussed when predicting multiple-choice tasks in B.2. Variance analysis. To quantify the variance of the task loss and accuracy, we compute the standard deviation of the last training checkpoints of the largest ladder model, which we denote as SDn. Similar to our calculation of relative error, we can compare SDn between tasks using relative standard deviation (also called the coefficient of variation), defined as: Relative SDn = SD (final checkpoints) Mean (final checkpoints) 100% (3) task where ladder models exhibit high variance during training will result in higher standard deviation across adjacent training checkpoints. This may indicate that one should anticipate higher error when predicting the final task performance at the target scale. To illustrate SDn, we show the intermediate checkpoints for the largest ladder model on OpenBookQA and MMLU in Figure 5, where we find that SDn captures the apparent noise between adjacent training checkpoints. We calculate the standard deviation for the task loss and task accuracy using the final 10 checkpoints of the largest ladder model in Table 4, alongside prediction errors for the 7B-4T model. Benchmarks with low SD10, such as MMLU and HellaSwag, also resulted in low 9 Preprint Figure 5: Intermediate checkpoints and relative standard deviation over the final 10 checkpoints (SD10) for the 1B-10xC ladder model on MMLU and OpenBookQA. The intermediate checkpoints for OBQA appear noisier than MMLU, resulting in higher SD10. We find that tasks where ladder models exhibit high intermediate checkpoint noise indicate higher prediction error  (Table 4)  . Results across all tasks are in Figure 9. prediction errors at the target scale, indicating these tasks are easier for the ladder to predict. We also find that the standard deviation over task loss (Loss SD10) is correlated with step 2 accuracy error (Pearson = 0.821, = 0.004 for 7B-4T and = 0.855, = 0.002 for 13B-5T). In practice, the SD10 can be reported alongside task loss and accuracy predictions to explain which benchmarks may have high error before running the target model. 5.2 How much compute is needed for predicting performance for each task? In this section, we consider the impact of the scale of the model ladder on the prediction for the target model. In general, the prediction is harder if the difference of scale between the ladder and the target models is larger. We explore this trade-off using the 7B-4T model as the target. In Figure 6, we order the ladder models by compute-FLOPs, and then start from the smallest compute model and progressively add larger compute models for function fitting. We observe that when using smaller number of FLOPs, the prediction error is significantly worse. For instance, on MMLU, using our full ladder (3.2% compute of the target model), we get 1.3% error. Reducing the number of ladder FLOPs to 0.1% of the target model compute increases the error to 12% which is an order of magnitude higher than the error with the full ladder. This increase in the prediction error is more observable in certain tasks. Interestingly, we see slight improvement in errors with less compute in ARC-Challenge and ARC-Easy, potentially due to the variance as seen in 5.1. We also consider the impact of each axis of the ladder models (model size and Chinchilla multiplier xC) on the prediction error for each task. Figure 7 shows the prediction errors for each step as we progressively increase the model size included in the ladder (i.e., ladder upto 760M will include 190M, 370M, 760M). We observe downward trend in the prediction error for most tasks as the ladder size gets closer to the target model. 10 Preprint Figure 6: Prediction error on the 7B-4T target model as function of the total computeFLOPs used in the ladder models for prediction. The left-most point uses only the smallest compute model (190M-1xC) for prediction. The right-most point uses the full ladder (all 16 models). We observe that the prediction error generally reduces as the ladder FLOPs increase. ARC-C, ARC-E, and OBQA display higher variation (which can be attributed to the variation analysis in 5.1), but still have downward trend. Figure 7: Prediction error on the 7B-4T target model when including up to model size (N) in the ladder for prediction. Eg. up to 760M will include 190M, 370M, 760M models trained to 1xC, 2xC, 5xC, 10xC. For most tasks, we observe downward trend in the prediction error as increases. Figure 8 similarly shows the impact of including models with longer training regimes. Interestingly, we do not see significant reduction in the prediction error as we include the ladder models trained to higher Chinchilla multipliers5. There is slight downward trend (noticeable in MMLU), but this dimension has more flexibility. For users of our approach, and future work, we encourage exploration in reducing the overall cost by considering both these dimensions. Concretely, given fixed budget for the ladder models, increasing the model size is more likely to improve the prediction error, compared to training for longer. 5The target model 7B-4T is trained to approximately 28xC. 11 Preprint Figure 8: Prediction error on the 7B-4T target model when including models trained up to chinchilla multiplier (xC). Eg. xC up to 2xC will include 190M, 370M, 760M, 1B models trained to 1xC, 2xC. The downward trend of the prediction error is less significant than with varying N."
        },
        {
            "title": "6 Ablating Design Choices",
            "content": "We made several design choices in our method, e.g., the two-step approach, using the task loss as an intermediate feature. In this section, we explore some alternatives to these choices. Table 6 provides summary of the design choices and the corresponding prediction errors. An overarching discussion is presented in 6.5 . 6.1 Task loss prediction: The impact of using compute-FLOPs instead of (N, D) In step 1, we used model size and training data size (N, D) to predict task loss. Here, we test if compute-FLOPs can be used directly for task loss prediction in the over-trained regime with the ladder models. We fit similar power function L(C) = A/Cα + (4) where A, α, are parameters to fit. Figure 11 shows the function fitting and prediction. Compared with Figure 3, the relative fitting error is higher than using (N, D) on all tasks, likely because Equation 4 has fewer free parameters than Equation 1. For the target models, the task loss prediction errors are generally worse than using (N, D), including on MMLU; ARC-Challenge and ARC-Easy are the outliers  (Table 5)  . Using compute-FLOPs fails to distinguish compute-optimal and overtrained models: they can have the same compute-FLOPs but different losses (Hoffmann et al., 2022). In principle, using compute-FLOPs is not expressive enough to generalize to overtrained regimes, and therefore we choose not to use it in our main method. 6.2 Task loss formulation: Incorporating both correct and incorrect answers Our intermediate feature, task loss, only considers the correct choice of each problem. However, task RC accuracy is determined by the losses of both correct and incorrect choices, which has been cited as major challenge for predicting downstream performance (Schaeffer et al., 2024): Acc = (cid:20) 1 1 i=1 arg max (cid:17) (cid:16) (i) = ˆk(i) (cid:21) , (5) 12 Preprint (i) is the loss over the kth answer option of the ith example and ˆk(i) is the label of where the correct answer. The loss terms may include some normalization, e.g., by the number of characters in each answer or the unconditional answer probability (see Gu et al., 2024). We explore taking into account incorrect answers by computing the task cross-entropy, which we define as the cross entropy loss over all answer choices by using the Lk terms as logits. In C.2, we investigate whether this is suitable intermediate feature in our task prediction framework. We observe that (1) on some tasks, this intermediate quantity predicts the task accuracy better than using the loss of the correct answer alone, but (2) the two-step predictions are overall worse due to severe errors on MMLU and ARC-Challenge. These tasks incur sizable extrapolation errors of 4-5% in step 1, suggesting that Chinchilla formulation from Equation 1 may not fully capture the scaling behavior of the task crossentropy. Furthermore, these errors are amplified in step 2, as the accuracy tends to be far more sensitive to changes in the task cross-entropy than the loss of the correct answer. 6.3 Task loss vs. general language modeling loss Our method used task loss as the intermediate feature to bridge training scale and task accuracy. Here, we ask if we can instead use general language modeling loss. Language modeling loss on held-out sets has been shown shown to follow the power law (Hoffmann et al., 2022). Since this loss reflects the capability of LMs, we consider if we can map it to task performance. We experiment with using the language modeling loss on the C4-en validation set (Raffel et al., 2019) as the intermediate feature. Figure 15 shows the function fitting and predictions. Using C4 loss resulted in higher average prediction error on the task we care most about MMLU. Prediction error is lower on ARC-Challenge and ARC-Easy. We conclude that using C4 loss can benefit certain tasks where task loss does not work well, and decided to use task loss in our main method because of its superiority on predicting MMLU. We discuss the results in more detail in C.3. 6.4 The impact of using two-step prediction instead of combining them into one step Here we ask, do we need an explicit intermediate feature at all? Can we directly predict task accuracy from the training scale (N, D) with one function? We explore this possibility by combining Equation 1 and Equation 2 into single parameterized function: Acc(N, D) = 1 + exp (cid:0) k(A/Nα + B/Dβ + L0)(cid:1) + and we merge parameters and L0 into A, B, E, so that we reduce to 7 free parameters: Acc(N, D) = 1 + exp (cid:0) (A/Nα + B/Dβ + E)(cid:1) + (6) By combining into single parameterized function, we remove the priori definition of specific intermediate feature (i.e., the A/Nα + D/Dβ + expression no longer carries specific meaning), while preserving the representation power of the function. This function can be harder to fit, since it has more free parameters, it is not convex, and we cannot use any data points collected from intermediate checkpoints as we did in step 2. We fit this parameterized function with data points from the final checkpoints of the ladder models, using the same optimization method as in step 1. Figure 16 shows the function fitting and predictions. The prediction error is worse than using the two-step approach on 4 out of 8 tasks, and the function fitting degenerated on CommonsenseQA. This single-step approach is not as robust as the two-step approach. We discuss the results in more detail in C.4. 13 Preprint Design choice MMLU HS ARC-C ARC-E PIQA CSQA SIQa OBQA (N, D) task loss (3) FLOPs task loss (6.1) 1.3% 0.3% 7.0% 13.3% 2.0% 11.7% 4.3% 1.3% 4.3% 2.1% 2.4% 5.3% 2.0% 12.5% 5.7% 0.5% Design choice MMLU HS ARC-C ARC-E PIQA CSQA SIQa OBQA (N, D) task loss (3) FLOPs task loss (6.1) 0.2% 1.2% 9.4% 16.0% 2.7% 18.5% 3.6% 0.9% 5.2% 2.6% 3.5% 7.0% 2.6% 18.8% 5.7% 1.4% Table 5: Comparison of design choices: We report the average step 1 prediction error for both target models 7B-4T (upper) and 13B-5T (lower). Design choice MMLU HS ARC-C ARC-E PIQA CSQA SIQa OBQA (N, D) task loss (3) 1.3% 1.4% 16.9% 9.4% 1.0% 4.2% 2.0% 10.6% Task loss over all choices (6.2) C4 loss (6.3) Single step (6.4) 18.3% 7.2% 21.3% 5.4% 3.1% 2.6% 0.7% 7.1% 2.2% 4.5% 1.4% 1.2% 0.7% 5.8% 6.2% 2.0% 5.6% 0.5% 21.7% 6.1% 0.7% 5.1% 5.4% 9.0% Design choice MMLU HS ARC-C ARC-E PIQA CSQA SIQa OBQA (N, D) task loss (3) 0.7% 2.5% 17.5% 11.4% 1.1% 4.7% 2.7% 7.8% Task loss over all choices (6.2) C4 loss (6.3) Single step (6.4) 20.2% 10.5% 19.5% 6.2% 2.9% 2.7% 1.2% 0.2% 5.0% 5.7% 3.9% 1.9% 1.5% 7.0% 7.6% 10.4% 7.1% 0.8% 22.6% 7.9% 0.6% 6.6% 6.8% 4.7% Table 6: Comparison of design choices: We report the average chained prediction error for both target models 7B-4T (upper) and 13B-5T (lower). We observe that for MMLU, our approach yields the lowest prediction error. We observe higher errors for ARC-C, ARC-E, and OBQA using our method, which also aligns with our variance analysis in 5.1, and find that using C4 as the intermediate feature works better for them. 6.5 Discussion of design choices for each task Table 6 compares prediction errors against alternative design choices. Our two-step method has low error for MMLU, PIQA, HellaSwag, and Social IQa, corresponding to the tasks with the lowest variance around task loss and accuracy, as observed in Table 4. Tasks with noisy mapping between their intermediate feature and accuracy, such as the high variance for task loss observed in ARC-Challenge, ARC-Easy and OpenBookQA, may lead to compounded error when using multi-step prediction approach. This may explain the higher prediction errors for the design choices in Table 6 which directly or indirectly rely on an intermediate representation of the task loss. For ARC-Challenge and OpenBookQA in particular, their high variance for both the intermediate task loss and accuracy make them challenging for the ladder to predict. HellaSwag and PIQA, the tasks where we observed the least variance, were the easiest to predict across all design choices. Interestingly, this low variance seems to indicate that the combined single step approach may be capable of predicting performance directly. Finally, MMLU and HellaSwag have large sample sizes 5x larger than the other tasks in this work. Given their low sample variance and noise around the prediction target, we expect their errors are most representative of the true difficulty of predicting downstream performance. For this reason, we present the two-step design as our main approach. Recommendations. Based on our experiments, we present the following guidelines to develop task scaling laws for new overtrained model / new task: Conduct variance analysis as described in 5.1 for the task(s) using the largest ladder model that can be trained based on available compute (we used 1.3B-10xC). Construct the rest of the ladder as described in 2.1 with wider range of model sizes compared to xC (but do train the Preprint models beyond 1xC), as discussed in 5.2. For tasks that display high variance, especially for the target metric task accuracy, we generally expect to see lower predictability. High variance in task loss can potentially be mitigated by using alternative intermediate features."
        },
        {
            "title": "7 Related Work",
            "content": "Scaling laws for language modeling. Kaplan et al. (2020) and Hoffmann et al. (2022) were among the first to postulate the functional form of language modeling losses as power function of model parameters and data size. The Chinchilla equation, in particular, has become the founding basis of many subsequent scaling law work (Muennighoff et al., 2023; Gadre et al., 2024; Zhang et al., 2024). This line of work focuses on the language modeling loss on held-out set with similar distribution as the pretraining data, and much are left to be done on understanding how language models scale on downstream tasks. Scaling laws for downstream tasks. Scaling predictions for downstream tasks has been explored in Gadre et al. (2024). Instead of making accuracy predictions directly for individual tasks, they compute the average top-1 error over 17 LLM-foundry (MosaicML, 2024) evaluation tasks as function of cross entropy loss on C4 (Raffel et al., 2019). Dubey et al. (2024) uses two-step prediction to first map the the training compute to the negative log-likelihood of the correct answer for single task in an evaluation benchmark, and then relate the log-likelihood to the task accuracy. Unlike our work, which uses fixed ladder of small models, they rely on first finding compute-optimal models. Chen et al. (2024) also employs two-stage approach for predicting downstream performance, but uses the pre-training loss instead of task-specific loss as the intermediate step. Hu et al. (2023) predicts performance on generative tasks by introducing an intermediate PassUntil metric, which is the number of repeated decoding required before landing on correct answer."
        },
        {
            "title": "8 Conclusion and Future Work",
            "content": "In this paper, we develop model ladders and task scaling laws and use them to predict task performance of pretrained language models. With less than 1% of the pretraining compute, we are able to predict the task performance of 7B-4T and 13B-5T models on individual multiple-choice tasks with good accuracy. In future work, we hope to reduce the noise in task evaluation metrics by increasing the size of these evaluation sets, which can hopefully yield even lower prediction errors. While we focused on the ranked classification (RC) format of multiple-choice tasks in this work, we hope to make accurate predictions when tasks are written in the multiple-choice (MC) format, which more accurately reflects the capability of larger LMs. MC format accuracy is harder to predict because all our ladder models are too small to exhibit meaningful signal for extrapolation; we show some preliminary results in B.2 but leave this as future work. Finally, we also hope to validate our method on larger models (e.g., at the 70B scale and beyond). 15 Preprint"
        },
        {
            "title": "Limitations",
            "content": "Our task performance prediction method is developed with and validated on the OLMo 2 family of models. In principle, the method should be transferrable to any transformer-based language models, though it has not been validated in practice. We focused on predicting performance on multiple-choice tasks written in ranked classification (RC) format, leaving multiple-choice format (MC) and generative tasks as future work."
        },
        {
            "title": "Acknowledgements",
            "content": "We would like to thank Yejin Choi, Luke Zettlemoyer, members of the H2lab, and Ziqi Ma for their invaluable feedback."
        },
        {
            "title": "References",
            "content": "Ai2. Olmo 2: The best fully open language model to date, 2024. URL https://allenai.org/ blog/olmo2. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenhang Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, K. Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Yu Bowen, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xing Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. ArXiv, abs/2309.16609, 2023. URL https://api.semanticscholar.org/CorpusID:263134555. Yonatan Bisk, Rowan Zellers, Ronan Le bras, Jianfeng Gao, and Yejin Choi. PIQA: Reasoning about physical commonsense in natural language. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):74327439, Apr. 2020. doi: 10.1609/aaai.v34i05.6239. URL https://ojs.aaai.org/index.php/AAAI/article/view/6239. Yangyi Chen, Binxuan Huang, Yifan Gao, Zhengyang Wang, Jingfeng Yang, and Heng Ji. Scaling laws for predicting downstream performance in llms. 2024. URL https: //api.semanticscholar.org/CorpusID:273323177. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. pp. 29242936, Minneapolis, Minnesota, June 2019. doi: 10.18653/v1/N19-1300. URL N19-1300. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv, 2018. URL http://arxiv.org/abs/1803.05457. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozi`ere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Cant on Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab A. AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriele Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guanglong Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan 16 Preprint Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Laurens Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Ju-Qing Jia, Kalyan Vasuden Alwala, K. Upasani, Kate Plawiak, Keqian Li, Ken-591 neth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren RantalaYeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline C. Muzzi, Mahesh Babu Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melissa Hall Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri S. Chatterji, Olivier Duchenne, Onur cCelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Chandra Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yiqian Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zhengxu Yan, Zhengxing Chen, Zoe Papakipos, Aaditya K. Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adi Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Ben Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, ShangWen Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzman, Frank J. Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory G. Sizov, Guangyi Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Han Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kaixing(Kai) Wu, KamHou, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Preprint Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sung-Bae Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Andrei Poenaru, Vlad T. Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xia Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. The llama 3 herd of models. ArXiv, abs/2407.21783, 2024. URL https://api.semanticscholar.org/CorpusID:271571434. Samir Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar, Suchin Gururangan, Mitchell Wortsman, Rulin Shao, Jean-Pierre Mercat, Alex Fang, Jeffrey Li, Sedrick Scott Keh, Rui Xin, Marianna Nezhurina, Igor Vasiljevic, Jenia Jitsev, Alexandros G. Dimakis, Gabriel Ilharco, Shuran Song, Thomas Kollar, Yair Carmon, Achal Dave, Reinhard Heckel, Niklas Muennighoff, and Ludwig Schmidt. Language models scale reliably with over-training and on downstream tasks. ArXiv, abs/2403.08540, 2024. URL https://api.semanticscholar.org/CorpusID:268379614. Yuling Gu, Oyvind Tafjord, Bailey Kuehl, Dany Haddad, Jesse Dodge, and Hannaneh Hajishirzi. Olmes: standard for language model evaluations, 2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and L. Sifre. Training compute-optimal large language models. ArXiv, abs/2203.15556, 2022. URL https://api.semanticscholar.org/CorpusID:247778764. Shengding Hu, Xin Liu, Xu Han, Xinrong Zhang, Chaoqun He, Weilin Zhao, Yankai Lin, Ning Ding, Zebin Ou, Guoyang Zeng, Zhiyuan Liu, and Maosong Sun. Predicting emergent abilities with infinite resolution evaluation. In International Conference on Learning Representations, 2023. URL https://api.semanticscholar.org/CorpusID:263672005. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeff Wu, and Dario Amodei. Scaling laws for neural language models. ArXiv, abs/2001.08361, 2020. URL https://api.semanticscholar.org/ CorpusID:210861095. Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Yitzhak Gadre, Hritik Bansal, Etash Kumar Guha, Sedrick Scott Keh, Kushal Arora, Saurabh Garg, Rui 18 Preprint Xin, Niklas Muennighoff, Reinhard Heckel, Jean-Pierre Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu, Thao Nguyen, Igor Vasiljevic, Sham M. Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke S. Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldani, Pang Wei Koh, Jenia Jitsev, Thomas Kollar, Alexandros G. Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, and Vaishaal Shankar. Datacomp-lm: In search of the next generation of training sets for language models. ArXiv, abs/2406.11794, 2024. URL https://api.semanticscholar.org/CorpusID:270560330. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. pp. 23812391, Brussels, Belgium, October-November 2018. doi: 10.18653/v1/D18-1260. URL D18-1260. MosaicML. Llm foundry, 2024. URL https://github.com/mosaicml/llm-foundry. Accessed: 2024-12-03. Niklas Muennighoff, Alexander M. Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models. ArXiv, abs/2305.16264, 2023. URL https://api.semanticscholar.org/ CorpusID:258888192. Tomer Porian, Mitchell Wortsman, Jenia Jitsev, Ludwig Schmidt, and Yair Carmon. Resolving discrepancies in compute-optimal scaling of language models. ArXiv, abs/2406.19146, 2024. URL https://api.semanticscholar.org/CorpusID:270764838. Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1140:67, 2019. URL https://api.semanticscholar.org/CorpusID:204838007. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An adversarial winograd schema challenge at scale. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):87328740, Apr. 2020. doi: 10.1609/aaai.v34i05.6399. URL https://ojs.aaai.org/index.php/AAAI/article/view/6399. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa: Commonsense reasoning about social interactions. pp. 44634473, Hong Kong, China, November 2019. doi: 10.18653/v1/D19-1454. URL D19-1454. Rylan Schaeffer, Hailey Schoelkopf, Brando Miranda, Gabriel Mukobi, Varun Madan, Adam Ibrahim, Herbie Bradley, Stella Biderman, and Sanmi Koyejo. Why has predicting downstream capabilities of frontier AI models with scale remained elusive? In Trustworthy Multi-modal Foundation Models and AI Agents (TiFA), 2024. URL https: //openreview.net/forum?id=AbHHrj9afB. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: question answering challenge targeting commonsense knowledge. pp. 41494158, Minneapolis, Minnesota, June 2019. doi: 10.18653/v1/N19-1421. URL N19-1421. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can machine really finish your sentence? pp. 47914800, Florence, Italy, July 2019. doi: 10.18653/v1/P19-1472. URL P19-1472. Ge Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou Leuang Yu, Danny Pan, Esther Cheng, Jie Liu, Qunshu Lin, Raven Yuan, Tuney Zheng, Wei Pang, Xinrun Du, Yiming Liang, Yi Ma, Yizhi Li, Ziyang Ma, Bill Yuchen Lin, Emmanouil Benetos, Huan Yang, Junting Zhou, Kaijing Ma, Minghao Liu, Morry Niu, Noah Wang, Quehry Que, Ruibo Liu, Si yang Liu, Shawn Guo, Soren Gao, Wangchunshu Zhou, Xinyue Zhang, Yizhi Zhou, Yubo Wang, Yuelin Bai, Yuhan Zhang, Yuxiang Zhang, Zenith Wang, Zhen 19 Preprint Yang, Zi-Kai Zhao, Jiajun Zhang, Wanli Ouyang, Wenhao Huang, and Wenhu Chen. Map-neo: Highly capable and transparent bilingual large language model series. ArXiv, abs/2405.19327, 2024. URL https://api.semanticscholar.org/CorpusID:270094960. 20 Preprint Task Step 1 Fitted Function L(N, D) = 38.07/N0.23 + 100.09/D0.24 + 0.45 MMLU L(N, D) = 11.23/N0.20 + 60.37/D0.26 + 0.50 HellaSwag L(N, D) = 702974.93/N0.79 + 38.45/D0.20 + 0.65 ARC-Challenge L(N, D) = 79412.07/N0.66 + 3957.51/D0.42 + 0.56 ARC-Easy L(N, D) = 405.66/N0.40 + 10.16/D0.15 + 0.72 PIQA CommonsenseQA L(N, D) = 56.86/N0.23 + 10.91/D0.11 + 0.00 Social IQa OpenBookQA L(N, D) = 1200.94/N0.45 + 7897.19/D0.48 + 0.95 L(N, D) = 86346.32/N0.69 + 137.35/D0.26 + 1.20 Task Step 2 Fitted Function Acc(L) = 0.74/(1 + exp(4.83(L 0.62))) + 1.00 MMLU Acc(L) = 0.73/(1 + exp(12.74(L 0.77))) + 0.99 HellaSwag Acc(L) = 0.78/(1 + exp(5.91(L 0.71))) + 1.00 ARC-Challenge Acc(L) = 0.65/(1 + exp(4.13(L 0.74))) + 1.00 ARC-Easy Acc(L) = 0.46/(1 + exp(5.03(L 0.96))) + 1.00 PIQA CommonsenseQA Acc(L) = 0.86/(1 + exp(2.21(L 1.13))) + 1.00 Acc(L) = 0.60/(1 + exp(7.16(L 0.89))) + 1.00 Social IQa Acc(L) = 0.79/(1 + exp(4.31(L 1.08))) + 1.00 OpenBookQA Table 7: Parameters for the fitted functions for Figure 3 and Figure 4."
        },
        {
            "title": "A Fitted Parameters",
            "content": "In Table 7, we list the parameters of the fitted functions in Figure 3 and Figure 4."
        },
        {
            "title": "B Additional Analyses",
            "content": "B.1 Additional variance analysis results Figure 9 shows the intermediate checkpoints for the largest ladder model across all tasks, along with the relative standard deviation over the final 10 checkpoints (% SD10). In Table 4, we show that this intermediate checkpoint noise corresponds to the difficulty of predicting the task performance for the target model. B.2 Predicting task accuracy under the MC format In this paper we have been focusing on the ranked classification (RC) format of tasks. Here we explore if we can make predictions when problems are written in the multiple-choice (MC) format. Unfortunately, all ladder models are small and show random performance in MC on all tasks (Gu et al., 2024), and thus it is not practical to fit the mapping from task loss to MC accuracy in step 2 with data points from the ladder models. Instead, we use data points from early intermediate checkpoints of the target models. Observing the MC accuracy curves during training of the 7B-4T and 13B-5T models (Figure 10 upper), we find that they have three phases: (1) very early in training, MC accuracy is random; (2) at some point, MC accuracy increases rapidly; (3) finally, it starts growing steadily with more training steps. The phase of rapid growth for model is strikingly identical across all tasks: around 70k steps for the 7B-4T model, and around 20k steps for the 13B-5T model. As result, the data points {(Li, Acci)} fall on peculiar shape that cannot be described by sigmoidal function (Figure 10 lower left). Therefore, we fit the sigmoidal curve on data points collected from intermediate checkpoints during the third phase of the MC accuracy curve. For 7B-4T, we use steps between 170k and 450k; for 13B-5T, we use steps between 50k and 150k. Noticing that the curves for the two models do not coincide, we fit function for each model separately (as opposed to single joint fitting in RC). We conduct case study with MMLU and show the fitting and 21 Preprint Figure 9: Intermediate checkpoints and standard deviation over the final 10 checkpoints (SD10) for the 1B-10xC ladder model. We find some tasks exhibit high noise between adjacent training checkpoints, which is indicative of the inherent difficulty in predicting performance for such tasks using the model ladder. prediction in Figure 10 (lower right). Using data points from the third phase, we can reliably extrapolate the mapping from task loss to MC accuracy. We predicted the MC accuracy of 7B-4T with 3.0% relative error, and that of 13B-5T with 1.0% relative error. When chaining this with the step 1 fitted function, our end-to-end prediction of MMLU accuracy has an absolute error of 0.3 points on the 7B-4T model, and 0.4 points on the 13B-5T model. Compute overhead. To fit the step 2 function for MC accuracy, we do need to collect data points from an early part of the target model training. 7B-4T is trained for 928k steps, and we use data points up to 450k, which is about 50% of the full training compute. 13B-5T is trained for 596k steps, and we use data points up to 150k, which is about 25% of the full training compute. We note this is significantly more compute than used in the model ladders for predicting RC performance. However, as noted above, smaller models (which are less compute intensive) dont provide useful signal here. This is an opportunity for future work. Preprint Figure 10: Upper: MC accuracy curves during training of 7B-4T and 13B-5T. Lower left: Task MC accuracy vs task loss, with data points from all intermediate checkpoints of 7B-4T and 13B-5T. sigmoidal function cannot fit the data points. Lower right: Task MC accuracy vs task loss, where the sigmoidal function (Equation 2) is fitted on data points from the intermediate checkpoints between 170k450k steps of 7B-4T, and between 50k150k steps of 13B-5T. = 5xC; = Figure 11: Step 1 using compute-flops instead of (N, D). = 1xC; = 2xC; 10xC. We report the average relative fitting error in parentheses following the task name, and prediction error in the plot next to the target model point."
        },
        {
            "title": "C Additional Details on Design Choices",
            "content": "C.1 Task loss prediction: The impact of using compute-FLOPs instead of (N, D) Figure 11 shows the function fitting and prediction in step 1, using compute-FLOPs as the input feature. 23 Preprint 7B-4T 13B-5T BPB TaskCE BPB TaskCE Error %Error Error %Error Error %Error Error %Error MMLU HellaSwag ARC-Challenge ARC-Easy PIQA CommonsenseQA Social IQa OpenBookQA 0.6 1.2 10.4 8.0 0.8 3.1 1.2 5.2 1.3% 9.0 5.9 1.4% 16.9% 13.1 4.5 9.4% 2.5 1.0% 1.9 4.2% 0.5 2.0% 3.5 10.6% 18.3% 0.3 2.1 7.2% 21.3% 11.1 5.4% 10.0 0.9 3.1% 3.5 2.6% 1.7 0.7% 3.7 7.1% 0.6% 10.4 8.7 2.5% 17.5% 12.3 5.4 11.4% 2.4 1.1% 2.0 4.7% 0.8 2.7% 0.1 7.8% 20.2% 10.5% 19.5% 6.2% 2.9% 2.7% 1.2% 0.2% Table 8: Comparison of original and new prediction errors for 7B-4T and 13B-5T models across tasks. Absolute (Error) and relative (%Error) differences are shown. C.2 Task cross-entropy as intermediate feature Our intermediate feature in step 1 is the conditional loss of the correct answer of task, from which we predict the accuracy in step 2. However, the accuracy is computed from the losses of both correct and incorrect continuations, as seen in Equation 5. We commonly maximize the accuracy by minimizing the task cross-entropy as surrogate loss, in which the Lk terms are used as logits: TaskCE = (cid:32) 1 i=1 (i) ˆk(i) + log exp (cid:16) (i) (cid:33) (cid:17) . (7) We refer to this as TaskCE to distinguish it from the language modeling cross-entropy over tokens. We investigate this cross entropy as an alternative intermediate feature, which takes into account the incorrect answer choices as an additional log partition term. Step 2 scaling with Task Cross-Entropy. When inspecting the ladder models, we notice that the task cross-entropy correlates with accuracy in more linear fashion than is the case for BPB. We therefore seek for function Acc(TaskCE) that is linear for large values of TaskCE, but for small values of TaskCE should approach an asymptote of perfect accuracy (where all the probability mass is concentrated on the correct answer). good candidate for this transition is the log-sigmoid function, leading us to conjecture the following formula: Acc(TaskCE) = 1 log σ (k(TaskCE TaskCE0)) , (8) where σ() is the sigmoid function, and a, k, TaskCE0 are constants to be fit. This expression is not valid in the regime of large task cross-entropies, where models perform random guessing. Thus, we only use data points from the last 50% of training for curve fitting. Results. Figure 12 and Figure 13show the results of fitting step 1 and step 2 with the task cross-entropy as output and input variable, respectively. Figure 14 visualizes the predictions of combining these two steps, and Table 8 compares the predictions errors between using task cross-entropy or the correct answer loss as the intermediate feature. We make the following observations: In step 2, the task cross-entropy is overall more accurate predictor of RC accuracy: The average prediction errors across 7B and 13B models is 2.75% vs. 3.6% for the correct answer loss in Figure 4. We note that the step 2 fit in Figure 13 is particularly good on ARC-Challenge and OpenBookQA, but substantially worse on HellaSwag. We note all our observations for HellaSwag are in the linear regime of the log-sigmoid, making it difficult to estimate the curvature of the transition to the horizontal asymptote. While the average fitting and extrapolation errors in step 1 of Figure 12 are moderate and comparable to Figure 3. However, we notice that the fitting errors for task cross-entropy 24 Preprint Figure 12: Predicting final task cross-entropy (Equation 7) from model parameters and token budget in step 1. Figure 13: Predicting the task metric from the task cross-entropy (Equation 7) in step 2. tend to be more skewed, i.e., the fitted Chinchilla curves tend to underestimate the performance of the 190M-1xC model and overstimate the 190M-10xC, while underestimating the 1B-1xC and overestimating the 1B-10xC. As these errors appear more systematic, it raises questions whether this parametric form is good fit over much larger scales. Notably, it results in substantial errors in the task cross-entropy of MMLU and ARC-Challenge (4.2% - 4.8%). Finally, we note that the values for task cross-entropy fall into narrow range. This means that the predictions in step 2 are highly sensitive to small changes in the task cross-entropy, e.g., difference in task cross-entropy of less than 0.05 nats in PIQA can make the difference between random task accuracy (at TaskCE = 0.69) and 83% accuracy (at TaskCE = 0.65). As consequence, small relative errors in step 1 (0.6% for the 13B model on PIQA) are amplified in Step 2 and result in larger overall errors in accuracy (2.9% for PIQA), despite good fit in step 2 (0.9% prediction error for the 13B accuracy). Similarly, step-1 prediction error of 4.7% for the 13B model on ARC-Challenge results in an overall error of 19.5%. 25 Preprint Figure 14: Chaining predictions from step 1 (Figure 12) and step 2 (Figure 13) with task cross-entropy as the intermediate feature. C.3 General language modeling loss as intermediate feature Figure 15 shows the function fitting and predictions using the C4 language modeling loss as intermediate feature (6.3). In step 1, the fitted function underestimates the C4 loss by 2.0% for 7B-4T and 3.8% for 13B-5T. This error is higher than that on task loss prediction for 4 out of 8 tasks. The step 2 error is also higher than using task loss on 4 out of 8 tasks. When chaining the two steps to predict task accuracy, using C4 loss as intermediate feature resulted in higher error on 5 tasks MMLU, HellaSwag, CommonsenseQA, Social IQA, and OpenBookQA. C.4 Combining step 1 and 2 into single step Figure 16 shows the function fitting and predictions using the single-step approach (6.4). The prediction error is higher on 4 out of 8 tasks MMLU, ARC-Challenge, CommonsenseQA, and Social IQA. In particular, on CommonsenseQA the fitted functions does not vary with respect to model size, indicating degenerated function fitting. We conclude that the single-step approach is not as robust as the two-step approach. 26 Preprint Figure 15: Using language modeling loss on C4-en validation as the intermediate feature. From top to bottom: step 1, step 2, and chaining the two steps. 27 Preprint Figure 16: Task RC accuracy vs training scale (N, D), with fitting on the single-step function in Equation 6."
        }
    ],
    "affiliations": [
        "Allen Institute for Artificial Intelligence",
        "Paul G. Allen School of Computer Science & Engineering, University of Washington",
        "Princeton University"
    ]
}