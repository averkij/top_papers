{
    "paper_title": "COIG-P: A High-Quality and Large-Scale Chinese Preference Dataset for Alignment with Human Values",
    "authors": [
        "M-A-P Team",
        "Siwei Wu",
        "Jincheng Ren",
        "Xinrun Du",
        "Shuyue Guo",
        "Xingwei Qu",
        "Yiming Liang",
        "Jie Liu",
        "Yunwen Li",
        "Tianyu Zheng",
        "Boyu Feng",
        "Huaqing Yuan",
        "Zenith Wang",
        "Jiaheng Liu",
        "Wenhao Huang",
        "Chenglin Cai",
        "Haoran Que",
        "Jian Yang",
        "Yuelin Bai",
        "Zekun Moore Wang",
        "Zhouliang Yu",
        "Qunshu Lin",
        "Ding Pan",
        "Yuchen Jiang",
        "Tiannan Wang",
        "Wangchunshu Zhou",
        "Shenzhi Wang",
        "Xingyuan Bu",
        "Minghao Liu",
        "Guoyin Wang",
        "Ge Zhang",
        "Chenghua Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Aligning large language models (LLMs) with human preferences has achieved remarkable success. However, existing Chinese preference datasets are limited by small scale, narrow domain coverage, and lack of rigorous data validation. Additionally, the reliance on human annotators for instruction and response labeling significantly constrains the scalability of human preference datasets. To address these challenges, we design an LLM-based Chinese preference dataset annotation pipeline with no human intervention. Specifically, we crawled and carefully filtered 92k high-quality Chinese queries and employed 15 mainstream LLMs to generate and score chosen-rejected response pairs. Based on it, we introduce COIG-P (Chinese Open Instruction Generalist - Preference), a high-quality, large-scale Chinese preference dataset, comprises 1,009k Chinese preference pairs spanning 6 diverse domains: Chat, Code, Math, Logic, Novel, and Role. Building upon COIG-P, to reduce the overhead of using LLMs for scoring, we trained a 8B-sized Chinese Reward Model (CRM) and meticulously constructed a Chinese Reward Benchmark (CRBench). Evaluation results based on AlignBench \\citep{liu2024alignbenchbenchmarkingchinesealignment} show that that COIG-P significantly outperforms other Chinese preference datasets, and it brings significant performance improvements ranging from 2% to 12% for the Qwen2/2.5 and Infinity-Instruct-3M-0625 model series, respectively. The results on CRBench demonstrate that our CRM has a strong and robust scoring ability. We apply it to filter chosen-rejected response pairs in a test split of COIG-P, and our experiments show that it is comparable to GPT-4o in identifying low-quality samples while maintaining efficiency and cost-effectiveness. Our codes and data are released in https://github.com/multimodal-art-projection/COIG-P."
        },
        {
            "title": "Start",
            "content": "COIG-P: High-Quality and Large-Scale Chinese Preference Dataset for Alignment with Human Values M-A-P, 2077AI"
        },
        {
            "title": "Abstract",
            "content": "Aligning large language models (LLMs) with human preferences has achieved remarkable success. However, existing Chinese preference datasets are limited by small scale, narrow domain coverage, and lack of rigorous data validation. Additionally, the reliance on human annotators for instruction and response labeling significantly constrains the scalability of human preference datasets. To address these challenges, we design an LLM-based Chinese preference dataset annotation pipeline with no human intervention. Specifically, we crawled and carefully filtered 92k high-quality Chinese queries and employed 15 mainstream LLMs to generate and score chosen-rejected response pairs. Based on it, we introduce COIG-P (Chinese Open Instruction Generalist - Preference), high-quality, large-scale Chinese preference dataset, comprises 1,006k Chinese preference pairs spanning 6 diverse domains: Chat, Code, Math, Logic, Novel, and Role. Building upon COIG-P, to reduce the overhead of using LLMs for scoring, we trained 8B-sized Chinese Reward Model (CRM) and meticulously constructed Chinese Reward Benchmark (CRBench). Evaluation results based on AlignBench [Liu et al., 2024a] show that that COIG-P significantly outperforms other Chinese preference datasets, and it brings significant performance improvements ranging from 2% to 12% for the Qwen2/2.5 and Infinity-Instruct-3M-0625 model series, respectively. The results on CRBench demonstrate that our CRM has strong and robust scoring ability. We apply it to filter chosen-rejected response pairs in test split of COIG-P, and our experiments show that it is comparable to GPT-4o in identifying low-quality samples while maintaining efficiency and cost-effectiveness. Our codes and data are released in https://github.com/multimodal-art-projection/COIG-P. 5 2 0 2 7 ] . [ 1 5 3 5 5 0 . 4 0 5 2 : r Figure 1. The results of different Chinese human preference datasets trained on Infinity-Instruct-3M-0625-Qwen2-7B."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Related Work 3 Data Curation 3.1 Query Collection . . . 3.2 Response Generation . 3.3 Scoring and Paring . 3.4 Human Evaluation . 3. Statics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Experiments Setup 4.1 Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Results 5.1 Overall Analysis . 5.2 Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3 Selecting Score Threshold of Pairing . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4 Comparing Chinese Human Preference Dataset . . . . . . . . . . . . . . . . . . . 6 Chinese Reward Model and Chinese Reward Benchmark 6.1 Chinese Reward Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2 Chinese Reward Benchmark . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.3 Downstream Task Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 Conclusion 8 Contributions and Acknowledgments Prompts Open-source datasets Examples 2 3 5 6 6 7 7 8 8 8 8 10 11 11 12 12 13 15 21 22 24 1. Introduction Large Language Models (LLMs) [OpenAI, 2024, Yang et al., 2024b,a, Dubey et al., 2024] have achieved remarkable success in various Natural Language Processing (NLP) tasks [Wu et al., 2025, Team et al., 2025, Wu et al., 2024, Li et al., 2024b, Wang et al., 2023, Kalla et al., 2023, Ray, 2023, Firat, 2023, Bang et al., 2023]. To enable LLMs to be better applied in real-life scenarios, researchers utilize reinforcement learning (RL) technology (e.g., PPO [Schulman et al., 2017], DPO [Rafailov et al., 2023], RLHF [Ziegler et al., 2019]) to endow the LLMs with grasping the human intention and preference. Language Dataset Number Quality Check English Chinese Arena [Chiang et al., 2024] UltraFeedback [Cui et al., 2023] Nectar [Zhu et al., 2023] HH-RLHF[Ganguli et al., 2022] H4 StackExchange [Lambert et al., 2023] PreferenceShareGPT [PreferenceShareGPT, 2024] Anthropic HH Goldenhuggingface [2024a] Ask Again [Xie et al., 2023] Orcaratgen [Just et al., 2024] CodeUF [Weyssow et al., 2024] Huozi [Huozi-Team, 2024] ZAKE [huggingface, 2024b] HH-FLHF-CN [huggingface, 2024c] CVALUES [Xu et al., 2023] GPT-4-LLM [Peng et al., 2023] Zhihu-Rlhf-3k [huggingface, 2024d] COIG-P (Ours) 55k 64k 183k 161k 10.8M 11.9 42.5k 2.6k 12k 19k 16k 77k 344k 145k 52K 3k 1,006k Table 1. The human preference alignment datasets. The Quality Check means whether the author demonstrated the quality of the dataset on the downstream task by training model. As one of the most widely spoken languages, Chinese holds significant value in the development of open-source datasets, which are crucial for fostering progress within the Chinese open-source NLP community. However, as shown in Table 1, Chinese human value preference datasets remain scarce and lack rigorous data validation, especially when compared to their English counterparts. On one hand, existing Chinese human preference datasets are not only limited in quantity but also suffer from quality issues. Notably, many of these datasets are derived from single source (e.g., zhihu)1, leading to concerns about representativeness and diversity. Moreover, some datasets lack rigorous data filtering and quality control processes, raising questions about their reliability and validity. On the other hand, introducing human annotation for chosen and rejected responses requires substantial human resources, and the inconsistency of manual annotations significantly increases the cost of data labeling. Although UltraFeedback [Cui et al., 2023] also uses the LLMs to annotate and score responses, they only use single LLM to score responses, which is likely to introduce bias. Besides, it is hard to choose high-quality chosen-rejected response pairs because they annotate 4 different scores from different dimensions for each response. Inspired by UltraFeedback [Cui et al., 2023], we propose an LLM-based Chinese preference dataset annotation pipeline to curate Chinese preference datasets without human annotation. Firstly, we collect and filter 92k Chinese queries covering comprehensive dimensions. In order to make LLMs efficiently learn the preferences of humans, we choose 15 open-source and closed-source LLMs to generate various responses to query and select 8 LLMs among them to score responses to form chosen and rejected response pairs. Based on it, we introduce COIG-P 1https://huggingface.co/datasets/liyucheng/zhihu_rlhf_3k 3 (Chinese Open Instruction Generalist - Preference), Chinese human value preference dataset that contains 1,006k samples, and each sample consists of query and pair of chosen and rejected responses. To reduce the overhead of using LLMs for scoring, we also trained Chinese Reward Model (CRM) and manually curated Chinese Reward Benchmark (CRBench). The data of CRBench undergoes cleaning, restructuring, and careful human verification to ensure its quality and diversity, which are essential for improving LLMs capability to align with humans values in the Chinese context and better apply them to real-world scenarios. We conduct experiments using the COIG-P dataset to align current mainstream LLMs with human values in Chinese through DPO, where significant improvement is observed on AlignBench. Besides, using our designed CRM achieves comparable performance with closed-source LLMs in scoring and pairing the DPO preference dataset. Our main contributions are as follows: We present an LLM-based annotation pipeline for Chinese preference datasets and use it to build COIG-P, high-quality, large-scale dataset for human value alignment. Experimental results show that existing mainstream LLMs (including the Qwen2/2.5 and InfinityInstruct-3M-0625 series) achieve significant performance gains ranging from 2% to 12% on this dataset. To demonstrate the effectiveness of our LLM-based Chinese preference dataset annotation pipeline, we compared COIG-P with other Chinese human preference datasets. COIG-P brought significant improvements to the model, far surpassing other datasets. In fact, most existing Chinese datasets even degraded model performance. To mitigate the substantial time and computational costs associated with using LLMs for scoring, we trained Chinese Reward Model (CRM) based on COIG-P and manually annotated Chinese Reward Benchmark (CRBench). Based on CRBench, we investigated the limitations of current mainstream reward models in scoring Chinese responses, while CRM demonstrated strong scoring capabilities in Chinese. Furthermore, we evaluated CRMs real-world annotation performance on test split of COIG-P, showing that its annotation quality is comparable to GPT-4o and significantly more efficient. 2. Related Work High-quality datasets play crucial role in the development of large language models (LLMs) [Raffel et al., 2020, Mishra et al., 2022, Wang et al., 2022b, Zeng et al., 2022, Longpre et al., 2023, Taori et al., 2023, Si et al., 2023, Chenghao Fan and Tian, 2023]. Beyond the construction of instruction-tuning data, increasing attention has been directed toward curating human preference datasets to enhance LLM alignment through reinforcement learning techniques (e.g., DPO, PPO). Recent efforts in preference data construction can be broadly categorized into two paradigms: human annotation and LLM-based annotation. Early English-language datasets primarily relied on manual annotations for preference comparisons. For example, the HH-RLHF dataset [Bai et al., 2022] proposed by Anthropic employs human annotators to assess assistant responses based on helpfulness and harmlessness, leading to significant advances in alignment. Similarly, Ethayarajh et al. [2022] collected user voting preferences from Reddit forums, yielding large-scale corpus of naturally annotated data. However, manual annotation is time-consuming and costly, posing challenges to scalability. As result, recent approaches increasingly leverage LLMs to automate preference data construction [Zhu et al., 2023, Cui et al., 2023, Lambert et al., 2023, PreferenceShareGPT, 2024, huggingface, 2024a, Chiang et al., 2024]. In addition to enhancing general alignment capabilities, 4 Figure 2. The data curation process of COIG-P. The left part is the query collection process, and the right part illustrates the generation of chosen and rejected responses. some studies focus on domain-specific alignment [Cui et al., 2023, Xie et al., 2023, Just et al., 2024, Weyssow et al., 2024]. These approaches typically involve generating multiple candidate responses to prompt using various LLMs, followed by ranking and evaluation via stronger modelsuch as GPT-4to produce high-quality preference annotations. While this strategy significantly improves scalability and efficiency, it also introduces potential biases, as evaluation models may favor responses that resemble their own outputs [Li et al., 2024a, Liu et al., 2024b]. Furthermore, even in LLM-driven pipelines, certain steps still require human involvement. For instance, Cui et al. [2023] does not provide the chosenrejected pairs. In the Chinese context, preference datasets have historically lagged behind in both scale and diversity. Early efforts were limited to small-scale, scenario-specific datasets constructed via human annotation, machine translation, or rule-based heuristics [Xu et al., 2023, huggingface, 2024b, Huozi-Team, 2024, Xinlu Lai, 2024], making them insufficient for training general-purpose dialogue models. Although recent attempts have explored LLM-based annotation in Chinese, the resulting datasets remain limited in quality and coverage [Peng et al., 2023, huggingface, 2024d,c]. Thus, there remains pressing need for high-quality, large-scale Chinese preference datasets. 3. Data Curation Existing LLMs still exhibit gap in aligning with human values, particularly in Chinese-language corpora, which limits their effectiveness in real-world applications. Therefore, as shown in Figure 2, we propose LLM-based Chinese preference dataset annotation pipeline to curate the COIG-P, human value preference dataset of Chinese corpus, without human intervention. 5 3.1. Query Collection Instruction tuning [Sanh et al., 2021, Wang et al., 2022a, Longpre et al., 2023, Ni et al., 2023] and RLHF [Schulman et al., 2017, Ziegler et al., 2019, Rafailov et al., 2023] play crucial role in LLMs success. Despite Chinese being one of the most widely spoken languages in the world, most of the Chinese instruction datasets [Yang, 2023, Bai et al., 2024] come from traditional NLP tasks, and the query format has significant gap compared to the way humans ask questions in daily life. Open-source, high-quality Chinese query data remains extremely scarce. As result, there is an urgent need to address the challenge of collecting large-scale, high-quality queries to improve LLMs alignment with human values in Chinese. As shown in the left part of Figure 2, we collect 92k high-quality Chinese queries. Enhancing the performance of LLMs in single domain may come at the cost of their reduced alignment with human values in other domains. Inspired by Liu et al. [2024a]s subtask designing, we collect queries from different domains including Chating (Chat. ÂØπËØù), Logic Reasoning (Logic. ÈÄªËæëÊé®ÁêÜ), Mathematics (Math. Êï∞Â≠¶), Novel Continuation (Novel. Â∞èËØ¥Áª≠ÂÜô), Role-Playing (Role. ËßíËâ≤ÊâÆÊºî), and Coding (Code. ‰ª£Á†Å). To support the growth of the Chinese open-source community, we have collected Chinese query data from Chinese Q&A platforms, including baiduzhidao2, zhihu3, and baidutieba4. We also collect the queries from Chinese Administrative Aptitude Test. Besides, we translate some queries from the English open-source dataset into Chinese, such as HotpotQA [Yang et al., 2018] and Haruhi-Zero-RolePlaying-movie-PIPPA5. The details of our used Open-source dataset refer to Appendix To maintain the quality of the collected queries, we conduct the deduplication and filtering: Deduplication: We utilize SentenceBERT to obtain query embeddings and compute the semantic similarity between different queries. Queries with high semantic similarity to others are removed to ensure diversity. Filtering: First, we employ Qwen2-72B [Yang et al., 2024a] to score the queries and discard those with scores below 5 based on whether it reflects question that typical user might ask. Then, we design some rules to remove queries that are not well-formed and whether. After these processes, we obtain 92,784 high-quality queries from the Chinese corpus. 3.2. Response Generation Relying solely on single LLM for response generation can lead to monotonous outputs. Therefore, we leverage diverse LLMs with distinct characteristics to enhance response diversity. Inspired by Cui et al. [2023], we utilize 15 different open-source and proprietary LLMs (i.e., Abab6.56, Baichuan47, Claude3.58, DeepSeek-V2 [DeepSeek-AI et al., 2024], Doubao-Pro9, Gemini1.5-Pro10, GPT-Turbo/3.5/4/4o11, Qwen-Max, Qwen2-72B [Yang et al., 2024a], Yi-1.5-34B, 2https://zhidao.baidu.com/ 3https://www.zhihu.com/ 4https://tieba.baidu.com/index.html 5https://huggingface.co/datasets/silk-road/Haruhi-Zero-RolePlaying-movie-PIPPA 6https://www.minimax.io/news/abab65-series 7https://platform.baichuan-ai.com/ 8https://www.anthropic.com/news/claude-3-5-sonnet 9https://team.doubao.com/zh/special/doubao_1_5_pro 10https://aistudio.google.com/app/prompts/new_chat 11https://chatgpt.com/ 6 Yi-Large [AI et al., 2025], GLM-4 [GLM et al., 2024a], and Moonshot12) to generate range of responses for each query. 3.3. Scoring and Paring For each query, we selected 8 LLMs (i.e., Claude3.5, DeepSeekV2, Doubao-Pro, GLM-4, GPT4o, GPT-4-Turbo, Qwen2-72B-Instruct, and Moonshot) to score the responses. Besides, we designed tailored prompts for different data domains, as detailed in Appendix A. To align LLMs with human values using DPO, we need to construct some pairs of chosen and rejected responses for each query. Specifically, we randomly sample two responses per query and retain only those pairs where the score difference between chosen and rejected responses exceeds predefined threshold. We explore various threshold values and ultimately select threshold of 2 in our study. For detailed analysis, please refer to subsection 5.3. After applying these filtering and pairing steps, we curated final dataset consisting of 1,006,949 samples, each containing query along with chosen and rejected response. 3.4. Human Evaluation To ensure the quality of our dataset, we randomly select 40 samples for each domain in our dataset and totally collect 240 samples to evaluate. We hire 2 postgraduate students who are familiar with the field of Natural Language Processing (NLP) to manually evaluate the quality of those samples. Specifically, we require the annotator to judge samples based on the following criteria: 1) whether the chosen response is better aligned with human preferences than the rejected response. 2) whether the chosen response is correct. Based on human evaluation, the dataset achieves an average accuracy of 90.83%, with domain-specific scores as follows: Logic 90%, Novel 90%, Role 90%, Code 95%, Math 85%, and Chat 95%. The consistently high accuracyexceeding 90% in most domainsdemonstrates the robustness and quality of the dataset generated and evaluated by LLMs. 3.5. Statics All Logic. Chat. Math. Novel. Role. Code. Sample Num 1,006,946 92,784 Query Num 54,617 8,816 702,398 37,323 155,872 27,259 34,483 6,682 19,363 4, 40,213 7,774 Table 2. The statistics of our COIG-P dataset. The sample numbers represent the number of samples in our dataset, and each sample consists of query and chosen and rejected response pair. The query number represents the quantity of our filtered high-quality queries. As shown in Table 2, we collected total of 92,784 high-quality Chinese corpus queries. The Chat and Math domains constitute the largest portions, each containing approximately 30,000 queries, while other domains have around 6,000 queries each. This distribution suggests that in everyday applications, users are more likely to engage with Math-related topics and casual conversations. For most domains, we generate around six response pairs per query. However, for the Chat 12https://moonshotteam.com/ domain, we curate approximately 20 response pairs per query, reflecting the relative simplicity of Chat-related queries. 4. Experiments Setup In this study, we utilize AlignBench [Liu et al., 2024a] as our primary benchmark to assess the alignment capabilities of LLMs. AlignBench is comprehensive, multi-dimensional benchmark specifically designed for evaluating LLM alignment in Chinese. Due to computational resource constraints, we employ GPT-4o-08-06 as the judge model and rerun the current mainstream LLMs on it for comprehensive comparison. Baselines. Following the AlignBench evaluation framework, we assess several widely recognized LLMs. As for close-source LLMs, we choose GPT-4o13 and Claude3.514. As for the open-source LLMs, we selected the latest high-performing LLMs, such as ChatGLM [GLM et al., 2024b], InternLM[Team, 2023] series, ,Llama3 [Dubey et al., 2024] and DeepSeek-R1-Distill series [DeepSeek-AI, 2025]. Backbones. To demonstrate the effectiveness of our COIG-P dataset, we evaluate its impact on SOTA LLMs within the 79B parameter range. Among these, Qwen2.5/2-7B-Instruct [Yang et al., 2024a,b] stands out as the most capable open-source LLM across various NLP tasks. Furthermore, we also choose the Infinity-Instruct-3M-0625 [of Artificial Intelligence , BAAI] series LLMs that have been specifically optimized for the Chinese corpus as our backbone model (i.e., Infinity-Instruct-3M-0625-Qwen2-7B, Infinity-Instruct-3M-0625-Llama3-8B, and Infinity-Instruct-3M-0625-Mistral-7B). 4.1. Implementation Details To validate the efficiency of the COIG-P dataset, we train the selected backbone models using the DPO method. Hyperparameters. Our experiments indicate that ùëèùëíùë°ùëé value of 0.1 yields the best performance across all LLMs. However, the optimal learning rate (lr) varies depending on the models capabilities. Specifically, we set ùëôùëü = 1ùëí 6 for Qwen2/2.5, while for other LLMs, we use 1ùëí 7. Computational Cost. Each backbone model is fully fine-tuned for one epoch on A800 GPUs, resulting in total of approximately 400 GPU hours per model. The cumulative computational cost for training all backbone models amounts to 2,000 GPU hours. 5. Results 5.1. Overall Analysis As shown in Table 3, to valid the efficiency of our COIG-P dataset, we conduct various experiments by training LLMs (i.e., Qwen2/2.5-7B and Infinity-Instruct-3M-0625 series models) using 13https://chatgpt.com/ 14https://claude.ai/ 8 Overall Reasoning ‰∏≠ÊñáÊé®ÁêÜ Language ‰∏≠ÊñáËØ≠Ë®Ä Model Ê®°Âûã GPT-4o Claude3.5-Sonnet Qwen2.5-72B-Inst Llama3.3-72B-Inst DS-R1-Dist-Qwen-32B DS-R1-Dist-Qwen-7B InternLM3-8B-Inst InternLM2.5-20B-Chat ChatGLM3-6B Qwen2.5-7B-Inst Qwen2-7B-Inst II-3M-0625-Qwen2-7B II-3M-0625-Llama3-8B II-3M-0625-Mistral-7B ÊÄªÂàÜ 6.93 6.58 6.80 5.52 6.13 4.74 6.00 5.75 3.46 5.90 5.35 4.96 3.83 3. Qwen2.5-7B-Inst Qwen2-7B-Inst II-3M-0625-Qwen2-7B II-3M-0625-Llama3-8B II-3M-0625-Mistral-7B 6.02 ( 2.03%) 5.47 ( 2.24%) 5.37 ( 8.26%) 4.30 (12.27%) 3.98 ( 6.70%) Avg. Math. Logi. Avg. Pro. Êé®ÁêÜ Êï∞Â≠¶ ÈÄªËæë ËØ≠Ë®Ä Âü∫Êú¨ ‰∏≠Êñá ÁªºÂêà ÊñáÊú¨ ËßíËâ≤ ‰∏ì‰∏ö ÊÄªÂàÜ ËÆ°ÁÆó Êé®ÁêÜ ÊÄªÂàÜ ‰ªªÂä° ÁêÜËß£ ÈóÆÁ≠î ÂÜô‰Ωú ÊâÆÊºî ËÉΩÂäõ Fund. Chi. Open. Writ. Role. Baseline 7.63 6.97 7.21 5.91 6.40 5.96 5.84 5.81 3. 6.49 6.00 6.71 5.20 6.05 4.90 5.14 4.84 3.25 Backbone 6.38 5.57 4.65 3.40 3.29 6.58 5.59 5.30 3.93 3.56 5.15 4.18 4.27 3.00 3.20 COIG-P 5.36 4.38 4.35 3.58 3.48 6.80 6.68 6.65 5.48 6.03 4.05 6.52 6.18 3.80 6.03 5.83 5.46 4.45 4.22 6.08 5.96 5.92 4.85 4.43 7.06 6.49 6.96 5.55 6.23 5.43 5.49 5.32 3.13 5.77 4.88 4.46 3.20 3. 5.97 4.98 4.83 3.75 3.52 6.81 6.93 6.63 5.49 6.04 4.28 6.04 6.09 3.81 5.99 5.22 5.03 4.21 3.94 5.87 5.07 5.47 4.71 4.69 6.81 6.64 6.50 4.76 5.93 3.57 6.50 5.90 2.86 5.86 5.64 4.98 3.57 3. 5.74 5.86 5.41 3.83 3.59 6.74 6.63 6.58 5.50 6.37 4.50 6.89 6.82 4.63 6.34 6.45 6.03 4.87 4.55 6.34 6.79 6.89 5.45 4.89 6.63 6.35 6.51 5.37 5.96 4.25 6.63 6.01 3.75 5.93 6.23 5.65 4.99 4. 6.24 6.12 6.07 5.29 4.77 6.47 6.41 6.67 5.93 6.14 4.30 6.91 6.55 4.20 6.08 6.06 5.84 5.12 4.96 6.41 6.35 6.16 5.60 4.97 7.35 7.12 7.00 5.81 5.77 3.40 6.12 5.71 3.54 6.01 5.40 5.20 3.95 3. 5.87 5.56 5.49 4.20 3.69 Table 3. Results on AlignBench and the score range for each metric in it is 0-10. The presents presents the improvement in the sub-task, overall improvement in the format of percentage, presents decrease in the sub-task. We re-evaluated current SOTA LLMs on this and benchmark using GPT-4o-0806. II-3M-0625 refers to Infinity-Instruct-3M-0625, while the COIG-P setting denotes LLMs trained on our dataset using DPO. DPO on our dataset. We also update the current mainstream LLMs on AlignBench. Based on COIG-P, mainstream LLMs have achieved significant improvements in overall performance. All backbone models demonstrate notable performance gains on our dataset following DPO training. In particular, II-3M-0625-Qwen2-7B and II-3M-0625-Llama3-8B achieved an increase of more than 0.41 in their overall scores. Within the II-3M-0625 series, the relative improvements range from 6% to 12%, indicating consistent and substantial enhancements. Even for Qwen2.5-7B-Inst, one of the strongest open-source LLMs, our dataset contributed to performance gain of 0.12. Additionally, for both Qwen2/2.5-7B, relative improvements also exceeded 2%, underscoring the effectiveness of our dataset in enhancing LLM capabilities. COIG-P consistently improves performance across all sub-tasks for most backbone models. For relatively weaker models, COIG-P can help them achieve comprehensive improvements across all subtasks (e.g., II-3M-0625-Qwen2-7B and I-3M-0625-Llama3-8B). For models that are relatively powerful (i.e., Qwen2.5-7B-Inst), DPO training can enhance their reasoning (‰∏≠ÊñáÊé®ÁêÜ) abilities. However, it may cause slight degradation in some fundamental language (‰∏≠ÊñáËØ≠Ë®Ä) subtasks, especially in Fundamental (Âü∫Á°Ä‰ªªÂä°). 9 The gap between open-source and closed-source models is small in Chinese preference alignment tasks. Compared to GPT-4o, Qwen2.5-72B-Inst shows only slight differences in scores across various tasks, and its overall score is even significantly higher than that of Claude3.5-Sonnet. By using our COIG-P dataset, the performance of the Qwen2.5-7B model can be improved to level close to that of DS-R1-Dist-Qwen-32B, making it overall score exceed 6.0. This demonstrates that many smaller open-source models, such as ChatGLM3-6B and DS-R1Dist-Qwen-7B, still have significant room for improvement in Chinese preference alignment. 5.2. Ablation Study Dataset Overall Reasoning ‰∏≠ÊñáÊé®ÁêÜ Language ‰∏≠ÊñáËØ≠Ë®Ä ËÆ≠ÁªÉÊï∞ÊçÆ ÊÄªÂàÜ Backbone COIG-P Chat Novel Role Logic Math Code 4.96 5.47 4.97 5.29 4.87 4.87 4.76 4.72 Avg. Math. Logi. Avg. Pro. Êé®ÁêÜ Êï∞Â≠¶ ÈÄªËæë ËØ≠Ë®Ä Âü∫Êú¨ ‰∏≠Êñá ÁªºÂêà ÊñáÊú¨ ËßíËâ≤ ‰∏ì‰∏ö ÊÄªÂàÜ ËÆ°ÁÆó Êé®ÁêÜ ÊÄªÂàÜ ‰ªªÂä° ÁêÜËß£ ÈóÆÁ≠î ÂÜô‰Ωú ÊâÆÊºî ËÉΩÂäõ Fund. Chi. Open. Writ. Role. 4.46 4.98 4.44 4.98 4.37 4.36 4.37 4. 4.65 5.59 4.86 5.74 4.73 4.85 4.78 4.69 4.27 4.38 4.02 4.23 4.00 3.87 3.96 3.78 5.46 5.96 5.50 5.60 5.38 5.37 5.14 5.20 5.03 5.07 5.19 5.69 5.06 5.07 4.79 4.65 4.98 5.86 5.31 5.09 4.97 5.02 5.09 4.95 6.03 6.79 5.87 6.00 5.66 6.05 5.53 5. 5.65 6.12 5.75 5.79 5.65 5.55 5.29 5.24 5.84 6.35 5.66 5.82 5.74 5.55 5.21 5.53 5.20 5.56 5.23 5.22 5.20 5.01 4.96 5.21 Table 4. Ablation study results. We trained Infinity-Instruct-3M-0625-Qwen2-7 on those datasets and evaluated them on AlignBench. The Backbone means the result of the raw Infinity-Instruct-3M-0625-Qwen2-7B. Excepting the data from the users daily interaction on the Internet (Chat), we also collect data from some specific domains including (Novel, Role, Logic, Math and Code). To this end, we conducted ablation studies to demonstrate that mixing data from different domains can better enhance the human value alignment capabilities of LLMs. The results are presented in Table 4. Overall, training the model on individual domain datasets performs worse than training it on combination of multiple domains. In fact, using data from certain domains alone can even harm the models overall performance. This demonstrates the effectiveness of our strategy of training the model with mixture of data from different domains. It is worth noting that training the model solely on the novel continuation task (Novel) leads to significant performance improvement. The models trained on the Novel dataset saw substantial boost in fundamental language ability (Fund.), reaching 5.69 an increase of 0.71. This enhancement in fundamental language skills directly contributed to the improvement in the models reasoning ability. 5.3. Selecting Score Threshold of Pairing For each query, we prompt the LLMs to generate multiple chosenrejected response pairs, and then filter out low-quality pairs based on scores assigned by the LLMs themselves. Specifically, we define threshold and discard any pair where the score difference between the chosen and rejected responses falls below this threshold. 10 To select suitable threshold, we randomly selected 1,000 queries in COIG-P. For each query, we formed potential chosenrejected pairs across all available responses and then applied varying thresholds to decide which pairs to keep based on the score judged by LLMs. As shown in Figure 3, we train InfinityInstruct-3M-0625-Qwen2-7B on datasets filtered with different thresholds and evaluate them on AlignBench. As the threshold increases up to 2.0, the models performance generally shows an upward trend; however, once the threshold surpasses 2.0, the performance gradually declines. Therefore, in this paper, we select 2.0 as the threshold to filter the data. 5.4. Comparing Chinese Human Preference Dataset Figure 3. Selection of the pairing score threshold. threshold of 0 indicates that the score of the chosen response is higher than that of the rejected response. Compared to other datasets, COIG-P shows the greatest improvement and demonstrates notable performance gains across all sub tasks. As illustrated in Table 5, our analysis indicates that only the COIG-P and ZAKE datasets positively contribute to Chinese language alignment capabilities, while the remaining datasets lead to significant performance declines. COIG-P achieves the highest performance across all metrics (overall: 5.47, reasoning: 4.98, language: 5.96), whereas ZAKE provides moderate improvements (overall: 5.11, reasoning: 4.63, language: 5.60). Nevertheless, the enhancement provided by ZAKE in Chinese language tasks is modest, surpassing the baseline by merely 0.20.3 points. Furthermore, its effect on reasoning is inconsistent, which enhances mathematical skills but negatively impacts logical reasoning, scoring approximately 0.4 points lower than COIG-P. In contrast, COIG-P brings 0.5 improvement to the model on most tasks. Other datasets, such as Zhihu-RLHF-7B, RINI-3K, Huozi, and particularly CVALUES (which achieves only 3.54 overall), lead to substantial performance declines. Moreover, these datasets lead to performance degradation for models across various subtasks. These findings indicate that, aside from few exceptions, the current Chinese preference datasets lack sufficient quality and quantity to adequately support the development needs of advanced LLMs. 6. Chinese Reward Model and Chinese Reward Benchmark The LLMs scoring ability is still under-explored, and using the closed-source LLM (i.e., GPT-4o, Cluade) and open-source LLMs with massive number of parameters (i.e., Qwen2.5-72B) posed significant obstacles to the development of Chinese datasets. Developing small-parameter LLMs is an urgent task. Therefore, we propose Chinese Reward Model (in subsection 6.1) and Chinese Reward Benchmark (in subsection 6.2) to fill the gap in this field. Dataset Overall Reasoning ‰∏≠ÊñáÊé®ÁêÜ Language ‰∏≠ÊñáËØ≠Ë®Ä ËÆ≠ÁªÉÊï∞ÊçÆ ÊÄªÂàÜ - Zhihu-Rlhf-3k CVALUES Huozi ZAKE RLHF-CN COIG-P (Ours) 4.96 4.75 3.54 4.75 5.11 3.79 5.47 Avg. Math. Logi. Avg. Pro. Êé®ÁêÜ Êï∞Â≠¶ ÈÄªËæë ËØ≠Ë®Ä Âü∫Êú¨ ‰∏≠Êñá ÁªºÂêà ÊñáÊú¨ ËßíËâ≤ ‰∏ì‰∏ö ÊÄªÂàÜ ËÆ°ÁÆó Êé®ÁêÜ ÊÄªÂàÜ ‰ªªÂä° ÁêÜËß£ ÈóÆÁ≠î ÂÜô‰Ωú ÊâÆÊºî ËÉΩÂäõ Fund. Chi. Open. Writ. Role. 4.46 4.16 3.22 4.32 4.63 3.41 4.98 4.65 4.51 3.14 4.60 5.29 3.49 5.59 4.27 3.82 3.29 4.04 3.98 3.34 4. 5.46 5.33 3.86 5.17 5.60 4.17 5.96 5.03 4.72 3.71 4.93 5.01 4.38 5.07 4.98 5.21 3.41 4.86 5.26 4.47 5.86 6.03 5.66 3.84 5.32 6.26 3.75 6.79 5.65 5.68 4.20 5.47 5.81 4.30 6.12 5.84 5.47 4.17 5.41 6.00 4.13 6. 5.20 5.27 3.82 5.06 5.23 4.00 5.56 Table 5. Performance comparison of LLMs trained on different Chinese human preference datasets. The backbone model used is Infinity-Instruct-3M-0625-Qwen2-7B. The - symbol represents the performance of the backbone model without additional training. 6.1. Chinese Reward Model Inspired by Ouyang et al. [2022], we use the Bradley-Terry (BT) Reward Modeling method. Specifically, we choose the Llama3.1-8B-Instruct as our Foundation model, and the objective function of the Bradley-Terry (BT) loss is as follows: P(ùëé1 ùëé2ùë•, ùëé1, ùëé2) = exp(ùëü(ùë•, ùëé1)) exp(ùëü(ùë•, ùëé1)) + exp(ùëü(ùë•, ùëé2)) = ùúé(ùëü(ùë•, ùëé1) ùëü(ùë•, ùëé2)), where ùë• present the query, ùëé1 presents the chosen response, and the ùëé2 presents the rejected response. Considering testing our CRM, we only use half of our COIG-P dataset to train our CRM and test our CRM in the rest. 6.2. Chinese Reward Benchmark In order to better evaluate the Chinese scoring capability of current LLMs, we have standardized the Chinese Reward Benchmark (CRBench). To ensure high-quality data annotation, we recruited three postgraduate students, each responsible for two specific domains. From the dataset, we randomly selected 5,000 samples and asked the annotators to assess whether each sample should be included based on the following criteria: 1) The query must be well-formed question and should not involve sensitive topics such as sex, politics, etc. 2) The chosen response of the selected sample must be correct. 3) The chosen response of the sample should better align with human preferences compared to the rejected response. The annotator will pause the annotation until the total number of samples in the benchmark exceeds 1,000. As shown in Table 6, we finally annotate 1,040 samples. All. Chat. Logic. Math. Code. Role. Novel. 1,040 129 274 101 80 81 Table 6. The static of our Chinese Reward Benchmark (CRBench). As shown in Table 7, we evaluate the current mainstream LLMs and reward models in the CRBench. Our CRM achieves the best performance among the discriminative reward 12 Model Ê®°Âûã Claude GPT-4o Conv. ÂØπËØù ÈÄªËæëÊé®ÁêÜ Êï∞Â≠¶ ‰ª£Á†Å ËßíËâ≤ÊâÆÊºî Â∞èËØ¥Áª≠ÂÜô ÊÄªÂàÜ Logic. Math. Code. Novel. Overall Role. Generative 86.82 96.12 74.67 88.27 61.68 72.63 92.08 98. 75.00 93.75 Discriminative Skywork-Reward-Gemma-2-27B Llama-3-OffsetBias-RM-8B RM-Mistral-7B ArmoRM-Llama3-8B Skywork-Reward-Llama-3.1-8B CRM (Ours) 62.02 34.11 86.82 58.91 75.97 79.07 53.60 54.93 61.33 44.27 52.00 69.60 54.01 68.98 61.68 41.97 49.27 66. 59.41 72.28 90.10 46.53 78.22 92.08 50.00 47.50 53.75 41.25 35.00 43.75 Table 7. Model performance comparison. 70.37 91.36 61.73 34.57 49.38 27.16 34.57 62.96 74.13 86. 55.67 55.58 65.87 44.13 54.13 69.71 models. Although the closed-source Generative model (GPT-4o and Claude3.5) achieves the best performance, the performance gap between CRM and them is also relatively small (i.e., the overall performance gap between Claude and CRM is less than 4%). Besides, the Logic(ÈÄªËæëÊé®ÁêÜ), Math(Êï∞Â≠¶), Role(ËßíËâ≤ÊâÆÊºî), and Novel(Â∞èËØ¥Áª≠ÂÜô) tasks remain challenging for most models. Except for GPT-4o, all models score below 75% on these tasks, with most clustering around 60%. This further highlights the necessity of our benchmark. 6.3. Downstream Task Validation Besides demonstrating our Chinese Reward Models ability on the Chinese Reward Benchmark, we also apply it to pairing responses and compare the result of our CRM with GPT-4o. We use our CRM and GPT-4o to filter data in the test split described in the subsection 5.3. We filter data when the score of the chosen response if lower than rejected response. It is worth mentioning that our CRM achieved close performance with GPT-4o in paring chosen-rejected pairs. As shown in Figure 4, in the test split, the model trained on the data selected by our CRM achieves an Overall score of 5.26, which is close to that of GPT-4o (5.28). In all sub-tasks, the CRMs results are also competitive with the GPT-4o. Our experiments demonstrate that our CRM has the practical ability to choose high-quality chosen-rejected response pairs. Our CRM is more effective than LLMs. Comparing the LLMs with large-scale parameters, using our CRM to score responses on 430k samples only cost 40 A800 GPU hours. It demonstrates that our model has notable speed advantage in data filtering, significantly reducing the cost of developing Chinese datasets. 7. Conclusion DPO and PPO have played significant role in aligning large language models (LLMs) with human value preferences. However, the lack of large-scale, high-quality Chinese preference data has limited the alignment of LLMs with human preferences in the Chinese context. To address this, we propose an LLM-based pipeline for constructing Chinese preference data and use it to create COIG-P, dataset containing 1,006k high-quality Chinese preference samples. We demonstrate on AlignBench that COIG-P brings 2%12% performance improvement to mainstream 13 Figure 4. The results of different reward models in scoring chosen-rejected pairs. We trained Infinity-Instruct-3M-0625-Qwen2-7B using dataset filtered by different reward models and evaluated them on AlignBench. LLMs, including the Qwen2/2.5 and Infinity-Instruct-3M-0625 series. Compared to existing Chinese preference datasets, COIG-P yields significantly better performance improvements. Furthermore, due to the scarcity of Chinese preference data, there is currently no strong Chinese reward model that can replace LLMs for scoring and reduce computational costs. To address this, we propose Chinese reward model along with corresponding Chinese reward benchmark. We also validate that our Chinese reward model achieves performance comparable to GPT-4o on downstream tasks involving real data annotation. 14 8. Contributions and Acknowledgments Multimodal Art Projection (M-A-P) is non-profit open-source AI research community, run by donations. The community members are working on research topics in wide range of spectrum, including but not limited to the pre-training paradigm of foundation models, largescale data collection and processing, and the derived applications on coding, reasoning, and music generation. Leading Authors Siwei Wu, UoM, M-A-P Jincheng Ren, M-A-P Xinrun Du, M-A-P Contributors Yiming Liang, M-A-P Jie Liu, M-A-P Yunwen Li, CUHK-Shenzhen Tianyu Zheng, M-A-P Boyu Feng, M-A-P Huaqing Yuan, M-A-P Zenith Wang, M-A-P Jiaheng Liu, M-A-P Wenhao Huang, M-A-P Chenglin Cai Haoran Que, M-A-P Corresponding Authors Minghao Liu, 2077AI Guoyin Wang Shuyue Guo, M-A-P Xingwei Qu, M-A-P Jian Yang Yuelin Bai, M-A-P Zekun Moore Wang, M-A-P Zhouliang Yu, M-A-P Qunshu Lin, Abaka.AI Ding Pan, M-A-P Yuchen Jiang, OPPO Tiannan Wang, OPPO Wangchunshu Zhou, OPPO Shenzhi Wang Xingyuan Bu Ge Zhang, M-A-P Chenghua Lin, UoM"
        },
        {
            "title": "References",
            "content": ". AI, :, A. Young, B. Chen, C. Li, C. Huang, G. Zhang, G. Zhang, G. Wang, H. Li, J. Zhu, J. Chen, J. Chang, K. Yu, P. Liu, Q. Liu, S. Yue, S. Yang, S. Yang, W. Xie, W. Huang, X. Hu, X. Ren, X. Niu, P. Nie, Y. Li, Y. Xu, Y. Liu, Y. Wang, Y. Cai, Z. Gu, Z. Liu, and Z. Dai. Yi: Open foundation models by 01.ai, 2025. URL https://arxiv.org/abs/2403.04652. Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, N. Joseph, S. Kadavath, J. Kernion, T. Conerly, S. El-Showk, N. Elhage, Z. HatfieldDodds, D. Hernandez, T. Hume, S. Johnston, S. Kravec, L. Lovitt, N. Nanda, C. Olsson, D. Amodei, T. Brown, J. Clark, S. McCandlish, C. Olah, B. Mann, and J. Kaplan. Training helpful and harmless assistant with reinforcement learning from human feedback, 2022. URL https://arxiv.org/abs/2204.05862. Y. Bai, X. Du, Y. Liang, Y. Jin, J. Zhou, Z. Liu, F. Fang, M. Chang, T. Zheng, X. Zhang, N. Ma, Z. Wang, R. Yuan, H. Wu, H. Lin, W. Huang, J. Zhang, C. Lin, J. Fu, M. Yang, S. Ni, and G. Zhang. Coig-cqia: Quality is all you need for chinese instruction fine-tuning, 2024. URL https://arxiv.org/abs/2403.18058. Y. Bang, S. Cahyawijaya, N. Lee, W. Dai, D. Su, B. Wilie, H. Lovenia, Z. Ji, T. Yu, W. Chung, et al. multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023, 2023. Z. L. Chenghao Fan and J. Tian. Chinese-vicuna: chinese instruction-following llama-based model. 2023. URL https://github.com/Facico/Chinese-Vicuna. W.-L. Chiang, L. Zheng, Y. Sheng, A. N. Angelopoulos, T. Li, D. Li, H. Zhang, B. Zhu, M. Jordan, J. E. Gonzalez, and I. Stoica. Chatbot arena: An open platform for evaluating llms by human preference, 2024. G. Cui, L. Yuan, N. Ding, G. Yao, W. Zhu, Y. Ni, G. Xie, Z. Liu, and M. Sun. Ultrafeedback: Boosting language models with high-quality feedback, 2023. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. DeepSeek-AI, A. Liu, B. Feng, B. Wang, B. Wang, B. Liu, C. Zhao, C. Dengr, C. Ruan, D. Dai, D. Guo, D. Yang, D. Chen, D. Ji, E. Li, F. Lin, F. Luo, G. Hao, G. Chen, G. Li, H. Zhang, H. Xu, H. Yang, H. Zhang, H. Ding, H. Xin, H. Gao, H. Li, H. Qu, J. L. Cai, J. Liang, J. Guo, J. Ni, J. Li, J. Chen, J. Yuan, J. Qiu, J. Song, K. Dong, K. Gao, K. Guan, L. Wang, L. Zhang, L. Xu, L. Xia, L. Zhao, L. Zhang, M. Li, M. Wang, M. Zhang, M. Zhang, M. Tang, M. Li, N. Tian, P. Huang, P. Wang, P. Zhang, Q. Zhu, Q. Chen, Q. Du, R. J. Chen, R. L. Jin, R. Ge, R. Pan, R. Xu, R. Chen, S. S. Li, S. Lu, S. Zhou, S. Chen, S. Wu, S. Ye, S. Ma, S. Wang, S. Zhou, S. Yu, S. Zhou, S. Zheng, T. Wang, T. Pei, T. Yuan, T. Sun, W. L. Xiao, W. Zeng, W. An, W. Liu, W. Liang, W. Gao, W. Zhang, X. Q. Li, X. Jin, X. Wang, X. Bi, X. Liu, X. Wang, X. Shen, X. Chen, X. Chen, X. Nie, X. Sun, X. Wang, X. Liu, X. Xie, X. Yu, X. Song, X. Zhou, X. Yang, X. Lu, X. Su, Y. Wu, Y. K. Li, Y. X. Wei, Y. X. Zhu, Y. Xu, Y. Huang, Y. Li, Y. Zhao, Y. Sun, Y. Li, Y. Wang, Y. Zheng, Y. Zhang, Y. Xiong, Y. Zhao, Y. He, Y. Tang, Y. Piao, Y. Dong, Y. Tan, Y. Liu, Y. Wang, Y. Guo, Y. Zhu, Y. Wang, Y. Zou, Y. Zha, Y. Ma, Y. Yan, Y. You, Y. Liu, Z. Z. Ren, Z. Ren, Z. Sha, Z. Fu, Z. Huang, Z. Zhang, Z. Xie, Z. Hao, Z. Shao, Z. Wen, Z. Xu, Z. Zhang, Z. Li, Z. Wang, Z. Gu, Z. Li, and Z. Xie. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model, 2024. URL https://arxiv.org/abs/2405.04434. 16 A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. K. Ethayarajh, Y. Choi, and S. Swayamdipta. Understanding dataset difficulty with mathcal vusable information. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 59886008. PMLR, 1723 Jul 2022. M. Firat. How chat gpt can transform autodidactic experiences and open education? 2023. D. Ganguli, L. Lovitt, J. Kernion, A. Askell, Y. Bai, S. Kadavath, B. Mann, E. Perez, N. Schiefer, K. Ndousse, A. Jones, S. Bowman, A. Chen, T. Conerly, N. DasSarma, D. Drain, N. Elhage, S. El-Showk, S. Fort, Z. Hatfield-Dodds, T. Henighan, D. Hernandez, T. Hume, J. Jacobson, S. Johnston, S. Kravec, C. Olsson, S. Ringer, E. Tran-Johnson, D. Amodei, T. Brown, N. Joseph, S. McCandlish, C. Olah, J. Kaplan, and J. Clark. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned, 2022. URL https://arxiv.org/ abs/2209.07858. T. GLM, :, A. Zeng, B. Xu, B. Wang, C. Zhang, D. Yin, D. Zhang, D. Rojas, G. Feng, H. Zhao, H. Lai, H. Yu, H. Wang, J. Sun, J. Zhang, J. Cheng, J. Gui, J. Tang, J. Zhang, J. Sun, J. Li, L. Zhao, L. Wu, L. Zhong, M. Liu, M. Huang, P. Zhang, Q. Zheng, R. Lu, S. Duan, S. Zhang, S. Cao, S. Yang, W. L. Tam, W. Zhao, X. Liu, X. Xia, X. Zhang, X. Gu, X. Lv, X. Liu, X. Liu, X. Yang, X. Song, X. Zhang, Y. An, Y. Xu, Y. Niu, Y. Yang, Y. Li, Y. Bai, Y. Dong, Z. Qi, Z. Wang, Z. Yang, Z. Du, Z. Hou, and Z. Wang. Chatglm: family of large language models from glm-130b to glm-4 all tools, 2024a. URL https://arxiv.org/abs/2406.12793. T. GLM, A. Zeng, B. Xu, B. Wang, C. Zhang, D. Yin, D. Zhang, D. Rojas, G. Feng, H. Zhao, et al. Chatglm: family of large language models from glm-130b to glm-4 all tools. arXiv preprint arXiv:2406.12793, 2024b. huggingface. Anthropic hh golden. https://huggingface.co/datasets/Unified-Lan guage-Model-Alignment/Anthropic_HH_Golden, 2024a. huggingface. Zake. https://huggingface.co/datasets/zake7749/kyara-chinese-p reference-rl-dpo-s0-30K, 2024b. huggingface. Hh rlhf cn. https://huggingface.co/datasets/dikw/hh_rlhf_cn, 2024c. huggingface. Zhihu rlhf 3k. https://huggingface.co/datasets/liyucheng/zhihu_rl hf_3k, 2024d. Huozi-Team. Huozi: Leveraging large language models for enhanced open-domain chatting. https://github.com/HIT-SCIR/huozi, 2024. H. A. Just, M. Jin, A. Sahu, H. Phan, and R. Jia. Data-centric human preference optimization with rationales. arXiv preprint arXiv:2407.14477, 2024. D. Kalla, N. Smith, F. Samaah, and S. Kuraku. Study and analysis of chat gpt and its impact on different fields of study. International journal of innovative science and research technology, 8(3), 2023. N. Lambert, L. Tunstall, N. Rajani, and T. Thrush. Huggingface h4 stack exchange preference dataset, 2023. URL https://huggingface.co/datasets/HuggingFaceH4/stack-exc hange-preferences. 17 T. Li, W.-L. Chiang, E. Frick, L. Dunlap, T. Wu, B. Zhu, J. E. Gonzalez, and I. Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline, 2024a. URL https://arxiv.org/abs/2406.11939. Z. Li, Q. Zang, D. Ma, J. Guo, T. Zheng, M. Liu, X. Niu, Y. Wang, J. Yang, J. Liu, et al. Autokaggle: multi-agent framework for autonomous data science competitions. arXiv preprint arXiv:2410.20424, 2024b. X. Liu, X. Lei, S. Wang, Y. Huang, Z. Feng, B. Wen, J. Cheng, P. Ke, Y. Xu, W. L. Tam, X. Zhang, L. Sun, X. Gu, H. Wang, J. Zhang, M. Huang, Y. Dong, and J. Tang. Alignbench: Benchmarking chinese alignment of large language models, 2024a. URL https://arxiv.org/abs/2311 .18743. Y. Liu, N. Moosavi, and C. Lin. LLMs as narcissistic evaluators: When ego inflates evaluation In L.-W. Ku, A. Martins, and V. Srikumar, editors, Findings of the Association for scores. Computational Linguistics: ACL 2024, pages 1268812701, 2024b. doi: 10.18653/v1/2024.findi ngs-acl.753. URL https://aclanthology.org/2024.findings-acl.753/. S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y. Tay, D. Zhou, Q. V. Le, B. Zoph, J. Wei, et al. The flan collection: Designing data and methods for effective instruction tuning. In International Conference on Machine Learning, pages 2263122648. PMLR, 2023. S. Mishra, D. Khashabi, C. Baral, and H. Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. In ACL, 2022. J. Ni, F. Xue, K. Jain, M. H. Shah, Z. Zheng, and Y. You. Instruction in the wild: user-based instruction dataset. https://github.com/XueFuzhao/InstructionWild, 2023. B. A. of Artificial Intelligence (BAAI). Infinity instruct. arXiv preprint arXiv:2406.XXXX, 2024. OpenAI. Introducing chatgpt. https://openai.com/index/chatgpt/, 2024. L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. B. Peng, C. Li, P. He, M. Galley, and J. Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023. PreferenceShareGPT. Preferencesharegpt. https://huggingface.co/collections/PJMix ers/preferencesharegpt-6655971b9ccb17d9670cdc7c, 2024. R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of Machine Learning Research, 21(140):167, 2020. URL http://jmlr.org/papers/v21/20-0 74.html. P. P. Ray. Chatgpt: comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope. Internet of Things and Cyber-Physical Systems, 3:121154, 2023. 18 V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, T. L. Scao, A. Raja, et al. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021. J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Q. Si, T. Wang, Z. Lin, X. Zhang, Y. Cao, and W. Wang. An empirical study of instruction-tuning large language models in chinese, 2023. R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab /stanford_alpaca, 2023. I. Team. Internlm: multilingual language model with progressively enhanced capabilities, 2023. P. Team, X. Du, Y. Yao, K. Ma, B. Wang, T. Zheng, K. Zhu, M. Liu, Y. Liang, X. Jin, Z. Wei, C. Zheng, K. Deng, S. Gavin, S. Jia, S. Jiang, Y. Liao, R. Li, Q. Li, S. Li, Y. Li, Y. Li, D. Ma, Y. Ni, H. Que, Q. Wang, Z. Wen, S. Wu, T. Hsing, M. Xu, Z. Yang, Z. M. Wang, J. Zhou, Y. Bai, X. Bu, C. Cai, L. Chen, Y. Chen, C. Cheng, T. Cheng, K. Ding, S. Huang, Y. Huang, Y. Li, Y. Li, Z. Li, T. Liang, C. Lin, H. Lin, Y. Ma, T. Pang, Z. Peng, Z. Peng, Q. Qi, S. Qiu, X. Qu, S. Quan, Y. Tan, Z. Wang, C. Wang, H. Wang, Y. Wang, Y. Wang, J. Xu, K. Yang, R. Yuan, Y. Yue, T. Zhan, C. Zhang, J. Zhang, X. Zhang, X. Zhang, Y. Zhang, Y. Zhao, X. Zheng, C. Zhong, Y. Gao, Z. Li, D. Liu, Q. Liu, T. Liu, S. Ni, J. Peng, Y. Qin, W. Su, G. Wang, S. Wang, J. Yang, M. Yang, M. Cao, X. Yue, Z. Zhang, W. Zhou, J. Liu, Q. Lin, W. Huang, and G. Zhang. Supergpqa: Scaling llm evaluation across 285 graduate disciplines, 2025. URL https://arxiv.org/abs/2502.14739. \"Teknium\". Character codex, 2024. https://huggingface.co/datasets/NousResearch/CharacterCodex. Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi. Self-instruct: Aligning language model with self generated instructions, 2022a. Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei, A. Arunkumar, A. Ashok, A. S. Dhanasekaran, A. Naik, D. Stap, et al. Super-naturalinstructions:generalization via declarative instructions on 1600+ tasks. In EMNLP, 2022b. Z. Wang, Q. Xie, Y. Feng, Z. Ding, Z. Yang, and R. Xia. Is chatgpt good sentiment analyzer? preliminary study. arXiv preprint arXiv:2304.04339, 2023. M. Weyssow, A. Kamanda, and H. Sahraoui. Codeultrafeedback: An llm-as-a-judge dataset for aligning large language models to coding preferences, 2024. S. Wu, Z. Peng, X. Du, T. Zheng, M. Liu, J. Wu, J. Ma, Y. Li, J. Yang, W. Zhou, Q. Lin, J. Zhao, Z. Zhang, W. Huang, G. Zhang, C. Lin, and J. H. Liu. comparative study on reasoning patterns of openais o1 model, 2024. URL https://arxiv.org/abs/2410.13639. S. Wu, Y. Li, X. Qu, R. Ravikumar, Y. Li, T. Loakman, S. Quan, X. Wei, R. Batista-Navarro, and C. Lin. Longeval: comprehensive analysis of long-text generation through plan-based paradigm, 2025. URL https://arxiv.org/abs/2502.19103. Q. Xie, Z. Wang, Y. Feng, and R. Xia. Ask again, then fail: Large language models vacillations in judgment. arXiv preprint arXiv:2310.02174, 2023. 19 s. Xinlu Lai. The dpo dataset for chinese and english with emoji. https://huggingface.co /datasets/shareAI/DPO-zh-en-emoji, 2024. G. Xu, J. Liu, M. Yan, H. Xu, J. Si, Z. Zhou, P. Yi, X. Gao, J. Sang, R. Zhang, J. Zhang, C. Peng, F. Huang, and J. Zhou. Cvalues: Measuring the values of chinese large language models from safety to responsibility, 2023. A. Yang, B. Yang, B. Hui, B. Zheng, B. Yu, C. Zhou, C. Li, C. Li, D. Liu, F. Huang, G. Dong, H. Wei, H. Lin, J. Tang, J. Wang, J. Yang, J. Tu, J. Zhang, J. Ma, J. Xu, J. Zhou, J. Bai, J. He, J. Lin, K. Dang, K. Lu, K. Chen, K. Yang, M. Li, M. Xue, N. Ni, P. Zhang, P. Wang, R. Peng, R. Men, R. Gao, R. Lin, S. Wang, S. Bai, S. Tan, T. Zhu, T. Li, T. Liu, W. Ge, X. Deng, X. Zhou, X. Ren, X. Zhang, X. Wei, X. Ren, Y. Fan, Y. Yao, Y. Zhang, Y. Wan, Y. Chu, Y. Liu, Z. Cui, Z. Zhang, and Z. Fan. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024a. A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei, H. Lin, J. Yang, J. Tu, J. Zhang, J. Yang, J. Yang, J. Zhou, J. Lin, K. Dang, K. Lu, K. Bao, K. Yang, L. Yu, M. Li, M. Xue, P. Zhang, Q. Zhu, R. Men, R. Lin, T. Li, T. Xia, X. Ren, X. Ren, Y. Fan, Y. Su, Y. Zhang, Y. Wan, Y. Liu, Z. Cui, Z. Zhang, and Z. Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024b. J. Yang. Firefly(ÊµÅËê§): ‰∏≠ÊñáÂØπËØùÂºèÂ§ßËØ≠Ë®ÄÊ®°Âûã. https://github.com/yangjianxin1/Fire fly, 2023. Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. W. Cohen, R. Salakhutdinov, and C. D. Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering, 2018. URL https://arxiv.org/abs/1809.09600. A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu, W. Zheng, X. Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022. B. Zhu, E. Frick, T. Wu, H. Zhu, and J. Jiao. Starling-7b: Improving llm helpfulness and harmlessness with rlaif, November 2023. D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. URL https://arxiv.org/abs/1909.08593. 20 Figure 5. The scoring prompt of Chat. domain. Figure 6. The scoring prompt of Math. domain. A. Prompts As shown in Figure 5, Figure 6, Figure 7, Figure 8,Figure 9, and Figure 10, we have designed different prompts for each field to score the responses corresponding to the queries. Figure 7. The scoring prompt of Code. domain. Figure 8. The scoring prompt of Novel. domain. B. Open-source datasets To enhance the quality of our queries dataset, we also collect from some open-source datasets by translating the query into Chinese: HotpotQA15, Online-IQ16, Ruozhiba17, olympiad task 15https://huggingface.co/datasets/hotpotqa/hotpot_qa 16https://github.com/huashuai/quhuashuai.com/blob/master/content/online-iq-tests.md 17https://huggingface.co/datasets/LooksJuicy/ruozhiba Figure 9. The scoring prompt of Logic. domain. Figure 10. The scoring prompt of Role. domain. translation18, Haruhi-Zero-RolePlaying-movie-PIPPA19, TAL-SCQ5K20, ANGO-S121, Character Codex [\"Teknium\", 2024], TheatreLM-v2.1-Characters22. 18https://huggingface.co/datasets/NMashalov/olympiad_task_translation 19https://huggingface.co/datasets/silk-road/Haruhi-Zero-RolePlaying-movie-PIPPA 20https://huggingface.co/datasets/math-eval/TAL-SCQ5K 21https://huggingface.co/datasets/AngoHF/ANGO-S1 22https://huggingface.co/datasets/G-reen/TheatreLM-v2.1-Characters 23 Figure 11. The sample of Chat. domain. Figure 12. The sample of Code. domain. C. Examples We show some examples of our COIG-P dataset. Each sample consists of query with Chosenrejected response pair. We present the sample in Figure 11, Figure 12, Figure 13, Figure 14, Figure 15, and Figure 16. 24 Figure 13. The sample of Math. domain. Figure 14. The sample of Role. domain. 25 Figure 15. The sample of Novel. domain. Figure 16. The sample of Logic. domain."
        }
    ],
    "affiliations": []
}