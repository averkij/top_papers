{
    "paper_title": "Bootstrapping World Models from Dynamics Models in Multimodal Foundation Models",
    "authors": [
        "Yifu Qiu",
        "Yftah Ziser",
        "Anna Korhonen",
        "Shay B. Cohen",
        "Edoardo M. Ponti"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "To what extent do vision-and-language foundation models possess a realistic world model (observation $\\times$ action $\\rightarrow$ observation) and a dynamics model (observation $\\times$ observation $\\rightarrow$ action), when actions are expressed through language? While open-source foundation models struggle with both, we find that fine-tuning them to acquire a dynamics model through supervision is significantly easier than acquiring a world model. In turn, dynamics models can be used to bootstrap world models through two main strategies: 1) weakly supervised learning from synthetic data and 2) inference time verification. Firstly, the dynamics model can annotate actions for unlabelled pairs of video frame observations to expand the training data. We further propose a new objective, where image tokens in observation pairs are weighted by their importance, as predicted by a recognition model. Secondly, the dynamics models can assign rewards to multiple samples of the world model to score them, effectively guiding search at inference time. We evaluate the world models resulting from both strategies through the task of action-centric image editing on Aurora-Bench. Our best model achieves a performance competitive with state-of-the-art image editing models, improving on them by a margin of $15\\%$ on real-world subsets according to GPT4o-as-judge, and achieving the best average human evaluation across all subsets of Aurora-Bench."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 6 0 0 6 0 . 6 0 5 2 : r a"
        },
        {
            "title": "Bootstrapping World Models from Dynamics Models\nin Multimodal Foundation Models",
            "content": "1Yifu Qiu, 3Yftah Ziser, 2Anna Korhonen, 1Shay B. Cohen, 1,2,3Edoardo M. Ponti 1Institute for Language, Cognition and Computation, University of Edinburgh 2Language Technology Lab, University of Cambridge 3NVIDIA {yifu.qiu,scohen,eponti}@ed.ac.uk"
        },
        {
            "title": "Abstract",
            "content": "To what extent do vision-and-language foundation models possess realistic world model (observation action observation) and dynamics model (observation observation action), when actions are expressed through language? While open-source foundation models struggle with both, we find that fine-tuning them to acquire dynamics model through supervision is significantly easier than acquiring world model. In turn, dynamics models can be used to bootstrap world models through two main strategies: 1) weakly supervised learning from synthetic data and 2) inference time verification. Firstly, the dynamics model can annotate actions for unlabelled pairs of video frame observations to expand the training data. We further propose new objective, where image tokens in observation pairs are weighted by their importance, as predicted by recognition model. Secondly, the dynamics models can assign rewards to multiple samples of the world model to score them, effectively guiding search at inference time. We evaluate the world models resulting from both strategies through the task of action-centric image editing on AURORA-BENCH. Our best model achieves performance competitive with state-of-the-art image editing models, improving on them by margin of 15% on real-world subsets according to GPT4o-as-judge, and achieving the best average human evaluation across all subsets of AURORA-BENCH1."
        },
        {
            "title": "Introduction",
            "content": "World models (observation action observation) [1, 2, 3, 4] can be successfully trained to simulate future trajectories given the history of past observations and actions. World models are instrumental in training embodied agents to endow them with specific abilities [5], such as grounding on affordances [6], spatio-temporal reasoning [7, 8], and planning [9, 10, 11]. However, learning specialised world model is challenging. Firstly, it requires large amount of real-world data [12] and even this data volume may be insufficient within the confines of the current training paradigm [13]. Secondly, the benefit of creating separate world model to train downstream embodied agent remains unclear because of possible compounding errors between the two models. Conversely, foundation models, such as vision-language models (VLMs), are already imbued with plenty of real-world knowledge of both action (in language form) and perception (in vision form), because of their large-scale pretraining. While such knowledge is not straightforward to elicit [14, 15, 16], we propose investigating promising alternative to specialised world models, by enhancing the knowledge implicitly stored inside foundation models. 1Our code and models are available at https://github.com/yfqiu-nlp/vlm-world-model. Preprint. Under review. Figure 1: Illustration of our two strategies to bootstrap world model from dynamics model in Vision-Language Models: (i) synthesising trajectories for weak supervision (left) and (ii) inferencetime verification of candidate observations (right). Firstly, we probe whether native VLMs already contain reliable world models, facilitated by model designs that combine various modalities into unified representation, i.e., sequences of tokens [17]. In particular, we frame the assessment of world models as the ability to solve action-centric image editing tasks [18]. In such tasks, the model predicts the next observation given the previous observation and an action expressed as language instruction. Based on our evaluation, we empirically demonstrate that existing open-source models do not prefer ground-truth trajectories compared to adversarially generated ones. Hence, we verify that the world model implicit in the original VLMs per se is not well grounded on real-world trajectories [14, 15, 16]. Surprisingly, we also find that acquiring dynamics model (observation observation action) via supervised fine-tuning is substantially easier than directly acquiring world model (observation action observation). Inspired by this observation, we propose two strategies to bootstrap the world model from the dynamics model in given VLM, namely (i) learning from synthetic trajectories in videos automatically labelled with actions by the dynamics model; and (ii) test-time verification of predicted observations sampled from the world model through the dynamics model. For the weak supervision strategy, which is reminiscent of [19], we use dynamics model fine-tuned on the AURORA dataset [18] to annotate motion key-frames pairs extracted from real-world videos with actions (in language form). Around 45 hours of unlabelled videos are sourced from movementsin-time [20], Kinetics700 [21, 22] and UCF-101 [23]. Together with the ground-truth trajectories in AURORA, the synthesised trajectory triplets (observation annotated action observation) are then used for supervised fine-tuning of the VLM world model. To effectively train the world model, we additionally propose loss-weighting method which weights the loss of each image token according to the visual difference between the ground-truth source and target observations, as estimated by recognition model. In the verification strategy, we show how using the VLM dynamics model to assign rewards to multiple samples generated by the VLM world model can effectively guide search at inference time. We conduct an extensive evaluation on MagicBrush, Action-Genome, Something-Something, WhatsUp and Kubric in AURORA-BENCH [18]. We focus on Chameleon-7B as the best available opensource foundation model, and transform it into world model (CWM; Chameleon World Model). We show that thanks to the synthetic data strategy to bootstrap world models from dynamics models, our general-purpose CWM can achieve an overall performance superior to state-of-the-art diffusion models specialised for image editing. In particular, CWM improves GPT4o-as-a-judge scores on the Something-Something, Action-Genome, and Kubric subsets of AURORA by 15%, 15% and 7%, respectively. Similarly, human evaluators rate CMW image editing consistently better. Inference-time verification can also improve AURORA-finetuned Chameleon to comparable degree as data synthesis, 2 Figure 2: Comparison of predicted negative log-likelihoods (lower values indicate stronger model preference) for ground-truth real-world trajectories versus four types of negative trajectories. Top: Action prediction task for the dynamics model (observation observation action). Bottom: Next observation prediction task for the world model (observation action observation). The legend shows the percentage of times the model prefers the ground-truth trajectory () over the negatives (). providing an effective training-free bootstrapping method. In some cases, it can even be combined with trajectory synthesis for compounded gains. To summarise our contributions: We empirically show that VLMs like Chameleon-7B do not exhibit clear preference for ground-truth real-world trajectories over heuristic-generated incorrect ones. We propose two strategies to bootstrap world model from dynamics model inside VLMs: (i) learning from unlabelled videos annotated with actions by dynamics model, and (ii) verifying the generated observations with the dynamics model at inference time. We conduct extensive evaluations on AURORA-BENCH: both GPT4o-as-a-judge and human raters demonstrate the effectiveness of our methods with considerable margin compared to the state-of-the-art image editing models."
        },
        {
            "title": "2 VLMs Lack a Consistent Preference for Real-World Trajectories",
            "content": "The first research question we investigate in this paper is: To what extent do VLMs exhibit preference for token sequences of actions and observations that align with real-world trajectories? To address this question, we evaluate VLM on ground-truth trajectories from 5 subsets of AURORABENCH [18]: MagicBrush, Something-Something, Action-Genome, Whatsup, and Kubric. Each subset contains 50 trajectory triplets of the form (os, a, ot), where os is the source observation, the action text, and ot the next observation. Specifically, we test Chameleon-7B [17] as VLM.2 We then manually curate four types of negative trajectories using heuristic rules: two that manipulate the observation component of the trajectory triplet, and two that manipulate the action component. We design two kinds of action-level manipulation: 1) Random Action: for given pair of observations, we substitute the original action with another randomly sampled within the same subset. 2) Counterfactual Action (CF): through Large Language Model (LLM),3 we rewrite the action to contradict the original one. We also test the following observation-level manipulations. 3) Copy Observation: we directly copy the source observation as the target observation. 4) Inverse Observation: we swap the source and target observations. If the VLM were well aligned with ground-truth trajectories, it should display clear preference by assigning them higher likelihood than the negative alternatives. In Figure 2, we compare the negative 2In our preliminary experiments, where we compared Chameleon-7B with alternative VLMs such as Vila-U [24] (see Table 1), it emerged as the best open-source VLM that allows for interleaving text and images. 3Specifically, Qwen-2-7b-Instruct. 3 Figure 3: Heatmap visualization of image token weights predicted by the recognition model on examples from UCF-101, Something-Something, MagicBrush, and Kubric. log-likelihood Chameleon-7B [17] assigns to each ground-truth trajectory against its corresponding manipulated one. We evaluate the VLM in two tasks: action prediction (i.e., as dynamics model) and next-observation prediction (i.e., as world model). For each kind of negative trajectory, we also indicate the percentage of samples where the model favours the reference trajectory ( Ref) versus the negative trajectory ( Type). Additionally, we report the Pearson correlation coefficient between the predicted likelihoods for the reference and negative trajectories; higher correlation suggests that the model struggles to distinguish between the two, assigning them similar likelihoods. From Figure 2, it emerges that Chameleon-7B displays very limited preference for the ground-truth trajectories in zero-shot setting. In the action prediction task (top panel), there is slightly higher tendency to favour the ground-truth; however, even in the best case (counterfactual action), the model prefers the reference in only 58.1% of the samples. The high correlation in likelihoods indicates that the VLM struggles also on visual manipulations. In the next-observation prediction task (bottom panel), the VLM mostly fails in effectively differentiating the ground truth from the negatives. An exception to this is the copy manipulation, where the model can always tell them apart. Although the underlying reason remains uncertain, one plausible explanation for this behaviour is that the models ability to solve next-observation prediction tasks depends on their alignment with training sequences: for instance, it is plausible that Chameleons data rarely features two identical adjacent images."
        },
        {
            "title": "3 Bootstrapping a World Model from a Dynamics Model in VLMs",
            "content": "Since we showed in Section 2 that Chameleon-7B displays higher proclivity as for action prediction than next-observation prediction, we first verify that this tendency is intensified when Chameleon-7B is fine-tuned on image editing trajectories (Section 3.1), as this results in the VLM acting reliably as dynamics model. Motivated by this, we propose two strategies to leverage the VLMs as dynamics models to enhance VLMs as world models: (i) generating synthetic trajectories by annotating largescale key-frame pairs from videos with actions predicted by the dynamics model, then using these as weak supervision to train the world model (Section 3.2); and (ii) using the dynamics model as verifier at test time to score candidate next observations sampled from the world model (Section 3.3). 3.1 Fine-tuning Chameleon as Dynamics Model First, we fine-tune Chameleon as Dynamics Model (CDM) pCDM(a os, ot), which predicts the probability of an action given the previous and next observations. As training data, we rely on high-quality triplets from AURORA [18] and the action recognition track of EPIC-Kitchen [25], which is based on videos with an egocentric view. We use the first and last frame in the EPIC-Kitchen video clips as the source and target observation os and ot and the annotated action as a. We provide full details on CDM training data and experimental setting Appendix A.8.1. Foreshadowing the results in Section 4.2, this significantly enhances action-prediction capabilities of Chameleon by wide margin. 3.2 Weakly Supervised Learning from Unlabelled Videos Synthetic Trajectories. Taking advantage of the resulting high-quality CDM, we then explore the first of our strategies to bootstrap world model in VLMs: we annotate pairs of motion key-frames of unlabelled videos with textual description of the action with the CDM. To ensure both scale and quality, we collect approximately 45 hours of video from Moments-in-Time [20], Kinetics-700 [21, 22], and UCF-101 [23], all of which consist of curated clips focused on human actions. To ensure the selected pairs of motion key-frames are meaningful, i.e., they express valid action, we then calculate the optical flow to quantify the dynamics per frame in the video clips, and select the top-Kf frames while ensuring that the interval between two selected frames is If . Specifically, we set If = 20 and Kf = 6 for all three datasets. This results in approximately 20K, 46K, and 21K annotated trajectory triplets from Moments-in-Time, Kinetics-700, and UCF-101, respectively. Finally, we apply filtering strategy to further guarantee the quality of the resulting triplets. We use the CDMs predicted likelihood for each trajectory triplet (os, aCDM, ot) as score, and apply stratified Top-K sampling4 to select subset of CDM-annotated trajectory triplets. We show statistics of the scores and action classes for the processed triplets in Figure 11. We also provide one example for each dataset in Figure 3. Fine-tuning Chameleon as World Model. Afterwards, we fine-tune Chameleon as World Model (CWM), pCWM(ot a, os) on both AURORAs supervised triplets Dsup and unsupervised triples Dunsup with actions sampled from the CDM. The world model CWM is trained with maximum likelihood estimation as an objective: min θ E(a,os,ot)Dsup [ log pθ(ot a, os)] + E(os,ot)Dunsup (cid:2)EˆapCDM(aos,ot) [ log pθ(ot ˆa, os)](cid:3) , (1) where θ are the parameters for CWM, and ˆa is action sampled from the CDM. Recognition-Weighted Training Loss. Nevertheless, the objective in Equation 1 is limited by treating all regions of the target observation equally, even if some of them remain identical to the source whereas others change. This may result in degenerate solutions such as always copying the source. As an alternative, we therefore propose novel training objective for world models that overcomes this assumption. This objective weights the loss of next-observation image tokens based on their importance. The intuition is that not all image patches in source and target observations contribute equally to modelling real-world transitions; instead, the model should focus on patches most indicative of the actions consequences. To this end, we leverage recognition model frec(os, ot), which outputs token-level weights aligned with Chameleons image token representations. These weights modulate the loss during training, emphasising learning on semantically meaningful regions and down-weighting irrelevant ones. We formulate our alternative objective as: min θ (cid:88) l=1 frec(os, ot)(l) (cid:16) log pθ(o(l) o(<l) (cid:17) , os, a) , (2) and o(<l) where θ are the parameters of CWM and is the number of tokens used to represent an image in Chameleon. o(l) represent the image tokens of ot at position and the history of previous positions, respectively. For simplicity, we use the pre-trained vector-quantised model of Chameleon as the recognition model, by computing the squared L2 norm of pre-quantized features zos Zos and zot Zot where Zos and Zot are the sets of features of source and target observations, respectively. We visualise the token weights in Figure 3, which capture the effects of acting on the source observation to yield the target one. 3.3 Test-time Verification Finally, we introduce an inference-time strategy which harnesses the CDM as verifier to enhance CWM performance. Inspired by recent work on scaling test-time compute [26, 27], we let the CWM generate candidate observations. Each candidate is paired with the source and scored by the CDM, which assigns each predicted likelihood, interpreted as reward. The final prediction of the CWM is selected by maximising the CDMs reward: ˆot = argmax i{1,...,N } pCDM (cid:16) os, o(i) (cid:17) , where o(i) pCWM(ot os, a), where ˆot is the selected prediction. 4The details of this algorithm are provided in Appendix A.5. 5 Table 1: Performance of dynamics models performance on action prediction, measured by text similarity metrics: BERTScore (BS; [32]), ROUGE-1/2/L (R-1, R-2, R-L; [33]) and BLEU [34]. VILA-U Fine-tuned Chameleon Zero-Shot (C-ZS) Chameleon Fine-Tuned (CDM) Chameleon Fine-Tuned (CDM) + DS BS 0.40 0.05 0.40 0. R-1 R-2 R-L BLEU 0.38 0.09 0.39 0.45 0.20 0.02 0.20 0.27 0.37 0.08 0.37 0.44 0.15 0.00 0.17 0.20 Figure 4: Comparison of negative log-likelihoods (lower values indicate stronger model preference) of the action predicted by CDM for ground-truth trajectories versus four types of negative trajectories."
        },
        {
            "title": "4 Experiments and Results",
            "content": "4.1 Experimental Setting Benchmarks. We select AURORA-BENCH [18] for evaluation of both dynamics and world models. This dataset provides high-quality data for action-centric edits, covering wide array of phenomena and assessing models alignment with the physical world, including temporal and spatial reasoning. We choose 5 subsets: MagicBrush for specialised image editing, Action-Genome (AG) and Something-Something (Something) for real-world actions and scenarios. Whatsup focuses on spatial reasoning, whereas Kubric contains synthetic samples from physical engine [28]. Baselines. We report Chameleons zero-shot performance (C-ZS). We also fine-tune Chameleon on AURORAs training set as our first baseline (C-FT). We compare CWM with C-FT in both single-prediction setting and in best-of-N setting. The latter provides ceiling performance for inference-time verification with CDM. Additionally, we include three state-of-the-art diffusion models specialised for image editing as baselines, such as PixInstruct [29], GoT [30] and SmartEdit [31]. As sanity check, we also report the metric scores obtained by simply copying the source observation input as the next-observation prediction (Copy). Metrics. For next-observation prediction evaluation, following [30], we rely on GPT4o-as-a-judge as it is the only metric that reliably penalises Copy. In Appendix A.3, we show four other metrics, e.g., CLIP, which assign high scores to Copy. GPT4o-as-a-judge scores consider two criteria, one for the editing success rate and one for visual consistency with the original. We take the minimum of the two as the final score. The prompt for GPT4o-as-a-judge is provided in Appendix A.6. 4.2 Chameleon Dynamics Model We evaluate the dynamics models based on the textual similarity of the predicted action with the ground-truth action in AURORA-BENCH, as shown in Table 1. Our results demonstrate that finetuning is necessary to elicit Chameleons ability to verbalise the dynamics from two observations. We then compare Chameleon fine-tuned on action prediction (CDM) with the fine-tuned version of another state-of-the-art VLM, VILA-U [24]. CDM is on par or superior to VILA-U fine-tuned, justifying our choice of Chameleon as foundation model for our experiments. Table 1 also provides an ablation showing that downsampling trajectories from Kubric (DS) in the training data further boosts CDM performance (CDM + DS). This suggests that data sourced from simulations do not necessarily translate into better dynamics modelling in real-world examples. We use the DS version of CDM in the rest of the experiments as the best-performing dynamics model. In Figure 4, we further evaluate CDM on discriminating between ground-truth and negative trajectories, as in Section 2. Now, 6 Table 2: Model performance on MagicBrush, AG, Something, WhatsUp, and Kubric from AURORABENCH in terms of GPT-4o scores. For C-FT and CWM, we report their performance for both single prediction and best-of-N. The average scores for each model are shown at the bottom. We bold the best model overall for each subset and highlight the best and worst scores among our variants for each setting. SE: SmartEdit. Detailed results with 5 metrics are presented in Table 3. Datasets Models Copy PixInstruct GoT SE C-ZS C-FT +Best-of-3 CWM +Best-of-3 MagicBrush AG Something WhatsUp Kubric Average 0.000 0.000 0.000 0.000 0.000 0.000 3.120 1.200 0.957 0.000 1. 1.430 5.960 1.610 2.620 1.580 3.920 6.710 3.080 2.810 0.755 3.700 0.000 0.170 0.370 0.146 0.140 2.520 2.480 3.110 0.880 7.300 3. 3.410 0.165 3.260 3.270 2.740 3.110 0.980 7.300 3.480 3.920 3.640 2.920 0.540 7. 3.670 3.920 3.640 3.310 0.540 7.780 3.840 we observe that CDM is mostly successful in identifying manipulated trajectories as such, except for Copy. These results corroborate the feasibility of annotating actions for key-frame pairs. 4.3 Chameleon World Model Figure 5: Detailed scores of GPT4o-as-a-judge evaluation for loss-weighting and standard training. We report the scores for Editing Success (ES) and Minimal Editing (ME). MB: MagicBrush, AG: Action-Genome, ST: Something-Something, WU: WhatsUp, KU: Kubric. We highlight the best and worst scores for each category. Automatic evaluation. Next, we test CWM on next-observation prediction for each of the AURORA-BENCH subsets, reporting GPT4-asa-judge scores in Figure 2. We first notice that the state-of-the-art image editing models (i.e., PixInstruct, GoT, SmartEdit) tend to specialise in the image editing benchmark, MagicBrush (5.96 and 6.71 GPT4o scores for GoT and SmartEdit). Nevertheless, in the actioncentric subsets, including Action-Genome (AG), Something and Kubric, they are mostly behind CWM and even C-FT. In particular, CWM outperforms all other models in these 3 subsets, achieving gains of 18%, 4%, and 86%, respectively, over the best diffusion baselines. In addition, it boosts the highest average performance across subsets, with an 8% increase. Crucially, comparing CWM and C-FT reveals the benefit of augmenting the training data with synthetic triplets bootstrapped from the CDM, as it yields 13% performance margin. CWM also outperforms C-FT on the best-of-N setting [35], indicating the potential for inference-time verification as best-of-N is effectively an oracle for its performance. Weighted ES () ME () Standard ES () ME () 8.46 8.13 7.20 7.19 8.70 8.17 8.03 7.01 7.25 8.49 3.68 2.37 2.78 0.76 7. 3.73 3.18 3.32 0.54 7.75 MB AG ST WU KU Avg. GPT4o 3.58 3.71 3. 3.37 7.80 7.94 Human Evaluation. Following [18], we conduct blind human evaluation comparing GoT, SmartEdit, C-FT, and our proposed CWM. We randomly sample 5 examples from each subset within AURORA-BENCH and present the outputs generated by each of the four models. Human annotators are asked to identify the best and worst generated observations based on three criteria: (1) Realism: the generated image should exhibit natural textures and lighting while remaining faithful to the input scene; (2) Instruction-Following Ability: the edit should clearly reflect the given instruction; and (3) Over-Editing: the modification should be minimal and focused, altering only what is necessary. Each model receives +1 point for being selected as the best, -1 for the worst, and 0 otherwise. We compute the average scores over 350 annotated samples, as reported in Table 7. The results align with automatic evaluations: image-editing models excel in the MagicBrush domain, but fall short on action-centric datasets such as Action-Genome, Something-Something, and Kubric. In contrast, CWM outperforms C-FT on all three of these datasets, highlighting its strength in next-observation prediction in real-world, action-centric trajectories. 7 Figure 6: Ablation study of synthetic trajectories (Synth.) and loss weighting (LW) in CWM. Numbers are GPT-4o-as-judge scores (, average of 3 runs). MB: MagicBrush, AG: ActionGenome, ST: Something-Something, WU: WhatsUp, KU: Kubric. Figure 7: Human evaluation results. indicates all results whose gap with respect to CWM is significant, based on Wilcoxon signed-rank test (p = 0.05). MB: MagicBrush, AG: ActionGenome, ST: Something-Something, WU: WhatsUp, KU: Kubric. CWM w/o Synth. w/o LW MB AG ST WU KU All 3.48 3.02 3.06 0.46 7. 3.43 -0.28 -0.35 -0.18 0.40 -0.03 -0.09 -0.22 -0.08 -0.19 0.08 -0.33 -0.15 GoT 0.06 MB AG -0.23 ST 0.00 WU 0.25 KU -0.52 -0.09 All SE 0.29 -0.46 -0.37 -0.38 -0.22 -0.23 C-FT CWM -0.32 0.32 0.18 0.14 0.34 -0.03 0.37 0.20 0.00 0.40 0.13 0. Figure 8: GPT-4o scores for test-time verification with samples, where {1, 2, 4, 8}. We use blue line for C-FT and red line for CWM, plotting the standard deviation as the shaded area. We indicate the scores for GoT (GT) and SmartEdit (SE) as horizontal lines. Ablation Study on Synthetic Trajectories. To assess the importance of extra supervision from CDM-synthetic trajectories, Table 6 reports GPT-4os scores for this ablation. We see performance drops on most datasetsparticularly on Something and AGwhen the additional training data from unlabelled videos is removed, highlighting the effectiveness of bootstrapping CWM with large-scale real-world data via CDM. An exception is the WhatsUp dataset, which focuses on specific actions within fixed scene; in this case, training in an open-domain setting may not transfer effectively. Ablation Study on Loss Weighting. Based on Table 6, we also observe consistent degradation when loss weighting is removed, demonstrating the benefit of explicitly incorporating the recognition model into visual next-token prediction. To better understand the effect of loss weighting, Table 5 reports the average scores for two criteria used in the GPT-4o-as-a-judge evaluation separately: Editing Success (ES), which measures how well the model captures the intended action and performs the corresponding edit, and Minimal Editing (ME), which assesses whether the model introduces unnecessary modifications. The full distribution of GPT-4o scores is provided in Appendix A.7. Our analysis reveals that the primary bottleneck for CWM remains its ability to reliably follow the instruction, as reflected by the fact that ES scores are significantly lower than ME scores. Loss weighting partly solves this problem, increasing the editing success and reducing copying behaviour, albeit at the cost of sometimes over-editing the source observation. Verification at Test Time. We evaluate test-time verification using CDM in Figure 8, comparing C-FT and CWM with 1, 2, 4, 8. Each experiment is repeated three times, and we visualise the mean and standard deviation. By increasing exploration on more candidate next observations, C-FT benefits from test-time verification on most datasets with real-world trajectories (e.g., AG, Something, WhatsUp), suggesting the effectiveness of CDMs trajectory preferences. Increasing does not always improve performance (MagicBrush, Kubric), suggesting that bootstrapping with dynamics model that shares the same foundation model backbone may be limiting. In contrast, CWM 8 Figure 9: qualitative case of real-world observation prediction, demonstrating CWMs ability to steer predictions using language and perform sequential predictions. More cases from AURORABENCH are in Appendix A.4. shows no clear gain, likely because it was trained with the synthetic trajectories and has already internalised CDMs preferencesas is evident from its strong = 1 performance. In summary, CDM-based verification boosts C-FTs performance to the same level as CWM, by leveraging more diverse samples at inference time rather than during training. Qualitative Example. Figure 9 presents real-world example demonstrating that CWMs predicted observations can be guided through language expressing actions. CWM is also capable of iteratively generating future observations in multiple steps while maintaining consistency with previous frames."
        },
        {
            "title": "5 Related Work",
            "content": "Despite the surge in interest for world modelling [1, 36, 37], previous works focused mostly on building specialised ad-hoc world models. These world models can be explicitly learnt as visual simulator [2, 3, 4], or enable planning with model-based reinforcement learning [38, 39, 40, 41, 11]. Instead, we focus on leveraging large-scale multimodal foundation models [42, 43, 24] to develop world models, which is more attractive due to the inductive bias they provide from their extensive training. This is possible thanks to frameworks that integrate observations, actions, and rewards into unified sequence of tokens in autoregressive Transformers [44], building on pioneering works such as Decision Transformers [45] and GATo [46]. Related to our work, [47] initialise the parameters of RL policies with VLMs, thus taking advantage of the abundant and general world knowledge encoded in their representations. 3D-VLA [48] introduces set of interaction tokens into Large Language Model to engage with the environment as an embodied agent. [49, 50] explore large-scale self-supervised learning via next token or frame prediction to build unified model absorbing internet knowledge, learning from interaction via video. AURORA-BENCH [18] was the first to approach world modelling through the lens of an action-centric image editing task. With advanced native VLMs capable of the interleaved generation [17, 51], we systematically investigate how this data may help us bootstrap world model, implicitly stored in the VLMs, with an easier-to-train dynamics model. Most similar to our work, [19] train dynamics model which aims to uncover the underlying action between video frames in unlabelled video frames from the Minecraft game. Through this model, they synthesise trajectories to train policy for sequential decision making. In contrast with [19], we focus on next-observation prediction as task to evaluate world modelling. First, this allows us to port the observation space to real-world frames, rather than simulated ones, hence assessing whether world models are well aligned with the physical environment. Second, this broadens the space of actions from few choices to the combinatorially infinite and expressive space of language, capturing significantly more diverse range of dynamics."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we explored whether we can develop world models from VLMs. By evaluating them on action-centric image editing AURORA-BENCH [18], we first show that these models lack clear preference for ground-truth real-world trajectories. To address this, we induce dynamics model from the same VLM to bootstrap world model using automatic annotation of unlabelled real-world videos and inference-time verification. Experiments confirm the effectiveness of both strategies, with our general-purpose world model achieving state-of-the-art performance compared to existing approaches specialised for image editing."
        },
        {
            "title": "References",
            "content": "[1] David Ha and Jürgen Schmidhuber. Recurrent world models facilitate policy evolution. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. [2] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical AI. arXiv preprint arXiv:2501.03575, 2025. [3] Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. [4] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators. 2024. URL https://openai. com/research/video-generation-models-as-world-simulators, 3:1, 2024. [5] Yiran Qin, Zhelun Shi, Jiwen Yu, Xijun Wang, Enshen Zhou, Lijun Li, Zhenfei Yin, Xihui Liu, Lu Sheng, Jing Shao, et al. WorldSimBench: Towards video generation models as world simulators. arXiv preprint arXiv:2410.18072, 2024. [6] Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, et al. Do as can, not as say: Grounding language in robotic affordances. In Conference on robot learning, pages 287318. PMLR, 2023. [7] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner Monologue: Embodied reasoning through planning with language models. In 6th Annual Conference on Robot Learning, 2022. [8] Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulic, and Furu Wei. Imagine while reasoning in space: Multimodal visualization-of-thought. arXiv preprint arXiv:2501.07542, 2025. [9] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. generalist agent. arXiv preprint arXiv:2205.06175, 2022. [10] Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, and Pieter Abbeel. Learning interactive real-world simulators. arXiv preprint arXiv:2310.06114, 1(2):6, 2023. [11] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse control tasks through world models. Nature, 640(8059):647653, 2025. [12] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with blockwise RingAttention. arXiv preprint arXiv:2402.08268, 2024. [13] Saman Motamed, Laura Culp, Kevin Swersky, Priyank Jaini, and Robert Geirhos. Do generative video models learn physical principles from watching videos? arXiv preprint arXiv:2501.09038, 2025. [14] Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, and Dorsa Sadigh. Physically grounded vision-language models for robotic manipulation. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 1246212469. IEEE, 2024. [15] Yifu Qiu, Zheng Zhao, Yftah Ziser, Anna Korhonen, Edoardo Ponti, and Shay Cohen. Are large language model temporally grounded? In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 70577076, 2024. [16] Mostafa Abdou, Artur Kulmizev, Daniel Hershcovich, Stella Frank, Ellie Pavlick, and Anders Søgaard. Can language models encode perceptual structure without grounding? case study in color. In Arianna Bisazza and Omri Abend, editors, Proceedings of the 25th Conference on Computational Natural Language Learning, pages 109132, Online, November 2021. Association for Computational Linguistics. [17] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. [18] Benno Krojer, Dheeraj Vattikonda, Luis Lara, Varun Jampani, Eva Portelance, Christopher Pal, and Siva Reddy. Learning Action and Reasoning-Centric Image Editing from Videos and Simulations. In NeurIPS, 2024. Spotlight Paper. [19] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. Video PreTraining (VPT): Learning to act by watching unlabeled online videos. Advances in Neural Information Processing Systems, 35:2463924654, 2022. [20] Mathew Monfort, Alex Andonian, Bolei Zhou, Kandan Ramakrishnan, Sarah Adel Bargal, Tom Yan, Lisa Brown, Quanfu Fan, Dan Gutfruend, Carl Vondrick, et al. Moments in time dataset: one million videos for event understanding. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 18, 2019. [21] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017. [22] Joao Carreira, Eric Noland, Chloe Hillier, and Andrew Zisserman. short note on the kinetics700 human action dataset. arXiv preprint arXiv:1907.06987, 2019. [23] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. UCF101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. [24] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. VILA-U: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. [25] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. Scaling egocentric vision: The EPIC-KITCHENS dataset. In Proceedings of the European conference on computer vision (ECCV), pages 720736, 2018. [26] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. [27] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. [28] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David Fleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, et al. Kubric: scalable dataset generator. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 37493761, 2022. [29] Tim Brooks, Aleksander Holynski, and Alexei Efros. InstructPix2Pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1839218402, 2023. [30] Rongyao Fang, Chengqi Duan, Kun Wang, Linjiang Huang, Hao Li, Shilin Yan, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, et al. GoT: Unleashing reasoning capability of multimodal large language model for visual generation and editing. arXiv preprint arXiv:2503.10639, 2025. 11 [31] Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan, Xiaodong Cun, Yixiao Ge, Jiantao Zhou, Chao Dong, Rui Huang, Ruimao Zhang, et al. SmartEdit: Exploring complex instructionbased image editing with multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 83628371, 2024. [32] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, and Yoav Artzi. BERTScore: Evaluating text generation with bert. In International Conference on Learning Representations, 2020. [33] Chin-Yew Lin. ROUGE: package for automatic evaluation of summaries. In Text summarization branches out, pages 7481, 2004. [34] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318, 2002. [35] Afra Amini, Tim Vieira, Elliott Ash, and Ryan Cotterell. Variational best-of-n alignment. The Thirteenth International Conference on Learning Representations, 2025. [36] Richard Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3:944, 1988. [37] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In International conference on machine learning, pages 25552565. PMLR, 2019. [38] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In International conference on machine learning, pages 25552565. PMLR, 2019. [39] Vincent Micheli, Eloi Alonso, and François Fleuret. Transformers are sample-efficient world models. In Deep Reinforcement Learning Workshop NeurIPS 2022, 2022. [40] Jan Robine, Marc Höftmann, Tobias Uelwer, and Stefan Harmeling. Transformer-based world models are happy with 100k interactions. arXiv preprint arXiv:2303.07109, 2023. [41] Eloi Alonso, Adam Jelley, Vincent Micheli, Anssi Kanervisto, Amos Storkey, Tim Pearce, and François Fleuret. Diffusion for world modeling: Visual details matter in atari. Advances in Neural Information Processing Systems, 37:5875758791, 2024. [42] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-LLaVA: Learning united visual representation by alignment before projection. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 59715984, 2024. [43] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-Pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. [44] Jialong Wu, Shaofeng Yin, Ningya Feng, Xu He, Dong Li, Jianye Hao, and Mingsheng Long. iVideoGPT: Interactive VideoGPTs are scalable world models. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 6808268119. Curran Associates, Inc., 2024. [45] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:1508415097, 2021. [46] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. generalist agent. arXiv preprint arXiv:2205.06175, 2022. 12 [47] William Chen, Oier Mees, Aviral Kumar, and Sergey Levine. Vision-language models provide promptable representations for reinforcement learning. In Automated Reinforcement Learning: Exploring Meta-Learning, AutoML, and LLMs, 2024. [48] Haoyu Zhen, Xiaowen Qiu, Peihao Chen, Jincheng Yang, Xin Yan, Yilun Du, Yining Hong, In and Chuang Gan. 3D-VLA: 3D Vision-Language-Action Generative World Model. International Conference on Machine Learning, pages 6122961245. PMLR, 2024. [49] Sherry Yang, Jacob Walker, Jack Parker-Holder, Yilun Du, Jake Bruce, Andre Barreto, Pieter Abbeel, and Dale Schuurmans. Video as the new language for real-world decision making. arXiv preprint arXiv:2402.17139, 2024. [50] Achint Soni, Sreyas Venkataraman, Abhranil Chandra, Sebastian Fischmeister, Percy Liang, Bo Dai, and Sherry Yang. VideoAgent: Self-improving video generation. arXiv preprint arXiv:2410.10076, 2024. [51] Ethan Chern, Jiadi Su, Yan Ma, and Pengfei Liu. ANOLE: An open, autoregressive, native large multimodal models for interleaved image-text generation. arXiv preprint arXiv:2407.06135, 2024. [52] Edward Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. [53] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural language processing. In Qun Liu and David Schlangen, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 3845, Online, October 2020. Association for Computational Linguistics."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Limitations While our approach demonstrates the effectiveness of our approaches across AURORA-BENCH, the authors would like to highlight few limitations we have discovered: Despite efforts to guide the model via supervised fine-tuning with loss weighting or inferencetime verification  (Table 2)  , we observe that the model may still resort to copying the source observation, especially under low sampling temperatures or ambiguous instructions. While we show preliminary results of language-steered observation prediction in Figure 9, fine-grained control remains limited, and understanding subtle instructions (e.g., spatial or quantitative edits) remains challenging. We observe variance across different runs of experiments, likely due to the sensitivity of sampling for generation in multimodal models. To address this, we report results averaged over multiple runs and include performance under the best-of-N sampling distribution during inference for robust comparison. We mostly conduct experiments using the native and unified VLM, Chameleon, as it is currently the only open-source VLM that supports interleaved image-text generation by default. This choice allows for fair and consistent benchmarking across our tasks. Moreover, Chameleon has demonstrated competitive performance in our settings. For example, its results are comparable to VILA-U in our dynamics prediction task. Future work should explore the generalisation to other multimodal foundation models with stronger capabilities. A.2 Broader Impact This work develops models for action-centric image editing for visual world modelling. While our primary aim is to advance fundamental research in world modelling, we acknowledge potential risks, particularly in the generation of realistic future observations. core concern is the potential misuse of the models for creating deceptive visual content, including fabricated action sequences or manipulated images that imply false causality. Although the model is not explicitly designed for these tasks, its ability to generate coherent visual predictions from the linguistic action could be adapted for such uses if deployed irresponsibly. Even in intended use, risks include over-reliance on generated outputs in downstream tasks such as robotic control, or interactive systems. Model failurese.g., copying artefacts, hallucinations, or broken object continuitycan lead to incorrect inferences or reinforce dataset biases. To mitigate potential misuse, we limit our model release to research purposes under non-commercial license and clearly communicate its capabilities and limitations. We urge caution when adapting them for deployment, particularly in settings with high societal or ethical sensitivity. A.3 Model Performance on AURORA-BENCH with 5 Metrics In addition to GPT4o-as-a-judge evaluation, we further employ diverse set of automatic metrics covering both low-level and semantic fidelity: 1) we compute the L1 distance between the predicted and target observation as pixel-level metric. 2) We extract visual features and compute the cosine similarity in their respective embedding spaces for several image encoders, including (CLIP-I and DINO), to assess semantic similarity. Additionally, to measure alignment between image content and the action semantics, we compute CLIP-T, the similarity between the edited image and its BLIPgenerated caption. These metrics are evaluated in addition to GPT4o-as-a-judge metric following previous works in image editing [31, 30, 18]. We report the detailed results with 5 metrics in Table 3. We notice that copy baseline exhibits the best performance as measured by the distance-based and visual encoder-based approach, as indicated in Table 2. This poses challenge to the reliability of the traditional metrics in fairly evaluating the action-centric image editing task. On the other hand, GPT4o-as-a-judge metric robustly assigns 0 score to Copy, indicating its robustness in detecting copying generation while putting GPT-as-a-judge as the most reliable metric to interpret. 14 Table 3: Model performance at MagicBrush, Action-Genome, Something, WhatsUp and Kubric on AURORA-BENCH. For C-FT and CWM We report both the model performance and their performance in the best-of-N distribution. We report the average GPT4o scores for each model at the bottom. We highlight the better GPT-4o scores for C-FT and CWM. We bold the best performance among all models, except Copy and best-of-N performances. SE: SmartEdit. Datasets Metrics Models Copy PixInstruct GoT SE CM C-FT +Best-of-3 CWM +Best-of-3 MagicBrush AG Something WhatsUp Kubric L1 CLIP-I CLIP-T DINO GPT-4o L1 CLIP-I CLIP-T DINO GPT-4o L1 CLIP-I CLIP-T DINO GPT-4o L1 CLIP-I CLIP-T DINO GPT-4o L1 CLIP-I CLIP-T DINO GPT-4o 0.027 0.959 0.289 0.931 0.000 0.069 0.943 0.279 0.929 0.000 0.135 0.870 0.275 0.797 0.000 0.039 0.954 0.326 0.908 0. 0.011 0.963 0.282 0.955 0.000 All GPT-4o 0.000 0.114 0.877 0.275 0.761 3.120 0.220 0.757 0.254 0.557 1. 0.232 0.709 0.238 0.453 0.957 0.138 0.817 0.287 0.615 0.000 0.104 0.796 0.259 0.676 1.880 1.430 0.063 0.930 0.286 0.881 5.960 0.174 0.846 0.280 0.785 1. 0.184 0.807 0.269 0.636 2.620 0.078 0.923 0.316 0.850 1.580 0.026 0.895 0.281 0.857 3.920 0.068 0.937 0.290 0.894 6.710 0.137 0.811 0.268 0.774 3.080 0.163 0.773 0.265 0.662 2. 0.067 0.888 0.312 0.805 0.755 0.064 0.868 0.271 0.798 3.700 0.287 0.671 0.227 0.292 0.000 0.314 0.609 0.214 0.258 0.170 0.293 0.649 0.232 0.297 0.370 0.251 0.721 0.243 0.424 0. 0.276 0.660 0.213 0.161 0.140 0.075 0.913 0.289 0.883 2.520 0.170 0.872 0.280 0.801 2.480 0.184 0.820 0.271 0.675 3.110 0.066 0.877 0.309 0.836 0.880 0.044 0.897 0.287 0.906 7. 3.140 3.410 0.165 3.260 0.075 0.914 0.289 0.883 3.270 0.168 0.872 0.284 0.817 2. 0.184 0.820 0.269 0.653 3.110 0.066 0.880 0.310 0.841 0.980 0.044 0.899 0.287 0.906 7.300 3.480 0.090 0.906 0.291 0.864 3.920 0.168 0.881 0.284 0.816 3. 0.196 0.804 0.268 0.666 2.920 0.070 0.870 0.306 0.831 0.540 0.044 0.897 0.287 0.902 7.320 3.670 0.078 0.909 0.291 0.864 3.920 0.167 0.883 0.284 0.816 3. 0.184 0.804 0.268 0.666 3.310 0.070 0.883 0.307 0.838 0.540 0.044 0.898 0.288 0.902 7.780 3.840 A.4 Qualitative Cases In this section, we present additional qualitative examples from AURORA-BENCH in Figure 10. We observe several common failure modes in image editing models. First, they sometimes fail to preserve the scene from the source observation (e.g., PixInstruct on Action-Genome and MagicBrush). Second, some models generate near-identical copies of the source as the target (e.g., GoT on SomethingSomething). Third, producing realistic outputs remains difficult, as seen in GoTs result on Kubric. Finally, maintaining object consistency is also challengeSmartEdit alters the object in WhatsUp, and CWM does so in Something-Something. Despite the challenges, we also observe several positive editing behaviours from CWM. On ActionGenome, CWM correctly predicts spatial changes, such as opening and closing drawer, which requires strong understanding of the spatial concepts. In Something-Something, it is the only model to accurately capture the spatial concept of falling down. On Kubric, it demonstrates basic counting ability by correctly adding one keyboard. In WhatsUp, CWM correctly grounds the action to the laptop, while other models mistakenly edit the monitor. 15 Figure 10: Qualitative examples of the predicted next observation from the state-of-the-art specialised image editing models, and our models including C-FT and CWM, on AURORA-BENCH. A.5 Details of Processing CDM Annotations for Unlabelled Videos Algorithm 1 Stratified Top-K Sampling with Action Class Uniformity Require: Trajectory triplet set = {(oi t, ai, si, ci)}N s, oi ai, ci is the class, number of samples i=1, where si is the predicted likelihood of for all class in round-robin order do Xc top unsampled item from class in if Xc = then 1: Sort descending by score si 2: Initialize , and class_counts[c] 0 for all 3: while < do 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: end for 14: 15: end while 16: return S {Xc} Remove Xc from class_counts[c] class_counts[c] + 1 end if if = then break end if We present the raw dataset statistics before sampling for Movements-in-Time, UCF-101 and Kinetics700 in Table 4. Figure 11 shows the distribution of CDMs predicted scores across action classes in Movements-in-Time, Kinetics700, and UCF-101. The predicted likelihoods are nearly uniform 16 Figure 11: Distributions of triplet log-likelihoods predicted by CDM on Movements-in-Time, UCF101, and Kinetics-700, based on 7K synthetic triplets per dataset. Triplets are uniformly sampled from each action class while maximising overall predicted likelihoods. 17 Table 4: Dataset statistics for the video and triplets from the trajectories annotated by CDM. OPV: observations (i.e., extracted key-frames) per video, APV: actions per video, WPA: words per action. Dataset Video Triplet Avg. Length Total Length #Samples #Avg. OPV #Avg. APV #Avg. WPA MIT UCF-101 Kinetics700 3.04 seconds 7.24 seconds 9.02 seconds 2.57 hours 26 hours 18 hours 19,658 10,965 26, 2.05 3.00 2.71 1.05 2.00 1.71 7.10 8.96 7.39 within each class, indicating that our sampling method maintains both class diversity and high overall likelihoods. The sampling procedure for CDM-annotated trajectories is detailed in Algorithm 1. A.6 Prompt template for using GPT4o-as-a-Judge evaluation. We provide the prompts used for evaluating image editing performance with GPT-4o in Figure A.6. We use GPT-4o-2024-11-20. The final score is the average of the minimum value of the two scores for each sample, as in [30]. Prompt Template for GPT4o-as-a-judge Evaluation You are professional digital artist. You will have to evaluate the effectiveness of the AI-generated image(s) based on the given rules. You will have to give your output in valid way of Python dictionary format (Keep your reasoning concise and short.): {{\"score\": [...], \"reasoning\": \"...\" }} and dont output anything else. Two images will be provided: The first being the original AI-generated image The second being an edited version of the first. The objective is to evaluate how successfully the editing instruction has been executed in the second image. Note that sometimes the two images might look identical due to failure in image editing. From scale of 0 to 10: score from 0 to 10 will be given based on the success of the editing. 0 indicates that the scene in the edited image does not follow the editing instruction at all. 10 indicates that the scene in the edited image follows the editing instruction text perfectly. If the object in the instruction is not present in the original image at all, the score will be 0. second score from 0 to 10 will rate the degree of minimal editing in the second image. 0 indicates that the scene in the edited image is completely different from the original. 10 indicates that the edited image can be recognised as minimally edited yet effective version of the original. Put the score in list such that: output score = [score1, score2], where score1 evaluates the editing success and score2 evaluates the degree of the minimal editing. Editing instruction: {instruction} 18 A.7 Detailed GPT4o Scores for Editing Success and Minimal Editing (a) Detailed GPT4o scores for CWM trained with the standard loss. (b) Detailed GPT4o scores for CWM trained with the L2-weighted loss. Figure 12: GPT4o scores distributions of editing success (ES) and minimal editing (OE) for CWM trained with standard loss or our loss-weighting method. Figure 12 shows the distribution of editing success (ES) and minimal editing (ME) scores for standard training and loss-weighted training. Loss weighting tends to improve editing success, with modest trade-off in minimal editing quality in most of the datasets. A.8 Implementation Details A.8.1 Chameleon Dynamics Model We fine-tune the Chameleon-7B checkpoint from the Anole-7B version [51] to predict the action given pair of observations, framed as an action-prediction task. The model is trained on merged dataset from Action-Genome, Kubric, MagicBrush, Something-Something from AURORAs annotated trajectories, and 15K EPIC-Kitchens processed by us. We downsample Kubrics trajectories to 10K. Training is performed for 10 epochs with batch size of 64, using learning rate of 2e-4 and cosine scheduling (500 warm-up steps). We use bfloat16 mixed-precision training and apply LoRA [52] for parameter-efficient fine-tuning (rank 16, α = 32, dropout 0.05). Only the completion loss is used to optimise the generation of action. Training is conducted on 4 NVIDIA-H100-80GB-HBM3 GPUs using DeepSpeed for distributed optimisation. A.8.2 C-FT Baseline We fine-tune the Chameleon-7B checkpoint from the Anole-7B version [51]. The model is trained on combined dataset from Action-Genome, Kubric, MagicBrush, and Something-Something, formatted as the image editing task. We downsample Kubrics trajectories to 10K. Training is conducted for 40 epochs with batch size of 96 using the AdamW optimiser and cosine learning rate scheduler (learning rate of 5e-4, 400 warm-up steps). We use mixed-precision training with bfloat16 and apply LoRA [52] for efficient fine-tuning (rank 16, α = 32, dropout 0.05). We only train the model with the truncated loss from the completion part. We use 4 NVIDIA-H100-80GB-HBM3 GPUs with DeepSpeed for distributed training. During inference, we apply logits processor to mask out non-image tokens, set the temperature to 1, and use top-1 sampling. We observe that temperature is critical in controlling model behaviour: lower values often cause the model to copy the source observation instead of generating meaningful edits. 19 A.8.3 Chameleon World Model We fine-tune the Chameleon-7B checkpoint from the Anole-7B version [51]. The model is trained on combined dataset from Action-Genome, Kubric, MagicBrush, Something-Something from AURORAs annotated trajectories, together with 7K trajectories from Movements-in-Time, 7K trajectories from UCF-101 and 7K trajectories from Kinetics700, formatted as the image editing task. Again, we downsample Kubrics trajectories to 10K. Training is conducted for 40 epochs with batch size of 96 using the AdamW optimiser and cosine learning rate scheduler (learning rate of 5e-4, 400 warm-up steps). We use mixed-precision training with bfloat16 and apply LoRA [52] for efficient fine-tuning (rank 16, α = 32, dropout 0.05). We only train the model with the truncated loss from the completion part, but we weight the image tokens using L2 strategy as introduced in Section 3. We use 4 NVIDIA-H100-80GB-HBM3 GPUs with DeepSpeed for distributed training. We use the same hyperparameters as C-FT during the inference time. A.8.4 Computing Resources All training experiments were conducted on compute node equipped with 4 NVIDIA H100 80GB GPUs, 256 CPU cores, and 256GB of memory. The total GPU hours required for training C-FT, CWM, and CDM were approximately 200, 400, and 100 hours, respectively. For inference, we used single NVIDIA A100 80GB GPU with 8 CPU cores and 128GB memory. Inference for C-FT and CWM takes approximately 1 GPU hour per model. When applying verification with = 8, inference time increases to around 8 GPU hours. CDM only takes around 0.3 GPU hours for inference. A.8.5 Assets and Licenses In this section, we list the public assets we used in this paper and the corresponding links. Datasets. We include the detailed license and URL for the datasets we used in this paper. AURORA and AURORA-BENCH [18]: MIT license, the reader can find the corresponding version we use in this paper in https://github.com/McGill-NLP/AURORA. Movements-in-Time [20]: BSD-2-Clause license and its own License for Non-Commercial Use, the reader can find the corresponding version we use in this paper in http://moments. csail.mit.edu/. UCF-101 [23]: unknown license, the reader can find the corresponding version we use in this paper in https://huggingface.co/datasets/flwrlabs/ucf101. Kinetics700 [21, 22]: Creative Commons Attribution 4.0 International License, the reader can find the corresponding version we use in this paper in https://research.google/ pubs/the-kinetics-human-action-video-dataset/. EPIC-Kitchens [25]: Creative Commons Attribution-NonCommercial 4.0 International License, the reader can find the corresponding version we use in this paper in https: //epic-kitchens.github.io/. Implementation. We use the other following code for the implementations: Transformers [53]: Apache-2.0 license. We use the 4.47.0 version, following the link at https://github.com/huggingface/transformers. DeepSpeed: We use the 0.14.4 version, following the link at https://github.com/ deepspeedai/DeepSpeed. Model. We use the following models or checkpoints for the implementations: Chameleon [17]: Chameleon Research License, the reader can find the corresponding version we use in this paper in https://github.com/facebookresearch/chameleon. Anole-7B [51]: Chameleon Research License and MIT License, the reader can find the corresponding version we use in this paper in https://github.com/GAIR-NLP/anole. 20 Figure 13: The screenshot for the instructions given to participants and the interface developed for conducting the evaluation. VILA-U [51]: MIT License, the reader can find the corresponding version we use in this paper in https://github.com/mit-han-lab/vila-u. SmartEdit [31]: Apache-2.0, the reader can find the corresponding version we use in this paper in https://huggingface.co/TencentARC/SmartEdit-7B. GoT [30]: MIT License, the reader can find the corresponding version we use in this paper in https://github.com/rongyaofang/GoT. PixInstruct [29]: the reader can find the corresponding version we use in this paper in https://github.com/timothybrooks/ instruct-pix2pix. PixInstruct customised license, 21 A.9 Details of Human Evaluation We conducted human evaluation using custom-built interface, with the full interface and instructions shown in Figure 13. total of 14 participants were recruited, all of whom are PhD-level graduate students or higher. Participation was voluntary. Each participant was asked to evaluate 25 samples, which typically required 1520 minutes to complete. The evaluation process, including recruitment, instructions, and data processing and storage, followed our institutions ethical guidelines for human subject research. All participants were informed of the purpose of the study and provided consent. No personally identifiable information was collected, and all data were stored and analysed in accordance with privacy standards. A.10 Safeguards CWM performs observation prediction through image generation and, while its outputs are taskspecific, we acknowledge that any generative model may carry potential for misuse. To mitigate these risks, we commit to the following safeguards upon release: The model will be released solely for research purposes under license that prohibits commercial use or any other harmful applications. The GitHub repository will include clear usage guidelines and terms of use, aligned with responsible AI principles. We will include disclaimer that the model is intended only for academic research in controlled environments. The datasets used for training are publicly available, action-centric image editing benchmarks that do not include sensitive or personally identifiable content. Given the targeted nature of our model and the safeguards in place, we believe the risk of misuse is limited. Nonetheless, we encourage responsible use and welcome feedback from the community regarding potential improvements to safety."
        }
    ],
    "affiliations": [
        "Institute for Language, Cognition and Computation, University of Edinburgh",
        "Language Technology Lab, University of Cambridge",
        "NVIDIA"
    ]
}