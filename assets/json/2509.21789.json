{
    "paper_title": "Visual Multi-Agent System: Mitigating Hallucination Snowballing via Visual Flow",
    "authors": [
        "Xinlei Yu",
        "Chengming Xu",
        "Guibin Zhang",
        "Yongbo He",
        "Zhangquan Chen",
        "Zhucun Xue",
        "Jiangning Zhang",
        "Yue Liao",
        "Xiaobin Hu",
        "Yu-Gang Jiang",
        "Shuicheng Yan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multi-Agent System (MAS) powered by Visual Language Models (VLMs) enables challenging tasks but suffers from a novel failure term, multi-agent visual hallucination snowballing, where hallucinations are seeded in a single agent and amplified by following ones due to the over-reliance on textual flow to relay visual information. Through turn-, layer-, and token-wise attention analyses, we provide detailed insights into the essence of hallucination snowballing regarding the reduction of visual attention allocation. It leads us to identify a subset of vision tokens with a unimodal attention peak in middle layers that best preserve visual evidence but gradually diminish in deeper agent turns, resulting in the visual hallucination snowballing in MAS. Thus, we propose ViF, a lightweight, plug-and-play mitigation paradigm that relays inter-agent messages with Visual Flow powered by the selected visual relay tokens and applies attention reallocation to amplify this pattern. The experiment results demonstrate that our method markedly reduces hallucination snowballing, consistently improving the performance across eight benchmarks based on four common MAS structures and ten base models. The source code will be available at: https://github.com/YU-deep/ViF.git."
        },
        {
            "title": "Start",
            "content": "VISUAL MULTI-AGENT SYSTEM: MITIGATING HALLUCINATION SNOWBALLING VIA VISUAL FLOW Xinlei Yu1 Chengming Xu2 Guibin Zhang1 Yongbo He3 Zhangquan Chen4 Zhucun Xue3 Jiangning Zhang3 Yue Liao1 Xiaobin Hu1 Yu-Gang Jiang5 Shuicheng Yan1 1National University of Singapore 4Tsinghua University xinlei.yu@u.nus.edu Corresponding Author 2Tencent Youtu Lab 3Zhejiang University ben0xiaobin0hu1@nus.edu.sg 5Fudan University 5 2 0 S 6 2 ]"
        },
        {
            "title": "A\nM",
            "content": ". [ 1 9 8 7 1 2 . 9 0 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Multi-Agent System (MAS) powered by Visual Language Models (VLMs) enables challenging tasks but suffers from novel failure term, multi-agent visual hallucination snowballing, where hallucinations are seeded in single agent and amplified by following ones due to the over-reliance on textual flow to relay visual information. Through turn-, layer-, and token-wise attention analyses, we provide detailed insights into the essence of hallucination snowballing regarding the reduction of visual attention allocation. It leads us to identify subset of vision tokens with unimodal attention peak in middle layers that best preserve visual evidence but gradually diminish in deeper agent turns, resulting in the visual hallucination snowballing in MAS. Thus, we propose ViF, lightweight, plug-and-play mitigation paradigm that relays inter-agent messages with Visual Flow powered by the selected visual relay tokens and applies attention reallocation to amplify this pattern. The experiment results demonstrate that our method markedly reduces hallucination snowballing, consistently improving the performance across eight benchmarks based on four common MAS structures and ten base models. The source code will be available at: https://github.com/YU-deep/ViF.git."
        },
        {
            "title": "INTRODUCTION",
            "content": "MAS equipped with advanced VLMs are rapidly emerging as solution for complex tasks, such as collaborative reasoning, multi-turn instruction following, and sophisticated multi-modal understanding, by enabling agents to communicate and collaborate over multiple turns so as to tackle problems that are intractable for single model (Cemri et al., 2025; Li et al., 2025b). However, this collaboration also exposes fundamental reliability failure due to the problem of multi-agent visual hallucination snowballing, that is, visual misinterpretations or over-preference to textual messages in previous agents that are amplified as information flows through subsequent agents, producing propagatively hallucinated outputs about the visual contents and, ultimately, catastrophic hallucination snowballing. This introduces new reliability and effectiveness challenges in VLM-based MAS that can not be addressed by single-agent research. It is noteworthy that the visual hallucination snowballing phenomenon in MAS is essentially different from such problem discussed in the previous works (Zhang et al., 2024b; Zhong et al., 2024), given that the hallucination snowballing arises from two distinct but interacting mechanisms, as shown in Figure 1a: (1) intrinsic hallucination, where individual VLM-based agent produces erroneous textual descriptions or assertions about visual contents, and (2) hallucination propagation, where the over-reliance on textual information flow, i.e, the generated text, compresses and selectively emphasizes visual features, allowing surviving hallucinated assertions to be treated as authoritative by downstream agents. Since later agents typically accept prior textual context as strong evidence, early hallucinations are hence amplified rather than corrected, producing snowballing effect across turns. Due to the interaction between these two mechanisms, reducing per-agent hallucination alone, as focused by previous works (Wang et al., 2025; Tang et al., 2025b; Yin et al., 2025; Li et al., 2025c; Zou et al., 2025; Tang et al., 2025a), cannot fully solve the hallucination propagation problem, thus failing to prevent multi-agent hallucination snowballing. 1 (a) An example of multi-agent hallucination snowballing, and comparison of visual information relay between existing i) textual flow approaches and ii) our proposed visual flow method. (b) Maps of average visual attention allocation across agent turns. (c) Relations between visual attention allocation reduction and hallucination snowballing. Figure 1: Introduction to the multi-agent visual hallucination snowballing phenomenon: (a) presents an example illustrating how it happens; (b) and (c) specify the visual attention allocation reduction in different agent turns, potentially contributing to the occurrence of hallucination snowballing. To diagnose how multi-agent pipelines lose visual fidelity across turns, we first conduct set of preliminary analyses that dissect attention dynamics among turn-wise, layer-wise, and token-wise, through which we empirically conclude that the hallucination snowballing can be evident by the reduction of attention allocated to vision tokens over agent turns, as indicated in Figure 1b and Figure 1c. Moreover, vision tokens characterized by unimodal attention peak in middle layers, as small but vital subset of all vision tokens, can best preserve vision-specific information and whose removal most degrades visual understanding, thus being significant for enhancing the visual information flow among agents. Such token pattern, however, diminishes in deeper agent turns, implying the gradual dominance of textual information flow, leading to the hallucination snowballing. Motivated by these insights, we propose plug-and-play mitigation strategy for multi-agent hallucination snowballing dubbed as ViF. Instead of relying solely on textual flows, an additional visual flow is introduced to relay visual evidence by selecting subset of visual relay tokens and being contextualized by previous instructions, then engaging them in the process of following agents. Such design can provide downstream agents with preserved visual evidence that resists visual-to-text information loss, meanwhile preventing textual priors from entirely displacing visual signals during subsequent agent turns. In addition, an attention reallocation mechanism is introduced to amplify the ideal attention patterns and preserve visual contributions into deeper agent turns. We evaluate ViF across eight benchmarks covering both comprehensive and hallucination tasks, demonstrating its striking effectiveness in alleviating hallucination snowballing in four different structures and ten base VLMs. Overall, our contributions are summarized as follows: We formalize the multi-agent visual hallucination snowballing phenomenon and systematically link it to visual attention degradation in deeper agent turns. We provide extensive analyses that identify subset of vision tokens that are critical for relaying visual information flow. We introduce ViF, plug-and-play method that optimizes inter-agent visual messages with visual flows and an attention reallocation mechanism to augment attention patterns. Comprehensive experiments validate the efficacy of our ViF to reduce hallucination snowballing, and additional analyses provide more convincing evidences."
        },
        {
            "title": "2 REQUISITE ANALYSES",
            "content": "As mentioned in Section 1, the hallucination snowballing can be presented by the negative correlation with the attention allocation to vision tokens. Quantitatively, as shown in Fig. 1c, the average attention allocation to vision tokens reduces from 0.165 to 0.099 at the 10th agent turn, and further to 0.063 at the 20th turn, with total 62% reduction. Furthermore, the reduction in the middle layer (-60%) is much more remarkable than that in the first (-21%) and last (-30%) layers. For more thorough understanding, we conduct extensive requisite analyses among various VLMs. For simplicity, we mainly focus on LLaVA-NeXT-7B (Liu et al., 2024b) on the POPE (Li et al., 2023) benchmark in the main paper, while Appendix provides more detailed settings and comprehensive results on six VLMs to support the generalization ability of our claims, from which several insights are derived, thus leading to our research motivations."
        },
        {
            "title": "2.1 ANALYTICAL EXPERIMENTS",
            "content": "Layer-Wise Attention Allocation in Different Agent Turns. To find out the underlying cause of visual hallucination snowballing in MAS, we begin by measuring the trend of layer-wise attention allocation among different agent turns. In VLMs with multi-modal architectures, the decoder dynamically allocates attention to three types of textual tokens (instruction, system and output tokens) and one visual token that produced by visual encoder. Other special tokens, such as start and end signals of visual input, are negligible and thus excluded. Figure 2: Layer-wise attention allocation of four tokens in different agent turns. As depicted in Figure 2, in general, when the agent turns increase, vision tokens receive gradually decreasing attention in all layers, while the attention being directed towards instruction tokens is raised accordingly; the attention allocations to system token and output token are relatively stable, without discernible trend of change. Focusing further on the middle layers of visual and instruction tokens, which are zoomed in, the opposite trend between visual and instruction tokens is more pronounced than other layers. In the first agent turn, scenario equivalent to single-agent setting, there exists an obvious unimodal morphology peak in vision attention, and allocation to instruction is reduced conversely. However, in the 20th turn, the vision attention peak has almost disappeared and evolved into fluctuation, and is redistributed to instruction tokens. Based on previous research (Yin et al., 2025; Zhang et al., 2025c) that textual and visual information are mainly fused and interacted in these layers, this tendency of attention in MAS suggests that agents in later turns tend to largely ignore the vision tokens and over-rely on the instruction tokens, including visual contents relayed by textual output from previous agents. This preference for textual tokens, however, partially leads to the multi-agent hallucination snowballing. The previous agent may experience visual-to-text information loss and potential cognitive bias when relaying visual evidence through textual flow. Conversely, vision tokens, as the initial visual semantic carrier, contain native and unbiased visual messages, which reduces the potential for hallucinations when relaying visual information. Based on these observations, we hypothesize: Can subset of vision tokens, acting as visual flow, directly relay visual information across agent turns? Dropping Subsets of Vision Tokens in Different Layers. To intuitively verify the hypothesis, we ablate specific subsets of vision tokens in shallow/middle/deep layers (implementation modified from (Zhang et al., 2025b)) and compare the corresponding performance degradation. We choose five subsets of vision tokens to ablate: (1) Random Tokens: randomly select tokens from the whole vision token set and maintain relatively uniform distribution in the image. (2) Inactive Tokens: 3 Table 1: Results of dropping vision token subsets in the shallow, middle, and deep layers. 25% Shallow Layers 75% 50% 100% 25% Middle Layers 75% 50% 85. 100% 25% Deep Layers 75% 50% 100% w/o Dropping (a) Random 51.8 33.4 44.5 40.7 38.4 46.8 30.5 54.7 78.9 6.3 66.1 19.1 62.7 22.5 59.0 26.2 84.4 0.8 83.2 2.0 82.9 2.3 82.6 2.6 55.1 30.1 46.2 39.0 41.8 43.4 32.5 52.7 84.3 0.9 82.9 2.3 81.5 3.7 78.3 6.9 85.0 0.2 84.6 0.6 84.3 0.9 84.6 0.6 (b) Inactive 41.9 43.3 35.6 49.6 29.6 55.6 20.8 64.4 79.4 5.8 64.2 21.0 56.4 28.8 52.3 32.9 83.6 1.6 82.7 2.5 81.8 3.4 81.6 3.6 (c) Rise 41.6 43.6 38.8 46.4 30.7 54.5 22.5 62.7 78.3 6.9 64.8 20.4 58.5 26.7 52.9 32.3 84.1 1.1 82.8 2.4 82.0 3.2 82.4 2.8 (d) Fall (e) Unimodal 42.1 43.1 37.6 47.6 30.0 55.2 22.8 62.4 52.9 32.3 44.5 40.7 36.6 48.6 25.3 59.9 84.4 0.8 83.0 2.2 82.3 2.9 81.8 3. Figure 4: The demonstrations of selected unimodal vision tokens in various cases. select the tokens with constantly low attention and tiny fluctuation. (3) Rise Tokens and (4) Fall Tokens: select tokens allocated with gradually upward or downward trend of attention. (5) Unimodal Tokens: select tokens allocated with an unimodal attention peak. In terms of the unimodal tokens, we introduce parameter ω to regulate the salience of the attention peak. As listed in Table 1, the ablation of vision tokens leads to varying degrees of performance degradation across layers. In shallow layers, all vision tokens are necessary for the visual understanding capacity; even ablating one-quarter of the tokens from any subset causes over 30% loss. On the contrary, in the deep layers, vision tokens play an insignificant role, with performance reduction of less than 1% even when dropping all inactive vision tokens. Figure 3 further highlights that, compared to the results of other subsets, the dropping of unimodal tokens in middle layers leads to more significant performance degradation. Specifically, the decrease from this subset is almost three times that of other subsets when dropping one-quarter of the tokens, and about twice when dropping half, three-quarters, and all tokens. In conclusion, the ablation study reveals that in shallow and deep layers, all vision tokens are almost equally important or unimportant; however, in the middle layers, vision tokens with unimodal morphology play much more crucial role in interaction information between vision and text tokens. Figure 3: Performance when dropping different vision token subsets in middle layers. Investigation of Unimodal Tokens. To validate our previous hypothesis that certain vision tokens can act as visual flow for relaying visual information, we visualize the unimodal vision tokens in various cases and track their ratios across agent turns. As demonstrated in Figure 4, we choose two images, each with three distinct questions, as examples. The selected tokens are highly semantically relevant and contain very few other irrelevant tokens. Besides, as depicted in Figure 5, the proportion of unimodal vision tokens continuously declines from 1.22% at the first agent turn to 0.10% at the 20th agent turn, while the percentages of the other two tokens slightly increase. The downward trend of the unimodal token proportion aligns with visual attention allocation among agent turns, which suggests that the reduction of unimodal vision tokens contributes significantly to the onset of hallucination snowballing through the disappearance of the visual attention peak. Thus, we believe that the subset of unimodal vision tokens could meet the hypothesis to relay visual evidence as visual flow. It ensures that semantically relevant visual messages can be relayed sufficiently, while avoiding carrying substantial irrelevant vision tokens with high efficiency. Figure 5: Proportions of vision tokens subsets in different agent turns. 2."
        },
        {
            "title": "INSIGHTS",
            "content": "Based on experimental results and analyses, three significant insights can be summarized: The visual evidence relayed in MAS, which is typically via textual flow, potentially results in multi-agent hallucination snowballing. When the agent turns increase, the average attention allocated to vision tokens reduces, and the attention peak in middle layers diminishes, while attention to instruction tokens increases accordingly; system and output tokens receive relatively stable attention. In middle layers, vision tokens with unimodal attention allocation relay visual information; all vision tokens are significant in shallow layers and less significant in deep layers."
        },
        {
            "title": "3 METHODOLOGIES",
            "content": "Building on the insights from the previous section, we propose straightforward and efficient plugand-play method named ViF to mitigate hallucination snowballing in VLM-based MAS. As demonstrated in Figure 6, our proposed method involves relaying the visual information from the previous agent via selected subset of vision tokens, and reallocating attention in middle and deep layers to facilitate this process. Besides, we provide suitable alternative for attention score based token selection, since in some recently released models with Flash-Attention 2/3 (Dao, 2024), the attention scores are not accessible."
        },
        {
            "title": "3.1 VISUAL INFORMATION RELAY",
            "content": "Leveraging the previous insights, we employ the unimodal vision tokens as additional visual flow to relay information from the previous agent. Specifically, we token-wise decompose the vision tokens = {v1, . . . , vm} according to the trend of the attention allocation in the middle layers, and select the vision tokens with unimodal morphology as initial visual relay tokens = {r1, . . . , rn} V, where m. However, the original selected tokens are semantically irrelevant, which are only tokenized by the vision encoder, without particular semantics. Thus, we contextualize the initial visual relay tokens with the instruction tokens as follows: (cid:98)R = (R I) [: n] , (1) where () is lightweight transformer block (Mehta et al., 2021), denotes concatenation. Here, we extract the former component to maintain the initial length of visual relay tokens (cid:98)R. To retain the spatial information of visual relay tokens, we apply the same positional encoding strategy as the previous agent. Then, the visual relay tokens will be transmitted to the subsequent agent, which will be inserted between the original vision tokens and instruction tokens, and be fed to the final LLM together with other tokens."
        },
        {
            "title": "3.2 ATTENTION REALLOCATION",
            "content": "Considering the insights that tokens are of different significance in the shallow, middle, and deep layers respectively, we reallocate attention to optimize attention patterns. Our objectives are to 5 Figure 6: Overview of our proposed ViF, including the generation of visual relay tokens and attention reallocation to alleviate multi-agent hallucination snowballing. activate the visual relay tokens and to optimize the distribution of attention among various tokens. Therefore, we amplify dynamic trends, both the upward and downward, of visual attention in the middle layers by adding temperature scaling to the Softmax operation in the middle layers: = Sof tmaxτ (S) = exp (cid:0) (cid:1) i=1 exp (cid:0) si τ τ (cid:80)m (cid:1) , (2) where τ is temperature parameter, and are attention score matrix and attention score, and is attention matrix. It promotes the emergence of vision tokens with unimodal morphology. Besides, in the middle layers, we collect the attention of inactive vision tokens and instruction tokens, and then reallocate the collected attention to other vision tokens, which is formulated as: i=1 si Mc, Mc (i, j) = ((i , V) (i , I)) , = α(cid:80)m (3) ˆs = + i=1 si (cid:80)l Mr, Mr (i, j) = (i , V) , (4) where Mc and Mr are the collection and reallocation mask matrices respectively, α is the reallocation coefficient, is the whole token set, is inactive vision token set, and = = {v1, . . . , vl}. During the reallocation, the sum of the total attention is always 1. Additionally, in the deep layers, the reallocation is from vision tokens to instruction tokens, with the same process."
        },
        {
            "title": "3.3 ALTERNATIVE OF ATTENTION SCORE BASED STRATEGY",
            "content": "To accelerate the computation and reduce memory, flash-attention (Dao, 2024) mechanisms are Inspired widely used in recently released models, making the attention scores not obtainable. by (Wen et al., 2025), we design Key-Norm (L2 norm of the key matrix) based alternative for the original attention score based method. More discussions about the alternative are in Appendix C.1."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "We conduct experiments on three comprehensive benchmarks: MME (Yin et al., 2024), MMBench (Liu et al., 2024d), MM-Vet (Yu et al., 2024), and five visual hallucanation benchmarks: CHAIR (Rohrbach et al., 2018), POPE (Li et al., 2023), AMBER (Wang et al., 2023), MMHalBench (Sun et al., 2023), HallBench (Guan et al., 2024). For detailed experimental settings, configurations, and additional results, please refer to Appendix D.1."
        },
        {
            "title": "4.1 MAIN RESULTS",
            "content": "Performance on Comprehensive and Hallucination Benchmarks. For comprehensive assessments of our proposed ViF, we first compare the results on six base VLMs, namely, LLaVA-v1.57B (Liu et al., 2024a), LLaVA-v1.6-7B, LLaVA-NeXT-7B (Liu et al., 2024b), LLaVA-OneVision7B (Li et al., 2025a), Qwen2-VL-7B (Wang et al., 2024a), and Qwen2.5-VL-7B (Bai et al., 6 Table 2: Results across eight comprehensive and hallucination benchmarks. indicates implementation with Key-Norm, while others use attention scores. The best and the second best values with our method are bolded and underlined respectively, and the rightmost column shows the average results. For identical values, we compare the following digit after the decimal point."
        },
        {
            "title": "Base Agent",
            "content": "MME MM Bench MM-Vet CHAIR POPE AMBER MMHalHall Bench Bench Avg."
        },
        {
            "title": "Linear",
            "content": "Layered Random Circular 52.4 68.7 32. 85.2 36.9 66.1 LLaVA-v1.5-7B +Ours LLaVA-v1.6-7B +Ours LLaVA-NeXT-7B +Ours LLaVA-OneVision-7B 1587.6 +Ours Qwen2-VL-7B +Ours Qwen2.5-VL-7B +Ours 1516.2 86.7 1531.8 15.6 67.8 1.7 34.6 2.2 51.7 0.7 88.6 1.9 87.8 2.6 42.8 2.3 50.6 2.6 3.5% 1511.7 86.7 1524.3 12.6 69.4 0.7 38.2 1.3 43.6 1.9 88.2 1.5 91.5 3.0 46.0 2.6 54.7 2.1 3.1% 1567.0 88.6 1585.2 18.2 73.5 1.4 49.3 2.2 42.9 1.7 90.4 1.8 89.3 2.3 48.1 2.2 55.3 2.4 3.2% 91.4 1598.8 11.2 83.4 1.1 59.9 1.5 37.2 1.1 93.0 1.6 93.9 2.6 49.6 1.8 56.1 2.4 2.6% 1686.4 90.5 1699.6 13.2 82.4 0.5 65.2 1.9 37.8 0.6 91.7 1.2 94.0 2.8 50.2 2.0 54.4 2.5 2.4% 89.9 1730.4 1746.2 15.8 85.3 1.4 69.1 1.8 37.6 1.0 91.4 1.5 94.4 1.6 53.7 2.1 56.2 2.4 2.5% 67. 92.8 91.3 72.1 88.5 87.0 47. 81.9 82.3 58.4 83.9 63.3 91. 51.9 38.6 53.8 51.6 48.0 40. 52.6 45.5 44.6 43.4 45.9 38. 53.7 52.9 47.8 48.2 38.4 49. 66.6 83.5 30.6 35.1 63.6 LLaVA-v1.5-7B +Ours LLaVA-v1.6-7B +Ours LLaVA-NeXT-7B +Ours LLaVA-OneVision-7B 1584.2 +Ours Qwen2-VL-7B +Ours Qwen2.5-VL-7B +Ours 1512.5 86.0 1520.9 8.4 64.7 1.1 32.0 1.4 49.3 0.3 87.6 1.6 86.1 2.6 43.7 2.4 48.9 2.3 2.7% 1508.0 86.8 1518.9 10.9 68.4 1.8 37.5 2.4 42.7 1.5 87.9 1.1 87.1 1.9 43.6 1.6 50.4 2.2 3.2% 1555.2 87.0 1571.3 16.1 70.7 1.5 46.5 2.5 42.5 1.7 89.2 2.2 87.7 2.1 47.2 2.5 51.6 2.1 3.5% 89.9 1596.7 12.5 82.0 1.4 59.5 1.6 36.4 1.2 91.5 1.6 90.9 1.8 46.9 1.7 54.5 1.8 2.5% 1679.0 88.6 1692.6 13.6 80.6 1.3 63.4 1.9 37.1 0.6 90.5 1.9 90.6 2.1 48.1 2.2 50.9 1.8 2.5% 1722.5 87.5 1737.0 14.5 82.4 1.4 63.8 1.7 36.0 0.8 89.6 2.1 91.7 1.6 49.3 2.3 53.2 2.0 2.6% 85.2 46.6 41.3 48.2 44. 42.0 89.1 79.3 57.9 80.6 81. 88.5 61.5 62.1 90.1 45.2 37. 37.6 52.7 49.1 45.9 36.8 51. 47.0 69.2 44.0 85.6 44.2 49. 44.7 49.4 69.0 33.1 67.1 36. 89.0 LLaVA-v1.5-7B +Ours LLaVA-v1.6-7B +Ours LLaVA-NeXT-7B +Ours LLaVA-OneVision-7B 1590.1 +Ours Qwen2-VL-7B +Ours Qwen2.5-VL-7B +Ours 1519.6 88.3 1537.6 18.0 68.4 1.3 34.7 1.6 49.8 0.4 90.2 1.9 92.2 3.2 47.0 2.4 55.0 2.2 2.8% 1519.8 88.6 1534.4 14.6 69.7 0.7 38.0 1.1 41.8 2.1 89.7 1.1 94.0 2.7 46.8 2.5 57.3 2.4 3.1% 1576.2 90.4 1596.1 19.9 75.3 2.3 50.1 0.9 41.6 1.8 93.0 2.6 92.9 3.7 49.3 2.1 58.4 3.0 3.5% 92.5 1605.2 15.1 85.1 1.4 59.7 1.2 37.1 0.6 94.1 1.6 94.9 3.0 48.5 2.2 59.1 2.9 2.6% 1690.1 90.8 1703.1 13.0 84.9 0.8 65.2 0.8 37.2 0.9 92.7 1.9 95.4 3.1 48.4 1.9 56.3 2.6 2.5% 1737.7 90.4 1756.8 19.1 88.4 1.9 68.1 1.1 37.0 0.8 92.8 2.4 95.8 2.2 50.6 1.7 59.5 2.7 2.5% 91.9 91.3 73. 49.2 89.2 84.1 83.7 58.5 86. 92.3 64.4 67.0 93.6 54.9 44. 52.8 43.9 44.3 43.4 55.4 37. 47.2 53.7 46.3 56.2 38.1 46. 48.9 37.8 56.8 52.7 69.7 37. 88.7 67.9 33.5 LLaVA-v1.5-7B +Ours LLaVA-v1.6-7B +Ours LLaVA-NeXT-7B +Ours LLaVA-OneVision-7B 1592.8 +Ours Qwen2-VL-7B +Ours Qwen2.5-VL-7B +Ours 88.9 1520.9 1539.1 18.2 68.4 0.5 36.0 2.5 51.3 1.4 90.4 1.5 91.6 2.9 44.7 2.3 54.1 2.4 3.4% 1519.5 88.5 1537.1 17.6 71.3 1.6 39.3 1.6 40.7 2.0 90.1 1.6 93.8 2.6 46.0 2.6 56.1 2.3 3.5% 1580.5 91.0 1599.5 19.0 74.6 1.4 51.8 2.3 41.2 1.8 93.3 2.3 92.7 3.3 51.1 3.2 55.7 2.6 3.8% 92.8 1606.1 13.3 84.6 0.6 60.2 1.1 36.9 1.8 94.0 1.2 95.0 2.8 49.4 2.1 56.9 2.3 2.7% 1692.8 91.6 1706.3 13.5 84.1 0.8 65.1 1.2 37.2 0.9 93.3 1.7 94.8 2.0 50.2 2.5 54.6 2.2 2.4% 1738.1 91.3 1756.2 18.1 86.6 1.4 68.1 1.4 37.4 0.8 93.4 2.1 95.9 2.4 52.5 2.4 57.3 2.4 2.6% 73. 91.2 92.2 83.3 89.4 49.5 85. 84.0 59.1 92.8 63.9 93.5 66. 43.0 51.7 42.4 43.4 42.7 53. 53.1 47.9 54.6 38.7 38.1 47. 54.9 47.7 52.4 38.2 50.1 Table 3: Results of larger-size models on circular MAS structure. Base Agent MME MM Bench MM-Vet CHAIR POPE AMBER MMHalHall Bench Bench Avg. 40.8 38.3 70.2 LLaVA-1.5-13B +Ours LLaVA-NeXT-13B 1583.5 +Ours LLaVA-NeXT-34B 1644.9 +Ours Qwen2.5-VL-32B 1886.1 +Ours 1528.7 90.0 1547.6 18.9 71.1 0.9 40.5 2.2 39.1 1.7 92.4 2.4 92.7 3.1 47.2 2.5 55.3 2.4 3.6% 91.9 1602.6 19.1 70.1 1.3 44.5 2.2 34.2 1.8 93.7 1.8 95.4 3.0 50.8 2.6 56.8 2.5 3.6% 91.4 1670.8 25.9 80.9 2.3 57.0 2.4 25.4 2.2 93.6 2.2 96.3 2.2 52.4 3.5 57.8 2.8 4.4% 92.5 1906.2 20.1 89.2 1.8 71.9 2.1 22.3 2.1 94.0 1.5 96.7 2.7 55.1 3.0 60.1 3.2 4.1% 54. 68.8 42.3 78.6 87.4 69.8 36. 89.6 52.9 94.1 44.7 48.2 92. 94.0 55.0 54.3 27.6 48.9 56. 24.4 52.1 2025). And we choose four common MAS structures, including linear (Hong et al., 2024), layered (Ishibashi & Nishimura, 2024), random (Qian et al., 2025), and circular (Qian et al., 2025) structures. As demonstrated in Table 2, our ViF consistently enhances the average performance of the six baselines by 2.4-3.8%, verifying the compatibility of our plug-and-play method on various MAS structures based on arbitrary base VLMs. Notably, on the MMHal-Bench and HallBench benchmarks, which are more sophisticated and have unsatisfactory baseline performance, our ViF achieves over 4% average improvement. When applied to the circular structure, which is 7 Table 5: Comparison results of other SOTA methods and ours on LLaVA-NeXT-7B and circular MAS structure. Orig. represents the original evaluation metric, and HS is our proposed one. CHAIR Orig. Baseline 43.0 HS 18.9 POPE Orig. 91.0 HS 29.1 AMBER Orig. 89.4 HS 31.1 MMHal-Bench HS Orig. HallBench Orig. HS Avg. Orig. HS 47.9 40. 53.1 47.4 MemVR 43.8 0.8 20.6 1.7 90.5 0.5 31.2 2.1 88.9 0.5 34.4 3.3 44.8 3.1 58.6 17.8 49.2 3.9 57.6 10.2 2.6% 18.4% 1.1% 3.1% VISTA 43.4 0.4 19.0 0.1 91.2 0.2 27.8 1.3 90.5 1.1 28.3 2.8 46.3 1.6 47.4 6.6 50.7 2.4 53.3 5.9 0.5% 5.4% FarSight 42.1 0.9 17.7 1.2 91.9 0.9 22.7 6.4 91.0 1.6 26.6 4.5 47.4 0.5 42.9 2.1 51.9 1.2 52.4 5.0 1.0% 3.8% 42.6 0.4 18.2 0.7 91.3 0.3 25.1 4.0 91.6 2.2 24.3 6.8 47.0 0.9 44.1 3.3 50.4 2.7 53.0 5.6 DeCo 1.6% 3.7% 42.1 0.9 18.8 0.1 91.4 0.4 22.8 6.3 91.9 2.5 22.7 8.4 46.5 1.4 47.8 7.0 49.9 3.2 53.8 6.4 TAME Ours 41.2 1.8 12.8 6.1 93.3 2.3 17.0 12.1 92.7 3.3 17.7 13.4 51.1 3.2 24.1 16.7 55.7 2.6 27.8 19.6 3.8% 39.8% Table 4: Multi-agent hallucination snowballing evaluations on LLaVA-NeXT-7B with proposed HS metric. Table 6: Ablation study on LLaVA-NeXT-7B and circular MAS structure, verifying the effectiveness of visual relay tokens and attention reallocation. MAS Structure Linear Layered Circular Random CHAIR POPE AMBER MMHalHall Bench Bench Avg. 17.2 12.4 4.8 16.4 9.3 15.9 10.9 22.6 12.7 24.8 15.4 35.8% 26.8 25.7 35.3 40.2 12.7 10.6 2.1 13.9 7.6 13.1 7.4 19.5 12.1 21.3 15.1 33.6% 21. 20.5 31.6 36.4 18.9 12.8 6.1 17.0 12.1 17.7 13.4 24.1 16.7 27.8 19.6 39.8% 29.1 31. 40.8 47.4 15.5 10.3 5.2 15.0 8.4 16.4 7.4 21.2 15.6 25.6 16.9 36.5% 23.4 23.8 36. 42.5 Setting CHAIR POPE AMBER MMHalHall Bench Bench w/o Relay Token (25%) w/o Relay Token (50%) w/o Relay Token (75%) 41.4 (+0.2) 92.9 (-0.4) 92.3 (-0.4) 50.7 (-0.4) 55.2 (-0.5) 42.3 (+1.1) 92.0 (-1.3) 91.6 (-1.1) 49.8 (-1.3) 54.8 (-0.9) 42.6 (+1.4) 91.7 (-1.6) 91.1 (-1.6) 49.1 (-2.0) 54.1 (-1.6) w/o Reallocation (Middle) 41.7 (+0.5) 92.1 (-1.2) 91.4 (-1.3) 49.9 (-1.2) 54.4 (-1.3) w/o Reallocation (Deep) 41.4 (+0.2) 92.7 (-0.6) 92.5 (-0.2) 50.9 (-0.2) 55.0 (-0.7) w/o Reallocation 42.2 (+1.0) 91.9 (-1.4) 91.5 (-1.2) 49.6 (-1.5) 54.2 (-1.5) Ours 41. 93.3 92.7 51.1 55.7 hallucination-concentrated with densest collaborations and interactions among agents, our ViF dramatically reduces hallucination snowballing and further improves the performance by 3% among the six base models, compared to the other three selected structures. As reported in Table 3, we also analyze the performance of our proposed ViF on the scaled-up models with higher parameters. It is observed that when equipped with our ViF, the larger base models featuring more than 30B parameters, e.g., LLaVA-NeXT-34B and Qwen-2.5-VL-32B, exhibit greater enhancement than all smaller ones, improving by more than 4% across all benchmarks. This indicates that our plug-and-play method effectively improves their comprehensive performance, likely because larger-parameter baselines possess stronger fundamental capabilities, and our approach specifically unlocks their latent potential in multi-agent scenarios. Multi-Agent Hallucination Snowballing Mitigation. In addition to the results of original metrics, we attempt to assess the level of hallucination snowballing in MAS quantitatively. Thus, we formally define hallucination snowballing score (HS) as in Equation 7, measuring both the hallucination level and propagation in MAS. As reported in Table 6, adding our ViF reduces at least 30% HS score on the average of five hallucination benchmarks, significantly mitigating the hallucination propagation from the textual flow of visual contents. Notably, the layered structure suffers the least from the detrimental snowballing, while in circular structure, where the initial hallucination snowballing is the most serious, the reduction of the score from our method is almost 40%. Comparison Results. We compare the results of another five plug-and-play and token-wise hallucination mitigation methodologies in multi-agent contexts, i.e., DeCo (Wang et al., 2025), FarSight (Tang et al., 2025b), VISTA (Li et al., 2025c), MemVR (Zou et al., 2025), and TAME (Tang et al., 2025a). These counterparts restrict the deepening of intrinsic hallucinations in single model to some extent, however, in multi-agent scenarios, the propagation of visual contents via textual flow still introduces vision-to-text cognitive bias and fails to restrain the snowballing of multi-agent hallucinations commendably. As shown in Table 5, our introduced ViF approach achieves distinctly superior performance among all the benchmarks on both their original metrics and our proposed HS score. It obtains at least 4.2% enhancements in original metrics and 34.4% in HS score on average, compared to other methods tailored for single model hallucination mitigation. Although these counterparts are impressively efficacious in single VLM, their performance is compromised when applied to MAS, because of the failure to deal with hallucination propagation among agents and further snowballing. Surprisingly, in MAS environments, the results of these counterparts are even inferior to the baseline, especially 8 on challenging ones. This counterintuitive observation is likely because they modify the initial paradigm of decoding or attention in VLMs, but retain the textual flow to relay visual information, which amplifies the preference for text over vision tokens. Our method, adopting visual flow to relay information among agents, cuts the HS score almost in half and delivers tangible improvements in the mitigation of hallucination snowballing."
        },
        {
            "title": "4.2 ADDITIONAL ANALYSES",
            "content": "Ablation Studies. To verify the effectiveness of each component in our ViF, we perform ablations on the visual relay tokens and the attention reallocation. As reported in Table 6, the improvement from visual flow to relay information is prominent, and the results are still better than most comparison methods even when ablating half of the visual relay tokens, showcasing excellent robustness. Additionally, the reallocation mechanism further optimizes the attention distribution among different tokens and activates visual relay tokens, which is beneficial to our designs of visual relay flow. Impact of the Number of Agent Turns. To achieve satisfactory completion, typically, MAS necessitates greater number of agent turns in more complicated and challenging tasks. However, the hallucination snowballing effect restricts the multi-turn collaboration among agents, where hallucinations are amplified and propagated, leading to suboptimal performance. Therefore, we compare our ViF with baselines and other counterparts to assess the impact of the number of agent turns. (a) MMHal-Bench (b) HallBench (c) MMHal-Bench (d) HallBench Figure 7: Impact of the number of agent turns. In (a) and (b), straight and dashed lines are the results with or without our ViF on various baselines and circular MAS structure, respectively. (c) and (d) show the results between other counterparts and our method based on LLaVA-NeXT-7B. As demonstrated in Figure 7, our method maintains an upward trend in performance as the number of agent turns increases, while both other contrast methods and baselines experience performance degradation instead. More precisely, when the agent turn is set to one, which is equivalent to single-agent context, ViF exhibits only marginal improvement over the baselines, and falls behind some other methods designed for hallucination mitigation in single model. As the turning trends of the baselines in Figure 7a and 7b show, the performance begins to deteriorate when the turns are only increased to 5, and at the 20th turn, their performance is even further less than that of single agent. Further compare with other methods as illustrated in Figure 7c and 7d, although hallucinations are mitigated in early turns to some extent, the hallucination snowballing phenomenon still suffers in later turns, essentially limiting the multi-agent collaboration and inhibiting the potential of MAS."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We unveil the phenomenon of multi-agent visual hallucination snowballing existing in MAS, where subsequent agents progressively amplify errors originating in single agent through textual information flow that relays visual messages. Based on extensive analyses, the essence of hallucination snowballing lies in subset of vision tokens with an unimodal attention peak, well-preserving the visual information, but these tokens gradually diminish with the increase in the agent turns. To alleviate this problem, plug-and-play method named ViF is proposed, which redefines the visual information flow in MAS. Specifically, we introduce visual flow to relay visual messages based on the selected unimodal vision tokens and utilize attention reallocation to optimize this pattern. Comprehensive experiments indicate that this novel paradigm is effective, robust, and compatible, paving the way for more efficient inter-agent visual information relay and more sophisticated MAS."
        },
        {
            "title": "REFERENCES",
            "content": "Dosovitskiy Alexey. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv: 2010.11929, 2020. Wenbin An, Feng Tian, Sicong Leng, Jiahao Nie, Haonan Lin, QianYing Wang, Guang Dai, Ping Chen, and Shijian Lu. Agla: Mitigating object hallucinations in large vision-language models with assembly of global and local attention. arXiv preprint arXiv:2406.12718, 2024. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Mert Cemri, Melissa Pan, Shuyi Yang, Lakshya Agrawal, Bhavya Chopra, Rishabh Tiwari, Kurt Keutzer, Aditya Parameswaran, Dan Klein, Kannan Ramchandran, et al. Why do multi-agent llm systems fail? arXiv preprint arXiv:2503.13657, 2025. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024a. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning In Proceedings of the IEEE/CVF Conference on Computer for generic visual-linguistic tasks. Vision and Pattern Recognition (CVPR), pp. 2418524198, 2024b. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In The Twelfth International Conference on Learning Representations (ICLR), 2024. Timothee Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers. In The Twelfth International Conference on Learning Representations (ICLR), 2024. Xuan Gong, Tianshi Ming, Xinpeng Wang, and Zhihua Wei. DAMRO: Dive into the attention mechanism of LVLM to reduce object hallucination. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 76967712, Miami, Florida, USA, November 2024. Association for Computational Linguistics. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for In Proentangled language hallucination and visual illusion in large vision-language models. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1437514385, 2024. Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jurgen Schmidhuber. Metagpt: Meta programming for multi-agent collaborative framework. In The Twelfth International Conference on Learning Representations (ICLR), 2024. Shengding Hu, Yuge Tu, Xu Han, Ganqu Cui, Chaoqun He, Weilin Zhao, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Xinrong Zhang, Zhen Leng Thai, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, dahai li, Zhiyuan Liu, and Maosong Sun. MiniCPM: Unveiling the potential of small language models with scalable training strategies. In First Conference on Language Modeling (COLM), 2024. Fushuo Huo, Wenchao Xu, Zhong Zhang, Haozhao Wang, Zhicheng Chen, and Peilin Zhao. Selfintrospective decoding: Alleviating hallucinations for large vision-language models. In The Thirteenth International Conference on Learning Representations (ICLR), 2025. 10 Yoichi Ishibashi and Yoshimasa Nishimura. Self-organized agents: llm multi-agent framework toward ultra large-scale code generation and optimization. arXiv preprint arXiv:2404.02183, 2024. Seongyun Lee, Sue Hyun Park, Yongrae Jo, and Minjoon Seo. Volcano: Mitigating multimodal hallucination through self-feedback guided revision. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (ACL: Long Papers), pp. 391404, Mexico City, Mexico, June 2024. Association for Computational Linguistics. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. Transactions on Machine Learning Research (TMLR), 2025a. ISSN 2835-8856. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 292305, Singapore, December 2023. Association for Computational Linguistics. Yubo Li, Xiaobin Shen, Xinyu Yao, Xueying Ding, Yidi Miao, Ramayya Krishnan, and Rema Padman. Beyond single-turn: survey on multi-turn interactions with large language models. arXiv preprint arXiv:2504.04717, 2025b. Zhuowei Li, Haizhou Shi, Yunhe Gao, Di Liu, Zhenting Wang, Yuxiao Chen, Ting Liu, Long Zhao, Hao Wang, and Dimitris N. Metaxas. The hidden life of tokens: Reducing hallucination of large vision-language models via visual information steering. In The Forty-second International Conference on Machine Learning (ICML), 2025c. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2629626306, 2024a. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024b. URL https:// llava-vl.github.io/blog/2024-01-30-llava-next/. Sheng Liu, Xu Zhang, Nitesh Sekhar, Yue Wu, Prateek Singhal, and Carlos Fernandez-Granda. Avoiding spurious correlations via logit correction. In The Eleventh International Conference on Learning Representations (ICLR), 2023. Shi Liu, Kecheng Zheng, and Wei Chen. Paying more attention to image: training-free method for alleviating hallucination in lvlms. arXiv preprint arXiv:2407.21771, 2024c. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European Conference on Computer Vision (ECCV), pp. 216233. Springer, 2024d. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations (ICLR), 2019. Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Han-Jia Ye. Ovis: Structural embedding alignment for multimodal large language model. arXiv preprint arXiv:2405.20797, 2024. Shiyin Lu, Yang Li, Yu Xia, Yuwei Hu, Shanshan Zhao, Yanqing Ma, Zhichao Wei, Yinglun Li, Lunhao Duan, Jianshan Zhao, et al. Ovis2. 5 technical report. arXiv preprint arXiv:2508.11737, 2025. Sachin Mehta, Marjan Ghazvininejad, Srinivasan Iyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. Delight: Deep and light-weight transformer. In International Conference on Learning Representations (ICLR), 2021. 11 Chen Qian, Zihao Xie, YiFei Wang, Wei Liu, Kunlun Zhu, Hanchen Xia, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, Zhiyuan Liu, and Maosong Sun. Scaling large language model-based multi-agent collaboration. In The Thirteenth International Conference on Learning Representations (ICLR), 2025. Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object hallucination in image captioning. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 40354045, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. Oscar Skean, Md Rifat Arefin, Dan Zhao, Niket Nikul Patel, Jalal Naghiyev, Yann LeCun, and Ravid Shwartz-Ziv. Layer by layer: Uncovering hidden representations in language models. In Forty-second International Conference on Machine Learning (ICML), 2025. Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023. Feilong Tang, Zile Huang, Chengzhi Liu, Qiang Sun, Harry Yang, and Ser-Nam Lim. Intervening In The Thirteenth anchor token: Decoding strategy in alleviating hallucinations for MLLMs. International Conference on Learning Representations (ICLR), 2025a. Feilong Tang, Chengzhi Liu, Zhongxing Xu, Ming Hu, Zile Huang, Haochen Xue, Ziyang Chen, Zelin Peng, Zhiwei Yang, Sijin Zhou, et al. Seeing far and clearly: Mitigating hallucinations In Proceedings of the Computer Vision and Pattern in mllms with attention causal decoding. Recognition Conference (CVPR), pp. 2614726159, 2025b. Qwen Team. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. Chenxi Wang, Xiang Chen, Ningyu Zhang, Bozhong Tian, Haoming Xu, Shumin Deng, and Huajun Chen. Mllm can see? dynamic correction decoding for hallucination mitigation. In The Thirteenth International Conference on Learning Representations (ICLR), 2025. Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Jiaqi Wang, Haiyang Xu, Ming Yan, Ji Zhang, et al. Amber: An llm-free multi-dimensional benchmark for mllms hallucination evaluation. arXiv preprint arXiv:2311.07397, 2023. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024a. Xintong Wang, Jingheng Pan, Liang Ding, and Chris Biemann. Mitigating hallucinations in large vision-language models with instruction contrastive decoding. In Findings of the Association for Computational Linguistics: ACL 2024, pp. 1584015853, Bangkok, Thailand, August 2024b. Association for Computational Linguistics. Zichen Wen, Yifeng Gao, Shaobo Wang, Junyuan Zhang, Qintong Zhang, Weijia Li, Conghui He, and Linfeng Zhang. Stop looking for important tokens in multimodal language models: Duplication matters more. arXiv preprint arXiv:2502.11494, 2025. Dingchen Yang, Bowen Cao, Guang Chen, and Changjun Jiang. Pensieve: Retrospect-then-compare mitigates visual hallucination. arXiv preprint arXiv:2403.14401, 2024. Hao Yin, Guangzong Si, and Zilei Wang. Clearsight: Visual signal enhancement for object hallucination mitigation in multimodal large language models. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), pp. 1462514634, 2025. Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and Enhong Chen. Woodpecker: Hallucination correction for multimodal large language models. arXiv preprint arXiv:2310.16045, 2023. Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models. National Science Review, 11(12):nwae403, 2024. 12 Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. In The Forty-first International Conference on Machine Learning (ICML), 2024. Xinlei Yu, Zhangquan Chen, Yudong Zhang, Shilin Lu, Ruolin Shen, Jiangning Zhang, Xiaobin Hu, Yanwei Fu, and Shuicheng Yan. Visual document understanding and question answering: multi-agent collaboration framework with test-time scaling. arXiv preprint arXiv:2508.03404, 2025. Zihao Yue, Liang Zhang, and Qin Jin. Less is more: Mitigating multimodal hallucination from an eos decision perspective. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL: Long Papers), pp. 1176611781, Bangkok, Thailand, August 2024. Association for Computational Linguistics. Bohan Zhai, Shijia Yang, Xiangchen Zhao, Chenfeng Xu, Sheng Shen, Dongdi Zhao, Kurt Keutzer, Manling Li, Tan Yan, and Xiangjun Fan. Halle-switch: Rethinking and controlling object existence hallucinations in large vision language models for detailed caption. arXiv preprint arXiv:2310.01779, 2023. Guibin Zhang, Yanwei Yue, Xiangguo Sun, Guancheng Wan, Miao Yu, Junfeng Fang, Kun Wang, Tianlong Chen, and Dawei Cheng. G-designer: Architecting multi-agent communication topologies via graph neural networks. arXiv preprint arXiv:2410.11782, 2024a. Guibin Zhang, Luyang Niu, Junfeng Fang, Kun Wang, LEI BAI, and Xiang Wang. Multi-agent architecture search via agentic supernet. In Forty-second International Conference on Machine Learning (ICLR), 2025a. Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A. Smith. How language model hallucinations can snowball. In The Forty-first International Conference on Machine Learning (ICML), 2024b. Shaolei Zhang, Qingkai Fang, Zhe Yang, and Yang Feng. Llava-mini: Efficient image and video large multimodal models with one vision token. In The Thirteenth International Conference on Learning Representations (ICLR), 2025b. Zhi Zhang, Srishti Yadav, Fengze Han, and Ekaterina Shutova. Cross-modal information flow in multimodal large language models. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), pp. 1978119791, 2025c. Weihong Zhong, Xiaocheng Feng, Liang Zhao, Qiming Li, Lei Huang, Yuxuan Gu, Weitao Ma, Yuan Xu, and Bing Qin. Investigating and mitigating the multimodal hallucination snowballing in large vision-language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL: Long Papers), pp. 1199112011. Association for Computational Linguistics, August 2024. Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao. Analyzing and mitigating object hallucination in large vision-language models. In The Twelfth International Conference on Learning Representations (ICLR), 2024. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. Xin Zou, Yizhou Wang, Yibo Yan, Yuanhuiyi Lyu, Kening Zheng, Sirui Huang, Junkai Chen, Peijie Jiang, Jia Liu, Chang Tang, and Xuming Hu. Look twice before you answer: Memory-space visual retracing for hallucination mitigation in multimodal large language models. The Fortysecond International Conference on Machine Learning (ICML), 2025."
        },
        {
            "title": "A RELATED WORKS",
            "content": "Visual Hallucination. The tendency of VLMs to generate plausible but non-factual or unsupported content, i.e., visual hallucination, is well-documented in previous works. common remedy has been to retrain or fine-tune models to better align outputs with ground truth (Zhou et al., 2024; Zhai et al., 2023; Yue et al., 2024), but these solutions often demand extensive training resources and additional data. Consequently, interest has grown in training-free techniques, including selffeedback correction (Lee et al., 2024; Yin et al., 2023), leveraging auxiliary models for external knowledge integration (Yang et al., 2024), and modifying decoding procedures (Wang et al., 2024b; Zou et al., 2025; Wang et al., 2025; Tang et al., 2025b; Li et al., 2025c; Tang et al., 2025a; Yin et al., 2025; Liu et al., 2023). In contrast to these papers that focus on single VLM agent, it is not sufficient to address the failure mode of hallucination snowballing that emerges in multi-agent collaboration, which is the core focus of our paper. Attention in VLM-Based Agents. The hallucination problem of VLMs can be mainly attributed to and indicated by the attention mechanism. Earlier work found that LVLMs tend to attend to broad, global image cues and miss prompt-relevant details (Darcet et al., 2024; Gong et al., 2024; An et al., 2024), behavior often traced to the Vision Transformer encoder (Alexey, 2020). To address this, some methods boost attention weights for pertinent image tokens (Liu et al., 2024c), others select or filter informative visual features and apply contrastive decoding to suppress hallucinations (Huo et al., 2025). Our work presents an extensive study on the attention allocation token and layer-wise analysis to provide better understanding of how multi-agent pipelines lose visual fidelity during inference turns. Based on this, we further introduce novel visual flow to alleviate hallucination snowballing in multi-agent systems."
        },
        {
            "title": "B REQUISITE ANALYSES",
            "content": "B.1 SETTINGS Attention Allocation. In Section 2, we first calculate the attention allocations of four tokens in different layers among different agent turns. Formally, we first denote the whole token set as , consisting of vision token subset V, instruction token subset I, system token subset S, and output token subset O. The attention matrix is obtained as Equation 2, and each attention score si,j indicates the attention from the ith token to the jth token. Thus, the attention allocation of specific token type should be the sum of the attention score where the target is this token, which could be calculated as follows: Allocation token type = (cid:88) (cid:88) iT jT Al (i, j) Mtoken type = (cid:88) (cid:88) iT jT si,j, (5) Mtoken type = (i , Ttoken type) (6) where denotes the average attention matrix in all attention heads of the lth layer in this context. The attention allocation of specific tokens explicitly represents the focus in each layer of the model when understanding the task and outputting the responses. Dropping Tokens in Certain Layers. To drop subsets of vision tokens in shallow, middle, or deep layers, we set the hidden states of the subset in specific layers to zero instead of physical removal, because the latter changes sequence length and disrupts sequence alignment of the attention mechanism. Moreover, the implementation is mainly modified from (Liu et al., 2024b). Token Selection. As described in Section 2, we select five subsets of vision tokens, and here we elaborate on the selection rules for each subset: (1) Random Tokens: we randomly select from all vision tokens, and limit the number of tokens in the subset to the average number in the other four subsets. Besides, we re-select the random tokens if the size of the largest connected component of selected tokens exceeds 10% of the total selected tokens, to avoid selecting centralized tokens that destroy randomness; (2) Inactive Tokens: we first calculate the average attention value across all layers for each token, then select tokens whose attention values are below the lower quartile and whose fluctuation does not exceed 20%; (3) Rise Tokens and (4) Fall Tokens: we select the tokens with Table 7: Results of the attention score based and other alternative strategies on LLaVA-NeXT-7B and circular MAS structure. Selection Strategy MME MM Bench MM-Vet CHAIR POPE AMBER MMHalHall Bench Bench 1585.6 (-13.9) 72.3 (-2.3) 49.7 (-2.1) 43.4 (+2.2) 90.5 (-2.8) 90.1 (-2.6) 49.0 (-2.1) 53.6 (-2.1) Value-Norm Value-Norm (+1 Buffer) 1587.4 (-12.1) 72.6 (-2.0) 49.9 (-1.9) 43.6 (+2.4) 90.6 (-2.7) 90.4 (-2.3) 49.4 (-1.7) 53.9 (-1.8) Value-Norm (+3 Buffer) 1588.9 (-10.6) 72.7 (-1.9) 50.3 (-1.5) 43.1 (+1.9) 90.8 (-2.5) 90.3 (-2.4) 49.2 (-1.9) 53.9 (-1.8) Value-Norm (+5 Buffer) 1590.2 (-9.3) 73.4 (-1.2) 50.5 (-1.3) 43.0 (+1.8) 91.1 (-2.2) 90.6 (-2.1) 49.4 (-1.7) 54.1 (-1.6) Value-Norm (+8 Buffer) 1589.8 (-9.7) 73.3 (-1.3) 50.7 (-1.1) 43.3 (+2.1) 90.9 (-2.4) 90.8 (-1.9) 49.6 (-1.5) 54.4 (-1.3) 1593.0 (-6.5) 74.0 (-0.6) 51.6 (-0.2) 41.5 (+0.3) 92.9 (-0.4) 92.1 (-0.6) 50.6 (-0.5) 55.1 (-0.6) Key-Norm 1594.4 (-5.1) 74.2 (-0.4) 51.7 (-0.1) 41.3 (+0.1) 93.1 (-0.2) 92.4 (-0.3) 50.9 (-0.2) 55.4 (-0.3) Key-Norm (+1 Buffer) 1595.9 (-3.6) 74.3 (-0.3) 51.9 (+0.1) 41.2 (-0.0) 93.1 (-0.2) 92.6 (-0.1) 51.2 (+0.1) 55.5 (-0.2) Key-Norm (+3 Buffer) 1595.2 (-4.3) 74.3 (-0.3) 51.7 (-0.1) 41.4 (+0.2) 92.8 (-0.5) 92.3 (-0.4) 50.9 (-0.2) 55.6 (-0.1) Key-Norm (+5 Buffer) Key-Norm (+8 Buffer) 1594.7 (-4.8) 74.4 (-0.2) 51.5 (-0.3) 41.6 (+0.4) 92.9 (-0.4) 92.3 (-0.4) 50.8 (-0.3) 55.4 (-0.3) 1599.5 Attention Score 55.7 74. 93.3 92.7 41.2 51.8 51.1 gradually upward or downward attention allocation in the consecutive layers. To filter out insignificant fluctuations and better reflect the overall attention trend of each token, we utilize tolerance threshold. When deviations from the trend in the opposite direction do not exceed this threshold, we still consider it as maintaining the original trend; (5) Unimodal Tokens: we select tokens with attention allocation of unimodal distribution, whose peak surpasses the salience threshold ω. B.2 ADDITIONAL RESULTS As discussed in Section 2, we use the results of the LLaVA-NeXT-7B (Liu et al., 2024b) model on POPE (Li et al., 2023) as an example. Here, we provide results of different base VLMs to verify the generality of our insights and to avoid model-specific conclusions. The token-wise attention allocations of vision, instruction, system, and output tokens of six common VLMs are demonstrated in Figure 10; the results of dropping different subsets of vision tokens in the shallow, middle and deep layers are listed in Table 12; and the proportion of different vision tokens among different agent turns are demonstrated in Figure 11. These experimental results from the six models are consistent with the previous conclusions, demonstrating excellent generalization across various models."
        },
        {
            "title": "C METHODOLOGIES",
            "content": "C.1 ALTERNATIVE OF ATTENTION SCORE BASED STRATEGY Given that Flash-Attention 2/3 (Dao, 2024) are widely used in the latest VLMs, resulting in attention scores that are not explicitly stored and are not accessible, we design an alternative token selection strategy inspired by (Wen et al., 2025). Specifically, we utilize the L2 norm of the key to replace the attention score, which reflects the feature strength of the token; higher value of the norm indicates that the token is relatively more prominent and has more significant semantics. Unlike the strategy introduced in (Wen et al., 2025), which adopts L1 norm, we choose L2 norm to amplify the difference between tokens and promote the token selection. Statistically, the overlap of the initially selected tokens of the two strategies is more than 70%; however, the total amount of the Key-Norm based strategy is less than that of the other. Thus, we add buffer tokens of initially selected tokens, which surround the initially selected token of the 3 3 space. To verify the effectiveness of the Key-Norm, we compare it with the attention score based strategy as well as Value-Norm, which is similar to our alternative. As illustrated in Table 7, we observe that the Key-Norm based strategies are superior to the Value-Norm based ones. Besides, selecting by Key-Norm with three buffer tokens almost achieves the same performance as attention scores, even surpassing them in partial benchmarks. For more intuitive results, we provide the visualization comparisons of selected tokens based on these strategies from real cases. As visualized in Figure 8, the initial Key-Norm based selection covers the most visual relay tokens compared to the attention score based selection; however, the former one is relatively more sparse, losing partially important visual semantics. Adding buffer tokens is good solution, which selects the surrounding tokens and supplements the information of the visual flow. It is worth noting that we add three buffer tokens when using the alternative strategy in our experiments, thereby balancing accuracy and efficiency. 15 Figure 8: Comparisons of selected tokens using the attention scores and other alternative strategies."
        },
        {
            "title": "D EXPERIMENTS",
            "content": "D.1 SETTINGS Baselines. To verify the generality, we totally adopt ten models covering from 7B to 34B in our experiments, which are listed as follows: LLaVA-v1.5 (Liu et al., 2024a) uses two-layer MLP to connect image features into the word embedding space, and we choose the 7B and 13B models, which have 32 and 40 layers in the decoder of the LLM, respectively. LLaVA-v1.6 (Liu et al., 2024b) increases resolution limits for input images with augmented data. It is the early version of LLaVA-NeXT, which has 32 layers in the LLM. LLaVA-NeXT (Liu et al., 2024b) has significant improvements in reasoning, OCR, and world knowledge, remaining the same structure as LLaVA-v1.6. We include 7B, 13B, and 34B models in our experiments, which have 32, 40, and 60 layers in the decoder. LLaVA-OneVision (Li et al., 2025a) is the most powerful base VLM in the LLaVA family, supporting single-image-, multi-image, and video scenes simultaneously. It uses the Qwen2 (Team, 2024) as the LLM, which has 28 layers. Qwen2-VL (Wang et al., 2024a) introduces naive dynamic resolution and multi-modal rotary position embedding (M-RoPE), achieving impressive image and video understanding. Its 7B model has 28 decoder layers. Qwen2.5-VL (Bai et al., 2025) compresses the vision tokens with an MLP-based fuser, aligns M-RoPE and absolute time, and meticulously designs three-stage training pipeline. We select the 7B and 32B models with 32 layers of the LLM. Benchmarks. We evaluate our method on eight widely adopted benchmarks, including three comprehensive benchmarks and five hallucination benchmarks, which are presented as follows: MME (Yin et al., 2024) is the first comprehensive benchmark and measures the perception and cognitive abilities of 14 challenging subtasks. Moreover, the metric is the total score across all the subtasks. MMBench (Liu et al., 2024d) consists of multiple-choice questions to assess over twenty different ability dimensions, offering hierarchical framework with three levels. During the assessment, GPT-4 serves as the final judge. In our experiments, we only include the English subset for evaluation. 16 Figure 9: The four structures of MAS in our experiments. MM-Vet (Yu et al., 2024) is comprehensive benchmark, which defines six core capacities and assesses them on complicated visual tasks. It provides GPT-4 based evaluator for open-ended outputs. CHAIR (Rohrbach et al., 2018) is captioning hallucination assessment benchmark, comparing the objects mentioned in the title with the objects actually existing in the image. Here, we utilize the CHAIRS metric, calculating the proportion of titles that contain at least one hallucinatory object. POPE (Li et al., 2023) merges several classic visual datasets, and generates binary questions of the existence of objects. Each image is paired with six questions, and we use the accuracy metric. AMBER (Wang et al., 2023) is tailored to assess both generative and discriminative tasks, including existence, attribute, and relation hallucination. We follow the AM BER score in the original paper. MMHal-Bench (Sun et al., 2023) is composed of high-quality image-question pairs to measure the hallucination, and the generated responses are automatically rated by GPT-4. HallBench (Guan et al., 2024) is meticulously handcrafted by experienced human experts, and evaluated by text-only GPT4-assisted evaluation framework. Evaluations. For the eight selected benchmarks, we largely follow their original evaluation metrics. Besides, to assess the level of hallucination snowballing in multi-agent contexts, we propose hallucination snowballing score (HS), which quantifies both the severity and propagation of hallucinations. The score could be formulated as follows: HS ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 1 1 + exp( 2 di) hi, (7) where is the hallucination propagation distance, and are the total distance and the number of agents in the multi-agent system. represents the severity of hallucination, centesimal-point scale score produced by judge model. The average score on benchmarks measures the hallucination snowballing; the deeper and broader the snowballing of the hallucination, the higher the score. MAS Structures. Some multi-agent systems, e.g., centralized or function-specialized ones, have complicated collaborative or functional division processes. As shown in Figure 9, to minimize uncertainty, we choose four distributed multi-agent system structures, i.e., linear (Hong et al., 2024), layered (Ishibashi & Nishimura, 2024), random (Qian et al., 2025), and circular (Qian et al., 2025) structures, that feature no central agent and relatively straightforward structure; all agents are equal in status, save for sequential dependencies within the system. Among the four MAS structures, circular ones have the densest collaborations and interactions among agents, and theoretically, the multi-agent hallucination snowballing effects are the most serious. Thus, all the experiments except the main results are conducted on the circular structure. Implementations. Experiments are conducted on four or eight NVIDIA H20 96G GPUs. The salience of unimodal morphology ω is 0.3, the temperature scaling τ is 0.8, the reallocation coefficient α1, α2 in the middle and deep layers are set to 0.1 and 0.3, and the temperature of generation is set to 1 for MAS. For other configurations of baselines, we refer to the original paper. Training Pipelines. Our proposed method can be integrated with other base VLMs in MAS to alleviate the multi-agent hallucination snowballing, requiring only one additional module for visual relay selection. Following the typical training paradigm, we employ two-stage training process including pre-training stage and instruction tuning stage, as reported in Table 8. Table 8: Two-stage training details. VLMs utilize various strategies to project vision tokens, such as MLP-based projectors. Modules Vision Encoder Projector Large Language Model Transformer Block (Ours) Settings Batch Size Learning Rate MM Learning Rate Warmup Ratio Optimizer Epoch Stage One Pre-Training Frozen Trainable Frozen Trainable 256 - 5e-4 0.05 Stage Two Instruction Tuning Frozen Trainable Trainable Trainable 256 1e-4 1e-5 0.02 AdamW (Loshchilov & Hutter, 2019) AdamW (Loshchilov & Hutter, 2019) 1 2 Table 9: Influence of the salience of unimodal morphology ω on LLaVA-NeXT-7B and circular MAS structure. Table 10: Influence of the temperature scaling τ on LLaVA-NeXT-7B and circular MAS structure. ω Ratio % CHAIR POPE AMBER 0.1 17.6 44.5 42.8 6.7 0.2 41.2 2.3 0.3 41.7 1.3 0.4 42.9 0.2 0.5 90.2 91.5 93.3 92.5 91.1 88.6 89.2 92.7 92.0 89.6 MMHalHall Bench Bench 45.9 49.1 51.1 49.8 47. 52.7 52.8 55.7 54.9 53.0 τ CHAIR POPE AMBER 0.6 44.1 0.7 43.0 0.8 41.2 0.9 41.0 1.0 41.7 91.4 92.4 93.3 93.1 92.5 90.8 91.8 92.7 92.2 91.6 MMHalHall Bench Bench 48.2 50.3 51.1 48.9 47.7 52.1 54.2 55.7 53.8 52.8 Table 11: Influence of the reallocation coefficient α1 and α2 on LLaVA-NeXT-7B and circular MAS structure. α1 α2 CHAIR POPE AMBER MMHalHall Bench Bench 0.0 0.2 43.2 0.0 0.3 42.8 0.0 0.4 43.0 0.1 0.2 41.3 0.1 0.3 41.2 0.1 0.4 41.8 0.2 0.2 42.9 0.2 0.3 43.1 0.2 0.4 42. 89.6 90.0 89.8 93.1 93.3 92.9 90.6 89.8 89.6 89.1 89.3 89.0 92.6 92.7 92.4 89.2 88.5 88.9 47.2 47.4 47.1 50.7 51.1 50.5 47.3 46.9 46.5 47.6 48.2 47.8 55.3 55.7 54.9 48.1 48.0 47.7 D.2 ADDITIONAL RESULTS Hyper-Parameter Analyses There are three key hyper-parameters in our proposed method, i.e., the salience of unimodal morphology ω when selecting visual relay tokens with unimodal distribution, the temperature scaling τ in Equation 2, and the reallocation coefficient α1 and α2 in Equation 3 of the middle and deep layers. As listed in Table 9, the lower the ω, the more proportions of visual tokens would be included; however, excessive visual relay tokens will not bring extra performance improvement but computation costs. When ω is set to 0.3, the model obtains the best results with relatively less token overhead. Besides, as shown in Table 10, and Table 11, when τ , α1, α2 are set to 0.8, 0.1, 0.3, our model exhibits the greatest potential."
        },
        {
            "title": "E LIMITATIONS",
            "content": "Although we conduct experiments on total of ten models with different sizes, which verifies the robustness of compatibility of our proposed method, more experiments are still recommended. For example, the results on smaller size VLMs, e.g., 3B, and also larger baselines, e.g., 72B, could provide further evidence. Besides, the inclusion of more series of baselines, such as InternVL series (Chen et al., 2024b;a; Zhu et al., 2025), Llama 3 (Grattafiori et al., 2024), Ovis series (Lu et al., 2024; 2025), and MiniMCP (Hu et al., 2024), is also beneficial. 18 (a) LLaVA-v1.5-7B (b) LLaVA-v1.6-7B (c) LLaVA-NeXT-7B (d) LLaVA-OneVision-7B (e) Qwen2-VL (f) Qwen2.5-VL Figure 10: Layer-wise attention allocation of six models in different agent turns, and denotes that using Key-Norm to replace the attention score. 19 Table 12: Results of six VLMs when dropping different selected subsets of vision token in the shallow, middle, and deep layers. Shallow Layers Middle Layers 25% 50% 75% 100% 25% 50% 75% 100% 25% 50% 75% 100% Deep Layers LLaVAv1.5-7B LLaVAv1.6-7B LLaVANeXT-7B w/o Dropping (a) Random 49.1 41.3 36.1 28.7 77.3 53.3 44.6 38.9 29.7 82.7 (b) Inactive 40.1 33.1 27.5 18.4 76.4 (c) Rise (d) Fall 40.0 36.4 28.2 19.9 75.2 (e) Unimodal 39.6 35.9 27.9 22.4 51. w/o Dropping (a) Random 49.9 42.2 37.2 28.0 77.0 52.1 43.8 39.6 30.5 83.6 (b) Inactive 39.2 33.8 28.2 18.5 78.9 (c) Rise (d) Fall 39.6 36.6 28.7 20.2 75.8 (e) Unimodal 40.9 36.4 28.0 20.9 46.0 w/o Dropping (a) Random 51.8 44.5 38.4 30.5 78.9 55.1 46.2 41.8 32.5 84.3 (b) Inactive 41.9 35.6 29.6 20.8 79.4 (c) Rise 41.6 38.8 30.7 22.5 78.3 (d) Fall (e) Unimodal 42.1 37.6 30.0 22.8 52.9 w/o Dropping (a) Random 52.7 45.1 41.4 33.2 82.4 58.8 46.5 44.9 36.4 83.6 (b) Inactive LLaVA44.7 39.5 31.1 22.4 81.2 OneVision-7B (c) Rise (d) Fall 45.2 42.1 33.9 24.8 82.6 (e) Unimodal 44.6 41.7 33.8 26.1 53.0 Qwen2VL-7B Qwen2.5 VL-7B w/o Dropping (a) Random 53.2 44.7 39.8 31.2 79.3 55.5 46.5 42.0 33.5 86.4 (b) Inactive 43.2 36.7 29.8 21.2 80.0 (c) Rise 43.0 40.1 31.6 23.4 78.9 (d) Fall (e) Unimodal 44.2 39.0 31.0 23.5 53. w/o Dropping (a) Random 51.5 44.8 39.2 31.2 80.0 56.5 46.3 41.9 33.2 83.7 (b) Inactive 42.4 36.6 30.4 20.9 80.3 (c) Rise (d) Fall 42.7 38.8 31.0 23.0 80.0 (e) Unimodal 42.1 38.1 30.4 23.1 54.0 83.0 64.1 78.9 62.0 62.5 43.1 83.3 63.6 82.4 61.6 62.9 42.9 85. 66.1 82.9 64.2 64.8 44.5 87.6 72.5 80.2 65.4 66.1 46.2 85.9 67.1 83.1 64.2 65.5 46.0 85. 66.3 82.2 63.6 64.2 44.7 60.2 56.2 81.6 81.1 80.6 80.2 80.1 76.7 82.8 81.9 82.1 81.7 54.5 50.8 81.0 79.3 79.8 80.0 55.6 50.0 82.4 81.6 81.4 80.6 35.0 22.6 81.6 81.3 80.5 80.9 60.1 58.1 81.8 81.1 80.7 80.9 78.6 76.1 82.9 83.1 82.7 82.6 54.7 51.2 81.4 80.8 80.4 80.1 57.5 50.3 82.5 82.4 81.0 81.2 33.9 23.4 82.4 81.6 80.6 80.1 62.7 59.0 84.4 83.2 82.9 82.6 81.5 78.3 85.0 84.6 84.3 84.6 56.4 52.3 83.6 82.7 81.8 81.6 58.5 52.9 84.1 82.8 82.0 82.4 36.6 25.3 84.4 83.0 82.3 81.8 66.8 63.7 87.5 86.6 86.2 86.5 74.4 67.8 87.7 87.3 87.2 87.0 62.4 55.5 87.1 86.8 86.7 86.5 59.6 53.6 87.2 86.7 86.6 86.4 39.7 27.5 86.8 86.3 86.5 85.7 62.6 59.0 84.8 84.0 83.5 83.0 83.2 78.4 85.6 85.4 85.2 85.7 56.5 53.9 85.2 84.8 84.1 83.7 60.0 52.8 84.4 84.2 84.6 83.9 37.1 25.7 84.5 84.0 83.9 83. 63.9 59.5 84.4 83.9 83.6 83.4 80.9 78.2 84.8 84.6 84.7 84.5 55.7 52.5 84.9 83.3 83.2 83.1 59.0 54.3 84.8 83.9 83.3 82.8 37.0 26.0 85.0 84.8 83.6 83.4 (a) LLaVA-v1.5-7B (b) LLaVA-v1.6-7B (c) LLaVA-NeXT-7B (d) LLaVA-OneVision (e) Qwen2-VL (f) Qwen2.5-VL Figure 11: Proportion of vision tokens subsets of six models in different agent turns."
        }
    ],
    "affiliations": [
        "Fudan University",
        "National University of Singapore",
        "Tencent Youtu Lab",
        "Zhejiang University"
    ]
}