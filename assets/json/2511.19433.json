{
    "paper_title": "Mixture of Horizons in Action Chunking",
    "authors": [
        "Dong Jing",
        "Gang Wang",
        "Jiaqi Liu",
        "Weiliang Tang",
        "Zelong Sun",
        "Yunchao Yao",
        "Zhenyu Wei",
        "Yunhui Liu",
        "Zhiwu Lu",
        "Mingyu Ding"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-language-action (VLA) models have shown remarkable capabilities in robotic manipulation, but their performance is sensitive to the $\\textbf{action chunk length}$ used during training, termed $\\textbf{horizon}$. Our empirical study reveals an inherent trade-off: longer horizons provide stronger global foresight but degrade fine-grained accuracy, while shorter ones sharpen local control yet struggle on long-term tasks, implying fixed choice of single horizons being suboptimal. To mitigate the trade-off, we propose a $\\textbf{mixture of horizons (MoH)}$ strategy. MoH rearranges the action chunk into several segments with different horizons, processes them in parallel with a shared action transformer, and fuses outputs with a light linear gate. It has three appealing benefits. 1) MoH exploits long-term foresight and short-term precision jointly within a single model, improving both performance and generalizability to complex tasks. 2) MoH is plug-and-play for full-attention action modules with minimal training or inference overhead. 3) MoH enables dynamic inference with adaptive horizons, which selects stable actions through cross-horizon consensus, achieving 2.5$\\times$ higher throughput than baselines while preserving superior performance. Extensive experiments over flow-based policies $π_0$, $π_{0.5}$, and one-step regression policy $π_{\\text{reg}}$ demonstrate that MoH yields consistent and significant gains on both simulations and real-world tasks. Notably, under mixed-task setting, $π_{0.5}$ with MoH reaches a new state-of-the-art with 99$\\%$ average success rate on LIBERO after only $30k$ training iterations. Project page: https://github.com/Timsty1/MixtureOfHorizons"
        },
        {
            "title": "Start",
            "content": "Dong Jing1,2, Gang Wang3, Jiaqi Liu2, Weiliang Tang3*, Zelong Sun1, Yunchao Yao2, Zhenyu Wei2, Yunhui Liu3, Zhiwu Lu1, Mingyu Ding2 1 RUC, 2 UNC, 3 CUHK 5 2 0 2 4 2 ] . [ 1 3 3 4 9 1 . 1 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Vision-language-action (VLA) models have shown remarkable capabilities in robotic manipulation, but their performance is sensitive to the action chunk length used during training, termed horizon. Our empirical study reveals an inherent trade-off: longer horizons provide stronger global foresight but degrade fine-grained accuracy, while shorter ones sharpen local control yet struggle on long-term tasks, implying fixed choice of single horizons being suboptimal. To mitigate the trade-off, we propose mixture of horizons (MoH) strategy. MoH rearranges the action chunk into several segments with different horizons, processes them in parallel with shared action transformer, It has three and fuses outputs with light linear gate. appealing benefits. 1) MoH exploits long-term foresight and short-term precision jointly within single model, improving both performance and generalizability to complex tasks. 2) MoH is plug-and-play for full-attention action modules with minimal training or inference overhead. 3) MoH enables dynamic inference with adaptive horizons, which selects stable actions through cross-horizon consensus, achieving 2.5 higher throughput than baselines while preserving superior performance. Extensive experiments over flow-based policies π0, π0.5, and one-step regression policy πreg demonstrate that MoH yields consistent and significant gains on both simulations and real-world tasks. Notably, under mixed-task setting, π0.5 with MoH reaches new state-of-the-art with 99% average success rate on LIBERO after only 30k training iterations. Project page: https: //github.com/Timsty1/MixtureOfHorizons 1. Introduction Vision-language-action models (VLAs) [6, 23] have recently attracted increasing attention for their remarkable ability to follow human instructions and execute complex robotic tasks, such as cloth folding [48], object arrangement [8], beverage preparation [15], and self-driving [21]. VLAs [6, 24, 26] enjoy the benefits of large-scale pre-trained vision-language *Gang Wang and Weiliang Tang were supported by HKCLR Corresponding Author Figure 1. Effect of action horizon on π0. The first 5 actions in the predicted chunk are executed at evaluation. Varying horizons lead to trade-off effects across four LIBERO task suites. Our mixture of horizons strategy alleviates this trade-off and raises overall success. Figure 2. Overview of the proposed mixture of horizons strategy, which integrates action chunks of multiple horizons via the shared action transformer and mixture gating mechanism. models (VLMs) [13, 52], on top of which they are built and further equipped with an action module for embodied control. Typically, modern VLAs adopt this action module with full attention and an action chunking strategy, allowing the model to predict sequence of future actions conditioned on the current observation and instruction. Although action chunking has proven effective [36, 46], the models performance is highly sensitive to the chunk length during training, which we refer to as horizon, specifying the temporal span of future actions being predicted. How to understand the effect of horizon selection and its underlying mechanism thus becomes key problem. Existing works generally adopt fixed horizon, which potentially leads to suboptimal performance and limits the models flexibility, e.g., adaptive control over latency at inference time. To fill this gap, we conduct study on the influence of horizon. Using the widely adopted π0 [6] as baseline, we evaluate horizons [10, 20, 30] on the LIBERO [29] benchmark. As shown in Figure 1 where tasks ranging from Spatial, Object, Goal, to Long require progressively longer trajectories, we uncover fundamental tradeoff: 1) longer horizons improve long-term planning hence better performance on long-horizon tasks, and 2) shorter horizons allow precise control and yield higher success rates on short-horizon tasks. This sensitivity highlights critical limitation of current VLA models: fixed, single horizon imposes an inherent bottleneck on generalization. This raises natural question: Can we integrate multiple horizons to jointly exploit long-term foresight and short-term precision within single model? Motivated by this idea, we propose mixture of horizons (MoH) strategy for action chunking. As illustrated in Figure 2, MoH rearranges each action chunk into multiple segments with different horizons, processes them in parallel using shared action transformer, and fuses their predictions at each timestep via lightweight linear gating mechanism with only 2k additional parameters. Incorporating horizons [10, 20, 30] through MoH effectively mitigates the trade-off observed in π0 and yields consistent gains across all task suites (Figure 1). MoH is universal and computationally efficient. It can be seamlessly plugged into any full-attention action module, regardless of whether it is flow-based or single-step prediction-based. In practice, existing action transformers are typically lightweight (around 300M parameters or fewer) when compared with the VLM backbones [6, 48] and benefit from tensor parallelism, MoH introduces minimal training and inference overhead relative to the original architecture. Moreover, MoH naturally enables dynamic inference scheme via cross-horizon consensus. At each timestep, the model outputs horizon-wise predictions together with their mixture. We treat each horizon as voter and identify actions that receive consistent support across horizons, forming self-truncating executable chunk while deferring uncertain actions to the next replanning iteration. We demonstrate that this dynamic inference mechanism via cross-horizon consensus improves both execution stability and inferencetime efficiency, e.g., even at 2.5 throughput, π0.5 with MoH still surpasses the performance of the baseline π0.5 [35]. We evaluate MoH on flow-based models (π0 [6] and π0.5 [35]) and one-step prediction model (πreg [6]) across both simulation environments (LIBERO [29], RoboTwin2.0 [12]) and real-world robotic tasks. Across all settings, MoH yields consistent and substantial improvements. Remarkably, under the mixed-task training setting, π0.5 with MoH achieves an average success rate of 99% on LIBERO after only 30k iterations, establishing new state of the art. Additional ablations and visualizations validate each component of MoH. Our contributions are threefold: 1. We present systematic study of the action chunking horizon in VLAs, revealing key trade-off between longterm foresight and short-term precision. 2. We introduce Mixture of Horizons, plug-and-play, lowoverhead approach that alleviates the above trade-off and improves performance and generalization. 3. We propose dynamic inference scheme via crosshorizon consensus for more stable and faster execution. 2. Related Work 2.1. Vision-Language-Action Models VLA models [10, 13, 1618, 33, 37, 40, 43, 49, 50] map visual observations and language instructions to executable actions for robotic manipulation. Early policy architectures, such as Diffusion Policy [14], typically employ relatively small networks and are designed for task-specific scenarios, achieving strong performance but limited generalization. With the rapid progress of VLMs[1, 3, 22, 41, 51, 52], recent approaches move toward more general-purpose embodied agents by coupling powerful VLM backbones with action heads or expert modules. For example, OpenVLA [23] pretrained VLA model on large-scale robotic datasets via discrete action token prediction. More recently, the π-series [6, 35] and related methods [26] adopt flowmatching [28] or diffusion-based policies to predict continuous low-level actions, and have become dominant design choice. Besides, many works attempt to improve spatial perception [25, 27, 33, 44] or the cross-embodied generalization [11, 47, 48]. VLA models represent promising pathway toward scalable embodied superintelligence. 2.2. Action Chunking Action chunking, popularized by ACT [46], allows policies to predict sequence of future actions at each control step instead of single next action. This design exposes the policy to temporal structure, supports high-frequency control, and enables smoother execution by fusing overlapping actions in different chunks. CogACT [26] further refines this idea with similarity-based weighting schemes. Consequently, chunked prediction combined with full-attention transformers over the action dimension has become standard component in modern VLA policies [4, 6, 8, 15, 20, 24, 42]. Despite its widespread use, the chunk length, termed horizon, is typically chosen heuristically. Existing findings [26, 36] suggest that performance is highly sensitive to this horizon and that different horizons are preferable for different task types. Moreover, prior works do not provide an available method to mitigate the trade-off between long-term foresight and short-term precision induced by fixed horizon. In this paper, we aim to solve this problem by introducing universal mixture-of-horizons training strategy. 3. Method 3.1. Preliminaries VLA models with action chunking. VLA models are sequential decision policies for end-to-end robotic manipulation. At each decision step t, the policy observes multi-view input Vt = {v(m) }M m=1, an optional history h<t = Vtk:t1, language instruction , and an optional proprioceptive state st. Instead of predicting single action, the policy outputs an action chunk of length H: At = (at,1, . . . , at,H ) = (at, . . . , at+H1) RHda , (1) where at,k = at+k1 Rda denotes the action at relative step within the chunk. Action chunking reduces the number of policy calls at test time and enables planning over temporally extended horizon. Recent advanced VLA models are typically built upon pre-trained VLM backbone that encodes (Vt, h<t, T, st) into context representation, followed by compact action transformer operating on action tokens. In most cases, the full-attention mechanism, where all action tokens attend to each other, is adopted. Many prior works [6, 24, 45] have demonstrated that this non-causal design consistently outperforms strictly autoregressive decoding for chunk prediction. Flow-matching policies. Flow-matching policies learn velocity field that transports Gaussian noise chunk to the target action chunk. Let ϵ (0, I) be noise chunk of the same shape as At, and let τ [0, 1] denote continuous time variable. standard reference path is the linear interpolation Figure 3. Overview of our mixture of horizons framework. The action-related input is rearranged into different horizons and then processed in parallel by shared action transformer. linear gate head, with only 2k parameters, produces per-step, per-horizon weights to fuse horizon-wise predictions into the final action predictions. This strategy is plug-and-play for any full-attention action transformer, including both flow-matching and one-step policies. They can be instantiated in either discretized classification or continuous regression form. (i) Discretized classification. Each scalar action dimension is quantized into bins. Let yk,d {1, . . . , B} be the target bin index for dimension of step k, and let pθ(yk,d Vt, h<t, T, st) be the predicted categorical distribution. The loss is A(τ ) = (1 τ ) ϵ + τ At, whose ground-truth velocity is u(ϵ, At) = dτ A(τ ) = At ϵ. (2) (3) Lcls(θ) = (cid:88) da(cid:88) k=1 d=1 log pθ(yk,d Vt, h<t, T, st) . (7) (ii) Continuous regression. Alternatively, the policy directly regresses continuous actions, e.g., with an ℓ1 loss: The flow-matching policy vθ is trained to approximate this velocity via (cid:17) (cid:16) (cid:13) (cid:13)vθ A(τ ) , τ, Vt, h<t, T, st Lfm(θ) = Eϵ,τ u(ϵ, At)(cid:13) 2 2. (cid:13) (4) At inference, an ODE solver [30] integrates the learned velocity field from τ = 0 to τ = 1 starting from A(0) = ϵ with step size τ : Lreg(θ) = (cid:88) da(cid:88) k= d=1 (cid:12) (cid:12) ˆAt,k,d At,k,d (cid:12) (cid:12). (8) Both flow-matching and one-step policies can be built on the basic full-attention action transformer with minor modifications. Specifically, flow-matching requires time embedding layer, while one-step prediction can be implemented by introducing learnable query token. A(τ +τ ) = A(τ ) + vθ (cid:16) A(τ ) , τ, Vt, h<t, T, st (cid:17) τ, (5) 3.2. Mixture of Horizons yielding A(1) as the final action chunk. One-step policies. One-step policies directly map the context to the final action chunk in single forward pass: ˆAt = gθ(Vt, h<t, T, st). (6) Motivation. As shown in Figure 1, training with single chunk horizon leads to trade-off: short horizons favor precise short-term control but lack foresight, whereas long horizons capture long-term structure but may sacrifice immediate motor accuracy. Our goal is to fuse multiple horizons within single policy so that it inherits the strengths of both. Action chunk rearrangement. We fix maximum horizon and set of candidate horizons = {h1, . . . , hN } with h1 < < hN = H. Given ground-truth chunk At = (at,1, . . . , at,H ), (9) we construct for each truncated chunk A(h) = (at,1, . . . , at,h) Rhda . (10) During training, all horizons share the same observation context (Vt, h<t, T, st) processed by VLM. For efficient computation, we pad each A(h) to length for batching and use horizon-specific attention mask that invalidates positions > h. This allows the shared action transformer to process all horizons in parallel in one forward pass. Since the VLM prefix is only computed once and the action transformer is lightweight, our MoH strategy adds negligible computational overhead in both training and inference. Gated Mixture. The shared action transformer produces hidden states (h) Rhd for each horizon H. An action head converts these into horizon-specific predictions ˆA(h) = (ˆa(h) t,1 , . . . , ˆa(h) t,h ), ˆa(h) t,k Rda . (11) As illustrated in Figure 3, following Occams razor principle [7], we adopt the simplest effective design for fusing horizons: linear layer is added on top of the shared action transformer as gating head, which produces logits gt,k,h for each step and horizon h. For given step k, only horizons with are valid; we mask out invalid horizons and normalize over the remaining ones: αt,k,h = exp(gt,k,h) hH:kh exp(gt,k,h) (cid:80) The final fused prediction at step is , H, h. (12) (13) ˆat,k = (cid:88) αt,k,h ˆa(h) t,k . hH:kh This simplest gating leaves the backbone unchanged and applies to any full-attention action transformer, seamlessly integrating with both flow-matching and one-step policies. Balance loss for horizon utilization. Without regularization, the gating network may collapse to some preferred horizons, preventing others from contributing. We encourage balanced utilization of horizons in the spirit of load-balancing losses used in the mixture-of-experts domain [19, 34]. Let αb,k,h be the gate weight for sample b, step k, and horizon h. Because the set of valid horizons depends on k, we partition the temporal dimension by the ordered boundaries {0, h1, . . . , hN }. For each interval (hi1, hi], the active horizons are Hi = {h : > hi1}, and Si denotes Algorithm 1 Dynamic Inference via Horizon Consensus 1: Input: horizons H, horizon-wise actions {ˆak}H MoH fused actions ˆa, weights {αk}H minimum steps n, minimum active horizons m. k=1, k=1, scaling ratio r, hHk αk ˆa ˆak Hk {h : h} dk (cid:80) 2: Output: executable prefix actions {ˆak}Kexec k=1 . 3: for = 1 to do 4: 5: 6: end for 7: thres Mean({ dk}n 8: Kexec 9: for = + 1 to do 10: 11: if Hk < or dk > thres then k=1) break active horizons disagreements threshold end if Kexec 12: 13: 14: end for 15: return {ˆak}Kexec k=1 the set of steps in this interval. We define the average usage of horizon Hi as α(i) = 1 Si (cid:88) (cid:88) b=1 kSi αb,k,h, (14) where is the batch size. The balance loss is the mean squared coefficient of variation of these averages: Lbal = 1 (cid:88) iI CV2(cid:0){α(i) }hHi (cid:1), CV2(p) = Var(p)/(Mean(p)2 + ε). (15) (16) where indexes intervals with Hi > 1 and ε is small constant. Minimizing Lbal discourages degenerate gates and ensures that all horizons are effectively utilized. Training objective. MoH is agnostic to the underlying policy loss. Let Lmix denote the loss computed on the fused predictions {ˆat,k}, and Lind = (cid:80) hH L(h) the sum of losses on the individual horizon-specific predictions { ˆA(h) }. For flow-matching policies, Lmix and L(h) are velocity-matching losses; for one-step policies, they are the corresponding classification or regression objectives from Section 3.1. The final training objective is = Lmix + λindLind + λbalLbal, (17) where λind and λbal are empirically set to 1 and 103. 3.3. Dynamic Inference via Horizon Consensus We design dynamic inference scheme via cross-horizon consensus for stable and fast inference. As illustrated in Method Size Iters Spatial Object Goal Long Average Octo [38] OpenVLA [23] CoT-VLA [45] π0-FAST [32] UniVLA [9] πreg [6] πreg with MoH (Ours) Diffusion Policy [14] SmolVLA [36] GR00T-N1 [5] OpenVLA-OFT [24] VLA-Adapter [39] X-VLA [48] Spatial Forcing [25] π0 [6] π0 with MoH (Ours) π0.5 [35] π0.5 with MoH (Ours) Regression or classification-based VLA - 150k 100k 30k 8k 30k 30k 78.9 84.7 87.5 96.4 96.5 97.8 99.0 85.7 88.4 91.6 96.8 96.8 98.2 98.8 Flow-matching or diffusion-based VLA - 100k 100k 150k 150k 60K 150k 30k 30k 30k 30k 78.3 93.0 94.4 97.6 97.8 98.2 99.4 97.4 97.6 98.8 98. 92.5 94.0 97.6 98.4 99.2 98.6 99.6 98.2 98.8 99.0 100 0.1B 7B 7B 3B 9B 3B 3B 30M 2B 3B 7B 0.5B 1B 7B 3B 3B 3B 3B 84.6 79.2 87.6 88.6 95.6 94.6 96.4 68.3 91.0 93.0 97.9 97.2 97.8 98.8 95.4 96.4 97.6 98.8 51.1 53.7 69.0 60.2 92.0 90.2 91. 50.5 77.0 90.6 94.5 95.0 97.6 96.0 84.2 87.4 95.4 98.4 75.1 76.5 83.9 85.5 95.2 95.2 96.4 72.4 88.8 93.9 97.1 97.3 98.1 98.5 93.8 95.1 97.7 99.0 Table 1. Comparison of VLA models on LIBERO. Iters is the abbreviation of training iterations. Best results are in bold. MoH consistently improves flow-matching and regression-based baselines. UniVLA and X-VLA use large training batch size of 192 and 128, seperately. Algorithm 1, at each step, horizon-wise predictions {ˆak}H k=1 serve as voters on the fused actions ˆa. We measure the ℓ1 disagreement between ˆa and all valid ˆak with gating weights α. The first steps provide data-dependent threshold, and we execute the longest action prefix whose disagreement remains below this threshold while enough horizons are still active. This yields self-truncating executable chunk where only cross-horizon-consistent actions are committed, and the remaining tail is deferred to replanning. 4. Simulation Experiments 4.1. Experimental Setup Simulation Setup. We evaluate our method on two widely used simulation benchmarks, LIBERO [29] and RoboTwin2.0 [12]. LIBERO contains four task suites: Spatial, Object, Goal, and Long. Each suite contains 10 tasks and 500 demonstrations in total, and is designed to probe generalization to different spatial layouts, objects, goals, or long-horizon tasks. RoboTwin is bimanual benchmark covering 50 diverse tasks. For each task, RoboTwin provides an easy setting with in-domain layouts and hard setting with domain randomization, including scene clutter, diverse background textures, lighting variations, and different tabletop heights. Due to computational limitations, we evaluate methods on 7 representative tasks from RoboTwin. For both benchmarks, we report success rate as evaluation metric. Base Models and Implementation Details. We select π series [6, 35] as our base models, including flow-matchingbased π0, π0.5, and regression-based πreg. All these models are built upon PaliGemma [3] and pre-trained on largescale embodied datasets [31]. Since π0 does not release regression-type base model, we obtain πreg by fine-tuning the released π0 base model with regression objective. Its architecture is presented in the Appendix. Following the official settings of the π-series, we train all models once on the mixed LIBERO training set containing all four task suites. Training is performed on 4 NVIDIA A100 GPUs for only 30k iterations with batch size of 32 and fixed random seed for all comparisons. No historical information (past observations or actions) is provided to the VLA models. Unless otherwise specified, the default horizon configuration for MoH is = {3, 6, . . . , 30} with stride of 3. Each training run finishes in less than 10 hours. Regarding RoboTwin, again following the official configuration, we train each model for 20 epochs using 50 clean demonstrations per task, training separate policy for each of the selected tasks. This corresponds to roughly 3k10k iterations depending on the task, after which we evaluate the policies on both the easy and hard modes. 4.2. Comparisons with Related Work LIBERO. For fair comparison, we evaluate each task suite with 500 trials, using identical random seeds across all policies. Following the official LIBERO setting, only the first 5 action steps of each predicted chunk are executed during Figure 4. Comparisons with state-of-the-art methods on RoboTwin 2.0 Benchmark. evaluation. As shown in Table 1, our MoH strategy brings consistent and substantial gains to the baselines π0, π0.5, and πreg. In particular, π0.5 with MoH attains an average success rate of 99%, establishing new state of the art on this benchmark. These results demonstrate that MoH effectively integrates precise short-horizon control and long-horizon foresight, thereby mitigating the inherent trade-off between them and further boosting overall performance. We also observe that the regression-based policy πreg, obtained by fine-tuning from the π0 base model, can even outperform the standard fine-tuned flow-matching-based π0. Given that LIBEROs training and evaluation settings are highly in-distribution, this indicates that the regression objective converges well on small-scale downstream tasks, further supporting the soundness of the πreg design. RoboTwin. We evaluate each easy and hard task with 100 trials, using identical random seeds across all policies for fair comparison. To accelerate evaluation, we execute the prefix 20 action steps of each predicted chunk. As shown in Figure 4, π0 equipped with MoH achieves the highest average success rate and consistently improves over the base π0 on most tasks, verifying the general effectiveness of our MoH strategy across diverse downstream scenarios. The gains on both easy and hard variants indicate that MoH not only accelerates in-distribution convergence, but also enhances robustness and generalization to more challenging task configurations. 4.3. Ablation Study Config Spatial Object Goal Long Avg π0.5 baseline +MoH d=10 +MoH d=5 +MoH d=3 +MoH d=2 +MoH d=1 {30} {10,20,30} {5,10,. . . ,30} {3,6,. . . ,30} {2,4,. . . ,30} {1,2,. . . ,30} 98.8 98.8 99.6 98.8 99.2 99.0 99.0 99.8 99.0 100 98.6 99. 97.6 95.4 97.7 97.6 96.8 98.3 98.4 96.2 98.3 98.8 98.4 99.0 98.4 97.0 98.3 98.4 96.2 98.3 Table 2. Ablation on horizon density for MoH with π0.5 backbone on LIBERO. We fix the maximum horizon Hmax = 30 and instantiate the candidate set = {d, 2d, . . . , Hmax} with varying stride d. Smaller corresponds to denser multi-scale horizons. Variant Spatial Object Goal Long Avg π0.5 baseline 98.8 +Loss reweighting, no MoH 99.2 98.8 +MoH with average fusion +MoH without Lbal 98.2 98.8 +MoH (ours) 99.0 99.6 99.2 100 97.6 95.4 97.7 99.2 94.4 98.1 98.6 96.8 98.4 99.0 96.8 98.5 98.8 98.4 99.0 Table 3. Ablation of mixture and balance strategies for π0.5 on LIBERO with Hmax = 30 and stride = 3. Starting from the single-horizon π0.5 baseline, we compare: (i) temporal loss reweighting applied to π0.5 without MoH, (ii) uniform mean fusion over valid horizons (no gating head), (iii) MoH without the balance loss Lbal, and (iv) our full MoH with gated fusion and Lbal. In this subsection, through extensive ablation experiments on the LIBERO benchmark, we aim to answer four questions: 1. How does the horizon density of MoH influence the VLA models performance? 2. Does the loss-reweighting strategy help alleviate the horizon trade-off? 3. How does simple mean fusion of horizons without gating network perform? 4. How essential is the gating balance loss, and how does it affect the learned gating weights? Effect of horizon density. To understand how many horizons are needed for effective multi-horizon fusion, we fix Hmax = 30 and vary the stride used to construct the candidate set = {d, 2d, . . . , Hmax}. Table 2 compares the single-horizon π0.5 (H = 30) with MoH variants that use increasingly dense horizon sets. Introducing just three horizons (d = 10) already improves the average success rate from 97.7% to 98.3%, indicating that combining few coarse scales helps reconcile short-term accuracy and longhorizon foresight. Further densifying the candidate horizons leads to consistent gains, and the configuration with stride = 3 achieves the best overall performance (99.0% success), with particularly notable improvements on longhorizon tasks. We also observe that even when the number of horizon groups increases to 15 or 30, the MoH strategy consistently improves over the baseline without causing any training collapse. π0.5+MoH with stride = 3 provides the strongest overall results, suggesting that more horizon groups are not always better. Instead, choosing an appropriate stride can simultaneously yield strong performance while controlling training and inference cost. Overall, these results show that MoH reliably benefits from access to multiple horizons, and that moderately dense set of horizons is sufficient to capture complementary temporal structures. Together with the failure and challenge analyses in Appendix (Figures 11), these results further suggest that π0.5 with MoH already achieves sufficiently high success rates on LIBERO. Many of the remaining failures are largely attributable to environmental issues or limitations in instruction-following, which are outside the scope of what MoH is designed to address. Effect of loss reweighting. Table 3 disentangles the contributions of the key MoH components. First, we test whether the gains can be attributed purely to loss weighting rather than multi-horizon modeling. Motivated by the implicit emphasis on early steps induced by the MoH objective, we construct loss reweighting only variant that applies analogous temporal weights directly to the single-horizon π0.5, without introducing additional horizons or gating. This variant indeed improves performance on three short-term suites, but further degrades the Long suite, thereby intensifying the trade-off. The higher average success rate comes at the cost of long-horizon robustness, confirming that MoHs improvements are not explained by loss reweighting alone. MoH with simple average fusion. Second, replacing gated fusion with naive average fusion over all valid horizons, corresponding to the third line in Table 3, successfully alleviates the short-vs-long-horizon trade-off and yields modest overall improvements over the baseline. This result strongly supports our motivation: even the simplest implementation of MoH already works well, and the additional gating network is expected to further improve performance. Effect of gating balance loss. Finally, we evaluate MoH w/o Lbal to assess the role of the balance loss. As shown in line 4 of Table 3, gating without Lbal already provides clear gain over the π0.5 baseline, showing that learned gated fusion across horizons is inherently beneficial. We also present the statistics of gating weights at each valid action step on the LIBERO-Long task suite in Figure 5. Without Lbal, the gate head tends to assign higher weights to action chunks with longer horizons, because longer horizons participate in more steps during action mixture. This introduces statistical and gradient bias during training and manifests as an imbalance Figure 5. Visualization of horizon weights of π0.5 with MoH on LIBERO-Long task suite. The regulation term Lbal encourages the distribution balance across horizons. Without Lbal, the gating weights present obvious distribution preference at all times. The weights of H3 drop to 0 at steps 4 and 5 as it is no longer active. Figure 6. Visualization of the overhead under different horizon settings. Since the action transformer is typically lightweight, and combined with tensor parallelism, MoH incurs very little additional overhead for both training and inference. in gating learning. After introducing Lbal, this bias is effectively suppressed, enabling the gating head to better leverage predictions from each horizon. Meanwhile, because Lbal acts only as regularization term, it does not forcibly flatten the weights, thereby avoiding excessive averaging. Together, these ablations demonstrate that (i) multihorizon collaboration, (ii) learnable gated fusion, and (iii) gating balance regularization jointly contribute to robustly alleviating the horizon trade-off problem. 4.4. Latency Comparison In Figure 6, we present the training and inference time cost of π0 and π0.5 under different horizon settings. Benefiting from data parallelism, MoH brings very little additional time overhead for both training and inference. Importantly, the inference latency is virtually unaffected, which means that MoH does not impact the control frequency and fully preserves the usability of VLA models. 4.5. Effect of Dynamic Inference We compare the dynamic inference scheme introduced in subsection 3.3 with using fixed-length prefix. By default, we set = 5, = 5, = 3 and then change the value of to observe the corresponding performance of π0.5 with MoH on LIBERO-Long. See Figure 8, dynamic inference consistly outperforms the basic fixed-length strategy. Notably, even when the throughput is increased to 2.5 the default Figure 7. Example of dynamic inference on LIBERO-Long. π0.5 with MoH runs dynamic inference with scaling ratio = 1.1. After each action chunk prediction, only the prefix actions with horizon consensus are executed. Shorter chunks are selected near decision points and fine-grained manipulation, whereas longer chunks are used during smooth, low-risk motions. 5. Real-world Experiments 5.1. Experimental Setup Platform and Task Setup. We conduct real-robot experiments on the platform developed by Hong Kong Centre for Logistics Robotics to evaluate the effectiveness of MoH. As shown in Figure 9 (a), we adopt the single arm setting, which consists of 7-DoF manipulator and 1-DoF gripper. primary camera and wrist camera are installed to provide visual observations for VLA models. We also install camera to record task completion process. We design three evaluation tasks: two short-horizon tasks, T1: put bread into the bowl and T2: pour milk into the cup, and one long-horizon task, T3: put the pen into the drawer and close it. These tasks jointly require instruction following, object relocation and rotation, and precise grasping and placement, providing comprehensive evaluation of VLA models in real-world settings. Base Models and Implementation Details. We adopt π0 and π0.5 as base models and investigate the impact of integrating the MoH strategy. For each task, we collect 30 expert demonstrations for finetuning. All models are trained in 10k iterations with batch size of 32. The prefix 5 actions in each predicted chunk are executed by default. To ensure fair evaluation, models with and without MoH are executed sequentially from the same initial scene configurations. After each pair of rollouts, we perturb object poses and goal locations and orientations before the next trial. Each model is evaluated with 10 rollouts per task. The limitations of action steps are set to 2000 steps for short-horizon tasks and 3000 steps for the long-horizon task. All trajectories are recorded and released in the supplementary material for checking. 5.2. Result and Analysis As shown in Figure 9(b), across all three tasks and for both base models, the MoH strategy yields consistent performance gains. During evaluation, we observe that MoH improves both long-horizon decision making and short-horizon Figure 8. Dynamic inference v.s. fixed-length prefix. A.S is abbreviation of average action step number. setting (5 steps), π0.5 with MoH under dynamic inference still outperforms the baseline π0.5. In Fig. 7, we visualize one rollout on LIBERO-Long under dynamic inference. For this trajectory, we display most timesteps together with the action-chunk lengths that are actually executed. clear pattern emerges: around decision points, such as when the robot changes its movement direction or commits to approaching new target object, and during fine-grained manipulation (e.g., grasping and lifting the bottle), the policy tends to select only the shortest horizon of 5 steps. In contrast, when the system is in relatively stable and low-risk phase, such as translating the grasped object or moving the arm through free space toward pre-grasp configuration, the executed chunks become noticeably longer. This behavior directly aligns with the motivation behind dynamic inference: allowing the agent to move quickly when the task is simple and risk is low, while acting more cautiously and updating its plan more frequently during critical decision-making and precise manipulation. The qualitative evidence here also suggests that MoH-based dynamic inference implicitly captures task phases and uncertainty, highlighting its potential to balance efficiency and robustness in long-horizon control. Figure 9. Experimental settings and results in real-world scenarios. fine-grained action prediction of the underlying policies. Please refer to test videos in Supplementary Materials. For example, in the first task, the baseline policies typically exhibit period of back-and-forth hesitation and slowly approach the bread before committing to grasp. With MoH, this dithering behavior is largely reduced: the robot moves more directly towards the bread and executes faster, more decisive grasp. On longer-horizon tasks, we also find that MoH leads to more accurate grasps, which in turn results in quicker task completion and higher success rates. We also notice an interesting phenomenon on the pour milk into cup task: π0.5 performs worse than π0. closer inspection reveals that, after lifting the milk bottle, π0.5 often hesitates between continuing to pour and putting the bottle back. Since both actions appear in the training set and the policy does not receive explicit action history as input, this suggests that π0.5 overfits this local conflict during training. In contrast, equipping the model with MoH helps alleviate this overfitting, enabling clearer modeling of short-range motions and long-horizon intent. Overall, the real-world experiments are consistent with our simulation results, confirming that MoH effectively combines long-horizon planning with short-term control for practical robotic manipulation. 6. Conclusion In this paper, we introduced Mixture of Horizons, plugand-play strategy that fuses multi-horizon action chunks in full-attention VLA policies to ease the trade-off between long-term foresight and short-term precision. Across simulator benchmarks and real-world tasks, MoH consistently improves both flow-matching and one-step regression policies, achieving new state of the art on LIBERO with π0.5. Ablations confirm the benefits of dense horizons, gated fusion, gating balance regularization and dynamic inference."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1, 2 [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [3] Lucas Beyer, Andreas Steiner, Andre Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. Paligemma: versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726, 2024. 1, 2, 5 [4] Homanga Bharadhwaj, Jay Vakil, Mohit Sharma, Abhinav Gupta, Shubham Tulsiani, and Vikash Kumar. Roboagent: Generalization and efficiency in robot manipulation via semantic augmentations and action chunking, 2023. 2 [5] Johan Bjorck, Fernando Castaneda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, et al. Gr00t n1: An open foundation model for generalist humanoid robots. arXiv preprint arXiv:2503.14734, 2025. 5 [6] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. pi 0: vision-languageaction flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. 1, 2, 3, 5 [7] Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred Warmuth. Occams razor. Information processing letters, 24(6):377380, 1987. [8] Qingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui, Yan Ding, Siyuan Feng, Shenyuan Gao, Xindong He, Xuan Hu, Xu Huang, et al. Agibot world colosseo: large-scale manipulation platform for scalable and intelligent embodied systems. arXiv preprint arXiv:2503.06669, 2025. 1, 2 [9] Qingwen Bu, Yanting Yang, Jisong Cai, Shenyuan Gao, Guanghui Ren, Maoqing Yao, Ping Luo, and Hongyang Li. Univla: Learning to act anywhere with task-centric latent actions. arXiv preprint arXiv:2505.06111, 2025. 5 Seongjin Choi, and Lijun Sun. survey on vision-languageaction models for autonomous driving, 2025. 1 [10] Jun Cen, Chaohui Yu, Hangjie Yuan, Yuming Jiang, Siteng Huang, Jiayan Guo, Xin Li, Yibing Song, Hao Luo, Fan Wang, et al. Worldvla: Towards autoregressive action world model. arXiv preprint arXiv:2506.21539, 2025. 2 [11] Chilam Cheang, Sijin Chen, Zhongren Cui, Yingdong Hu, Liqun Huang, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, arXiv preprint Xiao Ma, et al. Gr-3 technical report. arXiv:2507.15493, 2025. [12] Tianxing Chen, Zanxin Chen, Baijun Chen, Zijian Cai, Yibin Liu, Zixuan Li, Qiwei Liang, Xianliang Lin, Yiheng Ge, Zhenyu Gu, Weiliang Deng, Yubin Guo, Tian Nian, Xuanbing Xie, Qiangyu Chen, Kailun Su, Tianling Xu, Guodong Liu, Mengkang Hu, Huan ang Gao, Kaixuan Wang, Zhixuan Liang, Yusen Qin, Xiaokang Yang, Ping Luo, and Yao Mu. Robotwin 2.0: scalable data generator and benchmark with strong domain randomization for robust bimanual robotic manipulation, 2025. 2, 5 [13] Yi Chen, Yuying Ge, Yizhuo Li, Yixiao Ge, Mingyu Ding, Ying Shan, and Xihui Liu. Moto: Latent motion token as the bridging language for robot manipulation. arXiv preprint arXiv:2412.04445, 2024. 2 [14] Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. arXiv preprint arXiv:2303.04137, 2023. 2, 5 [15] InternVLA-M1 Contributors. Internvla-m1: spatially guided vision-language-action framework for generalist robot policy. arXiv preprint arXiv:2510.13778, 2025. 1, 2 [16] Can Cui, Pengxiang Ding, Wenxuan Song, Shuanghao Bai, Xinyang Tong, Zirui Ge, Runze Suo, Wanqi Zhou, Yang Liu, Bofang Jia, et al. Openhelix: short survey, empirical analysis, and open-source dual-system vla model for robotic manipulation. arXiv preprint arXiv:2505.03912, 2025. 2 [17] Shengliang Deng, Mi Yan, Songlin Wei, Haixin Ma, Yuxin Yang, Jiayi Chen, Zhiqi Zhang, Taoyu Yang, Xuheng Zhang, Wenhao Zhang, Heming Cui, Zhizheng Zhang, and He Wang. Graspvla: grasping foundation model pre-trained on billionscale synthetic action data. 2025. [18] Cunxin Fan, Xiaosong Jia, Yihang Sun, Yixiao Wang, Jianglan Wei, Ziyang Gong, Xiangyu Zhao, Masayoshi Tomizuka, Xue Yang, Junchi Yan, et al. Interleave-vla: Enhancing robot manipulation with interleaved image-text instructions. arXiv preprint arXiv:2505.02152, 2025. 2 [19] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity, 2022. 4 [20] Chongkai Gao, Zixuan Liu, Zhenghao Chi, Junshan Huang, Xin Fei, Yiwen Hou, Yuxuan Zhang, Yudi Lin, Zhirui Fang, Zeyu Jiang, and Lin Shao. Vla-os: Structuring and dissecting planning representations and paradigms in vision-languageaction models. arXiv preprint arXiv:2506.17561, 2025. 2 [21] Sicong Jiang, Zilin Huang, Kangan Qian, Ziang Luo, Tianze Zhu, Yang Zhong, Yihong Tang, Menglin Kong, Yunlong Wang, Siwen Jiao, Hao Ye, Zihao Sheng, Xin Zhao, Tuopu Wen, Zheng Fu, Sikai Chen, Kun Jiang, Diange Yang, [22] Dong Jing, Xiaolong He, Yutian Luo, Nanyi Fei, Wei Wei, Huiwen Zhao, Zhiwu Lu, et al. Fineclip: Self-distilled regionbased clip for better fine-grained understanding. Advances in Neural Information Processing Systems, 37:2789627918, 2024. 2 [23] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. 1, 2, 5 [24] Moo Jin Kim, Chelsea Finn, and Percy Liang. Fine-tuning vision-language-action models: Optimizing speed and success. arXiv preprint arXiv:2502.19645, 2025. 1, 2, 3, 5 [25] Fuhao Li, Wenxuan Song, Han Zhao, Jingbo Wang, Pengxiang Ding, Donglin Wang, Long Zeng, and Haoang Li. Spatial forcing: Implicit spatial representation alignment for visionlanguage-action model. arXiv preprint arXiv:2510.12276, 2025. 2, [26] Qixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen, Mozheng Liao, Fangyun Wei, Yu Deng, Sicheng Xu, Yizhong Zhang, et al. Cogact: foundational vision-language-action model for synergizing cognition and action in robotic manipulation. arXiv preprint arXiv:2411.19650, 2024. 1, 2 [27] Tao Lin, Gen Li, Yilei Zhong, Yanwen Zou, Yuxin Du, Jiting Liu, Encheng Gu, and Bo Zhao. Evo-0: Vision-languageaction model with implicit spatial understanding. arXiv preprint arXiv:2507.00416, 2025. 2 [28] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 2 [29] Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. Libero: Benchmarking knowledge transfer for lifelong robot learning. arXiv preprint arXiv:2306.03310, 2023. 2, 5 [30] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in neural information processing systems, 35:57755787, 2022. 3 [31] Abby ONeill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, et al. Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 68926903. IEEE, 2024. 5 [32] Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, and Sergey Levine. Fast: Efficient action tokenization for visionlanguage-action models. arXiv preprint arXiv:2501.09747, 2025. [33] Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, Computer Vision and Pattern Recognition Conference, pages 17021713, 2025. 3, 5 [46] Tony Z. Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware. In Proceedings of Robotics: Science and Systems, Daegu, Republic of Korea, 2023. 1, 2 [47] Jinliang Zheng, Jianxiong Li, Dongxiu Liu, Yinan Zheng, Zhihao Wang, Zhonghong Ou, Yu Liu, Jingjing Liu, Ya-Qin Zhang, and Xianyuan Zhan. Universal actions for enhanced embodied foundation models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 22508 22519, 2025. 2 [48] Jinliang Zheng, Jianxiong Li, Zhihao Wang, Dongxiu Liu, Xirui Kang, Yuchun Feng, Yinan Zheng, Jiayin Zou, Yilun Chen, Jia Zeng, et al. X-vla: Soft-prompted transformer as scalable cross-embodiment vision-language-action model. arXiv preprint arXiv:2510.10274, 2025. 1, 2, 5 [49] Ruijie Zheng, Yongyuan Liang, Shuaiyi Huang, Jianfeng Gao, Hal Daume III, Andrey Kolobov, Furong Huang, and Jianwei Yang. Tracevla: Visual trace prompting enhances spatialtemporal awareness for generalist robotic policies. arXiv preprint arXiv:2412.10345, 2024. [50] Zhide Zhong, Haodong Yan, Junfeng Li, Xiangchen Liu, Xin Gong, Tianran Zhang, Wenxuan Song, Jiayi Chen, Xinhu Zheng, Hesheng Wang, and Haoang Li. Flowvla: Visual chain of thought-based motion reasoning for vision-languageaction models. arXiv preprint arXiv:2508.18269, 2025. 2 [51] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 2 [52] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 1, 2 et al. Spatialvla: Exploring spatial representations for visuallanguage-action model. arXiv preprint arXiv:2501.15830, 2025. 2 [34] David Rau. Sparsely-gated mixture-of-experts pytorch implementation, 2019. 4 [35] Lucy Xiaoyang Shi, Brian Ichter, Michael Equi, Liyiming Ke, Karl Pertsch, Quan Vuong, James Tanner, Anna Walling, Haohuan Wang, Niccolo Fusai, et al. Hi robot: Open-ended instruction following with hierarchical vision-language-action models. arXiv preprint arXiv:2502.19417, 2025. 2, 5 [36] Mustafa Shukor, Dana Aubakirova, Francesco Capuano, Pepijn Kooijmans, Steven Palma, Adil Zouitine, Michel Aractingi, Caroline Pascal, Martino Russi, Andres Marafioti, et al. Smolvla: vision-language-action model for affordable and efficient robotics. arXiv preprint arXiv:2506.01844, 2025. 1, 2, [37] Weiliang Tang, Dong Jing, Jia-Hui Pan, Zhiwu Lu, Yun-Hui Liu, Li Erran Li, Mingyu Ding, and Chi-Wing Fu. Incentivizing multimodal reasoning in large models for direct robot manipulation. arXiv preprint arXiv:2505.12744, 2025. 2 [38] Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, et al. Octo: An open-source generalist robot policy. arXiv preprint arXiv:2405.12213, 2024. 5 [39] Yihao Wang, Pengxiang Ding, Lingxiao Li, Can Cui, Zirui Ge, Xinyang Tong, Wenxuan Song, Han Zhao, Wei Zhao, Pengxu Hou, Siteng Huang, Yifan Tang, Wenhui Wang, Ru Zhang, Jianyi Liu, and Donglin Wang. Vla-adapter: An effective paradigm for tiny-scale vision-language-action model. arXiv preprint arXiv:2509.09372, 2025. 5 [40] Yuqi Wang, Xinghang Li, Wenxuan Wang, Junbo Zhang, Yingyan Li, Yuntao Chen, Xinlong Wang, and Zhaoxiang Zhang. Unified vision-language-action model. arXiv preprint arXiv:2506.19850, 2025. 2 [41] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. 2 [42] Andy Zhai, Brae Liu, Bruno Fang, Chalse Cai, Ellie Ma, Ethan Yin, Hao Wang, Hugo Zhou, James Wang, Lights Shi, Lucy Liang, Make Wang, Qian Wang, Roy Gan, Ryan Yu, Shalfun Li, Starrick Liu, Sylas Chen, Vincent Chen, and Zach Xu. Igniting vlms toward the embodied space. arXiv preprint arXiv:2509.11766, 2025. 2 [43] Wenyao Zhang, Hongsi Liu, Zekun Qi, Yunan Wang, Xinqiang Yu, Jiazhao Zhang, Runpei Dong, Jiawei He, He Wang, Zhizheng Zhang, Li Yi, Wenjun Zeng, and Xin Jin. Dreamvla: vision-language-action model dreamed with comprehensive world knowledge. CoRR, abs/2507.04447, 2025. 2 [44] Zhengshen Zhang, Hao Li, Yalun Dai, Zhengbang Zhu, Lei Zhou, Chenchen Liu, Dong Wang, Francis EH Tay, Sijin Chen, Ziwei Liu, et al. From spatial to actions: Grounding vision-language-action model in spatial foundation priors. arXiv preprint arXiv:2510.17439, 2025. [45] Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, et al. Cot-vla: Visual chain-of-thought reasoning for vision-language-action models. In Proceedings of the"
        },
        {
            "title": "Appendix",
            "content": "Hyperparameter Value"
        },
        {
            "title": "GPUs\nTotal Batch Size\nOptimizer\nScheduler\nLearning Rate\nIterations\nWarmup Step\nMinimum LR\nMax Gradient Norm",
            "content": "4 A100 32 AdamW Warmup & Cosine Decay 5e-5 30k 1k 1e-6 1.0 Table 4. Training hyperparameters of π series on LIBERO. A. Training hyperparameters In Table 4, we present hyperparamters used to train π0, π0.5 and πreg on LIBERO mixed dataset. Regarding the RoboTwin, we only adjust the learning rate to 2.5e 5 and train models for 20 epochs on 50 clean demos for each task. B. Design of πreg Figure 10. Illustration of our designed πreg, with little modification based on π0. We introduce learnable query token as query input for action transformer. Actions are predicted in one forward pass. Since the π0 project does not release its regression type, we obtain the πreg by finetuning from the π0 base model. As shown in Figure 10, we introduce learnable query and expand it to the length of action chunk to serve as the input queries for action transformer. The action chunk are predicted in only one forward process. The training objective is continuous regression function introduced in Section 3.1. C. More Study on Horizon Effect In the Introduction 1, we analyzed the horizon effect on π0. In this section, we provide further study based on π0.5, Horizon Spatial Object Goal Average 10 20 30 MoH 99.0 98.8 98.8 98.8 98.8 98.2 99.0 99.8 98 97.6 97.6 97.6 92.4 94.6 95.4 96. 97.1 97.3 97.7 98.3 Table 5. Effect of action horizon on π0.5. The first 5 actions in the predicted chunk are executed at evaluation. Our mixture of horizons {10, 20, 30} strategy alleviates the trade-off caused by varying horizons and raises overall success. with results shown in Table 5. As the horizon increases, π0.5 exhibits similar trade-off across the four tasks: performance on short-horizon tasks fluctuates, while performance on long-horizon tasks steadily improves. In contrast, our MoH strategy effectively mitigates this trade-off and substantially improves the overall success rate. D. Challenge and Failure Case Analysis LIBERO. We manually inspect rollouts in LIBERO and identify four predominant failure modes, three of which are illustrated in Figure 11. The first two stem from artifacts of the environment rather than the VLA policy. In (a), the robot successfully completes the task, but the simulator fails to register success and the episode is terminated when the maximum number of action steps is reached. In (b), collision bug in the Spatial task suite causes the bowl and ramekin to become stuck together, making the task unsolvable. If we ignore these two environment-induced errors, π0.5 with MoH would achieve an impressive 99.8% success rate on LIBERO-Spatial, instead of the 98.8% reported in the main table. Panel (c) illustrates the third type of failure, where the model misidentifies the target object, revealing remaining limitations in the visual perception and instruction-following capabilities of current VLA models. Our MoH strategy is not designed to directly address these perceptionand languageunderstanding issues. The fourth and most frequent failure mode arises from insufficient low-level action precision, for which we provide demonstrations in the supplementary videos. RoboTwin. Figure 12 highlights several challenging factors and potential issues we observe in RoboTwin2.0 simulator. First, Figure 12(a) shows that RoboTwin can also fail to signal task completion even when the robot has clearly succeeded. Based on manual inspection of the termination code, we find that some success conditions are overly strict, and small errors in the absolute position thresholds can even bias the states that are labeled as successful. Figure 12(b) illustrates scene in open microwave Figure 11. Typical failure modes in LIBERO. Figure 12. Challenges and issues exist in RoboTwin simulator. E. Demonstrations To better illustrate the behavior of our policy, we present qualitative rollouts produced by π0.5 with MoH in both simulated and real environments. Figure 13 shows representative executions on several challenge LIBERO tasks and on our real-world setup. The policy is able to predict precise lowlevel motions and complete long-horizon, multi-stage goals. Figure 14 further visualizes trajectories on RoboTwin 2.0. These qualitative demonstrations show that π0.5 with MoH is capable to process diverse in-domain tasks and generalized to complex unseen environments. task with severe occlusion. In the view, the manipulator completely blocks the target object, making it difficult for the model to correctly infer the current state from the observation. Providing richer history information should help the policy make better decisions and control in such cases, thereby improving success rates in heavily occluded scenes. In Figure 12(c), many RoboTwin tasks start with the robot arm entirely outside the main cameras field of view. In this situation, VLA models can only infer the arms pose and state from its shadow or the proprioceptive inputs. We regard this as rather extreme setting: under normal circumstances, the camera configuration should provide sufficient informative observations. Otherwise, the model may learn shortcuts tailored to this special case, which is undesirable for generalization across diverse scenes. Under the random setting, RoboTwin randomizes the background, object locations, and orientations. Figure 12(d) shows an example of illegible target from the click alarm clock task where the alarm clock is placed facing away from the camera. From this viewpoint it is very hard for the model to recognize the object as an alarm clock, and the button region is barely visible, which often leads to failures. This suggests that there exist non-trivial number of scenes that are extremely difficult for VLA models to solve. Figure 13. Qualitative demonstrations of π0.5 with MoH on LIBERO and real-world tasks. Figure 14. Qualitative demonstrations of π0.5 with MoH on RoboTwin 2.0. The last four lines are collected under random settings."
        }
    ],
    "affiliations": [
        "CUHK",
        "RUC",
        "UNC"
    ]
}