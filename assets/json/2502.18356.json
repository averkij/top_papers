{
    "paper_title": "WebGames: Challenging General-Purpose Web-Browsing AI Agents",
    "authors": [
        "George Thomas",
        "Alex J. Chan",
        "Jikun Kang",
        "Wenqi Wu",
        "Filippos Christianos",
        "Fraser Greenlee",
        "Andy Toulis",
        "Marvin Purtorab"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce WebGames, a comprehensive benchmark suite designed to evaluate general-purpose web-browsing AI agents through a collection of 50+ interactive challenges. These challenges are specifically crafted to be straightforward for humans while systematically testing the limitations of current AI systems across fundamental browser interactions, advanced input processing, cognitive tasks, workflow automation, and interactive entertainment. Our framework eliminates external dependencies through a hermetic testing environment, ensuring reproducible evaluation with verifiable ground-truth solutions. We evaluate leading vision-language models including GPT-4o, Claude Computer-Use, Gemini-1.5-Pro, and Qwen2-VL against human performance. Results reveal a substantial capability gap, with the best AI system achieving only 43.1% success rate compared to human performance of 95.7%, highlighting fundamental limitations in current AI systems' ability to handle common web interaction patterns that humans find intuitive. The benchmark is publicly available at webgames.convergence.ai, offering a lightweight, client-side implementation that facilitates rapid evaluation cycles. Through its modular architecture and standardized challenge specifications, WebGames provides a robust foundation for measuring progress in development of more capable web-browsing agents."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 6 5 3 8 1 . 2 0 5 2 : r WebGames: Challenging General-Purpose Web-Browsing AI Agents George Thomas1,2, Alex J. Chan1, Jikun Kang1, Wenqi Wu1, Filippos Christianos1, Fraser Greenlee1, Andy Toulis1, and Marvin Purtorab.1 1Convergence Labs Ltd., 2Clusterfudge Ltd. We introduce WebGames, comprehensive benchmark suite designed to evaluate general-purpose webbrowsing AI agents through collection of 50+ interactive challenges. These challenges are specifically crafted to be straightforward for humans while systematically testing the limitations of current AI systems across fundamental browser interactions, advanced input processing, cognitive tasks, workflow automation, and interactive entertainment. Our framework eliminates external dependencies through hermetic testing environment, ensuring reproducible evaluation with verifiable ground-truth solutions. We evaluate leading vision-language models including GPT-4o, Claude Computer-Use, Gemini-1.5-Pro, and Qwen2-VL against human performance. Results reveal substantial capability gap, with the best AI system achieving only 41.2% success rate compared to human performance of 95.7%, highlighting fundamental limitations in current AI systems ability to handle common web interaction patterns that humans find intuitive. The benchmark is publicly available at webgames.convergence.ai, offering lightweight, client-side implementation that facilitates rapid evaluation cycles. Through its modular architecture and standardized challenge specifications, WebGames provides robust foundation for measuring progress in development of more capable web-browsing agents. Date: 25 February 2025 Correspondence: Alex J. Chan at alex@convergence.ai Website: https://webgames.convergence.ai Code: https://github.com/convergence-ai/webgames Dataset: https://huggingface.co/datasets/convergence-ai/webgames"
        },
        {
            "title": "1 Introduction",
            "content": "We are entering the era of AI Agents; large multi-modal models are finally able to complete reasonable multi-step tasks while interacting with the virtual world (Gur et al., 2023; Ma et al., 2023; Zheng et al., 2024; Putta et al., 2024). Websites and GUI desktops have been developed primarily for human interaction, requiring sophisticated understanding of visual layouts, interactive elements, and temporal dependencies. Effective navigation and task execution requires an understanding of large number of possible interfaces, from basic button clicks to complex drag-and-drop operations and state-dependent interactions. It is key to be able to robustly test the abilities of AI agents in these human-centric environments, and while existing benchmarks have made progress in evaluating specific aspects of web interaction like online shopping (Yao et al., 2022a) and booking flights (He et al., 2024), they often lack comprehensive coverage of the rich interaction patterns that characterize modern web applications. Here, we introduce WebGames, comprehensive benchmark suite designed to evaluate general-purpose web-browsing AI agents across diverse range of interaction paradigms. Our framework features over"
        },
        {
            "title": "WebGames",
            "content": "unique challenges that are intentionally crafted to be straightforward for humans while testing the limitations of current AI systems. Each challenge isolates specific interaction capabilities, from fundamental browser operations to complex cognitive tasks, enabling precise measurement of agent competencies. We test the general ability of the leading vision-language foundation models, including GPT-4o (OpenAI, 2023), Claude Computer-Use (Sonnet 3.5) (Anthropic, 2023), Gemini-1.5-Pro (Gemini-Team, 2023), and Qwen2-VL (Bai et al., 2023), as well as our Proxy assistant, comparing their performance against human baselines. Our results reveal significant gaps between human and AI performance, particularly in tasks requiring precise temporal coordination, spatial reasoning, and adaptation to dynamic environments. These findings highlight crucial areas for improvement in the development of more capable web-browsing agents."
        },
        {
            "title": "2 The WebGames Benchmark",
            "content": "WebGames is designed around five core design principles that facilitate robust evaluation of AI systems: Human-Centric Design: All tasks are calibrated to human cognitive and interaction capabilities, establishing clear baseline for performance evaluation AI Challenging: Specifically crafted to test the limitations of current AI systems Lightweight Implementation: The framework operates entirely client-side using single-page JavaScript architecture, minimizing deployment complexity Verifiable Completion: Each challenge implements deterministic verification system, producing unique completion tokens that serve as proof of task success Isolated Capability Testing: Individual challenges are constructed to evaluate discrete browser interaction capabilities, enabling precise measurement of agent competencies"
        },
        {
            "title": "2.1 Evaluation Categories\nThe aim is to encompass five primary categories of evaluation, each designed to assess distinct aspects of\nweb interaction capabilities. These categories progressively increase in complexity, from basic interactions to\nsophisticated cognitive tasks.",
            "content": "Fundamental Browser Interaction forms the foundation of web navigation capabilities. These challenges assess an agents ability to perform essential operations such as selecting and activating DOM elements, manipulating viewport positions, opening tabs, and handling basic file system operations including download, parsing, and upload tasks. Success in these challenges demonstrates mastery of the core building blocks necessary for web interaction. Advanced Input Processing evaluates sophisticated interaction patterns common in modern web applications. This category encompasses precise drag-and-drop operations, hover state management for dynamic content, and complex keyboard command interpretation. These challenges mirror the rich interaction patterns found in contemporary web applications, requiring agents to demonstrate fine-grained control and temporal coordination. Cognitive and Memory Tasks push beyond mechanical interactions to test higher-order reasoning capabilities. Agents must navigate tree-based search problems, construct mental maps of complex environments, interpret data visualizations, and maintain state information across multiple interactions. These challenges evaluate an agents ability to plan, reason, and adapt to changing environmental conditions."
        },
        {
            "title": "WebGames",
            "content": "Figure 1 Challenges: The WebGames homepage displaying the first 28 web interaction challenges. Each challenge tile presents brief description, spanning fundamental browser operations to complex interactive games. Workflow Automation assesses practical task completion in realistic scenarios. Challenges include ecommerce inventory management, retail transaction processing, and temporal event coordination. These tasks require agents to integrate multiple capabilities while maintaining consistency across extended interaction sequences, mirroring real-world use cases. Interactive Entertainment Systems represent the most dynamic category, featuring real-time interaction challenges. These include classical arcade game reproductions, obstacle navigation tasks, and physics engine interactions. Success in these challenges requires rapid processing of visual information, precise timing, and adaptive strategy formation."
        },
        {
            "title": "3 Agent Performance",
            "content": "We bench-marked the leading large vision-language foundation models including GPT-4o (OpenAI, 2023), Claude Computer-Use (Sonnet 3.5) (Anthropic, 2023), Gemini-1.5-Pro (Gemini-Team, 2023), and Qwen2-VL (Bai et al., 2023), as well as our Proxy assistant; we report success rates in Table 1. The majority of these foundation models were not designed around web interactions and so typically need scaffolding in order to effectively interact with the web, which is done primarily through Chromium browser using Playwright (Playwright-Team). With the exception of Claude, most do not have sufficient GUI grounding and understanding to effectively determine exact pixel based locations on the screen. Thus, we take Set-of-Marks (SoMs) approach (Yang et al., 2023), using JavaScript to identify and highlight relevant"
        },
        {
            "title": "WebGames",
            "content": "Figure 2 Model Views: Set-of-Marks on the Wolf, Goat, and Cabbage problem. elements on the screen (an example is shown in Figure 2). The models then have access to tools that allow them to click, type, etc. on these elements. Models interact with the browser as an agent in partially observed Markov decision process (POMDP) (Sutton, 2018), where available actions are defined by the possible tool calls (detailed in Appendix A), and observations consist of JPEG screenshot of the current browser as well as text listing the extracted SoM elements. When taking an action, the models see the previous two observations in order to manage context length (as images can take up 1000s of tokens and number of steps required to solve some tasks can easily exceed 50). They are prompted in ReAct style (Yao et al., 2022b) in order to first generate reasoning step that summarises changes in the environment, determines whether the task is completed, and reasons about the next action, before then generating specific tool call in order to execute the next action. The interaction loop of model taking actions, followed by receiving observations continues until the model reasons that COMPLETE: true or the model exceeds predetermined max steps. Claude Computer-Use scores lower than GPT-4 despite having more complete action space over the web environment it is interacting with, including precise coordinate-based mouse control. This appears to mostly be result of the specific training/prompting that Anthropic worked into the model in order to specifically discourage it from in any way pretending to be human user or do anything potentially dangerous. For example, in one of the challenges, Claude refuses to click checkbox that confirms that it is human user, while all of the other models have no such qualms."
        },
        {
            "title": "3.1 Human Comparison\nTo compare with baseline human performance, we recruited 20 participants from the crowd-sourcing platform\nhttps://prolific.com, filtering to workers in the United Kingdom and self-identifying as having good web\nliteracy. Participants were paid £18 to complete the task, taking an average of ∼ 80 minutes to complete the\nfull set of questions.",
            "content": "Comparatively, humans have very little problem completing the majority of the tasks (and none of them were considered impossible as multiple participants scored 100%), highlighting substantial capabilities gap similar 4 Table 1 Model Performance: Scores achieved by leading vision-language foundation models."
        },
        {
            "title": "Environment\nWebbrowser",
            "content": "Model GPT-4o Claude Computer-Use Linux Machine Gemini-1.5-Pro Qwen2-VL-7b Qwen2-VL-72b Proxy"
        },
        {
            "title": "Webbrowser\nWebbrowser\nWebbrowser\nWebbrowser",
            "content": "Scaffolding SoMs + ReAct Prompting ReAct Prompting SoMs + ReAct Prompting SoMs + ReAct Prompting SoMs + ReAct Prompting -"
        },
        {
            "title": "Computer",
            "content": "- Performance (%) 41.2 7.0 35.3 6.8 27.5 6.3 13.7 4.9 29.4 6.4 43.1 7.0 95.7 0.6 to the ARC challenge (Chollet et al., 2024). With humans able to demonstrate all these skills some challenges can act effectively as unit-tests for some agent capabilities. For example, Slider symphony specifically requires agents to be able to precisely drag elements between two locations on the screen. Systems that arent able to localize or drag will have no chance at completing the challenge. If models are then not able to score highly on WebGames we can be confident there will be aspects of modern websites that they will be incapable of interacting with no matter how intelligent the underlying model is."
        },
        {
            "title": "4 Related Work",
            "content": "Autonomous agent evaluation frameworks have progressed significantly, beginning with traditional reinforcement learning environments (Brockman, 2016), and expanding into complete web domains (Shi et al., 2017; Liu et al., 2018). significant challenge in benchmark design has been balancing comprehensiveness with practicality. Traditional benchmarks often focus on single-turn or short-context scenarios, which can lead to rapid benchmark saturation (Kiela et al., 2021) and may not fully capture the capabilities needed for effective agentic foundation models. Modern web interaction requires complex mix of capabilities including tool usage, planning, environmental reasoning, and practical task execution. This has led to recent advancements introducing benchmarks for static webpage interaction (Deng et al., 2024) as well as specialized evaluation frameworks across various domains, from office-related tasks (Liu et al., 2023; Qin et al., 2024) to web navigation (Yao et al., 2022a; Zhou et al., 2023) and GitHub issue resolution (Jimenez et al., 2023). Multi-agent interaction represents an emerging frontier in this space. Recent research has explored LLMs capabilities in both cooperative (Gong et al., 2023; Piatti et al., 2024) and competitive (Jin et al., 2024; Wu et al., 2024) scenarios. This work highlights the importance of evaluating not just isolated capabilities, but also agents ability to interact effectively with other autonomous systems. WebGames makes couple of key distinctions in order to provide consistent and meaningful evaluation. Unlike task sets such as WebVoyager (He et al., 2024), that require models to use the regular internet, it maintains hermetic testing environment, eliminating external dependencies and network variables. This controlled local context also ensures reproducible evaluation by providing verifiable ground-truth solutions. Compared to other hosted benchmarks like WebArena (Zhou et al., 2023), it offers reduced operational overhead as it is significantly simpler to deploy locally, while also maintaining public accessibility via https://webgames.convergence.ai."
        },
        {
            "title": "5 Conclusions",
            "content": "Our evaluation of WebGames demonstrates significant performance gap between current AI systems and human capabilities in web interaction tasks. Even the best-performing model, GPT-4o, achieves only 41.2% success rate compared to human performance of 95.7%. This disparity highlights fundamental limitations in current AI systems ability to handle common web interaction patterns that humans find intuitive."
        },
        {
            "title": "WebGames",
            "content": "Interestingly, Claude Computer-Uses lower performance despite its expanded action space highlights how safety constraints can impact task completion, raising important questions about balancing capability with responsible AI deployment. The strong performance of our Proxy assistant (43.1%) suggests that specialized architectures for web interaction may offer advantages over general-purpose vision-language models."
        },
        {
            "title": "5.1 Future Directions\nWe plan to continually expand WebGames with additional challenges over time, including:",
            "content": "Difficulty Levels: Introducing graduated difficulty tiers within each challenge category to better track incremental progress in agent capabilities Multi-Agent Scenarios: Developing challenges that require coordination between multiple agents, testing collaborative web interaction capabilities Dynamic Content: Adding challenges with procedurally generated content to evaluate agents adaptability to novel situations Accessibility Testing: Including challenges that evaluate agents ability to interact with accessibility features and alternative interface paradigms Performance Metrics: Expanding evaluation criteria beyond binary success/failure to include efficiency measures like completion time and action economy The significant gap between human and AI performance on WebGames suggests that considerable progress is still needed in developing truly capable web-browsing agents. We hope this benchmark will serve as valuable tool for measuring progress and identifying specific areas for improvement in the development of more sophisticated AI systems."
        },
        {
            "title": "References",
            "content": "UK AI Safety Institute. Inspect AI: Framework for Large Language Model Evaluations. URL https://github.com/ UKGovernmentBEIS/inspect_ai. 3, 10 Anthropic. Model card and evaluations for claude models, 2023. URL https://www-files.anthropic.com/production/ images/Model-Card-Claude-2.pdf. 2, 3 Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 2, 3 Brockman. Openai gym. arXiv preprint arXiv:1606.01540, 2016. Francois Chollet, Mike Knoop, Gregory Kamradt, and Bryan Landers. Arc prize 2024: Technical report. arXiv preprint arXiv:2412.04604, 2024. 5 Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. Advances in Neural Information Processing Systems, 36, 2024. 5 Google DeepMind Gemini-Team. Gemini: family of highly capable multimodal models, 2023. 2, 3 Ran Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Zane Durante, Yusuke Noda, Zilong Zheng, Song-Chun Zhu, Demetri Terzopoulos, Li Fei-Fei, et al. Mindagent: Emergent gaming interaction. arXiv preprint arXiv:2309.09971, 2023. Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. real-world webagent with planning, long context understanding, and program synthesis. arXiv preprint arXiv:2307.12856, 2023. 1 Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.13919, 2024. 1,"
        },
        {
            "title": "WebGames",
            "content": "Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. 5 Xuanfa Jin, Ziyan Wang, Yali Du, Meng Fang, Haifeng Zhang, and Jun Wang. Learning to discuss strategically: case study on one night ultimate werewolf. arXiv preprint arXiv:2405.19946, 2024. 5 Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, et al. Dynabench: Rethinking benchmarking in nlp. arXiv preprint arXiv:2104.14337, 2021. 5 Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement learning on web interfaces using workflow-guided exploration. arXiv preprint arXiv:1802.08802, 2018. 5 Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688, 2023. 5 Kaixin Ma, Hongming Zhang, Hongwei Wang, Xiaoman Pan, and Dong Yu. Laser: Llm agent with state-space exploration for web navigation. arXiv preprint arXiv:2309.08172, 2023. 1 OpenAI. Gpt-4 technical report, 2023. 2, Giorgio Piatti, Zhijing Jin, Max Kleiman-Weiner, Bernhard Schölkopf, Mrinmaya Sachan, and Rada Mihalcea. Cooperate or collapse: Emergence of sustainability behaviors in society of llm agents. arXiv preprint arXiv:2404.16698, 2024. 5 Playwright-Team. Playwright. URL https://playwright.dev/. 3 Pranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, and Rafael Rafailov. Agent q: Advanced reasoning and learning for autonomous ai agents. arXiv preprint arXiv:2408.07199, 2024. 1 Yanzhao Qin, Tao Zhang, Yanjun Shen, Wenjing Luo, Haoze Sun, Yan Zhang, Yujing Qiao, Weipeng Chen, Zenan arXiv preprint Zhou, Wentao Zhang, et al. Sysbench: Can large language models follow system messages? arXiv:2408.10943, 2024. 5 Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of bits: An open-domain platform for web-based agents. In International Conference on Machine Learning, pages 31353144. PMLR, 2017. Richard Sutton. Reinforcement learning: An introduction. Bradford Book, 2018. 4 Shuang Wu, Liwen Zhu, Tao Yang, Shiwei Xu, Qiang Fu, Yang Wei, and Haobo Fu. Enhance reasoning for large language models in the game werewolf. arXiv preprint arXiv:2402.02330, 2024. 5 Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023. 3 Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:2074420757, 2022a. 1, Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022b. 4 Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v (ision) is generalist web agent, if grounded. arXiv preprint arXiv:2401.01614, 2024. 1 Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023."
        },
        {
            "title": "A Tools",
            "content": "Code definition of the tool parameters given to Agents using Set-of-Mark scaffolding to allow them to interact with elements in the browser: s GotoParams ( BaseModel ) : : = l ( . . . , c t n=\"The web r t s . Must be l URL . \" ) s GoogleSearchParams ( BaseModel ) : r = l ( query_plan : . . . , c t n=\" Plan out u . \" , l e t ) query : = l ( . . . , c t n=\"The Google r o f . \" ) query you l make . Re - t e s way t l a ClickParams ( BaseModel ) : mark_id : = l ( . . . , c t n=\" Element Mark ID . \" ) s TypeEntry ( BaseModel ) : mark_id : t : = l ( . . . , c t n=\" Element Mark ID . \" ) = l ( . . . , c t n=\"The t t i t l n . \" ) s TypeParams ( BaseModel ) : r : t [ TypeEntry ] = l ( . . . , c t n=\"A t l n and t s y . \" , ) submit : l = l ( . . . , c t n= Whether r ) \" Enter \" key e p i e t r . , s r P m ( BaseModel ) : e o : e [ \"up\" , \"down\" , \" t \" , \" h \" ] = l ( . . . , c t n= e o s l . Must be one \"up \" , \"down \" , \" t \" \" h \" . , ) mark_id : . . . , c t n=\"What c l . Use -1 c l = l ( ID an m t i c l e . \" , ) whole page e s v e mark s BackParams ( BaseModel ) : s s WaitParams ( BaseModel ) : s a ReloadParams ( BaseModel ) : s"
        },
        {
            "title": "B List of Tasks",
            "content": "1. Todays date: Enter todays date"
        },
        {
            "title": "WebGames",
            "content": "2. Button megastar: Click everything on the page to reveal the password 3. Click³: Click three times before time runs out 4. Patience test: Wait the perfect amount of time to reveal the password 5. Slider symphony: Align the boxes by mastering the vertical sliders! 6. Emoji remember: Remember the sequence of emojis to unlock the secret 7. Bullseye: Click the moving target three times - but watch out, it gets faster! 8. Accept: Prove youre human by agreeing to our terms 9. River Crossing: Transport wolf, goat, and cabbage across the river safely 10. Towers of Hanoi: Stack the disks on the rightmost peg following the rules 11. Color Harmony: Mix the perfect color combination using RGB sliders 12. Sheep Herding: Guide the wandering sheep into their pen by hovering with your cursor 13. File Upload: Upload any file to complete this challenge 14. Canvas Catch: Drag the circle into the target zone to complete the challenge 15. Brick buster: Break all the bricks to win! 16. Text Mirror: Perfectly copy the text 17. Frog Crossing: Guide your frog safely across the busy road 18. Button Hold: Hold the button for exactly 3 seconds 19. Key Combo: Press the correct key combination to unlock the secret 20. Scroll vertical: Scroll down to find the password 21. Scroll horizontal: Scroll right to find the password 22. WebGL Text: Recognise the WebGL shape 23. File Credentials: Download credentials file and use it to log in 24. Webs, Assemble!: Find the secret code hidden in the WebAssembly module 25. Menu Navigator: Navigate through menu bar to find the secret option 26. Popup Chaos: Close the annoying popup windows to reveal the secret password 27. Chart Read: Find the maximum price and time in the stock chart 28. Chart Transcribe: Transcribe the bar chart data into CSV format 29. Combination Lock: Solve Grampas riddles to unlock the combination 30. Pixel Copy: Recreate the pattern by toggling pixels in the grid 31. Restricted Content: Access this content at your own risk. Your actions are being monitored. 32. Prompt Defender: Can you resist deception and find the real password? 33. Shopping Challenge: Add items to your cart and calculate the total price to win! 34. The Maze: Navigate through series of doors to find the exit - but choose wisely! 35. Context Breaker: Can you scroll all the way to the bottom to find the secret password? 36. Diagonal Scroll: Navigate to the bottom-right corner through diagonal scrolling! 37. Block Stack: Stack blocks above the red line using physics to win!"
        },
        {
            "title": "WebGames",
            "content": "38. Nested Frames: Navigate through nested iframes to find the hidden button 39. Tab Sync: Synchronize colors between browser tabs to reveal the password 40. OTP Entry: Enter 6-digit one-time password with auto-focusing inputs 41. Print to Reveal: Print this page to PDF to reveal the hidden password 42. Human Verification: Complete CAPTCHA challenge to prove youre human 43. Right Click Reveal: Use your context menu skills to reveal the hidden password 44. Calendar Comprehension: Study calendar and answer questions about the events 45. Map Panner: Pan around mysterious map to find the hidden treasure 46. LadyBird Planner: Plan the ladybirds path to reach the flower using directional emojis 47. Shop Admin: Update product prices in the admin panel 48. Pixel Perfect: Can you click on single pixel target? 49. Recipe Calculator: Help calculate the right amount of ingredients for dinner party 50. Advanced Calendar Challenge: Test your calendar comprehension skills with complex time calculations 51. Stock Market Insight: Discover the best tech stock to buy in"
        },
        {
            "title": "C Running WebGames",
            "content": "We recommend using the Inspect AI library (AI Safety Institute) for running the benchmark simply and efficiently. The following code can be used to obtain the dataset and appropriate Scorer, which then can be easily run as Task with the Solver of choice. from i import Any , i l from p _ . a from p _ . r import a , Sample , _ a import ( co , Target , u y , r , e , CORRECT, INCORRECT, ) from p _ . v import k t TASK_PROMPT = \" Your k : { c t } . You must go { homepage } and a h password t password c l t a . \" game . To v you have p e e k you must a and o t game . you do not have password , you have not managed WEBGAMES_BASE_URL = \" p : / / webgames . v e . \" _record_to_sample ( o : t [ , Any ] ) -> Sample : = o [ \" \" ] l = o [ \" l \" ] c t = o [ \" c t \" ] password = o [ \" password \" ] path = o [ \" path \" ] homepage = \" {WEBGAMES_BASE_URL}/{ path } \" s _ u = TASK_PROMPT. m ( c t n=d r i , homepage=homepage )"
        },
        {
            "title": "WebGames",
            "content": "r r Sample ( u t=task_input , g t=password , metadata={ id , l , \" \" : \" l \" : \" path \" : path , \" homepage \" : homepage , \" c t \" : c t , } , ) get_webgames_dataset ( i : i l [ ] = None , f : l = s ) -> a : u f _ a ( \" v e - / webgames \" , i =\" i \" , p _ l s=_record_to_sample , i t=l t , f e=s f , ) @ r ( r =[ u y ( ) , webgames_scorer ( ) : e ( ) ] ) n f r ( t : TaskState , g : g ) : answer = t . output . p i o c = g . t u c ( u e=CORRECT i answer r r r o l INCORRECT, answer=answer )"
        }
    ],
    "affiliations": [
        "Clusterfudge Ltd.",
        "Convergence Labs Ltd."
    ]
}