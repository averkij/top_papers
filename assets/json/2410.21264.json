{
    "paper_title": "LARP: Tokenizing Videos with a Learned Autoregressive Generative Prior",
    "authors": [
        "Hanyu Wang",
        "Saksham Suri",
        "Yixuan Ren",
        "Hao Chen",
        "Abhinav Shrivastava"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present LARP, a novel video tokenizer designed to overcome limitations in current video tokenization methods for autoregressive (AR) generative models. Unlike traditional patchwise tokenizers that directly encode local visual patches into discrete tokens, LARP introduces a holistic tokenization scheme that gathers information from the visual content using a set of learned holistic queries. This design allows LARP to capture more global and semantic representations, rather than being limited to local patch-level information. Furthermore, it offers flexibility by supporting an arbitrary number of discrete tokens, enabling adaptive and efficient tokenization based on the specific requirements of the task. To align the discrete token space with downstream AR generation tasks, LARP integrates a lightweight AR transformer as a training-time prior model that predicts the next token on its discrete latent space. By incorporating the prior model during training, LARP learns a latent space that is not only optimized for video reconstruction but is also structured in a way that is more conducive to autoregressive generation. Moreover, this process defines a sequential order for the discrete tokens, progressively pushing them toward an optimal configuration during training, ensuring smoother and more accurate AR generation at inference time. Comprehensive experiments demonstrate LARP's strong performance, achieving state-of-the-art FVD on the UCF101 class-conditional video generation benchmark. LARP enhances the compatibility of AR models with videos and opens up the potential to build unified high-fidelity multimodal large language models (MLLMs)."
        },
        {
            "title": "Start",
            "content": "Preprint. LARP: TOKENIZING VIDEOS WITH LEARNED AUTOREGRESSIVE GENERATIVE PRIOR Saksham Suri, Hanyu Wang, University of Maryland, College Park {hywang66, sakshams, yxren}@umd.edu Project page: https://hywang66.github.io/larp/ Yixuan Ren, Hao Chen, Abhinav Shrivastava {chenh, abhinav}@cs.umd.edu 4 2 0 2 8 2 ] . [ 1 4 6 2 1 2 . 0 1 4 2 : r Figure 1: LARP highlights. (a) LARP is video tokenizer for two-stage video generative models. In the first stage, LARP tokenizer is trained with lightweight AR prior model to learn an AR-friendly latent space. In the second stage, an AR generative model is trained on LARPs discrete tokens to synthesize high-fidelity videos. (b) The incorporation of the AR prior model significantly improves the generation FVD (gFVD) across various token number configurations. (c) LARP shows much smaller gap between its reconstruction FVD (rFVD) and generation FVD (gFVD), indicating the effectiveness of the optimized latent space it has learned."
        },
        {
            "title": "ABSTRACT",
            "content": "We present LARP, novel video tokenizer designed to overcome limitations in current video tokenization methods for autoregressive (AR) generative models. Unlike traditional patchwise tokenizers that directly encode local visual patches into discrete tokens, LARP introduces holistic tokenization scheme that gathers information from the visual content using set of learned holistic queries. This design allows LARP to capture more global and semantic representations, rather than being limited to local patch-level information. Furthermore, it offers flexibility by supporting an arbitrary number of discrete tokens, enabling adaptive and efficient tokenization based on the specific requirements of the task. To align the discrete token space with downstream AR generation tasks, LARP integrates lightweight AR transformer as training-time prior model that predicts the next token on its discrete latent space. By incorporating the prior model during training, LARP learns latent space that is not only optimized for video reconstruction but is also structured in way that is more conducive to autoregressive generation. Moreover, this process defines sequential order for the discrete tokens, progressively pushing them toward an optimal configuration during training, ensuring smoother and more accurate AR generation at inference time. Comprehensive experiments demonstrate LARPs strong performance, achieving state-of-the-art FVD on the UCF101 class-conditional video generation benchmark. LARP enhances the compatibility of AR models with videos and opens up the potential to build unified high-fidelity multimodal large language models (MLLMs). 1 Preprint."
        },
        {
            "title": "INTRODUCTION",
            "content": "The field of generative modeling has experienced significant advancements, largely driven by the success of autoregressive (AR) models in the development of large language models (LLMs) (Bai et al., 2023; Brown, 2020; Radford et al., 2019; Google et al., 2023; Touvron et al., 2023a;b). Building on AR transformers (Vaswani, 2017), these models are considered pivotal for the future of AI due to their exceptional performance (Hendrycks et al., 2020; 2021), impressive scalability (Henighan et al., 2020; Kaplan et al., 2020; Rae et al., 2021), and versatile flexibility (Radford et al., 2019; Brown, 2020). Inspired by the success of LLMs, recent works have begun to employ AR transformers for visual generation (Van Den Oord et al., 2017; Razavi et al., 2019; Esser et al., 2021; Hong et al., 2022; Ge et al., 2022; Kondratyuk et al., 2023; Wang et al., 2024). Additionally, several recent developments have extended LLMs to handle multimodal inputs and outputs (Lu et al., 2022; Zheng et al., 2024), further demonstrating the promising potential of AR models in visual content generation. All of these methods employ visual tokenizer to convert continuous visual signals into sequences of discrete tokens, allowing them to be autoregressively modeled in the same way as natural language is modeled by LLMs. Typically, visual tokenizer consists of visual encoder, quantization module (Van Den Oord et al., 2017; Yu et al., 2023b), and visual decoder. The generative modeling occurs in the quantized discrete latent space, with the decoder mapping the generated discrete token sequences back to continuous visual signals. It is evident that the visual tokenizer plays pivotal role, as it directly influences the quality of the generated content. Building on this insight, several works have focused on improving the visual tokenizer (Lee et al., 2022; Yu et al., 2023b), making solid progress in enhancing the compression ratio and reconstruction fidelity of visual tokenization. Most existing visual tokenizers follow patchwise tokenization paradigm (Van Den Oord et al., 2017; Esser et al., 2021; Wang et al., 2024; Yu et al., 2023b), where the discrete tokens are quantized from the encoded patches of the original visual inputs. While these approaches are intuitive for visual data with spatial or spatialtemporal structures, they restrict the tokenizers ability to capture global and holistic representations of the entire input. This limitation becomes even more pronounced when applied to AR models, which rely on sequential processing and require locally encoded tokens to be transformed into linear 1D sequences. Previous research (Esser et al., 2021) has demonstrated that the method of flattening these patch tokens into sequence is critical to the generation quality of AR models. Although most existing works adopt raster scan order for this transformation due to its simplicity, it remains uncertain whether this is the optimal strategy. In addition, there are no clear guidelines for determining the most effective flattening order. On the other hand, although the reconstruction fidelity of visual tokenizer sets an upper bound on the generation fidelity of AR models, the factors that determine the gap between them remain unclear. In fact, higher reconstruction quality has been widely reported to sometimes lead to worse generation fidelity (Zhang et al., 2023; Yu et al., 2024). This discrepancy highlights the limitations of the commonly used reconstruction-focused design of visual tokenizers and underscores the importance of ensuring desirable properties in the latent space of the tokenizer. However, very few works have attempted to address this aspect in improving image tokenizers (Gu et al., 2024; Zhang et al., 2023), and for video tokenizers, it has been almost entirely overlooked. In this paper, we present LARP, video tokenizer with Learned AutoRegressive generative Prior, designed to address the underexplored challenges identified in previous work. By leveraging ViT-style spatialtemporal patchifier (Dosovitskiy, 2020) and transformer encoder architecture (Vaswani, 2017), LARP forms an autoencoder and employs stochastic vector quantizer (Van Den Oord et al., 2017) to tokenize videos into holistic token sequences. Unlike traditional patchwise tokenizers, which directly encode input patches into discrete tokens, LARP introduces set of learned queries (Carion et al., 2020; Li et al., 2023) that are concatenated with the input patch sequences and then encoded into holistic discrete tokens. An illustrative comparison between the patchwise tokenizer and LARP is shown in Figure 2 (a) and the left part of Figure 2 (b). By decoupling the direct correspondence between discrete tokens and input patches, LARP allows for flexible number of discrete tokens, enabling trade-off between tokenization quality and latent representation length. This design also empowers LARP to produce more holistic and semantic representations of video content. Preprint. To further align LARPs latent space with AR generative models, we incorporate lightweight AR transformer as prior model. It autoregressively models LARPs latent space during training, providing signals to encourage learning latent space that is well-suited for AR models. Importantly, the prior model is trained simultaneously with the main modules of LARP , but it is discarded during inference, adding zero memory or computational overhead to the tokenizer. Notably, by combining holistic tokenization with the co-training of the AR prior model, LARP automatically determines an order for latent discrete tokens in AR generation and optimizes the tokenizer to perform optimally within that structure. This approach eliminates the need to manually define flattening order, which remains an unsolved challenge for traditional tokenizers. To evaluate the effectiveness of the LARP tokenizer, we train series of Llama-like (Touvron et al., 2023a;b; Sun et al., 2024) autoregressive (AR) generation models. Leveraging the holistic tokens and the learned AR generative prior, LARP achieves Frechet Video Distance (FVD) (Unterthiner et al., 2018) score of 57 on the UCF101 class-conditional video generation benchmark (Soomro, 2012), establishing new state-of-the-art among all published video generative models, including proprietary and closed-source approaches like MAGVIT-v2 (Yu et al., 2023b). To summarize, our key contributions are listed as follows: We present LARP, novel video tokenizer that enables flexible, holistic tokenization, allowing for more semantic and global video representations. LARP features learned AR generative prior, achieved by co-training an AR prior model, which effectively aligns LARPs latent space with the downstream AR generation task. LARP significantly improves video generation quality for AR models across varying token sequence lengths, achieving state-of-the-art FVD performance on the UCF101 classconditional video generation benchmark and outperforming all AR methods on the K600 frame prediction benchmark."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 DISCRETE VISUAL TOKENIZATION To enable AR models to generative high resolution visual contents, various discrete visual tokenization methods have been developed. The seminal work VQ-VAE (Van Den Oord et al., 2017; Razavi et al., 2019) introduces vector quantization to encode continuous images into discrete tokens, allowing them to be modeled by PixelCNN (Van den Oord et al., 2016). VQGAN (Esser et al., 2021) improves visual compression rate and perceptual reconstruction quality by incorporating GAN loss (Goodfellow et al., 2014) in training the autoencoder. Building on this, several works focus on improving tokenizer efficiency (Cao et al., 2023) and enhancing generation quality (Gu et al., 2024; Zheng et al., 2022; Zhang et al., 2023). Leveraging the powerful ViT (Dosovitskiy, 2020) architecture, ViT-VQGAN (Yu et al., 2021) improves VQGAN on image generationt tasks. Inspired by the success of image tokenization, researchers extend VQGAN to videos using 3D CNNs (Ge et al., 2022; Yan et al., 2021; Yu et al., 2023a). C-ViViT (Villegas et al., 2022) employs the temporal-causal ViT architecture to tokenize videos, while more recent work, MAGVIT-v2 (Yu et al., 2023b), introduces lookup-free quantization, significantly expanding the size of the quantization codebook. OmniTokenizer (Wang et al., 2024) unifies image and video tokenization using the same tokenizer model and weights for both tasks. It is worth noting that all of the above tokenizers follow the patchwise tokenization paradigm discussed in Section 1, and are therefore constrained by patch-to-token correspondence. Very recently, concurrent work (Yu et al., 2024) proposes compact tokenization approach for images. However, it neither defines flattening order for the discrete tokens nor introduces any prior or regularization to improve downstream generation performance. 2.2 VISUAL GENERATION Visual generation has been long-standing area of interest in machine learning and computer vision research. The first major breakthrough comes with the rise of Generative Adversarial Networks (GANs) (Goodfellow et al., 2014; Karras et al., 2019; 2020; Skorokhodov et al., 2022), known for their intuitive mechanism and fast inference capabilities. AR methods are also widely applied in 3 Preprint. represent video patches, circles Figure 2: Method overview. Cubes indicate continuous denote discrete tokens. (a) Patchwise video tokenizer used in previous embeddings, and squares works. (b) Left: The LARP tokenizer tokenizes videos in holistic scheme, gathering information from the video using set of learned queries. Right: The AR prior model, trained with LARP , predicts the next holistic token, enabling latent space optimized for AR generation. The AR prior model is forwarded in two rounds per iteration. The red arrow represents the first round, and the purple arrows represent the second round. The reconstruction loss Lrec is omitted for simplicity. visual generation. Early works (Van Den Oord et al., 2016; Van den Oord et al., 2016; Chen et al., 2020) model pixel sequences autoregressively, but are limited in their ability to synthesize highresolution content due to the extreme length of pixel sequences. Recent advancements in visual tokenization make AR generative models for visual content more practical. While all tokenizers discussed in Section 2.1 are suitable for AR generation, many focus on BERT-style (Devlin, 2018) masked visual generation (Chang et al., 2022), such as in Yu et al. (2023a;b; 2024). Diffusion models (Ho et al., 2020; Song et al., 2020; Peebles & Xie, 2023) have recently emerged to dominate image (Dhariwal & Nichol, 2021) and video synthesis (Ho et al., 2022), delivering impressive visual generation quality. By utilizing VAEs (Kingma, 2013) to reduce resolution, latent diffusion models (Rombach et al., 2022; Blattmann et al., 2023) further scale up, enabling multimodal visual generation (Betker et al., 2023; Saharia et al., 2022; Podell et al., 2023; Brooks et al., 2024)."
        },
        {
            "title": "3 METHOD",
            "content": "3.1 PRELIMINARY Patchwise Video Tokenization. As discussed in Section 1, existing video tokenizers adopt patchwise tokenization scheme, where latent tokens are encoded from the spatialtemporal patches of the input video. Typically, patchwise video tokenizer consists of an encoder E, decoder D, and quantizer Q. Given video input RT HW 3, it is encoded, quantized, and reconstructed as: = E(V), = Q(Z), ˆV = D(X), (1) fT fH where d refers to the spatialtemporally downsampled video feature maps with fW denotes the quantized discrete tokens, and ˆV is the latent dimensions per location, NT W reconstructed video. fT , fH , fW are the downsampling factors for the spatialtemporal dimensions T, H, , respectively. Despite different implementations of the encoder E, decoder D, and quantizer Q, all patchwise tokenizers maintain fixed downsampling factor for each spatialtemporal dimension. The latent vector Zi,j,k,: Rd at each position is typically the direct output of its spatialtemporally corresponding 4 Preprint. input video patch (e.g., same spatialtemporal location in CNNs, or token position in transformers). While this design is intuitive for 3D signals like video, it limits the discrete tokens to low-level patch features, hindering their ability to capture higher-level, holistic information. Moreover, this formulation introduces the challenge of flattening patch tokens into unidirectional sequence, which is critical for AR generation. Autoregressive Modeling. Given sequence of discrete tokens = (x1, x2, . . . , xn), we can train neural network to model the probability distribution pθ(x) autoregressively as follows: pθ(x) = (cid:89) i=1 pθ (xi x1, . . . , xi1, θ) , (2) where θ denotes the neural network parameters. This model can be conveniently trained by optimizing the negative log-likelihood (NLL) of pθ(x). During inference, it iteratively predicts the next token xi by sampling from pθ (xi x1, . . . , xi1, θ), based on the previously generated tokens. While autoregressive modeling imposes no direct constraints on data modality, it does require the data to be both discrete and sequential, which necessitates the use of visual tokenizer when applied to images or videos. 3.2 HOLISTIC VIDEO TOKENIZATION Patchify. LARP employs the transformer architecture (Vaswani, 2017) due to its exceptional performance and scalability. Following the ViT framework (Dosovitskiy, 2020), we split the input video into spatialtemporal patches, and linearly encode each patch into continuous transformer patch embeddings. Formally, given video input RT HW 3, the video is linearly patchified as follows: = P(V), = flatten(P), (3) where denotes the linear patchify operation, d is the spatialtemporal patches projected onto dimensions, and Rmd is the flattened d-dimentional patch embeddings. Here, fT , fH , fW are the downsampling factors for dimensions T, H, , respectively, and = is the total number of tokens. Importantly, the patch embeddings remain local in fT nature, and therefore cannot be directly used to generate holistic discrete tokens. fW fH fW fH fT Query-based Transformer. To design holistic video tokenizer, it is crucial to avoid directly encoding individual patches into discrete tokens. To achieve this, we adapt the philosophy of Carion et al. (2020); Li et al. (2023) to learn set of fixed input queries to capture the holistic information from the video, as illustrated in the left section of Figure 2 (b). For simplicity, LARP employs transformer encoder 1 architecture, as opposed to the transformer encoder-decoder structure used in Carion et al. (2020). In-context conditioning is applied to enable information mixing between different patch and query tokens. Formally, we define learnable holistic query embedding QL Rnd, where each embedding is d-dimensional. These query embeddings are concatenated with the patch embeddings along the token dimension. The resulting sequence, now of length (n + m), is then input to the LARP encoder and quantizer as follows: = E(QL E), = Q(Z1:n,:), (4) where denotes the concatenation operation, is the latent embeddings, and = (x1, . . . , xn) denotes the quantized discrete tokens. Note that only Z1:n,:, i.e., the latent embeddings corresponding to the queries embeddings, are quantized and used. This ensures that each discrete token xi has equal chance to represent any video patch, eliminating both soft and hard local patch constraints. The LARP decoder is also implemented as transformer encoder neural network. During the decoding stage, LARP follows similar approach, utilizing learnable patch query embeddings QP Rmd. The decoding process is defined as: ˆZ = Q1(x), ˆV = reshape(D(QP ˆZ)1:m,:), (5) 1Here and throughout this paper, transformer encoder refers to the specific parallel transformer encoder architecture defined in Dosovitskiy (2020) Preprint. where Q1 denotes the de-quantization operation that maps discrete tokens back to the continues latent embeddings ˆZ Rnd. These embeddings are concatenated with the patch query embeddings QP , and the combined sequence of length + is decoded into sequence of continuous vectors. The first vectors are reshaped to reconstruct the video ˆV RT HW 3. Crucially, although the latent tokens are now both holistic and discrete, no specific flattening order is imposed due to the unordered nature of the holistic query set and the parallel processing property of the transformer encoder. As result, is not immediately suitable for AR modeling. Stochastic Vector Quantization. While vector quantization (VQ) (Van Den Oord et al., 2017) has been widely adopted in previous visual quantizers (Esser et al., 2021; Ge et al., 2022), its deterministic nature limits the tokenizers ability to explore inter-code correlations, resulting less semantically rich codes. To address these limitations, LARP employs stochastic vector quantization (SVQ) paradigm to implement the quantizer Q. Similar to VQ, SVQ maintains codebook Rcd , which stores vectors, each of dimension d. The optimization objective LSVQ includes weighted sum of the commitment loss and the codebook loss, as defined in Van Den Oord et al. (2017). The key difference lies in the look-up operation. While VQ uses an arg min operation to find the closest code by minimizing the distance between the input vector Rd and all codes in C, SVQ introduces stochasticity in this process. Specifically, SVQ computes the cosine similarities between the input vector and all code vectors in C, interprets these similarities as logits, and applies softmax normalization to obtain the probabilities p. One index is then sampled from the resulting multinomial distribution (x). Formally, the SVQ process = Q(v) is defined as: = Ci vCi , = softmax(s), (x) = (cid:89) j=1 p1x=j , (6) (7) where 1 denotes the indicator function. To maintain the differentiability of SVQ, we apply the straight-through estimator (Bengio et al., 2013). The de-quantization operation is performed via straightforward index look-up, ˆv = Q1(x) = Cx, similar to the standard VQ process. Reconstructive Training. Following Esser et al. (2021); Ge et al. (2022); Yu et al. (2023a), the reconstructive training loss of LARP, Lrec, is composed of L1 reconstruction loss, LPIPS perceptual loss (Zhang et al., 2018), GAN loss (Goodfellow et al., 2014), and SVQ loss LSVQ. 3.3 LEARNING AN AUTOREGRESSIVE GENERATIVE PRIOR Continuous Autoregressive Transformer. To better align LARPs latent space with AR generative models, we introduce lightweight AR transformer as prior model, which provides gradients to push the latent space toward structure optimized for AR generation. key challenge in designing the prior model lies in its discrete nature. Simply applying an AR model to the discrete token sequence would prevent gradients from being back-propagated to the LARP encoder. Furthermore, unlike the stable discrete latent spaces of fully trained tokenizers, LARPs latent space is continuously evolving during training, which can destabilize AR modeling and reduce the quality of the signals it provides to the encoder. To address these issues, we modify standard AR transformer into continuous AR transformer by redefining its input and output layers, as depicted in the right section of Figure 2 (b). The input layer of standard AR transformer is typically an embedding look-up layer. In the prior model of LARP, this is replaced with linear projection that takes the de-quantized latents ˆZ as input, ensuring proper gradient flow during training. The output layer of standard AR transformer predicts the logits of the next token. While this does not block gradient propagation, it lacks awareness of the vector values in the codebook, making it unsuitable for the continuously evolving latent space during training. In contrast, the output layer of LARPs AR prior model makes predictions following the SVQ scheme described in Section 3.2. It predicts an estimate of the next tokens embedding, Rd , which has the same shape as codebook vectors Ci. Similar to SVQ, the predicted embedding is used to compute cosine similarities with all code vectors in C, as described in Equation (6). These similarities are then softmax-normalized and interpreted as probabilities, 6 Preprint. (a) Scaling LARP tokenizer sizes. (b) Scaling Number of discrete tokens. Figure 3: Scaling LARP tokenizer size and number of tokens. which are used to compute the negative log-likelihood (NLL) loss with the input tokens as the ground truth. To predict the next token, sample is drawn from the resulting multinomial distribution using Equation (7). This output layer design ensures that the AR prior model remains aware of the continuously evolving codebook, enabling it to make more accurate predictions and provide more precise signals to effectively train the LARP tokenizer. Scheduled Sampling. Exposure bias (Ranzato et al., 2015) is well-known challenge in AR modeling. During training, the model is fed the ground-truth data to predict the next token. However, during inference, the model must rely on its own previous predictions, which may contain errors, creating mismatch between training and inference conditions. While the AR prior model in LARP is only used during training, it encounters similar issue: as the codebook evolves, the semantic meaning of discrete tokens can shift, making the input sequence misaligned with the prior models learned representations. To address this problem, we employ the scheduled sampling technique (Bengio et al., 2015; Mihaylova & Martins, 2019) within the AR prior model of LARP. Specifically, after the first forward pass of the prior model, we randomly mix the predicted output sequence with the original input sequence at the token level. This mixed sequence is then fed into the AR prior model for second forward pass. The NLL loss is computed for both rounds of predictions and averaged, helping to reduce exposure bias and ensure more robust training. Integration. Although the AR prior model functions as standalone module, it is trained jointly with the LARP tokenizer in an end-to-end manner. Once the NLL loss Lprior is computed, it is combined with the reconstructive loss Lrec to optimize the parameters of both the prior model and the tokenizer. Formally, the total loss is defined as: = Lrec + αLprior, (8) where α is the loss weight, and Lrec is defined in Section 3.2. Since α is is typically set to small value, we apply higher learning rate to the parameters of the prior model to ensure effective learning. Importantly, the prior model is used solely to encourage an AR-friendly discrete latent space for LARP during training. It is discarded at inference time, meaning it has no effect on the inference speed or memory footprint."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 SETUP Dataset. We conduct video reconstruction and generation experiments using the Kinetics-600 (K600)(Carreira et al., 2018) and UCF-101(Soomro, 2012) datasets. In all experiments, we use 16-frame video clips with spatial resolution of 128128 for both training and evaluation following Ge et al. (2022); Yu et al. (2023a;b). Implementation Details. LARP first patchifies the input video. In all experiments, the patch sizes are set to fT = 4, fH = 8, and fW = 8, respectively. As result, 16128128 video clip is split into 41616 = 1024 video patches, which are projected into 1024 continuous patch embeddings in the first layer of LARP. For the SVQ quantizer, we utilize factorized codebook with size of 7 Preprint. Method #Params #Tokens rFVD gFVD Tokenizer Generator K600 UCF Diffusion-based generative models with continuous video tokenizers VideoFusion (Luo et al., 2023) HPDM (Skorokhodov et al., 2024) - - 2B 725M - - MLM generative models with discrete video tokenizers MAGVIT-MLM (Yu et al., 2023a) MAGVIT-v2-MLM (Yu et al., 2023b) 158M - 306M 307M AR generative models with discrete video tokenizers CogVideo (Hong et al., 2022) TATS (Ge et al., 2022) MAGVIT-AR (Yu et al., 2023a) MAGVIT-v2-AR (Yu et al., 2023b) OmniTokenizer (Wang et al., 2024) LARP-L (Ours) LARP-L-Long (Ours) LARP-L-Long (Ours) - 32M 158M - 82.2M 173M 173M 173M 9.4B 321M 306M 840M 650M 343M 343M 632M 1024 1280 2065 1024 1024 1280 1280 1024 1024 1024 - - 25 8.6 - 162 25 8.6 42 24 20 - - 9.9 4.3 109.2 - - - 32.9 6.2 6.2 5.1 173 66 76 58 626 332 265 109 191 107 102 Table 1: Comparison of video generation results. Results are grouped by the type of generative models. The scores for MAGVIT-AR and MAGVIT-v2-AR are taken from the appendix of MAGVIT-v2 (Yu et al., 2023b). LARP-L-Long denotes the LARP-L trained for more epochs. Our best results are obtained with larger AR generator. 8192 and dimension of = 8, following the recommendations of Yu et al. (2021). The softmax normalization in Equation (6) is applied with temperature of 0.03. The AR prior model in LARP is adapted from small GPT-2 model (Radford et al., 2019), consisting of only 21.7M parameters. Scheduled sampling for the AR prior model employs linear warm-up for the mixing rate, starting from 0 and reaching peak of 0.5 at 30% of the total training steps. We set AR prior loss weight α = 0.06 in our main experiments, and use learning rate multiplier of 50. We employ Llama-like Touvron et al. (2023a;b); Sun et al. (2024) transformer as our AR generative model. One class token [cls] and one separator token [sep] are used in the class-conditional generation task on UCF101 and frame prediction task on K600, respectively. Frechet Video Distance (FVD) (Unterthiner et al., 2018) serves as the main evaluation metric for both reconstruction and generation experiments. 4.2 SCALING To explore the effect of scaling the LARP tokenizer, we begin by varying its size while keeping the number of latent tokens fixed at 1024. As shown in Figure 3 (a), we compare the reconstruction FVD (rFVD) and generation FVD (gFVD) for three scaled versions of LARP : LARP-L, LARP-B, and LARP-S, with parameter counts of 173.0M, 116.3M, and 39.8M, respectively. All results are reported on the UCF-101 dataset. Interestingly, while rFVD consistently improves as the tokenizer size increases, gFVD saturates when scaling from LARP-B to LARP-L, suggesting that gFVD can follow different trend from rFVD. Notably, as shown in Figure 1 (c), LARP has already achieved the smallest gap between rFVD and gFVD, further demonstrating the effectiveness of the optimized latent space it has learned. One of LARPs key features is its holistic video tokenization, which supports an arbitrary number of latent discrete tokens. Intuitively, using more tokens slows down the AR generation process but improves reconstruction quality. Conversely, using fewer tokens significantly speeds up the process but may lead to lower reconstruction quality due to the smaller information bottleneck. To evaluate this trade-off, we use LARP-B and the default AR model, scaling down the number of latent tokens from 1024 to 512 and 256. The corresponding rFVD and gFVD results on the UCF-101 dataset are reported in Figure 3 (b). It is expected that both rFVD and gFVD increase when fewer tokens are used to represent video. However, the rate of degradation in gFVD slows down when reducing from 512 to 256 tokens compared to rFVD, indicating improved generative representation efficiency. 8 Preprint. Figure 4: Video reconstruction comparison with OmniTokenizer (Wang et al., 2024). 4.3 VIDEO GENERATION COMPARISON For video generation, we compare LARP with other state-of-the-art published video generative models, including diffusion-based models, Masked Language Modeling (MLM) methods, and AR methods. We use the UCF-101 class-conditional generation benchmark and the K600 frame prediction benchmark, where the first 5 frames are provided to predict the next 11 frames in 16-frame video clip. As shown in Table 1, LARP outperforms all other video generators on the UCF-101 dataset, setting new state-of-the-art FVD of 57. Notably, within the family of AR generative models, LARP significantly surpasses all other AR methods by large margin on both the UCF-101 and K600 datasets, including the closed-source MAGVIT-v2-AR (Yu et al., 2023b). Moreover, the last two rows of Table 1 demonstrate that using larger AR generator can significantly improve LARPs generation quality, hilighting the scalability of LARPs representation. 4.4 VISUALIZATION Video Reconstruction. In Figure 4, we compare video reconstruction quality of LARP with OmniTokenizer (Wang et al., 2024). LARP consistently outperforms OmniTokenizer, particularly in complex scenes and regions, further validating the rFVD comparison results shown in Table 1. Class-Conditional Video Generation. We present class-conditional video generation results in Figure 5. LARP constructs discrete latent space that better suited for AR generation, which enables the synthesis of high-fidelity videos, not only improving the quality of individual frames but also enhancing overall temporal consistency. Additional results are provided in the appendix. Video Frame Prediction. Video frame prediction results are displayed in Figure 6. The vertical yellow line marks the boundary between the conditioned frames and the predicted frames. We use 5 frames as input to predict the following 11 frames, forming 16-frame video clip, which is temporally downsampled to 8 frames for display. It is evident that LARP effectively predicts frames with diverse scenes and natural motions. Additional results are provided in the appendix. Configuration LARP-B No AR prior model No scheduled sampling in AR prior model Deterministic quantization Small AR prior model loss weight (α = 0.03) No CFG PSNR LPIPS rFVD gFVD 27.88 27.95 27.85 27.65 27.83 27. 0.0855 0.0830 0.0856 0.0884 0.0866 0.0855 31 23 27 27 28 31 107 190 142 149 120 Table 2: Ablation study. All configurations are modified from LARP-B model. 9 Preprint. Figure 5: Class-conditional video generation results on the UCF-101 dataset using LARP. Figure 6: Video frame prediction results on the K600 dataset using LARP. 4.5 ABLATION STUDY To assess the impact of the different components proposed in Section 3, we perform an ablation study, with results shown in Table 2. Clearly, the AR prior model contributes the most to the exceptional performance of LARP. As further validated in Figure 1 (b), the improvement from using the AR prior model remains consistent across different token numbers. The scheduled sampling for the AR prior model and the use of SVQ are also critical, as both are closely tied to the AR prior models effectiveness. The loss weight of the AR prior model and the use of CFG have relatively minor effects on the generative performance. Interestingly, the model without the AR prior achieves the best reconstruction results but the worst generation results, highlighting the effectiveness of the AR prior model in enhancing LARPs discrete latent space for generative tasks."
        },
        {
            "title": "5 CONCLUSION AND FUTURE WORK",
            "content": "In this paper, we introduce LARP, novel video tokenizer tailored specifically for autoregressive (AR) generative models. By introducing holistic tokenization scheme with learned queries, LARP captures more global and semantic video representations, offering greater flexibility in the number of discrete tokens. The integration of lightweight AR prior model during training optimizes the latent space for AR generation and defines an optimal token order, significantly improving performance in AR tasks. Extensive experiments on video reconstruction, class-conditional video generation, and video frame prediction demonstrate LARPs ability to achieve state-of-the-art FVD scores. The promising results of LARP not only highlight its efficacy in video generation tasks but also suggest its potential for broader applications, including the development of multimodal large language models (MLLMs) to handle video generation and understanding in unified framework. 10 Preprint."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "This work was partially supported by NSF CAREER Award (#2238769) and an Amazon Research Award (Fall 2023) to AS. The authors acknowledge UMDs supercomputing resources made available for conducting this research. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of NSF, Amazon, or the U.S. Government."
        },
        {
            "title": "REFERENCES",
            "content": "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. Advances in neural information processing systems, 28, 2015. Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013. James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2256322575, 2023. Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. Technical Report, 2024. URL https://openai. com/research/video-generation-models-as-world-simulators. Tom Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. Shiyue Cao, Yueqin Yin, Lianghua Huang, Yu Liu, Xin Zhao, Deli Zhao, and Kaigi Huang. Efficientvqgan: Towards high-resolution image generation with efficient vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 73687377, 2023. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pp. 213229. Springer, 2020. Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew Zisserman. short note about kinetics-600. arXiv preprint arXiv:1808.01340, 2018. Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1131511325, 2022. Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In International conference on machine learning, pp. 1691 1703. PMLR, 2020. Jacob Devlin. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 11 Preprint. Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1287312883, 2021. Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and Devi Parikh. Long video generation with time-agnostic vqgan and time-sensitive transformer. In European Conference on Computer Vision, pp. 102118. Springer, 2022. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. Gemini Team Google, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Yuchao Gu, Xintao Wang, Yixiao Ge, Ying Shan, and Mike Zheng Shou. Rethinking the objectives of vector-quantized tokenizers for image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 76317640, 2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and arXiv preprint Jacob Steinhardt. Measuring massive multitask language understanding. arXiv:2009.03300, 2020. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701, 2020. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:8633 8646, 2022. Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 44014410, 2019. Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 81108119, 2020. Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 12 Preprint. Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Rachel Hornung, Hartwig Adam, Hassan Akbari, Yair Alon, Vighnesh Birodkar, et al. Videopoet: large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023. Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1152311532, 2022. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pp. 1973019742. PMLR, 2023. Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: unified model for vision, language, and multi-modal tasks. In The Eleventh International Conference on Learning Representations, 2022. Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tieniu Tan. Videofusion: Decomposed diffusion models for high-quality video generation. arXiv preprint arXiv:2303.08320, 2023. Tsvetomila Mihaylova and Andre FT Martins. Scheduled sampling for transformers. arXiv preprint arXiv:1906.07651, 2019. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Jack Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021. MarcAurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training with recurrent neural networks. arXiv preprint arXiv:1511.06732, 2015. Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. Stylegan-v: continuous video generator with the price, image quality and perks of stylegan2. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 36263636, 2022. Ivan Skorokhodov, Willi Menapace, Aliaksandr Siarohin, and Sergey Tulyakov. Hierarchical patch diffusion models for high-resolution video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 75697579, 2024. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. Preprint. Soomro. Ucf101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Hung-Yu Tseng, Lu Jiang, Ce Liu, Ming-Hsuan Yang, and Weilong Yang. Regularizing generative adversarial networks under limited data. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 79217931, 2021. Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. Advances in neural information processing systems, 29, 2016. Aaron Van Den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In International conference on machine learning, pp. 17471756. PMLR, 2016. Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual descriptions. In International Conference on Learning Representations, 2022. Junke Wang, Yi Jiang, Zehuan Yuan, Binyue Peng, Zuxuan Wu, and Yu-Gang Jiang. Omnitokenizer: joint image-video tokenizer for visual generation. arXiv preprint arXiv:2406.09399, 2024. Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021. Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627, 2021. Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose Lezama, Han Zhang, Huiwen Chang, Alexander Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1045910469, 2023a. Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, et al. Language model beats diffusion tokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023b. Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is worth 32 tokens for reconstruction and generation. arXiv preprint arXiv:2406.07550, 2024. 14 Preprint. Jiahui Zhang, Fangneng Zhan, Christian Theobalt, and Shijian Lu. Regularized vector quantization for tokenized image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1846718476, 2023. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586595, 2018. Chuanxia Zheng, Tung-Long Vuong, Jianfei Cai, and Dinh Phung. Movq: Modulating quantized vectors for high-fidelity image generation. Advances in Neural Information Processing Systems, 35:2341223425, 2022. Sipeng Zheng, Bohan Zhou, Yicheng Feng, Ye Wang, and Zongqing Lu. Unicode: Learning unified codebook for multimodal large language models. arXiv preprint arXiv:2403.09072, 2024."
        },
        {
            "title": "A ADDITIONAL IMPLEMENTATION DETAILS",
            "content": "A.1 ADDITIONAL IMPLEMENTATION DETAILS OF THE LARP TOKENIZER. During the training of LARP, GAN loss (Goodfellow et al., 2014) is employed to enhance reconstruction quality. We use ViT-based discriminator (Dosovitskiy, 2020) with identical patchify settings to those of the LARP tokenizer. The discriminator is updated once for every five updates of the LARP tokenizer and is trained with learning rate that is 30% of the LARP tokenizers learning rate. To stabilize discriminator training, LeCam regularization (Tseng et al., 2021) is applied, following the approach of Yu et al. (2023a). GAN loss weight of 0.3 is used throughout the training. Fixed sin-cos positional encoding (Vaswani, 2017) is used in both the encoder and decoder of LARP. In the encoder, fixed 3D positional encoding is applied to each video patch, while in the decoder, fixed 1D positional encoding is added to each holistic token. Notably, since the patch queries and holistic queries are position-wise learnable parameters, they do not require additional positional encodings. In the SVQ module, we set the total quantization loss weight to 0.1. Additionally, we follow Esser et al. (2021) by using commitment loss weight of 0.25 and codebook loss weight of 1.0. Both the L1 reconstruction loss and the LPIPS perceptual loss are assigned weight of 1.0. In most experiments, we train the LARP tokenizer for 75 epochs on combined dataset of UCF-101 and K600 with batch size of 64, totaling approximately 500k training steps. Random horizontal flipping is used as data augmentation technique. Specifically, LARP-L-Long in Table 1 is trained for 150 epochs with batch size of 128. The Adam optimizer (Kingma, 2014) is used with base learning rate of 1e 4, β1 = 0.9, and β2 = 0.95, following warm-up cosine learning rate schedule. A.2 ADDITIONAL IMPLEMENTATION DETAILS OF THE AR GENERATIVE MODEL We use Llama-like transformers as our AR generative models. Unlike the original implementation and Sun et al. (2024), we utilize absolute learned positional encodings. token dropout probability of 0.1 is applied during training, with both residual and feedforward dropout probabilities also set to 0.1. Additionally, when training the AR generative models, the SVQ module of the LARP tokenizer is set to be deterministic, ensuring more accurate latent representation. Our default AR generative model consists of 632M parameters, as specified in Table 1. It is trained on the training split of the UCF-101 dataset for 1000 epochs with batch size of 32. The model used in the last row of Table 1, which also has 632M parameters, is trained for 3000 epochs on UCF-101 with batch size of 64. The AdamW optimizer (Loshchilov, 2017) is used with β1 = 0.9, β2 = 0.95, weight decay of 0.05, and base learning rate of 6e 4, following warm-up cosine learning rate schedule. 15 Preprint. When generating videos, we apply small Classifier-Free Guidance (CFG) scale of 1.25 (Ho & Salimans, 2022). We do not use top-k or top-p sampling methods."
        },
        {
            "title": "B ADDITIONAL VISUALIZATION RESULTS",
            "content": "B.1 VIDEO RECONSTRUCTION COMPARISON Additional video reconstruction results are provided in Figure 7. Across variety of scenes and regions, LARP consistently demonstrates superior reconstruction quality compared to OmniTokenizer Wang et al. (2024). B.2 CLASS-CONDITIONAL VIDEO GENERATION ON UCF-101 DATASET We provide additional class-conditional video generation results in Figure 8. These results further demonstrate LARPs ability to generate high-quality videos with both strong per-frame fidelity and temporal consistency across various action classes in the UCF-101 dataset. The generated videos show diverse scene dynamics, capturing fine-grained details and natural motion, highlighting LARPs effectiveness in handling complex generative tasks within this challenging dataset. Generated video files (in MP4 format) are available in the supplementary materials. B.3 VIDEO FRAME PREDICTION ON K600 DATASET We present additional video frame prediction results in Figure 9, further demonstrating LARPs capacity to accurately predict future frames in the K600 dataset. These results showcase LARPs ability to handle wide range of dynamic scenes, capturing temporal dependencies with natural motion and smooth transitions between predicted frames. The predictions highlight LARPs effectiveness in scenarios involving complex motion and scene diversity, underscoring its strong generalization capabilities in video frame prediction tasks. The predicted frames and the ground truth videos (in MP4 format) are available in the supplementary materials. 16 Preprint. Figure 7: Additional video reconstruction comparison with OmniTokenizer (Wang et al., 2024). Preprint. Figure 8: Additional class-conditional generation results on UCF-101 dataset. 18 Preprint. Figure 9: Additional video frame prediction results on K600 dataset."
        }
    ],
    "affiliations": [
        "University of Maryland, College Park"
    ]
}