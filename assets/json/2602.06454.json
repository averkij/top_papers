{
    "paper_title": "RelayGen: Intra-Generation Model Switching for Efficient Reasoning",
    "authors": [
        "Jiwon Song",
        "Yoongon Kim",
        "Jae-Joon Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large reasoning models (LRMs) achieve strong performance on complex reasoning tasks by generating long, multi-step reasoning trajectories, but inference-time scaling incurs substantial deployment cost. A key challenge is that generation difficulty varies within a single output, whereas existing efficiency-oriented approaches either ignore this intra-generation variation or rely on supervised token-level routing with high system complexity. We present \\textbf{RelayGen}, a training-free, segment-level runtime model switching framework that exploits difficulty variation in long-form reasoning. Through offline analysis of generation uncertainty using token probability margins, we show that coarse-grained segment-level control is sufficient to capture difficulty transitions within a reasoning trajectory. RelayGen identifies model-specific switch cues that signal transitions to lower-difficulty segments and dynamically delegates their continuation to a smaller model, while preserving high-difficulty reasoning on the large model. Across multiple reasoning benchmarks, RelayGen substantially reduces inference latency while preserving most of the accuracy of large models. When combined with speculative decoding, RelayGen achieves up to 2.2$\\times$ end-to-end speedup with less than 2\\% accuracy degradation, without requiring additional training or learned routing components."
        },
        {
            "title": "Start",
            "content": "RelayGen: Intra-Generation Model Switching for Efficient Reasoning Jiwon Song1, Yoongon Kim1, Jae-Joon Kim1 1Seoul National University {jiwon.song,yoon_g_kim,kimjaejoon}@snu.ac.kr https://github.com/jiwonsong-dev/RelayGen 6 2 0 2 6 ] . [ 1 4 5 4 6 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large reasoning models (LRMs) achieve strong performance on complex reasoning tasks by generating long, multi-step reasoning trajectories, but inference-time scaling incurs substantial deployment cost. key challenge is that generation difficulty varies within single output, whereas existing efficiency-oriented approaches either ignore this intra-generation variation or rely on supervised token-level routing with high system complexity. We present RelayGen, training-free, segmentlevel runtime model switching framework that exploits difficulty variation in long-form reasoning. Through offline analysis of generation uncertainty using token probability margins, we show that coarse-grained segment-level control is sufficient to capture difficulty transitions within reasoning trajectory. RelayGen identifies model-specific switch cues that signal transitions to lower-difficulty segments and dynamically delegates their continuation to smaller model, while preserving high-difficulty reasoning on the large model. Across multiple reasoning benchmarks, RelayGen substantially reduces inference latency while preserving most of the accuracy of large models. When combined with speculative decoding, RelayGen achieves up to 2.2 end-to-end speedup with less than 2% accuracy degradation, without requiring additional training or learned routing components."
        },
        {
            "title": "Introduction",
            "content": "The emergence of large reasoning models (LRMs) (Jaech et al., 2024; Guo et al., 2025) has enabled language models to solve complex reasoningintensive problems that were previously beyond reach. Recent progress in this area has been driven primarily by inference-time scaling, where substantially larger models are paired with longer generation trajectories, rather than by fundamental architectural changes. As result, models with 32B 1 Figure 1: RelayGen overview. Long reasoning generation exhibits difficulty variation within single output, enabling segment-level runtime model switching. parameters or more have become the de facto standard for long-form reasoning tasks (Qwen Team, 2025; Yang et al., 2025a; GLM et al., 2024). While this scaling trend improves reasoning accuracy, it significantly increases inference cost, making generation efficiency central challenge for deploying LRMs in practice (Sui et al., 2025; Liu et al., 2025). In real-world settings, the computational overhead of long reasoning trajectories often dominates overall deployment cost, limiting the scalability of LRMs. This raises natural question: do all parts of long reasoning output require large model? In practice, reasoning trajectories are heterogeneous, often interleaving high-difficulty reasoning with lower-difficulty continuation or consolidation. As illustrated in Figure 1, such difficulty variation arises within single generation, suggesting opportunities for dynamic model allocation. These observations suggest that long reasoning generation is inherently difficulty-heterogeneous rather than uniform. However, existing efficiencyoriented approaches differ substantially in how they characterize and exploit such heterogeneity, often making trade-offs between practical deployability and the granularity at which difficulty variation is modeled within single generation. To improve inference efficiency, prior work has explored combining smaller, more efficient models with large models during inference. Input-level routing methods (Chen et al., 2023; Ong et al., 2024; Zhang et al., 2025) represent the simplest form of this approach, selecting one model per input and assigning it to the entire generation. By construction, such methods treat all parts of an output uniformly and therefore cannot account for difficulty variation within single long reasoning generation, which limits their applicability to LRMs. More recent approaches attempt to account for intra-generation difficulty variation, primarily differing in the granularity at which routing decisions are made during decoding. Token-level routing approaches (Fu et al., 2025; Chen et al., 2025) query separately trained router before generating each token, enabling difficulty-aware decisions at every decoding step. While such token-level analysis provides principled mechanism for identifying challenging positions during reasoning, it requires training and integrating an online routing model, introducing additional supervision requirements and system complexity that hinder practical deployment. In contrast, part-level or step-level switching approaches (Yang et al., 2025b) make routing decisions over larger units within an output. These methods often leverage lexical or structural cues to approximate difficulty, but tend to rely on heuristic criteria rather than detailed analysis of modelspecific reasoning behavior. As result, it remains unclear how well the selected segments align with actual difficulty across different models and tasks. Consequently, existing approaches struggle to exploit difficulty variation within single long reasoning output. To address this gap, we analyze difficulty fluctuations along long reasoning generation trajectories of LRMs and observe that different parts of an output exhibit systematically different difficulty levels. In particular, reflective, paraphrasing, and post-reasoning continuation segments often exhibit lower uncertainty than core reasoning segments. Based on this observation, we propose RelayGen, training-free, segment-level runtime model switching framework. RelayGen allocates largemodel capacity to high-difficulty reasoning segments and hands off subsequent low-difficulty segments to smaller model once transition is detected. Rather than relying on supervised routing, RelayGen uses empirically grounded transition cues that are identified through difficulty analysis to guide segment-level model switching during generation. Our key contribution is simple, training-free routing mechanism grounded in empirical difficulty analysis, demonstrating that coarse-grained, segment-level control is sufficient to capture difficulty variation in long-form reasoning. This finding challenges the prevailing reliance on fine-grained, learned routing for efficient and accurate inference with large reasoning models. By operating at segment boundaries, RelayGen enables difficultyaware inference that remains fully composable with speculative decoding, property that is fundamentally incompatible with token-level routing schemes. Our contributions are summarized as follows: We provide an empirical analysis of difficulty variation within long reasoning trajectories of large reasoning models. We propose RelayGen, training-free, segment-level runtime model switching framework guided by empirically validated difficulty transitions. On AIME 2025 benchmark, RelayGen combines with speculative decoding to achieve up to 2.2 latency reduction with less than 2% accuracy degradation."
        },
        {
            "title": "2 Related Works",
            "content": "A line of work has explored improving inference efficiency by combining smaller models with more capable large models, delegating less demanding parts of generation to the former. Input-level routing. Early approaches primarily operate at the input level (Chen et al., 2023; Ong et al., 2024), selecting single model per request and assigning it to the entire generation. Such inputlevel routing methods have been widely adopted for general LLM inference and have also been extended to large reasoning models (Su et al., 2025; Pan et al., 2025). However, by construction, they assign uniform model throughout the generation and therefore cannot account for intra-generation difficulty variation that arises during the course of long reasoning generation. Token-level routing. More fine-grained approaches attempt to delegate generation within single output. Token-level routing methods such as R2R (Fu et al., 2025) and R-Stitch (Chen et al., 2025) selectively invoke large model at token 2 Figure 2: Given prompt, an LRM generates an output that naturally decomposes into long reasoning stage and subsequent answer stage, explicitly separated by special boundary tokens (e.g., <think>, </think>) positions estimated to be difficult. However, they rely on the assumption that difficulty can be localized at isolated token positions, often requiring trained routing models and additional supervision in practice. Segmentor step-level switching. Another class of approaches operates at coarser granularity by switching models over multi-token segments within an output. Speculative Thinking (Yang et al., 2025b) exemplifies this direction by dividing reasoning into steps using predefined delimiters and assigning steps based on heuristic cue tokens. These cues are manually specified and applied in modelagnostic manner, without being derived from an analysis of their actual impact on reasoning difficulty. As result, although such methods demonstrate that step-level intervention can be effective for long-form reasoning, the selected segments may not consistently align with regions of genuinely high or low difficulty for given model and task distribution, which can lead to accuracy degradation in challenging reasoning settings. Training-based switching. Related to these efforts, SplitReason (Akhauri et al., 2025) proposes training-based approach that learns modelinternal policy to decide when to hand off generation between models via special control tokens. As result, its design introduces distinct accuracyefficiency trade-off compared to training-free, runtime model allocation approaches based on externally observed generation dynamics. Speculative decoding. In parallel, speculative decoding (Leviathan et al., 2023) represents complementary line of work for reducing inference latency by jointly using small and large models. In this paradigm, small model (Kim et al., 2023) or learned speculator module (Li et al., 2025; Cai et al., 2024; Liu et al., 2024) proposes candiFigure 3: Probability margin trajectories of Qwen3-32B across long reasoning examples. date token sequences that are subsequently verified by large model, enabling lossless acceleration. Speculative decoding is orthogonal to routing or switching methods and can therefore be combined with them to further accelerate generation. However, because speculative decoding relies on predicting future tokens, it is difficult to be combined with token-level routing, while remaining naturally compatible with coarser-grained switching."
        },
        {
            "title": "3.1 Structure of Long Reasoning Outputs",
            "content": "As illustrated in Figure 2, LRMs generate outputs that are explicitly structured into reasoning stage followed by subsequent answer stage, typically delimited by special tokens such as <think> and </think>. During the reasoning stage, the model performs multi-step reasoning by exploring intermediate hypotheses and frequently revisiting earlier conclusions through reflection and selfchecking. The answer stage then produces the final response presented to the user, consolidating and summarizing the conclusions established during reasoning. This explicit structure indicates that long reasoning outputs are not monolithic, but instead consist of parts with distinct generation characteristics. In the following sections, we analyze how generation difficulty varies across these different parts."
        },
        {
            "title": "3.2 Observed Difficulty Transition Signals",
            "content": "during Reasoning To quantify generation difficulty, we define the probability margin as the difference between the probabilities of the top-1 and top-2 tokens. Given the output logits zt at generation step t, let pt = softmax(zt) denote the resulting token probability distribution. The probability margin is defined as mt = pt,(1) pt,(2) 3 Table 1: Answer-stage consistency when delegating answer generation from Qwen3-32B to Qwen3-0.6B. Total Samples Matches Matching Rate 728 727 99.86% Figure 4: Post-sentence probability margin following representative discourse-level cues. 3.3 Low-Difficulty Generation after Reasoning where pt,(1) and pt,(2) denote the largest and second-largest probabilities, respectively. Intuitively, larger margin indicates that the model more decisively prefers its most likely next token over competing alternatives, whereas smaller margin reflects higher uncertainty. All margin statistics in this section are computed from Qwen3-32B reasoning traces on AIME 2025 (Zhang and Math-AI, 2025) and GPQADiamond (Rein et al., 2024). Figure 3 shows that the probability margin fluctuates substantially over long reasoning trajectories, indicating non-uniform generation difficulty. Here, we use the term discourse-level cues to refer to lexical markers that signal shifts in the flow of reasoning discourse, including repair-oriented cues (e.g., wait, but) and conclusion-oriented cues (e.g., therefore, so). Such cues are often associated with transitions from active reasoning to reflective or checking phases, during which the model elaborates on previously established conclusions. To examine whether such transitions are reflected in generation uncertainty, we analyze how discourse-level cues correlate with probability margin dynamics. Specifically, for each occurrence of discourse-level cue, we compute post-sentence probability margin, the average probability margin over the remainder of the sentence in which the cue appears. The complete list of evaluated cues is provided in Appendix A. Figure 4 shows that different discourse-level cues are followed by systematically different postsentence probability margins, reflecting variation in subsequent generation difficulty. For example, cues such as Thus are frequently followed by concluding or summarizing statements with higher postsentence margins, whereas semantically similar cues such as so do not consistently exhibit the same behavior and may even be associated with lower margins. This variability indicates that cue effects cannot be reliably predicted by simple heuristics and depend on model-specific generation dynamics, motivating empirical, model-specific analysis. 4 The answer stage following the reasoning stage is typically easier to generate, as it does not involve independent reasoning but instead realizes conclusions established during the preceding stage in condensed form. Conditioned on the key-value cache of the reasoning stage, answer generation primarily involves summarization and formatting rather than exploration of new reasoning paths. Notably, despite its lower reasoning difficulty, the answer stage is conditioned on the entire preceding reasoning context, resulting in long effective keyvalue cache. As result, answer stage tokens often incur the highest per-token attention cost during generation, even though their semantic difficulty is low. This structure suggests that the answer stage may not require the full capacity of large model, as its generation is heavily constrained by previously decoded reasoning tokens. To examine this hypothesis, we conduct controlled handoff experiment in which small model generates the answer stage while being conditioned on reasoning traces produced by large model. Specifically, we generate full reasoning traces on problems from MATH500 (Lightman et al., 2023), AIME 2025, and GPQA-Diamond using Qwen3-32B, and then delegate the subsequent answer generation to Qwen3-0.6B. Although Qwen3-0.6B is substantially smaller and weaker when used alone, we observe almost no answer flipping when only the answer stage is delegated to the smaller model. Table 1 reports the consistency of the resulting answers compared to those produced entirely by the large model. Out of 728 evaluated samples, only single answer differs, corresponding to matching rate of 99.86%. This result indicates that answer generation is highly stable under model handoff when conditioned on preceding reasoning, in sharp contrast to the substantial difficulty fluctuation observed within the reasoning stage itself. dicators of low-difficulty segments. To identify reliable switch cues, we perform an offline selection procedure based on post-sentence probability margin, which is defined in Section 3.2. Importantly, this procedure does not involve optimization, learning, or parameter updates of any kind; it simply profiles generation statistics from fixed, pre-trained models. Using an existing heldout dataset as calibration set, we first generate reasoning traces using the large model. For each occurrence of candidate discourse-level cue in these traces, we compute the average probability margin over the remainder of the sentence, from the cue position to the next sentence boundary. We then compare this post-sentence margin against the global average probability margin computed across all token positions in the calibration traces. discourse-level cue is selected as switch cue if its post-sentence margin is higher than the global average by at least one standard error, indicating that the corresponding continuation is consistently easier for the model to generate. Cues that do not satisfy this criterion, including those associated with decreased or inconsistent margins, are excluded from the switch cue set. Importantly, this selection procedure does not involve learning routing model or introducing additional supervision. Although it relies on an offline calibration set, the procedure is strictly trainingfree and only uses empirical generation statistics extracted from existing reasoning traces. The resulting switch cue set is fixed at deployment time and used during runtime generation through simple token matching, as described in Section 4.3. More details on switch cue selection are provided in Appendix B."
        },
        {
            "title": "4.3 Runtime Switching Procedure",
            "content": "RelayGen implements runtime model switching using standard generation control mechanisms, without relying on learned routers or online difficulty estimation. As illustrated in Figure 5, switching is realized by configuring model-specific stop conditions and issuing successive generation requests. This design deliberately favors coarsegrained, externally observable control signals over fine-grained per-token routing, enabling simple and robust integration with existing inference systems. RelayGen is built on top of vLLM and leverages its support for prefix caching and OpenAI-compatible generation APIs. In RelayGen, generation begins with the large Figure 5: Runtime model switching semantics in RelayGen."
        },
        {
            "title": "4 RelayGen",
            "content": "4.1 Overview of RelayGen RelayGen is training-free, runtime framework for model switching during long reasoning generation. Instead of assigning an entire output to single model, RelayGen dynamically allocates model capacity within single generation, matching model size to local generation difficulty. RelayGen is motivated by empirical findings in Section 3, which show that (i) generation difficulty varies substantially within single reasoning trajectory, (ii) discourse-level cues correlate with transitions between higherand lower-difficulty regions, and (iii) post-reasoning answer generation remains highly stable despite high per-token attention cost. These observations suggest that different segments of single generation benefit from different model capacities. As illustrated in Figure 5, RelayGen implements segment-level runtime model switching. During the reasoning stage, generation is primarily performed by large model to preserve accuracy. When selected switch cue is encountered, subsequent continuation segments are temporarily delegated to smaller model. After the transition from reasoning to the answer stage, the remainder of the output is generated entirely by the small model. RelayGen operates entirely at runtime and requires neither additional training nor auxiliary routing components such as learned routers or control modules."
        },
        {
            "title": "4.2 Switch Cue Selection",
            "content": "RelayGen does not directly use the full set of discourse-level cues analyzed in Section 3.2 for runtime switching. Although many discourse-level cues are correlated with transitions in generation difficulty, not all such cues consistently lead to lower-uncertainty continuation. In particular, some cues are associated with increased uncertainty and further exploration, making them unsuitable as in5 Table 2: Pass@1 comparison of RelayGen and baselines. Results are reported for largesmall model pairs, denoted as Large / Small. Method Small Model Large Model Spec. Think. R2R RelayGen Qwen3-32 / 1.7B R1-Distill-Qwen-32B / R1-Distill-Qwen-1.5B MATH500 AIME 2025 GPQA-Diamond MATH500 AIME GPQA-Diamond 88.60 95.27 91.35 94.30 94.80 31.67 70.00 40.83 62.50 68.33 37.33 64.58 41.29 61.62 63.64 86.10 93.50 89.40 89.65 91.70 28.33 53.30 30.00 52.50 50. 30.30 60.61 42.68 48.99 56.82 model. Its generation request includes stop tokens corresponding to the selected switch cues, as well as the special token </think> that marks the transition from the reasoning stage to the answer stage. When switch cue is generated during reasoning, generation is temporarily halted and subsequent tokens are delegated to the small model. After </think> is generated, the remainder of the output is generated entirely by the small model. The small model uses sentence-ending punctuation as stop tokens. Upon reaching sentence boundary, control returns to the large model if reasoning is still ongoing, enabling sentence-bounded offloading that may occur multiple times within single reasoning trajectory. After the transition to the answer stage, the small model completes generation without further handoff. Despite repeated bidirectional switching during reasoning, RelayGen does not require re-prefilling the entire prompt at each switch. Leveraging vLLM (Kwon et al., 2023)s prefix caching, only newly generated tokens unseen by the target model are prefetched, keeping switching overhead minimal in practice. Because RelayGen performs switching at coarsegrained segment boundaries rather than at every decoding step, it is naturally compatible with speculative decoding methods such as Eagle-3 and MultiToken Prediction (Li et al., 2025; Liu et al., 2024). Speculative decoding can be applied whenever the large model is active without interfering with RelayGens switching logic, whereas token-level routing disrupts the draftverify process, making effective composition with speculative decoding challenging. We provide further analysis of this granularity mismatch in Appendix C. Overall, RelayGen features runtime model switching that preserves composability without introducing significant routing overhead."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Experimental Setup Models. We evaluate RelayGen using two representative largesmall model pairs. For the Qwen3 (Yang et al., 2025a) family, we use Qwen332B and Qwen3-1.7B, and for the R1-Distill (Guo et al., 2025) family, we use R1-Distill-Qwen-32B and R1-Distill-Qwen-1.5B. For all models, the maximum generation length is set to 32,768 tokens, with temperature 0.6 and top-p sampling at 0.95. For Qwen3 models, we additionally set top-k to 20. All sampling hyperparameters follow the recommended settings of each model family to ensure fair and stable comparison. Baselines. We compare RelayGen against singlemodel generation, where single model produces the entire output, as well as mid-generation switching baselines. These include R2R (Fu et al., 2025), which employs learned token-level router to select model at each decoding step, and Speculative Thinking (Yang et al., 2025b), which performs steplevel switching based on predefined lexical cues. Datasets. We evaluate all methods on multiple reasoning benchmarks spanning different domains and difficulty levels. For mathematical reasoning, we use AIME 2025 (Zhang and Math-AI, 2025) and MATH500 (Lightman et al., 2023), which exhibit substantially different difficulty levels. To assess generality beyond mathematics, we additionally report results on GPQA-Diamond (Rein et al., 2024), which focuses on graduate-level scientific reasoning. For all datasets, we generate four outputs per problem and report pass@1. Implementation Details. RelayGen is implemented on top of vLLM (Kwon et al., 2023) (version 0.13.0) using its OpenAI-compatible API. Separate inference engines are instantiated for the large and small models. Runtime model switching and prefix reuse are handled using standard generation 6 Table 3: Inference speedup from single-model inference with large model and large-model utilization across different generation acceleration methods. Error bars denote standard deviation over problems (5 problems, 5 runs per problem). Method Eagle-3 Spec. Think. R2R RelayGen RelayGen + Eagle-3 Speedup () Large-Model Utilization (%) 1.790.42 100.000.00 2.210.95 25.546. 1.300.18 19.273.33 1.290.20 69.803.62 2.200.21 69.493.02 controls and vLLMs built-in prefix caching mechanism, as described in Section 4.3. For offline calibration to derive model-pairspecific switch cues, we use the AMC 2023 dataset (Mathematical Association of America, 2023) consisting of 40 problems. For each problem, the large model generates four independent reasoning traces, resulting in total of 160 calibration samples. 5.2 Accuracy Evaluation Table 2 reports pass@1 on three reasoning benchmarks. We compare RelayGen against singlemodel generation and mid-generation switching baselines, including Speculative Thinking and R2R, with the large and small models serving as upper and lower bounds, respectively. Across both model pairs, RelayGen substantially improves accuracy over the small-only baseline while preserving most of the large models performance. RelayGen consistently outperforms Speculative Thinking on all benchmarks. Although Speculative Thinking also relies on lexical cues to guide mid-generation switching, it makes switching decisions without an explicit analysis of generation difficulty and operates at coarse, step-level granularity. As result, large portions of reasoning are delegated to the small model even when difficulty remains high, leading to significant accuracy degradation. Compared to R2R, which employs learned token-level router, RelayGen achieves accuracy that is comparable overall, while exhibiting different strengths across benchmarks. For the Qwen3 model pair, RelayGen outperforms R2R across all three benchmarks, including notable improvement on AIME 2025. For the R1-Distill model pair, RelayGen achieves higher accuracy on MATH500 and GPQA-Diamond, while R2R performs slightly better on AIME 2025. These results suggest that segment-level switching is sufficient to preserve most of the large models reasoning capability, particularly when difficulty variation aligns with discourse-level structure. Overall, RelayGen occupies favorable point in the accuracyefficiency spectrum, retaining large fraction of the large models performance while avoiding the supervised training and finegrained routing complexity required by token-level methods such as R2R. 5.3 Inference Latency and Composability We evaluate inference latency using the Qwen332B / 1.7B model pair on the AIME 2025 dataset. We randomly sample five problems and generate each problem five times, reporting the average endto-end generation latency. All experiments are conducted on two NVIDIA A100 80GB SXM GPUs under identical decoding configurations. We report normalized speedup relative to the large model, along with the fraction of tokens generated by the large model, to characterize the latencyaccuracy trade-off. Table 3 summarizes inference speedup and largemodel utilization across different generation acceleration methods. Speculative Thinking attains 2.21 of speedup compared to large-model-only baseline, primarily due to its low large-model utilization and coarse-grained switching. However, as shown in Section 5.2, this comes at the cost of substantial accuracy degradation. R2R achieves relatively moderate speedup despite using large model for merely 19.3% of tokens. This indicates that its potential compute savings are largely offset by routing and model-switching overhead, as R2R invokes neural router at every decoding step, disrupting continuous generation. RelayGen exhibits inherent speedup comparable to R2R when applied alone, despite retaining substantially higher large-model utilization. This efficiency stems from segment-level switching, which avoids the overhead of per-token routing decisions while preserving the large model for high-difficulty reasoning segments. As result, RelayGen achieves more favorable accuracylatency trade-off, as evidenced in Section 5.2. Crucially, RelayGen is fully compatible with speculative decoding. When combined with EagleTable 4: Effect of switch cue selection on pass@1. Table 5: Effect of calibration set size on pass@1. Cue usage AIME 2025 GPQA-Diamond # Calibration samples AIME 2025 GPQA-Diamond Selected All candidates 68.33 60.00 63.64 57.32 10 40 160 70.00 71.67 68.33 61.87 61.87 63. rable to that observed for the R1-Distill-Qwen-32B / R1-Distill-Qwen-1.5B pair. This suggests that the degradation primarily stems from the limited capacity of the small model rather than incompatibility across model families. Overall, RelayGen remains effective with heterogeneous model pairs when the small model has sufficient capacity. Sensitivity to Calibration Set Size. We study the sensitivity of RelayGen to the calibration set size using the Qwen3-32B / Qwen3-1.7B model pair, where our main experiments adopt 160 calibration samples. As shown in Table 5, reducing the calibration set size to 40 or even 10 samples does not lead to significant performance degradation on either AIME 2025 or GPQA-Diamond. In some cases, smaller calibration sets yield comparable or slightly higher pass@1 scores, suggesting that RelayGen is not overly sensitive to the exact choice of calibration size. Overall, these results indicate that RelayGen remains effective even with very small calibration sets, reinforcing its practicality in training-free and low-overhead deployment scenarios."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we showed that generation difficulty within long reasoning trajectories of large reasoning models is inherently heterogeneous, and that not all parts of single output require the full capacity of large model. Based on an offline empirical analysis of token-level uncertainty, we proposed RelayGen, training-free, segment-level runtime model switching framework that dynamically allocates model capacity within single generation without relying on learned routers or additional supervision. Across multiple reasoning benchmarks, RelayGen preserves most of the accuracy of large models while substantially reducing inference latency. Moreover, RelayGen composes effectively with speculative decoding, achieving up to 2.2 end-to-end speedup. More broadly, our results suggest that difficulty-aware inference for long-form reasoning can be effectively addressed through empirically grounded, coarse-grained runtime mechanisms, rather than fine-grained, supervised routing. Figure 6: Pass@1 under different small-model choices for each large model, shown relative to the large-modelonly baseline. 3, RelayGen achieves final speedup up to 2.20, substantially outperforming R2R while remaining entirely training-free. Moreover, the combined approach yields an additional 1.22 speedup over Eagle-3 alone, demonstrating that RelayGen complements speculative decoding by selectively reducing end-to-end latency without aggressive early switching. Overall, these results indicate that RelayGen prioritizes stable latency reduction through selective offloading, retaining most generation on the large model while remaining composable with existing inference acceleration techniques."
        },
        {
            "title": "5.4 Ablation Studies",
            "content": "Effect of Switch Cue Selection. To isolate the impact of switch cue selection, we conduct an ablation study on the AIME 2025 and GPQA-Diamond with Qwen3-32B and 1.7B model pair. We compare RelayGen with selected switch cues against variant that activates all discourse-level cue candidates as switching triggers. As shown in Table 4, using all candidate cues consistently reduces pass@1 for both benchmarks. These results indicate that indiscriminate use of discourse-level cues leads to suboptimal switching decisions, highlighting the importance of accurate cue selection for effective model switching. Effect of Changing Model Pairs. We evaluate the robustness of RelayGen to model pair selection by testing heterogeneous model pairs on AIME 2025 and GPQA-Diamond. Specifically, we consider Qwen3-32B / R1-Distill-Qwen-1.5B and R1-Distill-Qwen-32B / Qwen3-1.7B, with results shown in Figure 6. We find that the performance drop for the Qwen3-32B / R1-Distill-Qwen-1.5B pair, relative to its large-model baseline, is compa-"
        },
        {
            "title": "Limitations",
            "content": "RelayGen targets settings where long-form reasoning is explicitly externalized, as in modern LRMs, and where generation difficulty varies over extended trajectories. For tasks that do not require sustained reasoning or whose outputs lack clear long-sequence structure, the opportunity for effective segment-level model switching may be limited. RelayGen also assumes moderate capability gap between the large and small models. If the small model lacks sufficient reasoning capacity even for relatively low-difficulty segments, offloading can degrade overall performance. While our experiments focus on English reasoning data, the underlying principleswitching models at points of reduced reasoning difficultyis not inherently language-specific. We leave systematic exploration of such multilingual extensions to future work."
        },
        {
            "title": "7 Ethical Considerations",
            "content": "RelayGen is training-free inference-time framework and does not introduce new model capabilities, data sources, or learning signals beyond those of the underlying models. As such, it does not raise additional ethical concerns compared to standard deployment of large language models. Any biases or risks are inherited from the base models, and RelayGen only controls runtime allocation between them."
        },
        {
            "title": "References",
            "content": "Yash Akhauri, Anthony Fei, Chi-Chih Chang, Ahmed AbouElhamayed, Yueying Li, and Mohamed Abdelfattah. 2025. Splitreason: Learning to offload reasoning. arXiv preprint arXiv:2504.16379. Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason Lee, Deming Chen, and Tri Dao. 2024. Medusa: Simple llm inference acceleration framework with multiple decoding heads. arXiv preprint arXiv:2401.10774. Lingjiao Chen, Matei Zaharia, and James Zou. 2023. Frugalgpt: How to use large language models while reducing cost and improving performance. arXiv preprint arXiv:2305.05176. Zhuokun Chen, Zeren Chen, Jiahao He, Lu Sheng, Mingkui Tan, Jianfei Cai, and Bohan Zhuang. 2025. R-stitch: Dynamic trajectory stitching for efficient reasoning. arXiv preprint arXiv:2507.17307. Tianyu Fu, Yi Ge, Yichen You, Enshu Liu, Zhihang Yuan, Guohao Dai, Shengen Yan, Huazhong Yang, and Yu Wang. 2025. R2r: Efficiently navigating divergent reasoning paths with small-large model token routing. arXiv preprint arXiv:2505.21600. Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, and 37 others. 2024. Chatglm: family of large language models from glm-130b to glm-4 all tools. Preprint, arXiv:2406.12793. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, and 1 others. 2024. Openai o1 system card. arXiv preprint arXiv:2412.16720. Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik, Michael Mahoney, Amir Gholami, and Kurt Keutzer. 2023. Speculative decoding with big little decoder. Advances in Neural Information Processing Systems, 36:3923639256. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Yaniv Leviathan, Matan Kalman, and Yossi Matias. 2023. Fast inference from transformers via specIn International Conference on ulative decoding. Machine Learning, pages 1927419286. PMLR. Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. 2025. Eagle-3: Scaling up inference acceleration of large language models via training-time test. arXiv preprint arXiv:2503.01840. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Lets verify step by step. arXiv preprint arXiv:2305.20050. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, and 1 others. 2024. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437. Yue Liu, Jiaying Wu, Yufei He, Ruihan Gong, Jun Xia, Liang Li, Hongcheng Gao, Hongyu Chen, Baolong Bi, Jiaheng Zhang, and 1 others. 2025. Efficient inference for large reasoning models: survey. arXiv preprint arXiv:2503.23077. 9 Mathematical Association of America. 2023. American mathematics competitions. In American Mathematics Competitions. Https://maa.org/mathcompetitions. Isaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph Gonzalez, Waleed Kadous, and Ion Stoica. 2024. Routellm: Learning to route llms with preference data. arXiv preprint arXiv:2406.18665. Zhihong Pan, Kai Zhang, Yuze Zhao, and Yupeng Han. 2025. Route to reason: Adaptive routing for llm and reasoning strategy selection. arXiv preprint arXiv:2505.19435. Qwen Team. 2025. Qwq-32b: Embracing the power of reinforcement learning. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. 2024. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling. Jiayuan Su, Fulin Lin, Zhaopeng Feng, Han Zheng, Teng Wang, Zhenyu Xiao, Xinlong Zhao, Zuozhu Liu, Lu Cheng, and Hongwei Wang. 2025. Cp-router: An uncertainty-aware router between llm and lrm. arXiv preprint arXiv:2505.19970. Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Na Zou, and 1 others. 2025. Stop overthinking: survey on efficient reasoning for large language models. arXiv preprint arXiv:2503.16419. Chenlong Wang, Yuanning Feng, Dongping Chen, Zhaoyang Chu, Ranjay Krishna, and Tianyi Zhou. 2025. Wait, we dont need to\" wait\"! removing thinking tokens improves reasoning efficiency. arXiv preprint arXiv:2506.08343. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. 2025a. Qwen3 technical report. arXiv preprint arXiv:2505.09388. Wang Yang, Xiang Yue, Vipin Chaudhary, and Xiaotian Han. 2025b. Speculative thinking: Enhancing small-model reasoning with large model guidance at inference time. arXiv preprint arXiv:2504.12329. Yifan Zhang and Team Math-AI. 2025. American invitational mathematics examination (aime) 2025. Yiqun Zhang, Hao Li, Chenxu Wang, Linyao Chen, Qiaosheng Zhang, Peng Ye, Shi Feng, Daling Wang, Zhen Wang, Xinrun Wang, and 1 others. 2025. The avengers: simple recipe for uniting smaller language models to challenge proprietary giants. arXiv preprint arXiv:2505.19797. 10 Discourse-level Cues This appendix specifies the set of discourse-level cues considered throughout the analysis in RelayGen. We emphasize that this appendix only defines which tokens are treated as discourse-level cues and does not describe how switch cues are selected or used for model handoff, which is detailed separately in Appendix B. We define discourse-level cues as lexical tokens that do not introduce new problem-specific content, but instead signal structural transitions in reasoning process, such as progression, reconsideration, or consolidation of intermediate conclusions. The cue pool used in this work is constructed by starting from an existing list of reflection keywords introduced in (Wang et al., 2025), which focuses on in-reasoning reflections, and extending it with additional tokens that explicitly signal result consolidation or summary transitions (e.g., so, therefore). The motivation for this augmentation is to cover not only intermediate reasoning reflections, but also discourse signals that commonly precede conclusion formation in long-form reasoning outputs. Importantly, the resulting cue pool is fixed prior to any model-pair-specific calibration and serves as closed set of tokens considered in all subsequent analyses. For clarity, we categorize the discourse-level cues according to their intended discourse function in reasoning process. This categorization is descriptive and does not imply that all cues are suitable or effective as switch signals. Progression and Continuation Cues. These tokens indicate forward movement within an ongoing reasoning chain or the continuation of the current line of thought. Typical examples include now, then, next, and again. Reconsideration and Branching Cues. These tokens signal pause, correction, or exploration of an alternative reasoning path, often reflecting local uncertainty or revision. Examples include wait, however, alternatively, and instead. Inference and Transition Cues. These tokens mark transition from intermediate reasoning steps to an inferred statement or implication. Representative examples include thus, hence, and therefore. Result Consolidation and Summary Cues. These tokens frequently precede the consolidation or summarization of conclusions, particularly near the end of reasoning process. Examples include so, therefore, and related connective markers. All discourse-level cues considered in this work fall into one of the above categories. The complete cue list used in our analysis is summarized in Table 6. Category Discourse-Level Cues (Canonical Form) Progression / Continuation now, then, next, again Reconsideration / Branching wait, however, alternatively, but, maybe, hmm, oh Inference / Transition thus, hence, therefore, similarly, specifically Result Consolidation / Summary so, therefore, check, double-check, verify Reference / Enumeration another, other, any Discourse Acknowledgement ah Table 6: Canonical discourse-level cue pool used for analysis. For clarity, only canonical forms are shown; capitalization and punctuation variants are handled separately during tokenization."
        },
        {
            "title": "B Switch Cue Selection Details",
            "content": "This appendix describes how switch cues are selected from the discourse-level cue pool defined in Appendix A. We emphasize that being discourse-level cue does not necessarily imply that token consistently eases reasoning or leads to more stable continuation behavior. The effect of cue can vary substantially depending on model characteristics and learned representations, and even the direction of its impact may differ across models. As result, switch cues cannot be reliably chosen based on intuition alone and must instead be identified empirically. Profiling Method. To this end, we apply the same analysis framework introduced in Section 3.2 to set of reasoning traces generated on calibration set. For each discourse-level cue, we collect statistics over all occurrences of the cue in the generated traces. Specifically, after cue token appears, we measure the average probability margin from that token until the end of the sentence. This post-sentence probability margin is then compared against the global average probability margin computed over all token positions. We select, for each model pair, cues whose post-occurrence probability margin is, on average, higher than the global token-level average. Model Pair Dependency. Switch cue selection is performed independently for each largesmall model pair. This reflects the fact that different models may respond differently to the same discourse token due to variations in architecture, scale, and training data, leading to differences in both cue sensitivity and the direction of its effect. Calibration Setup. The profiling procedure is training-free and requires only modest number of reasoning traces. In our main experiments, we use the AMC 2023 dataset (Mathematical Association of America, 2023) consisting of 40 problems. For each problem, the large model generates four independent reasoning traces, resulting in total of 160 calibration samples, which are reused for profiling under the small model. To identify segments that are relatively easier for the small model to continue, we feed these traces into the small model and compute token-level probability margins, which are then aggregated at the discourse level for switch cue selection. As shown in the ablation study in Section 5.4, this procedure remains robust even when substantially fewer calibration samples are used. Calibration Overhead. RelayGen employs lightweight offline calibration procedure that is executed once per largesmall model pair. Table 7 reports the wall-clock time of this procedure measured on two NVIDIA A100 GPUs using the AMC 2023 dataset. The calibration cost is dominated by reasoning trace generation with the large model, which takes approximately 80 minutes, followed by probability margin extraction and discourse-level aggregation under the small model, which require an additional 20 minutes. Overall, the total one-time calibration cost is about 100 minutes and is incurred entirely offline, introducing no overhead during inference. Moreover, since RelayGen is robust to smaller calibration sets, as demonstrated in our ablation study, this cost represents conservative upper bound and can be significantly reduced in practice without degrading performance, as the calibration overhead scales with the number of calibration samples. Table 7: Offline calibration overhead for RelayGen measured on two NVIDIA A100 GPUs using the AMC 2023 dataset. Calibration Stage Large-model reasoning trace generation Probability margin extraction and cue-level aggregation Total calibration time (one-time, offline) Wall-clock Time 80 minutes 20 minutes 100 minutes Final Switch Cue Sets. The resulting switch cue sets selected for each model pair are summarized in Table 8. 12 LargeSmall Model Pair Selected Switch Cues Qwen3-32B / Qwen3-1.7B (Yang et al., 2025a) R1-Distill-Qwen-32B / R1-Distill-Qwen-1.5B (Guo et al., 2025) \"now\", \"Oh,\", \"Thus\", \"Again\", \"Thus,\", \"another,\", \"Now\", \"Therefore\", \"specifically,\", \"Alternatively\", { \"similarly\", \"alternatively,\", \"similarly,\", \"Again,\", \"Similarly,\", \"Now,\", \"Specifically,\", \"Hence\", \"Similarly\", \"Other\", \"now,\", \"hence\", \"Specifically\", \"So \", \"Therefore,\", \"Wait,\", \"Also\", \"So,\" } \"Now\", { \"Wait\", \"Therefore\", \"Oh,\", \"Similarly,\", \"Any\", \"Therefore,\", \"Alternatively,\", \"now,\", \"So,\", \"now\", \"verify\", \"Specifically,\", \"Alternatively\", \"Ah,\", \"wait\", \"So \" } \"similarly\", \"Now,\", \"Again,\", \"Thus,\", \"Hence,\", \"hence\", \"thus\", \"Thus\", Table 8: Model-pair-specific switch cue sets selected offline from the discourse-level cue pool defined in Appendix A. Granularity Mismatch between Token-Level Routing and Speculative Decoding Speculative decoding relies on clear separation between draft phase, in which lightweight model proposes contiguous block of tokens, and verification phase, in which larger model verifies these tokens in bulk. This design amortizes the cost of large-model computation over many accepted tokens and is most effective when long draft spans can be formed. In contrast, token-level routing strategies (Fu et al., 2025; Chen et al., 2025) determine model usage at every generation step, requiring routing decision before each token is generated. As result, the draft process is frequently interrupted, preventing the formation of long contiguous draft blocks and forcing verification to be repeatedly reset. This mismatch in decision and verification granularity fundamentally limits the effectiveness of speculative decoding when combined with token-level routing. Example 1 illustrates generation trace produced by token-level routing method, R2R (Fu et al., 2025). In the example, tokens generated by the large model are highlighted in red, while the remaining tokens are produced by the small model. As shown, the large model is invoked only for sparse positions, often for single token at time, before control immediately returns to the small model. This behavior reflects the core objective of token-level routing approaches: they aim to minimize large-model utilization to obtain speedups, often requiring additional machinery such as training an explicit router that decides model usage at every generation step. Example 1: R2R Generation Example Okay, so have this problem about Aya walking and stopping at coffee shop. Let me try to understand it step by step. First, Aya walks 9 kilometers every morning. When she walks at constant speed of kilometers per hour, the total time taken, including minutes in the coffee shop, is 4 hours. Then, when she increases her speed to + 2 km/h, the total time becomes 2 hours and 24 minutes, which is 2.4 hours. The question is asking, if she walks at + 1 2 km/h, how many minutes will the walk take, including the minutes in the coffee shop. Hmm, so need to find the time it takes when her speed is + 1 find the value of and t. Let me denote the walking time without the coffee shop as t1 and t2 when she walks at and + 2 km/h respectively. Then, the total time is t1 + (in hours) and t2 + (in hours). But wait, the problem says the total time including minutes. So, is the time spent in the coffee shop, which is the same in both cases. So, when she walks at km/h, the total time is 4 hours, which includes minutes. Similarly, when she walks at + 2 km/h, the total time is 2 hours and 24 minutes, which is 2.4 hours, and again includes minutes. So, let me convert all the times into hours to make it easier. The coffee shop time is in minutes, so need to convert it to hours. Since 1 hour is 60 minutes, minutes is t/60 hours. So, for the first scenario: Walking time + Coffee shop time = 4 hours Which is (9/s) + (t/60) = 4 For the second scenario: Walking time + Coffee shop time = 2.4 hours Which is (9/(s + 2)) + 2 km/h. But first, think need to (t/60) = 2.4 So, now have two equations: 1) 9/s + t/60 = 4 2) 9/(s + 2) + t/60 = 2.4 can subtract the second equation from the first to eliminate t/60. So, (9/s9/(s + 2)) = 4 2.4 = 1.6 So, 9/s 9/(s + 2) = 1.6 Let me compute this: 9 (1/s 1/(s + 2)) = 1.6 Factor out 9: 9 (1/2 1/(s + 2)) = 1.6 Compute 1/s 1/(s + 2) := (s + 2 s)/[s(s + 2)] = 2/[s(s + 2)] So, 9 (2/[s(s + 2)]) = 1.6 Simplify: 18/[s(s + 2)] = 1.6 Multiply both sides by s(s + 2): [...] However, this fine-grained routing leads to extremely short and fragmented large-model segments, which prevents effective composition with speculative decoding. Speculative decoding relies on forming long contiguous draft spans so that the target model can verify many tokens in bulk and amortize the verification cost. Under token-level routing traces such as Example 1, verification spans collapse to one or two tokens per invocation, eliminating most of the amortization benefit while leaving switching and verification overheads dominant. Moreover, even though the small model is used for the majority of tokens in token-level routing, the resulting speedup is often far from proportional, as we demonstrated in Section 5.3. This suggests that the overhead of making routing decisions at every generation step is non-negligible in practice. In contrast, RelayGen avoids per-token routing decisions by switching at coarser, segment-level granularity, enabling longer contiguous spans that compose naturally with speculative decoding. As result, RelayGen achieves comparable or greater speedups without aggressively suppressing large-model utilization or introducing per-step routing overhead."
        }
    ],
    "affiliations": [
        "Seoul National University"
    ]
}