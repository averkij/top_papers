{
    "paper_title": "MaskedMimic: Unified Physics-Based Character Control Through Masked Motion Inpainting",
    "authors": [
        "Chen Tessler",
        "Yunrong Guo",
        "Ofir Nabati",
        "Gal Chechik",
        "Xue Bin Peng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Crafting a single, versatile physics-based controller that can breathe life into interactive characters across a wide spectrum of scenarios represents an exciting frontier in character animation. An ideal controller should support diverse control modalities, such as sparse target keyframes, text instructions, and scene information. While previous works have proposed physically simulated, scene-aware control models, these systems have predominantly focused on developing controllers that each specializes in a narrow set of tasks and control modalities. This work presents MaskedMimic, a novel approach that formulates physics-based character control as a general motion inpainting problem. Our key insight is to train a single unified model to synthesize motions from partial (masked) motion descriptions, such as masked keyframes, objects, text descriptions, or any combination thereof. This is achieved by leveraging motion tracking data and designing a scalable training method that can effectively utilize diverse motion descriptions to produce coherent animations. Through this process, our approach learns a physics-based controller that provides an intuitive control interface without requiring tedious reward engineering for all behaviors of interest. The resulting controller supports a wide range of control modalities and enables seamless transitions between disparate tasks. By unifying character control through motion inpainting, MaskedMimic creates versatile virtual characters. These characters can dynamically adapt to complex scenes and compose diverse motions on demand, enabling more interactive and immersive experiences."
        },
        {
            "title": "Start",
            "content": "MaskedMimic: Unified Physics-Based Character Control Through Masked Motion Inpainting CHEN TESSLER, NVIDIA, Israel YUNRONG GUO, NVIDIA, Canada OFIR NABATI, NVIDIA, Israel GAL CHECHIK, NVIDIA, Israel and Bar-Ilan University, Israel XUE BIN PENG, NVIDIA, Canada and Simon Fraser University, Canada 4 2 0 2 2 2 ] . [ 1 3 9 3 4 1 . 9 0 4 2 : r Fig. 1. We present MaskedMimic, versatile control model that enables physically simulated characters to generate diverse behaviors from flexible userspecified constraints. MaskedMimic can be used for wide range of applications, including generating full-body motion from partially observed joint target positions, joystick steering, object interactions, path-following, text commands, and combinations thereof, such as text-stylized path following. Crafting single, versatile physics-based controller that can breathe life into interactive characters across wide spectrum of scenarios represents an Authors addresses: Chen Tessler, NVIDIA, Israel, ctessler@nvidia.com; Yunrong Guo, NVIDIA, Canada, kellyg@nvidia.com; Ofir Nabati, NVIDIA, Israel, ofirnabati@gmail. com; Gal Chechik, NVIDIA, Israel and Bar-Ilan University, Israel, gchechik@nvidia.com; Xue Bin Peng, NVIDIA, Canada and Simon Fraser University, Canada, japeng@nvidia. com. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). 2024 Copyright held by the owner/author(s). 0730-0301/2024/12-ART209 https://doi.org/10.1145/3687951 exciting frontier in character animation. An ideal controller should support diverse control modalities, such as sparse target keyframes, text instructions, and scene information. While previous works have proposed physically simulated, scene-aware control models, these systems have predominantly focused on developing controllers that each specializes in narrow set of tasks and control modalities. This work presents MaskedMimic, novel approach that formulates physics-based character control as general motion inpainting problem. Our key insight is to train single unified model to synthesize motions from partial (masked) motion descriptions, such as masked keyframes, objects, text descriptions, or any combination thereof. This is achieved by leveraging motion tracking data and designing scalable training method that can effectively utilize diverse motion descriptions to produce coherent animations. Through this process, our approach learns physics-based controller that provides an intuitive control interface without ACM Trans. Graph., Vol. 43, No. 6, Article 209. Publication date: December 2024. 209:2 Tessler, C. et al requiring tedious reward engineering for all behaviors of interest. The resulting controller supports wide range of control modalities and enables seamless transitions between disparate tasks. By unifying character control through motion inpainting, MaskedMimic creates versatile virtual characters. These characters can dynamically adapt to complex scenes and compose diverse motions on demand, enabling more interactive and immersive experiences. CCS Concepts: Computing methodologies Physical simulation; Procedural animation. Additional Key Words and Phrases: reinforcement learning, animated character control, motion tracking, motion capture data ACM Reference Format: Chen Tessler, Yunrong Guo, Ofir Nabati, Gal Chechik, and Xue Bin Peng. 2024. MaskedMimic: Unified Physics-Based Character Control Through Masked Motion Inpainting. ACM Trans. Graph. 43, 6, Article 209 (December 2024), 21 pages. https://doi.org/10.1145/"
        },
        {
            "title": "INTRODUCTION",
            "content": "The development of virtual characters capable of following dynamic user instructions and interacting with diverse scenes has been significant challenge in computer graphics. This challenge spans wide range of applications, including gaming, digital humans, virtual reality, and many more. For instance, character might be instructed to \"Climb the hill to the castle, wave to the guard, go inside, navigate to the throne room, and sit on the throne\". This scenario requires the integration of multiple complex behaviors: locomotion across uneven terrain, text-guided animation, and object interaction. Prior works in physics-based simulation has addressed these challenges by developing specialized controllers for specific tasks such as locomotion, object interaction, and VR tracking. These methods typically involve training controllers for each task [Hassan et al. 2023; Rempe et al. 2023; Winkler et al. 2022] or encoding atomic motions into reusable latent spaces, which are then combined by high-level controller to perform new tasks [Luo et al. 2024; Peng et al. 2022; Tessler et al. 2023; Yao et al. 2022]. These systems tend to lack versatility, as each new task requires the lengthy process of training new task-specific controllers. Additionally, these models often rely on meticulous handcrafted reward functions, which can be difficult to design and tune, often leading to unsolicited behaviors. The goal of this work is to develop versatile unified motion control model that can be conveniently reused across wide variety of tasks, eliminating the need for task-specific training and complex reward engineering. This approach not only simplifies the training process, but also enhances the models ability to generalize across different tasks. We propose framework that trains versatile control model by leveraging the rich multi-modal information within existing motion capture datasets, such as kinematic trajectories, text descriptions, and scene information. Our proposed framework, MaskedMimic1, trains single unified controller capable of executing wide range of tasks. The MaskedMimic model is trained on randomly masked motion sequences. Conditioned on masked motion sequence, MaskedMimic predicts actions that reproduce the original (unmasked) full-motion sequence. 1Detailed video demonstrations are provided in the project page https://research.nvidia.com/labs/par/maskedmimic/ ACM Trans. Graph., Vol. 43, No. 6, Article 209. Publication date: December 2024. Once trained, this inpainting approach provides an intuitive interface for directing the behavior of the simulated character. Through technique we call goal-engineering, akin to prompt-engineering from natural language processing, users can provide variety of different constraints that guide the controller to perform desired task. This approach offers several advantages over prior methods. Training on masked motion sequences enables the model to generalize to novel combinations of objectives. Intuitive partial constraints replace complex, error-prone reward functions, simplifying the design process (Figure 2). Moreover, training MaskedMimic as single unified model allows for positive transfer, where knowledge gained from one task enhances performance on others. For example, MaskedMimic outperforms prior task-specific methods in generating full-body motion from VR inputs, while also generalizing to moving across irregular terrain and novel objects. The central contributions of our work include: (1) MaskedMimic, unified physics-based character control framework. It produces full-body motions by inpainting from partial motion descriptions. These partial descriptions can include target keyframes, target joint positions/rotations, text instructions, object interactions, or any combination thereof. (2) suite of goal-engineering techniques enabling MaskedMimic to reproduce tasks from prior systems, including full-body tracking [Luo et al. 2023; Wang et al. 2020], VR tracking [Winkler et al. 2022], scene interaction [Hassan et al. 2023; Pan et al. 2024], terrain traversal [Rempe et al. 2023; Wang et al. 2024a], text-control [Juravsky et al. 2022], and more, within single model."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Physics-Based Character Animation: Early approaches in physicsbased animation focused on manually-designing task-specific controllers. These controllers can produce compelling results. However, they typically require lengthy engineering process for each task of interest, and do not scale well to the diverse repertoire needed for general-purpose control [de Lasa et al. 2010; Geijtenbeek et al. 2013; Lee et al. 2010a; Liu et al. 2010]. More recent work has shown how to learn these controllers to perform complex, scene aware behaviors, such as locomote [Rempe et al. 2023] or sit on objects [Hassan et al. 2023]. However, these approaches typically require manual selection of motions fitting the expected behaviors combined with delicate reward design. Compared to kinematic animation, physics enables scene-aware motions, such as object interactions [Hassan et al. 2023; Pan et al. 2024; Xiao et al. 2024] and locomotion across irregular terrain [Rempe et al. 2023; Wang et al. 2024a]. Our work leverages physics-based animation to learn robust behaviors that generalize to unseen terrains and objects. Human Object Interaction: Generating realistic human-object interactions (HOI) requires accurate modeling of the physical dynamics between humans and objects, particularly regarding contacts. While kinematic-based HOI methods have made progress in areas like 3D-aware scene traversal [Wang et al. 2022, 2021] and interactions[Xu et al. 2023; Zhao et al. 2023], they often produce unrealistic artifacts such as penetration and floating objects. MaskedMimic: Unified Physics-Based Character Control Through Masked Motion Inpainting 209:3 Fig. 2. Partial motion plans. MaskedMimic synthesizes full-body physics-based character animations. It achieves this by inpainting conditioned on multi-modal partial objectives. (a) The character climbs up hill by tracking target head coordinates. (b) Text-to-motion synthesis enables the character to perform waving motion. (c) The character navigates across irregular terrain by combining head-tracking with text-based style conditioning. (d) Interacting with goal object, in this case sitting on an armchair, is achieved by conditioning on the object. Recent advancements in physics-based HOI methods have addressed these issues by incorporating physics simulations into the motion generation process. Notable examples include PhysHOI [Wang et al. 2023], InterPhys [Hassan et al. 2023], and UniHSI [Xiao et al. 2024], which can produce more natural and physicallyplausible scene interactions. These methods leverage physics engines to ensure that the generated motions adhere to physical laws, resulting in more natural interactions. Our work builds upon these physics-based HOI efforts by introducing unified controller that is also capable of performing object interaction behaviors. By framing motion generation as an inpainting task from partial goals, our method, MaskedMimic, is able to interact with novel scene compositions, such as placing furniture on irregular terrain, extending the applicability of HOI systems to more diverse and complex scenarios. Text to motion: Text provides high-level interface for controlling virtual characters. The availability of large text-labeled motion datasets, such as BABEL [Punnakkal et al. 2021] and HumanML3D [Guo et al. 2022], have enabled the development of textconditioned motion models. Initial results focused on kinematic animation, with methods such as ACTOR [Petrovich et al. 2021] and MDM [Tevet et al. 2023a] showing promising text control capabilities. However, careful engineering of the text prompts are often necessary to elicit the desired behaviors from model, and the resulting motions nonetheless exhibit artifacts such as floating and sliding. PACER++ [Wang et al. 2024a] aimed to mitigate non-physical motion artifacts by combining kinematic diffusion models with physicssimulation. text-conditioned kinematic model is used to produce the upper-body motion, then physics-based controller is used to follow given path while matching the kinematically-generated upper-body motion. parallel line of work, PADL [Juravsky et al. 2022] and SuperPADL [Juravsky et al. 2024], trained physics-based controllers that can be directly conditioned on text commands. In this work, we develop single unified physics-based controller that can be directly conditioned on both text and kinematic constraints, without requiring separate text-to-motion model. This enables intuitive text-based stylization of the simulated motions. Latent Generative Models have emerged as more scalable and generalizable approach to address the inefficiency of task-specific controllers. These models utilize large motion datasets to map latent codes to different behaviors. Notable prior work includes ASE, CALM, and CASE [Dou et al. 2023; Peng et al. 2022; Tessler et al. 2023], which used an adversarial objective, and ControlVAE, PhysicsVAE, PULSE, and NCP [Luo et al. 2024; Won et al. 2022; Yao et al. 2022; Zhu et al. 2023], which leverage motion tracking. By modeling large skill corpus, these methods remove the need for task-specific data curation. However, their learned latent representations are often abstract, lacking intuitive grounding for user control. Consequently, to solve new tasks, additional hierarchical controllers are typically trained to produce desired motions through latent control, limiting their usability [Luo et al. 2024; Peng et al. 2022; Yao et al. 2022]. Our work, MaskedMimic, presents unified interface by formulating character control as motion inpainting problem over partial multi-modal constraints extracted directly from the data itself. Leveraging the VAE approach with motion tracking objective, single trained model supports locomotion across irregular terrain [Rempe et al. 2023], generating full-body motion from VR controller signals [Lee et al. 2023; Winkler et al. 2022], inbetweening [Gopinath et al. 2022], natural object interactions [Hassan et al. 2023], full-body tracking [Luo et al. 2023; Wang et al. 2020], and more. Motion Inpainting: Generating full-body motions from partial joint constraints is commonly called motion inpainting. Inpainting is fundamental problem in character animation with notable applications like motion inbetweening and VR tracking. In the task of inpainting, characters full-body motion must be inferred from sparse set of available sensors or keyframes. Prior work has explored inpainting for kinematic systems using autoregressive models [Huang et al. 2018; Yang et al. 2021; Zheng ACM Trans. Graph., Vol. 43, No. 6, Article 209. Publication date: December 2024. 209: Tessler, C. et al et al. 2023], variational inference [Dittadi et al. 2021], and diffusion[Du et al. 2023; Tevet et al. 2023b; Xie et al. 2024]. However, while sparse tracking models have been proposed for physically animated systems, they are specialized for fixed sparsity patterns. For example, Lee et al. [2023] proposed system for handling jointsparsity, with fixed pre-defined joints, such as those obtained from VR systems. In contrast, our unified physics-based system MaskedMimic supports flexible sparsity patterns. With MaskedMimic, any combination of modalities (joints, keyframes, objects, text) can be observed or unobserved. This enables various applications, from unconditional generation to multi-modal constraints like inbetweening or VR avatar control across irregular terrain, and object interactions. Previous kinematic inpainting methods struggle with such scenarios as they lack the physical grounding to reason about dynamics, contact, and multi-body interactions. In contrast, MaskedMimics physics-based formulation allows seamless transitions across modes while ensuring plausible motions that obey physical laws. The user can intuitively specify high-level multi-modal constraints, and MaskedMimic automatically synthesizes the corresponding physically plausible motions. This presents an expressive multi-modal control interface."
        },
        {
            "title": "3 PRELIMINARIES",
            "content": "Our framework consists of two stages. In the first stage, we train motion-tracking controller on large dataset of motion clips using reinforcement learning. Then, we distill that controller into versatile multi-modal controller using behavior cloning. We now review the fundamental concepts and notations behind our framework."
        },
        {
            "title": "3.1 Reinforcement Learning",
            "content": "The first stage of our approach leverages the framework of goalconditioned reinforcement learning (GCRL) to train versatile motion controller that can be directed to perform large variety of tasks. In this framework, an RL agent interacts with an environment according to policy ğœ‹. At each step ğ‘¡, the agent observes state ğ‘ ğ‘¡ and future goal ğ‘”ğ‘¡ . The agent then samples an action ğ‘ğ‘¡ from the policy ğ‘ğ‘¡ ğœ‹ (ğ‘ğ‘¡ ğ‘ ğ‘¡ , ğ‘”ğ‘¡ ). After applying the action, the environment transitions to new state ğ‘ ğ‘¡ +1 according to the environment dynamics ğ‘ (ğ‘ ğ‘¡ +1ğ‘ ğ‘¡ , ğ‘ğ‘¡ ), and the agent receives reward ğ‘Ÿğ‘¡ = ğ‘Ÿ (ğ‘ ğ‘¡ , ğ‘ğ‘¡ , ğ‘ ğ‘¡ +1, ğ‘”ğ‘¡ ). The agents objective is to learn policy that maximizes the discounted cumulative reward: ğ½ = Eğ‘ (ğœ ğœ‹ ) (cid:35) , ğ›¾ğ‘¡ğ‘Ÿğ‘¡ (cid:34) ğ‘‡ ğ‘¡ =0 (1) where ğ‘ (ğœ ğœ‹) = ğ‘ (ğ‘ 0)Î ğ‘‡ 1 ğ‘ (ğ‘ ğ‘¡ +1ğ‘ ğ‘¡ , ğ‘ğ‘¡ )ğœ‹ (ğ‘ğ‘¡ ğ‘ ğ‘¡ , ğ‘”ğ‘¡ ) is the likelihood ğ‘¡ =0 of trajectory ğœ = (ğ‘ 0, ğ‘0, ğ‘Ÿ0, . . . , ğ‘ ğ‘‡ 1, ğ‘ğ‘‡ 1, ğ‘Ÿğ‘‡ 1, ğ‘ ğ‘‡ ). The discount factor ğ›¾ [0, 1) determines the effective horizon of the policy."
        },
        {
            "title": "3.2 Behavioral Cloning",
            "content": "The second stage of our approach leverages behavioral cloning (BC) to distill teacher policy ğœ‹ , trained through RL, into more versatile student policy ğœ‹, which can be directed through multimodal inputs. The policy distillation process is performed using the DAgger method [Ross et al. 2011]. In this online-distillation process, ACM Trans. Graph., Vol. 43, No. 6, Article 209. Publication date: December 2024. trajectories are collected by executing the student policy and then relabeled with actions from the teacher policy: arg max ğœ‹ E(ğ‘ ,ğ‘”)ğ‘ (ğ‘ ,ğ‘”ğœ‹ ) Eğ‘ğœ‹ (ğ‘ ğ‘ ,ğ‘”) [logğœ‹ (ğ‘ğ‘ , ğ‘”)] . (2) ğ‘ (ğ‘ , ğ‘”ğœ‹) denotes the distribution of states and goals observed under the student policy. This form of active behavioral cloning mitigates drift inherent in supervised distillation methods [Ross et al. 2011]."
        },
        {
            "title": "4 SYSTEM OVERVIEW",
            "content": "This work introduces versatile controller for physics-based character animation. We aim to develop scalable system that can learn rich repertoire of behaviors from large and diverse multi-modal datasets. Our framework, illustrated in Figure 3, supports multiple control modalities, providing users with flexible and intuitive interface for directing the behavior of simulated characters. Our framework consists of two stages. First, we train fully-constrained motion tracking controller on large mocap dataset. This controllers inputs consist of the full-body target trajectories of desired motion. The fully-constrained controller is trained to imitate wide variety of motions, including those involving irregular terrains and object interactions. Next, this fully-constrained controller is distilled into more versatile partially-constrained controller. This partially constrained controller can be directed via diverse control inputs. The versatility of the partially-constrained controller arises from masked training scheme. During training, the controller is tasked with reconstructing target full-body motion given randomly masked inputs. This process enables the partially-constrained model to generate full-body motion from arbitrary partial constraints. Stage 1: Fully-Constrained Controller. The goal of physics-based motion tracking is to generate controls (such as motor actuations), which enable simulated character to produce motion {ğ‘ğ‘¡ } that closely resembles kinematic target motion { Ë†ğ‘ğ‘¡ } [Lee et al. 2010b; Peng et al. 2018; Silva et al. 2008; Wang et al. 2020]. We represent motion as sequence of poses ğ‘ğ‘¡ , where each pose ğ‘ğ‘¡ = (ğ‘ğ‘¡ , ğœƒğ‘¡ ) is encoded with redundant representation consisting of the the 3D cartesian positions of characters ğ½ joints ğ‘ğ‘¡ = (ğ‘0 ğ‘¡ ) and their rotations ğœƒğ‘¡ = (ğœƒ 0 ğ‘¡ ). To successfully track reference motion, controllers are typically provided with information that describes the motion it should imitate. For example, motion tracking controllers are commonly conditioned on the target future poses Ë†ğ‘ğ‘¡ [Luo et al. 2023; Wang et al. 2020]. We will refer to target poses as fully-constrained goals ğ‘”full , since the future poses provide complete information about the target motion the character should imitate. ğ‘¡ , ..., ğ‘ ğ½ ğ‘¡ , ..., ğœƒ ğ½ ğ‘¡ , ğ‘1 ğ‘¡ , ğœƒ ğ‘¡ Stage 2: Partially-Constrained Controller. In this work, we propose control model that extends beyond only conditioning on the full target poses to more versatile partially observable goals. For example, typical problem in VR is to generate full-body motion from only head and hands sensors. Similarly, in some cases controller may only observe an object (e.g., chair) and will then be required to generate realistic full-body motions that interact with the target object [Hassan et al. 2023; Pan et al. 2024]. Throughout the paper, we will refer to partially observable goals as ğ‘”partial . These partial goals specify only some elements of desired motion. To train versatile controller that can be directed using partial goals, ğ‘¡ MaskedMimic: Unified Physics-Based Character Control Through Masked Motion Inpainting 209: we propose simple training scheme that trains the controller on randomly masked observations of target motions. These masked observations are constructed using random masking function : ğ‘”partial ğ‘¡ = (ğ‘”full ). ğ‘¡"
        },
        {
            "title": "5 FULLY-CONSTRAINED CONTROLLER",
            "content": "In the first stage of our framework, we train fully-constrained motion tracking controller ğœ‹ FC using reinforcement learning. This controller can imitate large library of reference motions across irregular environments and interact with objects when appropriate. Since the motion dataset only consist of kinematic motion clips, the primary purpose of ğœ‹ FC is to estimate the actions (motor actuations) required to control the simulated character. ğœ‹ FC then provides the foundations that greatly simplifies the training process of more versatile controller in the subsequent stage."
        },
        {
            "title": "5.1 Model Representation",
            "content": "Our fully-constrained controller is trained end-to-end to imitate target motions by conditioning on the full-body motion sequence and observations of the surrounding environment, such as the terrain and object heightmaps. The training objective is formulated as motion-tracking reward and optimized using reinforcement learning [Mnih et al. 2016; Peng et al. 2018]. In this section, we detail the design of various components of the model. Character Observations: At each step, ğœ‹ FC observes the current humanoid state ğ‘ ğ‘¡ , consisting of the 3D body pose and velocity, canonicalized with respect to the characters local coordinate frame: ğ‘ ğ‘¡ = (ğœƒğ‘¡ ğœƒ root ğ‘¡ , (ğ‘ğ‘¡ ğ‘root ğ‘¡ ) ğœƒ root ğ‘¡ , ğ‘£ğ‘¡ ğœƒ root ğ‘¡ ) , (3) where denotes the quaternion difference between two rotations. In addition to the current state of the character, the policy also observes the next ğ¾ target poses from the reference motion ğ‘”FC ğ‘¡ = [ Ë†ğ‘“ğ‘¡ +1, . . . , Ë†ğ‘“ğ‘¡ +ğ¾ ]. The features for each joint Ë†ğ‘“ ğ‘— ğ‘¡ are canonicalized both relative to the current root, and relative to the current respective joint: Ë†ğ‘“ ğ‘— = ( Ë†ğœƒ ğ‘— ğœƒ ğ‘— ) . (4) The features for each target pose Ë†ğ‘ğ‘¡ +ğ‘˜ are also augmented with the time ğœğ‘¡ +ğ‘˜ from the current timestep to the target pose, resulting in the following representation: Ë†ğ‘“ğ‘¡ +ğ‘˜ = { Ë†ğ‘“ 1 ğ‘¡ +ğ‘˜ ğ‘¡ , Ë†ğœƒ ğ‘— ğœƒ root , ( Ë†ğ‘ ğ‘— ğ‘root , ( Ë†ğ‘ ğ‘— ğ‘ ğ‘— ğ‘¡ ) ğœƒ root ğ‘¡ ) ğœƒ root ğ‘¡ , . . . , Ë†ğ‘“ ğ½ , ğœğ‘¡ +ğ‘˜ }. ğ‘¡ +ğ‘˜ ğ‘¡ ğ‘¡ Fig. 3. The MaskedMimic framework: The first phase produces fullyconstrained controller ğœ‹ FC. This full-body tracker is trained using reinforcement learning to imitate kinematic motion recordings across wide range of complex scene-aware contexts. The second phase produces MaskedMimic. Treating ğœ‹ FC as teacher, through supervised limitation learning its knowledge is distilled into partially-constrained controller ğœ‹ PC. As ğœ‹ PC observes masked inputs, this process enables it to perform physics-based inpainting. Finally, at inference, without any further training, ğœ‹ PC is used to generate novel motions, in previously unseen scenes, from partial goals provided by the user. Scene Observations. To imitate motions on irregular terrain, we canonicalize the characters pose with respect to the height of the terrain under the characters root (i.e. pelvis). During training, the controller is provided with heightmap of the surrounding environment, with the heighmap oriented along the roots facing direction [Pan et al. 2024; Rempe et al. 2023]. The heightmap has fixed resolution, and records the height of the nearby terrain geometry and object surfaces. Actions: Similar to prior work [Peng et al. 2018; Tessler et al. 2023], we opt for proportional derivative (PD) control. We do not utilize residual forces [Luo et al. 2021; Yuan and Kitani 2020; Zhang et al. 2023] or residual control [Luo et al. 2022a]. The policys action distribution ğœ‹FC (ğ‘ğ‘¡ ğ‘ ğ‘¡ , ğ‘”full ) is represented using multi-dimensional Gaussian with fixed diagonal covariance matrix ğœğœ‹ = exp(2.9). ğ‘¡ ACM Trans. Graph., Vol. 43, No. 6, Article 209. Publication date: December 2024. 209:6 Tessler, C. et al"
        },
        {
            "title": "5.2 Model Architecture",
            "content": "Motion tracking is sequence modeling problem. The objective is to predict the next actions based on the current character state, surrounding terrain, and sequence of future target poses. Inspired by the success of transformers in natural language processing, we tokenize each of the inputs and design ğœ‹ FC as transformer-based controller. This choice of architecture allows the controller to attend to relevant information across the input sequence and capture the dependencies between the various input tokens. To further enhance the learning process, we employ critic network alongside the transformer-based controller. The critic is implemented as fully connected network that estimates the value function. This provides learning signal to guide the controller towards optimal actions. Once trained, this fully-conditioned controller provides the foundation for our unified character controller. In the following section, we introduce our physics-based motion inpainting approach. This allows users to specify partial or sparse motion constraints, such as keyframes or high-level goals, and synthesize complete motion sequences that satisfy these constraints while remaining consistent with the scene context."
        },
        {
            "title": "5.3 Reward Function\nThe reward ğ‘Ÿğ‘¡ encourages the character to track a reference motion\nby minimizing the difference between the state of the simulated\ncharacter and the target motion:",
            "content": "ğ‘Ÿğ‘¡ = ğ‘¤ gpğ‘Ÿ gp ğ‘¡ + ğ‘¤ grğ‘Ÿ gr ğ‘¡ + ğ‘¤ rhğ‘Ÿ rh ğ‘¡ + ğ‘¤ jvğ‘Ÿ jv ğ‘¡ + ğ‘¤ javğ‘Ÿ jav ğ‘¡ + ğ‘¤ egğ‘Ÿ eg ğ‘¡ , (5) ğ‘¡ where ğ‘Ÿ {} denote various reward components and and ğ‘¤ { } are their respective weights. The terms in the reward function encourages the character to imitate the reference motions global joint positions (gp), global joint rotations (gr), root height (rh), joint velocities (jv), joint angular velocities (jav), as well as an energy penalty (eg) to encourage smoother and less jittery motions [Lee et al. 2023]. more detailed description of the reward function is provided in the supplementary material."
        },
        {
            "title": "5.4 Training Playground",
            "content": "To train controller that can operate in more complex irregular scenes, we construct training environment, shown in Figure 4, composed of three distinct regions: (1) flat terrain, (2) irregular terrain, and (3) object playground. Flat Terrain. First, the flat terrain region is simple environment where the model can focus primarily on imitating the reference motions, as most of the training data was recorded on flat ground. This region is baseline for evaluating the models ability to imitate motions in simple, unobstructed setting. Irregular Terrain. The irregular terrain region contains wide variety of irregular terrain features, including stairs, rough gravellike terrain, and slopes (both smooth and rough). When the agent is imitating motion that does not involve object interactions, it can be spawned at any random location within flat and irregular terrain regions. This setup exposes the model to diverse terrain conditions, ACM Trans. Graph., Vol. 43, No. 6, Article 209. Publication date: December 2024. Fig. 4. Training scene (screenshot): The top region consists of standard flat terrain, enabling the controller to reproduce the original motions in setting that best represents how they were recorded. The central region contains irregular terrain with stairs, slopes, and rough surfaces, allowing the controller to learn robust motion skills on varied ground geometries. The bottom region is reserved exclusively for object interactions, ensuring that the agent can practice interacting with objects in clean and reproducible setup without interference from irregular terrain features. allowing it to learn robust locomotion skills that can accommodate different types of terrains. Object Playground. Finally, the object playground region is reserved for object interaction motions. This region consists of various objects placed on flat ground, such as chairs, tables, and couches. Characters are only initialized in this region when they are imitating motions that involve object interactions."
        },
        {
            "title": "5.5 Early Termination and Prioritized Motion Sampling",
            "content": "To improve the success rate on rare and more complex motions, we perform early termination [Luo et al. 2024; Peng et al. 2018]. Motions performed on flat terrain, are terminated once any joint position deviates by more than 0.25 meters. On irregular terrains, an episode is terminated when joint error exceeds 0.5 meters, providing the controller more flexibility to adapt the original reference motion to new environment. Furthermore, we prioritize training on motions with higher failure rate [Luo et al. 2022b; Zhu et al. 2023]. As some motions are not expected to succeed in all scenarios (e.g., front-flip or cartwheel up flight of stairs), the prioritized sampling only considers failures that occurred on flat terrain. The probability of prioritizing motion ğ‘šğ‘– is proportional to the probability of failing on that motion, clipped to minimal weight of 3ğ‘’ 3. This adaptive MaskedMimic: Unified Physics-Based Character Control Through Masked Motion Inpainting 209:7 sampling strategy is vital to ensure that the agent collects sufficient amount of data to reproduce more dynamic and challenging behaviors."
        },
        {
            "title": "6 VERSATILE PARTIALLY-CONSTRAINED CONTROLLER",
            "content": "Once the fully-constrained motion tracking model has been trained, it is then used to train versatile partially-constrained model, denoted by ğœ‹ PC. The training and inference process are illustrated in Figure 3. Given partial constraints, such as target positions for joints, text commands, or object locations, MaskedMimic generates diverse full-body motions that satisfy those constraints. ğœ‹ PC is trained to model the distribution of actions ğœ‹ FC (ğ‘ğ‘¡ ğ‘”full , ğ‘ ğ‘¡ ) predicted by the fully-constrained controller ğœ‹ FC, while only observing partial constraints ğ‘”partial . The partial constraints then provide users versatile and convenient interface for directing ğœ‹ PC to perform new tasks, without requiring task-specific training. ğ‘¡ ğ‘¡"
        },
        {
            "title": "6.1 Partial Goals\nThe objective of ğœ‹ PC is to produce motions that conform to con-\nstraints specified by partial goals, akin to the task of motion inpaint-\ning. In this work, we consider the following types of goals:",
            "content": "motions that adhere to the specified partial goals, while still allowing for natural variations and adaptability to different contexts. ğ‘¡ MaskedMimic consists of 3 components: learnable prior ğœŒ, an encoder E, and decoder D. The encoder (ğ‘§ğ‘¡ ğ‘ ğ‘¡ , ğ‘”full ) outputs latent distribution given the fully-observable future target poses from the desired reference motion. The decoder (ğ‘ğ‘¡ ğ‘ ğ‘¡ , ğ‘§ğ‘¡ ) is then conditioned on latent sampled from the encoders distribution, and produces an action for the simulated character. The final component is the learned prior ğœŒ (ğ‘§ğ‘¡ ğ‘ ğ‘¡ , ğ‘”partial ). The prior is trained to match the encoders distribution given only partially observed constraints. The learnable prior is crucial component of MaskedMimics design as it allows the model to generate natural motions from simple user-defined partial constraints at runtime, without requiring users to specify full target trajectories for the character to follow. The encoder is used solely for training, and is not utilized at runtime. ğ‘¡ The prior is modeled as Gaussian distribution over latents ğ‘§ğ‘¡ , with mean ğœ‡ğœŒ and diagonal standard deviation matrix ğœ ğœŒ , ğœ‡ğœŒ (cid:16) (cid:16) , ğœ ğœŒ (cid:16) (cid:16) ğ‘§ğ‘¡ ğ‘ ğ‘¡ , ğ‘”partial ğ‘¡ ğ‘ ğ‘¡ , ğ‘”partial ğ‘¡ ğ‘ ğ‘¡ , ğ‘”partial ğ‘¡ = ğœŒ (cid:17) (cid:17) (cid:17)(cid:17) . (6) The encoder is modeled as residual to the prior [Yao et al. 2022], (cid:17)(cid:17) (cid:17) (cid:16) (cid:17) (cid:17) ğœ‡ğœŒ (cid:16) ğ‘ ğ‘¡ , ğ‘”partial ğ‘¡ + ğœ‡ (cid:16) ğ‘ ğ‘¡ , ğ‘”full ğ‘¡ ğ‘ ğ‘¡ , ğ‘”full ğ‘¡ . , ğœ (cid:16) (cid:16) ğ‘§ğ‘¡ (cid:12) ğ‘ ğ‘¡ , ğ‘”full (cid:12) ğ‘¡ (cid:12) = (1) Any-joint-any-time: The model should support conditioning on target positions and rotations for any joint in arbitrary future timesteps. (2) Text-to-motion: The model should support high-level text commands, enabling more intuitive and expressive direction of the characters movements. (3) Objects: When available, the model should support objectbased goals, such as interacting with furniture. (7) This design ensures that the embedding from the encoder, having access to full observations of the target motion, stays close to the prior that only receives partial observations. During training the latent variables ğ‘§ğ‘¡ are sampled from the encoder. All component are trained using an objective that maximizes the log-likelihood of actions predicted by ğœ‹ FC and minimizes the KL divergence between the encoder and prior: To produce desired behavior, our model will support simultaneous conditioning on one or more of the aforementioned goals. For example, path following with raised arms can be achieved by conditioning the controller on target root trajectory and text command walking while raising your hands\". This flexibility allows for wide range of complex and expressive motions to be generated from concise partial specifications. To train ğœ‹ PC, flexible goals are extracted procedurally from mocap data by applying random masking. During training, ğœ‹ PC is trained to imitate the original full (unmasked) target motion by predicting the actions of the fully-constrained controller, which observes the ground-truth full target motion."
        },
        {
            "title": "6.2 Modeling Diversity with Conditional VAEs",
            "content": "Partial goals are an underspecified problem, as there may be multiple plausible motions that can satisfy given set of partial goals. For example, when conditioned on reaching target location within 1 second, there are large variety of motions that can achieve this goal. To address this ambiguity, we model ğœ‹ PC as conditional variational autoencoder (C-VAE). This generative model enables the ğœ‹ PC to model the distribution of different behaviors that satisfy particular set of constraints, rather than simply producing single deterministic behavior. By sampling from this learned distribution, the model can generate variety of realistic and physically-plausible (ğ‘ ,ğ‘”partial )ğ‘ (ğ‘ ,ğ‘”partialğœ‹ PC) (cid:16) (cid:16) ğ›¼ğ·KL Eğ‘ğœ‹ FC (ğ‘ğ‘ ,ğ‘”full) (cid:12) (cid:12) (cid:12) ğ‘ , ğ‘”full(cid:17)(cid:12) ğ‘ , ğ‘”partial(cid:17)(cid:17)(cid:105) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ğœŒ (cid:16) Eğ‘§E(ğ‘§ğ‘ ,ğ‘”full) [log (ğ‘ğ‘ , ğ‘§) , (8) where ğ‘”partial is constructed by applying random masking function to the original fully-observed goals: ğ‘”partial = (ğ‘”full). In the formulation above, ğœ‹ PC interacts with the environment, while ğœ‹ FC labels the target actions for every timestep [Ross et al. 2011, DAgger]. During inference, the encoder is discarded, and latents are sampled only from the prior ğœŒ."
        },
        {
            "title": "6.3 Training",
            "content": "We incorporate number of strategies to improve the stability and effectiveness of the resulting MaskedMimic model. These strategies include: structured masking, KL-scheduling, episodic latent noise, and observation history. Furthermore, during the distillation process, deterministic actions are sampled from both ğœ‹ FC and ğœ‹ PC to reduce stochasticity during data collection. Early termination is also applied during distillation to prevent ğœ‹ PC from entering states that were not observed during the training of ğœ‹ FC. Since ğœ‹ FC also trains with early termination, it may not provide appropriate actions in regions it has not experienced during training. Masking. Our masking process randomly removes individual target joints, the textual description, and the scene information (when ACM Trans. Graph., Vol. 43, No. 6, Article 209. Publication date: December 2024. 209:8 Tessler, C. et al (a) System overview: MaskedMimic is modeled as VAE with learned prior. The prior observes the partial goals, whereas the encoder, used only during training, observes both the full target pose and the partial objectives. During training, the encoder acts as residual to the prior. It learns to provide an offset, in the latent space, towards the precise requested motion. At inference, the encoder is no longer used and the solutions are sampled directly from the prior. (b) Detailed view: During training, features are extracted and masked from ground-truth motion sequences. The prior, transformer network, observes the current pose ğ‘ğ‘¡ , surrounding heightmap â„ğ‘¡ , past poses {ğ‘ğ‘¡ ğœ }, object representation ğ‘œğ‘¡ , text command ğ‘ğ‘¡ , and target future poses { Ë†ğ‘ğ‘¡ +ğœ }. Each input modality is tokenized (encoded) using modality-specific encoder ğ‘’ğ‘– (). Future poses are masked before encoding, ensuring the encoder only observes the conditionable joints. These tokens and the token masks are then provided to the prior (transformer). The token masks prevent the transformer from attending to unspecified inputs, such as keyframe without any target joints, or sequence without text or object conditioning. The encoder and decoder are modeled as fully-connected networks, and observe flattened concatenation of the input features. Fig. 5. MaskedMimic VAE Architecture. applicable) from the input goals to the model. To better ensure temporally coherent behaviors, we leverage masking scheme that is structured through time. randomly sampled mask in one timestep has chance of being repeated for multiple subsequent timesteps, as opposed to randomly re-sampling the mask at each step. We observe that randomly re-sampling the mask on each step reduces the ambiguity the model encounters during training. Therefore, the resulting model generalizes worse. This is because different joints are likely to be visible across different frames, the cross-frame information provides less ambiguous description of the requested motion. By using temporally consistent sampling scheme, we ensure that certain joints are observed for multiple consecutive frames, while other joints remain consistently hidden. To ensure the model supports high-level goals, such as textcommands and interaction with target object, all future poses can be masked out. This structured sampling mechanism guarantees that ğœ‹ PC encounters, and learns to handle, range of different ACM Trans. Graph., Vol. 43, No. 6, Article 209. Publication date: December 2024. masking patterns during training. This results in increased robustness to possible user inputs. We provide pseudo-code of our mask sampling strategy in the supplementary material. KL-scheduling. Similar to ğ›½-VAE [Higgins et al. 2016], we initialize the KL-coeff with low value of 0.0001, and linearly increase its value to 0.01 over the course of training. Starting with low KL coefficient enables the encoder-decoder to more closely imitate ğœ‹ FC. Increasing the coefficient then encourages the model to impose more structure into the learned latent space, to be more amenable to sampling from the prior at runtime. Episodic latent noise. During training, latents are sampled via the reparametrization trick. To further encourage more temporally consistent behaviors, we keep the noise\" parameter ğœ– ğ‘ (0, 1) fixed throughout the entire episode. Therefore, in each episode ğœ the latent variables are sampled according to ğ‘§ğœ ğ‘¡ , and the noise ğœ–ğœ is constant throughout an episode. ğ‘¡ = ğœ–ğœ ğœğœ ğ‘¡ + ğœ‡ğœ MaskedMimic: Unified Physics-Based Character Control Through Masked Motion Inpainting 209:9 Observation history. When conditioning on text commands, we find that providing ğœ‹ PC with past poses is crucial for generating long coherent motions that conform to the intent of given text command. Therefore, following Juravsky et al. [2024], we provide the prior with 5 observations subsampled from the observations in the past 40 timesteps."
        },
        {
            "title": "6.4 Observation Representations",
            "content": "We construct representation for each type of input modality that ğœ‹ PC can receive as input. The objective is to provide sufficiently rich representation, that is also computationally efficient and facilitates generalization to new tasks. Keyframes. future keyframe with partially observable joints is first canonicalized to the current pose (Equation (3)). The unobserved joints are then zeroed out, and the mask is appended alongside the time to reach the target frame ğœ [ Ë†ğ‘ğ‘¡ +ğœ maskğ‘¡ +ğœ , maskğ‘¡ +ğœ , ğœ]. Observations of poses from previous timesteps are represented in similar fashion, but all the joints are observed and no masking is applied. Objects. We represent objects using the positions of the 8 corners of bounding-box, canonicalized to the characters local coordinate frame. To identify different types of objects, we also provide an index representing the object type (e.g., chair, sofa, stool). Text. Each text command is encoded using XCLIP embeddings [Ni et al. 2022], which are trained on video-language pairs to better capture temporal relationships. By leveraging the spatio-temporal information in videos during training, the XCLIP embeddings can encode the temporal aspects of language crucial for describing motions, making them well-suited for representing text commands to be translated into character animations."
        },
        {
            "title": "6.5 Architecture",
            "content": "To provide unified architecture capable of processing multi-modal inputs, we model the prior using transformer-encoder. This enables variable length input tokens depending on the observable goals at each timestep. Each input modality (target pose Ë†ğ‘ğ‘¡ +ğœ , object bounding box ğ‘œğ‘¡ , terrain heightmap â„ğ‘¡ , current pose ğ‘ ğ‘¡ , text ğ‘¤ğ‘¡ , and historical pose ğ‘ğ‘¡ ğœ ) has unique encoder that is shared across all inputs of the same modality. When an input is masked out, we utilize the transformer masking mechanism to exclude the respective tokens. The output of the transformer is provided to two fullyconnected layers to output the mean and log-standard deviation for the prior distribution. Since the encoder always observes the full target frames as input, it is represented as fully connected model, as its inputs are always fixed size. The encoder observes the full future poses Ë†ğ‘ğ‘¡ +ğœ in addition to the masking applied to the keyframes, indicating which joints are visible to the prior. In addition, it observes the current pose ğ‘ ğ‘¡ and the terrain heightmap â„ğ‘¡ . Like the prior, two fully-connected output heads output the residual mean and the logstd for the encoder. Similarly, the decoder is also modeled as fully-connected network. It observes the current state ğ‘ ğ‘¡ , the sampled latent ğ‘§ğ‘¡ , and the terrain heightmap â„ğ‘¡ . The decoder then outputs deterministic action ğ‘ğ‘¡ . high level illustration is provided in Figure 5b."
        },
        {
            "title": "7 EXPERIMENTAL SETUP",
            "content": "All physics simulation are performed using Isaac Gym [Makoviychuk et al. 2021], each with 16,384 parallel environments, split across 4 A100 GPUs. Models are trained for approximately 2 weeks, amounting to approximately 30 (10) billion steps for ğœ‹ FC (ğœ‹ PC). The controllers operate at 30 Hz, and the simulation runs at 120 Hz. Detailed hyperparameter settings are available in the supplementary material. When training ğœ‹ PC, we train with joint conditioning for key subset of body parts: Left Ankle, Right Ankle, Pelvis, Head, Left Hand, and Right Hand."
        },
        {
            "title": "7.1 Datasets",
            "content": "To train unified controller capable of being directed using different control modalities, our models are trained using an aggregation of multiple datasets that collectively provide range of different modalities. Keyframe Conditioning. The core of our data is the AMASS dataset [Mahmood et al. 2019]. AMASS contains mocap recordings for wide range of diverse human behaviors, without scene information or text. From this dataset, we extract the joint positions, rotations, and their relative timings. This enables any-joint-any-time conditioning, where controller can be conditioned on target positions or rotations at various future timesteps. To improve generalization to new and unseen motions, we mirror the motions (flip left-to-right) as form of data augmentation. However, it has been observed in prior work that some motions in the AMASS dataset contain severe artifacts [Juravsky et al. 2024; Luo et al. 2023, 2024], including nonphysical artifacts such as intersecting body parts, floating, or scene interactions without objects (such as walking up staircase that does not exist). We follow the same filtering process as PHC [Luo et al. 2023] to obtain filtered dataset. Text Conditioning. To enable text control, we utilize the HumanML3D dataset [Guo et al. 2022]. HumanML3D provides breakdown of the AMASS motion sequences into atomic behaviors, each labeled with text descriptions. This allows MaskedMimic to be conditioned on text commands, providing more intuitive and expressive way to direct the characters movements. We use the mirrored-text for mirrored motions, for example, person turns left\" is converted to person turns right\". Scene Interaction. To synthesize natural interactions between characters and objects, we utilize the SAMP dataset [Hassan et al. 2021]. The SAMP dataset contains motion clips for interacting with different categories of objects, in addition to meshes of the corresponding objects for each motion clip. To train models that are able to interact with wider array of objects, objects are randomly sampled within the class of objects associated with each motion clip Hassan et al. [2023]. For example, multiple different armchair models can be used to train the same sitting behavior, allowing our model to be conditioned on different target objects for interaction at runtime. ACM Trans. Graph., Vol. 43, No. 6, Article 209. Publication date: December 2024. 209:10 Tessler, C. et al"
        },
        {
            "title": "7.2 Evaluation",
            "content": "To evaluate the effectiveness of our framework, we construct benchmark consisting of common tasks introduced by prior systems. For each tasks, we report success rate metric and an error rate metric. Both are aimed to complement one another. During evaluation, the MaskedMimic model is conditioned on the mean of the priors latent distribution. Sampling latents randomly produces more diverse behaviors, however, it can also produce less-likely solutions that are more likely to fail. This results in slight degradation of performance with respect to the raw tracking metrics. In all the evaluations, we analyze the performance of unified model. This model is not fine-tuned for any specific task, but instead, it is controlled through user-specified goals or goals extracted from kinematic recordings (for the motion-tracking tasks). Similarly, the qualitative results showcase motions generated from new user-generated goals or tracking new motions that were not observed during training. Qualitative results are best viewed in the supplementary video. Full-body tracking. We begin by evaluating both the fully constrained controller ğœ‹ FC and partially-constrained controller MaskedMimic ğœ‹ PC on the task of full-body motion tracking. Given target motion, the controllers are required to closely track the sequence of future target poses from the target motion. For this task, all features of the target future poses are fully observed by the controllers. This test establishes the baseline capability for motion generation, both in terms of success rates and tracking quality, and allows comparison to prior systems for motion tracking. Joint sparsity. To evaluate the models effectiveness for generating plausible motions from partial constraints, we first consider the task of VR tracking. In this task, the controllers no longer observe the full target poses. Instead, they are provided with the target head position and rotation, in addition to the hand positions [Winkler et al. 2022]. Note, MaskedMimic models are not explicitly trained for VR tracking. We compare to the results reported in Luo et al. [2024], consisting of 3 baselines: PULSE [Luo et al. 2024], ASE [Peng et al. 2022], and CALM [Tessler et al. 2023]. In addition, we evaluate the ability of tracking motions given varying joint targets. This is made possible by MaskedMimics support for any-joint conditioning during inference. Irregular terrains. To evaluate the robustness of our models to variations in the environment, we evaluate the performance of ğœ‹ FC and ğœ‹ PC on both full-body imitation and VR-tracking when spawned on randomly generated irregular terrains. The terrain consists of rough (gravel like) ground, stairs, and slopes (rough and smooth). Similar to the previous experiments, goals are still extracted from human motion data from AMASS. The controller is therefore evaluated on its ability to closely imitate large variety of motions while accommodating irregular terrains."
        },
        {
            "title": "7.3 Tasks",
            "content": "By training MaskedMimic on randomly masked input goals, the model learns versatile interface that can be easily used to direct the controller to perform new tasks. Performing new tasks often requires generalization to new and unseen scenarios. In this section, ACM Trans. Graph., Vol. 43, No. 6, Article 209. Publication date: December 2024. we evaluate MaskedMimics ability to generalize and handle userdefined goals, which the model was not explicitly trained on. To direct the model to perform new tasks, we construct simple finitestate-machines that transition between goals provided to the controller. This form of goal-engineering (akin to prompt-engineering for language models) enables MaskedMimic to perform range of new tasks, without additional task-specific training. Path-Following. The character is tasked with following 3D path. This path specifies target positions for the head (including height) at each timestep. By varying the target locations, the character can be directed to perform different locomotion styles, such as walking, crouched-walking, and crawling. The paths are randomly generated with varying heights and speeds. Each episode has length of 30 seconds. Steering. The steering task is analogous to joystick controller, where the characters movements are controlled by two target direction. One direction specifies the target heading direction and the other specifies the target direction (and speed). This enables separating the control between the direction the character should face and the direction it should move. Reach. In addition to locomotion, we show that MaskedMimic can also be used to perform more fine-grained control over the movement of individual body parts. The goal of the reach task is for the right hand to reach randomly changing target position. Once the target position changes, the character has 2 seconds to reach the position with its hand and stay at that location. Object Interaction. Finally, we show that MaskedMimic can also be directed to generate natural interactions with objects by conditioning the model on features of target object. In this task, we focus on sitting on set of held-out objects. These objects were not used during the training phase. The character is first initialized at random location between 2 and 10 meters away from the object."
        },
        {
            "title": "8 RESULTS",
            "content": "The MaskedMimic framework enables versatile physics-based character control by formulating the problem as motion inpainting from partial constraints. In this section, we present key results demonstrating the effectiveness and flexibility of our approach."
        },
        {
            "title": "8.1 Motion Tracking",
            "content": "Tables 1, 2 and 4 record the performance statistics for the motion tracking task, and Figure 6 shows examples of the behaviors our model produces. MaskedMimic exhibits robust behaviors and improved generalization to new (test) motions compared to prior motion tracking models. When provided full-body targets, MaskedMimic tracks the entire motion of karate fighter and dancer, matching hand and foot positions of fighting stance. When provided with partial target poses, for example recovering full motion from sparse VR sensors, MaskedMimic succeeds in reproducing cartwheel across irregular terrain from only sparse VR constraints, as well as running while following target trajectory for the head. Full-body tracking. Table 1 shows the performance on the fullbody motion tracking task. Performance is evaluated on both the MaskedMimic: Unified Physics-Based Character Control Through Masked Motion Inpainting 209:11 (a) Full-body tracking: punching (b) Full-body tracking: dancing (c) VR tracking: cartwheel (d) Path following: run Fig. 6. Motion tracking: MaskedMimic generates full-body motion when tracking signals extracted from unseen kinematic motions. Precise fighting and dancing moves when tracking full-body information, cartwheel from VR signals, and running by tracking the head (path following). The green spheres represent the target joint positions in each frame. Table 1. Full-body tracking, flat terrain: Tracking full-body kinematic recordings from the AMASS dataset [Mahmood et al. 2019]. We highlight be best performing model on test motions. FC (ours) PHC+ MaskedMimic (ours) PULSE Train Test Success MPJPE Success MPJPE 99.96% 100% 99.4% 99.8% 30.4 26.6 32.9 39.2 99.9% 99.2% 99.2% 97.1% 31.3 36.1 35.1 54. Table 2. VR tracking, flat terrain: Tracking VR-signals extracted from the AMASS dataset. In addition to the full-body tracking (MPJPE), we report that MaskedMimic received MPOJPE (VR tracking error) of 39.5 (train) and 45.8 (test). Train Test Success MPJPE Success MPJPE MaskedMimic (ours) PULSE ASE CALM 98.6% 99.5% 79.8% 16.6% 50 57.8 103 130.7 98.1% 93.4% 37.6% 10.1% 58.1 88.6 120.5 122.4 AMASS train and test splits. We consider trial \"failed\" if at any frame the average joint deviation is larger than 0.5m [Luo et al. 2021]. To complement the success metric, we report the MPJPE (Mean Per Joint Position Error, in millimeters). MPJPE measures how closely the character can track the target joint positions (in global coordinates). Our fully-constrained tracker FC outperforms PHC+ [Luo et al. 2023, 2024], reducing the tracking failure rate on unseen motions by 62.5%. In addition to lower failure rate, our controller also supports wider range of motions, irregular terrains, and object interactions. We attribute these performance improvements to our architecture and data augmentation techniques. key difference in our approach is the use of single unified network, in contrast to the mixture-of-experts (MoE) model employed by PHC. While MoE approaches, such as training different controllers for each task [Peng et al. 2018] or using progressively growing mixture of multi-motion trackers [Luo et al. 2023], have been applied to fully-constrained tracking problems, they present certain challenges. As motion variety increases, maintaining multiple experts becomes difficult in an online distillation (DAgger) regime. Moreover, the MoE architecture in PHC requires an additional gating network to select and blend between multiple networks (experts) depending on the target motion. Our experiments demonstrate that single monolithic network offers better generalization capabilities. This approach not only simplifies the architecture but also avoids the complexities associated with expert selection and blending. The superior performance of our model suggests that, in the context of full-body tracking, welldesigned unified network can effectively capture the diversity of motions without the need for specialized experts. When comparing MaskedMimic with PULSE [Luo et al. 2024], we observe that PULSE exhibits more pronounced overfitting to the training data, while MaskedMimic demonstrates superior generalization performance on the test set. This distinction can be attributed to two key factors. First, the expert used for training MaskedMimic possesses better generalization capabilities, characteristic that may transfer to the student during the distillation process. Additionally, MaskedMimic is designed to tackle wide range of tasks across diverse scenes, which likely contributes to the models enhanced generalization capabilities. This multi-task, multienvironment training approach appears to foster more robust and adaptable model, enabling it to perform well on unseen data and scenarios. VR tracking. Table 2 compares the performance of various models on the VR tracking task on flat terrain. We report metrics on the 3 available sensors, measuring success rate for tracking the head and hands, in addition to the MPJPE measured on the unseen full-body motion, and the tracking error on the observed joints (MPOJPE, ACM Trans. Graph., Vol. 43, No. 6, Article 209. Publication date: December 2024. 209:12 Tessler, C. et al Table 3. MaskedMimic, joint sparsity, flat terrain: Tracking partial-joint signals extracted from the AMASS dataset. Table 5. Tasks: MaskedMimic is evaluated on suite of tasks, where the model is directed to perform each task by conditioning on multi-modal goals. Train Test Success MPOJPE Success MPOJPE Full body Pelvis VR Head Hands Feet 99.4% 98.4% 98.6% 97.7% 95.2% 92.7% 32.9 31.4 39.5 42.6 60.2 88 99.1% 98.4% 98.1% 97.9% 93.4% 91.8% 35.1 33.4 45.8 45.6 69.6 94.3 Table 4. MaskedMimic, irregular terrain: We evaluate our models from both training stages on the task of tracking motions from the AMASS dataset across irregular terrains. Full-body VR Success MPJPE Success MPOJPE FC MaskedMimic Train Test Train Test 98% 98.2% 94.7% 95.4% 51.5 51 61.3 62.9 94.4% 93.6% 62.7 69.4 mean per observed joints positional error). We compare to PULSE [Luo et al. 2024], ASE [Peng et al. 2022], and CALM [Tessler et al. 2023]. These methods train reusable low-level controller. Then, they train new high-level controller to manage this specific task. In contrast, MaskedMimic is applied directly to this sparse tracking task without any additional training. Despite not being explicitly trained on this task, MaskedMimic outperforms other models by significant margin when evaluated on tracking target trajectories extracted from the AMASS test set. Joint sparsity. The results presented in Table 3 illustrate MaskedMimics capability to produce full-body motion while varying the conditioned joints. The analysis reveals hierarchy of difficulty in joint tracking. Specifically, foot tracking presents greater challenge compared to hand tracking. Additionally, pelvis tracking proves to be less demanding than tracking the head and hands. We find the last result particularly interesting. Reconstructing full-body motion from VR sensors is an important task for gaming and collaborative work. These results suggest that adding pelvis sensor, or predicting the pelvis positioning using head-mounted sensors, could provide noticeable boost in the ability to recover the users motion. Irregular terrains. Most prior models are not trained for interactions with objects and irregular terrains. Therefore, our evaluation on irregular terrain will compare the fully-constrained motion tracking controller with the more versatile MaskedMimic controller. As shown in Table 4, MaskedMimic exhibits similar success rates and tracking errors across both the train and test sets, when evaluated on randomly generated irregular terrain. These results further validate the robustness and generalization capabilities of our MaskedMimic model. ACM Trans. Graph., Vol. 43, No. 6, Article 209. Publication date: December 2024. Locomotion Steering Reach Flat Terrain Flat Terrain Flat Terrain Success Error 96.3% 96.3% 97.8% 93.8% 88.7% 87.3% 11.2 [cm] 12.5 [cm] 8.4 [cm/s] 8.4 [cm/s] 20.3 [cm] 21.7 [cm]"
        },
        {
            "title": "8.2 Goal-Engineering",
            "content": "To solve tasks with MaskedMimic , we utilize \"goal-engineering\", process akin to \"prompt-engineering\" for language models [Diab et al. 2022]. For each task, we construct simple finite-state-machine (FSM) that transitions between different goals provided to MaskedMimic . At each timestep, the FSM evaluates the current system state and determines and updates the current control scheme. For example, sitting on chair consists of (1) navigating towards the chair using inbetweening (any-joint-any-time constraints), then, once within range, (2) conditioning on the chair bounding-box representation. By conditioning MaskedMimic on different goals at each stage of the task, the controller can be directed to perform wide range of tasks without any task-specific training. The performance of MaskedMimic on the various tasks is recorded in Table 5, and performance on object interaction tasks is recorded in Table 6. For each task, we report the average performance statistics recorded across 5000 random episodes. Behaviors produced by our model on various tasks when directed through goal-engineering are shown in Figure 7. MaskedMimic is able to generate naturalistic motions that follow user-specified goals across variety of different irregular environments. For the steering task, we can construct joystick-like controller by simply conditioning the model on target movement and heading directions, and the controller is then able to generate natural motions that follow the given commands. When provided target positions for the hand, our model produces diverse motions to reach different target locations, such as reaching and bending over. By conditioning the model on target head position and orientation ensures, MaskedMimic tracks specific target path while also adhering to different height constraints. This then allows users to direct the character to walk up and down flight of stairs and crawl across flat ground. Text can also be used to stylize the resulting motions. For example, Figure 7f shows that target path can be specified for the head of the character, with text specifying the style for the rest of the body. Path-Following. The goal is to follow given trajectory. trial is marked as failed if, at any timestep, the character deviates by more than 2m from the goal position. We report the average displacement error for the 3D position in cm. To successfully follow given target trajectory, consisting of sequence of waypoint positions, we first compute the target rotation at each timestep using the direction between each two subsequent waypoints. At each timestep, MaskedMimic is provided the target positions and rotations at the MaskedMimic: Unified Physics-Based Character Control Through Masked Motion Inpainting 209: (a) Steering: rough terrain (b) Steering: stairs (c) Reach: flat terrain (d) Reach: stairs (e) Path-Following: walk (f) Path-Following + Text: person raises both hands and walks forward Fig. 7. Tasks: MaskedMimic can be used to solve new tasks across wide range of terrains by conditioning the model on different user-specified goals. (g) Path-Following: crawl next five timesteps, as well as target 0.8 seconds into the future to provide the model with information for longer term planning. When the character is more than 0.4m from the target position, we only provide the distant target at 0.8s as input to the model, thereby providing the model with more flexibility in terms of how to move closer to the path. We observed tradeoff between user control and success rate. By providing the model with more flexibility in the goal inputs and not tightly constraining the near-term goals, the success rate increases and tracking error decreases, but at the cost of reduced user control. Steering. This task provides two vectors, corresponding to the requested orientation and movement. The character needs to move in the direction and speed of the movement vector, while facing the orientation vector. We consider trial as failed if the orientation deviates by more than 45 degrees, and we report the speed error in cm/s, measured along the target direction. Each time the vectors change, the character has 2 seconds before measurement begins. We solve this task using any-joint-any-time control by conditioning on the pelvis rotation and location. To ensure the character moves as requested, we first condition the target rotation alongside the timeleft until measurement starts ( Ë†ğœƒ root, ğœ ğ‘¡). Then, we condition the root rotation and offset 1 second into the future ( Ë†ğœƒ root, Ë†ğ‘root, 1[ğ‘ ğ‘’ğ‘]), with Ë†ğ‘root = [ğ‘_ğ‘¥ root + ğ‘£_ğ‘¥, ğ‘_ğ‘¦root + ğ‘£_ğ‘¦, 0.9], to ensure it remains upright. Reach. This task presents problem of inbetweening. When the target position changes, the character has 2 seconds to reach the new position. The entire motion inbetween remains unspecified. trial is considered as fail if the hand deviates by more than 0.5m at the target timestep. We also report the average distance in cm, measured over the time the hand should remain in place. To solve this task, we only condition the model on the target position for the right hand, along with the remaining time to reach the target, clipped to maximum of 1/6 seconds. We found that if the target is set at the immediate next frames, MaskedMimic is less successful at recovering when losing balance. However, by providing goal further into the future, this provides MaskedMimic with more flexibility. The provided flexibility leads to more realistic and robust behaviors."
        },
        {
            "title": "8.3 Object Interaction and Ablation",
            "content": "The previous tasks were solved by leveraging any-joint-any-time constraints. In this task, the character is spawned at random location, far from an object. The goal is to reach the object and sit on it. To tackle this type of task, we combine three control modalities: ACM Trans. Graph., Vol. 43, No. 6, Article 209. Publication date: December 2024. 209:14 Tessler, C. et al (a) Armchair (b) Table (c) Stool (d) Chair Fig. 8. Objects: By conditioning MaskedMimic on the bounding-box of an object, our model is able to produce diverse motions for approaching and interacting with wide variety of test objects. In Figure 8e, the character successfully generalizes to interacting with sofa placed on irregular terrain, scenario that was not observed during training. (e) Sofa any-joint-any-time, text, and object conditioning. First, when the character is further than 2 meters from the target object, we utilize any-joint-any-time control. The character is conditioned on goal direction for moving towards the object at speed of 1[m/s]. While moving towards the object, it is also provided text command of the person walks normally\". These two commands allow the character to walk in stable and natural manner towards the provided object. Then, once within 2 meters of the object, the target direction and text command are removed. From this point, the controller is conditioned on the objects bounding box. Conditioning on the object leads the model to generate motion for interacting with the object (e.g. sitting). MaskedMimic is able to seamless transition between different controls and generate natural interactions with target object. The object interaction motions in the SAMP dataset [Hassan et al. 2021] consist of person walking towards an object and sitting down on it. As such, we find that by simply providing the character with the object representation was sufficient for it to generate natural object interaction motion. Figure 8 shows examples of motions generated by MaskedMimic when interacting with chairs and sofas. These objects are taken from test set, not observed during training. In Figures 8a to 8d, we observe diverse motions for approaching objects of varying shapes and sizes. Then, in Figure 8e, we place sofa on rough terrain. Although this combination of an object with irregular terrain was not observed during training, MaskedMimic succeeds in moving to the object and sitting/lying down. Notably, MaskedMimic does not produce single solution. The VAE architecture is designed to model multiple possible solutions for given set of constraints, enabling the model to generate diverse interactions for given object. As seen in Figure 8e, MaskedMimic sits on diverse locations and poses. We report the performance statistics in Table 6. As solving object interactions combines multiple control modalities, and requires precise scene awareness, this task also serves as an ablation study. In our ablation, we compare several core design decisions. trial is considered successful if the character successfully sits on the object during the episode, e.g., pelvis reaches within 20cm of valid seating position. In addition, we report the average minimal distance between ACM Trans. Graph., Vol. 43, No. 6, Article 209. Publication date: December 2024. Table 6. Objects + ablation: We evaluate MaskedMimic and conduct an ablation on various design decisions. Experiments are conducted on the sitting task with set of test objects. We evaluate versions of the model with key components removed (Section 6), and measure the impact on the average success rate and error (i.e. average minimal distance from valid sitting position on the object). Success Error [cm] MaskedMimic (ours) No history No VAE No residual prior No structured masking 96.9% 94.9% 93.2% 21.1% 0% 10.5 12.7 12.2 57.4 274. the object and the characters pelvis. When evaluating the impact of various design decisions, we find that: (1) Typically, motions start standing still. By providing the historical poses, MaskedMimic is less likely to get stuck in place. (2) The VAE structure ensures more robust controller by providing better way of encoding the diversity of solutions. (3) The inductive bias present in the residual architecture [Yao et al. 2022] ensures the prior properly controls the latent space encoding. non-residual prior proves incapable of controlling the motions. Finally, (4) having structured masking mechanism is crucial. This mechanism ensures the model observes repeating joints and sometimes only high-level commands."
        },
        {
            "title": "8.4 Text Control",
            "content": "In Figure 9, we present examples of text-to-motion control with MaskedMimic. All motions are initialized in neutral standing state. From this initial state, the model is provided different text commands, and MaskedMimic is then able to generate the corresponding behaviors. However, while MaskedMimic shows promising behavior when directed to perform relatively simple atomic behaviors (e.g., salut, kick, etc...), it struggles when provided with commands that require long-term reasoning, such as person takes 4 steps and then raises their hands\". We speculate that this is the result of MaskedMimic: Unified Physics-Based Character Control Through Masked Motion Inpainting 209:15 (a) \"a person kicking forward\" (b) \"a person standing on one leg trying to balance himself\" (c) \"a person dances like michael jackson\" (d) \"a person raises both hands in the air and walks forward\" (e) \"a person lifts both arms to the sides then steps forward and kneels down\" (f) \"a person raises their right arm in salute\" (g) \"a man bows very slowly\" (h) \"a man steps forward and does handstand\" Fig. 9. Text control: MaskedMimic generates full-body motion from text-only control. These examples were all generated from neutral pose, not indicative of the requested motion. the relatively short history in the models observations, which can hamper long-term reasoning capabilities. automate the goal-engineering process, for example by leveraging large-language-models [Ma et al. 2023; Wang et al. 2024b]."
        },
        {
            "title": "9 LIMITATIONS AND FUTURE WORK",
            "content": "Although MaskedMimic presents unified model for controlling physically simulated humanoids, there remains number of limitations with our model. We identify three main avenues for improvement: motion quality, automatic goal-engineering, and learning new capabilities. Motion Quality. While MaskedMimic demonstrates high success rates in generating diverse motions, there are three notable areas for improvement in terms of motion quality. First, some generated motions exhibit unnatural jittering behaviors. This could be mitigated by finetuning MaskedMimic using discriminative reward [Peng et al. 2021] to discourage unrealistic behaviors. Second, the controller does not perfectly replicate the entire training dataset, with some challenging motions such as backflips and breakdancing remaining difficult to reproduce. Furthermore, when navigating irregular terrain, the character tends to mimic standard walking motions rather than planning more suitable foot placements to avoid rugged regions. Although the controller is robust, it does not fully capture the longer-horizon planning behaviors observed in humans. We hypothesize that this limitation stems from the naive mapping of motions from flat to irregular terrains based on the root-to-floor distance normalization. Future work could address this issue by collecting motions with accompanying scene information or developing improved motion retargeting schemes. Goal-Engineering. Through goal-engineering, MaskedMimic can be directed to perform wide range of tasks, which prior methods required training task-specific models. However, designing goals for controlling large group of characters to produce natural behaviors in more complex scenes (e.g., crowds) could prove challenging and labour-intensive. We are interested in exploring methods to New Capabilities. Going beyond interactions with static scenes, we are interested in expanding the MaskedMimics capabilities to interact with dynamic scenes. For example, character should be capable of manipulating objects in the scene, moving objects around and using tools. Furthermore, we are interested in applying MaskedMimic to synthesize more complex multi-agent interactions."
        },
        {
            "title": "10 DISCUSSION",
            "content": "In this paper, we introduced MaskedMimic, unified model for physics-based character control through motion inpainting. Our model supports diverse control modalities and demonstrates robust performance across various tasks and environments. We presented novel approach that formulates the problem of generating motions for physically simulated characters as an inpainting process from partial information (masked). MaskedMimic is trained on an extensive corpus of randomly masked motion clips. These motion clips contain multi-modal inputs, such as target joint positions/rotations, text descriptions, and objects. By strategically masking out portions of the input motion data, the model is forced to learn to fill in the missing information in coherent and diverse manner. This allows the system to generate wide variety of plausible motions that satisfy flexible variety of different constraints, without requiring exhaustive manual specification of the entire target motion sequence. We demonstrate that many tasks in character animation can be intuitively described through the lens of partial constraints. These partial constraints specify the goal-directed aspects of the desired motion. Among the examples we presented were joystick steering, reaching, VR-tracking, full-body tracking, path-following, object interaction, and even text-to-motion synthesis. Remarkably, single unified control architecture can be used to perform all of these diverse tasks without any task-specific training or reward engineering. ACM Trans. Graph., Vol. 43, No. 6, Article 209. Publication date: December 2024. 209:16 Tessler, C. et al The key insight is that by learning to generate motions from partial specifications, the model acquires general understanding of how to produce realistic and physically-plausible character movements that can be directed towards various goals. This approach greatly simplifies the animation process and enables more intuitive and flexible control over the behaviors of physically simulated characters."
        },
        {
            "title": "11 ACKNOWLEDGEMENTS",
            "content": "We would like to express our sincere gratitude to Haggai Maron for his invaluable assistance in the writing process of this paper. His insights and feedback significantly improved the quality and clarity of our work. We also thank Liang Pan for scene interaction examples, which helped illustrate key concepts in our research. Additionally, we are grateful to the anonymous reviewers for their constructive comments that enhanced the final version of this paper."
        },
        {
            "title": "REFERENCES",
            "content": "Martin de Lasa, Igor Mordatch, and Aaron Hertzmann. 2010. Feature-Based Locomotion Controllers. ACM Trans. Graph. 29, 4, Article 131 (jul 2010), 10 pages. https: //doi.org/10.1145/1778765.1781157 Mohamad Diab, Julian Herrera, Bob Chernow, and Coco Mao. 2022. Stable Diffusion Prompt Book. Technical Report. Andrea Dittadi, Sebastian Dziadzio, Darren Cosker, Ben Lundell, Thomas Cashman, and Jamie Shotton. 2021. Full-body motion from single head-mounted device: Generating smpl poses from partial observations. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 1168711697. Zhiyang Dou, Xuelin Chen, Qingnan Fan, Taku Komura, and Wenping Wang. 2023. ASE: Learning conditional adversarial skill embeddings for physics-based characters. In SIGGRAPH Asia 2023 Conference Papers. 111. Yuming Du, Robin Kips, Albert Pumarola, Sebastian Starke, Ali Thabet, and Artsiom Sanakoyeu. 2023. Avatars grow legs: Generating smooth human motion from sparse tracking inputs with diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 481490. Thomas Geijtenbeek, Michiel van de Panne, and A. Frank van der Stappen. 2013. Flexible Muscle-Based Locomotion for Bipedal Creatures. ACM Transactions on Graphics 32, 6 (2013). Deepak Gopinath, Hanbyul Joo, and Jungdam Won. 2022. Motion In-betweening for Physically Simulated Characters. In SIGGRAPH Asia 2022 Posters. 12. Sebastian Grassia. 1998. Practical parameterization of rotations using the exponential map. Journal of graphics tools 3, 3 (1998), 2948. Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. 2022. Generating Diverse and Natural 3D Human Motions From Text. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 51525161. Mohamed Hassan, Duygu Ceylan, Ruben Villegas, Jun Saito, Jimei Yang, Yi Zhou, and Michael Black. 2021. Stochastic scene-aware motion prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 1137411384. Mohamed Hassan, Yunrong Guo, Tingwu Wang, Michael Black, Sanja Fidler, and Xue Bin Peng. 2023. Synthesizing physical character-scene interactions. In ACM SIGGRAPH 2023 Conference Proceedings. 19. Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. 2016. beta-vae: Learning basic visual concepts with constrained variational framework. In International conference on learning representations. Yinghao Huang, Manuel Kaufmann, Emre Aksan, Michael Black, Otmar Hilliges, and Gerard Pons-Moll. 2018. Deep inertial poser: Learning to reconstruct human pose from sparse inertial measurements in real time. ACM Transactions on Graphics (TOG) 37, 6 (2018), 115. Jordan Juravsky, Yunrong Guo, Sanja Fidler, and Xue Bin Peng. 2022. PADL: LanguageDirected Physics-Based Character Control. In SIGGRAPH Asia 2022 Conference Papers (Daegu, Republic of Korea) (SA 22). Association for Computing Machinery, New York, NY, USA, Article 19, 9 pages. https://doi.org/10.1145/3550469.3555391 Jordan Juravsky, Yunrong Guo, Sanja Fidler, and Xue Bin Peng. 2024. SuperPADL: Scaling Language-Directed Physics-Based Control with Progressive Supervised Distillation. In ACM SIGGRAPH 2024 Conference Papers. 111. Diederik Kingma and Jimmy Ba. 2014. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014). Sunmin Lee, Sebastian Starke, Yuting Ye, Jungdam Won, and Alexander Winkler. 2023. Questenvsim: Environment-aware simulated motion tracking from sparse sensors. In ACM SIGGRAPH 2023 Conference Proceedings. 19. ACM Trans. Graph., Vol. 43, No. 6, Article 209. Publication date: December 2024. Yoonsang Lee, Sungeun Kim, and Jehee Lee. 2010a. Data-Driven Biped Control. ACM Trans. Graph. 29, 4, Article 129 (jul 2010), 8 pages. https://doi.org/10.1145/1778765. 1781155 Yoonsang Lee, Sungeun Kim, and Jehee Lee. 2010b. Data-driven biped control. ACM Trans. Graph. 29, 4, Article 129 (jul 2010), 8 pages. https://doi.org/10.1145/1778765. 1781155 Libin Liu, KangKang Yin, Michiel van de Panne, Tianjia Shao, and Weiwei Xu. 2010. Sampling-based Contact-rich Motion Control. ACM Transctions on Graphics 29, 4 (2010), Article 128. Zhengyi Luo, Jinkun Cao, Kris Kitani, Weipeng Xu, et al. 2023. Perpetual humanoid control for real-time simulated avatars. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 1089510904. Zhengyi Luo, Jinkun Cao, Josh Merel, Alexander Winkler, Jing Huang, Kris M. Kitani, and Weipeng Xu. 2024. Universal Humanoid Motion Representations for PhysicsBased Control. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=OrOd8PxOO2 Zhengyi Luo, Ryo Hachiuma, Ye Yuan, and Kris Kitani. 2021. Dynamics-regulated kinematic policy for egocentric pose estimation. Advances in Neural Information Processing Systems 34 (2021), 2501925032. Zhengyi Luo, Shun Iwase, Ye Yuan, and Kris Kitani. 2022a. Embodied scene-aware human pose estimation. Advances in Neural Information Processing Systems 35 (2022), 68156828. Zhengyi Luo, Ye Yuan, and Kris Kitani. 2022b. From Universal Humanoid Control to Automatic Physically Valid Character Creation. arXiv preprint arXiv:2206.09286 (2022). Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023. Eureka: Human-Level Reward Design via Coding Large Language Models. In The Twelfth International Conference on Learning Representations. Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Gerard Pons-Moll, and Michael J. Black. 2019. AMASS: Archive of Motion Capture as Surface Shapes. In International Conference on Computer Vision. 54425451. Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, and Gavriel State. 2021. Isaac Gym: High Performance GPU Based Physics Simulation For Robot Learning. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). https://openreview.net/forum? id=fgFBtYgJQX_ Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asynchronous methods for deep reinforcement learning. In International conference on machine learning. PMLR, 19281937. Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang, Gaofeng Meng, Jianlong Fu, Shiming Xiang, and Haibin Ling. 2022. Expanding language-image pretrained models for general video recognition. In European Conference on Computer Vision. Springer, 118. Liang Pan, Jingbo Wang, Buzhen Huang, Junyu Zhang, Haofan Wang, Xu Tang, and Yangang Wang. 2024. Synthesizing physically plausible human motions in 3d scenes. In 2024 International Conference on 3D Vision (3DV). IEEE, 14981507. Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel Van de Panne. 2018. Deepmimic: Example-guided deep reinforcement learning of physics-based character skills. ACM Transactions On Graphics (TOG) 37, 4 (2018), 114. Xue Bin Peng, Yunrong Guo, Lina Halper, Sergey Levine, and Sanja Fidler. 2022. ASE: Large-scale Reusable Adversarial Skill Embeddings for Physically Simulated Characters. ACM Trans. Graph. 41, 4, Article 94 (July 2022). Xue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine, and Angjoo Kanazawa. 2021. Amp: Adversarial motion priors for stylized physics-based character control. ACM Transactions on Graphics (TOG) 40, 4 (2021), 120. Mathis Petrovich, Michael J. Black, and GÃ¼l Varol. 2021. Action-Conditioned 3D Human Motion Synthesis with Transformer VAE. In International Conference on Computer Vision (ICCV). Abhinanda R. Punnakkal, Arjun Chandrasekaran, Nikos Athanasiou, Alejandra QuirosRamirez, and Michael J. Black. 2021. BABEL: Bodies, Action and Behavior with English Labels. In Proceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR). 722731. Davis Rempe, Zhengyi Luo, Xue Bin Peng, Ye Yuan, Kris Kitani, Karsten Kreis, Sanja Fidler, and Or Litany. 2023. Trace and Pace: Controllable Pedestrian Animation via Guided Trajectory Diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1375613766. StÃ©phane Ross, Geoffrey Gordon, and Drew Bagnell. 2011. reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings, 627635. John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. 2015. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438 (2015). MaskedMimic: Unified Physics-Based Character Control Through Masked Motion Inpainting 209:17 Marco Silva, Yeuhi Abe, and Jovan Popovic. 2008. Simulation of Human Motion Data using Short-Horizon Model-Predictive Control. Computer Graphics Forum 27 (04 2008), 371 380. https://doi.org/10.1111/j.1467-8659.2008.01134.x Chen Tessler, Yoni Kasten, Yunrong Guo, Shie Mannor, Gal Chechik, and Xue Bin Peng. 2023. Calm: Conditional adversarial latent models for directable virtual characters. In ACM SIGGRAPH 2023 Conference Proceedings. 19. Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and Amit Haim Bermano. 2023a. Human Motion Diffusion Model. In The Eleventh International Conference on Learning Representations. https://openreview.net/forum?id=SJ1kSyO2jwu Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and Amit Haim Bermano. 2023b. Human Motion Diffusion Model. In The Eleventh International Conference on Learning Representations. https://openreview.net/forum?id=SJ1kSyO2jwu Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2024b. Voyager: An Open-Ended Embodied Agent with Large Language Models. Transactions on Machine Learning Research (2024). https://openreview.net/forum?id=ehfRiF0R3a Jingbo Wang, Zhengyi Luo, Ye Yuan, Yixuan Li, and Bo Dai. 2024a. PACER+: OnDemand Pedestrian Animation Controller in Driving Scenarios. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 718728. Jingbo Wang, Yu Rong, Jingyuan Liu, Sijie Yan, Dahua Lin, and Bo Dai. 2022. Towards diverse and natural scene-aware 3d human motion synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2046020469. Jingbo Wang, Sijie Yan, Bo Dai, and Dahua Lin. 2021. Scene-aware generative network for human motion synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1220612215. Tingwu Wang, Yunrong Guo, Maria Shugrina, and Sanja Fidler. 2020. Unicon: Universal neural controller for physics-based character motion. arXiv preprint arXiv:2011.15119 (2020). Yinhuai Wang, Jing Lin, Ailing Zeng, Zhengyi Luo, Jian Zhang, and Lei Zhang. 2023. Physhoi: Physics-based imitation of dynamic human-object interaction. arXiv preprint arXiv:2312.04393 (2023). Alexander Winkler, Jungdam Won, and Yuting Ye. 2022. Questsim: Human motion tracking from sparse sensors with simulated avatars. In SIGGRAPH Asia 2022 Conference Papers. 18. Jungdam Won, Deepak Gopinath, and Jessica Hodgins. 2022. Physics-based character controllers using conditional VAEs. ACM Transactions on Graphics (TOG) 41, 4 (2022), 112. Zeqi Xiao, Tai Wang, Jingbo Wang, Jinkun Cao, Wenwei Zhang, Bo Dai, Dahua Lin, and Jiangmiao Pang. 2024. Unified Human-Scene Interaction via Prompted Chainof-Contacts. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=1vCnDyQkjg Yiming Xie, Varun Jampani, Lei Zhong, Deqing Sun, and Huaizu Jiang. 2024. OmniControl: Control Any Joint at Any Time for Human Motion Generation. In The Twelfth International Conference on Learning Representations. https://openreview. net/forum?id=gd0lAEtWso Sirui Xu, Zhengyuan Li, Yu-Xiong Wang, and Liang-Yan Gui. 2023. Interdiff: Generating 3d human-object interactions with physics-informed diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 1492814940. Dongseok Yang, Doyeon Kim, and Sung-Hee Lee. 2021. Lobstr: Real-time lower-body pose prediction from sparse upper-body tracking signals. In Computer Graphics Forum, Vol. 40. Wiley Online Library, 265275. Heyuan Yao, Zhenhua Song, Baoquan Chen, and Libin Liu. 2022. ControlVAE: ModelBased Learning of Generative Controllers for Physics-Based Characters. ACM Transactions on Graphics (TOG) 41, 6 (2022), 116. Ye Yuan and Kris Kitani. 2020. Residual force control for agile human behavior imitation and extended motion synthesis. Advances in Neural Information Processing Systems 33 (2020), 2176321774. Haotian Zhang, Ye Yuan, Viktor Makoviychuk, Yunrong Guo, Sanja Fidler, Xue Bin Peng, and Kayvon Fatahalian. 2023. Learning physically simulated tennis skills from broadcast videos. ACM Transactions On Graphics (TOG) 42, 4 (2023), 114. Kaifeng Zhao, Yan Zhang, Shaofei Wang, Thabo Beeler, and Siyu Tang. 2023. Synthesizing diverse human motions in 3d indoor scenes. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 1473814749. Xiaozheng Zheng, Zhuo Su, Chao Wen, Zhou Xue, and Xiaojie Jin. 2023. Realistic FullBody Tracking from Sparse Observations via Joint-Level Modeling. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 1467814688. Qingxu Zhu, He Zhang, Mengting Lan, and Lei Han. 2023. Neural Categorical Priors for Physics-Based Character Control. ACM Transactions on Graphics (TOG) 42, 6 (2023), 116. ACM Trans. Graph., Vol. 43, No. 6, Article 209. Publication date: December 2024. 209: Tessler, C. et al"
        },
        {
            "title": "A STATE AND ACTION SPACE",
            "content": "In this work, we consider 3D physically-simulated humanoid character based on the neutral SMPL body shape, with 69 degrees of freedom. similar character was used, amongst others, in UHC [Luo et al. 2022b], PACER [Rempe et al. 2023], and PHC [Luo et al. 2023]. To encode the state, we follow the same representation technique from Peng et al. [2021]. The agent observes: Root (characters pelvis) height. Root rotation with respect to the characters local coordinate frame. Local rotation of each joint. Local velocity of each joint. Positions of hands and feet, in the characters local coordinate frame. The characters local coordinate frame is defined with the origin located at the root, the x-axis oriented along the root links facing direction, and the y-axis aligned with the global up vector. The 3D rotation of each joint is encoded using two 3D vectors, corresponding to the tangent and the normal of the links local coordinate frame expressed in the link parents coordinate frame [Peng et al. 2021]. In total, this results in 358D state space. To control the character, the agent provides an action ğ‘ that represents the target rotations for PD controllers, which are positioned at each of the characters joints. Similar to Juravsky et al. [2022]; Peng et al. [2022, 2021]; Tessler et al. [2023], the target rotation for 3D spherical joints is encoded using 3D exponential map [Grassia 1998], resulting in 69D action space. MASKEDMIMIC PRIOR CONSTRUCTION Any-joint-any-time constraints: body part can be constrained on both position and rotation. Any joint can be constrained at any timestep during the trajectory. Each such constraint is represented using 12D vector, representing the constraint (position or rotation) with respect to the humanoids current local coordinate frame (6D) and with respect to the corresponding body part. E.g., the left-hand spatial constraint will be [ğ‘left hand Ë†ğ‘left hand , ğ‘left hand ]. ğœ ğœ Ë†ğ‘root ğ‘¡ ğ‘¡ The translation constraints are zero-padded from 3D to 6D whereas the rotation constraints are represented naturally in 6D. When target-pose is not provided (no constraints at all), we mask it from entering the transformer. Target objects: An object is represented by its bounding box (8 coordinates, 3D each), its direction (6D), and its category type (1D). Each coordinate, and the direction, are represented w.r.t. the characters local coordinate frame. Text: To represent the textual labels we opt for using XCLIP [Ni et al. 2022], CLIP-like model trained on video-language pairs. XCLIP embeds each sentence into 512-dim vector. Historical poses: We store the previous 40 poses, with 1 out of each 8 provided at each step, similar to SuperPADL [Juravsky et al. 2024]. Each pose is represented w.r.t. the current characters local coordinate frame. time entry is appended to each pose vector, representing how far back that pose is. When historical data does not exist (start of episode), we mask out the non-existent historical poses. When motion is initialized mid-sequence, we initialize the historical data using the historic kinematic poses. Heightmap: The heightmap surrounding the character is represented by 16x16 samples on square grid, rotated with the character. The points are spaced with 10cm gaps. The heightmap is flattened and encoded using fully connected encoder. Similar to the current character state, the heightmap is always provided. Architecture: An illustration of the prior architecture is provided in Figure 10. Each of the observations above is encoded using shared encoder for each entry type. For example, all historical poses are encoded using the same encoder, resulting in 5 tokens. Whereas the text embedding is encoded using different and unique encoder. Before encoding, each of these inputs is normalized using running-mean std normalizer, standard practice in reinforcement learning literature. Each of the encoders above is constructed as fully connected network with hidden size [256, 256]. The transformer utilizes latent dimension of 512 whereas the internal feed-forward size is 1024. We use 4 layers and 4 self-attention heads. We do not use dropout. All the obtained tokens above are fed into the transformer, alongside the current-pose encoding. We take the first output from the transformer and pass that through two MLP-heads (latent dim 256 128 64) to obtain the distribution ğ‘ (ğœ‡, ğœ). For consistency, during training and inference, we maintain fixed noise throughout an episode. Using the reparametrization trick: we re-sample ğœ– when an episode terminates. Encoder: The encoder is modeled as an MLP, with hidden layers [1024, 1024, 1024]. The encoder is provided the current pose, heightmap, unmasked future poses and the masks for the future poses. The encoder main network output size is 1024 which is then mapped to the posteriors mean and variance using an MLP with hidden-size 512. ğ‘§ğ‘¡ = ğœ‡ğ‘¡ + ğœğ‘¡ ğœ– , (9) ACM Trans. Graph., Vol. 43, No. 6, Article 209. Publication date: December 2024. MaskedMimic: Unified Physics-Based Character Control Through Masked Motion Inpainting 209:19 Fig. 10. Illustration of the prior architecture. Each modality shares an encoder across all inputs of that same modality. Goal-modalities can be masked out. Each entry is token, provided to transformer-encoder model. The output from the transformer is provided to two fully-connected heads, producing the mean and log-std of the latent distribution. Decoder: The decoder observes the current characters pose, the terrain and the sampled latent z. It is modeled as fully connected network [1024, 1024, 1024]. Its output is the action, the PD targets. The full-architecture is illustrated in Figure 5a. B.1 Training hyperparameters Each episode starts from randomly sampled state from the distribution dictated by the prioritized sampling mechanism. For each episode we sample unique noise, which is then kept fixed until the episode is reset. This noise is used for the latent sampling using the reparametrization trick. During the episode, the agent interacts with the environment for rollouts of 32 steps. MaskedMimic is then trained with respect to the collected rollout using A2C [Mnih et al. 2016] with ğ›¾ = 0.99. We utilize GAE [Schulman et al. 2015] with ğœ = 0.95. The actor learning rate is fixed at 2ğ‘’ 5, the critic at 1ğ‘’ 4. We use the Adam optimizer [Kingma and Ba 2014] with betas [0.9, 0.999]. The activations are selected as ReLU. In addition, the observations are normalized using the standard running mean-std normalization scheme. For the reward parameters we set: and ğ‘¤ gr = 0.3, ğ‘¤ gt = 0.5, ğ‘¤ jv = 0.1, ğ‘¤ jav = 0.1, ğ‘¤ rh = 0.2, ğ‘¤ enrg = 0.0005 , ğ‘gr = 2, ğ‘gt = 100, ğ‘jv = 0.5, ğ‘jav = 0.1, ğ‘rh = 100 . (10) (11) For MaskedMimic the KL coeff is scaled from 0.0001 to 0.01 across 6000 epochs, starting from epoch 3000. We parallelize training over 16,384 Isaac Gym [Makoviychuk et al. 2021] environments across 4 A100 GPUs, for total of 14 days. The policy takes decisions at rate of 30 Hz. The latent space is defined using 64D. When training the fully constrained controller, we use mini-batch size of 16,384 on each GPU. We found that A2C provides more stability and thus improved end-performance. To ensure on-policy learning, we perform gradient accumulation over the entire epoch. For MaskedMimic we opt for 8,192 and single epoch using supervised behavior cloning objective. As MaskedMimic is trained with supervised learning objective, do not perform gradient accumulation and allow it to perform more gradient steps. B.2 Masking We describe the masking logic below, with snippet of the code in Listing 1. ACM Trans. Graph., Vol. 43, No. 6, Article 209. Publication date: December 2024. 209:20 Tessler, C. et al Our model is trained with ğ¾ = 11 future poses. The first 10 are reserved for near-term constraints (the 10 immediate frames), whereas the 11th entry is reserved for random long-term pose. For any of the first 10 future-poses ğ‘ğ‘¡ , the sparsity pattern depends on the pattern preceding it. With probability 98% we apply the same pattern as before, whereas with probability 2% we sample random subset of observable joints and their constraint type (position and rotation). In addition, with probability 1% we sample time gap. The size of the gap is randomly sampled between 1 and 9 (including). time gap denotes sequence of poses that are completely masked out. This way the model encounters the challenge of inbetweening but is ensured to have at least 1 observable future pose. When there are \"long-term\" constraints, such as long-term target pose, text, or target-object, we multiply the sampled time-gap by 4. This allows periods in which the model generates motion without any near-term constraints. In each episode, if there is an object, there is 20% chance it will be masked out (hidden). For text, there is 80% chance it will be masked out, and long-term target-pose will be provided with 20% chance. 1 # conditionable_bodies_ids : Tensor for supported body indices . 2 # time_gap_mask_steps : Counter for each env for how remaining time - gap . 3 # motion_text_embeddings_mask , object_bounding_box_obs_mask , target_pose_obs_mask ( long - distance target ): Mask for observation type , per env . 4 5 # Returns : mask of size [ num_envs , len ( conditionable_bodies_ids ) * 2] , whether the joint is constrained or not , for both position and rotational constraints . 6 7 def sample_body_masks ( self , num_envs : int , env_ids : Tensor , new_episode : bool ): 8 new_mask = torch . zeros ( num_envs , self . conditionable_bodies_ids . shape [0] , 2, dtype = torch . bool , device = self . device ) no_time_gap_or_repeated = torch . ones ( num_envs , dtype = torch . bool , device = self . device ) 9 10 11 13 14 15 16 17 19 20 21 22 23 25 26 27 28 29 31 32 33 34 35 37 38 39 40 41 43 44 45 46 47 if not new_episode : # Reset the time gap mask if the time has expired remaining_time = self . time_gap_mask_steps . cur_max_steps [ env_ids ] - self . time_gap_mask_steps . steps [ env_ids ] # Mark envs with active time - gap active_time_gap = remaining_time > 0 no_time_gap_or_repeated [ active_time_gap ] = False # For those without , check if it should start one restart_timegap = ( remaining_time <= 0) & ( torch . rand ( num_envs , device = self . device ) < self . config . time_gap_probability ) self . time_gap_mask_steps . reset_steps ( env_ids [ restart_timegap ]) # If we have text or object conditioning or target pose , then allow longer time gaps text_mask = restart_timegap & self . motion_text_embeddings_mask [ env_ids ]. view ( -1) object_mask = restart_timegap & self . object_bounding_box_obs_mask [ env_ids ]. view ( -1) target_pose_obs_mask = restart_timegap & self . target_pose_obs_mask [ env_ids ]. view ( -1) allow_longer_time_gap = text_mask object_mask target_pose_obs_mask self . time_gap_mask_steps . cur_max_steps [ env_ids [ allow_longer_time_gap ]] *= 4 # Where there 's no time - gap , we can repeat the last mask repeat_mask = ( remaining_time < 0) & ( torch . rand ( num_envs , device = self . device ) < self . config . repeat_mask_probability ) single_step_mask_size = self . conditionable_bodies_ids . shape [0] * 2 new_mask [ repeat_mask ] = self . target_bodies_masks [ env_ids [ repeat_mask ] , - single_step_mask_size :]. view ( -1 , self . conditionable_bodies_ids . shape [0] , 2) no_time_gap_or_repeated [ repeat_mask ] = False # Compute number of active bodies for each env max_bodies_mask = torch . rand ( num_envs , device = self . device ) >= 0.1 max_bodies = torch . where ( max_bodies_mask , self . conditionable_bodies_ids . shape [0] , 3) random_floats = torch . rand ( num_envs , device = self . device ) scaled_floats = random_floats * ( max_bodies - 1) + 1 num_active_bodies = torch . round ( scaled_floats ). long () # Create tensor of [ num_envs , len ( self . config . conditionable_bodies )] with the max_bodies dim being arange active_body_ids = torch . zeros ( num_envs , self . conditionable_bodies_ids . shape [0] , device = self . device , dtype = torch . bool ) constraint_states = torch . randint (0 , 3, ( num_envs , self . conditionable_bodies_ids . shape [0]) , device = self . device ) for idx in range ( num_envs ): ACM Trans. Graph., Vol. 43, No. 6, Article 209. Publication date: December 2024. MaskedMimic: Unified Physics-Based Character Control Through Masked Motion Inpainting 209:21 # Sample the active body ids for each env active_body_ids [ idx , np . random . choice ( self . conditionable_bodies_ids . shape [0] , size = num_active_bodies [ idx ]. item () , replace = False )] = True translation_mask = ( constraint_states <= 1) & active_body_ids rotation_mask = ( constraint_states >= 1) & active_body_ids new_mask [ no_time_gap_or_repeated , :, 0] = translation_mask [ no_time_gap_or_repeated ] new_mask [ no_time_gap_or_repeated , :, 1] = rotation_mask [ no_time_gap_or_repeated ] return new_mask . view ( num_envs , -1) Listing 1. Masking code snippet 49 50 51 53 54 55 56 57 ACM Trans. Graph., Vol. 43, No. 6, Article 209. Publication date: December 2024."
        }
    ],
    "affiliations": [
        "Bar-Ilan University, Israel",
        "NVIDIA, Canada",
        "NVIDIA, Israel",
        "Simon Fraser University, Canada"
    ]
}