{
    "paper_title": "Zero-AVSR: Zero-Shot Audio-Visual Speech Recognition with LLMs by Learning Language-Agnostic Speech Representations",
    "authors": [
        "Jeong Hun Yeo",
        "Minsu Kim",
        "Chae Won Kim",
        "Stavros Petridis",
        "Yong Man Ro"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We explore a novel zero-shot Audio-Visual Speech Recognition (AVSR) framework, dubbed Zero-AVSR, which enables speech recognition in target languages without requiring any audio-visual speech data in those languages. Specifically, we introduce the Audio-Visual Speech Romanizer (AV-Romanizer), which learns language-agnostic speech representations by predicting Roman text. Then, by leveraging the strong multilingual modeling capabilities of Large Language Models (LLMs), we propose converting the predicted Roman text into language-specific graphemes, forming the proposed Cascaded Zero-AVSR. Taking it a step further, we explore a unified Zero-AVSR approach by directly integrating the audio-visual speech representations encoded by the AV-Romanizer into the LLM. This is achieved through finetuning the adapter and the LLM using our proposed multi-task learning scheme. To capture the wide spectrum of phonetic and linguistic diversity, we also introduce a Multilingual Audio-Visual Romanized Corpus (MARC) consisting of 2,916 hours of audio-visual speech data across 82 languages, along with transcriptions in both language-specific graphemes and Roman text. Extensive analysis and experiments confirm that the proposed Zero-AVSR framework has the potential to expand language support beyond the languages seen during the training of the AV-Romanizer."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 3 7 2 6 0 . 3 0 5 2 : r Zero-AVSR: Zero-Shot Audio-Visual Speech Recognition with LLMs by Learning Language-Agnostic Speech Representations Jeong Hun Yeo1,* Minsu Kim1, Chae Won Kim1 Stavros Petridis2 Yong Man Ro1, 2Imperial College London 1KAIST {sedne246, ms.k, ymro}@kaist.ac.kr"
        },
        {
            "title": "Abstract",
            "content": "We explore novel zero-shot Audio-Visual Speech Recognition (AVSR) framework, dubbed Zero-AVSR, which enables speech recognition in target languages without requiring any audio-visual speech data in those languages. Specifically, we introduce the Audio-Visual Speech Romanizer (AVRomanizer), which learns language-agnostic speech representations by predicting Roman text. Then, by leveraging the strong multilingual modeling capabilities of Large Language Models (LLMs), we propose converting the predicted Roman text into language-specific graphemes, forming the proposed Cascaded Zero-AVSR. Taking it step further, we explore unified Zero-AVSR approach by directly integrating the audio-visual speech representations encoded by the AV-Romanizer into the LLM. This is achieved through finetuning the adapter and the LLM using our proposed multitask learning scheme. To capture the wide spectrum of phonetic and linguistic diversity, we also introduce Multilingual Audio-Visual Romanized Corpus (MARC) consisting of 2,916 hours of audio-visual speech data across 82 languages, along with transcriptions in both language-specific graphemes and Roman text. Extensive analysis and experiments confirm that the proposed Zero-AVSR framework has the potential to expand language support beyond the languages seen during the training of the AV-Romanizer. The code and models are available online. 1. Introduction Humans rely on multi-modal information for effective communication, combining verbal cues (e.g., spoken words), nonverbal signals (e.g., facial expressions, gestures), and paralinguistic auditory cues (e.g., tone of voice) to convey meaning effectively. Notably, the correlation between auditory speech and visual speech (i.e., lip movements) can significantly enhance speech comprehension, particularly in noisy environments. Leveraging this advantage, numerous studies [113] have explored Audio-Visual Speech Recognition (AVSR) using deep learning techniques. AVSR can *Equal Contribution. Corresponding Author. be viewed as multi-modal fusion of two uni-modal systems; Auditory Speech Recognition (ASR) [1416] and Visual Speech Recognition (VSR) [1723]. Prior works have demonstrated that jointly modeling both audio and visual speech representations can significantly improve the speech recognition performance of each uni-modal system, particularly in challenging noisy environments. Despite significant advancements, AVSR has primarily been developed and evaluated on English language corpora. To extend its capabilities to wider range of languages, recent efforts have proposed multilingual audio-visual speech datasets [24, 25], and have developed and evaluated multilingual AVSR methods [2628] on such databases. However, despite these performance improvements, existing approaches still face limitations in language expansion. This is because current multilingual AVSR datasets [24, 25] contain only nine languages, and it remains challenging to obtain sufficient amount of labeled audio-visual data with transcriptions for diverse languages. To address this limitation, our main explorations include: 1) learning languageagnostic audio-visual speech representations, which facilitate easier language expansion, and 2) proposing new multilingual audio-visual database that encompasses 82 languages, significantly expanding the linguistic diversity of existing datasets. language recognition ability1, In this paper, we present novel AVSR method that i.e., exhibits zero-shot scenario in which no speech data in the target language is used at training. To achieve this, we aim to represent all languages in Roman text, converting languagespecific graphemes into language-agnostic pronunciations. Here, we note that pre-trained Large Language Models (LLMs) [2931] already possess knowledge of modeling this Roman-grapheme mapping, and propose to leverage 1The zero-shot capability defined in this paper is limited to the speech modeling component. We assume that text data is available for all languages, whereas speech data is not accessible for some of them. Therefore, seen language refers to language that has been encountered during speech encoder training, whereas an unseen language refers to language that is only available through text data, without any corresponding speech. 1 this comprehensive multilingual processing ability of LLMs in our proposed zero-shot AVSR framework. Concretely, we propose an Audio-Visual Speech Romanizer (AVRomanizer), which predicts Roman text (i.e., languageagnostic) from input multilingual audio-visual speech data. We then demonstrate that the proposed AV-Romanizer can be directly employed to achieve zero-shot AVSR by converting the predicted Roman text into language-specific graphemes using pre-trained LLM, even when the target language is not used during the training of the AVRomanizer. We refer to this usage case as Cascaded ZeroAVSR, where the proposed AV-Romanizer and pre-trained LLM form cascaded system. Going one step further, we explore model that holds considerable promise, namely Zero-AVSR, where the LLM is also fine-tuned to match the specific use case for zeroshot recognition, thereby improving performance. Specifically, Zero-AVSR is trained in multi-task fashion composed of two tasks. The first task involves aligning the AV-Romanizer and LLM by fine-tuning an adapter and the LLM on seen languages, enabling the audio-visual speech representation obtained from the AV-Romanizer to be seamlessly embedded into the learned text space of the LLM. The second task focuses on learning to deromanize, where the LLM is trained to convert Roman texts into language-specific graphemes using only text data for both seen and unseen languages. Therefore, Zero-AVSR constructs knowledge connecting the audio-visual speech representation with the LLMs embedding space of seen languages at the first task, while extending the learned knowledge to more languages, including unseen languages, at the second task. In order to train the proposed Zero-AVSR, sufficient amount of audio-visual speech data is essential to capture the full spectrum of phonetic and linguistic diversity. In this context, we introduce Multilingual Audio-Visual Romanized Corpus (MARC), which provides Roman transcriptions for approximately 2,916 hours of audio-visual data across 82 languages, along with their corresponding transcriptions in language-specific graphemes. The contributions of this paper can be summarized as: We explore Zero-AVSR, the first zero-shot AVSR framework, designed to operate in scenarios where no speech training data is available for the target language. We propose the MARC dataset, which comprises Roman transcriptions for 2,916 hours of audio-visual speech data across 82 languages. We explore two types of zero-shot AVSR frameworks, one of which is Cascaded Zero-AVSR, framework that can be used with any type of LLM without fine-tuning, even in API form. The other is Zero-AVSR, an improved framework by finetuning the LLM with the proposed multi-task learning. 2. Related Work 2.1. Audio-Visual Speech Recognition Recently, AVSR has gained significant attention for its practical benefits, particularly in enhancing robust speech recognition in noisy environments. Along with the development of large-scale audio-visual datasets [32, 33], early work [2] introduced end-to-end AVSR frameworks based on Bidirectional Gated Recurrent Units (BGRUs). Subsequently, researchers improved these architectures by incorporating Transformer [1, 34] and Conformer [4, 35], leading to notable performance gains. Concurrently, other researchers have explored multimodal learning strategies, including self-supervised learning [6, 18, 27], leveraging knowledge from pre-trained ASR models in AVSR [9, 12], and harnessing the context-modeling capabilities of LLMs for speech recognition [11, 21]. Despite these remarkable advances, most existing AVSR research has primarily focused on English. To address this gap, some studies have begun exploring the effectiveness of AVSR in multilingual contexts, leveraging newly introduced multilingual audio-visual databases [24, 25] spanning nine languages. Building on the progress made in English-based AVSR, recent multilingual AVSR approaches [27, 28] have expanded its progress into multilingual AVSR models. In particular, because obtaining labeled multilingual audio-visual data is challenging, selfsupervised learning [27] has shown notable promise for improving performance by leveraging abundant unlabeled multilingual audio-visual data. While these efforts have successfully extended AVSRs effectiveness to nine languages, it remains challenging to expand its impact to additional languages. Unlike previous work, which mainly focused on improving performance on publicly available multilingual databases, we explore language expansion in multilingual AVSR by constructing zero-shot AVSR framework. The proposed framework can recognize speech in the target language without requiring any speech training data for that specific language. 2.2. Zero-Shot Speech Recognition Recent research in speech recognition has advanced toward supporting multiple languages by employing effective methods such as self-supervised learning, which has been validated on English ASR. This rapid progress has been made possible by the use of large-scale multilingual audio datasets and carefully designed training methods [37, 38]. However, obtaining sufficient amount of labeled data with transcriptions for all languages remains challenge. To overcome this limitation, researchers have begun to explore zero-shot speech recognition. Early works [39 41] proposed unsupervised ASR methods leveraging both unlabeled audio and text data. more efficient ap-"
        },
        {
            "title": "Romanizer",
            "content": "De-romanizer Target Language (CER)"
        },
        {
            "title": "Rus",
            "content": "Avg (w/o Eng) Llama3.1-70B [29] Llama3.1-70B [29] Uroman [36] Uroman [36] GPT-4o-mini [30] GPT-4o-mini [30] Llama3.1-70B [29] GPT-4o-mini [30] Llama3.1-70B [29] GPT-4o-mini [30] Llama3.1-70B [29] GPT-4o-mini [30] 34.9 24.6 15.0 20.0 11.8 13.1 8.4 3.1 4.1 1.7 3.6 1.9 16.9 10.6 5.8 8.6 5.5 7. 12.3 4.0 9.8 1.0 12.4 0.7 14.0 2.8 4.2 2.2 5.5 2.5 10.0 1.8 4.9 1.3 2.9 1.2 11.1 6.6 4.4 2.0 3.1 1.9 10.9 6.1 14.8 8.6 7.4 3.9 13.8 7.2 7.4 4.8 6.0 4. Table 1. Reconstruction test results on MuAViC to evaluate the effectiveness of different methods in romanization and de-romanization. proach that requires only unlabeled text was proposed by [42]. This method utilizes language-agnostic allophones, which are subsequently converted into language-specific phonemes [43]. Then, by using grapheme-to-phoneme (G2P) system [44], they generate words from phoneme sequences. More recently, [45] demonstrates that using romanized form instead of phonemes offers simpler yet effective alternative. Building on these findings, we also aim to develop zero-shot AVSR framework by employing romanized text. Unlike [45], which relies on language-specific language models to decode Roman text into language-specific graphemes, we demonstrate that pre-trained Large Language Model (LLM) can effectively perform this task, thereby eliminating the need for training multiple languagespecific language models. 2.3. Speech Large Language Model LLMs have significantly impacted Natural Language Processing (NLP), as demonstrated by their ability to perform wide range of tasks [46]. Building on this success, recent research efforts have begun exploring their effectiveness in other modalities [4749]. In particular, integrating speech with LLMs has effectively leveraged these models language understanding capabilities, leading to substantial improvements in the speech domain. To achieve this, simple and effective adaptation methods such as LoRA [50] and window-level Q-former [51], have been proposed to align audio-based speech with LLMs. Concurrently, these approaches have expanded the range of applications, enabling multi-task capabilities [51, 52] and multi-modal processing [11]. Similar to these research trends, we also explore unified Zero-AVSR framework that fine-tunes pre-trained LLM using QLoRA [53], enabling it to directly accept and process encoded audio-visual speech features. 3. Method We propose novel multilingual AVSR framework, ZeroAVSR, that enables zero-shot audio-visual speech recognition even though speech training data for the target language is unavailable. Specifically, we propose the AV-Romanizer, which predicts language-agnostic speech representations (i.e., Roman) from input audio-visual speech. Subsequently, we propose to leverage LLMs to convert these representations back into language-specific graphemes. 3.1. Multilingual Audio-Visual Romanized Corpus While it is known that different languages share common pronunciation features at the phoneme level [5456], the current publicly available AVSR datasets [25] may be insufficient for representing the phonemic diversity of diverse languages and for developing language-agnostic audiovisual speech representations. To address this limitation, we propose the Multilingual Audio-Visual Romanized Corpus (MARC). MARC is driven by mixing existing audiovisual corpora, LRS3 [32] and MuAViC [25], and unlabeled audio-visual datasets, VoxCeleb2 [57] and AVSpeech [58]. For the unlabeled datasets, we employ pre-trained language identification and ASR models [38] to obtain the language ID and the language-specific graphemes for each data point, similar to [9, 22, 28]. During language identification, as the pre-trained model may misclassify the language, we filter out annotations with prediction probabilities below predetermined threshold to reduce the errors. Finally, we convert the transcriptions of the above datasets into Roman text. Prior to this, we evaluate which method is most suitable for romanizing transcriptions between using romanization tool [36] and LLMs. To this end, we perform reconstruction test, where the ground-truth transcriptions are transformed into Roman text by using either romanization tool or one of the LLMs, and then they are de-romanized into the original transcriptions. However, as romanization is not fully reversible, the de-romanization process is non-trivial. One may construct language-specific lexicons to de-romanize Roman text [45], but this approach cannot fully capture the complex few-to-many mapping of deromanization. Instead, we employ LLMs as de-romanizer and find that they already possess the capability to transform between Roman and graphemes freely. Table 1 shows the reconstruction test results using different methods as romanizer and de-romanizer on MuAViC dataset. Through the test, we found that GPT-4o-mini [30] achieves the best performance when it is utilized as both romanizer and deromanizer. Therefore, we romanize the transcriptions of the proposed MARC with GPT-4o-mini. Figure 1. Illustration of (a) Audio-Visual Speech Romanizer (AV-Romanizer): It is pre-trained to learn language-agnostic representations by predicting romanized text through CTC loss. (b) Cascaded Zero-shot AVSR: By providing instructions along with the predicted Roman text from the proposed AV-Romanizer, diverse LLMs can be employed to predict graphemes from the predicted Roman text. Please note that, as the AV-Romanizer has learned to generate pronunciation information (Roman text) from the input speech, it can convert even unseen languages into Roman text. Since LLMs already contain information about the target language (key assumption of this paper), they can then convert this Roman text into the target language. The resulting dataset, MARC, is composed of 82 languages, and includes 2,916 hours of audio-visual data, and text transcriptions in both language-specific graphemes and Roman text. Detailed information about the dataset can be found in Sec. 6. 3.2. Audio-Visual Speech Romanizer If we train an acoustic model to predict the pronunciation of input speech instead of predicting language-specific graphemes, we can employ fixed number of characters for diverse languages and potentially represent languages even not used during training. Previous approaches [4244] have often relied on mapping allophones to phonemes. While these approaches can model language-agnostic representations, recent work [45] demonstrates that using romanized form offers simpler yet effective alternative. We also adopt Roman text as our language-agnostic representation. The proposed Audio-Visual Speech Romanizer (AVRomanizer) predicts Roman text for capturing pronunciation as it sounds from the input audio-visual speech, reIt comprises gardless of language, as shown in Fig. 1a. four main components: an audio encoder Fa, visual encoder Fv, transformer B, and linear layer for predicting Roman text. Given the training sample (xa, xv, y), where xa represents the log-filterbank features extracted from the input audio, xv denotes the video capturing lip movements, and is the target Roman text, we first encode the audio-visual speech features. Specifically, the audio features fa = Fa(xa) RT and the visual features fv = Fv(xv) RT are extracted using the audio and visual encoders, respectively. Note that the lengths of audio and visual features are synchronized through pre-processing and strided convolution. Next, we concatenate the audio and visual features along the channel dimension and then these combined features are fed into transformer encoder to encode the audio-visual speech features fav. This process can be formulated as follows: fav = B((fa fv)W ), where denotes concatenation along the channel dimension, and R2DD is weight matrix of linear layer applied to preserve the original dimensionality. Finally, linear layer is applied to predict the output roman tokens ˆy. The proposed AV-Romanizer is trained with Connectionist Temporal Classification (CTC) [59] objective function. After training on sufficient data that covers diverse range of phones, the AV-Romanizer can predict Roman text by capturing how input audio-visual speech is pronounced, even though the language was not used during training. 3.3. Cascaded Zero-shot AVSR the zero-shot If we can transform the Roman text into language-specific graphemes, then the speech recognition process is complete. To construct the complete AVSR pipeline, we cascade the AV-Romanizer and an LLM. The AV-Romanizer transcribes input audio-visual speech into Roman text. Then, the predicted Roman text is de-romanized by pre-trained LLM. Here, language recognition ability arises. Even though language has not been used to train the AV-Romanizer, it can still predict how the input speech sounds like, in Roman text. By taking the predicted text, pre-trained LLM can convert the text into the target language, which is within its knowledge. We named this cascaded form as Cascaded Zero-AVSR and depict in Fig. 1b. Specifically, an instruction to convert the Roman text into the target language is incorporated along with the pre4 Figure 2. Illustration of the proposed multi-tasks to train Zero-AVSR. (a) Task 1: Alignment between the AV Romanizer and LLM. By using the paired audio-visual speech inputs and ground truth roman text, we align the audio-visual speech representations into LLM space (i.e., text). (b) Task 2: Learning to de-romanize. By leveraging abundant text-only data, we fine-tune the LLM to de-romanize input roman text into language-specific graphemes. This approach enables us to cover wider range of languages, as text data is more readily available than audio-visual speech data for diverse languages. dicted Roman text from the AV-Romanizer. This text is then used as input for the LLMs as shown in Fig. 1b. Since this approach does not require finetuning the LLMs and the input is purely text-based, diverse LLMs can be employed for Cascaded Zero-AVSR, even in API form. 3.4. Zero-shot AVSR with Multi-task Training Although we have seen the potential of our proposed AVRomanizer to interact with LLMs at the text level as cascaded system, recent works [51, 52] have demonstrated that systems that directly integrate speech representations with LLMs can achieve optimal performance. Motivated by this, we explore an unified model, namely Zero-AVSR, which directly integrates audio-visual speech representations with LLMs instead of using the predicted Roman text itself. To this end, Zero-AVSR is trained on two tasks to enable zeroshot language recognition scenarios. The first task aims to align the audio-visual speech features encoded by the AVRomanizer with the text embeddings of LLMs. The second task involves learning to de-romanize languages, encompassing both seen and unseen languages. 3.4.1. Task 1: Aligning the AV-Romanizer with LLMs In this task, our goal is to align the audio-visual speech representations obtained from the AV-Romanizer with the text embeddings of LLMs. Since audio-visual speech has higher temporal resolution compared to text and thereby contains redundant information, length compressor is typically employed to reduce the computational burden, especially when incorporating with LLMs [11, 21, 60, 61]. As shown in Fig. 2a, given the audio-visual features fav extracted at the penultimate layer before the classification head of the AV-Romanizer, we first apply length compressor to halve the length of the features. Then, an adapter maps the compressed audio-visual speech features into the LLMs embedding space. Finally, the embedded audiovisual speech features are concatenated with the text embedding of the instruction to form the input to the LLM. By setting the target as language-specific graphemes, the model is trained using typical language modeling objective. Here, the pre-trained weights of the LLM including token embeddings and AV-Romanizer are kept frozen, while only the LoRA weights at the LLMs, length compressor, and the adapter are finetuned. As the task 1 requires audiovisual speech data, it can only be performed using seen languages. The knowledge for unseen languages will be extended in the task 2. 3.4.2. Task 2: Learning to De-romanize In this task, our goal is to train LLMs to de-romanize diverse languages. As the task 1 is performed using only seen languages, task 2 is essential to prevent LLMs from forgetting their multilingual ability which is the key for zero-shot language recognition. Specifically, as shown in Fig. 2b, the input is set to purely text, consisting of the instruction and the Roman text of all languages, including both seen and unseen languages. The target output is then set to languagespecific graphemes, constructing transformation task from Roman text to language-specific graphemes. During task 2 training, only the LoRA weights of the LLM are finetuned. 4. Experimental Setup 4.1. Dataset Multilingual Audio-visual Romanized Corpus (MARC) is the proposed dataset designed for zero-shot audio-visual speech recognition by labeling and integrating data from"
        },
        {
            "title": "Labeled",
            "content": "Support # Langs Target Language (WER(%))"
        },
        {
            "title": "Por Rus Eng",
            "content": "Non-Zero Shot Multilingual AVSR Models AV-HuBERT [6] u-HuBERT [62] XLAVS-R 300M [27] XLAVS-R 2B [27] MMS Zero-shot [45] Cascaded Zero-AVSR Zero-AVSR"
        },
        {
            "title": "AVSR",
            "content": "1,759* 1,759*(452*) 1,200(436K) 1,200(436K) 1,200 1, 1,200 1,200 9 9 9 89.4 52.0 46.2 17.4 20.3 20.8 22.1 44.7 89.3 52.1 46.4 17.3 20.5 21.2 21.9 44.4 81.7 44.7 24.3 10.9 14.4 12.8 13.2 32.7 79.3 44.4 19.0 9.1 12.3 10.6 11.2 25. 1.7 1.9 2.4 1.7 Zero-Shot Multilingual ASR/AVSR Models (436K) 1,759* 1,759* (40K) 1,078+ 84.9 31.5 47.9 17.7 33.6 19.0 35.5 42.8 35.7 2, 2,916 82+ 82+ 82.1 29.3 47.2 16.3 28.9 21.6 20.2 42.9 81.4 27.8 38.4 13.1 14.3 15.9 15.4 32.6 2. 1.5 Avg (w/ Eng) 35.0 35.0 26.3 23. 38.9 30.2 25.2 Table 2. Comparisons with state-of-the-art methods on MuAViC. Note that an asterisk (*) denotes the use of English-only data, and values in parentheses indicate the amount of audio-only data employed. four existing datasets: LRS3 [32], MuAViC [25], VoxCeleb2 [57], and AVSpeech [58]. Details for each source dataset and MARC can be found in Sec. 6. 4.2. Implementation details Pre-processing. We resample all video and audio to 25 fps and 16 kHz, respectively. We extract facial landmarks using the ReinaFace detector [9] and crop the mouth region to size of 96 96. For text data, we apply fairseqs text normalization. Finally, for data augmentation, we perform random cropping into size of 88 88 and horizontal flipping following [4, 6] during all training processes. Architecture. For the AV-Romanizer, we adopt the AVHuBERT [6] architecture, which comprises visual encoder, an audio encoder, an audio-visual fusion module, and transformer encoder. Additionally, we employ linear projection layer for predicting Roman text. The visual encoder uses ResNet-18 with 3D convolution layer, while the audio encoder consists of single linear layer. The transformer encoder, composed of 24 layers, features model dimension of 1024, feed-forward dimension of 4096, and 16 attention heads. For the length compressor, 1D convolution with kernel size of 2 and stride of 2 is utilized. For Zero-AVSR, we employ Llama3.2-3B [29] as the decoder and finetune it using QLoRA [50, 53], while GPT-4o-mini [30] is employed for Cascaded Zero-AVSR. Training and Evaluation. For training the AV-Romanizer, we employ three-stage scheduler with 10K warmup steps, 40K hold steps, and 50K decay steps, along with peak learning rate of 1e-4. Training is performed on 8 RTX 3090 GPUs with gradient accumulation set to 8. Also, the audio is randomly perturbed with acoustic noise sampled from MUSAN [63] with 0 dB SNR. For training the Zero-AVSR with an LLM, cosine scheduler is employed with 0.5K warmup steps and 29.5K decay steps, using 7 A6000 GPUs with gradient accumulation set to 9. After training ZeroAVSR, we evaluate its performance using beam search with width of 2 and temperature of 0.3. 5. Experimental Results 5.1. Comparison with the state-of-the-art methods Although our primary goal is language expansion, it is also crucial to ensure that the proposed methods perform on seen languages at level comparable to state-of-theart approaches for real-world applications. To validate this, we first evaluate the effectiveness of our proposed methods, Cascaded Zero-AVSR and Zero-AVSR, which are trained using audio-visual data from all languages in MARC. Table 2 summarizes the performance of our methods compared to several state-of-the-art approaches on the MuAViC [25] dataset. We organize the comparison into two groups: (i) multilingual audio-visual speech recognition models that support fixed set of nine languages, and (ii) methods that operate using Roman characters and cover wider range of languages. Among the multilingual models (Non-zero shot), both AV-HuBERT [6] and u-HuBERT [62] achieve an average WER of 35.0% across nine target languages. In contrast, XLAVS-R [27] models highlight the benefits of increased model capacity and the use of unlabeled multilingual audiovisual data, along with large-scale audio-only data (436K hours) during pre-training. Their 300M and 2B variants achieve average WERs of 26.3% and 23.6%, respectively. We observe high WERs on Arabic (Ara) for all methods. The audio-only zero-shot model, MMS Zero-shot [45], leverages 436K hours of unlabeled audio data for pretraining and then fine-tunes on 40K hours of labeled data spanning 1,078 languages. As result, it supports over 1,078 languages but exhibits relatively high average WER of 38.9%. By learning language-agnostic audio-visual speech representations and integrating the multilingual language understanding capabilities of LLMs, the proposed methods, Cascaded Zero-AVSR and Zero-AVSR, outperform the previous approach, achieving average WERs of 30.2% and 25.2%, respectively. Compared to existing multilingual AVSR approaches that support only 9 languages, the proposed methods significantly expand AVSR capabilities to broader range of languages by training on the 6 Avg (w/o Eng)"
        },
        {
            "title": "Train Dataset",
            "content": "# Train Language # Train Hours"
        },
        {
            "title": "Unseen\nLanguage",
            "content": "CER(%) Zero-shot Avg (w/o Eng)"
        },
        {
            "title": "Method",
            "content": "Unseen Lang. Cascaded Zero-AVSR (GPT-4o-mini)"
        },
        {
            "title": "Ara",
            "content": "Target Language (CER(%))"
        },
        {
            "title": "Ita Por Rus",
            "content": "67.6 17.1 23.2 8.3 14.3 10.2 10.0 20.6 53.7 46.9 22.3 8.2 13.7 9.7 9.8 20.2 55.6 17.0 45.3 8.1 14.5 10.0 9.6 21.2 54.8 17.2 22.5 19.0 14.3 10.3 10.1 20. 55.2 17.2 22.0 8.4 44.6 9.9 10.0 20.6 54.2 17.5 22.5 8.3 13.7 23.9 10.0 20.9 54.4 17.1 22.6 8.3 14.2 10.1 37.7 21.0 55.2 17.1 22.6 8.4 14.0 10.1 9.9 40.0 76.5 16.2 21.6 6."
        },
        {
            "title": "Deu",
            "content": "56.9 52.9 22.4 7.4 53.9 16.1 62.1 6.8 57.8 16.7 24.4 19.7 8.5 55.3 16.0 21.2 6.9 54.6 7.4 8. 8.1 7.0 6.9 6.8 7.5 7. 7.7 18.5 7.7 18.7 7.8 18.4 8.6 20.2 7.9 18.1 61.6 17.2 22.6 7. 58.0 16.3 24.7 7.5 57.7 16.0 22.7 7.1 8.1 7.7 7.7 25.1 7.7 18. 7.4 44.0 18.8 6.9 7.6 45.4 Zero-AVSR (Llama3.2-3B)"
        },
        {
            "title": "Rus",
            "content": "20.8 25.8 23.5 20.4 21.4 20. 22.9 21.9 19.7 26.4 24.3 19. 20.7 20.7 22.3 21.5 Table 3. The zero-shot language AVSR performances of Cascaded Zero-AVSR and Zero-AVSR on MuAViC. We train 8 AVRomanizers, setting each language as an unseen language, and evaluate their zero-shot performance, shown in blue-colored cells. proposed MARC, while achieving the comparable performances. In addition, Zero-AVSR achieves state-of-the-art performances in English (Eng) and German (Deu), with WERs of 1.5% and 27.8%, respectively. 5.2. Effectiveness on Unseen Languages To validate the effectiveness of the proposed methods on unseen languages, we train eight different models for each of Cascaded Zero-AVSR and Zero-AVSR. In each experiment, we train the model by excluding the audio-visual data of the target unseen language. For example, if Spanish (Spa) is the target unseen language, the AV-Romanizer is trained without using any Spanish data and then evaluated on 8 languages, including both seen and unseen languages. Given the dominant status of English, we exclude its results from our analysis to focus on other relatively low-resource languages, allowing us to better understand the effectiveness of Zero-AVSR in zero-shot speech recognition. For the Cascaded Zero-AVSR, we first predict the Roman text by using the AV-Romanizer and then de-romanize it using GPT-4o-mini [30]. The zero-shot AVSR performances (CER) of Cascaded Zero-AVSR for Arabic (Ara), German (Deu), Greek (Ell), Spanish (Spa), French (Fra), Italian (Ita), Portuguese (Por), and Russian (Rus) are 67.6%, 46.9%, 45.3%, 19.0%, 44.6%, 23.9%, 37.7%, and 40.0%, respectively. This indicates that even without training on any speech data, the Cascaded Zero-AVSR model can still predict transcriptions in unseen languages. For instance, the Cascaded Zero-AVSR can achieve 62.3% character-level accuracy when predicting Portuguese (Por) speech. For the Zero-AVSR, we employ Llama3.2-3B [29], state-of-the-art open-source LLM, as the decoder. The zero7 MuAViC [25] + MARC MuAViC [25] + MARC MuAViC [25] + MARC 8 8 40 8 8 40 81 8 8 40 81 745 1,944 2,418 2,793 724 1,921 2,395 2,770 698 1,851 2,326 2,"
        },
        {
            "title": "Spa\nSpa\nSpa\nSpa",
            "content": "62.3 61.0 49.5 40.0 34.8 28.5 26.0 23.9 35.5 29.9 20.5 19.0 48.3 28.3 25.1 21.9 49.0 25.9 23.7 20.9 46.3 25.4 22.6 20. Table 4. Impact of the proposed MARC on Cascaded Zero-AVSR, evaluated by varying data amount and number of languages. shot AVSR performances (CER) for the eight languages are as follows: Ara 76.5%, Deu 52.9%, Ell 62.1%, Spa 19.7%, Fra 54.6%, Ita 25.1%, Por 44.0%, and Rus 45.4%. Although the Cascaded Zero-AVSR and Zero-AVSR models are not directly comparable due to their different LLMs, we can confirm that incorporating speech features into the LLM and finetuning the model (i.e., Zero-AVSR) significantly improves performance on seen languages. Furthermore, zero-shot AVSR performance is also substantially improved when compared to the Cascaded Zero-AVSR using the same LLM, Llama3.2-3B, whose results can be found in Sec. 5.3.4. This result highlights the potential of the ZeroAVSR; If better LLM is employed, its performance can be significantly enhanced. 5.3. Ablation study 5.3.1. Effectiveness of the MARC Dataset To validate the effectiveness of the proposed MARC dataset, we conducted gradual data addition experiment starting with the MuAViC portion of the MARC dataset. Specifically, we conduct four experiments for each of three languages, Russian (Rus), Italian (Ita), and Spanish (Spa), totaling 12 experiments. We measure the performance of the Cascaded Zero-AVSR and each language is treated as target unseen language. The results are shown in Table 4. First, we train the AV-Romanizer using only MuAViC portion of the MARC dataset, and achieve average CERs (i.e., including both unseen and seen languages) of 48.3%, 49.0%, and 46.3% for Rus, Ita, and Spa, respectively. Next, by incorporating the remaining audio-visual data of the MARC with the same 8 languages in the MuAViC portion, we achieve significantly improved average CERs of 28.3%, 25.9%, and 25.4% for Rus, Ita, and Spa, respectively. The results demonstrate the effectiveness of using MARC dataset and show that incorporating more data is beneficial for improving AVSR performance. Then, to assess the impact of language diversity on"
        },
        {
            "title": "Method",
            "content": "Target Language (CER(%))"
        },
        {
            "title": "Por Rus",
            "content": "Avg (w/o Eng)"
        },
        {
            "title": "Train Dataset",
            "content": "# Train Language"
        },
        {
            "title": "Unseen\nLanguages",
            "content": "CER(%)"
        },
        {
            "title": "Ita Fra Por",
            "content": "Llama3.2-3B [29] 69.3 24.2 56.6 13.9 27.9 17.5 14.9 56.5 Mistral-7B [31] 71.2 26.7 32.8 13.5 23.9 16.2 17.7 36.5 Llama3.1-8B [29] 61.4 21.7 39.4 14.8 20.4 14.1 14.4 30. Llama3.1-70B [29] 56.1 18.1 26.7 GPT-4o-mini [30] 54.0 17.3 22.5 9.5 8.1 15.6 11.1 11.1 23. 14.2 10.4 9.9 21.2 35.7 29.6 27. 21.3 19.5 Table 5. Performances of Cascaded Zero-AVSR using different types of LLMs. The AV-Romanizer is trained on all 82 languages. modeling language-agnostic audio-visual speech representations, we train the AV-Romanizer using audio-visual speech data from 40 and 81 languages in the MARC dataset. That means the model is trained on more than just the 8 evaluation languages. The results show that incorporating data from 41 languages improves the performance for both unseen (i.e., zero-shot) and seen languages. Especially, the zero-shot performance for Russian is greatly improved from 61.0% CER to 49.5% CER. This suggests that incorporating wider range of diverse phonetic information through the use of multiple languages can significantly enhance zero-shot speech recognition performance. Similarly, employing total of 81 languages further improves both the zero-shot performance and the average performance. Again, this result confirms the importance of employing diverse languages and sufficient phonetic coverage in learning language-agnostic speech representations. 5.3.2. Ablation Study using Different LLM Models As the Cascaded Zero-AVSR does not finetune the LLM, its performance is heavily dependent on the choice of LLM used, as expected. To investigate the performance of Cascaded Zero-AVSR according to different types of LLMs, we perform ablation study by employing 5 different LLMs. The AV-Romanizer is trained on all 82 languages. Table 5 shows the ablation results. We can confirm that employing larger model, which has better ability to transform between Roman and language-specific graphemes, yields better performance. Notably, GPT-4o-mini [30] achieves the best performance among the baselines. Therefore, we employ GPT-4o-mini as the de-romanizer for other Cascaded Zero-AVSR experiments. Please note that we employ Llama3.2-3B [29] for training Zero-AVSR as it has feasible number of parameters. 5.3.3. Ablation Study on the Impact of Language Family Languages within the same language family [64] often exhibit similar grammatical structures and pronunciation patterns. Consequently, it is reasonable to expect that incorporating more data from languages within the same group as the target unseen language will enhance zero-shot speech recognition performance. To evaluate this hypothesis, we conduct ablation studies starting with baseline model trained without data from the Romance family, the Turkic"
        },
        {
            "title": "Baseline",
            "content": "67 Spa, Ita, Fra, Por 40.1 34.2 55.8 48.0 Adding Data from Different Language Family + Turkish (51 hrs) + Korean (129 hrs) 69 Spa, Ita, Fra, Por 38.1 33.6 61.0 48.2 Spa, Ita, Fra, Por 38.4 33.6 59.7 49.5 Adding Data from Romance Subgroup + Spanish (51 hrs) + Italian (129 hrs) 68 69 Ita, Fra, Por 11.5 28.1 56.6 44.0 Fra, Por 11.0 11.1 51.3 41. Table 6. Ablation study on the impact of using data from the same language family (Romance) on zero-shot speech recognition. family, and Korean. We then incrementally add data to the baseline model and build two variants: one incorporating Spanish and Italian (Romance languages) and another adding Turkish and Korean (non-Romance languages), ensuring that the same amount of data is added to each. By comparing these two variants, we can assess the impact of linguistic similarity on zero-shot speech recognition performance. We report and analyze the zero-shot AVSR performances on Romance languages, including Spanish (Spa), Italian (Ita), French (Fra), and Portuguese (Por). The ablation results are shown in Table 6. The first row shows the zero-shot AVSR performance of the baseline model. By gradually adding non-Romance languages, Turkish and Korean sequentially, the zero-shot performances are slightly improved for Spa and Ita, while there is no improvement for Fra and Por. In contrast, we can confirm that the zero-shot speech recognition performances for Romance languages are greatly improved by adding Spa and Ita, compared to adding the non-Romance languages. When Spa is added, the Ita and Por performances are improved to 28.1% and 44.0% CERs, respectively, while the performance of Fra remains unchanged. Furthermore, adding Italian data on top of the Spa added model results in an additional improvement, ultimately achieving 51.3% and 41.9% CERs for Fra and Por. These findings provide evidence that incorporating languages similar to the target language is more effective in zero-shot speech recognition. 5.3.4. The Effectiveness of Zero-AVSR In order to confirm the effectiveness of the Zero-AVSR compared to the Cascaded Zero-AVSR, we also report the zero-shot language AVSR performance of Cascaded ZeroAVSR using Llama3.2-3B, in Table 7. Therefore, since Cascaded Zero-AVSR and Zero-AVSR utilize the same LLM here, we can evaluate the effectiveness of finetuning the LLM by employing the proposed multi-task framework. By comparing the zero-shot speech recognition performances (i.e., shown in blue-colored cells), we can confirm that Zero-AVSR improves performance over all 8 languages, demonstrating the effectiveness of incorporating speech features directly into the LLM instead of employing text formula. Furthermore, when comparing the average Method Unseen Lang. Cascaded Zero-AVSR (Llama3.2-3B) Ara Deu Ell Spa Fra Ita Por Rus Ara Target Language (CER(%)) Ara Deu Ell Spa Fra Ita Por Rus 86.6 25.2 56.3 13.5 33.4 15.5 18.5 55.3 76.8 67.0 55.0 12.4 21.5 13.6 14.8 53. 68.5 25.5 75.2 13.9 29.2 15.3 14.8 58.8 73.6 25.0 53.8 34.8 20.3 18.8 16.1 53.9 76.8 24.7 56.7 15.5 82.2 14.7 17.6 54.3 73.7 25.2 54.2 12.7 26.4 36.7 14.8 55.9 74.5 25.7 54.1 17.1 21.4 14.4 63.0 55.2 68.6 24.6 55.8 14.1 22.2 16.8 14.6 79. 76.5 16.2 21.6 Deu 56.9 52.9 22.4 6.8 7.4 Zero-AVSR (Llama3.2-3B) Ell Spa Fra Ita Por Rus 53.9 16.1 62.1 6.8 57.8 16.7 24.4 19.7 55.3 16.0 21.2 61.6 17.2 22.6 58.0 16.3 24.7 57.7 16.0 22. 6.9 7.1 7.5 7.1 7.4 8. 8.1 8.5 54.6 8.1 7.7 7. 7.0 6.9 6.8 7.5 7.2 7. 7.7 7.8 8.6 7.9 25.1 7.7 18. 18.7 18.4 20.2 18.1 18.6 7. 6.9 44.0 18.8 7.6 45.4 Avg (w/o Eng) 37. 44.3 39.3 37.1 40.0 37.6 41. 38.1 19.7 26.4 24.3 19.0 20. 20.7 22.3 21.5 Table 7. The zero-shot language AVSR performances of Cascaded Zero-AVSR and Zero-AVSR using the same LLM (Llama3.2-3B) on MuAViC dataset. We train 8 AV-Romanizers, setting each language as an unseen language, and evaluate their zero-shot performance, which are shown in blue-colored cells. Figure 3. The performances of Cascaded Zero-AVSR and ZeroAVSR using audio-only (A) and audio-visual (AV) inputs under different SNR noise levels. CER, taking into account both seen and unseen languages, Zero-AVSR outperforms the Cascaded Zero-AVSR model. These results show the promise of Zero-AVSR that if better LLM is employed and finetuned, performance can be improved even more. 5.4. Noise-robustness Experiments By employing audio-visual speech inputs, we can achieve more robust noise performance in speech recognition compared to when we employ audio-only speech inputs. In this section, we analyze the performance of both Cascaded Zero-AVSR and Zero-AVSR by differing the acoustic noise levels from -5 dB SNR to 15 dB SNR. The acoustic noise is uniformly sampled among natural, babble, music, speech partitions from MUSAN [63]. The analysis results are 9 Figure 4. Examples of prediction results from the Cascaded ZeroAVSR on an unseen language, Japanese, on out-of-domain data. shown in Fig. 3. It shows the average WER for all 9 languages. In both Cascaded Zero-AVSR and Zero-AVSR, the audio-only (A) models performances are significantly degraded according to the noise become strong. However, we can confirm that the audio-visual (AV) models show robust performances over the different noise levels. 5.5. Zero-Shot Performance on Out-of-Domain We evaluate the extent to which the proposed Zero-AVSR framework can perform on out-of-domain data. To this end, we evaluate the zero-shot speech recognition performance on Japanese, whose language family is not presented in the training set of MARC. We measure the Japanese performance on the test set of CommonVoice [65] by using audioonly inputs. The Cascaded Zero-AVSR achieves 60.9% CER and the Zero-AVSR achieves 64.9% CER. These results demonstrate that the proposed Zero-AVSR framework can be employed for languages even when no data from the same language family is used, showing its scalability to more languages. The examples of prediction using the Cascaded Zero-AVSR are shown in Fig. 4. For example, as shown in the last row, the AV-Romanizer predicts the Roman text as asokoni kimera sanggatata mas, and the LLM de-romanizes this text into Japanese as あそこにきたら さんがたたます. Notably, despite not being perfect, the prediction was made without Japanese data being employed during the training of the proposed AV-Romanizer. 6. Detailed Information of MARC The MARC dataset is driven by combining the existing audio-visual speech datasets. Specifically, the labeled audio-visual speech datasets, LRS3 [32] and MuAViC [25], and the unlabeled audio-visual speech datasets, VoxCeleb2 [57] and AVSpeech [58], are combined. The information of each dataset is as follows: ing audio-visual speech data for 82 languages. Extensive experiments verified the effectiveness of Zero-AVSR. Lip Reading Sentences 3 (LRS3) [32] is dataset designed for AVSR and is one of the most widely used resources. It contains 433 hours of audio-visual English data with human-annotated transcriptions, sourced from TED and TEDx talks. Multilingual Audio-Visual Corpus (MuAViC) [25] is dataset for multilingual audio-visual speech recognition and translation, collected from TED and TEDx talks across nine languages and comprising 1,200 hours of data with humanannotated transcriptions. Since its English portion overlaps with LRS3 and the preprocessing differs due to different landmark detector, we exclusively use the English portion of LRS3 following the process in [9]. VoxCeleb2 [57] is dataset for speaker recognition containing 2,442 hours of multilingual audio-visual data. Although it includes speaker ID information, it lacks humanannotated text transcriptions and language labels. AVSpeech [58] is dataset aimed at isolating target speakers voice from mixed audio, sourced from YouTube videos and comprising 4,700 hours of multilingual audiovisual data. Like VoxCeleb2, it does not provide humanannotated text transcriptions or language labels. The unlabeled audio-visual datasets, VoxCeleb2 and AVSpeech, are labeled using language identification and ASR. For language identification, we use the MMS-LID1024 model2 with threshold of 0.95 for the confidence score. To generate language-specific graphemes, we utilize the pre-trained MMS-1B-ALL3 ASR model in conjunction with language adapter, which is selected based on the identified language. Furthermore, during the decoding stage, we leverage language-specific language models4. The resulting MARC dataset consists of 82 languages and approximately 2,916 hours of audio-visual data. The languages and their respective families [64] in the MARC dataset are listed in Tables 8 and 9. 7. Conclusion We presented zero-shot AVSR framework, Zero-AVSR, which extends language support beyond seen languages. The key idea behind this is 1) learning language-agnostic speech representations using Roman text, and 2) employing the LLMs to generate language-specific graphemes. To this end, we introduced the AV-Romanizer to convert audio-visual speech input into Roman text, which is then de-romanized into native scripts by LLMs. Depending on whether it involves fine-tuning an LLM, we explored two variants of frameworks, Cascaded Zero-AVSR and ZeroAVSR. Finally, we introduced the MARC dataset, compris2https://huggingface.co/facebook/mms-lid-1024 3https://huggingface.co/facebook/mms-1b-all 4https://huggingface.co/facebook/mms-cclms"
        },
        {
            "title": "References",
            "content": "[1] Triantafyllos Afouras, Joon Son Chung, Andrew Senior, Oriol Vinyals, and Andrew Zisserman. Deep audio-visual speech recognition. IEEE transactions on pattern analysis and machine intelligence, 44(12):87178727, 2018. 1, 2 [2] Stavros Petridis, Themos Stafylakis, Pingehuan Ma, Feipeng Cai, Georgios Tzimiropoulos, and Maja Pantic. End-toend audiovisual speech recognition. In 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 65486552. IEEE, 2018. 2 [3] Takaki Makino, Hank Liao, Yannis Assael, Brendan Shillingford, Basilio Garcia, Otavio Braga, and Olivier Siohan. Recurrent neural network transducer for audio-visual speech recognition. In 2019 IEEE automatic speech recognition and understanding workshop (ASRU), pages 905912. IEEE, 2019. [4] Pingchuan Ma, Stavros Petridis, and Maja Pantic. Endto-end audio-visual speech recognition with conformers. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 76137617. IEEE, 2021. 2, 6 [5] Sucheng Ren, Yong Du, Jianming Lv, Guoqiang Han, and Shengfeng He. Learning from the master: Distilling crossmodal advanced knowledge for lip reading. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1332513333, 2021. [6] Bowen Shi, Wei-Ning Hsu, Kushal Lakhotia, and Abdelrahman Mohamed. Learning audio-visual speech representation by masked multimodal cluster prediction. arXiv preprint arXiv:2201.02184, 2022. 2, 6 [7] Joanna Hong, Minsu Kim, Daehun Yoo, and Yong Man Ro. Visual context-driven audio feature enhancement for robust end-to-end audio-visual speech recognition. In Interspeech, 2022. [8] Dmitriy Serdyuk, Otavio Braga, and Olivier Siohan. Transformer-based video front-ends for audio-visual speech recognition for single and multi-person video. arXiv preprint arXiv:2201.10439, 2022. [9] Pingchuan Ma, Alexandros Haliassos, Adriana FernandezLopez, Honglie Chen, Stavros Petridis, and Maja Pantic. Auto-avsr: Audio-visual speech recognition with automatic In ICASSP 2023-2023 IEEE International Conferlabels. ence on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2023. 2, 3, 6, 10 [10] Joanna Hong, Minsu Kim, Jeongsoo Choi, and Yong Man Ro. Watch or listen: Robust audio-visual speech recognition with visual corruption modeling and reliability scoring. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1878318794, 2023. [11] Umberto Cappellazzo, Minsu Kim, Honglie Chen, Pingchuan Ma, Stavros Petridis, Daniele Falavigna, Alessio Brutti, and Maja Pantic. Large language models are strong audio-visual speech recognition learners. arXiv preprint arXiv:2409.12319, 2024. 2, 3, 5 10 [12] Andrew Rouditchenko, Yuan Gong, Samuel Thomas, Leonid Karlinsky, Hilde Kuehne, Rogerio Feris, and James Glass. Whisper-flamingo: Integrating visual features into whisper for audio-visual speech recognition and translation. arXiv preprint arXiv:2406.10082, 2024. 2 [13] Alexandros Haliassos, Rodrigo Mira, Honglie Chen, Zoe Landgraf, Stavros Petridis, and Maja Pantic. Unified speech recognition: single model for auditory, visual, and audioIn The Thirty-eighth Annual Conference on visual inputs. Neural Information Processing Systems, 2024. [14] Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang Chen, et al. Deep speech 2: End-to-end speech recognition in english and manIn International conference on machine learning, darin. pages 173182. PMLR, 2016. 1 [15] Suyoun Kim, Takaaki Hori, and Shinji Watanabe. Joint ctcattention based end-to-end speech recognition using multiIn 2017 IEEE international conference on task learning. acoustics, speech and signal processing (ICASSP), pages 48354839. IEEE, 2017. [16] Rohit Prabhavalkar, Takaaki Hori, Tara Sainath, Ralf Schluter, and Shinji Watanabe. End-to-end speech recognition: survey. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 32:325351, 2023. 1 [17] Pingchuan Ma, Stavros Petridis, and Maja Pantic. Visual speech recognition for multiple languages in the wild. Nature Machine Intelligence, 4(11):930939, 2022. 1 [18] Alexandros Haliassos, Pingchuan Ma, Rodrigo Mira, Stavros Petridis, and Maja Pantic. Jointly learning visual and auditory speech representations from raw data. arXiv preprint arXiv:2212.06246, 2022. 2 [19] Minsu Kim, Jeong Hun Yeo, Jeongsoo Choi, and Yong Man Ro. Lip reading for low-resource languages by learning and combining general speech knowledge and language-specific knowledge. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1535915371, 2023. [20] Minsu Kim, Jeonghun Yeo, Se Jin Park, Hyeongseop Rha, and Yong Man Ro. Efficient training for multilingual visual speech recognition: Pre-training with discretized visual speech representation. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 13111320, 2024. [21] Jeong Hun Yeo, Seunghee Han, Minsu Kim, and Yong Man Ro. Where visual speech meets language: Vsp-llm framework for efficient and context-aware visual speech processing. arXiv preprint arXiv:2402.15151, 2024. 2, 5 [22] Jeong Hun Yeo, Minsu Kim, Shinji Watanabe, and Yong Man Ro. Visual speech recognition for languages with limited labeled data using automatic labels from whisper. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1047110475. IEEE, 2024. 3 [23] Alexandros Haliassos, Andreas Zinonos, Rodrigo Mira, Stavros Petridis, and Maja Pantic. Braven: Improving selfsupervised pre-training for visual and auditory speech recogIn ICASSP 2024-2024 IEEE International Confernition. ence on Acoustics, Speech and Signal Processing (ICASSP), pages 1143111435. IEEE, 2024. 1 [24] Elizabeth Salesky, Matthew Wiesner, Jacob Bremerman, Roldano Cattoni, Matteo Negri, Marco Turchi, Douglas Oard, and Matt Post. tedx corpus arXiv preprint for speech recognition and translation. arXiv:2102.01757, 2021. 1,"
        },
        {
            "title": "The multilingual",
            "content": "[25] Mohamed Anwar, Bowen Shi, Vedanuj Goswami, WeiNing Hsu, Juan Pino, and Changhan Wang. Muavic: multilingual audio-visual corpus for robust speech recognition and robust speech-to-text translation. arXiv preprint arXiv:2303.00628, 2023. 1, 2, 3, 6, 7, 9, 10 [26] Andreas Zinonos, Alexandros Haliassos, Pingchuan Ma, Stavros Petridis, and Maja Pantic. Learning cross-lingual visual speech representations. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2023. 1 [27] HyoJung Han, Mohamed Anwar, Juan Pino, Wei-Ning Hsu, Marine Carpuat, Bowen Shi, and Changhan Wang. Xlavsr: Cross-lingual audio-visual speech representation learnarXiv preprint ing for noise-robust speech perception. arXiv:2403.14402, 2024. 2, 6 [28] Maxime Burchi, Krishna Puvvada, Jagadeesh Balam, Boris Ginsburg, and Radu Timofte. Multilingual audiovisual speech recognition with hybrid ctc/rnn-t fast conformer. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1021110215. IEEE, 2024. 1, 2, 3 [29] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 1, 3, 6, 7, 8 [30] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 3, 6, 7, [31] Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume LamarXiv preprint ple, Lucile Saulnier, et al. Mistral 7b. arXiv:2310.06825, 2023. 1, 8 [32] Triantafyllos Afouras, Joon Son Chung, and Andrew Zisserman. Lrs3-ted: large-scale dataset for visual speech recognition. arXiv preprint arXiv:1809.00496, 2018. 2, 3, 6, 9, 10 [33] Joon Son Chung and Andrew Zisserman. Lip reading in the In Computer VisionACCV 2016: 13th Asian Conwild. ference on Computer Vision, Taipei, Taiwan, November 2024, 2016, Revised Selected Papers, Part II 13, pages 87103. Springer, 2017. 2 [34] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. 2 [35] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et al. Conformer: Convolution11 augmented transformer for speech recognition. preprint arXiv:2005.08100, 2020. 2 arXiv [36] Ulf Hermjakob, Jonathan May, and Kevin Knight. Out-ofthe-box universal romanization tool uroman. In Proceedings of ACL 2018, system demonstrations, pages 1318, 2018. 3 [37] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech In Internarecognition via large-scale weak supervision. tional conference on machine learning, pages 2849228518. PMLR, 2023. 2 [38] Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, et al. Scaling speech technology to 1,000+ languages. Journal of Machine Learning Research, 25(97):152, 2024. 2, 3 [39] Da-Rong Liu, Kuan-Yu Chen, Hung-Yi Lee, and Lin-shan Lee. Completely unsupervised phoneme recognition by adversarially learning mapping relationships from audio embeddings. arXiv preprint arXiv:1804.00316, 2018. 2 [40] Kuan-Yu Chen, Che-Ping Tsai, Da-Rong Liu, Hung-Yi Lee, and Lin-shan Lee. Completely unsupervised speech recognition by generative adversarial network harmonized with iteratively refined hidden markov models. arXiv preprint arXiv:1904.04100, 2019. [41] Alexei Baevski, Wei-Ning Hsu, Alexis Conneau, and Michael Auli. Unsupervised speech recognition. Advances in Neural Information Processing Systems, 34, 2021. 2 [42] Xinjian Li, Florian Metze, David Mortensen, Alan Black, and Shinji Watanabe. Asr2k: Speech recognition for around 2000 languages without audio. arXiv preprint arXiv:2209.02842, 2022. 3, 4 [43] Xinjian Li, Siddharth Dalmia, Juncheng Li, Matthew Lee, Patrick Littell, Jiali Yao, Antonios Anastasopoulos, David Mortensen, Graham Neubig, Alan Black, et al. Universal phone recognition with multilingual allophone system. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 82498253. IEEE, 2020. 3 [44] Xinjian Li, Florian Metze, David Mortensen, Shinji Watanabe, and Alan Black. Zero-shot learning for grapheme to phoneme conversion with language ensemble. In Findings of the Association for Computational Linguistics: ACL 2022, pages 21062115, 2022. 3, 4 [45] Jinming Zhao, Vineel Pratap, and Michael Auli. Scaling simple approach to zero-shot speech recognition. arXiv preprint arXiv:2407.17852, 2024. 3, 4, 6 [46] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 3 [47] Kushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Abdelrahman Mohamed, et al. On generative spoken language modeling from raw audio. Transactions of the Association for Computational Linguistics, 9:13361354, 2021. [48] Siddique Latif, Moazzam Shoukat, Fahad Shamshad, Muhammad Usama, Yi Ren, Heriberto Cuayahuitl, Wenwu Wang, Xulong Zhang, Roberto Togneri, Erik Cambria, et al. Sparks of large audio models: survey and outlook. arXiv preprint arXiv:2308.12792, 2023. [49] Se Jin Park, Chae Won Kim, Hyeongseop Rha, Minsu Kim, Joanna Hong, Jeong Hun Yeo, and Yong Man Ro. Lets go real talk: Spoken dialogue model for face-to-face converIn Proceedings of the 62nd Annual Meeting of the sation. Association for Computational Linguistics (Volume 1: Long Papers), 2024. 3 [50] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 3, 6 [51] Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, and Chao Zhang. Salmonn: Towards generic hearing abilities for large language models. arXiv preprint arXiv:2310.13289, 2023. 3, 5 [52] Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou. Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models. arXiv preprint arXiv:2311.07919, 2023. 3, [53] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in neural information processing systems, 36:1008810115, 2023. 3, 6 [54] Tanja Schultz and Alex Waibel. Language-independent and language-adaptive acoustic modeling for speech recognition. Speech Communication, 35(1-2):3151, 2001. 3 [55] Ngoc Thang Vu, David Imseng, Daniel Povey, Petr Motlicek, Tanja Schultz, and Herve Bourlard. Multilingual deep neural network based acoustic modeling for rapid language adaptation. In 2014 IEEE international Conference on acoustics, speech and signal processing (ICASSP), pages 76397643. IEEE, 2014. [56] Minsu Kim, Jeongsoo Choi, Dahun Kim, and Yong Man Ro. Textless unit-to-unit training for many-to-many multilingual IEEE/ACM Transactions on speech-to-speech translation. Audio, Speech, and Language Processing, 2024. 3 [57] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. arXiv preprint Voxceleb2: Deep speaker recognition. arXiv:1806.05622, 2018. 3, 6, [58] Ariel Ephrat, Inbar Mosseri, Oran Lang, Tali Dekel, Kevin Wilson, Avinatan Hassidim, William Freeman, and Michael Rubinstein. Looking to listen at the cocktail party: speaker-independent audio-visual model for speech separation. arXiv preprint arXiv:1804.03619, 2018. 3, 6, 10 [59] Alex Graves, Santiago Fernandez, Faustino Gomez, and Jurgen Schmidhuber. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In Proceedings of the 23rd international conference on Machine learning, pages 369376, 2006. 4 [60] CHEN CHEN, Ruizhe Li, Yuchen Hu, Sabato Marco Siniscalchi, Pin-Yu Chen, EngSiong Chng, and Chao-Han Huck Yang. Its never too late: Fusing acoustic information into large language models for automatic speech recognition. In 12 The Twelfth International Conference on Learning Representations, 2024. 5 [61] Yuchen Hu, CHEN CHEN, Chao-Han Huck Yang, Ruizhe Li, Chao Zhang, Pin-Yu Chen, and EngSiong Chng. Large language models are efficient learners of noise-robust speech In The Twelfth International Conference on recognition. Learning Representations, 2024. 5 [62] Wei-Ning Hsu and Bowen Shi. u-hubert: Unified mixedmodal speech pretraining and zero-shot transfer to unlabeled modality. Advances in Neural Information Processing Systems, 35:2115721170, 2022. [63] David Snyder, Guoguo Chen, and Daniel Povey. Musan: music, speech, and noise corpus. arXiv preprint arXiv:1510.08484, 2015. 6, 9 [64] Kenneth Katzner and Kirk Miller. The languages of the world. Routledge, 2002. 8, 10 [65] Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler, Josh Meyer, Reuben Morais, Lindsay Saunders, Francis Tyers, and Gregor Weber. Common voice: massively-multilingual speech corpus. arXiv preprint arXiv:1912.06670, 2019."
        },
        {
            "title": "Romance",
            "content": "-"
        },
        {
            "title": "Hellenic",
            "content": "- Indo-European"
        },
        {
            "title": "Iranian",
            "content": "Indo-Iranian"
        },
        {
            "title": "Indic",
            "content": "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45"
        },
        {
            "title": "Hindi\nUrdu\nBengali\nPunjabi\nMarathi\nGujarati\nAssamese\nNepali\nSindhi\nOdia",
            "content": "eng deu nld afr ltz swe dan nob isl ita fra spa por ron cat glg ast oci cym gle ell rus ukr bel pol ces slk bul slv mkd bos hrv srp fas ckb tgk pus hin urd ben pan mar guj asm npi snd ory Table 8. The Data Statistics of the MARC dataset 1. 14 435.2 327.5 72.5 1.7 1.9 19.3 18.1 2.8 0.4 146.8 291.7 216.2 408.0 16.0 7.0 8.7 0.1 1.5 97.1 0. 21.5 123.8 3.8 5.9 50.5 20.6 4.9 3.9 4.6 0.3 0.8 2.6 0.9 7.6 0.05 0.1 0.3 99.0 8.6 8.8 3.0 8.2 1.6 0.2 4.5 0.5 0."
        },
        {
            "title": "Subgroup",
            "content": "Indo-European"
        },
        {
            "title": "Baltic",
            "content": "-"
        },
        {
            "title": "Uralic",
            "content": "Finno-Ugric"
        },
        {
            "title": "Independent",
            "content": "Mon-Khmer - - - - - -"
        },
        {
            "title": "Atlantic",
            "content": "Niger-Congo Benue-Congo Branch - - -"
        },
        {
            "title": "Southeastern",
            "content": "- - - - - - - - - - - - - - - - - Afro-Asiatic"
        },
        {
            "title": "Ethiopic",
            "content": "- - -"
        },
        {
            "title": "Number Language Name Language Code Video Hours",
            "content": "48 49 50 51 52 53 54 55 56 58 59 60 61 62 63 64 65 67 68 69 70 71 72 73 74 75 76 77 79 80"
        },
        {
            "title": "Hausa",
            "content": "lit lav hye fin est hun tur aze kaz kir uzb mon kat tel tam kan mal kor vie ind jav tgl mri wol swh lin lug sna ara mlt heb amh som orm hau 2.9 1.3 0.7 9.7 2. 11.0 50.6 1.9 3.8 0.1 0.3 1.1 1. 18.8 18.3 3.6 15.5 128.6 32.9 15.0 0.1 6.1 4.9 1. 1.2 0.9 0.04 3.8 96.7 1.5 14.3 1.2 4.6 0.1 0. Table 9. The Data Statistics of the MARC dataset 2."
        }
    ],
    "affiliations": [
        "Imperial College London",
        "KAIST"
    ]
}